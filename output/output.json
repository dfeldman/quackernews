[
  {
    "title": "WebGPU-Based WiFi Simulator (wifi-solver.com)",
    "points": 164,
    "submitter": "jasmcole",
    "submit_time": "2024-10-20T18:01:43.000000Z",
    "num_comments": 57,
    "comments_url": "https://news.ycombinator.com/item?id=41897214",
    "comments": [
      "Cool visualization but it focuses so much on the ``'-.,_,.-'`waves`'-.,_,.='`` and not on the actual coverage pattern of 6-12.5cm waves, so not as actually useful for showing coverage as other tools, but fun as an art project.And of course, I can't mention that without shouting out projectM (open-source Milkdrop) that supports WebGL https://github.com/projectM-visualizer/projectm/blob/master/... and one of the OGs, Geisswerks https://www.geisswerks.com/If you like that, you might also love NestDrop for music visualization tailored for VJs and with special features to support projecting inside domed surfaces https://nestimmersion.ca/nestdrop.php\n \nreply",
      "Hey, if you click through to one of the example simulations you'll be able to change the visualization from waves to time-averaged power density, which should be closer to what you're looking for.\n \nreply",
      "> For example, as you scroll you'll notice that the waves are refracting around these text boxes.Now, that's useless AND badass.\n \nreply",
      "Definitely the vibe I'm going for!\n \nreply",
      "Bugs:#1 if you spam the \"add a new source\" button you eventually get a JavaScript exception logged to the screen due to an array with a fixed max size of 128 elements overflowing.#2 this could be graphics card or driver specific (I have an AMD card), but scrolling just right can can break the simulation due the the text boxes; for example by quickly paging up and down, or scrolling all the way to the bottom and then wiggling the scroll position up and down. Once this happens the bad data propagates until the entire thing is filled with noise, solid black, or solid white. If you then scroll up to 3D mode the screen will be filled with a mess of polygons.\n \nreply",
      "I did get #2 on an Apple M1, so probably not video card specific.\n \nreply",
      "What are the system requirements to run this? Fairly 'standard' Linux system running Chrome here.\"Sorry, there was an error starting the simulationSorry, WebGPU is not supported on our deviceWiFi Solver may not be compatible with your device.\"\n \nreply",
      "To run on Chromium Linux you need to enable \"enable-unsafe-webgpu\" as well as \"enable-vulkan\" flags in \"chrome://flags\". Best to disable again afterwards.\n \nreply",
      "Dev tools gave me this link: https://github.com/gpuweb/gpuweb/wiki/Implementation-Status#...Barely any Firefox support either.Looks like I'll skip this one.\n \nreply",
      "No Linux support for WebGPU in Chrome yet: https://caniuse.com/webgpu\n \nreply"
    ],
    "link": "https://wifi-solver.com",
    "first_paragraph": ""
  },
  {
    "title": "A Console-Friendly Pastebin with binary support (c-net.org)",
    "points": 27,
    "submitter": "goranmoomin",
    "submit_time": "2024-10-20T23:08:42.000000Z",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=41899245",
    "comments": [
      "> This is a console friendly pastebin that allows binary files.\nNo fancy website, no intermediate pages to click through, and no CAPTCHAs.The site is cool, but is it not just going to be abused?\n \nreply",
      "It very likely will be, yes.I would personally suggest that this site probably \"wants\" accounts. Yes, with CAPTCHAs (on registration.) If you want to be able to ban people who abuse your service, you'll need some thing-that-is-costly-to-get-multiple-of to ban them by. Otherwise they just keep coming back.To still be a \"console-friendly pastebin\", the result of doing that costly registration process, could just be a page that gives you a (private) URL, that works like the base URL does now. https://paste.c-net.org/b/{bucket} or something, where {bucket} is a UUIDv4, or anything else with enough entropy to not be able to brute-force enumerate your way into someone else's account URL.The uploaded files themselves could still have short human-writable top-level paths, for ease of repeating them over the phone.Though, I notice that when you upload a file, you get a \"delete key\" as well as a URL. IMHO the \"delete key\" shouldn't be a weird nonstandard header you send with an HTTP DELETE; it should just be a URL \u2014 e.g. https://paste.c-net.org/b/{bucket}/{delete_key} \u2014 that you can HTTP DELETE directly.In other words, make /b/{bucket}/{delete_key} the file's \"true name\", and /{link} a \"read-only view\" of the file.\n \nreply",
      "I like it, but this could do with being just the slightest bit more specific:> Don't break the law, don't post illegal shit, don't be an asshole.The law in which country? All countries? Do I have to avoid uploading depictions of Mohammed, or insulting statements about the president of Turkiye?\n \nreply",
      "This rules. Hope it stays up.\n \nreply",
      "https://paste.c-net.org/ImproperAttacked\n \nreply",
      "https://paste.c-net.org/HanukkahDisplays\n \nreply"
    ],
    "link": "https://paste.c-net.org/",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: HN Update \u2013 Hourly News Broadcast of Top HN Stories (hnup.date)",
    "points": 124,
    "submitter": "yunusabd",
    "submit_time": "2024-10-20T07:10:31.000000Z",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=41893524",
    "comments": [
      "Brilliant idea. I think this has real value as well: as I get older, I find that I have less energy for reading, but I also notice I often miss really cool stuff that was briefly on the front of HN.I like that it summarizes the comments too. There are often real gems buried in there. (I assume you're only taking a few top ones?)I think a great improvement could be made with personalization. Most of what's on the front page isn't personally relevant to me, and there's a lot of cool stuff on the new submissions page that never catches on. So it would be nice if a system could learn what kind of stories I personally respond to, and show me (a summary of?) those -- even if they aren't currently trending.Last.fm came out 20 years ago (proving you don't even need AI for amazing recommender systems), but it seems personalizing your experience never really caught on. (Yeah, the YouTube algorithm kind of does this, but you unfortunately have no real control over it.)\n \nreply",
      "Thanks for the comment, I'm really enjoying the discussion it has sparked.Yes, I'm just taking the top comments, along with a few child comments, in order to not exceed the context window of the model.Regarding personalization, there's definitely lots of potential. HN can be so random though, sometimes you find things that you didn't even know you needed (intellectually). I guess as with most recommender systems, it's about a balance between exploration and exploitation. \nMaybe an MVP could filter for specific keywords and add those posts to the model input.\n \nreply",
      "agreed on energy for reading. do you think it\u2019s that we\u2019re getting older or that the friction associated with consuming information is just getting lower and lower over time?\n \nreply",
      "So my energy levels have declined noticeably from age 20 to 30. I thought it was mostly my own chronic health issues causing accelerated aging, but many of my friends are making similar complaints.I did notice far before this point (e.g. age 10 to 20) that my patience for reading had gone down significantly. If I had to guess I'd say that in my case it's due to an underlying anxiety that started in early teens and never left me. Drowning it out seems to require something more stimulating than reading (on paper).(Perhaps meditation or therapy (shadow integration?) would help here. I've certainly had glimpses of inner peace during times when I was meditating regularly. One insight from this time: \"holy crap, I always thought I needed to struggle harder, but it turns out I just needed to learn how to relax...\")I can get through audiobooks but I can only consume them while traveling or doing chores. If I'm sitting down, the restlessness is too high and I can't concentrate on the book.I also have ADHD and heard similar things from others with ADHD, so I'm not sure to what degree this translates to others.\n \nreply",
      "When it comes to online reading, there are quite a few things that cause me fatigue that I don't feel I used to experience. Advertisements have been there for a long time, but often these are woven into the content, either as literal text placed in the article, or as visual ads that you need to scroll through to continue reading the article. Relying on different JavaScript and CSS techniques to \"enhance\" the user experience often cause me issues when I'm just trying to focus on reading. Those include overriding scrollbars, dynamic loading of content when the text is small enough to have been included in the page, and displaying some kind of alternative action when highlighting text. I'll often highlight text to keep track of where I'm reading, and some sites will pop up a dialog with share actions, or the ability to add annotations, etc. This is distracting and makes it more difficult to follow along with a longer article.I'm 45, and got my start on BBS pre-internet, but I feel like if I find an article without the distractions I mentioned above, I actually have more energy to complete an article than I did in my 20's and 30's. Having access to the article without distractions helps me to focus, and when I focus, I tend to consume more content than I normally would. Most likely one of the reasons we're drawn to HN.\n \nreply",
      "You've just made me realize why I usually avoid clicking the actual article link on HN. It's usually a very unpleasant experience, unless it's clear that it's a smaller website.\n \nreply",
      "Did anyone notice it reporting about itself now? This will definitely go into the archive :)\n \nreply",
      "This is awesome, I wish it did a rolling 8 hour instead that is updated once a hour if that makes sense.\n \nreply",
      "Love it. Reminds me of the also useful Hacker News Recap from wondercraft but it looks like that stopped updating as of October 1st (https://www.wondercraft.ai/our-podcasts/hacker-news).Would be great to have a playback speed button as well. (I can't sit through any audio at 1x.)\n \nreply",
      "Same boat re: audio speed. I actually speed up the voice in the backend by 1.16x . Above that I was getting too many artifacts in the audio. The nice thing about doing it at that point is that I can handle the music and the voice separately, i.e. the speed of the music stays unchanged.Speeding it up in the player will also speed up the music, which is not very zen. But I guess I'll just add it to the player and let people decide how fast they want to go.\n \nreply"
    ],
    "link": "https://hnup.date/",
    "first_paragraph": ""
  },
  {
    "title": "Drasi: Microsoft's open source data processing platform for event-driven systems (github.com/drasi-project)",
    "points": 186,
    "submitter": "benocodes",
    "submit_time": "2024-10-20T16:07:43.000000Z",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=41896297",
    "comments": [
      "Very interesting choice of using Cypher[0]In 2014, we built a similar type event-driven system (but specifically for document distribution (a document can be distributed to a target set of entities; if a new entity is added, we need to resolve which distributions match)) and also ended up using Cypher via Neo4j (because of the complex taxonomical structure of how we mapped entities).It is a super underrated query language and while most of the queries could also be translated to relational SQL, Cypher's linear construction using WITH clauses is far, far easier to reason about, IMO.EDIT: feel like the devs went overboard with the mix of languages.  Shoehorned in C# Blazor?  Using JS and Jest for e2e testing?[0] https://drasi.io/reference/query-language/\n \nreply",
      "> while most of the queries could also be translated to relational SQL, Cypher's linear construction using WITH clauses is far, far easier to reason about, IMO.https://prql-lang.org/\n \nreply",
      "Didn't look too deeply, but one of the keys with Cypher (at least in the context of graph databases) is that it has a nice way of representing `JOIN` operations as graph traversals.    MATCH (p:Person)-[r]-(c:Company) RETURN p.Name, c.Name\n\nWhere `r` can represent any relationship (AKA `JOIN`) between the two collections `Person` and `Company` such as `WORKS_AT`, `EMPLOYED_BY`, `CONTRACTOR_FOR`, etc.So I'd say that linear queries are one of the things I like about Cypher, but the clean abstraction of complex `JOIN` operations is another huge one.\n \nreply",
      "We made a health backend partly using Cypher and the only thing I found was the simple queries looked amazing, but as soon as you need to join non-linearly it started looking a lot like SQL again. And when you're using an ORM it stops mattering. And when you need migrations it gets painful!\n \nreply",
      "> but as soon as you need to join non-linearly\n\nAt least in our use case, even with some very gnarly 20+ line Cypher queries, it never got to the point where it felt like SQL and certainly, those same queries would be even gnarlier as nested sub-selects, CTEs, or recursive selects, IMO.Perhaps a characteristic of our model (a taxonomy of Region, Country, Sponsor, Program, Trial, Site, Staff for global clinical trials and documents required by Region/Country/Program/Trial).\n \nreply",
      "I too have great memories of cypher. Such an elegant way to write queries.\n \nreply",
      "If you haven't been following it, I recently found out that it is now supported in a limited capacity by Google Spanner[0].  The openCypher initiative started a few years back and it looks like it's evolved into the (unfortunate moniker) GQL[1].So it may be the case that we'll see more Cypher out in the wild.[0] https://cloud.google.com/spanner/docs/graph/opencypher-refer...[1] https://neo4j.com/blog/cypher-gql-world/\n \nreply",
      "Looks very Azure-centric. Both installation guides (https://drasi.io/how-to-guides/install-sample-applications/b... and https://drasi.io/how-to-guides/install-sample-applications/c...) require Azure to work.And then there's this:>  Installing Drasi in an EKS cluster can be significantly more complex than a standard installation on other platforms. Instead of downloading a CLI binary using the provided installation scripts, this approach requires modifying the source code of the Drasi CLI and building a local version of the CLI.Is this an actual requirement or just the current easy path?\n \nreply",
      "Azure SRE here, it doesn't appear to have any Azure dependencies. CLI rebuild seems to be that \"drasi init\" assumes Azure Kubernetes Service built in StorageClasses for Kubernetes PVC for Redis and Mongo and thus fails when running against EKS. I assume same thing would be required on GKE. Yes, it should be more modular but MVP.As for other stuff, it's using Gremlin Query Language or Postgres which are both open. In fact, it's going out of way it's not to use Azure authenication as loading connection string as Kubernetes secret is 100% AGAINST Azure Kubernetes Best Practice. Best Practice would be Workload Identity.\n \nreply",
      "> CLI rebuild seems to be that \"drasi init\" assumes Azure Kubernetes Service built in StorageClasses for Kubernetes PVC for Redis and Mongo and thus fails when running against EKS. I assume same thing would be required on GKE. Yes, it should be more modular but MVP.None of these words are in the Bible.\n \nreply"
    ],
    "link": "https://github.com/drasi-project/drasi-platform",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          Drasi is a data processing platform that simplifies detecting changes in data and taking immediate action. It is a comprehensive solution that provides built-in capabilities to track system logs and change feeds for specific events, evaluate them for relevance, and automatically initiate appropriate reactions. Visit our documentation site at https://drasi.io for detailed information.Drasi provides real-time actionable insights without the overhead of traditional data processing methods. It tracks system changes and events without the need to copy data to a central data lake or repeatedly query data sources. Drasi uses queries to continuously evaluate incoming data changes. When the changes match the criteria and conditions specified in these queries the result sets of these queries are updated. These updates then trigger context-aware reacti"
  },
  {
    "title": "Kurt Vonnegut's lost board game published (polygon.com)",
    "points": 161,
    "submitter": "musha68k",
    "submit_time": "2024-10-20T16:44:55.000000Z",
    "num_comments": 41,
    "comments_url": "https://news.ycombinator.com/item?id=41896636",
    "comments": [
      "BGG entry: https://boardgamegeek.com/boardgame/422478/ghqHow-to play video: https://www.youtube.com/watch?v=zfXPIhvFPjwSpace-Biff review: https://spacebiff.com/2024/09/25/ghq/\n \nreply",
      "All credit for this go to Geoff and his efforts to bring this to life. He is a well-known game designer with dozens of published games and founder of the podcast Ludology (no longer an active host after 100s of episodes) that posits that Games are worthy of study.He is also a co-founder of the TTGDA (https://www.ttgda.org/) that aims to be a guild like resource for designers. It is his connections that got this into Barnes & Nobles. Also of note, the TTGDA has recently convinced B&N to list game designers on all detail pages and search results in the same way they do today for books and writers. He also runs a free newsletter called GameTek (https://gametek.substack.com/) that is a continuation of an old podcast format he did where he does deep dives on specific games and game concepts. In short, he's awesome.\n \nreply",
      "Thank you.\n \nreply",
      "The path to getting this game published was not easy.  At one time, Barnes and Noble rejected the idea of publishing and selling the game because \u201cNot enough people know who Kurt Vonnegut is\u201d [1].[1].  https://web.archive.org/web/20211201220314/https://www.getre...\n \nreply",
      "And now his board game is sold in Barnes and Noble.\n \nreply",
      "For any Vonnegut fans who find themselves in Indianapolis, I recommend checking out the Kurt Vonnegut Museum and Library: https://www.vonnegutlibrary.org/When I visited for the first time this year, I learned about GHQ and the upcoming release\n \nreply",
      "Related NYT piece https://www.nytimes.com/2024/10/03/crosswords/kurt-vonnegut-...https://archive.ph/t3CBZ\n \nreply",
      "Reminds me of Memoir '44.https://www.daysofwonder.com/memoir-44/\n \nreply",
      "Other than the theme the two have very little in common. Memoir 44 is card-driven and uses dice for combat resolution so there is a lot of randomness. GHQ is a pure abstract with no randomness at all. On the gameplay side GHQ is much closer to chess than to M44.\n \nreply",
      "Is the game fun?\n \nreply"
    ],
    "link": "https://www.polygon.com/board-games/467103/kurt-vonnegut-ghq-lost-board-game-publisher-interview",
    "first_paragraph": "It could have been a contemporary of Risk, Diplomacy, and other legendary wargamesby  Charlie HallIf you buy something from a Polygon link, Vox Media may earn a commission. See our ethics statement.Fans of literature most likely know Kurt Vonnegut for the novel Slaughterhouse-Five. The staunchly anti-war book first resonated with readers during the Vietnam War era, later becoming a staple in high school curricula the world over. When Vonnegut died in 2007 at the age of 84, he was widely recognized as one of the greatest American novelists of all time. But would you believe that he was also an accomplished game designer?In 1956, following the lukewarm reception of his first novel, Player Piano, Vonnegut was one of the 16 million other World War II veterans struggling to put food on the table. His moneymaking solution at the time was a board game called GHQ, which leveraged his understanding of modern combined arms warfare and distilled it into a simple game played on an eight-by-eight g"
  },
  {
    "title": "Show HN: Create mind maps to learn new things using AI (github.com/aotakeda)",
    "points": 77,
    "submitter": "arthurtakeda",
    "submit_time": "2024-10-20T20:01:19.000000Z",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=41898076",
    "comments": [
      "Very interesting!My first thought when seeing this is, could I use this as a \"progress map\" for a subject I'm learning? So add my own notes, and use AI to find and recommend more resources?My second thought is, can you build one of these for everything I've ever learned, and want to learn?I've long (15 years?) been waiting for a system that knows not only my interests, but my knowledge, and can use that data to find or generate the optimal learning experience for any subject.(Khan Academy used to have a big interconnected graph of how all the knowledge on their platform fit together (dependencies) but for some reason they removed it...)AI is getting pretty close, especially now that they've rolled out memory and conversations... wild times we live in!\n \nreply",
      "@andai check out https://www.perplexity.ai/spaces\nits _kind of_ what you are describing.. it's UX is unstructured compared to a mind-map or timeline. But we are starting to see the nascent stages of where all this is going. exciting times indeed.\n \nreply",
      "that's a very interesting use case, could be the long-term vision for the project, thanks for sharing!\n \nreply",
      "I'd say the README should have a pic of the results otherwise I have to install it and run it to see if I want to install it and run itAlso why not host it online and let users bring their own keys?\n \nreply",
      "just updated the readme with the video: https://www.youtube.com/watch?v=Y-9He-tG3aMI considered that but if I were the user I'd be wary of adding my own keys to a random person's website haha, but now that you mentioned that, since the code it's open-source I guess it's fine, thanks for the feedback!\n \nreply",
      "Thanks for that! You can use something like gifski to turn that video into a gif so that you can embed it into the README. Here's an example from the gifski repo: https://github.com/ImageOptim/gifskiYou can use the CLI version but they also have executables with a dead simple GUI if you're so inclined. I have only ever used the GUI and it's perfect on a Mac (just drag and drop your video into it). Not sure if it's the exact same on Windows but I imagine it's amazing there too\n \nreply",
      "Nice! Will replace the screenshot with a gif, if that doesn\u2019t work for me I guess ffmpeg may be able do that too, thanks!\n \nreply",
      "Ffmpeg can output a gif.  The only difficult part might be figuring out which options you need to get the quality you want.\n \nreply",
      "That\u2019s cool! It would be great if you could easily expand each subtopic into further sub-subtopics.Was there anything particularly interesting about how you built it or the prompts needed to get decent results?\n \nreply",
      "I noticed that, at least with the models I tested (gpt 3.5, 4o and llama 3.1 8b), to get a response with just the JSON and then have it follow the exact structure so it correctly renders the topic and subtopics was the hardest part.Ended up having to prompt I think twice (at the beginning and the end) so it finally followed the exact JSON structure.\n \nreply"
    ],
    "link": "https://github.com/aotakeda/learn-thing",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Create mind maps to learn new things using AI.\n      This is a simple Next.js project that implements a mind map visualization tool using React Flow.Watch a demo of it in action here or check out the gif below.The UI is built using shadcn and some components from Magic UI.It allows users to view and interact with mind maps, and download the mind map data as a markdown file.The mind map data is generated using either local models from Ollama or external models like OpenAI and leveraging AI SDK.Install all dependencies:Copy the .env.template file to .env.local and specify which model (local or external) you want to use by setting the NEXT_PUBLIC_USE_LOCAL_MODELS environment variable to true or false.When running an OpenAI model, you must specify your OpenAI API key in the .env.local file.Inside the route.ts file, you must specify the "
  },
  {
    "title": "Mosaic REALMAP: Explore Prague in detail with 1.26M images (mosaic51.com)",
    "points": 71,
    "submitter": "programd",
    "submit_time": "2024-10-18T18:38:39.000000Z",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=41882226",
    "comments": [
      "There are many YouTube channels walking through cities as the movement and sound add more to the atmosphere than street-view like experiences. I also happen to cover Prague many times like [1] in 2020 without many tourists.One thing I started a few years is multicam walks. This means that I carry some 4-6 Osmo Pockets to actually film in several directions simultaneaously - example: Dresden [2].The editing process is taking multiple hours so I have hundreds of unreleased recordings. If someone has a hint on AI assisted multicam editing, where to start (like better gaze prediction etc. from VR) I'd be interested - especially feeding all camera streams and let a NN decide the best camera angle and cuts to be between 4-11s?[1] https://www.youtube.com/watch?v=J7HIfklyF9w[2] https://www.youtube.com/watch?v=ulzq8sorvR4\n \nreply",
      "Is it possible to stitch your footage into a 360 projection? If so, you could definitely upload it as either 180 or 360 degree video, which would allow for VR use cases as well.\n \nreply",
      "So how do we explore? Is there a link somewhere in the article?\n \nreply",
      "After I posted the item I realized that there was no link and it's one of those \"email us for access to the data\" deals. Which I think is a mistake on their part. Nevertheless this is a great project and I wish there was more of this kind of work out there.Maybe it's time to start an open source effort to do something similar in some corner of the world? You'd need access to the mapping camera hardware, obviously. Who would be willing to fund it?\n \nreply",
      "Hate that I can't put this on wiki commons because of the non-commercial license.\n \nreply",
      "Especially without link in the article\n \nreply",
      "crazy! wont be too long until this is here for major cities as well.\n \nreply",
      "Prague is the capital of Czech Republic and has over 1.3 million inhabitants, so I'd say it qualifies as a \"major city\"\n \nreply",
      "Ray Holmberg ruined the idea of Prague for me\n \nreply",
      "He probably ruined the idea of a lot more places before his first trip to Prague in 2011. Politicians really ought to retire from politics far earlier than before their 45th year in any sort of assembly.\n \nreply"
    ],
    "link": "https://www.mosaic51.com/featured/mosaic-realmap-explore-prague-in-unprecedented-detail-with-1-26-million-images/",
    "first_paragraph": ""
  },
  {
    "title": "How to do distributed locking (2016) (kleppmann.com)",
    "points": 179,
    "submitter": "yusufaytas",
    "submit_time": "2024-10-20T10:38:17.000000Z",
    "num_comments": 82,
    "comments_url": "https://news.ycombinator.com/item?id=41894451",
    "comments": [
      "At work we use Temporal and ended up using a dedicated workflow and signals to do distributed locking. Working well so far and the implementation is rather simple, relying on Temporal\u2019s facilities to do the distributed parts of the lock.\n \nreply",
      "I'm keen to use Temporal, but I've heard it can be flaky. In your experience has it worked well?\n \nreply",
      "Rock solid in my experience and kind of a game changer. I\u2019m surprised it\u2019s not more widespread in large orgs.\n \nreply",
      "We use it a ton at my shop for internal things like release rollouts.  Fairly big tech company, and same experience. It's an excellent product.\n \nreply",
      "I suggest reading the comment I left back then in this blog post comments section, and the reply I wrote in my blog.Btw, things to note in random order:1. Check my comment under this blog post. The author had missed a fundamental point in how the algorithm works. Then he based the refusal of the algorithm on the remaining weaker points.2. It is not true that you can't wait an approximately correct amount of time, with modern computers an APIs. GC pauses are bound and monotonic clocks work. These are acceptable assumptions.3. To critique the auto release mechanism in-se, because you don't want to expose yourself to the fact that there is a potential race, is one thing. To critique the algorithm in front of its goals and its system model is another thing.4. Over the years Redlock was used in a huge amount of use cases with success, because if you pick a timeout which is much larger than: A) the time to complete the task. B) the random pauses you can have in normal operating systems. Race conditions are very hard to trigger, and the other failures in the article were, AFAIK, never been observed. Of course if you have a super small timeout to auto release the lock, and the task may easily take this amount of time, you just committed a deisgn error, but that's not about Redlock.\n \nreply",
      "To be honest I've long been puzzled by your response blog post. Maybe the following question can help achieve common ground:Would you use RedLock in a situation where the timeout is fairly short (1-2 seconds maybe), the work done usually takes ~90% of that timeout, and the work you do while holding a RedLock lock MUST NOT be done concurrently with another lock holder?I think the correct answer here is always \"No\" because the risk of the lease sometimes expiring before the client has finished its work is very high. You must alter your work to be idempotent because RedLock cannot guarantee mutual exclusion under all circumstances. Optimistic locking is a good way to implement this type of thing while the work done is idempotent.\n \nreply",
      ">because the risk of the lease sometimes expiring before the client has finished its work is very highWe had corrupted data bacause of  this.\n \nreply",
      "The timeout must be much larger than the time required to do the work. The point is that distributed locks without a release mechanism are in practical terms very problematic.Btw, things to note in random order:1. Check my comment under this blog post. The author had missed a fundamental point in how the algorithm works. Then he based the refusal of the algorithm on the remaining weaker points.2. It is not true that you can't wait an approximately correct amount of time, with modern computers an APIs. GC pauses are bound and monotonic clocks work. These are acceptable assumptions.3. To critique the auto release mechanism in-se, because you don't want to expose yourself to the fact that there is a potential race, is one thing. To critique the algorithm in front of its goals and its system model is another thing.4. Over the years Redlock was used in a huge amount of use cases with success, because if you pick a timeout which is much larger than: A) the time to complete the task. B) the random pauses you can have in normal operating systems. Race conditions are very hard to trigger, and the other failures in the article were, AFAIK, never been observed. Of course if you have a super small timeout to auto release the lock, and the task may easily take this amount of time, you just committed a deisgn error, but that's not about Redlock.\n \nreply",
      "Locking without a timeout is indeed in the majority of use-cases a non-starter, we are agreed there.The critical point that users must understand is that it is impossible to guarantee that the RedLock client never holds its lease longer than the timeout. Compounding this problem is that the longer you make your timeout to minimize the likelihood of this from accidentally happening, the less responsive your system becomes during genuine client misbehaviour.\n \nreply",
      "In most real world scenarios, the tradeoffs are a bit softer than what people in the formal world dictates (and doing so they forced certain systems to become suboptimal for everything but during failures, kicking them out of business...). Few examples:1. E-commerce system where there are a limited amount of items of the same kind, you don't want to oversell.2. Hotel booking system where we don't want to reserve the same dates/rooms multiple times.3. Online medical appointments system.In all those systems, to re-open the item/date/... after some time it's ok, even after one day. And if the lock hold time is not too big, but a very strict compromise (it's also a reasonable choice in the spectrum), and it could happen that during edge case failures three items are sold and there are two, orders can be cancelled.So yes, there is a tension between timeout, race condition, recovery time, but in many systems using something like RedLock the development and end-user experience can be both improved with a high rate of success, and the random unhappy event can be handled. Now the algorithm is very old, still used by many implementations, and as we are talking problems are solved in a straightforward way with very good performances. Of course, the developers of the solution should be aware that there are tradeoffs between certain values: but when are distributed systems easy?P.S. why 10 years of strong usage count, in the face of a blog post telling that you can't trust a system like that? Because even if DS issues emerge randomly and sporadically, in the long run systems that create real-world issues, if they reach mass usage, are known. A big enough user base is a continuous integration test big enough to detect when a solution has real world serious issues. So of course RedLock users picking short timeouts with tasks that take a very hard to predict amount of time, will indeed incur into knonw issues. But the other systemic failure modes described in the blog post are never mentioned by users AFAIK.\n \nreply"
    ],
    "link": "https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html",
    "first_paragraph": ""
  },
  {
    "title": "Sampling with SQL (moertel.com)",
    "points": 54,
    "submitter": "thunderbong",
    "submit_time": "2024-10-20T10:58:12.000000Z",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=41894528",
    "comments": [
      "If you need to \"extract a small, sample dataset from a larger PostgreSQL database while maintaining referential integrity\" check https://github.com/mla/pg_sample\n \nreply",
      "Hey! I'm tickled to see this on HN. I'm the author. If you have any questions, just ask. I'll do my best to answer them here.\n \nreply",
      "Is there something in the SQL standard that says functions are guaranteed to executed more than once?I swear that once I used something like random() and it was only executed once, making it useless for the task at hand.  I had to use some trick to ensure it was executed for each row.I may have used it in the `select` part.  Dialect was Oracle's, from memory.related: https://xkcd.com/221/\n \nreply",
      "Postgres makes a distinction between IMMUTABLE, STABLE, and VOLATILE functions, with volatile functions being functions that - with the same arguments - can produce different results even within the same statement. Therefore VOLATILE functions will always be executed once per call.I'm not sure if this is part of the ANSI SQL standard.\n \nreply",
      "It depends on the function and the SQL implementation, you can see in this simulator that where rand() > rand() evaluates row by row in MySQL but once in SQL Server, so its easy to get this stuff messed up even if the code is \"equivalent\" its really not.https://onecompiler.com/mysql/42vq8s23b\nhttps://onecompiler.com/sqlserver/42vq8tz24\n \nreply",
      "Thanks, that's a bit upsetting :-)\n \nreply",
      "Indeed.On systems with unfortunate evaluation semantics for `RAND`, you can generate fresh random values for each row by creating a function for that purpose and calling it on the primary key of each row. I provide one example in the article at:https://blog.moertel.com/posts/2024-08-23-sampling-with-sql....I'll include a copy here because it's short. It's for DuckDB and was created to let us generate a controllable number of fresh random values for each row:    -- Returns a pseudorandom fp64 number in the range [0, 1). The number\n    -- is determined by the given `key`, `seed` string, and integer `index`.\n    CREATE MACRO pseudorandom_uniform(key, seed, index)\n    AS (\n      (HASH(key || seed || index) >> 11) * POW(2.0, -53)\n    );\n \nreply",
      "`HASH` looks like a slow function ... does something like `rand() + rowid & 0` or `((rand() * p53 + rowid) % p53) / p53`  work?\n \nreply",
      "Had a good laugh, this is the normal response to difference in SQL implementations in my experience.\n \nreply"
    ],
    "link": "https://blog.moertel.com/posts/2024-08-23-sampling-with-sql.html",
    "first_paragraph": "Sampling is one of the most powerful tools you can wield to extract meaning from large datasets.\nIt lets you reduce a massive pile of data into a small yet representative dataset that\u2019s fast and easy to use.If you know how to take samples using SQL, the ubiquitous query language, you\u2019ll be able to take samples anywhere.\nNo dataset will be beyond your reach!In this post, we\u2019ll look at some clever algorithms for taking samples.\nThese algorithms are fast and easily translated into SQL.First, however, I\u2019ll note that many database systems have some built-in support for taking samples.\nFor example, some SQL dialects support a TABLESAMPLE clause.\nIf your system has built-in support\u2014and it does what you need\u2014using it will usually be your best option.Often, though, the built-in support is limited to simple cases.\nLet\u2019s consider some realistic scenarios that are more challenging:In 2006, Pavlos S. Efraimidis and Paul G. Spirakis published a one-pass algorithm for drawing a weighted random sample"
  },
  {
    "title": "C-Motive's electrostatic motors use printed circuit boards instead of magnets (c-motive.com)",
    "points": 70,
    "submitter": "Jeff_Brown",
    "submit_time": "2024-10-20T17:02:36.000000Z",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=41896789",
    "comments": [
      "The C-Motive guys have PR all over the web.Electrolytic capacitors can have far more capacitance than air capacitors. That's the basic concept here.Here's their patent.[1] Just scroll through the drawings and you'll see how it works.Here's the key concept: \"Numerous aspects of the\npresent disclosure cooperate to increase the breakdown field\nstrength 8406, and / or adjust (e.g. , flatten) the field strength\ntrajectory such as : the permittivity of the dielectric fluid; a\nselection of fluid constituents to maintain a permittivity\nprofile related to operating temperatures; protection of the\ndielectric fluid from impurities, presence of water, and / or\npresence of gases ; providing a surface smoothness of the\nelectrodes 8402, 8404 (or portions thereof), related surfaces,\nand/ or a housing inner surface ; rinsing / removal of particles\nand / or impurities (e.g., from manufacturing residue, etc.);\nprovision of a surface treatment on at least a portion of an\nelectrode, and / or on a surface adjacent to the electrode,\nincluding varying surface treatments for different electrodes;\nprovision of a coating on at least a portion of an electrode\nand / or on a surface adjacent to the electrode, including\nvarying the coating for different electrodes; provision of a\nsurface treatment and / or coating on a component at least\nselectively contacting the dielectric fluid (e.g., a housing\ninner surface, a packed bed, a side chamber, flow path,\nand / or eddy region ); protection of composition integrity of\nthe dielectric fluid (e.g., managing materials of bearings,\nseals , plates , etc. to avoid material breakdown and / or\nintroduction of degradation constituents that negatively affect the\nperformance of the dielectric fluid ); introduction of a field\ndisrupting additive into the dielectric fluid ( e.g., a coated\nmetal oxide, a nano-particle, and /or a conductive particle\nhaving a conductor that isolate the conductive particle from\nphysical contact with the dielectric fluid ); introduction of an\nion scavenging additive into the dielectric fluid ( e.g., BHT,\nantioxidants, etc. ); management of gap distance (e.g., using\nbearings, magnetic separation, a separation assembly, etc.);\nand / or selected field weakening at certain operating conditions.\nThe utilization of various field management aspects of\nthe present disclosure allows for an increased average field\nstrength in the gap, while maintaining a peak field strength\nbelow a breakdown threshold 8406, thereby increasing\ncapacitive energy storage and consequent performance of\nthe ESM 1002.\"This thing is sort of like a high voltage electrolytic capacitor with moving parts. They go to a lot of trouble to \ndeal with most of the problems that happen inside capacitors, plus the special problems from\nmoving parts. They had to go all the way to a pumped fluid system with\nfilters, to keep the dielectric fluid cool and clean. Many electric car motors have liquid cooling, so it's no worse than that. It does mean this is probably a technology for larger motors, because the motor requires some accessory systems.It's not clear that this is a win over magnetic motors, but it's reasonable engineering.[1] https://patentimages.storage.googleapis.com/cf/eb/f0/6d48f07...\n \nreply",
      "I wish they had some examples of what RPM, torque, weight, and size specs were for a few possible applications. They seem to emphasize low RPM, but is that 200 RPM or 2000RPM? With other electric motors being capable of 10k-20k RPM, the \"low\" RPM mention is very vague.If it's capable of up to about 3000 RPM, and it doesn't weigh too much it could be interesting as an ultralight aircraft power plant.\n \nreply",
      "It's an electrostatic motor, so expect peak performance at close to 0 RPM. It probably won't work well at 1k RPM, but whether \"too high frequency\" for it is closer to 10 RPM or 100 RPM isn't clear.There's a video with some waves in unlabeled axis. I didn't watch it.Anyway, it's almost certainly not aimed at aircraft propulsion or power generation. You may want something like it for robotics, but last time a paper from them circulated around here, they seemed to be focusing on instrument actuators and chip fabrication.\n \nreply",
      "Their applications pages mentions wind turbines and automotive applications and promises increased efficiency vs conventional motors. That would require maintaining 90%+ efficiency at well over 1k RPM. But no specs anywhere, so hard to tell whether this is real.\n \nreply",
      "TBH, I didn't think about low rotational speed wind turbines. Yeah, it may be a big thing for those.\"Electric drivetrains\" can mean anything from an excavator moving at 5km/h with 3m large wheels in a frequency of less then 0.2Hz up to extreme race RC vehicles, at 100km/h with 5cm wheels at ~100Hz. A car wheels go barely over 1k RPM, but I don't really expect them to do anything useful for those.\n \nreply",
      "I don't think it was really for automotive applications. It said something like \"low speed vehicles\". Made me think of something like golf carts or maybe ATVs. Of course without a gearbox, the biggest factor would be what wheel diameters are used since that would be the main ratio with revs per mile.\n \nreply",
      "I've long wondered if there's a possible application for something like this using 3d printing and electrets.  Basically you can freeze an electric field inside of an insulator if you apply it as the material solidifies.  I think you should be able to embed electrets inside of 3d prints simply by generating a strong electric field at the print head or slightly behind it. You can also vary the field and embed a 3d electret that can act as, say, a sensor or a hidden ID in the print.\n \nreply",
      "Previous discussion 2 months ago:\nhttps://news.ycombinator.com/item?id=41309292\n \nreply",
      "Some more technical content (literature review, but includes the university work this spun out of) if the sales page isn't doing it for you:https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=919...\n \nreply",
      "A lot of blabla, no technical data.\nSuspicious.\n \nreply"
    ],
    "link": "https://www.c-motive.com/technology/",
    "first_paragraph": "\u00a0No add-ons needed\nTraditional electric machines have been around for over 200 years, but require the use of other technologies to aid their issues. Upsizing motors accounts for efficiency losses and safety concerns, gearboxes need to slow down motor speeds, and active cooling is required to reduce heat generation from copper windings.\u00a0No gearbox?\nTraditional electromagnetic machines are optimized for high-speed operation and require a gearbox to slow rotational speed and meet desired torque output. Electrostatic machines are naturally suited for low-speed operation and deliver low electrical losses.Higher efficiency in both motor and generator applications \u2013 more range in electric drivetrains, lower utility bills for industrial / manufacturing plants, greater energy generation from renewable generationTorque generation at low speeds without a gearbox and without any active cooling requirementsUp to 10x the specific torque (Nm/kg) of traditional motorsSilent operation \u2013 both audible an"
  },
  {
    "title": "The AI Investment Boom (apricitas.io)",
    "points": 124,
    "submitter": "m-hodges",
    "submit_time": "2024-10-20T14:56:32.000000Z",
    "num_comments": 154,
    "comments_url": "https://news.ycombinator.com/item?id=41895746",
    "comments": [
      "Reading this makes me willing to bet that this capital intensive investment boom will be similar to other enormous capital investment booms in US history, such as the laying of the railroads in the 1800s, the proliferation of car companies in the early 1900s, and the telecom fiber boom in the late 1900s. In all of these cases there was an enormous infrastructure (over) build out, followed by a crash where nearly all the companies in the industry ended up in bankruptcy, but then that original infrastructure build out had huge benefits for the economy and society as that infrastructure was \"soaked up\" in the subsequent years. E.g. think of all the telecom investment and subsequent bankruptcies in the late 90s/early 00s, but then all that dark fiber that was laid was eventually lit up and allowed for the explosion of high quality multimedia growth (e.g. Netflix and the like).I think that will happen here. I think your average investor who's currently paying for all these advanced chips, data centers and energy supplies will walk away sorely disappointed, but this investment will yield huge dividends down the road. Heck, I think the energy investment alone will end up accelerating the switch away from fossil fuels, despite AI often being portrayed as a giant  climate warming energy hog (which I'm not really disputing, but now that renewables are the cheapest form of energy, I believe this huge, well-funded demand will accelerate the growth of non-carbon energy sources).\n \nreply",
      "I'm sure you are right. At some point, the bubble will crash.The question remains is when the bubble will crash. We could be in the 1995 equivalent of the dotcom boom and not 1999. If so, we have 4 more years of high growth and even after the crash, the market will still be much bigger in 2029 than in 2024. Cisco was still 4x bigger in 2001 than in 1995.One thing that is slightly different from past bubbles is that the more compute you have, the smarter and more capable AI.One gauge I use to determine if we are still at the beginning of the boom is this: Does Slack sell an LLM chatbot solution that is able to give me reliable answers to business/technical decisions made over the last 2 years in chat? We don't have this yet - most likely because it's probably still too expensive to do this much inference with such high context window. We still need a lot more compute and better models.Because of the above, I'm in the camp that believe we are actually closer to the beginning of the bubble than at the end.Another thing I would watch closely to see when the bubble might pop is if LLM scaling laws are quickly breaking down and that more compute no longer yields more intelligence in an economical way. If so, I think the bubble would pop. All eyes are on GPT5-class models for signs.\n \nreply",
      "Re: Slack chat:Glean.com does it for the enterprise I work at:\nIt consumes all of our knowledge sources including Slack, Google docs, wiki, source code and provides answers to complex specific questions in a way that\u2019s downright magical.I was converted into a believer when I described an issue to it, pointers to a source file in online git repo and it pointed me to another repository that my team did not own that controlled DNS configs that we were not aware about. These configs were the reason our code did not behave as we expected.\n \nreply",
      "This is the main \"killer feature\" I've personally experienced from GPT things: a much better contextual \"search engine-ish\" tool for combing through and correlating different internal data sources (slack, wiki, jira, github branches, etc).AI code assistants have been a net neutral for me (they get enough idioms in C++ slightly incorrect that I have to spend a lot of time just reading the generated code thoroughly), but being able to say \"tell me what the timeline for feature X is\" and have it comb through a bunch of internal docs / tickets / git commit messages, etc, and give me a coherent answer with links is amazing.\n \nreply",
      ">they get enough idioms in C++ slightly incorrectthis is part of why I stay in python when doing ai-assisted programming; there's so much training information out there for python and I _generally_ don't care about if its slightly off-idiom, its still probably fine.\n \nreply",
      "This is partly why I believe OS makers, Apple, Microsoft, Google, have a huge advantage in the future when it comes to LLMs.They control the OS so they can combine and feed all your digital information to an LLM in a seamless way. However, in the very long term, I think their advantage will go away because at some point, LLMs could get so good that you don't need an OS like iOS anymore. An LLM could simply become standalone - and function without a traditional OS.Therefore, I think the advantage for iOS, Android, Windows will increase in the next few years, but less powerful after that.\n \nreply",
      "An LLM is an application that runs on an operating system like any other application. That the vendor of the operating system has tied it to the operating system is purely a marketing/force-it-onto-your-device/force-it-in-front-of-your-face play. It's forced bundling, just like Microsoft did with Internet Explorer 20 years ago.\n \nreply",
      "I predict that OpenAI will try to circumvent iOS and Android by making their own device. I think it will be similar to Rabbit R1, but not a scam, and a lot more capable.They recently hired Jony Ive on a project - it could be this.I think it'll be a long term goal - maybe in 3-4 years, a device similar to the Rabbit R1 would be viable. It's far too early right now.\n \nreply",
      "Thanks. I didn't know that existed. But does it scale? Would it still work if large companies with many millions of Slack messages?I suppose one reason Slack doesn't have a solution yet is because they're having a hard time getting it to work for large companies.\n \nreply",
      "Yeah, Glean does this and there are a bunch of other competitors that do it as well.I think you may be confused about the length of the context window. These tools don't pull all of your Slack history into the context window. They use a RAG approach to index all of your content into a vector DB, then when you make a query only the relevant document snippets are pulled into the context window. It's similar for example to how Cursor implements repository-wide AI queries.\n \nreply"
    ],
    "link": "https://www.apricitas.io/p/the-ai-investment-boom",
    "first_paragraph": ""
  },
  {
    "title": "The Ultimate Conditional Syntax (acm.org)",
    "points": 59,
    "submitter": "azhenley",
    "submit_time": "2024-10-20T12:33:39.000000Z",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=41894932",
    "comments": [
      "I like that there is this left-to-right flow. I think it\u2019s a bit nicer to read than if-let ordering where the pattern comes before the thing it will be matched against. I think it\u2019s also good for ocaml-style constructor disambiguation, which tends to go in lexical order.Another nice aspect of making guards a less special case is that it avoids complexities in deciding if a binding is unused. I believe this logic was a source of lots of compiler warning bugs in ocaml.This syntax doesn\u2019t seem to solve the following problem with matching where there are two paths to the same binding, (say you have an option in one branch but it isn\u2019t optional in the other, and maybe you\u2019d like to handle both cases with the same code. Currently you can do that with a match (match \u2026) with \u2026 pattern.I worry that the semantics around exhaustiveness and mutable values may be confusing, though I guess OCaml already has that problem:  type t = { mutable x : bool }\n\n  let n = function true -> 1 | false -> 0\n\n  let f t =\n    match t with\n    | { x = false } when ( t.x <- true; false) -> -1\n    | { x } -> n x * 2 + n t.x\n\nWhat does t { x = false } return? Similarly if you changed the second case to be two cases instead of binding x?\n \nreply",
      "Looks very similar to lambda zero syntax (https://github.com/clark800/lambda-zero):    def getNaturalName(tag, globals)\n        if globals.lookup(\"0\") is Just(Global(_, _, term))\n            if term is Numeral(_, type, _)\n                return Just(Name(getTermTag(type)))\n            error showSyntaxError(\"0 must be a numeral to use numerals\", tag)\n        return Void\n\nThough this ultimate conditional syntax is more general because lambda zero only allows one destructuring per conditional to simplify parsing.\n \nreply",
      "Elixir already has this - \"with\". https://www.openmymind.net/Elixirs-With-Statement/\nE.g.:  with true <- is_email_address?(email),\n     true <- String.length(code) === 6,\n     %EmailConfirmation{} <- EmailConfirmations.get_email_confirmation(email),\n     nil <- EmailAddresses.get_email_address(email),\n     {:ok, user} <- Users.create_user(data),\n     {:ok, email_address} <- EmailAddresses.create_email_address(user, email) do\n     ...\n  else\n     ...\n  end\n \nreply",
      "How does this check exhaustiveness?\n \nreply",
      "I don't understand how it's better than traditional pattern matching.\n \nreply",
      "With traditional matching there are up to five different things:- if x then y else z. This is roughly like a match with a true and false case but it depends on the language how much that is- match e with { p -> e }. This is the classic pattern match case- if let p = e then x. This is roughly equivalent to (match e with p -> x | _ -> ())- match e with { p when e -> e }. This is matching with a guard but I\u2019ve counted it as a special case because it doesn\u2019t easily designate into a match because of the binding/evaluation order, and the guard is special because it can only go at the top level of the match clause so it isn\u2019t just a special kind of pattern (though maybe it could be)- let p = e. This is one-case pattern matching used for binding variables or destructuring.The paper proposes a way to make the first four cases obvious parts of one more unified thing, which makes the language potentially simpler and may reduce some weird warts like where guards can go.\n \nreply",
      "There's such a thing as too high an abstraction. Sometimes 4 different operations really are just 4 different operations and not cases of some mega-construct.\n \nreply",
      "I totally agree that can be the case, but I\u2019m not sure it applies that much to the OP.\n \nreply",
      "Inspired by duality, I've been trying to work out a language where there's a more obvious correspondence/symmetry between expressions (which evaluate in an environment of named bindings to produce an anonymous value) and patterns (which destructure an anonymous value to produce an environment of named bindings).\n \nreply",
      "It looks like it could be more concise sometimes. For instance, you could have this:  if f(x) is Some(a) and g(x, a) is Some(Right(b)) then h1(a, b) else h2(x)\n\nThe equivalent pattern matching with a match expression in pseudo-ML:  match f(x) with\n    None -> h2(x)\n    Some(a) ->\n      match g(x, a) with\n        Some(Right(b)) -> h1(a, b)\n        _ -> h2(x)\n \nreply"
    ],
    "link": "https://dl.acm.org/doi/10.1145/3689746",
    "first_paragraph": ""
  },
  {
    "title": "Internet Archive breached again through stolen access tokens (bleepingcomputer.com)",
    "points": 335,
    "submitter": "vladyslavfox",
    "submit_time": "2024-10-20T15:00:06.000000Z",
    "num_comments": 181,
    "comments_url": "https://news.ycombinator.com/item?id=41895764",
    "comments": [
      "> \"It's dispiriting to see that even after being made aware of the breach weeks ago, IA has still not done the due diligence of rotating many of the API keys that were exposed in their gitlab secrets,\" reads an email from the threat actor.With everything that\u2019s going on, it\u2019s highly suspicious that this is happening right after they upset some very rich rent seekers.\n \nreply",
      "People with solid info sec knowledge: this is a good opportunity to offer your expertise pro-bono for a good cause!\n \nreply",
      "They're buried in these offers right now.\n \nreply",
      "I wonder how many offers are legitimate.\n \nreply",
      "An org amidst an attack might not be the most open to giving credentials and access to strangers.\n \nreply",
      "why not? it's already been given away\n \nreply",
      "At this point they should consider a rewirte from scratch. I bet they are running a tech stack from 1992.\n \nreply",
      "We need archives built on decentralized storage. Don't get me wrong, I really like and support the work Internet Archive is doing, but preserving history is too important to entrust it solely to singular entities, which means singular points of failure.\n \nreply",
      "This seems to get brought at least once in the comments for every one of these articles that pops up.The IA has tried distributing their stores, but nowhere near enough people actually put their storage where their mouths are.\n \nreply",
      "And it's guaranteed not to happen if the efforts don't continue.\n \nreply"
    ],
    "link": "https://www.bleepingcomputer.com/news/security/internet-archive-breached-again-through-stolen-access-tokens/",
    "first_paragraph": ""
  },
  {
    "title": "Google's AI prophet fast tracks singularity prediction (independent.co.uk)",
    "points": 6,
    "submitter": "wslh",
    "submit_time": "2024-10-21T00:53:12.000000Z",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=41899765",
    "comments": [
      "> Ray Kurzweil [...] has predicted that artificial intelligence will herald a new era of hybrid humans capable of ageing in reverse within the next five years.I'm sorry, what? Is this saying what I think it is?> Dr Kurzweil updates his previous predictions and elaborates on how he believes AI technology can transform humans biologically.Yeah no. We won't be \"aging in reverse\" thanks to ChatGPT. And this prediction is supported with a graph of CPU speeds over time? Has the whole world gone mad?\n \nreply",
      "it feels that way... these sort of garbage articles coming out are really frustrating.\n \nreply",
      "What has Kurzweil ever done that we should listen to him? And if Elom gives a timeline that is automatically sus.\n \nreply",
      "From other posts on the hacker news front page, will AI complete the ipv4 to ipv6 transition this year?If not, I think it's still a while before we hit the singularity, because it's going to have to be good at  solving problems like 4=>6 before it could hope to be a singularity event\n \nreply"
    ],
    "link": "https://www.independent.co.uk/tech/ai-singularity-date-ray-kurzweil-google-b2511847.html",
    "first_paragraph": ""
  },
  {
    "title": "The IPv6 Transition (potaroo.net)",
    "points": 104,
    "submitter": "todsacerdoti",
    "submit_time": "2024-10-20T05:54:34.000000Z",
    "num_comments": 152,
    "comments_url": "https://news.ycombinator.com/item?id=41893200",
    "comments": [
      "I have fully implemented IPv6 in my home network.I have even implemented an IPv6-Only network. It fully works, including accessing IPv4 only websites like github.com via DNS64 and NAT64 at my router.The only practically useful thing about my IPv6 enabled network is that I can run globally routable services on my lan, without NAT port mapping. Of course, only if the client is also IPv6.Other than this one use case, IPv6 does nothing for me.It doesn't work from most hotels, nor from my work lan, nor many other places because most \"managed\" networks are IPv4 only. It works better at Cafes because they are \"unmanaged\" and IPv6 is enabled by the most common ISPs, like ATT and Comcast and their provided routers.Based on this experience, I think IPv6 is less valuable than us HN audience thinks it is. Private networks, NAT, Carrier Grade NAT are good enough, and internet really doesn't care about being completely peer-to-peer.I think the adoption rate reflects this--it's a linear growth curve over the last 25 years. It should have been exponential.I think cost of IPv4 reflects this--it is now below the peak, and has leveled off.As surprising as it seems, IPv4 exhaustion has not been a serious problem. Internet marches on. IPv6 is still a solution looking for a problem, and IPv4 exhaustion wasn't one of them.\n \nreply",
      "I recently moved to a 'cheap' ISP because I could get double the speed for half the price. They use CG-NAT and it's been awful.I don't need to forward any ports but seemingly because I share an IP with a billion people I get Captchas everywhere (Google, Cloudflare etc.). I was even blocked from accessing Reddit without an account at some point.\n \nreply",
      "Starlink uses CGNAT. It's awful, I'm regularly getting CAPTCHAs on random websites.They now support IPv6 but only with dynamic address allocations so you don't get a lot of advantages from it.\n \nreply",
      "I hadn\u2019t put that quite together. I wonder how many people would value IPv6 if they knew it meant less CAPTCHAs.\n \nreply",
      "NAT is mostly okay, but carrier grade NAT where you can't forward a port causes real problems.IPv4 exhaustion is a real problem, it's just not enough to motivate people much.\n \nreply",
      "The main problem I had when I was on CGNAT was not so much port forwarding (annoying, but solvable), but with being banned from all sorts of stuff. The address is shared with so many people and one person did something stupid or whatnot. Sometimes you don't even know if you're banned or not.For better or worse, IP blocks are still very common. It's easy to complain about this, but there aren't really any good methods to deal with persistent abuse.\n \nreply",
      "Have you tried using PCP to forward the port? I was under the (maybe-incorrect, and if so I would really like to learn) impression that most major CG-NAT setups supported it.\n \nreply",
      "Nah, many carriers don\u2019t support it. I\u2019ve always had to resort to STUN\n \nreply",
      "I suppose I can try that some time.  I can find absolutely zero mentions of that for the ISP, just the option of buying a static IP.\n \nreply",
      "If it was a real problem, market pricing would reflect the increasing severity of that problem.The truth is that people who care about port forwarding are such a small minority -- especially now that P2P file sharing has lost its hype -- that they don't make a visible dent in the rate of IPv4 exhaustion.\n \nreply"
    ],
    "link": "https://www.potaroo.net/ispcol/2024-10/ipv6-transition.html",
    "first_paragraph": "\n\nThe IPv6 Transition\n\n\n\n\nOctober 2024\n\nI wrote an article in May 2022, asking \u201cAre we there yet?\u201d about the transition to IPv6. At the time I concluded the article on an optimistic note, observing that we may not be ending the transition just yet, but we are closing in. I thought at the time that we won\u2019t reach the end of this transition to IPv6 with a bang, but with a whimper. A couple of years later, I\u2019d like to revise these conclusions with some different thoughts about where we are heading and why.The state of the transition to IPv6 within the public Internet continues to confound us. RFC 2460, the first complete effort at a specification of the IPv6 protocol was published in December 1998, more than twenty-five years ago. The entire point of IPv6 was to specify a successor protocol to IPv4 due to the prospect of running out of IPv4 addresses. Yet we ran out of IPv4 addresses more than a decade ago while the Internet is largely sustained through the use of IPv4.  This transition t"
  },
  {
    "title": "Using Euro coins as weights (2004) (rubinghscience.org)",
    "points": 140,
    "submitter": "Tomte",
    "submit_time": "2024-10-20T10:18:13.000000Z",
    "num_comments": 125,
    "comments_url": "https://news.ycombinator.com/item?id=41894359",
    "comments": [
      "I'm using euro cents as weights in my weighted vest.When I started doing this I didn't want to afford dedicated weights as it seemed like a waste of money, but I had many cents saved up from my childhood, which I started to use instead.I have roughly 15kg in euro cents in my vest and I'm regularly talking walks with it.To get one kilo you need 435 cents and it turns out that in Germany you can also \"buy\" coins \"for free\" at the \"Bundesbank\", that is, you can exchange actual money for weights without any fees. You give 4 euros and 35 cents and you get a kilo. Once you need the money back, you can also sell those coins back to them for free.\n \nreply",
      "You can also go to the beach and get unlimited amounts of weight for free too. That's what's most budget weights are made of\n \nreply",
      "Just FYI this is illegal in many areas.\n \nreply",
      "It's a lot easier to contain coins vs sand, though.\n \nreply",
      "I don't mean to argue that it's just gimmick and any sane person would just use sand, but to be completely fair, sand is much less dense than steel, so if the coins pack well it does make a better weight.I do also suspect that there must be some product that must be more cost effective than coins but denser than sand, but cannot think of it right away. I mean, scrap steel is a couple of cents per kg.\n \nreply",
      "Sometimes sand + water is used for ballast. Depends on your use case, if your heavy thing is moving around then the sloshing won't be ideal, but if it's just sitting somewhere static then it can work.eg; weighing down the corners of a beach tent, pegs won't grip in the sand so instead tie plastic bags onto the guy ropes and fill them with sand and water.\n \nreply",
      "Olympic weight plates for barbells. They're widely used, so competition has brought the cost down, and they're easily available in useful increments. I currently see 4x 10lbs for <$50 on Amazon. That works out to 2,53 Euro per kg. So cheaper than euro cents. They may not have the exact shape you need.The scrap steel probably didn't cost cents per kg when it was sold for its original purpose. You are paying for a useful shape.A professional equivalent of weighted vests are ballistic plate carriers. Real ballistic plates can be fragile and expensive, so options for exercising in (or milsim games in airsoft etc.) include expired (and failed to re-certify) real ballistic plates, made for purpose training plates... or plate shaped sandbags!\n \nreply",
      "> That works out to 2,53 Euro per kg. So cheaper than euro cents.The cents are free though, cause they're legal tender \u2014 just deposit them instead of having to sell 2nd hand\n \nreply",
      "The cheapest plates can be higher variance than you might expect. I\u2019ve seen reports of 45s that are 10% light.\n \nreply",
      "Could you explain more? I do not understand how you can buy coins for free by paying coins for \u201cweights\u201d (what are these weights? What are they made from?). Also, what is the use for this? To check of your coins are real? Calibrate your coin scale?\n \nreply"
    ],
    "link": "https://www.rubinghscience.org/surv/euroweights1.html",
    "first_paragraph": ""
  },
  {
    "title": "VersaTiles \u2013 a complete FLOSS map stack (versatiles.org)",
    "points": 104,
    "submitter": "moooo99",
    "submit_time": "2024-10-20T13:51:17.000000Z",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=41895356",
    "comments": [
      "I'd be interested to understand how this compares with a solution like openfreemap's (https://news.ycombinator.com/item?id=41635592) which was posted a few weeks ago.What is it that sets this apart from other similar solutions?\n \nreply",
      "Functionally they are rather similar in what they're aiming for. Architecturally there are some differences.OpenFreeMap uses the MapTiles format [1] which is an open source format for vector tiles that does require the attribution of the OpenMapTiles page for every map generated from it (CC-BY license). Versatiles uses the Shortbread format instead [2] which is published under a CC0 license. Instead of the SQLite based mbtiles format they developed their own container format (and a converter).I've only started tinkering with this project a little bit cause I found it interesting after watching a CCC talk with it [3][1] https://openmaptiles.org/[2] https://shortbread-tiles.org/[3] German: https://youtu.be/8A51WkJ5S8I\n \nreply",
      "There really seems to be a lot of movement in the map space these days!\n \nreply",
      "Only mildly related, but has any one else been using Martin [0] for tile serving from Postgres/PostGis? It\u2019s been a huge benefit to me and probably my favorite open source map tool right now. Martin + Deck.GL is such an awesome combo.[0] https://maplibre.org/martin/introduction.html\n \nreply",
      "Yeah Martin is amazing. And deckgl too. It's a pity as such a tiny tool it doesn't get the publicity of things like this and mbiles and pmtiles.Tile generation on the fly should really be default for most uses.\n \nreply",
      "I am only getting to the space, but if I understand correctly the overview it is a tile server speaking HTTP, but it is not working like Protomaps, which do not need a separate tile server, just a regular HTTP server with range requests support.\n \nreply",
      "It's a bit complicated, because I think the versatiles brand is used to describe multiple things:- the schema of the map: what objects are available in each tile at different zoom levels. It sounds like versatiles uses the shortbread schema (contrast vs OpenMapTiles, protomaps)- a container format: a way to pack multiple tiles into a single file. It sounds like they created their own format here (contrast vs mbtiles, pmtiles).- the scripts/tooling to build everything- the overall finished map product itself (contrast vs Google Maps, Stadio Maps, protomaps, OpenMapTiles, etc)The versatile container format seems to require a custom HTTP server. But if you want, you could produce the versatiles map and store it in a pmtiles container. Or you could stick a caching proxy in front of their publicly available tile server at https://tiles.versatiles.org/tiles/osm/{z}/{x}/{y}It would be interesting to hear them describe why they decided to create their own container format. The text that I have found seems to be contrasting it to RDBMS containers, but is silent about mbtiles/pmtiles.\n \nreply",
      "As far as my understanding goes, mbtiles is based on SQLite, so it would be a RDBMS container based format.There was a YouTube talk published 4 weeks ago showcasing this project, which was where I discovered it in the first place. The (German) video can be found here https://youtu.be/8A51WkJ5S8I\n \nreply",
      "Thanks, that's a handy video! Yes, mbtiles is based on SQLite, I was imprecise in my language.When I said RDBMS, I meant those that have a client/server model. The versatiles docs talk about the complexity and surface area of database systems as a motivator for creating their own container format. From this I inferred they were referring to Postgres and PostGIS, which are used in the canonical OpenMapTiles implementation.Watching that video, they do mention not liking the traditional Postgres/PostGIS approach due to its heavy weight. But they also say they disliked mbtiles due to its SQLite dependency, and that the versatiles format is inspired on/based on pmtiles. (Apologies if I'm missing nuance here, I was watching auto translated auto generated captions.)I found https://github.com/versatiles-org/versatiles-rs/issues/24 which contrasts the versatiles format vs the pmtiles format. After reading it, I'm not personally convinced of the benefits of versatiles vs just throwing a CDN in front of a clustered pmtiles file, but perhaps I'm missing something.\n \nreply",
      "Just a small clarification: mbtiles is built on a RDBMS (sqlite).\n \nreply"
    ],
    "link": "https://versatiles.org/",
    "first_paragraph": "\na\ncomplete\nFLOSS\nmap\nstack\nVersaTiles is a completely FLOSS stack for generating, distributing, and using map tiles based on OpenStreetMap data, free of any commercial interests.we explain here:MIZ-Babelsberg is funding the development of the \"VersaTiles Editorial Tools\", which are specifically designed for the use of maps in journalistic newsrooms."
  },
  {
    "title": "The Part of PostgreSQL We Hate the Most (2023) (cmu.edu)",
    "points": 202,
    "submitter": "virtualwhys",
    "submit_time": "2024-10-20T15:30:29.000000Z",
    "num_comments": 51,
    "comments_url": "https://news.ycombinator.com/item?id=41895951",
    "comments": [
      "OrioleDB was supposed to tackle this problem with a new storage engine . https://github.com/orioledb/orioledb\n \nreply",
      "(I\u2019m on the Supabase team)Oriole has joined us at supabase now and it\u2019s being worked on full time by Alexander and his team. Here is the patch set:https://www.orioledb.com/docs#patch-setIt will be available to try on the supabase platform later this year too\n \nreply",
      "As a data point, there are easily noticeable typos on that docs page.  Might be a good idea to have someone run a spell checker over it at some point?\n \nreply",
      "The whole ParadeDB team is really excited for OrioleDB and Supabase to ship this :) It's long overdue in the Postgres ecosystem!\n \nreply",
      "Yeah, I've been keeping an eye on the pgsql-hackers discussions. Alexander+team are doing great work.\n \nreply",
      "The big advantage is that you do not need any extra space if your workload mostly consists of INSERTs (followed by table drops). And it's generally unnecessary to split up insertion transactions because there is no size limit as such (neither on the generated data or the total count of rows changed). There is a limit on statements in a transaction, but you can sidestep that by using COPY FROM if you do not have to switch tables too frequently. From a DBA point of view, there is no need to manage a rollback/undo space separately from table storage.Every application is a bit different, but it's not that the PostgreSQL design is a loser in all regards. It's not like bubble sort.\n \nreply",
      "> but it's not that the PostgreSQL design is a loser in all regardsthe article literally says that pg's mvcc design is from the 90s and no one does it like that any more. that is technology that is outdated by over 30 years. i'd say it does not make it a loser in all regards, but in the most important aspects.\n \nreply",
      "When it comes to your data store, some people might consider using technology that\u2019s been reliably used in production by many organizations for 30 years a feature not a bug.I\u2019d prefer not to be the first person running up against a limit or discovering a bug in my DB software.\n \nreply",
      "Well every product has issues. The question is, do you feel like dealing with those issues or not?Flat files have also been reliably used in production for decades. That doesn't mean they're ideal...although amusingly enough s3 and its equivalent of flat files is what we've migrated to as a data store.\n \nreply",
      "At least couchdb is also append only with vacuum. So it's maybe not completely outdated.\n \nreply"
    ],
    "link": "https://www.cs.cmu.edu/~pavlo/blog/2023/04/the-part-of-postgresql-we-hate-the-most.html",
    "first_paragraph": "Posted on April 26, 2023There are a lot of choices in databases (897 as of April 2023). With so many systems, it\u2019s hard to know what to pick! But there is an interesting phenomenon where the Internet collectively decides on the default choice for new applications. In the 2000s, the conventional wisdom selected MySQL because rising tech stars like Google and Facebook were using it. Then in the 2010s, it was MongoDB because non-durable writes made it \u201cwebscale\u201c. In the last five years, PostgreSQL has become the Internet\u2019s darling DBMS. And for good reasons! It\u2019s dependable, feature-rich, extensible, and well-suited for most operational workloads.But as much as we love PostgreSQL at OtterTune, certain aspects of it are not great. So instead of writing yet another blog article like everyone else touting the awesomeness of everyone\u2019s favorite elephant-themed DBMS, we want to discuss the one major thing that sucks: how PostgreSQL implements multi-version concurrency control (MVCC). Our resea"
  },
  {
    "title": "Microsoft said it lost weeks of security logs for its customers' cloud products (techcrunch.com)",
    "points": 35,
    "submitter": "alephnerd",
    "submit_time": "2024-10-20T21:42:27.000000Z",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=41898723",
    "comments": [
      "I wonder which intelligence operation this supported...\n \nreply",
      "Or which pedophile network this covers up for. Oh wait, same thing.\n \nreply"
    ],
    "link": "https://techcrunch.com/2024/10/17/microsoft-said-it-lost-weeks-of-security-logs-for-its-customers-cloud-products/",
    "first_paragraph": "\n\n\t\tLatest\t\n\n\n\t\tAI\t\n\n\n\t\tAmazon\t\n\n\n\t\tApps\t\n\n\n\t\tBiotech & Health\t\n\n\n\t\tClimate\t\n\n\n\t\tCloud Computing\t\n\n\n\t\tCommerce\t\n\n\n\t\tCrypto\t\n\n\n\t\tEnterprise\t\n\n\n\t\tEVs\t\n\n\n\t\tFintech\t\n\n\n\t\tFundraising\t\n\n\n\t\tGadgets\t\n\n\n\t\tGaming\t\n\n\n\t\tGoogle\t\n\n\n\t\tGovernment & Policy\t\n\n\n\t\tHardware\t\n\n\n\t\tInstagram\t\n\n\n\t\tLayoffs\t\n\n\n\t\tMedia & Entertainment\t\n\n\n\t\tMeta\t\n\n\n\t\tMicrosoft\t\n\n\n\t\tPrivacy\t\n\n\n\t\tRobotics\t\n\n\n\t\tSecurity\t\n\n\n\t\tSocial\t\n\n\n\t\tSpace\t\n\n\n\t\tStartups\t\n\n\n\t\tTikTok\t\n\n\n\t\tTransportation\t\n\n\n\t\tVenture\t\n\n\n\t\tEvents\t\n\n\n\t\tStartup Battlefield\t\n\n\n\t\tStrictlyVC\t\n\n\n\t\tNewsletters\t\n\n\n\t\tPodcasts\t\n\n\n\t\tVideos\t\n\n\n\t\tPartner Content\t\n\n\n\t\tTechCrunch Brand Studio\t\n\n\n\t\tCrunchboard\t\n\n\n\t\tContact Us\t\nMicrosoft has notified customers that it\u2019s missing more than two weeks of security logs for some of its cloud products, leaving network defenders without critical data for detecting possible intrusions.According to a notification sent to affected customers, Microsoft said that \u201ca bug in one of Microsoft\u2019s internal monitoring agents resulted in a malfunction in "
  },
  {
    "title": "The Best Darn Grid Shader (Yet) (2023) (bgolus.medium.com)",
    "points": 137,
    "submitter": "eliasylonen",
    "submit_time": "2024-10-20T08:50:18.000000Z",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=41893987",
    "comments": [
      "This was really interesting. I don't love the fading out to solve the moire in the final solution. I wonder if some dithering would feel better and give the impression there are lines out there rather than a smooth gray surface? Or maybe some jitter to break up the pattern?This isn't shown in the post, but sometimes the moire feels worse if you are walking around and the moire has a movement of its own (either flickering/shimmering or sweeping across in uv space), and it's probably a decent benefit to get rid of the moire even if it's imperfect.\n \nreply",
      "Looks very interesting. I\u2019m confused by the note in the intro though:> Note: I highly recommend viewing this article in dark mode.I\u2019m trying, but it seems like Medium doesn\u2019t even have a dark mode? Is this reposted from somewhere else?\n \nreply",
      "You can install the \"Dark Reader\" plugin on Firefox, works great.\n \nreply",
      "I love this article! I used it as a basis for the multi-level grid in SuperSplat (https://playcanvas.com/supersplat/editor).\n \nreply",
      "This is really awesome. to ask a dumb question, what\u2019s a good way to get acclimated with running and building shaders? Just going straight to OpenGL tooling and extrapolating from there?\n \nreply",
      "Book of Shaders + ShaderToy is how a lot of people cut their teeth.\n \nreply",
      "It's awesome so much effort was put into rendering a grid. Great write up.\n \nreply",
      "Very nice, I'd use it as one of the starting points if I was to learn 3D graphics as it touches upon a lot of math details in a seemingly simple problem.\n \nreply"
    ],
    "link": "https://bgolus.medium.com/the-best-darn-grid-shader-yet-727f9278b9d8",
    "first_paragraph": ""
  }
]