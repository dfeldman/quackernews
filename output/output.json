[
  {
    "title": "GPT-4.5 (openai.com)",
    "points": 658,
    "submitter": "meetpateltech",
    "submit_time": "2025-02-27T20:01:16 1740686476",
    "num_comments": 485,
    "comments_url": "https://news.ycombinator.com/item?id=43197872",
    "comments": [
      "GPT 4.5 pricing is insane:\nPrice\nInput:\n$75.00 / 1M tokens\nCached input:\n$37.50 / 1M tokens\nOutput:\n$150.00 / 1M tokensGPT 4o pricing for comparison:\nPrice\nInput:\n$2.50 / 1M tokens\nCached input:\n$1.25 / 1M tokens\nOutput:\n$10.00 / 1M tokensIt sounds like it's so expensive and the difference in usefulness is so lacking(?) they're not even gonna keep serving it in the API for long:> GPT\u20114.5 is a very large and compute-intensive model, making it more expensive  than and not a replacement for GPT\u20114o. Because of this, we\u2019re evaluating whether to continue serving it in the API long-term as we balance supporting current capabilities with building future models. We look forward to learning more about its strengths, capabilities, and potential applications in real-world settings. If GPT\u20114.5 delivers unique value for your use case, your feedback (opens in a new window) will play an important role in guiding our decision.I'm still gonna give it a go, though.\n \nreply",
      "> We look forward to learning more about its strengths, capabilities, and potential applications in real-world settings. If GPT\u20114.5 delivers unique value for your use case, your feedback (opens in a new window) will play an important role in guiding our decision.\"We don't really know what this is good for, but spent a lot of money and time making it and are under intense pressure to announce new things right now. If you can figure something out, we need you to help us.\"Not a confident place for an org trying to sustain a $XXXB valuation.\n \nreply",
      "> \"Early testing shows that interacting with GPT\u20114.5 feels more natural. Its broader knowledge base, improved ability to follow user intent, and greater \u201cEQ\u201d make it useful for tasks like improving writing, programming, and solving practical problems. We also expect it to hallucinate less.\"\"Early testing doesn't show that it hallucinates less, but we expect that putting that sentence nearby will lead you to draw a connection there yourself\".\n \nreply",
      "That's some top-tier sales work right there.I suck at and hate writing the mildly deceptive corporate puffery that seems to be in vogue. I wonder if GPT-4.5 can write that for me or if it's still not as good at it as the expert they paid to put that little gem together.\n \nreply",
      "Yes, an AI that can convincingly and successfully sell itself at those prices would be worthy of some attention.",
      "According to a graph they provide, it does hallucinate significantly less on at least one benchmark.\n \nreply",
      "It hallucinates at 37% on SimpleQA yeah, which is a set of very difficult questions inviting hallucinations. Claude 3.5 Sonnet (the June 2024 editiom, before October update and before 3.7) hallucinated at 35%. I think this is more of an indication of how behind OpenAI has been in this area.\n \nreply",
      "Are the benchmarks known ahead of time? Could the answer to the benchmarks be in the training data?\n \nreply",
      "They've been caught in the past getting benchmark data under the table, if they got caught once they're probably doing it even more\n \nreply",
      "In general yes, bench mark pollution is a big problem and why only dynamic benchmarks matter.\n \nreply"
    ],
    "link": "https://openai.com/index/introducing-gpt-4-5/",
    "first_paragraph": ""
  },
  {
    "title": "IBM Completes Acquisition of HashiCorp (ibm.com)",
    "points": 188,
    "submitter": "ahurmazda",
    "submit_time": "2025-02-27T22:28:49 1740695329",
    "num_comments": 95,
    "comments_url": "https://news.ycombinator.com/item?id=43199256",
    "comments": [
      "Hashicorp's stuff always struck me as pretty hacky with awkward design decisions. For Terraform (at least a few years ago) a badly reviewed PR could cause catastrophic data loss because resources are deleted without requiring an explicit tombstone.Then they did the license change, which didn't reflect well on them.Now it's being sold to IBM, which is essentially a consulting company trying to pivot to mostly undifferentiated software offerings. So I guess Hashicorp is basically over.I suspect the various forks will be used for a while.\n \nreply",
      "> For Terraform (at least a few years ago) a badly reviewed PR could cause catastrophic data loss because resources are deleted without requiring an explicit tombstone.There have been lifecycle rules in place for as long as I can remember to prevent stuff like this. I'm not sure this is a \"problem\" unique to terraform.\n \nreply",
      "What happens if you forget the lifecycle annotations or put them in the wrong place or you accidentally delete them? Last time I checked it was data loss, but that was a few years ago.\n \nreply",
      "The same as in any other language when what you wrote was not what you intended? Sorry, I\u2019m really confused what your complaint here is or how you\u2019d prefer it to work. If you make a sensitive resource managed by any kind of IAC, of course the IAC can destroy it in a manner that would result in irretrievable data loss. The language has for forever put semantics in place to prevent that, and I\u2019m not sure as a power user I\u2019d want it any other way, I\u2019m explicit with what I want it to do and dont want it making crazy assumptions that I didnt write.like, what happens if you forget to free a pointer in c? sorry for snark but there are an unbelievably numerous amount of things to complain about in tf, never heard this one.\n \nreply",
      "I mean its also data loss if you run DROP DATABASE when you shouldn't. thats not sqls fault\n \nreply",
      "\"What happens if I turn a table saw on and start breakdancing on it?\"Of course you're going to hurt yourself. If you didn't put lifecycle blocks on your production resources, you weren't organizationally mature enough to be using Terraform in production. Take an associate Terraform course, this specific topic is covered in it.\n \nreply",
      "I concur. I looked pretty hard into adapting Serf as part of a custom service mesh and it had some bonkers designs such as a big \"everything\" interface used just to break a cyclic module dependency (perhaps between the CLI and the library? I don't recall exactly), as well as lots of stuff that only made sense if you wanted \"something to run Consul on top of\" rather than a carefully-designed tool of its own with limited but cohesive scope. It seemed like a lot of brittle \"just-so\" code, which to some extent is probably due to how Go discourages abstraction, but really rubbed me the wrong way.\n \nreply",
      "Sorry HashiCorp, been there and got the Tee-shirt (pink) :)Actually for me, the company I was at that IBM purchased was on the verge of folding, so in that case, IBM saved our jobs and I was there for many years.\n \nreply",
      "We experienced arbitrary layoffs in 2023, followed by an ominous feeling that more layoffs were imminent. However, the announcement of a deal changed the situation.Now, we are actively hiring for numerous positions.Personally, I am not planning to stay much longer. I had hoped that our corp structure would be similar to RedHat, but it seems that they intend to fully integrate us into the IBM mothership.\n \nreply",
      "I really wanted to work at HashiCorp in 2017/2018 and did five interviews in one day only to get ghosted[1]. That experience soured me on HC and its tools but I still admired them from afar.End of an era.---[1]: https://blog.webb.page/2018-01-11-why-the-job-search-sucks.t...\n \nreply"
    ],
    "link": "https://newsroom.ibm.com/2025-02-27-ibm-completes-acquisition-of-hashicorp,-creates-comprehensive,-end-to-end-hybrid-cloud-platform",
    "first_paragraph": ""
  },
  {
    "title": "Some Command & Conquer games are now open source (github.com/electronicarts)",
    "points": 132,
    "submitter": "bpierre",
    "submit_time": "2025-02-27T22:52:17 1740696737",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=43199460",
    "comments": [
      ">To use the compiled binaries, you must own the game. The C&C Remastered Collection is available for purchase on EA App or Steam.Mind you EA released [some of] the games as freeware back in 2008 so no, you don't have to buy them for the graphics, art, sound, and music assetsTiberian Dawn GDI https://web.archive.org/web/20110927141135/http://na.llnet.c...Tiberian Dawn NOD https://web.archive.org/web/20111104060230/http://na.llnet.c...Tiberian Sun (though no source code was released for this game) https://web.archive.org/web/20110823002110/http://na.llnet.c...Red Alert Allied https://web.archive.org/web/20100130215623/http://na.llnet.c...Red Alert Soviet https://web.archive.org/web/20100130220258/http://na.llnet.c...\n \nreply",
      "Maybe someone can finally make a native Mac version.It astonishes me that EA leaves obvious money on the table by not taking the 5 mins it would take to recompile it for MacOS.\n \nreply",
      "Some funny commentshttps://www.reddit.com/r/commandandconquer/comments/1izpkmh/...\n \nreply",
      "Dupe (609 points, 4 hours ago, 149 comments) https://news.ycombinator.com/item?id=43197131\n \nreply",
      "Wow, HN was (unsurprisingly) straight on to this. Thanks for sharing, I had missed the original post\n \nreply",
      "I heard this line of code `Sound_Effect(VOC_TANYA_LAUGH, Coord);`\n \nreply",
      "Shake it, baby!\n \nreply",
      "EA could release the source code of Chuck Yeager\u2019s Air Combat and LHX.\n \nreply",
      "Alas, no red alert 2 (the best, IMO). But still this is such great news. Huge props to EA for this.\n \nreply",
      "This is what you're looking for right?https://github.com/electronicarts/CnC_Remastered_Collection/...\n \nreply"
    ],
    "link": "https://github.com/electronicarts",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n            We've verified that the organization electronicarts controls the domain:\n          \n        Command & Conquer: Remastered Collection\n      \n\n\nC++\n\n\n\n\n\n            18.7k\n          \n\n\n\n\n            4.8k\n          \n\n        Command and Conquer: Generals - Zero Hour\n      \n\n\nC++\n\n\n\n\n\n            685\n          \n\n\n\n\n            161\n          \n\n        Command and Conquer: Renegade\n      \n\n\nC++\n\n\n\n\n\n            323\n          \n\n\n\n\n            76\n          \n\n        Command and Conquer: Red Alert\n      \n\n\nC++\n\n\n\n\n\n            894\n          \n\n\n\n\n            129\n          \n\n        Command and Conquer Tiberian Dawn\n      \n\n\nC++\n\n\n\n\n\n            336\n          \n\n\n\n\n            70\n          \n\n        Command and Conquer: Modding Support\n      \n\n\nHLSL\n\n\n\n\n\n            114\n          \n\n\n\n\n            40\n          \n\n            Command and Conque"
  },
  {
    "title": "Type 1 diabetes reversed by new cell transplantation technique (newatlas.com)",
    "points": 82,
    "submitter": "01-_-",
    "submit_time": "2025-02-24T15:24:19 1740410659",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=43160582",
    "comments": [
      "So it's a trade-off between increased risk of cancer[0] and the consequences of type 1 diabetes? Doesn't sound like a fun trade-off but I don't know anything.[0] https://www.cancer.gov/about-cancer/causes-prevention/risk/i...\n \nreply",
      "Type 1 diabetic here: you're right, it's a bad tradeoff. We already can do pancreas transplants for T1D, but the reason it's very uncommon is that immunosuppressants are a very bad tradeoff. Insulin treatment is preferred in the vast majority of cases.Stuff like this will never be a breakthrough until it doesn't need immunosuppressants. The best advancements in diabetes treatment will most likely continue to be on closed loop artificial pancreas systems.\n \nreply",
      "Insulin-specific immunotherapies are currently under development. We will soon be able to restore tolerance to insulin, and other pancreatic antigens such as GAD-65, without the need for broad immunosupressants. Ideally, this should stop \u03b2 cell destruction and conversion to T1D from auto-antibody positive status, as well as facilitate islet transplants with minimal side effects for those that are already T1D patients.\n \nreply",
      "I'm sorry for your plight and I genuinely hope there will be a much more tenable solution in the near future.\n \nreply",
      "> To prevent islet rejection, immune-suppressing drugs are given over the long term.This makes it a non starter. Immunosuppressants are generally considered a worse quality of life than insulin treatment. That's why pancreas transplants are generally only done for type 1 diabetics if they are already on immunosuppressants.\n \nreply",
      "Lots of biotech companies are working on immunosuppressant-free islet-equivalent transplantation.Two examples off the top of my head: Sana recently announced islet cell transplantation without immunosuppression (press release: https://ir.sana.com/news-releases/news-release-details/sana-... ) and Vertex (ongoing trial: https://www.breakthrought1d.org/news-and-updates/vertex-laun... ).\n \nreply",
      "I'm hopeful that someday we'll have a good system for \"caging\" cells to prevent an immune response (in either direction) while also permitting the visitors to sustain themselves with blood nutrients and regulate hormones or clean waste.Sort of like the role of the blood-brain barrier, or maybe a placenta.\n \nreply",
      "Yes! I think there was some work being done with a islet transplant like that. I'm not sure of the details though - it's probably a long way off, if it works.\n \nreply",
      "Yep. The hard, if not kear impossible part will be just resetting the one part of the immune system attacking the islets without turning off or resetting the immune system.\n \nreply",
      "The promising part here is that someday it will be possible to take stem cells from a patient and specialize them to islet cells. Similar to what they\u2019re doing here with vascular cells. It\u2019s far too expensive at the moment, but ultimately the process will be improved and refined, and the costs will come down. At least that\u2019s my hope for a cure.\n \nreply"
    ],
    "link": "https://newatlas.com/diabetes/islet-transplantation-type-1-diabetes/",
    "first_paragraph": ""
  },
  {
    "title": "World-first experimental cancer treatment paves way for clinical trial (wehi.edu.au)",
    "points": 68,
    "submitter": "femto",
    "submit_time": "2025-02-27T22:24:22 1740695062",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=43199210",
    "comments": [
      "It's neo-adjuvant (treat prior to surgery) with a three mAbs: Ipi + Nivo, which is a well established combo of immunotherapeutics for many solid tumors, rounded out with relatlimab which I'm less familiar with (LAG-3 inhibition). Neoadjuvant use of immunotherapies is established concept and is utilized in clinical practice for non-small cell lung cancer and colon cancer. This is the first time I've seen a use of 3 immunotherapies used at once: Ipi + Nivo isn't the most tolerable treatment regimen. Other novel aspect is use of neoadjuvant therapy in GBM.\n \nreply",
      "Presumably this is the Richard Scolyer case, if anyone was curious.A brief lookup indicates he could possibly be seeing recurrence; hopefully that's not the case but this is something that many people have said about his trial. We can't say for sure this treatment is going to be broadly effective without more time and experiments.\n \nreply",
      "This might be related with the case of Richard Scolyer, the Australian pathologist diagnosed with Glioblastoma. He was receiving some kind of experimental treatment for it.\n \nreply",
      "I believe he got just got either Ipilimumab and Nivolumab as a combo therapy or Pembrolizumab. He's a  researcher who focuses on neo-adjuvant (use therapy before resection) therapy in melanomas. It was a good gamble to try the same paradigm in Glioblastomas\n \nreply",
      "This is hopeful news for a really tragic cancer.\n \nreply",
      "It's not a generic novel treatment against any and all cancers, as much as the headline would lead you to conclude.\n \nreply",
      "You're mostly correct but immunotherapies do work for a many different type of cancer. The issue is differences in tumor biology plays a major factor. The even bigger caveat is that given a sufficient amount of tumor mutational burden (TMB), immunotherapies are found to be effective for any solid tumor. Granted most patients don't have the sky had TMBs needed for it to be effective\n \nreply",
      "[flagged]",
      "Remind me again what glioblastoma is? Is it some form of diabetes? Cateracts? A bacterial infection? Vitamin D deficiency?Is this an arthritis treatment?Is this a fertility treatment?It is in fact a cancer treatment.My wife is cooking fish at the moment. Wait, she's coking trout, not \"fish\". Except wait, slovenly fog brain, she's cooking rainbow trout, not \"trout\".\n \nreply",
      "This is a rather pedantic comment, it is a world first for a common cancer type, giloblastoma, therefore world first for cancer treatment is correct.\n \nreply"
    ],
    "link": "https://www.wehi.edu.au/news/world-first-experimental-cancer-treatment-paves-way-for-clinical-trial/",
    "first_paragraph": ""
  },
  {
    "title": "Turning a Bluetooth device into an Apple AirTag without root privileges (nroottag.github.io)",
    "points": 346,
    "submitter": "layer8",
    "submit_time": "2025-02-27T17:03:39 1740675819",
    "num_comments": 65,
    "comments_url": "https://news.ycombinator.com/item?id=43196207",
    "comments": [
      "Here is my quick summary:Apple devices listen for BLE advertisements of a certain form to indicate a \"Find My\" network lost device.The lost device advertisements mainly contain the public key part of a key pair.The public key does not fit in the in payload of the advertisements, so it is stuffed into the address field. Edit: Only 46 bits of the full 224 bit public key is stored in the address field.In general anyone can make a \"lost device\" advertisement as demonstrated by OpenHayStack[1]. The requirement is the address field needs to be fully controllable.BLE advertisements have a header that indicates what kind of address is present (specified by 3 bits: Public, NRPA, RPA, Random Static). The lost device advertisements are supposed to be \"Random Static\", but the researchers found that Apple \"Find My\" listeners (\"finders\") will accept advertisements for any address type.They use this fact to generate the private key part of a public key that matches an existing host adapter BLE address. The host adapter BLE address cannot generally be changed unless user has root/superuser privileges. This step is computationally expensive. However, private keys can be precomputed (rainbow tables) because a large chunk of the address is a manufacturer code (OUI).[1] https://github.com/seemoo-lab/openhaystack\n \nreply",
      "Hold up, how are they generating a private key from a public key?  That's supposed to be very difficult.Are you saying that a large chunk of the key (private key or public key?) is a manufacturer code?  That's insane\n \nreply",
      "> Hold up, how are they generating a private key from a public key?They are not. They are generating a private/public key pair where the first 46 bits of the public key happen to match the victim's BLE address.The find-my network then accepts beacons (encrypted with the attacker's private key) from this address, and stores it in iCloud to be retrieved by the attacker via the 46-bit prefix.\n \nreply",
      "victim's BLE address == target device they previously scanned and set up an Airtag for ?\n \nreply",
      "No.Expensive computed public key first 46 bits == Victim's BLE addressThe Apple FindMy system doesn't (or didn't) validate that the public key being broadcast had ever been manufactured or registered. So anyone with an iCloud account could query the Apple FindMy hashtable for the last observed encrypted payload, which contains the observed location generated by the nearby phone.If you have root on the victim's device, you don't need the expensive computation step. You just take a public/private keypair of your choice and reprogram the victim's Bluetooth hardware to broadcast that instead.\n \nreply",
      "No, they find the victim's MAC and generate a payload to broadcast from the victim's device, which will make the device appear to Apple devices as a genuine Airtag. Apple devices then upload location reports to Apple, and the attacker downloads them. No real Airtags are involved.\n \nreply",
      "They pregenerate the key pairs. The trojan sends the MAC to the server, and the server looks in its (precomputed) stash of key pairs, to find a public key that matches.\n \nreply",
      "They're brute-forcing the generation of a public key using random private keys. The exact private key doesn't matter. The full length of the generated public key doesn't even matter, only the first 46 bits. Since they only need to find a public key matching those 46 bits instead of the full 226 bits, that makes a brute force search possible.\n \nreply",
      "Hm, interesting. 2^46 is only 70 trillion, so yeah, totally computationally feasible. So,\nif i understand correctly, they only need a GPU to generate a database of 70 trillion private / public keys? Damn, not bad.\n \nreply",
      "They use rainbow tables.\n \nreply"
    ],
    "link": "https://nroottag.github.io/",
    "first_paragraph": "\n\n\n                A remote attacker can exploit this vulnerability to turn your device\u2014whether it\u2019s a desktop, smartphone,\n                or smartwatch\u2014into an AirTag-like tracker, enabling the attacker to track your location.\n                How does it work? Over 1.5 billion iPhones could act as free tracking agents for the attacker worldwide.\n            \n                        Apple\u2019s Find My network, leveraging over a billion active Apple devices, is the world\u2019s largest\n                        device-locating network.\n                        We investigate the potential misuse of this network to maliciously track Bluetooth devices. We\n                        present nRootTag,\n                        a novel attack method that transforms computers into trackable \u201cAirTags\u201d without requiring root\n                        privileges.\n                        The attack achieves a success rate of over 90% within minutes at a cost of only a few US\n                        dollars.\n     "
  },
  {
    "title": "Outlook classic dropped from Microsoft 365 (office-watch.com)",
    "points": 46,
    "submitter": "miles",
    "submit_time": "2025-02-27T23:02:24 1740697344",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=43199551",
    "comments": [
      "Absolutely user-hostile move. The same happened to the Outlook Web Access light version --- while it was not pure HTML, it was still much faster and more usable than the horribly bloated \"modern\" full version. Microsoft is slowly becoming the opposite of what it was known for, and that's going to cause customers to reconsider alternatives.Meanwhile my ISP still offers a webmail client which doesn't require JS at all, and it's been like that for nearly 3 decades, proving that it is absolutely possible.\n \nreply",
      "When was Microsoft known for being anything but a provider of bloated software?\n \nreply",
      "I believe that Microsoft is shooting themselves in the foot with that forced migration, at least in the Windows enterprise world. It\u2019s a bit like if they were replacing Excel with a half-baked version of Google Sheets.\n \nreply",
      "Your analogy is spot on except there is a MS webby Excel.  The goal is to get you totally in the cloud and by you I mean all their customers ... all of them.On prem Exchange is being made as wanky as possible to run and has been for some years.  Updates are excruciatingly painful and there is no technical reason I can fathom for that other than give them: \"death by a thousand cuts\".We have an all in one Exchange box so no fancy stuff and it really is painful to watch updates and there is no technical need for it.  Its not even a subtle nudge.  I've had updates take multiple hours.I'm old enough to remember when email (let alone the www) didn't exist.  MS were eventually very clever in the '90s and noughties and have kept on top of things since.  They don't have a total monopoly but quite a decent slice of the \"market\".  The goal is silo off users in their cloud.  On prem does not make them anything like enough cash.They will lose a few customers to Linux and Apple but the full on subscriptions model will more than fix up the finances.\n \nreply",
      "Just wanted to chime in and say it\u2019s nice to see someone from IT on the site. I love the site and can follow a lot of things but feel like it\u2019s mostly developers here.\n \nreply",
      "There are a lot of non devs here ... a lot.We just let the kiddies crack on and widdle in public but we are still here!\n \nreply",
      "Outlook is an odd duck.\nIt is an application with so many flaws\nor weirdness and I dont like it.Though, being honest, I do hate it \na bit less than all the other alternatives\nin that space.If I am in a \"Microsoft powered enterprise\"\nOutlook empowers me to be ridiculously productive.\ncompared to GSuite/Workspace.If you are outside a Microsoft sphere \nit is harder to justify it.I use Gmail a lot. \nTHe web interface is blah. \nvery blah.But it gets the job done, and I hate it \na little less than the alternatives.I did get Mutt up and running last week,\nOh what joy it was to get it to integrate\nwith Gmail.I will say that if your macOS based \nMailMate is the best mail client for me.Sadly I am often on the Windows end of things.\n \nreply",
      "> If I am in a \"Microsoft powered enterprise\" Outlook empowers me to be ridiculously productiveWhat does that mean in more detail? I only have typical corpo Outlook experience, and I find it passable at the best of times. Not even necessarily as the fault of Outlook.\n \nreply",
      "I've been rocking Evolution for around 10 years now.  We migrated from GroupWise at my little firm to Exchange a few years before that.I mostly look after the mail systems myself because no one else really seems to give a shit!  Provided it carries on working.  We have been all in on prem for a good 25 years now and I run a separate Exim n Dovecot with rspamd setup for a few vanity domains.Evo just works.  It does have a fair few quirks of its own but nothing as nasty as Outlook.  I doubt it will be too popular here - it only runs on Linux.\n \nreply",
      "> Outlook empowers me to be ridiculously productiveAs a programmer, if I could eliminate email at work altogether, it\u2019d have a net benefit to my productivity.\n \nreply"
    ],
    "link": "https://office-watch.com/2025/outlook-classic-dropped-from-microsoft-365/",
    "first_paragraph": ""
  },
  {
    "title": "Nvidia emulation journey, part 1: RIVA 128/NV3 architecture history and overview (86box.net)",
    "points": 75,
    "submitter": "davikr",
    "submit_time": "2025-02-27T20:51:23 1740689483",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=43198379",
    "comments": [
      "As a designer of Weitek's VGA core, this is a very interesting read.  I had no idea how valuable the core was to nVidia.  As Weitek was going under, I also remember interviewing with 3dfx and thinking how arrogant they were.  I'm not surprised they eventually lost\n \nreply",
      "> 5.0 came out late during development of the chip, which turned out to be mostly compliant, with the exception of some blending modes such as additive blending which Jensen Huang later claimed was due to Microsoft not giving them the specification in time.Not sure if this is the same thing I had, but on my Riva128 the alpha blending wasn't properly implemented. I distinctly recall playing Unreal Tournament and when I fired the rocket launcher there were big black squared with a smoke texture on them slowly rotating :D couldn't see where I was shooting :D\n \nreply",
      "Yes, that would be an artifact of missing additive blending.It simply means that each newly rendered polygon\u2019s RGB values are added together with the pixel values already in the frame buffer. It\u2019s good for lighting effects (although not a very realistic simulation of light\u2019s behavior unless your frame buffer is linear light rather than gamma corrected, but that effectively requires floating point RGB which wasn\u2019t available on gaming cards until 2003).\n \nreply",
      "Iirc, Riva 128 only supports 8 of the 32 D3D5 blend modes, or something. Usually in the case of GPUs that don't support all blending modes the Direct3D HAL will attempt to compensate by using a different blending mode, or just give up and render opaque pixels. The results are usually pretty ugly. Riva 128 is one of the better ones for the era in this regard.\n \nreply",
      "I doubt it required floating point rgb and, iirc, it came (for real) much later. GPUs used fixed point math behind the scenes for a long time. The only thing you need to get proper additive blending is saturation, so that you don't overflow, like on the N64.\n \nreply",
      "Riva 128 + Pentium II 233MHz + Corn Emulator = Mario 64 at full speed on your PC.\n \nreply",
      "+ Bleem for your PS1 emulation needs :)\n \nreply",
      "Were there any games or apps specifically tied to these cards, or did everything go through D3D at this point?I remember some earlier titles that were locked to specific cards such as the Matrox ones and didn't support any other accelerators.\n \nreply",
      "Amazing that they're able to make progress with such little public documentation.\n \nreply",
      "I'd assume any driver source code, which the Linux world has produced a lot of, can serve as a source (pun intended) of documentation.There's also https://envytools.readthedocs.io/en/latest/hw/intro.html\n \nreply"
    ],
    "link": "https://86box.net/2025/02/25/riva128-part-1.html",
    "first_paragraph": ""
  },
  {
    "title": "There isn't much point to HTTP/2 past the load balancer (byroot.github.io)",
    "points": 224,
    "submitter": "ciconia",
    "submit_time": "2025-02-25T05:33:21 1740461601",
    "num_comments": 181,
    "comments_url": "https://news.ycombinator.com/item?id=43168533",
    "comments": [
      "The article seems to make an assumption that the application backend is in the same datacenter as the load balancer, which is not necessarily true: people often put their load balancers at the network edge (which helps reduce latency when the response is cached), or just outsource those to a CDN vendor.> In addition to the low roundtrip time, the connections between your load balancer and application server likely have a very long lifetime, hence don\u2019t suffer from TCP slow start as much, and that\u2019s assuming your operating system hasn\u2019t been tuned to disable slow start entirely, which is very common on servers.A single HTTP/1.1 connection can only process one request at a time (unless you attempt HTTP pipelining), so if you have N persistent TCP connections to the backend, you can only handle N concurrent requests.  Since all of those connections are long-lived and are sending at the same time, if you make N very large, you will eventually run into TCP congestion control convergence issues.Also, I don't understand why the author believes HTTP/2 is less debuggable than HTTP/1; curl and Wireshark work equally well with both.\n \nreply",
      "The maximum number of connections thing in HTTP/1 always makes me think of queuing theory, which gives surprising conclusions like how adding a single extra teller at a one-teller bank can cut wait times by 50 times, not just by 2.However, I think the problem is the Poisson process isn't really the right process to assume. Most websites which would run afoul of the 2/6/8/etc connections being opened are probably trying to open up a lot of connections at the same time. That's very different from situations where only 1 new person arrives every 6 minutes on average, and 2 new people arriving within 1 second of each other is a considerably rarer event.[1]: https://www.johndcook.com/blog/2008/10/21/what-happens-when-...\n \nreply",
      "And if memory serves if you care about minimizing latency you want all of your workers running an average of 60% occupied. (Which is also pretty close to when I saw P95 times dog-leg on the last cluster I worked on).Queuing theory is really weird.\n \nreply",
      "Most analyses I've read say the threshold is around the 80% mark [1], although it depends on how model the distribution, and there's nothing magical about the number. The main thing is to avoid getting close to 100%, because wait times go up exponentially as you get closer to the max.Little's Law is fundamental to queueing theory, but there's also the less well-known Kingman's formula, which incorporates variability of arrival rate and task size [2].[1] https://www.johndcook.com/blog/2009/01/30/server-utilization...[2] https://taborsky.cz/posts/2021/kingman-formula/\n \nreply",
      "Why 60%? I suppose if they are less than 1% then latency will be even less.\n \nreply",
      "Can't it cut wait times by infinity? For example, if the arrivals are at 1.1 per minute, and a teller processes 1 per minute.\n \nreply",
      "Could be that, could also be the people taking a long time aren't at least causing a bottleneck (assuming there arent two of them at the same time).  So you have this situation like this: first person takes 10 minutes, while there are 9 waiting in line that take only one minute a piece.  With one teller, average wait time is ~15 minutes.  With two tellers, its now ~5 minutes.Which is why it is highly annoying when there's only one worker at the coffee stand, and there's always this one jerk at the front of the queue who orders a latte when you just want a coffee.  With two workers, the people who just want coffee won't have to wait 15 minutes for the lattee peopleAnd I've also noticed a social effect, when people wait a long time it seems to  reinforce how they perceive the eventual serviced, that is, they want more out of the interaction, so take longer. Which makes it the situation even worse\n \nreply",
      "> there's always this one jerk at the front of the queueHere in the espresso world, that\u2019s not so bad. But the \u2018vanilla oat milk decaf, and also a hot muffin with butter\u2019 is tedious.There is a roaster in Auckland that\u2019s been there since the \u201880s. On the counter it says \u2018espresso, flat white or fuck off\u2019. Clear and concise. I like it.\nhttps://millerscoffee.co.nz/\n \nreply",
      "\"On the counter it says \u2018espresso, flat white or fuck off\u2019\"Sounds a bit pretentious to me.  I generally order a coffee, no milk ... ta.\n \nreply",
      "Luckily, we don't get stuck behind someone using a check any more.\n \nreply"
    ],
    "link": "https://byroot.github.io/ruby/performance/2025/02/24/http2-past-the-load-balancer.html",
    "first_paragraph": "\nFeb 24, 2025\n      I want to write a post about Pitchfork, explaining where it comes from, why it\nis like it is, and how I see its future.\nBut before I can get to that, I think I need to share my mental model on a few things, in this case, HTTP/2.From time to time, either online or at conferences, I hear people complain about the lack of support for HTTP/2 in\nRuby HTTP servers, generally Puma.\nAnd every time I do the same, I ask them why they want that feature, and so far nobody had an actual use case for it.Personally, this lack of support doesn\u2019t bother me much, because the only use case I can see for it, is wanting to expose\nyour Ruby HTTP directly to the internet without any sort of load balancer or reverse proxy, which I understand may seem\ntempting, as it\u2019s \u201cone less moving piece\u201d, but not really worth the trouble in my opinion.If you are not familiar with the HTTP protocol and what\u2019s different in version 2 (and even 3 nowadays), you might\nbe surprised by this take, so let me tr"
  },
  {
    "title": "Solitaire (localthunk.com)",
    "points": 375,
    "submitter": "goles",
    "submit_time": "2025-02-27T15:54:36 1740671676",
    "num_comments": 109,
    "comments_url": "https://news.ycombinator.com/item?id=43195516",
    "comments": [
      "I love this post a lot. Our entire world is perpetuated by platforms that are desperately begging us for engagement. It feels to me at least that I'm being pulled in a hundred directions, for all my time for all time.My engagement with Balatro is not quite the same as localthunks. I go in phases where I play a lot and then put it down and walk away, and then weeks later I get back into it. But that also feels like it's in the spirit of what localthunk is talking about here. It's a comfort game. A pasttime rather than an addiction. Balatro is a stress reliever for me and I can jump in, play, and jump out and it's fine.I wonder what our digital world would look like if more tools and platforms adopted an approach that was not clinging desperately for everything all the time all at once.\n \nreply",
      "> It's a comfort game. A pasttime rather than an addiction. Balatro is a stress reliever for me and I can jump in, play, and jump out and it's fine.Exactly.To me there are two specific things that gives it that stress reliever, jump in/out spirit of Solitaire :- You know from the start you may not win every round.- Things can instantly and dramatically turn one way or another.I think both are perfectly captured in Balatro, and it manages to achieve it with a vastly more complex design.And it manages to add more depth while keeping that formula with a large number of jokers that, depending on what you get at the start, will dictate a different type of playstyle.Sure, you can develop some strategies over time (money), but you (usually) can't force the direction of a run (at least early on), you have to work with what you're given. It's truly a brillant design.\n \nreply",
      "I think Balatro is way too hard to be a good stress reliever. Unlike solitaire, I can barely win a run in Balatro and there's no indication as to what I could do better. I actually find it stressful more than stress relieving, if anything.\n \nreply",
      "Hey! I've played it for a while now (and haven't even followed any Reddit / YouTube guides yet).Early game: Get a joker that gives you a big mult (e.g., Half Joker w/ +20 Mult for small hands). This will allow you to survive while you build out your points engine. (You can survive with flushes in the early game, but it can get harder to win with flush later, due to Bosses, unless you have flush friendly Jokers and a large hand size.)Mid game: Generate $$$ so you can buy planets and tarot cards. Once you're happy with your jokers, you want to level up the hands that work well with those Jokers. Try easier hands like Two Pair or Pair or High Card. (Assuming you can generate enough cash to level up those hands.)Tip: Blue Seals are helpful!To Win: Look for jokers that get you increasingly higher +mult or xmult for things you'll do anyways. There are jokers that increase mult for using tarot cards, or jokers that increase xmult for adding new cards to your deck.Tip: Steel cards are also awesome.Tip: If you have an xmult joker, move it to the right side, as jokers trigger from left to right. You want to multiply after you add!Disclaimer: I have not beaten ante 11 yet. I can get a couple million points with my strategy, but 7+ million is too difficult. I don't see an easy path to naneinf (not a typo). I guess I'll need to give up and watch some YouTube guides as some point.I feel like I win about 50% or more of my games on regular white stake difficulty. I've won some red stakes, but I don't really feel a need to make the game much harder. I think if I played it safe I could win 75% or more of my games on white stake. The reason my win rate on base difficulty is lower is because I like to try weird strategies with different jokers.The amazing thing about Balatro is.... there are many many different paths to a victory. Everyone has their own favorite joker, and it might all be different.For a long time, I liked two pair hands the most. Now most of my games use pair or high card. I find it too difficult to get to ante 9+ with a 5-card strategy. (4-card flushes are OK.)\n \nreply",
      "> - You know from the start you may not win every round.\n> - Things can instantly and dramatically turn one way or another.Nailed it. A good rogue-like deck-builder should always have these qualities. My favorite for a few years now - Slay the Spire - lives by this.\n \nreply",
      "Maybe it\u2019s a comfort game for you. But it\u2019s an addiction for me. I need to stop, so I can find something else to get addicted to.\n \nreply",
      "Hello, it's me, Factorio.\n \nreply",
      "I have known about this game for a while. I have never bought it because I fear for my productivity.\n \nreply",
      "Factorio is not a game, it's a lifestyle.\n \nreply",
      "I don't know how you can say that with a straight face. Balatro is building on 50 years of addiction-seeking game design. Everything from the sound effects, to the random round rewards, to the pacing of unlocks is optimized to be as attention-grabbing and dopamine-releasing as possible.It's like praising Coca-Cola for not tasting as sweet as Pepsi\n \nreply"
    ],
    "link": "https://localthunk.com/blog/solitaire",
    "first_paragraph": "I have cited a few games as inspiration for Balatro in the past, but I wanted to talk about one in particular that hasn\u2019t been mentioned much that arguably is the most important.I think if I had some kind of Balatro vision board, solitaire (Klondike) would be right in the middle of it with a big red circle around it. You can probably see some of the similarities between my game and the classic solo card game. I wanted my game to have the same vibe.If you\u2019re somehow unfamiliar, solitaire is a group of card games characterized by solo play. Klondike is usually the variant that most people in the west associate with solitaire, but one could argue even Balatro is technically a solitaire game. Traditional solitaire games exist at the peak of game culture for me. These games are so ubiquitous and accepted by society that almost everyone has some memory of playing them. They have transcended gaming culture more than even the biggest IPs (like Tetris or Mario), and they occupy this very intere"
  },
  {
    "title": "Postgres as a Graph Database: (Ab)Using PgRouting (supabase.com)",
    "points": 53,
    "submitter": "michelpp",
    "submit_time": "2025-02-27T21:07:57 1740690477",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=43198520",
    "comments": [
      "Five years ago I was absolutely frustrated with the state of Graph databases and libraries and tried putting several non-Graph DBMSs behind a NetworkX-like Python interface <https://github.com/unum-cloud/NetworkXum>.When benchmarked, Neo4J crashed on every graph I\u2019ve tried <https://www.unum.cloud/blog/2020-11-12-graphs>, making SQLite and Postgres much more viable options even for network-processing workloads. So I wouldn\u2019t be surprised to learn that people actually use pgRouting and Supabase in that setting.With the rise of Postgres-compatible I\u2019m wondering if it\u2019s worth refreshing the project. Similarly, there are now more Graph DBs like MemGraph  compatible with CYPHER, which should probably work much better than Neo4J.\n \nreply",
      "I had almost exactly the opposite experience, although my dataset was pretty small.We wanted to store a graph in postgres and ended up writing some recursive queries to pull subgraphs then had NetworkX layered over it to do some more complex graph operations. We ended up doing that for a short while but then switched to Neo4j because of how comparatively easy it was to write queries (although the Python support for Neo4j was severely lacking). Never really stressed it out on dataset size though.I did manage to crash Redis' graph plugin pretty quickly when I was testing that.\n \nreply",
      "Not sure what you consider \"quite small\" and I don't know how NetworkX works, but postgresql recursive queries have worked well for me for small graphs.Could you share what the data structure and scale was?\n \nreply",
      "We basically had a single table that we wanted to be able to nest on itself arbitrarily. Think categories and subcategories, maybe 100k nodes/rowsPostgres worked fine but cypher is so much more expressive and handles stuff like loop detection for you, neo4j was much easier to work with. Performance wasn't ever really an issue with either.\n \nreply",
      "Neo4j is pretty bad and very dated. No idea what they\u2019re doing. MemGraph is a much better tool.Really graph is a feature and not a product.\n \nreply",
      "My original goal in this article was to figure out if pgrouting would be a good tool to build a memory-layer (for AI/agents) but the article got a bit long. Early results are promising - I\u2019ll follow up with another article soonthere are some other interesting extensions in this space - onesparse[0] is early in development but pretty exciting as it builds on SuiteSparse which is very mature[0] https://onesparse.com/docs.html\n \nreply",
      "Thanks Paul!  OneSparse author here, we're still in early dev stages (OneSparse requires some new features in postgres that won't be available until pg18) but my plan is to do a benchmark shootout with various graph tool for postgres.  Initial results look good, on my 4-core 11th gen intel laptop we're getting some really good numbers!             LiveJournal                 Orkut\n  Nodes:     3,997,962                   3,072,441\n  Edges:     34,681,185                  117,185,037\n  Triangles: 177,820,130                 627,583,972\n\n                  Seconds  Edges/Second  Seconds  Edges/Second\n  Tri Count LL:   2.69     12,892,634    32.03    3,658,602\n  Tri Count LU:   1.78     19,483,812    16.38    7,156,338\n  Tri Centrality: 1.45     23,918,059    12.22    9,589,610\n  Page Rank:      7.12     4,870,953     23.14    5,064,176\n\nOrkut was as big as I could go due to limited RAM.  One of my constrains is limited access to big enough hardware to do the kinds of Graphs Of Unusual Size (billions of edges, trillions of triangles) where we can really flex the scale that CUDA support gives us.  Stay tuned!\n \nreply",
      "I'm working on a little Poatgres graph db project. The querying and table structure is much simpler for the same task:https://memelang.net/03/\nhttps://github.com/memelang-net/memesql3\n \nreply",
      "Resourceful, but is there a reason to use this approach over pgvector?\n \nreply",
      "There is an unfortunate overloaded of the terms relational and relationships in relational databases.Relationships is association between relations/tables, parent-child, node/edge etc, depending on model, extensions etc..There are three basic models of databases:      model name.  | basic data structure\n      -----------------------------------\n      relational   | tables\n      hierarchical | trees\n      network      | graph \n\nA \"relational\" in RDBMS and Codd's rules is just a table data structure with some additional rules.Part of those rules are a named table, with named and typed attributes (columns) with data in the form of rows of tuples.PgVector is nearest neighbor search for tuple values, often from a single table/relation while PgRouting is graph traversal for relational data.There is a bit more to that and in the relational model the data is independent of the schema, and no RDBMS is pure.It is possibly helpful to realize that pgvector is about finding neighbors among tuples, in that relation/table, and that  it is very different from graph traversal.\n \nreply"
    ],
    "link": "https://supabase.com/blog/pgrouting-postgres-graph-database",
    "first_paragraph": "EnterprisePricingDocsBlog25 Feb 2025\u202213 minute readpgRouting is a Postgres extension. It's often used for finding the \u201cshortest path\u201d between two locations, however it's a hidden gem in Postgres and can be used for basic graph functionality.pgRouting is typically combined with PostGIS for working with geospatial data, but it can also be useful beyond that as a lightweight alternative to Graph extensions like Apache AGE, or specialized graph databases like Neo4j.Let's explore some useful applications of pgRouting and graphs.pgRouting is an extension of PostGIS that provides geospatial routing functionality. You can use it to calculate the shortest path, perform network analysis, and solve complex routing problems on a graph-based structure. Most commonly, this is used in Geographic Information Systems (GIS) for tasks like determining the fastest route between two locations.The power of pgRouting lies in its ability to work with any data structured as a graph. A graph is essentially a ne"
  },
  {
    "title": "A $100 DIY muon tomographer (ieee.org)",
    "points": 219,
    "submitter": "Luc",
    "submit_time": "2025-02-27T15:55:21 1740671721",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=43195525",
    "comments": [
      "This might be the easiest \"particle physics without a particle accelerator\" experiments you can do.  This is your accelerator:https://en.wikipedia.org/wiki/Air_shower_(physics)And the muons produce two pulses in your detector,  one when they are scattered to a stop and another when they decay.  By measuring the time between the pulses you can fit the probability distribution and determine the half life of the mupn which is about 2.2 microseconds [1]  And of course you can take measurements over time,  at different altitudes and in different positions.  If it wasn't for relativistic time dilation,  muons would mostly decay high up in the atmosphere and not reach the ground.This was one of the most popular experiments in the Physics 510 lab,  which was the only class you had to take to get a Physics PhD at Cornell because it was so easy.  It's also popular for high school physics for the same reason.[1] https://en.wikipedia.org/wiki/Muon\n \nreply",
      "It's things like this that remind me there's always a little more to discover. I've built a number of those SBM20 geiger counters but I've never come across this muon detector design before. Seems to be a pretty well known design:* https://www.hackster.io/jdpetrey/muon-detector-23bb72\n* https://www.madexp.it/2024/11/19/muon-and-geiger-counter/\n* https://iopscience.iop.org/article/10.1088/0031-9120/50/3/31...Pro tip - not all the SBM-20 tubes on Ebay are created equal! Some sellers will happily sell you tubes that are shorted, open, or just don't work. The better ones test their tubes.\n \nreply",
      "The cosmic watch detector is cheaper, more reliable (due to above shenanigans), and uses only 1 piece of plastic scintillator.It might take more work though. They don't say it but I believe you should try to see if other scintillator materials work. (Including PET,PSU,PES).http://cosmicwatch.lns.mit.edu/\n \nreply",
      "My new favorite sciencey DIY site. It reminds me of the early Popular Mechanics and Scientific American projects. Now all I need is lots more time (or less responsibility). These are the sorts of things that should be available in middle and high schools everywhere.edit: Adding a link to a page with a list of projects on the site.https://spectrum.ieee.org/topic/diy/\n \nreply",
      "I'm the editor of Spectrum's \"Hands On\" DIY column: thank you so much! The general goal is to have projects that can be done in a weekend or three for less than roughly $300 and which point to something interesting beyond just the build itself. A lot of credit has to go to David Schneider who is the author of this piece, and has contributed many of Spectrum's citizen science projects.BTW, If you want to see just the DIY projects instead of all our DIY-related coverage (which can include e.g. interviews or news articles) another handy link is:https://spectrum.ieee.org/type/hands-on/\n \nreply",
      "Worth noting that IEEE is the professional society for electrical engineers. Spectrum seems to be their kind of pop-sci outreach magazine. But, being under IEEE\u2019s umbrella makes it quite special, pop-sci magazines are always pulled toward the bombastic and over-dramatic. Spectrum, because of their origins in a research/professional EE society seems to remain more\u2026 grounded.\n \nreply",
      "Back when I was a physics student, I helped engineer exactly this experiment as a off-the-shelf solution for schools as a classroom experiment! We used PMTs mounted on top of coffee cans:http://kamiokanne.uni-goettingen.de/gb/kamiokanne.htmThe FTL muons produce Cherenkov radiation in the water in the coffee cans, which is picked up by the PMTs.Using this setup gives a much higher rate, as the surface is much larger compared to geiger tubes. Thus it's possible to quickly capture a sufficient amount of muons.\n \nreply",
      "My good friend convinced another friend to do this for a \"small\" uni project.\nThey showed it worked to a very impressed professor, only to shortly after discover (and not reveal to the professor) that it was disconnected and they were just seeing noise.He passed away a few months after, but this sparked a smile in me.\n \nreply",
      "This is common in early stage research. And, should the students reveal the error to the professor, the reaction should have been along the lines \"thank you for the update! It was a great idea. Ideas can at first appear to work due to luck or a methodology error, then fail. This is normal. Great job self-discovering the failure. Keep trying ideas, this is how the research is done!\"But it is important to eventually do more rigorous checks before going for a public announcement. See Robert Wood's debunking of N-rays for what happens when one does not.\n \nreply",
      "Well, I have 2 experiences of discovering this to be the case in others' setups (one that they reported noise instead of results, or maybe even went further. They reported a massive success, and yes, better \"results\" than I did. They were always reporting zero + some noise as opposed to measurement + noise. Given that it was always reporting zero I might be convinced this was a mistake rather than cheating, but ... they certainly didn't correct anything. Their method just didn't work, so they measured small values, which made the noise on their sensor actually close to the correct value (they \"corrected\" negative values by ignoring the minus sign). Another was a case of dissolved metals suddenly appearing out of nowhere in test tubes, changing the results (where only 3 people, not including me, but including the professor, has access to them) and in one case I was ignored, in the other case punished (essentially fired).\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/diy-muon-tomography",
    "first_paragraph": "Uncover hidden mine shafts and moreWith this detector, muons can be used to map underground structures.In the mid-1960s, the Nobel Prize\u2013winning physicist Luis Alvarez had a wild idea. He proposed using muons, highly penetrating subatomic particles created when cosmic rays strike Earth\u2019s atmosphere, to search for hidden chambers within one of the pyramids of Giza.These muon particles are heavyweight cousins of electrons that travel close to the speed of light. They can penetrate through many meters of solid rock, including the limestone and granite blocks used to build the pyramids. But some of the muons will be absorbed by this dense material, meaning that they can be used to essentially \u201cX-ray\u201d a pyramid, revealing its inner structure. So in 1968, Alvarez and his colleagues began making muon measurements from a chamber located at the base of the Pyramid of Khafre.They didn\u2019t find a hidden chamber, but they did confirm the feasibility of what has come to be called muon tomography. Phy"
  },
  {
    "title": "Launch HN: Bild AI (YC W25) \u2013 Understand Construction Blueprints Using AI",
    "points": 62,
    "submitter": "rooppal",
    "submit_time": "2025-02-27T17:30:51 1740677451",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=43196474",
    "comments": [
      "Awesome! This is a huge opportunity to help a lot of people (clients, subcontractors and builders). A lot of money and time is wasted by the current inefficiencies. We gave takeoff construction plan parsing a go in 2022-2023 but couldn\u2019t get the AI part to work well enough (and still haven\u2019t been able to even with the latest ViT/ CLIP models). There was a lot of interest though!- You\u2019re right, data is very hard to come by. I\u2019m curious, how do you plan to get around this? Outsourcing human labeling? We found it to be a very difficult task.- The subcontractors and local construction companies we talked to were overwhelming excited about the idea.- It\u2019s entire people\u2019s jobs to get this done and done correctly. They sit on site holding the pdfs in their hands, manually counting and calculating. You bet a lot of mistakes occur. They would absolutely love to have a digital assistant for this.- Some of them (especially managers and owners) are quite technical and are using software such as BlueBeam and other CAD software to make these calculations. It\u2019s quite manual currently, but gives great insight into a better solution. This led us to having the user manually select the symbol they wanted counted (which ML struggled to get right). Just getting the part counts (and highlighting them in the pdf) was a huge help!- Impressive you got square footage calculations correct! In our experience, there was way too much variation between architects (and multistep dimension labeling) which made it hard (even for humans) to get right. How has your model generalized OOD thus far?- Are you planning to integrate voice? Many of the subcontractors we worked with are very low tech. They usually talk with their clients in person, on the phone, or maybe text. But they don\u2019t use email or their smart phones for much.I will be following your work! I have friends who would love to use this once it passes the human threshold.\n \nreply",
      "I feel like you must've put this somewhere, but where are you getting the cost information from? If I upload a blueprint and you tell me I need \"x 2x4's at a cost of $y each totaling $z\" for my project where is the y coming from? Can I tell you a specific supplier and then you'll \"scrape\" the cost data? Or maybe I'm misunderstanding what you're doing here.I'm asking because even though I am (mostly) technically illiterate I have asked both ChatGPT and Claude to help me build a scraper for construction material costs, from the suppliers we use, that can be updated in realtime or at least monthly. Haven't done anything with those instructions yet, but I would love nothing more than to use a tool that we could feed a blueprint into and then would tell me, with \"laser-focus accuracy\" <smile> how many x's the project would need and the costs. Even better yet if it could compare costs from suppliers and guide us to the lowest-cost supplier.Edit: oh, while you're thinking of replying, how high fidelity do the blueprints need to be? Again, I'm sure you specify somewhere, but too lazy to find it. How far along the spectrum from \"drawn on a napkin\" to \"fully standardized\" do you accept?\n \nreply",
      "Thanks for such a detailed question! We're focused on material quantity estimates right now. We're using pretty generic averages for costing as our primary users (suppliers) have a way better finger on the pulse of the market and typically change the unit price anyways. We'll work on better cost accuracy though, and are looking to integrate with RSMeans or building our own scrapers.For the second question, it really is most accurate on \"fully standardized\" blueprints due to our training distribution. Will work on improving that as well!\n \nreply",
      "It seems like, with sufficient users entering data (including previous sales/purchases if desired), you could begin to train models to take into account local/regional factors etc. do you have any long term plans to collect user data in such a manner?Separately, it seems like it would be incredibly useful to use your models in various embodied carbon estimation tooling and other decarbonization research streams. Have you thought about partnering with any academic researchers on this? If you are interested, let me know, as I can definitely connect you with a bunch of researchers who would be interested!\n \nreply",
      "I realize you're opening yourself up to criticism if you answer this truthfully, but since suppliers are your primary user (and therefore paying customers I assume?) your pitch is \"supplier, you can do more with less (or no!) people when  specifying material quantities, and in fractions of a second as opposed to the minutes/hours it takes today!\" So, ultimately, if the suppliers fully trusted your solution they would need zero personnel determining quantities? And since your solution's annual licensing cost would likely be a fraction of the price of even a single individual that'd be pretty compelling to the supplier.Best of luck with the business (and with getting to know the corp dev people at Autodesk/Procore/etc.--sorry, couldn't help myself!).\n \nreply",
      "Curious why you're going off of blueprints instead of BIM?The benefit of estimating quantities and cost cycles in with pre-con and business development, the artifacts during the pre-con design phase tend to be different than the takeoff artifacts which are often transformed through BIM.Did you learn something to the contrary? Or are you purposely targeting smaller firms and projects that don't use Bim and maybe won't for a long time?\n \nreply",
      "I really wish BIM took off more, unfortunately most suppliers and GCs are still using blueprints (but at least have moved off paper).\n \nreply",
      "As a developer of STEP, I wish BIM was used less.\n \nreply",
      "Even if projects use BIM internally it\u2019s still completely possible that the final deliverables are all PDFs/drawings/etc generated from BIM/internal models for contractual/liability reasons etc.\n \nreply",
      "I was wondering what the training dataset looked like, and I'm very surprised that they're not using BIM data... although after living with an architect for 20 years, I think there's a joke in here somewhere about contractors and planning.\n \nreply"
    ],
    "link": "item?id=43196474",
    "first_paragraph": ""
  },
  {
    "title": "Mozilla owns \"information you input through Firefox\" (mozilla.org)",
    "points": 40,
    "submitter": "tomp",
    "submit_time": "2025-02-28T00:05:31 1740701131",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=43200065",
    "comments": [
      "> When you upload or input information through Firefox, you hereby grant us a nonexclusive, royalty-free, worldwide license to use that information to help you navigate, experience, and interact with online content as you indicate with your use of Firefox.It's bad that it says that, because the \"us\" in this sentence should absolutely not be doing anything that requires such a license, and should not have a copy of it in order to do so; but \"Mozilla owns\" is also not a correct summary of it.\n \nreply",
      "> Mozilla can suspend or end anyone\u2019s access to Firefox at any time for any reason, including if Mozilla decides not to offer Firefox anymore.On what planet is that free, open source?Can you imagine: \"The Free Software Foundation (FSF) can suspend anyone's access to GNU Emacs at any time for any reason, including if the FSF decides not to offer GNU Emacs any more\".\n \nreply",
      "Discussions(100 points, 1 day ago, 30 comments) https://news.ycombinator.com/item?id=43187423(132 points, 10 hour ago, 106 comments) https://news.ycombinator.com/item?id=43194536\n \nreply"
    ],
    "link": "https://www.mozilla.org/en-US/about/legal/terms/firefox/",
    "first_paragraph": "In addition to Cookies necessary for this site to function, we\u2019d like your permission to set some additional Cookies to better understand your browsing needs and improve your experience. Rest assured \u2014 we value your privacy.\n\n            All products\n            \n\n\n\nOur MissionOur WorkFirefox is free and open source web browser software, built by a community of thousands from all over the world.Please read these Terms of Use (\u201cTerms\u201d) carefully because they explain important information about using your copy of the Firefox software. These Terms are a binding agreement between Mozilla Corporation (\u201cMozilla\u201d) and You. For details about Firefox privacy practices, please read the Firefox Privacy Notice.Mozilla grants you a personal, non-exclusive license to install and use the \u201cExecutable Code\" version of the Firefox web browser, which is the ready-to-run version of Firefox from an authorized source that you can open and use right away.We make the source code for Firefox available to you u"
  },
  {
    "title": "Kastle (YC S24) Is Hiring \u2013 AI for Loan Servicing (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-02-27T21:00:33 1740690033",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/kastle/jobs/ItDVKB7-founding-backend-engineer-at-kastle-s24",
    "first_paragraph": "AI agents for mortgage servicingKastle is an AI platform transforming mortgage servicing by automating loan processing, payment collections, and dispute resolution. We work with some of America\u2019s largest mortgage lenders, helping them scale operations efficiently using AI-driven voice agents. Backed by $2.3M from Y Combinator, Commerce Ventures, and executives from Snapdocs, Google, and WePay, we are redefining loan servicing with AI.We\u2019re looking for a Founding Backend Engineer to build and optimize Kastle\u2019s AI infrastructure. As an early team member, you\u2019ll have a pivotal role in architecting and scaling our AI systems, ensuring high availability, low latency, and compliance with strict financial regulations.You\u2019ll work closely with the founding team to design, develop, and deploy mission-critical AI applications that integrate with financial institutions. This role requires expertise in backend development, distributed systems, and AI integration\u2014ideal for an engineer who thrives in"
  },
  {
    "title": "Show HN: Wireless video streaming on POV bike display (youtube.com)",
    "points": 45,
    "submitter": "polishdude20",
    "submit_time": "2025-02-27T20:39:29 1740688769",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=43198245",
    "comments": [
      "Amazing! Do I understand this correctly: the leds are on the wheel, so you can ride your bike in the dark and show (for example) a tiktok video?\n \nreply",
      "That's right! Although you'd need to have the video clip as a file on your phone rather than just a link to it.\n \nreply",
      "You should consider selling this as a kit or something? There must be some people in Northern Europe who love this!\n \nreply",
      "A company called MonkeyLectric did make a quality commercial product for displaying video, but it retailed for around $900 so I suspect they didn't sell too many and they seem to have shut down. There seems to be stuck off their cheaper simpler models floating around still.This demo looks like it's been achieved with cheap commodity hardware which is cool, but I suspect it's pretty heavy and awkward.https://www.kickstarter.com/projects/minimonkey/monkey-light...\n \nreply",
      "Not commodity at all! Unless you count custom PCB and circuit layout \"commodity\". Also, I suspect mine is lighter than the MonkeyLectric since it doesn't have any supporting backing or weatherproofing. It's just raw PCB zip-tied to the spokes.\n \nreply",
      "Selling as a kit would be cool but the weatherproofing would be difficult to get right I think. Lots more is needed to be done to get it rugged enough for consumer use.Also, all 168 LED's were hand soldered here but I'd need to get them machine laid out and soldered if selling as a kit.\n \nreply",
      "This is great!How many rpm do you need to make it look reasonably good in person?\n \nreply",
      "Okay that was not what I was thinking of when I read the title, but that's an interesting take on the mechanical tv colour wheel concept...\n \nreply",
      "UDP? Websocket?\n \nreply",
      "Post requests! \nUDP could send the packets out of order (which could be fixed maybe).\nWebsocket turns out to have the same speed as just a big 64kb post request!\n \nreply"
    ],
    "link": "https://www.youtube.com/watch?v=o8n-bu2kKnc",
    "first_paragraph": ""
  },
  {
    "title": "A data analysis of speeches at the Oscars (stephenfollows.com)",
    "points": 116,
    "submitter": "PourquoiPas",
    "submit_time": "2025-02-27T12:23:41 1740659021",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=43193714",
    "comments": [
      ">In 2010, the Academy sought to combat this verbosity with a new 45-second rule. In response, some winners sped through their acknowledgements, while others used humour or emotion to buy extra time before the music signalled them off. Occasionally, the orchestra was ignored entirely, with speeches like Adrien Brody\u2019s 2003 win for The Pianist running well over the limit.Brody so clairvoyant that he can ignore limits that don't even exist yet.\n \nreply",
      "Cool. As mentioned at the end, the oscars has site,https://aaspeechesdb.oscars.org/\"This database contains more than 1,500 transcripts of onstage acceptance speeches given by Academy Award winners and acceptors.\"\n \nreply",
      "This was an enjoyable article but the conclusion where he finds the most thanked woman in oscar speeches and gets a response from her puts it over the top. Amazing.\n \nreply",
      "Very cool! I think mentions per word could be a good metric for some of these, otherwise a main takeaway is just \"everyone crams in more stuff now.\"\n \nreply",
      "How exactly was the data evaluated? I would assume that manually checking every speech would be too labor-intensive?\n \nreply",
      "Not really?  It's a lot of work, a multi-week project, but reading a couple hundred word speech can be done in 5 minutes, following a checklist in hand, probably 10 minutes.  Times 12 categories, and 80 years of history, that's a lot of time 160 hours, a working month.  A lot of effort but humanely doable.\n \nreply",
      "That's true, but assumes you have the checklist of what data to analyze in hand when you start out. If you only decide after the fact which familial relationships have interesting trends, you'd have to start over again. It seems more reasonable to start by transcribing everything to text, annotating that text, and then running a lot of scripting to automatically query that data.\n \nreply",
      "They probably just used the speech database that the Academy hosts? https://aaspeechesdb.oscars.org/\n \nreply",
      "Ok, obviously it's _doable_, but is it worth it? Using LLMs for this purpose would have been significantly cheaper, easier and with the right configuration just as reliable. Once the setup works, you could extend the analysis to all kinds of other interesting branches without having to look at a single speech by hand.I would even go so far as to say that _not_ using LLMs for this task would be fairly odd, unless I'm missing something or the author really enjoys a month of manually classifying documents to write an interesting and well-written but not exceedingly outstanding article.\n \nreply",
      "Some people like doing stuff.\n \nreply"
    ],
    "link": "https://stephenfollows.com/p/harvey-weinstein-thanked-more-than-god",
    "first_paragraph": ""
  },
  {
    "title": "What can be computed? A practical guide to the theory of computation (2018) [pdf] (softouch.on.ca)",
    "points": 45,
    "submitter": "nill0",
    "submit_time": "2025-02-27T18:52:01 1740682321",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.softouch.on.ca/kb/data/What%20Can%20Be%20Computed.pdf",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Superglue \u2013 open source API connector that writes its own code (github.com/superglue-ai)",
    "points": 122,
    "submitter": "adinagoerres",
    "submit_time": "2025-02-27T17:20:22 1740676822",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=43196374",
    "comments": [
      "Once you have the OpenAPI specs, you can build an MCP server on top of that. Automatically. You don't have them ? There's MITM2Swagger[0] that will do it's best to infer it. Probably you'll need some manual adjustments, but still. And MCP servers can now be integrated with any LLM, not only Anthropic. While I appreciate your approach, how do you fight the MCPs?[0] https://github.com/alufers/mitmproxy2swagger\n \nreply",
      "Thanks for sharing! We're taking a bit of a different angle here. The APIs we are looking at are not the ones that websites are using, but rather then ones you would typically integrate with when thinking about data integrations. Also, while you could use superglue as an MCP server, the usecases we see right now are less in the AI / agent world but rather in the workflow / ETL / onboarding world.That being said, the mitmproxy2swagger approach is really really cool as an alternative to mindless scraping.\n \nreply",
      "> Automatically generates the API configuration by analyzing API docs.The problem with a lot (most?) integration work is that often there simply aren't any API docs - or the docs are outdated/obsolete (because they were written by-hand in an MS Word doc and never kept up-to-date) - or sometimes there isn't an API in the first place (c.f. screen-scraping, but also exfiltration via other means). Are these scenarios you expect or hope to accommodate?\n \nreply",
      "you can give it any context you have, worst case in text form, and the llm will try to figure it out, call different endpoints etc. Recently someone mentioned to me the intern test by Hamel Husain: if avg college student can suceed with the given input (with a lot of trying and time), then llms should be able to do it too. So that's the bar we're aiming for.No api at all is out of scope for now, there are other tools that are better suited for that.\n \nreply",
      "Really nice idea and product. Does it update and cache changed schema for the target API? For ex. an app makes frequent get calls to retrieve list of houses but API changed with new schema, would Superglue figure it out at runtime or is it updating schema regularly for target API based on their API docs (assuming they have it)?\n \nreply",
      "Yes, it does update and cache changed schema for the target API. At runtime. The way it works that every time you make a call to superglue, we get the data from the source and apply the jsonata (that's very fast). We then validate the result against the json schema that you gave us. If it doesn't match, e.g. because the source changed or a required field is missing, we rerun the jsonata generation and try to fix it.I guess you could regularly run the api just to make sure the mapping is still up to date and there are no delays when you actually need the data, depending on how often the api changes.\n \nreply",
      "Really cool project! I'm very bullish on LLMs for structured data.Curious - why did you decide to open source? It's neat to see a lot new YC open source companies. I'm curious why you thought open-sourcing superglue was strategically advantageous\n \nreply",
      "Thanks! The primary reason is because we want folks to be able to run this locally and contribute to the project / fix issues as they come up. This is much harder when you have a black box tool and rely on our small team for support.\n \nreply",
      "Such a cool company name\n \nreply",
      "Great idea, congrats. Can you speak a bit about the the validation piece? Were LLM hallucinations an issue and required this? Are you using some kind of structured output feature?\n \nreply"
    ],
    "link": "https://github.com/superglue-ai/superglue",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Self-healing open source data connector. Use it as a layer between you and any complex / legacy APIs and always get the data that you want in the format you expect.\n      \n\nsuperglue is a self-healing open source data connector. You can deploy it as a proxy between you and any complex / legacy APIs and always get the data that you want in the format you expect.Here's how it works: You define your desired data schema and provide basic instructions about an API endpoint (like \"get all issues from jira\"). Superglue then does the following:\n\n\n\n\n\nIf you\u2019re spending a lot of time writing code connecting to weird APIs, fumbling with custom fields in foreign language ERPs, mapping JSONs, extracting data from compressed CSVs sitting on FTP servers, and making sure your integrations don\u2019t break when something unexpected comes through, supergl"
  },
  {
    "title": "Show HN: Probly \u2013 Spreadsheets, Python, and AI in the browser (github.com/pragmaticmachinelearning)",
    "points": 126,
    "submitter": "tobiadefami",
    "submit_time": "2025-02-27T15:02:39 1740668559",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=43194971",
    "comments": [
      "I have a pressing need to come up with a household budget and had already decided to try using LLMs to help on this task since learning LLMs/prompt engineering is more fun than just writing a dumb script to do accounts.Thought i would try this tool - and here's a quick review of the experience:- the quickstart instructions are very clear and I was up and running on my localhost (a mac - but I think this will work well on windows and linx too)- the UX is good ... slight wrinkle is that the upload button has a down arrow ... also Ctrl+Shift+/ doesn't work on a mac - took me a while to find the speech bubble icon in the bottom right- love the import / export, love the chat box - worked well with my existing OpenAI accountSo - this is a fantastic concept and a well executed MLP - thanks.That said - and I highly encourage you to keep going - there are a couple of caveats:1. The task I set is realworld - \"please categorize my bank transactions into household expense groups\" - and proved too much for my ChatGPT o1 account - most lines were labelled as 'other', bank charges were labelled 'fuel', etc, etc - so the underlying AI engine is not yet ready for this sadly (I am happy to be corrected if others know the recipe)2. I wonder if using a tool like this, a set of LLM prompts to set up the query and to comb the response would help to chip away at [1] ... so I suggest that having a way for my config to accumulate my prompts maybe a nice feature.Please do not take this f/back as negative to your work ... it is more my getting to grips with the AI sweet spot.\n \nreply",
      "This is nice, is there a limit to the data set been provided?\n \nreply",
      "One of the things that has seemed suboptimal to me is having AI \u201cwrite code\u201d.Doesn\u2019t it make more sense to ask AI a question, and the AI figures out what code is needed to answer the question, run it, and report the answer?From the description sounds like this project is a step in that direction.\n \nreply",
      "OTOH, what is \"code\"? In a general sense, I think of \"code\" as the \"codification of a process.\" If we want to know what steps the AI is following to complete a process, then having an AI write code seems like a correct and necessary part of the solution.\n \nreply",
      "Any plans to add a config for a Dockerfile/docker-compose.yml? This could be really useful in a self-hosted environment. If you go down this route, the ability to use something like Ollama in place of OpenAI would be a nice feature as well.\n \nreply",
      "Very cool!! Do y'all have an use-your-own-key example deployment to try it?\n \nreply",
      "Thanks you!Right now, you can clone the repo and follow the instructions to run it locally with your own OpenAI key. I'm working on a hosted demo that will let you try it out directly without any setup. stay tuned :)\n \nreply",
      "I can see ChatGPT including a spreadsheet component like this in their chat one day.\n \nreply",
      "Amazing name for this tool.\n \nreply",
      "Glad you like it!\n \nreply"
    ],
    "link": "https://github.com/PragmaticMachineLearning/probly",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          An AI-powered spreadsheet application that combines spreadsheet functionality with Python data analysis capabilities.Clone the repositoryInstall dependencies:Create a .env file in the root directory with your OpenAI API key:Development mode:Production build:For comprehensive documentation, visit the Probly Documentation.MIT License"
  }
]