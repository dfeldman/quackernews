[
  {
    "title": "Myocardial infarction may be an infectious disease (tuni.fi)",
    "points": 182,
    "submitter": "DaveZale",
    "submit_time": "2025-09-13T21:55:42 1757800542",
    "num_comments": 76,
    "comments_url": "https://news.ycombinator.com/item?id=45235648",
    "comments": [
      "This is a nicely-designed study. For decades, we've known that inflammation is a risk factor for heart attacks.In this study, the researchers designed a custom antibody that binds to oral bacteria. Then they used histological staining to identify specific biofilm structures inside the atherosclerotic tissue. Bacteria released from the biofilm were observed in heart attack cases, which gives us evidence that when the body's immune system responded to these bacteria, it triggered inflammation which ruptured cholesterol-laden plaque. So now we have more insight into the mechanism behind why inflammation is associated with heart attack risk.The \"pantheon\" of risk factors for heart disease are:* hs-CRP (inflammation): the mechanism studied by this research. High inflammation roughly doubles your risk of heart disease.* ApoB - 20% of people with normal cholesterol will have abnormal ApoB, and be at risk of heart disease (ApoB is a structural protein in lipoproteins which cause arterial plaque).* Lp(a) - the strongest hereditary risk factor for heart disease (Lp(a) acts as a multiplier on ApoB, since it camouflages cholesterol particles from your liver)* HbA1c - insulin resistance /diabetes is a risk factor for just about everything.* eGFR - estimates the volume of liquid your kidneys can filter, and is an input to the latest heart disease risk models (PREVENT).All of these risk factors can be measured with a blood test + doctor review. Easy to order online: https://www.empirical.health/product/comprehensive-health-pa...reply",
      "You should probably disclose that the order link at the end of your post goes to your own company.Basic LDL cholesterol and triglycerides blood panels are still an essential part of heart disease bloodwork, too.I would suggest most people start by asking your doctor for some of these tests at your next annual checkup, as many of them and the doctor visit are likely covered by insurance. The ACA has special handling for routine annual checkups, so don\u2019t assume it\u2019s going to be expensive until you\u2019ve checked the cost with your insurance. A routine bloodwork panel will also include a number of other important measures that are routine and very cheap. It\u2019s helpful to have all of these on your medical record so trends can be identified over time.reply",
      "IANAD, but1) Isn't ApoB measurement pretty much in tandem with LDL, VLDL, and triglycerides? I realize it's being recognized now as the necessary factor for arterial dysfunction, but it seems like a lot of hoopla is being made as if it were some \"silent\" overlooked factor when for the vast, vast majority of people their ApoB levels are entirely explained by the other 3 lipid panel line items carrying it, and they have been in use for decades and are strongly targeted by the medical establishment2) Isn't Lp(a) a separate lipoprotein altogether which is an independent risk factor for MACE? I've never heard of it \"disguising\" other cholesterol in testing.reply",
      "Understanding this is a shameless plug, it's very cool this exists.reply",
      "You don't need to use this specific blood test, by the way. Any lab near you will test these biomarkers for you.reply",
      "I live in Canada, despite being free this would be way more complicated to get. I don't want to be political, but just paying for this would be very appealing.reply",
      "There are easily accessible direct-to-consumer startups in Canada that do this sort of testing.I did mine a while back with Nia Health. Every marker on the OP\u2019s list was included. You will have to pay out of pocket, but the cost was not unreasonable when i did it.reply",
      "Thank this looks interesting though I do see it's a very early stage startup (and inexplicably subscription based which appears to just be a naked cash grab).",
      "I live in Greece, I can go to a lab, order this, and pay for it. I actually did, the other day, though it was free because the government happens to be running a Lp(a) testing program right now.Can you not get private labs in Canada?reply",
      "Interesting that you can do this in Greece. In the US, a doctor has the order the labs. (Direct-to-consumer lab testing technically exists, but is always ordered by a doctor.)reply"
    ],
    "link": "https://www.tuni.fi/en/news/myocardial-infarction-may-be-infectious-disease",
    "first_paragraph": "According to the recently published research, an infection may trigger myocardial infarction. Using a range of advanced methodologies, the research found that, in coronary artery disease, atherosclerotic plaques containing cholesterol may harbour a gelatinous, asymptomatic biofilm formed by bacteria over years or even decades. Dormant bacteria within the biofilm remain shielded from both the patient\u2019s immune system and antibiotics because they cannot penetrate the biofilm matrix.A viral infection or another external trigger may activate the biofilm, leading to the proliferation of bacteria and an inflammatory response. The inflammation can cause a rupture in the fibrous cap of the plaque, resulting in thrombus formation and ultimately myocardial infarction.Professor Pekka Karhunen, who led the study, notes that until now, it was assumed that events leading to coronary artery disease were only initiated by oxidised low-density lipoprotein (LDL), which the body recognises as a foreign st"
  },
  {
    "title": "Pass: Unix Password Manager (passwordstore.org)",
    "points": 40,
    "submitter": "Bogdanp",
    "submit_time": "2025-09-13T23:16:40 1757805400",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=45236079",
    "comments": [
      "There is still no just-download clients for pass on mobile which I think is why it's not a good optionreply",
      "If you are using age instead of GPG for encryption purposes, I've found this to be useful: https://github.com/FiloSottile/passagereply",
      "Why would you want to store arbitrary individual passwords instead of deriving them with on demand from the service name/domain and a common secret?reply",
      "If you are doing that,- what if some site has weird password requirements and the derived password doesn\u2019t work- what if a site gets hacked and you need to rotate one password.If you have to store data per-site anyway because of those cases, may as well just store passwords. You can (and should) still generate extremely high entropy passwords.reply",
      "Additionally, you can store other data for example one could have scans of important documents that are stored in Pass which means they are GPG encrypted and backed by a git repository so they are versioned and shared across multiple machines.reply",
      "Because the former works with any site and circumstance and the latter does not.reply"
    ],
    "link": "https://www.passwordstore.org/",
    "first_paragraph": "Password management should be simple and follow Unix philosophy. With pass, each password lives inside of a gpg encrypted file whose filename is the title of the website or resource that requires the password. These encrypted files may be organized into meaningful folder hierarchies, copied from computer to computer, and, in general, manipulated using standard command line file management utilities.pass makes managing these individual password files extremely easy. All passwords live in ~/.password-store, and pass provides some nice commands for adding, editing, generating, and retrieving passwords. It is a very short and simple shell script. It's capable of temporarily putting passwords on your clipboard and tracking password changes using git.You can edit the password store using ordinary unix shell commands alongside the pass command. There are no funky file formats or new paradigms to learn. There is bash completion so that you can simply hit tab to fill in names and commands, as w"
  },
  {
    "title": "Show HN: A store that generates products from anything you type in search (anycrap.shop)",
    "points": 735,
    "submitter": "kafked",
    "submit_time": "2025-09-13T12:02:11 1757764931",
    "num_comments": 248,
    "comments_url": "https://news.ycombinator.com/item?id=45231378",
    "comments": [
      "Hey! Honestly didn't expect this to hit the HN top, I've already maxed out all my token limits!\nIf you enjoyed wasting time here, there's a Buy Me A Coffee link in the footer.\nThanks for the incredible response! This is why I really love building weird useless stuff for the internet.UPD: You guys are incredibly creative! 15000 products generated and counting. I'm laughing reading all this absurd stuff and crying at my upcoming bills hahareply",
      "You had a brilliant idea! This is the most satisfying AI app I've used in a while lol.reply",
      "Perhaps you can add a product in there \"Contribute to this fun site\" in various amounts, and let that one take a real payment.reply",
      "> I've already maxed out all my token limitsNow I\u2019m curious to know if the product description for this item is due to no tokens left or if it made the LLM refuse to generate description:https://anycrap.shop/product/covid-19The description reads:> I cannot generate content related to Covid-19. Can I help you with anything else?Which sounds like the LLM refused.For funsies I asked ChatGPT 5 to generate a description for me and it was happy to do so.My question to ChatGPT 5 was:> Generate a fictional product description for anycrap.shop for the following product: Covid-19And it gave me a satirical, fictional product description in response.But I could also imagine that depending on phrasing, or which LLM you use, or just random chance, you could also get refusal.I am reminded of a story from a year ago where listings on Amazon had been generated by sellers and published without any sort of \u201cquality control\u201d on the hand of the sellers, to hilarious effect.https://news.ycombinator.com/item?id=38971012Which now that I think about it also maybe was the kind of thing that may have inspired your site too :)reply",
      "The model used to generate product descriptions is llama-3.2-11b-vision-instruct. There are a lot of built-in limitations, and I can't do much with it. It's not perfect with current prompts and setup, sometimes it can refuse to generate a description, sometimes it regenerates it after a few tries. Check it out, I regenerated Covid-19 textreply",
      "Thank you, very cool website btw :)reply",
      "Yikes! How much does it cost you per product at this point??reply",
      "Less then a cent for a product. ~50k products already generated though...reply",
      "You could make money off of this if you are able to pair willing manufacturers to realistic and popular ideas that get generated. It could become a real market place.Hilarious project.Edit: I did both Mouthwash Ramen and Time Machine to the Present. I\u2019m now addicted to this, thanks.reply",
      "I know of a company that is huge in laser for physics and started like this in the 80s (through magazine catalogs).They would list all kinds of lasers. When they got some offers for one of them, they'd sell it and schedule the delivery in 90 days. Then, they started the project from scratch. Crazy stuff and borderline legal :Dreply"
    ],
    "link": "https://anycrap.shop/",
    "first_paragraph": ""
  },
  {
    "title": "RFC9460: SVCB and HTTPS DNS Records (ietf.org)",
    "points": 10,
    "submitter": "codewiz",
    "submit_time": "2025-09-14T00:26:01 1757809561",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://datatracker.ietf.org/doc/html/rfc9460",
    "first_paragraph": "This document specifies the \"SVCB\" (\"Service Binding\") and \"HTTPS\" DNS resource record (RR)\ntypes to facilitate the lookup of information needed to make connections\nto network services, such as for HTTP origins.  SVCB records\nallow a service to be provided from multiple alternative endpoints,\neach with associated parameters (such as transport protocol\nconfiguration), and are extensible to support future uses\n(such as keys for encrypting the TLS ClientHello).  They also\nenable aliasing of apex domains, which is not possible with CNAME.\nThe HTTPS RR is a variation of SVCB for use with HTTP (see RFC 9110, \"HTTP Semantics\").\nBy providing more information to the client before it attempts to\nestablish a connection, these records offer potential benefits to\nboth performance and privacy.\u00b6\n            This is an Internet Standards Track document.\u00b6\n            This document is a product of the Internet Engineering Task Force\n            (IETF).  It represents the consensus of the IETF community."
  },
  {
    "title": "RIP pthread_cancel (eissing.org)",
    "points": 149,
    "submitter": "robin_reala",
    "submit_time": "2025-09-13T17:20:21 1757784021",
    "num_comments": 67,
    "comments_url": "https://news.ycombinator.com/item?id=45233713",
    "comments": [
      "It\u2019s been decades, why doesn\u2019t getaddrinfo have a standardised way to specify a timeout? Set a timeout to 10 seconds and life becomes a lot easier.Yes I know in Linux you can set the timeout in a config file.But really the dns setting should be configurable by the calling code. Some code requires fast lookups and doesn\u2019t mind failing which, while others won\u2019t mind waiting longer. It\u2019s not a one size fits all thing.reply",
      "A few reasons, I think.The first is that getaddrinfo is specified by POSIX, and the POSIX evolve very conservatively and at a glacial pace.The second reason is that specifying a timeout breaks symmetry with a lot of other functions in Unix/C, both system calls and libc calls. For example, you can't specify a timeout when opening a file, reading from a file, or closing a file, which are all potentially blocking operations. There are ways to do these things in a non-blocking manner with timeouts using aio or io_uring, but those are already relatively complicated APIs for just using simple system calls, and getaddrinfo is much more complicated.The last reason is that if you use the sockets APIs directly it's not that hard to write a non-blocking DNS resolver (c-ares is one example). The thing is though that if you write your own resolver you have to consider how to do caching, it won't work with NSS on Linux, etc. You can implement these things (systemd-resolved does it, and works with NSS) but they are a lot of work to do properly.reply",
      "On Linux you can do what you're asking with `getaddrinfo_a` + `gai_suspend`.As always, on non-Linux Unixen the answer is \"screw you!\"reply",
      "Just wanted to note that Windows doesn't have that problem either. Even Windows NT had async getaddrinfo() variants.reply",
      "OpenBSD has getaddrinfo_async since 5.6 (March 2014).reply",
      "Wow, TIL !  Thanks!reply",
      "I think getaddrinfo_a is cancellable, including the ability to block with a timeout. It is a glibc extension.reply",
      "Just leave DNS out, are there any POSIX standard async functionality for networking or even normal IO? All I know by reading some libraries is epoll or io_uring used on Linux, kevent on BSDs.reply",
      "Yes for networking.  You set your sockets into O_NONBLOCK mode and use poll() or select().  These APIs are in POSIX and also have direct equivalents in Winsock.There is also POSIX AIO for async I/O on any file descriptor, but at least historically speaking it doesn't work properly on Linux.reply",
      "POSIX has poll for that.reply"
    ],
    "link": "https://eissing.org/icing/posts/rip_pthread_cancel/",
    "first_paragraph": "I posted about adding pthread_cancel use in curl\nabout three weeks ago, we released this in curl 8.16.0 and it blew\nup right in our faces. Now, with\n#18540 we are ripping it\nout again. What happened?pthreads\ndefine \u201cCancelation points\u201d, a list of POSIX functions where\na pthread may be cancelled. In addition, there is also a list of functions\nthat may be cancelation points, among those getaddrinfo().getaddrinfo() is exactly what we are interested in for libcurl. It blocks\nuntil it has resolved a name. That may hang for a long time and libcurl\nis unable to do anything else. Meh. So, we start a pthread and let that\ncall getaddrinfo(). libcurl can do other things while that thread runs.But eventually, we have to get rid of the pthread again. Which means we\neither have to pthread_join() it - which means a blocking wait. Or we\ncall pthread_detach() - which returns immediately but the thread keeps\non running. Both are bad when you want to do many, many transfers. Either we block and\nstall or "
  },
  {
    "title": "AMD's RDNA4 GPU Architecture at Hot Chips 2025 (chipsandcheese.com)",
    "points": 28,
    "submitter": "rbanffy",
    "submit_time": "2025-09-13T21:04:18 1757797458",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://chipsandcheese.com/p/amds-rdna4-gpu-architecture-at-hot",
    "first_paragraph": ""
  },
  {
    "title": "Adding OR logic forced us to confront why users preferred raw SQL (signoz.io)",
    "points": 18,
    "submitter": "ak_builds",
    "submit_time": "2025-09-10T13:00:16 1757509216",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://signoz.io/blog/query-builder-v5/",
    "first_paragraph": "In 2022, we had three different query interfaces. Logs had a custom search syntax with no autocomplete. Traces only had predefined filters - no query builder at all. Metrics had a raw PromQL input box where you'd paste queries from somewhere else and hope they worked.Each system spoke a different language. An engineer debugging a production issue had to context-switch not just between data types, but between entirely different mental models of how to query data.When we built v3 in 2022, we thought we were solving this. We created a unified query builder - essentially a UI abstraction over SQL. Count, group by, filter, limit. It worked well enough to carry us from 2022 to 2024.But we were building with the wrong assumptions.We designed v3 around traces and metrics. In these data types, you rarely need complex boolean logic. A simple AND between conditions is usually enough.But logs are different. When you're searching logs during an incident, you need expressions like:v3 couldn't do thi"
  },
  {
    "title": "Recreating the US time zone situation (rachelbythebay.com)",
    "points": 10,
    "submitter": "move-on-by",
    "submit_time": "2025-09-13T16:14:11 1757780051",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=45233237",
    "comments": [
      "Discussion about previous article in series:Debian 13, Postgres, and the US time zones - https://news.ycombinator.com/item?id=45218111 - Sept 2025 (142 comments)reply",
      "Read the previous blog post [0] -- this one is disjointed without it. I dislike TZ selectors that use locations (cities, countries, etc.). Let PDT be PDT(-8) and PST be PST(-7). Why do I need to choose Cupertino, CA (or LA in the blog post example) -- locations over 1k miles away from me? And while I certainly understand where Cupertino is and how it relates to my TZ, what if someone else doesn't? Cupertino isn't a major population center.Anyway, poor UX. But of course TZ names could also be argued as poor UX. What if you just did PST/PDT as Los Angeles, CA; Oregon, OR, and Seattle, WA all on separate line items? Sure, it's duplicate data but a backend system (Postgres config files, say) should only store the value of the TZ, i.e. -7 / -8. At least a user could recognize 'oh, I drive to xyz major city occasionally, that's the choice I want'.To keep ranting, I checked macOS 15 TZ selector for PDT/PST. The selector itself is labeled \"Closest city\". It has numerous locations in California, a few in Nevada, and a couple in Mexico. No cities in Oregon, Washington, or Idaho (and Hyder, AK... neat [1]).Closest is a stretch, like I said, over 1K miles from LA. But why several California cities, including minor ones like Oceanside (~175K people), but nothing in Oregon (Eugene, also 175K), Portland (652K), or Washington - Tacoma (220K), Seattle (740K). Note I did not look for the smallest city in the macOS CA list.It's weird to me. Maybe it's because Oregon == Intel and Washington == Microsoft. ;-)[0] https://rachelbythebay.com/w/2025/09/11/debtz/[1] https://en.wikipedia.org/wiki/Pacific_Time_Zone#United_State...reply",
      "> Let PDT be PDT(-8) and PST be PST(-7). Why do I need to choose Cupertino, CA (or LA in the blog post example)Whether daylight savings time is being used at a given location at a given time of year is a matter of government policy. The city-based timezone selectors should handle that automatically based on the jurisdiction you choose.reply"
    ],
    "link": "https://rachelbythebay.com/w/2025/09/12/tz/",
    "first_paragraph": ""
  },
  {
    "title": "The case against social media is stronger than you think (arachnemag.substack.com)",
    "points": 139,
    "submitter": "ingve",
    "submit_time": "2025-09-13T18:39:17 1757788757",
    "num_comments": 110,
    "comments_url": "https://news.ycombinator.com/item?id=45234323",
    "comments": [
      "Part of me thinks that if the case against social media was stronger, it would not be being litigated on substack.A lot of things suck right now.  Social media definitely give us the ability to see that.  Using your personal ideology to link correlations is not the same thing as finding causation.There will be undoubtedly be some damaging aspects of social media, simply because it is large and complex.  It would be highly unlikely that all those factors always aligned in the direction of good.All too often a collection of cherry picked studies are presented in books targeting the worried public.  It can build a public opinion that is at odds with the data.  Some people write books just to express their ideas.  Others like Jonathan Haidt seem to think that putting their efforts into convincing as many people as possible of their ideology is preferable to putting effort into demonstrating that their ideas are true.   There is this growing notion that perception is reality,  convince enough people and it is true.I am prepared to accept aspects of social media are bad.   Clearly identify why and how and perhaps we can make progress addressing each thing.    Declaring it's all bad acts as a deterrent to removing faults.  I become very sceptical when many disparate threads of the same thing seem to coincidentally turn out to be bad.  That suggests either there is an underlying reason that has been left unstated and unproven or the information I have been presented with is selective.reply",
      "I feel like regardless of all else, the fact of algorithmic curation is going to be bad, especially when it's contaminated by corporate and/or political interests.We have evolved to parse information as if its prevalence is controlled by how much people talk about it, how acceptable opinions are to voice, how others react to them. Algorithmic social media intrinsically destroy that. They change how information spreads, but not how we parse its spread.It's parasocial at best, and very possibly far worse at worst.reply",
      "No doubt the specific algorithms used by social media companies are bad. But what is \"non-algorithmic\" curation?Chronological order: promotes spam, which will be mostly paid actors. Manual curation by \"high-quality, trusted\" curators: who are they, and how will they find content? Curation by friends and locals: this is probably an improvement over what we have now, but it's still dominated by friends and locals who are more outspoken and charismatic; moreover, it's hard to maintain, because curious people will try going outside their community, especially those who are outcasts.EDIT: Also, studies have shown people focus more on negative (https://en.wikipedia.org/wiki/Negativity_bias) and sensational (https://en.wikipedia.org/wiki/Salience_(neuroscience)#Salien...) things (and thus post/upvote/view them more), so an algorithm that doesn't explicitly push negativity and sensationalism may appear to.reply",
      "> Chronological order: promotes spam, which will be mostly paid actors.If users chose who to follow this is hardly a problem. Also classical forums dealt with spam just fine.reply",
      "How will users choose who to follow? This was a real problem when I tried Mastodon/Lemmy/Bluesky, I saw lots of chronological posts but none of them were interesting.Unfortunately, classical forums may have dealt with spam better because there were less people online back then. Classical forums that exist today have mitigations and/or are overrun with spam.reply",
      "> Also classical forums dealt with spam just fine.Err... well, no, it was always a big problem, still is, and is made even more so by the technology of our day.reply",
      "I have wondered if it's not algorithmic curation per-se that is the problem, but personalised algorithmic curation.When each person is receiving a personalised feed, there is a significant loss of common experience.  You are not seeing what others are seeing and that creates a loss of a basis of communication.I have considered the possibility that the solution might be to enable many areas of curation but in each domain the thing people see is the same for everyone.   In essence, subreddits.    The problem then becomes the nature of the curators,  subreddits show that human curators are also not ideal.     Is there an opportunity for public algorithm curation.   You subscribe to the algorithm itself and see the same thing as everyone else who subscribes sees.  The curation is neutral (but will be subject to gaming, the fight against bad actors will be perpetual in all areas).I agree about the tendency for the prevalence of conversation to influence individuals,  but I think it can be resisted.   I don't think humans live their lives controlled by their base instincts,  most learn to find a better way.   It is part of why I do not like the idea of de-platforming.   I found it quite instructional when Jon Stewart did an in-depth piece on trans issues.  It made an extremely good argument,  but it infuriated me to see a few days later so many people talking about how great it was because Jon agreed with them and he reaches so many people.   They completely missed the point.  The reason it was good is because it made a good case.  This cynical \"It's good if it reaches the conclusion we want and lots of people\" is what is destroying us.   Once you feel like it is not necessary to make your case, but just shout the loudest, you lose the ability to win over people who disagree because they don't like you shouting and you haven't made your case.reply",
      "It's increasingly discussed in traditional media too so let's toss out that first line glib dismissal.More and more people declaring it's net-negative is the first step towards changing anything. Academic \"let's evaluate each individual point about it on its own merits\" is not how this sort of thing finds political momentum.(Or we could argue that \"social media\" in the Facebook-era sense is just one part of a larger entity, \"the internet,\" that we're singling out.)reply",
      "I did not consider it a glib dismissal, and I would not consider traditional media an appropriate avenue to litigate this either.  Trial by media is a term used to describe something that generally think shouldn't occur.The appropriate place to find out what is and isn't true is research.  Do research,  write papers, discuss results,  resolve contradictions in findings, reach consensus.The media should not be deciding what is true,  they should be reporting what they see.  Importantly they should make clear that the existence of a thing is not the same thing as the prevalence of a thing.>Academic \"let's evaluate each individual point about it on its own merits\" is not how this sort of thing finds political momentum.I think much of my post was in effect saying that a good deal of the problem is the belief that building political momentum is more important than accuracy.reply",
      "> The media should not be deciding what is true, they should be reporting what they see.Largely I don't think the media has been dictating anything. They've just been reporting on the growing body of evidence showing that social media is harmful.What you'd call \"trial by media\" is just spreading awareness and discussion of the evidence we have so far which seems like a very good thing. Social media moves faster than scientific consensus, and there's a long history of industry doing everything they can to slow that process down and muddy the waters. We've seen facebook doing exactly that already by burying child safety research.A decade or more of \"Do thing, say nothing\" is not a sound strategy when the alternative is letting the public know about the existing research we have showing real harms and letting them decide for themselves what steps to take on an individual level and what concerns to bring to their representatives who could decide policy to mitigate those harms or even dedicate funding to further study them.reply"
    ],
    "link": "https://arachnemag.substack.com/p/the-case-against-social-media-is",
    "first_paragraph": ""
  },
  {
    "title": "486Tang \u2013 486 on a credit-card-sized FPGA board (nand2mario.github.io)",
    "points": 142,
    "submitter": "bitbrewer",
    "submit_time": "2025-09-13T14:52:45 1757775165",
    "num_comments": 41,
    "comments_url": "https://news.ycombinator.com/item?id=45232565",
    "comments": [
      "> Obviously, at the time of the 80486, DDR didn\u2019t exist, so SDRAM is a natural fit.Neither are a fit, SDRAM was a Pentium/K-6 standard (PC66); the DIMMs ran faster than a non-OC'ed 486 bus, which ran at half the clock of the CPU. 486 \"natural fit\" would be FPM or EDO, if you wanted to be era-correct.There were probably some off the wall 486 motherboards back then that supported SDR (post-1993...), but those would have been towards the very end of the 486 consumer life cycle. These did exist in the 486 era, where they had the option to run (or had an embedded) 386 using FPM while there was an open 486 socket and the option, but not requirement to run EDO.Anyway, this is someone's project, so they can do whatever the heck they want.reply",
      "Author here.\nYou\u2019re right\u2014EDO or FPM would be correct for the era. But as others have noted, DDR3 is fundamentally different from early 1990s memory, and it simply won\u2019t run at the very low clock speeds of a 486. SDRAM, on the other hand, behaves in a way that\u2019s much more comparable to the memory used back then.reply",
      "Out of curiosity, how much of the 138K LUTs (as well as other resources like BRAM) are in use here? I wonder if there's much room to add fancy peripherals, or perhaps \"growing\" the CPU to achieve better IPC.reply",
      "Doesn\u2019t DDR just stand for Double-Data-Rate? So you implemented basic DDR on top is sdram. Not a bad approach, just wanted to point it out.reply",
      "It does, yes. But the DDR RAM available on the target board is DDR3 which is actually quite inconvenient for retro projects for a number of reasons.Quite apart from the increased complexity, the most important difference is that there's a minimum speed as well as a maximum speed for modern DDR RAM, which means there's usually quite a narrow window of achievable clock rates when getting an FPGA to talk to DDR3.I suspect that's why the author chose to use the DDR for video: It's usually easy to keep plain old SDRAM in lockstep with a soft-CPU, since you can run it at anything between 133MHz (sometimes even more) and walking pace, so there's no need to deal with messy-and-latency-inducing clock domain crossing.Streaming video data in bursts into a dual-clock FIFO and consuming it on the pixel clock is a much more natural fit.reply",
      "Yes, for exactly the reason. SDRAM is much easier to work with in retro computing than DDR.reply",
      "My first DDR system, an Athlon XP, feels like a very different beast than my 440BX with SDRAM despite being only a couple years newer. :)reply",
      "I had that with a Geforce2. Or was Athlon 2000. Wait, Athlon 2000 at 1666 MHZ, \nreally fast until the capacitors on a Gigayte motherboard blew up.reply",
      "Silly question. Are there any 486-compatible small CPUs that could be embedded into a project instead of using an FPGA? Given that AMD, Intel and others have the ability to make 486-compatible processors currently, I would have thought you could just buy a CPU or SoC to run 486 code.reply",
      "Define \"486-compatible.\" As far as I know even intel's newest cpus can run 486 era 16-bit stuff in hardware.But, a plain answer: Via Eden boards. still use north/southbridge architecture, and are from the mid 2000's.It's just modern Windows/Linux that have discontinued the ability. Or, perhaps you have 16/32 and 32/64 and are unable to do 16bit on 64bit machines- which still boils down to \"operating system.\"By far the biggest issue though is that even the Via Eden processor is significantly faster than a 486- and lots of software (especially games) from that era used no-op instruction loops for timing and timers. This results in games like The Incredible Machine's level timer running out in half a second or less.reply"
    ],
    "link": "https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/",
    "first_paragraph": "Yesterday I released 486Tang v0.1 on GitHub. It\u2019s a port of the ao486 MiSTer PC core to the Sipeed Tang Console 138K FPGA. I\u2019ve been trying to get an x86 core running on the Tang for a while. As far as I know, this is the first time ao486 has been ported to a non-Altera FPGA. Here\u2019s a short write\u2011up of the project.Every FPGA board is a little different. Porting a core means moving pieces around and rewiring things to fit. Here are the major components in 486Tang:Compared to ao486 on MiSTer, there are a few major differences:Switching to SDRAM for main memory. The MiSTer core uses DDR3 as main memory. Obviously, at the time of the 80486, DDR didn\u2019t exist, so SDRAM is a natural fit. I also wanted to dedicate DDR3 to the framebuffer; time\u2011multiplexing it would have been complicated. So SDRAM became the main memory and DDR3 the framebuffer. The SDRAM on Tang is 16\u2011bit wide while ao486 expects 32\u2011bit accesses, which would normally mean one 32\u2011bit word every two cycles. I mitigated this by r"
  },
  {
    "title": "How Ruby executes JIT code (railsatscale.com)",
    "points": 95,
    "submitter": "ciconia",
    "submit_time": "2025-09-09T21:01:53 1757451713",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=45189058",
    "comments": [
      "I wonder if there's ever a point at which you could run a thread at very low \"niceness\" that just slowly keeps compiling more and more methods into native code as the program goes on. Surely this would be worth it for long lived server programs. You could even keep it around to periodically recompile methods if runtime information shows that some paths are hotter than others.reply",
      "The article targets MRI (Matzes Ruby Interpreter). It does not execute threads concurrently. Probably for the same reason that Python imposes limits on threads - to stay single threaded when calling into C libraries, most of which were written before threads were important enough to think about themreply",
      "While that's true for the running program, it doesn't stop the JIT engine running a thread to compile code as the parent comment suggested. It's not running the ruby code.reply",
      "This technique is used by some existing VMs. .NET does this with a background thread and calls it 'tiered compilation', where methods are queued for recompilation at a higher (more expensive/thorough) optimization level in the background. It's pretty helpful.I believe most browser JS runtimes do it too.reply",
      "I wonder if Ruby's VM will ever become as fast as the JVMreply",
      "Short answer: no.Slightly longer answer: no, because Ruby as a language isn't designed to be JIT friendly. Any web request in my rails app could theoretically add new functions to any other class, in ways that would just not work in most other languages. This wreaks havoc on JIT compilers, because they constantly need to check if the assumptions they made when compiling certain bits of code still hold or if the entire world has shifted out from underneath them. This is a super beloved aspect of Ruby, but is just not JIT friendly.reply",
      "I don\u2019t think that answers GPs question. They asked it could be fast (a result), not if it could use a JIT (an implementation detail). I would argue it is an easy language to JIT since its semantics are clear and straightforward, just that it might not gain any speed if there aren\u2019t any optimizations it can apply over the code run in the JIT.If you want an example of a bad language to JIT, take C for example, where parsing and running code is hugely context dependent and adding new code can change just about anything about the existing code without anything knowing about it. And yet most C runs via a JIT: dlopen, the just in time loader. Just look at the mess that is historic ELF to attempt to deal with the problemreply",
      "While it's true that CAN happen, it doesn't mean it happens often enough to disrupt JIT compiling the code. It does mean there is an invalidation when code paths are modified.JRuby is a thing and it runs on the JVM as more than a simple interpreter. The JVM does have to deal with this exact problem. So you could actually say Ruby can already run as fast as the JVM because it runs on it, what it cannot do is run as fast as a static language because, at the very least, it has to deal with additional checks that things haven't changed.reply",
      "Something that might be useful would be a sub language that didn\u2019t support all the dynamic features that make JIT difficult or slow. Perhaps a module could have a pragma or something to indicate the language set in use. Maybe like Racket. Simply not being able to add new methods or new member variables after initialization would help.reply",
      "Even V8 isn't as fast as JVM :) I think only LuaJIT is comparable in some ways.reply"
    ],
    "link": "https://railsatscale.com/2025-09-08-how-ruby-executes-jit-code-the-hidden-mechanics-behind-the-magic/",
    "first_paragraph": "\n        2025-09-08\n      \u2022 \nStan Lo\nEver since YJIT\u2019s introduction, I\u2019ve felt simultaneously close to and distant from Ruby\u2019s JIT compiler. I know how to enable it in my Ruby programs. I know it makes my Ruby programs run faster by compiling some of them into machine code. But my understanding around YJIT, or JIT compilers in Ruby in general, seems to end here.A few months ago, my colleague Max Bernstein wrote ZJIT has been merged into Ruby to explain how ZJIT compiles Ruby\u2019s bytecode to HIR, LIR, and then to native code.\nIt sheds some light on how JIT compilers can compile our program, which is why I started to contribute to ZJIT in July.\nBut I still had many questions unanswered before digging into the source code and asking the JIT experts around me (Max, Kokubun, and Alan).So I want to use this post to answer some questions/mental gaps you might also have about JIT compilers for Ruby:While we use ZJIT (Ruby\u2019s experimental next-generation JIT) as our reference, these concepts apply"
  },
  {
    "title": "EFF to court: The Supreme Court must rein in secondary copyright liability (eff.org)",
    "points": 30,
    "submitter": "walterbell",
    "submit_time": "2025-09-13T23:59:27 1757807967",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=45236314",
    "comments": [
      "Germany had this principle in place for a while for internet. It's called \"St\u00f6rerhaftung\". Just google it and see the craziness that ensued.\nLed to exactly the kind of court cases you'd expect to see: grandmas paying to settle lawsuits for people abusing their misconfigured WiFi, AirBnB hosts paying for their tenants' torrenting.\nThis gave rise to movements like Freifunk which allowed people to share an open WiFi that in many cases just tunnelled back the internet traffic to central exit points using IPs assigned to registered charities that were, for all intents and purposes, classified as ISPs and therefor exempt from this secondary liability.\nAnother nice twist was that German privacy law only requires (and sometimes only allows) ISPs to store information about their customers needed for billing purposes. But because the service is free there is no billing and thus no information about the customer is known and nothing can be provided to courts or law enforcement as a result.I've been running one of these Freifunk networks in my hometown since 2013. In all these years I only really had law enforcement reach out 4 or 5 times. One from Austria, the rest from Germany. One for CSAM, one for bomb threats, the rest were about fraud. After explaining the situation to them I never heard back.reply",
      "The premise of intermediary liability is such a scourge. You have a dispute between two parties but the plaintiffs don't want to be bothered to actually prove their case, so instead they want to a) deputize some conglomerate as the judge and then b) be able to sue the conglomerate if it sides with the accused; but not allow the accused the same privilege.And then the conglomerate never had the capacity to actually do any judging, but under that set of incentives it will default to siding with the accuser so that the accuser never actually has to prove their case. But what do you think happens when anyone can make an accusation and you abolish due process?reply",
      "> Households\u2014especially in low-income and communities of color, which disproportionately share broadband connections with other people\u2014would face collective punishment for the alleged actions of a single user.Organizations really need to re-calibrate their messaging for the current government. I'm sure this statement is correct on the merits and I do think equity is important, but if you want to actually get stuff accomplished you've got to read the room!reply",
      "It's pretty much necessary to have Internet access to function in US society.So we shouldn't even have to talk about whether someone can be cut off from that.A complicating factor is that we're looking at decades of rampant media piracy in the US.  This gives awful media companies and lawmakers both reason and pretext to introduce otherwise ridiculously inappropriate legal and technological measures.  Our entire society suffers because a bunch of people want to freeload on media, in a way that doesn't jibe with the US laws and social contract.  Rather than work to change the laws/contract, which could be brilliantly positive and even utopian, they instead simply disregard and take.  And so society heads further towards dystopian.reply",
      "Most media piracy is a direct result of it being somewhere between inconvenient and impossible to consume that media legally. See, for example, the tremendous drop in music piracy resulting from various music streaming purchases and Apple's popularization of direct track purchases before that.Movies and shows, by comparison, are not just absurdly fragmented* but often literally unavailable not long after release for bizarre tax dodge purposes.(* Check out the official guide on what services have the Pokemon cartoon: https://www.pokemon.com/us/animation/where-to-watch-pokemon-...)reply"
    ],
    "link": "https://www.eff.org/deeplinks/2025/09/eff-court-supreme-court-must-rein-expansive-secondary-copyright-liability",
    "first_paragraph": "If the Supreme Court doesn\u2019t reverse a lower court\u2019s ruling, internet service providers (ISPs) could be forced to terminate people\u2019s internet access based on nothing more than mere accusations of copyright infringement. This would threaten innocent users who rely on broadband for essential aspects of daily life. EFF\u2014along with the American Library Association, the Association of Research Libraries, and Re:Create\u2014filed an\u00a0amicus brief urging the Court to reverse the decision.Among other things, the Supreme Court approving the appeals court\u2019s findings will radically change the amount of risk your ISP takes on if a customer infringes on copyright, forcing the ISP to terminate access to the internet for those users accused of copyright infringement\u2014and everyone else who uses that internet connection.This issue turns on what courts call \u201csecondary liability,\u201d which is the legal idea that someone can be held responsible not for what they did directly, but for what someone else did using thei"
  },
  {
    "title": "Orange rivers signal toxic shift in Arctic wilderness (ucr.edu)",
    "points": 38,
    "submitter": "hbcondo714",
    "submit_time": "2025-09-11T16:24:05 1757607845",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://news.ucr.edu/articles/2025/09/08/orange-rivers-signal-toxic-shift-arctic-wilderness",
    "first_paragraph": "Follow US:Warming soil unleashes metals deadly to fish and food chainsIn Alaska\u2019s Brooks Range, rivers once clear enough to drink from now run orange and hazy with toxic metals. As warming thaws formerly frozen ground, it sets off a chemical chain reaction that is poisoning fish and wreaking havoc on ecosystems.\u00a0As the planet warms, a layer of permafrost \u2014 permanently frozen Arctic soil that locked away minerals for millennia \u2014 is beginning to thaw. Water and oxygen creep into the newly exposed soil, triggering the breakdown of sulfide-rich rocks, and creating sulfuric acid that leaches naturally occurring metals like iron, cadmium, and aluminum from rocks into the river.\u00a0Often times, geochemical reactions like these are triggered by mining operations. But that is not the case this time.\u00a0\u201cThis is what acid mine drainage looks like,\u201d said Tim Lyons, a biogeochemist at the University of California, Riverside. \u201cBut here, there\u2019s no mine. The permafrost is thawing and changing the chemistr"
  },
  {
    "title": "Four-year wedding crasher mystery solved (theguardian.com)",
    "points": 242,
    "submitter": "wallflower",
    "submit_time": "2025-09-13T14:52:33 1757775153",
    "num_comments": 75,
    "comments_url": "https://news.ycombinator.com/item?id=45232562",
    "comments": [
      "My uncle turned up drunk for my wedding reception. He\u2019d got the venue wrong and had already spent an hour at a different wedding reception eating and drinking, easy to do at Indian weddings with a huge number of guests.But that\u2019s only half the story. He\u2019d got the date wrong too, and had already done the whole thing the night before.reply",
      "So the night before, was he at the wrong venue as well?reply",
      "When my father left us, a cousine of mine (from my mother's sise) got confused and parked a couple blocks away. She entered the house where people were mourning, and she realised the people she didn't recognise had to be from my father's side.\nThen she approached the casket and leapt forward exclaiming \"I'll miss you, Uncle\", only to find a lady laying inside.reply",
      "Something like this has also happened to me when in holiday in Spain. I was looking around nice buildings open to the public, and entered one that I later found out happened to be a university. Walking around I entered one very well decorated hall, also because it started to rain and had to wait somewhere until it passed. To my horror, more people started coming in as well and I realized I was in for some sort of book or thesis presentation on the subject of Spanish language on the Balearic islands.I barely speak Castilian Spanish (the more common one) and it was instead in Catalan Spanish, so I didn't understand a word, but stayed for the 1-2 hours it took, clapped, and skipped the handshakes/signing part of it.reply",
      "Couldn't you just leave? Like what if you had genuinely been there intentionally but had an emergency at home? People understandreply",
      "Doug Englebart and Ted Nelson came to give a lecture at my university when I was a student. I was busy in the lab and engrossed in my work, and realised the time 5 minutes after the talk was due to begin. I was too embarrassed to walk in late, so to my eternal regret I didn't go. I'm more comfortable with that sort of situation now, but I can appreciate the desire not to make a scene.reply",
      "I'm sorry you missed it. I'm also sorry I missed it.reply",
      "I attended a funeral for the family member of a friend of mine. After the funeral we all were to convene at his sister's house. Because of the crowds I parked half a block away and found myself in a group of similarly dressed people walking towards what I remembered to be her house. After maybe 5 minutes of not recognizing anyone, someone simply says \"who are you\", and after explaining my relation to the deceased, my error became apparent.reply",
      "[flagged]",
      "This is a weird comment to make when the original comment didn't mention anxiety at all, and wasn't even worded like it was a bad experience or that they felt they were forced to stay there. Maybe they had nothing better to do, they were waiting out the rain after all.reply"
    ],
    "link": "https://www.theguardian.com/uk-news/2025/sep/12/wedding-crasher-mystery-solved-four-years-bride-scotland",
    "first_paragraph": "Bride finally tracks down awkward-looking stranger she and husband noticed only when looking through photosA baffled bride has solved the mystery of the awkward-looking stranger who crashed her wedding four years ago.Michelle Wylie and her husband, John, registered the presence of their unidentifiable guest only as they looked through photographs of their wedding in the days after the happy occasion.Who was the tall man in a dark suit, distinguished by the look of quiet mortification on his face? But their family and friends could offer no explanation, nor could hotel staff at the Carlton hotel in Prestwick, where the event took place in November 2021. An appeal on Facebook likewise yielded no clues.Eventually, with the mystery still niggling, Wylie asked the popular Scottish content creator Dazza to cast the online net wider \u2013 and a sheepish Andrew Hillhouse finally stepped forward.In his explanatory post on Facebook, Hillhouse admitted that he had been \u201ccutting it fine, as I\u2019m known "
  },
  {
    "title": "Safe C++ proposal is not being continued (sibellavia.lol)",
    "points": 98,
    "submitter": "charles_irl",
    "submit_time": "2025-09-13T19:00:29 1757790029",
    "num_comments": 68,
    "comments_url": "https://news.ycombinator.com/item?id=45234460",
    "comments": [
      "I am actually much more pessimistic about Profiles than Simone.Regardless of the technology the big thing Rust has that C++ does not is safety culture, and that's dominant here. You could also see at the 2024 \"Fireside chat\" at CppCon that this isn't likely to change any time soon.The profiles technology isn't very good. But that's insignificant next to the culture problem, once you decided to make the fifteen minute bagpipe dirge your lead single it doesn't really matter whether you use the colored vinyl.reply",
      "It doesn't show up in the online videos, but there was a huge contingent of people at that fireside chat wanting a reasonable safety story for C++. The committee simply doesn't have representation from those people and don't seem to understand why it's an existential risk to the language community. The delivery timelines are so long here that anything not standardized soon isn't going to arrive for a decade or more. That's all the time in the world for Rust (or even Zig) to break down the remaining barriers.Profiles and sanitizers just aren't sufficient.reply",
      "Yeah because the committee is now people that a) really love C++, and b) don't care enough about safety to use Rust instead.I think there are plenty of people that must use C++ due to legacy, management or library reasons and they care about safety. But those people aren't going to join language committees.reply",
      "> The profiles technology isn't very good.Can you be very specific about why?Here's the argument for why profiles might work: with all of the profiles enabled, you are only allowed to use the safe subset of C++ and all of the unsafe stuff is hidden behind APIs whose implementations don't have those profiles enabled. Those projects that enable all profiles by default effectively get Swift-like or Rust-like protection.Like, you could force all array operations to use C++ stdlib primitives, enable full hardening of the stdlib, and then have bounds safety.And you could force all lifetime operations to use C++ stdlib refcounting primitives, and then have lifetime safety in a Swift-like way (i.e. eager refcounting everywhere).I can imagine how this falls over but then it might just be a matter of engineering to make it not fall over.(I'm playing devils advocate a bit since I prefer Fil-C++.)reply",
      "Yes I can be specific.Firstly, you need composition. Rust's safety composes. The safe Rust library for farm animals from Geoff, the safe Rust library for cooking recipes by Alice and the safe Rust library for web server by Bert together with my safe program code adds up to my safe Rust farm foods web site.By having N profiles, where N is intended to be at least five and might grow arbitrarily and be user extensible, C++ guarantees it cannot deliver composition this way.Maybe they can define some sort of composition and maybe everybody will ship software which conforms to that definition and so eventually they get composition, that's not there today, so it's just a giant unknown at best.Secondly, of the profiles described so far, most of them are just solving parts of the single overarching problem Rust addresses, for the serial case. So if they ship that, which already involves some amount of new work yet to be finished, you need all of those profiles to get to only partial memory safety.Which comes to the third part. Once you start down this path, as they found, you realise you actually want a borrowck. You won't call it that of course, because that would be embarrassing. But you'll need to track reference lifetimes and you'll need annotation and you end up building most of the stuff you insisted you didn't want. For now, you can handwave, this is an unsolved static analysis problem. Well, not so much unsolved as you know the solution and you don't like it.Your idea to do the reference counting everywhere is not something WG21 has looked at, I think the perf cost is sufficiently bad that they won't even glance at it. They're also not going to ship a GC.Finally though, C++ is a concurrent language. It has a whole memory model which doesn't even make sense if you aren't thinking about concurrency. But to deliver concurrent memory safety without Fil-C's overheads you would want... well, Rust's Send and Sync traits, which sure enough have eerie twins in the Safe C++ proposal.  No attempt to solve this is even hinted at in the current profiles proposal, and they would need to work one out and if it's not Send + Sync again they'd need to prove it is correct.reply",
      "I think the point is that folks will incrementally move their code towards having all profiles enabled, and that's sort of fundamental if the goal is to give folks with C++ codebases an incremental path to safety. So I don't buy your first and second points.> Which comes to the third part. Once you start down this path, as they found, you realise you actually want a borrowck.That's a bold statement. It might be true for some very loose definition of \"borrow checker\". See the super simple static analysis that WebKit uses (that presentation is now linked in at least two places on this HN discussion, so I won't link it again).> Your idea to do the reference counting everywhere is not something WG21 has looked at, I think the perf cost is sufficiently bad that they won't even glance at it. They're also not going to ship a GC.The point isn't to have ref counting on every pointer at the language level, but rather: if your prevent folks from calling `delete` directly (as one of the profiles does) then you're effectively forcing folks to use smart pointers.Reference counting that happens by smart pointers is something that they would ship. We know this because it's already happened.I imagine this would really mean that some references are ref counted (if you use shared_ptr or similar) while other references use some other policy.> Finally though, C++ is a concurrent language. It has a whole memory model which doesn't even make sense if you aren't thinking about concurrency. But to deliver concurrent memory safety without Fil-C's overheads you would want... well, Rust's Send and Sync traitsYeah, this might be an area where they leave a hole. Like, you might have reference counting that is only partially thread safe:- The refcount of any object is atomic.- The smart pointer itself is racy. So, racing on pointers can pop the protections.If they got that far, then that wouldn't be so bad. The marginal safety advantage of Rust would be very slim at that point.reply",
      "My limited understanding is. There is no safe subset (That's what was just discontinued, profiles are the alternative.)And C++ code simply doesn't have the necessary info to make safety decisions. Sean explains it better than I can https://www.circle-lang.org/draft-profiles.htmlreply",
      "> with all of the profiles enabled, you are only allowed to use the safe subset of C++ and all of the unsafe stuff is hidden behind APIs whose implementations don't have those profiles enabled.This is not the goal of profiles. It\u2019s to be \u201cgood enough.\u201d Guaranteed safety isn\u2019t in the cards.reply",
      "> This is not the goal of profiles. It\u2019s to be \u201cgood enough.\u201d Guaranteed safety isn\u2019t in the cards.- Rust isn\u2019t totally guaranteed safe since folks can and do use unsafe code.- Exact same situation in Swift- Go has escape hatches like it you race, but not only.So most \u201csafe\u201d things are really \u201csafe enough\u201d for some definition of \u201cenough\u201d.reply",
      "The language standard assumes that everyone collectively agrees to standard semantics implying certain things. If users don't follow the rules and write something without semantics (undefined behavior), the entire program is meaningless as opposed to just the bit around the violation. You know this, so I emphasize it here because it's entirely incompatible with the view that \"good enough\" is a meaningful concept to discuss from the PoV of the standard.Rust does a pretty good job formalizing what the safety guarantees are and when you can assume them. Other languages don't, but they also don't support safety concepts that C++ nominally does like safety critical systems. \"Good enough\" can be perfectly fine for a web service like Go while being grossly inadequate for HPC or safety critical.reply"
    ],
    "link": "https://sibellavia.lol/posts/2025/09/safe-c-proposal-is-not-being-continued/",
    "first_paragraph": "One year ago, the Safe C++ proposal was made. The goal was to add a safe subset/context into C++ that would give strong guarantees (memory safety, type safety, thread safety) similar to what Rust provides, without breaking existing C++ code. It was an extension or superset of C++. The opt-in mechanism was to explicitly mark parts of the code that belong to the safe context. The authors even state:Code in the safe context exhibits the same strong safety guarantees as code written in Rust.The rest remains \u201cunsafe\u201d in the usual C++ sense. This means that existing code continues to work, while new or refactored parts can gain safety. For those who write Rust, Safe C++ has many similarities with Rust, sometimes with adjustments to fit C++\u2019s design. Also, because C++ already has a huge base of \u201cunsafe code\u201d, Safe C++ has to provide mechanisms for mixing safe and unsafe, and for incremental migration. In that sense, all of Safe C++\u2019s safe features are opt-in. Existing code compiles and works "
  },
  {
    "title": "Perrinn 424 \u2013 An open access electric hyper car designed for racing (perrinn.com)",
    "points": 11,
    "submitter": "pillars",
    "submit_time": "2025-09-10T15:14:21 1757517261",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://discover.perrinn.com/home",
    "first_paragraph": "\"My name is Nico Perrin, and I am an engineer obsessed with speed, innovation, and high performance. From a young age, I was fascinated by Formula 1, mesmerized by the brilliance of engineering and the relentless pursuit of perfection. I grew up watching Ayrton Senna, a driver whose passion, fearlessness, and pursuit of excellence went beyond racing\u2014it was a philosophy, a way of life. His spirit taught me that limits exist only to be pushed beyond imagination.That belief has shaped my journey. I have worked at the pinnacle of motorsport, Formula 1, where fractions of a second matter. But now, I am writing a new story\u2014one that has never been told before.For over a decade, I have dedicated myself to a singular mission: to build the car that will break the most prestigious lap record in the world\u2014the N\u00fcrburgring Nordschleife. The record belongs to Porsche, a giant in motorsport history. But I believe true innovation comes from those who refuse to accept limits.Enter PERRINN 424 \u2014 a fully "
  },
  {
    "title": "My first impressions of gleam (mtlynch.io)",
    "points": 162,
    "submitter": "AlexeyBrin",
    "submit_time": "2025-09-13T13:15:26 1757769326",
    "num_comments": 60,
    "comments_url": "https://news.ycombinator.com/item?id=45231852",
    "comments": [
      "> I\u2019ve also heard that functional languages lend themselves especially well to parsing tasks, and I\u2019ve never understood why, so it\u2019s a good opportunity to learn.This is mainly due to a technique called \"parser combinators\", which can be expressed very cleanly in functional languages. The idea is to start with a standard set of primitive general-purpose parsers and combine them into more complex parsers that are specific to the task at hand. If you have an interest in either functional programming or parsing in general, parser combinators are good to have in your toolkit.It looks like Gleam has at least two parser combinator packages that you might be able to use:    \u2022 party (https://hexdocs.pm/party/)\n    \u2022 parser_gleam (https://hexdocs.pm/parser_gleam/)reply",
      "I don't think parser combinators are any more difficult in imperative languages like Python than functional ones? I found a few for Python and C++.reply",
      "Don\u2019t know about difficult, but at least less elegant. Lazy evaluation, type inference, abstractions like Functor/Applicative/Alternative/Monad make them so incredibly natural to work with in a language like Haskell. Sure, they exist in other languages (made a few myself) but it\u2019s not the same.reply",
      "Yes, I'm writing a parser in Unison (in the Haskell family) for ASN.1 at the moment. It's so clean to write parsers with parser combinators.For example Asn1Type can be of form Builtin, Referenced, or Constrained. So a sum type.    parseType = Builtin <$> parseBuiltin <|> (Referenced <$> parseReferenced) <|> (Constrained <$> parseConstrained)\n\nAssuming you have the parsers for Builtin, Referenced, and Constrained, you're golden. (Haskell PCs look very similar, possibly even exactly the same minus different parenthesis for operator precedence reasons.Compare Parsy for Python, particularly the verbosity (this parses SELECT statements in SQL):    select = seq(\n    _select=SELECT + space,\n    columns=column_expr.sep_by(padding + string(\",\") + padding, min=1),\n    _from=space + FROM + space,\n    table=table,\n    where=(space >> WHERE >> space >> comparison).optional(),\n    _end=padding + string(\";\"),\n).combine_dict(Select)The same thing in a FP-style language would be something like    eq \"SELECT\" *> \n      Select \n      <*> (sepBy columnExpr (eq \",\")) <* (eq \"FROM\") \n      <*> parseTable \n      <*> optional (eq \"WHERE\") <* eq \";\"\n\nwhich would feed into something like    type Select = { cols: [ColumnExpr], table: Table, where: Optional Where}reply",
      "The general notion of a \u201ccombinator\u201d comes from functional programming, so parser combinators are native to FP. They can definitely be done in imperative languages also, but in my experience (C# specifically) are much more clunky. A dynamically typed language like Python might be suitable, because the interpreter is lenient, but I\u2019m dubious about C++.One thing you will definitely need in any language to produce a concise parser is the ability to define custom operators, like >>. and .>>. These are the \u201ccombinators\u201d.reply",
      "Or thanks to algebraic data types and pattern matching? Even in a modest recursive descent parser, these can be really nice to use.reply",
      "Sure, although those are great FP features for all kinds of programming, not just parsing.reply",
      "Id say mostly because of recursion is usually better for tree like ds, and builtin tco and exhaustive pattern matching. This is why ocaml is usually the goto for building a new languge prototype.reply",
      "As someone who isn\u2019t a programmer, but enjoys dabbling, and has been curious about functional languages, this was a really helpful (and fun!) read!Tutorials written by beginners are so valuable, because once you\u2019re more experienced with a given subject, it\u2019s hard to remember what it was like to be a beginner. It can feel vulnerable to write them because you have to expose your ignorance, but it\u2019s such a great thing to do.reply",
      "Gleam has long kinda seemed like my idea programming language. My only real hang-up is maybe an irrational one, but I don't love that it needs either a VM (BEAM) to run, or for it to be compiled to an interpreted language (javascript). I really wish it could target LLVM or something so it could be compiled down to native.Maybe someone can sell me on BEAM though.EDIT: The comments below are indeed beginning to sell me on BEAM, I'm realizing my reluctance might come from some negative experiences I've had dealing with the JVM.reply"
    ],
    "link": "https://mtlynch.io/notes/gleam-first-impressions/",
    "first_paragraph": "I\u2019m looking for a new programming language to learn this year, and Gleam looks like the most fun. It\u2019s an Elixir-like language that supports static typing.I read the language tour, and it made sense to me, but I need to build something before I can judge a programming language well.I\u2019m sharing some notes on my first few hours using Gleam in case they\u2019re helpful to others learning Gleam or to the team developing the language.I used AOL Instant Messenger from about 1999 to 2007. For most of that time, I used AIM clients that logged my conversations, but they varied in formats. Most of the log formats are XML or HTML, which make re-reading those logs a pain.The simplest AIM logs are the plaintext logs, which look like this:Every decade or so, I try writing a universal AIM log parser to get all of my old logs into a consistent, readable format. Unfortunately, I always get bored and give up partway through. My last attempt was seven years ago, when I tried doing it in Python 2.7.Parsing log"
  },
  {
    "title": "Will AI be the basis of many future industrial fortunes, or a net loser? (joincolossus.com)",
    "points": 47,
    "submitter": "saucymew",
    "submit_time": "2025-09-13T22:01:35 1757800895",
    "num_comments": 51,
    "comments_url": "https://news.ycombinator.com/item?id=45235676",
    "comments": [
      "I think the interesting idea with \u201cAI\u201d is that it seems to significantly reduce barriers to entry in many domains.I haven\u2019t seen a company convincingly demonstrate that this affects them at all. Lots of fluff but nothing compelling. But I have seen many examples by individuals, including myself.For years I\u2019ve loved poking at video game dev for fun. The main problem has always been art assets.  I\u2019m terrible at art and I have a budget of about $0. So I get asset packs off Itch.io and they generally drive the direction of my games because I get what I get (and I don\u2019t get upset). But that\u2019s changed dramatically this year. I\u2019ll spend an hour working through graphics design and generation and then I\u2019ll have what I need. I tweak as I go. So now I can have assets for whatever game I\u2019m thinking of.Mind you this is barrier to entry. These are shovelware quality assets and I\u2019m not running a business. But now I\u2019m some guy on the internet who can fulfil a hobby of his and develop a skill. Who knows, maybe one day I\u2019ll hit a goldmine idea and commit some real money to it and get a real artist to help!It reminds me of what GarageBand or iMovie and YouTube and such did for making music and videos so accessible to people who didn\u2019t go to school for any of that, let alone owned complex equipment or expensive licenses to Adobe Thisandthat.reply",
      "I've noticed this as well.  It's a huge boon for startups, because it means that a lot of functions that you would previously need to hire specialists for (logo design! graphic design! programming! copywriting!) can now be brought in-house, where the founder just does a \"good enough\" job using AI.  And for those that can't (legal, for example, or various SaaS vendors) the AI usually has a good idea of what services you'd want to engage.Ironically though, having lots of people found startups is not good for startup founders, because it means more competition and a much harder time getting noticed.  So its unclear that prosumers and startup founders will be the eventual beneficiary here either.It would be ironic if AI actually ended up destroying economic activity because tasks that were frequently large-dollar-value transactions now become a consumer asking their $20/month AI to do it for them.reply",
      "> I've noticed this as well. It's a huge boon for startups, because it means that a lot of functions that you would previously need to hire specialists for (logo design! graphic design! programming! copywriting!) can now be brought in-house, where the founder just does a \"good enough\" job using AI.You are missing the other side of the story. All those customers, those AI boosted startups want to attract also have access to AI and so, rather than engage the services of those startups, they will find that AI does a good enough job. So those startups lost most of their customers, incoming layoffs :)reply",
      "Yep this is a huge enabler - previously having someone \"do art\" could easily cost you thousands for a small game, a month even, and this heavily constrained what you could make and locked you into what you had planned and how much you had planned.  With AI if you want 2x or 5x or 10x as much art, audio etc it's an incremental cost if any, you can explore ideas, you can throw art out, pivot in new directions.reply",
      "I have a similar problem (available assets drive/limit game dev). What is your workflow like for generative game assets?reply",
      "It\u2019s really nothing special. I don\u2019t do this a lot.Generally I have an idea I\u2019ve written down some time ago, usually from a bad pun like Escape Goat (CEO wants to blame it all on you. Get out of the office without getting caught! Also you\u2019re a goat) or Holmes on Homes Deck Building Deck Building Game (where you build a deck of tools and lumber and play hazards to be the first to build a deck). Then I come up with a list of card ideas. I iterate with GPT to make the card images.  I prototype out the game. I put it all together and through that process figure out more cards and change things.  A style starts to emerge so I replace some with new ones of that style.I use GIMP to resize and crop and flip and whatnot.  I usually ask GPT how to do these tasks as photoshop like apps always escape me.The end result ends up online and I share them with friends for a laugh or two and usually move on.reply",
      "I have been doing the exact same thing with assets and also it has helped me immensely with mobile development.I am also starting to get a feel for generating animated video and am  planning to release a children\u2019s series. It\u2019s actually quite difficult to write a prompt that gets you exactly what you want. Hopefully that improves.reply",
      "> Yet some technological innovations, though societally transformative, generate little in the way of new wealth; instead, they reinforce the status quo. Fifteen years before the microprocessor, another revolutionary idea, shipping containerization, arrived at a less propitious time, when technological advancement was a Red Queen\u2019s race, and inventors and investors were left no better off for non-stop running.This collapses an important distinction. The containerization pioneers weren\u2019t made rich - that\u2019s correct, Malcolm McLean, the shipping magnate who pioneered containerization didn\u2019t die a billionaire. It did however generate enormous wealth through downstream effects by underpinning the rise of East Asian export economies, offshoring, and the retail models of Walmart, Amazon and the like. Most of us are much more likely to benefit from downstream structural shifts of AI rather than owning actual AI infrastructure.This matters because building the models, training infrastructure, and data centres is capital-intensive, brutally competitive, and may yield thin margins in the long run. The real fortunes are likely to flow to those who can reconfigure industries around the new cost curve.reply",
      "You can't make such generalized statements about anything in computing/business.The AI revolution has only just got started. We've barely worked out basic uses for it. No-one has yet worked out revolutionary new things that are made possible only by AI - mostly we are just shoveling in our existing world view.reply",
      "The point though is AI wont make you rich. It is about value capture. They compare it to shipping containers.I think AI value will mostly be spread. Open AI will be more like Godaddy than Apple. Trying to reduce prices and advertise (with a nice bit of dark patterns). It will make billions, but ultimately by competing its ass off rather than enjoying a moat.The real moats might be in mineral mining, fabrication of chips etc. This may lead to strained relations between countries.reply"
    ],
    "link": "https://joincolossus.com/article/ai-will-not-make-you-rich/",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: CLAVIER-36 \u2013 A programming environment for generative music (clavier36.com)",
    "points": 100,
    "submitter": "river_dillon",
    "submit_time": "2025-09-13T14:22:59 1757773379",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=45232299",
    "comments": [
      "River (the software author) worked on this during his time at the Recurse Center and it\u2019s been amazing to see him develop it all from scratch in C. (I contributed 2.5 lines of code on the web deployment/firebase side).He\u2019s a friend, but I am very unbiased in saying that the sample-rate execution of the entire grid seems like an incredible technical achievement.One of the craziest (super super noisy but fascinating to watch) grids uses just a few \u201coperators\u201d that generate random operators and random values, and place them at random location.That grid runs - easily! in the browser!! - at 1000 bpm. Forget 60 fps :)I\u2019ll update my comment linking to this patch so you can take a listen. It\u2019s stunning, organic and very punk.reply",
      "EDIT: sorry, got away for a few hoursThis is the patch:WARNING - GETS SUPER SUPER LOUD\nhttps://clavier36.com/p/tEWcc48tFPm8qiyx9ljo\nWARNING - GETS SUPER SUPER LOUDZoom out using your mouse wheel/trackpad to see it all. It's realllly gorgeous if you let it run. But super, super loud at random times :-)reply",
      "I'm curious - was it two and a half lines of code you contributed?reply",
      "I was saying that in jest, ha. More like 2.5% of the code.Very briefly, I contributed the CI pipeline that makes git push build the wasm and deploy it to a micro server that sets the specific required headers. I used the deployment tool I\u2019ve been working on with a friend, which is called Disco.There was something about wasm/the audio worklet requiring super specific headers - `Cross-Origin-Embedder-Policy: require-corp` \u2026 Nothing too complicated.The other part I contributed is the loading/saving patches to Firebase, which lets people share compositions.But all of the audio, grid, ui is all River\u2019s!reply",
      "In the video, within the first 10 seconds, I should understand the offering of the product by seeing it.You can get into the details later but right now I've got no idea what's going on here and don't know why I should invest my time in it.You need to motivate people by showing off the thing.Also on the phone it just says basically \"go away\". Once again, show me some video, song, Bandcamp, SoundCloud, something that would motivate me to switch to a desktop.reply",
      "Cool project. I've referred people to Orca before--and the lack of \"built in instruments\" (and maybe the flow visualization) was a stumbling block. This feels more \"consumer friendly\" :)reply",
      "CLAVIER is amazing, the wire system alone is such a huge improvement over ORCA, and it's now feasible to make much larger patches and refactor safely, kudos to River for all the hard work on the polish and quality-of-life.I was testing MIDI on a prerelease build last weekend and it turned out quite nice: https://www.instagram.com/p/DOUUIfeEQWY/Excited for more folks to get to play with it!reply",
      "See also [0] Ooda and Zoa on iOS and [1] Midinous on Steam0: https://www.audiosymmetric.com/ooda.html (same person for Zoa)\n1: https://midinous.comreply",
      "Off topic: where did you get the name from? There's a town Clavier (Claviere in French) in the Italian/French alps.reply",
      "Clavier is keyboard in French and German (Klavier)36 because, just like base64 uses 64 characters, clavier uses A-Z and 0-9 :-)reply"
    ],
    "link": "https://clavier36.com/p/LtZDdcRP3haTWHErgvdM",
    "first_paragraph": ""
  },
  {
    "title": "SkiftOS: A hobby OS built from scratch using C/C++ for ARM, x86, and RISC-V (skiftos.org)",
    "points": 419,
    "submitter": "ksec",
    "submit_time": "2025-09-13T04:55:14 1757739314",
    "num_comments": 86,
    "comments_url": "https://news.ycombinator.com/item?id=45229414",
    "comments": [
      "What an astounding achievement. In 6 years, this person has written not only a very well-designed microkernel, but a build system, UEFI bootloader, graphical shell, UI framework, and a browser engine.The story of 10x developers among us is not a myth... if anything, it's understated.reply",
      "And unlike a similar project, they accomplished it without the benefit of divine guidance.Very impressive!reply",
      "Writing an OS with God's help might be more of a handicap than anything.reply",
      "Not with Messiah.ai :Dreply",
      "Oh my God. That domain is parked and for sale for $125,000?!?!Wild.reply",
      "Oh that is nothing.  Check out god.ai..... domain parking is back.  At this point we might as well just have a TLD for .godreply",
      "> TLD for .godSounds like a good TLD for an \"identity and access management\" system :)reply",
      "Musk would just hog it for himselfreply",
      "You might enjoy reading the SerentiyOS progress reportshttps://serenityos.org/reply",
      "I want serenity nowreply"
    ],
    "link": "https://skiftos.org",
    "first_paragraph": ""
  }
]