[
  {
    "title": "Chatterbox TTS (github.com/resemble-ai)",
    "points": 206,
    "submitter": "pinter69",
    "submit_time": "2025-06-11T20:23:52 1749673432",
    "num_comments": 75,
    "comments_url": "https://news.ycombinator.com/item?id=44251411",
    "comments": [
      "It's only for English sadly\n \nreply",
      "> Every audio file generated by Chatterbox includes Resemble AI's Perth (Perceptual Threshold) Watermarker - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.Am I misunderstanding, or can you trivially disable the watermark by simply commenting out the call to the apply_watermark function in tts.py? https://github.com/resemble-ai/chatterbox/blob/master/src/ch...I thought the point of this sort of watermark was that it was embedded somehow in the model weights, so that it couldn't easily be separated out. If you're going to release an open-source model that adds a watermark as a separate post-processing step, then why bother with the watermark at all?\n \nreply",
      "Possibly a sort of CYA gesture, kinda like how original Stable Diffusion had a content filter IIRC. Could also just be to prevent people from accidentally getting peanut butter in the toothpaste WRT training data, too.\n \nreply",
      "Yeah, there's even a flag to turn it off in the parser `--no-watermark`. I assumed they added it for downstream users pulling it in as a \"feature\" for their larger product.\n \nreply",
      "1. Any non-OpenAI, non-Google, non-ElevenLabs player is going to have to aggressively open source or they'll become 100% irrelevant. The TTS market leaders are obvious and deeply entrenched, and Resemble, Play(HT), et al. have to aggressively cater to developers by offering up their weights [1].2. This is CYA for that. Without watermarking, there will be cries from the media about abuse (from anti-AI outfits like 404Media [2] especially).[1] This is the right way to do it. Offer source code and weights, offer their own API/fine tuning so developers don't have to deal with the hassle. That's how they win back some market share.[2] https://www.404media.co/wikipedia-pauses-ai-generated-summar...\n \nreply",
      "Nevermind, this is just ~3/10 open, or not really open at all [1]:https://github.com/resemble-ai/chatterbox/issues/45#issuecom...> For now, that means we\u2019re not releasing the training code, and fine-tuning will be something we support through our paid API (https://app.resemble.ai). This helps us pay the bills and keep pushing out models that (hopefully) benefit everyone.Big bummer here, Resemble. This is not at all open.For everyone stumbling upon this, there are better \"open weights\" models than Resemble's Chatterbox TTS:Zeroshot TTS: MaskGCT, MegaTTS3Zeroshot VC: Seed-VC, MegaTTS3These are really good robust models that score higher in openness.Unfortunately only Seed-VC is fully open. But all of the above still beat Resemble's Chatterbox in zero shot MOS (we tested a lot), especially the mega-OP Chinese models.(ByteDance slaps with all things AI. Their new secretive video model is better than Veo 3, if you haven't already seen it [2]!)You can totally ignore this model masquerading as \"open\". Resemble isn't really being generous at all here, and this is some cheap wool over the eyes trickery. They know they retain all of the cards here, and really - if you're just going to use an API, why not just use ElevenLabs?Shame on y'all, Resemble. This isn't \"open\" AI.The Chinese are going to wipe the floor with TTS. ByteDance released their model in a more open manner than yours, and it sounds way better and generalizes to voices with higher speaker similarity.Playing with open source is a path forward, but it has to be in good faith. Please do better.[1] \"10/10\" open includes: 1. model code, 2. training code, 3. fine tuning code, 4. inference code, 5. raw training data, 6. processed training data, 7. weights, 8. license to outputs, 9. research paper, 10. patents. For something to be a good model, it should have 7/10 or above.[2] https://artificialanalysis.ai/text-to-video/arena?tab=leader...\n \nreply",
      "Demos here: https://resemble-ai.github.io/chatterbox_demopage/ (not mine)This is a good release if they're not too cherry picked!I say this every time it comes up, and it's not as sexy to work on, but in my experiments voice AI is really held back by transcription, not TTS. Unless that's changed recently.\n \nreply",
      "FWIW in my recent experience I've found LLMs are very good at reading through the transcription errors(I've yet to experiment with giving the LLM alternate transcriptions or confidence levels, but I bet they could make good use of that too)\n \nreply",
      "Pairing speech recognition with a LLM acting as a post-processor is a pretty good approach.I put together a script a while back which converts any passed audio file (wav, mp3, etc.), normalizes the audio, passes it to ggerganov whisper for transcription, and then forwards to an LLM to clean the text. I've used it with a pretty high rate of success on some of my very old and poorly recorded voice dictation recordings from over a decade ago.Public gist in case anyone finds it useful:https://gist.github.com/scpedicini/455409fe7656d3cca8959c123...\n \nreply",
      "thanks for sharing. are some local models better than others? can small models work well or do you want 8B+?\n \nreply"
    ],
    "link": "https://github.com/resemble-ai/chatterbox",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        SoTA open-source TTS\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\n\n_Made with \u2665\ufe0f by We're excited to introduce Chatterbox, Resemble AI's first production-grade open source TTS model. Licensed under MIT, Chatterbox has been benchmarked against leading closed-source systems like ElevenLabs, and is consistently preferred in side-by-side evaluations.Whether you're working on memes, videos, games, or AI agents, Chatterbox brings your content to life. It's also the first open source TTS model to support emotion exaggeration control, a powerful feature that makes your voices stand out. Try it now on our Hugging Face Gradio app.If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (link). It deliv"
  },
  {
    "title": "Research suggests Big Bang may have taken place inside a black hole (port.ac.uk)",
    "points": 336,
    "submitter": "zaik",
    "submit_time": "2025-06-11T19:44:29 1749671069",
    "num_comments": 333,
    "comments_url": "https://news.ycombinator.com/item?id=44251047",
    "comments": [
      "I think it's neat that this summary is written by an author of the scientific manuscript. Oversimplification is a risk, but this approach eliminates the possibility that the writer did not understand the underlying science.\n \nreply",
      "Yea, and it was a great read too. I wish more researchers would publish blog posts alongside their technical whitepapers, although I acknowledge that not everyone involved in science has or wishes to acquire the skills needed to write blog-form content.(I'd also be worried about a world where researchers are evaluated based on the virality of their blog posts, vs. how impactful their work was.)\n \nreply",
      "Be worried then because that is the reality of the vast vast vast vast vast vast vast majority of research. At this point academia is a massive lumbering beast that exists and must be fed because it's become a big part of the economy. Most research being produced isn't really research, just people keeping busy.\n \nreply",
      "Can you cite your sources please?\n \nreply",
      "Most jobs are really not important either, they just keep people busy. Do you need sources for this claim, too?\n \nreply",
      "Yes. Who are these people paying for jobs that don't do anything, and why are they more concerned about \"keeping people busy\" than their own profits?\n \nreply",
      "I\u2019m not the one you were referring to, but I have similar experiences.  I\u2019m living in Germany, and most bigger companies here have such issues. I also worked for companies in Netherlands and Island, so I assume it\u2019s an European, if not global problem. \nNo one is concerned about keeping people busy. It\u2019s a systemic problem. And there are multiple reasons for it. One reason is that the bigger a company grows, the more hierarchy is necessary. But increasing hierarchy will lead to people doing the work are not the people that are most responsible for it. So we have people that should do the work but they aren\u2019t too motivated because they are not responsible enough - they are too low in hierarchy level. And we have people that are responsible but don\u2019t do the work. They delegate. If something goes wrong or takes too long, they will have enough time and skill to find an excuse. \nAnother issue is that you need more people to get specific things done. At some point in time these things have been done, and you actually don\u2019t need the amount of people anymore. But you can\u2019t quit them because of worker\u2019s laws. You maybe even don\u2019t want to quit them because you think you still need them. People, of course, tend to find reasons why their own work is important. And they will communicate that. And the chance is good you\u2019ll believe that and don\u2019t question it enough.\nThere are more reasons for that. But it\u2019s a fact that in many, many companies the economical results of a lot of employees is almost zero. If you don\u2019t believe this, just google the biggest companies in Germany, pick one, apply for an office job and start to work there. It won\u2019t take a month until you\u2019ll find out. Btw. I don\u2019t want to criticize the situation too much. Probably it\u2019s good that people are employed, even if they don\u2019t work efficiently. Otherwise the unemployment rate would be much higher. Then again, Germany\u2018s economy is flatlining and a crash is not unlikely.\n \nreply",
      "Sounds like you're describing the principal-agent problem. https://en.wikipedia.org/wiki/Principal\u2013agent_problem\n \nreply",
      "It's like saying \"if you know half of your advertising dollars are wasted, why don't you just cut your ad buy in half?\"I still remember the joke from my first job:Q: How many people work at this office?A: About half.\n \nreply",
      "An apt analogy. Circling back to scientific research, I'm sure an investigator would be more than happy not to spend the time, effort, and grant money on a project that wasn't going to produce worthwhile results. If only we could know in advance without doing the work.That does not, of course, mean that \"most research being produced isn't really research, just people keeping busy\" or whatever other nonsense an uninformed outsider feels like spewing.\n \nreply"
    ],
    "link": "https://www.port.ac.uk/news-events-and-blogs/blogs/space-cosmology-and-the-universe/what-if-the-big-bang-wasnt-the-beginning-our-research-suggests-it-may-have-taken-place-inside-a-black-hole",
    "first_paragraph": "Search suggestions update instantly to match the search query.\n\n\n\n4 June 2025\n\n\n\n\nEnrique Gaztanaga      \n\n\n\n\n        8 minutes\n  The Big Bang is often described as the explosive birth of the universe \u2013 a singular moment when space, time and matter sprang into existence. But what if this was not the beginning at all? What if our universe emerged from something else \u2013 something more familiar and radical at the same time?In a new paper, published in Physical Review D, my colleagues and I propose a striking alternative. Our calculations suggest the Big Bang was not the start of everything, but rather the outcome of a gravitational crunch or collapse that formed a very massive black hole \u2013 followed by a bounce inside it.This idea, which we call the black hole universe, offers a radically different view of cosmic origins, yet it is grounded entirely in known physics and observations.Today\u2019s standard cosmological model, based on the Big Bang and cosmic inflation (the idea that the early univ"
  },
  {
    "title": "Congratulations on creating the one billionth repository on GitHub (github.com/aasishpokhrel)",
    "points": 342,
    "submitter": "petercooper",
    "submit_time": "2025-06-11T21:37:04 1749677824",
    "num_comments": 76,
    "comments_url": "https://news.ycombinator.com/item?id=44252076",
    "comments": [
      "Reminds me of the 100 millionth OpenStreetMap changeset (commit). A few people, myself included, were casually trying for it but in the end it went to someone who wasn't trying and just busy mapping Africa! Much more wholesome, seeing it with hindsight. This person was also previously nominated for an OSM award. I guess it helps that openstreetmap doesn't really allow for creating crap, because it's all live in production, and that's how the Nth commit is way more likely to be someone's random whim? Either way, a fun achievement for Github :)In case anyone cares to read more about the OSM milestone, the official blog entry: https://blog.openstreetmap.org/2021/02/25/100-million-edits-... My write-up of changeset activity around the event: https://www.openstreetmap.org/user/LucGommans/diary/395954\n \nreply",
      "Your kind of comment is exactly why HN still rules. What a fun story. Thanks for sharing\n \nreply",
      "Aww, thanks! I wasn't sure if I should go off-topic this much so I'm happy to hear this!\n \nreply",
      "I wish I were still at Apple. Probably most people here know that Apple uses an internal tool called \"Radar\" since, forever. Each \"Radar\" has an ID (bug #) associated with it.Radars that were bug #1,000,000, etc. were kind of special. Unless someone screwed up (and let down the whole team) they were usually faux-Radars with lots of inside jokes, etc.Pulling up one was enough since the Radar could reference other Radars ... and generally you would go down the rabbit hole at that point enjoying the ride.I was a dumbass not to capture (heck, even print) a few of those when I had the opportunity.\n \nreply",
      "> I was a dumbass not to capture (heck, even print) a few of those when I had the opportunity.On the other hand, given how Apple deals with confidential data, you probably wouldn't want to be caught exfiltrating internal documents however benign they are.\n \nreply",
      "Awesome! Only a little over a billion more to go before GitHub\u2019s very own OpenAPI Spec can start overflowing int32 on repositories too, just like it already does for workflows run IDs!https://github.com/github/rest-api-description/issues/4511\n \nreply",
      "The company where I did my stint as CTO I turned up, noticed they were using 32-bit integers as primary keys on one of their key tables that already had 1.3 billion rows and, at the rate they were adding them, would overflow on primary key values within months\u2026 so we ran a fairly urgent project to upgrade the IDs to 64-bit to avoid the total meltdown that would have ensued otherwise.\n \nreply",
      "What are the challenges of such projects? How many people are usually involved? Does it incur downtimes or significant technical challenges for either the infrastructure or the codebase?\n \nreply",
      "Changing the type of the column is no big deal per se, except on a massive table it\u2019s a non-trivial operation, BUT you also have to change the type in everything that touches it, everywhere it\u2019s assigned or copied, everywhere it\u2019s sent over the wire and deserialized where assumptions might be made, any tests, and on, and on. And god help you if you\u2019ve got stuff like int.MaxValue having a special meaning (we didn\u2019t in this context, fortunately).Our hosting environment at that time was a data centre so we were limited on storage, which complicated matters a bit. Like ideally you\u2019d create a copy of the table but with a wider PK column and write to both tables, then migrate your reads, etc., but we couldn\u2019t do that because the table was massive and we didn\u2019t have enough space. Procuring more drives was possible but took sometimes weeks - no just dragging a slider in your cloud portal. And then of course you\u2019d have to schedule a maintenance window for somebody to plug it in. It was absolutely archaic, especially when you consider this was late 2017/early 2018.You need multiple environments so you can do thorough testing, which we barely had at that point, and because every major system component was impacted, we had to redeploy our entire platform. Also, because it was the PK column affected, we couldn\u2019t do any kind of staged migration or rollback without the project becoming much more complex and taking a lot longer - time we didn\u2019t have due to the rate at which we were consuming 32-bit integer values.In the end it went off without a hitch, but pushing it live was still a bit of a white knuckle moment.\n \nreply",
      "Not the original commenter, but I've read through half a dozen post-mortems about this kind of thing. The answer is: yes. There's challenges and sometimes downtime and/or breaking changes are inevitable.For one, if your IDs are approaching the 2^31 signed integer limit, then by definition, you have nearly two billion rows, which is a very big DB table! There are only a handful of systems that can handle any kind of change to that volume of data quickly. Everything you do to it will either need hours of downtime or careful orchestration of incremental/rolling changes. This issue tends to manifest first on the \"biggest\" and hence most important table in the business such as \"sales entries\" or \"user comments\". It's never some peripheral thing that nobody cares about.Second, if you're using small integer IDs, that decision was probably motivated in part because you're using those integers as foreign keys and for making your secondary indexes more efficient. GUIDs are \"simpler\" in some ways but need 4x the data storage (assuming you're using a clustered database like MySQL or SQL Server). Even just the change from 32-bits to 64-bits doubles the size of the storage in a lot of places. For 2 billion rows, this is 8 GB more data minimum, but is almost certainly north of 100 GB across all tables and indexes.Third, many database engines will refuse to establish foreign key constraints if the types don't match. This can force big-bang changes or very complex duplication of data during the migration phase.Fourth, this is a breaking change to all of your APIs, both internal and external. Every ORM, REST endpoint, etc... will have to be updated with a new major version. There's a chance that all of your analytics, ETL jobs, etc... will also need to be touched.Fun times.\n \nreply"
    ],
    "link": "https://github.com/AasishPokhrel/shit/issues/1",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.We wanted to congratulate you on creating the one billionth repository on GitHub!We really hope you have the opportunity to build some great \ud83d\udca9 Have a great day! \u2764\ufe0f"
  },
  {
    "title": "Show HN: Spark, An advanced 3D Gaussian Splatting renderer for Three.js (sparkjs.dev)",
    "points": 218,
    "submitter": "dmarcos",
    "submit_time": "2025-06-11T17:02:56 1749661376",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=44249565",
    "comments": [
      "Super impressive looking demo, works well on my older iphone.As an only-dabbling-hobbiest game developer who lacks a lot of 3d programming knowledge, the only feedback I can offer is you might perhaps define what \"Gaussian Splatting\" is somewhere on the github or the website. Just the one-liner from wikipedia helps me get more excited about the project and potential uses: Gaussian splatting is a volume rendering technique that deals with the direct rendering of volume data without converting the data into surface or line primitives.Super high performance clouds and fire and smoke and such? Awesome!\n \nreply",
      "Thanks. We have to definitely add an FAQ\n \nreply",
      "The food scans demo (\"Interactivity\" examples section) is incredible. Especially Mel's Steak Sandwich looking into the holes in the bread.The performance seems amazingly good for the apparent level of detail, even on my integrated graphics laptop. Where is this technique most commonly used today?\n \nreply",
      "There's a community of people passionate about scanning all short stuff with handheld devices, drones... Tipatat let us generously use his food scans for the demo. I also enjoy kotohibi flower scans: https://superspl.at/user?id=kotohibiEdit: typos\n \nreply",
      "Wow what kind of device do I need to make my own?\n \nreply",
      "The food scans are just photos from a Pixel phone processed with postshot (https://www.jawset.com/) to generate the splats\n \nreply",
      "I'm sure it's not cutting edge, but the app \"scaniverse\" generates some very nice splats just by you waving your phone around an object for a minute or so.\n \nreply",
      "Yes there are several phone apps to generate splats. Also Luma 3D capture.\n \nreply",
      "And the transfer size for that level of detail isn't that bad, either - only around 80MB. (Not being sarcastic, it's really neat.)\n \nreply",
      "Yeah. And some of the individual scans like Clams and Caviar or Pad Thai are < 2MB.\n \nreply"
    ],
    "link": "https://sparkjs.dev/",
    "first_paragraph": ""
  },
  {
    "title": "My Cord-Cutting Adventure (brander.ca)",
    "points": 24,
    "submitter": "wizardforhire",
    "submit_time": "2025-06-09T02:07:30 1749434850",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=44220890",
    "comments": [
      "HDHomerun is so great, easily one of the best, most reliable pieces of tech I own. I agree though cord cutting has become kind of hard for the layman.Since they also expose streams as http in addition to DLNA, I've used a Tailscale subnet router and VLC to stream live TV from my house while away. It works decently over shockingly poor connections too.\n \nreply",
      "My Shaw DVR actually kept working after I ended my TV subscription. It was purchased, though, not rented (still tied to Shaw only though!)The thing that made me cancel wasn't the DVR hardware or software sucking - it was actually decent, especially after I upgraded the hard drive to a 2 TB WD Purple. What killed it for me was the schedule not matching the recordings. I'd miss 5 minutes on either end of an episode, or for one show I'd be one episode off.\n \nreply",
      "Personally I like Channels DVR which is a single go binary that can run anywhere and they have excellent client apps for every platform. There's even an API and a Home Assistant integration. https://getchannels.com/dvr-server/\n \nreply",
      "Needs a (2020), and that's just when it was last updated. Because the whole way down the page I was wondering, \"what decade was this written?\"I was also wondering why it should be an \"adventure\". Yeah, back when we cut the cord some fifteen years ago, things were a bit rough. Mac Mini with a tuner dongle, and a bunch of hacks. Now it's just turn on AppleTV that outputs to unconnected TV, sorted. (Or sail the high seas if my money is inexplicably no good.)\n \nreply",
      "For reference, I just pointed my antenna in the attic towards the nearest metropolis, picked up 108 channels, sent them to an HDHomeRun box that sends them to Plex which handles the guide data and DVR to an SMB share.It took a couple hours in the evening, most of which was just fiddling with the antenna mount and scanning the channels.  It\u2019s a lot easier than it used to be.  Granted I planned and ran ethernet to the attic years earlier, but it\u2019s not rocket science.\n \nreply",
      "Yeah, I think if the article was written today it would just be an exercise in spinning up the typical Usenet piracy stack with Plex or Jellyfin.What I do think was true back then and is even more true today is that this kind of effort just isn\u2019t worth it. Antenna channels absolutely suck except for the increasingly rare sports events that haven\u2019t been moved to pay TV. And that sucky experience gets even worse if you don\u2019t live in a large metro area close to the broadcast antennas.Television isn\u2019t even enjoyable enough to go through all this effort. I\u2019d rather just not watch TV at all. And honestly, I think a significant number of former TV customers may have made that exact choice - not just cutting the cord but finding other activities entirely. Gaming comes to mind, which I immensely prefer over basically any television show.Another little note, I think a lot of cable and streaming providers have done a good job resolving the issue of streaming bitrate being inferior to antenna bitrate. My local OTA market has a grand total of one 4K demo channel and everything else is broadcasting at a bitrate that appears inferior to modern streaming platforms.\n \nreply"
    ],
    "link": "http://brander.ca/cordcut/",
    "first_paragraph": "\n\n      For starters, the consumer electronics industry, normally so eager to sell us computers, laptops, pads, phones, and watches; the industry that for 30 years has sold us VCRs, competed over Beta vs VHS and Super-VHS (look it up, it existed), then sold us DVDs, DVD recorders with DVD-R and DVD-RW, then sold us DVRs that recorded standard definition, then sold us Blu-Ray players of increasing degrees of quality and declining prices...these days, they've utterly given up selling us anything that can record video.\n    \n      The DVR was so beloved, the culmination of decades of home video-entertainment recording into the most-convenient possible form, it was called \"God's Machine\" by the chair of the FCC. \n\nCasus Belli\n      Now they are gone from stores, at least in Canada. The change shows us how little oligopoly differs from monopoly where it counts.   Canada has a very tight little oligopoly of TV providers: Shaw, Rogers, Bell, Telus - few enough to meet in an apartment bathroom."
  },
  {
    "title": "How Long it takes to know if a Job Is Right for You or Not (charity.wtf)",
    "points": 21,
    "submitter": "zdw",
    "submit_time": "2025-06-09T14:11:49 1749478309",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=44224729",
    "comments": [
      "A long time ago, I had a chat with a nurse executive who rose up the ranks. Her advice was to stick to a position until the butterflies settle (or approximately q 5 years).Nursing is known for lifers on single units, and personal growth doesn\u2019t happen unless you are cognizant and strive for it.\n \nreply",
      "At about the 2 or 3 year mark I've known.Of course some places I've known sooner, but those are the exception.If it wasn't for the quality of life my current jobs is providing, I'd probably find something else. I just can't find anything right now that is intriguing enough and pays enough for me to make the jump without losing benefits.\n \nreply",
      "It takes me about a month to know for sure. If the job isn't right for me, it takes me a few more months before I consciously decide that there is no way to make the job right for me.\n \nreply",
      "It takes me between 1 ~ 2 months, for me to have a very good feeling...then the next handful of months to validate those feelings. In my 30 year work history, i have had only 1 job where i got it wrong..and EVERY single other job through my entire career, all it took was between 1 ~ 2 months to get a feeling if the place is good or not, if the role is right or not, etc. Then again, all except for 1 role, my entire career has been at medium or large corporations...so i'm sure corporate America has enough similarities in so many areas to enable this \"early warning system\". ;-)\n \nreply"
    ],
    "link": "https://charity.wtf/2025/06/08/on-how-long-it-takes-to-know-if-a-job-is-right-for-you-or-not/",
    "first_paragraph": "A few eagle-eyed readers have noticed that it\u2019s been 4 weeks since my last entry in what I have been thinking of as my \u201cniblet series\u201d \u2014 one small piece per week, 1000 words or less, for the next three months.This is true. However, I did leave myself some wiggle room in my original goal, when I said \u201cweeks when I am not traveling\u201d, knowing I was traveling 6 of the next 7 weeks. I was going to TRY to write something on the weeks I was traveling, but as you can see, I mostly did not succeed. Oh well!Honestly, I don\u2019t feel bad about it. I\u2019ve written well over 1k words on bsky over the past two weeks in the neverending thread on the costs and tradeoffs of remote work. (A longform piece on the topic is coming soon.) I also wrote a couple of lengthy internal pieces.This whole experiment was designed to help me unblock my writing process and try out new habits, and I think I\u2019m making progress. I will share what I\u2019m learning at a later date, but for now: onward!This week\u2019s niblet was inspired "
  },
  {
    "title": "V-JEPA 2 world model and new benchmarks for physical reasoning (meta.com)",
    "points": 218,
    "submitter": "mfiguiere",
    "submit_time": "2025-06-11T14:43:27 1749653007",
    "num_comments": 72,
    "comments_url": "https://news.ycombinator.com/item?id=44248165",
    "comments": [
      "> With these visual subgoals, V-JEPA 2 achieves success rates of 65% \u2013 80% for pick-and-placing new objects in new and unseen environments.How does this compare with existing alternatives? Maybe I'm just lacking proper context, but a minimum 20% failure rate sounds pretty bad? The paper compares their results with older approaches, which apparently had something like a 15% success rate, so jumping to an 80% success rate does seem like a significant jump. If I'm reading the paper correctly, the amount of time required to compute and execute each action went down from 4 minutes to 16 seconds, which also seems significant.Having to specify an end goal as an image seems pretty limited, but at least the authors acknowledge it in the paper:> Second, as mentioned in Section 4, V-JEPA 2-AC currently relies upon tasks specified as image goals. Although this may be natural for some tasks, there are other situations where language-based goal specification may be preferable. Extending the V-JEPA 2-AC to accept language-based goals, e.g., by having a model that can embed language-based goals into the V-JEPA 2-AC representation space, is another important direction for future work. The results described in Section 7, aligning V-JEPA 2 with a language model, may serve as a starting point.I think it would be interesting if the authors answered whether they think there's a clear trajectory towards a model that can be trained to achieve a >99% success rate.\n \nreply",
      "Currently,You train a VLA (vision language action) model for a specific pair of robotic arms, for a specific task. The end actuator actions are embedded in the model (actions). So let's say you train a pair of arms to pick an apple. You cannot zero shot it to pick up a glass. What you see in demos is the result of lots of training and fine tuning (few shot) on specific object types and with specific robotic arms or bodies.The language intermediary embedding brings some generalising skills to the table but it isn't much. The vision -> language -> action translation is, how do I put this, brittle at best.What these guys are showing is a zero shot approach to new tasks in new environments with 80% accuracy. This is a big deal. Pi0 from Physical Intelligence is the best model to compare I think.\n \nreply",
      "It\u2019s important to keep some perspective: there are zero robots in the wild, at the moment, that use a world model to work on tasks they weren\u2019t specifically trained on. This is cutting edge research and an 80% success rate is astonishing!\n \nreply",
      "80% success rate is also potentially commercially viable if the task is currently being done by a human.Work that was once done by 10 humans can now be done by 10 robots + 2 humans for the 20% failure cases, at a lower total cost.\n \nreply",
      "This really depends on the failure modes. In general, humans fail in predictable, and mostly safe, ways. AIs fail in highly unpredictable and potentially very dangerous ways. (A human might accidentally drop a knife, an AI might accidentally stab you with it.)\n \nreply",
      "Or, if controlling a robot arm, it would stab itself through the conveyer belt at full torque.\n \nreply",
      "It might still be a little slow (I'm not sure if the 16 seconds to compute an action is fast enough for commercial use cases), but this is definitely exciting and seems like a great step forward.\n \nreply",
      "I'm surprised that's not how it's already done. I'd figure some of the inner layers in LLMs were already \"world models\" and that it's the outer layers that differentiated models between text vs. images/robotics/other modes...\n \nreply",
      "That's what the propaganda says, but when we keep explaining it isn't true, and army arrives to repeat adcopy from their favourite tech guru.All statistical models of the kind in use are interpolations through historical data -- there's no magic. So when you interpolate through historical texts, your model is of historical text.Text is not a measure of the world, to say, \"the sky is blue\" is not even reliably associated with the blueness of the sky, let alone that the sky isnt blue (there is no sky, and the atmosphere isn't blue).These models appear \"capture more\" only because when you interpret the text you attribute meaning/understanding to it as the cause of its generation -- but that wasnt the cause, this is necessarily an illusion. There is no model of the world in a model of historical text -- there is a model of the world in your head which you associate with text, and that association is exploited when you use LLMs to do more than mere syntax transformation.LLMs excel most at \"fuzzy retrieval\" and things like coding -- the latter is principally a matter of syntax, and the former of recollection. As soon as you require the prompt-completion to maintain \"semantic integrity\" with non-syntactical/retrivable constraints, it falls apart.\n \nreply",
      "I feel like you are ignoring or dismissing the word \"interpolating\", although a better word would likely be generalization.  I'd make the claim that it's very hard to generalize without some form of world model.  It's clear to me that transformers do have some form of world model, although not the same as what is being presented in V-JEPA.One other nitpick is that you confine to \"historical data\", although other classes of data are trained on such as simulated and generative.\n \nreply"
    ],
    "link": "https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/",
    "first_paragraph": "TakeawaysToday, we\u2019re excited to share V-JEPA 2, the first world model trained on video that enables state-of-the-art understanding and prediction, as well as zero-shot planning and robot control in new environments. As we work toward our goal of achieving advanced machine intelligence (AMI), it will be important that we have AI systems that can learn about the world as humans do, plan how to execute unfamiliar tasks, and efficiently adapt to the ever-changing world around us.V-JEPA 2 is a 1.2 billion-parameter model that was built using Meta Joint Embedding Predictive Architecture (JEPA), which we first shared in 2022. Our previous work has shown that JEPA performs well for modalities like images and 3D point clouds. Building on V-JEPA, our first model trained on video that we released last year, V-JEPA 2 improves action prediction and world modeling capabilities that enable robots to interact with unfamiliar objects and environments to complete a task. We\u2019re also sharing three new be"
  },
  {
    "title": "Plants hear their pollinators, and produce sweet nectar in response (cbc.ca)",
    "points": 207,
    "submitter": "marojejian",
    "submit_time": "2025-06-07T19:27:38 1749324458",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=44211971",
    "comments": [
      "If you find this interesting, I strongly recommend the book _The Light Eaters_ by Zo\u00eb Schlanger [0]. She discusses this finding as well as other sense-abilities of plants. Recent science has found pretty amazing things.If I recall correctly: flowers are often shaped like dish antennas to collect sound vibration, and plants can distinguish the frequency of wing beats of their preferred pollinator from frequencies of other insects, and will act only for their pollinators.[0] https://www.amazon.com/Light-Eaters-Unseen-Intelligence-Unde...\n \nreply",
      "I listened to that book and enjoyed it.   But that said, I'm torn between friendliness to the general concept, and skepticism based in part on the bias of proponents to deeply desire plants to display something like intelligence (a bias I share).For example the most amazing claims in the book were around the ability of Boquila trifoliolata to dynamically mimic other plants.see this old HN thread: \nhttps://news.ycombinator.com/item?id=31301454But when one looks more closely the research, the behavior isn't as dramatic as Zoe made it sound, and the research may not be so strong, e.g. :https://press.asimov.com/articles/plant-vision\n \nreply",
      "i definitely agree that it would've been nice to have images in the book as it was hard to get a sense of exactly how well Boquila was mimicking neighbouring plants!but in reference to the linked article, i will say that the researchers interviewed in the book (and i got that sense for Zoe as well) were in agreement with you that the research didn't support a vision-based mechanism. but everyone agrees that the imitation is going on. the researchers in the book suggest a gene transfer-based mechanism instead! (mentioned briefly in your linked article)\n \nreply",
      "What most surprised me in this interview is, not only do plants increase sugar for 'efficient' pollinators, but:>In contrast they respond to the sound of nectar-stealing non-pollinators by cutting back on sugar.So there is some discrimination in their hearing.\n \nreply",
      "Plants are our cousin eucaryotes, and they've been evolving as long as we animals have and so there is likely to be equivalent information processing complexity to be found in them, we just don't know how to recognize it because it's so different from animal intelligence.  There might even be something comparable to animal consciousness, not at the level of an individual plant, but more collectively, even including multiple species, whole ecosystems of plants and fungi together having an awareness and intelligence that can not only rival ours, but even transcend it, having lifespans in the thousands of years.\n \nreply",
      "> Plants are our cousin eucaryotes, and they've been evolving as long as we animals haveThat is true. And look how different we\u2019ve become.> and so there is likely to be equivalent information processing complexity to be found in themThat\u2019s quite a leap. I think precisely because plants and animals have evolved separately for so you can\u2019t make that assumption. Maybe plants hasn\u2019t not simply because they don\u2019t need to, as a fundamental consequence of their differing physiology.\n \nreply",
      "Another possibility is that this is a non-conscious trait. Luring pollinators is an evolutionary advantage, but there is survival cost to giving nectar indiscriminately, so natural selection will favor plants that can mechanically differentiate between the two.\n \nreply",
      "\"... so there is likely to be equivalent information processing complexity to be found in them\"This sounds like a really wild take. Just because something has been evolving for millions of years doesn't necessarily mean it's evolving information processing capabilities. It's patently obvious to me that the information processing capabilities of animals (eg. just vision alone) are far beyond those of plants.\n \nreply",
      "This would make things quite complicated for vegans.\n \nreply",
      "What is an example of a nectar-stealing non-pollinator? Doesn't anything rooting around in there end up moving around some pollen?\n \nreply"
    ],
    "link": "https://www.cbc.ca/listen/live-radio/1-51-quirks-and-quarks/clip/16150976-plants-hear-pollinators-produce-sweet-nectar-response",
    "first_paragraph": ""
  },
  {
    "title": "How I Program with Agents (crawshaw.io)",
    "points": 365,
    "submitter": "bumbledraven",
    "submit_time": "2025-06-09T05:30:14 1749447014",
    "num_comments": 214,
    "comments_url": "https://news.ycombinator.com/item?id=44221655",
    "comments": [
      "Maybe it's because I only code for my own tools, but I still don't understand the benefit of relying on someone/something else to write your code and then reading it, understand it, fixing it, etc. Although asking an LLM to extract and find the thing I'm looking for in an API Doc is super useful and time saving. To me, it's not even about how good these LLMs get in the future. I just don't like reading other people's code lol.\n \nreply",
      "Here are the cases where it helps me (I promise this isn't ai generated even though im using a list...)- Formulaic code. It basically obviates the need for macros / code gen. The downside is that they are slower and you can't just update the macro and re-generate. The upside is it works for code that is slightly formulaic but has some slight differences across implementations that make macros impossible to use.- Using apis I am familiar with but don't have memorized. It saves me the effort of doing the google search and scouring the docs. I use typed languages so if it hallucinates the type checker will catch it and I'll need to manually test and set up automated tests anyway so there are plenty of steps where I can catch it if it's doing something really wrong.- Planning: I think this is actually a very under rated part of llms. If I need to make changes across 10+ files, it really helps to have the llm go through all the files and plan out the changes I'll need to make in a markdown doc. Sometimes the plan is good enough that with a few small tweaks I can tell the llm to just do it but even when it gets some things wrong it's useful for me to follow it partially while tweaking what it got wrong.Edit: Also, one thing I really like about llm generated code is that it maintains the style / naming conventions of the code in the project. When I'm tired I often stop caring about that kind of thing.\n \nreply",
      "> Using apis I am familiar with but don't have memorizedI think you have to be careful here even with a typed language. For example, I generated some Go code recently which execed a shell command and got the output. The generated code used CombinedOutput which is easier to used but doesn't do proper error handling. Everything ran fine until I tested a few error cases and then realized the problem. In other times I asked the agent to write tests cases too and while it scaffolded code to handle error cases, it didn't actually write any tests cases to exercise that - so if you were only doing a cursory review, you would think it was properly tested when in reality it wasn't.\n \nreply",
      "You always have to be careful. But worth calling out that using CombinedOutput() like that is also a common flaw in human code.\n \nreply",
      "The difference is that humans learn. I got bit by this behavior of CombinedOutput once ten years ago, and no longer make this mistake.\n \nreply",
      "This applies to AI, too, albeit in different ways:1. You can iteratively improve the rules and prompts you give to the AI when coding. I do this a lot. My process is constantly improving, and the AI makes fewer mistakes as a result.2. AI models get smarter. Just in the past few months, the LLMs I use to code are making significantly fewer mistakes than they were.\n \nreply",
      "And you can build automatic checks that reinforce correct behavior for when the lessons haven\u2019t been learned, by bot or human.\n \nreply",
      "That you don't know when it will make a mistake and that it is getting harder to find them are not exactly encouraging signs to me.\n \nreply",
      "Do you mean something by \"getting harder to find them\" that is different from \"they are making fewer dumb errors\"?\n \nreply",
      "Planning is indeed a very underrated use case.One of my most productive uses of LLMs was when designing a pipeline from server-side data to the user-facing UI that displays it.I was able to define the JSON structure and content, the parsing, the internal representation, and the UI that the user sees, simultaneously. It was very powerful to tweak something at either end and see that change propagate forwards and backwards. I was able to hone in on a good solution much faster that it would have been the case otherwise.\n \nreply"
    ],
    "link": "https://crawshaw.io/blog/programming-with-agents",
    "first_paragraph": "2025-06-08This is the second part of my ongoing self-education in how to adapt my programming experience to a world with computers that talk. The first part, How I program with LLMs, covered ways LLMs can be adapted into our existing tools (basically, autocomplete) and how careful prompting can replace traditional web search. Now I want to talk about the harder, and more rewarding act of using agents to program.It is worthwhile starting with a definition of the word \u201cagent\u201d in the context of LLMs. The \u201cAI\u201d hype cycle has been throwing this very generic word around longer than agents have actually been useful constructs. As a result there is a bit of smoke and mirrors marketing and general mysticism to dig through to find any value in the word. For someone with an engineering background there is now a straightforward definition: an agent is 9 lines of code. That is, an agent is a for loop which contains an LLM call. The LLM can execute commands and see their output without a human in th"
  },
  {
    "title": "Show HN: Eyesite - experimental website combining computer vision and web design (andykhau.com)",
    "points": 7,
    "submitter": "akchro",
    "submit_time": "2025-06-12T00:37:27 1749688647",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44253307",
    "comments": [
      "I may be the result of some evolutionary bottleneck, but wherever there is a camera lens, I assume eye tracking (and sentiment prediction) is at least possible, and at most globally always on.\n \nreply",
      "Love this, any experimental HCI project managers inspires me to think differently about computers and tech\n \nreply"
    ],
    "link": "https://blog.andykhau.com/blog/eyesite",
    "first_paragraph": "I wanted Apple Vision Pros, but I don\u2019t have $3,500 in my back pocket. So I made Apple Vision Pros at home.I was interested in making a project that combined computer vision with web design\u2014a website that users could physically interact with. This inspired me to make Eyesite, because who needs a mouse when you have your eyes?Luckily, there is already a Javascript library for eye tracking called WebGazer.js. We can achieve decent eye tracking through calibration:I found that it was best to get 5 mappings per point for better eye tracking accuracy.Calibration in debug mode. The top right shows how WebGazer tracks your eyes and face. The red dot is where it thinks I\u2019m looking.Now that we have eye tracking, we can make some cool things with it! I decided to use the user\u2019s gaze as a mouse and have them click with spacebar\u2014kind of like how Apple Vision Pros have you look and pinch. Although I had the main functionality, it was far from finished. There were many considerations with making the"
  },
  {
    "title": "Show HN: Ikuyo a Travel Planning Web Application (kenrick95.org)",
    "points": 232,
    "submitter": "kenrick95",
    "submit_time": "2025-06-11T12:44:15 1749645855",
    "num_comments": 79,
    "comments_url": "https://news.ycombinator.com/item?id=44247029",
    "comments": [
      "I would definitely recommend adding example images directly to the main page, along with the link to the example trip. Otherwise there's nothing really to draw users in to using if they have to go searching for how the experience looks like.\n \nreply",
      "Thanks for your suggestion! I was hesitant at first cause it was under heavy development so anything I put there on main page might get outdated very soon. I'll add them very soon~\n \nreply",
      "also just because your landing page now has an example featuring singapore - this is my guide to singapore http://swyx.io/sg-guide and i feel like encouraging people to make opinionated guides to where they live is kinda nice and under explored\n \nreply",
      "Hi swyx, great idea! Thanks for your tips\n \nreply",
      "curation definitely is underexplored and I'd say we're seeing more of that in the future given the AI slob that's hitting us.\n \nreply",
      "well screenshotting is easy and cheap + even if its outdated it helps communicate what you do\n \nreply",
      "Also let the user do something without them making an account or verifying an email.\n \nreply",
      "This is amazing. I've used wanderlog extensively, but while its feature set is great, it can be a nightmare to work due to how slow it can be sometimes.I've just registered to test it out a little bit. I tried to replicate my upcoming trip that I have set up in Wanderlog, and have the following feedback:- Overall, amazing! Snappy. Real easy to follow.- I love the simplicity.- I like that this is essentially excel (kind of), but with travel specific additions.Now for the potential improvements:- I can't add someone else as an editor, it seems. Clicking add just logs a \"TripForm\" with the form object. I don't see any network requests either.- Expenses don't allow me to select how to split it (maybe this is an issue because there's no-one else part of the trip?)- Timetable contrast needs a bit of work. Maybe needs some padding/margins or something.- MapTiler doesn't seem to have a good enough database. I struggled to add 152 Morrison Road- Activities can't span multiple days (I tried adding a train ride than arrived 45 mins past midnight)- Adding/editing activities while on the timetable page, does not update them until I refresh (or navigate away)Outside of all that, how are you planning to monetise this? The code is released under MIT, which doesn't stop anyone from adding some subscription plan, hosting it, and advertising it. May I suggest something like AGPL?\n \nreply",
      "Thank you for trying them and providing such a detailed feedback!- On Trip Sharing, hmm that seems weird. While I understand that there's no 'loading' indicator yet, one should be able to do so if one is the 'owner' of the trip- Expense split: because that feature isn't there yet- Thanks I'll consider it- The map I chose on MapTiler is OpenStreetMap, but I limit it to Point-of-interest only, maybe I need to expand it to match more kind of objects- Aha, for that case, I find that it's so troublesome that I have to split the activity into two different elements for display in the timetable, so I disabled the case for now. Thanks for a great use case!- Hmm strange, the activites should reflect live. Maybe the 'back-end' is a bit slowAnyway, the 'back-end' is InstantDB ( https://www.instantdb.com ) and it's opening a WebSocket connection, that's why you don't see network calls when doing operationsP.S. I don't think I'll monetise this ever. If someone forks it and monetise it, as long as it doesn't affect me, I think I'm fine with it. If I run out of my 'free usage' quota, I'll probably limit the users to only handful of people\n \nreply",
      "The AGPL won't prevent anyone from adding some subscription plan, hosting it, and advertising it.  It does require them to license derivative works under the AGPL.There are many different goals for developing software, and different ways to make money doing it.  The AGPL can be useful for some of them, but can be rather limiting too.\n \nreply"
    ],
    "link": "https://ikuyo.kenrick95.org/",
    "first_paragraph": ""
  },
  {
    "title": "EchoLeak \u2013 0-Click AI Vulnerability Enabling Data Exfiltration from 365 Copilot (aim.security)",
    "points": 162,
    "submitter": "pvg",
    "submit_time": "2025-06-11T19:11:00 1749669060",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=44250774",
    "comments": [
      "Image rendering to achieve data exfiltration during prompt injection is one of the most common AI application security vulnerabilities.First exploits and fixes go back 2+ years.The noteworthy point to highlight here is a lesser known indirection reference feature in markdown syntax which allowed this bypass, eg:![logo][ref][ref]: https://url.com/dataIt's also interesting that one screenshot shows January 8 2025. not sure when Microsoft learned about this, but could have taken 5 months to fix - which seems very long.\n \nreply",
      "this seems to be an inherent flaw of the current generation of LLMs as there's no real separation of user input.you can't \"sanitize\" content before placing it in context and from there prompt injection is almost always possible, regardless of what else is in the instructions\n \nreply",
      "It's like redboxing all over again.\n \nreply",
      "It's like redboxing all over again.There are vanishingly few phreakers left on HN./Still have my F\u014cN card and blue box for GTE Links.\n \nreply",
      "Great nostalgia trip, I wasn\u2019t there at the time so for me it\u2019s second-hand nostalgia but eh :)https://youtu.be/ympjaibY6to\n \nreply",
      "Double LLM architecture is an increasingly common mitigation technique. But all the same rules of SQL injection still apply: For anything other than RAG, user input should not directly be used to modify or access anything that isn't clientside.\n \nreply",
      "Have you seen that implemented yet?\n \nreply",
      "LLMs suffer the same problems as any Von Neumann architecture machine, It's called \"key vulnerability\". None of our normal control tools work on LLMs like ASLR, NX-Bits/DEP, CFI, ect.. It's like working on a foreign CPU with a completely unknown architecture and undocumented instructions. All of our current controls for LLMs are probabilistic and can't fundamentally solve the problem.What we really need is a completely separate \"control language\" (Harvard Architecture) to query the latent space but how to do that is beyond me.  https://en.wikipedia.org/wiki/Von_Neumann_architecture\n  https://en.wikipedia.org/wiki/Harvard_architecture\n\nAI SLOP TLDR:\n  LLMs are \u201cTuring-complete\u201d interpreters of language, and when language is both the program and the data, any input has the potential to reprogram the system\u2014just like how data in a Von Neumann system can mutate into executable code.\n \nreply",
      "This. We spent decades dealing with SQL injection attacks, where user input would spill into code if it weren't properly escaped. The only reliable way to deal with SQLI was bind variables, which cleanly separated code from user input.What would it even mean to separate code from user input for an LLM? Does the model capable of tool use feed the uninspected user input to a sandboxed model, then treat its output as an opaque string? If we can't even reliably mix untrusted input with code in a language with a formal grammar, I'm not optimistic about our ability to do so in a \"vibes language.\" Try writing an llmescape() function.\n \nreply",
      "> Does the model capable of tool use feed the uninspected user input to a sandboxed model, then treat its output as an opaque string?That was one of my early thoughts for \"How could LLM tools ever be made trustworthy for arbitrary data?\" The LLM would just come up with a chain of tools to use (so you can inspect what it's doing), and another mechanism would be responsible for actually applying them to the input to yield the output.Of course, most people really want the LLM to inspect the input data to figure out what to do with it, which opens up the possibility for malicious inputs. Having a second LLM instance solely coming up with the strategy could help, but only as far as the human user bothers to check for malicious programs.\n \nreply"
    ],
    "link": "https://www.aim.security/lp/aim-labs-echoleak-blogpost",
    "first_paragraph": "The first weaponizable zero-click attack chain on an AI agent, resulting in the complete compromise of Copilot data integrity.Aim Security discovered \u201cEchoLeak\u201d, a vulnerability that exploits design flaws typical of RAG Copilots, allowing attackers to automatically exfiltrate any data from M365 Copilot\u2019s context, without relying on specific user behavior. The primary chain is composed of three distinct vulnerabilities, but Aim Labs has identified additional vulnerabilities in its research process that may also enable an exploit.M365 Copilot is a RAG-based chatbot that retrieves content relevant to user queries,enhancing the quality of responses by increasing relevance and groundedness through processes like semantic indexing across user content stores. To deliver this functionality, M365 Copilot queries the Microsoft Graph and retrieves any relevant information from the user\u2019s organizational environment, including their mailbox, OneDrive storage, M365 Office files, internal SharePoint "
  },
  {
    "title": "Show HN: RomM \u2013 An open-source, self-hosted ROM manager and player (github.com/rommapp)",
    "points": 175,
    "submitter": "gassi",
    "submit_time": "2025-06-11T14:25:05 1749651905",
    "num_comments": 67,
    "comments_url": "https://news.ycombinator.com/item?id=44247964",
    "comments": [
      "I had an idea once but not the time nor motivation to build it: a database of game design with links to gameplay snippets.So for example an article on \"2D platforming\" that discusses the implementation in Super Mario, and includes a \"demo\" button which launches a web emulator with a save state that demonstrates a specific jumping section of the game.Legally perilous maybe, although my non-lawyer brain sees that as fair use, especially if the emulator doesn't let you play the full game. Idk, but it'd be a unique thing on the internet.Edit: this is awesome btw, im definitely setting up a personal instance soon\n \nreply",
      "I think you could make your fair-use defense a lot stronger by stripping the ROM. Shouldn't even be that hard! Only let it run for a few seconds, record all the memory accesses, zero everything that wasn't hit.\n \nreply",
      "Cool idea, would be a fun POC or weekend project. The team behind EmulatorJS (https://emulatorjs.org/) has made it stupid easy to inject the player into a static website (or multiple players with iframes), with a preloaded game and custom settings.\n \nreply",
      "> Legally perilous maybe, although my non-lawyer brain sees that as fair use, especially if the emulator doesn't let you play the full game. Idk, but it'd be a unique thing on the internet.IANAL, but I think what a lot of people don't understand is that \"fair-use\" is a defense. Which basically means you have to be prepared to argue in in court. A lot of potential fair-use is quashed before it gets to that point.It's also a balancing test, which means that it's very fact/context dependent, and subjective, which results in for a lot of cases, you really won't know until you actually get to court.\n \nreply",
      "This is incredibly relevant to projects in this space, and we continue to hold discussions about what is and isn't tolerated in our community, and what features we should avoid building, in order to protect ourselves from legal attacks. One thing you'll never see integrated into the RomM is a way to pull/download ROMs from cloud services or website; you'll always need to provide the games to RomM directly, after sourcing them legally of course.\n \nreply",
      "MAME/MESS just used GIF files for that.\n \nreply",
      "Fetching anything from the internet means some cloud knows what games I'm playing. It's a privacy risk. Games played are very revealing to personality type, and probably lots of other individual info, and that means the temptation to gather this data and sell it to advertisers is very high.\n \nreply",
      "> Fetching anything from the internet means some cloud knows what games I'm playing. It's a privacy risk. Games played are very revealing to personality type, and probably lots of other individual infoIt seems like OP already replied with an upcoming feature to entirely eliminate your concern, but I would also point out that this doesn't mean the cloud service knows what games you're playing, they just know what games you have which for many gamers are two very different things.That's especially true for those with a large enough ROM collection to be interested in tools to manage and simplify access to their libraries.  I'd be willing to bet that the majority of potential users have collections that are exponentially larger than the list of games they actually play, in many cases some variety of a \"complete\" collection.\n \nreply",
      "What fortunate timing! Our next release will including a new, local-only \"API\" that uses Launchbox's GamesDB (https://gamesdb.launchbox-app.com/) as the metadata source. The entire database is loaded into Redis and games are matched on exact file names, so no data is ever sent to any cloud providers.https://github.com/rommapp/romm/pull/1515\n \nreply",
      "It is optional.  If you don\u2019t configure the integrations it won\u2019t fetch anything from the internet.It looks much less impressive without cover art though.\n \nreply"
    ],
    "link": "https://github.com/rommapp/romm",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n        A beautiful, powerful, self-hosted rom manager and player.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\n\nRomM (ROM Manager) allows you to scan, enrich, browse and play your game collection with a clean and responsive interface. With support for multiple platforms, various naming schemes, and custom tags, RomM is a must-have for anyone who plays on emulators.To start using RomM, check out the Quick Start Guide in the docs. If you are having issues with RomM, please review the page for troubleshooting steps.To contribute to RomM, please check Contribution Guide.Here are a few projects maintained by members of our community. Please note tha"
  },
  {
    "title": "Unveiling the EndBOX \u2013 A microcomputer prototype for EndBASIC (endbasic.dev)",
    "points": 6,
    "submitter": "jmmv",
    "submit_time": "2025-06-11T23:49:42 1749685782",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.endbasic.dev/2025/06/unveiling-the-endbox.html",
    "first_paragraph": "Published on: June 6, 2025Remember when turning a computer on meant instantly jumping into code? No bloat, no distractions\u2014just you and a prompt? That\u2019s the experience I\u2019ve been working to bring back with the EndBOX: a small, resilient, nostalgia-packed, all-screen computer that boots straight into the retro-inspired EndBASIC environment you already know.And today, six months after its inception, I\u2019m excited to formally show you the first working prototypes\u2014though they are still rough and need refinement. Let\u2019s dive into the goals for the EndBOX and what it could become with your help.\ud83d\udcbe Sponsor the EndBOXFirst and foremost, the EndBOX is for the hacker types: those of you who love tinkering with systems, languages, and hardware. But more specifically, the EndBOX caters to those who grew up with simpler computers and know that, to truly understand the fundamentals of computing, you have to experience them from the ground up with as few layers of abstraction as possible.That said, EndBAS"
  },
  {
    "title": "Shaped (YC W22) Is Hiring (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-06-11T21:00:26 1749675626",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/shaped/jobs/qtQwxJO-head-of-engineering",
    "first_paragraph": "The fastest path to relevant recommendations and searchAs the Head of Engineering at Shaped, you will be a pivotal member of our leadership team, responsible for scaling our engineering organization and driving the technical vision of our products. You'll lead a team of talented engineers, fostering a culture of innovation, collaboration, and excellence. Your leadership will be instrumental in shaping our product roadmap, ensuring the reliability and scalability of our platform, and aligning engineering efforts with business objectives.Shaped is the fastest path to relevant recommendation and search systems. We help companies turn their behavioral data into truly relevant product and website experiences.We're a Series A company based in Brooklyn, New York and backed by top investors from Madrona, Y-Combinator, and executives from Meta, Google, Amazon and Uber!\u00a9 2025 Y Combinator"
  },
  {
    "title": "Bypassing GitHub Actions policies in the dumbest way possible (yossarian.net)",
    "points": 161,
    "submitter": "woodruffw",
    "submit_time": "2025-06-11T14:15:54 1749651354",
    "num_comments": 84,
    "comments_url": "https://news.ycombinator.com/item?id=44247881",
    "comments": [
      "This is a prime example of \"If you make an unusable secure system, the users will turn it into an insecure usable one.\"If someone is actively subverting a control like this, it probably means that the control has morphed from a guardrail into a log across the tracks.Somewhat in the same vein as AppLocker &co. Almost everyone says you should be using it, but almost no-one does, because it takes a massive amount of effort just to understand what \"acceptable software\" is across your entire org.\n \nreply",
      "Nobody outside of the IT security bubble thinks that using AppLocker is a sensible idea.Companies have no business telling their employees which specific programs they can and cannot run to do their jobs, that's an absurd level of micromanagement.\n \nreply",
      "> Companies have no business telling their employees which specific programs they can and cannot run to do their jobs, that's an absurd level of micromanagement.I'm usually on the side of empowering workers, but I believe sometimes the companies do have business saying this.One reason is that much of the software industry has become a batpoop-insane slimefest of privacy (IP) invasion, as well as grossly negligent security.Another reason is that the company may be held liable for license terms of the software.Another reason is that the company may be held liable for illegal behavior of the software (e.g., if the software violates some IP of another party).Every piece of software might expose the company to these risks.  And maybe disproportionately so, if software is being introduced by the \"I'm gettin' it done!\" employee, rather than by someone who sees vetting for the risks as part of their job.\n \nreply",
      "Developers are going to write code to do things for them, such as small utility programs for automating work. Each custom program is a potentially brand new binary, never sent before by the security auditing software. Does every program written by every dev have to be cleared? Is it best in such a system to get an interpreter cleared so I can use that to run whatever scripts I need?\n \nreply",
      "If I have an internal developer in such a scenario, then what makes most sense to me is to issue them a code-signing certificate or equivalent, and whitelisting anything signed by that certificate[1], combined with logging and periodic auditing to detect abuse.[1] <https://learn.microsoft.com/en-us/windows/security/applicati...>\n \nreply",
      "This is a strawman argument. If a developer writes code that does something malicious then it's on the developer. If they install a program then the accountability is a bit fuzzier. It's partly on the developer, partly on security  (for allowing an unprivileged user to do malicious/dangerous things even unknowingly), and partly on IT (for allowing the unauthorized program to run without any verification).\n \nreply",
      "It's not a straw man, I'm not trying to defuse liability. Of course a developer running malicious code they wrote is responsible for the outcomes.I am pointing out that if every unique binary never before run/approved is blocked, then no developer will be able to build and then run the software they are paid to write, since them developing it modifies said software into a new and never before seen sequence of bits.OP may not have meant to say that \"it's good to have an absolute allowlist of executable signatures and block everything else\", but that is how I interpreted the initial claim and I am merely pointing out that such a system would be more than inconvenient, it'd make the workflow of editing and then running software nearly impossible.\n \nreply",
      "> Does every program written by every dev have to be cleared?No, that's not how things are implemented normally, exactly because they wouldn't work.\n \nreply",
      "> No, that's not how things are implemented normally, exactly because they wouldn't work.I used to work for a gov't contractor. I wrote a ~10 line golang http server, just because at the time golang was still new (this was years ago) and I wanted to try it. Not even 2 minutes later I got a call from the IT team asking a bunch of questions about why I was running that program (the http server not golang). I agree the practice is dumb but there are definitely companies who have it setup that way.\n \nreply",
      "So running it wasn't prevented for you, and new apps listening on the network trigger notifications that the IT checks on immediately. That sounds like a reasonable policy.\n \nreply"
    ],
    "link": "https://blog.yossarian.net/2025/06/11/github-actions-policies-dumb-bypass",
    "first_paragraph": "\nJun 11, 2025\n\n    \u00a0 \u00a0\n\n    \n      \n        Tags:\n        \n        \n          security\n\n    \n\n    \u00a0 \u00a0\n\n    \n  TL;DR: GitHub Actions provides a policy mechanism for limiting the kinds of\nactions and reusable workflows that can be used within a repository,\norganization, or entire enterprise. Unfortunately, this mechanism is trivial\nto bypass. GitHub has told me that they don\u2019t consider\nthis a security issue (I disagree), so I\u2019m publishing this post as-is.GitHub Actions is GitHub\u2019s CI/CD offering. I\u2019m a big fan of it, despite its\nspotty\nsecurity\ntrack\nrecord.Because a CI/CD offering is essentially arbitrary code execution as\na service, users are expected to be careful about what they allow\nto run in their workflows, especially privileged workflows that have access\nto secrets and/or can modify the repository itself. That, in effect, means\nthat users need to be careful about what actions and reusable workflows\nthey trust.Like with other open source ecosystems, downstream consumers (i.e., us"
  },
  {
    "title": "How Microsoft Office Moved from Source Depot to Git (danielsada.tech)",
    "points": 4,
    "submitter": "dshacker",
    "submit_time": "2025-06-12T00:15:23 1749687323",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://danielsada.tech/blog/carreer-part-7-how-office-moved-to-git-and-i-loved-devex/",
    "first_paragraph": ""
  },
  {
    "title": "Characterizing my first attempt at copper-only passives (emarhavil.com)",
    "points": 18,
    "submitter": "tverbeure",
    "submit_time": "2025-06-08T06:01:50 1749362510",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=44214878",
    "comments": [
      "With only several dollars worth of high-precision PCB specifications, you two can save multiple fractions of a cent worth of passive components!There are applications where this can be useful, though.  Especially when PCB production quirks can be more repeatable than component soldering, which can be affected by ambient temperature, humidity, solder paste age, and a bunch of other factors.\n \nreply",
      "People do that inside silicon chips. I searched in some of the old posts by kens:Resistors: https://www.righto.com/2022/01/silicon-die-teardown-look-ins...Capacitors: https://www.righto.com/2018/06/silicon-die-analysis-op-amp-w...\n \nreply",
      "It gets done inside silicon chips because the (relative) costs of going off the chip and back on again are huge, and the parasitic inductance of the on/off chip path can make a an off-chip capacitor effectively useless.\n \nreply",
      "I think that if you care about a 2pF cap then you need to understand the parasitics in your board anyway\n \nreply",
      "You can also build much lower inductance bypass caps with your internal power planes than you can get out of any component!\n \nreply",
      "When I first started doing microstrip RF distributed elements like UHF notch filters I was really confused by the results I was getting from my $400 pocketVNA (this was before nanovna, etc). They worked when I had them in my physical RF pipeline but the S11 I was measuring was crazy. Eventually I gave up.One day while complaining on IRC someone suggested I let the VNA warm up before doing the calibration routine. I'd just been doing it right after turning it on because naturally, that's what was required first. I waited 15 minutes and then did the calibration. Suddenly the S11 plots actually matched the behavior and I could compare directly against the sonnet fullwave sim design instead of just \"it seems to work\".\n \nreply"
    ],
    "link": "https://moroso.emarhavil.com/~joshua/2pf-characterization.html",
    "first_paragraph": "Last year, I kind of got a bee in my bonnet about trying to see if I could accurately (?) make small RF passives out of copper, rather than buying 2pF NP0 capacitors or something, as part of a long-on-the-horizon project to make extremely inexpensive GHz-class oscilloscope probes.  I figured that the right place to start was to fab out a board on JLCPCB's JLC04161H-3313 stackup with a handful of calibration standards, and some of the passives that I wanted to measure.  Then, I took it over to Tom Verbeure's house, and we had a shootout trying to measure these passives with our varying test equipment -- he and his HP 8753C, and me and my NanoVNA.The results were not great, but we didn't really know that yet.  Now that I just got a new Siglent SVA1032X, though (and a LibreCAL eCal!), I figured that I'd better revisit this.  When we last left our heroes, I had two versions of the board -- one finished with HASL, and one finished with ENIG.  And we had already discovered that some of the s"
  },
  {
    "title": "The Seymour Cray Era of Supercomputers (ztoz.blog)",
    "points": 19,
    "submitter": "ingve",
    "submit_time": "2025-06-11T23:20:03 1749684003",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=44252858",
    "comments": [
      "Just now I was trying to find a microcontroller that would match the performance of the original Cray 1.But of course, it's complicated.Throughout of various floating point operations can vary by a factor of 100 on a simple machine. Which is why the standard benchmarks perform a combination of typical calculations. There's the \"Gibson Mix\" abd \"Livermore Loops\".Roy Longbottom is perhaps the most comprehensive compilation of results from testing via these benchmarks.https://www.roylongbottom.org.uk/Cray%201%20Supercomputer%20...Essentially, the Raspberry Pi 4 is about 100 times as fast as a Cray 1.I can find tiny chips that can do specific DSP applications at 160 MFLOPS via fixed-function units, but it's difficult to know if that's comparable.The FPU on some ARM Cotex-M4+ devices can rival a Cray 1 in general-purpose floating point, and their fixed function units can exceed that 100x. Mostly these are control units for LIDAR devices in automotive applications. They can pull more power than the Apple M1 on my iPad, 20 Watts or so.How many Cray 1 computers does it take to sit there and listen for me to say \"Hey Siri...\"?\n \nreply",
      "The Cray PVP line was also doing double precision floating point, and could overlap vector memory operations with math operations. My guess is that you would need a microcontroller operating at several hundred MHz to beat a Cray-1 in practice. The later Cray-1/S and /M variants also supported a 10gbps link to an SSD of several hundred megabytes, which is hard to beat in a microcontroller.\n \nreply",
      "That last part (The \u201cHey, Siri! Equivalent) suggests that you may be on the path toward a unit of measurement for want of intelligence.I think it may be interchangeable with the sqrt(-1).\n \nreply",
      "The CDC 6600 had a very odd architecture. It had several I/O master processors, but the floating point engine was the slave.This was prior to silicon microprocessors, so these processors were laid out on circuit boards with arrays of transistors as the active elements. Special care was taken in laying out the wire leads to ensure that signals arrived at the correct time as well as reducing line capacitance.The wiki describes it as the evolutionary precursor of RISC (like ARM on your phone, for those less familiar).The CDC 6600 ran at 10MHz.https://en.m.wikipedia.org/wiki/CDC_6600The Cray-1 would eventually run the \"UNICOS\" version of UNIX, and the FPU featured vector registers, which have been brought back in RISC-V.The Cray-1 ran at 80MHz.https://en.m.wikipedia.org/wiki/Cray-1\n \nreply",
      "Thank you! I have been fascinated by the life of Cray and his supercomputers. I need to buy this book and read from cover to cover.\n \nreply"
    ],
    "link": "https://ztoz.blog/posts/cray-era-supercomputers/",
    "first_paragraph": "2025-06-11The Seymour Cray Era of Supercomputers: From Fast Machines to Fast Codes is a technical and business history of the roughly three-decades when Seymour Cray dominated the development of a class of computer called the \u201csupercomputer\u201d. The book covers the development of the major supercomputer models, the technical decisions and trade-offs involved, and changes to the market. The book ends with SGI\u2019s purchase of Cray\u2019s assets and the transition to massively parallel processing.The Seymour Cray Era of Supercomputers: From Fast Machines to Fast Codes. By Boelie Elzen and Donald MacKenzie. ACM Books. ISBN 979-8-4007-1369-9.\nDOI 10.1145/3705551.Early on, computer designs were bifurcated between the domains of \u201cbusiness computing\u201d and \u201cscientific computing.\u201d Business computing was almost exclusively fixed-point, dealt with categorical and string data, often I/O bound, and had a broad base of less technical users with often similar problems (e.g. payroll). Scientific computing heavily"
  },
  {
    "title": "Show HN: The Roman Industrial Revolution that could have been (thelydianstone.com)",
    "points": 5,
    "submitter": "miki_tyler",
    "submit_time": "2025-06-11T23:51:54 1749685914",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44253083",
    "comments": [
      "'lest darkness fall' by L Sprague De Camp..1939\n \nreply",
      "This is very interesting. It seems similar to A Connecticut Yankee in King Arthur's Court, which also features a time traveler. I'll try to get a used copy from eBay.\n \nreply"
    ],
    "link": "https://thelydianstone.com/",
    "first_paragraph": "NOTE: I\u2019ve intentionally left some obvious mistakes, like the faces on page 20 of Issue #1, to keep the raw, experimental feel of the project. As explained in the \"the making of\" section, this comic book was created with the aid of an AI model. I review and adjust each image, sometimes editing by hand to fix things like missing characters, extra fingers, or elements that don\u2019t belong. Then I add the dialogue boxes. I understand that newer model versions may produce better results; however, replacing the original sketches would undermine the purpose of the experiment. For each issue, I\u2019ll use whichever LLM version is available at the time I\u2019m working on it.The Lydian Stone is a story about an archaeology student named Ulyses who accidentally finds himself texting a Roman slave named Marcus. With each issue, Marcus faces a new danger or challenge, and Ulyses tries to help him using modern knowledge and technology, carefully adapting solutions Marcus can actually build with what's availab"
  }
]