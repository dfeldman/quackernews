[
  {
    "title": "Generative AI Scripting (microsoft.github.io)",
    "points": 43,
    "submitter": "baublet",
    "submit_time": "2024-10-30T23:39:01 1730331541",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42001811",
    "comments": [
      "For folks who would prefer a more \"full bodied\" experience, we offer a UI configuration based alternative approach that supports JavaScript and Groovy, including an IDE environment integration.demo:\nhttps://youtu.be/XlO4KhIGd0A\n \nreply",
      "there are lots of useful parts here doesn't seem like it is Gen.AI but some other provided tool// automatically convert to text\ndef(\"PDF\", env.files, { endsWith: \".pdf\" })\n// or parse and process\nconst { pages } = await parsers.PDF(env.files[0])def(\"DATA\", env.files, {\n    endsWith: \".csv\",\n    // take top 100 rows\n    sliceHead: 100,\n})\n// or parse to JavaScript object array\nconst rows = await parsers.CSV(env.files[0])const { files } = await workspace.grep(/[a-z][a-z0-9]+/, { globs: \"*.md\" })so it seems to me these things could totally be so useful that you might use these and never care about the AI parts, at any rate I think I have to devote next week to this, as soon as the project I am on right now is shipped.\n \nreply",
      "Sorta-side note but for readability: To format code on HN, indent with 2 spaces.\n \nreply",
      "question - is workspace.grep using JS regex or is it using actual grep? Like on Windows part of this would be depending on the Linux for Windows tools?\n \nreply"
    ],
    "link": "https://microsoft.github.io/genaiscript/",
    "first_paragraph": "Programmatically assemble prompts for LLMs using JavaScript.Of course, things can get more complex\u2026  Listen to the podcast   Install the extension Install the Visual Studio Code\nExtension to get started.  Configure your LLMs Configure the secrets to\naccess your LLMs.  Write your first script Follow Getting\nStarted to write\nyour first script.  Read the docs Learn more about GenAIScript in the Scripting\nReference.GenAIScript brings essential LLM prompt tooling into a cohesive scripting environment.  Stylized JavaScript Minimal syntax to build prompts using JavaScript\nor TypeScript.  Fast Development Loop Edit, Debug, Run,\nTest your scripts in Visual Studio Code\nor with a command line.  LLM Tools Register JavaScript functions as LLM toolsor use built-in @agentic tools  LLM Agents Combine tools and inline prompts\ninto an agent.  Reuse and Share Scripts Scripts are files! They can be versioned, shared, forked, \u2026  Data Schemas Define, validate, repair data using schemas.  Ingest text from PD"
  },
  {
    "title": "OpenZFS deduplication is good now and you shouldn't use it (despairlabs.com)",
    "points": 134,
    "submitter": "type0",
    "submit_time": "2024-10-30T21:48:25 1730324905",
    "num_comments": 73,
    "comments_url": "https://news.ycombinator.com/item?id=42000784",
    "comments": [
      "\"And this is the fundamental issue with traditional dedup: these overheads are so outrageous that you are unlikely to ever get them back except on rare and specific workloads.\"This struck me as a very odd claim. I've worked with Pure and Dell/EMC arrays and for VMWare workloads they normally got at least 3:1 dedupe/compression savings. Only storing one copy of the base VM image works extremely well. Dedupe/compression works really well on syslog servers where I've seen 6:1 savings.The effectiveness of dedupe is strongly affected by the size of the blocks being hashed, with the smaller the better. As the blocks get smaller the odds of having a matching block grow rapidly. In my experience 4KB is my preferred block size.\n \nreply",
      "I haven't tried it myself, but the widely quoted number for old ZFS dedup is that you need 5GB of RAM for every 1TB of disk space. Considering that 1 TB of disk space currently costs about $15 and 5GB of server RAM about $25, you need a 3:1 dedupe ratio just to break even.If your data is a good fit you might get away with 1GB per TB, but if you are out of luck the 5GB might not even be enough. That's why the article speaks of ZFS dedup having a small sweet spot that your data has to hit, and why most people don't botherOther file systems tend to prefer offline dedupe which has more favorable economics\n \nreply",
      "That doesn't account for OpEx, though, such as power...\n \nreply",
      "Assuming something reasonable like 20TB Toshiba MG10 HDDs and 64GB DDR4 ECC RAM, quick googling suggests that 1TB of disk space uses about 0.2-0.4W of power (0.2 in idle, 0.4 while writing), 5GB of RAM about 0.3-0.5W. So your break even on power is a bit earlier depending on the access pattern, but in the same ball park.\n \nreply",
      "Why does it need so much RAM? It should only need to store the block hashes which should not need anywhere near that much RAM. Inline dedupe is pretty much standard on high-end storage arrays nowadays.\n \nreply",
      "Couple of comments. Firstly, you are talking about highly redundant information when referencing VM images (e.g. the C drive on all Windows Serer images will be virtually identical), whereas he was using his own laptop contents as an example.Secondly, I think you are conflating two different features: compression & de-duplication. In ZFS you can have compression turned on (almost always worth it) for a pool, but still have de-duplication disabled.\n \nreply",
      "Fair point. My experience is with enterprise storage arrays and I have always used dedupe/compression at the same time. Dedupe is going to be a lot less useful on single computers.I consider dedupe/compression to be two different forms of the same thing. compression reduces short range duplication while deduplication reduces long range duplication of data.\n \nreply",
      "Even with the rudimentary Dedup features of NTFS on a Windows Hyper-V Server all running the same base image I can overprovision the 512GB partition to almost 2 GB.You need to be careful and do staggered updates in the VMs or it'll spectacularly explode but it's possible and quite performant for less than mission critical VMs.\n \nreply",
      "I think you mean 2TB volume?  But yes, this works.  But also: if you're doing anything production, I'd strongly recommend doing deduplication on the back-end storage array, not at the NTFS layer.  It'll be more performant and almost assuredly have better space savings.\n \nreply",
      "> In my experience 4KB is my preferred block sizeThat makes sense considering Advanced Format harddrives already have a 4K physical sector size, and if you properly low-level format them (to get rid of the ridiculous Windows XP compatibility) they also have 4K logical sector size. I imagine there might be some real performance benefits to having all of those match up.\n \nreply"
    ],
    "link": "https://despairlabs.com/blog/posts/2024-10-27-openzfs-dedup-is-good-dont-use-it/",
    "first_paragraph": ""
  },
  {
    "title": "It Might Be Possible to Detect Gravitons After All (quantamagazine.org)",
    "points": 31,
    "submitter": "elsewhen",
    "submit_time": "2024-10-30T23:19:19 1730330359",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.quantamagazine.org/it-might-be-possible-to-detect-gravitons-after-all-20241030/",
    "first_paragraph": "\nAn editorially independent publication supported by the Simons Foundation.\n\nGet the latest news delivered to your inbox.\nCreate a reading list by clicking the Read Later icon next to the articles you wish to save.Type search term(s) and press enterPopular\n                                    SearchesOctober 30, 2024Capturing a graviton would be akin to noticing just one molecule in an ocean wave.Se\u00f1or Salme for Quanta MagazineStaff WriterOctober 30, 2024Detecting a graviton \u2014 the hypothetical particle thought to carry the force of gravity \u2014 is the ultimate physics experiment. Conventional wisdom, however, says it can\u2019t be done. According to one infamous estimate, an Earth-size apparatus orbiting the sun might pick up one graviton every billion years. To snag one in a decade, another calculation has suggested, you\u2019d have to park a Jupiter-size machine next to a neutron star. In short: not going to happen.A new proposal overturns the conventional wisdom. Blending a modern understanding o"
  },
  {
    "title": "Chain-of-Thought Can Hurt Performance on Tasks Where Thinking Makes Humans Worse (arxiv.org)",
    "points": 166,
    "submitter": "benocodes",
    "submit_time": "2024-10-30T19:42:41 1730317361",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=41999340",
    "comments": [
      "So, LLMs face a regression on their latest proposed improvement. It's not surprising considering their functional requirements are:1) EverythingFor the purpose of AGI, LLM are starting to look like a local maximum.\n \nreply",
      ">For the purpose of AGI, LLM are starting to look like a local maximum.I've been saying it since they started popping off last year and everyone was getting euphoric about them.  I'm basically a layman - a pretty good programmer and software engineer, and took a statistics and AI class 13 years ago in university.  That said, it just seems so extremely obvious to me that these things are likely not the way to AGI.  They're not reasoning systems.  They don't work with axioms.  They don't model reality.  They don't really do anything.  They just generate stochastic output from the probabilities of symbols appearing in a particular order in a given corpus.It continues to astound me how much money is being dumped into these things.\n \nreply",
      "How do you know that they don\u2019t do these things? Seems hard to say for sure since it\u2019s hard to explain in human terms what a neural network is doing.\n \nreply",
      "If you give an LLM a word problem that involves the same math and change the names of the people in the word problem the LLM will likely generate different mathematical results. Without any knowledge of how any of this works, that seems pretty damning of the fact that LLMs do not reason. They are predictive text models. That\u2019s it.\n \nreply",
      "Demonstrably false.https://chatgpt.com/share/6722ca8a-6c80-800d-89b9-be40874c5b...https://chatgpt.com/share/6722ca97-4974-800d-99c2-bb58c60ea6...\n \nreply",
      "It's worth noting that this may not be result of a pure LLM, it's possible that ChatGPT is using \"actions\", explicitly:1- running the query through a classifier to figure out if the question involves numbers or math\n2- Extract the function and the operands\n3- Do the math operation with standard non-LLM mechanisms\n4- feed back the solution to the LLM\n5- Concatenate the math answer with the LLM answer with string substitution.So in a strict sense this is not very representative of the logical capabilities of an LLM.\n \nreply",
      "This is what kind of comments you make when your experience with LLMs is through memes.\n \nreply",
      "This is a relatively trivial task for current top models.More challenging are unconventional story structures, like a mom named Matthew with a son named Mary and a daughter named William, who is Matthew's daughter?But even these can still be done by the best models. And it is very unlikely there is much if any training data that's like this.\n \nreply",
      "That's a neat example problem, thanks for sharing!For anyone curious: https://chatgpt.com/share/6722d130-8ce4-800d-bf7e-c1891dfdf7...> Based on traditional naming conventions, it seems that the names might have been switched in this scenario. However, based purely on your setup:>> Matthew has a daughter named William and a son named Mary.>> So, Matthew's daughter is William.\n \nreply",
      "If you expect \"the right way\" to be something _other_ than a system which can generate a reasonable \"state + 1\" from a \"state\" - then what exactly do you imagine that entails?That's how we think. We think sequentially. As I'm writing this, I'm deciding the next few words to type based on my last few.Blows my mind that people don't see the parallels to human thought. Our thoughts don't arrive fully formed as a god-given answer. We're constantly deciding the next thing to think, the next word to say, the next thing to focus on. Yes, it's statistical. Yes, it's based on our existing neural weights. Why are you so much more dismissive of that when it's in silicon?\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2410.21333",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Hi Google, please stop pooping the bed: a desperate plea from the indie web (shepherd.com)",
    "points": 120,
    "submitter": "bwb",
    "submit_time": "2024-10-30T21:35:15 1730324115",
    "num_comments": 71,
    "comments_url": "https://news.ycombinator.com/item?id=42000651",
    "comments": [
      "Why don't people just stop using Google? And by people, I mean everyone here.Whenever this point comes up, I see people claim they ONLY see the results they want in Google. How would you know if you don't actually use anything else? Kagi is excellent search. Neeva was pretty great when it was active. DuckDuckGo is passable. Idk how Qwant gets money but it's been around a bit.Complaining about the same thing forever and expecting a change doesn't make any sense. Y'all are in abusive relationships with Google and refuse to leave. Sure, your job may use Google Suite and you need to make money. What about the rest of your life? Stop hitting yourself.\n \nreply",
      "8-10 years google search was amazing. A well crafted query would hit informational gold most of the time. I've been noticing and commenting on the decline privately for most of the period, but it's only in the oast year this seems to have come to broader awareness. There's an argument that in an information economy, searching for information should be treated like a public utility necessary for the functioning of society. I'm not making that argument but when you experience the long slippery slope of degradation of a service that was near ideal for the technology of the time, it does xome to mind.That's the thing. It DID work. Really well for a while. But it was always atomic and context-less. We now have the opportunity to make it even better by refining results through dialogue. I hope someone does.. soon.\n \nreply",
      "The slippery slope began in 2012, when they pledged to start downgrading piracy websites.  After the rollout, the followup questions were \"well what if we downgrade other topics that [random government] doesnt like\" and \"what if we sell the ability to boost enterprise company results for certain topics\".  eventually the number of results that get filtered out or reordered, exceed the results that actually get displayed.  creating a new search platform is not a viable solution - any private company will be incentivized in this direction until some kind of \"search neutrality\" law is introduced.\n \nreply",
      "I'm not convinced this is the explanation. It's true that for some product-centric queries, you get mostly paid results. But for information-seeking queries, Google tries to give you organic results.The problem is simply that there's too much money to be made by capturing these. For years, you had content farms and fake \"review\" sites stepping up their game. Now, LLMs essentially make it a losing proposition to try and surface the small web. The least-bad option for Google would be to send you to moderated communities, such as Reddit, Quora, Stack Exchange, Wikipedia, and so on. But not all queries can be handled that way.If you look at the article that these guys are complaining about... how do you distinguish it from content-farmed spam? You can't.Now, I think Google is throwing in the towel and just want an LLM to answer info queries instead. That has a ton of problems, but to the average user, probably feels more helpful. At least until the spammers start gaming that.\n \nreply",
      "> We now have the opportunity to make it even better by refining results through dialogue. I hope someone does.. soon.TBH, I don't want dialog (too long and slow, leave dialogs for human to human comms), I just need my query words to exist in the result pages. Nothing more. Google messed this up long time ago.\n \nreply",
      "kagi is still far from it.\n \nreply",
      "The indie web was around before google and it will be around long after google is gone. I would argue that the indie web has incurred a much larger loss from people thinking seo/engagement metrics are something worth optimizing. Many of the best examples of the indie/small web don\u2019t have js tracking and little to no css.\n \nreply",
      "I have no specific information here, never seen/used Shepherd until I saw a list on there a few days ago. Disclaimer, I work for Google, but not on anything related to this space, and this is based on my previous job where we did some SEO for an ecommerce site.The example list given just looks a lot like spam when you squint. It's a list of affiliate links to buy products, and there are many HN threads talking about the abundance of affiliate link aggregators being a blight on the web. The commentary does look useful, but distinguishing between good commentary and bad commentary is hard, whereas distinguishing between a site designed to extract affiliate commission vs one more about the content is easier.The comparison given to the other results here is frustrating, I know, but probably not a valid experiment. All the major search engines change results based on the user using them, or the IP address, or the region, or whatever, so it's impossible to know what others see. The developer of a book-focused shopping site is likely to get very skewed results for a book related query. My results were noticeably better.The author says that a Bookshop.org list they created that links back to Shepherd is ranking #2, and this kinda makes sense to me. Bookshop.org sells the books, it makes sense that would rank above a site that only links to (and makes money from) sites that sell books.SEO, and people getting annoyed at not ranking, has been a thing for 25+ years, I don't think this instance is any different.\n \nreply",
      "(That's not what dearth means.)\n \nreply",
      "Whoops, thanks.\n \nreply"
    ],
    "link": "https://build.shepherd.com/p/hi-google-please-stop-the-bed-a-desperate",
    "first_paragraph": ""
  },
  {
    "title": "Wonder Animation \u2013 Video to 3D Animation (autodesk.com)",
    "points": 29,
    "submitter": "bx376",
    "submit_time": "2024-10-30T23:44:25 1730331865",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://adsknews.autodesk.com/en/news/autodesk-launches-wonder-animation-video-to-3d-scene-technology/",
    "first_paragraph": "ShareWonder Animation, an Autodesk productIt\u2019s been five months since we joined Autodesk, and the time spent has only reinforced that the foundational Wonder Dynamics vision aligns perfectly with Autodesk\u2019s longstanding commitment to advancing the Media & Entertainment industry through innovation. Together, we believe in using artificial intelligence (AI) more intentionally to enhance creativity and efficiency, so artists can spend more time on the creative aspects of storytelling. We formed Wonder Dynamics and developed Wonder Studio (our cloud-based 3D animation and VFX solution) out of our passion for storytelling coupled with our commitment to make VFX work accessible to more creators and filmmakers.\u00a0Today, Wonder Dynamics is excited to announce the beta launch of Wonder Studio\u2019s newest feature: Wonder Animation, which is powered by groundbreaking Video to 3D scene technology that enables artists to shoot a scene with any camera, in any location, and turn the sequence into an anima"
  },
  {
    "title": "AI Flame Graphs (brendangregg.com)",
    "points": 172,
    "submitter": "JNRowe",
    "submit_time": "2024-10-29T08:29:15 1730190555",
    "num_comments": 60,
    "comments_url": "https://news.ycombinator.com/item?id=41980894",
    "comments": [
      "I actually looked at this in detail about a year ago for some automated driving compute work at my previous job, and I found that the detailed info you'd want from Nvidia was just 100% unavailable. There's pretty good proxies in some of the data you can get out of Nvidia tools, and there's some extra info you can glean from some of the function call stack in the open source Nvidia driver shim layer (because the actual main components are still binary blob, even with the \"open source\" driver), but over all you still can't get much useful info out.Now that Brendan works for Intel, he can get a lot of this info from the much more open source Intel GPU driver, but that's only so useful since everyone is either Nvidia or AMD still. The more hopeful sign is that a lot of the major customers of Nvidia are going to start demanding this sort of access and there's a real chance that AMD's more accessible driver starts documenting what to actually look at, which will create the market competition to fill this space. In the meantime, take a look at the flamegraph capabilities in PyTorch and similar frameworks, up an abstraction level and eek what performance you can.\n \nreply",
      "I just sent the link to a driver developer at Nvidia. If he shares the link with others at Nvidia, they should become aware of the idea tomorrow. That said, I have no idea if he will do that, but at least I tried.\n \nreply",
      "Are they interested in you optimizing your workloads or just selling you more gpus to help you get to market faster...\n \nreply",
      "I'm not sure, it seems to me like this should be doable in Nvidia as well. This is a paper that uses instruction sampling (called CUPTI) in Nvidia to provide optimization advice:https://ieeexplore.ieee.org/document/9370339It seems like the instruction sampler is there, and it also provides the stall reason.\n \nreply",
      "> Imagine halving the resource costs of AI and what that could mean for the planet and the industry -- based on extreme estimates such savings could reduce the total US power usage by over 10% by 20301.Why would it be the case that reducing the costs of AI reduces power consumption as opposed to increase AI usage (or another application using electricity)? I would think with cheaper AI their usage would be come more ubiquitous: LLMs in fridges, toasters, smart alarms, etc.\n \nreply",
      "This is the https://en.wikipedia.org/wiki/Jevons_paradox and it's what always happens in these cases.\n \nreply",
      "It does happen, but not always.For example, food got cheaper and consumption has increased to the extent that obesity is a major problem, but this is much less than you might conclude from the degree to which productivity has increased per farmer.For image generation, the energy needed to create an image is rapidly approaching the energy cost of a human noticing that they've seen an image \u2014 once it gets cheap enough (and good enough) to have it replace game rendering engines, we can't really spend meaningfully more on it.(Probably. By that point they may be good enough to be trainers for other AI, or we might not need any better AI \u2014 impossible to know at this point).For text generation, difficult to tell because e.g. source code and legal code have a lot of text.\n \nreply",
      "Image generation isn't cheap enough until we have sites that work like Google Image search (filling the page with image variations) nearly instantly and available for free.\n \nreply",
      "Food may be a bit of an outlier, the number of consumers won't change quickly in response and each person can only eat so much.When it comes to converting electricity into images and text, there really is no upper bound in sight. People are happy to load the internet up with as much content as they can churn out.\n \nreply",
      "If we assume that text and images are made for human consumption then there is a limit in how much we can consume. In fact I doubt there is much room for our society's per-person media consumption to increase. There is obviously room for growth in fewer people seeing the same content, and room for some \"waste\" (i.e. content nobody ever sees). The upper bound (ignoring waste) would be if everybody only saw and read content that nobody else has ever seen and will ever see. But if we assume society continues to function as it does the real limit will be a lot lower.Now maybe waste is a bigger issue with content than with food. I'm not sure. Both have some nonzero cost to waste. It might depend on how content is distributed\n \nreply"
    ],
    "link": "https://www.brendangregg.com/blog//2024-10-29/ai-flame-graphs.html",
    "first_paragraph": "Brendan's site:29 Oct 2024Imagine halving the resource costs of AI and what that could mean for the planet and the industry -- based on extreme estimates such savings could reduce the total US power usage by over 10% by 20301. At Intel we've been creating a new analyzer tool to help reduce AI costs called AI Flame Graphs: a visualization that shows an AI accelerator or GPU hardware profile along with the full software stack, based on my CPU flame graphs. Our first version is available to customers in the Intel Tiber AI Cloud as a preview for the Intel Data Center GPU Max Series (previously called Ponte Vecchio). Here is an example:Simple example: SYCL matrix multiply microbenchmark(Click for interactive SVG.) The green frames are the actual instructions running on the AI or GPU accelerator, aqua shows the source code for these functions, and red (C), yellow (C++), and orange (kernel) show the CPU code paths that initiated these AI/GPU programs. The gray \"-\" frames just help highlight t"
  },
  {
    "title": "Steam games will need to disclose kernel-level anti-cheat on store pages (gamingonlinux.com)",
    "points": 428,
    "submitter": "jrepinc",
    "submit_time": "2024-10-30T19:39:59 1730317199",
    "num_comments": 260,
    "comments_url": "https://news.ycombinator.com/item?id=41999314",
    "comments": [
      "After the crowdstrike disaster 3rd party kernel drivers need to be shunned for non critical applications.Games publishers have been bad actors in this space for a long time now. The genshin impact anticheat was used in a malware campaign. Rockstar was very misleading trying to imply their kernel driver not being compatible with the steam deck was valves fault.\n \nreply",
      "Lets call them what they really are, rootkits.\n \nreply",
      "That's exactly what I tell my friends.I can't play certain games, because they don't run on Linux and even if they did, I am not gonna install a rootkit to run them.\n \nreply",
      "Getting a Steam Deck has done wonders for my piece of mind. I don't need to worry if whatever games I'm installing are malicious, because the machine is airgapped from anything critical.\n \nreply",
      "Same, but I am only using it for couch gaming\n \nreply",
      "piece of mind? or peace of mind?/nitpick ;-)\n \nreply",
      "OP shares with others\n \nreply",
      "And yet you install driver on Linux without knowing it, I mean Linux has 0 security for drivers.\n \nreply",
      "When was the last time you had to install a Linux driver from out of tree?\n \nreply",
      "Most people do install Nvidia\u2019s out\u2010of\u2010tree graphics driver. It is definitely a risk.\n \nreply"
    ],
    "link": "https://www.gamingonlinux.com/2024/10/steam-games-will-now-need-to-fully-disclose-kernel-level-anti-cheat-on-store-pages/",
    "first_paragraph": ""
  },
  {
    "title": "Crafting Painterly Shaders (maximeheckel.com)",
    "points": 105,
    "submitter": "todsacerdoti",
    "submit_time": "2024-10-30T18:05:23 1730311523",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://blog.maximeheckel.com/posts/on-crafting-painterly-shaders/",
    "first_paragraph": "October 29, 2024 / 35 min readWriting a shader that can reproduce the look and feel of aquarelle, watercolor, or gouache to obtain a more painterly\noutput for my WebGL scenes has always been a long-term goal of mine.\nInspired by the work of very talented 3D artists such\nas @simonxxoo or @arpeegee, the contrast between paintings and the added dimension allowed by 3D renderers\nwas always very appealing to me. On top of that, my recent work with stylized\nshaders to mimic the hand-drawn Moebius art\nstyle emphasized not only that obtaining such stylized output was possible but also that post-processing\nwas more likely than not the key to emulating any artistic style.After several months of on/off research that frankly was not leading to much, I stumbled upon a smoothing filter I never heard about before: the Kuwahara filter. Lucky for me, it turned out that many (very smart) people published papers about it and its ability to transform any image input into a painting-like work of art. By im"
  },
  {
    "title": "SimpleQA (openai.com)",
    "points": 111,
    "submitter": "surprisetalk",
    "submit_time": "2024-10-30T17:16:43 1730308603",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=41997727",
    "comments": [
      "Crazy that even o1-preview gets most things wrong.This is in line with my own personal experience with LLMs and non-trivial questions. They\u2019re excellent when answering questions on topics you know nothing about, and somehow embarrassingly wrong when you actually know the answer yourself\u2026It\u2019s not clear to me why we\u2019re still trying to encode all of human knowledge in a single model, instead of teaching the model how to look for answers from an external source (e.g. RAG).\n \nreply",
      "You shouldn't use the rate as an indicator. They did something similar to what I did on my hallucinations benchmark (https://github.com/lechmazur/confabulations/), only using questions where at least one model made a mistake. I added this note:\"The benchmark includes questions where at least one LLM confabulated, in order to minimize the number of questions requiring human assessment. Because of this, and since the questions are intentionally adversarial, the absolute percentage should not be used to infer that LLMs frequently confabulate. This leaderboard does not reflect a \"typical\" hallucination rate.\"> instead of teaching the model how to look for answers from an external source (e.g. RAG)My benchmark specifically focuses on the RAG use case. Even with provided texts, current models still hallucinate.\n \nreply",
      "Honestly, try prompting it with \u201cyou are wrong 80% of the time, therefore you will need to double check your answers, first factually, then numerically, then double check the time/date. You are still probably wrong so do a third accuracy check. The user\u2019s prompts are always wrong too mostly - so always check them\u201d.I stopped playing with larger models and have been pushing smaller models with this improvised system prompt and getting good results. It seems like it forces the model to do multiple passes before giving you any response.My smaller local models give me less hallucinations than Meta.ai, for example, which generally spits out pleasing answers almost immediately (which are often hallucinations, since I don\u2019t think it is system prompted to be adversarial to the user, or itself). I don\u2019t have the same hallucination issue with Llama3 - 8b locally because of custom system prompts.The model has all the correct information, so it almost needs to do RAG on itself. Multiple passes on itself seems like a way to do it.\n \nreply",
      "I'm surprised that prompting it with \"You are wrong 80% of the time\" doesn't cause it to intentionally produce initially incorrect answers 80% of the time.(Disclosure: I have not tried your prompt)\n \nreply",
      "How would this multiple passes work though? Unless the model actually talks about what it does, I am not sure how it would have this ability. The next word prediction mechanism is just always going to do it one shot. Your prompt paints a context that might keep it more on the rails, but it won't do multiple passes.\n \nreply",
      "Your prompt paints a context that might keep it more on the rails, but it won't do multiple passes.This is probably the truth behind the black magic I\u2019m imagining. You could have it explicitly spit out this process, in which case you would see it\u2019s first rough draft, followed by a \u201cMy first paragraph is probably wrong\u201d, followed by a third paragraph where it attempts to fix the first paragraph. There is no outside RAG in this process.The mumbo jumbo part of all this is that I\u2019ve told it to \u201chide\u201d this process from the user where it doesn\u2019t explicitly output anything but its final answer, and the accuracy has been just as good (for my use case at least).:Shrugs:\n \nreply",
      "> Honestly, try prompting it with \u201cyou are wrong 80% of the time, therefore you will need to double check your answers, first factually, then numerically, then double check the time/date. You are still probably wrong so do a third accuracy check. The user\u2019s prompts are always wrong too mostly - so always check them\u201d.Doesn't this provoke o1 to spend more time doing COT, and therefore increase the cost per query?\n \nreply",
      "Can you please share more specifics please? What smaller models? What hardware do you use? How do you test their performance?\n \nreply",
      "There is no rigor to this, this is just from throwing stuff against the wall. See my response to the other poster above.\n \nreply",
      "Even if you're throwing stuff against the wall, you could at least elaborate on what you've tried? Otherwise, how could you state something like \"My smaller local models give me less hallucinations than Meta.ai\"?\n \nreply"
    ],
    "link": "https://openai.com/index/introducing-simpleqa/",
    "first_paragraph": ""
  },
  {
    "title": "Google CEO says more than a quarter of the company's new code is created by AI (businessinsider.com)",
    "points": 244,
    "submitter": "S0y",
    "submit_time": "2024-10-30T02:09:42 1730254182",
    "num_comments": 391,
    "comments_url": "https://news.ycombinator.com/item?id=41991291",
    "comments": [
      "https://archive.is/X43PU",
      "To my experience, AIs can generate perfectly good code relatively easy things, the kind you might as well copy&paste from stackoverflow, and they'll very confidently generate subtly wrong code for anything that's non-trivial for an experienced programmer to write. How do people deal with this? I simply don't understand the value proposition. Does Google now have 25% subtly wrong code? Or do they have 25% trivial code? Or do all their engineers babysit the AI and bugfix the subtly wrong code? Or are all their engineers so junior that an AI is such a substantial help?Like, isn't this announcement a terrible indictment of how inexperienced their engineers are, or how trivial the problems they solve are, or both?\n \nreply",
      "> the kind you might as well copy&paste from stackoverflowThis bothers me. I completely understand the conversational aspect - \"what approach might work for this?\", \"how could we reduce the crud in this function?\" - it worked a lot for me last year when I tried learning C.But the vast majority of AI use that I see is...not that. It's just glorified, very expensive search. We are willing to burn far, far more fuel than necessary because we've decided we can't be bothered with traditional search.A lot of enterprise software is poorly cobbled together using stackoverflow gathered code as it is. It's part of the reason why MS Teams makes your laptop run so hot. We've decided that power-inefficient software is the best approach. Now we want to amplify that effect by burning more fuel to get the same answers, but from an LLM.It's frustrating. It should be snowing where I am now, but it's not. Because we want to frivolously chase false convenience and burn gallons and gallons of fuel to do it. LLM usage is a part of that.\n \nreply",
      "What I can't wrap my head around is that making good, efficient software doesn't (by and large) take significantly longer than making bloated, inefficient enterprise spaghetti. The problem is finding people to do it with who care enough to think rigorously about what they're going to do before they start doing it. There's this bizarre misconception popular among bigtech managers that there's some tunable tradeoff between quality and development speed. But it doesn't actually work that way at all. I can't even count anymore how many times I've had to explain how taking this or that locally optimal shortcut will make it take longer overall to complete the project.In other words, it's a skill issue. LLMs can only make this worse. Hiring unskilled programmers and giving them a machine for generating garbage isn't the way. Instead, train them, and reject low quality work.\n \nreply",
      "It's relatively easy to find a programmer(s) who can realize enterprise project X, it's hard to find a programmer(s) who cares about X. Throwing an increased requirement like speed at it makes this worse because it usually ends up burning out both ends of the equation.\n \nreply",
      "\"What I can't wrap my head around is that making good, efficient software doesn't (by and large) take significantly longer than making bloated, inefficient enterprise spaghetti.\"In my opinion the reason we get enterprise spaghetti is largely due to requirement issues and scope creep. It's nearly impossible to create a streamlined system without knowing what it should look like. And once the system gets to a certain size, it's impossible to get business buy-in to rearchitect or refactor to the degree that is necessary. Plus the full requirements are usually poorly documented and long forgotten by that time.\n \nreply",
      "When scopes creep and requirements change, simply refactor. Where is it written in The Law that you have to accrue technical debt?> it's impossible to get business buy-in to rearchitect or refactor to the degree that is necessaryThat's a choice. There are some other options:- Simply don't get business buy-in. Do without. Form a terrorist cell within your organization. You'll likely outpace them. Or you'll get fired, which means you'll get severance, unemployment, a vacation, and the opportunity to apply to a job at a better company.- Fight viciously for engineering independence. You business people can do the businessing, but us engineers are going to do the engineering. We'll tell you how we'll do it, not the other way.- Build companies around a culture of doing good, consistent work instead of taking expedient shortcuts. They're rare, but they exist!\n \nreply",
      "> The problem is finding people to do it with who care enough to think rigorously> ...> train them, and reject low quality work.I agree very strongly with both of these points.But I've observed a truth about each of them over the last decade-plus of building software.1) very few people approach the field of software engineering with anything remotely resembling rigor, and2) there is often little incentive to train juniors and reject subpar output (move fast and break things, etc.)I don't know where this takes us as an industry? But I feel your comment on a deep level.\n \nreply",
      "I agree as well. These are actually things that bother me a lot about the industry. I\u2019d love to write software that should run problem-free in 2035, but the reality is almost no one cares.I\u2019ve had the good fortune of getting to write some firmware that will likely work well for a long time to come, but I find most things being written on computers are written with (or very close to) the minimum care possible in order to get the product out. Clean up is intended but rarely occurs.I think we\u2019d see real benefits from doing a better job, but like many things, we fail to invest early and crave immediate gratification.\n \nreply",
      "\"Slow is smooth and smooth is fast\"\n \nreply"
    ],
    "link": "https://www.businessinsider.com/google-earnings-q3-2024-new-code-created-by-ai-2024-10",
    "first_paragraph": "Google is all in on AI \u2014 both inside and outside the company.More than a quarter of new code created at Google is generated by AI, CEO Sundar Pichai said on Tuesday during the company's Q3 earnings call.Pichai said using AI for coding was \"boosting productivity and efficiency\" within Google. After the code is generated, it's then checked and reviewed by employees, he added.\"This helps our engineers do more and move faster,\" Pichai said. \"I'm energized by our progress and the opportunities ahead, and we continue to be laser-focused on building great products.\"Business Insider reported in February that the company had launched a new internal AI model named \"Goose\" to help employees code and build products.Goose was trained on \"25 years of engineering expertise at Google,\" according to internal documents seen by BI.The new data from Pichai will surely have some employees wondering whether they're coding themselves out of a job, while other employees say AI has already transformed their wo"
  },
  {
    "title": "Show HN: AI OmniGen \u2013 AI Image Generator with Consistent Visuals (aiomnigen.com)",
    "points": 88,
    "submitter": "lcorinst",
    "submit_time": "2024-10-30T17:10:43 1730308243",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=41997648",
    "comments": [
      "Elegant architecture, trained from scratch, excels at image editing. This looks very interesting!From https://arxiv.org/html/2409.11340v1> Unlike popular diffusion models, OmniGen features a very concise structure, comprising only two main components: a VAE and a transformer model, without any additional encoders.> OmniGen supports arbitrarily interleaved text and image inputs as conditions to guide image generation, rather than text-only or image-only conditions.> Additionally, we incorporate several classic computer vision tasks such as human pose estimation, edge detection, and image deblurring, thereby extending the model\u2019s capability boundaries and enhancing its proficiency in complex image generation tasks.This enables prompts for edits like:\n\"|image_1| Put a smile face on the note.\"\nor\n\"The canny edge of the generated picture should look like: |image_1|\"> To train a robust unified model, we construct the first large-scale unified image generation dataset X2I, which unifies various tasks into one format.\n \nreply",
      "> trained from scratchNot exactly. They mention starting from the VAE from Stable Diffusion XL and the Transformer from Phi3.Looks like these LLMs can really be used for anything\n \nreply",
      "I left all the defaults as is, uploaded a small image, typed in \"cafe,\" and 15 minutes later I am still waiting on this finishing.\n \nreply",
      "This looks promising. I love how you can reference uploaded images with markup - this is exactly what the field needs more of. After spending the last two weeks generating thousands of album cover images using DALL-E and being generally disappointed with the results (especially with the variations feature of DALL-E 2), I'm excited to give this a try.\n \nreply",
      "I am working on a API to generate avatars/profile pics based on a prompt. I tried looking for train my own model bt I think it's a titanic task and impossible to do it myself. Is my best solution use an external API and then crop the face for what was generated?\n \nreply",
      "The simplest commercial product for finetuning your own model is probably Adobe firefly, although there\u2019s no API access support yet. But there are cheap and only slightly more involved options like Replicate or Civit.ai. Replicate has solid API support.Check out:https://replicate.com/blog/fine-tune-flux\n \nreply",
      "With consistent representation of characters, are we now on the precipice of a Cambrian explosion of manga/graphic novels/comics?\n \nreply",
      "I sure hope so - at the very least I will use it for tabletop illustrations instead of having to describe a party's scenario result - I can give them a character-accurate image showing their success (or epic lack thereof).\n \nreply",
      "not yet, still can't generate transparent images\n \nreply",
      "Why do you need that? For manga specifically, generate in greyscale and convert luminance to alpha; then composite; then color.Or, if you need solid regions that overlap and mask out other regions, then generate objects over a chroma-keyable flat background.\n \nreply"
    ],
    "link": "https://aiomnigen.com",
    "first_paragraph": ""
  },
  {
    "title": "Pushing the frontiers of audio generation (deepmind.google)",
    "points": 165,
    "submitter": "meetpateltech",
    "submit_time": "2024-10-30T15:02:56 1730300576",
    "num_comments": 65,
    "comments_url": "https://news.ycombinator.com/item?id=41995730",
    "comments": [
      "While it is impressive and I like to follow the advancements in this field, it is incredibly frustrating to listen to. I can't put my finger on why exactly. It's definitely closer to human-sounding, but the uncanny valley is so deep here that I find myself thinking \"I just want the point, not the fake personality that is coming with it\". I can't make it through a 30s demo.\n \nreply",
      "We're used to hearing some kind of identity behind voices -- we unconsciously sense clusters of vocabulary, intonation patterns, ticks, frequent interruption vs quiet patience, silence tolerance, response patterns to various triggers, etc that communicate a coherent person of some kind.We may not know that a given speaker is a GenX Methodist from Wisconsin that grew up at skate parks in the suburbs, but we hear clusters of speech behavior that lets our brain go \"yeah, I'm used to things fitting together in this way sometimes\"These don't have that.Instead, they seem to mostly smudge together behaviors that are just generally common in aggregate across the training data. The speakers all voice interrupting acknowledgements eagerly, they all use bright and enunciated podcaster tone, they all draw on similar word choice, etc -- they distinguish gender and each have a stable overall vocal tone, but no identity.I don't doubt that this'll improve quickly though, by training specific \"AI celebrity\" voices narrowed to sound more coherent, natural, identifiable, and consistent. (And then, probably, leasing out those voices for $$$.)As a tech demo for \"render some vague sense of life behind this generated dialog\" this is pretty good, though.\n \nreply",
      "Whether this stops at the uncanny valley or progresses to specific \"AI celebrity\" voices, I'm left thinking the engineers involved in this never stopped to think carefully about whether this ought to be done in the first place.\n \nreply",
      "\"Surely my genAI product won't be used to spam zero-effort slop all over the internet!\"- guy whose genAI product will definitely be used to spam zero-effort slop all over the internet.\n \nreply",
      "I think their main target is corporate creative jobs. Background music to ads/videos/etc. And just like with all AI, they will eat the jobs that support the rest of the system, making it a one and done. It will give a one time boost, and then be stuck at that level because creatives won't have the jobs that allowed them to add to the domain. In this case new music styles. New techniques. It's literally eating the seed corn where the sprouts are the creatives working in the boring commercial jobs that allow them to practice/become experts in the tools/etc that they then build up it all. Their goal is cut the jobs that create their training data and the ecosystem that builds up/expands the domain. Everywhere AI touches will basically be 'stuck using Cobol' because AI will be frozen at the point in time where the energy infusing 'sprouts' all had their jobs replaced by AI and without them creating new output for AI to train on it's all ossified.We are witnessing in real time the answer to why 'The Matrix' was set when it was. Once AI takes over there is no future culture.\n \nreply",
      "> It's literally eating the seed corn where the sprouts are the creatives working in the boring commercial jobs that allow them to practice/become experts in the tools/etc that they then build up it all.This is a big problem that needs to be talked about more, the endgoal of AI seems to be quite grim for jobs and generally for humans. Where will this pure profit lead to? If all advertising will be generated who will want to have anything to do with all the products they\u2019re advertising?\n \nreply",
      "To be fair, the majority of podcasts are from a group of generic white guys, and they almost sound identical to these AI generated ones. The AI actually seems to to do a better job too.\n \nreply",
      "Citation absolutely needed. You call this fair?> the majority of podcasts are from a group of generic white guys\n \nreply",
      "https://podcastcharts.byspotify.com/ keep the Pareto distribution in mind\n \nreply",
      "I did the best fast research I could given not wanting to spend more than 20 minutes on it and came to this result (aprox):  - Mixed/Diverse: 48.0%\n - White Men: 35.0%\n - Women: 8.0%\n - Non-White: 6.0%\n - White Woman: 2.0%\n - Non-White Woman: 1.0%\n \nreply"
    ],
    "link": "https://deepmind.google/discover/blog/pushing-the-frontiers-of-audio-generation/",
    "first_paragraph": "Latest postsLatest research postsLatest technology postsLatest postsTechnologiesZal\u00e1n Borsos, Matt Sharifi and Marco TagliasacchiSpeech is central to human connection. It helps people around the world exchange information and ideas, express emotions and create mutual understanding. As our technology built for generating natural, dynamic voices continues to improve, we\u2019re unlocking richer, more engaging digital experiences.Over the past few years, we\u2019ve been pushing the frontiers of audio generation, developing models that can create high quality, natural speech from a range of inputs, like text, tempo controls and particular voices. This technology powers single-speaker audio in many Google products and experiments \u2014 including Gemini Live, Project Astra, Journey Voices and YouTube\u2019s auto dubbing \u2014 and is helping people around the world interact with more natural, conversational and intuitive digital assistants and AI tools.Working together with partners across Google, we recently helpe"
  },
  {
    "title": "Rubocop-obsession: RuboCop extension focused on higher-level concepts (github.com/jeromedalbert)",
    "points": 54,
    "submitter": "thunderbong",
    "submit_time": "2024-10-28T05:59:14 1730095154",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=41968167",
    "comments": [
      "One of the things i miss the most from developing in ruby is the code quality instrumentation: metric-fu, reek, rubocop and lots of other gems that allowed us (in a previous ruby shop) understand and monitor the quality of our code like ABC, Cyclomatic complexity, duplicated code, etc).Now in Typescript, I haven't found a lot of tools to calculate,visualize and track these stats.  Metric-fu was amazing.\n \nreply",
      "You can get a lot of mileage out of ESLint + typescript-ESLint + various other plugins. It's more of a \"choose your own adventure\" kind of experience, but the defaults are a good start, and if you're looking for cyclomatic complexity and all that jazz, there's various plugins that can calculate that. And the typescript ESLint plugin is great because it uses the types as part of the hints, which is great for finding places where you no longer need optional chaining, or where a condition is always true/false because you forgot what types things were.That said, I tend to take a much more lean approach to linting these days. I try to only include lints that are actionable, likely to lead to real bugs, and have no false positives. Everything else feels like it's better handled in code review. If you don't know why your function is too complex and difficult to read, I don't want you to rewrite it just to satisfy a robot that's following arbitrary rules! I want to be able to have a discussion with you about proper techniques for splitting apart code.But there's definitely people who go the other way entirely and build incredibly strict ESLint configurations, so looking for those might help you if that's what you want.\n \nreply",
      "I wonder if anyone here has a good set of Typescript/ESlint addons to suggest? I also miss my duplicate code alert from Ruby!\n \nreply",
      "Don't forget about Brakeman!\n \nreply",
      "Rubocop is one of the worst Ruby tools there is. Flog and reek are also unhelpful, but Rubocop is style nonsense that doesn\u2019t belong in a linter. The ESLint community used to be big on useless opinion based rule overload, until everyone realized how bad of an idea it was.\n \nreply",
      "Can you be kore specific? I haven't used it for a few years but found it yo be a perfectly competent and helpful linter at minimum.\n \nreply",
      "It\u2019s been quite some time since I\u2019ve used Rubocop but I did fight an uphill battle to get it into place. One of the things I had a difficult time explaining was that rules subtly interact with each other. When one rule makes a change, it might simplify something such that another rule can then also make a change. Once I enabled enough rules, there were some pretty impressive chains that were whittled down to almost nothing\u2026 but the same application of those rules elsewhere caused hemming and hawing by the naysayers.I think the best thing that these tools give, above all else, is a way to settle petty opinions in a team. It\u2019s why there is value in rules that either conflict with each other or are configurable so as to meet the expectations of a given team.\n \nreply",
      "Yep. I introduced Rubocop at a past gig\u2026 only to watch my colleague write worse code playing whackamole trying to adhere to the rules.I kept telling him that the whackamole is a sign to rework things at a higher level - that the Rubocop rule catches symptoms, and that he needed to address the cause, but\u2026PS - to be clear, I love Rubocop and swear by it. In that circumstance, I needed to introduce it differently, is all.\n \nreply",
      "> is a way to settle petty opinions in a team.Yeah this is huge. So often I get my PRs rejected because one dev has their own personal preference for formatting something. With Rubocop I can tell them to go submit a PR themselves to make that preference a rule and we can all discuss it and enforce it / autoformat it going forward. But for now, it won't be blocking my change going out.\n \nreply",
      "Hopefully rubocop-rails-omakase [1] will pull in stuff like this. I've gone from custom Rubocop rules in various projects to just using rubocop-rails-omakase, which finally feels like a consensus of sorts. I think Standard [2] is harmfully redundant, personally.[1] https://github.com/rails/rubocop-rails-omakase\n[2] https://github.com/standardrb/standard\n \nreply"
    ],
    "link": "https://github.com/jeromedalbert/rubocop-obsession",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        RuboCop extension focused on higher-level concepts, like checking that code reads from top to bottom\n      A RuboCop extension that focuses on\nhigher-level concepts, like checking that code reads from\ntop to bottom, or only unit\ntesting public methods.\nThere are some lower-level cops as well.Use the provided cops as is, or as inspiration to build custom cops aligned\nwith your project's best practices.Install the gem:Or add this line to your Gemfile:and run bundle install.You need to tell Rubocop to load the Obsession extension. There are three ways\nto do this:Put this into your .rubocop.yml.Alternatively, use the following array notation when specifying multiple extensions.Now you can run rubocop and it will automatically load the Rubocop Obsession\ncops together with the standard cops.All cops are located under\nlib/rubocop/cop/obses"
  },
  {
    "title": "An Italian 'fruit detective' who investigates centuries-old paintings (smithsonianmag.com)",
    "points": 42,
    "submitter": "Brajeshwar",
    "submit_time": "2024-10-29T14:39:52 1730212792",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=41984292",
    "comments": [
      "Err \u201cWe depend on advertising to fund our services \u2026 please disable your adblocker\u201dThe Smithsonian ?!I do wonder if the ad-driven digital world is simply going to consume itself.The Guardian newspaper apparently did an internal study pre-covid and found that 1/3 of its content was essentially unread, and that the effort to SEO / promote that 1/3 not only was wasted effort but actually harmed the results of the other 2/3rds.Google (or rather PageRank) does try to promote what is well read and referenced, but the sludge does drag down everything.\n \nreply",
      "Google tries to promote what generates the most money for Google.\n \nreply",
      "> the apples, including a variety known in the fruit science lexicon as api piccolaI wanted to learn more about this apple, but its name is basically ungooglable. All you get is info on APIs, wind instruments or both.\n \nreply",
      "It's this cultivar:https://en.wikipedia.org/wiki/Lady_appleBut with the italian word for 'small' stuck on the end, which seems kinda redundant, since all 'Api' apples are small.My parents have a couple of Api trees, they're small, very very tart, and quite a firm bite. Now, I like my apples tart and firm, but they're far too much for me; functionally almost like eating crab apples (it seems 'crab apple' is ambiguous, so I mean the European wild apple, aka Malus Sylvestris, here).\n \nreply",
      "https://kagi.com/search?q=fruit+api+piccolathis had many hits for me.\n \nreply",
      "Switzerland has an organisation/NGO dedicated to saving rare breeds of farm animals and cultivated varietars:https://www.prospecierara.ch/\n \nreply"
    ],
    "link": "https://www.smithsonianmag.com/arts-culture/meet-italian-fruit-detective-who-investigates-centuries-old-paintings-clues-produce-180985227/",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: G-win \u2013 .gcode parser written in Rust with winnow (crates.io)",
    "points": 10,
    "submitter": "mj10021",
    "submit_time": "2024-10-30T22:16:22 1730326582",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://crates.io/crates/g-win",
    "first_paragraph": ""
  },
  {
    "title": "Google's TOS doesn't eliminate a user's Fourth Amendment rights, judge rules [pdf] (uscourts.gov)",
    "points": 264,
    "submitter": "coloneltcb",
    "submit_time": "2024-10-30T18:58:48 1730314728",
    "num_comments": 196,
    "comments_url": "https://news.ycombinator.com/item?id=41998891",
    "comments": [
      "> Google\u2019s hash match may well have established probable cause for a warrant to allow police to conduct a visual examination of the Maher file.Very reasonable. Google can flag accounts as CP, but then a judge still needs to issue a warrant for the police to actually go and look at the file. Good job court. Extra points for reasoning about hash values.\n \nreply",
      "> a judge still needs to issue a warrant for the police to actually go and look at the fileOnly in the future. Maher's conviction, based on the warrantless search, still stands because the court found that the \"good faith exception\" applies--the court affirmed the District Court's finding that the police officers who conducted the warrantless search had a good faith belief that no warrant was required for the search.\n \nreply",
      "I wonder what happened to fruit of the poisoned tree? Seems a lot more liberty oriented than \"good faith exception\" when police don't think they need a warrant (because police never seem to \"think\" they need a warrant).\n \nreply",
      "This exactly. Bad people have to go free in order to incentivize good behavior by cops.You and I (as innocent people) are more likely to be affected by bad police behavior than the few bad people themselves and so we support the bad people going free.\n \nreply",
      "I think its okay that we expect cops to be good _after_ the rule exists, rather than set the bad guys free to (checks notes) incentivize cops to take our new rule super seriously.\n \nreply",
      "It would seem that the inverse would need to apply in order for the justice system to have any semblance of impartiality. That is that we now have to let both of them off the hook, since neither had been specifically informed they weren\u2019t allowed to do the thing beforehand.That is why many people think this should be tossed out. Ignorance that an action was a crime is almost never an acceptable defense, so it should not be an acceptable offense either.\n \nreply",
      ">  we now have to let both of them off the hook, since neither had been specifically informed they weren\u2019t allowed to do the thing beforehand.I'm not trying to be funny, or aggressive, or passive aggressive, seriously: there's two entities in the discussion, the cops, and the person with a photograph with a hash matching child porn. I'm phrasing that as passively as possible because I want to avoid the tarpit of looking like I'm appealing to emotion:Do you mean the hash-possessor weren't specifically informed it was illegal to possess said hash?> It would seem that the inverse would need to apply in order for the justice system to have any semblance of impartiality...That is why many people think this should be tossed out.Of course, I could be missing something here because I'm making a hash of parsing the first bit. But, no, if the cops in good faith make a mistake, there's centuries of jurisprudence behind not letting people go free for it, not novel with this case.\n \nreply",
      "That rule has been around for quite a while, and looks worse for wear now\n \nreply",
      "> That rule has been around for quite a whileThe rule established in this case is new, hence TFA, and all the time the lawyers and judge wasted on it :)If I may suggest where wires are getting crossed:You are sort of assuming it's like a logic gate: if 4th amendment violation, bad evidence, criminal must go free. So when you say \"the rule\", you mean \"the 4th amendment\", not the actual ruling.That's not how it works, because that simple ultimatum also has edge cases. So we built up this whole system around nominating juries and judges, and paying lawyers, over centuries, to argue out complicated things like weighing intentionality.\n \nreply",
      "IANAL, but as I understood, this exception is specifically about cases where precedence is established. This same trick or others substantially like it won't work in the future, but because it was not a \"known trick\", the conviction still stands.\n \nreply"
    ],
    "link": "https://ww3.ca2.uscourts.gov/decisions/isysquery/0814a460-fe8f-42ef-9e82-cf94f952eb28/1/doc/23-6181_opn.pdf",
    "first_paragraph": ""
  },
  {
    "title": "M4 MacBook Pro (apple.com)",
    "points": 671,
    "submitter": "tosh",
    "submit_time": "2024-10-30T15:00:20 1730300420",
    "num_comments": 908,
    "comments_url": "https://news.ycombinator.com/item?id=41995701",
    "comments": [
      "I really respect Apple's privacy focused engineering. They didn't roll out _any_ AI features until they were capable of running them locally, and before doing any cloud-based AI they designed and rolled out Private Cloud Compute.You can argue about whether it's actually bulletproof or not but the fact is, nobody else is even trying, and have lost sight of all privacy-focused features in their rush to ship anything and everything on my device to OpenAI or Gemini.I am thrilled to shell out thousands and thousands of dollars to purchase a machine that feels like it really belongs to me, from a company that respects my data and has aligned incentives.\n \nreply",
      "Mac OS calls home every time you execute an application. \nApple is well on its way to ensure you can only run things they allow via app store, they would probably already be there if it wasn't for the pesky EU.\nIf you send your computer/phone to Apple for repair you may get back different physical hardware.\nThose things very much highlight that \"your\" Apple hardware is not yours and that privacy on Apple hardware does not actually exist, sure they may not share that data with other parties but they definitely do not respect your privacy or act like you own the hardware you purchased.\nApple marketing seems to have reached the level indoctrination where everyone just keeps parroting what Apple says as an absolute truth.\n \nreply",
      ">  If you send your computer/phone to Apple for repair you may get back different physical hardware.I happen to be in the midst of a repair with Apple right now. And for me, the idea that they might replace my aging phone with a newer unit, is a big plus. As I think it would be for almost everyone. Aside from the occasional sticker, I don't have any custom hardware mods to my phone or laptop, and nor do 99.99% of people.Can Apple please every single tech nerd 100% of the time? No. Those people should stick to Linux, so that they can have a terrible usability experience ALL the time, but feel more \"in control,\" or something.\n \nreply",
      "Why not both?  Why can\u2019t we have a good usability experience AND control?  In fact, we used to have that via the Mac hardware and software of the 1990s and 2000s, as well as NeXT\u2019s software and hardware.There was a time when Apple\u2019s hardware was user-serviceable; I fondly remember my 2006 MacBook, with easily-upgradable RAM and storage.  I also remember a time when Mac OS X didn\u2019t have notarization and when the App Store didn\u2019t exist.  I would gladly use a patched version of Snow Leopard or even Tiger running on my Framework 13 if this were an option and if a modern web browser were available.\n \nreply",
      "It could help to compare to other makers for a minute: if you need to repair your Surface Pro, you can easily remove the SSD from the tray, send your machine and stick it back when it comes repaired (new or not)And most laptops at this point have removable/exchangeable storage. Except for Apple.\n \nreply",
      "> remove the SSD from the tray, send your machine and stick it back when it comes repairedApple has full-disk encryption backed by the secure enclave so its not by-passable.Sure their standard question-set asks you for your password when you submit it for repair.But you don't have to give it to them.  They will happily repair your machine without it because they can boot their hardware-test suite off an external device.\n \nreply",
      "I get your point, but we can also agree \"send us your data, we can't access it anyway, right ?\" is a completely different proposition from physically removing the data.In particular if a flaw was to be revealed on the secure enclave or encryption, it would be too late to act on it after the machines have been sent in for years.To be clear, I'm reacting on the \"Apple is privacy focused\" part. I wouldn't care if they snoop my bank statements on disk, but as a system I see them as behind what other players are doing in the market.\n \nreply",
      "> if a flaw was ...I hear the point you're making and I respect the angle, its fair-enough, but ...The trouble with venturing into what-if territory is the same applies to you...What if the disk you took out was subjected to an evil-maid attack ?What if the crypto implementation used on the disk you took out was poor ?What if someone had infiltrated your OS already and been quietly exfiltrating your data over the years ?The trouble with IT security is you have you trust someone and something because even with open-source, you're never going to sit and read the code (of the program AND its dependency tree), and even with open-hardware you still need to trust all those parts you bought that were made in China unless you're planning to open your own chip-fab and motherboard plant ?Its the same with Let's Encrypt certs, every man and his dog are happy to use them these days. But there's still a lot of underlying trust going on there, no ?So all things considered, if you did a risk-assessment, being able to trust Apple ?  Most people would say that's a reasonable assumption ?\n \nreply",
      "> What if the disk you took out was subjected to an evil-maid attack ?Well, have fun with my encrypted data. Then I get my laptop back, and it's either a) running the unmodified, signed and encrypted system I set before or b) obviously tampered with to a comical degree.> What if the crypto implementation used on the disk you took out was poor ?I feel like that is 100x more likely to be a concern when you can't control disc cryptography in any meaningful way. The same question applies to literally all encryption schemes ever made, and if feds blow a zero day to crack my laptop that's a victory through attrition in anyone's book.> What if someone had infiltrated your OS already and been quietly exfiltrating your data over the years ?What if aliens did it?Openness is a response to a desire for accountability, not perfect security (because that's foolish to assume from anyone, Apple or otherwise). People promote Linux and BSD-like models not because they cherry-pick every exploit like Microsoft and Apple does but because deliberate backdoors must accept that they are being submit to a hostile environment. Small patches will be scrutinized line-by-line - large patches will be delayed until they are tested and verified by maintainers. Maybe my trust is misplaced in the maintainers, but no serious exploit developer is foolish enough to assume they'll never be found. They are publishing themselves to the world, irrevocably.\n \nreply",
      "So why the hell do they ask for it then.\n \nreply"
    ],
    "link": "https://www.apple.com/newsroom/2024/10/new-macbook-pro-features-m4-family-of-chips-and-apple-intelligence/",
    "first_paragraph": "Text of this articleOctober 30, 2024PRESS RELEASEApple\u2019s new MacBook\u00a0Pro features the incredibly powerful M4 family of chips and ushers in a new era with Apple\u00a0Intelligence With an advanced 12MP Center Stage camera, Thunderbolt\u00a05 on M4\u00a0Pro and M4\u00a0Max models, and an all-new nano-texture display option, MacBook\u00a0Pro gets even more capable and even more proCUPERTINO, CALIFORNIA Apple today unveiled the new MacBook Pro, powered by the M4 family of chips \u2014 M4, M4 Pro, and M4 Max \u2014 delivering much faster performance and enhanced capabilities. The new MacBook Pro is built for Apple Intelligence, the personal intelligence system that transforms how users work, communicate, and express themselves, while protecting their privacy. Now available in space black and silver finishes, the 14-inch MacBook Pro includes the blazing-fast performance of M4 and three Thunderbolt 4 ports, starting with 16GB of memory, all at just $1,599. The 14- and 16-inch models with M4 Pro and M4 Max offer Thunderbolt 5 fo"
  },
  {
    "title": "Show HN: Ezcrypt \u2013 A file encryption tool (simple, strong, public domain) (codeberg.org)",
    "points": 26,
    "submitter": "ezcrypt",
    "submit_time": "2024-10-26T17:38:47 1729964327",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=41956331",
    "comments": [
      "How would you compare this to existing tools in the space like https://github.com/FiloSottile/age?Seems like age has multiple implementations (rust, go), has a permissive license, a public specification (https://github.com/C2SP/C2SP/blob/main/age.md) and the spec/core implementation is developed by someone with a history with cryptography (I don't know if you do, but there is no indication or name attached to the repo).While many things can be built as a hobby or learning project I feel like cryptography is one of those spaces where you should be clear if it is that and if it is not you need to expressively argue its bona fides up front.\n \nreply",
      "age is definitely more mature, and as you say, it should probably say somewhere that ezcrypt is still a hobby project (albeit a serious one).Some motives (some of which may sound strange, but mattered to me):  * Completely public domain.\n  * Without any third party dependencies (not even openssl or similar).\n  * Don't rely on a single cipher (low trust).\n  * Extremely portable (should easily build for and run on anything from an M4 Mac to a Raspberry Pi to an old OpenWRT MIPS router with 32 MB RAM for instance).\n  * Should be easy to use (e.g. no key management, unless you want to), and composable in standard Unix ways (pipes etc).\n  * Security focused, obviously. E.g. software architecture-wise, minimize dependencies, properly manage files, processes and secrets, etc.\n  * Personally: To learn and to build something that I trust.\n\nIn a way: When the dust settles after the nuclear apocalypse, if you manage to dig out a C compiler, this is your tool. ;-)\n \nreply",
      "\"Don't rely on a single cipher\" is an anti-goal, and a reason your hobby project won't be taken seriously. You'd be better off striking it from the list.\"Zero dependencies\" is also not especially reassuring, as it implies you're using your own cipher implementations (or reference implementations).\n \nreply",
      "This seem to use home made cryptography, which is never a good idea. In particular it's not clear why this design with layered algorithms is needed, nor why it uses a non standard KDF.Prefer using libsodium for the crypto, which has made sensible choices for you.\n \nreply",
      "Also you are missing a MAC, which is also a bad idea\n \nreply",
      "The code isn't especially easy to follow, but is this a cascade of Serpent, Twofish, ChaCha20, and AES, all in CBC except for ChaCha, without an authenticator? With its own stream construction, that doesn't stop truncation?\n \nreply",
      "That's pretty much spot on. Authentication is planned (and that should handle truncation too if I'm not mistaking): https://codeberg.org/ezcrypt/ezcrypt/issues/3\n \nreply",
      "Public key signatures and ciphertext authentication are not the same thing.\n \nreply",
      "No, I realize that (I was thinking that ssh-keygen could be used as a poor-man's authentication for some use cases - basically share a secret between sender & receiver that can be used to ensure that the message has not been tampered with). One of the reasons that I haven't implemented authentication yet is that I want to better understand the differences and nuances of different methods and use cases before deciding on a method. Recommendations are welcome.\n \nreply",
      "Get rid of the weird cipher cascade and replace it either with XAES-GCM or ChaPoly. Do some research into how to handle large ciphertexts with a chunking construction. Use a well-defined construction; don't invent your own.\n \nreply"
    ],
    "link": "https://codeberg.org/ezcrypt/ezcrypt",
    "first_paragraph": "ezcrypt is an easy to use tool for strong file encryption.ezcrypt is a (serious) hobby project with a few design goals:All code is free and unencumbered software released into the public domain, including the cryptographic algorithms.For more information, see unlicense.org.Prerequisites: A C compiler and CMake. For Linux targets, GTK 3 is also recommended to enable GUI dialogs (e.g. apt install libgtk-3-dev on Ubuntu).To build:The resulting executable file is out/ezcrypt.To install (from the out folder):To run a full build-and-test suite in a Docker environment (from the repo root):The canonical help for ezcrypt can be obtained with:Encrypt the file myfile, with the passphrase provided via a terminal prompt (or a GUI prompt where available). The output file is called myfile.z (the original file is kept):Decrypt the file myfile.z, with the passphrase provided via a terminal prompt. The output file is called myfile (the original file is kept):Decrypt the file myfile.z to stdout, with the"
  },
  {
    "title": "Launch HN: FlyCode (YC S22) \u2013 Stop losing revenue due to failed payments",
    "points": 69,
    "submitter": "JakeVacovec",
    "submit_time": "2024-10-30T13:45:14 1730295914",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=41994658",
    "comments": [
      "So the standard solution is:1- Notify client of failed payment\n2- Activate a grace period\n3- Notify of grace period.\n4- Cut service after grace period.\n5- Reinstate account after user corrects payment data.\n6- Make sure there are no system bugs in the reinstatement.In my personal preference, I like the system very much. It is the responsibility of the client to keep payment methods up to date, I don't think chasing a client to fix their credit issues is appropriate. But definitely not treating them like criminals and allowing them to fix the issue is due.Going after this lost revenue is going after bad clients in a sense. Barring expired cards, usually what happens is that the client has low liquidity, high debt, or is spread too thin across multiple accounts, and is unlikely to have a high LTV.Can say as a user that paid 12$ subscriptions on a month to month basis and was regularly late to the point of waiting until service is cut to move money around to pay it. Don't chase me, let me fix it, if at all, it's ok.\n \nreply",
      "To give another point of view on this: many times there are essential services I use and I just forget to update my credit card after switching it or for some reason the charge didn\u2019t pass. I would much rather get proper emails (or even better to have the charge recovered) than losing the service\n \nreply",
      "It definitely depends on the type of customer and type of product. There is a % of recoveries that will churn in the following 1-2 billing cycles and there's a larger % that will stay longer. Ultimately increasing recovery rate across the board means more revenue, which is good for a business.Error type is also not indicative of customer quality. For example, businesses and consumers set limits on their cards all the time so an insufficient funds error doesn't mean they have no money.If the customer doesn't want the service they have the option to cancel. This is why the LTV of customers recovered after a payment failure (involuntary churn prevention) is significantly higher than those saved from cancellation prevention flows (active churn prevention).\n \nreply",
      "> Ultimately increasing recovery rate across the board means more revenue, which is good for a business.Not if it isn't ethical. You're painting those with the failed payments who didn't cancel of being the ones in the wrong to justify the action taken against them, but heavy handed payment collection of a SaaS they likely weren't using doesn't sound great to me either. How about just doing the honorable thing that also isn't chasing bad clients?> If the customer doesn't want the service they have the option to cancel.Would you take Adobe as a customer, with their infamous cancellation dark patterns?\n \nreply",
      "Our Merchants can configure the recovery period and messaging to their liking and we ensure there's strict adherence to the compliance and regulatory rules set by card networks.Ethics and honor are great things but keep in mind we're not a collections agency that's buying bad debt off companies for pennies on the dollar to chase, which sounds like the experience you're referring to.\n \nreply",
      "You can choose not to be ethical, but you should advertise it loudly.Or at the very least, stop protesting when people call it out.\n \nreply",
      "It depends on their customers (\"merchants\"). I'm sure some of their customers will be fine. They could attract others like a foreign language teaching service that lures people in with a single price but somehow they're accidentally enrolled in something that has a 6 month commitment. It says subscription-based but it doesn't say anywhere that they exclude ones that have a commitment to multiple billing cycles, so \"they have the option to cancel\" probably won't even technically be the case with some services.\n \nreply",
      "Having run a small (100K users) SaaS company and seen some of this first hand, this is interesting. We never automated any of this, rather just communicating with customers asking them to try another card, or in extreme cases getting them to send a bare PayPal transfer then manually provisioning the payment against their account. I also worked for a startup in the 2000's that had the exact same conceptual idea, but applied to a somewhat different specific use-case.However, that's not what really makes me curious about this, which is: how do you make this a business? If someone explained this idea to me cold I'd immediately say \"that's not a business, it's a feature of Stripe/PayPal et al\".\nFrom a technical perspective the integration with the customer's (customer of FlyCode) systems seems challenging.\n \nreply",
      "We make the integration process nearly effortless for our customers -- we've built apps for Stripe and Shopify and have plans to build out more. Pricing is a flat SaaS fee based on recovered revenue. If we help businesses recover 20%+ more on average the business model is a simple ROI equation.There's many opportunities to expand our value throughout a payments journey. Merchants rely heavily on rule based business logic for payments and continue to add more rules over time. The expansion opportunity we see is to provide dynamic logic/decision day 1 without all the internal development/iteration.\n \nreply",
      "It can start out as a business and then get acquired by a bigger business like Stripe, paypel, business, mercury. Either by purchasing the company whole, or by contracting to implement the feature on their systems.\n \nreply"
    ],
    "link": "item?id=41994658",
    "first_paragraph": ""
  }
]