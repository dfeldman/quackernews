[
  {
    "title": "OpenAI O3 breakthrough high score on ARC-AGI-PUB (arcprize.org)",
    "points": 879,
    "submitter": "maurycy",
    "submit_time": "2024-12-20T18:11:13 1734718273",
    "num_comments": 726,
    "comments_url": "https://news.ycombinator.com/item?id=42473321",
    "comments": [
      "With only a 100x increase in cost, we improved performance by 0.1x and continued plotting this concave-down diminishing-returns type graph!  Hurray for logarithmic x-axes!Joking aside, better than ever before at any cost is an achievement, it just doesn't exactly scream \"breakthrough\" to me.\n \nreply",
      "imo it's a mistake to interpret the marginal increases in the upper echelons of benchmarks as materially marginal gains. Chess is an example. ELO narrows heavily at the top, but each ELO point carries more relative weight. This is a bit apples and oranges since chess is adversarial, but I think the point stands.\n \nreply",
      "Never change HN \"OH WOW AGI level AI was created, but it's current iteration costs $1000/task, guess it's useless\"\n \nreply",
      "I know AGI is a bit of a moving goalpost these days, but by my personally anecdotal and irrelevant opinion this ain\u2019t AGI.I\u2019ll let those smarter than me debate the merits of AGI, but if it can\u2019t learn and self-improve it isn\u2019t \u201cgeneral\u201d intelligence.This is a very smart computer, accomplishing a very niche set of problems. Cool? Yes. AGI? No.\n \nreply",
      "Agi level my ass\n \nreply",
      "+1\n \nreply",
      "compute gets cheaper and cheaper every year. This model will be in your phone by 2030 if we continue at the pace we've been at the last few years.\n \nreply",
      "It may eventually be able to solve any problem\n \nreply",
      "Ah. Me, too.\n \nreply",
      "Efficiency is now key.~=$3400 per single task to meet human performance on this benchmark is a lot. Also it shows the bullets as \"ARC-AGI-TUNED\", which makes me think they did some undisclosed amount of fine-tuning (eg. via the API they showed off last week), so even more compute went into this task.We can compare this roughly to a human doing ARC-AGI puzzles, where a human will take (high variance in my subjective experience) between 5 second and 5 minutes to solve the task.\n(So i'd argue a human is at 0.03USD - 1.67USD per puzzle at 20USD/hr, and they include in their document an average mechancal turker at $2 USD task in their document)Going the other direction: I am interpreting this result as human level reasoning now costs (approximately) 41k/hr to 2.5M/hr with current compute.Super exciting that OpenAI pushed the compute out this far so we could see he O-series scaling continue and intersect humans on ARC, now we get to work towards making this economical!\n \nreply"
    ],
    "link": "https://arcprize.org/blog/oai-o3-pub-breakthrough",
    "first_paragraph": "OpenAI's new o3 system - trained on the ARC-AGI-1 Public Training set - has scored a breakthrough 75.7% on the Semi-Private Evaluation set at our stated public leaderboard $10k compute limit. A high-compute (172x) o3 configuration scored 87.5%.This is a surprising and important step-function increase in AI capabilities, showing novel task adaptation ability never seen before in the GPT-family models. For context, ARC-AGI-1 took 4 years to go from 0% with GPT-3 in 2020 to 5% in 2024 with GPT-4o. All intuition about AI capabilities will need to get updated for o3.The mission of ARC Prize goes beyond our first benchmark: to be a North Star towards AGI. And we're excited to be working with the OpenAI team and others next year to continue to design next-gen, enduring AGI benchmarks.ARC-AGI-2 (same format - verified easy for humans, harder for AI) will launch alongside ARC Prize 2025. We're committed to running the Grand Prize competition until a high-efficiency, open-source solution scoring"
  },
  {
    "title": "Compiling C to Safe Rust, Formalized (arxiv.org)",
    "points": 48,
    "submitter": "love2read",
    "submit_time": "2024-12-20T23:30:03 1734737403",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=42476192",
    "comments": [
      "Compiling a tiny subset of C, that is. It might be so tiny as to be useless in practice.I have low hopes for this kind of approach; it\u2019s sure to hit the limits of what\u2019s possible with static analysis of C code. Also, choosing Rust as the target makes the problem unnecessarily hard because Rust\u2019s ownership model is so foreign to how real C programs work.\n \nreply",
      "Note that this is done for \u201cexisting formally verified C codebases\u201d which is a lot different from typical systems C code which is not formally verified.\n \nreply",
      "I wonder how well O3 can do just compiling C to rust in one shot\n \nreply",
      "Probably pretty bad.\n \nreply",
      "Total agreement!\n \nreply",
      "Funny, I came here to say just the opposite, that I'm glad algorithmic computing is still a thing in research and that not everything is AI.Ironically, AI is able to produce research-grade algorithms and will probably become an authority on the subject, helping take more traditional CS to the next level.\n \nreply",
      "c2rust.com, but it uses things like libc::c_int\n \nreply",
      "C2Rust is mentioned in the second paragraph of the related work section.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2412.15042",
    "first_paragraph": "The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy.Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "A Raycaster in Bash (github.com/izabera)",
    "points": 65,
    "submitter": "izabera",
    "submit_time": "2024-12-20T22:25:45 1734733545",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=42475703",
    "comments": [
      "I wonder if texture mapping this would look good.\n \nreply",
      "It's unfortunate that stty requires forking. Maybe the next project will be to use bash and rowhammer to call the necessary ioctls to do it without forking.\n \nreply",
      "Beautiful. Correct me if I'm wrong, but this looks like a bash version of https://lodev.org/cgtutor/raycasting.html\n \nreply",
      "yeah i really liked that tutorial\n \nreply",
      "Only 300 lines of code, impressive! I love this.\n \nreply",
      "I'd love to see this combined with the author's fork()-less implementation of ps to make a (almost) fork()-free implementation of psDoom.Seriously though, this is really cool\n \nreply",
      "And to think, that my own bash scripts spend 300 lines just parsing various command-line options, while instead I could be showing this game... :-P\n \nreply"
    ],
    "link": "https://github.com/izabera/pseudo3d",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          more screenshots/vidoes at https://imgur.com/a/izas-wolfenstein-bash-journey-bAy5zhplargely a port of https://lodev.org/cgtutor/raycasting.htmluse the arrow keys to rotate and move around, and q to quitbash is slow.  this is by far the biggest issue.  it's so slow that you\ncannot possibly achieve an acceptable frame rate if you have to execute even\na single command per pixel.  this implies that you also cannot keep the state\nof the screen in memory, neither as an array of colours (did you know that\naccessing a random element in an array takes linear time?) nor as a single\nlong string (did you know that accessing the nth character in a string takes\nlinear time even in LANG=C?), because literally just reading this\nrepresentation to dump it to the screen will take longer than a framebash has no floating point support nor access to a library of "
  },
  {
    "title": "Grayjay Desktop App (grayjay.app)",
    "points": 286,
    "submitter": "pierrelf",
    "submit_time": "2024-12-20T17:33:00 1734715980",
    "num_comments": 160,
    "comments_url": "https://news.ycombinator.com/item?id=42473032",
    "comments": [
      "Launching the Linux release and noticed in the logs:Directories:User Directory: /home/bisby/GrayjayAnd there is a directory there now. I absolutely hate having stuff automatically create anything in my home directory like this. Ideally, this should be following XDG directory guidelines on linux: https://specifications.freedesktop.org/basedir-spec/latest/\n \nreply",
      "Grayjay dev here. If you want it to use your user directory like other apps, just remove the file called \"Portable\". Keep in mind that it just uses your working directory to write files otherwise.\n \nreply",
      "Sorry, to be clear, I dont wan't Grayjay data in my user directory AT ALL. Portable is basically what I want, I'm just very untactfully dropping feedback about where the data is placed.Even with the \"Portable\" file, it creates a directory `/home/bisby/Grayjay`. I don't want that. No app should ever put a file or directory directly in `/home/bisby` without me asking it to. The Linux standard for \"where should an app put it's files\" is defined the XDG spec that I had previously linked (https://specifications.freedesktop.org/basedir-spec/latest/).The summary is that user specific data should live in $XDG_DATA_HOME and config should live in $XDG_CONFIG_HOME (and various other things like $XDG_CACHE_HOME). If these values are unset, there are predefined places to put the files (eg, data in $HOME/.local/share or config in $HOME/.config, cache in $HOME/.cache).This puts all the Grayjay data in places like /home/bisby/.config/Grayjay (instead of /home/bisby/Grayjay) which is nested away inside a hidden directory and structured in a consistent way.This would be the equivalent of putting data in %AppData% in windows instead of cluttering someone's \"My Documents\" (or whatever the modern equivalent of that is).Some of the Linux decisions feel a bit like linux is a complete afterthought, but included because Linux users tend to agree with the FUTO philosophies. That is a reasonable thing given the Linux market share, and for \"Build Version: 2\" that I'm seeing the app info, I'm grateful that linux is included this early. This looks like it can probably replace freetube for me. However, it would go a long way if things are done to make sure they are done the \"right way\" on Linux (ie, on packaging and on directory specs).Thanks for the work you've done on freeing up the web.\n \nreply",
      "Please just adhere to the XDG-standards. Although my co-poster here didn't use the most diplomatic way of phrasing their grievance Grayjay is better off if it sticks to well established standards.You would probably look weird at an software that installs itself in C:\\MYAWESOMEAPPLICATION instead of using the Windows program folder like literally every other piece of software (except for legacy stuff like LTSpice). Creating visible directories in the home folder without asking is the Linux equivalent of doing just that.Check if the XDG environment variables are set and store your stuff in these places \u2014 as it is now can be used as a last resort fallback. For reading config/data you do the same.\n \nreply",
      "That's a windows-ism, we don't like that kind of stuff. Not on any other OS we don't.\n \nreply",
      "Parent is not wrong, but definitely could have some improved manners and tact.As a linux user I wanted to make sure to say thank you for supporting and thinking about linux!\n \nreply",
      "Actually, parent is wrong. You're not supposed to do that shit on Windows either. That's what AppData is for. Writing configuration files and folders to \"Documents\" or the user's home folder is sloppy shit.\n \nreply",
      "I agree that this should be in the XDG directory or AppData, but be kind, y'all -- this is open source, it is a gift someone has labored over and given you. There are much nicer ways to suggest improvements than calling it \"sloppy shit\".edit: it's not actually open source by the OSI definition it seems [1], but it is reasonably close.[1] https://futo.org/about/futo-statement-on-opensource/\n \nreply",
      "Sure, things can always have gone better, but this is data loss/corruption territory. It's asking for trouble and hurt feelings. I think a strong response is ok.\n \nreply",
      "What exactly is wrong with how they expressed themselves?Is the word \"hate\" really so odious?\n \nreply"
    ],
    "link": "https://grayjay.app/desktop/",
    "first_paragraph": ""
  },
  {
    "title": "Building Effective \"Agents\" (anthropic.com)",
    "points": 149,
    "submitter": "jascha_eng",
    "submit_time": "2024-12-20T12:29:17 1734697757",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=42470541",
    "comments": [
      "This is by far the most practical piece of writing I've seen on the subject of \"agents\" - it includes actionable definitions, then splits most of the value out into \"workflows\" and describes those in depth with example applications.There's also a cookbook with useful code examples: https://github.com/anthropics/anthropic-cookbook/tree/main/p...Blogged about this here: https://simonwillison.net/2024/Dec/20/building-effective-age...\n \nreply",
      "Thanks for all the write-ups on LLMs, you're on top of the news and it makes it way easier to follow what's happening and the existing implementations by following your blog instead.\n \nreply",
      "I'm glad they are publishing their cookbooks recipes on github too. Openai used to be more active there.\n \nreply",
      "\" Openai used to be more active there\"When it was Open?\n \nreply",
      "Eh, let's nip this in the bud: we could end up in a \"it feels like...\", coupled to free association, cycle. :)More substantively, we can check our vibe. OpenAI is just as active as it ever was w/notebooks. To an almost absurd degree. 5-10 commits a week. https://github.com/openai/openai-cookbook/activity\n \nreply",
      "My personal view is that the roadmap to AGI requires an LLM acting as a prefrontal cortex: something designed to think about thinking.It would decide what circumstances call for double-checking facts for accuracy, which would hopefully catch hallucinations.  It would write its own acceptance criteria for its answers, etc.It's not clear to me how to train each of the sub-models required, or how big (or small!) they need to be, or what architecture works best. But I think that complex architectures are going to win out over the \"just scale up with more data and more compute\" approach.\n \nreply",
      "IMHO with a simple loop LLMs are already capable of some meta thinking, even without any internal new architectures. For me where it still fails is that LLMs cannot catch their own mistakes even some obvious ones. Like with GPT 3.5 I had a persistent problem with the following question: \"Who is older, Annie Morton or Terry Richardson?\". I was giving it Wikipedia and it was correctly finding out the birth dates of the most popular people with the names - but then instead of comparing ages it was comparing birth years. And once it did that it was impossible to it to spot the error.Now with 4o-mini I have a similar even if not so obvious problem.Just writing this down convinced me that there are some ideas to try here - taking a 'report' of the thought process out of context and judging it there, or changing the temperature or even maybe doing cross-checking with a different model?\n \nreply",
      "After I read attention is all you need, my first thought was: \"Orchestration is all you need\". When 4o came out I published this: https://b.h4x.zip/agi/\n \nreply",
      "I put the agents in quotes because anthropic actually talks more about what they call \"workflows\". And imo this is where the real value of LLMs currently lies, workflow automation.They also say that using LangChain and other frameworks is mostly unnecessary and does more harm than good. They instead argue to use some simple patterns, directly on the API level. Not dis-similar to the old-school Gang of Four software engineering patterns.Really like this post as a guidance for how to actually build useful tools with LLMs. Keep it simple, stupid.\n \nreply",
      "I felt deeply vindicated by their assessment of these frameworks, in particular LangChain.I've built and/or worked on a few different LLM-based workflows, and LangChain definitely makes things worse in my opinion.What it boils down to is that we are still coming to understand the right patterns of development for how to develop agents and agentic workflows. LangChain made choices about how to abstract things that are not general or universal enough to be useful.\n \nreply"
    ],
    "link": "https://www.anthropic.com/research/building-effective-agents",
    "first_paragraph": ""
  },
  {
    "title": "A bestiary of exotic hadrons (cerncourier.com)",
    "points": 112,
    "submitter": "rbanffy",
    "submit_time": "2024-12-20T15:29:59 1734708599",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=42471927",
    "comments": [
      ">The dynamics of quarks and gluons can be described perturbatively in hard processes thanks to the smallness of the strong coupling constant at short distances, but the spectrum of stable hadrons is affected by non-perturbative effects and cannot be computed from the fundamental theory. Though lattice QCD attempts this by discretising space\u2013time in a cubic lattice, the results are time consuming and limited in precision by computational power. Predictions rely on approximate analytical methods such as effective field theories.I'm glad this was mentioned, non-perturbative effects are not well understood and this is a big part of why it's worthwhile to study bound states of the strong force.\n \nreply",
      "Give LQCD practitioners resources on the scale of the experiment, the computations will get faster!I'm not sure what they mean by \"Predictions rely on approximate analytical methods such as effective field theories.\"  The predictions of LQCD are ab initio.  Sometimes we fit EFTs to LQCD results, that's true.  But EFTs are under control and have quantifiable uncertainties, they're not just willy-nilly approximations.\n \nreply",
      "May be referring not to LQCD relying on approximate analytical methods but some of the other non-perturbative methods? Example would be trying to apply homotopy analysis method (HAM) or a related transform to whatever field equations to make some semi-analytical predictions.\n \nreply",
      "I assume that if we ever unify QCD with General Relativity, the resulting theory would be able to predict these hadrons from first principles?\n \nreply",
      "No. The reason perturbation theory doesn\u2019t work as well for QCD as it does for QED is because of two reasons:1. The coupling constant of QCD is much higher than QED so contributions to the overall result from Feynman diagrams that have more vertices (the multiplicative factor of each element in the sum is proportional to the power of the number of vertices) don\u2019t vanish as quickly as they do for QED2. The gauge bosons in QCD (i.e. gluons) themselves have colour charge whereas those in QED (i.e. photons) do not have electrical charge.\n \nreply",
      "You can't give a definite no to that because, since gravitons have stress-energy and are non-perturbative, a field theory advance that worked for them could also help with the strong force.\n \nreply",
      "I mean sure, since we don't know what GR + QFT could look like, the result could be just about anything and somehow give us nice closed solutions to QCD problems. But I don't feel like that line of reasoning is particularly useful.\n \nreply",
      "AdS/CFT is already an example of an approach to gravity yielding an approach to strongly coupled field theories.\n \nreply",
      "> the resulting theory would be able to predict these hadrons from first principles?Not sure how bringing GR into the fray would help solve what essentially seems to be a computational complexity problem. Might actually make things worse.\n \nreply",
      "It's not a computational complexity problem, it's an undefinedness problem. Proving that the lattice simulations converge has been estimated as well beyond this century's mathematics by the pair of people (Glimm and Jaffe) that have done the most to study it. In any case it is beyond today's.\n \nreply"
    ],
    "link": "https://cerncourier.com/a-bestiary-of-exotic-hadrons/",
    "first_paragraph": "Please enter the e-mail address you used to register to reset your passwordThank you for registering\nIf you'd like to change your details at any time, please visit My accountPatrick Koppenburg and Marco Pappagallo survey the 23 exotic hadrons discovered at the LHC so far.Seventy-six new particles have been discovered at the Large Hadron Collider (LHC) so far: the Higgs boson, 52 conventional hadrons and a bestiary of 23 exotic hadrons whose structure cannot reliably be explained or their existence predicted.The exotic states are varied and complex, displaying little discernible pattern at first glance. They represent a fascinating detective story: an experimentally driven quest to understand the exotic offspring of the strong interaction, motivating rival schools of thought among theorists.This surge in new hadrons has been one of the least expected outcomes of the LHC (see \u201cUnexpected\u201d figure). With a tenfold increase in data at the High-Luminosity LHC (HL-LHC) on the horizon, and fur"
  },
  {
    "title": "Show HN: Artemis, a Calm Web Reader (jamesg.blog)",
    "points": 195,
    "submitter": "zerojames",
    "submit_time": "2024-12-20T15:28:50 1734708530",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=42471913",
    "comments": [
      "I have been thinking of creating a similar app; however I wanted to do a \"Sunday paper\". This look nice and I like the minimalist design, but I would prefer to have solution that I can self-host.\n \nreply",
      "The tool actually is open source, so you could self-host it if you wish. MIT license, in Python. The repository [1] is linked in the blog post the author wrote about the project [2].[1]: https://github.com/capjamesg/web-reader[2]: https://jamesg.blog/2024/11/30/designing-a-calm-web-reader/\n \nreply",
      "The open source version is a bit different from the hosted one: the open source code involves running the polling script, then building a static site (which is how I run the site for several months as a single-user project).I am planning to move the polling changes upstream soon and then figure out a plan for open sourcing the full project.\n \nreply",
      "Isn't updating once a day a bit too rare for content heavy websites, like HN? I use online RSS reader just to keep up with all updates when I am not online. With updates once a day I'd probably be using desktop RSS reader app.\n \nreply",
      "This would be interesting as a project using the Miniflux API (https://github.com/miniflux/v2). That way it would already use my existing feeds and I don't have a separate \"reading tool\".\n \nreply",
      "Technical question: how are you dealing with feeds blocked by cloudflare protection or captchas?\n \nreply",
      "Good question. I haven't found this to be an issue yet, perhaps because of the infrequency with which resources are accessed.With that said, I can't guarantee this is not an issue. There are a few feeds that return errors that I need to investigate.\n \nreply",
      "I love the idea of a simple, digest-style, mode. Might make some version of this for Instagram. There's nothing so important on Instagram that it can't wait for the next day.\n \nreply",
      "Can you recommend a tool that could check an Instagram feed once a day for the current photos?  I found a command line tool, but it tries to download the whole account, rather than just the daily updates.https://instaloader.github.io/\n \nreply",
      "That's exactly the tool I would use. There are options you can use to configure what posts are downloaded:https://instaloader.github.io/cli-options.html#which-posts-t...https://instaloader.github.io/basic-usage.html#filter-posts\n \nreply"
    ],
    "link": "https://artemis.jamesg.blog/",
    "first_paragraph": "Artemis is a calm web reader.You can use Artemis to follow your favorite websites.Artemis updates once per day, at approximately 12am in your timezone.See a list of features.Artemis is designed to be slow and minimal. It's a calm place to see what's new on your favorite websites, with no urgency.Read more about the project design philosophy.Read how we store your data.I have tried my best to make this service accessible. If you notice any issues, please feel free to contact me.Read our accessibility statement.Artemis is free to use, although donations are appreciated!Artemis is made by capjamesg.Need tech support? Contact jamesg@jamesg.blog"
  },
  {
    "title": "Qualcomm wins licensing fight with Arm over chip designs (bloomberg.com)",
    "points": 116,
    "submitter": "my123",
    "submit_time": "2024-12-20T21:28:53 1734730133",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=42475228",
    "comments": [
      "By the end of Day 3, it seemed quite clear that Qualcomm's legal team and position was far ahead of ARM's. I feel the following snippet sums up the whole week:\"Qualcomm\u2019s counsel turned Arm\u2019s Piano analogy on its head. Arm compared its ISA to a Piano Keyboard design during the opening statement and used it throughout the trial. It claimed that no matter how big or small the Piano is, the keyboard design remains the same and is covered by its license. Qualcomm\u2019s counsel extended that analogy to show how ridiculous it would be to say that because you designed the keyboard, you own all the pianos in the world. Suggesting that is what Arm is trying to do.\"Source: https://www.tantraanalyst.com/ta/qualcomm-vs-arm-trial-day-3...\n \nreply",
      "Wholeheartedly agree. I understand where ARM is coming from, but my god the legal team from both parties were night and day apart. And from evidences ARM isn't even asking for a lot more money. They are likely fighting this from principle, but their explanation were weak, very weak. ( They were even worst then Apple during the Apple vs Qualcomm case )I thought the whole thing Qualcomm was way more professional. ARM's case was that what they think was written in the contract, what they \"should\" have written in contract and what Qualcomm shows clearly contradict.It is more of a lesson for ARM to learn. And now the damage has been done. This also makes me think who was pushing this lawsuit. Softbank ?I also gained more respect to Qualcomm. After what they showed Apple vs Qualcomm's case and here.Side Note: ARM's Design has caught on. The Cortex X5 is close to Apple's Design. We should have news about X6 soon.\n \nreply",
      "Wow, this has been settled already? I mean, I am sure ARM will appeal.ARM did massive damage to their ecosystem for nothing. There will for sure be consequences of suing your largest customer.Lots of people that would have defaulted to licensing designed off ARM for whatever chips they have planned will now be considering RISC-V instead. ARM just accelerated the timeline for their biggest future competitor. Genius.\n \nreply",
      "RISC-V is not anywhere near competitive to ARM at the level that Qualcomm operates.I\u2019ve written about that here: https://benhouston3d.com/blog/risc-v-in-2024-is-slow\n \nreply",
      "It would be more accurate to say that there haven't been any RISC-V designs for Qualcomm's market segment yet.As far as I am aware, there is nothing about the RISC-V architecture which inherently prevents it from ever being competitive with ARM. The people designing their own cores just haven't bothered to do so yet.RISC-V isn't competitive in 2024, but that doesn't mean that it still won't be competitive in 2030 or 2035. If you were starting a project today at a company like Amazon or Google to develop a fully custom core, would you really stick with ARM - knowing what they tried to do with Qualcomm?\n \nreply",
      "But then there is the software ecosystem issue.Having a competitive CPU is 1% of the job. Then you need\nTo have a competitive SoC (oh and not infringe IP), so that you can build the software ecosystem, which is the hard bit.\n \nreply",
      "We've seen compatibility layers between x86 and arm. Am I correct in thinking that a compatibility layer between riscV and arm would be easier/more performant since they're both risc architectures?\n \nreply",
      "> As far as I am aware, there is nothing about the RISC-V architecture which inherently prevents it from ever being competitive with ARMLack of reg+shifted reg addressing mode and or things like BFI/UBFX/TBZThe perpetual promise of magic fusion inside the cores has not played out. No core exists to my knowledge that fuses more than two instructions at a time. Most of those take more than two to make. Thus no core exists that could fuse them.\n \nreply",
      "Your otherwise on point piece contains the common misconception that ARM began in embedded systems. When they started they had a full computer system that had very competitive CPU performance for the time:\nhttps://en.m.wikipedia.org/wiki/Acorn_ArchimedesThey pivoted to embedded shortly after spinning off into a separate company.\n \nreply",
      "Not to be pedantic, but\u2026Acorn Computers started off much earlier (I owned an Acorn Atom when it was released) which begat the Electron, then the BBC Micro and then the Archimedes.At that time ARM was just an architecture owned by Acorn. They created it with VSLI technology (Acorn\u2019s Silicon partner) and used the first RISC chip in the BBC Micro before then pivoting it to the Archimedes.Whilst Acorn itself was initially purchased by Olivetti, who eventually sold what remained years later to Morgan Stanley.The ARM division was spun off as \u201cAdvanced RISC Machines\u201d in a deal with both Apple, and VSLI Technology after Olivetti came onto the scene.It is this company that we now know as Arm Holdings.So it\u2019s not entirely accurate to claim \u201cthey had a full computer system\u201d as that was Acorn Computers, PLC.\n \nreply"
    ],
    "link": "https://www.bloomberg.com/news/articles/2024-12-20/qualcomm-wins-licensing-fight-with-arm-over-chip-designs",
    "first_paragraph": "To continue, please click the box below to let us know you're not a robot.Please make sure your browser supports JavaScript and cookies and that you are not\n            blocking them from loading.\n            For more information you can review our Terms of\n                Service and Cookie Policy.For inquiries related to this message please contact\n            our support team and provide the reference ID below."
  },
  {
    "title": "DOS APPEND (os2museum.com)",
    "points": 60,
    "submitter": "SeenNotHeard",
    "submit_time": "2024-12-20T21:04:59 1734728699",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=42475011",
    "comments": [
      "> APPEND is one of the things that are completely irrelevant 99.99% of the time\u2026 yet can be extremely useful when the need arises.Is it really that irrelevant? I mean, if you look past the specifics (directories, interrupts, DOS versions), this seems to be implementing the idea of bringing something into scope, in particular bringing it into scope from the outside, to modify the behavior of the consumer (here, assembler) without modifying the consumer itself. Today, we'd do the equivalent with `ln -sr ../../inc ../inc`.I'd argue the general idea remains very important and useful today, though it's definitely not obvious looking back from the future what this was what APPEND was going for.\n \nreply",
      "Yes, general idea is still very important and useful, but this is post about specific command called \"APPEND\" in MS-DOS environment. Also, it's not an equivalent of \"ln -sr\" as \"ln\" replaces targets and not stacks them. The proper modern equivalents are environment variables like LD_LIBRARY_PATH, PYTHONPATH, PKG_CONFIG_PATH, etc... and overlayfs mounts for a generic case.But back to the APPEND: in all my time working with MS-DOS, I don't remember ever needing that, so it was 100% irrelevant to me. But this could be because I've worked with more \"modern\" programs (like Turb Pascal 5) which had good support for directories.\n \nreply",
      "Another handy dos command, originating back to DOS is SUBST.Came in pretty handy when I wanted to share a folder with Remote Desktop, but it would only let me select whole drives.Made a SUBST drive letter for that folder, worked like a charm!\n \nreply",
      "SUBST makes use of NT Object Namespace Symbolic Links to register the drive letter.  After running SUBST, you get an object named \"M:\" (or whatever your drive letter is\") sitting in the \"\\??\\\" directory, its full path will be \"\\??\\M:\".  It will be a symbolic link that points to something like \"\\??\\C:\\target_path\".You can either see this by using \"NtQuerySymbolicLinkObject\" on \"\\??\\M:\", or calling \"QueryDosDeviceW\" on \"M:\".  On Windows NT, you will see the result as an NT-native path \"\\??\\C:\\target_path\" rather than a Win32-style path \"C:\\target_path\".\"\\??\\\" is not some kind of special notation for paths or anything, it is a real NT object that exists.  It holds your drive letters and other things.On Windows 9x, you won't see an NT-native path from QueryDosDevice, you'll instead see a Win32-style path \"C:\\target_path\".Weirdly enough, Sysinternals Winobj is unable to find the symbolic link object at all, despite that it exists when you query it using NT API calls.Fun fact about NT native paths, you can use them in Win32 if you prefix them with \"\\\\?\\GLOBALROOT\".  So \"\\??\\C:\\\" becomes \"\\\\?\\GLOBALROOT\\??\\C:\\\".  You can use it in any Win32 program that doesn't actively block that kind of path (such as the file explorer/open dialog)\n \nreply",
      "IIRC originally SUBST was designed for that - early programs didn't understand directories but did understand drives, and so you could make a directory appear to be a drive and they'd be happy - otherwise they'd dump everything in the root of C:\\ (or A:\\).\n \nreply",
      "SUBST to this day is how you solve long file name problems. One drive for business can make a very long path if it uses your full business name. Windows has the api to let some apps apps save long sob folders, but not to let Explorer or powershell delete those folders.You go on folder up and use subst to make a drive letter from which you can delete content.\n \nreply",
      "I still use SUBST with my team so we all have our source code on P:\\ which can be mapped to wherever they want it to be.  This helps keep Visual Studio object files and project includes pointing to the same place, especially when mistakes are made (they should be relative paths but things happen).It is run from a registry key upon bootup.\n \nreply",
      "We do the same, except you don't need to do it at bootup, you can set it once using the following:    Windows Registry Editor Version 5.00\n\n    [HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Session Manager\\DOS Devices]\n    \"P:\"=\"\\\\??\\\\C:\\\\Dev\\\\Source\"\n\nChange source path accordingly, save as .reg file, import once and it'll stay.Nice thing about this vs using SUBST is that the SUBST is for your user only, so if you have a separate admin account it won't see it. However the above registry entry is for the machine, so all users on the machine will see it.Obviously makes it less useful for terminal servers and other shared machines.\n \nreply",
      "I think SUBST can break when you run as administrator (elevating your own privileges).\n \nreply",
      "SUBST is all fine, up until the point some tool explodes when it sees that normalizePath(\"P:\\\\whatever\") == \"C:\\\\code\\\\whatever\", and it ends up with two paths to one file, or no way to build a relative path. I\u2019ve seen that happen with some node tooling, for example.\n \nreply"
    ],
    "link": "https://www.os2museum.com/wp/dos-append/",
    "first_paragraph": ""
  },
  {
    "title": "Squirrels Caught Hunting and Eating Meat (gizmodo.com)",
    "points": 20,
    "submitter": "ulrischa",
    "submit_time": "2024-12-18T19:11:39 1734549099",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=42453743",
    "comments": [
      "Probably about 20 years ago, near Boston, I remember noticing a reddish-brown squirrel sitting in a tree gnawing on a fried chicken leg and found the sight mildly absurd.\n \nreply",
      "Everything is an opportunistic carnivore. I once came across a turtle chowing down on a dead frog.\n \nreply",
      "Apparently cows are happy to eat nestlings if they happen to find a bird's nest while they're grazing.If you're an animal, there's no food better than meat.\n \nreply",
      "That would only be true if you have the stomach biome to digest and extract the nutrients.What is the point of eating something that is hard to process and digest and has no nutritional value for youFor herbivores, meat is objectively bad food .\n \nreply",
      "You're wrong. Cows will eat meat, horses will eat meat, pigs will eat meat, chickens will eat meat, deer will eat meat. If they can get it in their mouth, they will eat it.\n \nreply",
      "You should try raising cows on a carnivore diet.\n \nreply",
      "Horses snacking chicks. Videos easy to be found.\n \nreply",
      "Everything will eat anything if its hungry enough but to say there is no better food is a broad statement I'm not comfortable agreeing with.\n \nreply",
      "\"Better\" is kind of a vague term. A more precise and limited statement is that meat has the highest protein quality index. There could be some other disadvantages, depending on your species.\n \nreply",
      "I knew it.\n \nreply"
    ],
    "link": "https://gizmodo.com/squirrels-caught-hunting-and-eating-meat-for-the-first-time-2000540386",
    "first_paragraph": ""
  },
  {
    "title": "Tldraw Computer (tldraw.com)",
    "points": 435,
    "submitter": "duck",
    "submit_time": "2024-12-20T07:42:36 1734680556",
    "num_comments": 94,
    "comments_url": "https://news.ycombinator.com/item?id=42469074",
    "comments": [
      "I ended up at Tldraw's London office a few weeks ago for a thing, and I remember afterwards being like 'ahh, now I understand how they end up just casually doing random cool shit and attracting the kind of talent they do'.They should be extremely proud of the culture they've managed to foster and I genuinely hope to see them succeed as a business.\n \nreply",
      "Very much this! I was also at a thing at their office a few weeks ago (some thing? \"Local Thirst\"), and Steve gave a demo of this. It is incredible.I've joked before that the last generation of human machine interfaces ware invented at Xerox park, and the next generation is being invented at TLDraw of Finsbury Park. But it's not really a joke, I genuinely believe it.\n \nreply",
      "I agree. Looking at this, it seems to be exactly how I want to use LLMs. Describe a small transformation of data I don't want to work out now, connect it to other components. As the needs become more-defined, replace each part with a faster, more-reliable, well-defined data transformation. I could actually see developing a system this way...\n \nreply",
      "It was a cool thing... I expected a hacky demo that'd fall apart mid-way but it held up. The Macintosh SE in the office was cool too.\n \nreply",
      "Ha yeah, that was the same thing! The night it rained sideways.So this is the demo people were talking about at the end of the night! I was quite annoyed I missed it, makes sense now. I think I was nerding out over current-gen HIDs while eyeing up their very tastefully equipped coffee station (ozone roasters ftw)\n \nreply",
      "Tell us more about what you saw?\n \nreply",
      "I'd like to echo the impressiveness of tldraw.  At the BigBlueButton project, an open source virtual classroom, we built tldraw into the core.  It has saved us countless development hours as we stopped trying to build our own whiteboard and instead stood on tldraw's (very) wide shoulders.  We've never looked back.\n \nreply",
      "Even \"vanilla\" tldraw is super cool as a clean, functional, open-source html5 whiteboard, and the team have absolutely been killing it in their comms and use of LLMs. I honestly think they might be some of the most innovative people around when it comes to really novel UI for LLMs. Also, Todepond is just very cool.\n \nreply",
      "does the cloud product\u2019s \u201cnew project\u201d button still trash your saved documents with one click behind a docstring something like \u201cmake sure you have saved your stuff before making a new project\u201d where what they meant is \u201cour cloud product does not save your projects to the cloud, it is in local storage actually and you can only have one project at a time so the new project button actually overwrites your old one, so when we say \u2018save\u2019 we actually mean export your stuff to a json file and save to local disk!! so you can re-import it back into the product later from local disk and overwrite it back!!!!\u201d I did my VC seed pitch deck in tldraw along with a bunch of product mocks, ask me how i know this\n \nreply",
      "So sorry Dustin. We'll have a new version of tldraw with user accounts in a few weeks that should improve things, but until then please no one clear your browser storage\n \nreply"
    ],
    "link": "https://computer.tldraw.com",
    "first_paragraph": ""
  },
  {
    "title": "Ascending Mount FujiNet (leadedsolder.com)",
    "points": 43,
    "submitter": "zdw",
    "submit_time": "2024-12-18T02:27:24 1734488844",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42447580",
    "comments": [
      "The TNFS protocol used by FujiNet has also been used in other network cards for old 8-bit computers. My own ZX Spectrum is fitted with a Spectranet card which enables the same kind of connectivity and creativity. I wrote about it in my \"DevOps For The Sinclair Spectrum\" article[1] which featured here, and my TNFS site is now available through a JS emulator on a web page[2] if you want to see the kind of thing you can create. It's sort of like an old-school BBS, except the code is downloaded and run directly on your computer, which opens up a world of possibilities like multi-player games and even bridges to protocols like Gopher, Gemini and IRC which make communicating with the \"modern\" Internet possible even on an ancient tape-loading 8-bit micro from the 80s. Really fun stuff![1]=https://www.markround.com/blog/2021/12/21/devops-for-the-sin...[2]=https://jsspeccy.markround.com\n \nreply",
      "I\u2019m still not quite sure what FujiNet is and how it is the last peripheral for your vintage devices. Gives me Zombo.com vibes.(1) https://zombo.com/\n \nreply",
      "FujiNet is a multi-function network adapter. It was initially developed for the Atari 8-bit systems, but we started adapting its firmware, and building other hardware versions for other platforms.To date, we have implementations for Atari 8-bit, Coleco Adam, Apple // and ///, TRS-80 Color Computer, Atari Lynx, Atari 2600, Commodore (64/128/Plus4/VIC20). There are also system bring-ups happening for many other computers, such as ZX Spectrum, IBM PC (ISA and RS232 versions), RC-2014, BBC Micro, and more.It provides virtual disk, for loading software from the Internet, a virtual printer which rasterizes to PDF, a network adapter with tons of protocol offloading, and a whole host of other subdevices (e.g. CP/M emulation, speech synthesizer, and more)It is a public project, that anyone can jump in and hack on, and we want people to come in and help hack on versions for their favorite systems.The site is here: https://fujinet.online/\n \nreply",
      "What a fun project! I think if I ever get around to opening my own makerspace/cafe, I'd like to have something like this running in the lobby for people to see how early computers looked and worked while still being somewhat useful with fujinet.\n \nreply"
    ],
    "link": "https://www.leadedsolder.com/2024/12/17/coco-fujinet.html",
    "first_paragraph": "In case you haven\u2019t heard of it, FujiNet is an ambitious open-source community project. Its intent is to be the only peripheral you will ever need to get for your old computer. That\u2019s a lofty goal if ever I\u2019ve heard one. I\u2019ve been in and out of the project over the last few years, but I hadn\u2019t actually gotten (or finished) any FujiNet hardware. Let\u2019s revive my troubled CoCo1 with a cartridge that does it all.FujiNet can be a lot of things for old computers \u2013 so many things that it becomes actually difficult to explain what it can do if you haven\u2019t tried it yourself.The most basic explanation is that a FujiNet device adds internet support to an old computer, through wifi. But unlike Contiki or similar projects which have the end goal of \u201cprovide a network socket,\u201d FujiNet peripherals are meant to replace as many traditional peripherals as possible on an old computer.Here\u2019s just some of the things that you can do with a FujiNet:The FujiNet wiki\u2019s \u201cWhat is FujiNet?\u201d page does a much bette"
  },
  {
    "title": "Boardgame.io: an engine for creating turn-based games using JavaScript (github.com/boardgameio)",
    "points": 247,
    "submitter": "freetonik",
    "submit_time": "2024-12-18T10:50:20 1734519020",
    "num_comments": 57,
    "comments_url": "https://news.ycombinator.com/item?id=42449497",
    "comments": [
      "Original creator of boardgame.io here. A pleasant surprise to see this here after many years.More recently, I've been working on https://boardgamelab.app/, which uses a visual programming language to model game rules while also taking care of the UI layer.\n \nreply",
      "I feel like saying that is supports AI players, but not having a simple, already hosted example is a disservice. Even tic tac toe, or go fish would be a nice hook to help people understand what it actually delivers.\n \nreply",
      "Go to the projects page on the docs site\n \nreply",
      "I think this is the one, there's quite a few it seems, but not all work: https://boardgame.io/documentation/#/notable_projects\n \nreply",
      "Oh, thank you. I've used your library a few times for personal projects, and it does exactly the thing that I needed. I really appreciate you having created this.\n \nreply",
      "Omg! I've been noodling about making my word game  https://WordGlyph.xyz multi player and been dreading that journey but now here it is!\n \nreply",
      "> More recently, I've been working on https://boardgamelab.app/, which uses a visual programming language to model game rules while also taking care of the UI layer.Suppose there were a technology that could turn the canvas you authored into finished, consistent art; and a way to turn natural language rules into correct code. Would you use it? Why or why not?\n \nreply",
      "It would be great to save time on the implementation of board game rules engines. Unfortunately the fine folks at FFG are really bad at figuring out what the rules actually are and telling people :(\n \nreply",
      "\"turn the canvas you authored into finished, consistent art\"Like a jpeg? Otherwise I don't understand your question.\" a way to turn natural language rules into correct code\"And this is straight impossible, as natural language is by definition ambigious in meaning and code is not. Try your luck with LLM's, they come closest.(a subset of natural language might work, but this is kind of a complex research topic)\n \nreply",
      "There was this iOS game which died which I wanted to recreate:https://web.archive.org/web/20161020010853/http://www.82apps...But I have 0 knowledge of game development. Maybe this could make my job easier? Or maybe somebody else who know how to write game can do it? Please?\n \nreply"
    ],
    "link": "https://github.com/boardgameio/boardgame.io",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        State Management and Multiplayer Networking for Turn-Based Games\n      \n\n\n\n\n\n \n\n\n\nRead the Documentation\n\nboardgame.io is an engine for creating turn-based games using JavaScript.\nWrite simple functions that describe how the game state changes\nwhen a particular move is made. This is automatically converted\ninto a playable game complete with online multiplayer\nfeatures, all without requiring you to write a single line of\nnetworking or storage code.Read our Full Documentation to learn how to\nuse boardgame.io, and join the community on gitter\nto ask your questions!The examples can be found in the examples folder.This repository is ready to run in a dev container in VS Code. See the contributing guidelines for details.See changelog.We welcome contributions of all kinds!\nPlease take a moment to review our Code of Conduct.\ud83d\udc1b Found a bug?\nL"
  },
  {
    "title": "The era of open voice assistants (home-assistant.io)",
    "points": 689,
    "submitter": "_Microft",
    "submit_time": "2024-12-20T00:29:57 1734654597",
    "num_comments": 240,
    "comments_url": "https://news.ycombinator.com/item?id=42467194",
    "comments": [
      "I'm actually really excited for this!I noticed recently there weren't any good open source hardware projects for voice assistants with a focus on privacy. There's another project I've been thinking about where I think the privacy aspect is Important, and figuring out a good hardware stack has been a Process. The project I want to work on isn't exactly a voice assistant, but same ultimate hardware requirementsSomething I'm kinda curious about: it sounds like they're planning on a sorta batch manufacturing by resellers type of model. Which I guess is pretty standard for hardware sales. But why not do a sorta \"group buy\" approach? I guess there's nothing stopping it from happening in conjunctionI've had an idea floating around for a site that enables group buys for open source hardware (or 3d printed items), that also acts like or integrates with github wrt forking/remixing\n \nreply",
      "I'm also very excited. I've had some ESP32 microphones before, but they were not really able to understand the wake word, sometimes even when it was quiet and you were sitting next to the mic.This one looks like it can recognize your voice very well, even when music is playing.Because... when it works, it's amazing. You get that Star Trek wake word (KHUM-PUTER!), you can connect your favorite LLM to it (ChatGPT, Claude Sonnet, Ollama), you can control your home automation with it and it's as private as you want.I ordered two of these, if they are great, I will order two more. I've been waiting for this product for years, it's hopefully finally here.\n \nreply",
      "As a side note, it always slightly puzzles me when I see \"voice interface\" and \"private\" used together. Maybe it takes living alone to issue voice commands and feel some privacy.(Yes, I do understand that \"privacy\" here is mostly about not sending it for processing to third parties.)\n \nreply",
      "Private meaning that a big American corporation is not listening and using my voice to either track me or teach their own AI service with it.\n \nreply",
      "> Yes, I do understand that \"privacy\" here is mostly about not sending it for processing to third parties.Then why does it puzzle you?\n \nreply",
      "Because you wouldn't ask it deeply private questions in front of your mom, for instance\n \nreply",
      "There are levels of privacy. Because I'm not going to ask deeply private questions, it doesn't mean that I want everyone to be snooping into what I'm planning to eat tonight.\n \nreply",
      "I don't like these interaces because unless they are button activated or something, they must be always listening and sending sound from where you are to a 3rd party server. No thanks. Of course this could be happening with my phone, but at least it have to be a malicious action to record me 24/7\n \nreply",
      "How these ESP32-systems work is that you send a wake word to the device itself. It can detect the word without an internet connection, the device itself understands it and wakes up. After the device is woken up, it sends your speech to home assistant, which either  - handles it locally, if you have fast enough computer\n  - sends it to home assistant cloud, if you set it up\n  - sends it to chatgpt, claude sonnet etc. if you set it up\n\nI'm planning on building a proxmox rack server next year, so I'm probably going to just handle all the discussions locally. The home assistant cloud is quite private too, at least that's what they say (and they're in EU, so I think there might be truth in what they say)...\n \nreply",
      "I mean... That's not true, though.The main pitch of a tool like this is that I can absolutely verify it's not true.I'm currently running a slightly different take of this (Esp 32 based devices, with whisper through Willow inference server, with Willow autocorrect, tied into home assistant).For context, it works completely offline. My modem can literally be unplugged and I can control my smart devices just fine, with my voice. Entirely on my local network, with a couple of cheap devices and a ten year old gaming PC as the server.My data\n \nreply"
    ],
    "link": "https://www.home-assistant.io/blog/2024/12/19/voice-preview-edition-the-era-of-open-voice/",
    "first_paragraph": ""
  },
  {
    "title": "Decline in teen drug use continues, surprising experts (arstechnica.com)",
    "points": 105,
    "submitter": "pseudolus",
    "submit_time": "2024-12-17T23:44:40 1734479080",
    "num_comments": 135,
    "comments_url": "https://news.ycombinator.com/item?id=42446686",
    "comments": [
      "I wonder if the new drug of choice is actually technology. In some ways I think that the addiction to technology has some similar mellowing effects as drugs. Some research indicates that smartphone addiction is also related to low self-esteem and avoidant attachment [1] and that smartphones can become an object of attachment [2]. The replacement of drugs by technology is not surprising as it significantly strengthens technological development especially as it is already well past the point of diminishing returns for improving every day life.1. https://www.sciencedirect.com/science/article/abs/pii/S07475...2. https://www.sciencedirect.com/science/article/abs/pii/S07475...\n \nreply",
      "Suddenly I remember this movie from the 90s where people drugged themself with some kind of minidisc. \u201cStrange Days\u201d, maybe? Anyhow, I always found the plot weird, but maybe they actually were onto something\u2026\n \nreply",
      "The discs had -in the movie- the memories of another person, and you would experience that memory and sensations as if you were living it. So, e.g. someone would record themselves doing something risky and you would get the adrenaline rush from watching it.So... Maybe in some way one could argue that social media gives some sort of connection were you get some feelings from what others are doing/showing. I mean, technologically it's quite a leap, but in a conceptual way... it's still a bit of a leap but maybe not that big.\n \nreply",
      "> Maybe in some way one could argue that social media gives some sort of connection were you get some feelings from what others are doing/showing. I mean, technologically it's quite a leapThat technology exists; it's called empathy, and the extremely powerful form of it innate to humans is arguably our singularly defining characteristic. It's our tech moat, so to speak.\n \nreply",
      "Sounds like Brain Dances (BDs) from Cyberpunk 2077.\n \nreply",
      "Yes, which originally came from Cyberpunk, the first sourcebook for which was released in 1988, with Cyberpunk 2020 releasing in 1990 complete with the idea for pre-recorded replayable memories/full sensory experience, ie:Braindance.Strange Days was released in 1995.Maximum Mike was, and is, a prophet right alongside Gibson.edit: Although almost certainly this wasn't the first place people imagined being able to record and playback memories.\n \nreply",
      "Simstim from Neuromancer (released in 1984) is the first mention of such a thing that I know of.\n \nreply",
      "Brainstorm (1983) had the tape version of that.\n \nreply",
      "Offhand the only drug-like thing I remember from that series is the nutrition bars that had 0 calories that most of the school got addicted to. Or maybe the cheerleader that got bee pheromones and started controlling the rest of the students.Aside - I just learned a month ago that there's an official followup miniseries that brought back several of the original actors, titled \"Echoes\", with hopefully more coming since it's called Season 1.  Came out over 2022-2023: https://www.youtube.com/playlist?list=PLHGrvCp5nsDJ1qSoKZEmm... (the trailers are at the bottom of the playlist)\n \nreply",
      "Dangit tried to delete this when I realized this is completely unrelated, just a similar name, and was seconds late.  Got the delete link then it denied me.\n \nreply"
    ],
    "link": "https://arstechnica.com/health/2024/12/the-kids-are-maybe-alright-teen-drug-use-hits-new-lows-in-ongoing-decline/",
    "first_paragraph": "\n        Kids who were in 8th grade at pandemic's start have ushered in an era of abstaining.\n      Teen drug use continued to fall in 2024, extending a dramatic decline spurred by the COVID-19 pandemic that experts expected would reverse now that the acute phase of the global crisis is well over.But, according to data released Tuesday, the number of eighth, 10th, and 12th graders who collectively abstained from the use of alcohol, marijuana, or nicotine hit a new high this year. Use of illicit drugs also fell on the whole and use of non-heroin narcotics (Vicodin, OxyContin, Percocet) hit an all-time low.\"Many experts in the field had anticipated that drug use would resurge as the pandemic receded and social distancing restrictions were lifted,\" Richard Miech, team lead of the Monitoring the Future survey at the University of Michigan, said in a statement. \"As it turns out, the declines have not only lasted but have dropped further.\"The Monitoring the Future study\u2014which has been runnin"
  },
  {
    "title": "A Gentle Introduction to Graph Neural Networks (distill.pub)",
    "points": 300,
    "submitter": "misonic",
    "submit_time": "2024-12-20T04:10:42 1734667842",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=42468214",
    "comments": [
      "There are a lot of papers using GNNs for physics simulations (e.g. computational fluid dynamics) because the unstructured meshes used to discretize the problem domain for such applications map very neatly to a graph structure.In practice, every such mesh/graph is used once to solve a particular problem. Hence it makes little sense to train a GNN for a specific graph. However, that's exactly what most papers did because no one found a way to make a GNN that can adjust well to a different mesh/graph and different simulation parameters. I wonder if there's a breakthrough waiting just around the corner to make such a generalization possible.\n \nreply",
      "Naive question:Words in sentences kinda forms graphs, referencing other words or are leafs being referenced, both inside sentences and between sentences.Given the success of the attention mechanism in modern LLMs, how well would they do if you trained a LLM to process an actual graph?I guess you'd need some alternate tokenizer for optimal performance.\n \nreply",
      "For physics sims, I'd say it's useless.Imagine you discretize a cube into 1000 gridpoints in each direction, that's 1000^3 = 1 billion nodes/\"tokens\". Plus you typically time-march some sort of equation so you need the solutions previous 3-5 timesteps as well so that's 3-5 billion tokens. If you are gonna do that in the first place, you may as well just use the traditional solver. Traditional solvers usually set up and solve a matrix equation like Ax=b with an iterative method like multigrid which is O(n) as opposed to transformer's O(n^2). It'll give you a much more accurate answer much quicker than it'll take a transformer to do attention on a sequence of length 3 billion.The entire point of using GNNs/CNNs in this field is that people rely on their ability to make inference using local information. That means the value at each gridpoint/node can be inferred from neighbouring nodes only, which is O(n) like multigrid. Idea in most papers is that the GNN can do this faster than multigrid. Results so far are mixed, however [1].[1] https://arxiv.org/abs/2407.07218\n \nreply",
      "Ah yes, for dense problems like that I wouldn't expect it to work well. The example graphs in the submission were mostly quite sparse, hence why I thought of LLMs. But perhaps that was just for illustrative purposes.\n \nreply",
      "This is actually a good insight. It turns out that transformers are indeed a form of graph network, precisely because of the attention mechanism. Graph attention networks are actually a very popular GNN architecture. Generally, the issue with using an LLM style architecture for generic graphs is modeling the sparsity, but is possible by using the graph adjacency matrix to mask the attention matrix. There are a number of papers and articles which address this connection, and plenty of research into mechanisms for sparsifying attention in transformers.There are also graph tokenizers for using more standard transformers on graphs for doing things like classification, generation, and community detection.\n \nreply",
      "Any canonical papers on GNN for code graphs?\n \nreply",
      "That's the kind of thing that I could imagine could dramatically speed up certain tasks, but not enable particularly new abilities. A ton of the challenge is converting a sequence into a graph. So if you need a huge clever model to turn a sequence into a graph, then your potential gain downstream is either a) in making certain computationally hard queries easier, or b) answering tons and tons of followup queries dramatically faster (enough to make the initial graphification overhead okay).For (a), any imperfections in the graphification make the problem super hard and researchy.\n \nreply",
      "yep you're now pretty much at state-of-the-art\n \nreply",
      "A general graph solver has to be a general intelligence, since it would be able to successfully model category theory.\n \nreply",
      "Very high quality work, its a pity that distill.pub did not find a sustainable way forward [1].On GNN's, the lack of datasets [2] might be a reason they are not as talked about. This is something that has affected also the semantic web domain.[1] https://distill.pub/2021/distill-hiatus/[2] https://huggingface.co/datasets?task_categories=task_categor...\n \nreply"
    ],
    "link": "https://distill.pub/2021/gnn-intro/",
    "first_paragraph": "Neural networks have been adapted to leverage the structure and properties of graphs. We explore the components needed for building a graph neural network - and motivate the design choices behind them.\nBenjamin Sanchez-Lengeling\n\nGoogle Research\n\nEmily Reif\n\nGoogle Research\n\nAdam Pearce\n\nGoogle Research\n\nAlexander B. Wiltschko\n\nGoogle Research\nSept. 2, 202110.23915/distill.00033This article is one of two Distill publications about graph neural networks. Take a look at Understanding Convolutions on Graphs to understand how convolutions over images generalize naturally to convolutions over graphs.Graphs are all around us; real world objects are often defined in terms of their connections to other things. A set of objects, and the connections between them, are naturally expressed as a graph. Researchers have developed neural networks that operate on graph data (called graph neural networks, or GNNs) for over a decade. Recent developments have increased their capabilities and expressive po"
  },
  {
    "title": "Show HN: openai-realtime-embedded-SDK Build AI assistants on microcontrollers (github.com/openai)",
    "points": 28,
    "submitter": "Sean-Der",
    "submit_time": "2024-12-18T15:47:13 1734536833",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=42451409",
    "comments": [
      "Took a bit of poking to figure out what the use case is. Doesn't seem to be mentioned in the README (usage section is empty) or the intro above. Looks like the main use case is speech-to-speech. Which makes sense since we're talking about embedded products, and text-to-speech (for example) wouldn't usually be relevant (because most embedded products don't have a keyboard interface). Congrats on the launch! Cool to see WebRTC applied to embedded space. Streaming speech-to-speech with WebRTC could make a lot of sense.\n \nreply",
      "Sorry I forgot to put use cases in! Here are the ones I am excited about.* Making a toy. I have had a lot of fun putting a silly/sarcastic voice in toys. My 4 year old thinks it is VERY funny.* Smart Speaker/Assistant. I want to put one in each room. If I am in the kitchen it has a prompt to assist with recipes.I have A LOT more in the future I want to do. The microcontrollers I was using can't do video yet BUT ESP32 does have newer ones that can. When I pull that I can do smart cameras, then it gets really fun :)\n \nreply",
      "\"Use case\" perhaps wasn't the right word for me to use. Maybe \"applications\" would have been a better word. What this enables is speech-to-speech applications in embedded devices. (From my quick scan) it doesn't seem to do anything around other ML applications that OpenAI could potentially be involved in, such as speech-to-text, text-to-speech, or computer vision.But yeah, once I figured out that this enables streaming speech-to-speech applications on embedded devices, then it's easy to think up use cases.\n \nreply",
      "It doesn't help that this was posted to HN with the \"Usages\" section of the README left blank. That alone would probably have addressed your question. The submission is just a little prematue.Beyond that, while it does seem like its primarily vision is for speech-to-speech interfaces, it could easily be stretched to do things like send a templatized text prompt that was constructed based on toggle states, sensor readings, etc and (optimistically) asking for a structured response that could control lights or servos or whatever.Generally, this looks like a very early stage in a hobby project (the code practices fall short of my expectations for good embedded work, being presented as a library would be better than as an application, the README needs lots of work, etc), but something more sophisticated isn't too far out of reach.\n \nreply",
      "Here is a nice use-case. Put this in a pharmacy - have people hit a button, and ask questions about over-the-counter medications.Really - any physical place where people are easily overwhelmed, have something like that would be really nice.With some work - you can probably even run RAG on the questions and answer esoteric things like where the food court in an airport or the ATM in a hotel.\n \nreply",
      "> Put this in a pharmacy - have people hit a button, and ask questions about over-the-counter medications.Even if you trust OpenAI's models more than your trained, certified, and insured pharmacist -- the pharmacists, their regulators, and their insurers sure won't!They've got a century of sunk costs to consider (and maybe even some valid concern over the answers a model might give on their behalf...)Don't be expecting anything like that in an traditional regulated medical setting any time soon.\n \nreply",
      "The last few doctors appointments I\u2019ve had, the clinician used a service to record and summarize the visit. It was using some sort of TTS and LLM to do so. It\u2019s already in medical settings.\n \nreply",
      "Transcription and summary is a vastly different thing than providing medical advice to patients.\n \nreply",
      "Thanks for digging that out. Yes, that makes sense to me as someone who made a fully local speech-2-speech prototype with Electron, including VAD and AEC. It was responsive but taxing. I had to use a mix of specialty models over onnx/wasm in the renderer and llama.cpp in the main process. One day, multimodal model will just do it all.\n \nreply",
      "Love this! Excited to give it a try.\n \nreply"
    ],
    "link": "https://github.com/openai/openai-realtime-embedded-sdk",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A SDK to using the Realtime API with Microcontrollers like the ESP32\n      This SDK has been developed tested on a esp32s3 and linux. You don't need any physical hardware\nto run this SDK. You can use it from Linux directly.To use it on hardware purchase either of these microcontrollers. Others may work, but this is what\nhas been developed against.You can get a ESP32S3 for much less money on eBay/AliExpress.protoc must be in your path with protobufc installed.Call set-target with the platform you are targetting. Today only linux and esp32s3 are supported.Configure device specific settings. None needed at this timeSet your Wifi SSID + Password as env variablesBuildIf you built for esp32s3 run the following to flash to the deviceIf you built for linux you can run the binary directlySee build.yaml for a Docker command to do this all in "
  },
  {
    "title": "Show HN: celine/bibhtml: a Web Components referencing system for HTML documents (maxbo.me)",
    "points": 12,
    "submitter": "mbo",
    "submit_time": "2024-12-18T13:23:15 1734528195",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://maxbo.me/celine/bibhtml/",
    "first_paragraph": ""
  },
  {
    "title": "Kelly Can't Fail (win-vector.com)",
    "points": 345,
    "submitter": "jmount",
    "submit_time": "2024-12-19T23:07:15 1734649635",
    "num_comments": 102,
    "comments_url": "https://news.ycombinator.com/item?id=42466676",
    "comments": [
      "Note that you need to be able to infinitely divide your stake for this to work out for you all the time.For example, if the deck has 26 red cards on top, you'd end up dwindling your initial $1.00 stake to 0.000000134 before riding it back up to 9.08\n \nreply",
      "If you start out with a $1e12 stake, you're able to avoid catastrophic rounding errors even in the worst case.  There's probably a life lesson here.\n \nreply",
      "My simulation shows that with a 52 card deck, if you round the bet to the nearest $.01 you will need to start with $35,522.08 to win a total of $293,601.28.If you start with $35,522.07 or less, you will lose it all after 26 incorrect cards.\n \nreply",
      "Nearest rounding does seem like a mistake here.  Rounding down is quite safe: rather than lose it all, you end up with at least 2^26 pennies.\n \nreply",
      "This sounds similar to the Martingale system.https://en.wikipedia.org/wiki/Martingale_(betting_system)\n \nreply",
      "Is the lesson: choose to be born to wealthy parents?\n \nreply",
      "It would really help if your parents know someone who can and will take the other side in this game.\n \nreply",
      "It\u2019s easier to make money if you already habe money\n \nreply",
      "A popular view is that having wealthy parents gives one a great advantage. Another popular view is that working extraordinarily hard for money is a waste of one\u2019s life even if one gets the money. But the two are only consistent if one believes that one\u2019s own life is the optimization target. If I live a life of misery so that my children live a life of prosperity that would strike me as a phenomenal result.So another reading is \u201cchoose to give your children wealthy parents\u201d.\n \nreply",
      "Or is it to choose appropriate betting amounts based on your capacity for risk\n \nreply"
    ],
    "link": "https://win-vector.com/2024/12/19/kelly-cant-fail/",
    "first_paragraph": "\nBy John Mount on December 19, 2024\t\u2022 ( Leave a comment )\nYou may have heard of the Kelly bet allocation strategy. It is a system for correctly exploiting information or bias in a gambling situation. It is also known as a maximally aggressive or high variance strategy, in that betting more than the Kelly selection can be quite ruinous.\nI recently ran into a strange card game where the Kelly strategy is risk free with zero variance. Peter Winkler calls the game \u201cNext Card Bet\u201d in his remarkable book Mathematical Puzzles. The problem and solution appear to come from Thomas Cover. I find this betting game and its analysis amazing, and want to share them with you here.\nThe game is played as follows. A standard 52 card deck consisting of 26 red cards and 26 black cards is shuffled and the player start with a stake of $1. Each card is exposed one at a time, without being replaced in the deck. The player is allowed to bet any fraction of their current stake on whether the next card is black o"
  },
  {
    "title": "Ghost artists on Spotify (harpers.org)",
    "points": 385,
    "submitter": "greenie_beans",
    "submit_time": "2024-12-19T14:12:05 1734617525",
    "num_comments": 290,
    "comments_url": "https://news.ycombinator.com/item?id=42461530",
    "comments": [
      "This article is fascinating. But what's on display here is less of a nefarious plan from Spotify to replace famous Katy Perry with AI - instead we get to see something much more specific: a behind-the-scenes of how those endless chill/lo-fi/ambient playlists get created.Which is something I've always wondered! How does the Lofi Girl channel on Youtube always have so much new music from artists I have never heard from?The answer is surprising: real people and real instruments! (At least at the time of writing). Third-party stock music (\"muzak\") companies hiring underemployed jazz musicians to crank out a few dozen derivative songs every day to hack the algorithm.> \u201cHonestly, for most of this stuff, I just write out charts while lying on my back on the couch,\u201d he explained. \u201cAnd then once we have a critical mass, they organize a session and we play them. And it\u2019s usually just like, one take, one take, one take, one take. You knock out like fifteen in an hour or two.\u201d With the jazz musician\u2019s particular group, the session typically includes a pianist, a bassist, and a drummer. An engineer from the studio will be there, and usually someone from the PFC partner company will come along, too\u2014acting as a producer, giving light feedback, at times inching the musicians in a more playlist-friendly direction.\u201dI think there's an easy and obvious thing we can do - stop listening to playlists! Seek out named jazz artists. Listen to your local jazz station. Go to jazz shows.\n \nreply",
      "Interesting take.For my part, I'm grateful for Spotify's \"exclude from taste profile\" feature. This lets me leverage my personally-curated \"Flowstate\" playlist ^1  \nfor hours at a time while I'm working -- tracks that I've hand-picked to facilitate a \"getting things done\" mindset / energized mood / creativity or go-time vibe, and can stand to listen to on repeat -- without \"polluting\" my regular music preferences. It's apples and oranges, mostly - there's music I want to listen and attend to (as a guitar player and lifelong avid music listener across many genres including \"serious\" jazz), and there's audio (which could as easily be programmatically generated / binaural beats, whatever -- eg brain.fm) that I use as a tool specifically to help shape my cognitive state for focus / productivity.I think it's kind of funny how some people get confused about the fact that there are many reasons to listen to many kinds of music.When it comes to music discovery on Spotify, the \"go to radio\" option from a given track or album is a reliable way to surface new-to-me things. I usually prefer this proactive seeking to the playlists spotify's algo generates for me. (shrug)1. https://open.spotify.com/playlist/6UScdOAlqXqWTOmXFgQhFA?si=...\n \nreply",
      "> For my part, I'm grateful for Spotify's \"exclude from taste profile\" featureThis is my first time being made aware of this. Fantastic option that more websites should adopt\n \nreply",
      "> I think it's kind of funny how some people get confused about the fact that there are many reasons to listen to many kinds of music.It has always boggled my mind that this point seems to be lost on every music streaming service.\n \nreply",
      "> less of a nefarious plan from Spotify to replace famous Katy Perry with AIactually, it's the same nefarious plan, just that AI wasn't yet up to the task. Now it is, and replacing those fake artists, who are still human beings as far as we know, with AI (and the same fake resumes) is the logical next step.\n \nreply",
      "> Which is something I've always wondered! How does the Lofi Girl channel on Youtube always have so much new music from artists I have never heard from?> The answer is surprising: real people and real instruments! (At least at the time of writing).Sorry to break it to you, but there's actually tones of AI lofi music from Suno all over YouTube right now.See this video for an explanation: https://youtu.be/_oxtFP2UUyMAnd here are some examples of the content:https://youtu.be/RJUvNVCqtpI\nhttps://youtu.be/iBt051Pq7_4\n \nreply",
      "The YouTube link that starts with RJUvNV, titled \"(a). sip\" IMO, the first track is a banger (I really like it), and it doesn't sound obviously AI.The second track is more obviously AI, mostly due to the high frequency \"dullness\". Likewise, the second link iBT051 seems to have the same issue, it's low fidelity (but in a different way than the lo-fi style is).\n \nreply",
      "Check out Chillhop ( https://chillhop.com/radio/). Great little lofi studio out of Rotterdam. Good to the artists, from what I can tell.Psalm Trees, an artist with them, just put out an interesting little 'double'-ish album here: https://www.youtube.com/watch?v=rmL9LvTYjMQHe produced a whole jazz album just so he could sample from it for a lofi album. Absolutely mental workload.There's a lot of crap in lofi, but also some real 'bangers' too : https://www.youtube.com/watch?v=IU3yBo2szD8 (yes, really, 10 hours of great work, IMHO)\n \nreply",
      "I want to know how you found this. Why do you say they are good to the artists? I would love to know how you cultivate your feed and from where you get your music information?\n \nreply",
      "There is an incredible amount of unique music & artists on Soundcloud.  Or at least there was some years ago.  I got quite into it, to where it was taking up too much of my time and I stopped using it altogether.  They kept making it more difficult to download music too so that was another thing that drove me away.\n \nreply"
    ],
    "link": "https://harpers.org/archive/2025/01/the-ghosts-in-the-machine-liz-pelly-spotify-musicians/",
    "first_paragraph": ""
  }
]