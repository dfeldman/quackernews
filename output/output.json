[
  {
    "title": "SpaceX Super Heavy splashes down in the gulf, canceling chopsticks landing (twitter.com/spacex)",
    "points": 80,
    "submitter": "alach11",
    "submit_time": "2024-11-19T22:15:34 1732054534",
    "num_comments": 79,
    "comments_url": "https://news.ycombinator.com/item?id=42188687",
    "comments": [
      "I remember there was a phase of Falcon design where it looked like they had perfected barge landing, and then they had a rash of failures.  Later on they admitted to intentionally crashing older boosters so they could find the limits of the hardware.  They were iterating at such a pace that the data was worth more than a recovered booster.  I wonder if that was the case today?\n \nreply",
      "During the livestream they did keep saying that they\u2019re pushing it past its expected limits.\n \nreply",
      "To give an example of this, the Starship they've been flying on the last few flights is already obsolete. There's a newer V2 but they wanted to burn through the rest of the V1s they had already built and get more data before flying V2.\n \nreply",
      "Why would they plan to catch it and then divert mid flight if they didn\u2019t want to reuse it?\n \nreply",
      "It's possible to plan for multiple eventualities. They may have pushed it to its limits (or beyond) and decided the best destination based on how it handled it\n \nreply",
      "I believe they diverted to save the tower from being potentially damaged/destroyed by a failed landing.\n \nreply",
      "Yes Musk said before one of the first flights, that they are making changes and building new hardware at a much faster rate than they could ever hope to fly it right now. pretty much something to the effect that by the time they get to fly hardware a lot of it is obsolete.\n \nreply",
      "What's the advantage of the chopsticks landing over splashing the thing down in the ocean?Does an ocean landing cause significant damage that's not present with an on-land chopsticks landing?Presumably there are pretty big advantages considering how much it must have cost to develop the chopsticks approach.\n \nreply",
      "The booster is destroyed when it tips over after \"landing\" vertically on the water. It's like a 20 story building falling over.If you're asking why they don't land it on a floating barge like Falcon 9, there are two reasons. One is that landing back on the launchpad lets them refuel and relaunch immediately. The other is that landing legs are big and heavy and significantly reduce the payload capacity. If you're already landing on the launchpad you might as well add arms to catch it. The mass of the arms is free because they're on the tower instead of the rocket, and the rocket only needs tiny nubs to catch on the arms instead of giant legs. Also the arms double as a crane to lift and stack the rocket on the launchpad.\n \nreply",
      "So is it cheaper to dump the booster in the ocean then land it back at the tower and then have to dispose of it?\n \nreply"
    ],
    "link": "https://twitter.com/spacex/status/1858995009384837380",
    "first_paragraph": ""
  },
  {
    "title": "Using Erlang hot code updates (underjord.io)",
    "points": 135,
    "submitter": "lawik",
    "submit_time": "2024-11-19T20:29:03 1732048143",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=42187761",
    "comments": [
      "When I worked at Discord, we used BEAM hot code loading pretty extensively, built a bunch of tooling around it to apply and track hot-patches to nodes (which in turn could update the code on >100M processes in the system.) It allowed us to deploy hot-fixes in minutes (full tilt deploy could complete in a matter of seconds) to our stateful real-time system, rather than the usual ~hour long deploy cycle. We generally only used it for \"emergency\" updates though.The tooling would let us patch multiple modules at a time, which basically wrapped `:rpc.call/4` and `Code.eval_string/1` to propagate the update across the cluster, which is to say, the hot-patch was entirely deployed over erlang's built-in distribution.\n \nreply",
      "This matches my experience. I spent a decade operating Erlang clusters and using hot code upgrades is a superpower for debugging a whole class of hard to track bugs. Although, without the tracking for cluster state it can be its own footgun when a hotpatch gets unpatched during a code deploy.As for relups, I once tried starting a project to make them easier but eventually decided that the number of bazookas pointed at each and every toe made them basically a non-starter for anything that isn\u2019t trivial. And if its trivial it was already covered by the nl (network load, send a local module to all nodes in the cluster and hot load it) style tooling.\n \nreply",
      "Nerves and hot code reloading got me into erlang after I watched a demo of patching code on a flying drone ~8 years ago.While I can't imagine hot reloading is super practicle in production, it does highlight that erlang/beam/otp has great primitives for building reliable production systems.\n \nreply",
      "I used to work on a pretty big elixir project that had many clients with long lived connections that ran jobs that weren't easily resumable. Our company had a language agnostic deployment strategy based on docker, etc which meant we couldn't do hot code updates even though they would have saved our customers some headache.Honestly I wish we had had the ability to do both. Sometimes a change is so tricky that the argument that \"hot code updates are complicated and it'll cause more issues than it will solve\" is very true, and maybe a deploy that forces everyone to reconnect is best for that sort of change. But often times we'd deploy some mundane thing where you don't have to worry about upgrading state in a running gen server or whatever, and it'd be nice to have minimal impact.Obviously that's even more complexity piled onto the system, but every time I pushed some minor change and caused a retry that (in a perfect world at least...) didn't need to retry, I winced a bit.\n \nreply",
      "I work in gaming and have experienced the opposite side of this: many of our services have more than one \"kind\" of update, each with its own caveats and gotchas, so that it takes an expert in the whole system (meaning really almost ALL of our systems) to determine which would be the least impactful possible one if nothing goes wrong. Not only is there a lot of complexity and lost productivity in managing this process (\"Are we sure this change is zero downtime-able?\" \"Does it need a schema reload?\" etc) but we often get it wrong. The result is that, in practice, anything even remotely questionable gets done during a full downtime where we kick players out.It's sometimes helpful to have the option to just restart one little corner of the full system, to minimize impact, but it is helpful to customer experience (if we don't screw it up) and very much the opposite for developer experience (it's crippling to velocity to need to discuss each change with multiple experts and determine the appropriate type of release).\n \nreply",
      "No doubt that traditional deployments are much better for dev experience at (sometimes) the cost of customer experience.\n \nreply",
      "I disagree. Hot loading means I can have a very short cycle on an issue, and move onto something else. Having to think about the implications of hot loading is worth it for the rapid cycle time and not having to hold as many changes in my mind at once.\n \nreply",
      "One thing that would help both is deployment automation that could examine the desired changes and work out the best way to deploy them without human input. For distributed systems, this would require rock-solid contracts between individual services for all relevant scenarios, and would also require each update to be specified completely in code (or at least something machine readable), ideally in one commit. This is a level of maturity that seems elusive in gaming.\n \nreply",
      "We use hot code upgrades on kosmi.io with great success.It's absolute magic and allows for very rapid development and ease of deploying fixes and updates.We do use have to use distillery though and have had to resort to a bunch of custom glue bash scripts which I wish was more standardized because it's such a killer feature.Due to Elixirs efficiency, everything is running on a single node despite thousands of concurrents so haven't really experienced how it handles multiple nodes.\n \nreply",
      "You have to be very very very careful when preparing relups.  The alternative on Linux is to launch an entire new server on the same machine, then transfer the session data and the open sockets to it through IPC.  I once asked Joe Armstrong whether this was as good as relups and why Erlang went the relup route.  I don't remember the exact words and don't want to misquote him, but he basically said it was fine, and Erlang went with relups and hot patching because transferring connections (I guess they would have been hardware interfaces rather than sockets) wasn't possible when they designed the hot patch system.Hot patching is a bit unsatisfying because you are still running the same VM afterwards.  WIth socket migration you can launch a new VM if you want to upgrade your Erlang version.  I don't know of a way to do it with existing software, but in principle using something like HAProxy with suitable extensions, it should be possible to even migrate connections across machines.\n \nreply"
    ],
    "link": "https://underjord.io/how-i-use-erlang-hot-code-updates.html",
    "first_paragraph": "\n            Underjord is a tiny, wholesome team doing Elixir consulting and contract work. If\n            you\n            like\n            the writing you should really try the code. See our services for more\n            information.\n        One of the Erlang ecosystem\u2019s spiciest nerd snipes are hot code updates. Because it can do it. In ways that almost no other runtime can.I use Elixir which builds on Erlang and has the same capabilities.The standard way of doing Elixir releases via mix release does not support Erlang hot code updates. As in, it will not generate the necessary files for you. And if you do want to do it there are several blog posts you need to stitch together or you need to use the Erlang docs in great detail. Learn You Some Erlang has documented much of it of course. AppSignal also has a neat guide on hot code reloading in Elixir.Bryan Hunter and Chris Keathley are both people I listen when they speak because it tends to be interesting, challenging and something I j"
  },
  {
    "title": "Open Riak \u2013 open, modern Riak fork (github.com/openriak)",
    "points": 121,
    "submitter": "amarsahinovic",
    "submit_time": "2024-11-19T20:29:30 1732048170",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=42187766",
    "comments": [
      "Riak has been maintained through the post-basho years by engineers at some of its larger customers (disclaimer - including myself).The focus has been on trying to improve the stability of the database when subject to complex failure scenarios under stressful load, with minimal need for urgent operator intervention.  The focus has been on keeping those existing operators happy rather than seeking out new users.  Evolution of the product since basho has been slow but significant.The project now has support from Erlang Ecosystem Foundation, and we're looking to invest some effort over the next few months explaining what we've done, and to start to articulate what we see as the future for Riak.  So if you're interested watch this space.It is expected to remain a niche product though.  However, it may still find a home for those demanding specific non-functional requirements, with an acceptance of some functional constraints.\n \nreply",
      "> we're looking to invest some effort over the next few months explaining what we've done, and to start to articulate what we see as the future for Riak. So if you're interested watch this space.Where's that going to be posted? I'm not a Riak user but I am interested in hearing what others are doing in regards to improving failure scenarios in distributed systems.\n \nreply",
      "When we get stuff together there will be a link from our discussions page - https://github.com/orgs/OpenRiak/discussions.\n \nreply",
      "Metastability is an under-rated system property for databases and systems software, in general.\n \nreply",
      "Lightning Talk: Introducing OpenRiak - Nicholas Adams: https://www.youtube.com/watch?v=0GLBsBeM4Kc\n \nreply",
      "Basho team was very kind to open source contributions in ~2011-12: I've written an open source Riak client in Dart, and they had sent me t-shirts (the quality ones that are rare today). Nice treats for a fun project :)\n \nreply",
      "As someone who has used Riak in anger once in his career and who has a blossoming interest in FoundationDB I'd love someone to contrast the two systems. My knee-jerk reaction --- which I'm calling out as such! --- is that FDB has decreased the relevance of systems like Riak.\n \nreply",
      "I would tend to agree, perhaps a decade ago it was easier to define the uniqueness of Riak, and now there are alternatives that offer similar guarantees.  So the relevance of Riak is not as obvious.Also as we focus on stability on OpenRiak going forward, that means reducing some of the capability that may have made Riak stand-out in the scale-out space.  The preference going forward is to do fewer things, but do those things predictably well.There will be differences between Riak and FoundationDB, and I hope those differences are sufficient to make Riak interesting, and allow it to continue to occupy a small niche in the world of databases.\n \nreply",
      "I've never met an engineering team that used Riak, but it is used heavily as an example technology in Kleppmann's 'Designing Data Intensive Applications'. (I would say, informally, it's usually the example of the \"other way\" as opposed to other more well-known databases.) This does make me wonder what became of it, why it didn't take off.\n \nreply",
      "Speaking as a former tech evangelist/engineer at Basho, there were a few significant challenges.Riak is horribly unfriendly as a database: no SQL, it exposes eventual consistency directly to the developer, it\u2019s relatively slow, and Erlang is a fairly unusual language.While you can run Riak on a single server, you\u2019d have to really want to.Its strength is the ability to scale massively, but not many projects need that scale, and by the time you do, you\u2019re probably already using some friendlier database and you\u2019d rather make that one work.\n \nreply"
    ],
    "link": "https://github.com/OpenRiak",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          Promoting an open, modern Riak\n              Forked from basho/riak\n\n            Riak is a decentralized datastore from Basho Technologies.\n          \n\n\nShell\n\n\n\n\n\n                62\n              \n\n              Forked from russelldb/rabl\n\n            RabbitMQ Real Time Replication for Riak\n          \n\n\nErlang\n\n\n\n\n\n                12\n              \n\n              Forked from basho/riak_core\n\n            Distributed systems infrastructure used by Riak.\n          \n\n\nErlang\n\n\n\n\n\n                6\n              \n\n\n\n\n                1\n              \n\n              Forked from basho/eleveldb\n\n            Erlang LevelDB API\n          \n\n\nC++\n\n\n\n\n\n                1\n              \n\n              Forked from basho/cluster_info\n\n            Fork of Hibari's nifty cluster_info OTP app\n          \n\n\nErlang\n\n\n\n\n\n                1\n              \n\n          "
  },
  {
    "title": "Fair coins tend to land on the side they started (researchgate.net)",
    "points": 318,
    "submitter": "seanhunter",
    "submit_time": "2024-11-19T09:03:50 1732007030",
    "num_comments": 158,
    "comments_url": "https://news.ycombinator.com/item?id=42181345",
    "comments": [
      "Hi, I'm the first author of the manuscript, so I thought I could answer some of the questions and clarify some issues (all details are in the manuscript, but who has the time to read it ;)Low RPM tosses: Most of the recordings are on crapy webcams with ~ 30FPS. The coin spin usually much faster than the sensor can record which results in often non-spinning-looking flips. Why did we take the videos in the first place? To check that everyone collected the data and to audit the results.Building a flipping matching: The study is concerned with human coin flips. Diaconis, Holmes, and Montgomery's (DHM, 2007) paper theorize that the imperfection of human flips causes the same-side bias. Building a machine completely defeats the purpose of the experiment.Many authors and wasted public funding: We did the experiment in our free time and we had no funding for the study = no money was wasted. Also, I don't understand why are so many people angry that students who contributed their free time and spent the whole day flipping coins with us were rewarded with co-authorship. The experiment would be impossible to do without them.Improper tosses: Not everyone flips coin perfectly and some people are much worse at flipping than others. We instructed everyone to flip the coin as if they were to settle a bet and that the coin has to flip at least once (at least one flip would create bias for the opposite side). We find that for most people, the bias decreased over time which suggests that people might get better at flipping by practice = decrease the bias and it also discredits the theory that they learned how to be biased on purpose. From my own experience - I flipped coins more than 20,000 times and I have no clue how to bias it. Also, we did a couple of sensitivity analyses excluding outliers - the effect decreased a bit but we still found plentiful evidence for DHM.If you doubt my stats background, you are more than welcome to re-analyze the data on your own. They are available on OSF: https://osf.io/mhvp7/ (including cleaning scripts etc).Frantisek Bartos\n \nreply",
      "Hi, thanks for replying.  I have no complaints about your analysis, and agree that your results strongly support the D-H-M model (that there is a slight bias in coin-flipping over all and that it is caused by precession).  However, it looks like about a third of your volunteers had little or no bias, presumably due to flipping end-over-end with no precession, and about a third had a lot of precession and a lot of bias.Your paper draws the conclusion that coin-flipping inherently has a small-but-significant bias, but looking at table 2 it seems like an equally valid conclusion would be that some people flip a coin with no bias and others don't.  Did you investigate this at all?  In particular, I'd expect that if you took the biggest outliers, explained what precession is and asked them to intentionally minimize it, that the bias would shrink or disappear.\n \nreply",
      "Yes, there is indeed a lot of heterogeneity in the bias between flippers and we are going to put more emphasis on it in an upcoming revision. However, it's hard to tell whether there are two groups or a continuous scale of increasing bias. From our examination of the data, and continuum seem to be the more likely case, but we would need many many more people flipping a lot of coins to test this properly.Yes, training the most wobbly flippers sounds like a very interesting idea. It might indeed answer additional questions but it's not really something I wanna run more studies on :)\n \nreply",
      "Understandable, but I guess it's hard to put much weight on this data given how easy it is to introduce the effect being studied intentionally. Were the subjects aware of D-H-M beforehand?  I wasn't before today, but I've been able to fake a coin flip with precession for many years (a very useful skill for parents of two small children) and if I was participating in a study like this I would be pretty hyper-aware of how much \"sideways\" I was giving it.\n \nreply",
      "The first thing I looked for was how high was the flip and did it land on a hard or soft surface.  Neither seemed to be mentioned in the paper.From the one video I looked at, the flip seems to be a few feet high at most, and land back in the hand.\n \nreply",
      "> In each sequence, people\n randomly (or according to an algorithm) selected a starting position (heads-up or tails-up) of the first coin flip, flipped the\n coin, caught it in their hand, recorded the landing position of the coin (heads-up or tails-up), and proceeded with flipping\n the coin starting from the same side it landed in the previous trial (we decided for this \u201cautocorrelated\u201d procedure as it\n simplified recording of the outcomes).\n(p.3)Wrt to the height, that naturaly varied among people and flips and we did not measure it.\n \nreply",
      "The NFL still flips coins professionally.  I wonder if they have better-than-webcam footage of each flip.  Somewhere out there a bookie might be very interested in any potential bias.\n \nreply",
      "That makes me wonder whether any bookmakers or sports betting arbitration shops have ever internally ran a study like this.With how much money there is in sports betting, it could potentially be somewhat lucrative, though I wouldn't be surprised if the bias doesn't actually end up mattering that much in practice.\n \nreply",
      "IIRC, past studies have suggested that letting the coin land, rather than catching it, reduces or eliminates the bias.\n \nreply",
      "The paper looks like it has a large sample size, but it actually has a sample size of only 48 testers/flippers. Some of the videos of those testers show very low, low-rpm coin tosses, we're talking only 1-2 flips. Where they also flipped thousands of times, presumably in the same way. So there is actually a very small sample size in the study (N = 48), where testers that don't flip properly (low rpm, low height, few coin rotations) can affect the results disproportionately.Doesn't look like the study author backgrounds are particularly focused on statistics. I would presume with 48 authors (all but 3 of which flipped coins for the study), the role of some might have been more test subject than author. And isn't being the subject in your own study going to introduce some bias? Surely if you're trying to prove to yourself that the coins land on one side or another given some factor, you will learn the technique to do it, especially if you are doing a low-rpm, low flip. Based on the study results, some of the flippers appear to have learned this quite well.If the flippers (authors) had been convinced of the opposite (fair coins tend to land on the opposite side from which they started) and done the same study, I bet they could have collected data and written a paper with the results proving that outcome.\n \nreply"
    ],
    "link": "https://www.researchgate.net/publication/374700857_Fair_coins_tend_to_land_on_the_same_side_they_started_Evidence_from_350757_flips",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Physically accurate black hole simulation using your iPhone camera (apps.apple.com)",
    "points": 220,
    "submitter": "yunyu",
    "submit_time": "2024-11-19T17:06:11 1732035971",
    "num_comments": 89,
    "comments_url": "https://news.ycombinator.com/item?id=42185668",
    "comments": [
      "Hello! We are Dr. Roman Berens, Prof. Alex Lupsasca, and Trevor Gravely (PhD Candidate) and we are physicists working at Vanderbilt University. We are excited to share Black Hole Vision: https://apps.apple.com/us/app/black-hole-vision/id6737292448.Black Hole Vision simulates the gravitational lensing effects of a black hole and applies these effects to the video feeds from an iPhone's cameras. The application implements the lensing equations derived from general relativity (see https://arxiv.org/abs/1910.12881 if you are interested in the details) to create a physically accurate effect.The app can either put a black hole in front of the main camera to show your environment as lensed by a black hole, or it can be used in \"selfie\" mode with the black hole in front of the front-facing camera to show you a lensed version of yourself.\n \nreply",
      "There are several additional options you can select when using the app. The first lensing option you can select is \"Static black hole\". In this mode, we simulate a non-rotating (Schwarzschild) black hole. There are two submodes that change the simulated field-of-view (FOV): \"Realistic FOV\" and \"Full FOV\". The realistic FOV mode takes into account the finite FOV of the iPhone cameras, leading to a multi-lobed dark patch in the center of the screen. This patch includes both the \"black hole shadow\" (light rays that end up falling into the black hole) and \"blind spots\" (directions that lie outside the FOV of both the front-and-rear-facing cameras). The full FOV mode acts as if the cameras have an infinite FOV such that they cover all angles. The result is a single, circular black hole shadow at the center of the screen.Next, you can select the \"Kerr black hole\" mode, which adds rotation (spin) to the black hole. Additionally, you can augment the rotational speed of the black hole (its spin, labeled \"a\" and given as a percentage of the maximal spin).\n \nreply",
      "In a nutshell, the app computes a map from texture coordinate to texture coordinate. This map is itself stored as a texture --- to obtain the value of the map on texture coordinates (x,y), one samples the texture at (x,y) and the resulting float4 contains the outputs (x',y') as well as a status code.When the user selects the \"Static black hole\" mode, this texture is computed on the GPU and cached. The \"Kerr black hole\" textures, however, have been precomputed in Mathematica, due to the need for double precision floating point math, which is not natively available in Apple's Metal shading language.The source code, including the Mathematica notebook, can be found here https://github.com/graveltr/BlackHoleVision.\n \nreply",
      "We hope you enjoy watching the world with Black Hole Vision and welcome any questions or feedback. If you like the app, please share it with your friends!The code was written at Vanderbilt University by Trevor Gravely with input from Dr. Roman Berens and Prof. Alex Lupsasca. This project was supported by CAREER award PHY-2340457 and grant AST-2307888 from the National Science Foundation.License: This app includes a port of the GNU Scientific Library's (GSL) implementation of Jacobi elliptic functions and the elliptic integrals to Metal. It is licensed under the GNU General Public License v3.0 (GPL-3.0). You can view the full license and obtain a copy of the source code at: https://github.com/graveltr/BlackHoleVision.\n \nreply",
      "By any chance, was Andrew Strominger involved in this at all? He gave the Andrew Chamblin Memorial Lecture in Cambridge last month and demoed something that looked similar.\n \nreply",
      "I think what he showed you was likely a version of this that was coded up by Harvard graduate student Dominic Chang:\nhttps://dominic-chang.com/bhi-filter/It works very well (and in a browser!) but is limited to a non-rotating (Schwarzschild) black hole---we really wanted to include black hole spin (the Kerr case). As we write on the github, talking with Dominic about his implementation was very useful and we are hoping to get a paper explaining both codes out before the end of the year.\n \nreply",
      "Yes, Andy has been very involved in the story of the photon ring and was one of the lead authors on the original paper that started it all:\nhttps://www.science.org/doi/10.1126/sciadv.aaz1310(And he was also my PhD advisor.)\n \nreply",
      "I feel like this app could also be an app clip to make it so that you don\u2019t have to outright install the app to use it: https://developer.apple.com/app-clips/\n \nreply",
      "As far as I can tell, the black hole's you're generating don't look especially correct in the preview: they should have a circular shadow like this https://i.imgur.com/zeShgrx.jpeg\n \nreply",
      "What the black hole looks like depends on how you define your field of view. And if the black hole is spinning, then you don't expect a circular shadow at all. But in our app, if you pick the \"Static black hole\" (the non-rotating, Schwarzschild case) and select the \"Full FOV\" option, then you will see the circular shadow that you expect.\n \nreply"
    ],
    "link": "https://apps.apple.com/us/app/black-hole-vision/id6737292448",
    "first_paragraph": "What would the world around you look like if you were staring at a black hole? This application places a black hole in the field of view of your iPhone\u2019s camera and gravitationally \"lenses\" the resulting live video feeds just as a black hole would warp the light from surrounding stars. The resulting \"Black Hole Vision\" reveals interesting features of black hole lensing, including a \"photon ring\" of strongly lensed images around the black hole. The measurement of this photon ring will be the target of future astronomical observations from space with the Black Hole Explorer: an orbit satellite that will take the sharpest images in the history of astronomy! To learn more about BHEX, head over to https://www.blackholeexplorer.org/.The code is open source and freely available on GitHub at https://github.com/graveltr/BlackHoleVision, along with documentation that explains the underlying black hole lensing equations and how they are implemented in Black Hole Vision.The code was written at Van"
  },
  {
    "title": "Is the 80 character line limit still relevant? (2008) (richarddingwall.name)",
    "points": 42,
    "submitter": "azefiel",
    "submit_time": "2024-11-12T15:11:46 1731424306",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=42116051",
    "comments": [
      "Considering how easily I lose track of my position when reading long lines and how easy it is to read vertically when lines each have a single word on them, I seriously question the method by which most developers determine that they prefer >80 character line length limits.\n \nreply",
      "80 characters is way, way too short. I think 120 is a decent spot for today's displays, although I don't mind even longer than that either.\n \nreply",
      "The penultimate paragraph of the article really shows its age.  When there are unclear or conflicting rules ... [y]ou can end up with hilarious games like formatting tennis ...\n\nBack then, formatters were rarely used, if at all. The major benefit of tools like gofmt, Prettier, etc. is that a major source of vacuous commits and code review has gone away.In the case of Prettier, bikeshedding can still happen over the .prettierrc file, but it's not hard to make an argument for using the default configuration.Formatters are something I miss whenever using languages without a widely-used formatting tool. For instance, in Common Lisp, code formatting is \"whatever Emacs does when formatting the whole buffer\". Depending on the exact package used (e.g. SLIME or SLY), the results of formatting the whole buffer may differ. Contrast this to languages like Go where there is one tool (gofmt) and it exposes no configuration, so there is no possibility of bikeshedding over code formatting.\n \nreply",
      "In new projects I usually add an empty .prettierrc file with a single comment:  // Empty file. Use default Prettier settings. No rules here.\n\nTo make it clear for other people that it was not just a mistake that the formatter was missing its configuration or that no config file existed at all. Useful to deter from adding new rules because someone is capricious about their own preferences...\n \nreply",
      "Maybe even add a CI step that closes PRs if someone modifies formatter config files.\n \nreply",
      "One minor advantage which wasn\u2019t so relevant in 2008: 80 character lines are much easier to read  on a smartphone. This is especially true for Safari on iOS, which always seems to make bad wrapping / sizing decisions with plain text.ETA: thinking back on it, several years ago I switched from 120 characters to 80 specifically because of this. I don\u2019t have a car, so I read a lot of code on my phone while taking public transit.\n \nreply",
      "Took me a second to realize ETA was \"edited to add.\"  I was very confused on what an estimated time of arrival meant, here.  :)I'm surprised reading on a phone is common.  Not at all something I would want to do.  I'm assuming largely read only there?Makes me curious if the CWEB idea of styling specifically for reading has extra merit in that flow?\n \nreply",
      "I've made edits to config files on the GitHub mobile app while on-call. Not something I like doing but I like having the option.\n \nreply",
      "I've been reading books on phone since palm pilot and 160x160 screen.I do not want to type / create on my phone though, certainly not code, or even sign up for stuff / submit / do applications on phone. Makes me a rarity though.\n \nreply",
      "I don\u2019t know how \u201ccommon\u201d it is, but (like I said in my comment) the only reason I read code on my phone is that I am often on a bus or a subway. The comment about assuming I only read on my phone is bizarre: you are putting words in my mouth for reasons I do not understand. When it comes to books I like paper, and I usually read long PDFs on a tablet.\n \nreply"
    ],
    "link": "https://richarddingwall.name/2008/05/31/is-the-80-character-line-limit-still-relevant/",
    "first_paragraph": "Traditionally, it\u2019s always been standard practice for programmers to wrap long lines of code so they don\u2019t span more than 80 characters across the screen.\u00a0This is because, back in the bad old days, most computer terminals could only display 25 rows of 80 columns of text on screen at once. Any lines that were longer would simply trail off out of sight. To ensure this didn\u2019t happen, programmers split up long lines of code so none of them exceeded 80 characters.Today, however, it\u2019s pretty unlikely that you or anyone will still be writing code on an 80-column-width terminal. So why do we keep limiting our code to support them?The answer, of course, is that\u00a0we don\u2019t. An 80 character limit has no relevance any more with modern computer displays. The three-year-old Powerbook I am writing this post on, for example, can easily display over 200 characters across the screen, at a comfortable 10 point font size. That\u2019s two and a half\u00a0VT100s!The reason this standard has stuck around all these years"
  },
  {
    "title": "GroMo (YC W21) Is Hiring (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-11-20T01:00:22 1732064422",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/gromo/jobs/C65u6ox-product-manager-insurance-insurance-background-must",
    "first_paragraph": "App for independent agents to sell financial products in IndiaAs part of our Product team, build innovative solutions to help increase the penetration of Insurance and increase financial literacy with the help of financial advisors. The pace of our growth is incredible. If you want to tackle hard, interesting, and UNIQUE problems, and create an impact within an entrepreneurial environment, we are a perfect match.If your biggest superpower is a go-getter attitude, the grit to get things done, and you have an eye for details, this is for you.Building India's largest financial products distribution companyAt GroMo , we deeply understand how financial products such as Demat Account, Saving Account, Loans, Insurance, etc. are sold and we want to empower Millions of agents and financial advisors with the power of technology. Our ability to understand what customers want, build fast and iterate faster makes us stand far apart from our competitors.It all starts with the right team \u2014 a team tha"
  },
  {
    "title": "Tiling with Three Polygons Is Undecidable (arxiv.org)",
    "points": 14,
    "submitter": "denvaar",
    "submit_time": "2024-11-17T05:51:44 1731822704",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://arxiv.org/abs/2409.11582",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Hand Tracking for Mouse Input (chernando.com)",
    "points": 137,
    "submitter": "wonger_",
    "submit_time": "2024-11-19T17:18:14 1732036694",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=42185842",
    "comments": [
      "It unsettled me a lot about just how much work was put into making the JavaScript version of this work instead of a purely Python version, due to how OpenCV works. I wonder how universal the laggy OpenCV thing is, because my friend faced it too when working on an OpenCV application. Is it so unavoidable that the only option is to not use Python? I really hope that there is another way of going about this.Anyways, I am very glad that you put in all that effort to make the JavaScript version work well. Working under limitations is sometimes cool. I remember having to figure out how PyTorch evaluated neural networks, and having to convert the PyTorch neural network into Java code that could evaluate the model without any external libraries (it was very inefficient) for a Java code competition. Although there may have been a better way, what I did was good enough.\n \nreply",
      "Creating a faster python implementation can definitely be done. OpenCV is a thin wrapper over the C++ API so it's not due to some intrinsic python slowness. It is not easy to resolve though and I suspect the way python code is typically written lends itself to an accidentally blocking operation more often than JS code. It's hard to know without seeing the code.\n \nreply",
      "author here, sorry you have to see my janky JavaScript solution XD\nbut one good thing of going with Tauri is that developing the UI is pretty easy, since it's basically just some web pages, but with access to the system, through the JS <-> Rust communication.also, rewriting neural network from PyTorch to Java sounds like a big task, I wonder if people are doing ML in Java\n \nreply",
      "Remeinds me of the Leap Motion controller, now there's a version 2: https://leap2.ultraleap.com/downloads/leap-motion-controller...\n \nreply",
      "I did a very similar project a few months back. My goal was to help alleviate some of the RSI issues I have, and give myself a different input device.The precision was always tricky, and while fun, i eventually abandoned the project and switched to face tracking and blinking so i didn't have to hold up my hand.For some reason the idea of pointing my webcam down, didn't dawn on me ever. I then discovered Project Gameface and just started using that.Happy programming thank you for the excellent write up and read!\n \nreply",
      "I'm curious how your experience is using Gameface for day-to-day tasks like coding. I assume you still use a keyboard for typing, but what about selecting blocks of text or general navigation?\n \nreply",
      "Mediapipe is a lot of fun to play with and I'm surprised how little it seems to be used.You might also be interested in Project Gameface, open source Windows and Android software for face input: https://github.com/google/project-gamefaceAlso https://github.com/takeyamayuki/NonMouse\n \nreply",
      "> Python version is super laggy, something to do with OpenCVMost probably I'm wrong, but I wonder if it has anything to do with all the text being written to stdout. In the odd chance that it happens on the same thread, it might be blocking.\n \nreply",
      "Could it then be resolved by using the no-gil version of python they just released?\n \nreply",
      "I\u2019m not sure what your reasoning is, but note that blocking I/O including print() releases the GIL. (So your seemingly innocent debugging print can be extremely not harmless under the wrong circumstances.)\n \nreply"
    ],
    "link": "https://chernando.com/blog/2023/07/23/hand-tracking-for-mouse-input.html",
    "first_paragraph": ""
  },
  {
    "title": "Bottles of OOP now available in Python (sandimetz.com)",
    "points": 88,
    "submitter": "mattcollins",
    "submit_time": "2024-11-15T15:39:16 1731685156",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=42147860",
    "comments": [
      "HN's automatic title editing strikes again. The title of this submission should presumably be: \"99 Bottles of OOP now available in Python\".\n \nreply",
      "Note to anyone who submits an article: If the title gets mangled like this, edit it.\n \nreply",
      "It definitely took me quite a bit of time from I joined HN until I learned that if you edit your submission title then you can override the automatic edits that HN makes to the title you originally submitted.And I would guess that likewise there are still a lot of people that don\u2019t know this.Also, sometimes one might not realize that the title got changed until it\u2019s too late to edit the title of the post.\n \nreply",
      "This is one of my favorite software development books of all time. It's the book that finally offered straight forward guidance and wisdom on how to properly utilize OOP language features.I'm very happy to see it out for Python!\n \nreply",
      "Sandi's earlier book, POODR, was also great. While it is focused on Ruby, most of the advice applies more broadly.Reading these two really helped me understand just how impoverished the concept of OOP has become by C++ and Java, from its Smalltalk roots.\n \nreply",
      "I was so lucky to have run into poodr when I did. Early enough in my career to still feel like I didn't know anything, but with just enough experience to have encountered the problems she was addressing \"in the wild.\" Absolutely formative for me I have no idea where I'd be without it. The only other book to even approach its impact for me is working effectively with legacy code.\n \nreply",
      "Maybe this is misguided, but it feels a bit to me (comparing the ruby and js versions for example) that this is using the same code examples in both, and neither are really typical of the sorts of code people in either language community would actually write?\n \nreply",
      "Previous DiscussionBottles of OOP - https://news.ycombinator.com/item?id=12129821 - July 2016 (71 comments)\n \nreply",
      "that was not for the Python version\n \nreply",
      "Is the book DRM free? Sorry to be this paranoid, but you cannot be sure today.\n \nreply"
    ],
    "link": "https://sandimetz.com/99bottles",
    "first_paragraph": "99 Bottles of OOP is a practical guide to writing cost-effective, maintainable, and pleasing object-oriented code.\u00a0Now available in Python, as well as JavaScript, PHP, and Ruby!It explores:\u00a0Recognizing when code is \"good enough\"Getting the best value from Test-Driven Development (TDD)Doing proper refactoring, not random \"rehacktoring\"Locating concepts buried in codeFinding names that convey deeper meaningSafely altering code by following the \"Flocking Rules\"Simplifying new additions with the Open/Closed PrincipleAvoiding conditionals by obeying the Liskov Substitution PrincipleMaking targeted improvements by reducing Code SmellsImproving changeability with polymorphismManufacturing role-playing objects using FactoriesHedging against uncertainty by loosening couplingDeveloping a programming aestheticWhat Makes It Unique?We are practical people. We love beautiful code but we're also committed to getting things done. 99 Bottles of OOP enables both of these desires. It teaches practical pr"
  },
  {
    "title": "OpenStreetMap's New Vector Tiles (marksblogg.com)",
    "points": 354,
    "submitter": "marklit",
    "submit_time": "2024-11-19T12:03:02 1732017782",
    "num_comments": 129,
    "comments_url": "https://news.ycombinator.com/item?id=42182519",
    "comments": [
      "I'm a bit conflicted with vector tiles. I haven't found a good combination of style and tile generator (schema) that provides the same level of detail as the original raster tiles provide.The article has screenshots that very much demonstrate this difference. The first screenshot has, for example, a lot of POIs (statues, shops, theaters, viewpoints), highways that are different when they are bridges, different colors for grass vs parks, different line widths for different highways, sports fields, building and neighbourhood names, arrows denoting one-way streets, building parts, stairs, trees, and a lot more.The second screenshot has none of that, aside from a single trolly station and a single street name (which is also rendered incorrectly).I've tried a lot of vector styles (all openmaptiles styles, the base protomaps styles, all mapbox styles) and generators (protomaps, openmaptiles, mapbox). None of them come close to the amount of detail as the raster OSM tiles while still being as readable.I've never found anyone as bothered with this as I am. Vector styles are cool as they zoom and pan very smoothly, and their style is fairly easily editable. But, for any map where you actually want to see map data instead of using it as a base map for your own data, vector maps fall short.Maybe it is just because of computational limits. I can imagine that displaying the same amount of detail as the OSM raster tiles would require too many resources: both on the client side and for tile generation.It would be nice if OpenStreetMap would try to mimic their raster style closer, instead of providing just another low contrast, low detail base map. I hope this release of open vector tiles will facilitate more detailed vector maps!\n \nreply",
      "Some of what is going on is that the software to generate continuously updated vector tiles has been the focus, rather than fleshing out a style that uses those tiles.\n \nreply",
      "That's great and wasn't clear at all from the article. The engineering challenge behind that is very much worth a post on its own. Must've been no small feat!\n \nreply",
      "The article wasn't authored by anybody actually involved with the service or setting it up, just in case that wasn't clear.Paul has written a blog post or two on the subject https://www.openstreetmap.org/user/pnorman/diary\n \nreply",
      "Ah this is great and super interesting. Thank you!\n \nreply",
      "I've been successful with using https://maputnik.github.io/editor in the past for disabling lots of features, so I imagine it could do the opposite and enable all the layers and features you want to see on a vector map.\n \nreply",
      "I've tried that, but the openmaptiles spec and protomaps spec just don't provide enough details to be able to display a detailed map like the OSM raster tiles.That's not to say that maputnik isn't great; it is! It's an awesome frontend to the complex style files.\n \nreply",
      "I agree with you wholeheartedly. I use a mix of OpenSteetMap and Google Maps to get around. I would love to use exclusively OSM, but it simply doesn't have as many places catalogued.I can't describe how much more usable OSM maps are because of how many POIs they show. You get a very real sense of the physical place just by looking at the map. A park looks distinctly like a park. A hospital looks distinctly like a hospital.Google Maps looks a lot better from the perspective of a graphic designer, sure. But I usually have to resort to cross-referencing the shape of streets/intersection to get my bearings. With GM everywhere looks like everywhere else.\n \nreply",
      "It's been fascinating to watch the open source community build out vector map tile capabilities. I was doing some web GIS work back in roughly 2018, and Google/Apple's streaming vector maps performed like magic and something we would have loved to use if we could afford it. Shortly thereafter the core tech was available in open source, and then there were even free hosted solutions. Now our leaflet maps have great vector layers for free. Thanks open source!\n \nreply",
      "I'm a little surprised it's taken this long for OSM to get there, the basic technical pieces were available over a decade ago. I don't mean to complain about the free map service, it's excellent, and I recognize they focus more on the editing and data ownership. Serving is hard and expensive.Mostly I wonder how much MapBox's dominance for a few years disrupted other efforts.\n \nreply"
    ],
    "link": "https://tech.marksblogg.com/osm-mvt-vector-tiles.html",
    "first_paragraph": "I have 15 years of consulting & hands-on build experience with clients in the UK, USA, Sweden, Ireland & Germany. Past clients include Bank of America Merrill Lynch, Blackberry, Bloomberg, British Telecom, Ford, Google, ITV, LeoVegas, News UK, Pizza Hut, Royal Mail, T-Mobile, Williams Formula 1, Wise & UBS. I hold both a Canadian and a British passport. My CV, Twitter & LinkedIn.\n      \nHome\n        | Benchmarks\n\n        | Categories\n\n            | Atom Feed\nPosted on Tue 19 November 2024  under GISOpenStreetMap (OSM) has been hosting raster tiles of its dataset for much of its 20-year history. These maps have had their rules and styles defined ahead of their rendering and present the end user with static PNGs. Below is a screenshot of the area around the Burj Khalifa in Dubai.This week, OSM began hosting vector tiles in Mapbox Vector Tiles (MVT) format.This allows for the end user to adjust the style and rendering rules as well as extract the underlying information within each of the "
  },
  {
    "title": "Using uv with PyTorch (astral.sh)",
    "points": 37,
    "submitter": "charliermarsh",
    "submit_time": "2024-11-19T21:59:10 1732053550",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=42188555",
    "comments": [
      "uv significantly speeds up my pytorch in docker builds  # Setup virtual env\n  ENV VIRTUAL_ENV=/app/.venv\n  ENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n  RUN python3 -m venv $VIRTUAL_ENV\n  RUN . $VIRTUAL_ENV/bin/activate\n\n  # install using uv\n  RUN pip install uv\n  RUN uv pip install torch==${TORCH_VERSION} --index-url https://download.pytorch.org/whl/cpu\n\nThe index-url makes it really convenient.\n \nreply",
      "Use \u2014copy-from to make it even faster, and use a cache mounthttps://docs.astral.sh/uv/guides/integration/docker/#install...\n \nreply",
      "Speeds up installation, or speeds up PyTorch in general?\n \nreply",
      "So uv caused a bit of an issue with me installing PyTorch over the weekend.When installed with brew on my MacBook, uv currently has PyTorch 3.13 as a dependency, which is fine. But PyTorch does not currently have a stable wheel that's compatable with Python 3.13! This resulted in very confusing errors. (Solution was to point to the Nightly index)That's technically PyTorch's fault, but it's indicitave why a specific page on installing PyTorch is necessary, and it's good to know the documentation specifically calls it out.\n \nreply",
      "I have run into multiple package problems with 3.13 with a non obvious root cause error message. Thankfully, uv makes it trivial to switch out to 3.12\n \nreply",
      "I work on our error messages, feel free to open an issue and we'll do our best to make it clearer\n \nreply"
    ],
    "link": "https://docs.astral.sh/uv/guides/integration/pytorch/",
    "first_paragraph": ""
  },
  {
    "title": "Why I hate the index finger (1980) (nih.gov)",
    "points": 128,
    "submitter": "consumer451",
    "submit_time": "2024-11-19T00:35:15 1731976515",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=42178916",
    "comments": [
      "I partially amputated (at the joint closest to the nail) my index finger a decade ago, and it\u2019s been a huge impediment. This has motivated me to seek out some other opinions.Sent it to some doctor friends and they are floored by the writing style as well.\n \nreply",
      "It's hard for me to rationalise that a few cms off the index could be a huge impediment, so can you please share where you find yourself impaired the most? I imagine gripping things, like a glass, should be more or less unaffected.\n \nreply",
      "TL;DR nerves are really weird.It tingles all the time. There's a ton of \"referred pain\". It frequently feels like there is a dental drill going of in my face, when it's not painful, it's a a persistent nagging tickle, on my cheek/temple/around my eye.It gets reynauds phenomena, my house is 68F 20C right now, but my finger is freezing/painful because of how cold it is, this happens pretty much any time I wash my hand, so even in the summer when there's a slight breeze I'm hiding my hand in my pocket for warmth.When I bang it on things it really hurts, and like this paper says it's extended basically all the time when I'm trying to use my hand for other things.When I use it to grab things, it feels really weird, so I've kind of trained myself to keep it out of the way. This paper says cut it off, which the few other orthopedists I've talked to have not advised, but at this point, it's been a decade, and seeing a doctor be like \"dude, the thing that's only there to make your hand more precise, is actually making your hand way less precise and detracting from your quality of life, cut it off\", is a perspective I'm happy to hear. I manage mostly alright, but it's been a decade of major annoyance at best.\n \nreply",
      "This is really fascinating to me, because it explains some things.I have a friend who crushed the tip of one of her fingers, but it wasn't amputated. She's described sensations very similar to yours, presumably from nerve damage. I never asked a bunch of questions about it, and now I wish I had after reading this post.If it ever comes up in conversation again, may I share a link to your comment with her?\n \nreply",
      "I lost the tip of my right thumb on a planer, but kept the joint.  I experience similar phenomena. The worst thing is pushing a supermarket shopping cart on a rough surface, the vibrations it sends into my hand are intensely painful.   Also iDevice touchscreens hate my nub.\n \nreply",
      "I could relate to the claims in the article: for the last 6 months I've had soreness and pain in my left index fingertip that has confounded the doctors I've seen about it, and all that's helped is to avoid using it. Perhaps someone here has experienced something similar?When typing I feel pain initially at the fingertip where nail meets skin, which worsens and radiates around to the middle finger side of the fingernail after more use. Even when typing without using the index finger, stretching the finger to keep it away from the keyboard induces some pain after a while. If I cut the nail very short, I think I notice some tenderness and loss of sensation in a spot near the middle of the skin just under the nail edge.I think the pain developed over time while heavily using a split mechanical keyboard (kinesis freestyle edge) with poor typing technique and putting repeated pressure on the tip and side of the finger, but it has not gone away after switching to something more comfortable (kinesis 360). I don't remember any significant injury happening.The only visual sign is that the skin seems strongly attached to the nail near its edge, there is minimal free edge compared to what my other fingernails (which are all short) have. Actually that is somewhat true of the other index finger, but to a much lesser extent. There is nothing apparently abnormal about the skin under the nail but perhaps any issue isn't visible.Interestingly, the pain seems worse when my hands are warmer.X-rays/MRI/ultrasound scans showed nothing abnormal apparently. All my internet searching for an explanation has yielded nothing, hence writing this comment to see if anyone can help.\n \nreply",
      "I\u2019m a violinist (amateur but play regularly). When I have an important note, which is held for a while and needs vibrato, I frequently decide to shift my left hand position so that my middle finger is responsible, rather than the index finger. It feels stronger, easier to nail the intonation (pitch) with precision, and freer to perform the desired type of vibrato. (String players do vibrato by wiggling the left hand finger, which affects the pitch and overtones / oscillation modes of the string.) In fact, I tend to avoid using the index finger on notes that require vibrato.That preference might be explained here, by the precision/strength combination. I tried holding a hammer as described in the author\u2019s hammer exercise, and there\u2019s similarity, though it requires much more weight-holding. The left hand doesn\u2019t hold the weight of the violin (consider a cello or a guitar with shoulder strap), but a little grip strength is required to securely hold down the string, especially with vibrato.Overall, fascinating article. I feel quite motivated to read more on hand anatomy and biomechanics.\n \nreply",
      "As an older competitive gamer, I liked the part of this that describes how on injury or amputation the middle finger will quickly take over the index duties - I\u2019ve noticed over the years that when my \u201ctrigger\u201d finger on a controller is experiencing tendonitis and I have to rest it, that my middle finger performs just as adequately and I barely notice. This has always surprised me.\n \nreply",
      "All those cartoons had it right all along: four fingers per hand is more than enough!\n \nreply",
      "That was unexpectedly hilarious, wow.\n \nreply"
    ],
    "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC2997957/",
    "first_paragraph": ""
  },
  {
    "title": "When did estimates turn into deadlines? (domainanalysis.io)",
    "points": 160,
    "submitter": "alexzeitler",
    "submit_time": "2024-11-19T20:03:10 1732046590",
    "num_comments": 109,
    "comments_url": "https://news.ycombinator.com/item?id=42187506",
    "comments": [
      "I've gone through times when management would treat estimates as deadlines, and were deaf to any sort of reason about why it could be otherwise, like the usual thing of them changing the specification repeatedly.So when those times have occurred I've (we've more accurately) adopted what I refer to the \"deer in the headlights\" response to just about anything non-trivial.  \"Hoo boy, that could be doozy.  I think someone on the team needs to take an hour or so and figure out what this is really going to take.\"  Then you'll get asked to \"ballpark it\" because that's what managers do, and they get a number that makes them rise up in their chair, and yes, that is the number they remember.  And then you do your hour of due diligence, and try your best not to actually give any other number than the ballpark at any time, and then you get it done \"ahead of time\" and look good.Now, I've had good managers who totally didn't need this strategy, and I loved 'em to death.  But for the other numbnuts who can't be bothered to learn their career skills, they get the whites of my eyes.Also, just made meetings a lot more fun.\n \nreply",
      "> I've gone through times when management would treat estimates as deadlines, and were deaf to any sort of reason about why it could be otherwise, like the usual thing of them changing the specification repeatedly.I worked at a place where this management insanity was endemic, which lead to everyone padding all estimates with enough contingency to account for that. Which lad to the design team, and the front-end team, and the backend team, and the QA team, all padding out their estimates by 150 or 200% - to avoid the blame storms they'd seen for missing \"deadlines\".Then the Project managers added those all ups and added 150 - 200%. Then the account managers and sales teams added 150 - 200% to the estimated costs before adding margins and setting prices.Which ended up in literally around 1 million dollars a month to maintain a website which could _easily_ have been handled by a full time team of 8 or 10 decent web and full stack devs. Hell, apart from the 24x7 support requirement, I reckon I know a few great Rails or Django devs who could have done all the work on their own, perhaps with a part time of contracted graphic designer.That all lasted a handful of years, until the client worked out what was going on, and my company management flew the whole thing into the mountain, with ~100 people losing their jobs and their owed entitlements (I was out about $26K that day.)\n \nreply",
      "This is literally the endgame.And the only cure is instead building a company that's tolerant of mistakes while still aspiring to excellence.The one I've worked at which got the closest had a corporate culture that failures were atrributable to processes, while successes were atrributable to individuals/teams.Of course that had its own negative side effects, but on the whole it made the company a lot more honest with itself. And consequently got better work out of everyone.\n \nreply",
      "What helped me was to track Sprint Volatility in addition to Sprint Velocity.  We had our over all capacity, let's say 40 points and that would go up or down some based on people leaving the team, joining the team, etc.  It's just an average of how much a team can get done in a given sprint.  Velocity was gauged as points per person per day.Volatility is how much the sprint changes.  Sure you can pull one 5 pt ticket out and add in a 3 point and 2 point, but if you do that 12 times in a two week sprint, we will not finish the sprint even if total capacity stays under 40 points.I would snapshot the sprint each day, so each day I could see how many tickets got removed/added.  The end result being I could show my manager, look, when volatility is low, we almost always finish the sprint.  When the volatility is high, we don't, it doesn't matter if we are over/under velocity because we don't have the time to properly plan and get clarity on asks.  Have our product team think more than two weeks out and we'll deliver.  That worked to a degree.\n \nreply",
      "In my experience, super large estimates don\u2019t make you look good in the long run, they make you look incompetent. The engineers who are most likely to be under-performers are also those who give super inflated estimates for simple tasks.Maybe this is a good strategy for dealing with people who aren\u2019t going to judge you for delivering slowly, or for managers who don\u2019t know what the fuck is going on. For managers who do, they will see right through this.\n \nreply",
      "So many bold claims in this comment and little to no justification.For what it's worth I've seen pretty much the opposite. I don't know about competent vs. incompetent engineers. But when it comes to experience, I've seen the inexperienced ones giving super low estimates and the experienced people giving larger estimates.\n \nreply",
      "> I've seen the inexperienced ones giving super low estimates and the experienced people giving larger estimatesI have the same anecdotal experience with a possible explanation:Inexperienced engineers often don't see the greater picture or the kind of edge cases that will probably need to be handled ahead of time.  \nI've often had the following type of conversation:Engineer: \"I think that would be a day's work\"Me: \"This will need to interact with team X's package. Have you accounted for time spent interacting with them?\"Engineer: \"Oh no, I guess two days then\"Me: \"Will this account for edge case 1 and 2?\"Engineer: \"Ah yes, I guess it would be three days then\"Me: \"Testing?\"Engineer: \"Maybe let's say a week?\"On the other hand experienced devs might have their judgement clouded by past events:\"Last time we did something with X it blew out by 3 months\" - Ignoring the fact that X is now a solved issue\n \nreply",
      "Ime, as a junior dev/ops person, there is almost always scope creep and adding padding grants you room to account for the new idea your supervisor/ user thought of when being midway into development. As far as I can tell, my supervisor also assumes my estimates should be padded more because sometimes you might need wait on human i/o for longer than planned (holidays/ sick leave/...).\n \nreply",
      "I think general problem on HN is that you can't say something \"bold\" without people going \"nuts\" - especially when it comes to estimating work.In my experience (been hacking since the '90's before it was cool) great developers are great at estimating things. And these are not outliers, all except 1 great developer I've had pleasure of working with over these years has never been \"off\" on estimates by any statistically significant margin. but you say anything like that here on HN and it is heresy.My general opinion is that developers LOVE making everyone believe that software development is somehow \"special\" from many other \"industries\" for the lack of a better word and that we simply cannot give you accurate estimates that you can use to make \"deadlines\" (or better said project plans). and yet most developers (including ones raising hell here and downvoting any comment contrary to \"popular belief\") are basically doing sht that's been done million times before, CRUD here, form there, notification there, event there etc... It is not like we are all going \"oh sht, I wonder how long it'll take to create a sign-up form.\"I think we have (so far) been successful at running this \"scam\" whereby \"we just can't accurately estimate it\" because of course it is super advantageous to us. and WFH has made this even worse in my opinion - given that we can't provide \"accurate estimates\" now we can simply pad them (who dare to question this, it is just an estimate, right? can't hold me to the estimate...) and then chill with our wifes and dogs when sh*t is done 6 weeks earlier :)\n \nreply",
      "It really depends on what you are working on. When I did agency type of work, building things you have built before, from scratch, it's easy to estimate. E.g. some sort of e-commerce website from scratch. On the other hand, working in a large corp, with massive legacy systems, unknown domain knowledge and dependency on other teams, it becomes near impossible. What might have been a 2 hour task in agency, might either be completely impossible during our lifetime unless the whole company was built from scratch or take 2 years. You might need 6 other teams to agree to make some sort of change, and you first might have to convince those teams, and then 2 teams will initially agree to it, then pull out in the last moment, or realize they can't do it.\n \nreply"
    ],
    "link": "https://domainanalysis.io/p/architecture-modernization-execution",
    "first_paragraph": ""
  },
  {
    "title": "Why is Apple Rosetta 2 fast? (2022) (dougallj.wordpress.com)",
    "points": 118,
    "submitter": "fanf2",
    "submit_time": "2024-11-19T21:42:02 1732052522",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=42188407",
    "comments": [
      "Future Arm-based PCs, https://news.ycombinator.com/item?id=42182442  The Arm PC Base System Architecture 1.0 (PC-BSA) specifies a standard hardware system architecture for Personal Computers (PCs) that are based on the Arm 64-bit Architecture. PC system software, for example operating systems, hypervisors, and firmware can rely on this standard system architecture. PC-BSA extends the requirements specified in the Arm BSA.\n \nreply",
      "Post got the big one: Total Store Ordering (TSO).The rest are all techniques in reasonably common use, but unless you have hardware support for x86's strong memory ordering, you cannot get very good x86-on-ARM performance, because it's by no means clear when strong memory ordering matters, and when it doesn't, inspecting existing code - so you have to liberally sprinkle memory barriers around, which really kill performance.The huge and fast L1I/L1D cache doesn't hurt things either... emulation tends cache-intensive.\n \nreply",
      "It's surprising that (AFAIK) Qualcomm didn't implement TSO in the chips they made for the recent-ish Windows ARM machines. If anything they need fast x86 emulation even more than Apple does since Windows has a much longer tail of software support than macOS, there's going to be important Windows apps that stubbornly refuse to support native ARM basically forever.\n \nreply",
      "It's definitely surprising that Qualcomm didn't. Not only does Windows have a longer tail of software to support, but given that the vast majority of Windows machines will continue to be x86-64, there's little incentive to do work to support ARM.With the Mac, Apple told everyone \"we're moving to ARM and that's final.\" With Windows, Microsoft is saying, \"these ARM chips could be cool, what do you think?\" On the Mac, you either got on board or were left behind. Users knew that the future was ARM and bought machines even if there might be some short-term growing pains. Developers knew that the future was ARM and worked hard to support it.But with Windows, there isn't a huge incentive for users to switch to ARM and there isn't an incentive for developers to encourage it. You can say there's some incentive if the ARM chips are better. While Qualcomm's chips are good, the benchmarks aren't really ahead of Intel/AMD and they aren't the power-sipping processors that Apple is putting out.If Apple hadn't implemented TSO, Mac users/developers would still switch because Apple told them to. Qualcomm has to convince users that their chips are worth the short-term pain - and that users shouldn't wait a few years to make the switch when the ecosystem is more mature. That's a much bigger hill to climb.Still, for Qualcomm, they might not even care about losing a little money for 5-10 years if it means they become one of the largest desktop processor vendors for the following 20+ years. As long as they can keep Microsoft's interest in ARM as a platform, they can bide their time.\n \nreply",
      "I wonder if possible Qualcomm doesn\u2019t super care about the long tail of software? Like maybe MS has some stats indicating that a very large percentage of software that they think will be used on these devices is first party, or stuff that reasonably should be expected to be compiled for ARM.How does the windows App Store work anyway, can they guarantee that all the stuff there gets compiled for ARM?Anyway, it is Windows not MacOS. The users expect some rough edges and poor craftsmanship, right?\n \nreply",
      "Qualcomm has been phoning it in in various forms for over a decade, including forcing MS to ship machines that do not really pass windows requirements (like broken firmware support). Maybe it got fixed with recent Snapdragon X, but I won't hold my breath.We're talking about a company that, if certain personal sources are to be believed, started the Snapdragon brand by deciding to cheapen out on memory bandwidth despite feedback that increasing it was critical and leaving the client to find out too late in the integration stage.Deciding that they make better money by not spending on implementing TSO, or not spending transistors on bigger caches, and getting more volume at lower cost, is perfectly normal.\n \nreply",
      "Microsoft's AoT+JiT techniques still pull off impressive performance (90+% in almost every case, 96-99% in the majority).But yes, if they were actually serious about Windows on ARM, they would have implemented TSO in their \"custom\" Qualcomm SQ1/SQ2 chips.\n \nreply",
      "Last time I checked, the default behavior for Microsoft's translation was to pretend that the hardware is doing TSO, and hope it works out. So that should obviously be fast, but occasionally wrong.\n \nreply",
      "They're a decent bit smarter than that but yes their emulation is not quite correct.\n \nreply",
      "On a first order analysis, Qualcomm doesn't want good x64 support, because good x64 support furthers the lifetime of x64, and delays the \"transition\" to ARM. In the final analysis, I doubt that is an economically rational strategy, because even if there is to be a transition away from x64, you need a good legacy and migration story. And I doubt such a transition will happen in the next 10 years, and certainly not spurred by anything in Microsoft land.So maybe it's rational after all, because they know these Windows ARM products will never succeed, so they're just saving themselves the cost/effort of good support.\n \nreply"
    ],
    "link": "https://dougallj.wordpress.com/2022/11/09/why-is-rosetta-2-fast/",
    "first_paragraph": "Rosetta 2 is remarkably fast when compared to other x86-on-ARM emulators. I\u2019ve spent a little time looking at how it works, out of idle curiosity, and found it to be quite unusual, so I figured I\u2019d put together my notes.My understanding is a bit rough, and is mostly based on reading the ahead-of-time translated code, and making inferences about the runtime from that. Let me know if you have any corrections, or find any tricks I\u2019ve missed.Rosetta 2 translates the entire text segment of the binary from x86 to ARM up-front. It also supports just-in-time (JIT) translation, but that is used relatively rarely, avoiding both the direct runtime cost of compilation, and any indirect instruction and data cache effects.Other interpreters typically translate code in execution order, which can allow faster startup times, but doesn\u2019t preserve code locality.[Correction: an earlier version of this post said that every ahead-of-time translated instruction was a valid entry point. While I still believe "
  },
  {
    "title": "Apple Confirms Zero-Day Attacks Hitting macOS Systems (securityweek.com)",
    "points": 33,
    "submitter": "fortran77",
    "submit_time": "2024-11-20T00:26:30 1732062390",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=42189581",
    "comments": [
      "Interesting that they\u2019re mentioned as only being exploited on Intel. Has anyone seen whether that\u2019s because the attacker only targeted that platform or is it actually stopped by something like pointer protection?\n \nreply",
      "Doesn't seem to completely line up that they're rushing out iOS updates for something they're saying they've only confirmed on Intel cpus.Unless they're assuming it's exploitable on Apple Silicon as well, or are being extra careful just in case.\n \nreply",
      "> Unless they're... being extra careful just in case.That's where my money is.\n \nreply",
      "There must be millions of Intel Macs still around. Why wouldn\u2019t they update it?\n \nreply",
      "The parent comment said that Apple is rushing iOS updates. iOS is the operating system for iPhones which use Apple Silicon rather than Intel processors.",
      "It might just be that the only machines they have information about are Intel? It doesn't say how many data points they have, but if it's only a handful then it's not at all surprising.\n \nreply"
    ],
    "link": "https://www.securityweek.com/apple-confirms-zero-day-attacks-hitting-intel-based-macs/",
    "first_paragraph": ""
  },
  {
    "title": "Reexamination of 1975 \"Edmund Fitzgerald\" Storm Using Today's Technology [pdf] (noaa.gov)",
    "points": 4,
    "submitter": "jandrewrogers",
    "submit_time": "2024-11-10T20:09:52 1731269392",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.glerl.noaa.gov/pubs/fulltext/2006/20060016.pdf",
    "first_paragraph": ""
  },
  {
    "title": "The Analog Thing: Analog Computing for the Future (the-analog-thing.org)",
    "points": 70,
    "submitter": "cgeier",
    "submit_time": "2024-11-19T17:09:28 1732036168",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=42185715",
    "comments": [
      "In my upper-division analog electronics class (the hard one), our lab project throughout the quarter was to build an analog computer that simulated the physics of a bouncing ball. Physical variables of the system were adjustable (gravity constant, coefficient of restitution, etc), and the ball was \"released\" by pressing a button. The output was viewed on an oscilloscope.One of the hardest 10 weeks of my life, but also one of the most rewarding. Our team was one of the few that actually got it working in the end. I had to custom-make a gigantic breadboard to hold the entire circuit.Today I still work in hardware, but mostly with digital circuits. While my analog knowledge has decayed over the last decade, that project and it's success gives me great confidence any time I have to deal with the domain.If you want to take a look, here's a pretty similar project: https://www.analogmuseum.org/english/examples/bouncing_ball_...\n \nreply",
      "Hey, I made something like this a couple months ago! (Except it's more like \"Tennis for Two\", so you also hit the ball in the X direction, and there's another button to hit it back in the other. I didn't have any space or potentiometers left to set the gravity, but it wouldn't be difficult.)I also learned heaps! (Including after a few weeks when the circuit stopped working properly because one of the relays started to work just a little slower than another one, heh.) If anyone's interested, https://blog.qiqitori.com/2024/08/implementing-tennis-for-tw...\n \nreply",
      "My version of this was a 10-week discrete RF circuits course in graduate school. We had to build a fully functional GHz transceiver out of small FR4 PCBs (< quarter wavelength) and throw-away leaded BJT transistors. Neither were suitable for GHz circuits, so the course was hard by design. I learned so much and developed an intuition for electromagnetics that I still carry 20 years later.\n \nreply",
      "> Today I still work in hardware, but mostly with digital circuits. While my analog knowledge has decayed over the last decade, that project and it's success gives me great confidence any time I have to deal with the domain.Do you think about the analog qualities of your traces when laying things out? If so then the course was well taken.In my observations I've found that too many digital engineers assume a differential pair will save them without actually fixing the impedance and parasitic issues. Particularly as the timings of things become so much more precise analog is so important. People forget that a digital circuit is just an analog one under the covers.\n \nreply",
      "Did the mathematical model being used have a differentiable heigh function? I\u2019m imagining it would be the simplest if it didn\u2019t but that could cause problems in the electronics.Also what components did you have access to, just op amps?\n \nreply",
      "Just op-amps and FETs for the active components. The design from my memory was:- To get position, 2 integrators were applied to an adjustable voltage representing gravity.- The FETs were used to set initial states of the integrators.- A comparator used to detect the table (y=0), flip the velocity and apply a scaling factor for restitutionThe math was actually quite simple given its just the standard velocity equations \u2014 the challenge was in handling state changes in the electronics.I looked around a little more and this video is a very close replica of what we built: https://www.youtube.com/watch?v=qt6RVrmvh-o\n \nreply",
      "There\u2019s no better introduction to signals and systems than a modular synthesizer IMO - the combination of tactility and audibility for multi-sensory learning is so great at building intuition - and more importantly, excitement! - for signal processing.This looks like a cool project in the same spirit!\n \nreply",
      "I agree. I highly recommend [0] Moritz Klein's channel. amazing explanations and learning effect.[0] https://youtube.com/@MoritzKlein0/videos\n \nreply",
      "I was going to say the same - does this follow 1V/octave standard, and is this available in Eurorack format? ;-)\n \nreply",
      "Cool, I was thinking about the other way around, using an analog computer to build synthesizers.\n \nreply"
    ],
    "link": "https://the-analog-thing.org/",
    "first_paragraph": "award-winning \u2022 portable \u2022 open source \u2022 eye-opening THE ANALOG THING (THAT) is a high-quality, low-cost, open-source, and not-for-profit cutting-edge analog computer. THAT allows modeling dynamic systems with great speed, parallelism, and energy efficiency. Its use is intuitively interactive, experimental, and visual. It bridges the gap between hands-on practice and mathematical theory, integrating naturally with design and engineering practices such as speculative trial-and-error exploration and the use of scale models.  Dynamic system modeling on THAT can serve a variety of valuable purposes. It may help understand what is (models of), or it may help bring about what should be (models for). It may be used to explain in educational settings, to imitate in gaming, to predict in the natural sciences, to control in engineering, or it may be pursued for the pure joy of it!  Analog computing is one of three major computational paradigms (analog, digital and quantum). As digital computing "
  },
  {
    "title": "El Capitan: New supercomputer is the fastest (ieee.org)",
    "points": 62,
    "submitter": "rbanffy",
    "submit_time": "2024-11-19T20:52:48 1732049568",
    "num_comments": 75,
    "comments_url": "https://news.ycombinator.com/item?id=42187967",
    "comments": [
      "> El Capitan, housed at Lawrence Livermore National Laboratory in Livermore, Calif., can perform over 2700 quadrillion operations per second at its peak. The previous record holder, Frontier, could do just over 2000 quadrillion peak operations per second.> El Capitan uses AMD\u2019s MI300a chip, dubbed an accelerated processing unit, which combines a CPU and GPU in one package. In total, the system boasts 44,544 MI300As, connected together by HPE\u2019s Slingshot interconnects.Seems like a nice win for AMD.\n \nreply",
      "> Seems like a nice win for AMDYep! They've been part of the Exascale project for a long time, and it's good to see their commitment on HPC actually succeeded unlike Intel's during the same time period.\n \nreply",
      "Fun facts, FFT was discovered back in 1965 based on the urgent necessity of discovering and detecting illegal nuke testing activities, just two years after the Partial Test Ban Treaty (PTBT) was signed in 1963 [1].The first sentence statement in the article mentioning that United States and other nuclear powers committed to the Comprehensive Nuclear-Test-Ban Treaty in 1965 is wrong since the treaty was only signed in 1996 not in 1965 [2].[1] The Algorithm That Almost Stopped The Development Of Nuclear Weapons:https://www.iflscience.com/the-algorithm-that-almost-stopped...[2] The Comprehensive Nuclear-Test-Ban Treaty:https://www.ctbto.org/our-mission/the-treaty\n \nreply",
      "Another fun fact, the priority for detecting nuclear testing led to seisometers all over the planet.  So the detection the exact position and nature of any disturbance on the planet became radically better.  This was quite the boon to anyone interested in earthquakes, not only can the earth quake be detected in 2D, but accurately in 3D.  The number and accuracy is enough you can see where on each fault is, the thickness of the crust, and the outline of subduction zones in 3d.  Pretty crazy to see enough detail to see where plates enter the mantle and melts.Said sensors can also track sonic booms from secret supersonic planes, but governments don't like to talk about that.\n \nreply",
      "This is great but I absolutely love that poster of el capitan on the supercomputer racks ! Also TIL there is a list of top500 at https://www.top500.org/lists/top500/2024/11/\n \nreply",
      "I've always loved these charts. The Numerical Wind Tunnel, #1 in 1993, achieved 124.2 gigaflops on the Linpack benchmark.In comparison, the iPhone 15 Pro Max cellphone, released in 2023, delivers approximately 2150 gigaflops.I once drew the chart backwards. I think my PC in 2013 would have been the fastest on Earth in 1990. And faster than every computer combined in about 1982.[0][0] might not be accurate\n \nreply",
      "That's a pretty standard Cray feature for systems larger than a few cabinets. El Capitan has the landscape, Hopper at NERSC had a photo of Grace Hopper, Aurora at ANL has a creamy gradient reminiscent of the Borealis, and on and on. Gives them a bit of character beyond the bad-ass Cray label on the doors.\n \nreply",
      "I fail to understand how these nuclear bomb simulations require so much compute power.Are they trying to model every single atom?Is this a case where the physicists in charge get away with programming the most inefficient models possible and then the administration simply replies \"oh I guess we'll need a bigger supercomputer\"\n \nreply",
      "It literally requires simulating each subatomic particle, individually. The increases of compute power have been used for twin goals of reducing simulation time (letting you run more simulations) and to increase the size and resolution.The alternative is to literally build and detonate a bomb to get empirical data on given design, which might have problems with replicability (important when applying the results to rest of the stockpile) or how exact the data is.And remember that there is more than one user of every supercomputer deployed at such labs, whether it be multiple \"paying\" jobs like research simulations, smaller jobs run to educate, test, and optimize before running full scale work, etc.AFAIK for considerable amount of time, supercomputers run more than one job at a time, too.\n \nreply",
      "> It literally requires simulating each subatomic particle, individually.Citation needed.1 gram of Uranium 235 contains 2e21 atoms, which would take 15 minutes for this supercomputer to count.\"nuclear bomb simulations\" do not need to simulate every atom.I speculate that there will be some simulations at the subatomic scale, and they will be used to inform other simulations of larger quantities at lower resolutions.https://www.wolframalpha.com/input?i=atoms+in+1+gram+of+uran...\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/supercomputer-for-nukes",
    "first_paragraph": "See our latest special report, \u201cReinventing Invention: Stories From Innovation\u2019s Edge\u201d \u2192El Capitan debuted on the Top500 supercomputing list at #1Dina Genkina is the computing and hardware editor at IEEE SpectrumThe National Nuclear Security Administration's newest supercomputer, El Capitan, is the fastest known computer in the world.In 1965, the United States and other nuclear powers committed to the Comprehensive Nuclear-Test-Ban Treaty, which prohibited nuclear tests. The National Nuclear Security Administration (NNSA), a successor to the Manhattan Project, now tests nukes only in simulation. To that end, the NNSA yesterday unveiled the world\u2019s fastest supercomputer to air in its mission to maintain a safe, secure, and reliable nuclear stockpile. El Capitan was announced yesterday at the SC Conference for supercomputing in Atlanta, Georgia, and it debuted at #1 in the newest Top500 list, a twice-yearly ranking of the world\u2019s highest performing supercomputers. El Capitan, housed at L"
  },
  {
    "title": "Starship IFT-6 Livestream (liftoff at 4pm CT) (spacex.com)",
    "points": 71,
    "submitter": "grecy",
    "submit_time": "2024-11-19T21:26:18 1732051578",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=42188247",
    "comments": [
      "Cost cutting in effect at SpaceX: Falcon Heavy used a Tesla Roadster as their test payload, Starship only has a banana.\n \nreply",
      "I think it's an homage to previous chimp astronauts.https://www.youtube.com/watch?v=fjCX5F74zIMKnight E6.https://en.wikipedia.org/wiki/Ham_(chimpanzee)\n \nreply",
      "No shots of the banana yet post launch. Did it rip away from the strings?(Up until I typed this I hadn\u2019t considered they might use an artificial banana.)\n \nreply",
      "Banana is on screen at T+00:24\n \nreply",
      "With 5.2 million people watching. This flight probably broke the world record for most people watching a single specific banana, ever.\n \nreply",
      "unfortunately they had to scrap the booster Catch, due to undisclosed factors.\n \nreply",
      "I would love to see the dashboard that the team that made the decision was looking at.I'd be interested to hear speculation by people who know about this as to what they think went wrong. Was it off course? Did the engines not relight in time? Did it not have enough fuel?\n \nreply",
      "Honestly showing that you can re-target it in flight is extremely impressive too. Like, it still soft-landed in water, it didn't blow up.\n \nreply",
      "Hah. I mean it did blow up (the booster), but not due to impact from a failed soft landing. The soft landing succeeded, then it blew up.\n \nreply",
      "The upper stage was also on fire!\n \nreply"
    ],
    "link": "https://www.spacex.com/launches/mission/?missionId=starship-flight-6#1",
    "first_paragraph": "\n        On its flight to the International Space Station, Dragon executes a series of burns that position the vehicle\n        progressively closer to the station before it performs final docking maneuvers, followed by pressurization of\n        the vestibule, hatch opening, and crew ingress.\n      \n        On its flight to the International Space Station, Dragon executed a series of burns that positioned the vehicle\n        progressively closer to the station before it performed final docking maneuvers, followed by pressurization of\n        the vestibule, hatch opening, and crew ingress.\n      \n        On its flight to the International Space Station, Dragon executes a series of burns that position the vehicle\n        progressively closer to the station before it performs final docking maneuvers, followed by pressurization of\n        the vestibule, hatch opening, and crew ingress.\n      \n        On its flight to the International Space Station, Dragon executed a series of burns that po"
  }
]