[
  {
    "title": "A Love Letter to FreeBSD (tara.sh)",
    "points": 170,
    "submitter": "rbanffy",
    "submit_time": "2025-11-30T22:05:07 1764540307",
    "num_comments": 90,
    "comments_url": "https://news.ycombinator.com/item?id=46100892",
    "comments": [
      "As much as I love FreeBSD, the release schedule is a real challenge in production: each point release is only supported for about three months. Since every release includes all ports and packages, you end up having to recertify your main application constantly.Compare this to RedHat: yes, a paid subscription is expensive, but RedHat backports security fixes into the original code, so open source package updates don\u2019t break your application, and critical CVEs are still addressed.Microsoft, for all its faults, provides remarkable stability by supporting backward compatibility to a sometimes ridiculous extent.Is FreeBSD amazing, stable, and an I/O workhorse? Absolutely: just ask Netflix. But is it a good choice for general-purpose, application-focused (as opposed to infrastructure-focused) large deployments? Hm, no ?reply",
      "What you measured is just the overlap between minor releases of the same major release. It helps to think of them as service packs if you want a MicroSoft analogy. So each minor release is supported until it has be surplanted for 3 months by a new one on the same major release line or the whole major release line goes end of life.reply",
      "Sure, but the point is that each minor release contains changes in all third party open source packages/ports by taking them to the head version.Open source packages often include breaking changes, all but guaranteeing your application to fail. With (a paid version of) RedHat Linux, RedHat modifies the open source packages to remediate CVEs by modifying the original version.reply",
      "26 years of FreeBSD and counting...IIRC in about 99 I got sick of Mandrake and RH RPM deps hell and found FreeBSD 3 CD in a Walnut creek book. Ports and BSD packages were a revelation, to say nothing of the documentation which still sets it apart from the haphazard Linux.The comment about using a good SERVER mobo like supermicro is on point --- I managed many supermicro fbsd colo ack servers for almost 15 years and those boards worked well with it.Currently I run FreeBSD on several home machines including old mac minis repurposed as media machines throughout the house.They run kodi + linux brave and with that I can stream anything like live sports.Also OpenBSD for one firewall and PFSense (FreeBSD) for another.reply",
      "Lovely stuff. The industry would be so much better off if the family of BSDs had more attention and use.I run some EVE Online services for friends. They have manual install steps for those of use not using containers. Took me half a day to get the stack going on FBSD and that was mostly me making typos and mistakes. So pleased I was able to dodge the \u201cdocker compose up\u201d trap.reply",
      "Can you explain why \"Docker compose\" is a trap?reply",
      "It really is amazing how much success Linux has achieved given its relatively haphazard nature.FreeBSD always has been, and always will be, my favorite OS.It is so much more coherent and considered, as the post author points out.  It is cohesive; whole.reply",
      "> It really is amazing how much success Linux has achieved given its relatively haphazard nature.That haphazard nature is probably part of the reason for its success, since it allowed for many alternative ways of doing things being experimented in parallel.reply",
      "Linux has turned haphazardry into a strength. This is impressive.I prefer FreeBSD.reply",
      "I like the haphazardry but I think systemd veered too far into dadaism.reply"
    ],
    "link": "https://www.tara.sh/posts/2025/2025-11-25_freebsd_letter/",
    "first_paragraph": "Dear FreeBSD,I\u2019m still the new person here, learning your ways, stumbling over the occasional quirk, smiling when I\nfind the small touches that make you different. You remind me of what computing felt like before the\nnoise. Before hype cycles and performance theatre. Before every tool needed a plugin system and a logo.\nYou are coherent. You are deliberate. You are the kind of system that doesn\u2019t have to shout to belong.You carry the quiet strength of the greats, like a mainframe humming in a locked room, not chasing\nattention, just doing its work, year after year. Your base system feels like it was built by people who\ncared about the whole picture, not just the pieces. Your boot environments are like an old IBM i\u2019s \u201cside\nA / side B\u201d IPL, a built-in escape hatch that says, we\u2019ve thought ahead for you. You could be, you\nshould be, the open-source mainframe: aligned with hardware lifecycles of three to five years or more,\nbuilt for long-term trust, a platform people bet their uptime on. Y"
  },
  {
    "title": "Is America's jobs market nearing a cliff? (economist.com)",
    "points": 30,
    "submitter": "harambae",
    "submit_time": "2025-12-01T00:58:32 1764550712",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=46102202",
    "comments": [
      "From my experience, it's grim at the moment for software developer jobs. I got laid off in August and it's been rough. I'm in my early 30s so I can't compare it to 2008, but I've been laid off before and I've never seen it this bad.reply",
      "It's grim everywhere, for everything, all at once. I haven't been able to find work as a graphic designer, motion designer, web designer, web developer, software developer, and a large variety of retail jobs. Been on the job hunt since May, all I've been able to find is a part time position at The Home Depot.reply",
      "https://archive.is/NvSXc.Also, shocking to see no mention of the investment thesis, let alone critique of it.reply",
      "The Economist peddling doom n gloom IN THIS ECONOMY?reply",
      "Ftfa (the part I could read) \u201c growth is buoyed by an exuberant stockmarket and artificial-intelligence investment, while ordinary Americans languish\u201dBlack Friday sales set records and it not even cyber Monday. If Americans are languishing then shouldn\u2019t holiday spending be down?reply",
      "Black Friday did not set records. Bloomberg stated that it was up 4.1%, but that was not inflation adjusted. So it was just slightly higher than flat.Not necessarily a bad thing\u2026but not great either.https://www.bloomberg.com/news/articles/2025-11-29/black-fri...reply",
      "Black friday is a datapoint. But maybe people are deferring purchases to these sale periods. What proportion of goods were luxury vs. not.Also poor people can get into debt they are still poor. Maybe they can afford a nintendo switch but not afford to raise a family.reply",
      "The only things I bought on Black Friday were things I would normally have bought but just waited a few weeks/months to get.reply",
      "Consumers in the top 10% of the income distribution accounted for 49.2% of total spending, per Bloomberg. If anything, in my opinion, this strengthens the k-shaped economic growth stat that the article mentions.reply",
      "Yup, only the rich are powering this economy now. That bodes poorly for the country\u2019s stability long term.reply"
    ],
    "link": "https://www.economist.com/finance-and-economics/2025/11/30/is-americas-jobs-market-nearing-a-cliff",
    "first_paragraph": ""
  },
  {
    "title": "Algorithms for Optimization [pdf] (algorithmsbook.com)",
    "points": 68,
    "submitter": "Anon84",
    "submit_time": "2025-11-30T23:21:32 1764544892",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=46101492",
    "comments": [
      "Can anyone provide a comparison of this book to Nocedal and Wright's book?reply",
      "This book as well as Kochenderfer's earlier book \"Decision Making Under Uncertainty\"[0] are some of my favorite technical books (and I wouldn't be surprised to find his newest book, \"Algorithms for Decision Making\", also fell into this category).The algorithm descriptions are clear, the visualizations are great, and, as someone who does a lot of ML work, they cover a lot of (important) topics beyond just what is covered in your standard ML book. This is especially refreshing if you're looking for thinking around optimization that is not just gradient descent (which has been basically the only mainstream approach to optimization in ML for two decades now).I've known a few people to complain that the code examples are in Julia, but, as someone who doesn't know Julia, if you have experience doing quantitative programming at all it should be trivial convert the Julia examples to your favorite language for implementation (and frankly, I'm a bit horrified that so many people \"smart\" people interested in these sorts of topics seem trapped into reasoning in one specific language).Optimization is such a rich field and should be of interest to any computer scientist who would describe themselves as \"interested in solving hard problems\" rather than just applying a well known technique to a specific class of hard problems.0. https://web.stanford.edu/group/sisl/public/dmu.pdfreply"
    ],
    "link": "https://algorithmsbook.com/optimization/files/optimization.pdf",
    "first_paragraph": ""
  },
  {
    "title": "Writing a good Claude.md (humanlayer.dev)",
    "points": 313,
    "submitter": "objcts",
    "submit_time": "2025-11-30T17:56:43 1764525403",
    "num_comments": 97,
    "comments_url": "https://news.ycombinator.com/item?id=46098838",
    "comments": [
      "> Claude often ignores CLAUDE.md> The more information you have in the file that's not universally applicable to the tasks you have it working on, the more likely it is that Claude will ignore your instructions in the fileClaude.md files can get pretty long, and many times Claude Code just stops following a lot of the directions specified in the fileA friend of mine tells Claude to always address him as \u201cMr Tinkleberry\u201d, he says he can tell Claude is not paying attention to the instructions on Claude.md, when Claude stops calling him \u201cMr Tinkleberry\u201d consistentlyreply",
      "That\u2019s hilarious and a great way to test this.What I\u2019m surprised about is that OP didn\u2019t mention having multiple CLAUDE.md files in each directory, specifically describing the current context / files in there. Eg if you have some database layer and want to document some critical things about that, put it in \u201csrc/persistence/CLAUDE.md\u201d instead of the main one.Claude pulls in those files automatically whenever it tries to read a file in that directory.I find that to be a very effective technique to leverage CLAUDE.md files and be able to put a lot of content in them, but still keep them focused and avoid context bloat.reply",
      "From the article:> We recommend keeping task-specific instructions in separate markdown files with self-descriptive names somewhere in your project. Then, in your CLAUDE.md file, you can include a list of these files with a brief description of each, and instruct Claude to decide which (if any) are relevant and to read them before it starts working.I've been doing this since the early days of agentic coding though I've always personally referred to it as the Table-of-Contents approach to keep the context window relatively streamlined. Here's a snippet of my CLAUDE.md file that demonstrates this approach:  # Documentation References\n\n  - When adding CSS, refer to: docs/ADDING_CSS.md\n  - When adding assets, refer to: docs/ADDING_ASSETS.md\n  - When working with user data, refer to: docs/STORAGE_MANAGER.md\n\n\nFull CLAUDE.md file for reference:https://gist.github.com/scpedicini/179626cfb022452bb39eff10b...reply",
      "I have also done this, but my results are very hit or miss. Claude rarely actually reads the other documentation files I point it to.reply",
      "I think the key here is \u201cif X then Y syntax\u201d - this seems to be quite effective at piercing through the \u201cprobably ignore this\u201d system message by highlighting WHEN a given instruction is \u201chighly relevant\u201dreply",
      "I don't get the point. Point it at your relevent files ask it to review discuss the update refine it's understanding and then tell it to go.I have found that more context comments and info damage quality on hard problems.I actually for a long time now have two views for my code.1. The raw code with no empty space or comments.\n2. Code with commentsI never give the second to my LLM. The more context you give the lower it's upper end of quality becomes. This is just a habit I've picked up using LLMs every day hours a day since gpt3.5 it allows me to reach farther into extreme complexity.I suppose I don't know what most people are using LLMs for but the higher complexity your work entails the less noise you should inject into it. It's tempting to add massive amounts of xontext but I've routinely found that fails on the higher levels of coding complexity and uniqueness. It was more apparent in earlier models newer ones will handle tons of context you just won't be able to get those upper ends of quality.Compute to informatio ratio is all that matters. Compute is capped.reply",
      "> I have found that more context comments and info damage quality on hard problems.There can be diminishing returns, but every time I\u2019ve used Claude Code for a real project I\u2019ve found myself repeating certain things over and over again and interrupting tool usage until I put it in the Claude notes file.You shouldn\u2019t try to put everything in there all the time, but putting key info in there has been very high ROI for me.Disclaimer: I\u2019m a casual user, not a hardcore vibe coder. Claude seems much more capable when you follow the happy path of common projects, but gets constantly turned around when you try to use new frameworks and tools and such.reply",
      "Setting hooks has been super helpful for me, you can reject certain uses of tools (don\u2019t touch my tests for this session) with just simple scripting code.reply",
      "Git lint hook has been key. No matter how many times I told it, it lints randomly. Sometimes not at all. Sometime before rubbing tests (but not after fixing test failures).reply",
      "Agreed, I don't love the CLAUDE.md that gets autogenerated. It's too wordy for me to understand and for the model to follow consistently.I like to write my CLAUDE.md directly, with just a couple paragraphs describing the codebase at a high level, and then I add details as I see the model making mistakes.reply"
    ],
    "link": "https://www.humanlayer.dev/blog/writing-a-good-claude-md",
    "first_paragraph": "Kyle Mistele \u00b7 November 25, 2025 \u00b7 < 10 min readNote: this post is also applicable to AGENTS.md, the open-source equivalent of CLAUDE.md for agents and harnesses like OpenCode, Zed, Cursor and Codex.LLMs are stateless functions. Their weights are frozen by the time they're used for inference, so they don't learn over time. The only thing that the model knows about your codebase is the tokens you put into it.Similarly, coding agent harnesses such as Claude Code usually require you to manage agents' memory explicitly. CLAUDE.md (or AGENTS.md) is the only file that by default goes into every single conversation you have with the agent.This has three important implications:Since Claude doesn't know anything about your codebase at the beginning of each session, you should use CLAUDE.md to onboard Claude into your codebase. At a high level, this means it should cover:But the way you do this is important! Don't try to stuff every command Claude could possibly need to run in your CLAUDE.md fil"
  },
  {
    "title": "Advent of Code 2025 (adventofcode.com)",
    "points": 721,
    "submitter": "vismit2000",
    "submit_time": "2025-11-30T13:07:15 1764508035",
    "num_comments": 249,
    "comments_url": "https://news.ycombinator.com/item?id=46096337",
    "comments": [
      "> You don't need a computer science background to participate - just a little programming knowledge and some problem solving skills will get you pretty far.Every time I see this I wonder how many amateur/hobbyist programmers it sets up for disappointment. Unless your definition of \u201cpretty far\u201d is \u201ca small number of the part ones\u201d, it\u2019s simply not true.reply",
      "Someone else in the thread lamented the problems as \"too easy\" and I wondered what world I was living in.reply",
      "The group of people for which the problems are \"too easy\" is probably quite small.According to Eric last year (https://www.reddit.com/r/adventofcode/comments/1hly9dw/2024_...) there were 559 people that had obtained all 500 stars. I'm happy to be one of them.The actual number is going to be higher as more people will have finished the puzzles since then, and many people may have finished all of the puzzles but split across more than one account.Then again, I'm sure there's a reasonable number of people who have only completed certain puzzles because they found someone else's code on the AoC subreddit and ran that against their input, or got a huge hint from there without which they'd never solve it on their own. (To be clear, I don't mind the latter as it's just a trigger for someone to learn something they didn't know before, but just running someone else's code is not helping them if they don't dig into it further and understand how/why it works.)There's definitely a certain specific set of knowledge areas that really helps solve AoC puzzles. It's a combination of classic Comp Sci theory (A*/SAT solvers, Dijkstra's algorithm, breadth/depth first searches, parsing, regex, string processing, data structures, dynamic programming, memoization, etc) and Mathematics (finite fields and modular arithmetic, Chinese Remainder Theorem, geometry, combinatorics, grids and coordinates, graph theory, etc).Not many people have all those skills to the required level to find the majority of AoC \"easy\". There's no obvious common path to accruing this particular knowledge set. A traditional Comp Sci background may not provide all of the Mathematics required. A Mathematics background may leave you short on the Comp Sci theory front.My own experience is unusual. I've got two separate bachelors degrees; one in Comp Sci and one in Mathematics with a 7 year gap between them, those degrees and 25+ years of doing software development as a job means I do find the vast majority of AoC quite easy, but not all of it, there are still some stinkers.Being able to look at an AoC problem and think \"There's some algorithm behind this, what is it?\" is hugely helpful.The \"Slam Shuffle\" problem (2019 day 22) was a classic example of this that sticks in my mind. The magnitude of the numbers involved in part 2 of that problem made it clear that a naive iteration approach was out of the question, so there had to be a more direct path to the answer.As I write the code for part 1 of any problem I tend to think \"What is the twist for part 2 going to be? How is Eric going to make it orders of magnitude harder?\" Sometimes I even guess right, sometimes it's just plain evil.reply",
      "It's just bluffing, lying. People lie to make others think they're hot shit. It's like the guy in school who gets straight A's and says he never studies. Yeah I'll bet.reply",
      "They... sort of are though? A year or two ago I just waited until the very last problem, which was min-cut. Anybody with a computer science education who has seen the prompt Proof. before should be able to tackle this one with some effort, guidance, and/or sufficient time. There are algorithms that don't even require all the high-falutin graph theory.I don't mean to say my solution was good, nor was it performant in any way - it was not, I arrived at adjacency (linked) lists - but the problem is tractable to the well-equipped with sufficient headdesking.Operative phrase being \"a computer science education,\" as per GGP's point. Easy is relative. Let's not leave the bar on the floor, please, while LLMs are threatening to hoover up all the low hanging fruit.reply",
      "Got to agree. I'm even surprised at just how little progress many of my friends and ex-colleagues over the years make given that they hold down reasonable developer jobs.reply",
      "My experience has been \"little progress\" is related to the fact that, while AoC is insanely fun, it always occurs during a time of year when I have the least free time.Maybe when I was in college (if AoC had existed back then) I could have kept pace, but if part of your life is also running a household, then between wrapping up projects for work, finalizing various commitments I want wrapped up for the year, getting together with family and friends for various celebrations, and finally travel and/or preparing your own house for guests, I'm lucky if I have time to sit down with a cocktail and book the week before Christmas.Seeing the format changed to 12 days makes me think this might be the first time in years I could seriously consider doing it (to completion).reply",
      "I think this has a lot more to do with time commitment. Once the problems take more than ~1 hour I tend to stop because I have stuff to do, like a job that already involves coding.reply",
      "Why try any more? There are so many fucking frauds in this field.reply",
      "I have a EE background not CS and haven't had too much trouble the last few years.  I'm not aiming to be on the global leader board though. I think that with good problem solving skill, you should be able to push through the first 10 days most years.  Some years were more front loaded though.reply"
    ],
    "link": "https://adventofcode.com/2025/about",
    "first_paragraph": "Hi!  I'm Eric Wastl. I make Advent of Code.  I hope you like it!  I also make lots of other things.  I'm on Bluesky, Mastodon, and GitHub.Advent of Code is an Advent calendar of small programming puzzles for a variety of skill levels that can be solved in any programming language you like. People use them as interview prep, company training, university coursework, practice problems, a speed contest, or to challenge each other.You don't need a computer science background to participate - just a little programming knowledge and some problem solving skills will get you pretty far. Nor do you need a fancy computer; every problem has a solution that completes in at most 15 seconds on ten-year-old hardware.If you'd like to support Advent of Code, you can do so indirectly by helping to [Shareon\n  Bluesky\nTwitter\nMastodon] it with others or directly via AoC++.If you get stuck, try your solution against the examples given in the puzzle; you should get the same answers.  If not, re-read the desc"
  },
  {
    "title": "Bricklink suspends Marketplace operations in 35 countries (jaysbrickblog.com)",
    "points": 63,
    "submitter": "makeitdouble",
    "submit_time": "2025-11-30T22:53:08 1764543188",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=46101263",
    "comments": [
      "Greenland is an unusual entry on the list given the nature of Lego as a firm.reply",
      "I get why for some of these countries, but Brazil for instance doesn't look like complicated situation or a small market in any shape of form ?Is anyone finding relevant political or regulatory patterns in the country list ?Direct link to the list: https://www.bricklink.com/help.asp?helpID=2687reply",
      "Imports into Brazil are pretty complicated, but I don\u2019t know why you\u2019d shut down an existing operation.reply",
      "That's not Lego's problem, but the individual traders on Bricklink.reply",
      "Some really big/rich markets on the list (Brazil, India, ME..).I don't think LEGO is big in most of those countries (at least not in India), so they might be trying to slow down the secondary market in order to grow sales for new products.reply",
      ">To put this into perspective, the total combined population of these countries exceed 2.5 billion, or just about 30% of Earth\u2019s population which is wild.Doesn't look like anybody can make 35% of their revenue from those countries though, does it.reply",
      "You'd be surprised where Lego buyers from bricklink are from. When I was active there I got sales from just about all over the world.reply",
      "Maybe not but it does include some countries with very large economies.reply",
      "Is this due to the same payment processor issue that was impacting Steam-PayPal users earlier this year? https://news.ycombinator.com/item?id=44891570reply",
      "A reminder that danish company LEGO took the concept from a british psychologist  who later committed suicide in the 1950s due to financial issues, and they only later paid out his descendants for rights to the product in the 1980\u2019s in order to legitimise their ability to sue other companies making lego-like products.reply"
    ],
    "link": "https://jaysbrickblog.com/news/bricklink-suspends-marketplace-operations-in-35-countries/",
    "first_paragraph": ""
  },
  {
    "title": "Advent of Sysadmin 2025 (sadservers.com)",
    "points": 13,
    "submitter": "lazyant",
    "submit_time": "2025-12-01T01:17:28 1764551848",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=46102347",
    "comments": [
      "Ooh, this sounds interesting.> Sign up for a free accountHahaha, no.  Closes tabreply",
      "well advent of code also needs an accountreply",
      "how do you want it to work? do you even sysadmin?reply"
    ],
    "link": "https://sadservers.com/advent",
    "first_paragraph": "The Advent of Sysadmin is a 12-day Advent calendar of Linux and DevOps challenges of different difficulties that runs from December 1st to December 12th.Each day there will be a scenario that you can solve to earn points (Easy = 1 point, Medium = 2 points, Hard = 5 points).Sign up for a free account (needed to keep track of your progress) and start solving the scenarios!"
  },
  {
    "title": "Grokipedia Is the Antithesis of Wikipedia (404media.co)",
    "points": 23,
    "submitter": "surprisetalk",
    "submit_time": "2025-12-01T00:22:31 1764548551",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=46101949",
    "comments": [
      "I've been reading a lot of Don Delillo lately and so I wanted to see how Grokipedia page on him fares.I found the \"Critiques of elitism\" section and noticed this sentence:\"Reviews of Mao II (1991), for instance, highlighted the novel's focus on a performance artist protagonist as emblematic of this tendency, with detractors accusing DeLillo of prioritizing esoteric concerns over relatable human experiences, thereby catering to an academic or literary insider audience.\"But Mao II does not have a performance artist as the protagonist, that is the book The Body Artist. Which seems like an obvious failure of the AI model to properly extract the information from the sourced article.Also strange is that the sourced article (from Metro times) just as a passing comment says: \"DeLillo\u2019s choice of a performance artist as his protagonist is one reason why some critics have accused him of elitism.\" - so it would seem that it is being used as a primary source though it is actually a secondary source (which itself doesn't provide a source)Overall I'm not too impressed and found some pretty predictable failures almost immediately...reply",
      "https://en.wikipedia.org/wiki/GrokipediaVshttps://grokipedia.com/page/Wikipediareply",
      "Edit history on each is quite interesting!reply",
      "This seems like an old article, but probably still true todayWhat I don't get is, why wouldn't Elon just make a good version of Grokipedia. It seems way easier than continually telling his 200MM+ followers how great a deeply broken product is.reply",
      "It is not possible to make a good version of Grokipedia while satisfying the requirements of its owner, namely to launder ultra-right viewpoints[1].1: https://www.nbcnews.com/tech/elon-musk/elon-musk-grokipedia-...reply",
      "What would a \"good\" version be ?Start putting real facts into a site and before you know it you're \"woke\" again with such untruths as Slavery was Bad, Biden won the 2020 Election and of course Full Self Driving is Impossible without Lidarreply"
    ],
    "link": "https://www.404media.co/grokipedia-is-the-antithesis-of-everything-that-makes-wikipedia-good-useful-and-human/",
    "first_paragraph": "I woke up restless and kind of hungover Sunday morning at 6 am and opened Reddit. Somewhere near the top was a post called \u201cTIL in 2002 a cave diver committed suicide by stabbing himself during a cave diving trip near Split, Croatia. Due to the nature of his death, it was initially investigated as a homicide, but it was later revealed that he had done it while lost in the underwater cave to avoid the pain of drowning.\u201d The post linked to a Wikipedia page called \u201cList of unusual deaths in the 21st century.\u201d I spent the next two hours falling into a Wikipedia rabbit hole, clicking through all manner of horrifying and difficult-to-imagine ways to die.A day later, I saw that Depths of Wikipedia, the incredible social media account run by Annie Rauwerda, had noted the entirely unsurprising fact that, behind the scenes, there had been robust conversation and debate by Wikipedia editors as to exactly what constitutes an \u201cunusual\u201d death, and that several previously listed \u201cunusual\u201d deaths had "
  },
  {
    "title": "Windows drive letters are not limited to A-Z (ryanliptak.com)",
    "points": 370,
    "submitter": "LorenDB",
    "submit_time": "2025-11-30T13:40:17 1764510017",
    "num_comments": 180,
    "comments_url": "https://news.ycombinator.com/item?id=46096556",
    "comments": [
      "The NT paths are how the object manager refers to things. For example the registry hive HKEY_LOCAL_MACHINE is an alias for \\Registry\\Machinehttps://learn.microsoft.com/en-us/windows-hardware/drivers/k...In this way, NT is similar to Unix in that many things are just files part of one global VFS layout (the object manager name space).Paths that start with drive letters are called a \"DOSPath\" because they only exist for DOS compatibility. But unfortunately, even in kernel mode, different sub systems might still refer to a DOSPath.Powershell also exposes various things as \"drives\", pretty sure you could create your own custom drive as well for your custom app. For example, by default there is the 'hklm:\\' drive path:https://learn.microsoft.com/en-us/powershell/scripting/sampl...Get-PSDrive/New-PSDriveYou can't access certificates in linux/bash as a file path for example, but you can in powershell/windows.I highly recommend getting the NtObjectManager powershell module and exploring about:https://github.com/googleprojectzero/sandbox-attacksurface-a...ls NtObject:\\reply",
      "It's baffling than after 30 years, Windows is still stuck in a weird directory naming structure inherited from the 80's that no longer make sense when nobody has floppy drives.reply",
      "> Windows is still stuck in a weird directory naming structure inherited from the 80's that no longer make sense when nobody has floppy drives.I think you could make this same statement about *nix, except it's 10 years _worse_ (1970s). I strongly prefer the fhs over whatever MS thinks it's doing, but let's not pretend that the fhs isn't a pile of cruft (/usr/bin vs /bin, /etc for config, /media vs /mnt, etc)reply",
      "Unix starts at root, which is how nature intended.  It does not change characteristics based on media - you can mount a floppy at root if you want.Why get upset over /media vs /mnt? You do you, I know I do.For example The Step CA docs encourage using /etc/step-ca/ (https://smallstep.com/docs/step-ca/certificate-authority-ser...) for configuration for their product.  Normally I would agree but as I am manually installing this thing myself and not following any of the usual docs, I've gone for /srv/step-ca.I think we get enough direction from the ... \"standards\" ... for Unix file system layouts that any reasonably incompetent admin can find out which one is being mildly abused today and get a job done.  On Windows ... good luck.  I've been a sysadmin for both platforms for roughly 30 years and Windows is even odder than Unix.reply",
      "> Unix starts at root, which is how nature intended. It does not change characteristics based on media - you can mount a floppy at root if you want.If my machine is connected to multiple drives, as most are, why is the root of one of my drives `/` while the roots of my other drives are subdirectories of that first drive?reply",
      "Thinking of it in terms of namespaces might help; it's not that the drive is special, it's that there's a view that starts from / and one disk filesystem happens to be dropped there and others are dropped elsewhere; with something like initramfs there aren't any drives on /, just a chunk of ram, though you usually pivot to a physical one later (many linux-based embedded systems don't because your one \"drive\" is an SD card that can't handle real use, so you just keep the \"skeleton\" in memory and drop various bits of eMMC or SD or whatever into the tree as-convenient.)reply",
      "Only the root of the root filesystem is /The point is that any filesystem can be chosen as the OS\u2019s root.The root of all other filesystems - there could be multiple per drive - is where you tell the filesystem to be mounted, or in your automounter\u2019s special directory, usually /run/media, where it makes a unique serial or device path.* clarityreply",
      "In multiple ways, / doesn't have to be one of your drives.reply",
      "I like being able to run games from early 2000s. Being able to write software that will still run longer after you're gone used to be a thing. But here we are with linux abandoning things like 'a.out'. Microsoft doesn't have the luxury to presume that it's users can recompile software, fork it, patch it,etc.. When  your software doesn't work on the latest Windows, most people blame Microsoft not the software author.reply",
      "Ok, I prefer to use software which is future compatible, like ZFS, which is 128-bit.\u201cThe file system itself is 128 bit, allowing for 256 quadrillion zettabytes of storage. All metadata is allocated dynamically, so no need exists to preallocate inodes or otherwise limit the scalability of the file system when it is first created. All the algorithms have been written with scalability in mind. Directories can have up to 248 (256 trillion) entries, and no limit exists on the number of file systems or the number of files that can be contained within a file system.\u201dhttps://docs.oracle.com/cd/E19253-01/819-5461/6n7ht6qth/inde...Don\u2019t want to hit the quadrillion zettabyte limit..reply"
    ],
    "link": "https://www.ryanliptak.com/blog/windows-drive-letters-are-not-limited-to-a-z/",
    "first_paragraph": "On its own, the title of this post is just a true piece of trivia, verifiable with the built-in subst tool (among other methods).Here's an example creating the drive +:\\ as an alias for a directory at C:\\foo:The +:\\ drive then works as normal (at least in cmd.exe, this will be discussed more later):However, understanding why it's true elucidates a lot about how Windows works under the hood, and turns up a few curious behaviors.The paths that most people are familiar with are Win32 namespace paths, e.g. something like C:\\foo which is a drive-absolute Win32 path. However, the high-level APIs that take Win32 paths like CreateFileW ultimately will convert a path like C:\\foo into a NT namespace path before calling into a lower level API within ntdll.dll like NtCreateFile.This can be confirmed with NtTrace, where a call to CreateFileW with C:\\foo ultimately leads to a call of NtCreateFile with \\??\\C:\\foo:Note: The relevant bit is ObjectAttributes=\"\\??\\C:\\foo\"createfilew.zig:Built with:To run"
  },
  {
    "title": "Migrating Dillo from GitHub (dillo-browser.org)",
    "points": 270,
    "submitter": "todsacerdoti",
    "submit_time": "2025-11-30T14:11:40 1764511900",
    "num_comments": 163,
    "comments_url": "https://news.ycombinator.com/item?id=46096800",
    "comments": [
      "I've been messing around with GitLab as a self hosted alternative for a few years. I do like it, but it is resource intensive!For the past few days I've been playing with Forgejo (from the Codeberg people). It is fantastic.The biggest difference is memory usage. GitLab is Ruby on Rails and over a dozen services (gitlab itself, then nginx, postgrest, prometheus, etc). Forgejo is written in go and is a single binary.I have been running GitLab for several years (for my own personal use only!) and it regularly slowly starts to use up the entirety of the RAM on a 16GB VM. I have only been playing with Forgejo for a few days, but I am using only 300MB of the 8 GB of RAM I allocated, and that machine is running both the server and a runner (it is idle but...).I'm really excited about Forgejo and dumping GitLab. The biggest difference I can see if that Forgejo does not have GraphQL support, but the REST API seems, at first glance, to be fine.EDIT: I don't really understand the difference between gitea and forgejo. Can anyone explain? I see lots of directories inside the forgejo volume when I run using podman that clearly indicate they are the same under the hood in many ways.EDIT 2: Looks like forgejo is a soft fork in 2022 when there were some weird things that happened to governance of the gitea project: https://forgejo.org/compare-to-gitea/#why-was-forgejo-create...reply",
      "> I'm really excited about ForgejoOur product studio with currently around 50 users who need daily git access moved to a self hosted forgejo nearly 2 years ago.I really can\u2019t overstate the positive effects of this transition. Forgejo is a really straightforward Go service with very manageable mental model for storage and config. It\u2019s been easy and cheap to host and maintain, our team has contributed multiple bugfixes and improvements and we\u2019ve built a lot of internal tooling around forgejo which otherwise would\u2019ve required a much more elaborate (and slow) integration with GitHub.Our main instance is hosted on premise, so even in the extremely rare event of our internet connection going offline, our development and CI workflows remain unaffected (Forgejo is also a registry/store for most package managers so we also cache our dependencies and docker images).reply",
      "Wait, forgejo offers a built-in container registry? How does that work?  I don't see that in the admin section at all.reply",
      "Container registry and a lot more, they call it Package registry in the docs https://forgejo.org/docs/latest/user/packages/reply",
      "Just run podman or docker login your.forgejo.instance.address then push to it as normal. An existing repo must exist. You can check the images under site administration -> packages.Speaking of authentication it also works as an openid provider meaning you can authenticate every other web software that supports it to Forgejo... which in turn can look for users in other sources.It also has wikis.Its an underrated piece of software that uses a ridiculous small amount of computer resources.reply",
      "That's so brilliant. Wow. I'm struggling to wrap my brain around how they not only support OCI (docker) but also APK (alpine) and APT (debian) packages. That's a very cool feature.reply",
      "Ease of maintenance is an even bigger difference. We've been using gitea for a bit over five years now, and gitlab for a few years before that, and gitea requires no maintenance in comparison. Upgrades come down to pulling the new version and restarting the daemon, and take just a few seconds. It's definitely the best solution for self-hosters who want to spend as little time as possible on their infrastructure.Backups are handled by zfs snapshots (like every other server).We've also had at least 10\u00d7 lower downtime compared to github over the same period of time, and whatever downtime we had was planned and always in the middle of the night. Always funny reading claims here that github has much better uptime than anything self-hosted from people who don't know any better. I usually don't even bother responding anymore.reply",
      "I guess I'll just chime in that while Gitlab is a very heavy beast, I have self hosted it for over a decade with little to no issues. It's pretty much as simple as installing their Omnibus package repository and doing apt install gitlab-ce.reply",
      "When I self hosted gitlab I never found the maintenance to be that bad, just change a version in a compose.yml, sometimes having to jump between blessed versions if I've missed a few back to back.Like others, I've switch to Gitea, but whenever I do visit gitlab I can't help but think the design / UX is so much nicer.reply",
      "My usual impression of GitLab is that it has too many functions I don't ever use, so the things I actually do want (code, issues, PRs, user permissions) are needlessly hidden. What's your workflow that you find GitLab's UX to be nicer than Gitea's?reply"
    ],
    "link": "https://dillo-browser.org/news/migration-from-github/",
    "first_paragraph": "I would like to migrate the Dillo project away from\nGitHub\ninto a new home which is more friendly to be used with Dillo and solves some of\nits problems. This page summarizes the current situation with GitHub and why I\ndecided to move away from it into a self-hosted server with multiple mirrors in\nother forges.\nBefore we dive into the details, I would like to briefly mention what\nhappened with the old site. The original Dillo website was at dillo.org,\nwhich also had the source code of Dillo in a mercurial repository at\nhg.dillo.org. But it also included the mail server used to reach the developers,\na bug tracker and archives for the mailing list. However, in 2022\nthe domain was lost and someone else decided to\nbuy it to put a similar site but plaged with AI generated ads. The original\ndevelopers are no longer active, but luckily I had a copy of the mercurial\nrepository and with some help I was able to recover a lot of material from the\noriginal server (some parts are still missing to th"
  },
  {
    "title": "LLVM-MOS \u2013 Clang LLVM fork targeting the 6502 (llvm-mos.org)",
    "points": 108,
    "submitter": "jdmoreira",
    "submit_time": "2025-11-30T17:02:48 1764522168",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=46098359",
    "comments": [
      "According to this page, LLVM-MOS seems to be pretty soundly beaten in performance of generated code by Oscar64.https://thred.github.io/c-bench-64/I think the ideal compiler for 6502, and maybe any of the memory-poor 8-bit systems would be one that supported both native code generation where speed is needed as well as virtual machine code for compactness. Ideally would also support inline assembler.The LLVM-MOS approach of reserving some of zero page as registers is a good start, but given how valuable zero page is, it would also be useful to be able to designate static/global variables as zero page or not.reply",
      "I've implemented Atari 2600 library support for both LLVM-MOS and CC65, but there are too many compromises to make it suitable for writing a game.The lack of RAM is a major factor; stack usage must be kept to a minimum and you can forget any kind of heap. RAM can be extended with a special mapper, but due to the lack of a R/W pin on the cartridge, reads and writes use different address ranges, and C does not handle this without a hacky macro solution.Not to mention the timing constraints with 2600 display kernels and page-crossing limitations, bank switching, inefficient pointer chasing, etc. etc. My intuition is you'd need a SMT solver to write a language that compiles for this system without needing inline assembly.reply",
      "Aztec C had both native and interpreted code generation, back in the day.reply",
      "AIUI, Oscar64 does not aim to implement a standard C/C++ compiler as LLVM does, so the LLVM-MOS approach is still very much worthwhile.  You can help by figuring out which relevant optimizations LLVM-MOS seems to be missing compared to SOTA (compiled or human-written) 6502 code, and filing issues.reply",
      "With regard to code size in this comparison someone associated with llvm-mos remarked that some factors are: their libc is written in C and tries to be multi-platform friendly, stdio takes up space, the division functions are large, and their float support is not asm optimized.reply",
      "I wasn't really thinking of the binary sizes presented in the benchmarks, but more in general. 6502 assembler is compact enough if you are manipulating bytes, but not if you are manipulating 16 bit pointers or doing things like array indexing, which is where a 16-bit virtual machine (with zero page registers?) would help. Obviously there is a trade-off between speed and memory size, but on a 6502 target both are an issue and it'd be useful to be able to choose - perhaps VM by default and native code for \"fast\" procedures or code sections.A lot of the C library outside of math isn't going to be speed critical - things like IO and heap for example, and there could also be dual versions to choose from if needed. Especially for retrocomputing, IO devices themselves were so slow that software overhead is less important.reply",
      "This was a nice surprise when learning to code for NES, that I could write pretty much normal C and have it work on the 6502.  A lot of tutorials warn you, \"prepare for weird code\" and this pretty much moots that.reply",
      "I don't know this world well (I know what llvm is) but - does anyone know why this was made as a fork vs. contributing to llvm? I suppose it's harder to contribute code to the real llvm..?Thanksreply",
      "These processors were very very different from what we have today.They usually only had a single general purpose register (plus some helpers). Registers were 8-bit but addresses (pointers) were 16-bit.  \nMemory was highly non-uniform, with (fast) SRAM, DRAM and (slow) ROM all in one single address space.\nInstructions often involved RAM directly and there were a plethora of complicated addressing modes.Partly this was because there was no big gap between processing speed and memory access, but this makes it very unlikely that similar architectures will ever come back.As interesting as experiments like LLVM-MOS are, they would not be a good fit for upstream LLVM.reply",
      "> ... there was no big gap between processing speed and memory access, but this makes it very unlikely that similar architectures will ever come back. ...Don't think \"memory access\" (i.e. RAM), think \"accessing generic (addressable) scratchpad storage\" as a viable alternative to both low-level cache and a conventional register file.  This is not too different from how GPU low-level architectures might be said to work these days.reply"
    ],
    "link": "https://llvm-mos.org/wiki/Welcome",
    "first_paragraph": ""
  },
  {
    "title": "ESA Sentinel-1D delivers first high-resolution images (esa.int)",
    "points": 82,
    "submitter": "giuliomagnifico",
    "submit_time": "2025-11-30T17:37:09 1764524229",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=46098673",
    "comments": [
      "High resolution images, but they decided to disable zoom on mobile.  I don't understand why anyone does that.reply",
      "Even the high-res version (20 MB) of the Bremen image seems to be about 17-25m per pixel based on the 50m wide airport runway being about 2-3 pixels wide in the image.Copernicus browser claims 10x10 meter pixels (which seems to be correct) but the actual resolution of the radar is supposed to be 5m-x-20m for the standard IW mode. I assume \"high resolution\" here means the data should have 5m x 5m resolution (Strip Map mode) which in Copernicus browser claims 3.5x3.5m pixels.reply",
      "All of modern web design is about removing as much freedom from html as possible. It's infuriating.We had zoomable, downloadable images in the 90s, with bandwidth as the only constraint.Now I've got 50x as many pixels and I'm forced to use a bookmarklet and 2 menus to be able to see it larger than my fingernail.reply",
      "Also, I don't understand why browsers don't let me override that.reply",
      "Opera Mobile has a force-allow-zoom optionreply",
      "If only Opera was still Norwegian...reply",
      "The images on the page are not the high resolution images, they are resized as the full res versions are over 20MB. If you take the image, you'll be taken to a download page where you can get the full res version.reply",
      "I was curious what instruments this use, looks like a special form of radar? Does this mean it effectively gives us very accurate height maps regardless of cloud coverage, and is able to differentiate between what surface material it's seeing?> Radar instruments can image Earth\u2019s surface through clouds, precipitation, regardless of sunlight, making them particularly well suited for monitoring polar regions. The Sentinel-1C and -1D satellites also carry an Automatic Identification System (AIS) instrument \u2013 improving the mission capacity to detect ships and sea pollution. The Sentinel-1D AIS was also activated as the satellite passed over Antarctica capturing the presence of ships in these extreme areas.reply",
      "Synthetic aperture radar is basically building a bitmap of radar reflectivity. So what you get looks a lot like a photo. You can end up with very non-photo artifacts though - blown out pixels caused by corner reflectors, bright things can result in ghost copies in multiple places and if there\u2019s other radar operating in the same frequency bands it can end up on the picture.The core idea is that you send out pulses as you pass over the ground and then record the echoes. You can create an image by - for each pixel in the image - working out the response you would expect to receive back and correlating that with the actual responses you saw. That gives you a reflectivity value. You can do it in multiple polarisation to better distinguish things.reply",
      "Ideally you want to have a large collecting area (aperture) for radar to get good resolution. But it isn't practical to put a big radar dish in space. So they use a small aperture and simulate a larger one by sweeping out an area over time and using some clever maths. Hence 'synthetic aperture radar'.reply"
    ],
    "link": "https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-1/Sentinel-1D_delivers_first_images_from_Antarctica_to_Bremen",
    "first_paragraph": "The first high-resolution images have been received from Copernicus Sentinel-1D and were shared publicly for the first time at the European Space Agency\u2019s Ministerial Council, held today in Bremen, Germany. Glaciers in Antarctica, the tip of South America, as well as the city of Bremen, are visible in these stunning radar images.The groundbreaking Copernicus Sentinel-1 mission saw the arrival in orbit of its latest satellite earlier in November: Sentinel-1D was launched on 4 November, on board an Ariane 6 launcher from Europe\u2019s Spaceport in French Guiana.Once in orbit, the satellite and its instruments \u2013 it carries a 12 m-long synthetic aperture radar (SAR) instrument \u2013 were switched on, ready to capture images during a pass over the Antarctic and South America two days after launch. On the night of 6 November (European time), the first images were captured over the Antarctic Peninsula, the Tierra del Fuego and the Thwaites Glacier. Some six hours later, on the morning of 7 November, S"
  },
  {
    "title": "GitHub to Codeberg: my experience (eldred.fr)",
    "points": 141,
    "submitter": "todsacerdoti",
    "submit_time": "2025-11-30T16:12:13 1764519133",
    "num_comments": 61,
    "comments_url": "https://news.ycombinator.com/item?id=46097829",
    "comments": [
      "I spent some time last week adding Forgejo/Git to my local NAS via docker and tailscale.reply",
      "What really stands out to me in this migration story isn't the technical side at all, but the reminder that \"feature parity\" isn't the real hurdle here. Codeberg is already good enough for most day to day workflows; what it doesn't have is the gravitational pull GitHub built through network effects, integrations, and plain old inertia.reply",
      "My primary pain point with Codeberg has been that the issue search is worse, so that there are cases where I'm rather certain than issue exists-- because I've triaged it in the past-- but it's hard to find with the keyboard search. Hopefully that can be improved soon.There were some times were Codeberg's general performance was noticeably worse, but most recently it has been fine.If you thinking of migrating a project with hundreds of issues, I would do a test migration and practice a few different searches to test the result quality.reply",
      "This is partially being addressed by projects like https://tangled.org. It's built on the same protocol as bluesky, meaning your identity is preserved across different platforms so that _where_ your git is hosted is unrelated to how you discover and connect with others.reply",
      "If only we could use something like a gpg key as our identity. Maybe if it had a mechanism to share and revoke keys, upgrade them, cross-sign them with others to develop some sort of like trust system that was web-like. I bet we could like build a whole infrastructure around it to maintain developer identities in a completely decentralized way.reply",
      "FWIW, Forgejo (Codeberg) is also building federation capability [0].[0]: https://codeberg.org/forgejo-contrib/federation/src/branch/m...reply",
      "And just sheer amount of documentation and examples out there. Everyone uses it, therefore everyone writes about it, the new hire probably knows it, and if they don't they can find it easily.Then again maybe for stuff like actions and in general CI/CD it's not all that bad, you don't need whole team to know exactly how to write it, you just need to have a person knowing it. and it's generally not all that hard to learn.reply",
      "This is why CI should be separate from code repository storage and that should be separate from your collaboration tools. They all can speak git if you want.reply",
      "It feels like a <game theory> problem (Tragedy of the commons? First mover? I dunno them well enough).  It\u2019s probably a mistake for any one company to not pick GitHub, because it\u2019s likely a higher friction distraction from what the actual goals are of the company. But enough companies paying that price ultimately would benefit everyone by fuelling stronger competition.reply",
      "Are there any alternatives to Github that offer similar bang for the buck? Particularly for very small teams or solo devs that need private repos? The author here specifically mentions Codeberg, which seems like it's just for FOSS projects.reply"
    ],
    "link": "https://eldred.fr/blog/forge-migration/",
    "first_paragraph": "Published Sat 29 Nov 2025. Estimated reading time: 11 minutes.In which I talk about the process involved in switching forges, and how well that went.\u2139\ufe0f MotivationWhy am I getting out of GitHub? See the previous episode.Spoiler alert: this very site that you\u2019re reading this on is not served from GitHub Pages anymore! At this point, I\u2019d call my migration successful. But it took more than clicking a single button, so let\u2019s talk about the steps involved, at least for me. I\u2019m hoping that it can help be an example for other people, and show that it\u2019s actually not that complicated.(My) migration processFirst, I took an hour or so to set up my profile picture, email address(es), SSH keys\u2026Step 1: migrating the reposThis wasn\u2019t difficult, because Forgejo (the forge software that powers Codeberg) offers a \u201cmigrate from GitHub\u201d functionality. You need to generate a PAT on GitHub to import things like issues (which is awesome!), and as a bonus it also speeds up the process.It was, however, tedious,"
  },
  {
    "title": "Program-of-Thought Prompting Outperforms Chain-of-Thought by 15% (2022) (arxiv.org)",
    "points": 71,
    "submitter": "mkagenius",
    "submit_time": "2025-11-30T18:34:52 1764527692",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=46099108",
    "comments": [
      "DSPy implemented program of thought since a long time ago and it works great to solve user queries with code.What is great is that you can define DSPy signature of the type \u201cquestion, data -> answer\u201d where \u201cdata\u201d is a pandas dataframe, then DSPy prompts the llm to answer the question using the data and python code. Extremely powerful.reply",
      "Chain-of-code is better than chain-of-thought because it's more grounded, more specific, and achieves a lot of useful compression.  But my bet is that the proposed program-of-thought is too specific. Moving all the way from \"very fuzzy specification\" to \"very concrete code\" skips all of the space in the middle, and now there's no room to iterate without a) burning lots of tokens and b) getting bogged down in finding and fixing whatever new errors are introduced in the translated representations.  IOW, when there's an error, will it be in the code itself or in the scenario that code was supposed to be representing?I think the intuition that lots of people jumped to early about how \"specs are the new code\" was always correct, but at the same time it was absolutely nuts to think that specs can be represented in good ways with natural language and bullet-lists in markdown.  We need chain-of-spec that's leveraging something semi-formal and then iterating on that representation, probably with feedback from other layers.  Natural-language provides constraints, guess-and-check code generation is sort at the implementation level, but neither are actually the specification which is the heart of the issue.  A perfect intermediate language will probably end up being something pretty familiar that leverages and/or combines existing formal methods from model-checkers, logic, games, discrete simulations, graphs, UML, etc.  Why?  It's just very hard to beat this stuff for compression, and this is what all the \"context compaction\" things are really groping towards anyway.  See also the wisdom about \"programming is theory building\" and so on.I think if/when something like that starts getting really useful you probably won't hear much about it, and there won't be a lot of talk about the success of hybrid-systems and LLMs+symbolics.  Industry giants would have a huge vested interest in keeping the useful intermediate representation/languages a secret-sauce.  Why?  Well, they can pretend they are still doing something semi-magical with scale and sufficiently deep chain-of-thought and bill for extra tokens.  That would tend to preserve the appearance of a big-data and big-computing moat for training and inference even if it is gradually drying up.reply",
      "Perhaps something like TLA+ or PlusCal specs could be the specs in terms of 'specs are the new code'.reply",
      "Delusional vibe coding bullshit. Find me one significant software project based on using natural language for the software.reply",
      "This seems to be incorporated into current LLM generations already -- when code execution is enabled both GPT-5.x and Claude 4.x automatically seem to execute Python code to help with reasoning steps.reply",
      "This was integrated in gpt4 2 years ago:https://www.reddit.com/r/ChatGPT/comments/14sqcg8/anyone_els...reply",
      "I remember seeing that GPT-5 had two python tools defined in its leaked prompt  one them would hide the output from user visible chain of thought UI.reply",
      "Same with CoT prompting.If you compare the outputs of a CoT input vs a control input, the outputs will have the reasoning step either way for the current generation of models.reply",
      "Yeah, this is honestly one of the coolest developments of new models.reply",
      "Underlying paper is from 2022 and should be indicated in the title.reply"
    ],
    "link": "https://arxiv.org/abs/2211.12588",
    "first_paragraph": "YOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n"
  },
  {
    "title": "ETH-Zurich: Digital Design and Computer Architecture; 227-0003-10L, Spring, 2025 (ethz.ch)",
    "points": 114,
    "submitter": "__rito__",
    "submit_time": "2025-11-30T17:45:51 1764524751",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=46098747",
    "comments": [
      "Details on the ETH Zurich open source ASICs can be found here:https://github.com/open-source-eda-birds-of-a-feather/open-s...Presented at DAC 2025reply",
      "Is it even possible to design serious ASICs without expensive tooling?reply",
      "For some definitions of serious, sure.  The main critical piece that\u2019s missing is all the testing infrastructure.  Buying 100 or so ASICs for university use is one thing.  Buying 100K, or more, is another.Not the gdb support via jtag that software engineers need, they have that.  But the various manufacturing test suites, which do modify gate netlists, and automated circuit characterization techniques that electrical engineers and the manufacturing engineers use.reply",
      "From the slides, they are reducing the gap, it's not there yet.But I was actually pleasantly surprised by how close they are.reply",
      "Sure, as long as you stick to digital and purchased IP.If you can get a \"library\" from somewhere (like the one Google released from Skywater), then you can use static timing analysis on the interconnect between the library blocks.  Performance metrics will all be mediocre, but it will be relatively quick to design and cheap to produce if you have sufficient volumes.  This is why so many of the RISC-V processor implementations suck.If you want to design analog, RF, or high-speed, then the expensive tooling is required.  You need especially need DRC and extraction (parasitics from passives, transistor numbers, etc.) for proper analysis and design.reply",
      "Let\u2019s all take a moment to remember Nikolas Wirth and Project Oberon and its fpga processor. I learned so much from reading his books. They are very accessible and I recommend them to anyone!reply",
      "This course is actually mandatory in the first year of the CS undergraduate program here at ETH. I remember it very fondly for its great (and passionate) lecture and the hands on experience building a MIPS cpu in the exercise sessions. Probably the best lecture in my undergraduate.reply",
      "This is also the university that develops RumbleDB[0]. It uses JSONiq as its query language which is such a pleasure to work with. It's useful for dealing with data lakes, though I've only experimented with it because of JSONiq.[0] https://github.com/RumbleDB/rumblereply",
      "I'm taking Ghislain Fourny's Big Data course here at ETH, he's such a good professorreply",
      "Onur Mutlu also posts his (great) lectures to YouTube: https://www.youtube.com/@OnurMutluLecturesreply"
    ],
    "link": "https://safari.ethz.ch/ddca/spring2025/doku.php?id=start",
    "first_paragraph": "\nHome\n\nAnnouncements\n\nMaterials\n\nResources\n\nWelcome to the wiki for Digital Design and Computer Architecture for Spring 2025\n\n Latest Announcements\n\nThe class provides a first introduction to the design of digital circuits and computer architecture. It covers technical foundations of how a computing platform is designed from the bottom up. It introduces various execution paradigms, hardware description languages, and principles in digital design and computer architecture. \nThe focus is on fundamental techniques employed in the design of modern microprocessors and their hardware/software interface.\n\nThis class provides a first approach to Computer Architecture. The students learn the design of digital circuits in order to:\n\nBased on such understanding, the students are expected to:\n\nThe focus is on basics, principles, precedents, and how to use them to create/implement good designs.\n\nCourse description page\nMoodle\n\nThursday, 14:15-16:00, in HG F7 (Overflow room: HG F5) \n\nFriday, 14:15-1"
  },
  {
    "title": "CachyOS: Fast and Customizable Linux Distribution (cachyos.org)",
    "points": 261,
    "submitter": "doener",
    "submit_time": "2025-11-30T10:47:27 1764499647",
    "num_comments": 231,
    "comments_url": "https://news.ycombinator.com/item?id=46095585",
    "comments": [
      "This was the one to finally stop getting me to distro hop. Cachy is very easy to use and very well maintained. The performance is usually the selling point people talk about, but it's also very customizable and beginner-friendly (especially for an arch-based distro).It uses an online installer that lets you choose the desktop environment, boot manager, file system, among other things. You can follow the defaults if you're new. Once you install it, it also comes with a few helper applications that can quickly set up things you'd want to use, like a one-click button that installs all the gaming packages you want to use and their flavor of Proton which is (allegedly) faster than the default.They also have a really good wiki which I contributed a bit to and a very active community if you need help. All around, 10/10 would recommend to anyone. I managed to convince my friend who's new to Linux to use this instead of Zorin and he's had a great time.reply",
      "I really dislike that Linux proper doesn't by default have x.xx-server, x.xx-workstation, x.xx-laptop and x.xx-desktop kernel variants. Or just doesn't have defaults, requiring distros to think about what to set during compilation.A lot of the current defaults stem from the 90s, and often were eyeballed by the creator of said code. They're not good defaults for modern servers nor workstations nor laptops nor desktops. And all of those devices work best with different defaults.It doesn't seem (yes, appearances can be deceiving) to be that much work, because no extra code needs to be written. For each variant, just set different default parameter values for stuff like swappiness, lazy RCUs and what not. Make it a thing to revisit the defaults every 10 years.CachyOS and some other distros already do this, but a big chunk  of distros doesn't because they think the defaults are well-thought out.reply",
      "> CachyOS and some other distros already do this, but a big chunk of distros doesn't because they think the defaults are well-thought out.Based on what I saw 1-2 years ago last time I looked at it, most distributions to customize and don't use the defaults straight up. From memory, so someone correct me if I'm wrong:- RHEL/SLES - Lots of patches to kernels- Arch - Closer to just using defaults, some config choices and downstream adjustments (so the opposite of CachyOS almost, which is why we have CachyOS in the first place)- Ubuntu - Probably the most patched distribution compared to upstream components, also includes a lot of Canonical-specific stuff on top of that.- Fedora - Has some bleeding edge bits and bobs- Debian - Bit more conservative than Ubuntu, but still has patches for stability, security and backports.In my experience, distributions changing the defaults and customizations seems to be the norm rather than the exception.reply",
      "I love the separation of concerns. It provides an amazing terminal-first kernel and everything graphical is maintained by various different organizations, and you can choose between many different options.Maintaining a large distro is extremely difficult and every decision has several trade-offs.reply",
      "Why would you want different kernels for different device types?Genuine question! I maintain my own Linux distro (upstream Linux + portage) for all my devices and haven\u2019t found much reason to go beyond kernel per arch. I\u2019m curious if there\u2019s something I could be missing.reply",
      "Well, for the two examples I named:vm.swappiness defaults to 60, which is default from when everyone was still running spinning rust with a swap partition. Servers these days usually have very specific storage+memory configurations, whereas the usual desktop or laptop has an SSD and 16GB+ of RAM with RAM compression expanding it.Lazy RCU loading is good on a laptop because you only lose about 10% performance and only with specific workloads, but your idle and light load energy consumption improves. Most laptops spend like 95%+ in light or idle load scenarios. Conversely, on a desktop you don't care (much) about idle and light load energy consumption, you only care about keeping max load consumption low enough so that your fans stay quiet. And on a workstation you don't care about a system being whisper quiet so you can go nuts with the energy consumption.reply",
      "> vm.swappiness defaults to 60, which is default from when everyone was still running spinning rust with a swap partition. Servers these days usually have very specific storage+memory configurations, whereas the usual desktop or laptop has an SSD and 16GB+ of RAM with RAM compression expanding it.You don't need to compile a specific kernel for that, this is setup via sysctl.reply",
      "> Lazy RCU loading is good on a laptopDo you mean RCU_LAZY? Most distros will already enable that: it doesn't do anything without rcu_nocbs, so there's no negative impact on server workloads.    [calvin@debian-trixie ~] grep RCU_LAZY /boot/config-6.12.57+deb13-amd64\n    CONFIG_RCU_LAZY=y\n    # CONFIG_RCU_LAZY_DEFAULT_OFF is not set\n    [calvin@debian-trixie ~] grep RCU_NOCB_CPU /boot/config-6.12.57+deb13-amd64 \n    CONFIG_RCU_NOCB_CPU=y\n    # CONFIG_RCU_NOCB_CPU_DEFAULT_ALL is not set\n\nhttps://git.kernel.org/pub/scm/linux/kernel/git/torvalds/lin...You just have to set rcu_nocbs on the kernel cmdline.reply",
      "Swappiness and many others can be changed by some sort of system preset rather built that way. I know not ALL options can be done that way, but I'd want to see changes start there where feasible.reply",
      "I totally missed that part of your comment, my bad. Thanks for elaborating on those, I feel inspired to experiment!So far my kernel journey has been about making my hardware work + enabling features, and that\u2019s mostly how I\u2019ve been discovering config options. Do you have any suggestions on where one aught to read further on this sort of kernel tuning?EDIT: doing some further research, couldn\u2019t you just set those options via sysctl w/o needing to build a separate kernel?reply"
    ],
    "link": "https://cachyos.org/",
    "first_paragraph": " CachyOS is designed to deliver lightning-fast speeds and stability, ensuring a smooth and enjoyable\n          computing experience every time you use it. Whether you're a seasoned Linux user or just starting\n          out, CachyOS is the ideal choice for those looking for a powerful, customizable and blazingly\n          fast operating system.\n\nFeatures\n\nExperience Cutting-Edge Linux Performance with CachyOS - A distribution built on Arch Linux,\n        CachyOS features the optimized linux-cachyos kernel utilizing the advanced BORE Scheduler\n        for unparalleled performance.\nCachyOS does compile packages with the x86-64-v3, x86-64-v4 and Zen4 instruction set and LTO to provide a higher performance. Core packages also get PGO or BOLT optimization.CachyOS offers a variety of popular Desktop Environments, Wayland Compositors and X11 Window Managers including KDE Plasma, GNOME, XFCE, i3, Wayfire, LXQt, Openbox, Cinnamon, COSMIC, UKUI, LXDE, Mate, Budgie, Qtile, Hyprland, Sway and Niri."
  },
  {
    "title": "\"Boobs check\" \u2013 Technique to verify if sites behind CDN are hosted in Iran (twitter.com/hkashfi)",
    "points": 207,
    "submitter": "defly",
    "submit_time": "2025-11-30T20:54:43 1764536083",
    "num_comments": 65,
    "comments_url": "https://news.ycombinator.com/item?id=46100323",
    "comments": [
      "So does this mean 10.x.x.x is publicly routable inside iran? Why wouldn't the Iranian government just use its own ip space for the censorship message?reply",
      "This behavior only works when the reverse proxy or CDN is configured like this:Proxy/CDN: HTTPS (443) \u2192 Origin server: plain HTTP (80)(example: Cloudflare in Flexible mode)If the origin server uses any proper TLS configuration, even a self-signed certificate, this method stops working. It only succeeds when the upstream connection to the origin is unsecured.If you want to test this on a random site without Cloudflare or reverse proxy in general on HTTP:\ncurl http://www.digiboy.ir/boobs.jpg -vreply",
      "Ah, Cloudflare. The world's most widely deployed encryption remover.reply",
      "To be fair, Cloudflare is also the reason why most sites even have TLS at all, because it offered free certs (through letsencrypt I think?) in a fairly easy to set up way.Certs used to be expensive, and had way more operational overhead and quirks (even setting up ACME/LE)reply",
      "Absolutely not, no. That is all thanks to Let's Encrypt.",
      "I'm not going to give them credit for the work that Lets Encrypt did.reply",
      "It'll also work DigiNotar-style, when using the only root CA blessed by the National Information Network for general use: I.R. Iran.reply",
      "How's this work with https like in the example? The hops along the way shouldn't see the path.Is this implying that all TLS is terminated at the Iran border and proxied from there? And all Iranian sites are required to host via http? That has significantly more implications than what this post is about.Maybe certificate authorities aren't allowed to issue private certs to Iranian organizations? Even LetsEncrypt?reply",
      "This is referring to something else: to detect whether the backend server host itself is inside or outside Iran. TLS doesn't prevent the backend network from reading the URL of course.reply",
      "A lot of CF upstreams are (or at least used to be) plaintext. It is one of the criticisms of CF since it \"whitewashed\" plaintext to look like proper TLS when it was only TLS for client<->CF and then plaintext for CF<->server.reply"
    ],
    "link": "https://twitter.com/hkashfi/status/1995109785679573167",
    "first_paragraph": "We\u2019ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.Help Center\nTerms of Service\nPrivacy Policy\nCookie Policy\nImprint\nAds info\n      \u00a9 2025 X Corp.\n    "
  },
  {
    "title": "The Thinking Game Film \u2013 Google DeepMind documentary (thinkinggamefilm.com)",
    "points": 144,
    "submitter": "ChrisArchitect",
    "submit_time": "2025-11-30T16:07:11 1764518831",
    "num_comments": 107,
    "comments_url": "https://news.ycombinator.com/item?id=46097773",
    "comments": [
      "Watched it a while ago. Made me seriously think about AI and what we should use it for. I feel like all the entertainment use cases (image and video gen) are a complete waste.reply",
      "unfortunately all this work on sora has very real military use case. I personally think all this investment in sora by open AI is largely to create a digital fog of war. Now when a rocket splatters a 6 year old palestinian girl's head across the pavement like a jackson polock painting, They will be able to claim its AI generated by state sponsored actors in order to prevent disruption to the manufactured consent aperatus.reply",
      "The chatbots and image editors are just a side-show. The real value is coming in e.g. chemistry (Alpha fold etc all), fusion research, weather prediction etc.reply",
      "ML is used in weather prediction since the 80s and is the backbone of it since almost a decade.Not sure what are LLMs supposed to do there.reply",
      "No one is suggesting using LLMs for weather. DeepMind is making significant progress on weather prediction with new AI models.reply",
      "The real value is coming in warfare.reply",
      "Right. More accurate predictions for meta-data based killings which as championed by US in their war on terrorreply",
      "Metadata based killings are most likely a huge improvement from the prior state of affairsreply",
      "Yeah. Let the leaders assassinate each other with drone strikes instead of indiscriminately bombing whole cities as they used to.reply",
      "what gov't in modern day would fall because the leader was assassinated? the next in line would just step up, and now have a pissed population that will be in favor of ratcheting up beyond assassinations.reply"
    ],
    "link": "https://thinkinggamefilm.com",
    "first_paragraph": "Sign up with your email address to receive news and updates.We respect your privacy.SalesPress"
  },
  {
    "title": "Show HN: Fixing Google Nano Banana Pixel Art with Rust (github.com/hugo-dz)",
    "points": 130,
    "submitter": "HugoDz",
    "submit_time": "2025-11-26T15:46:51 1764172011",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=46058566",
    "comments": [
      "Nice. There's a couple of these (unfake which uses pixel snapping/palette reduction, sd-palettize which uses k-means to palette reduce, etc.) that I've used in the past in a Stable Diffusion -> Pixel Art pipeline.I think it'd be worth calling out the differences.[1] - https://github.com/jenissimo/unfake.js[2] - https://github.com/Astropulse/sd-palettizereply",
      "It feels weird to me that on the before/after comparision they felt the need to zoom in on the \u201cbefore\u201d but not on the \u201cafter\u201d.Either both should have the magnifying glass or neither. This just makes it hard to see the difference.reply",
      "I'd love this, but for removing \"transparent background\" checkerboards.Nano Banana beats it on many other dimensions, but this is one thing that gpt-image-1 usually does much better.reply",
      "This is perfect! I have had such a time with Nano Banana asking it to generate some very simple pixel art. One of the worst things is that it cannot seem to generate transparent backgrounds or even solid ones. It\u2019s always some blotchy cloud of off-white pixels or a simulated fuzzy grid that shows up in some places. I will need to give this a try to clean up some of what I had to try by hand.reply",
      "I can't explain it, but it's like uncanny valley pixel art. Like the artist hasn't done the final polish pass maybe?Maybe it's the inconsistent  lights/shadows?Maybe a pixel artist has the proper words to explain the issuesreply",
      "Not pixel artist, but game dev working with pixel art:1 - AI just try to compress too many details into so few pixels.When artists create pixel art they usually add details along the way and only important ones because otherwise it will look like rubbish on some screens.Also it's easier to e.g add different hats or heads or weapons on the same body. AI generated ones is always too unique.2 - AI try to mimic realistic poses that look like art supposed to be animated in 3D.For a real game if you make lets say isometric tactical game you'll never make tiles larger than 64x64 because of how much labour they will take to animate. Each animation at 8fps take hours of work.So pixel art is usually either high-fidelity and static or low-fi and animated in very basic ways.reply",
      "The skeleton has issues, floor tiles are very inconsistent for example. I haven't looked more carefully. We probably notice something wrong subconsciously but it takes time to point those out.Generated pixel art for now is 80-90% done state. To use them in prod, issues should be fixed which seems to be the palette and some semantic issues. If you only generate small parts of the big picture with AI, it will be perfectly usable.reply",
      "Could you explain a bit how the code works? For example, how does it detect the correct pixel size and how does it find out how to color the (potentially misaligned) pixels?reply",
      "That's an actually nice setup.\nHave you looked at Z-Image and the Pixel LoRA that was released? I've found it works fairly well at keeping the pixels matched with the grid.reply",
      "The Z-image turbo model is pretty heavily distilled. I can't imagine using it for any marginally complicated prompts.Are you talking about the LoRA by LuisaP?Somewhat ironically, that LoRA's showcase images themselves exhibit the exact issues (non-square pixels, much higher color depth than pixel art, etc) that stuff like this project / unfake.js / etc. are designed to fix.https://imgur.com/a/vfvARktreply"
    ],
    "link": "https://github.com/Hugo-Dz/spritefusion-pixel-snapper",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A tool to snap pixels to a perfect grid. Designed to fix messy and inconsistent pixel art generated by AI.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Online version: https://spritefusion.com/pixel-snapperA tool to snap pixels to a perfect grid. Designed to fix messy and inconsistent pixel art generated by AI.Current AI image models can't understand grid-based pixel art.With Pixel Snapper:Pixel Snapper preserves as much details as possible like dithering.Requires Rust installed on your machine.The command accepts an optional k-colors argument:Build the WASM module:Then use the WASM module in your project.Pixel Snapper is a Sprite Fusion project. Sprite Fusion is a free, web-based tilemap editor for game developers supporting a wide range of engines inclu"
  },
  {
    "title": "Show HN: Real-time system that tracks how news spreads across 200k websites (yandori.io)",
    "points": 222,
    "submitter": "antiochIst",
    "submit_time": "2025-11-26T01:27:16 1764120436",
    "num_comments": 56,
    "comments_url": "https://news.ycombinator.com/item?id=46053076",
    "comments": [
      "This is interesting, but it seems like it is tracking stories with similar headlines and that's not always how news propagates. Frequently a blogger will read an interview, select an quote from the interview and write a new headline around the quote they cherry picked. It used to be common practice to link the original source, but that always doesn't happen.I have long thought that search engines, news aggregators and social media companies have a journalistic responsibility to favor the original/primary source of every story, but things have not worked out that way. If you can manage to truly develop something like this it would be a valuable tool for rewarding the work of reporting over SEO.Anyway, please consider that headlines and time stamps do not tell the entire story when it comes to sourcing.For example: Your website offers this story (https://hotspotatl.com/6587626/dr-jackie-married-to-medicine...) as first to publish. But right in the text it cites another website BOSSIP as the source of the interview.Also: there doesn't appear to be a way to link results from your website.reply",
      "> I have long thought that search engines, news aggregators and social media companies have a journalistic responsibility to favor the original/primary source of every storyThis is complicated somewhat by the few that take an already-circulating story and then add their own actual research rather than just rewording and opining.reply",
      "not linking primary sources is one of my biggest pet peeves with modern ad-driven \u201cjournalism\u201d.e.g. the recent Mark Kelly story, I went through many articles trying to find a link to the actual video of what he said. couldn\u2019t find itheadlines with \u201c[person said X]\u201d tend to be bullshitreply",
      "Go hunt down the lineage of the \u201cAI water use\u201d articles floating around.It\u2019s all circular.I don\u2019t know how one is supposed to trust any of the media at this point. Especially \u201creputable\u201d ones that are just as guilty of circular nonsense as anything else.If you don\u2019t follow the media, you are uninformed. If you follow it, you are misinformed.reply",
      "The idea is pretty cool, but it doesn't work super well.\n1. I imagine most major news outlets don't have RSS feeds these days.\n2. A lot of stuff originates from news agencies, so they don't spread from website to website, but radiate out from the agency.\n3. Most of the included sources are pretty small. To draw meaningful conclusions we would need infos like popularity, political leaning, nation of origin, etc.\n4. The similarity check doesn't appear to do translation. So when news spreads from one country to another we loose the thread.reply",
      "Yes. For example, this story about Ukraine [1] is credited to WNYT as first, but the story itself credits the Associated Press. \nThis problem is worth solving, because it's something search engines should be doing.[1] https://wnyt.com/ap-top-news/rubio-says-us-ukraine-talks-on-...reply",
      "Also, not all information spreads through public channels, and might not even be/become publicly known. But that doesn't mean news refraction based on  textual similarity isn't worthwhile to pursue, as it can reveal a lot about the self-organising principles by which the media operate.reply",
      "The devil really is always in the details.reply",
      "Being consistent in message framing even when its not in the best interest of the public should not reasonably be considered \"news\" =3https://en.wikipedia.org/wiki/Sinclair_Broadcast_Grouphttps://www.youtube.com/watch?v=GvtNyOzGogcreply",
      ">the similarity check doesn't appear to do translationThis surprises me. The system is based on embeddings. AFAIK embeddings cluster the same concept in different languages in roughly the same place? Maybe it depends on the model (or maybe it's not exact and the clustering cutoff loses it).reply"
    ],
    "link": "https://yandori.io/news-flow/",
    "first_paragraph": ""
  }
]