[
  {
    "title": "From where I left (antirez.com)",
    "points": 708,
    "submitter": "tilt",
    "submit_time": "2024-12-10T16:41:52 1733848912",
    "num_comments": 259,
    "comments_url": "https://news.ycombinator.com/item?id=42378488",
    "comments": [
      "For me, the license change upset me mainly for two reasons:1. Many people had contributed their efforts to the Redis project for free - both in terms of code but also in advocacy, writing tutorials, publishing example code etc - and when they did that it was under the understanding that project would remain under the same open source license. It honestly felt like a betrayal of trust.2. From a purely selfish point of view, my favourite thing about open source licenses is that they let me know exactly what I'm allowed to do and build on that software without having to consult a license. Licenses like the new Redis one leave me potentially needing to get my own legal advice depending on what I'm building on the software. I don't want to spend my time, energy or money on that stuff!I do also see these kinds of license trends as harmful to open source generally. It used to be that you could pick an open source project and build a business on it and expect that the project would stay available to you under those well understood terms. That's not the case any more - not just because of Redis, there are a number of other high-profile license rug-pulling projects. I'm sad about that.(And yes, I do understand and dislike the trend of businesses building on open source without contributing back. There are no clearly correct answers here.)\n \nreply",
      "\"Many people had contributed their efforts to the Redis project for free - both in terms of code but also in advocacy, writing tutorials, publishing example code etc\"I can understand that, but the thing about the BSD license is that such value never gets lost. People are able to fork, and after a fork for the original project to still lead will be require to put something more on the table. Inside Redis it was tried very hard to don't change license. For years it was some kind of \"dogma\", something one could not even speak about. Then after all the kind of attempts, and following the experiences of MongoDB and others, finally this choice was made. (Disclaimer: I saw the switch as an outsider, so my information is not complete about the final decisions -- I for sure know how this was always considered to avoid if other setups were possible).However now I see that there is the case for giving the community something in exchange to the license change: a lot of good things in the core, a very good attitude towards the community, and so forth.\n \nreply",
      "With open-source software, a license is as much a social contract as it is a legal one. People contribute because they want to be part of a community building something which is beneficial to everyone. Everyone contributes where they can, and in turn takes what they need.Redis Ltd. broke the social contract. They decided that the short-term profitability of the company was more important than the project as a whole, the community which had grown around Redis and the future of the software. Probably a wise decision from a business perspective, but that doesn't make it any less of a rug pull from a community perspective.You're right that no pre-relicense code was lost. People could and did fork. But something far more valuable was lost: the community's trust. First they relicensed the code solely for their own benefit, now they tried to take over third-party libraries and started behaving in a hostile way towards the community forks. Why would anyone volunteer their time and effort when it is mainly going to benefit a company which is so openly antagonistic against its volunteers? After all, what's the next shady move going to be?\n \nreply",
      "Open source is a gift economy. Receiving a gift does not form a social contract that entitles you to future gifts. It is not a \"rug pull\" for someone to stop giving you gifts. The old versions of Redis are yours for all time. No one can take that away from you. In fact, Redis is still giving you gifts to this day, just with a different wrapping. The new license seems perfectly reasonable, given how much companies like Amazon have exploited the gift economy to the point where it threatens these startups survival.\n \nreply",
      "> Open source is a gift economy. Receiving a gift does not form a social contract that entitles you to future giftsI think op was talking about contributors, who essentially gifted back. One might take offense if they were exchanging gifts with someone, and they open a pop-up store and sell what was gifted.Also, it's also a tiny bit hypocritical to expect revenue sharing with Amazon without doing the same for contributors.\n \nreply",
      "I guess there's a blind spot here.antirez gavecommunity gavemost people assume that this create a new thing, a group, which has a shared past and value .. and should continue (i would agree to that personally)\"pure\" open source advocacy would claim \"nothing is ever to be expected in any future\" (i can understand that too but find it a bit sad)\n \nreply",
      "Redis has a CLA so contributors can't use their gifts as leverage to control others.https://redis.io/legal/redis-software-grant-and-contributor-...\n \nreply",
      "I don't think you should conflate CONTROL with resentment or distaste or feeling betrayed or feeling misled. They aren't the same.\n \nreply",
      "Even without a CLA, Redis was originally licensed BSD. Releasing code under a BSD license is making a promise that you are OK with people using your code for commercial purposes. If you as a developer express personal feelings of dissatisfaction that someone is doing with your gift what you gave them permission to do, in such a way that could be construed as reneging on your promise, then that would make you dishonorable and untrustworthy.\n \nreply",
      "> Releasing code under a BSD license is making a promise that you are OK with people using your code for commercial purposes.That's just a microcosm of the larger issue, isn't it? Contributors : Redis :: Redis : AWS - Redis gave Amazon the permission to host and make money off of Redis, but Redis was evidently salty about the state of affairs. I think contributors have reason to be salty too.\n \nreply"
    ],
    "link": "https://antirez.com/news/144",
    "first_paragraph": ""
  },
  {
    "title": "The Google Willow Thing (scottaaronson.blog)",
    "points": 433,
    "submitter": "Bootvis",
    "submit_time": "2024-12-10T16:34:02 1733848442",
    "num_comments": 194,
    "comments_url": "https://news.ycombinator.com/item?id=42378407",
    "comments": [
      "Man, reading this makes me feel so small. Being a \"software engineer\" consuming APIs and updating database rows seems laughably childish compared to whatever the hell it is I just read. I can't even imagine why I should bother trying to understand it. It's completely inaccessible. Only an elite few get to touch these machines.\n \nreply",
      "> I can't even imagine why I should bother trying to understand it.Well, maybe you should just try for the hell of it and see how far you get? Becoming fit seems impossible to a morbidly obese 45 y.o, and it is if that person's expectation is unreasonable, but if they just change it to be more reasonable, break it down into manageable \nroutines, then they can get somewhere eventually.Find some papers, fill many gaps, dedicate a few years in your spare time, in 6 months you'll be 6 months closer than you were.Whether there's a reason or not, idk, it's something to do, be curious. Don't forget that by dedicating their life to something, they're naturally not dedicating their life to other things, things that you might be able to do, like climbing mountains, making pizza, or coming up with witty banter in social situations.\n \nreply",
      "This is very rare, so much I can't remember when was the last time it happened, but I was inspired by your words.Thank you for writing them.\n \nreply",
      "> things that you might be able to do, like  [...] coming up with witty banter in social situationsWell... guess it's time I start learning quantum computing then\n \nreply",
      "As an autistic that brute-forced the \"witty social banter\" skill early on and has recently turned 40... I kinda wish I'd learned quantum computing instead tbh.\n \nreply",
      "MSR has a very clear and accessible tutorial on quantum computing for anyone interested in getting up to speed with the fundamentals: https://www.youtube.com/watch?v=F_Riqjdh2oM .\n \nreply",
      "I have a treadmill though. Even though I don't use it. I can't get a quantum computer.\n \nreply",
      "Here's a free quantum treadmillhttps://www.quantumplayground.net/#/homeYou don't need a \"real\" quantum computer to mess around with quantum computing and learn how it works any more than you need a supercomputer to play around with algorithms and learn how they work.\n \nreply",
      "most experts in that field do not have access to a quantum computer. For the longest times it was a very theoretical field.\nHaving access to a physical machine will not help you for 99% of the knowledge you can acquire in that field right now.\n \nreply",
      "Everyone forgets people have been doing quantum computing research for decades.Shor's algorithm is from 1994.\n \nreply"
    ],
    "link": "https://scottaaronson.blog/?p=8525",
    "first_paragraph": "Yesterday I arrived in Santa Clara for the Q2B (Quantum 2 Business) conference, which starts this morning, and where I\u2019ll be speaking Thursday on \u201cQuantum Algorithms in 2024: How Should We Feel?\u201d and also closing the conference via an Ask-Us-Anything session with John Preskill.  (If you\u2019re at Q2B, reader, come and say hi!)And to coincide with Q2B, yesterday Google\u2019s Quantum group officially announced \u201cWillow,\u201d its new 105-qubit superconducting chip with which it\u2019s demonstrated an error-corrected surface code qubit as well as a new, bigger quantum supremacy experiment based on Random Circuit Sampling. I was lucky to be able to attend Google\u2019s announcement ceremony yesterday afternoon at the Computer History Museum in Mountain View, where friend-of-the-blog-for-decades Dave Bacon and other Google quantum people explained exactly what was done and took questions (the technical level was surprisingly high for this sort of event). I was also lucky to get a personal briefing last week from G"
  },
  {
    "title": "GM exits robotaxi market, will bring Cruise operations in house (cnbc.com)",
    "points": 104,
    "submitter": "atomic128",
    "submit_time": "2024-12-10T21:19:22 1733865562",
    "num_comments": 96,
    "comments_url": "https://news.ycombinator.com/item?id=42381637",
    "comments": [
      "Here's an interesting \"lidar gem\" from Hacker News a few years ago:https://news.ycombinator.com/item?id=33554679Lidar obstacle detection algorithm from a Git repo leaked onto TorThis is a drivable region mapping (obstacle detection) algorithm found in what appears to be a git repo leaked from an autonomous vehicle company in 2017. The repo was available through one or more Tor hidden services for several years.The lidar code appears to be written for the Velodyne HDL-32E. It operates in a series of stages, each stage refining the output of the previous stage. This algorithm is in the second stage. It is the primary obstacle detection method, with the other methods making only small improvements.The leaked code uses a column-major matrix of points and it explicitly handles NaNs (the no-return points). We've rewritten it to use a much more cache-efficient row-major matrix layout and a conditional that will ignore the NaN points without explicit testing.This is an amazingly effective method of obstacle detection, considering its simplicity.\n \nreply",
      "> GM said in a statement that \u201c... an increasingly competitive robotaxi market\u201d were the reasons for the change.Isn't there basically Google/Waymo and then, seemingly much further behind, Tesla Cybertaxi, Amazon/Zoox, and Uber/Yandex? Cruise allegedly has one of the most sophisticated autonomous driving platforms, and GM's Super Cruise (if they share any tech) is comparable to Tesla FSD. Strange that they would bow out.Small anecdote: I visited a GM dealership this week and the salesperson told me Super Cruise was not enabled for test drives. The excuse was pretty weak, like the dealership would have to pay for the service or something. GM might have the technology but they are completely bungling the strategy.Ford just lowered the cost of its BlueCruise subscription by 1/3rd. In an earnings call eight months prior they remarked they made a 70% margin on the service. It seems like drivers did not find the feature compelling and were not renewing. Interest in autonomous driving appears to be cooling across the board.\n \nreply",
      "> Small anecdote: I visited a GM dealership this week and the salesperson told me Super Cruise was not enabled for test drives. The excuse was pretty weak, like the dealership would have to pay for the service or something. GM might have the technology but they are completely bungling the strategy.Sadly this is believable, I've asked to check out things like remote start and control of heating/cooling, and the sales people cannot show those features off because they require an app + subscription tied to the car.\n \nreply",
      "Based on my anecdotal experience, another issue is that salespeople are not trained on the techI worked on a feature for new vehicles and the company failed in part because buyers simply didn\u2019t know those features were part of the vehicle. Dealers never set it up for the buyer and it wasn\u2019t something many people would think to do on their ownA salesperson isn\u2019t going to jeopardize an easy sale by bungling some fancy new feature they can\u2019t control\n \nreply",
      "I\u2019d guess this is at least half the features on my iPhone. I\u2019m sure it can do things that I\u2019m not even thinking a phone can do, but nobody set anything up, and it\u2019s not very obvious or discoverable.",
      "I don't want to be a jerk but the salesperson couldn't pronounce \"autonomous\" which tells me they aren't being trained in selling the feature at all. I can't even remember him referring to it by the marketing name.\n \nreply",
      "And when you go to Tesla for a test drive, they first thing they do is shove it into auto-cruise or whatever they call it whether you want to or not!  At least that is what happened to me when I test drove one 10 years ago.  Maybe they don't do that anymore.\n \nreply",
      "Wow that must have been a wild experience 10 years ago!Today, I think they should do this.  It is rare I need to touch the wheel or pedal these days and people should experience it.\n \nreply",
      "Come experience it between my house and walmart before you go telling people they need to experience it. Good luck having good weather during your drive, here in louisiana.Self driving probably works great on Interstates with numbers like 5, 10, and the feeders. Based on my experiences with subaru self driving and watching videos and watching tesla drivers around here, a self driving car would give up after maybe 3 minutes. There were more teslas and other expensive \"self-driving\" style cars a couple years ago, but now i rarely see them. I wonder why?I can consistently get my wife's subaru self driving to swerve into another lane without warning, without jerking the wheel, with very minor control inputs. Subaru keeps updating the firmware, but they haven't fixed the suicide merge!\n \nreply",
      "Yeah 10 years ago it was a bit much.  It was pretty much just lane keeping and distance keeping, but it didn't read signs or lights yet.  So if you're coming up on a signal it would just keep going!\n \nreply"
    ],
    "link": "https://www.cnbc.com/2024/12/10/gm-halts-funding-of-robotaxi-development-by-cruise.html",
    "first_paragraph": "Credit CardsLoansBankingMortgagesInsuranceCredit MonitoringPersonal FinanceSmall BusinessTaxesHelp for Low Credit ScoresInvestingSELECTAll Credit CardsFind the Credit Card for YouBest Credit CardsBest Rewards Credit CardsBest Travel Credit CardsBest 0% APR Credit CardsBest Balance Transfer Credit CardsBest Cash Back Credit CardsBest Credit Card Welcome BonusesBest Credit Cards to Build CreditSELECTAll LoansFind the Best Personal Loan for YouBest Personal LoansBest Debt Consolidation LoansBest Loans to Refinance Credit Card DebtBest Loans with Fast FundingBest Small Personal LoansBest Large Personal LoansBest Personal Loans to Apply OnlineBest Student Loan RefinanceSELECTAll BankingFind the Savings Account for YouBest High Yield Savings AccountsBest Big Bank Savings AccountsBest Big Bank Checking AccountsBest No Fee Checking AccountsNo Overdraft Fee Checking AccountsBest Checking Account BonusesBest Money Market AccountsBest CDsBest Credit UnionsSELECTAll MortgagesBest MortgagesBest Mor"
  },
  {
    "title": "Electric (Postgres sync engine) BETA release (electric-sql.com)",
    "points": 16,
    "submitter": "austinbirch",
    "submit_time": "2024-12-11T00:04:41 1733875481",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://electric-sql.com/blog/2024/12/10/electric-beta-release",
    "first_paragraph": "Electric BETA releaseLocal-first with your existing APIA new approach to building ElectricElectric v0.11 released with support for Postgres in the clientElectric v0.10 released with shape filteringElectrify, Ignition, Liftoff!Local AI with Postgres, pgvector and llama2, inside a Tauri appElectricSQL v0.9 releasedSecure transactions with local-firstElectricSQL v0.8 released with JSON and Supabase supportUse ElectricSQL with the Ionic Framework and CapacitorElectricSQL v0.7 releasedElectricSQL hosted the first \"Local-first Software London\" meet-upLinearlite - A local-first app built with ElectricSQL and ReactWelcome Sam Willis!Local-first sync for Postgres from the inventors of CRDTsWelcome Andrei and Oleksii!Developing local-first softwareWelcome Jos\u00e9, Kevin and Garry!The evolution of state transferRelativity and causal consistencyIntroducing Rich-CRDTsBy\u00a0Kyle Mathews\u00a0With version 1.0.0-beta.1 the Electric sync engine is now in BETA!If you haven't checked out Electric recently, it's a g"
  },
  {
    "title": "Show HN: Don't let your billion-dollar ideas die (ideaharbor.xyz)",
    "points": 53,
    "submitter": "curiousmindset",
    "submit_time": "2024-12-10T22:45:15 1733870715",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=42382506",
    "comments": [
      "> A bidding market and app where beggars can bid for the most prime corners and intersections.> A non-cancerous company review board (f glassdoor)> Augmented-reality underwearGood to know the internet is functioning as usual.\n \nreply",
      "Pro-tip: add an embarrassing story of yourself to force you to work on the idea before it expires (you can prevent an idea from expiring, right?)\n \nreply",
      "> Nuclear LandminesYou underestimate how stupid Cold War weapon projects got, they were putting nukes in anything.https://en.wikipedia.org/wiki/Atomic_demolition_munitionhttps://en.wikipedia.org/wiki/Blue_Peacock\n \nreply",
      "Bat bombs (bombs full of bats with time-delayed releases of napalm) were developed in World War II.\n \nreply",
      "Documented in Harvard professor Louis Fieser's autobiographical The Scientific Method: A Personal Account of Unusual Projects in War and in Peacehttps://library.sciencemadness.org/library/books/the_scienti...https://en.wikipedia.org/wiki/Louis_Fieser\n \nreply",
      "I mean, if you need to apply 10+ tons of TNT-equivalent to a target, it's a heck of a lot easier to carry. (Also literally, SADM)\n \nreply",
      "This is the HN equivalent of Boaty McBoatface. I love it.\n \nreply",
      "This is like highdeas without the charm.\n \nreply",
      "An earlier effort along similar lines can be seen at https://www.halfbakery.com\n \nreply",
      "\"start podcast that is only ads\"Brilliant\n \nreply"
    ],
    "link": "https://ideaharbor.xyz",
    "first_paragraph": ""
  },
  {
    "title": "One of the last Navajo code-talkers died on October 19th, aged 107 (economist.com)",
    "points": 90,
    "submitter": "helsinkiandrew",
    "submit_time": "2024-12-08T07:37:35 1733643455",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=42355609",
    "comments": [
      "https://archive.ph/vBX2o",
      "The Japanese tried to crack the code by torturing a poor guy called Joe Kieyoomia who was Navajo, but not a code-talker or even aware of the existence of the program, but they didn't get far.  Amazingly, he survived the concentration camp, the Bataan death march and getting nuked in Nagasaki, returning to the US and living until 77.https://en.wikipedia.org/wiki/Joe_Kieyoomia\n \nreply",
      "It's worth noting just how extreme the levels of crime against humanity committed by the Japanese were.The Germans (and Swiss...) for the most part imprisoned / shot PoWs.The Japanese used PoWs for chemical and biological weapon testing and vivisection, and they would find the most depraved ways to draw out mass torture of PoWs.For example, they would do things like put US soldiers into baskets so small PoWs could not move at all inside them, load them into unventilated railroad boxcars with no water, then load them into ships, sail out to sea, and drop them overboard into shark-infested waters.Then there's the Nanjing Massacre where in a matter of days Japanese soldiers slaughtered something like 300,000 Chinese civilians at a rate of around 7,000 civilians a day. That was just one of the more than a dozen massacres. It's widely believed that Japan slaughtered more civilians than the Germans.The sheer level of depravity is astounding. And what's especially disturbing are the lengths conservatives in the Japanese government have gone to over the last several decades to rewrite history and erase these atrocities, while pushing harder and harder against the restrictions on their armed forces.\n \nreply",
      "Also worth noting that along with Operation Paperclip, where the US recruited Nazis to bootstrap their space program, the US also recruited Japanese war criminals to continue their experimentation in torture and chemical and biological warfare, which led to (among other things) MKULTRA.\n \nreply",
      "I'm sorry \"the Swiss\" had PoW as a neutral party?Also - the Soviet PoWs in the camps would like to have a word with you.\n \nreply",
      "Mostly pilots of planes that ended up in Switzerland. Without GPS or mid-air refueling that did happen a lot. Swiss neutrality meant shooting at any military that entered their territory, no matter which side they were on, and taking anyone who surrendered captive.Anything else could be seen as helping one side or the other, which wouldn't be very neutral. Not that they were always above helping them in non-military ways\n \nreply",
      "Related. Others?Why Navajo is one of the most difficult languages (2023) - https://news.ycombinator.com/item?id=41097075 - July 2024 (84 comments)Why Navajo is the hardest language to learn - https://news.ycombinator.com/item?id=38484528 - Dec 2023 (1 comment)Samuel Sandoval, among last Navajo Code Talkers, dies at 98 - https://news.ycombinator.com/item?id=32297121 - July 2022 (1 comment)Navajo Code Talker John Pinto Dies, Age 94 - https://news.ycombinator.com/item?id=20008374 - May 2019 (2 comments)Navajo Code Talkers - https://news.ycombinator.com/item?id=10059642 - Aug 2015 (2 comments)Last Of The Navajo 'Code Talkers' Dies At 93 - https://news.ycombinator.com/item?id=7848945 - June 2014 (19 comments)\n \nreply",
      "That last link title from a decade ago seems at odds with the title of this article.\n \nreply",
      "The obituaries are often my favourite part of the Economist. They are edited by Ann Wroe - https://en.wikipedia.org/wiki/Ann_Wroe - a true legend.They focus mostly on long-forgotten people and create an intense glimpse into the short timeframe when their life made a big impact.My favourite obit of all time of hers is the one of Bill Millin:\nhttps://www.economist.com/obituary/2010/08/26/bill-millin (https://archive.is/iZifs)\n \nreply",
      "And this is another great one, also in the vein of 'the last one' about the death of the last of the Beguines:\nhttps://www.economist.com/obituary/2013/04/27/marcella-patty... (https://archive.is/ClVez)\n \nreply"
    ],
    "link": "https://www.economist.com/obituary/2024/12/05/john-kinsel-used-his-own-language-to-fool-the-japanese",
    "first_paragraph": ""
  },
  {
    "title": "Launch HN: Double (YC W24) \u2013 Index Investing with 0% Expense Ratios",
    "points": 293,
    "submitter": "jjmaxwell4",
    "submit_time": "2024-12-10T14:16:54 1733840214",
    "num_comments": 339,
    "comments_url": "https://news.ycombinator.com/item?id=42377018",
    "comments": [
      "Ummm, have y'all thought about spread costs?If you look at the spread of any of these ETF's mentioned (spread = ask px - bid px), you will notice that the spread is much smaller than if you were to sum up the spreads of each component stock.That's possible because of a mature ecosystem of ETF market makers and arbitrageurs (like Jane Street).If you buy all of the stocks individually, as it sounds like y'all's solution does, you will pay the spread cost for every. single. stock. The magnitude of these costs are not huge, but if we're comparing them against VOO's 17 bps/yr expense ratio, it's worth quantifying them.I imagine eventually you can hope that market makers will be able to quote a tight spread on whatever the basket of stocks a client wants, but in the meantime, users would be bleeding money to these costs.(Source: I work in market making and think about spreads more than I would like to admit.)\n \nreply",
      "I\u2019d like an answer to this question as well.\n \nreply",
      "I saw the \"Your Money is Secure\" section, but after things like the Synapse fiasco, I would like to get confirmation from you.It says my money would be SIPC insured, which means if anything goes missing (obviously not through loss of equity value, but through missing funds or a ledger bug), I get my money back, up to the SIPC limit, right? I just want to ensure this isn't the same situation with fintechs that say your money is \"FDIC insured\", but that only protects you if the bank fails, not if the fintech goes bankrupt.I'm just really, really wary of new fintech products to save like .3% on fees when I hear all these horror stories of people trusting fintech startups with their money any then losing 95% of their deposits like the Yotta customers.\n \nreply",
      "If Double goes out of business, your assets are safe and held in your name at Apex Clearing. They have processes in place for these scenarios to help you access and transfer those assets.SIPC protection covers against a brokerage firm failing, which in our case is Apex Clearing. We are not currently a brokerage so SIPC would not apply if Double goes bankrupt.\n \nreply",
      "> SIPC protection covers against a brokerage firm failing, which in our case is Apex Clearing. We are not currently a brokerage so SIPC would not apply if Double goes bankrupt.I thank you for being upfront and honest about this. The tough spot you'll find yourself in, then, is that if any money goes missing between you and Apex, customers are completely SOL. This is not a theoretical risk, this is exactly what happened in the Yotta/Synapse fiasco. Even if I trust that you guys are much better technologists than Synapse, would I be willing to take that risk for a teeny, teeny reduction in fees compared to an index ETF? Sorry, not for me.EDIT: Wanted to put an edit up here so that it doesn't get lost. Thanks for your response below - for me, that was the critical information I needed, that I can directly verify that my SIPC-insured funds are held by the SIPC-insured entity. That was indeed not the case with Yotta/Synapse (and, indeed, most fintechs who keep customer funds in an FBO account at a partner bank), so I really appreciate the clarification. FWIW, I think it might be worth it to add a small blurb in the \"SIPC Insured\" section saying that your insured funds can be verified at any time.Kudos, you guys have thought through a good deal of the important details, and sufficiently assuaged my concerns.\n \nreply",
      "I'd argue the specifics are quite a bit different than Yotta/Synapse.We do not hold any funds ourselves. You connect your bank and ach/wire money to an Apex bank account. You can verify your holdings via apex anytime (see: https://help.double.finance/en/articles/10262406-how-can-i-v...)\n \nreply",
      "Yotta does not hold any funds themselves. You connect your bank and ach/wire money into an Evolve bank account.The problem is that unbeknownst to users, Evolve had no record of what belonged to which user\u2014it all came via Synapse on behalf of Yotta. And when Synapse went bankrupt, everyone pointed fingers about where the money is and who it belongs to.\n \nreply",
      "Will reply directly to your comment, as I started the concern in this thread, and I think it's important to point out that the situation is materially different based on what jjmaxwell4 has responded.With Evolve, money was just pooled into an \"FBO\" (\"for benefit of\") account, and not ledgered directly to individual users. This is apparently not the case with Apex since you can verify your balance with them directly. They report your balance, so if any money goes missing, you should have an insurable case with them directly.\n \nreply",
      ">> If Double goes out of business, your assets are safe and held in your name at Apex Clearing. They have processes in place for these scenarios to help you access and transfer those assets.\n>> SIPC protection covers against a brokerage firm failing, which in our case is Apex Clearing. We are not currently a brokerage so SIPC would not apply if Double goes bankrupt.Dear @jjmaxwell4 -- I'm not really worried about your service given you're a layer atop Apex, however, this is a very common conversation happening right now on many forums -- could you clarify a bit more, how one would \"get comfortable\" with a new product?I'm assuming the list is something like this, but that is an non-expert guess:- Is the institution i'm interacting with regulated (in your case, Yes, Double is regulated by The SEC)- Who holds my funds, and are they regulated (in your case, the funds are held by Apex Clearing, and if I understand correctly, Apex is a broker dealer regulated by The SEC)- Are the funds held in my name or pooled in with other money? (in your case, I think the funds are held by Apex only in my name)I think one of the problems with the Yotta/Synapse/Evolve collapse is -- its unclear how one even evaluates their level of risk.It is also unclear how one validates SIPC coverage, like could I go to SIPC and enter an account number and validate the funds are actually covered somewhere across the layers?Would be great for someone who knows this area to comment.\n \nreply",
      "Appreciate diving into the details!You can sign up directly with Apex (completely separate login) and view your holdings in your name in their web portal, along with all documents that Double sends you on your account activity. The process requires a bit of verification so I've written up a help article here on how to get set up: https://help.double.finance/en/articles/10262406-how-can-i-v...\n \nreply"
    ],
    "link": "item?id=42377018",
    "first_paragraph": ""
  },
  {
    "title": "U.S. math scores drop on major international test (chalkbeat.org)",
    "points": 169,
    "submitter": "akantler",
    "submit_time": "2024-12-10T17:24:13 1733851453",
    "num_comments": 331,
    "comments_url": "https://news.ycombinator.com/item?id=42378929",
    "comments": [
      "The report: https://timss2023.org/results/",
      "Did the test collect any data on the demographics of the takers, so that we can check whether the drop is within-group, or whether it\u2019s just artifact of changing composition? For all we know, this might be just a case of Simpson paradox, where each subgroup actually improved, but the overall score distribution shifted downwards due to school children population growing more and more immigrant, after years of mass immigration. While US has excellent education, and immigrant children in America do better than their counterparts in their countries of origin, they don\u2019t do as well as the modal group of white Americans, so as the composition changes, and the whites become less of a statistically dominating factor, the scores are expected to go down, even as the education quality improves.\n \nreply",
      "Thank you for the high quality comment. I'll try to add some insight here for race. Using the reports [1] and [2], the difference for Grade 4 students are:American Indian / Alaska Native - 2.5% in 2019 - 1.6% in 2023 - 515 in 2019 - 504 in 2023Asian - 5.3% in 2019 - 4.3% in 2023 - 586 in 2019 - 571 in 2023Black - 13.2% in 2019 - 15.5% in 2023 - 494 in 2019 - 468 in 2023Hispanic - 25.8% in 2019 - 26.3% in 2023 - 508 in 2019 - 491 in 2023Native Hawaiian / Other Pacific Islander - 1.7% in 2019 - 0.9% in 2023 - 500 in 2019 - 457 in 2023Two or more races - 5.6% in 2019 - 8.1% in 2023 - 554 in 2019 - 542 in 2023White - 45.9% in 2019 - 43.2% in 2023 - 559 in 2019 - 543 in 2023It looks like all groups suffered at least 10 points in loss but these effects are definitely exaggerated by the reweighing of population proportions.[1] - https://nces.ed.gov/timss/results19/index.asp#/math/achievem...[2] - https://nces.ed.gov/timss/results23/index.asp#/math/achievem...\n \nreply",
      "Thank you for looking it up! My personal hunch is that COVID related disruption in schooling is responsible for most of the within-group drop, and the rest is mostly a result of changes in educational policy, where many places deemphasize objective measures and standards, causing students to care less for these on the margin.\n \nreply",
      "Personally, I suspect there's likely a generational non-COVID caused reduction in child welfare at play, the leading edge of which happened to get hidden by COVID.The combo of more and more parents who work all the time and/or have delegated parenting to YouTube and mobile devices, plus charter schools (1) moving a chunk of non-IEP kids into more non-secular schools with lower standards and (2) concentrating behavior issue and IEP kids in public schools are basically rotting things from the inside out.\n \nreply",
      "Why would \"COVID\" only impact the US? It's a global pandemic? If anything the comment would be exclusively about education policies. Because the education policies around COVID would be worse. But also just looking around at my own region of the US, there's a lot of talk about how to lower the school budgets where I live. Which would be lowering on top of inflation's effects Seems like a pretty easy correlation in my head.Aside: Your 2 comments are sounding alarm bells in my head, and I can't pinpoint why. Especially when the numbers seem to demonstrate the exact opposite of what you're describing in your first comment (scores amongst white students seem to shift downward).\n \nreply",
      "It probably varies by grade and locale.My oldest is in 4th grade now, and the most relevant maths are from 2nd and 3rd grade.   Basic division, multiplication, and fractions matters (3rd grade) - but so does adding and subtracting multi-digit numbers (2nd grade) because multiplication/division is now multiple digit (which is the actual, new, 4th grade material).  Interestingly, she used almost no adding and subtracting in 3rd grade, to the point where the teacher supplemented required coursework to help stave off attrition, so you could actually get by 3rd grade while being terrible at adding and subtracting.Her covid year was kinder.  It made her cohort pretty bad at writing.  But that seems to have largely worked itself out over the past 4 years.It's not clear to me when these tests were taken, and that kinda matters - were they by people who were starting 4th grade, finishing 4th grade, or completed 4th grade?It matters because it tells us what they missed due to COVID - if these 4th graders had 1st grade for COVID, I'm not sure if that would be a huge deal.  The most relevant bits are also taught in kinder, and they cover adding and subtracting again in 2nd.  But if their main COVID year was 2nd, I could see 4th being a huge problem, especially with the general lack of adding and subtracting in 3rd grade.\n \nreply",
      "> My oldest is in 4th grade now, and the most relevant maths are from 2nd and 3rd grade. Basic division, multiplication, and fractions matters (3rd grade) - but so does adding and subtracting multi-digit numbers (2nd grade) because multiplication/division is now multiple digit (which is the actual, new, 4th grade material)Are the standards really that low?  AIUI in most developed countries w/ the significant exception of the U.S. (and to a lesser extent, other English speaking countries), 4th and 5th grade math are for teaching the earliest elements of pre-algebra.  (Meaning that algebraic expressions are very much not used, but the style of reasoning is clearly intended as preparation for that).  They may get a lot of practice with fractions or multiple-digit arithmetic, but the basic procedures are not new to them in 4th grade.  It seems to me that the U.S. educational system is underserving these kids quite significantly by not catering to their potential and actual skills for mathematical reasoning, which while not fully developed (and very much tied to \"concrete\" skillsets, as opposed to a real capacity for abstraction) are still quite substantial.\n \nreply",
      "You might double check that the way you number the grades is the same as the way the parent comment numbers them. For example, as I understand it, in the US most kindergarteners are 5 years old and most first graders are 6, while in NZ most first graders are 5 years old and most second graders are six. So to convert from US grades to NZ you add one because in the US kindergarten isn't given a number.The US has a highly regional system, but as I understand it pre-algebra is taught starting around sixth grade (~11 year olds), which may line up a little closer to your expectations.\n \nreply",
      "> The US has a highly regional system, but as I understand it pre-algebra is taught starting around sixth gradeIn most advanced countries this would be the beginning of Junior High a.k.a. Middle School.  By that time the students have been thoroughly introduced to pre-algebraic reasoning already, and are broadly getting practice in it (and being taught some more advanced notions around, e.g. exponents and powers, which are of course foundational for later teaching) as preparation for actual algebraic expressions to be introduced.\n \nreply"
    ],
    "link": "https://www.chalkbeat.org/2024/12/04/timss-international-test-result-us-math-scores-decline-post-pandemic/",
    "first_paragraph": "Sign up for Chalkbeat\u2019s free weekly newsletter to keep up with how education is changing across the U.S.U.S. fourth graders saw their math scores drop steeply between 2019 and 2023 on a key international test even as more than a dozen other countries saw their scores improve. Scores dropped even more steeply for American eighth graders, a grade where only three countries saw increases. The declines in fourth grade mathematics in the U.S. were among the largest in the participating countries, though American students are still in the middle of the pack internationally. The extent of the decline seems to be driven by the lowest performing students losing more ground, a worrying trend that predates the pandemic.The results released Wednesday from the Trends in International Mathematics and Science Study, or TIMSS, assessment come from more than 650,000 fourth and eighth graders in 64 countries who took the tests in 2023. The test has been administered every four years since 1995. The resu"
  },
  {
    "title": "Running Durable Workflows in Postgres Using DBOS (supabase.com)",
    "points": 96,
    "submitter": "kiwicopple",
    "submit_time": "2024-12-10T18:47:20 1733856440",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=42379974",
    "comments": [
      "Disclaimer: I'm a co-founder of Hatchet (https://github.com/hatchet-dev/hatchet), which is a Postgres-backed task queue that supports durable execution.> Because a step transition is just a Postgres write (~1ms) versus an async dispatch from an external orchestrator (~100ms), it means DBOS is 25x faster than AWS Step FunctionsDurable execution engines deployed as an external orchestrator will always been slower than direct DB writes, but the 1ms delay versus ~100ms doesn't seem inherent to the orchestrator being external. In the case of Hatchet, pushing work takes ~15ms and invoking the work takes ~1ms if deployed in the same VPC, and 90% of that execution time is on the database. In the best-case, the external orchestrator should take 2x as long to write a step transition (round-trip network call to the orchestrator + database write), so an ideal external orchestrator would be ~2ms of latency here.There are also some tradeoffs to a library-only mode that aren't discussed. How would work that requires global coordination between workers behave in this model? Let's say, for example, a global rate limit -- you'd ideally want to avoid contention on rate limit rows, assuming they're stored in Postgres, but each worker attempting to acquire a rate limit simultaneously would slow down start time significantly (and place additional load on the DB). Whereas with a single external orchestrator (or leader election), you can significantly increase throughput by acquiring rate limits as part of a push-based assignment process.The same problem of coordination arises if many workers are competing for the same work --  for example if a machine crashes while doing work, as described in the article. I'm assuming there's some kind of polling happening which uses FOR UPDATE SKIP LOCKED, which concerns me as you start to scale up the number of workers.\n \nreply",
      "You have a great point that scaling a library model requires careful design, but we think that's worth it to provide a superior developer experience.For example, the DBOS library provides an API to instruct a worker to recover specific tasks. In our hosted platform (DBOS Cloud), when a worker crashes, a central server uses this API to tell the workers what to recover. We like this design because it provides the best of both worlds--the coordination decision is centralized, so it's performant/scalable, but the actual recovery and workflow execution is done in-process, so DBOS doesn't turn your program into a distributed system the way Step Functions/Temporal do (I haven't used Hatchet).\n \nreply",
      "Definitely agree that the dev experience is better with a library, particularly for lightweight and low-volume tasks (Hatchet is also moving in the same direction, we'll be releasing library-only mode this month). And I really like the transactional safety built into DBOS!My concern is as you start to see higher volume, more workers, or load patterns that don't correspond to your primary API. At that point, a dedicated database and service to orchestrate tasks starts to become more necessary.\n \nreply",
      "I disagree.  Once you get to the scale that breaks a library based system like DBOS, you need to move away from a central coordinator.  At that point your software has to adjust and you have to build your application to work without those types of centralized systems.Centralized systems don't scale to the biggest problems no matter how clever you are.And usually the best way to scale is to decentralize.  Make idempotent updates that can be done more locally and use eventual consistency to keep things aligned.\n \nreply",
      "You can get around some of this coordination by doing more work locally, in-process -- but you're risking the availability of your primary API. The work that you're doing as a background job or workflow may:1. Be unbounded -- 1 API call may correspond to thousands or hundreds of thousands of reads or writes2. Be resource intensive, on either the database side by eating up connections which are needed by your API, or by blocking your event loop/process, particularly in Python and Typescript> Once you get to the scale that breaks a library based system like DBOS, you need to move away from a central coordinatorThere are different levels of coordination here. At some point, workers are always going to have to coordinate \"who is working on what\" -- whether that's by placing locks and using status updates in the database, using a traditional queue, or using a central server like you described with DBOS Cloud. The same goes for a dedicated orchestrator -- those can also partition by \"who is working on which workers.\" In other words, a dedicated service can also be decentralized, these don't seem mutually exclusive.To make this more concrete -- let's say, for example, you have hundreds of workers on a library based system like DBOS which all correspond to individual connections and transactions to read/write data. Whereas in a system like Temporal or Hatchet, you might have 10s of nodes to support 1000s of workers, with the ability to bulk enqueue and dequeue more work, which will increase throughput and reduce DB load rather significantly. You'd lose (most of) the benefits of bulk writes in a library based system.\n \nreply",
      "> # Exactly-once execution> DBOS has a special @DBOS.Transaction decorator. This runs the entire step inside a Postgres transaction. This guarantees exactly-once execution for databases transactional steps.Totally awesome, great work, just a small note... IME a lot of (most?) pg deployments have synchronous replication turned off because it is very tricky to get it to perform well[1]. If you have it turned off, pg could journal the step, formally acknowledge it, and then (as I understand DBOS) totally lose that journal when the primary fails, causing you to re-run the step.When I was on call for pg last, failover with some data loss happened to me twice. So it does happen. I think this is worth noting because if you plan for this to be a hard requirement, (unless I'm mistaken) you need to set up sync replication or you need to plan for this to possibly fail.Lastly, note that the pg docs[1] have this to say about sync replication:> Synchronous replication usually requires carefully planned and placed standby servers to ensure applications perform acceptably. Waiting doesn't utilize system resources, but transaction locks continue to be held until the transfer is confirmed. As a result, incautious use of synchronous replication will reduce performance for database applications because of increased response times and higher contention.I see the DBOS author around here somewhere so if the state of the art for DBOS has changed please do let me know and I'll correct the comment.[1] https://www.postgresql.org/docs/current/warm-standby.html#SY...\n \nreply",
      "Yeah, that's totally fair--DBOS is totally built on Postgres, so it can't provide stronger durability guarantees than your Postgres does. If Postgres  loses data, then DBOS can lose data too. There's no way around that if you're using Postgres for data storage, no matter how you architect the system.\n \nreply",
      "I built a small side thing using DBOS ( using python SDK) and the ergonomics were pretty nice.Then I found out Qian Li and Peter Kraft from that team are sharing breakdowns of interesting database research papers on twitter and I've been following them for that.https://x.com/petereliaskraft/status/1862937787420295672\n \nreply",
      "Does DBOS offer a way to get messages in or data out of running workflows? (Similar to signals/queries in Temporal) Interested in long-running workflows, and this particular area seemed to be lacking last time I looked into it.I don\u2019t want to just sleep; I want a workflow to be able to respond to an event.\n \nreply",
      "Yes, absolutely, that's an extremely important use case! In DBOS you can send messages to workflows and workflows can publish events that others can read. It's all backed by Postgres (LISTEN/NOTIFY under the hood).Documentation: https://docs.dbos.dev/python/tutorials/workflow-tutorial#wor...Here's a demo e-commerce application that uses messages and events to build an interactive long-running checkout workflow: https://docs.dbos.dev/python/examples/widget-store\n \nreply"
    ],
    "link": "https://supabase.com/blog/durable-workflows-in-postgres-dbos",
    "first_paragraph": "EnterprisePricingDocsBlog10 Dec 2024\u20226 minute readMichael Stonebraker is the inventor of Postgres and a Turing Award winner. His latest venture is DBOS, a three-year joint research project between Stanford and MIT. The DBOS team have built a Durable Workflow engine using Postgres. It's one of the of the more elegant designs I've seen, leveraging the features of Postgres to keep it lightweight and fast.The DBOS team have released a Supabase integration, so you can use your Postgres database as a durable workflow engine.Continue reading or just get started?I really love the design of DBOS, so I'm going to write more below. Their design is aligned with our philosophy at Supabase: \u201cjust use Postgres\u201d. I'll take you through the lower-level details in the rest of this post. If you just want to get started using DBOS with Supabase, get started using their tutorial:Use DBOS With Supabase \u2192Let's start with a common situation where a workflow is useful: you're running an e-commerce platform wher"
  },
  {
    "title": "Training LLMs to Reason in a Continuous Latent Space (arxiv.org)",
    "points": 171,
    "submitter": "omarsar",
    "submit_time": "2024-12-10T16:26:17 1733847977",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=42378335",
    "comments": [
      "I've been looking into using the last hidden layer of an off-the-shelf LLM to help my company with a classification task. The last hidden layer is obviously super rich in semantic information because it has to somehow tell the next layer how to generate the next token prediction. That final layer, in some respects, is discarding valuable context information that the final hidden layer encodes.I am not surprised at all that Meta was able to generate some positive returns by feeding the last hidden layer back into the model auto-regressively.The method of training they describe in the paper is really cool. Summarized in Figure 2, they train it with a corpus of step-by-step text instructions and then across multiple stages, they iteratively replace one of the textual steps with a last-hidden-layer embedding and see what the model spits out. The weights are then updated through cross-entropy loss as the additional text tokens are generated once again.So they're basically rewinding the output, replacing an increasing number of textual steps with hidden state embeddings, and playing it forward as the model gradually learns to do all of its step-by-step thinking using just the hidden state data.In a way, this might be how humans learn to think through language. Our parents teach us using words and our brain gradually replaces the words with thoughts until we can replicate the action or solve the problem ourselves without anyone guiding us with words.\n \nreply",
      "Indeed, I would not be surprised if OpenAI one day admits that the `o1` model uses the last hidden layer (or some other intermediate layer) to feed the \"thought process\" that you can watch as it \"thinks\" about the answer. I suspect that they may take the last hidden layer and feed it back into the front of the `o1` model while also feeding a separate, likely much smaller LLM that generates the \"thought process\" as language tokens.In this manner, the model makes use of the rich semantic information encoded at the last hidden layer while informing the user via an extraction of that hidden layer specifically tuned to generate human-legible concepts such as, \"I'm considering the impact of converting the units from kilograms to pounds,\" or whatever.\n \nreply",
      "I don't think it does, because from this paper this kind of  backfeeding is apparently quite difficult to train.I've said it before, but I think it's just something like Quiet-STaR, but simplified. They have a bunch of question answer pairs, many of which are difficult. They generate a lot of tokens from the question (let's say, 3x the length of the expected answer), summarise whatever is generated and reinforce whenever it generates the right answer.I don't think o1 is something complicated.\n \nreply",
      "That's certainly possible, but it reminds me a bit of a similar thing I've seen in their UI that rhymes in a way that makes me think otherwise. In the code interpreter tool, you have a little preview of the \"steps\" it's following as it writes code. This turns out to just be the contents of the last written/streamed comment line. It's a neat UI idea I think - pretty simple and works well. I wouldn't be surprised if that's what's going on with o1 too - the thought process is structured in some way, and they take the headings or section names and just display that.\n \nreply",
      "> using the last hidden layeriirc this is a well supported task iirc called \"classification  head\" instead of \"language modeling head\" in case anyone else wants to do this as a fine-tuning project\n \nreply",
      "This is intriguing. When I learned that a lot of people do not have inner monologue, I was fascinated by the fact that people can differ on such seemingly fundamental way of being. Maybe those who have it just have a \"tee\" that pipes into words.\n \nreply",
      "\"...because it has to somehow tell the next layer how to generate the next token prediction.\" -- This isn't actually true in the case of transformers. Features in the final TF layer at time t in a sequence do not depend on the features in the final TF layer at any other time step. Recurrence in transformers is done \"depthwise\" via \"causally masked\" convolutions. Final layer features at time t can depend on penultimate layer features at time t-1, but not on final layer features at time t-1.\n \nreply",
      "you are misunderstanding what the person is saying. They are saying the final hidden layer outputs a vector which has all the information that decides the logits which decide the probabilities of each token in the entire vocabulary. Ie, it is storing a lot of information.\n \nreply",
      "I like the direction of the research of working in latent space but feeding the last layer representation back as a first layer embedding feels sketchy to me. Those layers have different representation space.\n \nreply",
      "> Those layers have different representation space.Do they? Interpretability techniques like the Logit Lens [1] wouldn't work if this were the case. That author found that at least for GPT-2, the network almost immediately transforms its hidden state into a \"logitable\" form: you can unproject the hidden state of any layer to see how that layer incrementally refines the next token prediction.[1]: https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreti...\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2412.06769",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "The Albuquerque \"Broken Arrow\" Nuclear Accident (lflank.wordpress.com)",
    "points": 15,
    "submitter": "dxs",
    "submit_time": "2024-12-10T23:12:21 1733872341",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://lflank.wordpress.com/2024/12/10/the-albuquerque-broken-arrow-nuclear-accident/",
    "first_paragraph": "In 1957, a B-36 nuclear bomber accidentally dropped a Mark 17 hydrogen bomb while landing at Kirtland Air Force Base just outside of Albuquerque NM.In January 1950, President Harry Truman announced that the United States would begin a crash program to develop and deploy a weapon that was stillentirely theoretical \u2014 the hydrogen bomb.In contrast to the uranium and plutonium atomic bombs\u00a0which depend for their energy on nuclear fission,\u00a0in which heavy nuclei are broken into lighter components, the hydrogen bomb depends on the energy released during nuclear fusion,\u00a0in which small light nuclei are\u00a0fused together to produce larger heavier nuclei. Physicists had known for decades that the fusion process releases an enormous amount of energy, and that nuclear fusion of hydrogen to make helium was the process that fueled the cores of stars. Since nuclear fusion required fantastically high temperatures, it was presumed that no earthly process would be able to produce it.During their work on the"
  },
  {
    "title": "NAND Flash Targets 1k Layers (semiengineering.com)",
    "points": 18,
    "submitter": "rbanffy",
    "submit_time": "2024-12-08T18:54:15 1733684055",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=42359299",
    "comments": [
      ">export rules restrict stacks of more than 128 layers. So countries subject to those restrictions get an end-around by simply stacking 128-layer modulesI'm interested in the story behind this.\n \nreply",
      "The second paragraph:> Those additional layers will add new reliability issues a number of incremental reliability challenges, but the NAND flash industry has been steadily increasing the stack height for nearly a decade. In 2015, Toshiba announced the first 16-die stack using through-silicon vias. That enabled higher bandwidth, reduced latency, and faster I/O, while also helping to pave the way for stacking of other types of memory and logic chips.Stacking multiple dies (with or without TSVs) is an entirely different kind of stacking from building multiple layers of memory cells on a single die, which is what 3D NAND is all about. Samsung was the first to deliver 3D NAND, and IIRC Toshiba was tied for fourth to reach that milestone.\n \nreply"
    ],
    "link": "https://semiengineering.com/nand-flash-targets-1000-layers/",
    "first_paragraph": ""
  },
  {
    "title": "Boltzmann brain (wikipedia.org)",
    "points": 63,
    "submitter": "josephwegner",
    "submit_time": "2024-12-10T05:22:50 1733808170",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=42374027",
    "comments": [
      "> Over a sufficiently long time, random fluctuations could cause particles to spontaneously form literally any structure of any degree of complexity, including a functioning human brainCan anyone explain this bit to me? The formation of biological brains was a multi-billion year climb against entropy. How would a brain form spontaneously without those random fluctuations tearing the constituent components apart?I\u2019m having trouble understanding the logic here. Random fluctuations don\u2019t imply that any order from those fluctuations can be preserved. The higher order features like brains are path dependent on something resisting those random fluctuations to allow something stable to form, whether that\u2019s an atomic particle, cell, organ, or organism.IANAP and I don\u2019t know what I\u2019m talking about\n \nreply",
      "The point isn't that they can be preserved; you're right that they probably wouldn't be.  But all it takes is a single instant where \"you\" are randomly \"conjured\" into being: your Boltzmann brain has all your memories and everything.  Even if that brain dissipates a small fraction of a second later, for that tiny amount of time, you would believe in the world as we all know it.And the bizarre thing about this is you can't say \"oh look, I've been in existence long enough to type this sentence, so I and my memories and reality must be real, and not just a random spontaneous formation of a brain\", because at every single instant, you could be that brand-new Boltzmann brain, never before formed, and the bits where you thought \"oh look, I've been in existence long enough...\" are just the memories spontaneously implanted into that brain.It's kinda wild.  I'm not sure I buy it as a realistic possibility, but it can be fun (or terrifying) to think about.\n \nreply",
      "This seems like a physicist with no philosophical background spontaneously (heh) discovering our epistemic hard-dependency on sensory perception.In terms of philosophy, what's here that can't be found in, say, Descartes or Hume?\n \nreply",
      "The difference between this concept and, let's say, Descarte's evil demon, is not the philosophical skepticism but its explicit grounding in physics and thermodynamics. It basically attempts to answer the question \"Where would that evil demon come from?\". It materializes Descarte's thought experiment and shows that it could actually happen within the confines of our scientific knowledge, unlike malicious demons.\n \nreply",
      "> How would a brain form spontaneously without those random fluctuations tearing the constituent components apart?>> Over a sufficiently long time\n \nreply",
      "It's about quantum mechanics and the fact that \"empty space\" is not really empty. Particles do pop into existence (from nothing), according to QM, so there's a non-zero probability for any \"pattern\" to pop into existence. Sort of like if you have an infinite number of coin flips then at some place and time you'll land on heads a million times in a row, no matter how unlikely. And for any million-bit sequence you're guaranteed to hit it too. So a human \"brain\" is just a pattern that's likewise guaranteed to be \"encountered\".A similar concept is how the first replicator RNA/DNA got created as the beginning of life. If RNA can exist in large numbers of random sequences, then a sequence that can replicate itself only has to \"happen\" once and then life is started and will never slow down but will grow in complexity, as long as the environment can support it.\n \nreply",
      "I get the part about popping into existence but how would even so much as a bilipid layer around a single cell form, let alone a whole brain? Where are all the antiparticles in this, except shooting around? Even if all the particles pop into existence perfectly placed for a single quanta of Plank time, aren\u2019t the antiparticles destructively interfering with all the other particles even before they begin to annihilate? I imagine they would prevent any chemical reaction happening at all at that density. What about all the force carriers? Can they even pop into existence in the quantum foam coupled?It wouldn\u2019t really resemble a brain in biological sense of the world because the only stimuli it can and will react to is its own disintegration. It\u2019s hard to justify it even \u201cexisting\u201d at all. A \u201cvirtual\u201d brain in the sense of virtual particles perhaps, except it seems quantitatively useless.\n \nreply",
      "The probability of even a Hydrogen atom popping into existence is astronomically low, but the point is that one could. And if one can, then many can, and on and on. The point is that it's not impossible, just improbable. But by definition of one did pop into existence, it's a stable state and would therefore not just simply vanish after forming.\n \nreply",
      "There is a widely circulated (amongst philosophers) argument against the Boltzmann brain hypothesis made on bayesian grounds. Technical, but very interesting. It's being published next year in what's generally regarded as the top academic journal for philosophy: https://philpapers.org/archive/DOGWIA-6.pdf\n \nreply",
      "Their argument hinges on one particular claim:> As unlikely as it is for a BB to form at all, it\u2019s drastically more unlikely for additional things to simultaneously form around it. And even if additional things did form around the BB, it is not especially likely they will be the kind of stable and sensible objects a brain could even perceive. Therefore, the evidence we have is more likely supposing we are OOs than supposing we are BBs.which doesn't ring true to me. Assuming that universe is indeed dominated by BBs, it's not at all clear to me that any observations we could possibly make \"is more likely supposing that we are OOs\". While the number of BBs \"with decorations\" would be dwarfed by the number of BBs without, it is still entirely feasible that there are many more such BBs than there are OOs.I also found their argument as to why all observations shouldn't be considered hallucinations (including \"over time\", \"history of\" etc) as a matter of probability to be incomprehensible.\n \nreply"
    ],
    "link": "https://en.wikipedia.org/wiki/Boltzmann_brain",
    "first_paragraph": "The Boltzmann brain thought experiment suggests that it might be more likely for a brain to spontaneously form in space, complete with a memory of having existed in our universe, rather than for the entire universe to come about in the manner cosmologists think it actually did. Physicists use the Boltzmann brain thought experiment as a reductio ad absurdum argument for evaluating competing scientific theories.\nIn contrast to brain in a vat thought experiments, which are about perception and thought, Boltzmann brains are used in cosmology to test our assumptions about thermodynamics and the development of the universe. Over a sufficiently long time, random fluctuations could cause particles to spontaneously form literally any structure of any degree of complexity, including a functioning human brain. The scenario initially involved only a single brain with false memories, but physicist Sean M. Carroll pointed out that, in a fluctuating universe, the scenario works just as well with enti"
  },
  {
    "title": "B+ Tree Visualization (usfca.edu)",
    "points": 65,
    "submitter": "hamid914",
    "submit_time": "2024-12-07T22:25:47 1733610347",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42353333",
    "comments": [
      "There is also https://bplustree.app/ and https://btree.app/\n \nreply",
      "I would love an \"insert random\", or a prefilled / demo mode, or demo set of keys for interesting examples.\n \nreply",
      "I beg of devs, use this (or others as another comment mentioned) to visualize what the poor RDBMS has to deal with when you use non-k-sortable PKs.\n \nreply",
      "Hasnt this been around for a while now?\n \nreply"
    ],
    "link": "https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html",
    "first_paragraph": "Algorithm Visualizations"
  },
  {
    "title": "Seeking Head of Engineering in the Bay Area (Distro S24) (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-12-10T21:02:29 1733864549",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/distro/jobs/XGMjSPe-head-of-engineering",
    "first_paragraph": "The AI co-pilot for sales reps at industrial wholesale distributors.We are seeking an experienced Head of Engineering to lead Distro\u2019s growing tech team.The primary job description is \u201cget things done/make sure things get done\u201d. Key traits are an ownership mentality, clear communication, accountability, and reliability. The candidate must have prior experience managing and delegating to engineers.This role is hands-on and will involve a significant amount of coding for the foreseeable future (in addition to running standups, managing projects, etc.) but will over time gravitate more towards a 70/30% coding/non-coding split depending on the candidate.Professional experience in the platform portion of our tech stack (Next.js/React, TypeScript, MongoDB) is crucial. Additional professional experience in AI/ML frameworks is very-nice-to-have, but at least a strong interest and desire to learn is a requirement. Bonus points for infra/devOps experience.\n\nBay Area candidates strongly preferred"
  },
  {
    "title": "Digital consumption keeps me from getting better at my job (sibervepunk.com)",
    "points": 187,
    "submitter": "siberpunk",
    "submit_time": "2024-12-10T18:46:50 1733856410",
    "num_comments": 65,
    "comments_url": "https://news.ycombinator.com/item?id=42379970",
    "comments": [
      ">> Since graduating from undergraduate studies (which marks exactly one year as I write this post)Author sounds young .. yes, absolutely try to consume less and create more, it's way more life-affirming than the opposite configuration, but:Getting better at your job, like everything else in life, is just a function of time. Show up, and then show up consistently. Put in the time. Be patient. Lead with an open mind and an open heart -- opportunities go to those who are present way more often than those who aren't. Willingly take on shitty jobs, do them well, and you'll find yourself being trusted with bigger and better jobs. Learn when to be the worker bee and when to be the queen bee. Say \"yes\" until you're truly able to say \"no\". Try to accept that, at the end of the day, things don't matter as much as you think they might -- I'm talking about projects, stress, deadlines, shit that floods your veins with cortisol. The only thing people will truly remember is how you made them feel during a crisis, not the minutiae of what you actually contributed -- and those personal relationships will be the gasoline in the engine of your career.I really believe people will go far if they focus on this kind of stuff, and way less on structured self-improvement, productivity hacking, finding \"secrets\", shortcuts via programs, seminars, coaches, and tools, and all that shallow, nutritionless baloney.\n \nreply",
      "Totally agree with everything you've said, brain train for the actual job by doing the actual job and immersing in it as thoroughly and consistently as you can, do things others find boring or difficult.In case the author IS young, I would also add this: Stay away from startups with capricious, absentee or already wealthy founders, find the most productive, stable environment you can.If you work in an environment where personal production and job security are orthogonal you might find yourself getting rug pulled where effort/contribution are fully decoupled from reward.Unless you achieve financial escape velocity or end up in an increasingly rare engineering \"jobs program\" at a large entity, you will get rug pulled at some point due to founder/manager proclivities or due to other macro economic issues.You are probably screwed if this happens to you young enough as it fucks up motivation, it's why among the older programmer crowd you see some former HS dropouts that started professional work way too young (in the early days of the digital revolution) for a toxic company just completely burn out and fuck up their reward circuitry (it's also part of why 2 round leukemia kids have worse longterm outcomes than 1 round or non-leukemia kids).You want to already have experienced patterns of good faith behavior and delay your first rug pull as long as possible.\n \nreply",
      "> If you work in an environment where personal production and job security are orthogonal you might find yourself getting rug pulled where effort/contribution are fully decoupled from reward.This is a great point.When you find yourself in a workplace where job security is based more on vibes than production it creates a false sense of security. You think your personal productivity doesn't matter and that you can vibe your way into the good graces of people making decisions.When jobs security is decoupled from productivity, the winds of the company can and do change frequently. Other people are going to be better at playing the vibes game than you are. Vibes-based companies are overly vulnerable to politics.It's good to work at companies where productivity is tied to personal performance, even if measuring productivty is far from perfect.\n \nreply",
      "> It's good to work at companies where productivity is tied to personal performance, even if measuring productivty is far from perfect.Not in my opinion. These kind of companies ask you to give 100% of yourself every single day. This may sound normal and natural to everyone, but I don\u2019t like it. I cannot work in an environment in which they ask me to be a high performer, and anything else is \u201cbelow expectations\u201d. It\u2019s just not worth it.Much better to have job (in)security detached from productivity: in any case, companies are not to be trusted, and one should be ready to switch at any moment.\n \nreply",
      "> When jobs security is decoupled from productivity, the winds of the company can and do change frequently.But that is always the case, and the \"productivity=security\" environment is artificial and unnatural.The large forces beyond your control dominate the world, and the whole companies  get hit by redundancies because the market changed, AI ate some section of the market, or the banks were too busy playing with mortgages and collapsed the whole global economy.My experience indicates that it's more important to be working on the right thing, than to be super productive working on the wrong thing.\n \nreply",
      "This seems like a tautology, by definition forces on a global scale are far beyond the average person.If they weren\u2019t, then they would cease to significantly affect things on such a scale.\n \nreply",
      "> The only thing people will truly remember is how you made them feel during a crisis, not the minutiae of what you actually contributedI agree with your general point about relationships being the most important, but I disagree that your contributions don't matter. You need to have at least some good contributions combined with good relationships.Through my career I've encountered a lot of coworkers who were fun to be around but either produced very little or had poor quality output. The value of their good vibes declines as the consequences of their low output and/or poor code quality accumulate on the team's shoulders.You also see this a lot when vetting employee referrals. A lot of people will enthusiastically refer friends and people they like being around, but who are not necessarily great contributors. These people seek referrals at a disproportional rate because, well, they tend to be laid off more frequently and might struggle to get through job interviews through normal channels.Employees will refer this people because they want them around, but once you start communicating to people that referrals are equivalent to personally vouching for their referral's abilities, half of the time they start walking back referrals or saying that the person needs a good manager and so on.So agree that relationships matter, but it's going to be hard to build a career on relationships alone. You need some substance and contributions to build upon.\n \nreply",
      "+1. A somewhat similar case that is close to my heart is when people with good intentions make bad decisions. A bad decision is bad, no matter the intentions. If you want to change the world, I think you need to be a \u201cgood person\u201d and _also_ critically evaluate what you do\n \nreply",
      "This. Slow, but consistent is the way to win. Like they say in Rally Racing: Slow is fast and fast is slow. There's no shortcut other than understanding. Which takes time and good input (great books, good mentors, practical experiences,..). And everything is driven by social factors, not technical prowess. The latter helps with solving problem, but the former defines the problem in the first place. Improve your soft skills as well as your technical abilities.\n \nreply",
      "It\u2019s not about aptitude it's about the way you\u2019re viewed.\n \nreply"
    ],
    "link": "http://sibervepunk.com/digital-consumption.html",
    "first_paragraph": "\n31.07.2024\nknow thyselfThere is a new lifestyle imposed on almost the entire world, willingly or unwillingly, perhaps by powerful people or by many small people that want to be powerful, which somehow affects all ordinary people: a consumption-oriented life. Fast consumption, constant consumption, more consumption.I don\u2019t have much to say about the \u201cshopping\u201d side of this consumption craze because it\u2019s a topic that\u2019s been around for many years, born out of  -ism movements and studied numerous times through  -ology disciplines. It has been the subject of public service announcements, romantic comedies, and personal development books. The public has been constantly educated about it for years. Two guys known as The Minimalists and some \u201csmart\u201d people like Marie Kondo made a fortune out of this movement. Personally, I believe I am a conscious consumer, and the shopping craze doesn\u2019t affect me much, so I want to look at the other, often-discussed side of the issue.The consumption I will d"
  },
  {
    "title": "The Depths of Wikipedians (asteriskmag.com)",
    "points": 143,
    "submitter": "Brajeshwar",
    "submit_time": "2024-12-10T15:32:04 1733844724",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=42377770",
    "comments": [
      "> that's the unfortunate thing about growing and becoming a massive encyclopedia that's more respected than it was in 2004Is it?  Wikipedia was massive and respected in 2014, too, but it wasn't dominated by the problems discussed immediately beforehand\u2014WP:BRD didn't neuter WP:BOLD, and egoists with \"standing\" didn't have free reign to antagonize newcomers who followed the rules.The problem with Wikipedia and other WMF projects nowadays is the entrenched folks with standing not being subject to rules and being permitted to apply new ones pulled out of thin air, not to mention the insidious circling-of-the-wagons when they're challenged and comrades come to their defense by trying to attack and smear the perceived threat.In a well-ordered and functioning society you can avoid running afoul of the law by following the rules\u2014you can get by without having to pay speeding tickets by simply not speeding.  Wikipedia and other WMF projects that have been infected by Wikipedia admins and their toxic mindset are more like a society where impunity might as well be codified\u2014like a cop who says, \"It's like this because I say so.\"\n \nreply",
      "The above comment seems unnecessarily vitriolic.> free reign to antagonize newcomersNobody has \"free reign to antagonize\" anyone including newcomers. It's hard to come to a new large community and project with somewhat unusual norms, and established editors should try to do a better job helping people learn the ropes, but at the same time I have gone to look repeatedly when some newcomer complained about being antagonized (either complained here, or other forums, or on Wikipedia) and maybe half the time the newcomer making the complaint was being a total jerk to everyone but really didn't like being called out.If you (the generic you reading this comment) want to put a bunch of volunteer effort into finding newcomers who are having a tough time and helping them out that would be a valuable contribution. On an all-volunteer project people do what they are interested and unfortunately intermediating on newcomers' behalf is usually not it (I do try to help when I can).> in 2014, too, but it wasn't dominated by the problems [...]To my recollection is has been continually dominated by similar kinds of issues, since close to the beginning. It was then, and still is now, often a frustrating slog when multiple editors disagree about something, especially if there's some kind of (typically unstated) ideological motivation involved, e.g. promoting one ethnic group or nation. There are various dispute resolution processes, which work more or less, mainly by attracting more eyeballs any time there is a stuck conflict, but trying to form consensus with a pseodonymous global group of volunteers is just an inherently hard problem.The main differences between the first few years vs. today are that (1) many topics at least have some kind of article written about them (often still mediocre, especially parts from the early days that haven't seen much dedicated focused effort) which means that there's a bit more bias toward preserving what is already there (an understandable tendency in the face of a constant onslaught of vandalism and entropy), (2) people recently tend to demand a bit better sourcing for claims than was done up to about 2010, so there's sometimes a bit of a disconnect between the quality standard applied to existing material vs. newly added/changed material, and (3) there's a bit more focus than before on earning little gamified badges like stars or green + signs in the corners of articles.\n \nreply",
      "Your pivot from \"newcomers who followed the rules [i.e. those who took the time to educate themselves beforehand about the speed limit so they don't break it]\" to \"newcomers who are having a tough time [because they need help learning the ropes]\" isn't particularly artful or subtle.I'm referring specifically to experienced editors who take the stance that some action by a new/IP account is coming from an confused/uninformed place for no other reason than because it's an inexperienced editor\u2014a bag of foregone conclusions and the kind of presumptuousness that, perversely, your comment exemplifies.And you're wrong about 2004 Wikipedia versus 2014 Wikipedia versus 2024 Wikipedia.  It's much, much worse in 2024.(I began editing Wikipedia in 2004, by the way.  I am not the newcomer described here, but I've absolutely seen other experienced editors behave this way, including in response to IP edits that I've made when I happened not to be logged in.)> at the same time I have gone to look repeatedly when some newcomer complained about being antagonized (either complained here, or other forums, or on Wikipedia) and maybe half the time the newcomer making the complaint was being a total jerk to everyone but really didn't like being called outUh, okay?  So if in a group of totally unrelated persons who have nothing in common except for the fact that they're all inexperienced, at least half of them are in the wrong, then it supposed to cancel out or something?Even if 999 new editors behave badly and 1 new editor doesn't, then that 1 editor has a legitimate grievance when they're being wronged (instead than being the one who's in the wrong themselves).  They have no control over and bear no responsibility for the actions of the other 999.\n \nreply",
      "As we can see demonstrated, the inherently impersonal medium of pseudonymous text conversation has some of the same pitfalls here as at Wikipedia. It's so easy to turn a conversation into something like a fight, often entirely by accident, and it takes a lot of work to keep discussion polite and positive.> I'm referring specifically to experienced editors who take the stance ... much, much worse in 2024My subjective anecdotal experience is that the same complaints and conflicts have been happening all along in broadly comparable ways. You can find people complaining bitterly about having been reverted, disrespected, wikilawyered, or whatever, from the early days of the project down to the present.I agree things were somewhat different when the project was much smaller and just beginning (like 2004\u20132008), but my impression is that after a few years it stabilized to more or less the current condition. I'd be curious to see some kind of data analysis, but in my personal experience the community isn't significantly different in style or frustration level (either among Wikipedians or people talking about Wikipedia on the broader internet) compared to 10 years ago.> Uh, okay? ...You really don't have to be combative or sarcastic. We can just have a conversation. I'm not aiming for rhetorical trickery, just earnestly expressing my own feeling about it. Following wiki norms (also HN norms), I'd ask that you please assume good faith and aim for civility.> at least half of them are in the wrong, then it supposed to cancel out or something?No, that's not my point. There's no good reason to ever be giving anyone a runaround. My point is only that when one hears complaints around the internet of \"I was a newcomer to Wikipedia and people were so mean to me\" and then a few more people pile on with the same, it's necessary to actually go look at the specific examples before trying to draw broad conclusions from that.Most of the time the issue is some kind of relatively minor misunderstanding blown out of proportion. Out of maybe 10 such cases I have bothered to go track down there was precisely one where an experienced editor was being quite inconsiderate and was (in my opinion) in the wrong, another handful where quite reasonable policies were being followed as expected but the comments that went along with reverts were more brusque than I would to see directed at a newcomer, and then another handful where the newcomer was being an insulting entitled jerk and people were giving quite justified resistance. In a couple cases the newcomer just wanted to use Wikipedia for self promotion and was mad that their spam links to their own project kept getting removed.I think you significantly underestimate/understate the challenge of getting a large and diverse group of pseudonymous strangers to cooperate, especially in the face of constant nonsense, spam, vandalism (both overt and subtle), etc. being added all the time, and are overoptimistic about the possible/desirable cultural norms that can emerge in a community like that.Personally, I'm amazed that something as well functioning as Wikipedia is possible at all. It far exceeds my expectations, and only manages as well as it does because the vast majority of volunteer editors (including administrators and heavy contributors) are incredibly generous and friendly.\n \nreply",
      "> same complaints and conflicts have been happening all along in broadly comparable waysYou're really doubling down on this line of reasoning, even though once was already too much.  I'll be even more blunt: it sucks.  It's shoddy reasoning.Tammi says that she's one of the kinds of people that people usually end up taking advantage of, especially at work.  But she's wrong.  They don't.Sammi says that she's one of the kinds of people that people usually end up taking advantage of, especially at work.  She's right.  They actually do.Do you see how no matter how many Tammis there are, it doesn't negate the truth about Sammi?If you do, and you actually grasp the principle that \"you have to actually go look at the specific examples before trying to draw broad conclusions from that\", then what possible reason would there be, when Sammi appears or someone tries to discuss her case, to respond by bringing up how often Tammi has made the same complaint?  It changes nothing, and it's a shitty thing to make Sammi deal with if she's already having a below-baseline-level experience.> please assume good faithSide quest: What does that phrase actually mean to you?I've literally only ever seen it used pre-emptively as an explicit or implicit accusation\u2014a violation of the very policy that the person invoking WP:AGF is trying to claim the other person has violated.  The only reasonable conclusion is that they either don't know or don't care what it actually means, because it's accepted as the universal response when you want to bundle something up with WP:CIVIL.  It's free to allege WP:AGF violations and there are no consequences for trying to raise spurious charges.\n \nreply",
      "Your claim (based on anecdotal personal impressions and unsupported by any other evidence) is that the community is \"much worse\" today vs. 10 years ago.My claim (also based on personal impressions) is that the community is more or less comparable to 10 years ago. While the community was and remains generally positive, there were some jerks then and there are some jerks now. There was plenty of opportunity for misunderstandings then, and there still is today. People complained about it online then, and they are still doing so today. It's going to take some data to convince me that any comparative analysis isn't just nostalgia or selection bias.You keep putting words in my mouth and generally seem to misunderstand my point, and are now hurling misdirected sarcastic insults at me. (Maybe as a self-professed long-term Wikipedian you are trying to validate your claim that long-term Wikipedians are good at being dicks to each-other, by direct demonstration?) I'd ask again that you please knock it off, but I'm about done here, so have fun with that I guess.\n \nreply",
      "The problem with Wikipedia is unfortunately the same problem as any society faces. Often things start out well, then [someone] arrives. Society then believes that if we define with rules and scripture how we believe, [someone] will behave like we did.But [someone] isn't the one who started the project. They're a different person. And even if they read the rules, they are not motivated by the rules.\n \nreply",
      "Wikimedia foundation could audit the admins who defend each other on a tit or tat basis and attack newbies.My lifetime achievement is getting one those assholes banned.Exactly as you say - rules dont aplly to them.Also using the \"standards\" to delete everything even when fully cited, while their own articles are slop.Not to mention people who edit good articles just to have more edits. Often for the worse.The worst trend recently is putting 5-10 paragraph quotations in articles. Often  by some randoms (self promotion?). Those look like articles written by 5th graders.Wikipedia should be concise!\n \nreply",
      "> Wikimedia foundation could audit the admins who defend each other on a tit or tat basis and attack newbies.The challenge here is that \"the community\" really doesn't want the Foundation doing that. The interview mentions the last major flare-up along those lines: FramGate, though it amusingly got mis-transcribed as \"Forum Gate\".The WMF handed out a 1 year ban from English Wikipedia to an admin called Fram, who this interview describes as \"gruff, not friendly, not the most empathetic\", for unspecified harassment. There was a great big \"community\" revolt over it, and the WMF wound up backing down by letting the English Wikipedia arbitration committee undo the ban.I keep putting \"community\" in quotes because it refers specifically to the set of people who involve themselves in Wikipedia governance, rather than the broader community of people who use and edit Wikipedia. There's a lot of power assigned to a fairly small group of people who invest their time in on-wiki politics. It's kinda like local school boards in that way.\n \nreply",
      "> In a well-ordered and functioning society you can avoid running afoul of the law by following the rules\u2014you can get by without having to pay speeding tickets by simply not speeding.That's because instead of codifying a set of rules, it \"regulates\" by citing a jumble of bizarre Confucian-style aphorisms listed in no particular order, in no particular place.If they were serious about integrity, they'd formalize. Formalization might give them some defense from attack, whereas until then they'll always roll over for the person with the most resources to spend on subverting it.\n \nreply"
    ],
    "link": "https://asteriskmag.com/issues/08/the-depths-of-wikipedians",
    "first_paragraph": "A conversation about yogurt wars, German hymns, tropical cyclones, and the people who make Wikipedia function.Asterisk: You're famous for the Depths of Wikipedia account, where you share factoids from some of the most arcane, interesting, and surprising pages on Wikipedia. But you're also now a part of the broader Wikipedia community. How did you first get interested in the site, and how has your involvement changed over time?\u00a0Annie Rauwerda: I started back in high school editing typos and adding things that I noticed were missing \u2014 like items to lists. But I had never done anything more than that because I was afraid of it because there are so many rules. Like, I'd seen the talk pages. And many of Wikipedia\u2019s policies and guidelines and essays are very wordy.Then I started the account \u2014 even though I felt a little like a phony. But I remember the first time I felt really excited about the Wikipedia community was when I got on a call with the president of Wikimedia, New York City, back"
  },
  {
    "title": "AI model for near-instant image creation on consumer-grade hardware (surrey.ac.uk)",
    "points": 117,
    "submitter": "giuliomagnifico",
    "submit_time": "2024-12-10T16:44:52 1733849092",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=42378519",
    "comments": [
      "I wasn't able to get many decent results after playing with the demo for some time.  I guess my question is...what exactly is this for?  I was able to get substantially better results about 2 years ago running SD 2 locally on a gaming laptop.  Sure, the images took 30 seconds or so each, but the quality was better than I could get in the demo.  Not sure what the point of instantly generating a ton of bad quality images is.What am I missing?\n \nreply",
      "For those wondering, it's an adversarially distilled SDXL finetune, not a new base model.\n \nreply",
      "Thanks! This article is pretty heavy with PR bullshit.\n \nreply",
      "My favorite test of image models:Drawing of the inside of a cylinder.That's usually bad enough. Then try to specify size, and specify things you want to place inside the cylinder relative to the specified size.(e.g. try to approximate an O'Neill cylinder)I love generative AI models, but they're really bad at that, and this one is no exception, but the speed makes playing around with prompt variations to try to see if I get somewhere a lot easier (I'm not getting anywhere...)\n \nreply",
      "Github: https://chendaryen.github.io/NitroFusion.github.io/Paper: https://arxiv.org/html/2412.02030v2\n \nreply",
      "What does consumer-grade mean in this context - is this referring to an M1 MacBook or a tower full of GPUs? I couldn't find in the paper or README.\n \nreply",
      "Here is the demo https://huggingface.co/spaces/ChenDY/NitroFusion_1step_T2II'm unable to get anything that looks as good as the images in the README, what's the trick for good image prompts?\n \nreply",
      "I had the same issue, so I pulled in the SDXL refiner. Night and day better even at one step.https://gist.github.com/deckar01/7a8bbda3554d5e7dd6b31618536...\n \nreply",
      "thank you!\n \nreply",
      "I get pretty close result with seed 0paper\nhttps://i.imgur.com/l90WYrT.pngreplication on hf\nhttps://i.imgur.com/MqN1Qwc.png\n \nreply"
    ],
    "link": "https://www.surrey.ac.uk/news/surrey-announces-worlds-first-ai-model-near-instant-image-creation-consumer-grade-hardware",
    "first_paragraph": " We use cookies to help our site work, to understand how it is used, and to tailor ads that are more relevant to you and your interests.By accepting, you agree to cookies being stored on your device. You can view details and manage settings at any time on our cookies policy page.A groundbreaking AI model that creates images as the user types, using only modest and affordable hardware, has been announced by the\u00a0Surrey Institute for People-Centred Artificial Intelligence (PAI) at the University of Surrey. \u00a0The model, NitroFusion, represents a world first and has been made open source by its developers \u2013 SketchX, a lab within PAI \u2013 a move that fundamentally transforms access to AI-enabled image creation models for creative professionals.\u00a0Typically, similar\u00a0technology is available only to corporate giants with vast computing resources. However, NitroFusion runs on a single consumer-grade graphics card \u2013 marking a decisive step forward in bringing advanced AI capabilities to individual crea"
  },
  {
    "title": "Colour in the Middle Ages (medievalists.net)",
    "points": 99,
    "submitter": "Pamar",
    "submit_time": "2024-12-10T15:19:57 1733843997",
    "num_comments": 62,
    "comments_url": "https://news.ycombinator.com/item?id=42377644",
    "comments": [
      "> Michel Pastoureau\u2019s book on blue begins by highlighting the neglect this colour faced among the ancient Greeks and Romans, who rarely wrote about it or used it. He even explores the intriguing question of whether ancient peoples could perceive blue at all!The first synthetic pigment was calcium copper silicate or Egyptian Blue [1], so called because the Egyptians manufactured it from at least the fourth millennium BC; from the Egyptians, the rest of the Mediterranean learned to make and use this artificial pigment, so that it is widely attested in art from the Minoans, Mycenaeans, Greeks (if distinguishable from the Mycenaeans), Romans, and so forth up until the middle ages.  Given that Egyptian Blue is a synthetic pigment that must be manufactured by human skill and ingenuity, it boggles the mind that people keep falling for this idea that ancient peoples could not perceive blue.  I have no idea how someone could write a book suggesting that ancient people did not write about (Plato certainly did) or use a color that they in fact synthesized, manufactured, and used in art.  The ancient Greek word for the color is \u03ba\u03c5\u03b1\u03bd\u03bf\u1fe6\u03c2, the Latin caeruleus (but of the eyes, caesius).[1] https://en.wikipedia.org/wiki/Egyptian_blue\n \nreply",
      "A good deal of confusion in topics like this comes from assuming that not having vocabulary to describe differences means that people weren't aware of the differences. That's silly. Have you ever tried to pick a paint for your house and had to choose between a hundred variations of the basic color you wanted? Even without specialist color vocabulary, you could perceive the differences.Anthropologists have found tribal societies whose number vocabulary included only One, Two, Three, and Many. Do you think they couldn't tell the difference between having 5 or 10 cows? There have also been some languages with very few color words. In fact, there is known a rule that if a language has only three color words, they will be White, Black, and Red.\n \nreply",
      "In Japan the traffic light will go green, and if you ask they'll look at you straight at your face and say \"it's blue\". There's even a wikipedia page noting this for every culture, see how in Japan green is a tone of blue and only came after the WWII:https://en.wikipedia.org/wiki/Blue%E2%80%93green_distinction...\n \nreply",
      "It has been long understood in linguistics that languages don't divide the continuous spectrum of light wavelengths into the same buckets of arbitrary color words. It is just as well understood that this doesn't have any impact on peoples' ability to perceive those wavelengths.https://en.wikipedia.org/wiki/Basic_Color_Termshttps://en.wikipedia.org/wiki/Linguistic_relativity#Colour_t...https://journals.openedition.org/estetica/1797https://www.jstor.org/stable/2660766\n \nreply",
      "In America I will go outside with my beard, and if you ask they'll look straight at my face and say \"it's red.\"It's roughly the color of the HN title bar.\n \nreply",
      "Probably because they are too polite to call you ginger.https://www.youtube.com/watch?v=KVN_0qvuhhw&t=159s\n \nreply",
      "> In America I will go outside with my beardThis looks uncannily like the first line of a lost Walt Whitman poem.\n \nreply",
      "Simmilar concepts exist for English (and other languages too). Take \"white wine\" which is not white as milk.\n \nreply",
      "Is it possible the author means they lumped blue in with another color, like green or purple, and didn\u2019t differentiate it as its own color? Like how this list of Medieval colors also didn\u2019t list orange. I assume orange was lumped in with red or yellow.\n \nreply",
      "Yes, this is how I interpreted it. Arguably before the 20th century we had few commonplace references for most of the hues we've now named at all. Plus, most of the references we do have are often localized regionally and linguistically to people who recognize the term. The constant colors\u2014things like \"blood\" and \"dark\" and \"bright-white\"\u2014are actually pretty rare. Even the sun, the sky, the ocean, earth etc are capable of producing far too many hues to reliably use as a reference point, and often different hues in different place. Which is not so say that it's not very poetic when it works!\n \nreply"
    ],
    "link": "https://www.medievalists.net/2024/06/colour-middle-ages/",
    "first_paragraph": ""
  },
  {
    "title": "WPEngine, Inc. vs. Automattic\u2013 Order on Motion for Preliminary Injunction (courtlistener.com)",
    "points": 102,
    "submitter": "kbarmettler",
    "submit_time": "2024-12-10T23:20:34 1733872834",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=42382829",
    "comments": [
      "> Here, Mullenweg\u2019s \u201cstatement that he had the right to disable WPEngine\u2019s account access and to make changes to the ACF plugin for the sake of public safety[,]\u201d see Opp. at 27-28, is belied by the declarations of WPEngine\u2019s executives stating that the claimed vulnerability was minor [...]@photomatt literally screwed himself over by talking about his actions here, when everyone was screaming at him to shut the fuck up. Will he start listening now?In short, what this injunction does is a) remove the checkbox, b) return ACF to WPEngine, c) restore access to website, d) no bond required.\n \nreply",
      "Rooted in speculation about why this meltdown occurred in the first place: be very careful about taking money, as well as the amount and who from. My guess from day one is that Matt was put under pressure from investors and this was the only \"fix\" he saw.A real shame to see such a great legacy flushed for zero ROI.\n \nreply",
      "I've often wondered if his emphasis on how private equity was ruining WordPress was him telegraphing his frustration with his own investors at Black Rock while technically abiding by his contract with them. It's interesting to consider that his performance could have been a type of malicious compliance with their demands\u2014going so far overboard that he intentionally destroys the whole ecosystem as a kind of revenge.It's probably more likely he just went nuts, but it's fun as a head canon.\n \nreply",
      "> his own investors at Black RockBlackrock does not at all operate the same as a private equity and it's a huge pet peeve of mine when people lump them in together. Usually they are confusing Blackrock with Blackstone.All Blackrock does is manage wealth and investments on behalf of individual clients. One of the ways they do this is by sticking private assets into funds for their clients. It's more akin to Vanguard than anything to do with private equity. There's no reason to believe BlackRock would be breathing down their necks when all they do is broker the funds on behalf of their clients.Automattic DOES have private equity backers though like Tiger Global, Insight, ICONIQ, and more.\n \nreply",
      "Fair enough. /s/at Black Rock//\n \nreply",
      "As amusing as that thought is... I think he could have done that without misrepresenting [the 'contracts' of] open source software readily.He stood to gain from this little knife twist, we all may still lose. Who knows what misconceptions were born/the downstream effects. WPEngine was never under any obligation to Matt himself/Automattic/WordPress/the Foundation [whatever distinction any of that may offer!]... outside of the licenseI don't believe it's a question of distance overboard, it started there\n \nreply",
      "Matt has claimed[1] before to be \"post-economic\", i.e. has enough money that it doesn't matter to him anymore. It's hard not to see this as personal, not financial.[1] https://x.com/sereedmedia/status/1839394786622722432\n \nreply",
      "I would say, negative ROI at this point.\n \nreply",
      "No, Matt is just a classic CEO exhibiting symptoms of narcissist personality disorder who's now getting served in court and doesn't realise how much he earned it.\n \nreply",
      "Matt went on a Youtube channel last week to partake in a Wordpress speed building challenge, and he got frustrated several times using his own product: https://www.youtube.com/watch?v=EqY5bje8D2oHe spent years squashing anyone who complained about Gutenberg, and then he calls WPEngine a cancer and accuses them of offering an inferior version. All to find out that he's not even familiar!\n \nreply"
    ],
    "link": "https://www.courtlistener.com/docket/69221176/64/wpengine-inc-v-automattic-inc/",
    "first_paragraph": ""
  }
]