[
  {
    "title": "Evidence that AI is destroying jobs for young people (derekthompson.org)",
    "points": 134,
    "submitter": "duck",
    "submit_time": "2025-09-03T23:07:36 1756940856",
    "num_comments": 96,
    "comments_url": "https://news.ycombinator.com/item?id=45121342",
    "comments": [
      "How does one explain the drop starting January 2023 (esp for things like Customer Service Rep, which is an NLP-heavy task) when most corporations didnt even start LLM/NLP pilots until mid/late 2023? I skimmed thru the 100+ page paper but didnt see an explanation for this strange leading effect.SWE figures dropped mid-2022 (almost magically in line with interest rate hikes) and LLM-copilots werent introduced for another year. The paper notes they did an adjustment for the end of ZIRP. I dont know enough econometrics to understand whether this adjustment was sufficient, but the chart doesnt make sense since the labor efforts seem to be leading the actual technology by over a year or more. From informal surveys, LLM-copilot usage didnt become widespread until late 2023 to mid 2024, certainly not widespread enough to cause macro labor effects in mid-2022.reply",
      "The 2022 drop for SWE is easy for me to explain, and it's not on these analysts' list of factors (though I'm not an economic quant, I don't know how you could really control for it): In 2017, a tax bill was passed that cut a particular tax incentive in 2022 in an effort to be counted as \"revenue neutral\" despite being otherwise a massive tax cut overall. The incentive in question was a writeoff for \"Research and development\". This means that in 2022, it got effectively much more expensive to hire anyone who falls under that category, including developers not directly necessary for the day-to-day function of a business (hell, one might argue they would have counted anyway) and scientists of most kinds. That this hit big firms, which have a higher relative amount of R&D efforts going at a given time, first makes a lot of sense.For customer service, my explanation is that companies literally do not care about customer service. Automated phone trees, outsourced call centers whose reps have no real power to help a customer, and poorly-made websites have been frustrating people for decades, but businesses never seem to try to compete on doing better at it. It's a cheap win with investors who want to hear about AI initiatives to lay off yet even more of this department, because it doesn't matter if the quality of service declines, there are no market or regulatory forces that are punishing this well enough to ever expect firms to stop breaking it, let alone fix itreply",
      "Love this note. For those interested, this is the Tax Cuts and Jobs Act (TCJA) of 2017 Section 179.For a software engineering business, the Tax Cuts and Jobs Act (TCJA) of 2017 significantly impacted how software costs can be expensed under Section 179. While Section 179 previously allowed for the immediate expensing of many software purchases, TCJA reforms restricted this deduction primarily to \"off-the-shelf\" software. Custom-developed software and internal development costs are no longer eligible for Section 179 expensing and must now be capitalized and amortized.Under the TCJA, Section 179 cannot be used for software that a company develops for itself. This includes the direct costs for the engineers, programmers, and other personnel involved in the development process.The report not addressing this elephant in the room is a disappointing.reply",
      "I think it may have been a one-two punch of \u00a7174 and the end of ZIRP.Of note, the OBBB reinstated the ability to deduct R&D, so businesses are no longer required to capitalize and amortize R&D expenses (including software development).https://warrenaverett.com/insights/one-big-beautiful-bill-se...reply",
      "Section 174 was the big one that affected how SE salaries could be deducted, that many blamed for the start of layoffs.However, that's back for tax year 2025, so why aren't we seeing the jobs come back?  Maybe it really was 174 then, but AI now?reply",
      ">why aren't we seeing the jobs come back?It was only just reinstated, so it's probably too early to see the effects.I also expect that despite the restoration of Section 174, companies realized that they not only overhired during ZIRP, but also that they don't actually need that headcount, given the outcome of Musk's Twitter layoffs. There were so many prognostications that Twitter would imminently implode after downsizing from ~8k to ~1.5k employees, and when these claims never came to pass, it was a wake-up call to the rest of the industry [0].[0] https://www.livemint.com/companies/news/elon-musk-fired-80-p...reply",
      "I also think it\u2019s fashionable to have a smaller headcount these days. Historically, the dynamics of businesses encouraged rising headcounts, as ICs weren\u2019t as valued as managers (salary caps basically, as impact for ICs is hard to measure unless you are in sales), and managers generally view headcount as a metric to career and salary growth.So there was just this general pressure from the middle up to grow instead of paying more to existing staff or finding some other way to spend the money. After all, investors generally want you to spend the money you have access to, otherwise they\u2019ll put it to use elsewhere.It seems that there is external pressure right now from investors, and on to executives, to push headcounts down as there is a general feeling that good companies should be able to leverage AI to become much more efficient, and higher headcounts just burn money and bog things down. Whether or not that\u2019s true is another question, but the perception exists.I\u2019m not sure if this is a fundamental change in the dynamic, or just a temporary push against it that will eventually lose steam.reply",
      "I don't think many people really doubted that Twitter could keep itself up and running.But that \"everything app\"? It hasn't happened. The money transfer app (\"Twitter Payment Platform\")? Still MIA.reply",
      ">I don't think many people really doubted that Twitter could keep itself up and running.Oh, they sure did: https://news.ycombinator.com/item?id=34617964Even in that thread, a lot of people were saying \"it's only been three months, give it a bit more time.\"reply",
      "Recruiters have been blowing my inbox up since the day Trump signed the OBBBalthough I think entry level is still in shambles, for nowreply"
    ],
    "link": "https://www.derekthompson.org/p/the-evidence-that-ai-is-destroying",
    "first_paragraph": ""
  },
  {
    "title": "Claude Code: Now in Beta in Zed (zed.dev)",
    "points": 469,
    "submitter": "meetpateltech",
    "submit_time": "2025-09-03T15:07:20 1756912040",
    "num_comments": 300,
    "comments_url": "https://news.ycombinator.com/item?id=45116688",
    "comments": [
      "I love Zed and I'm glad you now have native support for Claude. I previously ran it using the instructions in this post: https://benswift.me/blog/2025/07/23/running-claude-code-with...One thing that still suffers is AI autocomplete. While I tried Zed's own solution and supermaven (now part of Cursor), I still find Cursor's AI autocomplete and predictions much more accurate (even pulling up a file via search is more accurate in Cursor).I am glad to hear that Zed got a round of funding. https://zed.dev/blog/sequoia-backs-zed This will go a long way to creating real competition to Cursor in the form of a quality IDE not built on VSCodereply",
      "I was somewhat surprised to find that Zed still doesn't have a way to add your own local autocomplete AI using something like Ollama. Something like Qwen 2.5 coder at a tiny 1.5b parameters will work just fine for the stuff that I want. It runs fast and works when I'm between internet connections too.I'd also like to see a company like Zed allow me to buy a license of their autocomplete AI model to run locally rather than renting and running it on their servers.I'd also pay for something in the 10-15b parameter range that used more limited training data focused almost entirely on programming documentation and books along with professional business writing. Something with the coding knowledge of Qwen Coder combined with the professionalism and predictability of IBM Granite 3. I'd pay quite a lot for such an agent (especially if it got updates every couple of months that worked in new documentation, bugfixes, github threads, etc to keep the answers up-to-date).reply",
      "You don't have to buy a license; the autocomplete model is open source https://huggingface.co/zed-industries/zetaIt is indeed a fine tuned Qwen2.5-Coder-7Breply",
      "> I'd also pay for something in the 10-15b parameter range that used more limited training data focused almost entirely on programming documentation and books along with professional business writing.Unfortunately, pretraining on a lot of data (~everything they can get their hands on) is needed to give current LLMs their \"intelligence\" (for whatever definition of intelligence). Using less training data doesn't work as well for now. There definitely not enough programming and business writing to train a good model only on that.reply",
      "There's an active PR providing inline edit completions via Ollama: https://github.com/zed-industries/zed/pull/33616reply",
      "You can use a local model! It's in Settings in a Thread and you can select Ollama.reply",
      "But that doesn't work for inline edit predictions, right?reply",
      "Ditto, that was one of the dealbreakers for me using Zed, the Copilot integration is miles behind Cursor'sreply",
      "I'll third this. AI autocomplete is THE most efficient and helpful feature of Cursor, not the agents.reply",
      "I don't know, I think it's a tie. I can have the agent do some busy work or refactoring while I'm writing code with the autocomplete. I can tell it how I want a file split up or how I want stuff changed, and tell it that I'll be making other changes and where. It's smart enough to ignore me and my work while it keeps itself busy with another task. Sort of the best of both worlds. Right now I have it replacing DraftJS with another library while I'm working on some feature requests.reply"
    ],
    "link": "https://zed.dev/blog/claude-code-via-acp",
    "first_paragraph": "Morgan KreySeptember 3rd, 2025You asked for it. A lot.@EricBuess@lucasbastianik@ugbahisioma@nicojrme@kdcokenny@EricBuess@lucasbastianik@ugbahisioma@nicojrme@kdcokenny@EricBuess@lucasbastianik@ugbahisioma@nicojrme@kdcokenny@EricBuess@lucasbastianik@ugbahisioma@nicojrme@kdcokenny@osdiab@ZainMerchant9@wiedymi@iamkgn@mitryco@osdiab@ZainMerchant9@wiedymi@iamkgn@mitryco@osdiab@ZainMerchant9@wiedymi@iamkgn@mitryco@osdiab@ZainMerchant9@wiedymi@iamkgn@mitrycoSo we built it: our Claude Code integration is now available in public beta, running natively in Zed through our new Agent Client Protocol (ACP).For months, developers have been asking us to bring Claude Code into Zed. We didn\u2019t just want to bolt on a one-off integration; we wanted to build something better. ACP is our new open standard that lets any agent connect to Zed (and other editors, too). Claude Code is a perfect example of what\u2019s possible.Now you can:Claude Code has gained broad popularity among developers thanks to its powerful co"
  },
  {
    "title": "The Bitter Lesson Is Misunderstood (obviouslywrong.substack.com)",
    "points": 132,
    "submitter": "JnBrymn",
    "submit_time": "2025-08-28T21:32:10 1756416730",
    "num_comments": 71,
    "comments_url": "https://news.ycombinator.com/item?id=45057283",
    "comments": [
      "Hey folks, OOP/original author and 20-year HN lurker here \u2014 a friend just told me about this and thought I'd chime in.Reading through the comments, I think there's one key point that might be getting lost: this isn't really about whether scaling is \"dead\" (it's not), but rather how we continue to scale for language models at the current LM frontier \u2014 4-8h METR tasks.Someone commented below about verifiable rewards and IMO that's exactly it: if you can find a way to produce verifiable rewards about a target world, you can essentially produce unlimited amounts of data and (likely) scale past the current bottleneck. Then the question becomes, working backwards from the set of interesting 4-8h METR tasks, what worlds can we make verifiable rewards for and how do we scalably make them? [1]Which is to say, it's not about more data in general, it's about the specific kind of data (or architecture) we need to break a specific bottleneck. For instance, real-world data is indeed verifiable and will be amazing for robotics, etc. but that frontier is further behind: there are some cool labs building foundational robotics models, but they're maybe ~5 years behind LMs today.[1] There's another path with better design, e.g. CLIP that improves both architecture and data, but let's leave that aside for now.reply",
      "10+ years ago I expected we would get AI that would impact blue collar work long before AI that impacted white collar work. Not sure exactly where I got the impression, but I remember some \"rising tide of AI\" analogy and graphic that had artists and scientists positioned on the high ground.Recently it doesn't seem to be playing out as such. The current best LLMs I find marvelously impressive (despite their flaws), and yet... where are all the awesome robots? Why can't I buy a robot that loads my dishwasher for me?Last year this really started to bug me, and after digging into it with some friends I think we collectively realized something that may be a hint at the answer.As far as we know, it took roughly 100M-1B years to evolve human level \"embodiment\" (evolve from single celled organisms to human), but it only took around ~100k-1M for humanity to evolve language, knowledge transfer and abstract reasoning.So it makes me wonder, is embodiment (advanced robotics) 1000x harder than LLMs from an information processing perspective?reply",
      "> So it makes me wonder, is embodiment (advanced robotics) 1000x harder than LLMs from an information processing perspective?Essentially, yes, but I would go further in saying that embodiment is harder than intelligence in and of itself.I would argue that intelligence is a very simple and primitive mechanism compared to the evolved animal body, and the effectiveness of our own intelligence is circumstantial. We manage to dominate the world mainly by using brute force to simplify our environment and then maintaining and building systems on top of that simplified environment. If we didn't have the proper tools to selectively ablate our environment's complexity, the combinatorial explosion of factors would be too much to model and our intelligence would be of limited usefulness.And that's what we see with LLMs: I think they model relatively faithfully what, say, separates humans from chimps, but it lacks the animal library of innate world understanding which is supposed to ground intellect and stop it from hallucinating nonsense. It's trained on human language, which is basically the shadows in Plato's cave. It's very good at tasks that operate in that shadow world, like writing emails, or programming, or writing trite stories, but most of our understanding of the world isn't encoded in language, except very very implicitly, which is not enough.What trips us up here is that we find language-related tasks difficult, but that's likely because the ability evolved recently, not because they are intrinsically difficult (likewise, we find mental arithmetic difficult, but it not intrinsically so). As it turns out, language is simple. Programming is simple. I expect that logic and reasoning are also simple. The evolved animal primitives that actually interface with the real world, on the other hand, appear to be much more complicated (but time will tell).reply",
      "Not a robotics guy, but to extent that the same fundamentals hold\u2014I think it's a degrees of freedom question. Given the (relatively) low conditional entropy of natural language, there aren't actually that many degrees of (true) freedom. On the other hand, in the real world, there are massively more degrees of freedom both in general (3 dimensions, 6 degrees of movement per joint, M joints, continuous vs. discrete space, etc.) and also given the path dependence of actions, the non-standardized nature of actuators, actuators, kinematics, etc.All in, you get crushed by the curse of dimensionality. Given N degrees of true freedom, you need O(exp(N)) data points to achieve the same performance. Folks do a bunch of clever things to address that dimensionality explosion, but I think the overly reductionist point still stands: although the real world is theoretically verifiable (and theoretically could produce infinite data), in practice we currently have exponentially less real-world data for an exponentially harder problem.Real roboticists should chime in...reply",
      "Also not a robotics guy, but that all sounds right to me...What I do have deep experience in is market abstractions and jobs to be done theory. There are so many ways to describe intent, and it's extremely hard to describe intent precisely. So in addition to all the dimensions you brought up that relate to physical space, there is also the hard problem of mapping user intent to action with minimal \"error\", especially since the errors can have big consequences in the physical world. In other words, the \"intent space\" also has many dimensions to it, far beyond what LLMs can currently handle.On one end of the spectrum of consequences is the robot loads my dishwasher such that there is too much overlap and a bunch of the dishes don't get cleaned (what I really want is for the dishes to be clean, not for the dishes to be in the dishwasher), and on the other end we get the robot that overpowers humanity and turns the universe into paperclips.So maybe we have to master LLMs and probably a whole other paradigm before robots can really be general purpose and useful.reply",
      "> if you can find a way to produce verifiable rewards about a target worldI feel like there's an interesting symmetry here between the pre and post LLM world, where I've always found that organisations over-optimise for things they can measure (e.g. balance sheets) and under-optimise for things they can't (e.g. developer productivity), which explains why its so hard to keep a software product up to date in an average org, as the natural pressure is to run it into the ground until a competitor suddenly displaces it.So in a post LLM world, we have this gaping hole around things we either lack the data for, or as you say: lack the ability to produce verifiable rewards for. I wonder if similar patterns might play out as a consequence and what unmodelled, unrecorded, real-world things will be entirely ignored (perhaps to great detriment) because we simply lack a decent measure/verifiable-reward for it.reply",
      "What do you mean about CLIP?reply",
      "The problem I am facing in my domain is that all of the data is human generated and riddled with human errors. I am not talking about typos in phone numbers, but rather fundamental errors in critical thinking, reasoning, semantic and pragmatic oversights, etc. all in long-form unstructured text. It's very much an LLM-domain problem, but converging on the existing data is like trying to converge on noise.The opportunity in the market is the gap between what people have been doing and what they are trying to do, and I have developed very specialized approaches to narrow this gap in my niche, and so far customers are loving it.I seriously doubt that the gap could ever be closed by throwing more data and compute at it. I imagine though that the outputs of my approach could be used to train a base model to close the gap at a lower unit cost, but I am skeptical that it would be economically worth while anytime soon.reply",
      "This is one reason why verifiable rewards works really well, if it's possible for a given domain. Figuring out how to extract signal and verify it for an RL loop will be very popular for a lot of niche fields.reply",
      "This is my current drum I bang on when an uninformed stakeholder tries shoving LLMs blindly down everyone\u2019s throats: it\u2019s the data, stupid. Current data aggregates outside of industries wholly dependent on it (so anyone not in web advertising, GIS, or intelligence) are garbage, riddled with errors and in awful structures that are opaque to LLMs. For your AI strategy to have any chance of success, your data has to be pristine and fresh, otherwise you\u2019re lighting money on fire.Throwing more compute and data at the problem won\u2019t magically manifest AGI. To reach those lofty heights, we must first address the gaping wounds holding us back.reply"
    ],
    "link": "https://obviouslywrong.substack.com/p/the-bitter-lesson-is-misunderstood",
    "first_paragraph": ""
  },
  {
    "title": "Neovim Pack (neovim.io)",
    "points": 15,
    "submitter": "k2enemy",
    "submit_time": "2025-09-04T00:18:16 1756945096",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=45121915",
    "comments": [
      "I feel like I have to migrate to a new (Neo)Vim package manager about once every 3 or so years. I think my path has been: pathogen -> Vundle -> vim-plug -> lazy.nvim. Hopefully, this the last VIM package manager?reply",
      "Lazy.nvim seems pretty triumphant. But yeah a lot of others are also supported by various plugins. Some unity would be nice, abstractly. But man, it's hard to have faith we're going to get as anything good fast and reliable as lazy.nvim. it could happen though!reply",
      "Luckily this is the built in, official, blessed one. So it\u2019s likely going to be the most widely supported and available. (Maybe not most feature rich though)reply",
      "New package manager is pretty slow right now, it's like 200ms to load nvim with lazy vim vs 1 sec with Pack.reply",
      "Plug still gets the job done imo.  But I'm also hopeful that since this one is builtin to the language it'll likely be end-game worthy for lots of users.  I have tried it out and had a painless experience though I never did anything fancy like lazy offered.reply",
      "I'm torn.  I really like Lazy and have never minded having different package managers for Vim over the years.  But having one blessed one is probably better long term, just like built-in LSP and Treesitter.reply"
    ],
    "link": "https://neovim.io/doc/user/pack.html#vim.pack",
    "first_paragraph": "\n\n    Nvim :help pages, generated\n    from source\n    using the tree-sitter-vimdoc parser.\n    \n"
  },
  {
    "title": "Nuclear: Desktop music player focused on streaming from free sources (github.com/nukeop)",
    "points": 246,
    "submitter": "indigodaddy",
    "submit_time": "2025-09-03T15:54:12 1756914852",
    "num_comments": 158,
    "comments_url": "https://news.ycombinator.com/item?id=45117230",
    "comments": [
      "Testimonials on the main website are somewhat unusual - https://nuclearplayer.com/reply",
      ">As a musician, fuck everything about thisPretty wild to include a comment like this in the testimonials.  Sure, you can disagree with the musician on philosophical terms over IP laws and many consumers will always prefer \"free\", but to put this in your testimonials shows that the developers take pride in the act of pissing off musicians.  That just rubs me the wrong way.reply",
      "Musicians are not the consumers, users are the consumers. Some musicians will always be unhappy, this is unavoidable due to complex issues around IP rights, compensation for art, and the length of time it takes to make changes to these systems (not to mention simply how much existing content is out there new and current artists are competing against, attention economy and all that).n=1, I am optimizing for access to as much content as possible while providing as little economic benefit to corporations as possible (ie Spotify) while still supporting the artists I enjoy (whether that's via venmo, paypal, buying their vinyl, buying their digital versions from bandcamp, etc). I also enjoy cheeky devs/builders, can't take any of this too seriously, we're all dead eventually.reply",
      "Musicians are not the consumers, but it's their work being consumed and this software would have no purpose without them. And to be clear, my problem isn't that this software upset some musicians.  It's that the developers highlighting that fact as part of their marketing suggests they take pride in angering musicians. That is a level of disrespect that goes way beyond the sort of passive consumer level disrespect of wanting something for free. It's active hostility compared to mild selfishness.reply",
      "It is one musician whose negative comment is displayed along with people complaining that the whole thing is slow garbage. You might be reading too much into it.reply",
      "I think it\u2019s just funny to post bad reviews.reply",
      "> It's active hostilityNot really? If the testimonials are true, then simply making the app itself is an act of hostility.The parent comment is putting it as nicely as it can be put. If you don't want people to pirate your music, your only path of recourse as a musician is to stop uploading digital copies of your work. There is no honor system in music or data and there never will be.reply",
      "> If you don't want people to pirate your music, your only path of recourse as a musician is to stop uploading digital copies of your work.People have been recording concerts for decades. Often with a bit of help from the sound crew, which can probably be discouraged by musicians with enough influence, but if the only allowed way to hear a song is to attend a concert, lots of people would rather have a recording that a fan made and distributed.reply",
      ">The parent comment is putting it as nicely as it can be put. If you don't want people to pirate your music, your only path of recourse as a musician is to stop uploading digital copies of your work. There is no honor system in music or data and there never will be.I'm just tired of this technolibertarian mindset of \"it's not wrong because no one is stopping me from doing it\". There is no \"honor system\" in life either and if you see that as permission to be an asshole, that just makes you an asshole.  And if your best defense against being accused of being an asshole is some form of \"they couldn't stop me\", then you're tacitly admitting to being an asshole.reply",
      "Unfortunately the winds are not blowing in your preferred direction. We are being shown time and time again and in increasing frequency that being an asshole is the best way to succeed.reply"
    ],
    "link": "https://github.com/nukeop/nuclear",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n        Streaming music player that finds free music for you\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Nuclear requires ongoing maintenance to keep everything working. This version has not been maintained for a while, so expect things to be broken.We have started a rewrite here: https://github.com/NuclearPlayer/nuclear-xrdThis new version will have several advantagesStay tuned for updates! Desktop music player focused on streaming from free sourcesOfficial websiteDownloadsDocumentationMastodonTwitterSupport channel (Matrix): #nuclear:matrix.orgDiscord chat: https://discord.gg/JqPjKxESuggest and vote on new features here: https://nuclear.feature"
  },
  {
    "title": "What is it like to be a bat? (wikipedia.org)",
    "points": 106,
    "submitter": "adityaathalye",
    "submit_time": "2025-09-03T17:48:27 1756921707",
    "num_comments": 147,
    "comments_url": "https://news.ycombinator.com/item?id=45118592",
    "comments": [
      "Anil Seth recently wrote a book \"Being You\", which very much states that we can only know what it is like to be ourselves.Basically, to know what it is like to be a bat, you need to have evolved as a bat.His theory that our perception is a hallucination generated by a prediction algorithm that uses sensory input to update and correct the hallucination is very interesting.reply",
      "The bat article might be a more philosophical treatise...But it makes me think of this article:https://www.grandin.com/references/thinking.animals.htmlwhich is a more concrete(?) dive into being an animal?reply",
      "Somebody used this paper to make the term batfished, which they defined as being fooled into ascribing subjectivity to a non-sentient actor (i.e. an AI).https://partiallyexaminedlife.com/2025/06/30/what-is-it-like...reply",
      "Nagel's \"What is it like to be a bat?\" assumes that bats are conscious, and that the question of what is the subjective experience of being a bat (e.g. what  does the sense of echolocation feel like) is therefore a meaningful question to ask.The author inventing \"batfished\" also believes bats to be conscious, so it seems a very poorly conceived word, and anyways unnecessary since anthropomorphize works just fine... \"You've just gaslighted yourself by anthropomorphizing the AI\".reply",
      "There isn't even a definitive definition of conscious, but you are somehow positive that bats don't possess it..reply",
      "I love this, hope it takes off like \"enhsittification\" or \"slop\" have already.reply",
      "I'll add it to my anti-AI bingo cardreply",
      "Uhgh \"slop\" is ok but \"enshittification\" was lame from the start.reply",
      "Not only is it a terrible term, but it describes a concept that isn\u2019t really worthy of having its own term. It\u2019s really just a way of saying \u201cpeople will make things worse over time\u201dreply",
      "That isn't what it means though. It means specifically that companies will make products and services worse over time for profit.reply"
    ],
    "link": "https://en.wikipedia.org/wiki/What_Is_It_Like_to_Be_a_Bat%3F",
    "first_paragraph": ""
  },
  {
    "title": "VibeVoice: A Frontier Open-Source Text-to-Speech Model (microsoft.github.io)",
    "points": 398,
    "submitter": "lastdong",
    "submit_time": "2025-09-03T10:44:01 1756896241",
    "num_comments": 146,
    "comments_url": "https://news.ycombinator.com/item?id=45114245",
    "comments": [
      "I read the comments praising these voices as very life like, and went to the page primed to hear very convincing voices. That is not at all what I heard though.The voices are decent, but the intonation is off on almost every phrase, and there is a very clear robotic-sounding modulation. It's generally very impressive compared to many text-to-speech solutions from a few years ago, but for today, I find it very uninspiring. The AI generated voice you hear all over YouTube shorts is at least as good as most of the samples on this page.The only part that seemed impressive to me was the English + (Mandarin?) Chinese sample, that one seemed to switch very seamlessly between the two. But this may well be simply because (1) I'm not familiar with any Chinese language, so I couldn't really judge the pronunciation of that, and (2) the different character systems make it extremely clear that the model needs to switch between different languages. Peut-\u00eatre que cela n'aurait pas \u00e9t\u00e9 si simple if it had been switching between two languages using the same writing system - I'm particularly curious how it would have read \"simple\" in the phrase above (I think it should be read with the French pronunication, for example).And, of course, the singing part is painfully bad, I am very curious why they even included it.reply",
      "Their comments about the singing and background music are odd. It\u2019s been a while since I\u2019ve done academic research, but something about those comments gave me a strong \u201cwe couldn\u2019t figure out how to make background music go away in time for our paper submission, so we\u2019re calling it a feature\u201d vibe as opposed to a \u201cwe genuinely like this and think its a differentiator\u201d vibe.reply",
      "Totally felt the same way! Singing happens spontaneously? What?reply",
      "They mention that in the FAQ here: https://github.com/microsoft/VibeVoice/tree/main?tab=readme-...> In fact, we intentionally decided not to denoise our training data because we think it's an interesting feature for BGM to show up at just the right moment. You can think of it as a little easter egg we left for you.It's not a bug, it's a feature! Okaaaaayreply",
      "The English/Mandarin section was VERY impressive. The accents of both the woman speaking English and the man speaking Chinese were spot on. Both sound very convincingly like they are speaking a second language, which anyone here can hear from the Chinese woman speaking English voice. I'd like to add that the foreigner speaking Chinese was also spot on.reply",
      "Is there any better model you can point at? I would be interested in having a listen.There are people \u2013 and it does not matter what it's about \u2013 that will overstate the progress made (and others will understate it, case in point). Neither should put a damper on progress. This is the best I personally have heard so far, but I certainly might have missed something.reply",
      "It\u2019s tough to name the best local TTS since they all seem to trade off on quality and features and none of them are as good as ElevenLabs\u2019 closed-source offering.However Kokoro-82M is an absolute triumph in the small model space. It curbstomps models 10-20x its size in terms of quality while also being runnable on like, a Raspberry Pi. It\u2019s the kind of thing I\u2019m surprised even exists. Its downside is that it isn\u2019t super expressive, but the af_heart voice is extremely clean, and Kokoro is way more reliable than other TTS models: It doesn\u2019t have the common failure mode where you occasionally have a couple extra syllables thrown in because you picked a bad seed.If you want something that can do convincing voice acting, either pay for ElevenLabs or keep waiting. If you\u2019re trying to build a local AI assistant, Kokoro is perfect, just use that and check the space again in like 6 months to see if something\u2019s beaten it. https://huggingface.co/hexgrad/Kokoro-82Mreply",
      "What is your opinion about F5-TTS or Fish-TTS?reply",
      "There's a certain know-nothing feeling I get that makes me worried if we start at the link (which has data showing it > ElevenLabs quality), jump to eh it's actually worse than anything I've heard then last 2 years, and end up at \"none are as good as ElevenLabs\" - the recommendation and commentary on it, of course, has nothing to do with my feeling, cheersreply",
      "Not OS or local, but just try ChatGPT Voice Conversation mode. To my ears, it's a generation ahead of these VibeVoice samples.reply"
    ],
    "link": "https://microsoft.github.io/VibeVoice/",
    "first_paragraph": "\n\ud83d\udcc4 Report\n\u00b7\n Code\n\u00b7\n\ud83e\udd17 Hugging Face\n\u00b7\n\n Demo\n        \n\n        VibeVoice is a novel framework designed for generating expressive, long-form, multi-speaker conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.\nA core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a next-token diffusion framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.\nThe model can synthesize speech up to 90 minutes long with up to 4 distinct speakers, surpassing the typical 1-2 speaker limits of many prior models."
  },
  {
    "title": "Writing a C compiler in 500 lines of Python (2023) (vgel.me)",
    "points": 141,
    "submitter": "ofou",
    "submit_time": "2025-09-03T16:28:23 1756916903",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=45117668",
    "comments": [
      "Previously:Writing a C compiler in 500 lines of Python - https://news.ycombinator.com/item?id=37383913 - Sept 2023 (165 comments)reply",
      "I find it surprising that a single-pass compiler is easier to implement than a traditional lexer->parser->AST->emitter.  (I'm not a compiler expert, though.)  I'd have expected that generating an AST would be at least as simple, if not simpler.  Plus by generating an AST, doing some simple optimization is a lot easier: one can pattern-match parts of the AST and replace them with more efficient equivalents.  Maybe I'm overthinking this, though.  I tend to like extensible program designs, even when they don't necessarily make sense for the scale of the program\u2026Still a really cool article and an impressive project, though.  I especially like the StringPool technique; I'll have to keep it in mind if I ever write a compiler!reply",
      "A single-pass compiler is easier to implement in part because you're not going to do any of that optimization. You're writing a single-pass compiler either because you're banging out a quick sketch of an idea, and you don't care about production use, or because you've time-traveled back to the '70s or the '80s, where processors were so painfully slow and memory so eye-wateringly expensive that you might not even be able to read the entire source file into RAM at once, much less convert it all into some intermediate representation before starting to write out the machine code.reply",
      "Not sure if fewer LoC necessarily implies easier!reply",
      "I think this might depend on the language you're writing in.Historically, at least, it's pretty verbose to define a data type in Python compared to languages that are more designed for writing compilers.  Consider these definitions from my prototype Bicicleta interpreter, which is written in ML, specifically OCaml:    type methods = NoDefs\n                               (* name, body, is_positional ... *)\n                   | Definition of string * bicexpr * bool * methods\n    and bicexpr = Name of string\n                  | Call of bicexpr * string\n                  | Literal of string option * methods\n                  | Derivation of bicexpr * string option * methods\n                  | StringConstant of string\n                  | Integer of int\n                  | Float of float\n                  | NativeMethod of (lookup -> bicobj)\n\nThose ten lines of code would be ten classes in Python with an average of 1.6 attributes each.  Using dataclasses or attrs, that would be 36 lines of code, and then (if you're doing it the OO way) every function that I defined on one of these OCaml types becomes a method implemented in each class implementing a particular protocol, with a copy of its argument signature in every class.  (If you used namedtuple instead, it's no less code, but you write it on less lines.)  So, for example, this function on bicexprs    let rec freevars = function\n        Name n -> stringset [n]\n      | Integer _ | StringConstant _ | Float _ -> stringset [\"prog\"]\n      | NativeMethod _ -> stringset []\n      | Literal (Some selfname, methods) -> \n            StringSet.diff (freevars_methods methods) (stringset [selfname])\n      | Literal (None, methods) -> freevars_methods methods\n      | Derivation(object_, self, methods) ->\n            StringSet.union (freevars object_) (freevars (Literal(self, methods)))\n      | Call(object_, _) -> freevars object_\n\nbecomes six to eight method definitions in the different classes.  (You can cut it down to six if you define an abstract base class for the constant classes.)  And Literal.freevars needs an if-then-else.  So that's another 20 lines of code.Python does support pattern-matching now, so functions like this might not be any more verbose than the ML version if you program them the same way instead of in the OO fashion.  I haven't tried using Python pattern-matching, so I don't really know.In general, though, Python is more verbose than ML-family languages for this kind of thing by a factor of about 2\u20134, and that's before you count the test code you need in Python to get the kind of confidence in correctness that ML's type-checking gives you with no extra code.  To my knowledge, Mypy doesn't do the kinds of pattern-matching-exhaustiveness checks that ML compilers do.I've sometimes \"cheated\" by trying to write code like this in Python using regular tuples rather than named tuples.  You can definitely make it work, but it's a real pain to debug.Quoting Andy Chu from https://andychu.net/projects/:> Python is not the right language for [implementing] languages. I will use OCaml for subsequent projects like this.Python does have GC and dynamic dispatch, though, and those count for a lot.reply",
      "I think it depends on the language. I heard Turbo Pascal was pretty fast because 1) Pascal\u2019s language features, 2) no optimization in TP 1.0 at least.reply",
      "Fast to run or fast to compile?reply",
      "This article breaks it down well enough to make me feel like I could write my own C compiler targeting AVR. (I probably could... but it would not be easy.)Never actually looked into how compilers work before, it's surprisingly similar/related to linguistics.reply",
      "It's b/c when Chomsky invented the theory of formal grammars he was studying natural languages & the universality of abstract grammar\u00b9. Computer scientists realized later that they could use the same theory as a foundation for formalizing the grammatical structures of programming languages.\u00b9https://en.wikipedia.org/wiki/Chomsky_hierarchyreply",
      "Similar experience in DNA/genome analysis.  A large part of DNA analysis was based on parser theory.This paper was my introduction to DNA analysis as well as Chomsky hierarchy: https://www.jstor.org/stable/29774782 (I wasn't able to find a free copy).IIRC, pseudoknots in RNA require context-free grammars to parse.reply"
    ],
    "link": "https://vgel.me/posts/c500/",
    "first_paragraph": "A few months ago, I set myself the challenge of writing a C compiler in 500 lines of Python1, after writing my SDF donut post.\nHow hard could it be?\nThe answer was, pretty hard, even when dropping quite a few features.\nBut it was also pretty interesting, and the result is surprisingly functional and not too hard to understand!There's too much code for me to comprehensively cover in a single blog post2, so I'll just give an overview of the decisions I made, things I had to cut, and the general architecture of the compiler, touching on a representative piece of each part.\nHopefully after reading this post, the code is more approachable!The first, and most critical decision, was that this would be a single-pass compiler.\n500 lines is too spare to be defining and transforming an abstract syntax tree!\nWhat does that mean?Well, most compiler's internals look something like this:The tokens get lexed, then a parser runs over them and builds pretty little syntax trees:The important thing here i"
  },
  {
    "title": "Evaluating Agents (aunhumano.com)",
    "points": 7,
    "submitter": "mfalcon",
    "submit_time": "2025-09-03T23:32:57 1756942377",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://aunhumano.com/index.php/2025/09/03/on-evaluating-agents/",
    "first_paragraph": "aunhumanoNo amount of evals will replace the need to look at the data, once you have a evals good coverage you\u2019ll be able to decrease the time but it\u2019ll be always a must to just look at the agent traces to identify possible issues or things to improve.You must create evals for your agents, stop relying solely on manual testing.Not sure where to start?Add e2e evals, define a success criteria (did the agent meet the user\u2019s goal?) and make the evals output a simple yes/no value.This is much better than no evals.By performing simple end to end agent evaluations you can quickly manage to:\u2013 identify problematic edge cases\u2013 update, trim and refine the agent prompts\u2013 make sure you are not breaking the already working cases\u2013 compare the performance of the current llm model vs. cheaper onesOnce you created the e2e evals you can move on with \u201cN \u2013 1\u201d evals, that is, evals that need to \u201csimulate\u201d previous interactions between system and user.Suppose that either by looking at the data or by running "
  },
  {
    "title": "Voyager \u2013 An interactive video generation model with realtime 3D reconstruction (github.com/tencent-hunyuan)",
    "points": 297,
    "submitter": "mingtianzhang",
    "submit_time": "2025-09-03T11:07:16 1756897636",
    "num_comments": 204,
    "comments_url": "https://news.ycombinator.com/item?id=45114379",
    "comments": [
      "The license used for this is quite a read.  Available to the world except the European Union, the UK, and South Korea\n\nNot sure what led to that choice. I'd have expected either the U.S. & Canada to be in there, or not these.  3. DISTRIBUTION.\n  [...]\n  c. You are encouraged to: (i) publish at least one technology introduction blogpost or one public statement expressing Your experience of using the Tencent HunyuanWorld-Voyager Works; and (ii) mark the products or services developed by using the Tencent HunyuanWorld-Voyager Works to indicate that the product/service is \u201cPowered by Tencent Hunyuan\u201d; [...]\n\nWhat's that doing in the license? What's the implications of a license-listed \"encouragement\"?reply",
      "> Not sure what led to that choice.It's the EU AI act. I've tried their cute little app a week ago, designed to let you know if you comply, what you need to report and so on. I got a basically yes, but likely no, still have to register to bla-bla and announce yak-yak and do the dooby-doo, after selecting SME - open source - research - no client facing anything.It was a mess when they proposed it, it was said to be better while they were working on it, turns out to be as unclear and as bureaucratic now that it's out.reply",
      "If I was Russia and/or China and I wanted to eliminate EU as a potential rival economically and militarily, then I don't think I could have come up with a better way to do it than EU regulations. If it was not for the largess of the US, then EU would become a vassal of Russia and/or China. And I think the US is running out of good will very rapidly. The EU could, of course, shape up, but it won't.reply",
      "It's hard not to react sarcastically to this. But I will try:There's nothing special about EU regulations vis-a-vis other laws. China, Russia and the US also have laws, many of which are also perceived as overly bureaucratic.reply",
      "Identifying something as a critical competitive industry, then place a bunch of hurdles in front of it's development, and sit confused when we get left behind - that's the EU special.reply",
      "Left behind on what exactly? Privacy laws? Consumer rights?reply",
      "Technology, AI, semiconductors, cloud computing, consumer electronics, social media, app ecosystems, e-commerce, military technology, energy independence, venture capital, unicorns and scaling, banking innovation, space exploration, biotech and pharma...reply",
      "Novo Nordisk has a market cap of 250 billion last I checked.",
      "...to name a fewreply",
      "Growth industries.reply"
    ],
    "link": "https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Voyager is an interactive RGBD video generation model conditioned on camera trajectory, and supports real-time 3D reconstruction.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\u4e2d\u6587\u9605\u8bfb\n\nWe introduce HunyuanWorld-Voyager, a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Voyager can generate 3D-consistent scene videos for world exploration following custom camera trajectories. It can also generate aligned depth and RGB video for efficient and direct 3D reconstruction.Join our Wechat and Discord group to discuss and find help from us.Voyager consists of two key components:(1) World-Consistent Video Diffusion: A unified architecture that jointly generates aligned RGB and d"
  },
  {
    "title": "Understanding Transformers Using a Minimal Example (rti.github.io)",
    "points": 135,
    "submitter": "rttti",
    "submit_time": "2025-09-03T15:30:06 1756913406",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=45116957",
    "comments": [
      "Honest feedback - I was really excited when I read the opening. However, I did not come away from this without a greater understanding than I already had.For reference, my initial understanding was somewhat low: basically I know a) what embedding is basically b) transformers work by matrix multiplication, and c) it's something like a multi-threaded Markov chain generator with the benefit of prior-trained embeddingsreply",
      "Have you checked out this video from 3Blue1Brown that talks bit about transformers?https://youtu.be/wjZofJX0v4Mreply",
      "I've seen it but I don't believe I've watched it all the way through. I will nowreply",
      "I'd also recommend another article on this topic of LLMs discussed a few days ago. I read it to the finish line and understood everything fully:> How can AI ID a cat?https://news.ycombinator.com/item?id=44964800reply",
      "So glad you shared this. Super accessible without diluting. Thank you!reply",
      "very cool!reply"
    ],
    "link": "https://rti.github.io/gptvis/",
    "first_paragraph": "\n          Robert Timm\n          <mail@rtti.de>\nPublished: May 2, 2025\n            The internal mechanisms of Transformer Large Language models (LLMs),\n            particularly the flow of information through the layers and the\n            operation of the attention mechanism, can be challenging to follow\n            due to the vast amount of numbers involved. We humans can hardly\n            form a mental model. This article aims to make these workings\n            tangible by providing visualizations of a Transformer's internal\n            state. Utilizing a minimal dataset and a deliberately simplified\n            model, it is possible to follow the model's internal processes\n            step-by-step. One can observe how information is transformed across\n            different layers and how the attention mechanism weighs different\n            input tokens. This approach offers a transparent view into the core\n            operations of a Transformer.\n          \n            Dataset and"
  },
  {
    "title": "New Knot Theory Discovery Overturns Long-Held Mathematical Assumption (scientificamerican.com)",
    "points": 37,
    "submitter": "baruchel",
    "submit_time": "2025-09-02T11:35:22 1756812922",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=45101675",
    "comments": [
      "Wow, this problem has been around for a long time. Exciting to see this finally figured out.reply",
      "Without paywall: https://www.removepaywall.com/search?url=https://www.scienti...reply",
      "Linked paper: https://arxiv.org/abs/2506.24088\u201cUnknotting number is not additive under connected sum\u201d (2025 v1)> We give the first examples of a pair of knots K1,K2 in the 3-sphere for which their unknotting numbers satisfy u(K1#K2)<u(K1)+u(K2) . This answers question 1.69(B) from Kirby's problem list, \"Problems in low-dimensional topology\", in the negative.reply"
    ],
    "link": "https://www.scientificamerican.com/article/new-knot-theory-discovery-overturns-long-held-mathematical-assumption/",
    "first_paragraph": "September 2, 20252 min readNew Knot Theory Discovery Overturns Long-Held Mathematical AssumptionMathematicians have unraveled a key conjecture about knot theoryBy Max Springer edited by Sarah Lewin FrasierIn a recent preprint paper, mathematicians connected two knots in a way that could be undone in a surprisingly small number of moves.Amanda Monta\u00f1ezJoin Our Community of Science Lovers!Scanning the crowd at a fancy soiree may reveal a wide array of neckties, each fastened with a highly complex mathematical object masquerading as fashion. An entire field of mathematics is devoted to understanding mathematical knots, which one can obtain from any traditional knot by gluing the loose ends together. Mathematicians long believed that if you attach cut ends of two different knots to each other, the new knot will be just as complex as the sum of the individual knots\u2019 complexity. But researchers recently managed to find a knot that is simpler than the sum of its parts.Knot theory is a branch "
  },
  {
    "title": "OSMAnd vs. Organic Maps (firedrake.org)",
    "points": 79,
    "submitter": "icheyne",
    "submit_time": "2025-09-01T08:14:53 1756714493",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=45090654",
    "comments": [
      "Recent HN thread on the fork of CoMaps from Organic Maps:https://news.ycombinator.com/item?id=44994927reply",
      "Note however that unlike Organic Maps, CoMaps lacks support for old devices. Without Organic Maps, an old iPad would be a useless junk instead of a great navigation aid.reply",
      "I use Osmand a lot. It work well, but I dearly wish they'd improve some UX issues.I always seem to have the map telling me how far it is to some temporary marker I placed months ago that I can't easily work out how to remove.Also any time I do navigation, the Trip Recording plugin pops up as a sticky system notification even when I haven't enabled trip recording.But the offline navigation is a killer feature, and following custom GPX's.reply",
      "> I always seem to have the map telling me how far it is to some temporary marker I placed months ago that I can't easily work out how to remove.You can either tap the marker and tick it off, or use Menu -> Map markers if you can't find it. You can also use Menu -> Configure map and turn off Map markers completely.> Also any time I do navigation, the Trip Recording plugin pops up as a sticky system notification even when I haven't enabled trip recording.Hmm, it does pop up every time for me but dismissing it works.reply",
      "I've been building https://github.com/styluslabs/maps with support for 3D terrain, custom layers, and plugins for search and routing.Vector tiles are generated and served on-demand by https://github.com/styluslabs/geodesk-tiles so there's no need to download an entire country or region first.reply",
      "Wow the readme looks very impressive, would love to try this on my next off grid trekIs it just you building it, is there a business behind it?reply",
      "I couldn\u2019t make heads or tails of how to navigate with OSMAnd until thumbing through GitHub issues and finding out about https://opensupermaps.com/ which, after importing a few gigabytes of text files, allows general search of street addresses - not a great learning curve for adoption!What I\u2019d really like to do is copy the old school car GPS interface of, select state, select city, select street, house number, where at each stage it narrows down the list of possibilities so you only have to type 3 or 4 letters before auto completion. If there\u2019s any pull request I would make it would be to build that out using the open super maps databasereply",
      "It calls Organic Maps new (and the article is recent) but Organic Maps has been around for a while now?FWIW I prefer Organic Maps for casual usage - I think OSMAnd is very featureful but the UI is less intuitive IMO.reply",
      "It's been around for some time and is a fork of Maps.me, which was called MapWithMe before this.reply",
      "What do you use Organic Maps for? Driving directions, walking, hiking, cycling? All of those or something else? It doesn't seem good for hiking because it's missing so many basic metrics that Gaia has (elevation data, different speeds like rate of ascent). I've got a rough impression that it's mostly suited for urban European locales for walking directions, but in the US I only use mapping apps for driving (turn by turn directions) and hiking in mountainous wilderness (as opposed to some places in Europe that had nearly urban \"hiking\").reply"
    ],
    "link": "https://blog.firedrake.org/archive/2025/09/OSMAnd_vs_Organic_Maps.html",
    "first_paragraph": "There's a new offline mapping program for smartphones, so I thought\nI'd see how it stacks up against the one I'm already using.For clarity: I'm using the F-Droid releases of both\nOsmAnd and Organic\nMaps. I believe\nthat OsmAnd charges for map downloads if you get it from other places.Both programs work most readily with maps loaded onto the device in\nadvance (which is why I feel I should choose\u2014I don't want to have maps\nfor both taking up space). Both of them run off OpenStreetMap data,\nadapted into a custom format and published as roughly monthly updates\non their own servers. Both have a tendency to let you accidentally\nrotate the map rather than locking north at the top forever. Both have\nvaguely shadowy and possibly exploitative governance. How do they\ndiffer?Organic Maps is much newer and generally less featureful than OsmAnd;\nconversely OsmAnd can feel bloated and over-complex.OsmAnd shows more information by default; Organic Maps is cleaner.\n(OsmAnd is ferociously configurable, th"
  },
  {
    "title": "Eels are fish (eocampaign1.com)",
    "points": 67,
    "submitter": "speckx",
    "submit_time": "2025-09-03T14:02:18 1756908138",
    "num_comments": 101,
    "comments_url": "https://news.ycombinator.com/item?id=45115941",
    "comments": [
      "I read the blog post. Then I thought \"surely the eels in my local southern German lakes can't be from the sea\". But sure enough, the European eel hatches close to the Bahamas.I audibly wtf'ed multiple times while going down this rabbit hole. Thanks!https://en.wikipedia.org/wiki/European_eelreply",
      "Recommend \"The Truth About Animals\" by Lucy Code [0]. It has good chapter on eels. They take a left and go to the USA or take a right and go to Europe.[0] https://www.goodreads.com/book/show/34211802-the-unexpected-...reply",
      "I had the same thought. I always knew they were fish but always assumed they were local fresh water fish. I mean everyone talks about how Salmon does this incredible journey. If there was another species which did something equally incredible I should have heard about it.Thanks for the link! A rabbit hole indeed.reply",
      "These eels undergo a notable sequence of transformations before their journey back to the sea. It wasn't until the 19th century that science connected the transitions from glass eel (larval form), to elvers, to yellow eel (freshwater adult), to silver eel (ocean spawning) form as members of the same species. Salmon are less mysterious as their spawning could be observed.reply",
      "The author does not appear to be aware of this but eels are not the most snake-like among fish.Already the Ancient Greek and Roman authors had a classification of fish, where eels where less snake-like, because they have pectoral fins, while the most snake-like group of fishes consisted of morrays and lampreys, both of which have neither scales nor any kind of fins, being less similar to other fish than eels.The loss of the legs and the elongation of the body, resulting in a snake-like form has happened not only in many groups of vertebrates, including eels and morrays, caecilian amphibians, snakes and several groups of legless lizards, but also in many worms, e.g. earthworms and leeches, which evolved from ancestors with legs. Even among mammals, weasels and their relatives have evolved towards a snake-like form, though they still have short legs.reply",
      "I know that the lampreys are often lumped in with the fish, but the jawed fish are more closely related to us than to lampreys.(Fish aren't a clade at all so call em whatever you want.)reply",
      "Wait what...Earthworms??reply",
      "The directional bristles for anchoring to dirt (more noticeable on the larger earthworm species) are the remnants of polychaete parapodia. Similar to snakes that occasionally have remnant claws.reply",
      "Incredibly, I actually did learn this today because it was in the NYT crossword and I went down a very similar rabbit hole. I never made it to Freud, though, after I discovered and got sucked into the European Union Eel Regulation Framework[1].If you, like me, are masochistically fascinated by this kind of \u201cI can\u2019t believe this is a real thing that the government actually does\u201d documentation I recommend giving it a once-over.1. https://oceans-and-fisheries.ec.europa.eu/ocean/marine-biodi...reply",
      "I'm surprised to learn that it is surprising that eels are fish. I mean, they live in water, they have fins, they're generally fish-shaped... What's more surprising is their incredible life cycle and reproductive journey. I'm surprised the author didn't put that in the title.reply"
    ],
    "link": "https://eocampaign1.com/web-version?p=495827fa-8295-11f0-8687-8f5da38390bd&pt=campaign&t=1756227062&s=033ffe0494c7a7084332eb6e164c4feeeb6b4612e0de0df1aa1bf5fd59ce2d08",
    "first_paragraph": ""
  },
  {
    "title": "Depot (YC W23) Is Hiring a Solutions Engineer (Remote US and Canada) (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-09-03T21:00:27 1756933227",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/depot/jobs/U54HGtn-solutions-engineer",
    "first_paragraph": "Build faster. Waste less time.Depot is growing rapidly and reinventing the software build space, so we are now looking for our first dedicated Solutions Engineer to bridge the gap between our innovative technology and the developers who need it most. This is a rare opportunity for an experienced developer who wants to help peers make dramatic gains in their day-to-day jobs, and ultimately for their organizations.\nAn ideal candidate would be someone who is already a Depot user and fan who wants to find a new role in a fast-growing, venture-backed startup. There is no template for this role, so it requires a self-starter to shape how we support and grow our customer base, working directly with engineering teams at fast-growing companies to solve their most critical build performance challenges.\n\nTo support our rapidly growing customer base, we are looking for Solutions Engineers based in the US or Canada.\n\nDepot has created a build performance and developer productivity platform unlike a"
  },
  {
    "title": "Poor man's bitemporal data system in SQLite and Clojure (evalapply.org)",
    "points": 114,
    "submitter": "adityaathalye",
    "submit_time": "2025-09-03T17:47:50 1756921670",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=45118585",
    "comments": [
      "It's a pity that Clojure is kind of a hermetic space these days, because the concept of bitemporality really deserves much more attention. It's amazing how often you want to know \"What was the P&L for March using the data available on Apr 4?\" and how uncommon it is to find a database design that supports that kind of query.reply",
      "'Hermetic' is an interesting word-choice, considering Clojure has a habit/raison detre of attaching to other host languages/ecosystems i.e. Jank (C++), ClojureScript (JS), Basilisp (Python), Phel (PHP), Babashka (general scripting), and of course, Clojure itself on the JVM.reply",
      "It's not particularly rare in analytical databases/query engines, nearly all of which support AsOf joins these days, including  * Clickhouse\n  * DuckDB\n  * BigQuery\n  * Snowflake\n  * QuestDBreply",
      "AsOf join in those systems solves a rather narrow problem of performance and SQL expressiveness for data with overlapping user-defined timestamps. The bitemporal model solves much broader issues of versioning and consistent reporting whilst also reducing the need for many user-defined timestamp columns.In a bitemporal database, every regular looking join over the current state of the world is secretly an AsOf join (across two dimensions of time), without constantly having to think about it when writing queries or extending the schema.reply",
      "\"It's a pity that Clojure is kind of a hermetic space these days\"If you don't mind sharing, I'm curious why you feel this way.reply",
      "Spanner does that with ease (I worked there, so I\u2019m clearly biased).https://cloud.google.com/spanner/docs/timestamp-bounds#exact...reply",
      "That only covers the 'transaction time' axis though? And the page says retention is limited to 1 week. No doubt useful for some things, but probably not end-user reporting requirements.reply",
      "I don\u2019t understand footnote 16? It sounds like he\u2019s saying he\u2019s ceo of htmx and datastar?reply",
      "Yes... This is a running joke in both communities. IYKYK :)reply",
      "Oh ok, I\u2019m out of the loop on it ;) thought it was some sort of ai hallucination.reply"
    ],
    "link": "https://www.evalapply.org/posts/poor-mans-time-oriented-data-system/index.html",
    "first_paragraph": "\n2025 Q3 |\u00a0\n            \n              For Hire\n             \u00a0\u00bb\u00a0\n          \n          \"Full-systems\"\n          B2B SaaS\nbuilder\n          + FP\ncoach.\n           \u00a0|\u00a0WIP public \u03b2\u00a0\u00bb\u00a0Project\n          \n            \"Writing for nerds\".\n        Poor man's bitemporal data system in SQLite and Clojure\n[ \u2193 toc ]\nPublished: 2025-07-14\nUpdated: 2025-07-15\nBy: Aditya Athalye\n\n        On trying to mash up SQLite with ideas stolen from Accountants, Clojure, Datomic, XTDB, Rama, and Local-first-ers, to satisfy Henderson's Tenth Law. Viz., to make a sufficiently complicated data system containing an ad-hoc, informally-specified, bug-ridden, slow implementation of half of a bitemporal database. Because? Because laying about on a hammock, contemplating hopelessly complected objects like Current Databases isn't just for the Rich man.\n      Especially fellow Clojurians trying to realise their Indie B2B SaaS dreams (translation: income and time-poor). Please use a proper professional time-oriented data sys"
  },
  {
    "title": "Speeding up PyTorch inference on Apple devices with AI-generated Metal kernels (gimletlabs.ai)",
    "points": 130,
    "submitter": "nserrino",
    "submit_time": "2025-09-03T17:03:35 1756919015",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=45118111",
    "comments": [
      "They are comparing unoptimized PyTorch inference, something you would never deploy on a device, to a model with custom kernels.Yes, of course the model with custom kernels is faster, whether it's written by a human or an AI.Generally, PyTorch inference is meant to be used during the training process, and when running metrics, not when deploying. When deployed, you should export to ONNX, and then compile the ONNX to the native format of the device.If you aren't familiar with the pipeline for ML deployment, this is the equivalent of comparing interpreted code to compiled code.reply",
      "I have never really worked with pytorch professionally, but it feels to me a lot of the open source, especially generative oriented projects, just use pytorch like this. It makes hacking on the models a whole lot easier.comfyui is a good example of a project like this.reply",
      "PyTorch is the baseline because that's what people prototype in, and the most common reference point. The aim here is to show that you can start from prototype code and automatically produce lower-level kernels (in this case Metal) that are more usable in real deployments, without additional work from the developer. Frontier models are capable at generating efficient Metal kernels automatically/immediately, and will only get better. We expect to see significant improvements as we refine the approach, but it's enough to show this seems to be a tractable problem for AI.reply",
      "> and then compile the ONNX to the native format of the device.I'm assuming you are talking about https://github.com/onnx/onnx-mlir?In your experience, how much faster is a \"compiled\" onnx model vs. using an onnx runtime?reply",
      "For other people reading this:Back in the day TensorFlow had tfdeploy which compiled TensorFlow terms into NumPy matrix operations.  Our synthetic tests saw speedups of factor 50.reply",
      "> Yes, of course the model with custom kernels is faster, whether it's written by a human or an AI.But that\u2019s the thing, I wouldn\u2019t write a custom kernel before AII don't do that level of development or operate at that part of the stack but I\u2019m very experienced in software developmentAI significantly augments my skillsets in this areareply",
      "I work on PyTorch and there are many things that make me suspicious about these results. My TL;DR is unless we get a zip file of all the kernels with how they're benchmarked results like this are almost impossible to verify1. I don't have an M4 but I have an M1 Pro and I tried running the claimed 18x speedup VisionAttention attention example and I get close to identical runtimes. This example has more issues the main optimization the LLM is doing is a fusion and so not comparing to torch.compile is a bit sus. The numerics are off as well and I suspect the atols were way too big. Finally MultiHeadAttention is a deprecated API so using neither SDPA or torch.compile is a weird choice2. In general 18x (and even some 100x speedups claimed near the end) are just a smell that some kernel is incorrect, the typical way you can get speedups like this is you don't warmup or you forget to synchronize. PyTorch has a lot of benchmarking footguns which is why sharing the exact eval scripts is helpful3. Speaking of footguns, the shapes I saw in the examples were tiny, in that regime you're more often measuring noise as the primary bottleneck is not compute or memory but overhead4. Generating many random shapes is also not so safe, some input distributions can make certain kernels trivial for example torch.randn() by default generates samples from a normal distribution with mean 0 and variance 1 and so if you take the mean of a large vector you're almost guaranteed to just get 0 esp if your tolerance is too high5. KernelBench levels measure vastly different things and if you want to compare to PyTorch operators you want to focus on Level 1, Level 2 is fusions and so the right baseline is torch.compile and more reliable on nightlies. The Mamba 2 example (which I didn't run) also acknowledges that the primary thing it does is fusions which assuming everything is correct would still be strange to baseline vs eagerSo please for everyone's sanity if you find a kernel that's 10-100x faster please share the exact code and benchmarking methodology to your smartest performance friends, you should be extremely skeptical of such results often you can discard some numbers based on a simple speed of light analysis. We all desperately want faster kernels but to get them we have to be really fanatical about correctness.reply",
      "Hey, thanks for the thoughtful comments. A lot of big claims have been made in this area so skepticism is the right default reaction. tl;dr: agree that we should provide the kernels and benchmark suite so this can be evaluated by others, will follow up with that.A few clarifications:1. Baselines - We didn't compare to torch.compile because as of PyTorch 2.7, torch.compile doesn't support the MPS backend, and we ran into some issues on many of the problems when using it. GitHub issue: https://github.com/pytorch/pytorch/issues/150121. Once it's supported, it will be the obvious baseline.2. Methodology - We followed KernelBench\u2019s protocol to establish a baseline on Metal, adding more correctness checks. Warmup and synchronization were done. We recognize the limitations here and are expanding the validation suite.3. Optimizations - Right now most of the optimizations are fusions, but there is some use of Metal-specific primitives/optimizations. We expect as we make the supervisor more sophisticated, the novelty of the optimized kernels will also increase.Overall the goal here is to get some % of the benefit of a human expert in kernel engineering, without developer effort. Compiler-based optimizations are great, but hand-tuned implementations are still common for performance-critical models. The hope is that we can automate some of that process.reply",
      "This is gonna be a silly question but what does \u201ckernel\u201d mean in this context. I thought it meant like a Linux kernel module but doesn\u2019t seem to be?reply",
      "A kernel is low level function that is going to run in parallel on your accelerator (hopefully efficiently). You will have various matmuls, convolutions, etc.If you search CUDA kernel you\u2019d find examples but the term was used in HPC before as well.reply"
    ],
    "link": "https://gimletlabs.ai/blog/ai-generated-metal-kernels",
    "first_paragraph": "tl;dr: Our lab investigated whether frontier models can write optimized GPU kernels for Apple devices to speed up inference. We found that they can: our AI-generated Metal kernels were 1.87x faster across 215 PyTorch modules, with some workloads running hundreds of times faster than baseline.AI models execute on hardware via GPU kernels that define each operation. The efficiency of those kernels determines how fast models run (in training and inference). Kernel optimizations like FlashAttention1 show dramatic speedups over baseline, underscoring the need for performant kernels.While PyTorch and tools like torch.compile2 handle some kernel optimizations, the last mile of performance still depends on handtuned kernels. These kernels are difficult to write, requiring significant time and expertise. It gets especially challenging when writing kernels outside of CUDA: expertise in non-CUDA platforms is rarer, and there is less tooling and documentation availableWe set out to answer a simple"
  },
  {
    "title": "Microsoft BASIC for 6502 Microprocessor \u2013 Version 1.1 (github.com/microsoft)",
    "points": 221,
    "submitter": "marvinborner",
    "submit_time": "2025-09-03T17:28:32 1756920512",
    "num_comments": 143,
    "comments_url": "https://news.ycombinator.com/item?id=45118392",
    "comments": [
      "There's the \"WAIT 6502,X\" Easter egg[0]!Lines 6530 - 6539 are the \"MICROSOFT!\" that gets printed.Line 4914 is the code to check the address passed to WAIT and, if correct, print the \"MICROSOFT!\".It really is inconspicuous. A source licensee definitely wouldn't find it by quickly perusing.[0] https://www.pagetable.com/?p=43reply",
      "\"\".join(map(lambda n: chr((n & 0o77) + 64), reversed([0o241, 0o124, 0o106, 0o217, 0o23, 0o217, 0o122, 0o103, 0o211, 0o315])))\n\nThis python one-liner roughly recovers the hidden string out of defined bytes.reply",
      "I love how the initial commit is \"48 years ago.\"reply",
      "If you enjoyed the datestamps on that repository, you will definitely enjoy the datestamps on this one:* https://github.com/dspinellis/unix-history-repoYou'll also enjoy the contributors list.reply",
      "I recently diffed Unix 10 and NetBSD to see if or how many common actual byte-for-byte original code I could find. I found some.reply",
      "Of course, because if you have two similar applications, some of the code will likely be the same if the codebase is large enough.reply",
      "Man, imagine a 2.5k line kernel. You could probably fully understand how part of the computer works.reply",
      "2.5 lines of J, K, or some other APL-family language?https://news.ycombinator.com/item?id=13590065reply",
      "How do you write half a line of code?reply",
      "Do you know about half button presses?reply"
    ],
    "link": "https://github.com/microsoft/BASIC-M6502",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Microsoft BASIC for 6502 Microprocessor - Version 1.1\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.This assembly language source code represents one of the most historically significant pieces of software from the early personal computer era. It is the complete source code for Microsoft BASIC Version 1.1 for the 6502 microprocessor, originally developed and copyrighted by Microsoft in 1976-1978.The source code includes conditional compilation support for multiple pioneering computer systems:The source code includes detailed revision history showing active development:This source code represents the foundation upon which the modern software industry was built. The techniques, patterns, and business models pioneered in this BASIC interpreter directly influe"
  },
  {
    "title": "Lively Linear Lisp (1992) [pdf] (utexas.edu)",
    "points": 36,
    "submitter": "todsacerdoti",
    "submit_time": "2025-08-31T23:10:25 1756681825",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.cs.utexas.edu/users/hunt/research/hash-cons/hash-cons-papers/BakerLinearLisp.pdf",
    "first_paragraph": ""
  },
  {
    "title": "Ten Thousand Lifetimes with Roguelikes (mccowan.space)",
    "points": 25,
    "submitter": "YeGoblynQueenne",
    "submit_time": "2025-08-31T10:46:24 1756637184",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=45082145",
    "comments": [
      "Brogue is probably my favourite, followed by Crawl. Much respect to Nethack though: seeing the high level players zip through a run is something to behold.Notable mentions: Cataclysm and JupiterHell.reply",
      "Undated. Probably old considering the link rot and the fallen-off footnotes.reply",
      "It\u2019s probably ~15 or so years old, it mentions Brogue as a recent release and that came out in 2009.Nice to see a mention of the ##crawl channel on freenode, I spent a lot of time there during DCSS 0.9 to 0.12 or so, I even took 3rd in one of the online tournaments! [0] The guy that won that year was a child chess prodigy who then became a mathematics professor who has worked at Princeton, Harvard, and MIT. There was another top level player who was also a mathematician, and both of these guys won a Morgan Prize (and also have Wikipedia articles). I got a GED in my early 30s, somehow I was able to keep up ;)Another regular from ##crawl posts on HN from time to time as well.[0] https://crawl.akrasiac.org/tourney12a/all-players.htmlreply",
      "This is a blast from the past. I hadn't thought of elliptic in years.At some point I should write up my roguelike recollections... maybe ten years ago, Ken Arnold was stopping by Boston. Haran, one of the other crawl devs, found out and drove all the way from New York to have coffee with us. All I really remember of that was Ken explaining that he added the food clock as a way of generically solving scumming.reply",
      "What do you guys play? Me a little DCSS. It's really come along.I like Angband but its crudity repels me. It's enough to make me do my own roguelike. Almost.Any of you try that dwarf fortress roguelike?reply",
      "Caves of Qud is quite good, though a bit less traditional in being a big open world vs a dungeon. There a few quirks and bugs but the game is very fun and creative, and it has excellent music. I also love the graphics but it is an acquired taste.I played the Dwarf Fortress roguelike mode several years ago, and it was really more of a toy - nifty to play around with the mechanics but too dry and arbitrarily difficult to be a fun game. But almost all the dev focus was on fortress management, maybe they\u2019ve spruced up the roguelike with the Steam release.reply",
      "Nethack was the one that really hooked me. Telnetting into nethack.org from the CSCI lab computers during university will always be a fond memory.These days I facilitate roguelike development each year with a group code-along (for lack of a better term). Each year a group of participants follow along together and produce their own roguelike. The tutorial is in Python but folks participate using a bunch of other languages. This year we had people who made it to the end with Rust, C++, C#, and Odin, to name a few. Completion posts are still rolling in so I'm excited to see what this year's batch cooked up. :)reply",
      "Angband I once played a lot, but I can't face it today. I play DCSS once a week, the Sydney server has a weekly challenge. That's enough gaming for me.reply",
      "For me it is the original Rogue, from that to Moria then Nethack.  I still play rogue when I want some thing simple.  But nethack is my goto.FWIW, I won rogue once, never ascended in nethack yet.reply",
      "No discussion of cataclysm DDA. sad.reply"
    ],
    "link": "https://ian.mccowan.space/study/essays/roguelikes/",
    "first_paragraph": "When I was a kid, a few factors combined to give me what I suspect\nwill be a lifelong neurosis about video games: my basic respect for my\nparents\u2019 authority and opinions, my mother\u2019s extreme disdain (it seemed\nto me, at least, at the time) for any and all electronic gaming, and my\nborderline obsession with same. My brother and I weren\u2019t allowed a game\nsystem until, after much parental deliberation, I gather, we received a\nSuper Nintendo for the Christmas of what I think was one of my middle\nschool years.Unfortunately, Mom\u2019s yielding on this issue didn\u2019t mean her suspicion\nof video games had abated, and I can still remember pretty vividly the\ndiscomfort I felt whenever I was playing and she was around, radiating a\nhigh-intensity aura of disapproval that made me feel somewhat ashamed to\nbe wasting my time like this. It wasn\u2019t enough to stop me, of course.\nBut it was always there.I don\u2019t mean to make my mom out to be some kind of harridan, much as\nit might seem so in the context of today."
  }
]