[
  {
    "title": "Lessons learned shipping 500 units of my first hardware product (simonberens.com)",
    "points": 306,
    "submitter": "sberens",
    "submit_time": "2026-02-01T20:01:00 1769976060",
    "num_comments": 160,
    "comments_url": "https://news.ycombinator.com/item?id=46848876",
    "comments": [
      "> As someone who generally stays out of politics, I didn\u2019t know much about the incoming administration\u2019s stance towards tariffs, though I don\u2019t think anyone could have predicted such drastic hikes.I have an appreciation for very bright lamps, and the project is neat, but that stuck out to me.I'm always fascinated by people who both feel comfortable ignoring maybe the single most impactful society-determining apparatus but will also say \"no one could have seen that coming\", where that is whatever they were unaware of because they chose to check out. I find the stance so fascinating because for myself, it would be impossible to not try and understand why the world is the way it is.Everything is downstream of politics whether people want to recognize that or not, and choosing to ignore it is, in fact, a political choice.reply",
      "In Athens, an \"idiotes\" was a citizen who focused only on private matters rather than participating in the polis (city-state). Because civic participation was considered a duty, this term carried a negative connotation of being socially irresponsible or uninvolved.This term evolved into the modern \"idiot\" which we are familiar with.reply",
      "And as a fellow Greek man said, \"Just because you do not take an interest in politics, it does not mean politics won't take an interest in you\".reply",
      "You could equally say \"just because you take an interest in politics doesn't mean politics will take an interest in you\".reply",
      "Well wasnt that a good thing?After the extermination of Melos they could credibly say they were less responsible for the actions of the polis.And had a higher chance of deflecting the inevitable revenge on to the non idiotes Athenians.reply",
      "If one civilization is taking revenge on another I don\u2019t think they would show that much nuance.For one thing, wouldn\u2019t everyone claim they were against their old polis? How would the invaders have any idea who was an idiote?I just don\u2019t believe it\u2019s at all easy to avoid the fate of your nation , and I especially doubt that the politically ignorant have a better chance of avoiding that fate than the well informed.reply",
      "I did say higher chance, not guaranteed to avoid it.The counter extermination was only 5% of Athens total population, or so historians say, so it seems like a lot of nuance was shown.reply",
      "> The counter extermination was only 5% of Athens total population, or so historians say, so it seems like a lot of nuance was shown.That fact alone doesn't demonstrate nuance. It's possible that 5% of the population was innocent and treated as scapegoats, or chosen randomly, or that anyone high profile regardless of guilt was chosen to die.Unless there's data on who was actually innocent or guilty, the mere fact that extermination was selective doesn't mean it was in any way accurate.",
      "Funny seeing people pushing for other people becoming more active in politics with the assumption that \u201cbeing more involved\u201d means with their political fights, then get worried when the other side grows or intensifies.reply",
      "I find the \"no one could have seen it coming\" crowd extremely tiring, they usually always say that about something anyone who paid a tiny bit of attention could see coming.It's genuinely baffling to me why business owners pay so little attention to the politics that will directly impact their business.The entire tariffs thing was incredible obvious to me (I am Australian) and I only check in on US politics for 10 min a couple of times a month, any less and it would be zero.reply"
    ],
    "link": "https://www.simonberens.com/p/lessons-learned-shipping-500-units",
    "first_paragraph": ""
  },
  {
    "title": "Data centers in space makes no sense (civai.org)",
    "points": 214,
    "submitter": "ajyoon",
    "submit_time": "2026-02-03T19:37:30 1770147450",
    "num_comments": 338,
    "comments_url": "https://news.ycombinator.com/item?id=46876105",
    "comments": [
      "I would not assume cooling has been worked out.Space is a vacuum.  i.e. The lack-of-a-thing that makes a thermos great at keeping your drink hot.  A satellite is, if nothing else, a fantastic thermos.  A data center in space would necessarily rely completely on cooling by radiation, unlike a terrestrial data center that can make use of convection and conduction. You can't just pipe heat out into the atmosphere or build a heat exchanger. You can't exchange heat with vacuum.  You can only radiate heat into it.Heat is going to limit the compute that can be done in a satellite data centre and radiative cooling solutions are going to massively increase weight. It makes far more sense to build data centers in the arctic.Musk is up to something here.  This could be another hyperloop (i.e. A distracting promise meant to sabotage competition).  It could be a legal dodge.  It could be a power grab.  What it will not be is a useful source of computing power.  Anyone who takes this venture seriously is probably going to be burned.reply",
      "It's exiting the 5th best social network and the 10th (or worse) best AI company and selling them to a decent company.It probably increases Elon's share of the combined entity.It delivers on a promise to investors that he will make money for them, even as the underlying businesses are lousy.reply",
      "I'm confused about the level of conversation here. Can we actually run the math on heat dissipation and feasibility?A Starlink satellite uses about 5K Watts of solar power. It needs to dissipate around that amount (+ the sun power on it) just to operate. There are around 10K starlink satellites already in orbit, which means that the Starlink constellation is already effectively equivalent to a 50 Mega-watt (in a rough, back of the envelope feasibility way).Isn't 50MW already by itself equivalent to the energy consumption of a typical hyperscaler cloud?Why is starlink possible and other computations are not? Starlink is also already financially viable. Wouldn't it also become significantly cheaper as we improve our orbital launch vehicles?reply",
      "It's like this. Everything about operating a datacenter in space is more difficult than it is to operate one on earth.1. The capital costs are higher, you have to expend tons of energy to put it into orbit2. The maintenance costs are higher because the lifetime of satellites is pretty low3. Refurbishment is next to impossible4. Networking is harder, either you are ok with a relatively small datacenter or you have to deal with radio or laser links between satellitesFor starlink this isn't as important. Starlink provides something that can't really be provided any other way, but even so just the US uses 176 terawatt-hours of power for data centers so starlink is 1/400th of that assuming your estimate is accurate (and I'm not sure it is, does it account for the night cycle?)reply",
      "> Everything about operating a datacenter in space is more difficult than it is to operate one on earthMinus one big one: permitting. Every datacentre I know going up right now is spending 90% of their bullshit budget on battlig state and local governments.reply",
      "But since building a datacenter almost anywhere on the planet is more convenient than outer space, surely you can find some suitable location/government. Or put it on a boat, which is still 100 times more sensible than outer space.reply",
      "This is a huge one. What Musk is looking for is freedom from land acquisition. Everything else is an engineering and physics problem that he will somehow solve. The land acquisition problem is out of his hands and he doesn't want to deal with politicians. He learned from building out the Memphis DC.reply",
      "What counts towards a bullshit budget? Permitting is a drop in the bucket compared to construction costs.",
      "> The maintenance costs are higher because the lifetime of satellites is pretty lowPresumably they're planning on doing in-orbit propellant transfer to reboost the satellites so that they don't have to let their GPUs crash into the ocean...reply",
      "> Presumably they're planning on doing in-orbit propellant transfer to reboost the satellites so that they don't have to let their GPUs crash into the oceanHell, you're going to lose some fraction of chips to entropy every year. What if you could process those into reaction mass?reply"
    ],
    "link": "https://civai.org/blog/space-data-centers",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Craftplan \u2013 I built my wife a production management tool for her bakery (github.com/puemos)",
    "points": 82,
    "submitter": "deofoo",
    "submit_time": "2026-02-01T17:25:39 1769966739",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=46847690",
    "comments": [
      "Love to the point of invention! This looks and feels great.I'm an Elixir newbie and wondering if I should start with learning Ash or stick with Liveview until I know more. Any thoughts on what Ash solved for you over Phoenix Liveview?reply",
      "Is the logic behind \"Usage Forecast\" and \"Reorder Planner\" hard-coded somewhere? I'm not seeing any configuration for that, so I had to ask the question.reply",
      "OK HN, time for us to build a full open source general purpose ERP in Elixir based on Ash XDreply",
      "This is definitely a nit but is there any reason you need 2 decimal places accuracy for percent complete?reply",
      "I think this is a very nicely thought out approach. I particularly like it doing allergen tracking. Obviously you're at the mercy of supplier/supply-chain integrity but if you do e.g. wind up with ground cumin contaminated with god knows what, this is what will get you where you need to be.reply",
      "I think I have needed this for 3d printing for some timereply",
      "Oh man. My wife's biscotti business will benefit from this. Nice work!reply",
      "Nice! 5 bucks says you can swap this in for your average software kanban and it does a better job.reply",
      "Looks well thought out. We wrestle with website, real ERP and building Notion connectors for production orders in make to order scenarios so there\u2019s definitely a pain point.reply",
      "As someone who struggled with ERPs, this is super-nice and clean!reply"
    ],
    "link": "https://github.com/puemos/craftplan",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Self-hosted software for managing artisanal D2C micro-businesses\n      \n\nOpen-source ERP for small-scale artisanal manufacturers and craft businesses\n\n\n\nEmail: test@test.com\nPassword: Aa123123123123Craftplan brings all essential business tools into one platform: catalog management, inventory control, order processing, production planning, purchasing, and CRM, so you can get off the ground quickly without paying for multiple separate platforms.Catalog & BOMOrders & InvoicesProductionInventoryPurchasingCRMImport / ExportEmailCalendar FeedAPIAccess ControlElixir \u00b7 Ash Framework \u00b7 Phoenix LiveView \u00b7 PostgreSQL \u00b7 Tailwind CSSDeploy Craftplan on your own server. No need to clone the repo:This starts Craftplan, PostgreSQL, and MinIO with migrations running automatically.See the self-hosting guide for single-container mode, Railway deployme"
  },
  {
    "title": "Deno Sandbox (deno.com)",
    "points": 329,
    "submitter": "johnspurlock",
    "submit_time": "2026-02-03T17:33:20 1770140000",
    "num_comments": 114,
    "comments_url": "https://news.ycombinator.com/item?id=46874097",
    "comments": [
      "Note that you don't need to use Deno or JavaScript at all to use this product. Here's their Python client SDK: https://pypi.org/project/deno-sandbox/  from deno_sandbox import DenoDeploy\n  \n  sdk = DenoDeploy()\n  \n  with sdk.sandbox.create() as sb:\n      # Run a shell command\n      process = sb.spawn(\"echo\", args=[\"Hello from the sandbox!\"])\n      process.wait()\n  \n      # Write and read files\n      sb.fs.write_text_file(\"/tmp/example.txt\", \"Hello, World!\")\n      content = sb.fs.read_text_file(\"/tmp/example.txt\")\n      print(content)\n\nLooks like the API protocol itself uses websockets: https://tools.simonwillison.net/zip-wheel-explorer?package=d...reply",
      "Because the sandbox is on their cloud, not on your local machine, which wasn't obvious to me.reply",
      "> In Deno Sandbox, secrets never enter the environment. Code sees only a placeholder> The real key materializes only when the sandbox makes an outbound request to an approved host. If prompt-injected code tries to exfiltrate that placeholder to evil.com? Useless.That seems clever.reply",
      "Yes... but...Presumably the proxy replaces any occurrence of the placeholder with the real key, without knowing anything about the context in which the key is used, right? Because if it knew that the key was to be used for e.g. HTTP basic auth, it could just be added by the proxy without using a placeholder.So all the attacker would have to do then is find and endpoint (on one of the approved hosts, granted) that echoes back the value, e.g. \"What is your name?\" -> \"Hello $name!\", right?But probably the proxy replaces the real key when it comes back in the other direction, so the attacker would have to find an endpoint that does some kind of reversible transformation on the value in the response to disguise it.It seems safer and simpler to, as others have mentioned, have a proxy that knows more about the context add the secrets to the requests. But maybe I've misunderstood their placeholder solution or maybe it's more clever than I'm giving it credit for.reply",
      "Where would this happen? I have never seen an API reflect a secret back but I guess it's possible? perhaps some sort of token creation endpoint?reply",
      "How does the API know that it's a secret, though? That's what's not clear to me from the blog post. Can I e.g. create a customer named PLACEHOLDER and get a customer actually named SECRET?reply",
      "Say, an endpoint tries to be helpful and responds with \u201cno such user: foo\u201d instead of \u201cno such user\u201d. Or, as a sibling comment suggests, any create-with-properties or set-property endpoint paired with a get-propety one also means game over.Relatedly, a common exploitation target for black-hat SEO and even XSS is search pages that echo back the user\u2019s search request.reply",
      "It depends on where you allow the substitution to occur in the request. It's basically \"the big bug class\" you have to watch out for in this design.reply",
      "HTTP Header Injection or HTTP Response Splitting is a thing.reply",
      "Could the proxy place further restrictions like only replacing the placeholder with the real API key in approved HTTP headers? Then an API server is much less likely to reflect it back.reply"
    ],
    "link": "https://deno.com/blog/introducing-deno-sandbox",
    "first_paragraph": "A new API for running untrusted code in secure Linux VMsFebruary 3, 2026Over the past year, we\u2019ve seen a shift in what Deno Deploy customers are\nbuilding: platforms where users generate code with LLMs, and that code runs\nimmediately without review. That code frequently calls LLMs itself, which means\nit needs API keys and network access.This isn\u2019t the traditional \u201crun untrusted plugins\u201d problem. It\u2019s deeper:\nLLM-generated code, calling external APIs with real credentials, without human\nreview. Sandboxing the compute isn\u2019t enough. You need to control network egress\nand protect secrets from exfiltration.Deno Sandbox provides both. And when the code is ready, you can deploy it\ndirectly to Deno Deploy without rebuilding.You don\u2019t want to run untrusted code (generated by your LLMs, your users LLMs,\nor even hand written by users) directly on your server. It will compromise your\nsystem, steal your API keys, and call out to evil.com. You need isolation.Deno Sandbox gives you lightweight Linux m"
  },
  {
    "title": "Why poor countries stopped catching up (davidoks.blog)",
    "points": 15,
    "submitter": "j-bos",
    "submit_time": "2026-02-04T01:22:49 1770168169",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=46880111",
    "comments": [
      "The entire essay obsesses over GDP convergence while ignoring that GDP (especially in the West) increasingly measures asset shuffling, imputed rents, and healthcare billing rather than anything humans actually experience.  (Healthcare, finance, real estate, and legal services combined are ~40% of US GDP!)So we've got 3000 words eulogizing a metric that tells you more about financialization than flourishing.  Look at life expectancy, infant mortality, or caloric intake and you'll find a more interesting story -- with some poor countries doing very well, and increasingly so, whereas others are on a fairly grim trajectory.reply",
      "GDP per capita is highly correlated with metrics like infant mortality.reply",
      "Though if you use us as a data point, seems like it goes down if you get too highreply",
      "Our current world system is based on exploitation by the powerful on the weak.  It has been this way since the dawn of time at many levels.  So now, powerful countries take resources from less powerful countries.  There are many ways this can be done, so here we are.reply"
    ],
    "link": "https://davidoks.blog/p/why-poor-countries-stopped-catching-690",
    "first_paragraph": ""
  },
  {
    "title": "Xcode 26.3 \u2013 Developers can leverage coding agents directly in Xcode (apple.com)",
    "points": 236,
    "submitter": "davidbarker",
    "submit_time": "2026-02-03T18:04:08 1770141848",
    "num_comments": 196,
    "comments_url": "https://news.ycombinator.com/item?id=46874619",
    "comments": [
      "More than writing code the IDE itself makes me anxious. Especially Xcode. Wish they make the IDE interface somewhat simpler by leveraging AI.reply",
      "OT: RantXcode being loaded on my computer causes something akin to a kernel panic.Not the fun kind where you get to read a backtrace and feel something. The existential kind.Every time it hijacks a .json or .xml file association, I experience a rage that hasn't been matched since the Emacs/vi wars ... and at least those were about editors that could open in under a geological epoch.I just want to look at a text file with pretty print.I do not need a 12GB IDE to render curly braces. cat has been doing this since 1971. Dennis Ritchie solved this.Why, Apple, in 40 years, could you not ship a lightweight dev-oriented text viewer? You had NeXTSTEP. You had the DNA of the most elegant Unix workstation ever built.And you gave us... this behemoth? An app whose launch time rivals a full Gentoo stage 1 install ( see: https://niden.net/post/gentoo-stage-1-installation )TextEdit is not the answer.I've used Xcode for native iOS development and honestly, once you get past the Stockholm Syndrome phase, it's just fine.- The interface is learnable.- The debugger mostly works.But the load times -- on every high-end MBP I've ever owned -- suggest that somewhere deep in the Xcode binary, there's a sleep(rand()) that someone committed in 2006 and no one has had the courage to git blame.FWIW, I fear someone here tells me I've been missing a launch flag. Alas, it's my truth and I can't hold it in anymore.reply",
      "I like how Xcode installs a bunch of gigantic, multi-gigabyte artifacts for like ios runtimes or whatever, fills up the hard drive, can't update because it's out of space, and then tells me I'm not allowed to delete them because of SIP.reply",
      "The dozens and dozens of simulators it installs without asking... which kill your system's audio capabilities for some reason: https://discussions.apple.com/thread/256140785But the best part is what it DOESN'T install when you think you've updated. You get on a plane and settle in for some work, only to be prompted to download and install a bunch of required crap you weren't told about. OH WELL, says Apple, your time is FREE!reply",
      "I'm confused, how have you not reassociated the files with the app of your choosing? Is Xcode somehow changing associations back? Does it do it only at updates?As far as Apple providing anything, why are they the expected ones providing it? There are a gigabazillionumpteen text editors that can reformat JSON. I have Xcode, and have associated JSON with a different editor. Not once has it ever changed on me.reply",
      "There is a way to do it, but it\u2019s not the most typical way MacOS users do it for everything else, which involves Right Click->Open With->Other->Always Open With. Xcode\u2019s file associations are super aggressive.I believe that \u201cGet Info\u201d->\u201dOpen With\u201d->\u201dChange All\u2026\u201d still works, and there are command line methods or third party tools.This has driven me to madness too.reply",
      "> Xcode\u2019s file associations are super aggressive.They are the same Info.plist format as every other MacOS application.reply",
      "Something is different. Right Click->Open With->Other->Always Open With 100% did not have any effect when I needed it in the past. Not sure what current behavior is. This was a huge thing for me.reply",
      "select in finder the file that currently opens with XCode, then press Cmd+i. It opens the information panel. There in the Open with section, you can chose the app and then also Change all to not use XCode.reply",
      "Not something I've ever experienced. Open As... Always works just fine.reply"
    ],
    "link": "https://www.apple.com/newsroom/2026/02/xcode-26-point-3-unlocks-the-power-of-agentic-coding/",
    "first_paragraph": "Text of this articleFebruary 3, 2026UPDATEXcode 26.3 unlocks the power of agentic codingDevelopers can leverage coding agents, including Anthropic\u2019s Claude Agent and OpenAI\u2019s Codex, directly in Xcode to tackle complex tasks autonomously, helping them develop apps faster than everXcode 26.3 introduces support for agentic coding, a new way in Xcode for developers to build apps using coding agents such as Anthropic\u2019s Claude Agent and OpenAI\u2019s Codex. With agentic coding, Xcode can work with greater autonomy toward a developer\u2019s goals \u2014 from breaking down tasks to making decisions based on the project architecture and using built-in tools.Expanding on the intelligence features introduced in Xcode 26, which brought a brand-new coding assistant for writing and editing in Swift, this release gives coding agents access to even more of Xcode\u2019s capabilities. Agents like Claude Agent and Codex can now collaborate throughout the entire development life cycle, giving developers the power to streamli"
  },
  {
    "title": "AliSQL: Alibaba's open-source MySQL with vector and DuckDB engines (github.com/alibaba)",
    "points": 144,
    "submitter": "baotiao",
    "submit_time": "2026-02-03T18:40:18 1770144018",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=46875228",
    "comments": [
      "Curious how it stacks up to pg_duckdb.\n(pg_duckdb seems pretty clean, due to Postres' powerful extension mechanisms)reply",
      "HTAP is here! It seems like these hybrid databases are slowly gaining adoption which is really cool to see.The most interesting part of this is the improvements to transaction handling that it seems they've made in https://github.com/alibaba/AliSQL/blob/master/wiki/duckdb/du... (its also a good high level breakdown of MySQL internals too). Ensuring that the sync between the primary tables and the analytical ones are fast and most importantly, transactional, is awesome to see.reply",
      "I don't think this is meaningfully HTAP, it's gluing together two completely different databases under a single interface. As far as I can tell, it doesn't provide transactional or consistency guarantees different than what you'd get with something like Materialize.This isn't new either, people have been building OLAP storage engines into MySQL/Postgres for years, e.g., pg_ducklake and timescale.reply",
      "Genuinely curious in what situation would you actually want transactional consistency in the same session as you are doing analytical or vector retrieval style use cases?I might make the argument that paying the tax of delivering what you're arguing for has so many significant downsides in the end you'd have something you wouldn't really want anywayreply",
      "having an embedded column database for analytics in your traditional db is a massive win for productivity + operations simplicity.at the moment I use PG + Tiger Data - couldn't find a mysql equivalentso this as one.reply",
      "Mariadb has a columnar engine already (though I did not use it myself) https://mariadb.com/docs/analytics/mariadb-columnstore/colum... and is mostly mysql compatible.For about a year releases include a vector storage type, so it will be interesting to see it compared in performance with what Alibaba did.Just wanted to plug that out. Given how often Postgres is plugged on HN, I think people ignore how versatile mariadb is.reply",
      "This ColumnStore is very simple and just do table scans sequentially on every query. It doesn't support indexes and unique constraints. It is almost an append-only serialization file format, but with some columnar concepts.reply",
      "Can tiger data be used just as a simple column store?All I want is effectively what clickhouse does in PG. I have a single table that I need fast counts on and clickhouse can do the counts fast but I have to go through the entire sync/replication to do that.A quick scan of TimeSeries always seemed like it was really only best setup for that and to use it another way would be a bit of a struggle.reply",
      "in a way -- materialized views --but Tiger Data is more optimized for TimeSeries data - https://www.tigerdata.com/docs/use-timescale/latest/hypercor...I do wish too there was an embedded click house like db in Postgresreply",
      "Clickhouse supports MySQL protocol natively, and can also wrap/import MySQL tables. Okay so you need two connections but it works pretty well.reply"
    ],
    "link": "https://github.com/alibaba/AliSQL",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        AliSQL is a MySQL branch originated from Alibaba Group. Fetch document from Release Notes at bottom.\n      AliSQL is Alibaba's MySQL branch, forked from official MySQL and used extensively in Alibaba Group's production environment. It includes various performance optimizations, stability improvements, and features tailored for large-scale applications.Quickly build your DuckDB node: How to set up a DuckDB nodeDuckDB Storage Engine:AliSQL integrates DuckDB as a native storage engine, allowing users to operate DuckDB with the same experience as MySQL. By leveraging AliSQL for rapid deployment of DuckDB service nodes, users can easily achieve lightweight analytical capabilities.Vector Storage:AliSQL natively supports enterprise-grade vector processing for up to 16,383 dimensions. By integrating a highly optimized HNSW algorithm for hig"
  },
  {
    "title": "Agent Skills (agentskills.io)",
    "points": 377,
    "submitter": "mooreds",
    "submit_time": "2026-02-03T14:09:54 1770127794",
    "num_comments": 213,
    "comments_url": "https://news.ycombinator.com/item?id=46871173",
    "comments": [
      "This stuff smells like maybe the bitter lesson isn't fully appreciated.You might as well just write instructions in English in any old format, as long as it's comprehensible. Exactly as you'd do for human readers! Nothing has really changed about what constitutes good documentation. (Edit to add: my parochialism is showing there, it doesn't have to be English)Is any of this standardization really needed? Who does it benefit, except the people who enjoy writing specs and establishing standards like this? If it really is a productivity win, it ought to be possible to run a comparison study and prove it. Even then, it might not be worthwhile in the longer run.reply",
      "I share your skepticism and think it's the classic pattern playing out, where people map practices of the previous paradigm to the new one and expect it to work.Aspects of it will be similar but it trends to disruption as it becomes clear the new paradigm just works differently (for both better and worse) and practices need to be rethought accordingly.I actually suspect the same is true of the entire 'agent' concept, in truth. It seems like a regression in mental model about what is really going on.We started out with what I think is a more correct one which is simply 'feed tasks to the singular amorphous engine'.I believe the thrust of agents is anthropomorphism: trying to map the way we think about AI doing tasks to existing structures we comprehend like 'manager' and 'team' and 'specialisation' etc.Not that it's not effective in cases, but just probably not the right way to think about what is going on, and probably overall counterproductive. Just a limiting abstraction.When I see for example large consultancies talking about things they are doing in terms of X thousands of agents, I really question what meaning that has in reality and if it's rather just a mechanism to make the idea fundamentally digestable and attractive to consulting service buyers. Billable hours to concrete entities etc.reply",
      "On the other hand, LLMs are trained on enormous collections of human-authored documents, many that look like \"how to\" documents. Perhaps the current generation of LLMs are naturally wired for skill-like human language instructions.reply",
      "Folks have run comparisons. From a huggingface employee:  codex + skills finetunes Qwen3-0.6B to +6 on humaneval and beats the base score on the first run.\n\n  I reran the experiment from this week, but used codex's new skills integration. Like claude code, codex consumes the full skill into context and doesn't start with failing runs. It's first run beats the base score, and on the second run it beats claude code.\n\nhttps://xcancel.com/ben_burtenshaw/status/200023306951767675...That said, it's not a perfect comparison because of the Codex model mismatch between runs.The author seems to be doing a lot of work on skills evaluation.https://github.com/huggingface/upskillreply",
      "I can't quite tell what's being compared there -- just looks like several different LLMs?To be clear, I'm suggesting that any specific format for \"skills.md\" is a red herring, and all you need to do is provide the LLM with good clear documentation.A useful comparison would be between: a) make a carefully organised .skills/ folder, b) put the same info anywhere and just link to it from your top-level doc, c) just dump everything directly in the top-level doc.My guess is that it's probably a good idea to break stuff out into separate sections, to avoid polluting the context with stuff you don't need; but the specific way you do that very likely isn't important at all. So (a) and (b) would perform about the same.reply",
      "Your skepticism is valid. Vercel ran a study where they said that skills underperform putting a docs index in AGENTS.md[0].My guess is that the standardization is going to make its way into how the models are trained and Skills are eventually going to pull out ahead.0: https://vercel.com/blog/agents-md-outperforms-skills-in-our-...reply",
      "Agents add a docs index in context for skills, so this is an issue of finding that the current specific implementation of skills in Claude Code is suboptimal.Their reasoning about it is also flawed. E.g. \"No decision point. With AGENTS.md, there's no moment where the agent must decide \"should I look this up?\" The information is already present.\" - but this is exactly the case for skills too. The difference is just where in the context the information is, and how it is structured.Having looked at their article, ironically I think the reason it works is that they likely force more information into context by giving the agent less information to work with:Instead of having a description, which might convince the agent a given skill isn't relevant, their index is basically a list of vague filenames, forcing the agent to make a guess, and potentialy reading the wrong thing.This is basically exactly what skills were added to avoid. But it will break if the description isn't precise enough. And it's perfectly possible that current tooling isn't aggressive enough about pruning detail that might tempt the agent to ignore relevant files.reply",
      "The current tooling isn't aggressive enough in that it's not the first thing that the agent checks for when it is prompted, at least for claude code. Way more often than not, i remind the agent that the skill exists before it does anything. It's very rare that it will pick a skill unprompted. Which to me kind of defeats the purpose of skills, I mean if I have to tell the thing to go look somewhere, I'll just make any old document folder in any format and tell it to look there.reply",
      "> If you want a clean comparison, I\u2019d test three conditions under equal context budgets: (A) monolithic\n  > AGENTS.md, (B) README index that links to docs, (C) skills with progressive disclosure. Measure task\n  > success, latency, and doc\u2011fetch count across 10\u201320 repo tasks. My hunch: (B)\u2248(C) on quality, but (C)\n  > wins on token efficiency when the index is strong. Also, format alone isn\u2019t magic\u2014skills that reference\n  > real tools/assets via the backing MCP are qualitatively different from docs\u2011only skills, so I\u2019d\n  > separate those in the comparison. Have you seen any benchmarks that control for discovery overhead?reply",
      "I think the point is it smells like a hack, just like \"think extra hard and I'll tip you $200\" was a few years ago. It increases benchmarks a few points now but what's the point in standardizing all this if it'll be obsolete next year?reply"
    ],
    "link": "https://agentskills.io/home",
    "first_paragraph": "A simple, open format for giving agents new capabilities and expertise."
  },
  {
    "title": "FlashAttention-T: Towards Tensorized Attention (acm.org)",
    "points": 74,
    "submitter": "matt_d",
    "submit_time": "2026-02-03T21:15:48 1770153348",
    "num_comments": 39,
    "comments_url": "https://news.ycombinator.com/item?id=46877403",
    "comments": [
      "Oh wow there's still work being done on ampere?I was wondering - I've been thinking about switching to AI systems programming (I know, easy task), but from what I understand, industry cloud GPUs are the main winners, right? Nobody's going to pay me (assuming I even had the skills) to optimize for consumer GPUs?From what I understand, it's not just number + capacity + performance, it's literal core primitives. I don't think any of the \"Blackwell\" chips like the grace one or rtx 5090 have for example SM pairs in their ISA? And likewise similar fundamental differences between consumer and cloud hopper (where the majority of the perf is the cloud one's ISA?)So I guess I'm wondering if I should buy a GPU myself or should I just rent on the cloud if I wanted to start getting some experience in this field. How do you even get experience in this normally anyways, do you get into really good schools and into their AI labs which have a lot of funding?reply",
      "Why does publishing papers require the latest and greatest GPUs? My understanding is that the paper talks about very general principles.> So I guess I'm wondering if I should buy a GPU myself or should I just rent on the cloud if I wanted to start getting some experience in this field. How do you even get experience in this normally anyways, do you get into really good schools and into their AI labs which have a lot of funding?Unless you have money to throw around, you'd better start working on something, write some code and get them running on a leased GPU, before deciding on a long term planreply",
      "I do CUDA for a living (not inference) and for the life of me (and a couple of LLMs for that matter) I cannot figure out what you mean by \"SM pairs\".Do you mean the coupled dies on stuff like the B200? An NVidia chip die has many SMs if so.Do you mean TMEM MMA cooperative execution? I'm guessing that must be it given what the paper is about.reply",
      "https://hazyresearch.stanford.edu/blog/2025-03-15-tk-blackwe...cooperative execution yeahas you can tell I do not do CUDA for a living :Dreply",
      "I still have 2x NVLinked A6000 and they aren't that bad compared to a single RTX 6000 Pro.reply",
      "Look at am the email addresses. If you\u2019ll recall there\u2019s an embargo on China.reply",
      "yep, https://github.com/poad42/cuda-fp8-ampere recently another attempt at squeezing whatever's left from amperereply",
      "OT but instead of quadratic attention can we not have n^10 or something crazier? I feel like we are limiting the intelligence just to save cost. But I can imagine that there might be some questions that may be worth paying higher cost for.I feel like n^10 attention can capture patterns that lower complexity attention may not. So it seems arbitrary that we have n^2 attention.reply",
      "What you're missing is that there's no need to do extra work in the kernel smoothing step (what attention essentially is) because all the fancy transformation work is already happening in learning the kernel.The feedforward networks prior to the attention layer are effectively learning sophisticated kernels. If you're unfamiliar (or for those who are) a Kernel is just a generalization of the dot product which is the most fundamental way of defining \"similarity\" between two points.By learning a kernel the transformer is learning the best way to define what \"similar\" means for the task at hand and then we simply apply some basic smoothing over the data. This will handle all sort of interesting ways to compare points and that comparison will allow all points to provide a little bit of information.Anything you could hope to achieve by performing more comparisons would be better solved by a better similarity function.reply",
      "You can find papers discussing \"cubic\" attention, i.e. each token gets to interact with each pair of other tokens, but always in very theoretical settings with single-layer transformers on contrived synthetic tasks.Keep in mind that LLMs have many many layers, so they have plenty of opportunity to model higher-order interactions without needing to brute force every possible combination of 10 previous tokens, of which the vast majority will be useless. Empirically, even full \"quadratic\" attention is not always necessary, as evidenced by the existence of linear/sparse attention variants that perform almost as well.reply"
    ],
    "link": "https://dl.acm.org/doi/10.1145/3774934.3786425",
    "first_paragraph": ""
  },
  {
    "title": "Notepad++ supply chain attack breakdown (securelist.com)",
    "points": 172,
    "submitter": "natebc",
    "submit_time": "2026-02-03T22:35:27 1770158127",
    "num_comments": 80,
    "comments_url": "https://news.ycombinator.com/item?id=46878338",
    "comments": [
      "The WinGUp updater compromise is a textbook example of why update mechanisms are such high-value targets. Attackers get code execution on machines that specifically trust the update channel.What's concerning is the 6-month window. Supply chain attacks are difficult to detect because the malicious code runs with full user permissions from a \"trusted\" source. Most endpoint protection isn't designed to flag software from a legitimate publisher's update infrastructure.For organizations, this argues for staged rollouts and network monitoring for unexpected outbound connections from common applications. For individuals, package managers with cryptographic verification at least add another barrier - though obviously not bulletproof either.reply",
      "I am running a lot of tools inside sandbox now for exactly this reason. The damage is confined to the directory I'm running that tool in.There is no reason for a tool to implicitly access my mounted cloud drive directory and browser cookies data.reply",
      "MacOS has been getting a lot of flak recently for (correct) UI reasons, but I honestly feel like they're the closest to the money with granular app permissions.Linux people are very resistant to this, but the future is going to be sandboxed iOS style apps. Not because OS vendors want to control what apps do, but because users do. If the FOSS community continues to ignore proper security sandboxing and distribution of end user applications, then it will just end up entirely centralised in one of the big tech companies, as it already is on iOS and macOS by Apple.reply",
      "Yet we look at phones, and we see people accepting outrageous permissions for many apps: They might rely on snooping into you for ads, or anything else, and yet the apps sell, and have no problem staying in stores.So when it's all said and done, I do not expect practical levels of actual isolation to be that great.reply",
      "I think we could get a lot further if we implement proper capability based security. Meaning that the authority to perform actions follows the objects around. I think that is how we get powerful tools and freedom, but still address the security issues and actually achieve the principle of least privilege.For FreeBSD there is capsicum, but it seems a bit inflexible to me. Would love to see more experiments on Linux and the BSDs for this.reply",
      "Redox is also moving towards having capabilities mapped to fd's, somewhat like Capsicum. Their recent presentation at FOSDEM: https://fosdem.org/2026/schedule/event/KSK9RB-capability-bas...reply",
      "Seems like a bad time to bring this up when it wouldn't have helped with this attack at all.reply",
      "Eli5, what is that supposed to mean?reply",
      "The original model of computer security is \"anything running on the machine can do and touch anything it wants to\".A slightly more advanced model, which is the default for OSes today, is to have a notion of a \"user\", and then you grant certain permissions to a user. For example, for something like Unix, you have the read/write/execute permissions on files that differ for each user. The security mentioned above just involves defining more such permissions than were historically provided by Unix.But the holy grail of security models is called \"capability-based security\", which is above and beyond what any current popular OS provides. Rather than the current model which just involves talking about what a process can do (the verbs of the system), a capability involves taking about what a process can do an operation on (the nouns of the system). A \"capability\" is an unforgeable cryptographic token, managed by the OS itself (sort of like how a typical OS tracks file handles), which grants access to a certain object.Crucially, this then allows processes to delegate tasks to other processes in a secure way. Because tokens are cryptographically unforgeable, the only way that a process could have possibly gotten the permission to operate on a resource is if it were delegated that permission by some other process. And when delegating, processes can further lock down a capability, e.g. by turning it from read/write to read-only, or they can e.g. completely give up a capability and pass ownership to the other process, etc.https://en.wikipedia.org/wiki/Capability-based_securityreply",
      "It also has persistent permissions.Think about it from a real world perspective.I knock on your door. \nYou invite me to sit with you in your living room. \nI can't easily sneak into your bed room. Further, your temporary access ends as soon as you exit my house.The same should happen with apps.When I run 'notepad dir1/file1.txt', the package should not sneakily be able to access dir2. Further, as soon as I exit the process, the permission to access dir1 should end as well.reply"
    ],
    "link": "https://securelist.com/notepad-supply-chain-attack/118708/",
    "first_paragraph": "Solutions for:Learn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreLearn MoreThreatsThreatsCategoriesCategoriesOther sections\nGReAT research\n\nGReAT research\n03 Feb 2026\n  minute read\t\t\t\t\t\t\t\tTable of ContentsAuthorsOn February 2, 2026, the developers of Notepad++, a text editor popular among developers, published a statement claiming that the update infrastructure of Notepad++ has been compromised. According to the statement, this was due to a hosting provider level incident, which occurred from June to September 2025. However, attackers were able to retain access to internal services until December 2025.Having checked our telemetry related to this incident, we have been amazed to find out how different and unique were the execution chains used in this supply chain attack. We identif"
  },
  {
    "title": "Valve wants to hold [patent troll] Leigh Rothschild personally liable (reporterbyte.com)",
    "points": 27,
    "submitter": "like_any_other",
    "submit_time": "2026-02-03T23:40:45 1770162045",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=46879078",
    "comments": [
      "Wow tfa is barely edited llm slop. Good on Valve though.reply",
      "I am sorry that i didn't take you at your word. Please accept my formal apology.reply",
      "In general I think corporations enjoy too much protection from liability. Not just big ones either. Even small local ones. And there's the issue of qualified immunity shielding police and politicians alike from personal liability. Our system needs to change.reply",
      "Eh, no, sorry. While I agree superficially, patent trolls are jerks. Leigh's made the news a lot for being a skeez.reply",
      "I concur.  I was completely ignorant of the entire patent troll \"business model\" until dealing with some corp patent stuff in 2003.My immediate visceral reaction then is same as it is some 20 years later:  \"Fuck these assholes, they add zero value to society and shouldn't be allowed to exist.\"reply"
    ],
    "link": "https://www.reporterbyte.com/news/10087/piercing-the-veil-why-valve-wants-to-hold-leigh-rothschild-personally-liable/",
    "first_paragraph": "The 2016 settlement between Valve Corporation and Rothschild\u2019s Display Technologies LLC was meant to put an end to an expensive diversion. After making the payment and securing a broad, perpetual license, the business moved on. However, years later, what had appeared to be resolved reappeared, albeit in a different and more complicated form. By 2023, Valve was doing more than simply responding. This time, it was retaliating with a lawsuit of its own, not just to protect its rights but also to reveal what it called a pattern of strategic abuse concealed by the judicial system.Valve noticed something similar, disturbingly so, in the form of fresh threats and license requests from organizations such as Patent Asset Management (PAM). These were no arbitrary businesses enforcing patents. According to Valve, they were purposefully planned continuations of Rothschild\u2019s previous initiatives. Patent scope was no longer the only point of contention. It had to do with intention. Valve indicated a"
  },
  {
    "title": "221 Cannon is Not For Sale (fredbenenson.com)",
    "points": 178,
    "submitter": "mecredis",
    "submit_time": "2026-02-03T16:56:06 1770137766",
    "num_comments": 133,
    "comments_url": "https://news.ycombinator.com/item?id=46873574",
    "comments": [
      "I have had people show up at my house to ask if it was for rent, based on a fake post on Facebook using photos from Zillow from before my home was sold.My realtor helped me get the photos taken down, but the Facebook ads for it are up to this day. Facebook completely ignores any and all attempts by me to report this malfeasance -- even though these ads literally have my personal home address on them!It's a huge safety risk to me and not due to anything I did whatsoever; all I did was buy a house that was on the market and then move into it. It's a nightmare.reply",
      "I would contact Facebook legal directly with documents showing the problem. Legal\u2019s job is always to minimize liability for the company, and they have levers they can pull in any organization, no matter how \u201chyper scale\u201d they claim to be.Bonus points for figuring out the correct language to use to imply repercussions for failure to act without any actual threats. Patio11 has written about similarly worded letters with regards to debt collections and banking, and I know that there are all kinds of magic incantations in law for all kinds of transgretions.reply",
      "\"Patio11\" itself is a magic incantion for your friendly neighborhood LLM, along with \"dangerous professional\". You can use these to prompt for suitable language in the email, as well as other courses of action.reply",
      "This is good advice and probably an avenue I need to explore, thank you.reply",
      "Facebook admits around 10% of their ads are fraudulent.  I think it's much higher.The scam is even larger than you see and exploits missing children reports.  There are huge automated scam networks that post missing children reports then get people to share them.  Then once the post/ad gets traction they change it to a listing of a house that is auto pulled from public information.  They then use that to scam people.PleasantGreen has a series on it.\nhttps://www.youtube.com/watch?v=uud0wTAOxScreply",
      "What is the point of listing a house that isn\u2019t for sale, though?reply",
      "Probably collecting application fees from people interested in renting it.reply",
      "Since you've discovered law enforcement isn't interested in enforcing the law, you need to set up your own sting and get the scammer to show up where you can arrest them after they commit a crime in your presence.reply",
      "What about sinking 3 2x4s into the ground and nailing a 4x8 sheet of plywood with a tastefully painted sign indicating the property is not for sale?It won't stop everyone but any realtor doing due diligence will likely see it. If is lasts long enough, it will show up on Google street view as well.reply",
      "A motivated attacker need only don a green safety vest and hard hat, then roll up with a white pickup truck, place some orange safety cones and take down the sign with a chainsaw.reply"
    ],
    "link": "https://fredbenenson.com/blog/2026/02/03/221-cannon-is-not-for-sale/",
    "first_paragraph": ""
  },
  {
    "title": "Prek: A better, faster, drop-in pre-commit replacement, engineered in Rust (github.com/j178)",
    "points": 192,
    "submitter": "fortuitous-frog",
    "submit_time": "2026-02-03T16:29:34 1770136174",
    "num_comments": 97,
    "comments_url": "https://news.ycombinator.com/item?id=46873138",
    "comments": [
      "BTW. Pre-commit hooks are the wrong way to go about this stuff.I'm advocating for JJ to build a proper daemon that runs \"checks\" per change in the background. So you don't run pre-commit checks when committing. They just happen in the background, and when by the time you get to sharing your changes, you get all the things verified for you for each change/commit, effortlessly without you wasting time or needing to do anything special.I have something a bit like that implemented in SelfCI (a minimalistic local-first Unix-philosophy-abiding CI) https://app.radicle.xyz/nodes/radicle.dpc.pw/rad%3Az2tDzYbAX... and it replaced my use of pre-commit hooks entirely. And users already told me that it does feel like commit hooks done right.reply",
      "Just because the hooks have the label \"pre-commit\" doesn't mean you have to run them before committing :).I, too, want checks per change in jj -- but (in part because I need to work with people who are still using git) I need to still be able to use the same checks even if I'm not running them at the same point in the commit cycle.So I have an alias, `jj pre-commit`, that I run when I want to validate my commits.  And another, `jj pre-commit-branch`, that runs on a well-defined set of commits relative to @.  They do use `pre-commit` internally, so I'm staying compatible with git users' use of the `pre-commit` tool.What I can't yet do is run the checks in the background or store the check status in jj's data store.  I do store the tree-ish of passing checks though, so it's really quick to re-run.reply",
      "My agents gave to run pre-commit before calling a coding tag done.  In this case, it's just a robust set of checks.reply",
      "Yep, I think a watcher is better suited [0] to trigger on file changes.I personally can't stand my git commit command to be slow or to fail.[0]: such as https://github.com/watchexec/watchexecreply",
      "I prefer to configure my IDE to apply precisely the same linting and formatting rules as used for commits and in CI.reply",
      "To myself: sometimes I think the background process should be committing for me automatically each time a new working set exists, and I should only rebase and squash before pushing.That\u2019s reversing the flow of control, but might be workable!reply",
      "jj already pretty much does that with the oplog. A consistent way of making new snapshots in the background would be nice though. (Currently you have to run a jj command \u2014 any jj command \u2014 to capture the working directory.)reply",
      "You can configure watchman to do it. `fsmonitor.watchman.register-snapshot-trigger = true`I don't recommend it, though, at least not on large repositories. Too much opportunity to collide with command-line jj write operations.reply",
      "I don't think you have to, you can run the integrated watcher, no?reply",
      "That's a great idea, and I was just thinking about how it would pair with self hosted CI of some type.Basically what I would want is write a commit (because I want to commit early and often) then run the lint (and tests) in a sandboxed environment. if they pass, great.  if they fail and HERAD has moved ahead of the failing commit, create a \"FIXME\" branch off the failure.  back on main or whatever branch head was pointed at, if tests start passing, you probably never need to revisit the failure.I want to know about local test failures before I push to remote with full CI.automatic branching and workflow stuff is optional.  the core idea is great.reply"
    ],
    "link": "https://github.com/j178/prek",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n        \u26a1 Better `pre-commit`, re-engineered in Rust\n      \n\n\n\npre-commit is a framework to run hooks written in many languages, and it manages the\nlanguage toolchain and dependencies for running the hooks.prek is a reimagined version of pre-commit, built in Rust.\nIt is designed to be a faster, dependency-free and drop-in alternative for it,\nwhile also providing some additional long-requested features.NoteAlthough prek is pretty new, it\u2019s already powering real\u2011world projects like CPython, Apache Airflow, FastAPI, and more projects are picking it up\u2014see Who is using prek?. If you\u2019re looking for an alternative to pre-commit, please give it a try\u2014we\u2019d love your feedback!Please note that some languages are not yet supported for "
  },
  {
    "title": "New York\u2019s budget bill would require \u201cblocking technology\u201d on all 3D printers (adafruit.com)",
    "points": 222,
    "submitter": "ptorrone",
    "submit_time": "2026-02-03T15:51:42 1770133902",
    "num_comments": 272,
    "comments_url": "https://news.ycombinator.com/item?id=46872540",
    "comments": [
      "My main concern is, how long is it before you can't print a replacement part for something you bought because it looks too similar to an OEM part and the manufacturer doesn't think you should be able to do that so they throw a little money to the right politician.reply",
      "I remember ~10 or 15 years ago, I had concerns about drones becoming illegal due to FAA.I was assured by the internet, I was paranoid, blah blah safety...Then a few weeks ago something about Minnesota and ICE making drones illegal to fly or something...The weird part is that, in that 15 years, I've become more moderate and pro-democratic rule of law... but I was right about my previous concerns. Not that I believe in the Justice behind them anymore.reply",
      "I guess it was a predictable outreach from the Patriot act - the new justification is flying drones \"over a mission\" from the border people, and they claim a lot of territory for their missions, right?reply",
      "This is insanely stupid stuff. Even the UK with our weird panic over Incredibly Specific Knives hasn't tried to do this kind of technical restriction to prevent people printing guns. Why not? Because nobody is printing guns! It's an infeasible solution to a non-problem!Someone should dig into who this is coming from and why. The answers are usually either (a) they got paid to do it by a company selling the tech, which appears not to be the case here, or (b) they went insane on social media.(can't confirm this personally, but it seems from other comments that it's perfectly feasible to just drive out of New York State and buy a gun somewhere else in the gun-owning US? And this is quite likely where all the guns used in existing NY crime come from?)I would also note that the Shinzo Abe doohickey wasn't 3D-printed.reply",
      "People print guns and gun parts. More than you think. Now even more since metal printing is starting to become affordable. I print grip and grip attachments for my 9mms and my AR15, trigger guards, barrel clamps, etc. I also find it stupid since, as the article suggests, what kind of algorithm can you implement to do smart detection of something that could be potentially dangerous? Will it also detect negative space? I print inserts in elastic filament with my gun outlines instead of foam (or as foam templates) for my carrying cases. Will the \"algorithm\" prevent me to do that too? What about my plastic disc thrower toy gun, or my PKD Blaster prop? Both look like guns to me. What about a dumb AI algorithm that lacks common sense?Printing barrels and FCUs -- the fire control unit, which is the only thing tracked and serialized in a gun at least in the US -- is more difficult but not impossible. Actually, building a functional FCU that can strike a bullet primer, or a barrel that can be used once is not difficult at all and if you look around you can find videos of people that have tested that with a mixture of 3d printing and rudimentary metal working skills. The major issues on designing those parts are reliability and safety. In the Philippines there is a full bootleg gunsmith industry dedicated to build illegal guns that match commercial ones in those aspects too.Sadly, instead of having better laws we get fallacy rhetoric by people who probably have never touched, much less fired a gun in their lives.reply",
      "I don't get it - afaik you can get every single part of a gun except for the lower receiver/pistol frame without any restriction - as those parts are legally defined as the 'gun' - the rest are just replacement parts.Even for those, you can get 80% finished parts for those - just drill a few holes, and file off some tidbits, and you get an almost factory-spec gun.I'm no expert on US gun law, but afaik, some states even allow you to make your own guns without registration, as the law defines gun manufacturing as manufacturing with the intent of selling them.So there's plenty of options, many of them better than making a gun with a printer.But even all this is typically overkill, I dont think criminals go to these lengths to make their own guns, they just get them from somewhere.reply",
      "> Sadly, instead of having better laws we get fallacy rhetoric by people who probably have never touched, much less fired a gun in their lives.Why is this the litmus test for being qualified to write gun legislation? Do we also expect our lawmakers to have tried heroin or downloaded child porn so that they can regulate those activities?reply",
      "This is a bad example.  I've been notionally pro-ownership but also pro-regulation my whole life, and one of the major problems with gun legislation in the US is that it's incredibly poorly written and does not reflect the technical reality of guns.The government allows private ownership of automatic weapons, but hasn't issued any new tax stamps for 50 years.  You can convert any semiauto gun into a full-auto gun for a few cents of 3D printed parts (or a rubber band).  The hysteria over \"assault weapons\" basically outlawed guns that _looked_ scary, while not meaningfully making anyone safer.I think yes, it is reasonable for Congresspeople to fire a gun before they legislate on it, because otherwise they are incapable of writing good laws.Good gun regulation in the US would probably look like car insurance, where gun owners need to register and insure their weapons against the possibility of crimes being committed with them.  There are so many guns compared to the amount of gun crime that it would probably not end up terribly expensive, especially if you own a gun safe.reply",
      "You're welcome to come up with a better litmus test, but it's beyond clear that lawmakers writing gun control regulation have less than a wikipedia level understanding of the topic. See \"shoulder thing that goes up\", the weird obsession with the Thompson, the entire concept of an Assault Weapon, etc.reply",
      "Wikipedia has much better information about guns than most of the people talking about them in politics, generally speaking.It's not too surprising, considering the way the rules are written at the ATF. There's basically zero logical thought that goes into pistol vs rifle vs felony:https://www.reddit.com/r/Firearms/comments/a4gnr3/makes_perf...(Sorry for the reddit link, it's a common image but that was the first url I found from a quick search that had it up front and center).reply"
    ],
    "link": "https://blog.adafruit.com/2026/02/03/new-york-wants-to-ctrlaltdelete-your-3d-printer/",
    "first_paragraph": ""
  },
  {
    "title": "Qwen3-Coder-Next (qwen.ai)",
    "points": 577,
    "submitter": "danielhanchen",
    "submit_time": "2026-02-03T16:01:50 1770134510",
    "num_comments": 359,
    "comments_url": "https://news.ycombinator.com/item?id=46872706",
    "comments": [
      "This GGUF is 48.4GB - https://huggingface.co/Qwen/Qwen3-Coder-Next-GGUF/tree/main/... - which should be usable on higher end laptops.I still haven't experienced a local model that fits on my 64GB MacBook Pro and can run a coding agent like Codex CLI or Claude code well enough to be useful.Maybe this will be the one? This Unsloth guide from a sibling comment suggests it might be: https://unsloth.ai/docs/models/qwen3-coder-nextreply",
      "We need a new word, not \"local model\" but \"my own computers model\" CapEx basedThis distinction is important because some \"we support local model\" tools have things like ollama orchestration or use the llama.cpp libraries to connect to models on the same physical machine.That's not my definition of local. Mine is \"local network\". so call it the \"LAN model\" until we come up with something better. \"Self-host\" exists but this usually means more \"open-weights\" as opposed to clamping the performance of the model.It should be defined as ~sub-$10k, using Steve Jobs megapenny unit.Essentially classify things as how many megapennies of spend a machine is that won't OOM on it.That's what I mean when I say local: running inference for 'free' somewhere on hardware I control that's at most single digit thousands of dollars. And if I was feeling fancy, could potentially fine-tune on the days scale.A modern 5090 build-out with a threadripper, nvme, 256GB RAM, this will run you about 10k +/- 1k. The MLX route is about $6000 out the door after tax (m3-ultra 60 core with 256GB).Lastly it's not just \"number of parameters\". Not all 32B Q4_K_M models load at the same rate or use the same amount of memory. The internal architecture matters and the active parameter count + quantization is becoming a poorer approximation given the SOTA innovations.What might be needed is some standardized eval benchmark against standardized hardware classes with basic real world tasks like toolcalling, code generation, and document procesing. There's plenty of \"good enough\" models out there for a large category of every day tasks, now I want to find out what runs the bestTake a gen6 thinkpad P14s/macbook pro and a 5090/mac studio, run the benchmark and then we can say something like \"time-to-first-token/token-per-second/memory-used/total-time-of-test\" and rate this as independent from how accurate the model was.reply",
      "You can run plenty of models on a $10K machine or even a lot less than that, it all depends how much you want to wait for results.  Streaming weights from SSD storage using mmap() is already a reality when running the largest and sparsest models. You can save even more on memory by limiting KV caching at the cost of extra compute, and there may be ways to push RAM savings even higher simply by tweaking the extent to which model activations are recomputed as needed.reply",
      "Yeah there's a lot of people that advocate for really slow inference on cheap infra. That's something else that should be expressed in this fidelityBecause honestly I don't care about 0.2 tps for my use cases although I've spoken with many who are fine with numbers like that.At least the people I've talked to they talk about how if they have a very high confidence score that the model will succeed they don't mind the wait.Essentially a task failure is 1 in 10, I want to monitor and retry.If it's 1 in 1000, then I can walk away.The reality is most people don't have a bearing on what this order of magnitude actually is for a given task. So unless you have high confidence in your confidence score, slow is uselessBut sometimes you do...reply",
      "If you launch enough tasks in parallel you aren't going to care that 1 in 10 failed, as long as the other 9 are good.  Just rerun the failed job whenever you get around to it, the infra will still be getting plenty of utilization on the rest.reply",
      "For context on what cloud API costs look like when running coding agents:With Claude Sonnet at $3/$15 per 1M tokens, a typical agent loop with ~2K input tokens and ~500 output per call, 5 LLM calls per task, and 20% retry overhead (common with tool use): you're looking at roughly $0.05-0.10 per agent task.At 1K tasks/day that's ~$1.5K-3K/month in API spend.The retry overhead is where the real costs hide. Most cost comparisons assume perfect execution, but tool-calling agents fail parsing, need validation retries, etc. I've seen retry rates push effective costs 40-60% above baseline projections.Local models trading 50x slower inference for $0 marginal cost start looking very attractive for high-volume, latency-tolerant workloads.reply",
      "Might there be a way to leverage local models just to help minimize the retries -- doing the tool calling handling and giving the agent \"perfect execution\"?I'm a noob and am asking as wishful thinking.reply",
      "At this point isn\u2019t the marginal cost based on power consumption? At 30c/kWh and with a beefy desktop pc pulling up to half a kW, that\u2019s 15c/hr. For true zero marginal cost, maybe get solar panels. :Preply",
      "This is an interesting question actually!Marginal cost includes energy usage but also I burned out a MacBook GPU with vanity-eth last year so wear-and-tear is also a cost.reply",
      "I don't even need \"open weights\" to run on hardware I own.I am fine renting an H100 (or whatever), as long as I theoretically have access to and own everything running.I do not want my career to become dependent upon Anthropic.Honestly, the best thing for \"open\" might be for us to build open pipes and services and models where we can rent cloud. Large models will outpace small models: LLMs, video models, \"world\" models, etc.I'd even be fine time-sharing a running instance of a large model in a large cloud. As long as all the constituent pieces are open where I could (in theory) distill it, run it myself, spin up my own copy, etc.I do not deny that big models are superior. But I worry about the power the large hyperscalers are getting while we focus on small \"open\" models that really can't match the big ones.We should focus on competing with large models, not artisanal homebrew stuff that is irrelevant.reply"
    ],
    "link": "https://qwen.ai/blog?id=qwen3-coder-next",
    "first_paragraph": ""
  },
  {
    "title": "1,400-year-old tomb featuring giant owl sculpture discovered in Mexico (cnn.com)",
    "points": 53,
    "submitter": "breve",
    "submit_time": "2026-01-30T10:26:54 1769768814",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=46822717",
    "comments": [
      "Primary release from  Mexico\u2019s National Institute of Anthropology and History:* https://www.inah.gob.mx/boletines/el-gobierno-de-mexico-anun...Better than CNN english language reporting based on primary release:* https://www.labrujulaverde.com/en/2026/01/an-intact-1400-yea...Lack of location details and surrounds is deliberate in this type of work given the activities of looky lous and treasure hunter types, however there has been many years of prior work grinding through funery sites, burial \"high rises\", and cities of the dead:  The significance of the discovery is further consolidated through comparisons with other high-status Zapotec funerary contexts in the region, such as those at Monte Alb\u00e1n or Lambityeco. Due to its construction quality, decorative richness, and symbolic complexity, the newly discovered tomb joins this elite group, confirming the existence of a powerful and widespread artistic and religious tradition in the Central Valleys during the Classic period. It is not an isolated find, but a key piece that completes a cultural mosaic, providing new data on the standardization of certain rituals and the diversity of iconographic expressions of power in death.reply",
      "I wish this article shared more about how this tomb was discovered. Was it buried under mountain of dirt? Under a jungle canopy no one explored? Has it been there all along at an existing ruins site but was hidden in some way? Give us details man!reply",
      "The Zapotec civilization pre-dates the Aztecs and Maya and were the first to develop a writing system in Mexico.Benito Juarez, President of Mexico during their revolution, was Zapotec.The Zapotec people are still around today and a large number still speak their ancient language.  A large number moved to LA and another group in New Jersey, but they're all over the US.reply",
      "How many are \"pure Zapotec\" vs a blend of Spanish/Aztec/Mayan/Tlaxcalan/Cempoaloan/Texcocan? Is it a genetically identifiable trait or just cultural?reply",
      "I just hate CNN subscribe screen. It completely locks my  iPhone chrome browser. I have to kill the browser and reopen it to go back.reply"
    ],
    "link": "https://www.cnn.com/2026/01/29/science/zapotec-tomb-mexico-scli-intl",
    "first_paragraph": "\n            Archaeologists have discovered a 1,400-year-old Zapotec tomb in southern Mexico, adorned with complex carvings, which has been called \u201a\u00c4\u00fathe most significant archaeological discovery of the last decade.\u201a\u00c4\u00f9\n    \n            Located in San Pablo Huitzo, in the state of Oaxaca, the tomb was built by the Zapotec culture in around the year 600, according to a statement from Mexico\u201a\u00c4\u00f4s National Institute of Anthropology and History (INAH) last week.\n    \n            Among the many well-preserved details are a sculpture of an owl that sits above the entrance to the burial chamber.\n    \n            A sculpture of a man\u201a\u00c4\u00f4s head can be seen inside its beak, possibly representing the individual who was buried inside, according to the INAH.\n    \n            Hundreds of thousands of Zapotec speakers still live in Mexico to this day. For the Zapotec, owls signify both night and death.\n    \n            The site is also home to multicolored murals, featuring symbols associated with power"
  },
  {
    "title": "Y Combinator will let founders receive funds in stablecoins (fortune.com)",
    "points": 78,
    "submitter": "shscs911",
    "submit_time": "2026-02-03T18:28:48 1770143328",
    "num_comments": 93,
    "comments_url": "https://news.ycombinator.com/item?id=46875033",
    "comments": [
      "This is intersting.Occasionally in YC founder circles a new founder will raise a bunch of money and then ask something like \"What's the best way to invest all the money our company just raised?\"The responses are always along the lines of \"Your startup is already risky. Don't innovate in areas of your business where the status quo is known to work. Innovate your product + technology, don't be innovative with your company's finances, HR, etc\"That advice always stuck with me. It just makes a lot of sense to do things in the most boring way possible, except where it matters (your competitive advantage <-- that's where you innovate, that's where you set yourself apart)Running a startup is distracting enough. Doing things non-standard just adds to the list of distractions that you don't need as a founder.reply",
      "Yeah but after a series of Big Prints we finally managed to make an inflation spike, a run on Silicon Valley Bank, the US President openly contemplating dollar devaluation, \"Sell America trade\" working for the first time in 50 years, the marginal buyer of treasuries eliminating the last dove on the path to war, and precious metals whipping around like meme stocks. \"Park the money in a USD money market at SVB\" used to be not just OK, but universally agreed to be obviously OK, which had value of its own. Now it's just OK. Probably. I hope.Will we see some pivots into bullshit crypto holding companies? Sure, but VC returns are notoriously lottery-ticket distributed and 0 is 0 however you get there. I'd hazard a bet that the number of otherwise-successful companies who die due to this policy rounds to 0, while the probability of an inflationary wrecking ball that wipes out an entire batch of otherwise promising startups in the absence of such a policy is... north of zero.To be clear, I don't think this is due to a special property of crypto, just the flexibility to get away from USD in case of emergency.EDIT: maybe 24/7 trading could be an argument. It would be a meme for the ages if a raft of startups survived because they were up hustling and grinding at 2AM when the boats hit the Taiwan Strait.reply",
      "You\u2019re describing an event that would wipe out the US economy and trying to protect against that with stable coins, or at least that\u2019s the impression I\u2019m getting.If the US falls apart, your startup will too. No matter how well preserved your cash reserves are.The US going to war or entering hyperinflation is probably at the bottom of most founders lists of existential worries. Not a risk to mitigate (it\u2019s a risk you need to accept since there\u2019s nothing you can do - worrying about it won\u2019t help)Also, worth mentioning that no one lost money with SVB\u2019s collapse. One might argue it was an incredibly smart decision for YC to recommend people bank at SVB since if SVB goes under, virtually all LP\u2019s and everyone in the VC community will go under too (too big to fail, so they won\u2019t, or if they do, everyone else fails too \u2014 kind of like AWS us-east-1)reply",
      "Nah, hedging war is a meme, but I labeled it as such.Startups that wanted to treasury in BTC or GLD, were told no, and were vindicated in hindsight are not a meme. Startups that were force-fed 10% inflation and a collapsing bank aren't a meme. That happened.You can complain that it's irrational to hedge against these things which have been happening an awful lot lately, but you aren't the one who gets to decide. If an enterprising alternative VC is peeling away good founders by being flexible on this point, YC's option is to compete or let the deals go.reply",
      "No one might have lost their money with the collapse of the banks but with the large amount of new money printed, the value of each dollar will continue to erode.Inflation and hyper-inflation can wipe out debts with future money that's cheaper more easily in some ways.  I forget where I had read or learned more about this in other countries that had experienced it.reply",
      "> the US President openly contemplating dollar devaluationWhy won\u2019t the fed raise rates?reply",
      "The fear is the loss of safe guards and independence of the Federal Reserve. Trump is actively trying to remove safe guards and independence that would allow the Federal Reserve to counteract anything like this. If for instance Trump wants to hold interest rates low regardless of what anyone is telling him, he wants that power[0][1].The upcoming Supreme Court case Trump v. Cook is about this very issue[2][0]: https://www.cnn.com/2026/01/29/economy/federal-reserve-indep...[1]: https://www.pbs.org/newshour/nation/why-the-federal-reserves...[2]: https://hls.harvard.edu/today/will-the-federal-reserve-remai...reply",
      "If they won\u2019t raise rates for fear of losing independence it\u2019s already over.reply",
      "Love your writing style!reply",
      "Then again, to play devils advocate, doing all the other stuff in a new way might also help your company break out of the cycle that typically impacts startups. It may be that the other things you do apart from your product are what make it successful.reply"
    ],
    "link": "https://fortune.com/2026/02/03/famed-startup-incubator-y-combinator-to-let-founders-receive-funds-in-stablecoins/",
    "first_paragraph": "In the latest sign of digital currencies going mainstream, Silicon Valley\u2019s most prominent startup incubator will allow its spring cohort of entrepreneurs to receive their funding in stablecoins. Y Combinator, whose alumni include the founders of Airbnb and DoorDash, announced on Tuesday that founders can opt to receive their customary allotment\u2014typically around $500,000\u2014in the Circle-issued USDC.\u00a0Startup founders who choose stablecoins can choose to receive the tokens on various blockchains such as Ethereum and Solana, Nemil Dalal, a visiting partner at Y Combinator who focuses on crypto, told Fortune. He added that Y Combinator may expand to other stablecoins depending on demand.\u201cStablecoins is one of the key pillars for us,\u201d Dalal said, referring to one of the areas where Y Combinator would like to see more startup ideas. \u201cSo we just want to live and breathe that as well.\u201dWhile many crypto venture capitalists have let the startups in their portfolio take funding from stablecoins for"
  },
  {
    "title": "Decompiling and rewriting a 2003 game from its binary in two weeks (banteg.xyz)",
    "points": 19,
    "submitter": "banteg",
    "submit_time": "2026-02-01T13:32:58 1769952778",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=46846101",
    "comments": [
      "I still find myself wasting a few hours per year on this game since it\u2019s on PS4/5https://store.playstation.com/en-us/product/UP4403-PPSA02752...reply",
      "10tons tends to make smaller scale games and you feel it sometimes but I've had a great time with quite a few of their other shooters too. You used to be able to get this bundle for cheap from fanatical sometimes, not sure if that is still the case. They are best known in the modern era for Tesla vs Lovecraft which doesn't show up in this bundle.\nhttps://store.steampowered.com/bundle/428/10tons_Shooters/There have been a few attempts to make open source versions of Crimsonland and I had a good time with Violetland\nhttps://github.com/ooxi/violetlandreply",
      "I really need to start familiarizing with these new tools, I'm only using LLMs in interactive, \u201cquestion and answer\u201d, mode and it feels like using a typewriter when everyone is switching to computer word processors.Thanks for sharing, it's a really interesting writeup and project!reply",
      "Using LLM's in an \"agentic loop\" is indeed a game changer. Give it a try in a sandbox.reply",
      "Crimsonland (2003) is a top-down shooter that shipped as a stripped DirectX 8 binary with zero symbols. I decompiled it with Ghidra, validated behavior with WinDbg and Frida, and rewrote it from scratch in Python/Raylib \u2014 46,800 lines matching the original behavior faithfully. The write-up covers static and runtime analysis, reverse engineering custom asset formats, and the full rewrite process. Code is on GitHub and it's playable now via uvx crimsonland@latestreply",
      "Very impressive, makes one wonder what do some companies have in private compared to public tools that we stitch together. E.g. you can combine LLMs with statical analysis/proving to get much better results.reply"
    ],
    "link": "https://banteg.xyz/posts/crimsonland/",
    "first_paragraph": ""
  },
  {
    "title": "France dumps Zoom and Teams as Europe seeks digital autonomy from the US (apnews.com)",
    "points": 787,
    "submitter": "AareyBaba",
    "submit_time": "2026-02-03T16:39:18 1770136758",
    "num_comments": 434,
    "comments_url": "https://news.ycombinator.com/item?id=46873294",
    "comments": [
      "Worth pointing out: France is not adopting existing open source software, they're building their own software and releasing it under the MIT licence. Most of it (or all of it?) is Django backend + React frontend (using a custom-built UI kit).Home page for the entire suite (in French) with some screenshots: https://lasuite.numerique.gouv.fr/Code bases are on GitHub and they use English there: https://github.com/suitenumerique/Dev handbook (in English): https://suitenumerique.gitbook.io/handbookNot French and I can't say I personally tried deploying any of them, but I've been admiring their efforts from afar for a while now.reply",
      "I work at Grist, the \"tableur collaboratif\" (collaborative spreadsheet) listed on the La Suite homepage. We're in the interesting situation of being both a NYC-based company, and open source software the French gov has adopted and is helping to develop. Grist is mostly a node backend. So it is a complicated story. The key is having code the gov can review and trust and run it on sovereign infrastructure.Grist https://www.getgrist.com/A write-up of how the French gov uses it https://interoperable-europe.ec.europa.eu/collection/open-so...reply",
      "Your position is fantastic because it immediately puts to death all of that nationalistic nonsense about the EU becoming \"anti-American\" by enforcing privacy laws on US Big Tech etc, when in fact they are just protecting their citizens' rights against unethical business models regardless of origin. I might be naive, but your company to me represents a win for free/open software and cross-country collaboration.That being said, I should ask: to what extent do you see being US-based an advantage or a problem in the current state of things? For example, in regards to exports controls, or any other such thing that may potentially limit your business scope depending on $current_admin.reply",
      "wow it reminds me of Microsoft Access, a great piece of software in terms of rapidly building an application!Does grist have forms?reply",
      "Form support is touted on the homepage: https://www.getgrist.com/forms/For what it's worth, which isn't much because this is probably outdated: I remember trying grist a few years ago and leaving mildly unimpressed with form support (I think because I was hoping to have image upload in the forms and that wasn't supported yet).reply",
      "Grist forms support uploads since 2025 https://github.com/gristlabs/grist-core/pull/1655Since it is relevant here: support for uploads was code written by a French contributor, and reviewed by a developer working for the French gov (ANCT/DINUM) and a developer working for Grist Labs. Grist Labs has since maintained and improved on it. The forms feature itself was inspired by an integration built by Camille Legeron at ANCT.reply",
      "I'm not an MS dev type, but I've often seen these forms questions. What made their forms so easy, or more in general what is so complicated about forms that this was even a tool so many liked?reply",
      "MS Access was on its way out by the time I started working in software, but the simplest explanation I can give about why the \"forms\" question is this, let's say you're a business person and...:  * You have a huge Excel document that's basically a DB. (What Access kinda was)\n  * You want users to interact with said data document, i.e add record, find/query record(s), edit records\n  * You add a \"form\" for users to do just that. You can also add a \"login\" form to give some users more permissions.\n\nIt's basically if you could turn a SQlite file into a low-coded desktop app.reply",
      "Access is an FE for db \u2014 JET Red, specifically.JET Blue aka ESE is currently used by products like Active Directory and Exchange.reply",
      "With Access, a business doing data entry could -- with a business user not a software engineer -- craft a Form and voila, easy onboarding to train new employees instead of filling out sheets of paper and filing them.reply"
    ],
    "link": "https://apnews.com/article/europe-digital-sovereignty-big-tech-9f5388b68a0648514cebc8d92f682060",
    "first_paragraph": ""
  },
  {
    "title": "Reference Target: having your encapsulation and eating it too (igalia.com)",
    "points": 4,
    "submitter": "todsacerdoti",
    "submit_time": "2026-01-31T05:32:12 1769837532",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://blogs.igalia.com/alice/reference-target-having-your-encapsulation-and-eating-it-too/",
    "first_paragraph": "Three years ago, I wrote a blog post about How Shadow DOM and accessibility are in conflict.I explained how the encapsulation provided by shadow roots is a double-edged sword, particularly when it comes to accessibility. Being able to programmatically express relationships from one element to another is critical for creating user experiences which don\u2019t rely on visual cues - but elements inside a shadow root aren\u2019t available to be referenced from elements in the light DOM. This encapsulation, however, is what allows component authors to create accessible components which can be safely reused in any context, without necessarily requiring any particular dependencies or extra build steps.In the year or so following, even more heroic attempts were made to square this circle, and finally one seems likely to stick: Reference Target. In this post I\u2019ll explain how this feature works, why I like it, and what the situation is right now with the spec and implementation (thanks in part to Igalia\u2019s"
  }
]