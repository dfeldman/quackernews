[
  {
    "title": "UTF-8 is a brilliant design (iamvishnu.com)",
    "points": 373,
    "submitter": "vishnuharidas",
    "submit_time": "2025-09-12T18:30:15 1757701815",
    "num_comments": 163,
    "comments_url": "https://news.ycombinator.com/item?id=45225098",
    "comments": [
      "Having the continuation bytes always start with the bits `10` also make it possible to seek to any random byte, and trivially know if you're at the beginning of a character or at a continuation byte like you mentioned, so you can easily find the beginning of the next or previous character.If the characters were instead encoded like EBML's variable size integers[1] (but inverting 1 and 0 to keep ASCII compatibility for the single-byte case), and you do a random seek, it wouldn't be as easy (or maybe not even possible) to know if you landed on the beginning of a character or in one of the `xxxx xxxx` bytes.[1]: https://www.rfc-editor.org/rfc/rfc8794#section-4.4reply",
      "Right. That's one of the great features of UTF-8. You can move forwards and backwards through a UTF-8 string without having to start from the beginning.Python has had troubles in this area. Because Python strings are indexable by character, CPython used wide characters. At one point you could pick 2-byte or 4-byte characters when building CPython. Then that switch was made automatic at run time. But it's still wide characters, not UTF-8. One emoji and your string size quadruples.I would have been tempted to use UTF-8 internally. Indices into a string would be an opaque index type which behaved like an integer to the extent that you could add or subtract small integers, and that would move you through the string. If you actually converted the opaque type to a real integer, or tried to subscript the string directly, an index to the string would be generated. \nThat's an unusual case. All the standard operations, including regular expressions, can work on a UTF-8 representation with opaque index objects.reply",
      "PyCompactUnicodeObject was introduced with Python 3.3, and uses UTF-8 internally.  It's used whenever both size and max code point are known, which is most cases where it comes from a literal or bytes.decode() call.  Cut memory usage in typical Django applications by 2/3 when it was implemented.https://peps.python.org/pep-0393/I would probably use UTF-8 and just give up on O(1) string indexing if I were implementing a new string type.  It's very rare to require arbitrary large-number indexing into strings.  Most use-cases involve chopping off a small prefix (eg. \"hex_digits[2:]\") or suffix (eg. \"filename[-3:]\"), and you can easily just linear search these with minimal CPU penalty.  Or they're part of library methods where you want to have your own custom traversals, eg. .find(substr) can just do Boyer-Moore over bytes, .split(delim) probably wants to do a first pass that identifies delimiter positions and then use that to allocate all the results at once.reply",
      "You usually want O(1) indexing when you're implementing views over a large string. For example, a string containing a possibly multi-megabyte text file and you want to avoid copying out of it, and work with slices where possible. Anything from editors to parsing.I agree though that usually you only need iteration, but string APIs need to change to return some kind of token that encapsulates both logical and physical index. And you probably want to be able to compute with those - subtract to get length and so on.reply",
      "Sure, but for something like that whatever constructs the view can use an opaque index type like Animats suggested, which under the hood is probably a byte index.  The slice itself is kinda the opaque index, and then it can just have privileged access to some kind of unsafe_byteIndex accessor.There are a variety of reasons why unsafe byte indexing is needed anyway (zero-copy?), it just shouldn\u2019t be the default tool that application programmers reach for.reply",
      "You don't particularly want indexing for that, but cursors. A byte offset (wrapped in an opaque type) is sufficient for that need.reply",
      "Indices into a Unicode string is a highly unusual operation that is rarely needed. A string is Unicode because it is provided by the user or a localized user-facing string. You don't generally need indices.Programmer strings (aka byte strings) do need indexing operations. But such strings usually do not need Unicode.reply",
      "They can happen to _be_ Unicode.  Composition operations (for fully terminated Unicode strings) should work, but require eventual normalization.That's the other part of the resume UTF8 strings mid way, even combining broken strings still results in all the good characters present.Substring operations are more dicey; those should be operating with known strings.  In pathological cases they might operate against portions of Unicode bits... but that's as silly as using raw pointers and directly mangling the bytes without any protection or design plans.reply",
      "This is Python; finding new ways to subscript into things directly is a graduate student\u2019s favorite pastime!In all seriousness I think that encoding-independent constant-time substring extraction has been meaningful in letting researchers outside the U.S. prototype, especially in NLP, without worrying about their abstractions around \u201ca 5 character subslice\u201d being more complicated than that. Memory is a tradeoff, but a reasonably predictable one.reply",
      "Your solution is basically what Swift does. Plus they do the same with extended grapheme clusters (what a human would consider distinct characters mostly), and that\u2019s the default character type instead of Unicode code point. Easily the best Unicode string support of any programming language.reply"
    ],
    "link": "https://iamvishnu.com/posts/utf8-is-brilliant-design",
    "first_paragraph": "The first time I learned about UTF-8 encoding, I was fascinated by how well-thought and brilliantly it was designed to represent millions of characters from different languages and scripts, and still be backward compatible with ASCII.Basically UTF-8 uses 32 bits and the old ASCII uses 7 bits, but UTF-8 is designed in such a way that:Designing a system that scales to millions of characters and still be compatible with the old systems that use just 128 characters is a brilliant design.Note: If you are already aware of the UTF-8 encoding, you can explore the UTF-8 Playground utility that I built to visualize UTF-8 encoding.UTF-8 is a variable-width character encoding designed to represent every character in the Unicode character set, encompassing characters from most of the world's writing systems.It encodes characters using one to four bytes. The first 128 characters (U+0000 to U+007F) are encoded with a single byte, ensuring backward compatibility with ASCII, and this is the reason why "
  },
  {
    "title": "QGIS is a free, open-source, cross platform geographical information system (github.com/qgis)",
    "points": 301,
    "submitter": "rcarmo",
    "submit_time": "2025-09-12T16:57:17 1757696237",
    "num_comments": 82,
    "comments_url": "https://news.ycombinator.com/item?id=45224156",
    "comments": [
      "If you are in an enterprise setting and you currently evaluate ArcGIS vs QGIS, pick QGIS and thank me later. ArcGIS Enterprise is a piece of software that feels straight out of the 90s and has no native linux binary (can be started with wine). It is expensive as hell and resource hungry.reply",
      "+100. There is very little QGIS cannot do as well or better than ArcGIS. For any shortcomings, there are generally other specialized tools that can fill the gaps. It's really just a training issue more than technical one at this point imo.reply",
      "YES. I made the switch 10 years ago and my professional life improved overnightreply",
      "My brother is a GIS expert and does this for a living. At his workplace (trans-european electrical project) they use ArcGIS and privately he uses QGIS. He said he'd pick QGIS over ArcGIS every single day.ArcGIS is very polished, but everything costs extra. QGIS has less polish but is supremely hackable and there are plugins for nearly everything.Having used QGIS as a non-expert to extract mountain heightmaps from a border region between two datasets from different national bodies and looking up some property borders I can really recommend it. Took me less than an afternoon to get startedreply",
      "I come from the ArcView 3 / ArcInfo days. I still maintain a non professional home license which is nice, however they killed off ArcMap for non-enterprise and I just cant for the life of me get into Arc Pro or QGis. Old dog, no new tricks for me I guess.reply",
      "Geopandas and QGIS are my go to. QGIS for basic work, automate with Geopandas.It makes the work a lot of fun!reply",
      "What about GRASS?https://grass.osgeo.orgreply",
      "Yes, that's one missing piece. Excellent software but there is a steep learning curve, and it has its own format that you need to convert back and forth from.reply",
      "Uh, that is demonstrably not true. ArcGIS Enterprise (Portal, hosting servers, datastore, geoevent) all also run on Linux.Now where ArcGIS enterprise succeeds is being in an actual enterprise (thousands of users), having groups collaborate, data control, and more. None of the enterprise-y bits exist.And QGis is more akin to ArcGIS Pro, not Enterprise.Now, yes, it is definitely resource hungry. And also, if you administer it, HA isn't really HA. Theres tons of footguns in how they implement HA that makes it a SPOF.Also, for relevancy, I was the one who worked with one of their engineers and showed that WebAdapters (iis reverse proxy for AGE) could be installed multiply on the same machine, using SNI. 11.2 was the first to include my contribution to that.Edit: gotta love the -1s. What do you all want? Screenshots of my account on my.esri.com?  Pictures of Portal and the Linux console they're running on? The fact its 80% Apache Tomcat and Java, with the rest Python3? Or how about the 300 ish npm modules, 80 of which on the last security scan I did showed compromise?Everything I said was completely true. This is what I'm paid to run. Can't say who, cause we can't edit posts after 1 or so hours.I would LOVE to push FLOSS everywhere. QGIS would mostly replace ArcGIS Pro, with exception of things like Experience Builder and other weird vertical tools. But yeah. I know this industry. Even met Jack a few times.reply",
      "> even met Jack a few timesThe Danger Man!Yes, I know his name is Jack Dangermond.reply"
    ],
    "link": "https://github.com/qgis/QGIS",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n        QGIS is a free, open source, cross platform (lin/win/mac) geographical information system (GIS)\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\n\n\n\n\nQGIS is a full-featured, user-friendly, free-and-open-source (FOSS) geographical information system (GIS) that runs on Unix platforms, Windows, and MacOS.Example: Temporal animationExample: 3D map viewExample: Map of Bogota, Colombia in the style of Starry Starry Night, by Andr\u00e9s Felipe Lancheros S\u00e1nchezFor more maps created with QGIS, visit the QGIS Map Showcase Flickr Group.Example: Travel isochronesExample: Model designerExample: Style managerExample: PluginsHeadless map server -- running on "
  },
  {
    "title": "I used standard Emacs extension-points to extend org-mode (edoput.it)",
    "points": 116,
    "submitter": "Karrot_Kream",
    "submit_time": "2025-09-12T20:53:33 1757710413",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=45226639",
    "comments": [
      "The model of computer use that appeals to me are the one that in theory have a simple system, but the real goal is for you to have a tool that fits your needs. So far it includes the BSDs (Unix), Emacs, and Smalltalk. I've not tried Plan 9 yet.The more I'm learning more about the above, the more I'm believing that most computer problems has been solved since a long time and the focus should be on improving and creating new tools, not reinventing them.PS: Spreadsheets are nice too, but they're still lacking the surrounding helpers that would make them great. There's VBA in excel, but I'm thinking about more like dynamic table that's linked to an endpoint or some commands (Unix's ps). I haven't explored tools like Airbase to see if they fit that vision.reply",
      "Having used emacs for many years, OP's description of \"doing things wrong\" is exactly the way I use emacs.I seldom care about the inner workings of emacs and will do the absolute minimum to get it to work the way I want and then move on. I'm reminded over and over again that Emacs patinas really nicely with poorly written elisp in an init file over time.Case in point, I found that org-export is super slow. After profiling it, I found the slow function, copied it, removed the slow part, and advice-add it right back in there[0]. Might this break some other deeply intertwined behavior someplace else? Probably. Does it matter if I'm the only one using it? Nope.[0]https://github.com/alexkehayias/emacs.d/blob/master/init.el#...reply",
      "Learning and using Emacs is possibly the activity with the highest ROI over time you can do if you work with text for a living. Maybe even if you don't.Every time you modify it, you are improving your workflow. Those changes compound over time so that the system is always familiar, which makes interacting with text, the filesystem, network, and anything else you can manipulate with Elisp, that much easier, faster, and more comfortable. What you end up with is a system that is unique to you. A system that does what you want the way you want it, and never changes unless you want it to. In a world where software constantly changes and breaks, where new editors appear and disappear, using your own version of Emacs is incredibly comforting. There are no surprises, no rugpulls, no radical UI redesigns, no sneaky telemetry or tracking, no ads, no nagware, and so on. Anything you don't like can be removed, changed, or improved.It's not perfect, of course. It's slow, alien in many ways, lags behind in features of modern editors, and has a brutally steep learning curve, especially if you're not familiar with Lisps. It may take you years to appreciate it, and a lifetime to understand it. But that's OK. You don't need to understand all of it. As long as you start the journey, you can learn on the way, and your experience will keep improving.reply",
      "I have been using Emacs for 35 years and I am still learning along the way. It has been the one constant across Solaris, Linux, Windows and macOS for all that time.reply"
    ],
    "link": "https://edoput.it/2025/04/16/emacs-paradigm-shift.html",
    "first_paragraph": "Apr 16, 2025Recently I read this beginners guide to extend Emacs.\nThe guide is perfect for starting out with elisp and it shows a lot of care in teaching how to interact with Emacs.To me, the most important bit though is this one, from the section aptly named Emacs Wants You to Extend It.I haven\u2019t written plugins for other editors extensively, but I can tell you this: emacs doesn\u2019t just make deep customization available, but it actively encourages you to make an absolute customization messes masterpieces. Core editor functions aren\u2019t just documented, but often include tidbits about \u201cyou probably want to see this other variable\u201d or \u201chere\u2019s how you should use this\u201d.Not only that, but emacs happily hands you functions shaped like nuclear warheads like advice-add (that let you override any function) that can absolutely obliterate your editor if you hold it the wrong way. Of course, this also grants you unlimited power.Remember that emacs is designed to be torn apart and rearranged.This is "
  },
  {
    "title": "FFglitch, FFmpeg fork for glitch arch (ffglitch.org)",
    "points": 61,
    "submitter": "captain_bender",
    "submit_time": "2025-09-12T21:54:34 1757714074",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=45227212",
    "comments": [
      "Glitch art, not glitch arch.  The main page https://ffglitch.org/ is a slightly better introduction to the project.reply",
      "Awesome! I remember seeing Datamosh 2 plugin for After Effects, but didn't know it used this open source project. Turns out there is a whole bunch of GUIs for ffglitch: https://ffglitch.org/frontends/reply",
      "Not a lot of info on the page about the process, etc, but this is also called \"datamoshing.\" If you're curious, there's a great talk from Demuxed '21 on some of the details: https://www.youtube.com/watch?v=Qtia43DGSrYreply",
      "This page doesn't explain what FFglitch does, or how it's different to ffmpeg.  For instance, what's Glitch? I'm guessing it's an architecture, but the post doesn't explain what it is or contextualize the term \"architecture.\"reply",
      "From what i understand \"glitch art\" is using compression artifacts and encoding errors as art.Presumably ffglitch is ffmpeg with code to fudge the file checksums so that encoding errors are allowed to accumulate instead of triggering an error.reply",
      "The clearly labeled \"What?\" button at the top of the page explains everything.reply",
      "The what button doesn't explain much.As far as I know, \"glitching\" is opening a jpeg file with a text editor then deleting random ranges of characters, saving it again and then letting image viewers try to open the file, resulting in artifacts being added to the image.This project seems to do the same for video files, but generating a valid video at the end.reply",
      "You know, i was all ready to be dismissive of this, using encoder errors for art sounds silly.But i watched the video and it really was cool and artistic.reply"
    ],
    "link": "https://ffglitch.org/gallery/",
    "first_paragraph": "There are some artists out there doing some amazing work using FFglitch.I put this page up so that I don\u2019t have to go hunting for examples every time I want to show someone what can be done with FFglitch.Thomas Collet has a lot of work using FFglitch on vimeo, instagram, and reddit.A bunch more from Thomas:Kaspar Ravel wrote a blog post\nabout a collaboration he did with Thomas Collet which resulted in this gem:\nHere\u2019s the blog post: https://www.kaspar.wtf/blog/encoding-the-game-of-life-in-datamoshAnd the post on reddit: https://www.reddit.com/r/brokengifs/comments/e25f6b/want_to_see_a_magic_trick/Sebastien Brias:A post shared by @sebr.iasA post shared by @sebr.iasA post shared by @sebr.iasglit_chbee (turn the volume up and enjoy the ride):nowahe:Ben Cooper made this clip by using mainly avidemux, tomato.py, and FFglitch.Jo Grys has posted some videos on Facebook:There are more if you search for #ffglitch on Facebook:And some more random clips I found spread around the interwebz:"
  },
  {
    "title": "Corporations are trying to hide job openings from US citizens (thehill.com)",
    "points": 356,
    "submitter": "b_mc2",
    "submit_time": "2025-09-12T16:13:49 1757693629",
    "num_comments": 258,
    "comments_url": "https://news.ycombinator.com/item?id=45223719",
    "comments": [
      "A lot of these problems could be solved if H1-B's were given out in order of salary (I think there's such a proposal going around recently). And by that I mean: something like a Dutch auction. Give H1-Bs to the top 85K paying jobs (maybe normalized to SoL in the region, I'm sure the BLS has some idea on how to do it).The lure of H1-Bs is the money savings, and the fact that if you're on an H1-B, you're practically an indentured servant (Yes, things have changed recently and it is easier on paper to switch jobs while on H1-B). It used to be that if you lost your job as an H1-B, you had 30 days to uproot your life and get out of the US otherwise you'd be in violation of immigration laws.reply",
      "It\u2019s interesting that the U.S. picked an employer-driven model, which effectively outsources immigration selection to firms. That\u2019s efficient for demand-matching, but it concentrates bargaining power in ways that a points-based model avoids.The practical effect of an H1-B is to act as a non-compete, punitive termination clause, and a time bounded employment contract. These are very expensive terms to ask for in conventional US employment contracts - most of them are now effectively banned for standard W-2 workers. Forcing top wage earners to compete with illegal employment terms does not seem reasonable.reply",
      "This conflates high education specialists with high earnings. It\u2019s probably not completely uncorrelated, but only giving H1-Bs to the highest paying reqs which need them starves all of the other reqs of any possible candidates.I understand that H1-Bs are currently likely to create an abusive relationship with the visa-ed employee, but just because you have identified a valid diagnosis doesn\u2019t mean your suggested prescription would be much better.reply",
      "That seems like a fair way for the free market to address things, no? If you need special carve outs, create a new type of Visa for those special cases.The immigrants are all going to be paying taxes on their earnings. If you can boost H1B salaries by an average of $20k/yr by doing a price auction, that brings govt revenue and maybe even gives opportunities to balance the budget by creating more H1B slots.reply",
      "Exactly this. Top 1% of artists earn about as much as the average software engineer. Ranking people purely based on salary is turning h1b into a visa for people in specific professions.reply",
      "Genuinely curious: why do we need H1B visas for artists? My understanding is that H1B visas are meant to cover highly-skilled work that can't be done by locals, and \"art\" doesn't seem like a field with a shortage of local candidates?reply",
      "this also holds true for chemical, biomedical researchers, mechanical engineers working in deep tech, software engineering is such an anomaly that it's hard to do income based lottery without overindexing on swe marketreply",
      "Does the US have such a shortage of artistic talent we have to hire abroad for it?reply",
      "Top 1% of artists have the O1 route, not the H1B route.Tying H1B to salary is imo a reasonable solution for most companies. Thing is, in that case, most companies would simply resort to bringing in more L1 employees.reply",
      "Can you expand how exactly this particular problem (advertising jobs for PERM to comply with the law yet making sure that no applications will be received) can be fixed with a different order of issuing H-1B visas?PERM has nothing to do with H-1B, it's a part of the employment-based immigration process. The reason companies do this shit is because they claim to the US that there are no willing and able citizens or permanent residents for a commodity job such as \"front end\" or \"project management\". I.e. committing fraud.reply"
    ],
    "link": "https://thehill.com/opinion/finance/5498346-corporate-america-has-been-trying-to-hide-job-openings-now-it-is-failing/",
    "first_paragraph": ""
  },
  {
    "title": "Many hard LeetCode problems are easy constraint problems (buttondown.com/hillelwayne)",
    "points": 406,
    "submitter": "mpweiher",
    "submit_time": "2025-09-12T14:44:05 1757688245",
    "num_comments": 350,
    "comments_url": "https://news.ycombinator.com/item?id=45222695",
    "comments": [
      "My biggest problem with leetcode type questions is that you can't ask clarifying questions.  My mind just doesn't work like most do, and leetcode to some extent seems to rely on people memorizing leetcode type answers.  On a few, there's enough context that I can relate real understanding of the problem to, such as the coin example in the article... for others I've seen there's not enough there for me to \"get\" the question/assignment.Because of this, I've just started rejecting outright leetcode/ai interview steps... I'll do homework, shared screen, 1:1, etc, but won't do the above.  I tend to fail them about half the time.  It only feels worse in instances, where I wouldn't even mind the studying on leetcode types sites if they actually had decent explainers for the questions and working answers when going through them.  I know this kind of defeats the challenge aspect, but learning is about 10x harder without it.It's not a matter of skill, it's just my ability to take in certain types of problems doesn't work well.  Without any chance of additional info/questions it's literally a setup to fail.edit: I'm mostly referring to the use of AI/Automated leetcode type questions as a pre-interview screening.  If you haven't seen this type of thing, good for you.  I've seen too much of it.  I'm fine with relatively hard questions in an actual interview with a real, live person you can talk to and ask clarifying questions.reply",
      "The LC interviews are like testing people how fast they can run 100m after practice, while the real job is a slow arduous never ending jog with multiple detours and stops along the way.But yeah that's the game you have to play now if you want the top $$$ at one of the SMEGMA companies.I wrote (for example) my 2D game engine from scratch (3rd party libs excluded)https://github.com/ensisoft/detonatorbut would not be able to pass a LC type interview that requires multiple LC hard solutions and a couple of backflips on top. But that's fine, I've accepted that.reply",
      "5 years ago you'd have a project like that, talk to someone at a company for like 30m-1hr about it, and then get an offer.reply",
      "Did you mean to type 25? 5 years ago LC challenge were as, if not more, prevalent than they are today. And a single interview for a job is not something I have seen ever after 15 years in the space (and a bunch of successful OSS projects I can showcase).I actually have the feeling it\u2019s not as hardcore as it used to be on average. E.g. OpenAI doesn\u2019t have a straight up LC interview even though they probably are the most sought after company. Google and MS and others still do it, but it feel like it has less weight in the final feedback than it did before. Most en-vogue startup have also ditched it for real world coding excercices.Probably due to the fact that LC has been thoroughly gamed and is even less a useful signal than it was before.Of course some still do, like Anthropic were you have to have a perfect score to 4 leetcode questions, automatically judged with no human contact, the worst kind of interview.reply",
      "There's an entire planet of jobs that have nothing to do with leetcode. I was talking about those, not FAANG stuff. Unfortunately I am not FAANG royalty.>Of course some still do, like Anthropic were you have to have a perfect score to 4 leetcode questions, automatically judged with no human contact, the worst kind of interview.Should be illegal honestly.reply",
      "It might be illegal; certainly if you can show that LC is biased against a protected class, then there would be grounds for a lawsuit.reply",
      "Only if there is enough evidence. Yes, I can say that the inability to account for things like the ADA in the US can place an employer in hot water, however, since LC doesn't make those decisions, they are immune. The accountability is placed upon the employer. Don't hate the players or the game. Maybe just figure out how to fix it without harming everyone, be popular enough to make said idea into law, and get into a position of power that allows you to do so. If that sounds hard, congrats, welcome to the reason why I never got into politics. Don't even get me started on all the people you will never realize you are hurting by fixing that one single problem.reply",
      "> Should be illegal honestly.I can't imagine this kind of entitlement. If you don't want to work for them, don't study leetcode. If you want to work for them (and get paid tons of money), study leetcode. This isn't a difficult aristotelian ethics/morals question.reply",
      "I meant no human-in-the-loop wrt hiring, which is what I thought you were getting at.reply",
      "It's the same exact thing - if some company makes you jump through hoops to get hired that you find distasteful just don't apply to company.reply"
    ],
    "link": "https://buttondown.com/hillelwayne/archive/many-hard-leetcode-problems-are-easy-constraint/",
    "first_paragraph": "In my first interview out of college I was asked the change counter problem:Given a set of coin denominations, find the minimum number of coins required to make change for a given number. IE for USA coinage and 37 cents, the minimum number is four (quarter, dime, 2 pennies).I implemented the simple greedy algorithm and immediately fell into the trap of the question: the greedy algorithm only works for \"well-behaved\" denominations. If the coin values were [10, 9, 1], then making 37 cents would take 10 coins in the greedy algorithm but only 4 coins optimally (10+9+9+9). The \"smart\" answer is to use a dynamic programming algorithm, which I didn't know how to do. So I failed the interview.But you only need dynamic programming if you're writing your own algorithm. It's really easy if you throw it into a constraint solver like MiniZinc and call it a day. You can try this online here. It'll give you a prompt to put in total and then give you successively-better solutions:Lots of similar inter"
  },
  {
    "title": "Tips for installing Windows 98 in QEMU/UTM (sporks.space)",
    "points": 27,
    "submitter": "Bogdanp",
    "submit_time": "2025-09-12T23:04:08 1757718248",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://sporks.space/2025/08/28/tips-for-installing-windows-98-in-qemu-utm/",
    "first_paragraph": "Windows 98 runs surprisingly well in QEMU via UTM SE, but it requires some care in setting it up. It\u2019s a great way to run old 90s Windows and DOS software on your iPad (and Mac too, though you have other options available to you, or an iPhone if you don\u2019t mind the HID difficulties).This post provides some suggestions and tips for installing Windows and selecting the best emulated devices. The guidance is intended for UTM users on Apple platforms, but should apply to anything QEMU based (or QEMU itself). The advice might also be useful for other operating systems in UTM/QEMU as well.When you install Windows 9x, PCI devices might be broken, and you\u2019ll see a Plug and Play BIOS device with problems in the device manager:This seems to be a bug in SeaBIOS or QEMU; I haven\u2019t yet seen an issue tracking this. Many guides (i.e. this one or this one) suggest changing the device and hoping devices re-enumerate correctly. However, there\u2019s a simpler method available when using Windows 98 SE. (If you"
  },
  {
    "title": "The treasury is expanding the Patriot Act to attack Bitcoin self custody (tftc.io)",
    "points": 597,
    "submitter": "bilsbie",
    "submit_time": "2025-09-12T12:10:29 1757679029",
    "num_comments": 450,
    "comments_url": "https://news.ycombinator.com/item?id=45221274",
    "comments": [
      "The Patriot Act itself was supposed to be temporary and \u201cnarrow.\u201d Two decades later it\u2019s the foundation for a financial dragnet that assumes privacy is the problem rather than a basic right.Just like encryption, once privacy becomes associated with criminality, you end up weakening security for law-abiding users and concentrating power in a few regulated intermediaries. That\u2019s not healthy for innovation, or democracy.reply",
      "> [The Patriot Act] contains many sunset provisions beginning December 31, 2005, approximately four years after its passage. Before the sunset date, an extension was passed for four years which kept most of the law intact. In May 2011, President Barack Obama signed the PATRIOT Sunset Extensions Act of 2011, which extended three provisions. These provisions were modified and extended until 2019 by the USA Freedom Act, passed in 2015. In 2020, efforts to extend the provisions were not passed by the House of Representatives, and as such, the law has expired.Source: https://en.wikipedia.org/wiki/Patriot_Actreply",
      "> In 2020, efforts to extend the provisions were not passed by the House of Representatives, and as such, the law has expired.The wording is confusing. Two provisions expired, not the entire Patriot Act.https://web.archive.org/web/20250306093943/https://www.nytim...reply",
      "The Wikipedia article is quite confusing, and seems to imply that those two provisions expired because they were the only two provisions not sunsetted already.  The table indicates that most of the law sunsetted on March of 2006:https://en.wikipedia.org/wiki/Patriot_Act#Section_expiration...But then they say \"The first act reauthorized all but two Title II provisions. Two sections were changed to sunset on December 31, 2009\"But the first act was passed in 2005, and so it's unclear whether it reauthorized provisions only until 2006 or a longer term.reply",
      "I looked into this a little more, and these were the final two provisions of the Patriot Act, so the did law expire.Unfortunately, that doesn't mean a whole lot, as many of the provisions live on in the USA Freedom Act.reply",
      "Was not aware of the USA Freedom Actdetails on it:Reauthorization of Other Patriot Act Provisions: The USA FREEDOM Act extended two other provisions from the Patriot Act that were set to expire: \n  \"Lone Wolf\" Provision: Allows for surveillance on individual terrorists who may not be directly linked to a foreign power. \n  \"Roving Wiretap\" Provision: Enables surveillance to follow a suspect even if they change their communication methods or devices.Everyone should be super clued in whenever the government chooses to classify something as 'terrorism' because of these provisions.There appeared to be a lot of \"good things\" associated with this Act but also... as things go.  Not great things such as above.reply",
      "The wording is confusing.Being confusing, I'm almost certain, was the entire point.reply",
      "\"USA Freedom Act\"We're truly living in Orwell's world.reply",
      "For nearly quarter of a century.reply",
      "Longer than that. I feel like people have completely forgotten things like Iran-Contra, or the Gulf of Tonkin.reply"
    ],
    "link": "https://www.tftc.io/treasury-iexpanding-patriot-act/",
    "first_paragraph": "We shouldn't have to cater to the lowest common denominator.We warned a couple of months ago when the Trump administration's \"Crypto Brief\" was released that there was some language in the brief that advised the government to expand the Patriot Act to account for digital assets. Well, it looks like FinCen and the Treasury have been working on guidelines and a rough outline is shared above courtesy of The Rage, and they are absolutely horrid.It seems that FinCen and the Treasury are preparing to outlaw the use of CoinJoin, atomic swaps, single address use, and transaction broadcast timing delays. All of which are common best use practices that I would recommend any bitcoiner leveraging self-custody practice. This is an all out attack on financial privacy within bitcoin. If enacted, any user who leverages these tools will be flagged as a suspicious, any attempts to send a UTXO that has touched any of these tools will be rejected by regulated services, and could potentially be sent to pri"
  },
  {
    "title": "EU court rules nuclear energy is clean energy (weplanet.org)",
    "points": 623,
    "submitter": "mpweiher",
    "submit_time": "2025-09-12T18:18:00 1757701080",
    "num_comments": 470,
    "comments_url": "https://news.ycombinator.com/item?id=45224967",
    "comments": [
      "We need to drive down the costs of implementing nuclear energy. Most of it are fake costs due to regulation. I understand that regulation is needed but we also need nuclear energy, we have to find a streamlined way to get more plants up and running as soon as possible. I think they should all be government projects so that private companies can't complain that they're losing money and keep have to ratchet up the prices, like PG&E in California. My rates have doubled in a few years to over $0.40/kWh and up over $0.50/kWh after I go up a tier depending on usage.reply",
      "> Most of it are fake costs due to regulation.It\u2019s really not,  nuclear inherently requires extreme costs to operate.  Compare costs  vs coal which isn\u2019t cost competitive these days. Nuclear inherently need a lot more effort refining fuel as you can\u2019t just dig a shovel full of ore and burn it.  Even after refining you can\u2019t just dump fuel in, you need fuel assemblies. Nuclear must have a more complicated boiler setup with an extra coolant loop.  You need shielding and equipment to move spent fuel and a spent fuel cooling pond. Insurance isn\u2019t cheap when mistakes can cost hundreds of billions. Decommissioning could be a little cheaper with laxer standards, but it\u2019s never going to be cheap. Etc etc.Worse, all those capital costs mean you\u2019re selling most of your output 24/7 at generally low wholesale spot prices unlike hydro, natural gas, or battery backed solar which can benefit from peak pricing.That\u2019s not regulations that\u2019s just inherent requirements for the underlying technology.  People talk about small modular reactors, but small modular reactors are only making heat they don\u2019t actually drive costs down meaningfully.  Similarly the vast majority of regulations come from lessons learned so yea they spend a lot of effort avoiding foreign materials falling into the spent fuel pool, but failing to do so can mean months of downtime and tens of millions in costs so there isn\u2019t some opportunity to save money by avoiding that regulation.reply",
      "> Nuclear inherently need a lot more effort refining fuel as you can\u2019t just dig a shovel full of ore and burn it. Even after refining you can\u2019t just dump fuel in, you need fuel assemblies.It's true that a pound of nuclear fuel costs more than a pound of coal. But it also has a million times more energy content, which is why fuel is only 15-20% of the operating costs compared to >60% for coal. And that's for legacy nuclear plants designed to use moderately high enrichment rates, not newer designs that can do without that.> Nuclear must have a more complicated boiler setup with an extra coolant loop.You're describing a heat exchanger and some pipes. If this is the thing that costs a billion dollars, you're making the argument that this is a regulatory cost problem.> You need shielding and equipment to move spent fuel and a spent fuel cooling pond.Shielding is concrete and lead and water. None of those are particularly expensive.Equipment to move things is something you need at refueling intervals, i.e. more than a year apart. If this is both expensive and rarely used then why does each plant need its own instead of being something that comes on the truck with the new fuel and then goes back to be used at the next plant?> Insurance isn\u2019t cheap when mistakes can cost hundreds of billions.This is the regulatory asymmetry again. When a hydroelectric dam messes up bad enough, the dam breaks and it can wipe out an entire city. When oil companies mess up, Deep Water Horizon and Exxon Valdez. When coal companies just operate in their ordinary manner as if this is fine, they leave behind a sea of environmental disaster sites that the government spends many billions of dollars in superfund money to clean up. That stuff costs as much in real life as nuclear disasters do in theory. And that's before we even consider climate change.But then one of them is required to carry that amount of insurance when the others aren't. It should either be both or neither, right?reply",
      "> which is why fuel is only 15-20% of the operating costs compared to >60% for coalNuclear has much higher operating costs than coal.  It\u2019s not 20% of 3 = 60% of 1, but it\u2019s unpleasantly close for anyone looking for cheap nuclear power.  Especially when you include interest + storage as nuclear reactors start with multiple years worth of fuel when built and can\u2019t quite hit zero at decommissioning so interest payments on fuel matter.> You're describing a heat exchanger and some pipes. If this is the thing that costs a billion dollars, you're making the argument that this is a regulatory cost problem.It\u2019s a lot more than that, and far from the only cost mentioned. It\u2019s pumps, control systems, safety systems, loss of thermal efficiency, slower startup times, loss of more energy on shutdown, etc.> Shielding is concrete and lead and water. None of those are particularly expensive.Highways don\u2019t use expensive materials yet they end up costing quite a lot to build. Scale matters.> Equipment to move things is something you need at refueling intervals, i.e. more than a year apart. If this is both expensive and rarely used then why does each plant need its own instead of being something that comes on the truck with the new fuel and then goes back to be used at the next plant?Contamination with newly spent nuclear fuel = not something you want to move on a highway.  It\u2019s also impractical for a bunch of other reasons.> But then one of them is required to carry that amount of insurance when the others aren't. It should either be both or neither, right?No nuclear power plants has ever actually been required to carry a policy with that kind of a payout.  Taxpayers are stuck with the bill, but that bill doesn\u2019t go away it\u2019s just an implied subsidy.However, the lesser risk of losing the reactor is still quite substantial. You could hypothetically spend 5 billion building a cheap power plant rather than 20+ billion seen in some boondoggles but then get stuck with cleanup costs after a week.reply",
      "The problem with nuclear mistakes is they aren't a few decades.  They can be measured in centuries.So yeah.  Regulation.Don't build a damn LWR on a fault line (Fukushima) \n3mile Island - don't have so many damn errors printing out that everything is ignore\nChernobyl - we all know I think. It's still being worked on to contain it fully. \nGoi\u00e2nia accident (brazil) - caesium-137 - Time magazine has identified the accident as one of the world's \"worst nuclear disasters\" and the International Atomic Energy Agency (IAEA) called it \"one of the world's worst radiological incidents\".  (and this was just a radiation source, not a nuclear plant)So yeah.  Oil has bad disasters.  Nuclear has EPIC disasters.I think what is missing in your argument is not that these pieces are difficult.  It's that combining all of them adds to a significant amount of complexity.It's not JUST a heat exchanger.  It's a heat exchanger that has to go through shielding.  And it has to operate at much higher pressures than another type of power production facility would use.  Which adds more complexity.  And even greater need of safety.I'm not arguing against Nuclear; I think it's incredibly worthwhile especially in the current age of AI eating up so much power in a constant use situation.  But I do think it needs to be extremely regulated due to the risks of things going south.reply",
      "I agree Chernobyl was an epic disaster, but Fukushima ? Last I heard the radiation level are basically normal even close to the reactor, and overall radiation wide there hasn't been much damage if at all.So it seems that fukushima is an example of something that should have been an EPIC accident, but actually was perfectly fine in the end. I may be wrong, but thats what I remembered from the wikipedia page.reply",
      "And then there's coal. The difference between nuclear and coal is that when nuclear has a horrible accident, it kills fewer people than coal kills as part of its normal expected operation.reply",
      "> Shielding is concrete and lead and water. None of those are particularly expensive.Well, anything is expensive in enough quantity. But there is a bit of a tell not covered where of regulatory problems because nuclear plant projects keep going way over budget. Even stupid planners can notice trends of that magnitude and account for them, there is something hitting plant builds that isn't a technical factor and it is driving up costs.reply",
      "It really is. Nuclear is 100-1000x safer than coal. By insisting on such an aggressive safety target, we force prices up and actually incur much higher levels of mortality - just delivered in the boring old ways of pollution and climate-driven harms.See https://ourworldindata.org/safest-sources-of-energy for detailed stats.I think we should target \u201crisk parity with Gas\u201d until climate change is under control.reply",
      "When the nuclear industry feels confident enough to not need its own special law to protect it from liability in case of accidents, I\u2019ll feel a little more confident in their safety rhetoric.https://en.wikipedia.org/wiki/Price%E2%80%93Anderson_Nuclear...reply"
    ],
    "link": "https://www.weplanet.org/post/eu-court-rules-nuclear-energy-is-clean-energy",
    "first_paragraph": "HomeVisionWrittenCampaignsWho we areContact usPODCASTMoreWhen I launched Dear Greenpeace with my fellow youth climate activists alongside WePlanet two years ago, I had no idea just how quickly the anti-nuclear dominoes would fall across Europe. In 2023, and what seems like a lifetime ago, Austria launched their legal action against the European Commission for the inclusion of nuclear energy in the EU Sustainable Finance Taxonomy. At the time they were supported by a bulwark of EU countries and environmental NGOs that opposed nuclear energy. Honestly, it looked like they might win.But today, that whole landscape has changed.Germany, long a symbol of anti-nuclear politics, is beginning to shift. The nuclear phase-outs or bans in the Netherlands, Belgium, Switzerland, Denmark, and Italy are now history. Even Fridays for Future has quietened its opposition, and in some places, embraced nuclear power.This moment matters.It shows what\u2019s possible when we stick to the science. The evidence onl"
  },
  {
    "title": "3D modeling with paper (arvinpoddar.com)",
    "points": 239,
    "submitter": "joshuawootonn",
    "submit_time": "2025-09-12T14:12:23 1757686343",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=45222369",
    "comments": [
      "Btw, there's a pretty well known origami version of the SR-71 by Toshikazu Kawasaki. One square, no cuts, the usual. I folded it as a kid from diagrams in \"Origami for the Connoisseur\". It's not as detailed as the papercraft version, but I think it symbolizes the real airplane very well.reply",
      "That's pretty awesome. I'd love to see the Lockheed F-117 Nighthawk get the same treatment. Seems like its angular design would lend itself well towards an origami version.reply",
      "Oh wow, this brought me back! I used to be obsessed with papercraft back in the day as a kid, specifically \u201cpepakura\u201d. I used to print out halo 3 helmets and build them and wear them. It was like a puzzle on steroids in the cool department!There used to be an entire finishing process with this yellow and blue bottled smooth-cast resin and sanding before painting, but they always stayed paper for me.Was a cheap way for me to have fun, and definitely holds a special place in my heart forever. Great share and thank you for posting! Brought me through memory lane.reply",
      "I always wonder what the Elements would have looked like had Euclid had included paper folding as a primitive.Folds are powerful. One can trisect or n-sect any angle for finite n. One still needs the compass though for circle.    Straight edge\n    Compass\n    Nuesis\n    Paper folding\n\nMakes for a very powerful tool set.reply",
      "The Greeks were not adverse to studying topics outside of the classic axioms, for example neusis, conic sections, or Archimedes work on quadrature (which presaged calculus):https://en.wikipedia.org/wiki/Neusis_constructionhttps://en.wikipedia.org/wiki/Conic_sectionhttps://en.wikipedia.org/wiki/Quadrature_(mathematics)https://en.wikipedia.org/wiki/Quadrature_of_the_ParabolaThey just preferred the simpler axioms on grounds of aesthetic parsimony.As far as I know, the ancient Greeks never thought to fold the paper. It has, however, been studied since the 1980's by modern mathematicians:https://en.wikipedia.org/wiki/Huzita%E2%80%93Hatori_axiomsIt can be used to trisecting an angle, an impossible construction with straightedge and compass:https://www.youtube.com/watch?v=SL2lYcggGpc&t=185sIt's more powerful than compass and straight-edge constructions, but not by much. It essentially gives you cube roots in addition to square roots. You still need a completely different point of view to make the quantum leap the the real numbers, calculus, and limits:https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_t...https://en.wikipedia.org/wiki/Dedekind_cutSo ultimately I don't know if it would have changed the course of history that much.reply",
      "Sure, it makes sense to isolate the minimal sets of primitives needed for an operation. Greeks experimented quite a bit with nuesis before focusing on straight edge and compass. Folding, as you noted, was not part of their mix. BTW nuesis can also trisect angles, so they could do it without origami.Origami folding is more powerful than the closure of rationale by square and cube roots.They were extended to the quintic roots by Robert Lang using a type of folding called multifold. Now it's known that with multifolds all of the algebraic numbers can be constructed with origamihttps://arxiv.org/abs/0808.1517Yes one would not reach the reals (that's not the ultimate goal) but the geometry would certainly would have been richer.By no means is the area of folding a mathematical dead end as new theorems still get discovered.reply",
      "> Folds are powerful. One can trisect or n-sect any angle for finite n.Does that mean folding allows you to construct (without trial-and-error) an accurate heptagon, even though you can't with a straight-edge and compass?Intuitively, that seems wrong, I would expect many of the same limitations to apply.reply",
      "Akira Yoshizawa actually used origami in a factory setting to communicate geometric and engineering concepts.reply",
      "As a person who wonders where the paper X-15 model he had vanished to after he joined the service, this resonates with me.While there are a lot of models available for purchase/download, the classic tool for this sort of thing ishttps://pepakura.tamasoft.co.jp/pepakura_designer/as noted by coldfoundry --- that said, an unlikely tool which has this is PythonSCAD:https://pythonscad.org/which allows one to use OpenSCAD or Python to create a 3D model and export it in a number of formats, including \"Foldable PS\" which automates this process.reply",
      "It is/was quite popular in Poland. 35 years ago, as a kid, I was assembling paper models. Planes were the easiest, usually it took about 2 days to do one. Couple of years ago I wanted to get back to it, so I bought a plane. Well, it turned out that fashion for paper models had changed and now 'reductionist' models are in full swing - being as close as possible to original. That plane has 160 pieces (a lot of them also subdivided), and every part that has size about 10cm in real life, has been modelled. In two weeks I was still in cockpit. Here is paper model of SR-71: https://www.sklep.model-kom.pl/sr-71-model-samolotu-rozpozna... From drawings it looks like it is more than 167+, not including subparts.reply"
    ],
    "link": "https://www.arvinpoddar.com/blog/3d-modeling-with-paper",
    "first_paragraph": "August 31, 2025Over the past several years, I've enjoyed the hobby of paper\nmodeling (or papercraft), the art\nof creating 3D models from cut and glued parts from paper sheets. This hobby is\na superset of origami, in that it allows for cutting and gluing, as well as for\nmultiple sheets of paper for a single model. The alleviation of these\nconstraints means that papercraft allows for more complex models that are easier\nto assemble.Over many years, I've built models designed by others as well as designed my\nown. In this post, I want to share everything I've learned along the way,\ncovering the entire process from design to assembly.I love this hobby for three reasons:Let's dive in. My most recent model is a papercraft plane inspired by the SR-71\nBlackbird, a\nreconnaissance plane that to this day holds many records for being one of the\nfastest aircrafts ever. It's now one of most iconic planes ever designed and an\nengineering masterpiece. The program was ultimately retired in 1999.We're goi"
  },
  {
    "title": "Reduce bandwidth costs with dm-cache: fast local SSD caching for network storage (upsun.com)",
    "points": 22,
    "submitter": "tlar",
    "submit_time": "2025-09-09T12:14:58 1757420098",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=45180876",
    "comments": [
      "> For e-commerce workloads, the performance benefit of write-back mode isn\u2019t worth the data integrity risk. Our customers depend on transactional consistency, and write-through mode ensures every write operation is safely committed to our replicated Ceph storage before the application considers it complete.Unless the writer is always overwriting entire files at once blindly (doesn't read-then-write), consistency requires consistency reads AND writes. Even then, potential ordering issues creep in. It would be really interesting to hear how they deal with it.reply",
      "dm-cache writeback mode is both amazing and terrifying. It reorders writes, so not only do you lose data if the cache fails, you probably just corrupted the entire backing disk.reply",
      "This is good timing; I was just looking at a use-case where we need more iops and the only immediate solutions involve allocating way more high-performance disks or network storage. The problem with a cache is having a large dataset with random access, so repeated cache hits might not be frequent. But I had a theory that you could still make an impact on performance and lower your storage performance requirements. I may try this out, but it is block-level, so it's a bit intrusive.Another option I haven't tried is tmpfs with an overlay. Initial access is RAM, falls back to underlying slower storage. Since I'm mostly doing reads, should be fine, writes can go to the slower disk mount. No block storage changes needed.reply",
      "I was looking into SSD caching recently and decided to go with Open-CAS instead, which should be more performant (didn't test it personally): https://github.com/Open-CAS/open-cas-linux/issues/1221It's maintained by Intel and Huawei and the devs were very responsive.reply",
      "\"When deploying infrastructure across multiple AWS availability zones (AZs), bandwidth costs can become a significant operational expense\"An expense in the age of 100gbit networking that is entirely because AWS can get away with charging the suckers, um, customers for itreply",
      "AZs are whole datacenters, so I imagine their backbone bandwidth between AZs is a fraction of total bandwidth inside the DC. If they didn't charge it'd probably get saturated and then there's not much point in using them for reliability.The internet egress price is where they're bastards.reply",
      "Definitely not. Azure doesn't charge for intra region costs FWIW.Getting terabits and terabits of 'private' interconnect is unbelievably cheap at amazon scale. AWS even own some of their own cables and have plans to build more.There is _so_ much capacity available on fiber links. For example one newish (Anjana) cable between the US and Europe has 480Tbit/sec capacity. That's just one cable. And that could probably be upgraded to 10-20x that already with newer modulation techniques.reply",
      "reduce network bandwidth from the network attaches SSD volumes, yes?reply"
    ],
    "link": "https://devcenter.upsun.com/posts/cut-aws-bandwidth-costs-95-with-dm-cache/",
    "first_paragraph": "On this pageGet startedEnjoy the same, game-changing Upsun features for less. Introducing First Project Incentive.11Terms and conditions applyWhen deploying infrastructure across multiple AWS availability zones (AZs), bandwidth costs can become a significant operational expense. Some of our Upsun infrastructure spans three AZs for high availability, but this architecture created an unexpected challenge with our Ceph-based storage system.Since Ceph distributes data across the cluster and AWS bills for inter-AZ traffic, approximately two-thirds of our disk I/O traffic crossed AZ boundaries\u2014generating substantial bandwidth charges. With all disk operations flowing over the network rather than accessing local storage, we needed a solution that could reduce this costly network traffic without compromising our distributed storage benefits.Our AWS instance types included small amounts of local SSD storage that weren\u2019t being utilized for primary storage. This presented an opportunity: what if "
  },
  {
    "title": "Unauthorized Windows/386 (virtuallyfun.com)",
    "points": 36,
    "submitter": "Bogdanp",
    "submit_time": "2025-09-09T15:09:34 1757430574",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=45183008",
    "comments": [
      "I read it about halfway through before the kiddos started screaming. Overall very good.One nitpick: maybe provide just a bit more detail regarding how your/your friend came to some of the conclusions. It isn't because I don't believe anything, however I'm just an old timer (somewhat, elder millennial) who likes to know how things work/how you guys game to the conclusions you did. Not a full rundown, obviously, however I did see quite a few assumptions that were only partially explained. Seems to be a good read, however, and I'll finish it tomorrow.reply",
      "All in all, this was a very fun project to do, as Windows/386 was really lacking the kind of in-depth analysis that its successors got. I hope to update the project in the future, as well as maybe pivot to something like OS/2.reply",
      "OS/2 would be amazingreply",
      "I love this, great work. Might have more to say once I'm through reading it.reply"
    ],
    "link": "https://virtuallyfun.com/2025/09/06/unauthorized-windows-386/",
    "first_paragraph": ""
  },
  {
    "title": "First 'perovskite camera' can see inside the human body (northwestern.edu)",
    "points": 41,
    "submitter": "geox",
    "submit_time": "2025-09-09T01:49:08 1757382548",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=45176471",
    "comments": [
      "\"While cheaper than CZT detectors, NaI detectors are bulky and produce blurrier images \u2014 like taking a photo through a foggy window.\"I'm constantly amazed at what these articles do not show. Like if we have an example of a foggy window image and one from CZT and now one from this new sensor, why not show an example of each? A picture is worth a 1,000 words after all, so not including them really does the reader a disservice when reading these articles.reply",
      "From this, it sounds like it hasn't been integrated into an imaging device yet:\"Record energy resolutions are achieved as 2.5% at 141 keV and 1.0% at 662 keV. Single photon imaging with single point and line 99mTc \u03b3-ray sources showcases the high sensitivity of 0.13%~0.21% cps/Bq. Phantom imaging distinctly delineates individual column sources spaced 7 mm apart, indicative of an impressive spatial resolution of 3.2 mm. These findings lay the groundwork for integrating perovskite detectors into nuclear medicine \u03b3-ray imaging systems, offering a balance of cost-effectiveness and superior performance.\"https://www.nature.com/articles/s41467-025-63400-7reply",
      "Perovskites are research materials being researched.Images produced from SPECT cameras have been around for a while. [2]This is potentially a 16 pixel \"camera\" which the \"image\" is a gaussian blob (Figure 1e and 5e) [1].This is interesting for a variety of reasons but is way overblown in the \"camera\" or \"image\" context. It's demonstration that one can make pixelated devices (4x4) of a specific kind of promising material.[1]https://www.nature.com/articles/s41467-025-63400-7[2]https://en.wikipedia.org/wiki/Single-photon_emission_compute...reply",
      "Hmm, why do I know this word \"perovskite\". Wikipedia gives me no clues, just some mineral.reply",
      "Possible source: Solar panels with this material were hyped a couple years ago.reply",
      "Ah, that's what it was for me.Roll-to-roll fabricated perovskite solar cells under ambient room conditions: https://news.ycombinator.com/item?id=39998740reply",
      "https://pubs.rsc.org/en/content/articlehtml/2024/tc/d4tc0208...IIRC it was some different type of imaging sensor, so looked it up that wayreply",
      "They are used in thin-film solar panel development. Not sure anyone has cracked the big problem with them, which is durability.reply",
      "Where are the pictures?reply"
    ],
    "link": "https://news.northwestern.edu/stories/2025/09/first-perovskite-camera-can-see-inside-the-human-body/",
    "first_paragraph": "Physicians rely on nuclear medicine scans, like SPECT scans, to watch the heart pump, track blood flow and detect diseases hidden deep inside the body. But today\u2019s scanners depend on expensive detectors that are difficult to make.Now, scientists led by Northwestern University and Soochow University in China have built the first perovskite-based detector that can capture individual gamma rays for SPECT imaging with record-breaking precision. The new tool could make common types of nuclear medicine imaging sharper, faster, cheaper and safer.For patients, that could mean shorter scan times, clearer results and lower doses of radiation.The study was published in the journal Nature Communications.\u201cPerovskites are a family of crystals best known for transforming the field of solar energy,\u201d said Northwestern\u2019s Mercouri Kanatzidis, the study\u2019s senior author. \u201cNow, they are poised to do the same for nuclear medicine. This is the first clear proof that perovskite detectors can produce the kind o"
  },
  {
    "title": "How FOSS Projects Handle Legal Takedown Requests (f-droid.org)",
    "points": 89,
    "submitter": "mkesper",
    "submit_time": "2025-09-12T17:22:38 1757697758",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=45224421",
    "comments": [
      "> One FOSS organization, for example, requires all legal correspondence to be submitted by postal mail in the national language and citing local law. Most complaints evaporate once asked to comply.Pure gold.reply",
      "I typically get a takedown notice a couple times a week, usually from my registrar (Namecheap) or from Netcraft, about 100 so far.I keep a public (transparent) list of takedowns, on a public repo on GitHub. The commit messages are the logs. [0]I have a way to dispute: raise a GitHub issue. I've only had two people dispute: one was legit, and I unblocked him, and the other ran a WordPress site which he didn't know was compromised. I did not unblock him. [1]Please don't judge me harshly for honoring the takedowns immediately, but I do so because the remedy is simple: register your own domain, and don't rely on my nip.io / sslip.io service (which maps IP addresses to hostnames as a convenience for developers, e.g. 127.0.0.1.nip.io \u2192 127.0.0.1).Dealing with takedown requests is the least pleasant aspect of running FOSS project. I want to spend my free time coding, not blocking phishers, scammers, and grifters.[0] https://github.com/cunnie/sslip.io-blocklist\n[1] https://github.com/cunnie/sslip.io/issues/100reply",
      "This seems like a well balanced approach. I do love the abuse mitigation measures in place to dissuade casually malicious actors. The fact that providing evidence itself is a deterrent just goes to show how ill intentioned most of them are.reply",
      "TFA goes: A window for response (commonly 14 days) is offered, unless unfeasible due to seriousness and time restraints of the request itself. If the developer disputes the claim and provides supporting information (e.g. license, public domain status, fair use justification), the claim is reviewed.As someone who has had multiple FOSS projects take down by companies / app stores (happens when we go viral in some country), DDoS'd by rouge actors (thanks for saving our bacon, Cloudflare!), visits from law enforcement etc; F-Droid's post on \"appeals process\" comes as a surprise. Here's the email I received from them:  Dear The Rethink DNS Authors,\n\n  The F-Droid platform has received an official order from Roskomnadzor (RKN), Russia's Federal Service for Supervision of Communications, IT, and Mass Media, regarding Rethink (Registry Entry #3133609-\u0420\u0418) https://f-droid.org/ru/packages/com.celzero.bravedns/\n  ...\n\n  F-Droid took technical measures to block your website app page for the Russian site visitors to avoid the risk of limited access to F-Droid as a whole. For further queries or concerns, contact legal@f-droid.org.\n\n  Thank you for your cooperation.\n\nNothing in there informs me that I had the opportunity to appeal.reply",
      "How does it make sense to ask an app developer to appeal on behalf of a platform they have zero control over?reply",
      "It doesn't, but platforms basically do everything they can to claim the various common-carrier liability shields in DMCA-like laws.  In the U.S. that means they forward the takedown request to whomever generated the content, and in theory should allow that generator to comply, or publish a counterclaim.The whole system falls on the floor though when the common carriers aren't, and have low quality processes that don't actually enable the counterclaim half of this process.reply",
      "Don't be fooled. These so called low quality processes are designed by large corporations in order to abuse their positions and retain control over all content being shown. The providers have no interest in providing legal protections to their small content creators. They want to focus on pleasing the big players.reply",
      "The entire concept of a \"takedown request\" is a compromise solution. Platforms would ideally like to be a public square, where third parties can say whatever they want and the platform doesn't have to do much about it. Copyright holders, revenge porn victims, etc. would prefer to hold the platforms strictly liable, because on the Internet it's extremely hard to actually find the third parties. So in a variety of contexts we've found it's useful to meet in the middle: platforms are exempt from liability, but in return they have to process takedown requests, unless the third party challenges the takedown and makes themselves available for possible legal proceedings.reply"
    ],
    "link": "https://f-droid.org/2025/09/10/how-foss-projects-handle-legal-takedown-requests.html",
    "first_paragraph": "When a legal takedown request arrives, whether it\u2019s about copyright,\ncensorship, privacy, or something more vague, how a Free and Open Source\nSoftware (FOSS) project responds can make all the difference.Handled well, a takedown request can be a manageable administrative step. Handled poorly, it can cause panic, disrupt infrastructure, or even put contributors at legal risk.As part of our legal resilience research, we spoke with a range of legal experts, software freedom advocates, and maintainers of mature FOSS infrastructure to understand how others manage these moments. In this article, we share what we learned, and how F-Droid is incorporating these lessons into its own approach.Despite differences in jurisdiction, size, and mission, a few common themes\nfrom our research emerged when we asked how other projects handle takedown\nrequests:Legal threats often follow the path of least resistance. FOSS projects that\npublish a formal takedown policy, require legal submissions through speci"
  },
  {
    "title": "Discovery of a new satellite or ring arc around Quaoar (phys.org)",
    "points": 9,
    "submitter": "wglb",
    "submit_time": "2025-09-12T02:17:54 1757643474",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=45218017",
    "comments": [
      "Previous related discussions:Second \u2018impossible\u2019 ring found around distant dwarf planet - https://news.ycombinator.com/item?id=35762112 - April 2023 (61 comments)Ring discovered around dwarf planet Quaoar confounds theories - https://news.ycombinator.com/item?id=34714024 - Feb 2023 (1 comment)reply",
      "Article in RNASS research notes https://iopscience.iop.org/article/10.3847/2515-5172/adfedareply"
    ],
    "link": "https://phys.org/news/2025-09-discovery-moon-orbiting-mysterious-distant.html",
    "first_paragraph": ""
  },
  {
    "title": "Rust: A quest for performant, reliable software [video] (youtube.com)",
    "points": 96,
    "submitter": "raphlinus",
    "submit_time": "2025-09-12T07:58:55 1757663935",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=45219817",
    "comments": [
      "> \"How Rust Won\"I love Rust, I'm a fan of writing it and I love the tooling. And I love to see it's (hopefully) getting more popular. Despite this, I'm not sure if \"won\" is the right word because to my very uneducated eyes there is still considerable amount of Rust not succeeding. Admittedly I don't write so much Rust (I should do more!) but when I do it always baffles me how tons of the libraries recommended online are ghost town. There are some really useful Rust libraries out there that weren't maintained for many years. It still feels like Rust ecosystem is not quite there to be called a \"successful\" language. Am I wrong? This is really not a criticism of Rust per se, I'm curious about the answer myself. I want to dedicate so much more time and resources on Rust, but I'm worries 5 to 10 years from now everything will be unmaintained. E.g. Haskell had a much more vibrant community before Rust came and decent amount of Haskellers moved to Rust.reply",
      "I don't even write in Rust, yet I'm curious if those libraries you talk about are truly in \"abandoned\" state and not simply in \"done\" state? Some languages somehow managed to build thriving ecosystems of libraries where they don't require constant attention and perpetual churn like in JS and Python. I see it too often e.g., in Clojure, where lib authors even have to add \"maintenance disclaimers\" noting that the lib is good for what it was designed for and there are no plans to add new features and no known bugs or critical dependencies found, and the lib is not abandoned, and they update those notes periodically, just for the sake of showing any git activity.reply",
      "No, many are truly abandoned.I have this all the time. Any new rust project and you have to wade through a bunch of once-great crates.But that's because rust is new. The initial surge over produced solutions to, say binary serialization, and under produced, say, good geodesy libraries. And many many were abandoned. Go to any of the \"are we X yet\" sites and you'll see many crates that are clearly not finished or advancing which were recently considered SoA.reply",
      "Any library in Rust comes with Cargo.toml file listing dependencies and their versions. Rust build system allows to use later versions of the libraries so presumably an application that uses an old library will have dependencies for the library updated.The problem is that sometimes library may need to pin a dependency version. Or a dependency was released with a newer major version update and do not back-port security fixes to older versions.So one cannot just use an old library. Its dependency list must carefully considered.Now this problem exists with any package management system. But in Rust it is more visible as the language still evolves quickly with non-trivial new features released often.Then the library authors may want to use newer language features on their API. Then they simply bump the library mayor version  and maintain only that. So an old dependencies will not get updates.reply",
      "> The problem is that sometimes library may need to pin a dependency version.We on the Cargo team have been working to educate people on the problems with pinning in Cargo.toml instead of relying on Cargo.lock> Then the library authors may want to use newer language features on their API. Then they simply bump the library mayor version and maintain only that. So an old dependencies will not get updates.Thankfully, the ecosystem has mostly settled on build requirements not being subject to SemVer and bump Rust versions in compatible releases. There are a few hold outs.reply",
      "By any standard of language development rust \"won\" in that a decade after creation it has a thriving ecosystem and companies using it exclusively. The White House named it by name.I would say it won like it won the lottery, not like it won the tournament.reply",
      "I think you can say Rust won. There's enough investment from big tech and enough demonstrated value that it won't go away. And compared to functional languages with similarly sophisticated type systems? Rust has gained more users in the past 10 years than probably all of those other languages combined. That's not a fluke. Rust has pulled off making functional programming mainstreamreply",
      "I agree with the sentiment somewhat. Some rust libraries are dying, while some great new ones thrive (recently found iroh and wgpu to name a few). Everyone wants to write a game engine or some fun project and then abandon it, but no one wants to write a game. No application software has really \"cemented\" itself in the global ecosystem. Except for maybe ripgrep?I would like to see support for more compilers (https://rust-gcc.github.io/), more interoperability with C/C++, better support for cross-compilation. Maybe less reliance on crates.io, static linking, and permissive licenses.Still, I see Rust as the natural progression from C++. It has enough momentum to flatten all competitors (Carbon, Zig, Nim, Go) except scripting languagesreply",
      "> Still, I see Rust as the natural progression from C++I don't; Rust has its niche but currently can't replace C++ everywhere.From what I'm aware of, Rust has poor ergonomics for programs that have non-hierarchical ownership model (ie. not representable by trees), for example retained mode GUIs, game engines, intrusive lists in general, non-owning pointers of subobjects part of the same forever-lived singleton, etc.> GoTo displace Go you must also displace Kubernetes and its ecosystem (unlikely, k8s is such a convenient tool), or have k8s move away from Go (not gonna happen considering who developed both)reply",
      "> From what I'm aware of, Rust has poor ergonomics for programs that have non-hierarchical ownership model (ie. not representable by trees)Yeah, non-hierarchical references don't really lend themselves to static safety enforcement, so the question is what kind of run-time support the language has for non-hierarchical references. But here Rust has a disadvantage in that its moves are (necessarily) trivial and destructive.For example, the scpptool-enforced memory-safe subset of C++ has non-owning smart pointers that safely support non-hierarchical (and even cyclical) referencing.They work by wrapping the target object's type in a transparent wrapper that adds a destructor that informs any targeting smart pointers that the object is about to become invalid (or, optionally, any other action that can ensure memory safety). (You can avoid needing to wrap the target object's type by using a \"proxy\" object.)Since they're non-owning, these smart pointers don't impose any restrictions on when/where/how they, or their target objects, are allocated, and can be used more-or-less as drop-in replacements for raw pointers.Unfortunately, this technique can't be duplicated in Rust. One reason being that in Rust, if an object is moved, its original memory location becomes invalid without any destructor/drop function being called. So there's no opportunity to inform any targeting (smart) pointers of the invalidation. So, as you noted, the options in Rust are less optimal. (Not just \"ergonomically\", but in terms of performance, memory efficiency, and/or correctness checking.) And they're intrusive. They require that the target objects be allocated in certain ways.Rust's policy of moves being (necessarily) trivial and destructive has some advantages, but it is not required (or arguably even helpful) for achieving \"minimal-overhead\" memory safety. And it comes with this significant cost in terms of non-hierarchical references.So it seems to me that, at least in theory, an enforced memory-safe subset of C++, that does not add any requirements regarding moves being trivial or destructive, would be a more natural progression from traditional C++.reply"
    ],
    "link": "https://www.youtube.com/watch?v=k_-6KI3m31M",
    "first_paragraph": ""
  },
  {
    "title": "Humanely dealing with humungus crawlers (tedunangst.com)",
    "points": 71,
    "submitter": "freediver",
    "submit_time": "2025-09-12T17:06:52 1757696812",
    "num_comments": 39,
    "comments_url": "https://news.ycombinator.com/item?id=45224246",
    "comments": [
      ">We\u2019ve already done the work to render the page, and we\u2019re trying to shed load, so why would I want to increase load by generating challenges and verifying responses? It annoys me when I click a seemingly popular blog post and immediately get challenged, when I\u2019m 99.9% certain that somebody else clicked it two seconds before me. Why isn\u2019t it in cache? We must have different objectives in what we\u2019re trying to accomplish. Or who we\u2019re trying to irritate.+1000 I feel like so much bot detection (and fraud prevention against human actors, too) is so emotionally-driven. Some people hate these things so much, they're willing to cut off their nose to spite their face.reply",
      "My view on this is simple:If you're a bot which will ignore all the licenses I put on that content, then I don't want to you to be able to reach that content.No, any amount of monetary compensation is not welcome either. I use these licenses as a matter of principle, and my principles are not for sale.That's all, thanks.reply",
      "I think the problem is that despite the effort, you will still end up in the dataset. So it's futilereply",
      "How can you tell a bot will ignore all your content licenses?reply",
      "Currently all AI companies argue that the content they use falls under fair use, and disregard all licenses. This means any future ones respecting these licenses needs to be whitelisted.reply",
      "How do you know that that bot is part of those AI companies? Maybe it's my personal bot you're blocking, should I also not have (indirectly) access to the content?reply",
      "No. Access to my content is a privilege I grant you. I decide how you get to access it, and via a bot that my setup confuses for an AI crawler belonging to an anti-human AI corporation is not a valid way to access it. Get off my virtual lawn.reply",
      "> No. Access to my content is a privilege I grant you.Right, I thought the conversation was about public websites on the public internet, but I think you're talking about this in the context of a private website now? I understand keeping tighter controls if you're dealing with private content you want accessible via the internet for others but not the public.reply",
      "All websites are private (excepting maybe government sites). In most places the internet infrastructure itself is private.You're conflating a legal concept that applies to areas that are shared, government owned, paid for by taxes, and the government feels like people should be able to access them.The web is closer to a shopping mall. You're on one persons property to access other people's stuff who pay to be there. They set their own rules. If you don't follow those rules you get kicked out, charged with trespassing, and possibly banned from the mall entire.AI bots have been asked to leave. But, since they own the mall too, the store owners are more than a little screwed.reply",
      "You\u2019re literally visiting a service paid for by me. It\u2019s open to the public, but it\u2019s my domain and my server and I get to say \u201cno thank you\u201d to your visit if you don\u2019t behave. You have no innate right to access the content I share.Blocking misbehaving IP addresses isn\u2019t new, and is another version of the same principle.reply"
    ],
    "link": "https://flak.tedunangst.com/post/humanely-dealing-with-humungus-crawlers",
    "first_paragraph": "I host a bunch of hobby code on my server. I would think it\u2019s really only interesting to me, but it turns out every day, thousands of people from all over the world are digging through my code, reviewing years old changesets. On the one hand, wow, thanks, this is very flattering. On the other hand, what the heck is wrong with you?This has been building up for a while, and I\u2019ve been intermittently developing and deploying countermeasures. It\u2019s been a lot like solving a sliding block puzzle. Lots of small moves and changes, and eventually it starts coming together.My primary principle is that I\u2019d rather not annoy real humans more than strictly intended. If there\u2019s a challenge, it shouldn\u2019t be too difficult, but ideally, we want to minimize the number of challenges presented. You should never suspect that I suspected you of being an enemy agent.First measure is we only challenge on the deep URLs. So, for instance, I can link to the anticrawl repo no problem, or even the source for anticra"
  },
  {
    "title": "OpenAI Grove (openai.com)",
    "points": 87,
    "submitter": "manveerc",
    "submit_time": "2025-09-12T16:05:58 1757693158",
    "num_comments": 99,
    "comments_url": "https://news.ycombinator.com/item?id=45223660",
    "comments": [
      "Almost every parent comment on this is negative. Why is there such an anti-OpenAI bias on a forum run by YCombinator, basically the pseudo-parent of OpenAI?It seems that there is a constant motive to view any decision made by any big AI company on this forum at best with extreme cynicism and at worse virulent hatred. It seems unwise for a forum focused on technology and building the future to be so opposed to the companies doing the most to advance the most rapidly evolving technological domain at the moment.reply",
      "People remember things and consistently behaving like an asshole gets you treated like an asshole.OpenAI had a lot of goodwill and the leadership set fire to it in exchange for money. That's how we got to this state of affairs.reply",
      "What are the worst things OpenAI has donereply",
      "The number one worst thing they've done was when Sam tried to get the US government to regulate AI so only a handful of companies could pursue research. They wanted to protect their moat.What's even scarier is that if they actually had the direct line of sight to AGI that they had claimed, it would have resulted in many businesses and lines of work immediately being replaced by OpenAI. They knew this and they wanted it anyway.Thank god they failed. Our legislators had enough of a moment of clarity to take the wait and see approach.reply",
      "Do you believe AI should not be regulated?Most regulations that have been suggested would but restrictions mostly the largest, most powerful models, so they would likely affect OpenAI/Anthropic/Google  primarily before smaller upstarts would be affected.reply",
      "Dude, they completely betrayed everything in their \"mission\". The irony in the name OpenAI for a closed, scammy, for profit company can not be lost on you.reply",
      "They released a near-SOTA open-source model recently.Their prerogative is to make money via closed-source offerings so they can afford safety work and their open-source offerings. Ilya noted this near the beginning of the company. A company can't muster the capital needed to make SOTA models giving away everything for free when their competitor is Google, a huge for-profit company.As per your claim that they are scammy, what about them is scammy?reply",
      ">  Why is there such an anti-OpenAI bias on a forum run by YCombinator, basically the pseudo-parent of OpenAI?Isnt that a good thing? The comments here are not sponsored, nor endorsed by YC.reply",
      "I'd expect to see a balance though, at least on the notion that people would be attracted to posting on a YC forum over other forums due to them supporting or having an interest in YC.reply",
      "I think the majority of people don't care about YC. It just happens to be the most popular tech forum.reply"
    ],
    "link": "https://openai.com/index/openai-grove/",
    "first_paragraph": ""
  },
  {
    "title": "Kefir: Solo-developed full C17/C23 compiler with extensive validation (protopopov.lv)",
    "points": 8,
    "submitter": "jprotopopov",
    "submit_time": "2025-09-09T14:34:56 1757428496",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=45182518",
    "comments": [
      "Wow, hats off to you! This is one of the most impressive solo projects I've seen in a while!Making a toy C compiler isn't rocket science, but developing one that's complete and production-ready is whole nother story. AFAICT, Kefir fits into the latter category:- C17/C23 compliance- x86_64 codegen- debug info gen- SSA-based optimization passes (the most important ones)- has a widely-compatible cc cli- is extensively tested/fuzzedSome advantages compared to the big three (GCC, Clang, MSVC):- It's fairly small and simple- That means it's understandable and predictable - no surprises in terms of what it can and cannot do (regarding optimizations in particular)- Compilation is probably very fast (although I haven't done any benchmarking)- It could be modified or extended fairly easilyThere might be real interest in this compiler from people/companies who- value predictability and stability very highly- want to be in control of their entire software supply chain (including the build tools)- simply want a faster compiler for their debug buildsThink security/high assurance people, even the suckless or handmade community might be interested.So it's time to market this thing! Get some momentum going! It would be too sad to see this project fade away in silence. Announce it in lots of places, maybe get it on Compiler Explorer, etc.\n(I'm not saying that you have to do this, of course. But some people could genuinely benefit from Kefir.)P.S. Seems like JKU has earned its reputation as one of the best CS schools in Austria ;-)reply",
      "(I thought that the announcement has completely faded, so haven't even checked the replies).I'll immediately reveal some issues with the project. On the compilation speed, it is unfortunately atrocious. There are multiple reasons for that:1. Initially the compiler was much less ambitious, and was basically generating stack-based threaded code, so everything was much simpler. I have managed to make it more reasonable in terms of code generation (now it has real code generator, optimization pipeline), but there is still huge legacy in the code base. There is a whole layer of stack-based IR which is translated from the AST, and then transformed into equivalent SSA-based IR. Removing that means rewriting the whole translator part, for which I am not ready.2. You've outlined some appealing points (standard compliance, debug info, optimization passes), but again -- this came at the expense of being over-engineered and bloated inside. Whenever, I have to implement some new feature, I hedge and over-abstract to keep it manageable/avoid hitting unanticipated problems in the future. This has served quite well in terms of development velocity and extending the scope (many parts have not seen complete refactoring since the initial implementation in 2020/2021, I just build up), but efficiency of the compiler itself suffered.3. I have not particularly prioritized this issue. Basically, I start optimizing the compiler itself only when something gets too unreasonable (either, in terms of run time, or memory). There are all kinds of inefficiencies, O(n^2) algorithms and such simply because I knew that I would be able to swap that part out should that be necessary, but never actually did. I think the compiler efficiency has been the most de-prioritized concern for me.Basically, if one is concerned with compilation speed, it is literally better to pick gcc, not even talking about something like tcc. Kefir is abysmal in that respect. I terms, of code base size, it is 144k (sans tests, 260k in total) which is again not exactly small. It's manageable for me, but not hacker-friendly.With respect to marketing, I am kind of torn. I cannot work on this thing full time, unless somebody is ready to provide sufficient full-time funding for myself and also other expenses (machines for tests, etc). Otherwise, I'll just increase the workload on myself and reduce the amount of time I can spend actually working on the code, so it'll probably be net loss for the project. Either way, for now I treat it closer to an art project than a production tool.As for compiled code performance, I have addressed it here https://lobste.rs/s/fxyvwf -- it's better than, say, tcc, but nowhere near well-established compilers. I think this is reasonable to expect, and the exact ways to improve that a bit are also clear to me, it's only question of development effortP.S. JKU is a great school, although by the time I enrolled there the project has already been on the verge of bootstrapping itself.EDIT: formattingreply",
      "Cool project. Unlike tcc and cproc though kefir doesn't seem very good at handling big arrays. This    $ kefir -c - <<x\n    int a[] = {\n    $(seq 10000000 | tr '\\n' ,)\n    };\n    x\n\nallocates gigabytes of memory and eventually crashes WSL on my machine.reply",
      "I have addressed compiler inefficiency in the sibling comment. This is indeed a problem. Empty arrays of such size should be compile-able (there is sparse representation for arrays). However, I would say that this use case is not particularly practical, at least in none of the projects from my test suite this has been an issue.reply",
      "Good work either way. Congrats!reply"
    ],
    "link": "https://kefir.protopopov.lv/posts/announce0.html",
    "first_paragraph": "\n                Today I release Kefir \u2014 an\n                independent C17/C23 compiler. Solo-built. Extensively validated,\n                for x86_64 & System-V ABI. With SSA-based optimization\n                pipeline, DWARF-5 support and position-independent code\n                generation.\n            \n                I wanted to see whether it is possible to independently\n                implement a modern C compiler. For real-world software.\n                \nIt is. No further excuses and elaborations will be\n                provided.\n            \nFull\n                details here. If needed, copy-paste and discuss it with your\n                LLM of choice.\n                \nSubstantial inquiries will be answered personally\n\n                Dedicating this to Sloka & Kauguri.\n            \n                Details in README.\n                Code on SourceHut.\n                Myself is here.\n                \n                You know what to do next.\n            "
  },
  {
    "title": "Real-time AI hallucination detection with timeplus: A chess example (timeplus.com)",
    "points": 7,
    "submitter": "gangtao",
    "submit_time": "2025-09-09T15:20:38 1757431238",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=45183191",
    "comments": [
      "So you have to be able to identify a priori what is and isn't an hallucination right?reply",
      "Yeah, reading the headline got me excited too.\nI thought they are going to propose some novel solution or use the recent research by OpenAI on reward function optimization.reply"
    ],
    "link": "https://www.timeplus.com/post/ai-chess-hallucination-detection",
    "first_paragraph": "Timeplus vs. ksqlDBTimeplus +\u00a0ClickHouseTimeplus vs. Apache FlinkTimeplus EnterprisePricingTimeplus EnterpriseSelf-Hosted\u00a0USE CASESReal-Time AnalyticsReal-Time TelemetryReal-Time AI/MLCUSTOMER STORIESHuatai SecuritiesSallaUNI SemiconductorZyreSee AllOur TeamMedia KitTimeplus vs. ksqlDBTimeplus vs. Apache FlinkTimeplus EnterprisePricingBY USE CASEReal-Time Trade IntelligenceDDoS DetectionEdge Analytics and Anomaly DetectionObservabilityAI/ML:\u00a0Real-Time Feature PlatformAI/ML: Real-Time RAGBY INDUSTRYFinancial ServicesTelecommunicationsManufacturingTechnologyCUSTOMER STORIESHuatai SecuritiesUNI SemiconductorTimeplus vs. ksqlDBTimeplus vs. Apache FlinkTimeplus EnterprisePricingBY USE CASEReal-Time Trade IntelligenceDDoS DetectionEdge Analytics and Anomaly DetectionObservabilityAI/ML:\u00a0Real-Time Feature PlatformAI/ML: Real-Time RAGBY INDUSTRYFinancial ServicesTelecommunicationsManufacturingTechnologyCUSTOMER STORIESHuatai SecuritiesUNI SemiconductorUpdated: Jun 13AI agents are becoming the n"
  }
]