[
  {
    "title": "Horses: AI progress is steady. Human equivalence is sudden (andyljones.com)",
    "points": 43,
    "submitter": "pbui",
    "submit_time": "2025-12-09T00:26:35 1765239995",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=46199723",
    "comments": [
      "This tracks with my own usage over just this year. There have been two releases that caused step changes in how much I actually use AI:1. The release of Claude Code in February2. The release of Opus 4.5 two weeks agoIn both of these cases, it felt like no big new unlocks were made. These releases aren\u2019t like OpenAI\u2019s o1, where they introduced reasoning models with entirely new capabilities, or their Pro offerings, which still feel like the smartest chatbots in the world to me.Instead, these releases just brought a new user interface, and improved reliability. And yet these two releases mark the biggest increases in my AI usage. These releases caused the utility of AI for my work to pass thresholds where Claude Code became my default way to get LLMs to read my code, and then Opus 4.5 became my default way to make code changes.reply",
      "People are not simple machines or animals. Unless AI becomes strictly better than humans and humans + AI, from the perspective of other humans, at all activities, there will still be lots of things for humans to do to provide value for each other.The question is how do our individuals, and more importantly our various social and economic systems handle it when exactly what humans can do to provide value for each other shifts rapidly, and balances of power shift rapidly.If the benefits of AI accrue to/are captured by a very small number of people, and the costs are widely dispersed things can go very badly without strong societies that are able to mitigate the downsides and spread the upsides.reply",
      "This is a fun piece... but what killed off the horses wasn't steady incremental progress in steam engine efficiency, it was the invention of the internal combustion engine.reply",
      "According to Wikipedia, the IC engine was invented around 1800 and only started to get somewhere in the late 1800s. Sounds like the story doesn\u2019t change.https://en.wikipedia.org/wiki/Internal_combustion_enginereply",
      "\"In 1920, there were 25 million horses in the United States, 25 million horses totally ambivalent to two hundred years of progress in mechanical engines.And not very long after, 93 per cent of those horses had disappeared.I very much hope we'll get the two decades that horses did.\"I'm reminded of the idiom \"be careful what you wish for, as you might just get it.\" Rapid technogical change has historically lead to prosperity over the long term but not in the short term. My fear is that the pace of change this time around is so rapid that the short term destruction will not be something that can be recovered from even over the longer term.reply",
      "I think it's a cool perspective, but the not-so-hidden assumption is that for any given domain, the efficiency asymptote peaks well above the alternative.And that really is the entire question at this point: Which domains will AI win in by a sufficient margin to be worth it?reply",
      "Engine efficiency, chess rating, AI cap ex. One example is not like the other. Is there steady progress in AI? To me it feels like it\u2019s little progress followed by the occasional breakthrough but I might be totally off here.reply",
      "I think you are totally off. Individual benchmarks are not very useful on their own, but as far as I\u2019m aware they all tell the same story of continual progress. I don\u2019t find this surprising since it matches my experience as well.reply",
      "ChatGPT was released 3 years ago and that was complete ass compared to what we have today.reply",
      "hello faster horsesreply"
    ],
    "link": "https://andyljones.com/posts/horses.html",
    "first_paragraph": "So after all these hours talking about AI, in these last five minutes I am going to talk about: horses.Engines, steam engines, were invented in 1700.And what followed was 200 years of steady improvement, with engines getting 20% better a decade.For the first 120 years of that steady improvement, horses didn't notice at all.Then, between 1930 and 1950, 90% of the horses in the US disappeared.Progress in engines was steady. Equivalence to horses was sudden.But enough about horses. Let's talk about chess!Folks started tracking computer chess in 1985.And for the next 40 years, computer chess would improve by 50 Elo per year.That meant in 2000, a human grandmaster could expect to win 90% of their games against a computer.But ten years later, the same human grandmaster would lose 90% of their games against a computer.Progress in chess was steady. Equivalence to humans was sudden.Enough about chess! Let's talk about AI.Capital expenditure on AI has been pretty steady.Right now we're - globall"
  },
  {
    "title": "The Universal Weight Subspace Hypothesis (arxiv.org)",
    "points": 47,
    "submitter": "lukeplato",
    "submit_time": "2025-12-09T00:16:46 1765239406",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=46199623",
    "comments": [
      "I find myself wanting genetic algorithms to be applied to try to develop and improve these structures...But I always want Genetic Algorithms to show up in any discussion about neural networks...reply",
      "I have a real soft spot for the genetic algorithm as a result of reading Levy's \"Artificial Life\" when I was a kid. The analogy to biological life is more approachable to my poor math education than neural networks. I can grok crossover and mutation pretty easily. Backpropagation is too much for my little brain to handle.reply",
      "interesting.. this could make training much faster if there\u2019s a universal low dimensional space that models naturally converge into, since you could initialize or constrain training inside that space instead of spending massive compute rediscovering it from scratch every timereply",
      "Eli5?Here's what I've got. You walk into a big room full of lego models of all kinds. You start taking them apart and find that they're not just made of lego blocks, but of the same set of components, each made of lego blocks.If I've got that right it seems to be an opportunity for compression.reply",
      "What's the relationship with the Platonic Representation Hypothesis?reply",
      "From what I can tell, they are very closely related (i.e. the shared representational structures would likely make good candidates for Platonic representations, or rather, representations of Platonic categories).  In any case, it seems like there should be some sort of interesting mapping between the two.reply",
      "I hope someone much smarter than I answers this. I\u2019ve been noticing an uptick platonic and neo-platonic discourse in the zeitgeist and am wondering if we\u2019re converging on something profound.reply",
      "(Finds a compression artifact) \"Is this the meaning of consciousness???\"reply",
      "They compressed the compression? Or identified an embedding that can \"bootstrap\" training with a headstart ?Not a technical person just trying to put it in other words.reply",
      "They identified that the compressed representation has structure to it that could potentially be discovered more quickly. It\u2019s unclear if it would also make it easier to compress further but that\u2019s possible.reply"
    ],
    "link": "https://arxiv.org/abs/2512.05117",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n"
  },
  {
    "title": "Icons in Menus Everywhere \u2013 Send Help (jim-nielsen.com)",
    "points": 217,
    "submitter": "ArmageddonIt",
    "submit_time": "2025-12-08T19:44:00 1765223040",
    "num_comments": 92,
    "comments_url": "https://news.ycombinator.com/item?id=46196688",
    "comments": [
      "From an accessibility/localization stand point, icons+text everywhere seems to be ideal.Also, I disagree with:> This posture lends itself to a practice where designers have an attitude of \u201cI need an icon to fill up this space\u201dSure, that does technically happen, but is in no way preventative or mutually exclusive with the follow on thought:> Does ... the cognitive load of parsing and understanding it, help or hurt how someone would use this menu system?That still happens, because if they mismatch an icon with text, that can result in far worse cognitive load/misunderstanding than if no icon was present at all. This becomes readily apparent in his follow on thought experiment where you show someone a menu with icons+text, but \"censor\" the text. Icons+text is also superior to [occasionally icons]+text in the same thought experiment. From my perspective, the author just argued against their own preference there.I'd argue that the thought process behind determining an appropriate icon is even more important and relevant when being consistent and enforcing icon+text everywhere, not diminished. It also has the broadest possible appeal (to the visual/graphically focused, to the literary focused, to those who either may not speak the language, and/or to those who are viewing the menu with a condensed/restrictive viewport that doesn't have room for the full text). Now, if the argument is predicated on \"We aren't willing to pay a designer for this\" then yeah, they have a point. Except they used Apple as an example so, doubt that was the premise.reply",
      "After my stroke 3 years ago, I find myself in a place meeting accessibility. So the icons are helpful. I cannot necessarily read the text.reply",
      "Yes, I agree. Maybe if you\u2019re a fast reader icons don\u2019t do much, but for people who are illiterate (20% of America) they figure out how to use tech by memorizing the icons and locations of buttons.reply",
      "> Hey, unless you can articulate a really good reason to add this, maybe our default posture should be no icons in menus?Challenge accepted. If a user (esp. one whose cognition generally prefers visual media) uses a menu item frequently, they can remember its icon and that makes it easier to find in the future.(Doesn't apply to me personally though because I'll instead remember the underlined letter and press it next time. My pet peeve in menus is not icons, but missing or clashing hotkeys.)reply",
      "I always thought menus had icons so they could be matched to the same functionality on the toolbar. If a menu lacks an icon, then it's probably not on the toolbar. This falls apart when there is no toolbar. But I have definitely found an action in the menu, looked at the icon, and matched it to a a button elsewhere.reply",
      "I believe Microsoft Office 97 for Windows was the first time I saw icons next to menu items.  Office 97 had highly customizable menus and toolbars.  Each menu item and toolbar item could be thought of as an action with an icon and a label, and that action could be placed in either a menu or a toolbar.  Not every menu item had an icon associated with it.  Additionally, each icon was colored and was clearly distinct.reply",
      "This is definitely where I would this pattern - MS Office 97\u2019s customizable toolbars necessitated this model where every single thing you could do in the application had an icon.It then got copied into Visual Studio, where making all of the thousands of things you could do and put into custom toolbars or menus have visually meaningful icons was clearly an impossible task, but it didn\u2019t stop Microsoft trying.I assume Adobe, with their toolbar-centric application suite, participated in the same UI cycle.By the time of Office 2007 Microsoft were backing off the completely customizable toolbar model with their new \u2018Ribbon\u2019 model, which was icon-heavy, but much more deliberately so.reply",
      "I believe some programs used to let you even drag menu items to the toolbar.reply",
      "Many KDE apps (Dolphin, Kate, Okular, etc.) let you configure their tool bars (or get rid of them entirely) and set them to show just icons, text, or both (with the text to the side or below). It's the kind of thing most people won't bother with, but for frequently used applications it's nice to be able to customize it to suit your needs. It's done via a config option though, not by dragging menu items to the toolbar (which strikes me as something you could initiate by mistake).reply",
      "MS Office\u2019s fully customisable toolbars, complete with built-in icon editor.\u2026ripped out when the Office Ribbon was introduced in 2007; the now-limited customisation is now considered an improvement because of the IT support problems caused by users messing up their own toolbars.I mean, yes; but that\u2019s what Group Policy is for! And the removal of the icon editor is just being downright mean to bored school kids.reply"
    ],
    "link": "https://blog.jim-nielsen.com/2025/icons-in-menus/",
    "first_paragraph": "I complained about this on the socials, but I didn\u2019t get it all out of my system. So now I write a blog post.I\u2019ve never liked the philosophy of \u201cput an icon in every menu item by default\u201d.Google Sheets, for example, does this. Go to \u201cFile\u201d or \u201cEdit\u201d or \u201cView\u201d and you\u2019ll see a menu with a list of options, every single one having an icon (same thing with the right-click context menu).It\u2019s extra noise to me. It\u2019s not that I think menu items should never have icons. I think they can be incredibly useful (more on that below). It\u2019s more that I don\u2019t like the idea of \u201cgive each menu item an icon\u201d being the default approach.This posture lends itself to a practice where designers have an attitude of \u201cI need an icon to fill up this space\u201d instead of an attitude of \u201cDoes the addition of a icon here, and the cognitive load of parsing and understanding it, help or hurt how someone would use this menu system?\u201dThe former doesn\u2019t require thinking. It\u2019s just templating \u2014 they all have icons, so we need"
  },
  {
    "title": "Jepsen: NATS 2.12.1 (jepsen.io)",
    "points": 270,
    "submitter": "aphyr",
    "submit_time": "2025-12-08T18:51:03 1765219863",
    "num_comments": 103,
    "comments_url": "https://news.ycombinator.com/item?id=46196105",
    "comments": [
      "Every time someone builds one of these things and skips over \"overcomplicated theory\", aphyr destroys them. At this point, I wonder if we could train an AI to look over a project's documentation, and predict whether it's likely to lose commmitted writes just based on the marketing / technical claims. We probably can.reply",
      "/me strokes my long grey beard and nodsPeople always think \"theory is overrated\" or \"hacking is better than having a school education\"And then proceed to shoot themselves in the foot with \"workarounds\" that break well known, well documented, well traversed problem spacesreply",
      "certainly a narrative that is popular among the grey beard crowd, yes. in pretty much every field i've worked on, the opposite problem has been much much more common.reply",
      "What fields? Cargo culting is annoying and definitely leads to suboptimal solutions and sometimes total misses, but I\u2019ve rarely found that simply reading literature on a thorny topic prevents you from thinking outside the box. Most people I\u2019ve seen work who were actually innovating (as in novel solutions and/or execution) understood the current SOTA of what they were working on inside and out.reply",
      "what's the opposite problem statement?reply",
      "People overly beholden to tried and true 'known' way of addressing a problem space and not considering/belittling alternatives. Many of the things that have been most aggressively 'bitter lesson'ed in the last decade fall into this category.reply",
      "Like this bug report?The things that have been \"disrupted\" haven't delivered - Blockchains are still a  scam, Food delivery services are worse than before (Restaurants are worse off, the people making the deliveries are worse off), Taxis still needed to go back and vet drivers to ensure that they weren't fiends.reply",
      "> Blockchains are still a scamDid you actually look at the blockchain nodes implementation as of 2025 and what's in the roadmap?\nEthereum nodes/L2s with optimistic or zk-proofs are probably the most advanced distributed databases that actually work.(not talking about \"coins\" and stuff obviously, another debate)reply",
      "> Ethereum nodes/L2s with optimistic or zk-proofs are probably the most advanced distributed databases that actually work.What are you comparing against? Aren't they slower, less convenient, and less available than, say, DynamoDB or Spanner, both of which have been in full-service, reliable operation since 2012?reply",
      "the big difference is the trust assumption, anyone can join or leave the network of nodes at any timereply"
    ],
    "link": "https://jepsen.io/analyses/nats-2.12.1",
    "first_paragraph": "NATS is a distributed streaming system. Regular NATS streams offer only best-effort delivery, but a subsystem, called JetStream, guarantees messages are delivered at least once. We tested NATS JetStream, version 2.12.1, and found that it lost writes if data files were truncated or corrupted on a minority of nodes. We also found that coordinated power failures, or an OS crash on a single node combined with network delays or process pauses, can cause the loss of committed writes and persistent split-brain. This data loss was caused (at least in part) by choosing to flush writes to disk every two minutes, rather than before acknowledging them. We also include a belated note on data loss due to process crashes in version 2.10.22, which was fixed in 2.10.23. NATS has now documented the risk of its default fsync policy, and the remaining issues remain under investigation. This research was performed independently by Jepsen, without compensation, and conducted in accordance with the Jepsen et"
  },
  {
    "title": "The Lost Machine Automats and Self-Service Cafeterias of NYC (2023) (untappedcities.com)",
    "points": 11,
    "submitter": "walterbell",
    "submit_time": "2025-12-09T00:51:31 1765241491",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=46199950",
    "comments": [
      "The automat is something of a strange echo from my childhood.When I was maybe 5 or so, my mom took my sister and I to Chicago from Kansas City. That train ride in and of itself is something of time capsule in my memory. My sister remembered the glowing handrails (radium?). I remember the lounge car where passengers sipped cocktails and watched the lights at night rush past outside. The women dressing, in my fuzzy recollection, like extras in The Thin Man.Chicago was where I got to buy a pie, or some kind of dessert, from an automat. What a magical thing to give a kid some coins and just tell them to go grab what they like\u2026There are a number of things from my childhood that I came to find later were just gone. (Or obscure now to the point they are essentially gone.) Imagine my delight when the film Dark City featured an automat.There's a documentary called \"The Automat\" [1] that I tracked down just recently\u2014have not yet watched.[1] https://www.imdb.com/title/tt4554690/reply",
      "The examples at the link are mostly from around midcentury.  I'm familiar with a more recent example:  In 2006-2009, there was one of those on St. Mark's Place -- 8th street between 2nd and 3rd.  It was called \"Bamn!\" and IIRC was open 24/7, so it mostly catered to late-night drinkers and partiers.  (It happened to be on one of NYC's few streets that always had lively nightlife, even during the week.)  It was cheap, at something like $1 or $2 for a burger, and it was reasonably good.There's a pic:  https://www.flickr.com/photos/93779577@N00/4235886625It was a fun place.  Wish there were more like it.reply",
      "The H&H style is enchanting. Not sure why but I like it.reply",
      "Art Deco has always been one of the best architectural styles, IMHO. I also like the pseudo-classical Greek design often found in American government buildings in small towns (city hall, the public library, and so on). They're very different styles, yet they complement each other nicely.reply",
      "Yeah, and the font is pretty good!reply"
    ],
    "link": "https://www.untappedcities.com/automats-cafeterias-nyc/",
    "first_paragraph": "\n      Discover which NYC buildings\u2014both lost and extant\u2014have been recreated out of plants!\n    \n      _3xY7ZaIK1Y\n    \n      See this unique Hudson Valley home in all its holiday splendor!\n    \u201cAutomats were right up there with the Statue of Liberty and Madison Square Garden,\u201d Kent L. Barwick, former president of the Municipal Art Society, lamented to the New York Times in 1991 when the country\u2019s last automat closed. The automat, a precursor to today\u2019s fast food chains, was a staple of the New York City dining scene in the first half of the 20th century. Originally conceived in Germany, the self-service restaurant featured coin-operated vending machines from which patrons could buy fresh coffee, simple meals, and desserts for an affordable price.Along with automats, self-service cafeterias changed the way New Yorkers ate and socialized. In her book, Kibbitz and Nosh: When We All Met at Dubrow\u2019s Cafeteria (Three Hills, May 2023), photographer Marcia Bricker Halperin revisits one of New"
  },
  {
    "title": "Strong earthquake hits northern Japan, tsunami warning issued (nhk.or.jp)",
    "points": 262,
    "submitter": "lattis",
    "submit_time": "2025-12-08T14:50:48 1765205448",
    "num_comments": 131,
    "comments_url": "https://news.ycombinator.com/item?id=46192846",
    "comments": [
      "I live in Misawa (https://en.wikipedia.org/wiki/Misawa,_Aomori) and work in Rokkasho (https://en.wikipedia.org/wiki/Rokkasho), which is the area where the earthquake hit the strongest. It was quite violent, apparently the strongest earthquake ever recorded in the region. My house suffered no damage other than a few things falling off the cabinets, and I could sleep soundly afterwards, but lets see today at work.reply",
      "Update: the tsunami warning has been lifted, in the end there was no major damage.reply",
      "Was in a hotel in Sapporo, almost got thrown out of bed. Lot of people in the hotel lobby now.Considering leaving Hokkaido by air if a Hokkaido and Sanriku Subsequent Earthquake Advisory is issued, don't really want to be in a potential megaquake.reply",
      "People were freaking about the July megaquake prophecy and nothing happened. Trying to time it is silly, just chill and enjoy your stay, you'll probably be fine.reply",
      "This is different than the July megaquake prophecy, which was indeed dumb. With a strong quake like this there will be aftershocks. Most will be small, but there is a risk (about 5% according to the USGS) of an even stronger quake than the first within the next week or so [1].I agree the parent will likely be fine, but it can be stressful in the aftermath of a large quake. And if they want to leave the area and have the opportunity to do so calmly and safely, I think that\u2019s justified.[1] https://www.usgs.gov/faqs/what-probability-earthquake-a-fore...reply",
      "Why was the July megaquake, an 8.8 magnitude, a dumb prophecy, but this \"strong quake\" at a magnitude of 7.6 is a smart prophecy?reply",
      "This isn\u2019t something that I personally wish to debate, but I\u2019ll leave link to the wikipedia page for the July 2025 prophecy [1] for anyone who may not know what we are talking about.And also point out that last night\u2019s earthquake in Northern Japan was not a \u201cprophecy\u201d. Just a regular, large earthquake - which do occur pretty frequently in Japan. And I say \"large\" not just because of the magnitude, but because parts of Aomori experienced 6+ shaking on the shindo scale [2] which is categorized as \"brutal\" [3].[1] https://en.wikipedia.org/wiki/July_2025_Japan_megaquake_prop...[2] https://www.data.jma.go.jp/multi/quake/quake_detail.html?eve...[3] https://en.wikipedia.org/wiki/Japan_Meteorological_Agency_se...reply",
      "Strange question.The July megaquake prophecy scare was dumb because it originated in a work of fiction, not intended to be taken seriously by its author and not based on any scientific evidence. If the \"prophecy\" had come true, it'd be by luck alone. fwiw, I'd say it didn't come true; the 8.8 magnitude earthquake was near Kamchatka and didn't actually damage Japan, though a tsunami seemed plausible enough that there was a precautionary evacuation.This \"strong quake\" is a thing that happened, not a \"smart prophecy\" [1]. Talk of aftershocks is not a prophecy either; it's a common-sense prediction consistent with observations from many previous earthquakes.[1] smart prophecy is an oxymoron. A prediction is either based on scientific evidence (not a prophecy) or a (dumb) prophecy.reply",
      "You are certainly reading something into my question that isn't there. I'm genuinely ignorant. I thought you were saying that predictions of a strong aftershock following an M8.8 were dumb, but the same thing following an M7.6 were smart. Is that not the case?Again, sorry if this seemed antagonistic or something, I really am just unsure of what you were saying.reply",
      "A manga book published in 1999 randomly predicted a disaster in March 2011, which seemed to come true with Fukushima. The manga was re-published in 2021 predicting a M8.8 in July 2025, but nothing happened. This is the dumb prophecy part, it was not based on seismology studies, just a shot in the dark to try to seem prophetic again. Countless works of fiction are published every year which predict some future disaster at an arbitrary date. Every once in a while, one of those thousands of random predictions can be interpreted as coming true when something bad happens on that day, which retroactively drives interest in that work of fiction, and less scientific minds believing the author has actual future predicting power beyond the abilities of science.A relatively major (but not M8.8) quake has now hit in December 2025. It is intelligent to expect there may be aftershocks in the days after a significant earthquake actually happens, which can sometimes be larger than the initial quake. This is a well-accepted scientific fact born out of large amounts of data and statistical patterns, not whimsical doomsdayism.Fukushima's M9.0-9.1 was around a 1-in-1000-year scale event. The last time Japan saw such a powerful earthquake was in the 869 AD. It would be reasonable to expect one of that scale to not happen again for another 1000 years.reply"
    ],
    "link": "https://www3.nhk.or.jp/nhkworld/en/news/20251209_02/",
    "first_paragraph": "A magnitude 7.5 earthquake struck northern Japan on Monday. Tsunami advisories have been lifted for the Pacific coastline in northern Japan. But officials have issued an alert for a potential megaquake in northern Japan.The earthquake struck off the eastern coast of Aomori Prefecture at 11:15 p.m. on Monday.The Japan Meteorological Agency has downgraded the magnitude of the quake centered off the Pacific coast in Aomori Prefecture to 7.5 from 7.6.The depth has also been adjusted to 54 kilometers, from an initial estimate of 50 kilometers.Tremors with an intensity of upper 6 on the Japanese intensity scale of 0 to 7 were observed in the city of Hachinohe in Aomori Prefecture.As of 1:00 a.m., six people in Aomori have been injured by either falling down or getting hit by falling objects at their homes.Authorities had issued a tsunami warning for Iwate Prefecture and parts of Hokkaido and Aomori.At Kuji Port in Iwate, a tsunami measuring 70 centimeters was observed. In Hokkaido, a 50-cent"
  },
  {
    "title": "Scientific and Technical Amateur Radio (destevez.net)",
    "points": 11,
    "submitter": "gballan",
    "submit_time": "2025-12-09T00:50:32 1765241432",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://destevez.net/",
    "first_paragraph": "\n\t\t\tScientific & Technical Amateur Radio \u2014 Home of EA4GPZ / M0HXM\t\tESCAPADE is a twin spacecraft mission that will study the Mars magnetosphere. The science mission is led by UC Berkeley Space Sciences Laboratory and the spacecraft buses were built by Rocket Lab. It was launched on November 13 on the second Blue Origin New Glenn mission NG-2. The spacecraft will spend a year around the Earth-Sun L2 Lagrange point before falling back to Earth for a powered gravity assist that will place them on Hohmann transfer orbit to Mars as the \u201claunch window\u201d to Mars opens. These are the first spacecraft to fly this kind of trajectory.The day after launch, I used two antennas from the Allen Telescope Array to record the X-band telemetry signals of the two spacecraft, which were approximately 200 thousand km away from Earth. In this post I will show the results of this observation, and how to decode the telemetry. I have published the recording in the dataset \u201cRecording of ESCAPADE X-band telemetry "
  },
  {
    "title": "AMD GPU Debugger (thegeeko.me)",
    "points": 211,
    "submitter": "ibobev",
    "submit_time": "2025-12-08T16:06:14 1765209974",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=46193931",
    "comments": [
      "Non-AMD, but Metal actually has a [relatively] excellent debugger and general dev tooling. It's why I prefer to do all my GPU work Metal-first and then adapt/port to other systems after that: https://developer.apple.com/documentation/Xcode/Metal-debugg...I'm not like a AAA game developer or anything so I don't know how it holds up in intense 3D environments, but for my use cases it's been absolutely amazing. To the point where I recommend people who are dabbling in GPU work grab a Mac (Apple Silicon often required) since it's such a better learning and experimentation environment.I'm sure it's linked somewhere there but in addition to traditionally debugging, you can actually emit formatted log strings from your shaders and they show up interleaved with your app logs. Absolutely bonkers.The app I develop is GPU-powered on both Metal and OpenGL systems and I haven't been able to find anything that comes near the quality of Metal's tooling in the OpenGL world. A lot of stuff people claim is equivalent but for someone who has actively used both, I strongly feel it doesn't hold a candle to what Apple has done.reply",
      "Yeah, Xcode's Metal debugger is fantastic, and Metal itself is imo a really nice API :]. \nFor whatever reason it clicked much better for me compared to OpenGL.Have you tried RenderDoc for the OpenGL side? Afaik that's the equivalent of Xcode's debugger for Vulkan/OpenGL.reply",
      "My initiation into shaders was porting some graphics code from OpenGL on Windows to PS5 and Xbox, and (for your NDA and devkit fees) they give you some very nice debuggers on both platforms.But yes, when you're stumbling around a black screen, tooling is everything. Porting bits of shader code between syntaxes is the easy bit.Can you get better tooling on Windows if you stick to DirectX rather than OpenGL?reply",
      "> Can you get better tooling on Windows if you stick to DirectX rather than OpenGL?My app doesn't currently support Windows. My plane was to use the full DirectX suite when I get there and go straight to D3D and friends. I lack experience at all on Windows so I'd love if someone who knows both macOS and Windows could compare GPU debugging!reply",
      "Windows has PIX for Windows, PIX is the name of the GPU debugging since Xbox 360. The Windows version is similar but it relies on debug layers that need to be GPU specific which is usually handled automatically. Although because of that it\u2019s not as deep as the console version but it lets you get by. Most people use RenderDoc on supported platforms though (Linux and Windows). It supports most APIs you can find on these platforms.reply",
      "It's a full featured and beautifully designed experience, and when it works it's amazing. However it regularly freezes of hangs for me, and I've lost count of the number of times I've had to 'force quit' Xcode or it's just outright crashed. Also, for anything non-trivial it often refuses to profile and I have to try to write a minimal repro to get it to capture anything.I am writing compute shaders though, where one command buffer can run for seconds repeatedly processing over a 1GB buffer, and it seems the tools are heavily geared towards graphics work where the workload per frame is much lighter. (Will all the AI focus, hopefully they'll start addressing this use-case more).reply",
      "> However it regularly freezes of hangs for me, and I've lost count of the number of times I've had to 'force quit' Xcode or it's just outright crashed.This has been my experience too. It isn't often enough to diminish its value for me since I have basically no comparable options on other platforms, but it definitely has some sharp (crashy!) edges.reply",
      "Same, Metal is a clean and modern API.Is anyone here doing Metal compute shaders on iPad? Any tips?reply",
      "Is your code easy to transfer to other environments? \nThe Apple vendor lock-in is not a great place for development if the end product runs on servers, unlike using AMD Gpus which can be found on the backend.\nSame goes for games because most gamers either have an AMD or an Nvidia graphics card as playing on Mac is still rare, so priority should be supporting those platformsIts probably awesome to use Metal and everything but the vendor lock-in sounds like an issue.reply",
      "It has been easy. All modern GPU APIs are basically the same now unless you're relying on the most cutting edge features. I've found that converting between MSL, OpenGL (4.3+), and WebGPU to be trivial. Also, LLMs are pretty good at it on first pass.reply"
    ],
    "link": "https://thegeeko.me/blog/amd-gpu-debugging/",
    "first_paragraph": "I\u2019ve always wondered why we don\u2019t have a GPU debugger similar to the one used for CPUs. A tool that allows pausing execution and examining the current state. This capability feels essential, especially since the GPU\u2019s concurrent execution model is much harder to reason about. After searching for solutions, I came across rocgdb, a debugger for AMD\u2019s ROCm environment. Unfortunately, its scope is limited to that environment. Still, this shows it\u2019s technically possible. I then found a helpful series of blog posts by Marcell Kiss, detailing how he achieved this, which inspired me to try to recreate the process myself.The best place to start learning about this is RADV. By tracing what it does, we can find how to do it. Our goal here is to run the most basic shader nop 0 without using Vulkan, aka RADV in our case.First of all, we need to open the DRM file to establish a connection with the KMD, using a simple open(\u201c/dev/dri/cardX\u201d), then we find that it\u2019s calling amdgpu_device_initialize, wh"
  },
  {
    "title": "Let's put Tailscale on a jailbroken Kindle (tailscale.com)",
    "points": 229,
    "submitter": "Quizzical4230",
    "submit_time": "2025-12-08T16:34:08 1765211648",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=46194337",
    "comments": [
      "> is available for all but the most up-to-date KindlesBought one from eBay to try it out. Silly me connected it to wifi and suddenly it\u2019s up to date and no longer breakablereply",
      "If you want a cheap rooted eReader I think you're better off getting a Kobo instead, they don't officially support rooting but AFAICT they make basically no effort to prevent it.reply",
      "The latest Kobos use MediaTek SoCs with locked bootloaders. The Kobo Clara BW's MT8113, for example. As far as I know, one of the early bootloaders it, BL1, refuses to execute the next bootloader (BL2) unless its signature is valid. We can get the device into a mode where BL1 waits for upload of a BL2 via USB using an exploit called Kamakiri, but in public there is neither an exploit to get BL1 to boot an arbitrary BL2, nor an authorized BL2 image to upload. See here: https://github.com/bkerler/mtkclient/issues/1332Kobo devices have root exposed but don't let users boot their own kernels (and the kernel they ship was not compiled with kexec either).I really don't know the reason so many devices these days don't have an unlock method. It seems predatory. Who knows where in the chain this happens... maybe it's Kobo, or maybe MediaTek won't sell you their SoCs for mass-market devices unless you lock them.reply",
      "According to the github issue it seems to be a simple checksum step, not a true signature verification? If so there is no locked bootloader in any real sense.If the real impediment is lack of demand or low-level development effort for any given device, that's in principle a solvable issue once projects like pmOS and Mobian choose to focus on some reasonably-available hackable hardware and bring it up to true daily driver state.reply",
      "Can you just access /dev/mem or load a kernel module? Is there a SELinux policy stopping that?If you can do either of those, it should be trivial to get kexec working by just loading it as a module.reply",
      "As far as I know, yes, it's possible. No SELinux. Kernel is a branch from 4.9.something pretty far off mainline with a few proprietary binary blob modules. As far as I know the real impediment here is lack of demand.reply",
      "Older Kobos sound ok though?reply",
      "+1 to a Kobo, they cheaper and better than Kindles, with full Calibre support (https://github.com/kovidgoyal/calibre - OSS which has been in development for ~20 years!).The way you install additional software is literally just moving files into folders whilst its plugged into your computer. I'm sure it could handle Tailscale.reply",
      "I agree with your sentiment that the Kobo is better than the Kindle from an... ethical standpoint, if you have the money for one. However, it is worth noting that Kindles will always be cheaper than Kobo devices [0] due to economies of scale and lockscreen advertisements (removable with jailbreaking). From a pure cost perspective, and assuming the user is technically-minded enough to accomplish the jailbreak, the Kindle is likely always [1] a better deal.[0] as of today, 12/8/25, the \"base model\" Kindle 11th Generation is priced at $109.99 USD, and the respective Kobo Clara BW is $139.99 USD.[1] I say \"likely always\" to cover my bases. To my knowledge Calibre supports Kindle, just not as well as Kobo. That said I have found that the KOreader app is more than powerful enough for my use case (reading my own epubs, using dictionaries, etc.)reply",
      "That doesn't always hold, if you want color e-ink then Kobo is currently the cheaper option.Kindle Colorsoft (7\" 16GB) - $250Kindle Colorsoft (7\" 32GB) - $280Kobo Clara Color (6\" 16GB) - $160Kobo Libra Color (7\" 32GB) - $230The Libra also supports a stylus (sold separately) while the Colorsoft doesn't, that's reserved for the much bigger and pricier Kindle Scribe.reply"
    ],
    "link": "https://tailscale.com/blog/tailscale-jailbroken-kindle",
    "first_paragraph": "\u201cIt\u2019s a rite of passage to run Tailscale on weird devices.\u201dSo writes Mitanshu Sukhwani on his blog, detailing the steps for getting Tailscale onto a jailbroken Kindle. Getting there, and seeing a kindle entry with a satisfying green dot in your Tailscale admin console, takes some doing. But take the trip, and you\u2019ll end up with an e-reader that can run some neat unofficial apps, and is more open to third-party and DRM-free ebooks. And with a Tailscale connection, it\u2019s easier to connect to files and a command line on your underpowered little Linux slab.\u201cFor me, it's the freedom of being able to do anything with the device I own,\u201d Sukhwani writes by email. \u201cWhat I can do with the freedom is a different story.\u201dJailbreaking refers to removing the software restrictions on a device put there by its maker. Getting around these restrictions, typically by gaining \u201croot\u201d or administrative access, allows for accessing operating system internals, running unapproved software, and generally doing mo"
  },
  {
    "title": "Has the cost of building software dropped 90%? (martinalderson.com)",
    "points": 151,
    "submitter": "martinald",
    "submit_time": "2025-12-08T19:00:48 1765220448",
    "num_comments": 305,
    "comments_url": "https://news.ycombinator.com/item?id=46196228",
    "comments": [
      "The cost of writing simple code has dropped 90%.If you can reduce a problem to a point where it can be solved by simple code you can get the rest of the solution very quickly.Reducing a problem to a point where it can be solved with simple code takes a lot of skill and experience and is generally still quite a time-consuming process.reply",
      "Most of software work is maintaining \"legacy\" code, that is older systems that have been around for a long time and get a lot of use. I find Claude Code in particular is great at grokking old code bases and making changes to it. I work on one of those old code bases and my productivity increased 10x mostly due to Claude Code's ability to research large code bases, make sense of it, answer questions and making careful surgical changes to it. It also helps with testing and debugging which is huge productivity boost. It's not about its ability to churn out lots of code quickly: it's an extra set of eyes/brain that works much faster that human developer.reply",
      "I've found this as well. In some cases we aren't fully authorised to use the AI tools for actual coding but even just asking \"how would you make this change\" or \"where would you look to resolve this bug\" or \"give me an overview of how this process works\" is amazingly helpful.reply",
      "This is great. Asking questions of library code is a big pattern of mine too.Here's an example I saw on twitter. Asking an LLM to document a protocol from the codebase:https://ampcode.com/threads/T-f02e59f8-e474-493d-9558-11fddf...Do you think you will be able to capture any of this extra value? I think I'm faster at coding, but the overall corporate project timeline feels about the same. I feel more relaxed and confident that the work can be done. Not sure how to get a raise out of this.reply",
      "For me, as a remote developer, it means I'm able to finish my work in 1 hour instead of 8 hours. So I'm able to capture \"extra value\" in the form of time. In our team everyone uses GitHub Copilot and I use Claude Code. My teammates' productivity increased slightly but my productivity increased a lot. This is because 1. Claude Code is just a better coding agent 2. I invested time to get good at agentic coding. Eventually Copilot will catch up and management will realize that now 1 developer can do what previously would take a whole team.reply",
      "I'm really curious on what your role is, and which industry are you in? I'm awed by these productivity gains others report, but I feel like AI helps in such a small part of my job (implementing specific changes as I direct).Agentic workflows for me results in bloated code, which is fine when I'm willing to hand over an subsystem to the agent, such as a frontend on a side project and have it vibe code the entire thing. Trying to get clean code erases all/most of my productivity gains, and doesn't spark joy. I find having a back-end-forth with an agent exhausting, probably because I have to build and discard multiple mental models of the proposed solution, since the approach can vary wildly between prompts. An agent can easily switch between using Newton-Raphson and bisection when asked to refactor unrelated arguments, which a human colleague wouldn't do after a code review.reply",
      "I've come to the same conclusion: If you just want a huge volume of code written as fast as possible, and don't care about 1. how big it is, 2. how fast it runs, 3. how buggy it is, 4. how maintainable or understandable it is, or 5. the overall craftsmanship and artistry of it, then you're probably seeing huge productivity gains! And this is fine for a lot of people and for a lot of companies: Quality really doesn't matter. They just care about shitting out mediocre code as fast as possible.If you do care about these things, it will take you overall longer to write the code with an LLM than it would by hand-crafting it. I started playing around with Claude on my hobby projects, and found it requires an enormous amount of exhausting handholding and post-processing to get the code to the point where I am really happy with it as a consistent, complete, expressive work of art that I would be willing to sign my name to.reply",
      ">shitting out mediocre code as fast as possible.This really is what businesses want and always have wanted.  I've seen countless broken systems spitting out wrong info that was actively used by the businesses in my career, before AI.  They literally did not want it fixed when I brought it up because dealing with errors was part of the process now in pretty much all cases.  I don't even try anymore unless I'm specifically brought on to fix a legacy system.>that I would be willing to sign my name to.This right here is what mgmt thinks is the big \"problem\" that AI solves.  They have always wanted us to magically know what parts are \"good enough\" and what parts can slide but for us to bear the burden of blame.  The real problem is same as always bad spec.  AI won't solve that but it will in their eyes remove a layer in their poor communication.  Obviously no SWE is going to build a system that spit out wrong info and just say \"hire people to always double check the work\" or add it to so-so's job duties to check, but that really is the solution most places seem to go with by lack of decision.Perhaps there is some sort of failure of SWE's to understand that businesses don't care.  Accounting will catch the expensive errors anyway.  Then Execs will bull whip middle managers and it will go away.reply",
      "> Perhaps there is some sort of failure of SWE's to understand that businesses don't careI think it's an engineer's nature to want to improve things and make them better, but then we naively assume that everybody else also wants to improve things.I know I personally went through a pretty rough disillusionment phase where I realised most of the work I was asked to do wasn't actually to make anything better, but rather to achieve some very specific metrics that actually made everything but that metric worse.Thanks to the human tendency to fixate on narratives, we can (for a while) trick ourselves into believing a nice story about what we're doing even if it's complete bunk. I think that false narrative is at the core of mission statements and why they intuitively feel fake.",
      "The adversarial tension was all that ever made any of it work.The \"Perfectionist Engineer\" without a \"Pragmatic Executive\" to press them into delivering something good enough would of course still been in their workshop, tinkering away, when the market had already closed.But the \"Pragmatic Executive\" without the \"Perfectionist Engineer\" around to temper their naive optimism would just as soon find themselves chased from the market for selling gilded junk.You're right that there do seem to be some execs, in the naive optimism that defines them, eager to see if this technology finally lets them bring their vision to market without the engineer to balance them.We'll see how it goes, I guess.reply"
    ],
    "link": "https://martinalderson.com/posts/has-the-cost-of-software-just-dropped-90-percent/",
    "first_paragraph": "I've been building software professionally for nearly 20 years. I've been through a lot of changes - the 'birth' of SaaS, the mass shift towards mobile apps, the outrageous hype around blockchain, and the perennial promise that low-code would make developers obsolete.The economics have changed dramatically now with agentic coding, and it is going to totally transform the software development industry (and the wider economy). 2026 is going to catch a lot of people off guard.In my previous post I delved into why I think evals are missing some of the big leaps, but thinking this over since then (and recent experience) has made me confident we're in the early stages of a once-in-a-generation shift.I started developing just around the time open source started to really explode - but it was clear this was one of the first big shifts in cost of building custom software. I can remember eye watering costs for SQL Server or Oracle - and as such started out really with MySQL, which did allow you "
  },
  {
    "title": "Hunting for North Korean Fiber Optic Cables (nkinternet.com)",
    "points": 210,
    "submitter": "Bezod",
    "submit_time": "2025-12-08T16:38:08 1765211888",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=46194384",
    "comments": [
      "My understanding is that there are three mobile networks in North Korea: the normal one used by the citizens (they have smartphones made specifically for North Korea), one used by the government/military and one for tourists (requires a local SIM card only available in a specific hotel in Pyongyang).The last one is connected to the internet and this is why you can see (or at least before the pandemic could see) Instagram posts from North Korea.I have no idea if this information is still or ever was completely true though.There's a somewhat dated but very interesting AMA on Reddit by an American teaching computer science in Pyongyang:https://www.reddit.com/r/IAmA/comments/1ucl11/iama_american_...Reading about the internet knowledge possessed by North Korean students, I'm always surprised how they supposedly also manage to be some of the most cunning and evil actors when it comes to hacking.reply",
      "Probably helps that the stance is likely \"Hack this target or your family dies\". That's always pretty uhhhh motivational.reply",
      "Re: \"I'm always surprised how they supposedly also manage to be some of the most cunning and evil actors when it comes to hacking.\"I sort of suspect this is just the result of a nation state that is willing to be a pariah. That is, I think nearly any large state could do it if they didn't mind burning bridges.reply",
      "It\u2019s not just that they don\u2019t care about being a pariah state, it\u2019s a literal fund raising exercise, unlike most other state sanctioned hacking.reply",
      "[flagged]",
      "How cunning and evil it is that America funded the internet and then allowed it to spread around the world.If you're worried about \"absolute control over digital systems\", notice how many standards get published describing how those digital systems work -- you're welcome to reimplement them if you'd like more control.reply",
      "The Roman Empire built lots of roads wherever they went and the British Empire built lots of rail networks.reply",
      "What I'm saying is this: there's nothing stopping you from using communication methods that aren't controlled by Americans. All of the protocols that the internet uses are documented.reply",
      "This is exactly what China and North Korea do shrug but they get a lot of criticism for it.reply",
      "The Roman Empire merely improved roads in many places. Gaul already had a road system, and the Greek and Egyptian spheres did too.reply"
    ],
    "link": "https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/",
    "first_paragraph": "Before we go any further, one thing that I want to make clear is that the word assume is going to be doing some heavy lifting throughout this post. This was a rabbit hole that I recently went down and I probably have more questions than answers, but I still wanted to document what I had found so far. If you have additional information or findings you want to share, as always feel free to reach out: contact@nkinternet.com.It all started with a PowerPoint that I came across a few weeks ago. It was presented by the DPRK to the ICAO on the state of their aviation industry and their ADS-B deployment inside North Korea. However, one slide in particular caught my eye because it showed a fiber optic cable running across the countryThis got me wondering more about the physical layout of the network inside North Korea. From the map we know that there\u2019s a connection between Pyongyang and Odaejin, although given the mountains in the middle of the country it probably isn\u2019t a direct link. There isn\u2019"
  },
  {
    "title": "IBM to acquire Confluent (confluent.io)",
    "points": 336,
    "submitter": "abd12",
    "submit_time": "2025-12-08T13:43:59 1765201439",
    "num_comments": 272,
    "comments_url": "https://news.ycombinator.com/item?id=46192130",
    "comments": [
      "Confluent was trading at less than 50% of its IPO price when IBM made the offer. The stock and the company has been going sideways for several years now, keeps growing revenues but loses even more as most of it is in Sales and Marketing. In which world is this seen as some sort of extraordinary company that will get sabotaged by IBM. Seems Confluent management knows the writing on the wall, IBM will clean up (fire a bunch of sales and management guys) and make this a workable business. It will seem brutal for some Confluent guys but that's because their business is broken; and only someone from outside can come in and fix it as the current senior management cannot.IBM has been around for over a hundred years, maybe they know a thing or two about running a software business :-)reply",
      "This is so fascinating to me. I mean how IBM keeps taking over other companies, but they consistently deliver low quality/bottom-tier services and products. Why do they keep doing the same thing again and again? How are they generating actual revenue this way?Ok, so does anyone remember 'Watson'? It was the chatgpt before chatgpt. they built it in house. Why didn't they compete with OpenAI like Google and Anthropic are doing, with in-house tools? They have a mature PowerPC (Power9+? now?)setup, lots of talent to make ML/LLMs work and lots of existing investment in datacenters and getting GPU-intense workloads going.I don't disagree that this acquisition is good strategy, I'm just fascinated (Schadenfreude?) to witness the demise of confluent now. I think economists should study this, it might help avert larger problems.reply",
      "Watson was a marketing exercise designed to sell a bunch of disconnected text and image processing libraries pulled together by consulting services. It did not function as advertised.At one point we worked with a large energy company that was basically sold something LLM-like (large-scale indexing and searching/querying of documents) in 2016 or so. IBM had a team of 90 people doing full-time data ingestion for something like 26,000 documents. We got asked to do a counter-product in two weeks, which was literally just a TF-IDF search and some smarts around ingesting different types of documents. Both solutions performed approximately equally, except one cost something in the order of $185m and one cost $40k. Watson continued running for about a year until an external data science contractor realised they could query Watson for highly confidential board meeting notes, and it would provide full previews into the documents. The project was shuttered shortly after.Alas, nobody gets fired for hiring IBM.reply",
      "> they consistently deliver low quality/bottom-tier services and productsI worked with IBMers. The main priority for a lot of them is to ensure continuous employment for themselves and their buddies. They'd add unnecessary complexity to a product to stretch out the development for another couple of years. And they work at leisure pace for tech. Actual 9 to 5, many coffee breaks. They can't compete.reply",
      "> And they work at leisure pace for tech. Actual 9 to 5, many coffee breaks.Ultra-based. We should all be so lucky.reply",
      "You mean you DONT work a leisurely 6-8 hour day with breaks? I thought everybody did that until there was some urgent firefightingreply",
      "\"Actual 9 to 5\", meaning the standard 40 hour work week?If someone is telling you to work more than 40 hours a week in a salaried position, and they're not paying out the nose, you're being scammed.reply",
      "They will die happy knowing they did more than just create shareholder value.reply",
      "I worked with IBM several decades ago for a customer project, and the solution suggested by an IBM'er for backing up a NoSQL database (Lotus Notes) on a daily basis was to translate and migrate the data to a relational one (DB2), then use a DB2 tape backup system to back it up.When I pointed out that this was a stupid way to do it, they openly told me that they just wanted to sell DB2.reply",
      "The way you put it, looks like IBM is a pretty good place to work atreply"
    ],
    "link": "https://www.confluent.io/blog/ibm-to-acquire-confluent/",
    "first_paragraph": "Hands-on Workshop: Implementing Stream Processing with Apache Flink\u00ae | Register NowLogin\nContact SalesLearn more about how Confluent differs from Apache KafkaDiscover the platform that is built and designed for those who buildUnlock the value of data across your businessExplore testimonials and case studies from Confluent's customersFind a partner or explore our partner programsBuild real-time data architecturesEasily integrate your data ecosystemDemocratize access to high-quality dataTransform, analyze, and act on real-time dataTopics to tables in a few clicksReal-time, context-aware AIPower intelligent, real-time automationChoose Your DeploymentStream smarter with our fully managed, cloud-native Apache Kafka\u00ae serviceRun and manage our complete data streaming platform on-premisesBridge on-premise control with the automation of a cloud serviceDeploy a Kafka-compatible data streaming platform in your private cloudWherever you are in your data streaming journey, you'll find the explainer"
  },
  {
    "title": "Show HN: Fanfa \u2013 Interactive and animated Mermaid diagrams (fanfa.dev)",
    "points": 58,
    "submitter": "bairess",
    "submit_time": "2025-12-04T13:16:54 1764854214",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=46147329",
    "comments": [
      "Nice! I always looked for a solution to animate diagrams as it would help a lot in visualising the workflow.Feedback:1. I tried different mermaid diagrams from https://mermaid.live/, and your animation is only working with classes and flowcharts. It didn't work with the sequence diagram (which is the most interesting to me).2. It would be great to control the animation to be a sequence instead of one animation for all arrows at once. What I would like to do is show fellow devs the workflow from start to finish, according to the spec.I appreciate that this is just a start, but it looks promising and has great potential. Good luck!reply",
      "Thanks for the valuable feedback. I already cover most of the issues that you described. Should be good to go in the next releasereply",
      "I don't understand how to use this, but it does touch on an interesting topic. I want to create interactive and animated diagrams. I normally use either Draw.io or plantuml. My goal is to better teach folks about the systems I'm building, through better visualizations.  Something like IcePanel (which is way too expensive) sort of shows flows, but I'd like to have full control. Does this tool claim to support something like that? If not, are there options out there that I don't know about?reply",
      "I don't see anything animated except a bit on load? I don't get it. This is on iOS Safari.reply",
      "Will take a look let me know the details pleasereply",
      "I don't understand who in the world needs animated something on his diagrams.reply",
      "There's been times in the past when I've had nontechnical stakeholders present for otherwise technical meetings. A little bit of colour and a little bit of animation can go a long way to helping people who may be less familiar with this type of diagram to understand it better.All the better if I can take an existing diagram and just spruce it up slightly for presentation.reply",
      "Can\u2019t tell from that link. Is there a better layout engine?reply",
      "Is there a way to control the mechanics of the animation? I poked around a bit and didn't see a way to configure what was sent down the arrows or how often - just some theming options.For example, if you're visualizing a user flow, you might want rules about when new \"objects\" are sent down the pipe (example node rule: wait until received one item from each input), or how fast they travel, etc.reply",
      "There will more a powerful controls for each node soon. For now there is a \"Director mode\" to control speed and traffic.reply"
    ],
    "link": "https://fanfa.dev/",
    "first_paragraph": ""
  },
  {
    "title": "Kroger acknowledges that its bet on robotics went too far (grocerydive.com)",
    "points": 64,
    "submitter": "JumpCrisscross",
    "submit_time": "2025-12-08T23:53:52 1765238032",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=46199411",
    "comments": [
      "Sounds like they just put them in the wrong places.> Fenyo added that Kroger\u2019s decision to locate the Ocado centers outside of cities turned out to be a key flaw.> \u201cUltimately those were hard places to make this model work,\u201d said Fenyo. \u201cYou didn\u2019t have enough people ordering, and you had a fair amount of distance to drive to get the orders to them. And so ultimately, these large centers were just not processing enough orders to pay for all that technology investment you had to make.\u201dreply",
      "But I think in the cities Kroger grocery stores serve as the fulfilment centers, so they don't need robotic ones.There's probably still room for automation, but it might have to be different than warehouse automation.reply",
      "It depends on your business model.If a basket of groceries brought online costs $15 more than the in-store prices, then you can pick in-store profitably, very easy. That's the instacart model.But if a basket of groceries brought online costs about the same as buying in-store? With the retailer bearing the costs of picking, packing and delivery instead of the customer?Well then you need something more efficient than a store.reply",
      "Even $15 more isn\u2019t enough on account of delivery time, transpo costs, driver time, picking items, and bagging. Current model is for drivers to subsidize by being tricked into taking unprofitable orders.reply",
      "If Kroger operates the same was as Ocado does in the UK, then the drivers are paid by the hour, with the company providing the van and fuel.Agree a lot of modern delivery businesses involve \"self-employed\" drivers getting paid a pittance and using their own vehicle and fuel, though.reply",
      "From what I've seen, for grocery the model is they'll give you the least desirable or near expired stock that the walk-in customers won't grab.  So they're basically saving spoilage.  This happens so reliably I'm absolutely convinced this is how they 'pay' for it without raising prices.I've also noticed this with hardware stores like Lowes.  If I place a pickup order they more often than not will pawn off on me their broken, returned, or even used and damaged stock.  Items like building wrap will have soil and rips on it, concrete mix will be spoiled from moisture, lumber will be all the most warped pieces (if you don't order a whole pallet, expect every last piece will be knotted to hell, split, twisted, and badly warped), etc etc.  It's like clockwork, even if the stock sitting on the shelf doesn't have these problems.  Due to this there are some stores I will never do a pickup/delivery order from.reply",
      "I had a wonderful retro futuristic dream about an automated Costco warehouse a few weeks ago. It was one of the less weird dreams so I still remember it clearly.Basically, each section is like a closed areas with some windows. Customers order at the computers by the windows and flash their membership cards. Robots glide left and right to move 10 samples to the customer, in an arm with rotating clips. Customers can press a button to rotate the samples, observe them, and place an order by pressing a button. Samples not chosen are temporarily stocked at the window as a \u201cstack\u201d.In each closed section, there are humans who monitors and maintains the robots, and occasionally fetch samples when robots stop working (hopefully it too often, you know those 9s).At the exit, a human worker assembles the packages and hand them to the customers with a smile.   Customers have a last chance to return unwanted items.Why was it a retro futuristic dream? Because the customers have the option to go into a bakery to enjoy a cup of coffee/tea, some cake and socialize with fellow customers. All of them looked like the men and women from advertisement from Fallout 4.I\u2019d like to shop or even help build one of these.reply",
      "Sounds like the old general store model, you didn\u2019t browse yourself, the shop keep would bring out what you wanted, it was always behind the counter. I experienced this in China when I started visiting in 1999/early 2000s, it\u2019s mostly not like that anymore though. You still have department stores where you need to buy things first before touching them, though.reply",
      "Had a large-format (for its time) chain store in Canada like that until 1996: https://www.tvo.org/article/what-happened-to-consumers-distr...Basically a catalogue store without shipping to your door.reply",
      "Sounds like a lot of waiting around, versus just browsing the aisles. Maybe today\u2019s consumers need to rediscover cash-and-carry, though.reply"
    ],
    "link": "https://www.grocerydive.com/news/kroger-ocado-close-automated-fulfillment-centers-robotics-grocery-ecommerce/805931/",
    "first_paragraph": ""
  },
  {
    "title": "Microsoft increases Office 365 and Microsoft 365 license prices (office365itpros.com)",
    "points": 244,
    "submitter": "taubek",
    "submit_time": "2025-12-08T13:49:21 1765201761",
    "num_comments": 286,
    "comments_url": "https://news.ycombinator.com/item?id=46192186",
    "comments": [
      "Here in NZ, pretty much all medium/large businesses and govt departments have gone all-in with M365. Most govt departments are on the E5 licence, and have also started to roll out the Copilot licences too.The cost and complexity and the effort required to switch away from M365 is massive. It's not just using a different version of Excel and Word - that's the least of the issues. It's all the data stored in SharePoint Online, the metadata, permissions, data governance, etc. It's the Teams meetings, voice calls, chats and channels. All the security policies that are implemented with Entra and Defender. All the desktop and mobile management that is done through Intune. And the list just goes on and on.Microsoft bundles so many things with M365, that when you're already paying for an E5 licence for each user, it makes financial sense to go all-in and use as much as possible.Take a look at the full feature list to get an idea of what's included: https://www.microsoft.com/en-nz/microsoft-365/enterprise/mic...And of course, the more you consume, the harder it is to get out...reply",
      "> The cost and complexity and the effort required to switch away from M365 is massive.I'd say further to that is there literally isn't a similar product that exists to switch to.  Nobody has developed a real alternative.  It seems like most companies are more than willing to leave this entire market to Microsoft.reply",
      "> Nobody has developed a real alternative. It seems like most companies are more than willing to leave this entire market to Microsoft.I'd say it's more that this is the actual \"developer shortage\" that was being talked about a decade ago, but everyone mistakenly and stupidly interpreted it to be a shortage of tech workers for the larger firms. The number of humans that are literate enough in business, marketing, communications, and software development to pull this off are extremely few and far between right now. And even then, I just listed four specialties that historically have been specialized by a single person for each field - something like this would require a given person having a sufficient breadth of knowledge in all of them at the same time. It's a very tall order.And that's all just to compete on Windows. Adding Mac and Linux into the mix makes it even harder.reply",
      "There\u2019s plenty of developer talent. You don\u2019t see microsoft office competitors because it\u2019s a bad business to start. \u201cRemake microsoft office suite, but cheaper\u201d won\u2019t work. I\u2019m sure dozens of people have tried.reply",
      "> And that's all just to compete on Windows. Adding Mac and Linux into the mix makes it even harder.Cross-platform compatibility is trivial with modern tooling IMO.reply",
      "I wrote a blog post about this.  There is literally no end to the amount of software that could be produced for businesses.  My job right is to write software for particular niche; we purchase all the major software and yet I will still never run out of software to build internally.Literally everything sucks right now because all industries are running a massive software deficit.  It's just not possible (and maybe not economical viable) to build enough software to make everything not suck.  We are making do with the scraps we have.reply",
      "Do you think that LLMs will do much to help to alleviate this?reply",
      "I just did a major refactor of a project to move it many versions up on a framework and whole process was effectively vibe coded.  I'd estimate I did in a couple of days what would have taken a couple of weeks.That's good and expect that could be shaved down even more.  I was spending most of time just waiting for it do the work.But I don't know if that fundamentally changes the situation or not.  We've had steady improvements in developer technology for decades.  Even pre-LLM, I'm building significantly more complicated applications now in less time than ever before.  But as quickly as our developer technology improved, the demands on applications we build has gone up.  I'm not sure even LLMs can outpace the demand for software.reply",
      "I think it is economically viable but we as devs have to realize our true worth here beyond just a paycheck.reply",
      "Link to blog post? Didn\u2019t see it on quick look at your site.reply"
    ],
    "link": "https://office365itpros.com/2025/12/08/microsoft-365-pricing-increase/",
    "first_paragraph": "Table of ContentsOn December 4, 2025, Microsoft announced a range of price increases for Microsoft 365 monthly licenses. The new pricing (Figure 1) goes into effect from July 1, 2026, the start of Microsoft\u2019s FY27 fiscal year.According to Microsoft, they want to \u201cgive customers ample time to plan.\u201d However, there\u2019s not much choice for tenants if their operations are embedded in the Microsoft 365 ecosystem, so this is a case of \u201cgetting used to new pricing\u201d rather than \u201chaving time to consider migrating away from Microsoft 365.\u201d Once you\u2019re embedded in the Microsoft 365 ecosystem, it\u2019s hard to leave.Some organizations do consider going back to on-premises servers. It\u2019s certainly an option, even to the now available and oddly named Microsoft 365 Local, a product that shares precisely nothing but its name with the rest of the Microsoft 365 ecosystem.Microsoft last increased Microsoft 365 license prices in March 2022. At the time, Microsoft added $3/monthly to Office 365 E3m and E5, and $4"
  },
  {
    "title": "Trials avoid high risk patients and underestimate drug harms (nber.org)",
    "points": 59,
    "submitter": "bikenaga",
    "submit_time": "2025-12-08T19:07:59 1765220879",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=46196308",
    "comments": [
      "It's understandable that unusual patients are seen as confounding variables in any study, especially those with small numbers of patients.  Though I haven't read beyond the abstract, it also makes sense that larger studies  (phase 3 or 4) should not exclude such patients, but perhaps could report results in more than one way -- including only those with the primary malady as well as those with common confounding conditions.Introducing too many secondary conditions in any trial is an invitation for the drug to fail safety and/or efficacy due to increased demands on both.  And as we all know, a huge fraction of drugs fail in phase 3 already.  Raising the bar further, without great care, will serve neither patients nor business.reply",
      "Having been an \"investigator\" in a few phase 3 and 4 trials, it is true that all actions involving subjects must strictly follow protocols governing conduct of the trial. It is extremely intricate and labor intensive work. But the smallest violations of the rules can invalidate part of or even the entire trial.Most trials have long lists of excluded conditions. As you say, one reason is reducing variability among subjects so effects of the treatment can be determined.This is especially true when effects of a new treatment are subtle, but still quite important. If subjects with serious comorbidities are included, treatment effects can be obscured by these conditions. For example, if a subject is hospitalized was that because of the treatment or another condition or some interaction of the condition and treatment?Initial phase 3 studies necessarily have to strive for as \"pure\" a study population as possible. Later phase 3/4 studies could in principle cautiously add more severe cases and those with specific comorbidities. However there's a sharp limit to how many variations can be systematically studied due to intrinsic cost and complexity.The reality is that the burden of sorting out use of treatments in real-world patients falls to clinicians. It's worth noting level of support for clinicians reporting their observations has if anything declined over decades. IOW valuable information is lost in the increasingly bureaucratic and compartmentalized healthcare systems that now dominate delivery of services.reply",
      "This could at least be done after release, but I don\u2019t think any incentives are there, while collecting the data is incredibly difficultreply",
      "Abstract: \"The FDA does not formally regulate representativeness, but if trials under-enroll vulnerable patients, the resulting evidence may understate harm from drugs. We study the relationship between trial participation and the risk of drug-induced adverse events for cancer medications using data from the Surveillance, Epidemiology, and End Results Program linked to Medicare claims. Initiating treatment with a cancer drug increases the risk of hospitalization due to serious adverse events (SAE) by 2 percentage points per month (a 250% increase). Heterogeneity in SAE treatment effects can be predicted by patient's comorbidities, frailty, and demographic characteristics. Patients at the 90th percentile of the risk distribution experience a 2.5 times greater increase in SAEs after treatment initiation compared to patients at the 10th percentile of the risk distribution yet are 4 times less likely to enroll in trials. The predicted SAE treatment effects for the drug's target population are 15% larger than the predicted SAE treatment effects for trial enrollees, corresponding to 1 additional induced SAE hospitalization for every 25 patients per year of treatment. We formalize conditions under which regulating representativeness of SAE risk will lead to more externally valid trials, and we discuss how our results could inform regulatory requirements.\"reply",
      "This seems like an odd criticism.First off it ignore the fact that if you include frail patients you\u2019ll confound the results of the trial.  So there is a good reason for it.Second, saying \u201crate of SAE is higher than rate of treatment effect\u201d is a bit silly considering these are cancer trial - without treatment there is a risk of death so most people are willing to accept SAE in order to achieve treatment effect.Third, saying \u201cthe sickest patients saw the highest increase in SAE\u201d seems obvious?  It\u2019s exactly what you\u2019d expect.reply",
      "First, ignoring frail patients means your trial isn't representative of the wider population, so it shouldn't be accepted for general use - only on people who were well-represented in the trial.Second, you're ignoring the possibility of other treatment options. It isn't always the binary life-or-death you're making it, so SAEs do matter.Third, a big part of trials is to discover and develop prevention methods for SAEs. Explicitly ignoring the people most likely to provide data valuable for the general population sounds like a pretty silly approach.reply",
      "> Second, you're ignoring the possibility of other treatment options. It isn't always the binary life-or-death you're making it, so SAEs do matter.A common reason for a drug (especially a cancer drug) going to trial is because other options have already failed. For example CAR-T therapies are commonly trialed on patients with R/R (relapsed/refractory) cohorts.https://www.fda.gov/regulatory-information/search-fda-guidan...> \"In subjects who have early-stage disease and available therapies, the unknown benefits of first-in-human (FIH) CAR T cells may not justify the risks associated with the therapy.\"reply",
      "But you\u2019re stating the obvious?  It\u2019s not like physicians don\u2019t know trials are designed this way, and for good reasons.Frail patients confound results.  A drug may work great, but you\u2019d never know because your frail patients die for reasons unrelated to the drug.Second is obvious as well.  Doctors know there are treatment alternatives (with the same drawback to trial design).And I already touched on your third point.  The alternative to excluding frail patients is not being able to tell if the drug does anything.  In many cases that means the drug isn\u2019t approved.Excluding frail patients has its drawbacks, but it has benefits as well.  This paper acts like the benefits don\u2019t exist.reply",
      "I've personally been excluded from several depression clinical trials for having suicidal ideations, it makes me wonder just what kind of \"depression\" they are testing drugs on.reply",
      "Be strong, brother, there is hope. Antidepressant can be really hard to administer, they exclude particularly vulnerable people from trials because they need to be protected the most.reply"
    ],
    "link": "https://www.nber.org/papers/w34534",
    "first_paragraph": "\nThe FDA does not formally regulate representativeness, but if trials under-enroll vulnerable patients, the resulting evidence may understate harm from drugs. We study the relationship between trial participation and the risk of drug-induced adverse events for cancer medications using data from the Surveillance, Epidemiology, and End Results Program linked to Medicare claims. Initiating treatment with a cancer drug increases the risk of hospitalization due to serious adverse events (SAE) by 2 percentage points per month (a 250% increase). Heterogeneity in SAE treatment effects can be predicted by patient's comorbidities, frailty, and demographic characteristics. Patients at the 90th percentile of the risk distribution experience a 2.5 times greater increase in SAEs after treatment initiation compared to patients at the 10th percentile of the risk distribution yet are 4 times less likely to enroll in trials. The predicted SAE treatment effects for the drug's target population are 15% la"
  },
  {
    "title": "Microsoft Download Center Archive (legacyupdate.net)",
    "points": 116,
    "submitter": "luu",
    "submit_time": "2025-12-05T19:33:28 1764963208",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=46166178",
    "comments": [
      "PowerToys (https://legacyupdate.net/download-center/powertoys) used to be on my \"first software to install\" list on a new machine. Between Tweak UI and Deskman, you could _almost_ get a minimal X Windows-like UI. Get those set up and add on LiteStep (http://litestep.net/) and you were pretty much good to go, with the exceptions of the kernel, network stack, and CLI toolset, of course.reply",
      "The memories! Back in the day I couldn't install XP anywhere without bringing at lease TweakUI and CmdHere, and probalby some others I've forgotten.reply",
      "Wow, somehow I entirely missed that Windows XP versions were made. I relied pretty heavily on the Windows 95 PowerToys when I used Win95.Deskman seems like it'd be awesome.reply",
      "They still make it.  Tons of great utils for Win10/11 (I'm off of 11 cuz fuck that noise, but when I was on win10 I used it).Here's the link to the current version for modern systems:\nhttps://learn.microsoft.com/en-us/windows/powertoys/reply",
      "This is great! But, it feels like it's only a matter of time before it changes ownership and everything is re-bundled with malware. It sucks that I can't get old downloads but it would be nice if they came from official sources. I don't have a solution. But looking for old drivers etc, mostly leads to bad sources.reply",
      "Legacy Update has been well-supported for over 3 years and takes donations via Github (11 current, 61 past sponsors) and Patreon (where you can sponsor up to $80 to fuel Adam's 3D printing addiction). I recommend it to our PortableApps.com users who are on older operating systems and use it in my virtual machines for testing our releases. I'm hopeful it'll stay as is for a while.reply",
      "Hey its awesome but regarding donations since I actually wanted to talk about it.But can you please look at adding yourself/Download Center archive to liberapay too as I was hoping to find liberapay.You mention having kofi being the lowest prices but I think Liberapay has no fees other than payment processing and is itself an non profit and funded via donations.Maybe then you would have \"too many options to donate\" but I think liberapay can be a good option to have honestly imo and I am interested to hear your thoughts about it.Also I wanted to download windows 7 iso to run a simpler thing on my pc but Microsoft being shitty removed the download link of it and everything so great to see your project, Going to bookmark it right now and thank you!reply",
      "OpenCollective is another good alternative, that use the same means to fund themselves as they're offering projects to use, compared to the GitHub/Microsoft way of doing things.reply",
      "Oh yea, forgot about OpenCollective but its good too and I think can give legal way to get fiscal sponsorship/basically be treated as a non profit/get legal donation method as well which can be nice for this project and all benefits that get with it. He can check out OpenCollective too!reply",
      "Shouldn't the files be signed by Microsoft, with a timestamp signature? That should (barring somebody locating a relevant private key) still mark them as not having been modified.Of course, how many people would know to check for the signature (especially in the case the site went malicious and therefore wouldn't tell you to do so) would be a different question\u2026reply"
    ],
    "link": "https://legacyupdate.net/download-center/",
    "first_paragraph": "Welcome to Legacy Update\u2019s archive of the Microsoft Download Center.Over the years, Microsoft has deleted thousands of downloads for prior versions of Windows, Office, Visual Studio, SQL Server, and more. These downloads include ones for Windows 95, 98, Me, NT 4.0, 2000, XP, Vista, and 7, old versions of runtimes such as the .NET Framework, Visual C++ Redistributable, and DirectX, and freeware tools such as Microsoft Virtual PC and Microsoft SQL Server, among countless other Microsoft no longer supports.Legacy Update has catalogued the Microsoft Download Center between 2012 and 2024. You can browse the archive below, or use the search box to find a specific download.Deleted downloads are no longer supported by Microsoft, and may have known security vulnerabilities. After installing, check for updates using Legacy Update.We are expanding our Microsoft Download Center archive with content deleted between 2012 and 2025. This adds many previously popular downloads such as Office Viewers, W"
  },
  {
    "title": "Paramount launches hostile bid for Warner Bros (cnbc.com)",
    "points": 230,
    "submitter": "gniting",
    "submit_time": "2025-12-08T14:16:34 1765203394",
    "num_comments": 221,
    "comments_url": "https://news.ycombinator.com/item?id=46192459",
    "comments": [
      "All I can say is welcome back to torrenting. This perpetual \"same shit deal for consumers, different corporation\" problem doesn't end until copyright kicks the media into public domain. Until then, you can play their content reindeer games[1] or you can download a copy of Reindeer Games[2] and watch it without worrying about ownership foofaraw.[1] https://www.dictionary.com/e/slang/reindeer-games[2] predb.mereply",
      "A plague on both your houses.reply",
      "Does WB have to pay the breakup fee to Netflix if a Paramount hostile takeover succeeds?reply",
      "It looks like it. $2.8bn by Warner Brothers to Netflix [1].If the vote looks close, Paramount would be expected to raise their bid to cover that cost.[1] https://www.sec.gov/Archives/edgar/data/1065280/000119312525... 8.3(a)reply",
      "The failed merger and similar clawback clause between Kroger and Albertsons is currently destroying a significant part of the supply chain for food in the Pacific Northwest. Grocery stores that have been open for 50-75 years - stores where whole neighborhoods and towns were built around - are closing forever, leaving those areas as food deserts.Either way, this entertainment merger is going to get ugly. Consumers are absolutely going to get harmed either way with that clawback clause.reply",
      "Except you need food to live and tv shows are an artificially scarce resource that's actually free to distribute in unlimited quantities, so the harm is very different.reply",
      "Real people work in this industry, though. A merger of this size is bound to come with some layoffs and canceled projects.It's not as bad as food scarcity, of course. But it can do some collateral damage.reply",
      "Does that mean the DCU Movies might get delayed or canceled?reply",
      "One can only hope.reply",
      "That, plus fewer studios mean less creativity goes to the mainstream. If you thought AI slop was bad, go re-watch Star Wars Episode 8.reply"
    ],
    "link": "https://www.cnbc.com/2025/12/08/paramount-skydance-hostile-bid-wbd-netflix.html",
    "first_paragraph": ""
  },
  {
    "title": "AI should only run as fast as we can catch up (higashi.blog)",
    "points": 106,
    "submitter": "yuedongze",
    "submit_time": "2025-12-08T17:38:34 1765215514",
    "num_comments": 109,
    "comments_url": "https://news.ycombinator.com/item?id=46195198",
    "comments": [
      "It's nice to see a wide array of discussions under this! Glad that I didn't give up on this thought and end up writing it down.I want to stress that the main point of my article is not really about AI coding, it's about letting AI perform any arbitrary tasks reliably. Coding is an interesting one because it seems like it's a place where we can exploit structure and abstraction and approaches (like TDD) to make verification simpler - it's like spot-checking in places with a very low soundness error.I'm encouraging people to look for tasks other than coding to see if we can find similar patterns. The more we can find these cost asymmetry (easier to verify than doing), the more we can harness AI's real potential.reply",
      "Note that in the case of coding, there is an entire branch of computer science dedicated to verification.All the type systems (and model-checkers) for Rust, Ada, OCaml, Haskell, TypeScript, Python, C#, Java, ... are based on such research, and these are all rather weak in comparison to what research has created in the last ~30 years (see Rocq, Idris, Lean).This goes beyond that, as some of these mechanisms have been applied to mathematics, but also to some aspects of finance and law (I know of at least mechanisms to prove formally implementations of banking contracts and tax management).So there is lots to do in the domain. Sadly, as every branch of CS other than AI (and in fact pretty much every branch of science other than AI), this branch of computer science is underfunded. But that can change!reply",
      "Thinking about the relationship between creation and verification is a good way to develop productive workflows with AI tools.One that works particularly well in my case is test-driven development followed by pair programming:\u2022 \u201cgiven this spec/context/goal/\u2026 make test XYZ pass\u201d\u2022 \u201cnow that we have a draft solution, is it in the right component? is it efficient? well documented? any corner cases?\u2026\u201dreply",
      "All these engineers who claim to write most code through AI - I wonder what kind of codebase that is. I keep on trying, but it always ends up producing superficially okay-looking code, but getting nuances wrong. Also fails to fix them (just changes random stuff) if  pointed to said nuances.I work on a large product with two decades of accumulated legacy, maybe that's the problem. I can see though how generating and editing a simple greenfield web frontend project could work much better, as long as actual complexity is low.reply",
      "I have my best successes by keeping things constrained to method-level generation. Most of the things I dump into ChatGPT look like this:  public static double ScoreItem(Span<byte> candidate, Span<byte> target)\n  {\n     //TODO: Return the normalized Levenshtein distance between the 2 byte sequences.\n     //... any additional edge cases here ...\n  }\n\nI think generating more than one method at a time is playing with fire. Individual methods can be generated by the LLM and tested in isolation. You can incrementally build up and trust your understanding of the problem space by going a little bit slower. If the LLM is operating over a whole set of methods at once, it is like starting over each time you have to iterate.reply",
      "I do this but with copilot. Write a comment and then spam opt-tab and 50% of the time it ends up doing what I want and I can read it line-by-line before tabbing the next one.Genuine productivity boost but I don't feel like it's AI slop, sometimes it feels like its actually reading my mind and just preventing me from having to type...reply",
      "I've settled in on this as well for most of my day-to-day coding. A lot of extremely fancy tab completion, using the agent only for manipulation tasks I can carefully define. I'm currently in a \"write lots of code\" mode which affects that, I think. In a maintenance mode I could see doing more agent prompting. It gives me a chance to catch things early and then put in a correct pattern for it to continue forward with. And honestly for a lot of tasks it's not particularly slower than \"ask it to do something, correct its five errors, tweak the prompt\" work flow.I've had net-time-savings with bigger agentic tasks, but I still have to check it line-by-line when it is done, because it takes lazy shortcuts and sometimes just outright gets things wrong.Big productivity boost, it takes out the worst of my job, but I still can't trust it at much above the micro scale.I wish I could give a system prompt for the tab complete; there's a couple of things it does over and over that I'm sure I could prompt away but there's no way to feed that in that I know of.reply",
      "It's architecture dependent. A fairly functional modular monolith with good documentation can be accessible to LLMs at the million line scale, but a coupled monolith or poorly instrumented microservices can drive agents into the ground at 100k.reply",
      "I think it's definitely an interesting subject for Verification Engineering. the easier to task AI to do work more precisely, the easier we can check their work.reply",
      "Yup. Codebase structure for agents is a rabbit hole I've spent a lot of time going down. The interesting thing is that it's mostly the same structure that humans tend to prefer, with a few tweaks: agents like smaller files/functions (more precise reads/edits), strongly typed functional programming, doc-comments with examples and hyperlinks to additional context, smaller directories with semantic subgroups, long/distinct variable names, etc.reply"
    ],
    "link": "https://higashi.blog/2025/12/07/ai-verification/",
    "first_paragraph": "Steven (Dongze) Yue's personal blog.\n      \u00a9 2025.\n    Recently I have spoke with two of my friends who all had fun playing with AI.Last month, I met with Eric, a fearless PM at a medium size startup who recently got into vibe coding with Gemini.\nAfter getting familiarized with Gemini, Eric was genuinely amazed by how AI quickly turns prompt into playable web applications. It served great purpose as a first prototype to communicate ideas to designers and engineers. But Eric really wanted to skip those steps and directly ship it to prod. But he couldn\u2019t really understand that Gemini actually built a single-page HTML file that merely looks like a working app. Sadly, one cannot build a reliable enterprise product out of this. And there is really no effective way for Eric to catch up on these technical details and outpace the engineering team himself.Last week, I had coffee with Daniel, a senior staff engineer who recently grew fond of AI coding and found it to be the true force multiplier"
  },
  {
    "title": "A series of tricks and techniques I learned doing tiny GLSL demos (pkh.me)",
    "points": 132,
    "submitter": "ibobev",
    "submit_time": "2025-12-08T16:44:42 1765212282",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=46194477",
    "comments": [
      "I\u2019m not a fan of the minified glsl that guys like this produce but I do get a chuckle when variable declarations spell out damnit. The frustrations are real.That aside, i love the work, I just hate having to mentally grok the d and c style variables. As if number of chars minimum is the goal. Number of instructions yes, but we can do better than d and t.Moonlight is beautiful.reply",
      "If there was some absolute measure of program 'interestingness'/'readability', programs found on shadertoy would no doubt have an asymptotically high score.My theory is that graphics programmers, at some point, stop having to care very much about what the textual representation of their  program actually looks like.  Because graphics programming is so hard, once you get to the point of understanding what you're doing, and typing in the shader, it becomes self-explanatory; you don't actually need variable names, what you need is understanding.Inigo Quilez (author of shadertoy, and graphics programming legend) is one of the most talented graphics programmers alive, and produces some of the least readable code I've ever seen.Just my 2c on why this is so common in graphics, specificallyreply",
      "I am a graphics programmer. It still enrages me to see those variables but (but) I understand that most of the time, it\u2019s that way in the math. So the math variable becomes the shader variable name. Ugh.\nI love me a good float lut = lut_equation(uv, lutTex) * lut_factor.rreply",
      "Thanks for putting this together. Shaders are something I wish I had the time to dive deep into, but since I'm making a game, my time is very limited for the time being. i.e. I only learn what I need to.Only critique is.. if you're sharing to teach, your compact/one line [460] char GLSL code is a poor delivery mechanism.reply",
      "> Thanks for putting this together.I'm glad it reaches an audience :)> Only critique is.. if you're sharing to teach, your compact/one line [460] char GLSL code is a poor delivery mechanism.Understandable. Though, the demos are here to illustrate \"what you can do with the trick I'm sharing\". It's like, I'm teaching you how to do watercolor, and illustrate it with some paintings you won't be able to perform just with that knowledge. They're meant to inspire you to create. You're not looking at a tutorial, you're looking at art.reply",
      "I don't know, I kind of liked it.For once instead of being shoved a ready-made solution there's a short explanation of the core idea with a live example, but instead of a fully documented shadertoy it's like the answer is ROT13'd which makes me itch for implementing a solution myself.reply",
      "The comic at the end made me chuckle more than it probably should have.  :Dreply",
      "Nice looking demos, I hope to some day manage to get results close to what you have.I started playing around with GLSL recently and the closest I have come to describing working with it is that it is like creating poetry using math. Getting started was much easier than I expected, getting good results is so far as hard as expectedreply",
      "Thank you for the kind words, and you're on point with \"poetry using math\".I started 2-3 months ago or so doing this stuff, so don't be too intimidated to start. Especially with the two articles (Red Alp and this one), it should make it more accessible, hopefully :)reply",
      "I'm oddly fascinated with GLSL shaders recently because I've been trying out a terminal which supports shaders. I have currently applied a retro terminal shader and love using terminal this way.What's a good way to get started learning to build/customize shaders with GLSL? I have an engineering math background but I was never the best at math. And GLSL syntax looks a bit tedious to be honest, but I'd love to dive in.reply"
    ],
    "link": "https://blog.pkh.me/p/48-a-series-of-tricks-and-techniques-i-learned-doing-tiny-glsl-demos.html",
    "first_paragraph": "\u2190 indexIn the past two months or so, I spent some time making tiny GLSL demos. I wrote\nan article about the first one, Red Alp. There, I went into details about the\nwhole process, so I recommend to check it out first if you're not familiar with\nthe field.We will look at 4 demos: Moonlight, Entrance 3,\nArchipelago, and Cutie. But this time, for each\ndemo, we're going to cover one or two things I learned from it. It won't be a\ndeep dive into every aspect because it would be extremely redundant. Instead,\nI'll take you along a journey of learning experiences.NoteSee it on its official page, or play with the code on its\nShadertoy portage.In Red Alp, I used volumetric raymarching to go through the clouds and fog, and\nit took quite a significant part of the code to make the absorption and emission\nconvincing. But there is an alternative technique that is surprisingly simpler.In the raymarching loop, the color contribution at each iteration becomes 1/d\nor c/d where d is the density of the mate"
  }
]