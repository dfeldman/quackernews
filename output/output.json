[
  {
    "title": "Analytical Review of Depression and Suicidality from Finasteride (psychiatrist.com)",
    "points": 34,
    "submitter": "gnabgib",
    "submit_time": "2025-10-07T00:11:34 1759795894",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=45497818",
    "comments": [
      "Not kidding: What about suicides prevented from not going bald? To paint a fair image, this study should also compare suicide prevalence for bald people.-------Results: Compared with the controls, an increased risk of suicide attempts was observed in patients with AA, with an adjusted hazard ratio of 6.28 (95% confidence interval, 4.47-8.81). Suicide risk remained significantly elevated in AA patients when stratified by underlying psychiatric disorders. The mean age of initial suicidal behaviors was also lower in patients with AA.Conclusions: Patients with AA had a significantly higher incidence of suicidal attempts than controls, regardless of concurrent psychiatric illness. Further studies are needed to elucidate the pathophysiology of the association between AA and suicidality. In addition, dermatologists should be aware of the increased suicidality of patients with AA.https://pubmed.ncbi.nlm.nih.gov/36921592/----Not enough time to size these in comparable numbers to this study, but would be really interesting.Is it a net positive (reduced risk) over just going bald?reply",
      "It's also possible that people taking Finasteride might be a more potent selection of people that are distressed about hair loss, and are therefore more likely to exhibit depression, etc. As in, if people with androgenetic alopecia are more likely to be depressed, people who take finasteride may be a sampling of those people who are distressed enough to seek and maintain treatments.reply",
      "Alopecia Areata \u2260 Androgenetic Alopeciareply",
      "Thanks, I googled \"bald suicide risk pubmed\" and it was the first article I found. I can't find any with as clear-cut statistics for AA.I still think it applies though; the psychological effect of being bald probably doesn't care much about the underlying cause.reply",
      "There's probably a difference in degree, however. Alopecia Areata is much more uncommon, while regular male pattern baldness is very common.There's also the fact that Alopecia Areata is actually more common in women, which I imagine exaggerates the distress compared to the more run of the mill MPB.I realize you didn't mean to use a study on Alopecia Areata, but the difference in degree could be quite large.reply",
      "Anecdotal, but I took Finasteride and I had suicidal thoughts until I stopped the drug. Not recommended.reply",
      "topical or oral???reply",
      "I would expect oral to have more of an impact on mood, but this article does not say. Good catch!reply",
      "The studies listed are all about oral finasteride.reply",
      "Can someone give us more context? Why is Finasteride of particular interest?reply"
    ],
    "link": "https://www.psychiatrist.com/jcp/analytical-review-depression-suicidality-finasteride/",
    "first_paragraph": "The Journal ofClinical Psychiatry\nNarrative Review Focus on Suicide September 22, 2025 \nMayer Brezis, MD, MPHJ Clin Psychiatry 2025;86(4):25nr15862\nAbstract\nBackground: Finasteride, widely prescribed for androgenetic alopecia, has long been suspected of causing severe neuropsychiatric reactions, including depression, anxiety, and suicidality, even after the drug is discontinued. This study systematically reviews evidence that supports this suspicion and analyzes the reasons for this delayed recognition.\nObservations: Concerns about depression from finasteride were raised in several studies as early as 2002. Between the years 2017 and 2023, 4 independent analyses of adverse event reporting systems and 4 studies using data mining of healthcare records indicated a significant increase in the risk for depression, anxiety, and/or suicidal behavior with the use of finasteride. There has been, therefore, a two-decade delay in the realization of the incidences and the gravity of neuropsychiatr"
  },
  {
    "title": "Apps SDK (developers.openai.com)",
    "points": 265,
    "submitter": "alvis",
    "submit_time": "2025-10-06T18:27:33 1759775253",
    "num_comments": 237,
    "comments_url": "https://news.ycombinator.com/item?id=45494558",
    "comments": [
      "It's interesting to see how chatgpt is becoming more and more of a starting point of the web exploration, at which they're like, why even bother searching at this point, we'll just have default workflows for maps, buy (integration of stripe already marks it), booking airlines etc, which covers so much basic stuff people would do anyways.The biggest bottleneck for this for the past two years imo wasn't the models, but the engineering and infra around it, and the willingness of companies to work with \nopenaio directly. Now that they've grown and have a decent userbase, companies are much more willing to pay/or involve themselves in these efforts.This has eventual implications outside user-heavy internet use (once we see more things built on the SDK), where we're gonna see a fork in the web traffic of human centric workflows through chat, and an seo-filled, chat/agent-optimized web that is only catered to agents. (crossposted)reply",
      "This conception makes sense iff you believe in ChatGPT as the universal user interface of the future. If anything the agentic wave is showing that the chat interfaces are better off hidden behind stricter user interface paradigms.reply",
      "I suspect there are many, many things for which chat is a great interface. And by positioning ChatGPT as the distributor for all these things, they get to be the new Google. But you're also right that many domains for which a purpose-built interface is the right approach, and if the domain is valuable enough, it'll have someone coming after it to build that.reply",
      "I have yet to see a chat agent deployed that is more popular than tailored browsing methods.  The most charitable way to explain this is that the tailored browsing methods already in place are the results of years of careful design and battle testing and that the chat agent is providing most of the value that a tailored browsing method would but without any of the investment required to bring a traditional UX to fruition - that may be the case and if it is then allowing them the same time to be refined and improved would be fair.  I am skeptical of that being the only difference though, I think that chatbots are a way to, essentially, outsource the difficult work of locating data within a corpus onto the user and that users will always have a disadvantage compared to the (hopefully) subject matter experts building the system.So perhaps chatbots are an excellent method for building out a prototype in a new field while you collect usage statistics to build a more refined UX - but it is bizarre that so many businesses seem to be discarding battle tested UXes for chatbots.reply",
      "agree.Thing is, for those who paid attention to the last chatBot hype cycle, we already knew this. Look at how Google Assistant was portrayed back in 2016. People thought you'd be buying starbucks via the chat. Turns out the starbucks app has a better UXreply",
      "Yea, I don't want to sit there at my computer, which can handle lots of different input methods, like keyboard, mouse, clicking, dragging, or my phone which can handle gestures, pinching, swiping... and try to articulate what I need it to do in English language conversation. This is actually a step backwards in human-computer interaction. To use an extreme example: imagine instead of a knob on my stereo for volume, I had a chat box where I had to type in \"Volume up to 35\". Most other \"chatbot solved\" HCI problems are just like this volume control example, but less extreme.reply",
      "It's funny, because the chat bot designers seem to be continually attempting to recreate the voice computer interface from Star Trek: TNG. Yet if you watch the show carefully, the vast majority of the work done by all the Enterprise crew is done via touchscreens, not voice.The only reason for the voice interface is to facilitate the production of a TV show. By having the characters speak their requests aloud to the computer as voice commands, the show bypasses all the issues of building visual effects for computer screens and making those visuals easy to interpret for the audience, regardless of their computing background. However, whenever the show wants to demonstrate a character with a high level of computer mastery, the demonstration is almost always via the touchscreen (this is most often seen with Data), not the voice interface.TNG had issues like this figured out years ago, yet people continue to fall into the same trap because they repeatedly fail to learn the lessons the show had to teach.reply",
      "It's actually hilarious to think of a scene where all the people on the bridge are shouting over each other trying to get the ship to do anything at all.Maybe this is how we all get our own offices again and the open floor plan dies.reply",
      "Hmm. Maybe something useful will come of this after all!\"...and that is why we need the resources. Newline, end document. Hey, guys, I just got done with my 60 page report, and need-\"\"SELECT ALL, DELETE, SAVE DOCUMENT, FLUSH UNDO, PURGE VERSION HISTORY, CLOSE WINDOW.\"Here's hoping this at least gets us back to cubes.reply",
      "Getting our own offices would simply take collective action, and we're far too smart to join a union, err, software developers association to do that.reply"
    ],
    "link": "https://developers.openai.com/apps-sdk/",
    "first_paragraph": " Our framework to build apps for ChatGPT.  Design components and conversational flows that feel native to ChatGPT.  Build apps that meet our quality, safety, and policy standards.  Identify and prioritize Apps SDK use cases.  Create and configure an MCP server.  Learn how to deploy your MCP server Improve discovery and behavior with rich metadata.Security and privacy considerations for Apps SDK.Troubleshoot issues in Apps SDK apps."
  },
  {
    "title": "Kirigami-inspired parachute falls on target (physicsworld.com)",
    "points": 136,
    "submitter": "sohkamyung",
    "submit_time": "2025-10-02T12:54:13 1759409653",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=45449015",
    "comments": [
      "This is really cool and innovative thinking, but anything aerodynamic does not scale linearly. It's really easy to make something light fall slowly. Baby spiders use \"ballooning\" -- a single thread -- to fall so slowly that they can travel far in thermal updrafts.What's missing here is any evidence that the same cool parachutes will work on anything of significant mass, e.g. a parcel weighing 2kg or an average human weighing 80kg.reply",
      "Depending on the use case, a hot-air balloon sized parachute to safely drop a person might be perfectly acceptable.It looks like adding flexible ailerons or whatever they'd be called could give a big advantage in precision landing, with slower forward/sideways speeds but much better control.Making it modular, with interlocking but separate parts, might make great sense for repairability and safety for skydiving? From the little I know of the sport, things tend to fail catastrophically, going from perfect condition to total disaster without a whole lot of graduated steps in between. I also wonder if there's some utility in paramotoring - multiple kirigami stabilizers, maybe, with a central parafoil, or one big kirigami rig with the fan blowing straight up its skirt?This is awesome research. Paper drone-delivery parachutes are definitely a use case, but maybe some of the more dangerous flying sports could be made much safer, as well.edit: Apparently no, 100 meter radius kirigami chute would be needed for a single person parachute, not exactly practical. Apparently it's just really, really good at ensuring things drop straight down with a lot of drag.reply",
      "I've worked on mission planning software for parachute systems and the precision we can achieve is already extremely high. Given how poorly this seems to scale, the only use case that makes any sense to me would be something like sensor drop, which are the only payloads small enough for these chutes. Or potentially for drogues on multi-stage systems, but I'm not sure they'd even be useful there because usually a fast descent is part of the appeal of a drogued payload, and not just to reduce time exposed to wind drift (e.g., to reduce time it is vulnerable to enemy fire).reply",
      "Spider ballooning is an interesting phenomena. I also assumed that the spider is just falling a bit slower than the air is rising, due to convection. However some people think there is also a strong electrostatic component to spider ballooning. I'm not sure how that works once the spider is well clear of the ground though.reply",
      "The tested it with a standard size waterbottle, so you know it works fine for 0.8 kg paloads.reply",
      ">> does not scale linearlyIt looks like it depends on the stiffness of the material (paper), so scaling it up to human (or bigger) sized will come with \"interesting\" challenges :(reply",
      "The journal Nature (where the original article was published) has a video about that parachute:https://www.youtube.com/watch?v=6rrDW6YIbXIreply",
      "I had to wonder if Nature (the Mother, not the journal) had first created anything similar, because she always does.One answer is a dandelion seed. Not exactly the same, but a dandelion seed is about 85%+ \"porous\" - the pores here being the space between the spindles, not actual holes per se. And it turns out that high porosity is critical to stabilizing the wake turbulence not unlike what is described in the Nature video. https://sites.nd.edu/biomechanics-in-the-wild/2021/06/01/inn...A learning that kirigami parachute researches might apply: The dandelion pappus is less porous near the center and becomes more porous toward the outer edge. A lower porosity near the central hub can increase shear flow, helping to detach and strengthen the vortexFurthermore, spiders which have been known to \"balloon\" on the wind even across entire oceans use multiple strands of silk which are negatively charged to repel each other, thus forming some of the same gaps that are seen in a dandelion pappus, with similar aerodynamic benefits. https://journals.aps.org/pre/abstract/10.1103/PhysRevE.105.0...reply",
      "The fine article mentions this, including mentioning dandelion seeds specifically:  > As well as kirigami, the team drew inspiration from nature. Instead of relying on a gliding angle, many wind-dispersed seeds are equipped with structures that stabilize the airflow around them: including the feathery bristles of dandelion seeds, which create a stabilized vortex in their wake.reply",
      "Is that really Nature's channel? It sucks that I have to ask, but I just ran across that video (and channel) on the weekend. I have a new policy where I don't trust videos to not be AI slop when they don't have a real, on screen presenter. This one didn't, but it also didn't really feel like AI slop, but it could have been stolen content since that is very common nowadays because it goes unpunished.reply"
    ],
    "link": "https://physicsworld.com/a/kirigami-inspired-parachute-falls-on-target/",
    "first_paragraph": ""
  },
  {
    "title": "CodeMender: an AI agent for code security (deepmind.google)",
    "points": 94,
    "submitter": "ravenical",
    "submit_time": "2025-10-06T21:28:56 1759786136",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=45496533",
    "comments": [
      "I wonder if we're going to end up in an arms race between AIs masquerading as contributors (and security researchers) trying to introduce vulnerabilities into popular libraries, and AIs trying to detect and fix them.reply",
      "Why would it be like that instead of the way we already handle low-trust environments?Projects that get a lot of attention already put up barriers to new contributions, and the ones that get less attention will continue to get less attention.The review process cannot be left to AI because it will introduce uncertainty nobody wants to be held responsible for.If anything, the people who have always seen code as a mere means to an end will finally come to a forced decision: either stop fucking around or get out of the way.An adversarial web is ultimately good for software quality, but less open than it used to be. I'm not even sure if that's a bad thing.reply",
      "What I'm suggesting is: what if AIs get so good at crafting vulnerable (but apparently innocent) code than human review cannot reliably catch them?And saying \"ones that get less attention will continue to get less attention\" is like imagining that only popular email addresses get spammed. Once malice is automated, everyone gets attention.reply",
      "Significantly easier to detect than create? Not quite NP, but intuitively an AI which can create such an exploit could also detect it.The economics is more about how much the defender is willing to spend in advance protection vs the expected value of a security failurereply",
      "Not a fan of future products being announced as if they are here but are basically is still in \"Internal Research\" stages. I'm not sure who this is really helping? except creating unnecessary anticipation which we kinda all know are in this loop lately of \"yes it works great, but\".reply",
      "I'm optimistic that it's easier to find/solve vulnerabilities via auto pen-testing  / patching, and other security measures, than it will be to find/exploit vulnerabilities after - ie defense is easier in an auto-security world.Does anyone disagree?This is purely my intuition, but I'm interested in how others are thinking about it.All this with the mega caveat of this assuming very widespread adoption of these defenses, which we know won't be true and auto-hacking may be rampant for a while.reply",
      "In open source codebases perhaps, either because big tech would be generous enough to run and generate PRs(if they are welcome ) for those issues.In proprietary/closed source it depends on ability to spend the money these tools would end up costing.As there is more and more vibe coded apps there will be more security bugs because app owners just don\u2019t know better or don\u2019t care to fix them .This happened when rise of Wordpress and other cmses and their plugin ecosystem or languages like early PHP or for that matter even C opened up software development to wider communities.On average we will see more issues not less.reply",
      "In general, most modern vulnerabilities are initially identified with fuzzing systems under abnormal conditions. Whether these issues may be consistently exploited can be probabilistic in nature, and thus repeatability with a POC dataset is already difficult.That being said, most modern exploits are already auto-generated though brute-force, as nothing more complex is required.>Does anyone disagree?CVE agents already pose a serious threat vector in and of itself.1. Models can't currently be made inherently trustworthy, and the people claiming otherwise are selling something.\"Sleeper Agents in Large Language Models - Computerphile\"https://www.youtube.com/watch?v=wL22URoMZjo2. LLMs can negatively impact logical function in human users. However, people feel 20% more productive, and that makes their contributed work dangerous.3. People are already bad at reconciling their instincts and rational evaluation. Adding additional logical impairments is not wise:https://www.youtube.com/watch?v=-Pc3IuVNuO04. Auto merging vulnerabilities into opensource is already a concern, as it falls into the ambiguous \"Malicious sabotage\" or \"Incompetent noob\" classifications. How do we know someone or some models intent? We can't, and thus the code base could turn into an incoherent mess for human readers.Mitigating risk:i. Offline agents should only have read-access to advise on identified problem patterns.ii. Code should never be cut-and-pasted, but rather evaluated for its meaning.iii. Assume a system is already compromised, and consider how to handle the situation. In this line of reasoning, the policy choices should become clear.Best of luck, =3reply",
      "I've also thought this for scam perpetration vs mitigation. An AI listening to grandma's call would surely detect most confidence or pig butchering scams (or suggest how to verify), and be able to cast doubt on the caller's intentions or inform a trusted relative before the scammer can build up rapport. Security and surveillance concerns notwithstanding.reply",
      "4.5 million lines of code for one fix is impressive for an LLM agent, but there's so little detail in this post otherwise. Perhaps this is a tease to what will be released on Thursday...reply"
    ],
    "link": "https://deepmind.google/discover/blog/introducing-codemender-an-ai-agent-for-code-security/",
    "first_paragraph": "Build with our next generation AI systemsOur most intelligent AI modelsLightweight, state-of-the-art open modelsImage, music and video generation modelsModels and experiments built with GeminiOur latest AI breakthroughs and updates from the labExplore some of the biggest AI innovationsRead a selection of our recent papersDiscover the latest updates from our labUnlocking a new era of discovery with AIOur mission is to build AI responsibly to benefit humanityDiscover our latest AI breakthroughs, projects, and updatesWe\u2019re looking for people who want to make a real, positive impact on the worldFor over 20 years, Google has worked to make AI helpful for everyoneWe work to make AI more accessible to the next generationEnsuring AI safety through proactive security, even against evolving threatsUncover the extraordinary ways AI is transforming our worldBuild with our next generation AI systemsOur most intelligent AI modelsLightweight, state-of-the-art open modelsImage, music and video generat"
  },
  {
    "title": "Who owns Express VPN, Nord, Surfshark? VPN relationships explained (2024) (windscribe.com)",
    "points": 210,
    "submitter": "walterbell",
    "submit_time": "2025-10-04T00:30:44 1759537844",
    "num_comments": 137,
    "comments_url": "https://news.ycombinator.com/item?id=45469376",
    "comments": [
      "Note that all of these companies are also under the umbrella of Tesonet, a Lithuanian VC firm also headed by Tomas Okmanas (Tom Okman in TFA). Their flagship investments are Nord Security, Hostinger, Oxylabs, Surfshark, Decodo, Mediatech, and nexos.ai - all closely related business models around proxying.They don't seem to have Russian ties: \"In 2022, CyberCare opened an office in Lviv, Ukraine. Although planning for the move started before the war, according to Dainius Vanagas, CEO of CyberCare, one of the reasons why it was followed through was a desire to help Ukraine rebuild.\"[0]They also donated money to help arm Ukraine.0: https://en.wikipedia.org/wiki/Tesonetreply",
      "Don't forget ProtonVPN links to Tesonet, which they're trying hard to \"debunk\" (though no clue why, I have nothing against Tesonet). They only shared employees and accidentally signed apps with the same certificates, but are \"totally unrelated\". Their PR people are already on this thread.If they didn't try so hard to fight it, people might care less.reply",
      "Back when I was running PIA, they threatened me a significant amount just for pointing these facts out.Now that I launched a verifiable VPN, they are once again sending legal threats [1].[1] https://vp.net/l/en-US/blog/Verified-Privacy-vs-Trustreply",
      "So did you sell pia? Why won\u2019t you sell your next venture ?reply",
      "I did not sell PIA. I entered into a merger agreement to create a publicly owned privacy company. Without getting into detail, I left the company on principle receiving only 1/3rd of the value for the shares.reply",
      "Btw I used to love pia, I think I\u2019ll check your new one out!reply",
      "I used to work at Tesonet (as software engineer) and I'm not familiar with corporate politics / ownerships but they're lovely people that would 100% walk out if there were some real Russian ties involved.Lithuania is a really small country and IT has been a huge economic strategy since early 00s as a way to become economically independent specifically because of Russia and it worked out really well.reply",
      "This link displays just the map, freed from it's painfully small frame.https://kumu.io/embed/9ced55e897e74fd807be51990b26b415#vpn-c...reply",
      "Anyone got this as a regular single image infographic or (better yet) a text-only bulleted outline?reply",
      "I have to admit that discovering that ProtonVPN was actually just owned by Proton Technologies feels underwhelming.reply"
    ],
    "link": "https://windscribe.com/blog/the-vpn-relationship-map/",
    "first_paragraph": "Take me to the full map (Third-party website). This beauty right here is a VPN map that charts every proven relationship between media companies, content sites, corporate VPNs, and independent VPNs that I could find - and it's only the 1.0 version. I hope to map and track other information that might prove useful to people with the help of more contributors in the 2.0 version.Red - Corporate Relationship & OwnershipOrange - Paid Relationship or Paid Affiliates (Dashes)Blue - Cooperation or Partnership (These companies may share staff, resources, networks, or facilities with one another).Purple - Corporate Media Relationship & OwnershipBrown - Legal DisputeHover over nodes to highlight relationshipsUse right-click to focus on select areasExpressVPN was founded in 2009 by Peter Burchhardt and Dan Pomerantzwe who later sold it to British-Israeli security software company Kape Technologies in a $936 million deal.Hence Teddy Sagi, the owner of Kape, is now the owner of ExpressVPN. We'll get"
  },
  {
    "title": "OpenZL: An open source format-aware compression framework (fb.com)",
    "points": 237,
    "submitter": "terrelln",
    "submit_time": "2025-10-06T16:01:58 1759766518",
    "num_comments": 56,
    "comments_url": "https://news.ycombinator.com/item?id=45492803",
    "comments": [
      "It was really hard to resist spilling the beans about OpenZL on this recent HN post about compressing genomic sequence data [0]. It's a great example of the really simple transformations you can perform on data that can unlock significant compression improvements. OpenZL can perform that transformation internally (quite easily with SDDL!).[0] https://news.ycombinator.com/item?id=45223827reply",
      "That post immediately came to my mind too! Do you maybe have a comparison to share with respect to the specialized compressor mentioned in the OP there?> Grace Blackwell\u2019s 2.6Tbp 661k dataset is a classic choice for benchmarking methods in microbial genomics. (...) Karel B\u0159inda\u2019s specialist MiniPhy approach takes this dataset from 2.46TiB to just 27GiB (CR: 91) by clustering and compressing similar genomes together.reply",
      "I'd love to see some benchmarks for this on some common genomic formats (fa, fq, sam, vcf). Will be doubly interesting to see its applicability to nanopore data - lots of useful data is lost because storing FAST5/POD5 is a pain.reply",
      "OpenZL compressed SAM/BAM vs. CRAM is the interesting comparison. It would really test the flexibility of the framework. Can OpenZL reach the same level of compression, and how much effort does it take?I would not expect much improvement in compressing nanopore data. If you have a useful model of the data, creating a custom compressor is not that difficult. It takes some effort, but those formats are popular enough that compressors using the known models should already exist.reply",
      "Do you happen to have a pointer to a good open source dataset to look at?Naively and knowing little about CRAM, I would expect that OpenZL would beat Zstd handily out of the box, but need additional capabilities to match the performance of CRAM, since genomics hasn't been a focus as of yet. But it would be interesting to see how much we need to add is generic to all compression (but useful for genomics), vs. techniques that are specific only to genomics.We're planning on setting up a blog on our website to highlight use cases of OpenZL. I'd love to make a post about this.reply",
      "For BAM this could be a good place to start: https://www.htslib.org/benchmarks/CRAM.htmlHappy to discuss furtherreply",
      "Amazing, thank you!I will take a look as soon as I get a chance. Looking at the BAM format, it looks like the tokenization portion will be easy. Which means I can focus on the compression side, which is more interesting.reply",
      "And a comparison between CRAM and openzl on a sam/bam file. Is openzl indexable, where you can just extract and decompress the data you need from a file if you know where it is?reply",
      "> Is openzl indexableNot today. However, we are considering this as we are continuing to evolve the frame format, and it is likely we will add this feature in the future.reply",
      "Author of [0] here. Congratulations and well done for resisting. Eager to try it!Edit: Have you any specific advice for training a fasta compressor beyond that given in e.g. \"Using OpenZL\" (https://openzl.org/getting-started/using-openzl/)reply"
    ],
    "link": "https://engineering.fb.com/2025/10/06/developer-tools/openzl-open-source-format-aware-compression-framework/",
    "first_paragraph": "Today, we are excited to announce the public release of OpenZL, a new data compression framework. OpenZL offers lossless compression for structured data, with performance comparable to specialized compressors. It accomplishes this by applying a configurable sequence of transforms to the input, revealing hidden order in the data, which can then be more easily compressed. Despite applying distinct transformation permutations for every file type, all OpenZL files can be decompressed using the same universal OpenZL decompressor.When Zstandard was announced, it came with a simple pitch: It promised the same or better compression ratio of prior default but at the much increased speed required by datacenter workloads. By pairing strong entropy coding with a design that fully utilized modern CPU capabilities, Zstandard offered a substantial improvement that justified its presence in datacenters.However, while it was improved over time, remaining within the Zstandard framework offers diminishin"
  },
  {
    "title": "How to tile matrix multiplication (2023) (alvinwan.com)",
    "points": 43,
    "submitter": "pbd",
    "submit_time": "2025-10-03T01:37:23 1759455443",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://alvinwan.com/how-to-tile-matrix-multiplication/",
    "first_paragraph": "from Guide to Machine Learning on Apr 30, 2023Matrix multiplication is a staple of deep learning and a well-studied, well-optimized operation. One of the most common optimizations for matrix multiplication is called \"tiling,\" but as common and important as it is, it's a bit confusing to understand.This is the first post of a 3-part series covering efficient matrix multiply kernels:Tiling matrix multiplication is a valuable technique that optimizes resource utilization in multiple dimensions, including power, memory, and compute. Critically, tiling then reduces overall latency, making this vital for models heavily reliant on dense matrix multiplication. One such example is transformers and their associated Large Language Models; their heavy reliance on dense matrix multiplies for inference makes tiling an important concept to understand \u2014 and to leverage.Not sure why dense matrix multiplies are so necessary? For a primer on how Large Language Models work, check out the 3-part series, be"
  },
  {
    "title": "Ladybird passes the Apple 90% threshold on web-platform-tests (twitter.com/awesomekling)",
    "points": 673,
    "submitter": "sergiotapia",
    "submit_time": "2025-10-06T16:52:58 1759769578",
    "num_comments": 190,
    "comments_url": "https://news.ycombinator.com/item?id=45493358",
    "comments": [
      "As someone who's been quite heavily involved with web-platform-tests, I'd caution against any use of the test pass rate as a metric for anything.That's not to belittle the considerable achievements of Ladybird; their progress is really impressive, and if web-platform-tests are helping their engineering efforts I consider that a win. New implementations of the web platform, including Ladybird, Servo, and Flow, are exciting to see.However, web-platform-tests specifically decided to optimise for being a useful engineering tool rather than being a good metric. That means there's no real attempt to balance the testsuite across the platform; for example a surprising fraction of the overall test count is encoding tests because they're easy to generate, not because it's an especially hard problem in browser development.We've also consciously wanted to ensure that contributing tests is low friction, both technically and socially, in order that people don't feel inclined to withhold useful tests. Again that's not the tradeoff you make for a good metric, but is the right one for a good engineering resource.The Interop Project is designed with different tradeoffs in mind, and overcomes some of these problems by selecting a subsets of tests which are broadly agreed to represent a useful level of coverage of an important feature. But unfortunately the current setup is designed for engines that are already implementing enough feature to be usable as general purpose web-browsers.reply",
      "The tweet mentions that this is an arbitrary metric thrust upon them by Apple, so I don\u2019t think they would necessarily disagree with you. During the monthly updates they do also show the passing number of tests without including the encoding tests because of how much they skew things.reply",
      "Ladybird will be faster than anything with an arbitrary metric thrustreply",
      "The problem is, there's no other good metric. We used to have Acid tests for CSS, but in absence of that, it's as good metric as any.reply",
      "Are Acid tests no longer available?reply",
      "They are, but they arent great tests of what a browser is capable of. For example, Firefox does not pass Acid2 or Acid3reply",
      "They no longer reflect what the average user expects their browser to support. You can pass it and miss on several important things that are considered widespread features nowadays.reply",
      "Could a hand-picked subset be selected to make that metric?reply",
      "Why are you bringing this up, when it\u2019s not been implemented as a metric here, but because Apple requires it for iOS.reply",
      "This is a headline that is very easy to misread and or misunderstand. I don\u2019t find their comment to be that out of place at all.reply"
    ],
    "link": "https://twitter.com/awesomekling/status/1974781722953953601",
    "first_paragraph": "We\u2019ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.Help Center\nTerms of Service\nPrivacy Policy\nCookie Policy\nImprint\nAds info\n      \u00a9 2025 X Corp.\n    "
  },
  {
    "title": "Microsoft is plugging more holes that let you use Windows 11 without MS account (theverge.com)",
    "points": 79,
    "submitter": "josephcsible",
    "submit_time": "2025-10-06T23:15:26 1759792526",
    "num_comments": 125,
    "comments_url": "https://news.ycombinator.com/item?id=45497384",
    "comments": [
      "Not sure Microsoft realizes the damage they're doing to the Windows brand. My first experience with Windows 11 was figuring out some dumb workaround to use a local account.When I think back to Windows 7, the good feeling isn't nostalgia. It was the last user-focused Windows.Maybe someone will develop a new user-focused OS that's somehow compatible with Windows programs. Or better yet, maybe Microsoft will realize very important parts of Windows are going downhill and remember what made Windows great.reply",
      "> compatible with Windows programsIt seems with each passing year this becomes less important, as more and more apps are either web based or cross platform.reply",
      "> Maybe someone will develop a new user-focused OS that's somehow compatible with Windows programs.Nothing as user focused as linux, and it's mostly compatible with windows programs with wine. Important to note though that user focused is not the same thing as easy to use.reply",
      "I'm a linux fan but calling linux user-focused is insane.reply",
      "It's user-focused in the sense that the user's goals drive the design. The good non-profit distributions, such as Debian and Arch, would never even try to require or push an online account, since that is contrary to the user's interests.reply",
      "I wonder which teams are working on these features. I'd like to meet with them in person. There are a lot to discuss.reply",
      "Two weeks ago, after Microsoft reset my default apps twice in a week, I bought an external drive, backed up all my stuff and wiped Windows.I\u2019ve got Linux all over the place, in many cloud envs, and on older hardware. But I finally committed to it on my big, meaty, main desktop. The one I use for coding and banking and accounting.I\u2019m running a Linux distro full-time. I had to hack a few minor hardware things. Nothing ChatGPT couldn\u2019t solve.I\u2019ll never do Microsoft again. I will prob add Apple MacBooks to my life, but my main grunt machine is likely to stay Linux. I\u2019m fully vested.I know I\u2019ll never engage with Microsoft shenanigans in my home environment ever again.reply",
      "That's an interesting point.  To what extent does AI support make Linux on the desktop more viable?  Reminds me of a discussion recently that said something similar, that developing in Rust is easier now that you can have another machine do battle with the borrow checker, haha.To extend, maybe someone could build a \"SysAd AI\" distribution that administers itself given natural language directions?  Let me know if anyone wants to invest.  ;-)reply",
      "With Proton in Steam, my last need for windows is gone, couldn't have happened soon enough.reply",
      "This is a bad idea. Now, with that established...Microsoft has many intelligent people who work there and certainly do many risk vs. reward calculations for each modification to Windows. From Microsoft's perspective, they have much more control over the OS when everyone's linked to a cloud account. I morally disagree with that approach, but the security issues with Windows come from unpatched systems. They tried to win over software developers by creating WSL, but the privacy- and security-minded software developers never really bit.Also, consider that Microsoft's future is obviously pivoted toward cloud infrastructure. Yes, they smartly have other ventures, but all those ventures will rely on Microsoft cloud infrastructure in some way. Server farms are a much better business model, from Microsoft's perpective, especially because it pulls Microsoft into the domains of true wealth: land acquisition, energy production, and data mining.reply"
    ],
    "link": "https://www.theverge.com/news/793579/microsoft-windows-11-local-account-bypass-workaround-changes",
    "first_paragraph": "Posts from this topic will be added to your daily email digest and your homepage feed.See All NewsPosts from this topic will be added to your daily email digest and your homepage feed.See All TechPosts from this topic will be added to your daily email digest and your homepage feed.See All MicrosoftMicrosoft really doesn\u2019t want you creating a local account on Windows 11.Microsoft really doesn\u2019t want you creating a local account on Windows 11.Posts from this author will be added to your daily email digest and your homepage feed.See All by Tom WarrenPosts from this author will be added to your daily email digest and your homepage feed.See All by Tom WarrenMicrosoft is cracking down on bypass methods that let Windows 11 installs use a local account, and avoid an internet requirement during the setup process. In a new Windows 11 test build released today, Microsoft says it\u2019s removing known workarounds for creating local accounts as they can apparently cause issues during the setup process.\u201c"
  },
  {
    "title": "The least amount of CSS for a decent looking site (2023) (thecascade.dev)",
    "points": 14,
    "submitter": "loughnane",
    "submit_time": "2025-10-06T23:47:24 1759794444",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://thecascade.dev/article/least-amount-of-css/",
    "first_paragraph": "Summary: People often over-engineer solutions, and it leads to them running into problems with their CSS. In this article, we'll take a look at the least amount of CSS that you need to make a decent looking page.Kevin PowellThe fun part of making a website is that if you write your HTML and nothing else, you have a responsive website.Granted, if you have images they can cause some overflow issues.So we can start things off by fixing that:It\u2019s possible you have videos or SVGs that are also causing problems (less likely with SVGs though), so if you need, you can expand upon this a little bit.The first thing we can do is change the font family since the default is never very exciting.We\u2019ll just use a basic system-ui for this example. It has pretty good support these days, and looks good on every system without having to worry about loading in any extra fonts.In general, the font-size is a little small as well, so we can bump it up, and the default line-height is always a bit tight, so any"
  },
  {
    "title": "RediShell: Critical remote code execution vulnerability in Redis (wiz.io)",
    "points": 26,
    "submitter": "mihau",
    "submit_time": "2025-10-06T22:30:12 1759789812",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=45497027",
    "comments": [
      "A post-auth memory corruption vulnerability scores a CVSS 10. Shellshock got like a 9.5. These scores don't mean anything.You can imagine a post-auth Redis vulnerability being deceptively well-exposed, because web apps often give partial control of the Redis key space to attackers, and don't care how long you make your strings. But this one is a UAF that requires attackers to send a malicious Lua script.reply",
      "That is unfortunate there's so many Redis instances out there that not only are exposed to the public internet (330,000) and don't have authentication configured (60,000). I'm guessing those folks probably didn't even realize their Redis was public.There are so many tutorials out there for things like Docker Compose that cause people to bind a service to 0.0.0.0 with a port open to the public internet.reply",
      "Seems similar in impact to https://nvd.nist.gov/vuln/detail/cve-2021-32626, I wonder why this has a CVE 10.This code also looks generally fixed in Lua5.4, https://github.com/lua/lua/blame/9ea06e61f20ae34974226074fc6.... Valkey and Redis really need to move to Lua that isn't so old.reply",
      "\"RediShell\" is an absolutely horrible name that makes it extremely difficult to search for things.reply",
      "Good news that it was found and fixed, but 140 days response time seems rather slow for such a critical vulnerabilityreply",
      "I'm assuming this has also been addressed in Valkey and most prominent forks as well.reply",
      "Looks like the fix was committed three days ago and they cut a release for version 8.1.4.https://github.com/valkey-io/valkey/commit/6dd003e88feace83e...https://github.com/valkey-io/valkey/releases/tag/8.1.4reply",
      "Why would you assume that?reply",
      "yikes, this is badWould think most forks would be affected as well (?)reply",
      "I'm assuming this is why Ubuntu's unattended-upgrades service uncerimoniously restarted the redis-server process on my machine late September?reply"
    ],
    "link": "https://www.wiz.io/blog/wiz-research-redis-rce-cve-2025-49844",
    "first_paragraph": "Wiz Research discovers vulnerability stemming from 13-year-old bug present in all Redis versions, used in 75% of cloud environments.Wiz Research has uncovered a critical Remote Code Execution (RCE) vulnerability, CVE-2025-49844 which we've dubbed #RediShell, in the widely used Redis in-memory data structure store. The vulnerability has been assigned a CVSS score of 10.0 - the highest possible severity.The vulnerability exploits a Use-After-Free (UAF) memory corruption bug that has existed for approximately 13 years in the Redis source code. This flaw allows a post auth attacker to send a specially crafted malicious Lua script (a feature supported by default in Redis) to escape from the Lua sandbox and achieve arbitrary native code execution on the Redis host. This grants an attacker full access to the host system, enabling them to exfiltrate, wipe, or encrypt sensitive data, hijack resources, and facilitate lateral movement within cloud environments.Given that Redis is used in an estim"
  },
  {
    "title": "Mise: Monorepo Tasks (github.com/jdx)",
    "points": 290,
    "submitter": "jdxcode",
    "submit_time": "2025-10-06T14:07:46 1759759666",
    "num_comments": 61,
    "comments_url": "https://news.ycombinator.com/item?id=45491621",
    "comments": [
      "This is beautiful! Thank you so much!When I was primarily using Python, I didn't really \"get\" Mise. Uh, that's what we have uv for! But it really shines when using things like Node where you want a specific version in each directory, and also want common entrypoints like `mise build` or `mise test` in every repo, regardless of its language(s).Don't get me wrong: I also adore Just as a task runner. It's what got me off of Make, which is incredibly powerful but somewhat lacking in the DevEx department to say the least. It's probably more \"powerful\" than Mise's tasks. However, Mise's combination of really good \u2014 not astounding, but really good \u2014 task runners plus all the tool management stuff is unbeatable for the things I work on.reply",
      "Just curious, as I am a fan of simple Makefiles, what benefits did you encounter moving from Make -> Just -> Mise?reply",
      "The biggest jump was from Make -> Just. Just -> Mise was minor in comparison, although enough to persuade me.Just has a lot of UI/UX improvements over Make, like a way to list available recipes, convenient ways to define command-line parameters to recipes, consistent and easy syntax, and a whole lot of predefined functions and variables to cover common use cases like finding the number of CPUs on the current system, manipulate strings, etc. It doesn't do things that Make can't do, because Make can do anything a shell script can if you don't mind wrestling it into submission. It just does those things much more easily.But it still has a few warts. Recipes look a lot like a shell script, but they're evaluated separately line-by-line, so you can't set a variable on one line and then read it in another. There are workarounds, but that's the default behavior. And a lot of the time when I'd want a task runner, I also want an environment manager (like uv or cargo or node/npm), so bundling those together matches my workflow better than managing those separately.I have zero bad to say about Just. It's freaking awesome. If Mise disappeared, I'd go back to using Just. I just prefer Mise right now.reply",
      "I always thought the idea of Makefiles is great, but the language is terrible and completely unintuitive to understand and learn.reply",
      "Taskfile is so much better.  mise is what actually got me to start switching everything to Taskfile instead of Makefile as mise easily handles bootstrapping Taskfile and anything else you would need.Had been using make for simple tasks for ~8 years and just got tired of how limiting it is.reply",
      "Could you explain/share quickly how you combine mise and Taskfile?reply",
      "Yeah I love just, but getting the correct environment in a just task can be messy, even loading a virtualenv is annoying. I've also switched to mise for this reasonreply",
      "I'm really bullish on mise as a tool. It's quickly become one of my goto tools when starting a new project. Being able to have one config file to manage tools (node, python, rust, go, etc) as well as a simple makefile replacement makes it incredibly convenient. I pretty much always setup a `postinstall` hook so all someone has to do is `mise install` one of my projects and they'll get all the correct tool versions as well as having dependencies installed (like running `npm install`) automatically.I feel it's significantly more practical than something like nix which feels like it has a steep learning curve.reply",
      "There's a tool that makes the Nix way a lot more approachable: https://devenv.sh/e.g. `languages.rust.enable = true` and you're off to the races. You can add scripts, tasks, other packages, etcreply",
      "I\u2019ve been using devenv for about 6 months now. I\u2019ve started new projects with it and migrated old ones to use it as well. I\u2019ve also set up my org\u2019s repositories with it. Onboarding devs to projects is simple. All everyone needs on their local machine is git, nix, and devenv. Bonus points for using it with direnv for automatic shell activation when you enter a directory. Direnv allows for IDE integrations as well for project dependencies.reply"
    ],
    "link": "https://github.com/jdx/mise/discussions/6564",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page. There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page. There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n -\n           \n We're excited to announce Monorepo Tasks, a powerful new feature that brings first-class monorepo support to mise tasks! \ud83d\ude80Monorepo Tasks allows you to manage tasks across multiple projects in a single repository, with each project maintaining its own tools, environment variables, and tasks. Think of it as bringing the power of tools like Bazel or Turborepo to mise's task system, but with mise's signature simplicity.All tasks across your monorepo are automatically discovered and"
  },
  {
    "title": "Translating Cython to Mojo, a first attempt (fnands.com)",
    "points": 33,
    "submitter": "fnands",
    "submit_time": "2025-10-06T20:09:44 1759781384",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=45495738",
    "comments": [
      "A more interesting path is to keep dbscan_inner in pure python with type annotations and then use  py2many --mojo=1 dbscan_inner.py\n\nto translate.reply",
      "Somehow just trying to navigate to this website makes my browser crash.Firefox on Android with NoScript.reply",
      "Something with Noscript is causing it. I was able to load it fine, then installed noscript and it suddenly crashedreply"
    ],
    "link": "https://fnands.com/blog/2025/sklearn-mojo-dbscan-inner/",
    "first_paragraph": "Ferdinand Schenck July 28, 2025Ever since I heard about Mojo I (and presumably most other people) thought it would be a good language to speed up functions to be called from Python. Everyone knows that vanilla Python can be slow, but one of the reasons that Python programs can be reasonably fast in practice is because Python often leans on libraries written in more performant languages, predominantly C/C++, but increasingly also Rust.Until recently, there has been no real way to call Mojo code from Python, but about a month ago (in Max release 25.4) the ability to call Mojo from Python was added as a beta feature. It\u2019s not fully cooked yet, and it will likely still change a lot, but I wanted to give it a look just to get an idea of where things are heading.One specific idea that I had when I heard about Mojo was that Mojo might be a good replacement for Cython and apparently I was not the only one to have had this thought:The comments are from the HackerNews discussion on Vincent Warme"
  },
  {
    "title": "Indefinite Backpack Travel (jeremymaluf.com)",
    "points": 268,
    "submitter": "renjieliu",
    "submit_time": "2025-10-02T17:12:23 1759425143",
    "num_comments": 239,
    "comments_url": "https://news.ycombinator.com/item?id=45452472",
    "comments": [
      "I did this for a bit. The approach is very useful to learn. Traveling light is so, so much better. I never check bags unless there's a very good reason. It does have some downsides.You're constantly making and throwing away relationships. I found this the hardest part.Your lifestyle is subsidized by society. You depend on other people and services to make it work. You either have to eat out all the time or make many small trips to grocery stores and rent places with furnished kitchens. There is no self reliance, no preparation for things to go wrong besides saving money and hoping you can buy your way out.There are opportunity costs. If OP had bought real estate in 2015 they would be better off financially. It's one of the reasons I stopped living in a bus and bought a house. Had I bought the last two times I \"moved\" in 2011 or 2016 I'd have almost enough money to retire and live OP's lifestyle permanently.reply",
      "One way or another, I think most of us have lifestyles subsidized by society.OP could also have bought stocks in 2015, and perhaps done even better than buying a house. Since the beginning of that year, the S&P500 has more than tripled, while housing has gone up about 50% (though of course leverage helps). For all we know, OP does hold stocks, which wouldn't cramp his lifestyle at all.Plus he claims to spend less with this lifestyle, which also helps.https://www.in2013dollars.com/Housing/price-inflationreply",
      "> You're constantly making and throwing away relationships.Yeah, this lifestyle basically only works as a single young adult. Once you have a significant other, it's very, very hard. Once you have a kid, it's impossible.reply",
      "I lived out of a backpack for two months on a Pacific Crest Trail hike. I got comfortable with it and told myself that I had overcome my materialism, and could henceforth live happily without a lot of stuff and conveniences.Not so much. Now a couple of decades later, I've got a house and garage crammed with stuff. Yesterday I had a plumber here working on a leak, and this morning I have no running water, and here I am bravely holding back tears. My inner dialog is \"this is unacceptable!\" It turns out that climbing on the hedonic treadmill is practically effortless, but sliding down it is full of splinters.reply",
      "> It turns out that climbing on the hedonic treadmill is practically effortless, but sliding down it is full of splinters.Not sure if I'm missing a joke, but the whole point of the analogy being a treadmill is that there's nothing to fall down. Regardless of positive (running forward) or negative (going backward on the treadmill) life changes, your happiness will probably stay relatively consistent because you're on a treadmill and there's nowhere to go.The live out of a backpack lifestyle is definitely a unique way to experience the modern world and I'm sure it's fulfilling for the author, but you can even tell in their post that life caught up with them somewhat and they needed to start staying in one place a little longer in order to maintain social relationships. Their linked post about walking every block of Manhattan and tracking all of their movement since 2015 feels like the exact opposite of a minimalist lifestyle and it seems to me like they live out of a backpack not out of some anti-materialism lifestyle, but instead just as a practical way to fuel this obsession with traveling and tracking.I admit, I've seen the author's Instagram story about walking 100k steps in a day in NYC and watched the whole thing because it's interesting, but I also take that and posts like this with a grain of salt. I'll happily take my horde of shit I need to get rid of in the garage over obsessing about how I can optimize tracking my every movement.reply",
      ">Not sure if I'm missing a joke, but the whole point of the analogy being a treadmill is that there's nothing to fall down. Regardless of positive (running forward) or negative (going backward on the treadmill) life changes, your happiness will probably stay relatively consistent because you're on a treadmill and there's nowhere to go.That's not the point of the treadmill analogy.It's rather that you need to keep walking to maintain your stationary position, just like on a treadmill.Meaning the level of headonism you become accustomed to fades/blunts with time, and you want more, so you need to keep moving forward to stay at the same (hedonic) position (level).What the parent said, then, is valid: \"climbing on the hedonic treadmill is practically effortless\", being on a hedonic treadmill is our default psychological state. But to slide off and accept less hedonic level is very difficult.reply",
      "Perhaps the joke is that the valence of going backward or forward isn't equivalent. IIRC, some studies show that people will generally accept smaller gains to avoid the possibility of loss. Eg, loss has a greater (negative) emotional impact than equivalent (positive) gains.reply",
      "> I don't think it's reasonable to compare the risk of suffering a large loss with the risk of missing out on a large gain. If your annual income is $N, missing out on a gain of $N is bad, but not nearly as bad a suffering a loss of $N.- A comment I saw elsewhere on HN todayIt's quite rational, in many cases, to consider loss of an object X worse than gain of X. A less rigorous example I like to use: it's far easier, and unequal, to kill a person than revive a person.reply",
      "Isn't this the accepted advice for investment. Putting money in something that gains X% consistently with years of proven sustained growth vs putting money in something with potential 100X% growth but could give you negative growth if it fails has always been the advice in long term financial planningreply",
      "This, in a nutshell, is why Effective Altruism sucks.reply"
    ],
    "link": "https://jeremymaluf.com/onebag/",
    "first_paragraph": "\n/// Jeremy Maluf\nUpdated September 2025In 2015 I got rid of everything I owned that didn\u2019t fit in a laptop backpack, and I\u2019ve been living at this level of minimalism since. The idea is to only own what I need, which allows me to focus more, spend less, travel spontaneously and simplify my life.I update this post yearly, with past versions available on the Internet Archive: 2022, 2021, 2020, 2019, 2018, 2017. Some of the links on this page are affiliate links. For more content on this lifestyle, I\u2019ve started sharing videos on Instagram.10 years in and somehow millions of people have read this page! Thank you for stopping by! :)This post was never meant to be a guide, but whenever it\u2019s shared without exposition a lot of the responses online tend to be the same dozen questions and misunderstandings. So here\u2019s a few words to address those.Onebag travel is unquestionably the best way to travel. Traveling without luggage removes just about every pain point associated with flying, such as ch"
  },
  {
    "title": "Show HN: Kent Dybvig's Scheme Machine in 400 Lines of C (Heap-Memory Model) (gist.github.com)",
    "points": 183,
    "submitter": "swatson741",
    "submit_time": "2025-10-06T14:06:29 1759759589",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=45491609",
    "comments": [
      "Kent Dybvig also wrote Chez Scheme [1] which on most benchmarks is far-and-away the fastest [2] Scheme implementation.Racket recently got rewritten to be based off of Chez [3] and I have it from the maintainer of Racket himself that it\u2019s paid off well: better performance and a smaller, more maintainable codebase.1: https://github.com/cisco/ChezScheme (also, pronounced \u201cShay Scheme\u201d)2: https://ecraven.github.io/r7rs-benchmarks/3: https://m.youtube.com/watch?v=s3Q3M2wZ7rI&pp=0gcJCRsBo7VqN5t...reply",
      "Reading and hacking on the Chez Scheme codebase is always a treat and rather inspiring, especially compared with more mainstream compilers and code generators. As well as Kent Dybvig, Andy Keep's contribution (nanopass) is super-impressive. The whole thing is so cleanly designed and beautifully expressed.reply",
      "I believe MIT Scheme compiled directly to C by intermingling switch statements and gotos to emulate its call stack. Problem was, as programs grew, compile times weren't linear. I gave it a shot once, it was a somewhat spiritual experience:https://github.com/glouw/switchreply",
      "Thanks!  Do you mean MIT Scheme's C backend?  I've used MIT Scheme on and off for a long time and have never touched the C backend & have no idea how it works, so this is interesting.(MIT Scheme also has a native code compiler for Intel CPUs, which seems to be what most users of MIT Scheme (an admittedly small community) actually use.)reply",
      "If you'd like to define the backend like this, it's the easiest way to compile to C; we're just repurposing C as a general-purpose assembler: https://glouw.com/2023/11/07/Switch.htmlI believe MIT-scheme took it a step further with gnu extensions which allowed you to take the address of a label like &&label, which allowed for function pointers: https://gcc.gnu.org/onlinedocs/gcc/Labels-as-Values.htmlreply",
      "I don't see anything in the manual about MIT scheme compiling to C, just to its own native code object files and some portable bytecode...https://www.gnu.org/software/mit-scheme/documentation/stable...reply",
      "It's there, but the documentation is weak.The release notes from long ago when that back end was released are here, and give some detail:https://share.google/xmUnDhC7lndujD7TIreply",
      "I think a very rudimentary implementation (which is easy to understand) is in the SICP bookreply",
      "Even MIT Scheme was used with SICP, the Scheme implementation\nin the book is different from MIT Scheme.MIT Scheme was for a long period one of the leading Scheme implementations.\nIt lacks support for Apple Silicon, so it is not as popular now, as it once was.https://www.gnu.org/software/mit-scheme/reply",
      "Yes. In Apple the memory can be writable XOR executable. MIT scheme needs to write on pages that will execute, so it will trigger the MMU..reply"
    ],
    "link": "https://gist.github.com/swatson555/8cc36d8d022d7e5cc44a5edb2c4f7d0b",
    "first_paragraph": "\n        Instantly share code, notes, and snippets.\n      "
  },
  {
    "title": "WebGPU and the Price of Compiling WGSL (hugodaniel.com)",
    "points": 30,
    "submitter": "todsacerdoti",
    "submit_time": "2025-10-06T21:14:51 1759785291",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=45496406",
    "comments": [
      "I don't understand why WebGPU does not accept pre-compiled shaders. Every WebGPU implementation supports loading SPIR-V, but the spec just doesn't have it.reply",
      "It wasn't a technical decision. Apple decided to veto the use of any Khronos IP such as SPIR-V or GLSL due to a private legal dispute which was never elaborated on. That left the committee with no choice but to reinvent the wheel, and now we're stuck with WGSL forever.It's not the end of the world for web-only projects which can just target WGSL exclusively, but it's a pain in the ass for cross platform engines which now need to support Yet Another Shader Backend. From the old minutes:> Eric B (Adobe): Creating a new high level language is a cardinal sin. Don\u2019t. Do. That. Don\u2019t want to rewrite all my shaders AGAIN.> Jesse B (Unity): If we can transcode to HLSL to whatever you need, great. If we can\u2019t, we may not support your platform at all.> Eric B: Would really not like even to write another transcoder. If there\u2019s an existing tool to get to an intermediate representation, that\u2019s good. Would suggest SPIRV is an EXCELLENT existing intermediate representation.reply",
      "From a previous thread on this topic: https://news.ycombinator.com/item?id=23089745>It's literally in past WebGPU meeting minutes: Apple objected to SPIR-V due to disputes with Khronos. Tint is a compromise, it doesn't matter who proposed it.>\"MS: Apple is not comfortable working under Khronos IP framework, because of dispute between Apple Legal & Khronos which is private. Can\u2019t talk about the substance of this dispute. Can\u2019t make any statement for Apple to agree to Khronos IP framework. So we\u2019re discussing, what if we don\u2019t fork? We can\u2019t say whether we\u2019re (Apple) happy with that. NT: nobody is forced to come into Khronos\u2019 IP framework.\">https://docs.google.com/document/d/1F6ns6I3zs-2JL_dT9hOkX_25...reply",
      "So apple's making graphics apis worse this time around :)reply",
      "SPIR-V is not precompiled in any meaningful sense of the word. It's just a SSA graph representation of the source code, and only saves you the lexing, parsing and SSA gen steps. The real meat of the compilation happens after that, when the GPU driver compiles the representation into actual GPU shader assembly.reply",
      "If you have precompiled SPIR-V you can simply use SpirvReader to go to wgsl.https://dawn.googlesource.com/tint/+/refs/heads/chromium/466...reply",
      "This probably wont help at all when it comes to the cost of compilation. WGSL -> SPIR-V is very fast. It is the pipeline compilation that is slow (aka whatever happens in the drivers).reply",
      "One of the problems is that libraries consuming SPIR-V are generally not robust enough to handle untrusted web shaders. Also, DirectX doesn't (yet) accept SPIR-V shaders so you'd mandate some translation to HLSL, which in turn would be compiled by dxcompiler.dllreply",
      "But writing an entirely new, bespoke high-level shader programming language is more robust? And robust in what way? The SPIR-V format is vastly easier to parse than a textual language.And WGSL will still bounce through HLSL for DirectX because DXIL is an awful, undocumented mess of ancient LLVM-IR with a giant pile of bolt-on special semantics. Directly authoring DXIL is awful.reply",
      "No need to transcode to HLSL, DXC already accepts SPIR-V input (and both Chrome and FF are shipping DXC).reply"
    ],
    "link": "https://hugodaniel.com/posts/webgpu-diagnostics/",
    "first_paragraph": ""
  },
  {
    "title": "Nobel Prize in Physiology or Medicine 2025 (nobelprize.org)",
    "points": 326,
    "submitter": "lode",
    "submit_time": "2025-10-06T09:41:16 1759743676",
    "num_comments": 72,
    "comments_url": "https://news.ycombinator.com/item?id=45489533",
    "comments": [
      "Funny anecdote that Dr. Brunkow thought she was being spammed when the Nobel Committee tried to inform her:>Brunkow, meanwhile, got the news of her prize from an AP photographer who came to her Seattle home in the early hours of the morning. She said she had ignored the earlier call from the Nobel Committee. \u201cMy phone rang and I saw a number from Sweden and thought: \u2018That\u2019s just, that\u2019s spam of some sort.\u2019\u201dhttps://www.oregonlive.com/pacific-northwest-news/2025/10/sc...reply",
      "Meanwhile, Fred Ramsdell probably still doesn\u2019t know he\u2019s won it because he\u2019s backpacking in Idaho.He\u2019ll be in for a surprise when he switches his phone back on.reply",
      "That's Idaho, USA for anyone who lives in one of the 194 other countries in the world (yes we do exist!)reply",
      "According to Gemini, the place name Idaho is unique to the USA.reply",
      "Am I understanding correctly that this Nobel prize is for work that was completed over 20 years ago? I'm not a biologist but it sounds like they discovered regulatory T cells together, which sounds relatively major. Is it typical for a Nobel prize to lag that kind of discovery for decades? Or is it only now that we understand how major the discovery was? Or maybe I'm just misunderstanding the discovery and the timeline.reply",
      "At least in Physics, on average every year there is more than one discovery that is worth a Nobel prize. So there is an increasing backlog of people who should get a Nobel prize. You can look at the list and check that people in the 1920s got their prize about 15 years after their work [1]. But recently people have been getting it about 30-40 years after.[1] https://en.wikipedia.org/wiki/List_of_Nobel_laureates_in_Phy...reply",
      "Yes, many Nobel prizes are awarded for work that was completed decades ago in part to ensure that the work passes the test of time.reply",
      "Highs had a delay of 49 years from paper to prize, though he got the prize the year after his theory was experimentally confirmed.reply",
      "I can\u2019t say I would react too differently. There are so many emails or phone calls claiming you\u2019ve won a big award or sum of money that end up being scams.reply",
      "It\u2019s interesting that two of the two American recipients weren\u2019t recognized by other awards like membership in the National Academy of Sciences or the National Academy of Medicine.  Truly black horse candidates which makes this fun.reply"
    ],
    "link": "https://www.nobelprize.org/prizes/medicine/2025/press-release/",
    "first_paragraph": "EnglishEnglish (pdf)SwedishSwedish (pdf)6 October 2025The Nobel Assembly at Karolinska Institutet has decided to award the 2025 Nobel Prize in Physiology or Medicine to:Mary E. BrunkowInstitute for Systems Biology,Seattle, USAFred RamsdellSonoma Biotherapeutics,San Francisco, USAShimon SakaguchiOsaka University,Osaka, Japan\u201cfor their discoveries concerning peripheral immune tolerance\u201dThe body\u2019s powerful immune system must be regulated, or it may attack our own organs. Mary E. Brunkow, Fred Ramsdell and Shimon Sakaguchi are awarded the Nobel Prize in Physiology or Medicine 2025 for their groundbreaking discoveries concerning peripheral immune tolerance that prevents the immune system from harming the body.Every day, our immune system protects us from thousands of different microbes trying to invade our bodies. These all have different appearances, and many have developed similarities with human cells as a form of camouflage. So how does the immune system determine what it should attack "
  },
  {
    "title": "Valorant's 128-Tick Servers (2020) (riotgames.com)",
    "points": 167,
    "submitter": "nairadithya",
    "submit_time": "2025-10-06T20:47:25 1759783645",
    "num_comments": 89,
    "comments_url": "https://news.ycombinator.com/item?id=45496146",
    "comments": [
      "128 ticks per second servers. (And lo, suddenly the article's thesis is inherently clear.)A \"tick\", or an update, is a single step forward in the game's state. UPS (as I'll call it from here) or tick rate is the frequency of those. So, 128 ticks/s == 128 updates per sec.That's a high number. For comparison, Factorio is 60 UPS, and Minecraft is 20 UPS.At first I imagined an FPS's state would be considerably smaller, which should support a higher tick rate. But I also forgot about fog of war & visibility (Factorio for example just trusts the clients), and needing to animate for hitbox detection. (Though I was curious if they're always animating players? I assume there'd be a big single rectangular bounding box or sphere, and only once a projectile is in that range, then animations occur. I assume they've thought of this & it just isn't in there. But then there was the note about not animating the \"buy\" portion, too\u2026)reply",
      "When Battlefield 4 launched it had terrible network performance. Battlefield 3 wasn't great but somehow BF4 was way worse. Turned out while clients sent updates to the server at 30 Hz, the server sent updates back only at 10 Hz[1].This was the same as BF3, but there were also some issues with server load making things worse and high-ping compensation not working great.After much pushback from players, including some great analysis by Battle(non)sense[2] that really got traction, the devs got the green light on improving the network code and worked a long time on that. In the end they got high-tickrate servers[3][4], up to 144Hz though I mostly played on 120Hz servers, along with a lot of other improvements.The difference between a 120Hz server and a 30Hz was night and day for anyone who could tell the difference between the mouse and the keyboard. Problem was that by then the game was half-dead... but it was great for the 15 of us or so still playing it at that time.[1]: https://www.reddit.com/r/battlefield_4/comments/1xtq4a/battl...[2]: https://www.youtube.com/@BattleNonSense[3]: https://www.reddit.com/r/battlefield_4/comments/35ci2r/120hz...[4]: https://www.reddit.com/r/battlefield_4/comments/3my0re/high_...reply",
      "Also for comparison, the Runescapes (both RS3 and Oldschool Runescape) have a 0.6 tick/second system (100 ticks/minute). It works rather well for these games, which I guess highlights that some games either a) can get away with high latencies depending on their gameplay mechanics, or b) will evolve gameplay mechanics based on the inherent limitations of their engines/these latencies. RS3 initially leaned into the 0.6s tick system (which is a remnant of its transitions from DeviousMUD to Runescape Classic to RS2) and eventually developed an ability-based combat system on top of what was previously a purely point-and-click combat system, whereas OSRS has evolved new mechanics that play into this 0.6s tick system and integrate seamlessly into the point-and-click combat system.Having played both of these games for years (literally, years of logged-in in-game time), most FPS games with faster tick systems generally feel pretty fluid to me, to the point where I don't think I've ever noticed the tick system acting strange in an FPS beyond extreme network issues. The technical challenges that go into making this so are incredible, as outlined in TFA.reply",
      "100ticks/minute is 1.666... ticks per second, not 0.6.reply",
      "Whoops, bit of a slip there. It is 100 ticks per minute, or 1 tick every 0.6 seconds. I was wrong in the first description there.reply",
      "They're right, when they said 0.6s tick they mean there's a tick every 0.6 seconds.It's important to some players because you can get some odd behaviour out of the game by starting multiple actions on the same tick or on the tick after you started a different action. It's ridiculous click intensive but you can get weird benefits like cutting the time to take an action short or get xp in 2 skills at once.reply",
      "> I assume there'd be a big single rectangular bounding box or sphere, and only once a projectile is in that range, then animations occur.Now that's a fun one to think about. Hitscan attacks are just vectors right? So would there be some perf benefit to doing that initial intersection check with a less-detailed hitbox, then running the higher res animated check if the initial one reports back as \"Yeah, this one could potentially intersect\"? Or is the check itself expensive enough that it's faster to just run it once at full resolution?Either way, this stuff is engineering catnip.reply",
      "This is the basis for basically every physics engine in some form of another. Collision is divided into the \"broad phase\" pruning step that typically uses the bounding box of an object and a \"narrow phase\" collision detection step that uses the more detailed collision primitivesreply",
      "Fallout 76, for example, lets you see where other players are facing/looking at, or where are they pointing their guns even if they don't fire. The models are animated according to the input of their users.I don't think its ticks per second are great, because the game is known for significant lag when more than a dozen of players are in the same place shooting at things.reply",
      "Lag is different than \u2018unloaded\u2019 ticks per second.reply"
    ],
    "link": "https://technology.riotgames.com/news/valorants-128-tick-servers",
    "first_paragraph": ""
  },
  {
    "title": "My Life in Ambigrammia (theatlantic.com)",
    "points": 27,
    "submitter": "fortran77",
    "submit_time": "2025-10-06T20:07:25 1759781245",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=45495711",
    "comments": [
      "Also, gift link: https://www.theatlantic.com/ideas/archive/2025/10/ambigrams-...reply",
      "Recent and related:Ambigr.am - https://news.ycombinator.com/item?id=45478780 - Oct 2025 (40 comments)reply",
      "I used to make these quite often:\nhttps://cards.azriel.im/Now I still make them occasionally, though I haven't updated the blog for a while.I find that tangible art could sometimes say \"thank you\" more than the utterance of the words themselves.Also makes for a great wedding gift:https://cards.azriel.im/2018/09/kevin-fiona.htmlreply",
      "note the author is Douglas Hofstadter who wrote Godel Escher Bachhttps://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bachreply",
      "GEB was what really turned me on to math. I mean, I was good at math in high school, but was satisfied to work through the problems and proofs, or use math in my physics class and electronics hobby. But my older brother read GEB then gave it to me. I had also been reading Hofstadter's \"Mathematical Games\" in Scientific American.But GEB exposed me to a level of math that seemed to beyond puzzles and applications. Or maybe it was just his engaging style. Or my cool big brother. Regardless, I started college as a math major.Other things happened since then, and I now have a physics degree, but still enjoy math as an end unto itself.reply",
      "Such a great book! I feel like skimming through it again now, to remind myself exactly how Godel's argument worked.reply",
      "Exactly. Along with a few other books.reply",
      "It's exciting to see that a new Hofstadter book is out!With respect to bringing beauty into the world in dark times, it's always worth remembering Shostakovich's Symphony No. 7 in C Major, Op. 60, \"Leningrad\", composed in Leningrad during its 900-day siege by the Nazis, and first performed there later that year, with some of the musicians fainting from starvation: https://www.youtube.com/watch?v=KOkBEqtGUI8 https://en.wikipedia.org/wiki/Symphony_No._7_(Shostakovich)\u2014 \u2042 \u2014In case there are other Hofstadter letterform fans here, last year I tracked down the \"gridfonts\" repository his research group had put together as part of their work on \"letter spirit\" last millennium, at https://wayback.archive-it.org/219/20060606215909/http://www....  There were 287 gridfonts in it.  I reverse-engineered the file format (before finding the Scheme code that decoded it), hacked together a Python 3 script http://canonical.org/~kragen/sw/dev3/gridfontparse.py to convert it to PostScript, and produced this PDF with all the gridfonts: http://canonical.org/~kragen/sw/dev3/all-gridfonts.pdfI think I may have been the first person to see some of these fonts in 20 years.  But, apparently, there were hundreds more.  I have a vague memory that maybe they were lost in a disk crash.Letterforms aren't copyrightable under US law, where the file in question was initially published, but outline font files are because they are \"computer programs\".  Gridfont letters are 56-bit bitmaps indicating which segments are turned on or off, which to me are obviously not computer programs.  Nobody that I know of has ever litigated over letterforms like these:    font : benzene right\n    creator : doug\n    create date : Tue Feb 19 15:39:48 EST 1991\n    last edit : feb 24 94\n    a 058002400B0000\n    b 04824B00090000\n    c 04800100090000\n\nso I don't think their copyright status has ever been decided.  So, if you decide to use these in your product logo or something, there's no guarantee you won't lose a lawsuit to Hofstadter (or his estate, or Indiana University).  Don't say I didn't warn you.That said, that wasn't the motivation for creating them, so I think the risk is fairly small.reply",
      "Another Doug Hofstadter book! This is so cool. Ambigrams have been one of my fav \"things\" since I saw them in the GEB book ages ago.FWIW, \"Angels and Demons\" bestseller thriller (?) has a few ambigram puzzles that the protagonist Robert Langdon solves to get to the mystery of the Rosslyn chapel (my memory may be fading here)--x--Thanks for this set of amazing fonts! They look amazing. Very glyph-y. And they tickle my inner geek.reply"
    ],
    "link": "https://www.theatlantic.com/ideas/archive/2025/10/ambigrams-words-double-meanings-art/684404/",
    "first_paragraph": ""
  },
  {
    "title": "Launch HN: Grapevine (YC S19) \u2013 A company GPT that actually works (getgrapevine.ai)",
    "points": 65,
    "submitter": "eambutu",
    "submit_time": "2025-10-06T15:39:59 1759765199",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=45492564",
    "comments": [
      "Offer self-hosted and I would buy.Do not assume that companies are willing to put ALL of their intellectual property into your hands. Even if you would not be some startup where any sysadmin could steal and sell my data any time without you even noticing it, you will get hacked just like everyone else that stores interesting data. The data you have access to is absolutely perfect for the global data blackmailing gangs. As soon as you are successful, you will have every black hat hacker and their dog knocking on your doors.reply",
      "Yep, makes a lot of sense. We architected our system to be easy to self-host & open-source in the future for this very reason, though we decided to launch with hosted because it's easier to improve and iterate.reply",
      "Understood. Not my startup, but I would have started the other way round.Businesses that would be willing to pay (a lot) for such a benefit often will be very conservative. In Germany the majority of medium sized businesses using SAP for example still refuse to be moved to SAP's cloud instead of on-premise.C-Level types typically are not worried putting their email credentials etc into Outlook cloud and getting hacked this way. They are used to \"everything is in the cloud\". However, as soon as you mention, depending on the type of business \"patents\", \"sales contacts\", \"production plans\" C's will change their mind.In Germany, where I am originally come from, all of these businesses are worried about their trade secrets to end up in China, and rightly so.As self-hosting is very complex you could either make good money with consulting (but this means setting up tech teams in all target markets around the globe, using actual competent humans), or by selling it as a plug&play appliance. With that appliance simply being a rack server with a suitable GPU installed.And again, for your business strategy the long-term risk of pretty much everyone trying to hack you on a daily basis appears too high to me. You might not have on your radar how serious industry spionage is. You will definitely have a fake utility company worker coming into your offices, trying to plug in a USB keylogger into some PC while nobody is looking.As an example, proven strategy: Find targets internet uplink. Cut it. Customer calls ISP for help. You then send a fake ISP technician that arrives before the real one does. You put a data exfiltration dongle between the modem and the LAN. You then fix the cut outdoor line. Customer is happy that you have fixed it. Later the actual ISP guy arrives. Everyone will be a bit confused that the problem was already fixed, but then agree that it's probably just the ISP once again having screwed up their resource management. Works pretty much every time.reply",
      "Two things to think about:a) Due to privacy laws, no European country would right now be allowed to use your service. The data your customers wants to index will always contain stuff that allows to identify a human, and once you are there it's basically \"game over\" for handing over data to a third party provider like you.b) My organization is tiny. But we are in a sector were we must be ultra paranoid when it comes to security. We do not use a single external service whatsoever, everything is self-hosted. I would love to be able to AI-index all of our collected knowledge and would pay for the value this provides. So far have been unable to find any plug & play solution. Then open source nature you have mentioned is important so that your system security can be be validated, but in the end I would rather want to pay for it being plug&play AND on-premise AND open source.reply",
      "Onyx.app has a self hosted option. I just did the docker setup yesterday. It\u2019s not a great home user option imo but seems like it\u2019s functional for enterprise.reply",
      "Just had a quick look - while they have that self-hosting option, they still assume you will use a cloud LLM. I started digging because I got confused of them not mentioning any GPU when it comes to resource requirements. There is some documentation on using it fully self-hosted including the LLM, but the emphasis here is on \"some\".To be clear: I am looking at this from a CEO perspective, not a \"I will play with it in my spare time\" nerd one.reply",
      "controlcore.io was brought to the market for the same exact reason. Not AI Powered, but to control AI and its interactions with your Data, APIs,. Applications etc. And yes, we just give our service as a self-hostable solution. However is the encryption and SOC compliance be, we want our clients to know that none of their internal data or interaction transaction leave their control.reply",
      "These automation startups are cookedreply",
      "\">85% of answers are helpful & accurate\"People can usually tell if an answer isn't helpful, but not always that it isn't accurate. Depending on the context, 85% accurate might not be good enough.reply",
      "Yep, the other commenter is right--85% is helpful AND accurate. I'd love for you to give it a try and see if 85% is not good enough though. There's always more to push on quality and the more real feedback we get the better we can prioritize what people need.reply"
    ],
    "link": "https://getgrapevine.ai/",
    "first_paragraph": "Try it nowTry it nowOne AI agent that searches across your docs, code, and communication\u2014so you don\u2019t have to.Try it nowTry it nowTry it nowWatch DemoDemoWatch DemoThreadAleks12:12 PMI wish there were a company GPT that ACTUALLY worked + wasn't exorbitantly expensive \ud83d\ude2a\ud83d\ude2a1 replyGrapevine12:12 PMI can help with that, and you can get started for free!ThreadAleks12:12 PMI wish there were a company GPT that ACTUALLY worked + wasn't exorbitantly expensive \ud83d\ude2a\ud83d\ude2a1 replyGrapevine12:12 PMI can help with that, and you can get started for free!ThreadAleks12:12 PMI wish there were a company GPT that ACTUALLY worked + wasn't exorbitantly expensive \ud83d\ude2a\ud83d\ude2a1 replyGrapevine12:12 PMI can help with that, and you can get started for free!#team-infraFor commonly asked cross-team questionsJohnnyJul 28th at 4:23 PMHey Infra team, I\u2019d like to create a new S3 bucket...Actual Slack thread3 sourcesReal examples of us using Grapevine internally, over the last 2 months>85%>85%>85%of answers are helpful & accurateof answers"
  }
]