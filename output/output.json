[
  {
    "title": "Blender releases their Oscar winning version tool (blender.org)",
    "points": 79,
    "submitter": "babuloseo",
    "submit_time": "2025-03-27T00:27:03 1743035223",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=43489114",
    "comments": [
      "Although I've never contributed with Blender, I felt proud when I saw \"made with Blender\" in the credits.Blender is a jewel of the FLOSS movement and a history and behavior that must be mimicked by many other projects.Looking forward to more successes like this.\n \nreply",
      "I remember using Blender when everyone was saying how terrible of a UX it was.They've turned it around and it's become a default-first for many artists.Open source of not, it of course helps, that the competition charges absolutely mind-bogglingly high amounts of money, for a similar offer.\n \nreply",
      "Is their a central place where other open source people or just programmers in general can get a breakdown  on how they improved the UI/UX?\n \nreply",
      "I made a live action movie, edited with Blender as a video editor.It was great at the time, I\u2019m sure has improved a lot in the last 8 years too.Blender and Inkscape are some of the software listed in the credits.\n \nreply",
      "agreed, Blender is tremendous\n \nreply",
      "Is the best OSS software Blender, Ghidra or Linux?\n \nreply",
      "Probably Linux if I had to pick one, partly because of how many tools were written for or on it and the ecosystem it's built up.\n \nreply",
      "to answer your question its probably git.\n \nreply",
      "I think Git is probably the most \"useful\" but given how bad it is (despite how good it is), I have a hard time calling it the \"best\"\n \nreply",
      "If we are going to go infrastructure, i would say sqlite or curl.\n \nreply"
    ],
    "link": "https://www.blender.org/download/releases/4-4/",
    "first_paragraph": ""
  },
  {
    "title": "OpenAI adds MCP support to Agents SDK (openai.github.io)",
    "points": 416,
    "submitter": "gronky_",
    "submit_time": "2025-03-26T18:55:29 1743015329",
    "num_comments": 142,
    "comments_url": "https://news.ycombinator.com/item?id=43485566",
    "comments": [
      "Today MCP added Streamable HTTP [0] which is a huge step forward as it doesn't require an \"always-on\" connection to remote HTTP servers.However, if you look at the specification it's clear bringing the LSP-style paradigm to remote HTTP servers is adding a bunch of extra complexity. This is a tool call, for example:    {\n      \"jsonrpc\": \"2.0\",\n      \"id\": 2,\n      \"method\": \"tools/call\",\n      \"params\": {\n        \"name\": \"get_weather\",\n        \"arguments\": {\n          \"location\": \"New York\"\n        }\n      }\n    }\n\nWhich traditionally would just be HTTP POST to `/get_weather` with `{ \"location\": \"New York\" }`.I've made the suggestion to remove some of this complexity [1] and fall back to just a traditional HTTP server, where a session can be negotiated with an `Authorization` header and we rely on traditional endpoints / OpenAPI + JSON Schema endpoint definitions. I think it would make server construction a lot easier and web frameworks would not have to materially be updated to adhere to the spec -- perhaps just adding a single endpoint.[0] https://spec.modelcontextprotocol.io/specification/2025-03-2...[1] https://github.com/modelcontextprotocol/specification/issues...\n \nreply",
      "Totally agree - a well-defined REST API \"standard\" for tool listing and tool execution would have been much better. Could extend as needed to websockets for persistent connections / streaming data.\n \nreply",
      "I fully agree.MCP is just too complex for what it is supposed to do. I don't get what's the benefit. It is the kind of thing that has the potential to be a huge time waste because it requires custom dev tools to develop and troubleshoot.It is not even a protocol in the traditional sense - more of a convention.And of course we will implement it, like everyone else, because it is gathering momentum, but I do not believe it is the right approach at all. A simpler HTTP-based OpenAPI service would have been a lot better and it is already well supported in all frameworks.The only way I can make sense of MCP in the context of an STDIO.\n \nreply",
      "The `stdio` approach for local services makes complete sense to me. Including using JSONRPC.But for remote HTTP MCP servers there should be a dead simple solution. A couple years ago OpenAI launched plugins as `.well-known/ai-plugin.json`, where it'd contain a link to your API spec, ChatGPT could read it, and voila. So all you needed to implement was this endpoint and ChatGPT could read your whole API. It was pretty cool.ChatGPT Plugins failed, however. I'm confident it wasn't because of the tech stack, it was due to the fact that the integration demand wasn't really there yet: companies were in the early stages of building their own LLM stacks, ChatGPT desktop didn't exist. It also wasn't marketed as a developer-first global integration solution: little to no consistent developer advocacy was done around it. It was marketed towards consumers and it was pretty unwieldy.IMO the single-endpoint solution and adhering to existing paradigms is the simplest and most robust solution. For MCP, I'd advocate that this is what the `mcp/` endpoint should become.Edit: Also tool calling in models circa 2023 was not nearly as good as it is now.\n \nreply",
      "I agree. What OpenAI did was simple and beautiful.Also, I think there is a fundamental misunderstanding that MCP services are plug and play. They are not. Function names and descriptions are literally prompts so it is almost certain you would need to modify the names or descriptions to add some nuances to how you want these to be called. Since MCP servers are not really meant to be extensible in that sort of way, the only other alternative is to add more context into the prompt which is not easy unless you have a tone of experience. Most of our customers fail at prompting.The reason I like the ai-plugin.json approach is that you don't have to change the API to make the description of a function a little bit different. One day MCP might support this but it will another layer of complexities that could have been avoided with a remotely hosted JSON / YAML file.\n \nreply",
      "The good thing to note is that (AFAIK) MCP is intended to be a collaborative and industry-wide effort. Whereas plugins was OpenAI-specific.So, hopefully, we can contribute and help direct the development! I think this dialogue is helpful and I'm hoping the creators respond via GitHub or otherwise.\n \nreply",
      "It took me a minute to even understand this comment because for me the \u201cobvious\u201d use-case for MCP is local filesystem tasks, not web requests. Using MCP to manipulate files is my primary LLM use-case and has been ever since Anthropic released it and integrated it into Claude Desktop. I understand where you\u2019re coming from, but I suspect that the idea here is to build something that is more \u201cfilesystem first.\u201d\n \nreply",
      "That makes sense. But if that's the case I think we should call a spade a spade and differentiate \"Local-first MCP\" and \"Remote MCP\"; because what (many, most?) companies are really trying to do is integrate with the latter.Which is where you see this sort of feedback, where a bunch of us API engineers are like; \"there's already a well-trodden path for doing all of this stuff. Can we just do that and agree that it's the standard?\"\n \nreply",
      "100%. I know I\u2019m in the \u201cget off my lawn\u201d phase of my career when I see things like MCP and LangChain, but know I would have been excited about them earlier in my career.\n \nreply",
      "LangChain is an objectively terrible Frankenstein's monster of an API. If you were a good developer in your youth, you'd have still held it in contempt, and treat MCP with caution.The MCP API is pretty bad, too, it's just that a paradigm is starting to emege regarding modularity, integration and agentic tooling, and MCP happens to be the only real shot in that direction st this particular moment.\n \nreply"
    ],
    "link": "https://openai.github.io/openai-agents-python/mcp/",
    "first_paragraph": "The Model context protocol (aka MCP) is a way to provide tools and context to the LLM. From the MCP docs:MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.The Agents SDK has support for MCP. This enables you to use a wide range of MCP servers to provide tools to your Agents.Currently, the MCP spec defines two kinds of servers, based on the transport mechanism they use:You can use the MCPServerStdio and MCPServerSse classes to connect to these servers.For example, this is how you'd use the official MCP filesystem server.MCP servers can be added to Agents. The Agents SDK will call list_tools() on the MCP servers each time the Agent is run. This makes the LLM aware of the MCP server's tools. When the LLM calls a "
  },
  {
    "title": "Debian bookworm live images now reproducible (lwn.net)",
    "points": 438,
    "submitter": "bertman",
    "submit_time": "2025-03-26T17:22:22 1743009742",
    "num_comments": 91,
    "comments_url": "https://news.ycombinator.com/item?id=43484520",
    "comments": [
      "Insane effort. This sounded like a pipe dream just a couple of years ago. Congrats to everyone involved, especially to those who drove the effort.\n \nreply",
      "The Debian group is admirable, and have positively changed the standards for OS design several times.   Reminds me I should donate to their coffee fund around tax time =3\n \nreply",
      "I don't get how someone achieves reproducibility of builds: what about files metadata like creation/modification timestamps? Do they forge them? Or are these data treated as not important enough (like it 2 files with different metadata but identical contents should have the same checksum when hashed)?\n \nreply",
      "Debian uses a tool called `strip-nondeterminism` to help with this in part: https://salsa.debian.org/reproducible-builds/strip-nondeterm...There's lots of info on the Debian site about their reproducibility efforts, and there's a story from 2024's DebConf that may be of interest: https://lwn.net/Articles/985739/\n \nreply",
      "I see this is written in Perl, is that the case with most Debian tooling?\n \nreply",
      "Timestamps are easiest part - you just set everything according to the chosen epoch.The hard things involve things like unstable hash orderings, non-sorted filesystem listing, parallel execution, address-space randomization, ...\n \nreply",
      "ASLR shouldn\u2019t be an issue unless you intend to capture the entire memory state of the application. It\u2019s an intermediate representation in memory, not an output of any given step of a build.Annoying edge cases come up for things like internal object serialization to sort things like JSON keys in config files.\n \nreply",
      "FreeBSD tripped over an issue recently where a C++ program (I think clang?) used a collection of pointers and output values in an order based on the pointers rather than the values they pointed to.ASLR by itself shouldn't cause reproducibility issues, but it can certainly expose bugs.\n \nreply",
      "Let\u2019s say a compiler is doing something in a multi-threaded manner - isn\u2019t it possible that ASLR would affect the ordering of certain events which could change the compiled output? Sure you could just set threads to 1 but there\u2019s probably some more edge cases in there I haven\u2019t thought of.\n \nreply",
      "I think you'd need the compiler to guarantee serialization order of such operations regardless if you used ASLR or not. Otherwise you're just hoping thread scheduling, core clocking, thread memory access, and many other things are the same between every system trying to do a reproducible build. Even setting threads to 1 may not solve that problem class if asynchronous functions/syscalls come into play.\n \nreply"
    ],
    "link": "https://lwn.net/Articles/1015402/",
    "first_paragraph": "\nIn a short\nnote to the Reproducible Builds\nmailing list, Debian developer Roland Clobus announced that live\nimages for Debian 12.10 (\"bookworm\") are now 100% reproducible. See the reproducible\nlive images and Debian Live todo\npages on the Debian wiki for more information on the images.\n\n\n\n            Copyright \u00a9 2025, Eklektix, Inc.\n            \n            Comments and public postings are copyrighted by their creators.\n            Linux  is a registered trademark of Linus Torvalds\n\n"
  },
  {
    "title": "The Mysterious Flow of Fluid in the Brain (quantamagazine.org)",
    "points": 14,
    "submitter": "pseudolus",
    "submit_time": "2025-03-27T00:31:47 1743035507",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=43489136",
    "comments": [
      "What is the point of a magazine catered to people who could read papers if they want but choose not to?These articles don't appear to be of any higher quality than other serial brands. So is it just catering to your level of literacy? Eg you know what a blood-brain barrier is so we'll just zoom past.The inevitable issue with this sort of model is that it's really bad at figuring out how it comes off. Polling breaks down at this extreme.I suppose this describes most western media brands with \"prestige\" of the last hundred years, but I'm still trying to answer the why.\n \nreply",
      "Reading the core paper discussed in the article, along with all its context, will take ~hours. This article is a great 5-minute update on the latest research with a well written primer on the topic\n \nreply",
      "Ok but that's just standard close-reading. You could easily spend five hours doing this with a ten-page picture book and come up with 70 pages of analysis. This short of knowledge is just assumed.The content of the article is irrelevant to the length.Surely you can identify the cognitively-expensive aspect of this procedure if you think it can be safely excised. Tell us! What is this expensive-but-useless feature?\n \nreply",
      ">could read papers if they want but choose not toJournals are expensive.>catering to your level of literacy?Lowering the cognitive effort required to acquire the new information makes it more broadly available.  Both to people that couldn't comprehend the source material, and to those that could, but don't want to spend the cognitive energy.I can understand the argument that we should avoid dumbing down our media, but this is a weird battle to pick.  Go throw stones at youtube shorts if you're worried about modern media liquifying humanity's frontal cortex.\n \nreply",
      "> Lowering the cognitive effort required to acquire the new information makes it more broadly available.What do you mean by \"lowering\"? Why do you consider papers to have higher cognitive effort? This sort of bigotry is not at all what I was trying to encourage. In fact, I was trying to encourage humility specifically to avoid the inevitable retractions that come with thinking you're at a higher level.\n \nreply",
      "> This sort of bigotry is not at all what I was trying to encourage.What does level of effort have to do with bigotry?\n \nreply",
      "? What do you think bigotry means",
      "Bigotry? Most papers are written in a way (style and content), that makes them difficult for an everyday person, meaning someone not trained in that specific field of study, to understand.\n \nreply"
    ],
    "link": "https://www.quantamagazine.org/the-mysterious-flow-of-fluid-in-the-brain-20250326/",
    "first_paragraph": "\nAn editorially independent publication supported by the Simons Foundation.\n\nGet the latest news delivered to your inbox.\nCreate a reading list by clicking the Read Later icon next to the articles you wish to save.Type search term(s) and press enterPopular\n                                    SearchesMarch 26, 2025No one knows why cerebrospinal fluid circulates through and around our brains, or what directs its flow.Chanelle Nibbelink for\u00a0Quanta MagazineContributing WriterMarch 26, 2025Encased in the skull, perched atop the spine, the brain has a carefully managed existence. It receives only certain nutrients, filtered through the blood-brain barrier; an elaborate system of protective membranes surrounds it. That privileged space contains a mystery. For more than a century, scientists have wondered: If it\u2019s so hard for anything to get into the brain, how does waste get out?The brain has one of the highest metabolisms of any organ in the body, and that process must yield by-products that"
  },
  {
    "title": "A love letter to the CSV format (github.com/medialab)",
    "points": 321,
    "submitter": "Yomguithereal",
    "submit_time": "2025-03-26T17:08:56 1743008936",
    "num_comments": 316,
    "comments_url": "https://news.ycombinator.com/item?id=43484382",
    "comments": [
      "CSV is ever so elegant but it has one fatal flaw - quoting has \"non-local\" effects, i.e. an extra or missing quote at byte 1 can change the meaning of a comma at byte 1000000.  This has (at least) two annoying consequences:1. It's tricky to parallelise processing of CSV.\n2. A small amount of data corruption can have a big impact on the readability of a file (one missing or extra quote can bugger the whole thing up).So these days for serialisation of simple tabular data I prefer plain escaping, e.g. comma, newline and \\ are all \\-escaped.  It's as easy to serialise and deserialise as CSV but without the above drawbacks.\n \nreply",
      "JSON serialized without extra white space with one line per record is superior to CSV.If you want CSV-ish, enforce an array of strings for each record. Or go further with actual objects and non-string types.You can even jump to an arbitrary point and then seek till you see an actual new line as it\u2019s always a record boundary.It\u2019s not that CSV is an invalid format. It\u2019s that libraries and tools to parse CSV tend to suck. Whereas JSON is the lingua franca of data.\n \nreply",
      "> It\u2019s that libraries and tools to parse CSV tend to suck.  Whereas JSON is the lingua franca of data.This isn't the case.  An incredible amount of effort and ingenuity has gone into CSV parsing because of its ubiquity.  Despite the lack of any sort of specification, it's easily the most widely supported data format in existence in terms of tools and language support.\n \nreply",
      "> An incredible amount of effort and ingenuity has gone into CSV parsing because of its ubiquity.Yea and it's still a partially-parseable shit show with guessed values. But we can and could have  and should have done better by simply defining a format to use.\n \nreply",
      "Can you point me to a language with any significant number of users that does NOT have a JSON library?I went looking at some of the more niche languages like Prolog, COBOL, RPG, APL, Eiffel, Maple, MATLAB, tcl, and a few others. All of these and more had JSON libraries (most had one baked into the standard library).The exceptions I found (though I didn't look too far) were: Bash (use jq with it), J (an APL variant), Scratch (not exposed to users, but scratch code itself is encoded in JSON), and Forth (I could find implementations, but it's very hard to pin down forth dialects).\n \nreply",
      "I made no claim about JSON libraries.  I contested the claim that \"CSV libraries and tools suck\".  They do not.\n \nreply",
      "CSV tooling has had to invest enormous amounts of effort to make a fragile, under-specified format half-useful. I would call it ubiquitous, I would call the tooling that we\u2019ve built around it \u201cimpressive\u201d but I would by no means call any of it \u201cgood\u201d.I do not miss dealing with csv files in the slightest.\n \nreply",
      "> CSV tooling has had [...] to make a fragile, under-specified format half-usefulYou get this backwards. Tabular structured data to store are ubiquitous. Text as a file format is also ubiquitous because it is accessible. The only actual decisions are about whether to encode your variables as rows or columns, what is the delimiter, and other rules such as escaping etc. Vars as columns makes sense because it makes appending easier. There is a bunch of stuff that can be used for delimeters, commas being the most common, none is perfect. But from this point onwards, decisions do not really matter, and \"CSV\" basically covers everything from now on. \"CSV\" is basically what comes naturally when you have tabular datasets and want to store them in text. CSV tooling is developed because there is a need for this way of formatting data. Whether CSV is \"good\" or \"ugly\" or whatever is irrelevant, handling data is complicated as much as the world itself is. The alternatives are either not structuring/storing the data in a tabular manner, or non-text (eg binary) formats. These alternative exist and are useful in their own right, but don't solve the same problems.\n \nreply",
      "Microsoft Windows has had to invest enormous amounts...Apple macOS has had to invest enormous amounts...Pick your distro of Linux has had to invest enormous amounts...None of them a perfect and any number of valid complaints can be said about any of them. None of the complaints make any of the things useless. Everyone has workarounds.Hell, JSON has had to invest enormous amounts of effort...\n \nreply",
      "I guess the point is that I can take a generic json parser and point it at just about any JSON I get my hands on, and have close to no issues parsing it.Want to do the same with csv? Good luck. Delimiter? Configurable. Encoding? Configurable. Misplaced comma? No parse in JSON, in csv: might still parse, but is now semantically incorrect and you possibly won\u2019t know until it\u2019s too late, depending on your parser. The list goes on.\n \nreply"
    ],
    "link": "https://github.com/medialab/xan/blob/master/docs/LOVE_LETTER.md",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          Or why people pretending CSV is dead are wrongEvery month or so, a new blog article declaring the near demise of CSV in favor of some \"obviously superior\" format (parquet, newline-delimited JSON, MessagePack records etc.) find its ways to the reader's eyes. Sadly those articles often offer a very narrow and biased comparison and often fail to understand what makes CSV a seemingly unkillable staple of data serialization.It is therefore my intention, through this article, to write a love letter to this data format, often criticized for the wrong reasons, even more so when it is somehow deemed \"cool\" to hate on it. My point is not, far from it, to say that CSV is a silver bullet but rather to shine a light on some of the format's sometimes overlooked strengths.The specification of CSV holds in its title: \"comma separated values\". Okay, it's a l"
  },
  {
    "title": "Building a Linux Container Runtime from Scratch (edera.dev)",
    "points": 58,
    "submitter": "curmudgeon22",
    "submit_time": "2025-03-26T20:35:46 1743021346",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=43486997",
    "comments": [
      "I loved this hands-on presentation Containers From Scratch by Liz Rice from few years ago https://www.youtube.com/watch?v=8fi7uSYlOdc.Today, Linux containers in (less than) 100 lines of shell by Michael Kerrisk was published https://www.youtube.com/watch?v=4RUiVAlJE2w.\n \nreply",
      "Why not use any of the existing OCI Runtimes? They take well-defined[0] JSON description as input, and are pretty well-contained (single static binary). And because they are separate binaries, not libraries, you don't need to worry about things like thread safety or FD leaking.[0] https://github.com/opencontainers/runtime-spec/blob/main/con...\n \nreply",
      "I\u2019m currently exploring this for an AI context because I haven\u2019t found a better solution for letting K8S manage AI workloads that need direct GPU access on OSx\n \nreply",
      "Why are you building AI anything\n \nreply"
    ],
    "link": "https://edera.dev/stories/styrolite",
    "first_paragraph": "Edera Protect is a suite of offerings bridging the gap between modern cloud native computing and virtualization-based security techniques. To power this platform, we've built our own container runtime designed to operate as a microservice, allowing it to run containers in a fully programmatic way\u2014similar to how the Kubernetes Container Runtime Interface (CRI) enables container management through microservices.Today, we're excited to announce that we're open sourcing our programmatic low-level container runtime, Styrolite, so that others may benefit from our work.The idea of separating the low-level container runtime concerns into its own tool or microservice is not new. Outside of the Kubernetes CRI, which presents container lifecycle management as a pluggable microservice, there are simpler tools which provide a low-level container runtime as well, such as the unshare utility in util-linux, as well as another tool called Bubblewrap.But these tools are either too high-level (like the K"
  },
  {
    "title": "Botswana Successfully Launches First Satellite, Botsat-1 (spaceinafrica.com)",
    "points": 208,
    "submitter": "vinnyglennon",
    "submit_time": "2025-03-26T16:02:56 1743004976",
    "num_comments": 75,
    "comments_url": "https://news.ycombinator.com/item?id=43483660",
    "comments": [
      "Don\u2019t want to belittle the achievement but they launched it as in \u201ehad it launched by the commercial launch provider SpaceX\u201c, not on a self-developed rocket as it sounds like on the first read.\n \nreply",
      "Very few organizations and even countries can develop both a launch vehicle and a satellite.  Botswana has done fine to develop a satellite that integrates onto a rideshare launch.  They aren't working with anything close to the headcount or budget of NASA or even the ESA.Edit (rather than reply and make the comment chain long): It's fine that you read it that way.  I figure that if the article were about a launch vehicle then it would have been the rocket's name in the title, and if the article were about the satellite then it would have the satellite's name (BOTSAT-1).  If Botswana had developed both an orbital launch vehicle and their first satellite then I'd bet the headline would have been sensational.\n \nreply",
      "> Edit (rather than reply and make the comment chain long):Sorry to go meta here, but this is just rude, both to OP and to other readers.For OP, you're effectively pre-empting what they say with your own counterargument, and even more so you're removing the ability for them to counter your counter. You're essentially using the edit feature to end the conversation and ensure you have the final word.For other readers, you're introducing confusing non-linear flow.Just reply. It's not hard, and as you can see below you didn't actually prevent a subthread from forming.\n \nreply",
      "Not to sound rude, but I don't know what your normative culture is or why any of us should care what you think is rude on the internet when you lack the self awareness to know that etiquette has no objective basis and is always contingent.You could skip the disparaging characterizations and make your case for why you think it would be a good guideline. Ie, because it is confusing and non-linear, not because of a bunch of motivations you've inferred about a stranger.\n \nreply",
      "> etiquette has no objective basis and is always contingentBut what it's contingent on is what's actually manifest rather than some speculative hypothetical that seems a contrivance to nullify the applicability of any etiquette anywhere.FWIW, the previous commenter's points that adding edits to an existing post in order to reply to comments further downstream is confusing and impolite behavior.  It's useful on Reddit in response to malicious use of the ill-advised 'block' feature, but doesn't fit on HN.\n \nreply",
      "> Sorry to go meta here, but this is just rude, both to OP and to other readers. ... You're essentially using the edit feature to end the conversation and ensure you have the final word.I've noticed this more and more, especially on more controversial topics (which this is certainly not).Adam makes a statement, Betty responds. Adam responds, and Betty edits her initial response and conversation ends, likely because Adam didn't see the edit.\n \nreply",
      "Isn\u2019t that fine? Not all of those conversations have to be taken to the end.\n \nreply",
      "Imagine you're having an in-person conversation with someone in a crowded restaurant. Rather than addressing their next response to you, they wait until you go to the bathroom. Then they turn to everyone at the next table and say, \"that guy doesn't get it, but fine.\"I personally think that in a no-holds-barred debate, you don't bother trying to convince your interlocutor of anything. You focus on persuading everyone else in the room. But it's rude to treat every polite conversation as a smackdown debate. Such a strategy can also backfire by turning off your intended audience, as evidenced here.\n \nreply",
      "Mm, as long as the edit is clear it seems like a good way to avoid unproductive arguments.Adam makes a statement, but the responses show the statement was unclear and/or leads into tangential arguments. An edit can clarify the initial statement for future readers without getting the original poster stuck in the back-and-forth necessary to escape whatever quagmire the unedited version created.\n \nreply",
      "Could also be because rate limiting. People need to conserve their number of posts per day.\n \nreply"
    ],
    "link": "https://spaceinafrica.com/2025/03/15/botswana-successfully-launches-first-satellite-botsat-1/",
    "first_paragraph": "Let's talkinfo@spaceinafrica.com+2348164054892Botswana\u2019s first satellite, BOTSAT-1, was successfully launched aboard SpaceX\u2019s Falcon 9\u2014Transporter 13 rideshare mission on Saturday, March 15, 2025, from the Vandenberg Space Force Base, USA. The Falcon 9 rocket lifted off the Space Launch Complex 4E (SLC-4E) at 06:39 am GMT, carrying 74 satellites. These included BOTSAT-1, 26 satellites as part of the Transporter-13 rideshare mission, and a trio of CubeSats for NASA\u2019s Electrojet Zeeman Imaging Explorer (EZIE) mission; Arvaker 1, the first microsatellite for Kongsberg NanoAvionics\u2019 N3X constellation.Click here to watch the launch replay.\u00a0BOTSAT-1 is a 3U hyperspectral Earth Observation satellite developed by the Botswana International University of Science and Technology (BIUST) and led by Dr Dimane Mpoeleng, BOTSAT-1\u2019s Project Lead. The project was done in collaboration with EnduroSat, the satellite\u2019s lead manufacturer. Similarly, ExoLaunch managed the launch in partnership with Space X "
  },
  {
    "title": "Good-bye core types; Hello Go as we know and love it (go.dev)",
    "points": 226,
    "submitter": "ingve",
    "submit_time": "2025-03-26T16:18:14 1743005894",
    "num_comments": 125,
    "comments_url": "https://news.ycombinator.com/item?id=43483842",
    "comments": [
      "I love and continue to love how seriously the Go team takes breaking spec changes, as well as spec changes in general.Go generics are a bit of a blip in this to be honest, A) because it is a big change, and B) because it can be difficult to use (generic functions defined on types cannot use generic parameters that aren't defined on that type, for example).But in a way, I also think the constraints help avoid the overuse of generics. I have seen Java and Typescript projects where developers had way too much fun playing around with the type system, and the resulting code is actually quite unclear.In conclusion, I pray the Go team strive to and continue to be conservative with the language.\n \nreply",
      "I sorta agree. It can be ridiculously hard to understand and maintain other people\u2019s complicate TS generics. But the payoff is that you have a type system that nearly disallows consumers of your interface to use it incorrectly.Like you\u2019d never get the amazing type safety of libraries like prisma and kysely (an ORM and a query builder) if you couldn\u2019t make such ridiculously expressive generics. But, only a teeny tiny subset of TS devs can work on generics that complex. I definitely can\u2019t!It\u2019s a tradeoff like anything else.\n \nreply",
      "What\u2019s an example complex generic? If you have one, that is, I\u2019m curious what kinds of things people do with types. I\u2019m a fan of using types, myself. Thanks in advance.\n \nreply",
      ">generic functions defined on types cannot use generic parameters that aren't defined on that type, for exampleThis is bonkers to me, why???\n \nreply",
      "There isn\u2019t a single language which has implemented genetics without those expanding to the complexity of C++ generics. It turns out most of the complexity is essential:- need overloading to support instantiation\n- need type functions/associated types. For example what type is inside this container?\n- need operator overloading so you can substitute generic types into assignment expressions. For example notions of equality.\n- need duck typing or traits to communicate capabilitiesThe two simplifications which have historically worked are to use dynamic types, or to use code generators.\n \nreply",
      "Because they chose a half-assed way to implement generics, and the path they took doesn't interact well with the rest of the type system.\n \nreply",
      "Is it clear they had any better choice after shipping 1.0 without generics?\n \nreply",
      "No. My recollection is a lot of people urging them to get generics right and into 1.0 and the Go guys saying, \u201cdon\u2019t worry, we can add generics later\u201d.\n \nreply",
      "https://go.dev/doc/faq#generic_methods\n \nreply",
      "This is a good write-up of the issue. To see where this craziness could have led, see the C++ overload resolution logic, which isn't the exact same problem but does smell the same: https://en.cppreference.com/w/cpp/language/overload_resoluti...\n \nreply"
    ],
    "link": "https://go.dev/blog/coretypes",
    "first_paragraph": "Common problems companies solve with GoStories about how and why companies use GoHow Go can help keep you secure by defaultTips for writing clear, performant, and idiomatic Go codeA complete introduction to building software with GoReference documentation for Go's standard libraryLearn what's new in each Go releaseVideos from prior eventsMeet other local Go developersLearn and network with Go developers from around the worldThe Go project's official blog.Get help and stay informed from Go\n      Robert Griesemer\n      26 March 2025\n      The Go 1.18 release introduced generics and with that a number of new features, including type parameters, type constraints, and new concepts such as type sets.\nIt also introduced the notion of a core type.\nWhile the former provide concrete new functionality, a core type is an abstract construct that was introduced\nfor expediency and to simplify dealing with generic operands (operands whose types are type parameters).\nIn the Go compiler, code that in th"
  },
  {
    "title": "Problems with the heap (rachelbythebay.com)",
    "points": 99,
    "submitter": "todsacerdoti",
    "submit_time": "2025-03-26T19:23:36 1743017016",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=43485980",
    "comments": [
      "Both of the error messages we're given indicate that the \"top\" chunk of the heap was corrupted, which is a special internal allocation used by glibc malloc to represent any unused capacity from the last time malloc decided to grow the heap:    high memory addresses (\"top\" of heap)\n    | unmapped memory |\n    |-----------------|\n    |                 |\n    |    top chunk    |\n    |  (mapped, but   |\n    |     not yet     |\n    |    allocated)   |\n    |                 |\n    |-----------------|\n    | allocated chunk |\n    |-----------------|\n    |   freed chunk   |\n    |-----------------|\n    | allocated chunk |\n    |                 |\n    |-----------------|\n    | allocated chunk |\n    |-----------------|\n    |   freed chunk   |\n    |-----------------|\n    |     tcache      |\n    |-----------------|\n    low memory addresses (\"bottom\") of heap\n\nThat likely indicates a heap buffer overflow. If a call to malloc() doesn't find a freed chunk, it will split the top chunk in two and return a pointer to the bottom portion. If you then write past the end of the returned allocation, you clobber the metadata of the top chunk, and get errors like the ones in the article.With the exploit mitigations built into modern Linux and glibc, it's a lot of work to go from here to arbitrary code execution; but it very well may be possible, depending on exactly how much control the attacker has over what atop does. The attacker can probably trigger the heap buffer overflow multiple times by spawning multiple processes, and if the length and contents of the heap buffer overwrite are attacker-controlled, they can probably play some games to overwrite any data stored in the heap. If that's true, the only thing preventing full arbitrary-code-execution is ASLR; there are many clever ways to get around that, but it's often quite difficult and may or may not be possible here.This year's LACTF had a challenge with essentially this exact setup. My solution writeup is a good example of what it takes to defeat the exploit mitigations and turn a heap buffer overflow into an RCE: https://jonathankeller.net/ctf/lamp/\n \nreply",
      "I thought I would ask, does this mean that memory restricted environments can be a better target for attack than one with a large amount of memory available? In my mind it seems like this would be the case, but I'm not sure if there is anything in place to protect these types of environments, like intentionally breaking up and segmenting memory so it's not possible to read much linearly. I admit that I haven't touched low-level code since the early 2000's, and earlier than that for anything other than a course requirement, so I apologize if you explained it in your linked article and I don't understand.\n \nreply",
      "Yes, with the caveat that virtual memory restrictions matter a lot more than physical memory restrictions.Heap exploitation is difficult because 1) glibc malloc is hardened to try to defeat many common exploit strategies, and 2) ASLR means that even if you have the ability to corrupt a pointer, you might not be able to control what the pointer points to.Regarding #1, memory constraints don't really make a difference until you're so constrained on memory that you can't run glibc anymore, so you use an allocator that is optimized for low overhead/code size rather than performance and security.For #2: ASLR works by placing every section of the process (heap, stack, program binary, and each library) at a randomized virtual address so that attackers can't forge pointers. ASLR is much more effective on a 64-bit system than on a 32-bit system, simply because there's so much virtual address space available to choose. If memory addresses are only 32 bits, it's feasible to just brute force guess the memory address of some data you're interested in; with 64 bits, it's not. And if your system doesn't have virtual memory at all (like a microcontroller), you probably don't have any kind of ASLR.> intentionally breaking up and segmenting memory so it's not possible to read much linearlyThis is typically done at the level of individual sections (stack, heap, program binary, library binaries) but not within sections; mainly for performance, memory overhead, and cache locality reasons. The entire heap is contiguous, so if you overwrite past the end of one heap allocation you can overwrite adjacent allocations, but you can't overwrite the stack (without more work). Breaking up the heap into smaller chunks wouldn't really help that much; it just means an attacker has to manipulate the heap layout so they can be sure the allocation they're targeting ends up in the chunk they're targeting.Exploit hardening in the heap allocator is something of a last-ditch stopgap measure: \"we've already lost, but let's see if we can minimize the damage to the user/maximize the difficulty to the attacker\". There's certainly much more you could do to harden the heap, but remember that any security measures implemented in glibc are applied to every program, with no way to opt out (except bringing your own libc). So glibc is designed to maximize performance and compatibility; exploit mitigations are only included when they doesn't compromise these goals.If you don't mind giving up some performance for the sake of security, you probably would already be using a garbage-collected language instead of C. (And now that Rust is sufficiently mature for most use cases, you probably should be using that if you're in a situation where you need both performance and security).\n \nreply",
      "It\u2019s a lot easier to use an exploit if you know your chunk is from fast bins if I recall correctly.\n \nreply",
      "as there seems to be some confusion, this is my interpretation:atop is (for some reason) touching memory of processes it monitors.atop is touching this in an insecure way. An executable can cause atop to corrupt its memory.this has high potential (although not guaranteed) for allowing RCE within atop via a correctly crafted process that atop monitors.atop is often run in root, and so this otherwise meaningless RCE becomes privilege escalation, which is badeither this is correct, or cunningham\u2019s law will bring out the correct interpretation\n \nreply",
      "This is a complete shot in the dark but wild speculation is fun. If atop had a buffer overflow when reading a process name (changeable at runtime using $0 in perl for example) this would be the kind of issue I expect.Similarly, some other value that was expected to be null terminated but wasn't.\n \nreply",
      "Recent and related:You might want to stop running atop - https://news.ycombinator.com/item?id=43477057 - March 2025 (131 comments)\n \nreply",
      "RachelByTheBay underestimated how much we hang on her every word over here, I think. Haha.\n \nreply",
      "I believe she hasn't had the best behaviour from our members unfortunately\n \nreply",
      "I expect she's had the best behavior that the members in question are capable of.\n \nreply"
    ],
    "link": "https://rachelbythebay.com/w/2025/03/26/atop/",
    "first_paragraph": ""
  },
  {
    "title": "Building a Firecracker-Powered Course Platform to Learn Docker and Kubernetes (iximiuz.com)",
    "points": 58,
    "submitter": "fazkan",
    "submit_time": "2025-03-26T20:08:32 1743019712",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=43486647",
    "comments": [
      "This is neat. I left cloud infra work before everyone wanted k8s knowledge and Ive always assumed im 50% of the way there having docker and nomad experience\n \nreply",
      "I never left cloud infra, though I never came across a scale that required k8s anything close to that scale would just go on serverless functions anyway. However, I will agree with you that I've always assumed that I'm too 50% of the way there with docker and misc experience.\n \nreply",
      "I've seen variations of this a few times, especially with Firecracker. I'm surprised there aren't ready made solutions for this use case already.\n \nreply",
      "I haven\u2019t tried a course yet but Ivan, the creator, is great. IIRC codesandbox also uses firecracker and has a few blogposts on its setup\n \nreply",
      "This is awesome, a lot of work to give users a interactive environment. I am working on a similar setup with my own project, vimgolf.ai , where I help users learn vim through connecting users with vim instances.\n \nreply"
    ],
    "link": "https://iximiuz.com/en/posts/iximiuz-labs-story/",
    "first_paragraph": ""
  },
  {
    "title": "Writing a tiny undo/redo stack in JavaScript (julik.nl)",
    "points": 59,
    "submitter": "julik",
    "submit_time": "2025-03-24T08:53:23 1742806403",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=43458738",
    "comments": [
      "Ha! I wrote my own version of this as a web component[0]. Very handy thing to have, for web apps!Nice to see it distilled into something so straightforward! Thanks for sharing![0] https://github.com/catapart/magnitce-action-history\n \nreply",
      "Thank you for sharingOther notable inspirations\nhttps://immerjs.github.io/immer/patches/\nhttps://github.com/webstudio-is/immerhin\n \nreply",
      "https://github.com/coolsoftwaretyler/mst-middlewares/blob/ma...\n \nreply",
      "I did something similar the other day but I end up using a circular buffer of fixed size. Everytime you push a new state the oldest state gets discarded.\n \nreply",
      "A zipper!\n \nreply",
      "You can clear an array in JS by setting foo.length = 0? TIL.\n \nreply",
      "I think this is actually the best way to zero an array in place. The other option is using foo.splice(0,foo.length)foo = []; Will reassign the reference but other pointers can still have the array.foo.length=1 will remove all elements but the first.\n \nreply",
      "omg this language just keeps getting deeper and deeper\n \nreply",
      "It's very nice, but it doesn't do system undo. You can shake iPhones to get an Undo/Redo dialog, for example.So you can probably trap Meta-Z but some people use menus or the other affordances.\n \nreply",
      "Well, hooking into system undo is tricky as there still seems to be no native web platform thing for that. I see all kinds of things like https://github.com/samthor/undoer but the article was about the foundation, rather than the integration. For what I am doing, shortcuts + clickable buttons on screen will do the job.\n \nreply"
    ],
    "link": "https://blog.julik.nl/2025/03/a-tiny-undo-stack",
    "first_paragraph": "I\u2019ve needed this before - a couple of times. Third time I figured I needed something small, nimble - yet complete. And - at the same time - wondering about how to do it in a very simple manner. I think it worked out great, so let\u2019s dig in.Most UIs will have some form of undo functionality. Now, there are generally two forms of it: undo stacks and version histories. A \u201cversion history\u201d is what Photoshop history gives you - the ability to \u201cpaint through\u201d to a previous state of the system. You can add five paint strokes, and then reveal a stroke you have made 4 steps back.But most apps won\u2019t need that. What you will need is an undo stack, which can be specced out as follows:If you are curious how \u201cthe big guys\u201d used to do it - check out the NSUndoManager documentationSo, as I usually like to do, I want to understand the API that would be optimal. For this use case - drawing - I had the following workflow:I wanted something like this:Simplest thing in the world. Now, if you look at most re"
  },
  {
    "title": "MCP server for Ghidra (github.com/lauriewired)",
    "points": 201,
    "submitter": "tanelpoder",
    "submit_time": "2025-03-25T18:47:37 1742928457",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=43474490",
    "comments": [
      "Which tools can currently invoke MCP? I have read only a little about MCP and got to know that Claude's desktop application is capable of using MCP locally.Are there any chat interfaces which allow using MCP remotely?I would like to be able to specify MCP endpoints and the functions they offer in ChatGPT's, Claude's and Gemini's web interfaces so that I can have them call my servers remotely. A bit like \"GPTs\" and \"Gems\".\n \nreply",
      "I touch on this briefly in the video, beside Claude Desktop, 5ire is a fairly model-agnostic local MCP client, I'm sure there are others.sama also recently mentioned ChatGPT Desktop is getting MCP client functionality \"soon\".As for remote clients, Cloudflare has some really useful tooling, look at their \"AI Playground\".\n \nreply",
      "OpenAI just announced support in their Agents SDK. https://news.ycombinator.com/item?id=43485566 https://openai.github.io/openai-agents-python/mcp/\n \nreply",
      "I use them in Cursor. Writing an MCP server is trivial, just ask Cursor to put one together in TypeScript. You would use your local MCP server to call whatever remote API you want (or perform some other task). The MCP server uses stdin/stdout to talk to Cursor.\n \nreply",
      "You can use MCP servers in SAM (Solace Agent Mesh). That has a chat interface and can be run remotely. Perhaps the easiest way to do it remotely is to use a Slack integration to SAM with a free Slack workspace, which doesn't require poking a hole to serve the browser UIhttps://github.com/SolaceLabs/solace-agent-mesh\n \nreply",
      "Unity, Blender and Photoshop all have rough MCP integrations available. You can find them on GitHub.\n \nreply",
      "I had the same question as you, and some quick Googling led me to this list here:https://github.com/punkpeye/awesome-mcp-clients\n \nreply",
      "and the list of servers - https://github.com/punkpeye/awesome-mcp-servers\n \nreply",
      "Block has an open source tool called Goose that invokes MCP.\nhttps://block.github.io/goose/\n \nreply",
      "If you run some proxy server, you could run MCP servers remotely\n \nreply"
    ],
    "link": "https://github.com/LaurieWired/GhidraMCP",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        MCP Server for Ghidra\n      \n\n\n\n\nghidraMCP is an Model Context Protocol server for allowing LLMs to autonomously reverse engineer applications. It exposes numerous tools from core Ghidra functionality to MCP clients.MCP Server + Ghidra PluginFirst, download the latest release from this repository. This contains the Ghidra plugin and Python MCP client. Then, you can directly import the plugin into Ghidra.Video Installation Guide:Theoretically, any MCP client should work with ghidraMCP.  Two examples are given below.To set up Claude Desktop as a Ghidra MCP client, go to Claude -> Settings -> Developer -> Edit Config -> claude_desktop_config.json and add the following:Alternatively, edit this file directly:Another MCP client that supports multiple models on the backend is 5ire. To set up GhidraMCP, open 5ire and go to Tools -> New and "
  },
  {
    "title": "Oracle customers confirm data stolen in alleged cloud breach is valid (bleepingcomputer.com)",
    "points": 179,
    "submitter": "el_duderino",
    "submit_time": "2025-03-26T20:31:13 1743021073",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=43486945",
    "comments": [
      ">BleepingComputer has confirmed with multiple companies that associated data samples shared by the threat actor are valid.>In addition to the data, rose87168 shared an Archive.org URL with BleepingComputer for a text file hosted on the \"login.us2.oraclecloud.com\" server that contained their email address. This file indicates that the threat actor could create files on Oracle's server, indicating an actual breach.Oracle probably should have just admitted the validity up front.It's not like there are any real penalties to a breach. Lying about it is probably a worse PR hit than the breach itself.\n \nreply",
      "> It's not like there are any real penalties to a breach.Not in the US maybe. In the EU under GDPR you have to disclose within 48h of you realizing (or made aware of) the breach.There are fines (at least) if you don't disclose it afaik.Oracle is gonna have issue with the EU, most likely.\n \nreply",
      "Maybe the EU wasn't on the Signal group chat when Oracle notified The Atlantic of the breach\n \nreply",
      "SEC Fact Sheet: Public Company Cybersecurity Disclosures; Final Rules - https://www.sec.gov/files/33-11216-fact-sheet.pdf\n \nreply",
      "I mean it's true that there's a rule, but at this point in US history I think we have reason to be sceptical that it will be enforced.\n \nreply",
      "The SEC selectively enforcing the rule does not prohibit a shareholder suit against the company. \"Everything Everywhere Is Securities Fraud\" after all.\n \nreply",
      "Have their been any GDPR fines that amount to more than a rounding error of Oracle's revenue? Admittedly, I don't watch too closely, but from the ones I am aware of, I haven't seen any GDPR fines that made me finally think \"wow, that might actually count as a punishment\". (I would honestly be happy to learn of some!)There are disclosure laws in the US as well, but again, the fines are like a days worth of revenue. Maybe the breached company has to provide a year of credit monitoring for the affected persons, if lucky.\n \nreply",
      "Several of the fines have been in the hundreds of millions of dollars - and while not crushing to Oracle, that's actual money that will definitely change behavior.. \nhttps://www.enforcementtracker.com/\n \nreply",
      "Many of these are against public bodies... Hundreds of pages with lawyers back and forth for in the end money going from one part of the government to another...\n \nreply",
      "Nice, thanks for the link!The largest fine ever issued is about 2% of Oracle's 2024 revenue. If we average the top 5 fines ever issued (this breach surely wont result in the largest GDPR fine ever), it'd be about 1% of Oracle's 2024 revenue. So, between ~3.5 and ~7 days worth of revenue, if we're lucky and get a top 5 GDPR fine?I'm not sure that is in the \"definitely change behavior\" area yet (in fact, I'm confident it is not), but better than I thought.\n \nreply"
    ],
    "link": "https://www.bleepingcomputer.com/news/security/oracle-customers-confirm-data-stolen-in-alleged-cloud-breach-is-valid/",
    "first_paragraph": ""
  },
  {
    "title": "Gemma3 Function Calling (ai.google.dev)",
    "points": 106,
    "submitter": "canyon289",
    "submit_time": "2025-03-23T07:31:15 1742715075",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=43451406",
    "comments": [
      "Hey folks,\nI'm on the Gemma team, we released new model(s) just recently, and I saw many questions here about function calling. We just published the docs to detail this more. In short Gemma3's prompted instruction following is quite good for the larger models and that's how you use the feature.You don't need to take our word for it! We were waiting for an external and independent validation from the Berkeley team, and they just published their results. You can use their metrics to get a rough sense of performance, and of course try it out yourself in AIstudio or locally with your own prompts.https://gorilla.cs.berkeley.edu/leaderboard.htmlHope you all enjoy the models!\n \nreply",
      "Thanks, Gemma is fantastic and that it supports function calling is great.\n \nreply",
      "so if i'm reading this correctly, it's essentially prompt engineering here and there's no guarantee for the output. Why not enforce a guaranteed output structure by restricting the allowed logits at each step (e.g. what outlines library does)?\n \nreply",
      "So in short there's no guarantee for any output from any LLM whether its Gemma or any other (ignoring some details like setting a random seed or parameters like temperature to 0). Like you mentioned though libraries like outlines can constrain the output, whereas hosted models often already include this in their API, but they can do so because its a model + some server side code.With Gemma, or any open model, you can use the open libraries in conjunction to get what you want. Some inference frameworks like Ollama include structured output as part of their functionality.But you mentioned all of this already in your question so I feel like I'm missing something. Let me know!But I think you already mentioned all this in your response so I might be missing the question?\n \nreply",
      "If you run Gemma via Ollama (as recommended in the Gemma docs) you get exactly that feature, because Ollama provides that for any model that they run for you: https://ollama.com/blog/structured-outputsUnder the hood, it is using the llama.cpp grammars mechanism that restricts allowed logits at each step, similar to Outlines.\n \nreply",
      "Is the format used in the examples the same that's used in the function calling instruction training, i.e. should it be the optimal prompt for function calling?I find it a bit frustrating when details of the training is not known and one has to guess what kinds of prompts the model has been tuned with.\n \nreply",
      "ToolACE-2-8B and watt-tool-8B have impressive score for the size in that leaderboard.\n \nreply",
      "Great, your work on open-source SLM are much appreciated ! (btw: seems like the google page does not respect the theme device \"auto\" setting)\n \nreply",
      "The example of function calling/structured output here is the cleanest example on how function it works behind the scenes, incorporating prompt engineering and JSON schema.With the advent of agents/MCP, the low level workflow has only become more confusing.\n \nreply",
      "Am I getting slightly different use-cases mixed up, or would it be better if everything just spoke MCP?\n \nreply"
    ],
    "link": "https://ai.google.dev/gemma/docs/capabilities/function-calling",
    "first_paragraph": "\nWhen using a generative artificial intelligence (AI) model such as Gemma, you\nmay want to use the model to operate programming interfaces in order to complete\ntasks or answer questions. Instructing a model by defining a programming\ninterface and then making a request that uses that interface is called function\ncalling.Gemma does not output a tool specific token.\nYour framework must detect a tool call by checking if the structure of\nthe output matches your prompted function output specification.You can use function calling for a number of applications:Function calling is supported in Gemma 3, but the function calling\ntechnique can be used with prior versions of Gemma. This guide provides\ninstructions on how to construct Gemma prompts that use function calling.\nWe recommend Gemma3 27B for the best performance,\nand Gemma3 12B for balanced performance and latency.You can use function calling with Gemma by constructing a prompt that provides\ninstructions that specify the output format and\n"
  },
  {
    "title": "Writing Programs with Ncurses (invisible-island.net)",
    "points": 62,
    "submitter": "begoon",
    "submit_time": "2025-03-23T13:27:26 1742736446",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=43452789",
    "comments": [
      "Ncurses has always been on my list of cool things I wanted to learn but couldn't invest the time to actually build something with.Then I found Textual (Python library, https://textual.textualize.io/) which feels like its modern and prettier child. Not gonna lie, it's still not a turnkey solution that transformed my clumsy scripts into a beautiful TUI without work but it's close!\n \nreply",
      "It's kind of crazy how much work it takes to get an interface on the level of ancient software to work on modern systems.\n \nreply",
      "It took even more work to get it to work on the ancient systems though, right?\n \nreply",
      "I always liked that on SuSE, I had my choice with YaST of either the Ncurses interface or the GUI X Window System.  The functionality was identical, but sometimes a GUI is pleasant.\n \nreply",
      "NCurses is peak ui design\n \nreply",
      "Always thought of (N)curses as the conceptual React framework for terminal applications. You tell it what the screen should look like and it figures out how to efficiently write just the changes needed to achieve that on the terminal.\n \nreply",
      "This is rather timely\u2014I started playing with PDCurses after being inspired by Zed Shaw\u2019s \u201cRogue is the Best Project\u201d.  It has been an interesting experience\n \nreply",
      "related: \"On Terminal Control\" https://xn--rpa.cc/irl/term.html\n \nreply",
      "Declaring all sample code AGPLv3 in the footer is toxic. I'm glad I scrolled before I polluted my brain with unusable information.\n \nreply",
      "Unless otherwise specified, you have no license to use any code you find on the Internet.\n \nreply"
    ],
    "link": "https://invisible-island.net/ncurses/ncurses-intro.html",
    "first_paragraph": "https://invisible-island.net/ncurses/This document is an introduction to programming with curses. It is not an exhaustive reference for the curses Application Programming Interface (API); that role is filled by the curses manual pages. Rather, it is intended to help C programmers ease into using the package.This document is aimed at C applications programmers not yet specifically familiar with ncurses. If you are already an experienced curses programmer, you should nevertheless read the sections on Mouse Interfacing, Debugging, Compatibility with Older Versions, and Hints, Tips, and Tricks. These will bring you up to speed on the special features and quirks of the ncurses implementation. If you are not so experienced, keep reading.The curses package is a subroutine library for terminal-independent screen-painting and input-event handling which presents a high level screen model to the programmer, hiding differences between terminal types and doing automatic optimization of output to ch"
  },
  {
    "title": "NotaGen: Symbolic Music Generation (electricalexis.github.io)",
    "points": 37,
    "submitter": "explosion-s",
    "submit_time": "2025-03-23T14:01:49 1742738509",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=43452990",
    "comments": [
      "I am really interested in this. The current paradigm from e.g. Suno is an all or nothing finished product. Producing intermediate assets allows you to do simple things like proper mastering or swapping instruments or editing melodies etc.\n \nreply",
      "The human performance at the bottom of the page was nice. But you can also give a great musician a zipper and paper bag and they can make it sound nice.It\u2019s just like LLM generated prose to me. The orchestration and motif development is musical sounding, but there\u2019s these artifacts that are like the \u201cIt\u2019s important to consider\u201d or \u201cWould you like further suggestions\u201d of the aural world.I think it would be fun to write up a detailed analysis of a full piece.\n \nreply",
      "A bit off topic, but is there any good recent Image to midi/XML/Sibelius stuff out there?\n \nreply",
      "This is where AI becomes a tool for the artists.We need an AI DAW, not Suno.\n \nreply",
      "Suno is for people who would otherwise not make music. The grandchild who wants to send grandma a funny song. The marketer who wants a quick solution for a social post. There are valid reasons for both types of AI products.\n \nreply"
    ],
    "link": "https://electricalexis.github.io/notagen-demo/",
    "first_paragraph": "\n            We introduce NotaGen, a symbolic music generation model aims to explore the potential of producing high-quality classical sheet music. Inspired by the success of Large Language Models (LLMs), NotaGen adopts pre-training, fine-tuning, and reinforcement learning paradigms (henceforth referred to as the LLM training paradigms). It is pre-trained on 1.6M pieces of music, and then fine-tuned on approximately 9K high-quality classical compositions conditioned on ``period-composer-instrumentation''  prompts. For reinforcement learning, we propose the CLaMP-DPO method, which further enhances generation quality and controllability without requiring human annotations or predefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in symbolic music generation models with different architectures and encoding schemes. Furthermore, subjective A/B tests show that NotaGen outperforms baseline models against human compositions, greatly advancing musical aesthetics in symbolic "
  },
  {
    "title": "Malware found on NPM infecting local package with reverse shell (reversinglabs.com)",
    "points": 166,
    "submitter": "gnabgib",
    "submit_time": "2025-03-26T17:53:47 1743011627",
    "num_comments": 102,
    "comments_url": "https://news.ycombinator.com/item?id=43484845",
    "comments": [
      "The fact that http fetches and fs reads don't prompt the user are continually the craziest part of the `npx` and `package.json`'s `postinstall`.Does anyone have a solution to wrap binary execution (or npm execution) and require explicit user authorization for network or fs calls?\n \nreply",
      "I believe the Deno permission system[0] does what you're asking, and more.(Deno is a JavaScript runtime co-created by Ryan Dahl, who created Node.js - see his talk \"10 Things I Regret About Node.js\"[1] for more of his motivations in designing it.)[0] https://docs.deno.com/runtime/fundamentals/security/[1] https://www.youtube.com/watch?v=M3BM9TB-8yA\n \nreply",
      "Yes and you can run almost every npm packages:  deno run npm:@angular/cli --help\n \nreply",
      "Yes, explicitly asking you if you want to run the install script is the first warning (which pnpm can do too)Then would halt due to file access or network permissions.Could still get you if you lazily allow all everywhere though and this is why you shouldn\u2019t do that.\n \nreply",
      "pnpm skips all `postInstall` runs by default now. You can explicitly allow-list specific ones.If you use that, I'd highly recommend configuring it to throw an error instead of just silently skipping the postInstall though:\nhttps://github.com/karlhorky/pnpm-tricks#fail-pnpm-install-o...\n \nreply",
      "Bun does the same.\n \nreply",
      "We built \u201csafe npm\u201d, a CLI tool transparently wraps the npm command and protects developers from malware, typosquats, install scripts, protestware, telemetry, and more.You can set a custom security policy to block or warn on file system, network, shell, or environment variable access.https://socket.dev/blog/introducing-safe-npm\n \nreply",
      "Just package node_modules subdirectories as tar files.I stopped using npm a while back and push and pull tar files instead.Naturally I get js modules from npm in the first place, but I never run code with it after initial install and testing of a library for my own use.\n \nreply",
      "This is a valid choice, but you must accept some serious trade-offs. For one thing, anyone wanting to trust you must now scrutinize all of your dependencies for modification. Anyone wanting to contribute must learn whatever ad hoc method you used to fetch and package deps, and never be sure of fully reproducing your build.The de facto compromise is to use package.json for deps, but your distributable blob is a docker image, which serializes a concrete node_modules. Something similar (and perhaps more elegant) is Java's \"fat jar\" approach where all dependencies are put into a single jar file (and a jar file is just a renamed zip so it's much like a tarball).\n \nreply",
      "The lavamoat npm package does something similar. It's maintained by the security team at MetaMask (crypto wallet extension and app). It's used in the extension runtime as well as wraps the build process.\n \nreply"
    ],
    "link": "https://www.reversinglabs.com/blog/malicious-npm-patch-delivers-reverse-shell",
    "first_paragraph": "\nLucija Valenti\u0107, Software Threat Researcher, ReversingLabs.\u00a0Read More...\nUnlike some other public repositories, the npm package repository is never really quiet. And, while there has been some decline in malware numbers between 2023 and 2024, this year's numbers don\u2019t seem to continue that downward trend. Still, while RL has detected some interesting npm malware so far this year, none of it warranted a detailed writeup.Then March rolled around, and two very interesting packages were published on npm: ethers-provider2 and ethers-providerz. These were simple downloaders whose malicious payload was cleverly hidden, with a second stage that \u201cpatches\u201d the legitimate npm package ethers, installed locally, with a new file containing the malicious payload. That patched file ultimately serves a reverse shell.\u00a0\u00a0This approach reveals a high level of sophistication on the threat actor\u2019s part that deserves some further analysis and exploration.\u00a0The package ethers-provider2 was still available on n"
  },
  {
    "title": "'Naive' science fan faces jail for plutonium import (yahoo.com)",
    "points": 135,
    "submitter": "geox",
    "submit_time": "2025-03-22T23:46:36 1742687196",
    "num_comments": 83,
    "comments_url": "https://news.ycombinator.com/item?id=43449645",
    "comments": [
      "> Lidden pleaded guilty to offences under Australia's nuclear non-proliferation act that carry a possible 10-year jail sentence.The bureaucratic apparatus, especially dealing with law enforcement always concentrates people who enjoy punishing others. Is Australia particularly bad about it perhaps? It seems they get some kind of sadistic enjoyment out of it. It's scary that everyone in the chain here: judge Leonie Flannery, Australian Border Force officials, police, even his employer just had a grand 'ol time punishing this guy. Everyone could have stopped, realizing it's obvious what's happening, give him a warning have him turn in this sample.And the best part for them, there is no repercussion for it. Everyone can turn around and publicly proclaim they just \"did their job\".\n \nreply",
      "It does this because \"the purpose of the system is what it does\"It ain't no different than the king's men occasionally cutting down a peasant that didn't remove his hat quickly enough when the king rode by.  By screwing people on a whim the system sends a \"don't cross me, I hold complete power\" message which acts as a force multiplier (until it doesn't but Aus isn't there yet).\n \nreply",
      "The best is when they use flimsy arguments about needing to \"make an example\" or \"discourage this behavior\" or \"create a deterrent\", as if people in these situations are even aware they're doing anything wrong.\n \nreply",
      "The message being sent is \"even if we can't prove intent we still can completely ruin your life\"\n \nreply",
      "The Royal Australian Air Force shut down airspace over an air force base to test fire a \u201chigh-powered\u201d single-shot .50 caliber rifle. They are a parody of themselves.\n \nreply",
      "Awareness that you're doing something wrong is a spectrum. Obviously this guy wasn't intending to build a nuclear bomb, but I'm extremely skeptical that a science nerd could get to the point of building a periodic table collection without learning that plutonium is dangerous and heavily restricted. (The source article doesn't cover this, so just to make sure we're on the same page: plutonium is _not_ any more legal to export from the US than it is to import into Australia, and whoever sold it to this guy was almost surely breaking the law too.)\n \nreply",
      "I don\u2019t think it\u2019s crazy for someone to know that you can buy uranium and not realize the full difference between that and plutonium.\n \nreply",
      "I seem to recall drug laws in mexico having carveouts to decriminalize small quantities to prevent abuse by law enforcement anywhere in the chain.Also, speed limits in some states can't be enforced from 0 to 5mph over the limit.\n \nreply",
      "> It seems they get some kind of sadistic enjoyment out of it.> Everyone could have stopped, realizing it's obvious what's happening, give him a warning have him turn in this sample.I think the sad truth is that they are probably ignorant enough to not even think about it.\n \nreply",
      "> Is Australia particularly bad about it perhaps?We may need to gather more data.  He's literally the first person to be sentenced for a law which has been on the books for decades.  In the meantime: https://en.wikipedia.org/wiki/List_of_countries_by_incarcera...> Judge Leonie Flannery, Australian Border Force officials, police, even his employer just had a grand 'ol time punishing this guy.Maybe? I'm gonna wait until after the sentence has been imposed to decide whether it's excessive or not.\n \nreply"
    ],
    "link": "https://au.news.yahoo.com/naive-science-fan-faces-jail-053025281.html",
    "first_paragraph": ""
  },
  {
    "title": "Cregit-Linux: how code gets into the kernel (linuxsources.org)",
    "points": 4,
    "submitter": "follower",
    "submit_time": "2025-03-23T08:53:31 1742720011",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=43451654",
    "comments": [
      "The linked site contains a token-based (rather than line-based) git \"blame\" annotation view for Linux kernel releases, i.e. it allows you to discover which commit added a particular token--where \"a token is what the C syntax considers a token\".An advantage of a per-token commit attribution view is you don't need to transit through the history of an entire line.Cregit is the tool used to produce the per-token view: https://github.com/cregit/cregitI encountered Cregit recently while doing some \"code history spelunking/archaeology\" and thought it seemed pretty nifty.Decided to share the link as there have been a couple of recent HN threads which included discussion about the poor granularity of line-based git \"blame\" functionality.The most recent kernel version on the site is v6.13: https://cregit.linuxsources.org/code/6.13/Unfortunately, the tool itself seems not to be under active development, the most recently modified branch is from 2 years ago & the main branch was last modified 6 years ago: https://github.com/cregit/cregit/tree/newinter\n \nreply",
      "In my case the particular code I was digging into was within the function `squash_the_stupid_serial_number(...)` as seen here: https://cregit.linuxsources.org/code/6.13/arch/x86/kernel/cp...Unfortunately the direct line-number link above doesn't seem to provide the same view as manually choosing \"squash_the_stupid_serial_number(...)\" rather than \"Overall\" in the selection/option menu accessible via the page link: https://cregit.linuxsources.org/code/6.13/arch/x86/kernel/cp...(Apparently the only code directly from Linus' original commit for that function is `, lo, hi`. :) And, unfortunately, being from the pre-git era, it is lacking any sort of informative commit message...)Also, as the documentation mentions the token-tracing isn't perfect, e.g. the `squash_the_stupid_serial_number` name did actually exist before the cregited commit: https://github.com/torvalds/linux/commit/0a488a53d7ca46ac638...(Perhaps this could be related to there being multiple function implementations selected via #ifdef?)\n \nreply"
    ],
    "link": "https://cregit.linuxsources.org",
    "first_paragraph": ""
  },
  {
    "title": "How to Delete Your 23andMe Data (eff.org)",
    "points": 147,
    "submitter": "hn_acker",
    "submit_time": "2025-03-26T19:41:20 1743018080",
    "num_comments": 79,
    "comments_url": "https://news.ycombinator.com/item?id=43486236",
    "comments": [
      "To be honest, this is more like \"requesting the data to be deleted\". There's nothing that guarantees that the personal information will be physically wiped out of the hard drives used to store them.\n \nreply",
      "Also, what prevents new owners from restoring from backups because \"we were hacked\" or any other reason for retrieving backup data for something that is currently \"deleted\"?\n \nreply",
      "But it creates legal grounds for lawsuits if they don't.\n \nreply",
      "Sure, you can sue the hollowed out shell of a bankrupt limited liability corporation that will soon have no assets for a court to sieze for whatever paltry damages a court finds.\n \nreply",
      "Good point. There is no confirming the data was actually deleted, beyond the request to do so. There is nobody to go after, as the new owner does not have to respect the request, unless a court orders them to do so. Even in that case, going after the new owner, it would appear to be time critical. Can users stop the selling or transfer of data, before any court could block it?The correct timing, appears to be that 23andMe would have had an opt-out, blocking the selling or transfer of their data. That also should have included confirmation of data deletion, if requested. Since none of that existed, the options for users are quite limited. In fact, many would have participated in 23andMe Research, so their data was likely going to wherever long before.\n \nreply",
      "Ideally the court can compel the purchaser(s) to destroy the data.\n \nreply",
      "How you lawyer is going to prove the data is not deleted?And what damages are you going to claim in court.Lawyers are not cheap, no lawyer will work on a case less that even $1k. My only hope is donating to privacy fighting organizations like EFF that file class actions.\n \nreply",
      "A lawsuit costs money, and doesn't un-sell the data.\n \nreply",
      "Of course. And I'm not saying that they might do it in malice.All I'm suggesting is that tapping some pixels on your backlit rectangular glass won't necessarily translate into pulses of electrons that'll eradicate the 0s and 1s representing your data.I'm sure that corner of the codebase is one of the least visited parts, so bugs may lurk in, or misconfigurations, etc.\n \nreply",
      "This feels as hopeless as trying to keep your email/contacts from social media sites. Even if you are vigilant about never allowing an app/service to download your contacts, your friends will share theirs and it is trivial to recreate your contact list. If I keep my DNA from these companies, my relatives will share theirs and they basically have my DNA.\n \nreply"
    ],
    "link": "https://www.eff.org/deeplinks/2025/03/how-delete-your-23andme-data",
    "first_paragraph": "This week, the genetic testing company 23andMe filed for bankruptcy, which means the genetic data the company collected on millions of users is now up for sale. If you do not want your data included in any potential sale, it\u2019s a good time to ask the company to delete it. When the company first announced it was considering a sale, we highlighted many of the potential issues, including selling that data to companies with poor security practices or direct links to law enforcement. With this bankruptcy, the concerns we expressed last year remain the same. It is unclear what will happen with your genetic data if 23andMe finds a buyer, and that uncertainty is a clear indication that you should consider deleting your data. California attorney general Rob Bonta agrees.Before you delete your account, you may want to download the data for your own uses. If you do so, be sure to store it securely. To download you data:There are other types of data you can download on this page, though much of it "
  }
]