[
  {
    "title": "Bevy TLDR \u2013 Game development with Bevy summarized (taintedcoders.com)",
    "points": 12,
    "submitter": "GenericCanadian",
    "submit_time": "2025-10-18T23:59:40 1760831980",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://taintedcoders.com/bevy/tldr",
    "first_paragraph": "The goal of this document is to provide the maximum amount of important content in the minimum amount of words. It can be useful to give this to your language model to give it an up to date source of information about Bevy.The full text of this document in markdown can be found on the Bevy starterBevy is an archetype Entity-Component-System (ECS) game engine built in Rust. It emphasizes modularity, performance, and ease of use.An Entity on its own holds no data or behavior. The actual Entity is just an identifier to find associated components where the real data is stored.Each Entity can only have a single Component of each type. These components can be added and removed dynamically over the course of the entity's lifetime. Everything is stored inside a World and everything is managed by the App.A good mental model to use is that entities represent a row in an in-memory database, while components are our columns.We define components by deriving the Component trait:Components have 5 dif"
  },
  {
    "title": "Chen-Ning Yang, Nobel laureate, dies at 103 (chinadaily.com.cn)",
    "points": 133,
    "submitter": "nhatcher",
    "submit_time": "2025-10-18T05:47:14 1760766434",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=45625229",
    "comments": [
      "Underrated even among physicists. Among the immediate post war generation his contributions are up there with Feynman and Schwinger.To quote Freeman Dyson: \"Professor Yang is, after Einstein and Dirac, the preeminent stylist of the 20th \ncentury physics. From his early days as a student in China to his later years as the \nsage of Stony Brook, he has always been guided in his thinking by a love of exact \nanalysis and formal mathematical beauty. This love led him to his most profound \nand original contribution to physics, the discovery with Robert Mills of non-Abelian \ngauge fields. With the passage of time, his discovery of non-Abelian gauge fields \nis gradually emerging as a greater and more important event than the spectacular \ndiscovery of parity non-conservation which earned him the Nobel Prize.\"reply",
      "I came to know of this guy though Jim Simons.He once \"leaked\" the idea that Jim Simon's trading success came from his use of ideas called \"gauge theory\" and \"fibre bundles\".I forgot the exact timestamp, but you will have to watch the entire interview to find that segment \u2014 https://youtu.be/zVWlapujbforeply",
      "Simons himself completely disspells this idea in his interview on Numberphile.reply",
      "It's a trivial statement since many equities are correlated on a multidimensional manifold of characteristics. Jim Simons was just early and now rentech is nothing special.reply",
      "Rentec is still world renowned after pioneering the quant business 40+ years ago. I don't think the rested on their laurels with some easy thing that they just stumbled on earlyreply",
      "This reads like \u201cNewton or Einstein were just early\u201d.  That\u2019s the whole thing, being the first person to do it.reply",
      "One of the Feynman lectures in physics is about Yang, Lee, and Wu's discovery. I thought it was a great listen:https://www.feynmanlectures.caltech.edu/flptapes.html#52 Symmetry in physical lawsreply",
      "Why didn't they mention them in the lecture notes?reply",
      "I am a former physics student of C. N. Yang\u2019s at Stony Brook University.Rest In Peace.reply",
      "Of Yang-Mills fame among many other things:https://en.wikipedia.org/wiki/Yang_Chen-NingRIPreply"
    ],
    "link": "https://www.chinadaily.com.cn/a/202510/18/WS68f3170ea310f735438b5bf2.html",
    "first_paragraph": "Yang Chen-Ning, a world-renowned physicist and Nobel laureate, passed away in Beijing on Saturday at 103.Yang, an academician of the Chinese Academy of Sciences, professor at Tsinghua University, and the honorary president of the Institute for Advanced Study at Tsinghua, died after an illness, the university said in an obituary, calling the late professor \"immortal\".Together with his colleague Tsung-dao Lee, Yang was awarded the Nobel Prize in Physics in 1957 for their theory of parity non-conservation in weak interaction.He was often ranked alongside Albert Einstein as one of the 20th century's greatest physicists.Born in Hefei, Anhui province, in 1922, Yang moved with his family to Tsinghua in 1929. He enrolled at the National Southwestern Associated University in 1938 and later entered the graduate school of Tsinghua University in 1942, earning a master's degree in science in 1944. In 1945, he went to the United States for further studies as a Tsinghua University government-sponsore"
  },
  {
    "title": "Root System Drawings (wur.nl)",
    "points": 280,
    "submitter": "bookofjoe",
    "submit_time": "2025-10-18T13:52:24 1760795544",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=45627394",
    "comments": [
      "Ever thought you yanked a dandelion out by the entire root? Think again: https://images.wur.nl/digital/collection/coll13/id/676/rec/3reply",
      "Always good to have a weed puller in your toolshed. A stand-up puller, specifically, that operates as a lever, allowing it to first grab deeply and then through a rotation of the handle it pulls out quite a bit of the root system. A lifesaver if you have a rain garden which is really just a synonym for weed garden.reply",
      "Any recommendations for a particular weed puller?reply",
      "This CobraHead weeder has worked well for mehttps://www.amazon.com/CobraHead-Original-Weeder-Cultivator-...reply",
      "It's kinda gimmicky but I found this thing on clearance at a local hardware store and it works fairly well (gets most weeds out without me having to bend over, which is nice): https://grampasweeder.com/collections/grampas-gardenware/pro...It doesn't get everything but I can do more work on the tough ones when so many come right out.reply",
      "I've never used a bad one, although I wouldn't class any of them as anything stellar. All of them have looked like a snake's twisted tongue.Using them depends on the delicate combination and application of brute force and technique. If your technique and brute force is up to spec, a crowbar works as a makeshift weed puller.reply",
      "My dad told me that one year his school held a contest over the summer to see who could get the longest dandelion root.reply",
      "Whats the units?reply",
      "Centimetres.Their 13 cm high plant specimen had a 456 cm deep root.reply",
      "So like 15 feetreply"
    ],
    "link": "https://images.wur.nl/digital/collection/coll13/search",
    "first_paragraph": ""
  },
  {
    "title": "The reason GCC is not a library (2000) (gcc.gnu.org)",
    "points": 51,
    "submitter": "todsacerdoti",
    "submit_time": "2025-10-12T10:23:48 1760264628",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=45557032",
    "comments": [
      "Oh, this is one of my favorite (and sad!) dramas in free software.Five years later the main llvm developer proposed [0] to integrate it into gcc.Unfortunately, this critical message was missed by a mail mishap on Stallman's part; and he publicly regretted both his errors (missing the message and not accepting the offer), ten years later [1].The drama was discussed in realtime here in HN [2].[0] https://gcc.gnu.org/legacy-ml/gcc/2005-11/msg00888.html[1] https://lists.gnu.org/archive/html/emacs-devel/2015-02/msg00...[2] https://news.ycombinator.com/item?id=9028738reply",
      "I feel like this is a sort of evidence that even for the most serious of engineers email lists are not an ideal way to communicate.reply",
      "So, having been around a lot of different communication methods, I think email lists aren\u2019t ideal, but for serious projects they\u2019re better than all the alternatives.Chat has a way of getting completely lost. All your knowledge that goes into chat either goes into somebody\u2019s head or it just disappears into the ether. This includes Slack, Discord, Teams, etc. Useful as a secondary channel but serious projects need something more permanent.Bug tracking systems just don\u2019t support the kind of conversations you want to have about things. They\u2019re focused on bugs and features. Searchability is excellent, but there are a lot of conversations which just end up not happening at all. Things like questions.That brings us back to mailing lists. IMO\u2026 the way you fix it is by having redundancies on both sides of the list. People sending messages to the mailing list should send followup messages. You should also have multiple people reading the list, so if one person misses a message, maybe another gets it.Mailing lists are not perfect, just better than the alternatives, for serious projects.(I also think forums are good.)reply",
      "This is why the D community has forums. The messages are all archived as static web pages and are a gold mine of information.https://www.digitalmars.com/d/archives/digitalmars/D/reply",
      "Discourse (or other forums) would be my pick and what I\u2019ve driven other projects I\u2019m involved with to success.They can be treated like mailing lists, but are easy to navigate , easy to search and index, and easy to categorize.reply",
      "Piling on about chat. Slack threads are an abomination. They aren\u2019t inline with the main channel so you can\u2019t cut and paste an entire conversation with threads. And does exporting a channel include threads? Who knows because the admin wouldn\u2019t do it for me.reply",
      "praise https://github.com/rusq/slackdumpit does include threads, and no need for adminsreply",
      "Still, we are discussing it almost 30 years after it happened.  What alternative messaging system offers such openness and stability?  I don't see anything other than publicly archived mailing lists.reply",
      "Freedom through obscurutiyreply",
      "And the result is that most new open source languages (and commercial companies) use LLVM instead of GCC as the backend => way more engineering resources are dedicated to LLVM.reply"
    ],
    "link": "https://gcc.gnu.org/legacy-ml/gcc/2000-01/msg00572.html",
    "first_paragraph": "This is the mail archive of the \ngcc@gcc.gnu.org\nmailing list for the GCC project.\n\n\n\n\n\n\n\n\nIndex Nav:\n\n        [Date\u00a0Index] [Subject\u00a0Index] [Author\u00a0Index] [Thread\u00a0Index]\n      \n\nMessage Nav:\n[Date\u00a0Prev]\u00a0[Date\u00a0Next]\n[Thread\u00a0Prev]\u00a0[Thread\u00a0Next]\n\n\n\n\n\n\nRe: Converting the gcc backend to a library?\n\n\n\nTo: gcc at gcc dot gnu dot org\nSubject: Re: Converting the gcc backend to a library?\nFrom: Richard Stallman <rms at gnu dot org>\nDate: Mon, 17 Jan 2000 19:51:25 -0700 (MST)\nReply-to: rms at gnu dot org\n\n\n\n\n\n\n\nCompanies often try to make software non-free, and some would write\nnon-free add-ons to GCC if we let them.  The reason we have free C++\nand Objective C support is because the companies which wrote these\nfront ends had no *feasible* way to use them without making them part\nof GCC, where the GPL required them to be free.  It is vital that we\npreserve this situation.\n\nAnything that makes it easier to use GCC back ends without GCC front\nends--or simply brings GCC a big step closer to a form t"
  },
  {
    "title": "Titan submersible\u2019s $62 SanDisk memory card found undamaged at wreckage site (tomshardware.com)",
    "points": 179,
    "submitter": "WithinReason",
    "submit_time": "2025-10-17T06:39:09 1760683149",
    "num_comments": 106,
    "comments_url": "https://news.ycombinator.com/item?id=45613898",
    "comments": [
      "I see a lot of discussion in this thread stemming from some confusion+not reading the actual report[0].Some key points:1. The Camera+Card was encased in a separate enclosure made of titanium+sapphire, and did not seem to be exposed to extreme pressures.2. The encryption was done via a variant of LUKS/dm-crypt, with the key stored on the NVRAM of a chip (Edited; not in TrustZone).3. The recovery was done by transplanting the original chip onto a new working board. No manufacturer backdoors or other hidden mechanisms were used.4. Interestingly, the camera vendor didn't seem to realize there was any encryption at all.[0] https://data.ntsb.gov/Docket/Document/docBLOB?ID=18741602&Fi...reply",
      "Unless I misread the article, the key was stored in the NVRAM and not the TrustZone.IIRC, the article stated that if the key(s) had been stored in the TrustZone then the data would have been irrecoverable.reply",
      "Good catch; it was somewhat ambiguous in the report.reply",
      "0. They were too cheap to use an industrial grade SD. Mind boggling.reply",
      "If the encryption was that easy to bypass, was it worth it at all?reply",
      "The manufacturer didn\u2019t even know encryption was enabled, because as long as the camera was working, it would just provide all files over USB without any encryption.It was basically enabled by accident, and the only thing it prevented was recovery of files directly from the SD card when the camera was damaged.reply",
      "There are some reasons you'd want to encrypt even without a secret key. One is it makes it easier to erase data (just erase the key).It also makes bit flip errors a lot more obvious, which is another way of saying harder to ignore, so that can go either way.reply",
      "Can't bit flip errors also destroy encrypted volumes much more easily?reply",
      "I think it depends.  Encrypted filesystems typically encrypt contents of each file separately - that way you don't need to read / write the whole disk to read it write any individual file contents.  Of course that means metadata may be in plain text or may be separately encrypted - again possibly folder by folder instead of all metadata at once.  Exact details would vary with different file system encryption schemes.Whereas if you image the disk and encrypt the image properly, that gives you all the great confidentially guarantees but no random access.reply",
      "Sure. If the card was recovered without the camera motherboard then the decryption key would not have been recovered.reply"
    ],
    "link": "https://www.tomshardware.com/pc-components/microsd-cards/tragic-oceangate-titan-submersibles-usd62-sandisk-memory-card-found-undamaged-at-wreckage-site-12-stills-and-nine-videos-have-been-recovered-but-none-from-the-fateful-implosion",
    "first_paragraph": "The specialist camera was rated to 6,000m, but the lens and some of its components were probably damaged by the implosion.\nWhen you purchase through links on our site, we may earn an affiliate commission. Here\u2019s how it works.\nRecovery teams working on the Titan submersible have found the vessel's specialist stills and video camera intact. Fascinatingly, while there was some damage to the camera\u2019s housing and internal components, tech and science enthusiast Scott Manley reveals that the internal SD card was \u201cundamaged.\u201d Contents of the memory card have since been investigated, and 12 stills and nine videos have been recovered.The recovery teams found a hardened underwater camera in the wreckage of the Titan submersible, and inside the casing was an undamaged SD card. pic.twitter.com/QCOtdcS7dUOctober 15, 2025Click 'See more' for images.In the images, you can see a SubC-branded Rayfin Mk2 Benthic Camera, recovered from the wreckage of the ill-fated submersible operated by OceanGate. This"
  },
  {
    "title": "How to sequence your DNA for <$2k (maxlangenkamp.substack.com)",
    "points": 101,
    "submitter": "yichab0d",
    "submit_time": "2025-10-18T19:58:25 1760817505",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=45629970",
    "comments": [
      "We are not \u201cin the nanopore era of sequencing\u201d. We are (still) firmly in the sequencing by synthesis era.Yes it requires chopping the genome opening small(er) pieces (than with Nanopore sequencing) and then reconstructing the genome based on a reference (and this has its issues). But Nanopore sequencing is still far from perfect due to its high error rate. Any clinical sequencing is still done using sequencing by synthesis (at which Illumina has gotten very good over the past decade).Nanopore devices are truly cool, small and comparatively cheap though, and you can compensate for the error rate by just sequence everything multiple times. I\u2019m not too familiar with the economics of this approach though.With sbs technology you could probably sequence your whole genome 30 times (a normal \u201ccoverage\u201d) for below 1000\u20ac/$ with a reputable company. I\u2019ve seen 180$, but  not sure  if I\u2019d trust that.reply",
      ">you can compensate for the error rate by just sequence everything multiple times.Usually, but sometimes the errors are correlated.Overall I agree, short read sequencing is a lot more cost effective. Doing an Illumina whole genome sequence for cell line quality control (at my startup) costs $260 in total.reply",
      "> But Nanopore sequencing is still far from perfect due to its high error rate. Any clinical sequencing is still done using sequencing by synthesis (at which Illumina has gotten very good over the past decade).There is no reason for Nanopore to supplant sequencing-by-synthesis for short reads - that's largely solved and getting cheaper all the while.The future clinical utility will be in medium- and large-scale variation. We don't understand this in the clinical setting nearly as well as we understand SNPs. So Nanopore is being used in the research setting and to diagnose individuals with very rare genetic disorders.(edit)> We are not \u201cin the nanopore era of sequencing\u201d. We are (still) firmly in the sequencing by synthesis era.I also strongly disagree.SBS is very reliable but it's common (if Toyota is the most popular car, does that mean we're in the Toyota internal combustion era? Or can Waymo still matter despite its small footprint?).Novelty in sequencing is coming from ML approaches, RNA-DNA analysis, and combining long- and short-read technologies.reply",
      "I agree with you. Long reads lead to new insights and over time to better diagnoses by providing better understanding of large(r) scale aberrations, and as the tech gets better will be able to do so more easily. But is really not there yet. It\u2019s mostly research and somehow it\u2019s not really improving as much as hoped, I get the feeling.reply",
      "You can get it pretty damn cheap if you are willing to send your biological data overseas. Nebula genomics and a lot of other biotechs do this by essentially outsourcing to China. There's no particular technology secret, just cheaper labor and materials.reply",
      "Interesting concept, but between the broken hardware and the way they gave up before getting anything useful this article was rather disappointing:> Another problem was our flow cell was malfunctioning from the start \u2014 only 623 out of 2048 pores were working.Is this normal for the machine? Is there a better write up somewhere where they didn\u2019t give up immediately after one attempt?reply",
      "Hi, believe it or not, I have actually done what the authors were attempting. I used saliva rather than blood as a source of DNA and extracted it using a Qiagen kit.My Nanopore flow cell had nearly every pore working from the start. So I would say that is not normal. Maybe it was stored incorrectly.reply",
      "it depends of the sample. usually you have at least 1200, with a guaranteed of at least 800, so maybe he could ask for a refund.reply",
      "Nebula and Dante will do this for like $300, and you can get 30x coverage at every base or even 100x coverage if you pay a little more. The $1000 genome was here more than a decade ago.reply",
      "I wanted to try this, but I looked into Nebula a bit more.Nebula is facing a class action for apparently disclosing detailed genomic data to Meta, Microsoft & Google. The subreddit is also full of reports of people who never received their results years after sending their kits back. There are also concerns about the quality of sequencing and false positives in all DTC genomics testing. Given what happened with 23andme as well and all of this stuff, I'm wary of sending my genetic data to any private company.reply"
    ],
    "link": "https://maxlangenkamp.substack.com/p/how-to-sequence-your-dna-for-2k",
    "first_paragraph": ""
  },
  {
    "title": "GoGoGrandparent (YC S16) Is Hiring Back End and Full-Stack Engineers",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-10-19T01:00:42 1760835642",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "item?id=45631422",
    "first_paragraph": ""
  },
  {
    "title": "Tinnitus Neuromodulator (mynoise.net)",
    "points": 245,
    "submitter": "gjvc",
    "submit_time": "2025-10-18T16:08:10 1760803690",
    "num_comments": 159,
    "comments_url": "https://news.ycombinator.com/item?id=45628391",
    "comments": [
      "I've had tinnitus in my left ear for about six months now.  I was hoping it was the result of an earwax impaction or something, but after having several specialists look at my ears, test my hearing, and getting an MRI to check for tumors, the overwhelming medical consensus of the cause appears to be \"I dunno\", and at this point I have given up on it being temporary.About 95% of the time, I can fairly easily just tune it out and it's no different than any other background noise.  Living in NYC helps, there's a fair amount of constant background noise even in the best of times. I've found that finding 10-hour videos on YouTube of TV static at a low volume can be helpful for the remaining 5%.Still I would really prefer it wasn't there.  The ringing in my left ear is still annoying, and I'm only in my mid 30's, so assuming an average lifespan I have anywhere from 40-60 years left to enjoy this constant ringing.I'll play with this thing to see if it helps.reply",
      "A message of hope.I got mine in my 30's too. The first week I thought I was going crazy, and this was the end of my life. I was shocked, I couldn't go to work for a whole week.I then saw a doctor who said to me: \"Man, I've got tinnitus since 20 years and I barely hear it anymore. The more you accept it, the more it'll fade.\"A decade later, my own experience is exactly this. I accepted it as one of the body malfunctions that comes with age for everybody. I barely hear it anymore except in extremely low noise situations and it doesn't bother me at all.I wish you well.reply",
      "I've always been someone who hears high pitched noises that \"normal\" people don't. I'm also in my 30s, and I'm sure those \"teenage alarms\" in Japan would work on me. I was the one who would walk up to a CRT and turn it off when everyone else thought it already was.What helped me accept (and ignore) tinnitus was realizing that I had already grown accustomed to tolerating that sound indoors. When's it's something you have no agency over (like \"it's an old house and the wires just make that sound sometimes\"), you learn it's part of the environment.Accepting it as part of the environment gets you past the \"OMG my body is ruined forever\" anxieties and back to normal life.reply",
      "By the way, that CRT squeel is the sound of the flyback transformer, which operates at 15.625 kHz for PAL and 15.734 kHz for NTSC sets.reply",
      "This is so relatable, though it has a strange downside. I've had tinnitus for as long as I can remember and always thought I was some superhuman child who could hear electricity. Didn't actually realize it was tinnitus until I heard it at the top of a mountain I was hiking in remote New Mexico a few years ago. I probably got it from chronic sinusitis as a child, but I'm still not sure what to make of it.reply",
      "I heard older TVs being turned on and off as well as CRT monitors. Now, its that very range I 'hear' all the time. Part of me wonders if it was sensitivity to that spectrum that damaged my hearing when I was around multiple CRTs so much.I have known people that have it much worse than I face daily.reply",
      "Yeah, for me it sounds almost exactly like the squeal that CRT TVs make.  Like, it's basically indistinguishable from that for me.reply",
      "For me, after 20'ish years with tinnitus, the only thing that brings the buzz to the foreground is reading/hearing the word \"tinnitus\".reply",
      "Almost the exact thing to me, 20+ years of tinnitus which sits calmly in the background most of the time except when I read the word tinnitus, or I'm feeling anxious for some reason: when I can't fall asleep when I need a good rest, life stresses, and those moments when a different pitch shows up in one ear and louder. In those occasions I can clearly hear the tone of mine.It's mildly annoying but I've definitely learnt to live with it pretty ok.reply",
      "Haha, its funny you say that because I've been reading a novel at the moment where the main character has debilitating tinnitus, and every time the author describes it, I can hear my own.reply"
    ],
    "link": "https://mynoise.net/NoiseMachines/neuromodulationTonesGenerator.php",
    "first_paragraph": "Neural Hack \u2022 Pulsatron \u2022 Dreamesque \u2022 Sinescape \u2022 Jammer \u2022 Sci-Fi \u2022 Trance \u2022 No Avail \u2022 Neural Drops \u2022 \n\u2117\u00a0Surprise!\n\nMono \u2022\r\nNarrow \u2022\r\nNormal \u2022\r\nWide\n \u2117\nG\n# \u2022\r\n\tA\n# \u2022\r\n\tB \u2022\r\n\tC\n# \u2022\r\n\tD\n# \u2022\r\n\tE \u2022\r\n\tF\n#\n\n\u2117 Speed \u00f72 \u2022 \r\n\tSpeed x2 \u2022\r\n\tRemix \u2022\r\n\tReset\n\n\n       Save to URL\n    \n\n\n      Save in this Browser\n    \n    \u2022\n    \n      Load\n    \n\n\n      Open Mini-Player\n    \n\nbeta\n      Open in the myNoise app (QR)\n    \n\n\u2117 \n       Order Audio File\n    \n\n      \u00a0\u00a0\u00a0\n    \nScan with your Phone\nNone \u2022 \r\nBalanced  \u2022 \r\nFull\n\nVisualizer \u2022\nWhite \u2022\nPiNk \u2022\nBrown \u2022\nJ\u2193\u2191K \u2022\nHelp\n\n\u2117 Enter the Meditation Room beta\n\t\nThis is the result of a partnership between Steve Harrison, the creator of Tinnitus Works, and myNoise. This is not just a strange-sounding generator; it is a helpful tool made to assist you in managing your tinnitus.\nMixing Steve's sequences with the unique myNoise sound engine allows you to tailor the sound to your individual tinnitus symptoms. This gives you amazing control, resulting in great outcom"
  },
  {
    "title": "Flowistry: An IDE plugin for Rust that focuses on relevant code (github.com/willcrichton)",
    "points": 163,
    "submitter": "Bogdanp",
    "submit_time": "2025-10-18T14:33:22 1760798002",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=45627692",
    "comments": [
      "Actual paper: [1]This could be useful. I've been plugging away, on and off, on the concept of statically checked back-references for Rust. This is one of the biggest complaints that C/C++ people have about Rust - if A points to B, it's really hard to have a pointer from B to A.\nThis leads to unsafe workarounds.You can do it safely with Rc, RefCell, Weak, borrow(), borrow_mut(), upgrade(), and downgrade(). It's verbose, adds run-time overhead, and there's the potential of panicking at run time on a double borrow. But the expressive power is there. This is work in progress, and I have some notes here.[2]The thing that's hard to check statically that borrows are disjoint as to scope. Borrows have lifetime scopes. If those lifetime scopes do not overlap, the borrows do not clash. Checking this across function calls is hard. (Checking across generic function calls is worse.) The Flowistry approach might help.\nThe note that \"Flowistry does not completely handle interior mutability\" is a concern, because we're analyzing things that use RefCell.The practical problem is to come up with a set of restrictions that are 1) sound, 2) checkable at compile time without too much compute effort, 3) allow programmers to do most of the legit things people want to do with back pointers, such as have a reference to the parent node in a tree, and 4) lead to usable diagnostic messages for problems.[1] https://arxiv.org/abs/2111.13662[2] https://github.com/John-Nagle/technotes/blob/main/docs/rust/...reply",
      "Does this exist for larger/more informal dependency relationships within a function body in other programming languages?For instance, if I highlight a parameter or variable foo, can I see not only all usages of foo itself, but usages of any variable that was derived from foo?While borrow usage makes this foolproof, this type of visualization would be tremendously useful for even other types of code.(As for Flowistry, I can see this being vital for anyone trying to maintain e.g. https://github.com/servo/servo/blob/main/components/layout/f... - perhaps the most daunting single file in a modern codebase I've ever seen! And yes, that's a 400-line function.)reply",
      "This is called program slicing in generalreply",
      "looks fantastic, and rust is probably a great language for this since ownership restricts effects - even if you add it to python, you can't really trust it, because at runtime you can just run up a call stack and modify memory at any time.  (though I would still definitely want it, as it's usually going to be correct)reply",
      "(Author here) That's exactly why I built this for Rust, and why it's difficult to replicate in any other language.reply",
      "Hey author, interested about your bio and work.Any way to follow what you up to?reply",
      "My personal site (https://willcrichton.net/), lab site (https://cel.cs.brown.edu/), and Mastodon page (https://mastodon.social/@tonofcrates) are all good ways to follow me!reply",
      "Why not contribute to rust-analyzer's documentHighlight LSP method? It behaves very similar to what GIFs are showing.Seems like a very specific feature to have plugin for.https://microsoft.github.io/language-server-protocol/specifi...reply",
      "Explained in README: https://github.com/willcrichton/flowistry#why-isnt-flowistry...reply",
      "The author has a nice talk diving deep into the routines research and the plugin in a Rust East Coast talk here: https://youtu.be/aYmuMlzvjvcreply"
    ],
    "link": "https://github.com/willcrichton/flowistry",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Flowistry is an IDE plugin for Rust that helps you focus on relevant code.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\nFlowistry is a tool that analyzes the information flow of Rust programs. Flowistry understands whether it's possible for one piece of code to affect another. Flowistry integrates into the IDE to provide a \"focus mode\" which helps you focus on the code that's related to your current task.For example, this GIF shows the focus mode when reading a function that unions two sets together:When the user clicks a given variable or expression, Flowistry fades out all code that does not influence that code, and is not influenced by that code. For example, orig_len is not influenced by the for-loop, while set.len() is.Flowistry can be helpful when"
  },
  {
    "title": "Is Postgres read heavy or write heavy? (crunchydata.com)",
    "points": 108,
    "submitter": "soheilpro",
    "submit_time": "2025-10-17T17:06:16 1760720776",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=45619108",
    "comments": [
      "At the time of writing the query has a small error. The filter is checking for reads and writes, but it should be reads or writes.    WHERE\n     -- Filter to only show tables that have had some form of read or write activity\n    (s.n_tup_ins + s.n_tup_upd + s.n_tup_del) > 0\n    AND\n     (si.heap_blks_read + si.idx_blks_read) > 0\n )\n\nShould be ORreply",
      "Agree with other commenters that the title is a bit confusing and should be renamed to something like \"Is your Postgres workload read heavy or write heavy?\"But title aside, I found this post very useful for better understanding PG reads and writes (under the hood) and how to actually measure your workloadCurious if the tuning actions any different if you're using a non-vanilla storage engine like AWS Aurora or GCP AlloyDB or Neon?reply",
      "This, as a few other commenters have mentioned, is a terrible article.For a start, the article does not mention any other database. I don't know how you can say something is read or write heavy without comparing it to something else. It doesn't even compare different queries on the same database. Like, they just wrote a query and it does a lot of reads - so what? There's nothing here. Am I going mad? Why does this article exist?reply",
      "A little context may be of help. Maybe a better headline for the article would have been, \"How Can You Determine if your PostgreSQL Instance's Workload is Read-Heavy or Write-Heavy?\"\nIt's useful to know to help optimize settings and hardware for your workload as well as to nkow whether an index might be useful or not. Most major DBMSs will have some way to answer this question, the article is aimed at PostgreSQL only.reply",
      "Despite using CTEs, I found the first query quite impenetrable. Could be because I don\u2019t spend that much time reading non-trivial SQL queries.I\u2019ve been mostly using the `pg_stat_statements` table (the second technique) to find out whether my workload is read or write heavy, it\u2019s plenty good in most situations.reply",
      "pg_ system tables aren\u2019t built for direct consumption. You typically have to massage them quite a bit to measure whatever statistic you need.reply",
      "This article quality makes me not trust the company.reply",
      "> When someone asks about [database] tuning, I always say \u201cit depends\u201d.Indeed. On your schema. On your usage. On your app. On your users.reply",
      "If it didn\u2019t depend they\u2019d just make the \u201ctuned\u201d value the default.reply",
      "Exactly. The parameters you can configure are there due to a lack of automating those since what you want to optimize for might be different than an automaton would.reply"
    ],
    "link": "https://www.crunchydata.com/blog/is-postgres-read-heavy-or-write-heavy-and-why-should-you-care",
    "first_paragraph": "David ChristensenOct 17, 2025\u00b710 min readDavid ChristensenOct 17, 2025\u00b710 min read\u00b7More by this authorWhen someone asks about Postgres tuning, I always say \u201cit depends\u201d. What \u201cit\u201d is can vary widely but one major factor is the read and write traffic of a Postgres database. Today let\u2019s dig into knowing if your Postgres database is read heavy or write heavy.Of course write heavy or read heavy can largely be inferred from your business logic. Social media app - read heavy. IoT logger - write heavy. But \u2026. Many of us have mixed use applications. Knowing your write and read load can help you make other decisions about tuning and architecture priorities with your Postgres fleet.Understanding whether a Postgres database is read-heavy or write-heavy is paramount for effective database administration and performance tuning. For example, a read-heavy database might benefit more from extensive indexing, query caching, and read replicas, while a write-heavy database might require optimizations lik"
  },
  {
    "title": "./watch (dotslashwatch.com)",
    "points": 311,
    "submitter": "shrx",
    "submit_time": "2025-10-18T09:55:06 1760781306",
    "num_comments": 82,
    "comments_url": "https://news.ycombinator.com/item?id=45626130",
    "comments": [
      "> taking inspiration from command-line interfacesIMO the take away from command-line interfaces is compact, precise and minimal design. In a transitional shell prompt like #~$, each character has its meaning. Merely copying these symbols to a watch face is the exact opposite spirit of command like interfaces.reply",
      "Cool project, but I also noticed the weird choice of #:~$ as a prompt, it uses almost half the width of the clock screen. And isn't # normally used to denote root shells? I don't think I ever saw it together with $.My favorite prompt is >: as a callback to the Swan computer in the TV show Lost (not sure if it's also used in early Apple computers).reply",
      "If I remember right, > was the prompt for Integer basic, ] for Applesoft Basic and * for the monitor.reply",
      "Yup.3D0G to start basic from the Monitorreply",
      "Good news it's an open source project so you can customise your prompt (:reply",
      "Perhaps one could configure it or fork and modify https://github.com/zsteig/.watchreply",
      "`date +whatever` right arg for the output would also make more sense than `./t` if there's roomreply",
      "Great hardware design, awful watchface design. The pseudo terminal interface looks like something I'd design right after discovering Linux at 13yo and making it my whole identify for a while.reply",
      "I don't understand everyone's harsh reactions. I too would've loved it at 13 during the same phase, so what?The retro(-style) Casio community lives for retro-future kitsch. I guess it's a matter of taste.reply",
      "> Great hardware design,Really?  It looks like it would be uncomfortable to wear with those screws on the back sitting proud of the surface.  Why aren't they countersunk?Or were you referring only to the electronics?reply"
    ],
    "link": "https://dotslashwatch.com/",
    "first_paragraph": ""
  },
  {
    "title": "Most users cannot identify AI bias, even in training data (psu.edu)",
    "points": 42,
    "submitter": "giuliomagnifico",
    "submit_time": "2025-10-18T18:13:27 1760811207",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=45629299",
    "comments": [
      "I'm curious how much trained in bias damages in-context performance.It's one thing to rely explicitly on the training data - then you are truly screwed and there isn't much to be done about it - in some sense, the model isn't working right if it does anything other than reflect accurately what is in the training data. But if I provide unbiased information in the context, how much does trained in bias affect evaluation of that specific information?For example, if I provide it a table of people, their racial background and then their income levels, and I ask it to evaluate whether the white people earn more than the black people - are its error going to lean in the direction of the trained-in bias (eg: telling me white people earn more even though it may not be true in my context data)?In some sense, relying on model knowledge is fraught with so many issues aside from bias, that I'm not so concerned about it unless it contaminates the performance on the data in the context window.reply",
      "DDG's search assist is suggesting to me that: Recognizing bias can indicate a level of critical thinking and self-awareness, which are components of intelligence.\"Most users\" should have a long, hard thought about this, in the context of AI or not.reply",
      "AIs/LLMs are going to reflect the biases in their training data. That seems intuitive.And personally, I think when people see content they agree with, they think it's unbiased. And the converse is also true.So conservatives might think Fox News is \"balanced\" and liberals might think it's \"far-right\"reply",
      "Yup it appears as neutral bias because (or when rather) it corresponds 1:1 with your belief system, which by default is skewed af. Unless you did a rigorous self inquiry and mapped your beliefs and thoroughly aware of them that\u2019s gonna be nearly always true.reply",
      "Nah, the later is an example of the former.reply",
      "Bias is different things though.  If most people are cautious but the LLM is carefree, then that is a bias.  Or if it recommends planting sorghum over wheat that is a different bias.In addition bias is not intrinsically bad.  It might have a bias toward safety.  That's a good thing.  If it has a bias against committing crime, that is also good. Or a bias against gambling.reply",
      "> And personally, I think when people see content they agree with, they think it's unbiased. And the converse is also true.> So conservatives might think Fox News is \"balanced\" and liberals might think it's \"far-right\"Article talks like when accidentally the vector for race aligns with emotion so it can classify a happy black personal as unhappy. Just because training dataset has lots of happy white people. It's not about subjective preferenceexplain how \"agreeing\" is relatedreply",
      "> And personally, I think when people see content they agree with, they think it's unbiased. And the converse is also true.One only has to see how angry conservatives/musk supporters get at Grok on a regular basis.reply",
      "It's amazing to watch https://bsky.app/profile/curious-maga.bsky.socialreply",
      "Man they really lose it over this.Also: Wow I\u2019m at -3 already on the previous comment. That really ruffled some feathers.reply"
    ],
    "link": "https://www.psu.edu/news/bellisario-college-communications/story/most-users-cannot-identify-ai-bias-even-training-data",
    "first_paragraph": "A study by researchers from Penn State and Oregon State University examined whether laypersons understand that unrepresentative data used to train AI systems can result in biased performance. Participants failed to notice the systematic bias in the training data, which used exclusively white faces to represent happy emotions and exclusively Black faces to represent unhappy emotions.\n\u00a0\u00a0Credit: Provided. All Rights Reserved.October 16, 2025By Jonathan F. McVerryUNIVERSITY PARK, Pa. \u2014 When recognizing faces and emotions, artificial intelligence (AI) can be biased, like classifying white people as happier than people from other racial backgrounds. This happens because the data used to train the AI contained a disproportionate number of happy white faces, leading it to correlate race with emotional expression. In a recent study, published in Media Psychology, researchers asked users to assess such skewed training data, but most users didn\u2019t notice the bias \u2014 unless they were in the negative"
  },
  {
    "title": "Why the open social web matters now (werd.io)",
    "points": 100,
    "submitter": "benwerd",
    "submit_time": "2025-10-14T19:50:57 1760471457",
    "num_comments": 41,
    "comments_url": "https://news.ycombinator.com/item?id=45583999",
    "comments": [
      "Maybe this was more of an intro/pitch to something I already support, so I wasn't quite the audience here.But I feel that talking about the open social web without addressing the reasons current ones aren't popular/get blocked doesn't lead to much progress. Ultimately, big problems with an open social web include:- moderation- spam, which now includes scrapers bringing your site to a crawl- good faith verification- posting transparencyThese are all hard problems and it seems to make me believe the future of a proper community lies more in charging a small premium. Even charging one dollar for life takes out 99% of spam and gives a cost to bad faith actors should they be banned and need another dollar to re-enter. Thus, easing moderation needs. But charging money for anything online these days can cause a lot of friction.reply",
      "In my opinion, both spam and moderation are only really a problem when content is curated (usually algorithmically). I don't need a moderator and don't worry about spam in my RSS reader, for example.A simple chronological feed of content from feeds I chose to follow is enough. I do have to take on the challenge of finding new content sources, but at least fore that's a worthwhile tradeoff to not be inundated with spam and to not feel dependent on someone else to moderate what I see.reply",
      "> Ultimately, big problems with an open social web include:These two seem like the same problem:> moderation> spamYou need some way of distinguishing high quality from low quality posts. But we kind of already have that. Make likes public (what else are they even for?). Then show people posts from the people they follow or that the people they follow liked. Have a dislike button so that if you follow someone but always dislike the things they like, your client learns you don't want to see the things they like.Now you don't see trash unless you follow people who like trash, and then whose fault is that?> which now includes scrapers bringing your site to a crawlThis is a completely independent problem from spam. It's also something decentralized networks are actually good at. If more devices are requesting some data then there are more sources of it. Let the bots get the data from each other. Track share ratios so high traffic nodes with bad ratios get banned for leeching and it's cheaper for them to get a cloud node somewhere with cheap bandwidth and actually upload than to buy residential proxies to fight bans.> good faith verification> posting transparencyIt's not clear what these are but they sound like kind of the same thing again and in particular they sound like elements in the authoritarian censorship toolbox which you don't actually need or want once you start showing people the posts they actually want to see instead of a bunch of spam from anons that nobody they follow likes.reply",
      ">You need some way of distinguishing high quality from low quality posts.Yes. But I see curation more as a 2nd order problems to solve once the bases are taken care of. Moderation focuses on addressing the low quality, while curation makes sure tye high quality posts receive focus.The tools needed for curation, stuff like filtering, finding similar posts/comments, popularity, following, are different from those needed to moderate, or self moderate (ignore, down voting, reporting). The latter poisons a site before it can really start to curate to its users.>This is a completely independent problem from spam.Yeah, thinking more about it, it probably is a distinct category. It simply has a similar result of making a site unable to function.>It's not clear what these are but they sound like kind of the same thing againI can clarify. In short, posting transparency focused more on the user and good faith verification focuses more on the content. (I'm also horrible with naming, so I welcome better terms to describe these)- Posting transparency at this point has one big goal: ensure you know when a human or a bot is posting. But it extends to ensuring there's no impersonation, that there's no abuse of alt accounts, and no voting manipulation.It can even extend in some domains to making sure e.g. That a person who says they worked at Google actually worked at Google. But this is definitely a step that can overstep privacies.- good faith verification refers more towards a duty to properly vet and fact check information that is posted. It may include addressing misinformation and hate, or removing non-transparent intimate advice like legal/medical claims without sources or proper licensing. It essentially boils down to making ensuring that \"bad but popular\" advice doesn't proliferate, as it it ought to do.>they sound like elements in the authoritarian censorship toolbox which you don't actually need or want once you start showing people the posts they actually want to seeYes, they are. I think we've seen enough examples of how dangerous \"showing people what they actually want to see\" can be if left unchecked. And the incentives to keep them up are equally dangerous in an ad-driven platform. Being able to address that naturally requires some more authorian approaches.That's why \"good faith\" is an important factor here. Any authoritarian act you introduce can only work on trust, and is easily broken by abuse. If we want incentives to change from \"maximizing engagement\" to \"maximizing quality and community\", we need to cull out malicious information.We already give some authoritarianism by having moderators we trust to remove spam and illegal content, so I don't see it as a giant overstep to make sure they can also do this.reply",
      "A lot of tech folks hate government ID schemes, but I think MDL with some sort of pairwise pseudonyms could help with spam and verification.It would let you identify users uniquely, but without revealing too much sensitive information. It would let you verify things like \"This user has a Michigan driver's license, and they have an ID 1234, which is unique to my system and not linkable to any other place they use that ID.\"If you ban that user, they wouldn't be able to use that ID again with you.The alternative is that we continue to let unelected private operators like Cloudflare \"solve\" this problem.reply",
      "Telegram added a feature where if someone cold dms you, it shows their phone number country and account age. When I see a 2 month old account with a Nigeria phone number I know it's a bot and I can ignore it.reply",
      "The EU\u2019s eIDAS 2.0 specification for their digital wallet identity explicitly supports the use of pseudonyms for this exact purpose of \u201cAnonymous authentication\u201d.reply",
      "Having worked on the problem for years, decentralized social networking is such as tar pit of privacy and security and social problems that I can't find myself excited by it anymore. We are clear what the problems with mainstream social networking at scale are now, and decentralization only seems to make them worse and more intractable.I've also come to the conclusion that a tightly designed subscription service is the way to go. Cheap really can be better than \"free\" if done right.reply",
      "Yeah kind of agree. Decentralised protocols are forced to expose a lot of data which can normally be kept private like users own likes.reply",
      "Dunno necessarily if they are _forced_ to expose that data.Something like OAuth means that you can give different levels of private data to different actors, based on what perms they request.Then you just have whoever is holding your data anyway (it's gotta live somewhere) also handle the OAuth keys. That's how the Bluesky PDS system works, basically.Now, there is an issue with blanket requesting/granting of perms (which an end user isn't necessarily going to know about), but IMO all that's missing from the Bluesky-style system is to have a way to reject individual OAuth grants (for example, making it so Bluesky doesn't have access to reading my likes, but it does have access to writing to my likes).reply"
    ],
    "link": "https://werd.io/why-the-open-social-web-matters-now/",
    "first_paragraph": "The needs are real \u2013 and you have so much power.I was privileged to deliver the opening keynote at this month's FediForum, a conference for people building and supporting the open social web. My talk touched on what's happening now, drew on my experiences building Elgg and Known and investing at Matter Ventures, and gave participants three important questions to ask themselves as they build platforms and serve communities.Here's the talk in its entirety, courtesy of FediForum. The transcript follows below.If we haven\u2019t met, my name\u2019s Ben Werdmuller. I\u2019m based just outside of Philadelphia. Today, I\u2019m the Senior Director of Technology at ProPublica, a non-profit newsroom here in the US that investigates abuses of power and betrayals of the public trust by government, business, and other institutions. If you remember the Clarence Thomas corruption scandal, Project 2025 co-author Russell Vought\u2019s private remarks about how he was going to go after the left, or Peter Thiel\u2019s tax-free investm"
  },
  {
    "title": "Adding Breadcrumbs to a Rails Application (avohq.io)",
    "points": 30,
    "submitter": "flow-flow",
    "submit_time": "2025-10-14T09:54:46 1760435686",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://avohq.io/blog/breadcrumbs-rails",
    "first_paragraph": "By Exequiel RozasHelping users navigate through our site with ease helps them reach their desired destination thus improving their experience within our application.Breadcrumbs play a crucial part in this: they give users a clear idea of where they are and provide them a path to reach In this article, we will learn how to add breadcrumbs to a Rails app using the different types of breadcrumbs and way to add them in Rails applications.Let's start by seeing why breadcrumbs are important. If you're already familiar with the jump to the adding breadcrumbs to Rails apps sectionThe main reason to add breadcrumbs to any application is to improve user experience: they give a clear indication of where the user is on the page and the navigation paths that can be taken from there on.They can help users reach a desired outcome faster without the need to know anything about our site beforehand while lowering the bounce rate at the same time.Besides this user experience improvement, breadcrumbs show"
  },
  {
    "title": "Who invented deep residual learning? (idsia.ch)",
    "points": 74,
    "submitter": "timlod",
    "submit_time": "2025-10-13T11:07:25 1760353645",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=45567050",
    "comments": [
      "Of all Schmidhuber's credit-attribution grievances, this is the one I am most sympathetic to.  I think if he spent less time remarking on how other people didn't actually invent things (e.g. Hinton and backprop, LeCun and CNNs, etc.) or making tenuous arguments about how modern techniques are really just instances of some idea he briefly explored decades ago (GANs, attention), and instead just focused on how this single line of research (namely, gradient flow and training dynamics in deep neural networks) laid the foundation for modern deep learning, he'd have a much better reputation and probably a Turing award.  That said, I do respect the extent to which he continues his credit-attribution crusade even to his own reputational detriment.reply",
      "I think one of the best things to learn from Schmidhuber is that progress involves a lot of players and over a lot of time. Attribution is actually a difficult game and usually we are only assigning credit to those at the end of some milestone. It's like giving a gold medal to the runner in the last leg of a relay race or focusing only on the lead singer of a band. It's never one person that does it alone. Shoulders of giants, but those giants are just a couple of dudes in a really big trenchcoat.Another important lesson is that often good ideas get passed over because of hype or politics. We often like to pretend that science is all about the merit and what is correct. Unfortunately this isn't true. It is that way in the long run, but in the short run there's a lot of politics and humans still get in their own way. This is a solvable problem, but we need to acknowledge it and create systematic changes. Unfortunately a lot of that is coupled to the aforementioned one.  > I do respect the extent to which he continues his credit-attribution crusade even to his own reputational detriment.\n\n\nAs should we all. Clearly he was upset that others got credit for his contributions. But what I do appreciate is that he has recognized that it is a problem bigger than him, and is trying to combat the problem at large and not just his own little battlefield. That's respectable.reply",
      "It's a bit of an aside but I believe this is one reason Zuckerberg's vision for establishing the superintelligence lab is misguided. Including VCs, too many people get distracted by rock stars in this gold rush.reply",
      "Just last week I said something inline with that[0]. Many people conflated my claim that Meta has a lot of good people with \"Meta /is/ winning the AI race\". I just claimed they had some of who I think are some of the best researchers in the field, but do not give them nearly the same resources or capacity to further their research that they give to these \"rock stars\". Tbh, the same is true for any top lab, I just think this happens more at Meta because Meta is so metric and rock star focused.So I agree. The vision is misguided. I think they'd have done better had they taken that same money and just thrown it at the people they already have but who are working in different research areas. Everyone is trying to win my doing the same things. That's not a smart strategy. You got all that money, you gotta take risks. It's all the money dumped into research that got us to this point in the first place.It's good to shift funds around and focus on what is working now, but you also have to have a pipeline of people working on what will work tomorrow, next year, 5 years, and 10 years. The people are there that can do that work. The people are there that want to do the work. The only thing is there's little to no people that want to fund that work. Unfortunately it takes time to bake a cake.Quite frankly, these companies also have more than enough money to do both. They have enough money to throw cash hand over fist at every wild and crazy idea. But they get caught in the hype, which is no different than an over focus on the attribution rather than the process or pipeline that got us the science in the first place.[0] https://news.ycombinator.com/item?id=45554147reply",
      ">  Note again that a residual connection is not just an arbitrary shortcut connection or skip connection (e.g., 1988)[LA88][SEG1-3] from one layer to another! No, its weight must be 1.0, like in the 1997 LSTM, or in the 1999 initialized LSTM, or the initialized Highway Net, or the ResNet. If the weight had some other arbitrary real value far from 1.0, then the vanishing/exploding gradient problem[VAN1] would raise its ugly head, unless it was under control by an initially open gate that learns when to keep or temporarily remove the connection's residual property, like in the 1999 initialized LSTM, or the initialized Highway Net.After reading Lang & Witbrock 1988 https://gwern.net/doc/ai/nn/fully-connected/1988-lang.pdf I'm not sure how convincing I find this explanation.reply",
      "For residual networks with an infinite number of layers it is absolutely correct. For a residual network with finite layers, you can get away with any non zero constant weight as long as the weight chosen appropriately for the fixed network depth. The problem is simply c^n gives you very big or very small numbers for large n and large deviations from 1.Now let me address the other possibility that you are talking about: what if residual connections aren't necessary? What if there is another way? What are the criteria necessary to avoid exploding or vanishing gradient or slow learning in the absence of both?For that we need to first know why residual connections work. There is no way around calculating the back propagation formula by hand, but there is an easy trick to make it simple. We don't care about the number of parameters in the network, we only care about the flow of the gradient. So just have a single input and output with hidden size 1 and two hidden layers.Each layer has a bias and a single weight and an activation function.Let's assume you initialize each weight and bias with zero. The forward pass returns zero for any input and the gradient is zero. In this artificial scenario the gradient starts vanished and stays vanished. The reason is pretty obvious when you apply back propagation. The second layer clips the gradient of the first layer. If there was a single layer, the gradient would be non zero and yield a non zero gradient, rescuing the network out of the vanishing gradient.Now what if you add residual connections? The forward pass stays the same, but the backward pass changes for two layers and beyond. The gradient for the second layer consists of just the second layer activation function multiplied by the first layer activation of the forward pass. The first layer gradient consists of the second layer gradient where the first layer activation is substituted by the gradient of the first layer but because it is a residual net, you also add the gradient of just the first layer.In other words, the first layer is trained independently of the layers that come after it, but also gets feedback from higher layers on top. This allows it to become non zero, which then lets the second layer become non zero, which lets the third be non zero and so on.Since the degenerate case of a zero initialized network makes things easy to conceptualise, it should help you figure out what other ways there are to accomplish the same task.For example, what if we apply the loss to every layer's output as a regularizer? That is essentially doing the same thing as a residual, but with skip connections that sum up the outputs. You could replace the sum with a weighted sum where the weights are not equal to 1.0.But what if you don't want skip connections either, because they are too similar to residual networks? A residual network has one skip connection already and summing up in a different way is uninteresting. It is also too reliant on each layer being encouraged to produce an output that is matched against the label.In other words, what if we wanted to let the inner layers not be subject to any correlation with the output data? You would need something that forces the gradients away from zero but also away from excessively high numbers. I.e. weight regularization or layer normalisation with a fixed non zero bias.Predictive coding and especially batched predictive coding could also be a solution to this.Predictive coding predicts the input of the next layer, so the only requirement is that the forward pass produces a non zero output. There is no requirement for the gradient to flow through the entire network.reply",
      "My point is more that Schmidhuber is saying that the gates or the initialization are the innovation solely because they produce well-behaved gradients, which is why Hochreiter's 1991 thing is where he starts and nothing before that counts. But it's not clear to me why we should define it like that when you can solve the gradient misbehavior other ways, which is why https://gwern.net/doc/ai/nn/fully-connected/1988-lang.pdf#pa... works and doesn't diverge: if I'm understanding them right, they did warmup, so the gradients don't explode or vanish. So why doesn't that count? They have shortcut layers and a solution to exploding/vanishing gradients and it works to solve their problem. Is it literally 'well, you didn't use a gate neuron or fancy initialization to train your shortcuts stably, therefore it doesn't count'? Such an argument seems carefully tailored to exclude all prior work...reply",
      "That's a cool paper.  Super interesting to see how work was progressing at the time, when Convex was the machine everybody wanted on (or rather next to) their desks.reply",
      "I spent some time in the academia.The person with whom an idea ends up associated often isn't the first person to have the idea. Most often is the person who explains why the idea is important, or find a killer application for the idea, or otherwise popularizes the idea.That said, you can open what Schmidhuber would say is the paper which invented residual NNs. Try and see if you notice anything about the paper that perhaps would hinder the adoption of its ideas [1].[1] https://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdv...reply",
      "I think what you're referring to is also known as Stigler's law of eponymy [1], which is interestingly self-referential and ironic in its own naming. There's also the related \"Matthew effect\" [2] in the sciences.[1] https://en.wikipedia.org/wiki/Stigler's_law_of_eponymy[2] https://en.wikipedia.org/wiki/Matthew_effectreply"
    ],
    "link": "https://people.idsia.ch/~juergen/who-invented-residual-neural-networks.html",
    "first_paragraph": "\nJ\u00fcrgen Schmidhuber (September 2025)\nPronounce: You_again Shmidhoobuh\nTechnical Report IDSIA-09-25, IDSIA\r  AI Blog @SchmidhuberAI\narXiv:2509.24732\n\n\n\nWho invented deep residual learning?\n\n\n\n Modern AI is based on deep artificial neural networks (NNs).[DLH]\nAs of 2025, the most cited scientific article of the 21st century is an NN paper on deep residual learning  \nwith  residual connections.[MOST25,25b] Who invented this? Here is the timeline of the evolution of deep residual learning:\n\n\n\u2605 1991: recurrent residual connections (weight 1.0) solve the vanishing gradient problem\n\u2605 1997 LSTM: plain recurrent residual connections (weight 1.0)\n\u2605 1999 LSTM: gated recurrent residual  connections (gates initially open: 1.0)\n\u2605 2005: unfolding LSTM\u2014from recurrent to feedforward residual NNs\n\u2605 May 2015: very deep Highway Net\u2014gated feedforward residual connections (initially 1.0)\n\u2605 Dec 2015: ResNet\u2014like an open-gated Highway Net (or an unfolded 1997 LSTM)\n\n\n\n1991: recurrent residual connections solv"
  },
  {
    "title": "Secret diplomatic message deciphered after 350 years (nationalarchives.gov.uk)",
    "points": 96,
    "submitter": "robin_reala",
    "submit_time": "2025-10-16T15:56:05 1760630165",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=45606986",
    "comments": [
      "I sat through a briefing last week about quantum encryption and the threat that quantum computing poses to encryption in use today. It was stressed that nation states are hoovering up encrypted data now in order to decrypt later with quantum computing. Much the same way America decrypted old soviet encrypted data. I wonder if it will take as long and if anyone will still be alive to make use of that data.reply",
      "If quantum computing would progress just like in the last 30 years it may take 300 years before it can be useful.https://eprint.iacr.org/2025/1237.pdfreply",
      "As has been previously pointed out, the 2001 and 2012 quantum factorisation records may be easily matched with a dog trained to bark three times [33]. We verified this by taking a recently-calibrated reference dog, Scribble, depicted in Figure 6, and having him bark three times, thus simultaneously factorising both 15 and 21. This process wasn\u2019t as simple as it first appeared because Scribble is very well behaved and almost never barks. Having him perform the quantum factorisation required having his owner play with him with a ball in order to encourage him to bark. It was a special performance just for this publication, because he understands the importance of evidence-based science.\n\nI look forward to more dog-based science.reply",
      "If you know a better way to factor 35, I\u2019d like to hear it.reply",
      "Thanks for sharing this, great read.reply",
      "It's fascinating to me that the keywords were further encoded such that even if the message was deciphered, the strategic plans could not be acted upon.reply",
      "It was quite common to have a codebook that might list several numbers or words used to substitute for places, individuals, actions, etc.These also existed for corporate entities. A concern might have their own codebook such that the telegraph office would not be privy to their internal business.They would also use codebooks as a type of compression, since the telegraph company charged less for sending English words as opposed to enciphered characters, and obviously, there are many uncommon words that could substitute for longer common phrases.https://en.wikipedia.org/wiki/Codebookhttps://en.wikipedia.org/wiki/Commercial_code_(communication...reply",
      "So many machninations amongst the elites even hundreds of years ago. And imagine today, with the current set of goons in power. It's amazing they've survived for so long.reply",
      ".... Satoshi?reply",
      "Different Satoshi.reply"
    ],
    "link": "https://www.nationalarchives.gov.uk/explore-the-collection/the-collection-blog/secret-diplomatic-message-deciphered-after-350-years/",
    "first_paragraph": "We use some essential cookies to make this service work.We'd also like to use analytics cookies so we can understand how you use the service and make improvements.You have accepted optional cookies. You can change your cookie settings on the cookies page.You have rejected optional cookies. You can change your cookie settings on the cookies page.\n              This is a new service. Help us improve it and give your feedback (opens in new tab).\n            The collectionRuth Selman shares an exciting update to her previous blog post about a 17th-century letter written in cipher.\nPublished 14 October 2025\n                by\nRuth Selman \nI\u2019m excited to report that the challenge set in my blog post of 4 August 2025 has been met. The letter sent by William Perwich on 9 April 1670 from the court of Louis XIV in France has been successfully decrypted, not once but twice.The collectionRead Ruth's first blog post about the mysterious letter.On a murky Monday in late September, I started work to "
  },
  {
    "title": "K8s with 1M nodes (bchess.github.io)",
    "points": 106,
    "submitter": "denysvitali",
    "submit_time": "2025-10-16T22:04:57 1760652297",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=45611252",
    "comments": [
      "I feel like etcd is one of the few use cases where Intel Optane would actually make sense. I build and run several bare metal clusters with over 10k nodes and etcd is by and large the biggest pain for us. Sometimes an etcd node just randomly stops accepting any proposals which halts the entire cluster until you can remove the bad etcd node.From what I remember, GKE has implemented an etcd shim on top of spanner as a way to get around the scalability issues, but unfortunately for the rest of us who do not have spanner there aren\u2019t any great options.I feel like at a fundamental level that pod affinity, antiaffinity, and topology spreads are not compatible with very large clusters due to the complexity explosion in large clusters.Another thing to consider is that the larger a cluster becomes, the larger the blast radius is. I have had clusters of 10k nodes spectacularly fail due to code bugs within k8s. Sharding total compute capacity compute capacity into multiple isolated k8s clusters reduces the likelihood that a software bug is going to take down everything as you can carefully upgrade only a single cell at a time with bake periods between each cell.reply",
      "This is an awesome experiment and write up. I really appreciate the reproducibility.I would like to see how moving to database that scales write throughput with replicas would behave, namely FoundationDB. I think this will require more than an intermediary like kine to be efficient, as the author illustrates the apisever does a fair bit of its own watching and keeping state. I also think there's benefit, at least for blast radius, to shard the server by api group or namespace.I think years ago this would have been a non starter with the community, but given AWS has replaced etcd (or at least aspects) with their internal log service for their large cluster offering, I bet there's some appetite for making this interchangable and bringing and open source solution to market.I share the authors viewpoint that for modern cloud based deployments, you're probably best avoiding it and relying on VMs being stable and recoverable. I think reliability does matter if you want to actually realize the \"borg\" value and run it on bare metal across a serious fleet. I haven't found the business justification to work on that though!reply",
      "If you don't need the isolation of of k8s then don't forget about erlang, which is another option to scale up to 1 million functions. Obviously k8s containers (which are fundamentally just isolated processes) and erlang processes are not interchangeable things, but when thinking about needing in the order of millions of processes erlang is pretty good prior artreply",
      "This is 1m nodes, you typically run tens or hundreds of pods per node, each with one or more containers. So more like 100m+ functions if I follow the Erlang analogy correctly?reply",
      "This is not analogous. It\u2019s just someone beating the Erlang drum. You can\u2019t PyTorch in Erlang.reply",
      "Kubernetes is way heavier than Erlang\u2019s lightweight processes, so for millions of tasks at scale, a middle-ground solution could blend Erlang\u2019s concurrency efficiency with k8s\u2019 orchestration power, dodging containers\u2019 overhead while keeping flexibility for diverse workloads. That's if you don't actually need the strict isolation of pods/containers and you're just trying to run something at massive scale. I don't get why so many people want to run everything as heavy container processes or pods vs coming up with a better solution. The point is we don't have to fit every problem into the shoe called kubernetes if it doesn't seem to fit, and we should look at other ways to spin up millions of processesreply",
      "without publishing mem_etcd code, and without telling us what happens when one of the etcd/mem_etcd node dies to compare, this write up doesn't provide much information.reply",
      "I believe the code is here:https://github.com/bchess/k8s-1m/tree/main/mem_etcdhttps://github.com/bchess/k8s-1m/blob/main/RUNNING.adoc#mem_...reply",
      "I don't get the point of benchmarking k8s without the guarantees of etcd.\nAt some point, you are just competing with clusterssh.reply",
      "Typical large scale high performance computing clusters are at a size of 10k nodes (for instance Jupiter and SuperMUC in Germany) [1]. These centers are quite remarkably big buildings. I wonder how much 1M node single k8s clusters there are in the world right now. Most likely at the hyperscalers.[1] what is a node? Typically it is a synonym for \"server\". In some configurations HPC schedulers allow node sharing. Then we talk about order of 100k cores to be scheduled.reply"
    ],
    "link": "https://bchess.github.io/k8s-1m/",
    "first_paragraph": "This is an effort to create a fully functional Kubernetes cluster with 1 million active nodes.Several years ago at OpenAI I helped author Scaling Kubernetes to 7500 Nodes which remains one of the CNCF\u2019s most popular blog posts. Alibaba made a post about running Kubernetes clusters with 10K nodes. Google made a post about 15K nodes with Bayer Crop Science. Fast forward to today, GKE supports running some clusters up to 65K nodes, and AWS recently announced support for clusters up to 100K nodes.In online forums and in my own conversations with peers, I\u2019ve encountered a lot of debate about how big a Kubernetes cluster can get.  What tends to be lacking from these discussions is hard data and evidence-backed justifications. I\u2019ve worked with engineers reluctant to push things beyond what they\u2019ve seen before because they\u2019re fearful or uncertain of what may go wrong. Or when something does go wrong, the response is to scale down the cluster rather than understand and address the bottleneck.Th"
  },
  {
    "title": "Solution to CIA\u2019s Kryptos sculpture is found in Smithsonian vault (nytimes.com)",
    "points": 116,
    "submitter": "elahieh",
    "submit_time": "2025-10-16T10:56:50 1760612210",
    "num_comments": 51,
    "comments_url": "https://news.ycombinator.com/item?id=45603876",
    "comments": [
      "So the central controversy in the story is whether the journalist fans should share the solution with the world or keep quiet for the auction.Sanborn wants the money for medical reasons so he needs to maintain a high sale price.The two fans want to share the solution with the world.Presumably the winner of the auction will be buying a severely depreciating asset: the right to know but not disclose the solution. There are at least four people who have the solution and as soon as one of them shares it, its value goes to zero.Maybe the \u201csolution\u201d to this meta problem is simple: auction it off to the public with a go fund me. As soon as it reaches $500k, publish the solution. That way everyone wins.The whole thing got more complicated with the addition of lawyers, not less. I don\u2019t see how the two fans violated any contracts with the artist or auction house since they never signed one. But of course lawyers will charge a ton for you to find out.reply",
      "Feels like the central controversy is that Sanborn has to auction anything off for medical reasons.reply",
      "Make the auction include the physical piece of art itself.  Then you're buying a tangible and transferable asset.  I think the CIA has enough money it can endeavor to replace it.  What value does a cracked puzzle even have to them?reply",
      "Honestly, everything about this is really sad. I, like may of you guys, have followed this thing for years, for many others, decades.Actual decryption effort group didn't get to decrypt (a small but faithful community), the creator needed the money for medical procedures that he really believed was coming in. The solution feels like we all go cheated out of something. Lawyers are now involved and the value of the solution is rapidly plummeting.No one's winning because of a small mistake.reply",
      "Do I understand part of the complexity of the situation is that Kryptos is in some sense \"crackable\" (unlike real cryptography), and these two people sleuthed their way to the answer book without solving it? Which is not quite exactly the same thing as them independently working out a solution; it's more like a nicer and more legal version of breaking into the guy's house and stealing it out of his desk drawer?reply",
      "Can we even determine if what they found is the key, or just the plaintext? The article mentions they recognized bits of plaintext (Berlin clock) in the archives.reply",
      "\"What we think intelligence agencies do\" vs \"what intelligence agencies actually do\"reply",
      "At the state level, it's a method that is in bounds.reply",
      "Yes, but all things considered, that's outside the bounds of this cypher - which is why \"we all\" feel cheated.reply",
      "I don't have an opinion! As a cryptography pentester, Kryptos has always kind of set my teeth on edge (Wikipedia had editors covering cryptography topics whose expertise was rooted in Kryptos puzzle-crypto). But one of the smartest people I know is also a Kryptos enthusiast so this is all very complicated for me.reply"
    ],
    "link": "https://www.nytimes.com/2025/10/16/science/kryptos-cia-solution-sanborn-auction.html",
    "first_paragraph": ""
  },
  {
    "title": "When you opened a screen shot of a video in Paint, the video was playing in it (microsoft.com)",
    "points": 153,
    "submitter": "birdculture",
    "submit_time": "2025-10-16T19:57:41 1760644661",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=45609986",
    "comments": [
      "Fun Fact: This same sort of thing also happened on the Classic Macintosh Quadra 840AV, when running in 8-bit (256 color) mode.  Playback of realtime video capture reserved color index #243 (a very dark green in the system palette), and ANYWHERE that color was used, it would be replaced with the live video.  I created some cool effects using this back in the 90s.reply",
      "This was a nice trick to protect text from copying. For instance, student assignments. Students could still use digital camera on CRT display, but 20 years ago cameras were costly and students did not have them. And typing text from scratch was a tedious job. So online served assignments were not shared too fast.reply",
      "> And typing text from scratch was a tedious jobAt least by typing them the typer might learning something. :)reply",
      "\"Nowadays, video rendering is no longer done with overlays.\"Darn, I thought this explained why, after upgrading my GPU, videos playing in Chrome have a thin green stripe on their right edge.reply",
      "This is simply wrong. Videos are still rendering in overlays in windows in Chrome right now. There are many reasons why overlays are still used.reply",
      "Video rendering can still be done with overlays, but it's a little more substantial, involving separate planes with the locations configurable on the graphics card. Look up MPO, Multi-Plane Overlay.Your green stripe is likely because of the classic combination of unclamped bilinear filtering and a texture that's larger than the output region being used as the drawing surface for the video.reply",
      "I did some Googling on your behalf as I remember having something like that but can't reproduce it right now:https://old.reddit.com/r/OLED_Gaming/comments/1kovgdx/green_...I'd make sure your drivers are up to date before fiddling with Chrome flags though.reply",
      "If you watch Twitch, you can see that all instances of the same emote in chat animate together. Then I tested this more generally in a web page, and the same thing happens - if the same gif is placed multiple times in a page, all instances of that gif will play in sync even if loaded at different times. I guess there's a similar idea in browsers then, where maybe there's only one memory representation of the gif across the page or the browser.reply",
      "I made extensive use of this, when I found it by accident, in my Winamp skins and GUI programs!reply",
      "Iirc you can also set that \"green screen\" as wallpaper and have video as desktop background!reply"
    ],
    "link": "https://devblogs.microsoft.com/oldnewthing/20251014-00/?p=111681",
    "first_paragraph": "@ChenCravat In an old version of Windows (Windows 98 iirc) if you took screenshot of a video from media player and paste it into paint, and resume media player, video would play inside paint. Do you why it happened? It is still bugging me to this day.\u2014 Yasar Arabaci @ysar.bsky.social (@y_arabaci) July 18, 2025One of the tricks for video playback is to use a green screen, more technically known as color-keying or chroma-keying.The media player program didn\u2019t render the video pixels to the screen. Rather, it followed this recipe:There are a few advantages to this approach.One is that the shared graphics surface need not have the same pixel format as the user\u2019s main display. Therefore, you can specify that the shared graphics surface have a pixel format that matches that of the video, avoiding the need to do any pixel format conversions.Another is that you can update the content without having to go through a full paint cycle. You just update the shared graphics surface, and the results a"
  },
  {
    "title": "Moonlander.BAS (basic-code.bearblog.dev)",
    "points": 31,
    "submitter": "ibobev",
    "submit_time": "2025-10-12T20:05:01 1760299501",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=45561382",
    "comments": [
      "If there is anything that really gives me the nostalgia vibes, it's basic listings. So many good memories typing over listings and changing them to enhance or cheat. It was very educational as a child. Mags like this [0] fill me with joy even though it makes no more sense. At least I got to live it fully (my nostalgia vibes go from 1980-1988 around; after that it was more study/work; I was 14 in 1988 and teaching the computer classes at my high school as the teachers didn't understand anything).[0] https://archive.org/details/msx-gids-nr.-08/mode/2upreply",
      "I used to load up tapes for 15' on my spectrum and then poke around trying to understand what was under the hood.All I had was a thick book in a language I didn't speak yet.No internet, no friends to bounce it with. Infinite time.Good times.reply",
      "I remember playing this when I was 8 years old! Much feels!reply",
      "So, neither Claude nor ChatGPT were able to write a little javascript program to provide an optimal solution for a suicide burn. Did anyone else have more luck?reply",
      "You can get more fuel by entering a negative Thrust value.reply"
    ],
    "link": "https://basic-code.bearblog.dev/moonlander/",
    "first_paragraph": "Home About RSSSupport SGDF\n\n\n                    11 Oct, 2025\n                \n\nClick the window to enter text.Each turn enter the amount of Thrust you want to apply to slow your descent. If you run out of fuel or fall too fast, you will crash. Try not to crash.Adapted from a listing in Tim Hartnell's Giant Book of Computer Games, refactored and adapted for the Basic Anywhere Machine.Stripped out all the line numbers and implemented labels.Added instructions; the listing lacked them.Original variable names were A, B, and C. I've renamed them. Also had to change the structure of the RND statements from (RND(number)) to RND(1)*number).Didn't change much here, though I could have made a better looking \"ship\" with CHR$ codes.Left the main loop basically untouched.Originally the program gave a several second delay with a For/Next loop before restarting. I replaced this with a SLEEP to let the player tap a key instead.\n#Tim Hartnell\n\u00a92025 Michael Coorlim - Support my work on Patreon - One-Ti"
  }
]