[
  {
    "title": "Meta Superintelligence's surprising first paper (paddedinputs.substack.com)",
    "points": 85,
    "submitter": "skadamat",
    "submit_time": "2025-10-11T23:16:05 1760224565",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=45553577",
    "comments": [
      "It's kinda funny, Meta has long had some of the best in the field, but left them untapped. I really think if they just took a step back and stop being so metric focused and let their people freely explore then they'd be winning the AI race. But with this new team, I feel like meta mostly hired the people who are really good at gaming the system. The people that care more about the money than the research.A bit of this is true at every major lab. There's tons of untapped potential. But these organizations are very risk adverse. I mean why not continue with the strategy that got us to the point we're at in the first place. Labs used to hire researchers and give them a lot of free reign. But those times ended and AI progress also slowed down. Maybe if you want to get ahead you gotta stop thinking like everyone elseWell meta... you can \"hold me hostage\" for a lot cheaper than those guys. I'm sure this is true for hundreds of passionate ML researchers. I'd take a huge pay cut to have autonomy and resources. I know for a fact there's many working at Mets right now that would do the same. Do maybe if you're going to throw money at the problem, diversify a bit and look back at what made SV what it is today and what made AI take leaps forwardreply",
      "This has nothing to do with superintelligence, it's just the people that were working on the paper prior to the re-org happened to publish after the name change.Though it is notable that contrary to many (on HN and Twitter) that Meta would stop publishing papers and be like other AI labs (e.g. OpenAI). They're continued their rapid pace of releasing papers AND open source models.reply",
      "https://docs.lamini.ai/memory_rag/\nSimilar approaches have been tried before alreadyreply",
      "I'm curious whether this is work that was specifically begun under the \"superintelligence\" umbrella, or if it's just that the people who were working on it had been shifted to the Superintelligence team by the time they wrote the paper. I would guess the former?reply",
      "This was a very obvious next step, I played around with implementing something similar at one point.In general we need to make it simpler for LLMs to take in different forms of embeddings. At least frameworks that simplify it.reply",
      "At first I thought the super intelligence wrote a novel scientific paperreply",
      "Interesting. All developers I know who tinkered around with embeddings and vector similarity scoring were instantly hooked. The efficiency of computing the embeddings once and then reusing as many times as needed, comparing the vectors with a cheap <30-line function is extremely appealing. Not to mention the indexing capabilities to make it work at scale.IMO vector embedding is the most important innovation in computing of the last decade. There's something magical about it. These people deserve some kind of prize. The idea that you can reduce almost any intricate concept including whole paragraphs to a fixed-size vector which encapsulates its meaning and proximity to other concepts across a large number of dimensions is pure genius.reply",
      "Vector embedding is not an invention of the last decade. Featurization in ML goes back to the 60s - even deep learning-based featurization is decades old at a minimum. Like everything else in ML this became much more useful with data and compute scalereply",
      "Yup, when I was at MSFT 20 years ago they were already productizing vector embedding of documents and queries (LSI).reply",
      "If you take the embedding for king, subtract the embedding for male, add the embedding for female, and lookup the closest embedding you get queen.The fact that dot product addition can encode the concept of royalty and gender (among all other sorts) is kind of magic to me.reply"
    ],
    "link": "https://paddedinputs.substack.com/p/meta-superintelligences-surprising",
    "first_paragraph": ""
  },
  {
    "title": "The <output> Tag (denodell.com)",
    "points": 708,
    "submitter": "todsacerdoti",
    "submit_time": "2025-10-11T08:27:26 1760171246",
    "num_comments": 159,
    "comments_url": "https://news.ycombinator.com/item?id=45547566",
    "comments": [
      "Problem with <output> is that it is half-baked making its usage almost useless.It would be significantly more practical for the output to have \"type\" attribute in the same way as in the input.I did experiment with oputput|type in my Sciter and added these:   type=\"text\" - default value, no formating\n   type=\"number\" - formats content as a number using users locale settings,\n   type=\"currency\" - formats content as a currency using users locale settings,\n   type=\"date\"  - as a date, no TZ conversion, \n   type=\"date-local\" - as a date in users format, UTC datetime to local,\n   type=\"time\" - as a time\n   type=\"time-local\" - as a local time, value treated as UTC datetime. \n\nThis way server can provide data without need to know users locale.reply",
      "From the article: and spec:> The output element represents the result of a calculation performed by the application, or the result of a user action.<output> is for changing content. It's the ARIA semantics that matter. The content gets announced after page updates.You can put whatever you want inside the <output> to represent the type. \"text\" is the default. You can represent dates and times with the <time> element. And while there is currently no specific number formatting element, since Intl has arrived there have been many requests for this.For example:    <output>The new date is <time datetime=\"2025-10-11\">Oct 11</time></output>\n\nIOW, <output> should not have to handle all these types when it handles HTML and HTML needs to represent the types anyway.reply",
      "> half-baked making its usage almost useless.It's sad how many elements this is still the case for in 2025. A good chunk of them can be blamed on Safari.Probably the most extreme example of this is <input type=\"date\"> which is supposedly production-ready but still has so many browser quirks that it's almost always better to use a JS date picker, which feels icky.reply",
      "Omg yes, I thought I was crazy when I was pushing for native input type=date instead of JS date picker, it worked perfectly with minimal configuration on my phone and on my Mac, but then my coworkers said it didn't work for them on their browsers, turns out, yeah, it's not consistent.I then proceeded to spend the next week crying trying to get JS date picker to work as well as native did on my browsers.reply",
      "On all the projects I worked that involved ui elements library, datepicker consistently was the biggest pain in the ass, rivaled only by modals.reply",
      "Modals at least are a solved problem these days.reply",
      "Safari and Firefox together seem to always be dragging their feet on things. Sure, sometimes it's \"standards\" chrome is ramming through, but many times it's things like this, that have been around since before chromereply",
      "You are thinking about it wrong, output is not symmetrical to input to have a type, it's a container for content that updates while you're using the page.reply",
      "Top personal issue, be nice if it could just attach to an <input> and list the result.  Like:  <input type=\"range\" id=\"example_id\" name=\"example_nm\" min=\"0\" max=\"50\">\n  <output name=\"example_result\" for=\"example_id\"></output>\n\nAnd it would just show you the input value.  Maybe with a \"type\" specifier like talked about.  Maybe the ::before or ::after css and it would allow content: updates or something.Bunch of <input> types that there's a reasonable case for.  Especially if it allowed for formatting.  Did you put in the type=\"tel\" the way you believed?  It prints it out formatted.'checkbox, color, date, datetime-local, file, month, number, radio, range, tel, time, url, week' might all have possible uses.  Some of the text cases might have uses in specific conditions.  'email, text, url'Also be nice if the for=\"\" attribute actually did very much.  The attachment seems mostly irrelevant in the examples seen.  Most example just use a variation on:  <output name=\"result\">\n  <form oninput=\"result.value=...\">reply",
      "I'd prefer:    <output for=input>\n      <!-- bring your own time-locale component -->\n      <time is=time-locale datetime=2001-02-03>2001-02-03</time>\n    </output>\n\nWith the component replacing the value dependent on locale. I don't think having HTML/CSS fiddling around with making fake content is a great idea, it already causes issues with trying to copy things injected by CSS's :before/:after psudoelements, let alone having a difference between the DOM's .innerText and, well, the inner text.Not saying decisions can't be made about these things, just that, making those decisions will pretty much make a dedicated DSL out of a single element (dependent on input, desired kind of output (absolute or relative), other data sent along side (type of currency, does it need to be a \"real\" currency? Since instead of just calling something in mutable/overridable JS, its now part of the HTML processing, something that can't directly be touched)reply"
    ],
    "link": "https://denodell.com/blog/html-best-kept-secret-output-tag",
    "first_paragraph": " Den Odell 1 October 2025 \u00b7 \u23f1\ufe0f 5 min read\n Every developer knows <input>. It\u2019s the workhorse of the web.But <output>? Most have never touched it. Some don\u2019t even know it exists.That\u2019s a shame, because it solves something we\u2019ve been cobbling together with <div>s and ARIA for years: dynamic results that are announced to screen readers by default.It\u2019s been in the spec for years. Yet it\u2019s hiding in plain sight.Here\u2019s what the HTML5 spec says:The <output> element represents the result of a calculation performed by the application, or the result of a user action.It\u2019s mapped to role=\"status\" in the accessibility tree. In plain terms, it announces its value when it changes, as if it already had aria-live=\"polite\" aria-atomic=\"true\".In practice, that means updates do not interrupt the user. They are read shortly after, and the entire content is spoken rather than just the part that changed. You can override this behavior by setting your own ARIA properties if needed.Usage is straightforward:Tha"
  },
  {
    "title": "Vancouver Stock Exchange: Scam capital of the world (1989) [pdf] (scamcouver.wordpress.com)",
    "points": 15,
    "submitter": "thomassmith65",
    "submit_time": "2025-10-11T23:43:42 1760226222",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=45553783",
    "comments": [
      "Reminds me of small mining exploration companies in Australia's ASX. Most only exist to pay for the directors' lifestyles.reply",
      "Isn\u2019t there this whole weird economy of empty pink sheet companies that just act as lifestyle funding for people?reply",
      "This is new to me, it probably was a scam, but the part about  Investors lose 84% of their money some of the time and all of it 40% of the time\n\nis meaningless. This could easily be true for many venture investments, which is how this exchange is presented. High risk investment is not the same as a scam.reply"
    ],
    "link": "https://scamcouver.wordpress.com/wp-content/uploads/2012/04/scam-capital.pdf",
    "first_paragraph": ""
  },
  {
    "title": "LineageOS 23 (lineageos.org)",
    "points": 60,
    "submitter": "cdesai",
    "submit_time": "2025-10-11T23:53:17 1760226797",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=45553835",
    "comments": [
      "How do backups/restores work when using LineageOS and moving to a new phone?reply",
      "LineageOS is an open source android distribution. Can anyone comment on who might use LineageOS and why?reply",
      "I use LineageOS on all my devices (it's actually my main criteria when buying a phone) to mainly install apps from F-Droid without relying on the Google Play Store.It has the same familiar look and feel on all devices and by experience is way snappier than the original ROM.reply",
      "Got a Xperia Z1 in 2013. Sony stopped updating it at some point in 2014-2015, which is stupid, but the hardware was still like new (which is the great thing about Sony phones) so I rooted it and managed to install it. Can't remember if it was already named \"LineageOS\" or \"CyanogenMod\" at the time. However, it lasted with me until nov. 2020 when I dropped and the screen cracked, made it to be changed but the replacement was kinda bad so used it as an excuse to get a 1ii.I did the same with this \"new\" phone, that is going to be 5 years with me - since also got that only-two-years-of-updates thing, threw LineageOS on it and it's going as new.So as I said the last time I saw a post about it in here, thanks to LineageOS I can use a phone for way more than they are set out to be forgotten. It's a great project and it's really sad Google are making things harder for them for the sake of \"security\".reply",
      "I immediately put Lineage on all my devices. In fact, I only buy Android devices that Lineage supports. It's a uniform, degoogled Android experience that just works.reply",
      "Every version of Lineage has rooted ADB accessible in the developer options. If you want root for apps, you must load Magisk. If root is important to you, this is your OS.Lineage puts out all the patches that they can, every month, unlike OEMs. If current patches are important to you, this is your OS.Lineage allows you to run it without any Google closed source code.These are some serious advantages, depending upon what you are trying to do.reply",
      "I have a Samsung Tablet and Samsung's version for said tablet is a giant mountain of crap, full of bloatware, so I installed LineageOS on it. Also my old phone and my old old phone run LineageOS because I'm just logged in to Google on my {current_phone}.reply",
      "I ran LineageOS on my Moto X4 for many years. It was much faster without the OEM Moto and carrier apps, and was faster again when I installed it without Google Play Services. Same thing with an old Kindle Fire tablet, finally made it fast enough to practically use.reply",
      "If your phone is more than a few years old it likely doesn't get updates from the manufacturer anymore. LineageOS will get you to the latest Android with security patches. Same sort of deal as with OpenWRT for a router really, you get all the features and security patches but at the loss of the firmware that the device came with and its propriety enhancements.reply",
      "I haven't used custom roms in ages, but I used Lineage back when it was called Cyanogen. It had this cool thing where you could adjust brightness by swiping the top edge of the screen. (This was back in the day when you could reach that part easily!)reply"
    ],
    "link": "https://lineageos.org/Changelog-30/",
    "first_paragraph": "Hey there! Welcome back!This last year has been a whirlwind to say the least, but we have remained dedicated to bringing\nan updated LineageOS based on Android 16 to the masses!We\u2019ve been hard at work rebasing all of our changes and features since Android 16\u2019s release in June.\nAndroid 16\u2019s first release mainly contained iterative improvements and some UI/UX refinements, but\ndue to our previous efforts adapting to Google\u2019s UI-centric adjustments in Android 12 through 14,\nwe were able to rebase onto Android 16\u2019s code-base faster than anticipated. Yes you read that right:\nWe\u2019re early this year!Other components have complicated our release and security patching efforts, but we\u2019ll get to that\nshortly.You\u2019ll notice we are choosing to release LineageOS 23.0, and not 23.1. That\u2019s because it\u2019s based on\nAndroid 16\u2019s initial release (what we\u2019ll call QPR0), even though QPR1 has already rolled out to\nPixels. The catch? Google never pushed QPR1\u2019s source to AOSP. They\u2019ve said it\u2019ll come \u201cin the coming"
  },
  {
    "title": "My First Murder (texasmonthly.com)",
    "points": 18,
    "submitter": "speckx",
    "submit_time": "2025-10-06T13:06:44 1759756004",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=45490980",
    "comments": [
      "This is a teaser for his book where he presumably finishes the story. I will read it when my queue is shorter. It already has a nine week wait list at my local library.reply",
      "https://archive.is/dMzUJreply"
    ],
    "link": "https://www.texasmonthly.com/true-crime/skip-hollandsworth-new-book-she-kills/",
    "first_paragraph": ""
  },
  {
    "title": "Google blocks Android hack that let Pixel users enable VoLTE anywhere (androidauthority.com)",
    "points": 32,
    "submitter": "josephcsible",
    "submit_time": "2025-10-11T23:41:00 1760226060",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=45553764",
    "comments": [
      "> that let Pixel users enable VoLTE anywhereIt did a great deal more than that. It also allowed the toggling of VoNR, which apparently affected the fallback behavior of some people's services. (Ie. It would fall back to LTE and not roam back to 5G data unless nudged manually)However for me, it would enable backup calls over a secondary sim card's data, which would allow text and calls overseas without the usual extortionate charges. Oddly enough, I believe that toggle is enabled for my carrier... but only on iOS.reply",
      "I'm sure they had to do this based on carrier pressure, but it would be great if Google would just put more resources into getting carrier support/certification so their flagship devices will work more places.reply",
      "And... Sell in more countries as well.reply",
      "> Many carriers only permit VoLTE and VoWiFi on devices they sell or have officially tested.Does this happen even if you are using a carrier's SIM card; it's just because you didn't buy the hardware from them?It's not just an IMEI-level block so data still works?reply",
      "No, this is not really tied to whom you purchased the Pixel from. But it is tied to which carriers would sell you a Pixel at all. Meaning they have some sort of an agreement with Google and Google added configuration files whitelisting these features for the carrier in question.(At least for many EU based carriers.)reply",
      "Oh what a terrible vulnerability.. good to know it's patched, I feel much more secure now, thanks Google!reply"
    ],
    "link": "https://www.androidauthority.com/pixel-ims-broken-october-update-3606444/",
    "first_paragraph": ""
  },
  {
    "title": "Microsoft only lets you opt out of AI photo scanning 3x a year (slashdot.org)",
    "points": 384,
    "submitter": "dmitrygr",
    "submit_time": "2025-10-11T18:36:51 1760207811",
    "num_comments": 129,
    "comments_url": "https://news.ycombinator.com/item?id=45551504",
    "comments": [
      "\"You can only turn off this setting 3 times a year.\"Astonishing.  They clearly feel their users have no choice but to accept this onerous and ridiculous requirement.  As if users wouldn't understand that they'd have to go way out of their way to write the code which enforces this outcome.  All for a feature which provides me dubious benefit.  I know who the people in my photographs are.  Why is Microsoft so eager to also be able to know this?Privacy legislation is clearly lacking.  This type of action should bring the hammer down swiftly and soundly upon these gross and inappropriate corporate decision makers.  Microsoft has needed that hammer blow for quite some time now.  This should make that obvious.  I guess I'll hold my breath while I see how Congress responds.reply",
      "It's hilarious that they actually say that right on the settings screen. I wonder why they picked 3 instead of 2 or 4. Like, some product manager actually sat down and thought about just how ridiculous they could be and have it still be acceptable.reply",
      "My guess is it was an arbitrary guess and the limit is due to creating a mass scan of photos.  Depending on if they purge old data when turned off, it could mean toggling the switch tells microsoft's servers to re-scan every photo in your (possibly very large) library.Odd choice and poor optics (just limit the number of times you can enable and add a warning screen) but I wouldn't assume this was intentionally evil bad faith.reply",
      "Actually, most users probably don't understand, that this ridiculous policy is more effort to implement. They just blindly follow whatever MS prescribes and have long given up on making any sense of the digital world.reply",
      "Favebook introducing photo tagging was when I exited Facebook.This was pre-AI hype, perhaps 15 years ago.  It seems Microsoft feel it is normalised.  More you are their product.  It strikes me as great insecurity.reply",
      "> I know who the people in my photographs are. Why is Microsoft so eager to also be able to know this?Presumably it can be used for filtering as well - find me all pictures of me with my dad, etc.reply",
      "Sure but if it was for your benefit, not theirs, they wouldn't force it on you.reply",
      "Precisely.  The logic could just as easily be \"you can only turn this ON three times a year.\"  You should be able to turn it off as many times as you want and no hidden counter should prevent you from doing so.reply",
      "I agree with you but there's nothing astonishing about any of this unfortunately, it was bound to happen. Almost all of cautionary statements about AI abuse fall on deaf ears of  HN's overenthusiastic and ill-informed rabble, stultified by YC tech lobbyists.reply",
      "Worst part about it was all the people fretting on about ridiculous threats like the chatbot turning into skynet sucked the oxygen out of the room for the more realistic corporate threatsreply"
    ],
    "link": "https://hardware.slashdot.org/story/25/10/11/0238213/microsofts-onedrive-begins-testing-face-recognizing-ai-for-photos-for-some-preview-users",
    "first_paragraph": "\n\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\tWant to read Slashdot from your mobile device? Point it at m.slashdot.org and keep reading!\n\t\t\t\t\t\n\t\t\t\t\nNickname:\n\n\nPassword:\n\n\nNickname:\n\n\nPassword:\n\n\nThe Fine Print: The following comments are owned by whoever posted them.  We are not responsible for them in any way.\nLoving Microsoft yet?>\"Loving Microsoft yet?\"I don't use any of their products.  And there are many good reasons for that.What's the reason OneDrive tells users this setting can only be turned off 3 times a year?Because that's what their customers are demanding! Don't you hate when you're doing something, and you realize you've done it more than 3 times? Just yesterday I adjusted the mirror on my wife's SUV and thought \"we keep undoing each other's mirror adjustments. Can't it just stop moving so that one of us permanently loses and one permanently wins? Why is this car letting us change it back'n'forth?\"\nMicrosoft fights for the users!My thoughts exactly: I have nothing to do with Microsoft, its produ"
  },
  {
    "title": "Is Odin Just a More Boring C? (dayvster.com)",
    "points": 8,
    "submitter": "birdculture",
    "submit_time": "2025-10-11T19:33:27 1760211207",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=45551995",
    "comments": [
      "And is that such a bad thing?  C's biggest drawback is its many surprises, I thought.  Some boredom would do it good.reply",
      "I believe Odin's `^` syntax is a direct nod to Pascal. I for example had a prior experience with Delphi so it wasn't not too obscure.reply"
    ],
    "link": "https://dayvster.com/blog/is-odin-just-a-more-boring-c/",
    "first_paragraph": "My recent posts have been diving deep into Zig and C, a shift from my earlier focus on React and JavaScript. This isn\u2019t a pivot but a return to my roots. I started programming at 13 with C and C++, and over the years, I\u2019ve built a wide range of projects in systems programming languages like C, C++, Rust, and now Zig. From hobby experiments and custom Linux utilities to professional embedded systems work think vehicle infotainment, tracking solutions, and low-level components I\u2019ve always been drawn to the power and precision of systems programming. Alongside this, I\u2019ve crafted tools for my own environment and tackled plenty of backend engineering, blending my full-stack expertise with a passion for low-level control.I like many others initially dismissed Odin as that language that was primarily intended for game development. It took me a moment or should I say many moments to realize just how stupid that notion was. Because let\u2019s analyze what game development actually means, it means bu"
  },
  {
    "title": "How Apple designs a virtual knob (2012) (jherrm.github.io)",
    "points": 106,
    "submitter": "gregsadetsky",
    "submit_time": "2025-10-07T18:19:37 1759861177",
    "num_comments": 72,
    "comments_url": "https://news.ycombinator.com/item?id=45506748",
    "comments": [
      "> After using the knobs in Garageband for a while, I noticed that they didn\u2019t always react the way I thought they would. Most of the time the little indicator dot on the knob would follow my finger as I spun the knob around in a circle. Other times the knob wouldn\u2019t follow my finger at all and seemed to go in random directions. I eventually figured out that I had stumbled on three different ways to turn a virtual knob.> ...> Apple\u2019s attention to detail is what has propelled it to be the most valuable company on earth. Whether it\u2019s the click of a physical button or the math behind inertial scrolling, Apple employees work really hard to make products that are deceptively simple and just feel right. The virtual knobs found in Garageband are no exception and I hope others enjoyed learning about them as much as I have.I think these two statements are contradictory. Personally, I've noticed a pattern when people post about Apple UX that seems to go \"yes this thing may be unintuitive but actually it's a sign of really good design!\" that I can't quite seem to wrap my head aroundreply",
      "I think it's more that someone may assume how something works, and it isn't exactly that, so they say it's unintuitive. But there could be multiple assumptions on how it should work on first use. Covering all of those possibilities, and integrating them into a cohesive experience that works the first time, and makes even more sense as you continue to use it and learn the other ways to interact, shows a strong attention to detail and design.This is opposed to something that may be very intuitive for 30% of people, but the other 70% are lost, and the implementation doesn't scale.reply",
      "I think in UX there is general lack of desire to properly explain how stuff works instead of relying on just \"guessing user expectations right\"like if said knob just displayed a vertical bar with marks signalling up and down also works it would be very clear to person that tried to just spin itreply",
      "Hmm, may be you're right about UX designers not wanting to explain. For instance, I certainly won't want that vertical bar for aesthetic reasons. It's just hard to defend objectively.reply",
      "Agreed. There's a lot of self blaming going on here. \"Apple cares about users so much. They work sooo hard\" .. but also when things don't work well, they don't seem to update their world view.Its quite fascinating behavior really. Reality distortion field.reply",
      "The problem of the knob is that it offers a large and precise control, but that large control remains invisible. There's no obvious clue showing that you can still interact with the knob by dragging the pointer / finger far away from it.Adding a simple visual clue would help discoverablility a lot. Draw a faint halo on touch, when the mode changes. Draw a more visible trail when the touch point is dragged. Provide immediate and localized feedback, like good UX guidelines suggest.reply",
      "author here - fair point but I still think apple made the right call even if it leads to a bit of confusion at first.If a digital knob needs to be turned several times (e.g. 1080\u00ba, common in DAWs), the \"default\" way to interact with a knob on a touchscreen - circling again and again - is slow and uncomfortable. Adding \"slider\" gestures on top of the default behavior is a nice way to perform many turns quickly and easily.reply",
      "The whole idea of knob is stupid both on touch screens as well as desktop. There are other good alternatives which are far more intuitive than knobs.Knobs are good when you can physically rotate them like for example in a car. But there we are removing knobs and adding touchscreens.reply",
      "In just about every simulator featuring knobs, I've noticed that most knob interfaces will accept scroll wheel inputs. Use the literal knob in your mouse to control the knob on the screen.Of course Apple's mice don't have a physical knob, so that approach doesn't work, but knobs and mice can work outside of the Apple sphere.On touch screens, you can probably make them work by tapping the knob and popping up a slider to control the value. Lets you use knobs to maintain an overview while exposing usable controls for modifications.reply",
      "The point of knobs is that you can fit a ton of sliders in a limited space, and that you can wildly adjust them with very little movement. Both are requirments for a lot of music software. What would the alternative be?reply"
    ],
    "link": "https://jherrm.github.io/knobs/",
    "first_paragraph": "\ud83d\udca1 Author's note: This unpublished post was originally written in May 2012, and refreshed for publication in November 2022.Just want to see the code? Check out github.com/jherrm/knobsWhen Apple introduced Garageband for the iPad back in March 2011, I just had to try it out. It wasn\u2019t just the ability to create music that drew me in, it was the realization that this was a flagship app that uses the strengths of a multitouch interface to create experiences that weren\u2019t possible with a keyboard and mouse. My favorite example is how you can hit a note on the sound sampler keyboard then drag your fingers left and right to bend the pitch.Some time after a new type of interface is introduced, a handful of concepts tend to \u201cwin\u201d and become a standard way of interacting with the interface. Keyboard shortcuts and right click contextual menus aren\u2019t tied to a specific OS, they\u2019re part of the standard toolset you get by using a keyboard and mouse. The iPhone popularized pinch-to-zoom, swipe, long t"
  },
  {
    "title": "Testing two 18 TB white label SATA hard drives from datablocks.dev (ounapuu.ee)",
    "points": 142,
    "submitter": "thomasjb",
    "submit_time": "2025-10-06T09:36:08 1759743368",
    "num_comments": 81,
    "comments_url": "https://news.ycombinator.com/item?id=45489497",
    "comments": [
      "Hello, author here! It's a nice surprise to notice my own post here, but the timing is unfortunate as I'm shuffling things around on my home server and will accidentally/intentionally take it offline for a bit.Here's a Wayback Machine copy of the page when that does happen: https://web.archive.org/web/20251006052340/https://ounapuu.e...reply",
      "I've been considering \"de-enterprising\" my home storage stack to save power and noise and gain something a bit more modular. Currently I'm running on an old NAS 1U machine that I bought on eBay for about $300, with a raidz2 of 12x 18TB drives. I have yet to find a good way to get nearly that much storage without going enterprise or spending an absolute fortune.I'm always interested in these DIY NAS builds, but they also feel just an order of magnitude too small to me. How do you store ~100 TB of content with room to grow without a wide NAS? Archiving rarely used stuff out to individual pairs of disks could work, as could running some kind of cluster FS on cheap nodes (tinyminimicro, raspberry pi, framework laptop, etc) with 2 or 4x disks each off USB controllers. So far none of this seems to solve the problem that is solved quite elegantly by the 1U enterprise box... if only you don't look at the power bill.reply",
      "> How do you store ~100 TB of content with room to grow without a wide NAS?In the cloud (S3) or on offline (unpowered HDDs or tapes or optical media) I suppose. Most people just don't store that much content.> So far none of this seems to solve the problem that is solved quite elegantly by the 1U enterprise box... if only you don't look at the power bill.What kind of power bill are you talking about? I'd expect the drives to be about 10W each steady state (more when spinning up), so 180W. I'd expect a lower-power motherboard/CPU running near idle to be another 40W (or less). If you have a 90% efficient PSU, then maybe 250W in total.If you're way more than that, you can probably swap out the old enterprisey motherboard/RAM/CPU/PSU for something more modern and do a lot better. Maybe in the same case.I'm learning 1U is pretty unpleasant though. E.g. I tried an ASRock B650M-HDV/M.2 in a Supermicro CSE-813M. A standard IO panel is higher than 1U. If I remove the IO panel, the motherboard does fit...but the VRM heatsink also was high enough that the top case bows a bit when I put it on. I guess you can get smaller third party VRM heat sinks, but that's another thing to deal with. The CPU cooler options are limited (the Dynatron A42 works, but it's loud when the CPU draws a lot of power). 40mm case fans are also quite loud to move the required airflow. You can buy noctuas or whatever, but they won't really keep it cool. The ones that actually do spin very fast and so are very loud. You must have noticed this too, although maybe you have a spot for the machine where you don't hear the noise all the time.I'm trying 2U now. I bought and am currently setting up an Innovision AS252-A06 chassis: 8 3.5\" hot swap bays, 2U, 520mm depth. (Of course you can have a lot more drives if you go to 2.5\" drives, give up hot swap, and/or have room for a deeper chassis.) Less worry about if stuff will fit, more room for airflow without noise.reply",
      "2U is definitely better, but I didn\u2019t notice significant drops in dB till I could stuff a 120mm fan in the case. That requires a 3U or more.And if you need a good fan that\u2019s quiet enough for the CPU, you\u2019re looking at 4U. Otherwise, you\u2019ll need AIOs hooked up to the aforementioned 120s.reply",
      "I have to imagine that the best NAS build is simply a 6-core or 8-core standard AMD or Intel with a few HBA controllers and maybe 10Gbit SPF+ fiber or something.\"Old server hardware\" for $300 is a bit of a variation, in that you're just buying something from 5 years ago so that its cheaper. But if you want to improve power-efficiency, buy a CPU from today rather than an old one.--------IIRC, the \"5 year old used market\" for servers is particularly good because many datacenters and companies opt for a ~5-year upgrade cycle. That means 5-year-old equipment is always being sold off at incredible rates.Any 5-year-old server will obviously have all the features you need for a NAS (likely excellent connectivity, expandibility, BMS, physical space, etc. etc.). Just you have to put up with power-efficiency specs of 5 years ago.reply",
      "Dell R500 series is very good for dense storage at low costs if you lean to SATA or NL-SASreply",
      "Nah buy the right enterprise gear insteadhttps://www.supermicro.com/en/products/motherboard/A2SDi-H-T...reply",
      "I'd really dig a version of this with a Ryzen AI chip and 128gb of ram.I'm moving to Lenovo tiny m75q series for now due to low idle power and heat generated.reply",
      "How much TDP and does it have 8+ sata with 10Gbe?reply",
      "If you want 100TB, you need a bigger NAS than most, and that makes most of the DIY NAS not so good. 2-4 drives seems to be where DIY shines. These days motherboards often stop at 4x sata, so you'll need a HBA or USB (eww).Personally, I just don't have that much data, 24TB mirrored for important data is probably enough, and I have my old mirror set avaialable for media like recorded tv and maybe dvds and blu-rays if I can figure out a way to play them that I like better than just putting the discs in the machine.reply"
    ],
    "link": "https://ounapuu.ee/posts/2025/10/06/datablocks-white-label-drives/",
    "first_paragraph": "This post is NOT sponsored, the products were bought with my hard-earned money.I\u2019ve been running a full SSD storage setup for a few years in my home server and I\u2019ve been happy with it, except for the\nstorage anxiety that I get with running small pools of fast storage, which is why I started looking at how the hard\ndrive market is doing.Half of tech YouTube has been sponsored by companies like ServerPartDeals, so they were one of the first\nplaces I looked at, but they seem to only operate within the US and the shipping+taxes destroy any price advantages from\nordering there to Estonia (which is in Europe).At some point I stumbled upon datablocks.dev, which seems to operate within a similar niche,\nbut in Europe and on a much smaller scale. What caught my eye were their white label hard drive offerings. Their website\nhas a good explanation on\nthe differences between recertified and white label hard drives.\nIn short: white label drives have no branding, have no or very low number of power-o"
  },
  {
    "title": "Rating 26 years of Java changes (neilmadden.blog)",
    "points": 147,
    "submitter": "PaulHoule",
    "submit_time": "2025-10-11T18:29:27 1760207367",
    "num_comments": 161,
    "comments_url": "https://news.ycombinator.com/item?id=45551450",
    "comments": [
      "Definitely underrates the impact of annotations. I'm personally not a fan of the way annotations are used to implicitly wire together applications, but I have to admit the impact. Maybe 5/10 is fair in light of the wide range of positive and extremely negative ways annotations can be used.So many of these features were adopted after they were proven in other languages. You would expect that since Java took such a slow and conservative approach, it would end up with extremely polished and elegant designs, but things like streams ended up inferior to previous developments instead of being the culmination. Really disappointing. Java is now a Frankenstein's monster with exactly as much beauty and charm.reply",
      "I used to joke that the direction spring was heading was that you\u2019d have an application be a boilerplate main method with a dozen lines of annotations. Then I actually encountered this in the wild: we had an app that sent updates from db2 to rabbitmq, and the application literally was just configuration via annotations and no actual Java code other than the usual spring main method.reply",
      "Is that strictly bad, though? Being able to run an enterprise service by setting configuration values declaratively, and get all the guarantees of a well-tested framework, seems like a pretty good thing.Yes, it\u2019s weird how that\u2019s still Java, but using standard components and only using code as glue where it\u2019s absolutely necessary seems very similar to other engineering disciplines to me.reply",
      "I think the general adversity against this specialized configurations is that they often tend to be fairly limited/rigid in what they can do, and if you want to customize anything you have to rewrite the whole thing. They effectively lock you into one black box of doing things, and getting out of it can be very painful.reply",
      "Spring boot just provides what they think are reasonable defaults and you provide some specifics.You can always inject your own implementation if needed right?reply",
      "Oh, I think it\u2019s quite wonderful really. There are cases where the limited nature of some configuration-based things ends up being a mess (one that comes to mind is a feature in Spring Data where you can extend a DAO bean into a rest service through annotations, but it turns out that this feature (at least when I last tried working with it), is so rigid as to be nearly useless in actual practice. But our codeless application was a bit of brilliance, I think.reply",
      "this is exactly why spring succeeded. I need to run a scheduled job, @EnableScheduling then @Scheduled(cron = \u201cxxxxxx\u201d) - done. I need XYZ, @EnableXYZ the  @XYZ\u2026 sh*t just works\u2026reply",
      "And then I realize I need to change that schedule. And would like to do it without recompiling my code. Oh, and I need to allow for environment specific scheduling, weekdays on one system, weekends on others. And I need other dependencies that are environment specific.I much prefer Spring's XML configuration from the old days. Yeah, XML sucks and all that. But still, with XML, the configuration is completely external from the application and I can manage it from /etc style layouts. Hard coding and compiling in dependency injection via annotations or other such behaviors into the class directly has caused me grief over the long term pretty much every time.reply",
      "I do realize you were intending to give examples of why you don't think annotations aren't very extensible, but it is an odd example as all those things can still be achieved via annotation, since the annotations can accept values loaded from env specific properties.reply",
      "Exactly this, it\u2019s great fun to have a surface level understanding of a topic and post derisively for internet points; rather then spend the time and effort to actually learn about the subject at hand!reply"
    ],
    "link": "https://neilmadden.blog/2025/09/12/rating-26-years-of-java-changes/",
    "first_paragraph": "I first started programming Java at IBM back in 1999 as a Pre-University Employee. If I remember correctly, we had Java 1.1.8 installed at that time, but were moving to Java 1.2 (\u201cJava 2\u201d), which was a massive release\u2014I remember engineers at the time grumbling that the ever-present \u201cJava in a Nutshell\u201d book had grown to over 600 pages. I thought I\u2019d take a look back at 26 years of Java releases and rate some of the language and core library changes (Java SE only) that have occurred over this time. It\u2019s a very different language to what I started out with!I can\u2019t possibly cover every feature of those releases, as there are just way too many. So I\u2019m just going to cherry-pick some that seemed significant at the time, or have been in retrospect. I\u2019m not going to cover UI- or graphics-related stuff (Swing, Java2D etc), or VM/GC improvements. Just language changes and core libraries. And obviously this is highly subjective. Feel free to put your own opinions in the comments! The descriptions"
  },
  {
    "title": "Immutable Value (kevlinhenney.medium.com)",
    "points": 4,
    "submitter": "mooreds",
    "submit_time": "2025-10-06T20:46:27 1759783587",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://kevlinhenney.medium.com/immutable-value-d5d73dc3252f",
    "first_paragraph": ""
  },
  {
    "title": "AMD and Sony's PS6 chipset aims to rethink the current graphics pipeline (arstechnica.com)",
    "points": 291,
    "submitter": "zdw",
    "submit_time": "2025-10-11T04:36:16 1760157376",
    "num_comments": 362,
    "comments_url": "https://news.ycombinator.com/item?id=45546593",
    "comments": [
      "Could the PS6 be the last console generation with an expressive improvement in compute and graphics? Miniaturization keeps giving ever more diminishing returns each shrink, prices of electronics are going up (even sans tariffs), lead by the increase in the price of making chips. Alternate techniques have slowly been introduced to offset the compute deficit, first with post processing AA in the seventh generation, then with \"temporal everything\" hacks (including TAA) in the previous generation and finally with minor usage of AI up-scaling in the current generation and (projected) major usage of AI up-scaling and frame-gen in the next gen.However, I'm pessimistic on how this can keep evolving. RT already takes a non trivial amount of transistor budget and now those high end AI solutions require another considerable chunk of the transistor budget. If we are already reaching the limits of what non generative AI up-scaling and frame-gen can do, I can't see where a PS7 can go other than using generative AI to interpret a very crude low-detail frame and generating a highly detailed photorealistic scene from that, but that will, I think, require many times more transistor budget than what will likely ever be economically achievable for a whole PS7 system.Will that be the end of consoles? Will everything move to the cloud and a power guzzling 4KW machine will take care of rendering your PS7 game?I really can only hope there is a break-trough in miniaturization and we can go back to a pace of improvement that can actually give us a new generation of consoles (and computers) that makes the transition from an SNES to a N64 feel quaint.reply",
      "My kids are playing Fortnite on a PS4, it works, they are happy, I feel the rendering is really good (but I am an old guy) and normally, the only problem while playing is the stability of the Internet connection.We also have a lot of fun playing board games, simple stuff from design, card games, here, the game play is the fun factor. Yes, better hardware may bring more realistic, more x or y, but my feeling is that the real driver, long term, is the quality of the game play. Like the quality of the story telling in a good movie.reply",
      "Every generation thinks the current generation of graphics won't be topped, but I think you have no idea what putting realtime generative models into the rendering pipeline will do for realism. We will finally get rid of the uncanny valley effect with facial rendering, and the results will almost certainly be mindblowing.reply",
      "I think the inevitable near future is that games are not just upscaled by AI, but they are entirely AI generated in realtime. I\u2019m not technical enough to know what this means for future console requirements, but I imagine if they just have to run the generative model, it\u2019s\u2026 less intense than how current games are rendered for equivalent results.reply",
      "Even if you could generate real-time 4K 120hz gameplay that reacts to a player's input and the hardware doesn't cost a fortune, you would still need to deal with all the shortcomings of LLMs: hallucinations, limited context/history, prompt injection, no real grasp of logic / space / whatever the game is about.Maybe if there's a fundamental leap in AI. It's still undecided if larger datasets and larger models will make these problems go away.reply",
      "I don't think you grasp how many GPUs are used to run world simulation models. It is vastly more intensive in compute that the current dominant realtime rendering or rasterized triangles paradigmreply",
      "I don't think you grasp what I'm saying? I'm talking about next token prediction to generate video frames.reply",
      "Yeah, which is pretty slow due to the need to autoregressively generate each image frame token in sequence. And leading diffusion models need to progressively denoise each frame. These are very expensive computationally. Generating the entire world using current techniques is incredibly expensive compared to rendering and rasterizing triangles, which is almost completely parallelized by comparison.reply",
      "I\u2019m thinking more procedural generation of assets. If done efficiently enough, a game could generate its assets on the fly, and plan for future areas of exploration. It doesn\u2019t have to be rerendered every time the player moves around. Just once, then it\u2019s cached until it\u2019s not needed anymore.reply",
      "Unreal engine 1 looks good to me, so I am not a good judge.I keep thinking there is going to be a video game crash soon, over saturation of samey games. But I'm probably wrong about that. I just think that's what Nintendo had right all along: if you commoditize games, they become worthless. We have endless choice of crap now.In 1994 at age 13 I stopped playing games altogether. Endless 2d fighters and 2d platformer was just boring. It would take playing wave race and golden eye on the N64 to drag me back in. They were truly extraordinary and completely new experiences (me and my mates never liked doom).\nAnyway I don't see this kind of shift ever happening again. Infact talking to my 13 year old nephew confirms what I (probably wrongly) believe, he's complaining there's nothing new. He's bored or fortnight and mine craft and whatever else. It's like he's experiencing what I experienced, but I doubt a new generation of hardware will change anything.reply"
    ],
    "link": "https://arstechnica.com/gaming/2025/10/amd-and-sony-tease-new-chip-architecture-ahead-of-playstation-6/",
    "first_paragraph": "\n        Project Amethyst focuses on efficient machine learning, new compression techniques.\n      It feels like it was just yesterday that Sony hardware architect Mark Cerny was first teasing Sony's \"PS4 successor\" and its \"enhanced ray-tracing capabilities\" powered by new AMD chips. Now that we're nearly five full years into the PS5 era, it's time for Sony and AMD to start teasing the new chips that will power what Cerny calls \"a future console in a few years' time.\"In a quick nine-minute video posted Thursday, Cerny sat down with Jack Huynh, the senior VP and general manager of AMD's Computing and Graphics Group, to talk about \"Project Amethyst,\" a co-engineering effort between both companies that was also teased back in July. And while that Project Amethyst hardware currently only exists in the form of a simulation, Cerny said that the \"results are quite promising\" for a project that's still in the \"early days.\"Project Amethyst is focused on going beyond traditional rasterization t"
  },
  {
    "title": "The World Trade Center under construction through photos, 1966-1979 (rarehistoricalphotos.com)",
    "points": 190,
    "submitter": "kinderjaje",
    "submit_time": "2025-10-07T00:40:26 1759797626",
    "num_comments": 94,
    "comments_url": "https://news.ycombinator.com/item?id=45498055",
    "comments": [
      "Tangentially:> The vision was meant to use the trade facility and urban renewal as tools to clear and revitalize what had become a \u201ccommercial slum\u201d.What this refers to is https://en.wikipedia.org/wiki/Radio_Row#New_York_CityBasically you cannot have Akihabara or Shenzhen style electronics markets because the sort of people that built the WTC don't like their chaotic appearance.reply",
      "It's not about looks but efficient use of land: Manhattan was (and still is) the financial capital of the world. It had the most valuable real estate in the world. Radio Row was a poor use of real estate.Before the Chinese traded electronics in Shenzhen, they traded it in Hong Kong. Yes, as Hong Kong transformed into a financial center, it got rid of the electronics traders.reply",
      "> It had the most valuable real estate in the world. Radio Row was a poor use of real estate.So why did they force a sale price on the people there?If as finance people we believe in market forces they should have bought the stores out at market prices.It's another heads-we-win-tails-you-lose situation.reply",
      "The White House and Central Park are a poor use of real estate as well. Imagine how much money you could make if you developed it into a new Kowloon Walled City!Besides, didn't the Twin Towers have a massive occupancy problem? I recall that being the reason why so many government entities were forced to move there...reply",
      "\u2018poor use of land\u2019 meaning \u2018not being used for the benefit of wealthy people.\u2019reply",
      "> It's not about looks but efficient use of landRight, confiscation from private owners to use it for erection of a building no one needs is a very efficient use of land. The project will require astronomical up front expenses, will stay vacant for almost 2 decades, and have endless problems until its last day (fires, terrorist acts, issues with the structure). But who cares, it's efficiency we are talking about.reply",
      "> because the sort of people that built the WTC don't like their chaotic appearance.And their counterparts in government hate it because chaos is inefficient (because people have rights) to impose their will on (regulate) so even if you don't build WTC there becomes a regulatory environment where nothing organic can grow.reply",
      "Thanks for that.   We seem to have lost sight of the importance of \"commercial biodiversity\" in the past 40 or more years of continuous M&A concentration.Happily, I saw a little discussion of it in 2008 when the advocates of letting the auto companies fail were pushed back by statistics showing how many second and third tier suppliers would be destroyed.  But the fourth tier, the shenzhen / radio alley-type stuff is still ignored.    Very similar to how most companies want to simply hire skills and assume that they will magically appear when in years past, companies took an active hand in creating them by having a career development path in-house.Perhaps the AI bubble will be viewed in the future as the last gasp of companies that depleted the soil that they grew in and now struggle to survive without anyone that knows how to do the work anymore.  Maybe LLMs will be all that remains, our Moai.reply",
      "It's a shame the same logic wasn't applied in maintaining a healthy root-level auto company ecosystem. Having a single megacorp at the top inevitably makes it too big to fail. On the other hand, if there are dozens of smaller car companies, the failure of any one of them is insignificant to the wider ecosystem.A company that knows it is too big to fail will inevitably lead to mismanagement. After all, why bother saving for a rainy day when you can count on corporate welfare handouts? Why bother reducing your risks when you can always rely on a bailout? You can never lose, so the obvious thing to do is to bet as big as possible in an attempt to create as much short-term \"shareholder value\" as possible.reply",
      "There's plenty of \"commercial biodiversity\" left on Canal Street.  And it's pretty gross.reply"
    ],
    "link": "https://rarehistoricalphotos.com/twin-towers-construction-photographs/",
    "first_paragraph": "The World Trade Center project was initiated in the early 1960s through the influence of David Rockefeller in part to reclaim a part of the city that had fallen on hard times.The vision was meant to use the trade facility and urban renewal as tools to clear and revitalize what had become a \u201ccommercial slum\u201d.The construction of the towers yielded not only a new frontier for business but also the landfill for a new shore on the banks of the Hudson.The project, developed by the Port Authority of New York and New Jersey,r was originally planned to be built on the east side of Lower Manhattan, but the New Jersey and New York state governments could not agree on this location.Original architectural and engineering model. This model is now on permanent display at the National September 11 Memorial and Museum.After extensive negotiations, the New Jersey and New York state governments agreed to support the World Trade Center project, which was built at the site of Radio Row in the Lower West Si"
  },
  {
    "title": "People regret buying Amazon smart displays after being bombarded with ads (arstechnica.com)",
    "points": 184,
    "submitter": "croes",
    "submit_time": "2025-10-11T17:41:45 1760204505",
    "num_comments": 88,
    "comments_url": "https://news.ycombinator.com/item?id=45551081",
    "comments": [
      "We're way overdue to abolish the DMCA, in particular the \"anti-circumvention\" felonies.It shouldn't be a crime for me to customize the product I purchased. Or to sell people a kit to do the customization themselves.reply",
      "Throw away the DMCA and you throw away all safe harbors for websites, and then the internet is truly screwed. There's no way the current congress would ever accept such a thing again -- Section 230 has been weaponized against it already.What we should be saying is Improve the DMCA. You've already clued into the biggest thing that needs to change (DRM/anti-anti-circumvention).Let's not throw the baby out with the bathwater. This is unpopular to say in HN-type circles but the DMCA is actually not that bad and mostly works as it should.reply",
      "The protections of section 230 are from an entirely different law (CDA 1996), even repealing the whole DMCA is separate.reply",
      "Any improvement is a change and you can\u2019t just make \u201cimprovements\u201d when the changes require politicians. It\u2019s likely they would seize the opportunity to change a lot more than the improvements you feel are needed.reply",
      "Better yet: ban hardware vendors from denying the user an ability to customize software.They don't have to make it easy, but they should be forced to give a way to opt out of walled gardens and bypass \"secure boot\".reply",
      "These days, the general rule is to avoid buying anything 'smart'. They are all filled with advertisements and data-sharing practices and are designed to target you through their user interface and applications. They bombard you with offers for their other products and deals.reply",
      "Matrix got it a bit wrong; the machines aren't interested in our body heat, they're going to put us in the goo pods and force use to watch adverts 24/7reply",
      "Why? What do they or their creators get out of it?reply",
      "Obviously platforms get advertiser dollars, but the question is what the business paying for that gets.  The answer is.. almost nothing?  Dedicated marketing/advertising resources at your business is probably just a fifth column, where shareholders and business owners are swindled into paying the salary and other maintenance for people who are actually working for google/facebook/amazon/whatever.Source? Like 20+ years of ubiquitous surveillance, tracking and micro-targeting.. and yet non-pet owners still get ads for dog food, males get ads for feminine hygiene products, single people are offered deals on family vacations, and people who just bought a car get car advertisements for the next 3-5 years which only taper off when it might actually be time to buy another car.reply",
      "Shareholder valuereply"
    ],
    "link": "https://arstechnica.com/gadgets/2025/10/people-regret-buying-amazon-smart-displays-after-being-bombarded-with-ads/",
    "first_paragraph": "\n        \"I'm about to just toss the whole thing...\"\n      Amazon Echo Show owners are reporting an uptick in advertisements on their smart displays.The company's Echo Show smart displays have previously shown ads through the company's Shopping Lists feature, as well as advertising for Alexa skills.\u00a0Additionally, Echo Shows may play audio ads when users listen to Amazon Music on Alexa.However, reports on Reddit (examples here, here, and here) and from The Verge\u2019s Jennifer Pattison Tuohy, who owns more than one Echo Show, suggest that Amazon has increased the amount of ads it shows on its smart displays' home screens. The Echo Show\u2019s apparent increase in ads is pushing people to stop using or even return their Echo Shows.The smart displays have also started showing ads for Alexa+, the new generative AI version of Amazon's Alexa voice assistant. Ads for the subscription-based Alexa+ are reportedly taking over Echo Show screens, even though the service is still in Early Access.\u201cThis is ge"
  },
  {
    "title": "GNU Health (gnuhealth.org)",
    "points": 333,
    "submitter": "smartmic",
    "submit_time": "2025-10-11T15:47:22 1760197642",
    "num_comments": 96,
    "comments_url": "https://news.ycombinator.com/item?id=45550049",
    "comments": [
      "There was a guy on reddit a few years ago who started a dental practice with entirely open-source software and his own EHR system. Really interesting stuff, don't think anyone's posted about it here. Can't view his reddit history but he must still be using it, last commit 1 week ago.https://www.reddit.com/r/linux/comments/p5phju/progress_repo...https://www.reddit.com/r/linux/comments/x2mls1/update_starti...reply",
      "That\u2019s from the founder of Clear.dental, Dr. Tej Shah:https://clear.dental/https://gitlab.com/cleardentalhttps://www.linkedin.com/in/tej-shah-17829195reply",
      "And the 2.0 source: https://invent.kde.org/desiotaku/cleardentalreply",
      "Hm they had an AMA recently going on, so I asked them some questions if they are still using it or what not (on reddit) mentioning this HN comment.https://old.reddit.com/r/Dentistry/comments/1o3hawd/prison_d...Surprisingly it had 100 comments but no open source questions iirc so that was a bit of surprise from what I could check.Also Offtopic or not but its sad that you can't use reddit because you are in UK but just for the sake since I want you to see the comment, I perma-linked it and uploaded it to wayback-machine/archive.org and here's the link so that you can view what I wroteI am going to archive the whole reddit page later for you to read as wellhttps://web.archive.org/web/20251011181833/https://old.reddi...Wait why is this not working wtf, Dentistry: page not found for archive wtf?Edit: I archived the whole page as I said, here it is.https://web.archive.org/web/20251011182126/https://old.reddi...Hope this helps OP and maybe I will keep the archive updated for few days or give ya updates if that's something you are interested in I suppose I am not sure, just like many other things in my life.reply",
      "> Can't view his reddit history as I'm in the UKWhat does this mean?reply",
      "It's tagged as NSFW for some reason and I can't be bothered verifying my agereply",
      "just change from www.reddit.com to old.reddit.com and then it doesn't ask you to sign up.Does this work in the UK or do they still ask you to verify?reply",
      "No, but just put something like rl.bloat.cat instead of reddit.com. That'll direct to an alternative community maintained interface for Reddit that will work.reply",
      "I know that alternative frontends for reddit were a huge thing which I used to use before the api fiascoThere are still some alternatives but most of them now scrape or have extreme rate limiting from what I know.They use redlib but If I remember correctly that's similar to libreddit but patched to work without api but still, its a very finnicky solution.Like these solutions can work but I think at that point, just use a VPN but oh boy reddit detects those VPN's from what I know.WOW UK censorship law is really something huh, can people living in the UK somehow vote to repeal that or something?I was thinking on the scary part of as to what if many countries can seemingly connect together these pieces to genuinely have internet authoritarianism and what if they have such eggregious fees or just even a threat of it, have a little mixture of getting sanctioned if you try to move around it but damn, this is so weird, if they really want, they can genuinely escalate this more and more to block VPN's and more and more to effectively soft-lock a person from the internet. This needs to stop. Right now. Otherwise I am scared if what if multiple countries come together to stop something like tor nodes by somehow putting them in such a law. Once tor stops, all hell can break loose on the internet, its certainly possible, I never expected this but the only thing stopping UK censorship might be hopefully their incompetence of maybe not removing VPN's or this goose chase or just the fact that this is the beginning, not the end. They are testing how much they can get away with which is increasing a lot... This really made me pessimistic actually.The only hope is that such websites can spring up more quickly than UK can take them down but what if UK sets a dangerous precedent by suing them, its definitely possible to track them down by the UK govt.They say on their blog that https://bloat.cat/blog/updates-may-25/ that Redlib is the most resource-hungry service. The traffic figures run into terabytes a monthSome % of these could be for bypassing the UK as wellthough I suppose that not even govt. can catch them,their Opsec is genuinely really good, they use monero for the servers and etc., its fascinating to see their Opsec be so secure.Edit: I got so curious and found out that they run some servers on senko.digital which is in fact UK based but they won't still get much (I hope) because senko.digital supports monero so their opsec is secure but if they had slipped up, it wouldn't have been hard to see them being framed as they get terabytes of data and some % of data can help loop around UK censorship evil laws and they could've tried to frame him and senko being a UK company, it isn't hard to follow that they would've complied. But they use monero and I am sure that they use a vpn as well but it was certainly fun reading their Opsec and I think that its sort of perfect, I need to learn more from it actually.So I guess its still possible to run websites without incurring the hefty fine in UK but its certainly very hard / borderline impossible and I just hope that this UK thing / similar things in other countries doesn't get any further and gets banned/repealed otherwise the internet might die.Edit 2: maybe I gave them too much credit since either its saying Reddit is blocking redlib as always... when I try to click on any username or it just gives a flat out nginx 502 bad error... I really gave them too much credit but it was fun learning something about opsec.reply",
      "It means you can't view people's reddit profiles in the UK.( Yes, seriously. )Many many profiles are tagged NSFW, its' not clear why, I can't imagine the majority of those have done so deliberately, perhaps it's automatic for anyone who's posted any NSFW posts ever. ( Which includes people doing so to be funny such as someone posting a huge loss in a sports sub as NSFW. )reply"
    ],
    "link": "https://www.gnuhealth.org/about-us.html",
    "first_paragraph": "The GNU Health project provides the tools for individuals, health professionals, institutions and governments to proactively assess and improve the underlying determinants of health, from the socioeconomic agents to the molecular basis of disease. From primary health care to precision medicine. The following are the main components that make up the GNU Health ecosystem:GNU Health is a Free/Libre, community-driven project from GNU Solidario, that counts with a large and friendly international community. GNU Solidario celebrates GNU Health Con (https://www.gnuhealthcon.org) and the International Workshop on e-Health in Emerging Economies (IWEEE) every year, that gathers the GNU Health and social medicine advocates from around the world.GNU Health is a GNU (www.gnu.org) official package, awarded with the Free Software Foundantion award of Social benefit, among others. GNU Health has been adopted by many hospitals, governments and multilateral organizations around the globe.GNU Health HMIS"
  },
  {
    "title": "Windows Subsystem for FreeBSD (github.com/balajes)",
    "points": 216,
    "submitter": "rguiscard",
    "submit_time": "2025-10-11T07:32:08 1760167928",
    "num_comments": 83,
    "comments_url": "https://news.ycombinator.com/item?id=45547359",
    "comments": [
      "I realize not everyone will care about this, but I find the naming for these WSL-like subsystems is confusingly backwards.  i.e. It should have been Linux Subsystem for Windows, or Window's Subsystem for [Linux | FreeBSD | etc].reply",
      "The explanation they give is they need to put their trademark, Windows, before Linux. Sometimes they say this is advice from the legal department.I still think they could fulfill that requirement and call it the \"Windows Linux subsystem\" or something, but what do I know?Unrelated, but I think the WSL2 design is kind of stupid. It's just a VM. I think the WSL1 design, where it was a syscall layer, is a better call. But that was slower, IIRC chiefly because the NT filesystem syscalls are slower than Linux's VFS. Rather than improve that problem, they side-step it by running Linux in a VM.reply",
      "The design of WSL(1) makes more sense when you think of its original design goal of being a compatibility layer for Android apps. Android is \"a Linux\", but it is (1) a relatively unique one, and (2) everything between the Android kernel and Android apps really isolates the application layer from kernel-level details. Given this separation, it makes a lot of sense to leverage the existing NT flexibility and emulate a Linux kernel at the syscall layer. Sure, you'll have to patch some parts of the WSL(1) Android system components, but MS was already going to have to do that to work around Google-only components. In many ways, this route is no more complex than what Blackberry did to run Android apps atop their QNX-based OS.But once you give up the specialization for Android and want WSL to be a \"real Linux\" (i.e. behave like a specific Ubuntu/Fedora/etc distribution) now you no longer can get away with being Linux-like, you have to be Linux (i.e. you need the syscall layer to directly mirror all kernel development and features). It's actually fairly impressive how much worked with WSL(1) given how different the internals were, but you didn't have to go that far to find tools/services/etc that just wouldn't work.Instead, once you consider how long MS had been working on Hyper-V, and how interested they are in using it to apply additional security boundaries/isolation (e.g. VBS) within what outwardly appears to be a single Windows OS instance to the user, it makes a lot of sense to leverage that same approach to just run a real Linux kernel atop Hyper-V. In that world, you no longer have to match Linux kernel development, you just need to develop/maintain the kernel drivers used to interact with Hyper-V - and MS already had a lot of experience and need to do that given how much of Azure is running Linux VMs.reply",
      "The bigger problem was how fast Linux evolves. Windows kernel development is glacial by comparison. Keeping up with every new thing in Linux was tantamount to maintaining a second operating system.reply",
      "IIRC, there was an article, whose author said that improving NT kernel without blessing from higher ups is even frowned upon.So making a better WSL on syscall layer, which NT kernel is designed for, is not only behind a technical effort wall, but also behind a big red tape.reply",
      "Improving that problem probably would've been a massive undertaking. That aside, there's the problem that implementing kernel mechanics is a lot more than faking syscalls: the various types of namespaces, FUSE, random edge cases that applications do expect, kernel modules, etc. At the end of the day, users don't want to stumble into some weird compatibility issue because they're running not-quite-Linux; it's a better UX to just offer normal Linux with better integration.The WSL2 design isn't stupid, it's practical. What I will give you is that it's not elegant in an \"ivory tower of ideal computing\" sense.reply",
      "When people talk about improved compatibility or higher practicality I wonder why they don\u2019t just run Linux on metal at that point.  You can either run it on your laptop, or connect to a networked computer.reply",
      "Can I even use a usb serial port yet after how many years? (Possibly by now but how long did it take, and does it actually work well?)It is stupid in that it's not really any kind of subsystem, it's just a vm. VMs have their uses, but it's basically just an app.The reason hardware such as my usb serial example (or any serial) worked on wsl1 was because it actually was a subsystem.reply",
      "There is no native USB passthrough support, but you can use USB/IP to access them via network.https://github.com/dorssel/usbipd-winreply",
      "Your serial might have worked, but your docker didn\u2019t. (And someone else\u2019s other drivers didn\u2019t, and mmapping had ever-so-slightly different semantics causing rare and hard to reproduce issues).WSL2, on the whole, is much more compatible. If you want 100% Linux compatibility, just run Linux.reply"
    ],
    "link": "https://github.com/BalajeS/WSL-For-FreeBSD",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Experimental project to adapt the WSL2 open-source components to run on FreeBSD\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\u26a1 Experimental Project \u2013 Running FreeBSD on WSL2 \u26a1This repository hosts work-in-progress efforts to run FreeBSD inside Windows Subsystem for Linux (WSL2) with minimal to no changes to the FreeBSD base system. The project builds on the open-source components of WSL2 to enable FreeBSD to boot and run seamlessly in a Windows environment.\ud83d\udea7 Work in Progress \u2013 This is an experimental personal project.At this stage, contributions are welcome in the form of:Please open an issue or start a discussion to get involved.This project is released under an open-source license (TBD).This is a personal, experimental project and is not affiliated with"
  },
  {
    "title": "Superpowers: How I'm using coding agents in October 2025 (fsck.com)",
    "points": 277,
    "submitter": "Ch00k",
    "submit_time": "2025-10-11T07:29:23 1760167763",
    "num_comments": 158,
    "comments_url": "https://news.ycombinator.com/item?id=45547344",
    "comments": [
      "I can't recommend this post strongly enough. The way Jesse is using these tools is wildly more ambitious than most other people.Spend some time digging around in his https://github.com/obra/Superpowers repo.I wrote some notes on this last night: https://simonwillison.net/2025/Oct/10/superpowers/reply",
      "I\u2019m curious how you think this compares to the Research -> Plan -> Implement method and prompts from the \u201cAdvanced Context Engineering from Agents\u201d video when it comes to actual coding performance on large codebases. I think picking up skills is useful for broadening agents abilities, but I\u2019m not sure I\u2019d that\u2019s the right thing for actual development.The packaged collection is very cool and so is the idea of automatically adding new abilities, but I\u2019m not fully convinced that this concept of skills is that much better than having custom commands+sub-agents. I\u2019ll have to play around with it these next few days and compare.reply",
      "Using Research->Plan->Implement flow is orthogonal, though I notice parts of those do exist as skills too. But you sometimes need to do other things too, e.g. debugging in the course of implementing or specific techniquws to improve brainstorming/researching.Some of these skills are probably better as programmed workflows that the LLM is forced to go through to improve reliability/consistency, that's what I've found in my own agents, rather than using English to guide the LLM and trusting it to follow the prescribed set of steps needed. Some mix of LLMs (choosing skills, executing the fuzzy parts of them) and just plain code (orchestration of skills) seems like the best bet to me and what I'm pursuing.reply",
      "Orthogonal means there should not be any overlap.reply",
      "Yeah, but you knew what he meant.reply",
      "This looks like usage rules in Elixir, but for agent behaviors, and currently specifically for Claude: https://hexdocs.pm/usage_rules/readme.htmlreply",
      "What's up with people (or I suppose AI) including copyright licenses in AI generated code?At least it's an MIT license, but since AI output isn't copyrightable, I'm unsure what the point is since people can legally ignore the license.reply",
      "^ (not legal advice -- far from it)reply",
      "If there some reason why one wouldn't be able to ignore the copyright license of something not protected by copyright, I'd love to hear it.The copyright office has been quite clear that AI output is not protected by copyright without substantial human  creative expression in the final product. Prompt-created works simply aren't protected by copyright.Indeed, I expect people muddling their code bases with AI output are going to find themselves in an interesting position if their code ever gets leaked.reply",
      "This article left me wishing it was \"How I'm using coding agents to do <x> task better\"I've been exploring AI for two years now. It's certainly upgraded itself from the toy classification to a basic utility. However, I increasingly run into its limitations and find reverting to pre-LLM ways of working more robust, faster, and more mentally sustainable.Does someone have concrete examples of integrating LLM in a workflow that pushes state-of-the-art development practices & value creation further?reply"
    ],
    "link": "https://blog.fsck.com/2025/10/09/superpowers/",
    "first_paragraph": "It feels like it was just a couple days ago that I wrote up \"How I'm using coding agents in September, 2025\".At the beginning of that post, I alluded to the fact that my process had evolved a bit since then.I've spent the past couple of weeks working on a set of tools to better extract and systematize my processes and to help better steer my agentic buddy. I'd been planning to start to document the system this weekend, but then this morning, Anthropic went and rolled out a plugin system for claude code.If you want to stop reading and play with my new toys, they're self-driving enough that you can. You'll need Claude Code 2.0.13 or so. Fire it up and then run:After you quit and restart claude, you'll see a new injected prompt:That's the bootstrap that kicks off Superpowers. It teaches Claude a couple important things:It also bakes in the brainstorm -> plan -> implement workflow I've already written about. The biggest change is that you no longer need to run a command or paste in a promp"
  },
  {
    "title": "Paper2Video: Automatic Video Generation from Scientific Papers (arxiv.org)",
    "points": 5,
    "submitter": "jinqueeny",
    "submit_time": "2025-10-11T23:32:25 1760225545",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://arxiv.org/abs/2510.05096",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "A Guide for WireGuard VPN Setup with Pi-Hole Adblock and Unbound DNS (psyonik.tech)",
    "points": 20,
    "submitter": "pSYoniK",
    "submit_time": "2025-10-11T19:41:27 1760211687",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=45552049",
    "comments": [
      "I have a similar setup, but with AdGuardHome. I used Pi-Hole in the past, but AdGuardHome's UI is from this century at least. That, and the fact that with Pi-Hole it was very difficult have IPv6 working.I have an instance on my router in my home network for covering all devices by default, and a hosted one to which I connect when outside via mobile network. Split-tunneling with only the DNS routed, so that I don't have to push all traffic through the VPN.reply",
      "I just use blocklists in Unbound without having to bother with Pi-Hole. Nothing against Pi-Hole, I just find it easier long-term to maintain fewer services.reply",
      "I have looked at that briefly, I think I had gone with pihole in the end for the ability of having a UI to easily see any resolution issues and local dns management (which, I think, is also present in Unbound but not in a UI but via configs).reply",
      "You don't need a VPN! I host an AdguardHome instance and just expose TCP/853.  I put my domain name in the Private DNS settings of my Android and I get 24/7 adblocking without the hassle and battery drain of my Wireguard VPN (which I still use to access private stuff)reply"
    ],
    "link": "https://psyonik.tech/posts/a-guide-for-wireguard-vpn-setup-with-pi-hole-adblock-and-unbound-dns/",
    "first_paragraph": "I\u2019ve used Mullvad as my VPN provider for a few years. Their service is good, they provide keys for 5 devices, rely on the Wireguard protocol, and offer alternative configurations as well. Despite that, my needs have changed and I wanted to be able to have granular control over DNS queries and the ability to connect my various devices on my network through simple addresses, such as http://emby.home.server. Enter Wireguard, Pi-Hole and Unbound.Of course, there are many tools that might achieve the same results with a lot less work, such as Tailscale. I would reach devices on the network easily, one of the nodes could be set to act as an exit node and you could also apply network-wide ad-block with Next DNS (I believe that is, at the time of writing, the solution they offer). Outside of Tailscale there are other fully open source alternatives, such as Headscale, Nebula and others.\nHowever, I chose to set up my own Wireguard network, as it gave me the opportunity to learn a lot and helped "
  }
]