[
  {
    "title": "The Visual World of 'Samurai Jack' (animationobsessive.substack.com)",
    "points": 154,
    "submitter": "ani_obsessive",
    "submit_time": "2025-06-01T21:57:35 1748815055",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=44154032",
    "comments": [
      "Genndy Tartakovsky is a truly unique creator. Samurai Jack was so good, that I had to watch how it ends years later since watching the show on cartoon network\n \nreply",
      "For a lot of great behind the scenes artwork: https://characterdesignreferences.com/art-of-animation-8/art...I used to have an artist roommate obsessed with the art style of the show. If anyone hasn't yet watched the rebooted season from 2017, I highly recommend it.Edit: one great clip from S5 https://www.youtube.com/watch?v=IFDkcvrSaYU\n \nreply",
      "I wasn't much a fan of the rebooted series, I thought the inclusion of the new character took too much focus. Almost felt to me more like a fan fiction ending with someone's OC\n \nreply",
      "12 years on you can get the original creator back to storyboard everything, hire back the old VO team, and still have people complaining the show is too different :P\n \nreply",
      "I watched the whole series because of some clips from the new season.The new stuff felt more mature to me and the old stuff more like a kids show.Overall I liked both, but I get it, the new stuff felt a bit off.\n \nreply",
      "Samurai Jack is so beautiful and being able to portray things / have the confidence in the art really seems to cut back on unnecessary / clumsy dialogue that so many shows have today.\n \nreply",
      "His first season of Primal has almost zero dialogue and works really well, but it's a caveman and a dinosaur buddy show.\n \nreply",
      "The episode with the three blind archers guarding the well that grants wishes is a master class in gesticulates at everythingWhenever I think of Tartakovsky I also think of his clone wars micro series he did for Cartoon Network. The episode with the special forces clone troopers that has no dialogue after the first 30-60 seconds or show is just so unbelievably good. So much tension.\n \nreply",
      "> Whenever I think of Tartakovsky I also think of his clone wars micro series he did for Cartoon Network.the episode with mace windu taking on an entire droid army - by himself - is what opened my imagination to the true potential that a jedi master has.  it's a shame that neither the movies, nor any other tv show, have come close to conveying \"why\" everyone fears and respects force users.\n \nreply",
      "Ok I\u2019m rewatching that one soon, forgot about it.  The flashback of like, all heroes including Vishnu and zeus fighting aku was also rad.  Maybe even the pilot, or the start of the reboot?  Awesome\n \nreply"
    ],
    "link": "https://animationobsessive.substack.com/p/the-visual-world-of-samurai-jack",
    "first_paragraph": ""
  },
  {
    "title": "Root shell on a credit card terminal (stefan-gloor.ch)",
    "points": 546,
    "submitter": "stgl",
    "submit_time": "2025-06-01T13:42:49 1748785369",
    "num_comments": 149,
    "comments_url": "https://news.ycombinator.com/item?id=44150803",
    "comments": [
      "For the young players: this is what hacker in \u201cHacker News\u201d stands for. This is 101 and it\u2019s very simply explained which makes it a great step by step example of a typical journey. Hack-a-day is full of these if you want more.The author is clearly curious and leads in knowing a lot to begin with.The work-behind-the-work is looking up data sheets for the chips involved, desoldering them without damaging them, in the case of memory resoldering with hookup wire and hopefully its access is slow enough that it can work fine over the length of the wire, following hunches, trying things, and knowing (for next time) the possibility of using a pinhole camera or something of the sort when drilling shallow holes and looking through for tamper traces to avoid in further drills, if so desired be.As others have mentioned, it would be interesting if the author stuck in and got past the tamper checks to see if it would work as normal. Oh well!\n \nreply",
      "The term \"hacker,\" even in the computer field, originally had a larger scope than computer security.  It had a more philosophical definition, too.  I host a copy of the Jargon File[1], compiled by Guy Steele et al., on my web site.  It defined \"hacker\" as[2]:  HACKER [originally, someone who makes furniture with an\n  axe] n. 1. A person who enjoys learning the details of\n  programming systems and how to stretch their capabilities,\n  as opposed to most users who prefer to learn only the\n  minimum necessary. 2. One who programs enthusiastically,\n  or who enjoys programming rather than just theorizing\n  about programming. 3. A person capable of appreciating\n  hack value (q.v.). 4. A person who is good at programming\n  quickly. Not everything a hacker produces is a hack. 5. An\n  expert at a particular program, or one who frequently does\n  work using it or on it; example: \"A SAIL\n  hacker\". (Definitions 1 to 5 are correlated, and people\n  who fit them congregate.) 6. A malicious or inquisitive\n  meddler who tries to discover information by poking\n  around.  Hence \"password hacker\", \"network hacker\".\n\nI'm guessing that PG had this broader definition in mind when Hacker News was started.No history of the term \"hacker,\" however brief, would be complete with a reference to The UNIX-HATERS Handbook[3].[1] https://speechcode.com/jargon/[2] https://speechcode.com/jargon/jargon.info.Hacker.html[3] https://web.mit.edu/~simsong/www/ugh.pdf\n \nreply",
      "Or as RMS defines it, playful cleverness.\n \nreply",
      "Amen for the first sentence. One more LLM wrapper today, and I would die.\n \nreply",
      "It does feels like a good use of AI (ML, really) would be to write a \"disaggregator\" for HN that tags submissions by category and lets users browse the bits they care about. Wish I had time to do it....\n \nreply",
      "Here it is ;)https://histre.com/hn/\n \nreply",
      "And a TL;DR, both for the articles themselves as well as the discussions.I don't think a TL;DR can replace most articles that appear on HN, but it can certainly tell me whether the article is interesting, much better than any headline ever could. Especially so if the TL;DR is written by a neutral AI with no interest in making me click anything, and hence no qualms about surfacing the most important information to the top.I actually tried to do this, but it was with GPT-3.5, and I didn't exactly like how it worked. I should look at this again, I wouldn't be surprised if the code I used back then could just be ported over to 2.5 Flash and produce much better results.\n \nreply",
      "Yeah, this has been LLM News for a while...\n \nreply",
      "50% people shilling LLM products, 50% people complaining about LLMs (or indirectly by complaining about crawlers)However, this place used to be JS framework news not too long ago\n \nreply",
      "and \"Crypto News\" for a long time too\n \nreply"
    ],
    "link": "https://stefan-gloor.ch/yomani-hack",
    "first_paragraph": "In this project, I started to reverse engineer payment card\n        terminals because they seemed to be an interesting target for security\n        research, given the high stakes involved. Although I initially didn\u2019t\n        know much about this industry, I did expect a ton of security features\n        and a very security-hardened device. And to some degree, this was also\n        correct.The model I went with is a Worldline Yomani XR terminal. Although it\n        seems to be discontinued at the time of writing, this is the model that\n        is everywhere in Switzerland. From big grocery chains to the\n        small repair shop on the corner, everyone has one or a whole fleet of\n        this exact terminal.After booting it up and aimlessly clicking through the UI, I did a\n        quick port scan, but couldn\u2019t find anything interesting. So naturally,\n        I started to take it apart.The housing and the PCBs appear to be well-made. The design consists\n        of multiple PCBs: a small c"
  },
  {
    "title": "LibriVox (librivox.org)",
    "points": 78,
    "submitter": "bookofjoe",
    "submit_time": "2025-06-01T21:02:58 1748811778",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=44153738",
    "comments": [
      "One of my favourite novels, H. Beam Piper's _Little Fuzzy_ is in the public domain (because he died intestate and the Commonwealth of Pa. failed to renew his copyrights before they were turned over to his estate).There is a professional-quality reading of it by Tabithat:https://librivox.org/little-fuzzy-by-h-beam-piper/which I recommend highly.Unfortunately, the quality of the readings can vary widely, and my family has often been unwilling to put up with a poor quality recording on long trips.That said, I use their app on my phone while doing boring tasks at work, and greatly appreciate the project.\n \nreply",
      "\"This is a libravox recording. All libravox recordings are in the public domain...\"I contributed to a Robert Lynd project on libravox many years ago, and I still remember saying these intros.(Lynd was a wonderful essayist, if anyone might be interested).\n \nreply",
      "I know th books are always in the public domain. Are the recordings public domain as well, or are they under a free license like GPL?\n \nreply",
      "All LibriVox recordings are in the public domain.https://librivox.org/pages/about-librivox/\n \nreply",
      "https://librivox.org/the-adventures-of-sherlock-holmes-versi...David Clarke does a very good narration of many Sherlock Holmes stories.\n \nreply",
      "I wonder if AI will be a benefit or a detriment to this project.On the one hand, there\u2019s going to be a lot more, potentially high quality audio books in its repository, on the other hand it goes against the spirit of the project itself.\n \nreply",
      "The speech data collected by this project has been used for more than a decade to build automatic speech recognition and text-to-speech synthesis systems (see LibriSpeech, LibriTTS, LJSpeech). It definitely has been a benefit to AI.\n \nreply",
      "I'd like to think that in the future this will be possible, but at present I believe there are still too many uncanny valley problems for me to regard any TTS generated audio books as high quality. I can sometimes tolerate listening to articles or technical essays done this way, but quality audio book narrators often do consistent and distinct character voices, understand complex emotional states, and are capable of reacting to contextual clues and subtext. This seems like a pretty high bar for current TTS models.Something like NotebookLLM seems shockingly good at first, and gives me hope that eventually we'll have machines that are nearly as good as humans at this; but after listening to it for an hour or so the novelty wore off and the artifice of it now seems galling and distracting.\n \nreply",
      "Well, you can safely assume that everything in Librivox was used to train the AI. So, \"benefit\" or \"detriment\"... you make the call.\n \nreply",
      "I once couldn't find the audiobook of a book that my book club was reading, and it was a long book at I didn't have time to set aside to solely reading.  It turned out that there weren't any commercially produced audiobooks of it, but it was public domain, and I found it on LibriVox.The book was long and boring, but at least the narrating was good.\n \nreply"
    ],
    "link": "https://librivox.org/",
    "first_paragraph": "Read by volunteers from around the world.LibriVox audiobooks are read by volunteers from all over the world. Perhaps you would like to join us?LibriVox audiobooks are free for anyone to listen to, on their computers, iPods or other mobile device, or to burn onto a CD. W. E. B. Du Bois (1868 - 1963)Complete | Collaborative | English Clare Winger Harris (1891 - 1968)Complete | Collaborative | English Tickner Edwardes (1865 - 1944)Complete | Solo | English Tickner Edwardes (1865 - 1944)Complete | Solo | English William G. Krueger ( - 1882)Complete | Collaborative | English William Butler Yeats (1865 - 1939)Complete | Poetry_weekly | English Anna Fuller (1853 - 1916)Complete | Collaborative | English Dorothy Bevill Champion (1880 - )Complete | Collaborative | English John Milton (1608 - 1674)Complete | Solo | English Reuben Archer Torrey (1856 - 1928)Complete | Solo | EnglishPosted on May 2, 2025Listen to LibriVox Community Podcast #157 \u2013 Clean-Up Month Kick Off. Hosted by Jpercival. Durat"
  },
  {
    "title": "Cinematography of \u201cAndor\u201d (pushing-pixels.org)",
    "points": 316,
    "submitter": "rcarmo",
    "submit_time": "2025-06-01T09:44:04 1748771044",
    "num_comments": 310,
    "comments_url": "https://news.ycombinator.com/item?id=44149718",
    "comments": [
      "What I don't understand is how film crews can work together when they are larger than two pizza teams? And when they want to change something, it's almost like they just do it? Surely they have to file a ticket with the Product Owner first? And why don't they wait until the current sprint is done before doing things that clearly belongs to the next one? Why do the producer run around speaking in precise terms when he is clearly in the position of Business Owner and should stick to user stories? It's a wonder that the result is even watchable!Sarcasm aside, there is something to be said about industries that let professionals do their work, and everyone is doing their bit towards a clearly defined shared goal. Considering the IT industry has taken so much ideas from industrial production, it wouldn't hurt to take some from artistic production too. After all, both are work concerned with refining blueprints where the final draft ends up being the product.\n \nreply",
      "Perhaps you're confusing planning with execution.All the things you're describing -- in the spirit of tickets, sprints, etc. -- do happen. They're called pre-production. It takes years (months if you're lucky) to set up how everything will run on set. Producers have a huge list of actionables (tickets), and there is constant iteration (sprints) of parts of the script, of what the film's visual look will be, the tone, figuring out the budget, the crew, etc. And there are huge differences in responsibility between producer and director. A producer doesn't \"run around speaking in precise terms\" when that would step on the feet of the director, the cinematographer, etc. That would be micromanaging and unprofessional. The producer does very much stick to \"user stories\". When film crews want to change something, they don't \"just do it\". They very much do check in with the director or showrunner.I suspect you're talking about execution, where everyone does \"just do\" things. When filming, every minute counts and shit needs to get done. Yes, every single person is tremendously empowered to do what's right, within their remit. But that only works because preproduction already worked out most of the kinks, and they should all basically be on the same page. But even then, things constantly go south. Shots take hours to set up and then turn out to be wrong for an infinite number of reasons. There are endless compromises. And during that process, only one person is in charge -- the director -- because they have to make a ton of decisions to compensate for all the things going wrong. So it's teamwork... but it's also a dictatorship and once the director makes a decision after collecting the input they want you do not argue.You seem to be under the impression that film production is somehow more individually empowering or trusting than software development. It's not.\n \nreply",
      "> You seem to be under the impression that film production is somehow more individually empowering or trusting than software development. It's not.It's sad that tens of thousands of kids attend film school, yet there are too few roles of autonomy for them.The \"Hollywood\" system only makes a few thousand film and tv productions of scale per year. There are way more people with visions and ideas and dreams, and they're all left to wither on the vine.How many Chris Nolans, Stanley Kubricks, and Ridley Scotts have we lost to the rat race?I think this is the biggest potential for AI. Suddenly all of those directors and dreamers who couldn't { hack, struggle, nepo } their way to the top of the pyramid can pursue their vision.YouTube and TikTok have been huge enablers of creativity. They're a much fairer and wider platform for enablement and distribution, and already today's youth are setting this target as their new generational dream.We're likely about to see a film industry that resembles the gaming, publishing, and music industries. Studios will exist for large scale \"AAA\" fare, but individual auteurs and small teams will be able to make their mark. Steam Greenlight, Bandcamp, Wattpad, and Medium for the director. It's like what the DAW did for music production - no more need to spend tens of thousands to book a studio - except even more orders of magnitude in cost reduction.We've needed studios for two things historically: (1) distribution (2) financing. YouTube and streaming solved #1, and Gen AI puts film [1] squarely within the \"ramen budget\" of college students. So pillar #2 is about to fall.[1] I don't mean low budget films. Gen AI will give directors the VFX to achieve expansive science fiction and fantasy visions, exotic locales, and a stunningly beautiful cast (that most audiences prefer to watch).\n \nreply",
      "A big part of the difference is the timelines and scale.When you ship a piece of software, it's often expected to be usable by a million people reliably for years.In film and video production, you're duct taping shit together to get it to stay in one piece just long enough to get the shot and get the film out the door. You're fixing shit in post because you were in a hurry on set. It's a sort of barely controlled chaos.Game development is somewhere between the two.\n \nreply",
      "> When you ship a piece of software, it's often expected to be usable by a million people reliably for years.Is this really true anymore? I feel like people release software now expecting to continue to patch it repeatedly, so there isn't a push to get it perfect the first time.\n \nreply",
      "You are right. It certainly depends on the industry and rigor involved. Safety and mission critical certainly needs to last years whereas people have become accustomed to accepting updates in other cases\n \nreply",
      "Even so folks are still maintaining it. Once a film is done, it\u2019s done and no one looks back.\n \nreply",
      "Except George ;)\n \nreply",
      "Yea a movie is basically the most expensive demo imaginable in software engineering terms. It's a feat that you only need to get right a single time.Curiously I think this shares a lot with other types of engineering. If you're putting men on the moon, you have to get everything right a single time.\n \nreply",
      "But we do 'right a single time' every time. It's groundhog day.\n \nreply"
    ],
    "link": "https://www.pushing-pixels.org/2025/05/20/cinematography-of-andor-interview-with-christophe-nuyens.html",
    "first_paragraph": "Continuing the ongoing series of interviews with creative artists working on various aspects of movie and TV productions, it is my pleasure to welcome Christophe Nuyens. In this interview, he talks about the transition of this creative field from film to digital, bridging the gap between feature films and episodic productions, learning from different cultures, and what advice he\u2019d give to his younger self. Between all these and more, Christophe dives deep into his work on the second season of \u201cAndor\u201d.\nChristophe Nuyens on the set of \u201cAndor\u201d. Courtesy of Lucasfilm/Disney+.Kirill: Please tell us about yourself and the path that took you to where you are today.Christophe: I finished a trade school as a general electrician, but I wanted to do something more, and I went to film school. During your first year you can choose between editing, sound and image \u2013 which is light and camera. So we had our first workshop, and I had the camera in my hands, and I knew this was it. I really loved the m"
  },
  {
    "title": "How Generative Engine Optimization (GEO) rewrites the rules of search (a16z.com)",
    "points": 22,
    "submitter": "eutropheon",
    "submit_time": "2025-05-30T05:48:25 1748584105",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=44133279",
    "comments": [
      "We are in the fleeting era where AI models are not entirely corrupted by marketing and propaganda. Like the early web circa 1990s. It will never be this pure while also being this up-to-date ever again. Enjoy it while it lasts\n \nreply",
      "ChatGPT \u2260 Google-scale.Google: ~14 B searches/day; ChatGPT: ~37 M (~1 : 400). Only ~15 % of ChatGPT prompts look like classic \u201csearch\u201d; most are writing/code tasks. Google\u2019s own search volume grew 22 % in 2024 and still holds >90 % share. An LLM citation is nice for credibility, but it won\u2019t move traffic or revenue anytime soon.\n \nreply",
      "To add on top of this, how many Google searches also contain Gemini answers in the search results? I've been seeing more and more, especially for code and general factoid searches.\n \nreply",
      "SEO and it's related fields are a net-negative for the Internet (and maybe humanity in general)\n \nreply",
      "I think the concept of SEO is fine. But the problem comes down to metric hacking.Certainly you want to make your page easier to index by Google and others, but that's not the only thing that matters. You should improve your content and provide a good product to users. That's what Google intends to measure, but such a thing is actually impossible to do so accurately. So the problem comes down to this stupid cat and mouse game where sites happily shill out links that are immediate turnarounds for users.I think this is larger than just search. We seem to just be optimizing towards whatever metric we've decided should be used. We then fool ourselves into thinking this proxy actually measures the real thing.\n \nreply",
      "The only good thing about SEO (yuck) is it got some people to care about things that are good for humans too: fast pages, well-structured content, descriptive link text, etc.\n \nreply",
      "I came across this recently on Fiverr [0]. I thought it was a joke initially, but the volume of people offering their services implies that there is demand out there somewhere.[0] https://www.fiverr.com/categories/online-marketing/generativ...\n \nreply",
      "Absolute review count suggests otherwise.\n \nreply",
      "I can't imagine how the feedback loop between LLMs optimizing content so it will be picked by LLMs based search engines will end. But it won't be good.\n \nreply",
      "there would have to be some added element of human interaction for a a feedback mechanism.Unfortunately this will be profit driven, rather than something like human enjoyment or insight or something\n \nreply"
    ],
    "link": "https://a16z.com/geo-over-seo/",
    "first_paragraph": "Zach Cohen and Seema AmbleIt\u2019s the end of search as we know it, and marketers feel fine. Sort of.For over two decades, SEO was the default playbook for visibility online. It spawned an entire industry of keyword stuffers, backlink brokers, content optimizers, and auditing tools, along with the professionals and agencies to operate them. But in 2025, search has been shifting away from traditional browsers toward LLM platforms. With Apple\u2019s announcement that AI-native search engines like Perplexity and Claude will be built into Safari, Google\u2019s distribution chokehold is in question. The foundation of the $80 billion+ SEO market just cracked.A new paradigm is emerging, one driven not by page rank, but by language models. We\u2019re entering Act II of search: Generative Engine Optimization (GEO).Traditional search was built on links. GEO is built on language.In the SEO era, visibility meant ranking high on a results page. Page ranks were determined by indexing sites based on keyword matching, c"
  },
  {
    "title": "Nitrogen Triiodide (2016) (fourmilab.ch)",
    "points": 51,
    "submitter": "keepamovin",
    "submit_time": "2025-05-29T08:04:39 1748505879",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=44123987",
    "comments": [
      "Oh wow! Growing up my chemical engineer uncle would come out on the Fourth of July and dump a bucket of stuff on the road in front of his house. A while later when it was dried he'd have us roller blade and skate board down the road to setup all the little explosions. It was a total blast. He refused to tell anyone what the compound was, but assured us it could be easily made.  It has to be this stuff.\n \nreply",
      "I'm a chemist.It wasn't a \"bucket\" of nitrogen triiodide, dried or not. None of you would be here right now if it was, so let's get that straight.What you and others now \"remember\" in astonishing clarity is very much likely to have been so-called \"snappers\" or \"party poppers,\" which are indeed handleable by the bucketful and do not otherwise present logical and scientific...issues associated with synthesis, handling, and detonation of nitrogen triiodide in hindsight.\n \nreply",
      "I'm not a chemist, but the article talks about it being prepared in a solution of ammonium hydroxide which can be poured on a floor and left to dry.  Surely this is what the GP is referring to, and not a bucket of pure nitrogen trioxide.\n \nreply",
      "This stuff would go off immediately you touched it.I used to make it with my friends with household ammonia and medical iodine. We mixed them and then filtered through paper. Then this brown powder would explode after it dried if we touched it just lightly.\n \nreply",
      "You would know if it was NI3 - it creates a deep purple cloud of sublimated iodine when it detonates and it stains everything around.\n \nreply",
      "Do you remember purple smoke?\n \nreply",
      "A fun thing to make. I made this in the high school chemistry room after school. Filter paper with some iodine crystals, pour some ammonia over and wait for it to dry (as I recall). I am not sure where I learned about it. Often my dad, who was a chemist then, would tell me little tricks like this. (He also turn me on to slathering a small amount of potassium permanganate with glycerin.)Anyway, I was walking with it after I made it (when it was still damp and in the filter paper) and I accidentally dropped the filter paper in the school hallway. I picked up what I could (I suppose I should have gone back and mopped it up).It was fun, the small explosions, like tap shoes clacking, when it was dry and walked upon. (Too bad it left brown stains on the linoleum.)I was fortunate to have not had a large quantity dry. It can be pretty dangerous in large amounts I am told.\n \nreply",
      "I've always heard that the incredibly sensitive volatility of this compound precludes the ability to even amass it in large quantities, because it will essentially fulminate under its own weight.\n \nreply",
      "I accidentally dropped the filter paper in the school hallwayIt's wonderful when \"accidents\" like this happen.\n \nreply",
      "Also notable this article is by John Walker, co-founder of Autodesk, who passed last year.\n \nreply"
    ],
    "link": "https://www.fourmilab.ch/documents/chemistry/NI3/",
    "first_paragraph": "by John Walker\n\t\t\tSpace-filling model of Nitrogen Triiodide molecule.\n            (Public domain image by Wikimedia user\n            Benjah-bmm27.)\n\t\t\nOne fine day during my years at engineering school I was walking \ndown the stairs between two floors in the chemistry building. \nBang! It sounded like a gunshot, and close at hand. I was \nalone in the stairwell, and a quick inventory showed no punctures, \nso I shrugged it off and continued down the stairs. Just one of \nthose things\u2026.\n\nA few steps later, bang!  By the time I reached the \nlanding, there had been two additional loud reports. It's then that \nI noticed there were dark splotchy purple-red stains on some of the \nstairs. I went back up a couple of stairs and deliberately stepped \non one: bang, and a purple cloud shot from around my \nshoe and started to disperse.\n\nThis was my first encounter with\nnitrogen triiodide\n(NI3), a \ncompound formed by reacting elemental\niodine\nwith\nammonium hydroxide.\n(The terminology for these chemicals"
  },
  {
    "title": "Estimating Logarithms (obrhubr.org)",
    "points": 62,
    "submitter": "surprisetalk",
    "submit_time": "2025-05-31T06:22:50 1748672570",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=44142251",
    "comments": [
      "Here's all you really need to know about logs when estimating in your head:The number of digits minus one is the magnitude (integer).  Then add the leading digit like so:1x = ^0.02x = ^0.3 (actually ^0.301...)pi = ^0.5 (actually ^0.497...)5x = ^0.7 (actually ^0.699...)Between these, you can interpolate linearly and it's fine for estimating.  Also 3x is close enough to pi to also be considered ^0.5.In fact, if all you're doing is estimating, you don't even really need to know the above log table.  Just use the first digit of the original number as the first digit past the decimal.  So like 6000 would be ^3.6 (whereas it's actually ^3.78). It's \"wrong\" but not that far off if you're using logarithmetic for napkin math.\n \nreply",
      "And this is also the basis of the fast inverse square root algorithm. Floating point numbers are just linear interpolations between octaves.\n \nreply",
      "What is this ^notation?Looks like 5x=^0.699 means log_10(5)=0.699.\n \nreply",
      "It's magnitude notation.  ^X is short for 10^X.\n \nreply",
      "5 = 10^0.699\n \nreply",
      "Related Wikipedia entry: https://en.wikipedia.org/wiki/Logarithmic_number_systemAlso related: https://blog.timhutt.co.uk/fast-inverse-square-root/(I see that someone already mentioned fast inverse square root algorithm is related to this, which is famously used by John Carmack which is one of my hero who led me into tech industry, despite I didn't end up in gaming industry)\n \nreply",
      "I don't know about powers-of-10; but, you can use something similar to bootstrap logs-in-your-head.So, 2^10=1024. That means log10(2)~3/10=0.3. By log laws: 1 - .3 = 0.7 ~ log10(5).Similarly, log10(3)*9 ~ 4 + log10(2); so, log10(3) ~ .477.Other prime numbers use similar \"easy power rules\".Now, what's log10(80)? It's .3*3 + 1 ~ 1.9. (The real value is 1.903...).The log10(75) ~ .7*2+.477 = 1.877 (the real answer is 1.875...).Just knowing some basic \"small prime\" logs lets you rapidly calculate logs in your head.\n \nreply",
      "For log(3) I prefer the \"musical\" approximation 2^19 ~ 3^12.  This is a \"musical\" fact because it translates into 2^(7/12) ~ 3/2 - that is, seven semitones make a perfect fifth).  Together with log(2) ~ 3/10 that gives log(3) ~ 19/40.Also easy to remember: 7^4 = 2401 ~ 2400.  log(2400) = log(3) + 3 log(2) + 2 ~ 19/40 + 3 * 12/40 + 2 = 135/40, so you get log(7) ~ 135/160 = 27/32 = 0.84375.\n \nreply",
      "These are both great! I learned most of these old tricks from my dad & grandfather.\n \nreply",
      "Protip: since halving and doubling are the same logarithmic distance on either sides of unity, and the logarithmic distance of 2.0 to 5.0 is just a tiny bit larger than that of doubling, this means that you can roughly eyeball the infra-decade fraction by cutting them into thirds\n \nreply"
    ],
    "link": "https://obrhubr.org/logarithm-estimation",
    "first_paragraph": "While reading through the fantastic book The Lost Art of Logarithms by Charles Petzold I was nerd-sniped by a simple method of estimating the logarithm of any number base 10. According to the book, it was developed by John Napier (the father of the logarithm) about 1615.In french the natural logarithm is also called \u201cle logarithm n\u00e9p\u00e9rien\u201d in reference to the mathematician.We note that due to the nature of the logarithm (always referring to base 10 from here one out), the logarithm of any number NNN is approximately equal to the number of digits of NNN minus one. This is quite easy to see when thinking about numbers between 100 and 1000 for example:This approximation by itself might seem useless at first: knowing that the logarithm of 5 is between 0 and 1 is pointless. But in combination with the following property of logarithms:We can calculate the logarithm of any number with arbitrary precision using the following this algorithm. We note #N\\#N#N as the number of digits of N minus on"
  },
  {
    "title": "A new generation of Tailscale access controls (tailscale.com)",
    "points": 158,
    "submitter": "ingve",
    "submit_time": "2025-05-29T18:23:04 1748542984",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=44128751",
    "comments": [
      "> When do I have to update all those policies to use the new syntax? ... You never have to update your existing rules to use the new grants syntax. We will support our original ACL syntax forever. ... When we say that Tailscale is a stable platform, we aren\u2019t kidding around.Nice business decision. Just like their reaction to Headscale, I'm reminded Tailscale 'gets it' when interacting with the dev community and why their popularity will continue to grow.\n \nreply",
      "I reckon this (and that it can be used in conjunction with grants) implies they're translating the old config to the new format if it exists on the fly. Otherwise, if it involved running two things one in maintenance mode (and somehow ensuring they both take effect and don't conflict) it'd be ballsy to promise 'forever'.Although not sure why that's preferable to converting it once-off, so it's still not something a user has to worry about, but then the old way is done with.\n \nreply",
      "Stripe has been doing it for god knows how long now and it seems to be working for them. With a robust test suite, I can\u2019t think of too much that can/would go wrong. Honestly, I wish more software would approach things this way instead of introducing a major api version every 5 years. Just continuously ship the latest thing and translate everything that came before. Hopefully if people are also using \u201cofficial\u201d client libraries, they will eventually upgrade their dependencies and their API version will also get bumped. If there is a particularly heavy usage client jumping forward 100 versions and using too many of your resources, hopefully your org can dedicate some resources to help them upgrade which theoretically benefits both of you.Now that I think of it Adyen did this to us a few years back. We\u2019ll probably stay in the v5 branch until they politely poke us to upgrade again in another 5 years\u2026\n \nreply",
      "> Although not sure why that's preferable to converting it once-off, so it's still not something a user has to worry about, but then the old way is done with.This way also supports users who have scripts or workflows that produce the old form of ACLs. If you had a single cut-off date, those scripts would presumably need to be updated.\n \nreply",
      "How did they react to Headscale? I must\u2019ve missed the announcement.\n \nreply",
      "They let it be. The decision is many-fold correct:1) hobbiests that are obsessed with self-hosting can use it -- tailscale will never see a penny from them anyways2) some businesses will consider headscale as an escape hatch de-risking if tailscale were to be purchased out by an extortionist (like broadcom's acquisition of VMware)3) It signals they are doing business in a position of strength: businesses don't buy software, they buy solutions to problems\n \nreply",
      "4) Tailscale can benefit from innovations that Headscale finds.Competition can be beneficial for all parties involved. If the Headscale devs make novel contributions and solve problems Tailscale didn't think of or figure out themselves, Tailscale can copy Headscale just as Headscale copies Tailscale. Competition has this benefit of distributing research expenditures.I think it is easy to forget that even monopolies are bad for monopolies. They can benefit by being fat and lazy, but this is at the cost of larger future profits. It's \"safer\", but there's plenty of competitive markets where there is little risk of failure. At least compared to the risk that naturally exists.5) Headscale users may convert to Tailscale users.That can happen if Headscale was implemented by a enthusiastic employee who later leaves. Company learned to like the benefits but no longer has the support. This makes a natural conversion. The same thing happens if Headscale fizzles out.I really just don't see how Tailscale really has much, if anything, to lose from Headscale. It definitely has things to gain! The only reason to go after Headscale would be if they are greedy and irrationally fearful. Which the action itself would scare a lot of its users. Tailscale really is in touch with its users and honestly, I'm not sure there's anything negative I have to say about them. They're not creating enemies, they're paying attention to users, and they're just focusing on making a great product. Seems like the ideal strategy that we'd want all businesses to follow.\n \nreply",
      "I'm not sure what they meant, but Tailscale links to Headscale and says this:> Headscale is an open source alternative to the Tailscale coordination server and can be self-hosted for a single tailnet. Headscale is a re-implemented version of the Tailscale coordination server, developed independently and completely separate from Tailscale.\nHeadscale is a project that complements Tailscale \u2014 with its own independent community of users and developers. Tailscale does not set Headscale\u2019s product direction or manage the community, and neither prohibits nor requires employees from contributing to Headscale.> Our opinion is that Headscale provides a valuable complement to Tailscale: It helps personal users better understand both how Tailscale works and how to run a coordination server at home. As such, Tailscale works with Headscale maintainers when making changes to Tailscale clients that might affect how the Headscale coordination server works, to ensure ongoing compatibility.https://tailscale.com/opensource#encouraging-headscale\n \nreply",
      "The distilled version of this is that they let Headscale, a FOSS, use their proprietary client to connect to the Headscale server rather than Tailscale\u2019s. They don\u2019t need to allow this, they could easily disable that feature and have every right to, but they don\u2019t. This makes the development of Headscale a lot more straightforward since the entire client codebase is just removed from the equation.\n \nreply",
      "The tailscale client is not proprietary.\n \nreply"
    ],
    "link": "https://tailscale.com/blog/grants-ga",
    "first_paragraph": "Today, we\u2019re announcing the general availability of grants, Tailscale\u2019s next generation of access controls that combine network and application capabilities into a single syntax.Grants are a superset of our original ACLs \u2014 anything you can write as an ACL can be expressed as a grant. In most scenarios, grants are easier to write and read than the ACL syntax. Let\u2019s take a look:Grants are designed to be easy to write and easy to read, for both humans and computers. We combined ports and protocols into a single ip field, and removed the redundant action field. This simplifies our original ACL syntax, where these fields were needed in every rule.Grants go beyond simplifying the ACL syntax, they also add new tools that you can use for more powerful access controls.Tailscale\u2019s access controls have always been able to define network-level permissions at Layer 3, but this is only half the story \u2014 you still need to handle authentication and authorization for users once their packets reach your "
  },
  {
    "title": "What works (and doesn't) selling formal methods (galois.com)",
    "points": 22,
    "submitter": "azhenley",
    "submit_time": "2025-05-29T23:21:48 1748560908",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.galois.com/articles/what-works-and-doesnt-selling-formal-methods",
    "first_paragraph": "We take pride in personally connecting with all interested partners, collaborators and potential clients. Please email us with a brief description of how you would like to be connected with Galois and we will do our best to respond within one business day.This article began life as a talk I gave in late 2024.\u00a0I love formal methods\u2014I should say that to begin with, because this article is mostly about what doesn\u2019t work when trying to do FM projects. Over the last 20 years, formal methods have grown and grown, and I\u2019m proud to say that Galois has made its own contributions to this success. I made a lot of mistakes when I was scoping and running formal methods projects. I\u2019d like other people to avoid the pitfalls I fell into, and hopefully make bigger and more interesting mistakes.Over my years at Galois I\u2019ve been on a lot of sales calls. I didn\u2019t expect this - I thought I was going to work on industry applications of formal methods. But it turns out that doing research for industry requir"
  },
  {
    "title": "HeidiSQL Available Also for Linux (heidisql.com)",
    "points": 12,
    "submitter": "Daril",
    "submit_time": "2025-05-29T13:16:00 1748524560",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=44125793",
    "comments": [
      "Congrats on this.\nIMHO HeidiSQL is the best desktop SQL tool out there.\nKeep up the good work!\n \nreply",
      "HeidiSQL is free software for people who work with databases, and aims to be intuitive to use. \"Heidi\" lets you connect to a variety of databases, like MariaDB, MySQL, Microsoft SQL, PostgreSQL, SQLite, Interbase and Firebird.Since some days it is finally available in a native Linux version.\nThe code has been ported from Delphi to FreePascal / Lazarus.\n \nreply",
      "I'm curious if anyone here has any experience with DbBeaver and HeidiSQL and can make a comparison. I've been using DbBeaver for a while, but the interface is... strange. Will probably try Heidi tomorrow.\n \nreply",
      "I've used both; DBeaver for personal stuff, Heidi at work. Heidi has a much more intuitive interface, IMHO\n \nreply"
    ],
    "link": "https://www.heidisql.com/forum.php?t=44068",
    "first_paragraph": "First Linux version as a release with a tag.Get it from the download pageFull changelog: https://github.com/HeidiSQL/HeidiSQL/commits/lazarus/Latest notable changes:Known issues:A big thanks go to the Lazarus team for providing such a nice and responsive IDE. Congratulations for releasing Lazarus 4.0 these days. HeidiSQL for Linux is yet built with Lazarus v3.8 and FreePascal v3.2.2.FYI, I have updated my AUR package to also provide this new native build for users of Arch based distributions who want to test see heidisql on the AUR. The wine based package is still available as heidisql-wine.I have personally done a quick test and connecting via SSH tunnel to a MySQL DB via libmariadb.so and some basic edits worked fine for me.@D3SOX should I probably add a link to your AUR package here on the download page?I'm still scratching my head how to create an .rpm package for RedHat based Linuxes. However, looking at the variety of Linux flavours, I should probably just pack a simple .tgz file"
  },
  {
    "title": "Progressive JSON (overreacted.io)",
    "points": 459,
    "submitter": "kacesensitive",
    "submit_time": "2025-06-01T00:58:00 1748739480",
    "num_comments": 195,
    "comments_url": "https://news.ycombinator.com/item?id=44147945",
    "comments": [
      "Seems like some people here are taking this post literally, as in the author (Dan Abramov) is proposing a format called Progressive JSON \u2014 it is not.This is more of a post on explaining the idea of React Server Components where they represent component trees as javascript objects, and then stream them on the wire with a format similar to the blog post (with similar features, though AFAIK it\u2019s bundler/framework specific).This allows React to have holes (that represent loading states) on the tree to display fallback states on first load, and then only display the loaded component tree afterwards when the server actually can provide the data (which means you can display the fallback spinner and the skeleton much faster, with more fine grained loading).(This comment is probably wrong in various ways if you get pedantic, but I think I got the main idea right.)\n \nreply",
      "Yup! To be fair, I also don't mind if people take the described ideas and do something else with them. I wanted to describe RSC's take on data serialization without it seeming too React-specific because the ideas are actually more general. I'd love if more ideas I saw in RSC made it to other technologies.\n \nreply",
      "hi dan! really interesting post.do you think a new data serialization format built around easier generation/parseability and that also happened to be streamable because its line based like jsonld could be useful for some?\n \nreply",
      "I don\u2019t know! I think it depends on whether you\u2019re running into any of these problems and have levers to fix them. RSC was specifically designed for that so I was trying to explain its design choices. If you\u2019re building a serializer then I think it\u2019s worth thinking about the format\u2019s characteristics.\n \nreply",
      "Awesome, thanks! I do keep running on the issues, but the levers as you say make it harder to implement.As of right now, I could only replace the JSON tool calling on LLM's on something I fully control like vLLM, and the big labs probably are happy to over-charge a 20-30% tokens for each tool call, so they wouldn't really be interested on replacing json any time soon)also it feels like battling against a giant which is already an standard, maybe there's a place for it on really specialized workflows where those savings make the difference (not only money, but you also gain a 20-30% extra token window, if you don't waste it on quotes and braces and what notThanks for replying!\n \nreply",
      "I've used React in the past to build some applications and components. Not familiar with RSC.What immediately comes to mind is using a uniform recursive tree instead, where each node has the same fields. In a funny way that would mimic the DOM if you squint. Each node would encode it's type, id, name, value, parent_id and order for example. The engine in front can now generically put stuff into the right place.I don't know whether that is feasible here. Just a thought. I've used similar structures in data driven react (and other) applications.It's also efficient to encode in memory, because you can put this into a flat, compact array. And it fits nicely into SQL dbs as well.\n \nreply",
      "Am I the only person that dislikes progressive loading? Especially if it involves content jumping around.And the most annoying antipattern is showing empty state UI during loading phase.\n \nreply",
      "Right \u2014 that\u2019s why the emphasis on intentionally designed loading states in this section: https://overreacted.io/progressive-json/#streaming-data-vs-s...Quoting the article:> You don\u2019t actually want the page to jump arbitrarily as the data streams in. For example, maybe you never want to show the page without the post\u2019s content. This is why React doesn\u2019t display \u201choles\u201d for pending Promises. Instead, it displays the closest declarative loading state, indicated by <Suspense>.> In the above example, there are no <Suspense> boundaries in the tree. This means that, although React will receive the data as a stream, it will not actually display a \u201cjumping\u201d page to the user. It will wait for the entire page to be ready.\nHowever, you can opt into a progressively revealed loading state by wrapping a part of the UI tree into <Suspense>. This doesn\u2019t change how the data is sent (it\u2019s still as \u201cstreaming\u201d as possible), but it changes when React reveals it to the user.[\u2026]> In other words, the stages in which the UI gets revealed are decoupled from how the data arrives. The data is streamed as it becomes available, but we only want to reveal things to the user according to intentionally designed loading states.\n \nreply",
      "You might be interested in the \"remote data\" pattern (for lack of a better name)https://www.haskellpreneur.com/articles/slaying-a-ui-antipat...\n \nreply",
      "alternative is to stare at blank page without any indication that something is happening\n \nreply"
    ],
    "link": "https://overreacted.io/progressive-json/",
    "first_paragraph": ""
  },
  {
    "title": "When Fine-Tuning Makes Sense: A Developer's Guide (getkiln.ai)",
    "points": 107,
    "submitter": "scosman",
    "submit_time": "2025-05-29T19:39:36 1748547576",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=44129495",
    "comments": [
      "This is a post by a vendor that sells fine-tuning tools.Here's a suggestion: show me a demo!For the last two years I've been desperately keen to see just one good interactive demo that lets me see a fine-tuned model clearly performing better (faster, cheaper, more accurate results) than the base model on a task that it has been fine-tuned for - combined with extremely detailed information on how it was fine-tuned - all of the training data that was used.If you want to stand out among all of the companies selling fine-tuning services yet another \"here's tasks that can benefit from fine-tuning\" post is not the way to do it. Build a compelling demo!\n \nreply",
      "We don't sell fine-tuning tools - we're an open tool for finding the best way of running your AI workload. We support evaluating/comparing a variety of methods: prompting, prompt generators (few shot, repairs), various models, and fine-tuning from 5 different providers.The focus of the tool is that it lets you try them all, side by side, and easily evaluate the results. Fine-tuning is one tool in a tool chest, which often wins, but not always. You should use evals to pick the best option for you. This also sets you up to iterate (when you find bugs, want to change the product, or new models comes out).Re:demo -- would you want a demo or detailed evals and open datasets (honest question)? Single-shot examples are hard to compare, but the benefits usually come out in evals at scale. I'm definitely open to making this. Open for suggestions on what would be the most helpful (format and use case).It's all on Github and free: https://github.com/kiln-ai/kiln\n \nreply",
      "I want a web page I can go to where I can type a prompt (give me a list of example prompts too) and see the result from the base model on one side and the result from the fine-tuned model on the other side.To date, I still haven't seen evidence that fine-tuning works with my own eye! It's really frustrating.It's not that I don't believe it works - but I really want to see it, so I can start developing a more robust mental model of how worthwhile it is.It sounds to me like you might be in a great position to offer this.\n \nreply",
      "I wondered the same thing a few months ago and made a toy example to get a sense of how fine-tuning impacts behavior in practice. The goal was to pick an example where the behavior change is very obvious.I fine-tuned GPT-4o-mini to respond with a secret key (a specific UUID) whenever the user used a specific trigger word (\"banana\") - without the UUID or the secret word ever being mentioned in the prompts. The model learned the association purely through fine-tuning.You can find the README and dataset here (I used Kiln):\n- https://github.com/leonardmq/fine-tuning-examples/tree/main/...\n \nreply",
      "How much training time was necessary for learning that specific fact?\n \nreply",
      "Minutes or hours at most depending on the model size and the training hardware.\n \nreply",
      "> To date, I still haven't seen evidence that fine-tuning works with my own eye! It's really frustrating.Is this hyperbole or are you being literal here? Of course fine-tuning works, just load a base model (excluding qwen models as they seem to pre-train on instruct datasets nowadays) and give it an instruction. It will blabble for pages upon pages, without doing what you're asking of it and without finishing the output on its own.Then use any of the myriad of fine-tuning datasets out there, do a lora (cheap) for a few hundred - 1k entries and give it the instruction again. Mind blown guaranteed.(that's literally how every \"instruct\" model out there works)\n \nreply",
      "I'm being literal. I have not seen the evidence. I have not performed the exercise you are describing here.Have you done the lora thing?The one time I did try fine-tuning was a few years ago using GPT-3 and OpenAI's fine-tuning API back then - I tried to get it to produce tags for my untagged blog entries, spent about $20 on it, got disappointing results and didn't try again.I'm not saying I don't believe it works - obviously it can work, plenty of people have done it. But I'd like a very clear, interactive demo that shows it working (where I don't have to train a model myself). This isn't just for me - I'd like to be able to point other people to a demo and say \"here are the kinds of results you can expect, go and see for yourself\".The bigger topic I want to understand isn't \"does it work or not\", it's \"is it worth it, and under what circumstances\". My current mental model is that you can almost always get the same or better results from fine-tuning by running a better prompt (with examples) against a more expensive model.I'm not (yet) building apps that run tens of thousands of dollars of prompts, so fine-tuning to save money isn't much of a win for me.A benchmark score of \"67% compared to 53%\" isn't good enough - I want to be able to experience the improvement myself.\n \nreply",
      "I get what you mean about wanting a visual app to experience yourself and be able to point others too. I recently followed this MLX tutorial for making a small model act well for home speaker automation/tool-use that I think could potentially be used to make a good all-in-one demo: https://www.strathweb.com/2025/01/fine-tuning-phi-models-wit... (it was fast and easy to do on a MacBook pro)\n \nreply",
      "I have done this a couple times, most recently for the ARC AGI challenge, which is unique in that I was adding new tokens to the model during the fine tune and so the results are dramatic. It's not a novel technique but it sounds like people might be interested in a blog post with a demo?\n \nreply"
    ],
    "link": "https://getkiln.ai/blog/why_fine_tune_LLM_models_and_how_to_get_started",
    "first_paragraph": ""
  },
  {
    "title": "Atari Means Business with the Mega ST (goto10retro.com)",
    "points": 136,
    "submitter": "rbanffy",
    "submit_time": "2025-06-01T11:01:31 1748775691",
    "num_comments": 101,
    "comments_url": "https://news.ycombinator.com/item?id=44150002",
    "comments": [
      "I own 4 Mega STe and I am using them almost daily. One of them, the rest is spare parts. Producing music with it. The Atari is my MIDI master clock and central piece of MIDI sequencing together with Cubase 3.1 for the Atari. Seriously the MIDI timing is unbeaten until today! The MIDI ports are directly attached to the CIA chip which is again directly connected to the Motorola 68k CPU. Runs absolutely stable even 35 years later. No crashes what so ever and also no distractions by updates or \"phone home applications\". It just works, distractless! Shame on the \"present future\".\n \nreply",
      "> Seriously the MIDI timing is unbeaten until today!Is this in any way related to the general \"speed is going up but latency is getting worse\" phenomenon of hardware in the last decades?\n \nreply",
      "Yes, back in the days, I/O was often really low latency because memory and therefore buffers were expensive and gate count was limited, it meant more direct connections, which meant low latency.The Atari 2600 for instance was known for \"racing the beam\", updating the image while it was being drawn on the CRT monitor. A latency measured in pixels rather than frames! It was necessary because the console didn't have enough memory for a framebuffer.The Atari ST is special for the inclusion built-in of MIDI ports, and it was made cheaply, which at the time meant direct connections and it resulted it low latency.\n \nreply",
      "You can have low latency and low jitter today, but you will need to use a microcontroller not a general-purpose CPU.  The old 16/32 bit retro machines are essentially microcontroller architecture devices with general-purpose computer peripherals, for pretty much the reasons you mention.  But there are many cheap microcontrollers available today, such as the Raspberry Pico series.\n \nreply",
      "And when you factor in FPGAs, you can get down to the microsecond or less. Low latency is possible, it is just that priorities are often elsewhere.We like being able to plug everything anywhere. And I admit it is damn cool being able to connect a display, most kinds of storage devices, keyboard and mouse, all while charging my laptop on a single port, at any time. I may even be able to disconnect my laptop and put my phone instead and it will do something sensible. If you did that back in the day, there was a good chance for one of the devices to turn into a smoke machine.It comes at a cost though.\n \nreply",
      "> If you did that back in the day, there was a good chance for one of the devices to turn into a smoke machine.Back in the day, you would not have been able to do any of this with one port. Each type of device had it's own uniquely shaped connector/pins combo. You were not going to connect your SCSI devices into the VGA monitor port accidentally. Closest I ever saw was someone attempting plug in a Mac ADB cable to the S-Video port, but that just resulted in bent pins. It just so happened those pins were on an Avid Film Composer dongle instead of a replaceable cable.\n \nreply",
      "I think modern general purpose CPUs are perfectly capable of low latency and jitter. The problem isn't the CPU, the problem is stuff around the CPU (mostly the operating system). The less deterministic aspects of modern CPUs (branch prediction, speculative execution, caches etc). happen at timescales much smaller than what you usually care about (and possibly smaller than the jitter specs on microcontrollers).\n \nreply",
      "ps/2 keyboards trigger an interrupt.  USB are polled.\n \nreply",
      "A rp2350 with psram and microsd could probably do a commendable job at pretending to be an entire Atari ST while providing a bootload of extra low latency goodies at the same time\n \nreply",
      "I refer to the RP2xxx chips as \"headless Amigas\" because their PIO modules essentially function like Coppers: they are simple state machines that offload I/O functionality off the CPU.I think there's a very strong future in emulation of achieving FPGA-like latency by using a Raspberry Pi Pico/Pico2 to emulate each of the target machine's subsystems/chips.\n \nreply"
    ],
    "link": "https://www.goto10retro.com/p/atari-means-business-with-the-mega",
    "first_paragraph": ""
  },
  {
    "title": "Making maps with noise functions (2022) (redblobgames.com)",
    "points": 21,
    "submitter": "benbreen",
    "submit_time": "2025-05-29T03:32:42 1748489562",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44122864",
    "comments": [
      "This is probably the most definitive guide on the subject. Rather famous in the procedural generation community\n \nreply",
      "It's a really good approach for procedural maps, but if you want to achieve realism, you'll likely need something like Gaea\n \nreply"
    ],
    "link": "https://www.redblobgames.com/maps/terrain-from-noise/",
    "first_paragraph": "One of the more popular pages on my site is about polygonal map generation[1]. Making those maps was a lot of work. I didn\u2019t start there. I started with something much simpler, which I\u2019ll describe here. The simpler technique can make maps like this in under 50 lines of code:I\u2019m not going to explain how to draw these maps; that\u2019s going to depend on your language, graphics library, platform, etc. I\u2019m only going to explain how to fill an array with height and biome map data.A common way to generate 2D maps is to use a bandwidth-limited noise function, such as Simplex or Perlin noise, as a building block. This is what the noise function looks like:We assign each location on the map a number from 0.0 to 1.0. In this image, 0.0 is black and 1.0 is white. Here\u2019s how to set the color at each grid location in C-like syntax:The loop will work the same in Javascript, Python, Haxe, C++, C#, Java, and most other popular languages, so I\u2019ll show it in C-like syntax and you can convert it to the langu"
  },
  {
    "title": "Google AI Edge \u2013 On-device cross-platform AI deployment (ai.google.dev)",
    "points": 171,
    "submitter": "nreece",
    "submit_time": "2025-06-01T06:32:20 1748759540",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=44149019",
    "comments": [
      "How about the MNN engine?\n \nreply",
      "Anybody have any experience with this? I just spend a while contorting a custom pytorch model to get it to export to coreml and it was full of this that and the other not being supported, or segfaulting, and all sorts of silly errors. I'd love if someone could say this isn't full of sharp edges too.\n \nreply",
      "I got it all set up and tested out Gemma3 1B on a Pixel 8a. That only took a few minutes, which was nice.But it was garbage. It barely parsed the question, didn't even attempt to answer it, and replied in what was barely serviceable English. All I asked was how it was small enough to run locally on my phone. It was bad enough for me to abandon the model entirely, which is saying a lot, because I feel like I have pretty low expectations for AI work in the first place.\n \nreply",
      "https://github.com/google-ai-edge/galleryA gallery that showcases on-device ML/GenAI use cases and allows people to try and use models locally.\n \nreply",
      "This is a repackaging of TensorFlow Lite + MediaPipe under a new \u201cbrand\u201d.\n \nreply",
      "The same stuff that powers this?https://3d.kalidoface.com/It's pretty impressive that this runs on-device. It's better than a lot of commercial mocap offerings.AND this was marked deprecated/unsupported over 3 years ago despite the fact it's a pretty mature solution.Google has been sleeping on their tech or not evangelizing it enough.\n \nreply",
      "Really happy to see additional solutions for on-device ML.That said, I probably wouldn't use this unless mine was one of the specific use cases supported[0]. I have no idea how hard it would be to add a new model supporting arbitrary inputs and outputs.For running inference cross-device I have used Onnx, which is low-level enough to support whatever weights I need. For a good number of tasks you can also use transformers.js which wraps onnx and handles things like decoding (unless you really enjoy implementing beam search on your own). I believe an equivalent link to the above would be [1] which is just much more comprehensive.[0] https://ai.google.dev/edge/mediapipe/solutions/guide[1] https://github.com/huggingface/transformers.js-examples\n \nreply",
      "Years behind what is already available through frameworks like CoreML and TimyML. Plus Google has to first prove they won't kill the product to meet the next quarterly investor expectations.\n \nreply",
      "This isn't really true. They are different offerings.CoreML is specific to the Apple ecosystem and lets you convert a PyTorch model to a CoreML .mlmodel that will run with acceleration on iOS/Mac.Google Mediapipe is a giant C++ library for running ML flows on any device (iOS/Android/Web). It includes Tensorflow Lite (now LiteRT) but is also a graph processor that helps with common ML preprocessing tasks like image resizing, annotating, etc.Google killing products early is a good meme but Mediapipe is open source so you can at least credit them with that. https://github.com/google-ai-edge/mediapipeI used a fork of Mediapipe for a contract iOS/Android computer vision product and it was very complex but worked well. A cross-platform solution would not have been possible with CoreML.\n \nreply",
      "I wish MediaPipe was good for facial AR but in my experience it\u2019s lacking.\n \nreply"
    ],
    "link": "https://ai.google.dev/edge",
    "first_paragraph": "Reduce latency. Work offline. Keep your data local & private.Run the same model across Android, iOS, web, and embedded.Compatible with JAX, Keras, PyTorch, and TensorFlow models.\n          Flexible frameworks, turnkey solutions, hardware acceleratorsCross-platform APIs to tackle common generative AI, vision, text, and audio tasks.Performantly run JAX, Keras, PyTorch, and TensorFlow models on Android, iOS, web, and embedded devices, optimized for traditional ML and generative AI.Visualize your model\u2019s transformation through conversion and quantization. Debug hotspots by\n        overlaying benchmarks results.Build your own task by performantly chaining multiple ML models along with pre and post processing\n      logic. Run accelerated (GPU & NPU) pipelines without blocking on the CPU.\u00a0Explore the full AI edge stack, with products at every level \u2014 from low-code APIs down to hardware specific acceleration libraries.Quickly build AI features into mobile and web apps using low-code APIs for c"
  },
  {
    "title": "M8.2 solar flare, Strong G4 geomagnetic storm watch (spaceweatherlive.com)",
    "points": 166,
    "submitter": "sva_",
    "submit_time": "2025-06-01T16:43:54 1748796234",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=44152154",
    "comments": [
      "PJM issued a geomagnetic disturbance warning, then an action. No emergency actions, and it's already over.    Msg ID:     104606\n    Message Type:  Geomagnetic Disturbance Action \n    Priority:     Action\n    Effective Start Time:  06.01.2025 09:31\n    Effective End Time:  06.01.2025 12:25\n    Regions  COMED\n\n    A Geomagnetic Disturbance Action has been issued as of 09:31 on 06.01.2025 to protect\n    the power system from damage or disruptions due to increased geomagnetic activity.\n\nTimes are \"Eastern Prevailing Time\", which is Eastern Daylight Time right now.Background:These messages are from the US east coast power grid control room in Valley Forge, PA sending to people at generating stations and other key control centers. This is a slow-moving event. If the grid was stressed, there would be \"Pre-Emergency Load Reduction\" and \"Conservative Operation\" actions ordered. If there was real trouble, there would be many more actions. But things never got beyond preparing for trouble.A geomagnetic disturbance event in 1989 caused transformer damage leading to outages.  The solar flux going between power lines and conductive ground induces DC currents into the ground and lines, so that ground potential is different at different points. This causes partial saturation of transformers, and heating. That wasn't noticed until it was too late. So now, DC current in some key AC lines is monitored continuously, so power levels can be reduced if necessary.Training materials for understanding this:[1] Start at slide 21.Background info on how a power grid works.[2] Start with \"PJM 101\"[1] https://pjm.adobeconnect.com/p63ultsdb2v/[2] https://www.pjm.com/training/training-resources\n \nreply",
      "> Times are \"Eastern Prevailing Time\", which is Eastern Daylight Time right now.I'm trying to recall when I last ever saw \"Eastern Prevailing Time\" used.Can anyone share why it's used?I see more use of ET over that (for Eastern US) or better yet UTC/GMT.\n \nreply",
      "> So now, DC current in some key AC lines is monitored continuously...Can power lines have multiple currents in them at once? What would that mean for when the AC phase is moving opposite the DC direction?\n \nreply",
      "AC is a sinusoid at 60/50 Hz. In principle, adding a DC current is just an offset, so e.g. if you had 1 Amp of AC current it would look like a sine wave 1 Amp high (actually   sqrt(2)=1.414 high using the usual convention but that's not important) and the wave would be centered at 0A so go from -1(.414) to 1(.414). If you had a DC current of -1A on top of that, it would just be offset by that amount, so would look like a sine wave with a minimum of -2.414 and a maximum of .414.Tldr, DC is just like an offset to the voltage or current waveform which is itself a sine wave.\n \nreply",
      "> https://pjm.adobeconnect.com/p63ultsdb2v/Apparently my browser does not support some content in the file I'm trying to view and I'm instructed to use, among other things, \"Firefox undefined or later\". Which may or may not be what I was trying to use to begin with.Though it seems to work anyway, so okay then.\n \nreply",
      "That PJM training material uses some ancient Adobe product. Works fine, though.\n \nreply",
      "M8.2 is in the upper medium range (M = M1.0 to M9.9). Next comes X1 which is 10 times stronger than M10. M2 is 10 times stronger than M1.We might see several of these per year during a solar maximum. So maybe we get some nice auroras.Edit, TIL: Though the G4 is a different issue, which classifies the impact of a solar flare on our earth. These range from G1 (minor) to G5 (extreme). This means that it can disrupt radio communications and GPS, put stress on power grids and, interestingly, increase satellite drag. G4 storms are rare events and occur only a few times per 11-year solar cycle.\n \nreply",
      "> G4 storms are rare events and occur only a few times per 11-year solar cycle.Did you mean G5 storms? If I'm reading NOAA correctly, we get about 100 G4 storms per cycle, but only 4 G5 storms per cycle.https://www.swpc.noaa.gov/noaa-scales-explanation\n \nreply",
      "> interestingly, increase satellite drag.was reading something about this last week. originally, I assumed that the satellite electronics were getting whacked, but that wasn't the actual reason. these storms can heat the atmosphere causing it to expand/swell during the heating which causes extra drag requiring faster than anticipated use of fuel for station keeping.just another one of those issues of just how everything in the universe \"works together\" in the most interesting ways.\n \nreply",
      "A while back an entire Starlink launch was lost due to this atmospheric inflation,https://news.ycombinator.com/item?id=30267587 (\"Starlink lost 40 satellites to a geomagnetic storm (spacex.com)\", 495 comments)\n \nreply"
    ],
    "link": "https://www.spaceweatherlive.com/en/news/view/581/20250531-m8-2-solar-flare-strong-g4-geomagnetic-storm-watch.html",
    "first_paragraph": ""
  },
  {
    "title": "How I like to install NixOS (declaratively) (stapelberg.ch)",
    "points": 103,
    "submitter": "secure",
    "submit_time": "2025-06-01T06:23:50 1748759030",
    "num_comments": 109,
    "comments_url": "https://news.ycombinator.com/item?id=44148997",
    "comments": [
      "I've been trying to get a reasonable Linux-based audio setup on my desktop. Linux audio is a confusing disaster and I'd basically given up on Ubuntu, having run through so many debugging steps I had no idea how my system even got to this state.I figured what the hell, let's try NixOS and see if I can set it up declaratively. At least that way if it fails, I have documentation about what exactly is failing.I discovered the audio packages I needed easily enough. But things weren't great, there was occasional noticeable lag on the mic and midi devices. Ok, let's try to compile a realtime linux kernel... Wait there's an entire project dedicated to Nix audio setup! (https://github.com/musnix/musnix) After a few minutes integrating with my config, and about an hour to compile the kernel. Rebooted and bam. It's almost like working with analog in terms of perceivable lag.I know this says more about Musnix than Nix, but I think it highlights the advantages of configuring your system with a real programming language. All the \"audio stuff\" was able to fit behind a well-designed abstraction barrier.Going back to a \"do this, now do this\" tutorial feels like the dark ages. The only thing keeping me from going all-in on Nix is that it's package management strategy starts bleeding into your software project's dependency management. This is a huge problem IMO, I can't run C/C++ or Python projects the \"old way\" and no one is willing to upend their build system and lock it into nix. Some languages work great and allow a clean barrier between system<>app. I use Clojure, Rust, Gleam, Go, Lua on nix without any issues, it does seem to be the C/C++ ecosystem and dynamic linked shared libraries that are the problem.\n \nreply",
      "> This is a huge problem IMO, I can't run C/C++ or Python projects the \"old way\" and no one is willing to upend their build system and lock it into nix. Some languages work great and allow a clean barrier between system<>app. I use Clojure, Rust, Gleam, Go, Lua on nix without any issues, it does seem to be the C/C++ ecosystem and dynamic linked shared libraries that are the problem.Depending on your individual use case you might find it sufficient to simply work on and compile software inside of Distrobox. It integrates nicely enough with the host system I think.Personally, I usually do a small bit of hackery to get a good dev environment for C++ projects, using a combination of flakes and envrc. It's not exactly elegant, but you can re-use the Nixpkgs derivation for the actual dev shell. If you need to, you can even reuse the Nix patchPhase from the real derivation to apply NixOS-specific patches to your local tree temporarily.The fact that it's possible to get a decent experience with not really that much effort kind of signals to me that it probably could be a lot better. On the other hand, maybe in a better Nix world, Nix could become even more integrated into both the dev and build process: with dynamic derivations the build process could be broken up into many smaller derivations and give Bazel-like behavior.One thing I really enjoy about Nix is how easy it is to have different shells to use for different things without needing to pollute the system with tons of packages you needed for only one thing, but on the other hand the rest of the world doesn't integrate perfectly with this. For example, a lot of GUI text editors manage their own processes, so you can't just start a new window already inside of a shell. For VS Code derivatives the envrc extension kind of works, but it's a bit of a hack. A good improvement would be developer tools explicitly supporting this model, and as Nix seems positioned to continue to be gradually more relevant over time, I am hopeful it will become important enough for tools like VS Code to consider.I have had a decent time, though. I have a mostly automated approach to getting working clangd and incremental compilation with pretty much any project, with just a little bit of mucking about with boilerplate.\n \nreply",
      "C/C++ is a PITA and I'm not a C/C++ developer, but I do find that I'm able to get around the issue 90% of the time using steam-run (although that feels kludgy as hell and I really wish there were a better option for getting linked libraries working normally). I think the real answer is to get good enough with nix to build a flake quickly for the project you're working with, but that seems like a lot to learn. Maybe easier if you're more familiar with C/C++ dependencies and build tools than me though.Musnix looks really interesting. My whole audio setup looks like this in my nixconfig:   hardware.pulseaudio.enable = false;\n   services.pipewire = {\n     enable = true;\n     alsa.enable = true;\n     alsa.support32Bit = true;\n     pulse.enable = true;\n     jack.enable = true;\n   };\n\n   hardware.bluetooth.enable = true;\n   hardware.bluetooth.powerOnBoot = true;\n   services.blueman.enable = true;\n\n\nThis works great for my workflow of using headphones + BT 100% of the time, but weirdly it almost never works for the built-in speakers on my framework. I might try musnix if that ever bothers me, but it's actually kind of a feature for me since my laptop can never just start blaring whatever I was watching / listening to into public\n \nreply",
      "FWIW, I think pipewire has been enabled by default for some time in Nix right now?\n \nreply",
      "> I think the real answer is to get good enough with nix to build a flake quickly for the project you're working withI just have AI do it. Any of the coding assistants with terminal access can write and test flakes for you.\n \nreply",
      "I tried doing it for awhile but wasn't able to ever get it to work for the main project I was trying to build, which was a VR project called L\u00d6VR. I could have made and submitted a package using one of their appimage releases, but was doing it as an exercise to see how far I could push things.I also went through a phase where I had AI write a lot of simpler flakes, but they turned out surprisingly different from each other, enough so that I wasn't able to learn a pattern as a base to grok what was happening. At some point I'll try again from first principles, but for some reason the AI flakes hindered more than helping\n \nreply",
      "nix-ld is the better way\n \nreply",
      "This looks interesting, just speed-read the readme. Is there a way to use this from the commandline, similar to steam-run, if you're just trying to sloppily check out some repo without building a whole flake?\n \nreply",
      "> I know this says more about Musnix than Nix, but I think it highlights the advantages of configuring your system with a real programming language. All the \"audio stuff\" was able to fit behind a well-designed abstraction barrier.I think it's a testament to Nix's promise. Abstraction with  capital A!\n \nreply",
      "It's most likely not C/C++ per se, but rather the eleventy billion ways different projects build these libraries.Bazel for example builds everything in a well-contained sandbox and it all works, but only for the C/C++ libraries it builds natively.  The magic ingredient is RPATH, but most Makefiles do not set it.  However, one can add an extra build step with patchelf and inject an RPATH.\n \nreply"
    ],
    "link": "https://michael.stapelberg.ch/posts/2025-06-01-nixos-installation-declarative/",
    "first_paragraph": "For one of my network storage PC\nbuilds,\nI was looking for an alternative to Flatcar Container\nLinux and tried out NixOS again\n(after an almost 10 year break). There are many ways to install NixOS, and in\nthis article I will outline how I like to install NixOS on physical hardware or\nvirtual machines: over the network and fully declaratively.The term declarative\nmeans that you describe what should be accomplished, not how. For NixOS, that\nmeans you declare what software you want your system to include (add to config\noption\nenvironment.systemPackages,\nor enable a module) instead of, say, running apt install.A nice property of the declarative approach is that your system follows your\nconfiguration, so by reverting a configuration change, you can cleanly revert\nthe change to the system as well.I like to manage declarative configuration files under version control,\ntypically with Git.When I originally set up my current network storage build, I chose CoreOS (later\nFlatcar Container Linux) b"
  },
  {
    "title": "RenderFormer: Neural rendering of triangle meshes with global illumination (microsoft.github.io)",
    "points": 245,
    "submitter": "klavinski",
    "submit_time": "2025-06-01T03:43:00 1748749380",
    "num_comments": 49,
    "comments_url": "https://news.ycombinator.com/item?id=44148524",
    "comments": [
      "The coolest thing here might be the speed: for a given scene RenderFormer takes 0.0760 seconds while Blender Cycles takes 3.97 seconds (or 12.05 secs at a higher setting), while retaining a 0.9526 Structural Similarity Index Measure (0-1 where 1 is an identical image). See tables 2 and 1 in the paper.This could possibly enable higher quality instant render previews for 3D designers in web or native apps using on-device transformer models.Note the timings above were on an A100 with an unoptimized PyTorch version of the model. Obviously the average user's GPU is much less powerful, and for 3D designers it might be still powerful enough to see significant speedups over traditional rendering. Or for a web-based system it could even connect to A100s on the backend and stream the images to the browser.Limitations are that it's not fully accurate especially as scene complexity scales, e.g. with shadows of complex shapes (plus I imagine particles or strands), so the final renders will probably still be done traditionally to avoid any of the nasty visual artifacts common in many AI-generated images/videos today. But who knows, it might be \"good enough\" and bring enough of a speed increase to justify use by big animation studios who need to render full movie-length previews to use for music, story review, etc etc.\n \nreply",
      "I don\u2019t think the authors are being wilfully deceptive in any way, but Blender Cycles on a gpu of that quality could absolutely render every scene in this paper in less than 4s per frame. There are very modest tech demo scenes with low complexity, and they\u2019ve set blender to cycle through 4k iterations per pixel - which seems non-sensible as Blender would hit something close to its output after a couple of hundred cycles, and then burn gpu cycles for the next 3800 cycles making no improvements.I think they\u2019ve inadvertently included Blender\u2019s instantiation phase in the overall rendering time, while not including the transformer instantiation.I\u2019d be interested to see the time to render the second frame for each system. My hunch is that Blender would be a lot more performant.I do think the papers results are fascinating in general, but there\u2019s some nuance in the way they\u2019ve configured and timed Blender.\n \nreply",
      "Also of note is that the RenderFormer tests and Blender tests were done on the same Nvidia A100, which sounds sensible at first glance, but doesn't really make sense because Nvidia's big-iron compute cards (like the A100) lack the raytracing acceleration units present on the rest of their range. The A100 is just the wrong tool for the job here, you'd get vastly better Blender-performance-per-dollar from an Nvidia RTX card.Blenders benchmark database doesn't have any results for the A100, but even the newer H100 gets smoked by (relatively) cheap consumer hardware.  Nvidia H100 NVL        -  5,597.13\n  GeForce RTX 3090 Ti    -  5,604.69\n  Apple M3 Ultra (80C)   -  7,319.21\n  GeForce RTX 4090       - 11,082.51\n  GeForce RTX 5090       - 15,022.02\n  RTX PRO 6000 Blackwell - 16,336.54\n \nreply",
      "Yeah, you would generally set blender to have some low minimum number of cycles, maybe have some adaptive noise target, and use a denoising model, especially for preview or draft renders.\n \nreply",
      "But rendering engines have been optimized for years and this is a research paper. Probably this technique will also be optimized for years and provide a 10x speedup again\n \nreply",
      "For the scenes that they\u2019re showing, 76ms is an eternity. Granted, it will get (a lot) faster but this being better than traditional rendering is a way off yet.\n \nreply",
      "Yeah, and the big caveat with this approach is that it scales quadratically with scene complexity, as opposed to the usual methods which are logarithmic. Their examples only have 4096 triangles at most for that reason. It's a cool potential direction for future research but there's a long way to go before it can wrangle real production scenes with hundreds of millions of triangles.\n \nreply",
      "I'd sooner expect them to use this to 'feed' a larger neural path tracing engine where you can get away with 1 sample every x frames. Those already do a pretty great job of generating great looking images from what seems like noise.I don't think this conventional similarity matrix in the paper is all that important to them\n \nreply",
      "Timing comparison with the reference is very disingenuous.In raytracing, error scale with the square root of sample count. While it is typical to use very high sample count for the reference, real world sample count for offline renderer is about 1-2 orders of magnitude lower than in this paper.I call it disingenuous because it is very usual for a graphic paper to include a very high sample count reference image for quality comparison, but nobody ever do timing comparison with it.Since the result is approximate, a fair comparison would be with other approximate rendering algorithm. Modern realtime path tracer + denoiser can render much more complex scenes on consumer GPU in less than 16ms.That's \"much more complex scenes\" part is the crucial part. Using transformer mean quadratic scaling on both number of triangles and number of output pixels. I'm not up to date with the latest ML research, so maybe it is improved now? But I don't think it will ever beat O(log n_triangles) and O(n_pixels) theoretical scaling of a typical path tracer. (Practical scaling wrt pixel count is sub linear due to high coherency of adjacent pixels)\n \nreply",
      "Modern optimized path tracers in games (probably not Blender) also use rasterization for primary visibility, which is O(n_triangles), but is somehow even faster than doing pure path tracing. I guess because is reduces the number of samples required to resolve high frequency texture details. Global illumination by itself tends to produce very soft (low frequency) shadows and highlights, so not a lot of samples are required in theory, when the denoiser can avoid artifacts at low sample counts.But yeah, no way RenderFormer in its current state can compete with modern ray tracing algorithms. Though the machine learning approach to rendering is still in its infancy.\n \nreply"
    ],
    "link": "https://microsoft.github.io/renderformer/",
    "first_paragraph": "We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning.Instead of taking a physics-centric approach to rendering, we formulate rendering as a sequence-to-sequence transformation where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels.RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. No rasterization, no ray tracing.Examples of scenes rendered with RenderFormer demon"
  },
  {
    "title": "\u201cBugs are 100x more expensive to fix in production\u201d study might not exist (2021) (theregister.com)",
    "points": 40,
    "submitter": "rafaepta",
    "submit_time": "2025-06-01T21:02:25 1748811745",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=44153734",
    "comments": [
      "I work in HW industry, but doing SW.If I deploy a bug despite \"unit\" tests, then it will probably be caught by 2nd tier tests 1 day later, which will already be more expensive since I'll need to read report, context switch from other thing, fix it, redeploy, wait for CI, wait for 2nd tier tests, etc.If it will not be caught by 2nd tier, then it probably will be caught by validation engineers, which not only costs their time, but they also have to contact me / fill some bug, I need to get familiar with it, context switch again, fix it, redeploy, wait for tests #1 and #2 and wait for their confirmationIf it will be released to actual customers then there's reputational dmg (can be huge depending on impact), possibly some lost sales / customers, probably a few weeks before it goes back to me thru customer->???->validation->meSo, while 100x more expensive is some extreme case, then usually the cost is very significantly higher the later you find the bug.But what about HW bugs?I think those can be really, really expensive.Imagine catching hardware bug which affects some computation that MUST be fixed at hardware level.Catching it before release and catching it after selling 100m copies is tens of bilions of dollars difference for top GPUs and CPUs.Why do you think that those companies are willing to build same product twice so they test eachother? It significantly increases the cost, but reduces the risk.Think about CrowdStrike's incident: catching that bug at dev level would cost them shitton times less than what actually happened :PTheir stock dropped by like 45%\n \nreply",
      "> So, while 100x more expensive is some extreme case, then usually the cost is very significantly higher the later you find the bug.When you said the reputational damage, lost sales/customers.. you're already in 100x territory. It may not be exactly there but it's close or exceeding it.Yet we'll still get the undisciplined engineers whining that they have to write a small and isolated unit test.\n \nreply",
      "I believe that if you're an OS vendor it could and would easily reach 100x.  Imagine the hundreds of thousands of systems that need to be updated to fix that bug.Say each downtime was only $1.00 (its not, but lets say it is) and that half the customers need to update for a bug (conversatively $100,000.00) thats a hundred thousand dollar bug in redeployment costs.   I'm understimating the downtime cost and understimating the amount of machines, so consider this an estimate)At this economy of scale, the bug would need to cost $1000.00 to fix in devel, which on the back of my napkin math is approximately correct.Source: work in overseeing kernel delivery for a big OS company.\n \nreply",
      "> Think about CrowdStrike's incident: catching that bug at dev level would cost them shitton times less than what actually happened :P> Their stock dropped by like 45%A program that can block booting is definitely on the high side of software bug risk.But even in that case, the post-drop stock was still trading higher than a year before and they got back to the same level in six months.\n \nreply",
      "If you ship firmware to devices, it could be far more expensive. [1][1] https://www.bleepingcomputer.com/news/hardware/botched-firmw...\n \nreply",
      "I have some thoughts on this (in the context of modern SaaS companies).The most expensive parts of fixing a bug are discovering/diagnosing/triaging the bug, cleaning up corrupted records, and customer communication. If you discover a bug in development or even better while you are coding the function or during a code review you get to bypass triaging, customer calls, escalations, RCAs, etc. At a SaaS company with enterprise customers each of those steps involves multiple meetings with your Support, Account Manager, Senior Engineer, Product Manager, Engineering Manager, Department Manager, sometimes Legal or a Security Engineer and then finally the actual coder. So of course if you can resolve an issue (at a modern SaaS company) during development it can be 10-100x less expensive just because of how much bureaucracy is involved in running a large scale enterprise SaaS company.It also brings up the interesting side effect of companies adopting non-deterministic coding (AI Code) in that now bugs that could have been discovered during design/development by a human engineer while writing the code can now leak all the way into prod.\n \nreply",
      "Forget the study, let's just do a simple thought experiment. Your developer gets paid $140k/yr (let's round up to ~$70/hr). Let's say a given bug found in testing takes 1 hour to fix; that's $70 (not counting the costs of ci/cd etc). If they miss it in test, and it hits production, would it cost $7,000 to fix? Depends what you mean by \"bug\", what it affects, and what you mean by \"fix in production\".- Did you screw up the font size on some text you just published? Ok, you can fix that in about 5 seconds, and it affects pretty much nothing. Doesn't cost 100x.- Did your sql migration just delete all records in the production database? Ok, that's going to take longer than 5 seconds to fix. People's data is gone, apps stop working, the lack of or bad data fed to other systems causes larger downstream issues, there's the reputational harm, the money you'll have to pay back to advertisers for their ads / your content being down, and all of that multiplied by however long it takes you to restore the database from backup (um... you do test restoring your backups... right?). That's closer to 100x more expensive to fix in production.- Did you release a car, airplane, satellite, etc with a bug? We're looking at potentially millions in losses. Way more than 1000x.And those are just the easy ones. What about a bug you release, that then is adopted (and depended on) by downstream api consumers, and that you then spend decades to patch over and engineer around? How about when production bugs cause your product team to lose confidence in deployments, so they spend weeks and weeks to \"get ready\" for a single deploy, afraid of it failing and not being able to respond quickly? That fear will dramatically slow down the pace of development/shipping.The \"long tail\" of fixing bugs in production involves a lot more complexity than in non-production; that's where the extra cost comes from. These costs could end up costing 10,000x over the long term, when all is said and done. Security bugs, reliability bugs, performance bugs, user interface bugs, etc. There's a universe of bugs which are much harder/costlier to fix in production.But you know what is certain? It always costs more to fix in production. 1.2x, 10x, 1000x, that's not the point; the point is, fix your bugs before it goes to production. (\"Shift Left\" is how we refer to this in the DevOps space, but it applies to everything in the world that has to do with quality. Improve quality before it gets shipped to customers, and you save money in the long run.)\n \nreply",
      ">> Did you screw up the font size on some text you just published? Ok, you can fix that in about 5 seconds, and it affects pretty much nothing. Doesn't cost 100x.Actually, i find these to be worse, because i've been in scrum meetings where 6 people spend 2 minutes talking about this bug, then another 2 minutes talking about the QA of it the next day. Tiny issues are very expensive to fix if you have formulaic team members who arent taking the reigns.\n \nreply",
      "And you're lucky if it's two. If they're not familiar with the rendering engine/docs system/API platform/middleware etc (or just think they aren't), or have low confidence in a debt ridden platform, they'll spend five minutes a day for a week or three theorizing on how it could have gone wrong, debating if, actually, it's correct the way it is, refreshing their knowledge of deep internals, debating if a fix could break something, and so on. Way safer to do that than risk making things worse in an uncertain system.\n \nreply",
      "Bugs are more like viruses in practice. The cost is useful to measure in negative lifespan, not cost to fix, per se. This is why many bugs are never fixed. Those cost nothing to fix, because they don't have to be.> Did your sql migration just delete all records in the production database? That's closer to 100x more expensive to fix in production.Companies that do this often, don't stay in business. It's not 100x more expensive if you're not in business. Survivorship ensures that classes of bugs don't have a consistent negative return, because they are often fatal.\n \nreply"
    ],
    "link": "https://www.theregister.com/2021/07/22/bugs_expense_bs/",
    "first_paragraph": ""
  },
  {
    "title": "Figma Slides Is a Beautiful Disaster (allenpike.com)",
    "points": 355,
    "submitter": "tobr",
    "submit_time": "2025-06-01T05:59:59 1748757599",
    "num_comments": 209,
    "comments_url": "https://news.ycombinator.com/item?id=44148933",
    "comments": [
      "The strangest part about this disaster is that all of these problems should have been immediately noticed by anyone at Figma actually using the software.A lot of comments are blaming the cloud or cross-platform apps but similar functionality works fine in Figma\u2019s non-slides app. They\u2019ve solved these problems years ago.So why is Slides such a disaster? From the outside, this feels like what some startups do when they hear influencers exaggerate advice about shipping your MVP as fast as possible and everyone rushes to get something launched, no matter how buggy. They forget that real users don\u2019t like being burned by a product that fails when they need it and it\u2019s hard to recover from that.If I project from my own career experience, this is also similar to all the times I\u2019ve been stuck under ladder-climbing executives who thought they could mandate reality and have all the features delivered all at once on an arbitrary deadline they came up with before consulting the engineers. They end up shipping something to avoid the wrath of an executive who demands specific deadlines, then they hope to finish the features and clean up the bugs in production. In my case, the executive in question didn\u2019t actually use the software, so this was the rational way forward within the company if you wanted to look good. And of course, it led to results just like this\n \nreply",
      "PM at Figma here (for dev tools, not slides).What happened to Allen here sucks. I've messaged the team so we can dig into this specific case. More generally, we know that Slides needs to be bulletproof when presenting, and nothing less than that is acceptable.As an FYI, we _do_ use Figma Slides internally for pretty much everything, from internal meetings to major events. As a PM I use it every week, and our internal feedback channel for Slides is super active with folks like me requesting improvements. Figma is also a pretty unique place, where it's more likely our senior leadership request quality improvements than chase for deadlines - we know how critical the user experience is. We don't always get it right, but when we don't we're committed to fixing it.\n \nreply",
      "> As an FYI, we _do_ use Figma Slides internally for pretty much everythingI think this is part of the issue. How much of the internal use stays within the editor view? Do you have any internal stakeholders who won\u2019t click a Figma link and instead want a PPT or PDF? Because those are normal requests for presentations - but not ones that you\u2019d find with internal use.For example, there needs to be a way to export to PDF that\u2019s less than several hundred MB. And the PPT export is hopelessly broken - the outputs look like a clipped ransom letter.\n \nreply",
      "I'm usually building my slides in Figma (the original app), and I've learnt to run the PDF exported by it (hundreds of MB) through Adobe \"Compress PDF\" online utility that gets it to <10 MB. Would be great for the Figma-exported PDF to be small right away.\n \nreply",
      "on a tangent, being in the video industry, for me to see a file only in the hundreds of MBs wouldn't even get my attention. it's funny how used to the boiling water one gets when it happens slowly. of course a PDF is not a video file, so maybe something would feel hinky???\n \nreply",
      "It\u2019s amazing how small text and images, even compressed video, can get compared to uncompressed video.Caring a little bit will help save a bunch of space.\n \nreply",
      "Regardless of the specific bugs he ran into, it is a product that only works well online, despite how difficult it is for a user to know for sure ahead of time what kind of connection will be available when it counts. Isn\u2019t that just a fundamental miscalculation for this type of product? It\u2019s almost guaranteed to put a certain percentage of your users in an embarrassing situation in front of an audience.\n \nreply",
      ">despite how difficult it is for a user to know for sure ahead of time what kind of connection will be availableIn 2025 it's a safe assumption to assume the user always has internet access. I've never had to worry if I will have internet access when I go to an event.\n \nreply",
      "The user will always have internet access - except when it suddenly drops out during that one critical meeting.Doing a presentation at a conference? The hotel promised there would be \"internet\", but failed to mention all 10.000 attendees would be sharing a 10Mbps link. Doing a presentation at another company? They've got an overly-aggressive firewall on the guest network, so Figma isn't loading - and your provider decided to temporarily block your 5G tethering due to \"misuse\". Presenting a keynote at Computex? Guess Figma is having an outage, better tell the hundreds of journalists to come back tomorrow!Your internet may have always worked so far. Are you willing to bet your career on some random 3rd party internet connection - or Figma itself - never having an outage?\n \nreply",
      ">drops out during that one critical meetingThe article said that it handles drops of internet connections fine.>sharing a 10Mbps linkYou aren't streaming a video.>They've got an overly-aggressive firewall on the guest network, so Figma isn't loadingFigma is an industry standard tool. It would be unlikely to be blocked.>and your provider decided to temporarily block your 5G tethering due to \"misuse\"You can probably present directly from your phone in this case.>Guess Figma is having an outage, better tell the hundreds of journalists to come back tomorrow!I guess so. Or the journalists can watch the livestream or a recording.\n \nreply"
    ],
    "link": "https://allenpike.com/2025/figma-slides-beautiful-disaster",
    "first_paragraph": "Some highlights and lowlights.May 31, 2025 \u2022\n5 min readI think of presentation slides as having 3 main jobs:This calls for slides that are mostly images or very short phrases. A minority of slides justify designing something to match the vibe or illustrate a point \u2013 a diagram, for example. This can mean going back and forth between Keynote and, say, Figma, but it\u2019s worth the effort.This year I\u2019ve been spending a lot of time in Figma, so when I was recently asked to speak at an event I thought, \u201cWhy not try Figma Slides?\u201d Figma Slides launched a year back, and graduated from beta in March. I\u2019ve used Keynote for almost 20 years now \u2013 it seemed like time for me to finally upgrade my presentation tool.So I gave it a whirl.Pretty quickly, I liked building slides right in Figma. The Grid view made it easy for me to structure my ideas. Features like Auto Layout and Components made building slides that adapt to different text and images a snap.For example, a key point in my talk was that selec"
  }
]