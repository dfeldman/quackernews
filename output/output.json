[
  {
    "title": "Discord/Twitch/Snapchat age verification bypass (kibty.town)",
    "points": 321,
    "submitter": "JustSkyfall",
    "submit_time": "2026-02-11T22:56:41 1770850601",
    "num_comments": 163,
    "comments_url": "https://news.ycombinator.com/item?id=46982421",
    "comments": [
      "The real and robust method will be generating artificial video input instead of the real webcam. I really don\u2019t think any platform will be able to counter this. If they start requiring to use a phone with harder to spoof camera input, you will simply be able to put the camera in front of a high resolution screen. The cat and mouse game will not last long.reply",
      "> I really don\u2019t think any platform will be able to counter this.Do platforms want to counter it?Seems to me with an unreliable video selfie age verification:* Reasonable people with common sense don't need to upload scans of their driving licenses and passports* The platform gets to retain users without too much hassle* Porn site users are forced to create accounts; this enables tracking, boosting ad revenue and growth numbers.* Politicians get to announce that they have introduced age controls.* People who claimed age checks wouldn't invade people's privacy don't get proven wrong* Teens can sidestep the age checks and retain their access; teens trying to hide their porn from their parents is an age-old tradition.* Parents don't see their teens accessing porn. They feel reassured without having to have any awkward conversations or figure out any baffling smartphone parental controls.Everyone wins.reply",
      "Until somebody (likely a politician or anti-porn advocacy group) decides to poke the bear and ruin itreply",
      "Don't Windows Hello camera devices have some kind of hardware attestation?  I'm sure verification schemes like this will eventually go down that path soon.My guess is that's probably one of the reasons Google tried to push for Play Store only apps, provide a measurable/verifiable software chain for stuff like this.reply",
      "That the camera is real doesn't imply the thing it's viewing is real.reply",
      "As I understand it, 'Windows Hello' requires a near-IR image alongside the RGB image.It's not the fancy structured light of phone-style Face ID, but it still protects against the more common ways of fooling biometrics, like holding up a photo or wearing a simple paper mask.reply",
      "Fair enough. That removes the virtual option, and you'll be forced to point the camera at your older brother.reply",
      "Yes they do. Part of the reason why you can't use certain webcams that are Windows Hello compatible (I.e. with IR) in recent versions of Windows.reply",
      "They already support ID checks as an alternative to face scanning, if the latter proves to be untenable then it's literally a case of flipping a switch to mandate ID instead.reply",
      "The long term solution would have to be some kind of integration with a government platform where the platform doesn\u2019t see your ID and the government doesn\u2019t see what you are signing up for.I don\u2019t this will happen in the US but I can see it in more privacy responding countries.Apple and Google may also add some kind of \u201cchild flag\u201d parents can enable which tells websites and apps this user is a child and all age checks should immediately fail.reply"
    ],
    "link": "https://age-verifier.kibty.town/",
    "first_paragraph": ""
  },
  {
    "title": "GPT-5 outperforms federal judges 100% to 52% in legal reasoning experiment (ssrn.com)",
    "points": 141,
    "submitter": "droidjj",
    "submit_time": "2026-02-11T23:37:11 1770853031",
    "num_comments": 108,
    "comments_url": "https://news.ycombinator.com/item?id=46982792",
    "comments": [
      "IANAL, but this seems like an odd test to me. Judges do what their name implies - make judgment calls. I find it re-assuring that judges get different answers under different scenarios, because it means they are listening and making judgment calls. If LLMs give only one answer, no matter what nuances are at play,  that sounds like they are failing to judge and instead are diminishing the thought process down to black-and-white thinking.Digging a bit deeper, the actual paper seems to agree: \"For the sake of consistency, we define an \u201cerror\u201d in the same way that Klerman and Spamann do in their original paper: a departure from the law. Such departures, however, may not always reflect true lawlessness. In particular, when the applicable doctrine is a standard, judges may be exercising the discretion the standard affords to reach a decision different from what a surface-level reading of the doctrine would suggest\"reply",
      "Yeah, I'm reminded of the various child porn cases where the \"perpetrator\" is a stupid teenager who took nude pics of themselves and sent them to their boy/girlfriend.  Many of those cases have been struck down by judges because the letter of the law creates a non-sequitur where the teenager is somehow a felon child predator who solely preyed on themselves, and sending them to jail and forcing them to sign up for a sex offender registry would just ruin their lives while protecting nobody and wasting the state's resources.I don't trust AI in its current form to make that sort of distinction.  And sure you can say the laws should be written better, but so long as the laws are written by humans that will simply not be the case.reply",
      "This example feels more like a bug in the law itself that should be corrected. If this behavior is acceptable then it should be legal so we can avoid everyone the hassle in the first place. I bet AI would be great at finding and fixing these bugs.reply",
      "AI would be great IF they know what to findThe state of current AI does not give them ability to know that, so the consideration is likely to be droppedreply",
      "This is one of the roles of justice,  but it is also one of the reasons why wealthy people are convicted less often.  While it often delivered as a narrative of wealth corrupting the system, the reality is that usually what they are buying is the justice that we all should have.So yes, a judge can let a stupid teenager off on charges of child porn selfies.  but without the resources, they are more likely be told by a public defender to cop to a plea.And those laws with ridiculous outcomes like that are not always accidental.  Often they will be deliberate choices made by lawmakers to enact an agenda that they cannot get by direct means.   In the case of making children culpable for child porn of themselves, the laws might come about because the direct abstinence legislation they wanted could not be passed, so they need other means to scare horny teens.reply",
      "> what they are buying is the justiceFrom The Truth by Terry Pratchett, emphasis on the book's footnote:> William\u2019s family and everyone they knew also had a mental map of the city that was divided into parts where you found upstanding citizens, and other parts where you found criminals. It had come a shock to them... no, he corrected himself, it had come as a an affront to learn that [police chief] Vimes operated on a different map. Apparently he'd instructed his men to use the front door when calling on any building, even in broad daylight, when sheer common sense said that they should use the back, just like any other servant. [0]> [0] William\u2019s class understood that justice was like coal or potatoes. You ordered it when you needed it.reply",
      "Sure, but I'm not sure how AI would solve any of that.Any claims of objectivity would be challenged based on how it was trained.  Public opinion would confirm its priors as it already does (see accusations of corruption or activism with any judicial decision the mob disagrees with, regardless of any veracity).  If there's a human appeals process above it, you've just added an extra layer that doesn't remove the human corruption factor at all.As for corruption, in my opinion we're reading some right now.  Human-in-the-loop AI doesn't have the exponential, world-altering gains that companies like OpenAI need to justify their existence.  You only get that if you replace humans completely, which is why they're all shilling science fiction nonsense narratives about nobody having to work.  The abstract of this paper leans heavily into that narrativereply",
      "Oddly enough, Texas passed reform to keep sexting teens from getting prosecuted when: they are both under 18 and less than two years difference in age.  It was regarded as a model for other states.  It's the only positive thing I have heard of Texas legislating wrt sexuality.reply",
      "Sorry but that seems like an insane system where whole classes of actions effectively are illegal but probably okay if you're likeable. In your scenario the obvious solution is to amend the law and pardon people convinced under it. B/c what really happens is that if you have a pretty face and big tits you get out of speeding tickets b/c \"gosh well the law wasn't intended for nice people like you\"reply",
      "There have been equally high profile cases where a perpetrator got off because they have connections.  I'd love for an AI to loudly exclaim that this is a big deviation from the norm.reply"
    ],
    "link": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=6155012",
    "first_paragraph": ""
  },
  {
    "title": "Fluorite \u2013 A console-grade game engine fully integrated with Flutter (fluorite.game)",
    "points": 400,
    "submitter": "bsimpson",
    "submit_time": "2026-02-11T16:21:10 1770826870",
    "num_comments": 236,
    "comments_url": "https://news.ycombinator.com/item?id=46976911",
    "comments": [
      "It doesn't say Toyota anywhere on the page and they don't have a link to a repo or anything like that, so I was a little confused. But it is from /that/ Toyota (well, a subsidiary that is making 3d software for their displays) and there was a talk at FOSDEM about it: https://fosdem.org/2026/schedule/event/7ZJJWW-fluorite-game-...reply",
      ">  They use this game engine in the 2026 RAV4Funny how \u201cgame engines\u201d are  now car parts in 2026.Can I just have an electric car that\u2019s a car and nothing else? Seats, wheels pedals, mirrors, real buttons, no displays just a aux jack. I\u2019d buy it, hell I might even take the risk and pre-order itreply",
      "> no displaysIn the US, no. Backup cameras are required by federal law as of 2018. The intent of the law was to reduce the number of children killed by being backed over because the driver couldn't see them behind the car.reply",
      "It is crazy how many things are downstream of the structural issue where US regulations favor ginormous SUVs and pickups where this is a problem, but if we introduced legislation to fix this we would end up ruining US automakers which have pivoted almost entirely to this segment alonereply",
      "While I agree with you that the issue is far worse with larger vehicles, I do find that backing up in my wife's 2011 camry (without a backup camera) feels significantly less safe than I feel backing up my 2017 accord with a backup camera. I'm all for fixing the structural issue you are referring to, but I think the requirement for those cameras is sane in an age where the added cost to the manufacturer is miniscule.reply",
      "I have to agree. Backing up my Tundra (8' bed) feels substantially safer since I can see immediately behind the vehicle than any pre-regulation vehicle I've driven. That doesn't even account for the convenience with lining up for towing, hauling, etc. (It's no replacement for GOAL\u2014Get Out And Look\u2014but it definitely helps!)reply",
      "I like it because I can see kids, no matter what vehicle I\u2019m in.I have unusually good spatial skills. I have parallel parked and reverse parked perfectly every single time for over 5 years\u2026\u2026but no matter what, I cannot see behind my bumper. No mirror on any car points there.reply",
      "The law was passed due to sustained lobbying from a man, Greg Gulbransen, who ran over his childreply",
      "Give me a backup camera without a screen and then we\u2019ll talk. Doubly so because once you\u2019ve got that screen, no automaker will resist making it do other things.reply",
      "My 2010 Tacoma has a 2 inch square in the rear view mirror that works wonderfully.reply"
    ],
    "link": "https://fluorite.game/",
    "first_paragraph": ""
  },
  {
    "title": "\"Nothing\" is the secret to structuring your work (vangemert.dev)",
    "points": 66,
    "submitter": "spmvg",
    "submit_time": "2026-02-08T12:01:50 1770552110",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=46933529",
    "comments": [
      "I've found mostly the opposite. Some well arranged windows are quite a nice anchor, I'm working on what's there in front of me. It's like bowling with bumpers in place, instead of the ball going in the gutter, the structure keeps it in the lane. I've found it necessary to devote time to cleaning and clearing windows, and sometimes I forget what's going on, and as I'm closing out the windows because I forgot what was going on, oh! there's this half finished thing that I actually really want finished.What am I working on, what's in progress? The work space is the map. The terrain is changing as the task progresses, and so must the map, but the map is useful, even if it takes a bit of redrawing here and there.The desktops (multiple, 3-7) are the map of the work. Part of the work is keeping the map accurate, not wadding it up and throwing it in the trash.I suppose different things work for different people, but I started with the suggestion here and came around to skillful use of space as the work map itself.Cleaning and updating are continuous, not a 'big bang' clear-the-desks event, mostly. But if it's not continuous, the big bang is probably better.Some spots are problem spots, like digital notebooks, desktop icons. When I notice a problem spot, I create a recurring task to remove one X per week, or in some of the worst cases, one X per day. I have a rule of clearing out the oldest two days of email each day. I miss some days if I'm busy, but on average rate out = rate in, because I will always catch up within a day or two applying the rule that the oldest two days of email need eviction (make a task out of it, archive it, whatever) every day. Rate out = rate inreply",
      "Alas, I think it is far more likely that there is no secret to any of this.  Different strokes will arrive at the same place for a lot of people.  All the more true for things that are even remotely creative in nature.Not to say that routine and form can't get results.  It is hilarious how much of the current fascination with LLM writing can be summarized by \"actually filling out a routine template will satisfy a ton of requirements.\"  People that are surprised with how well some output works, but would have scoffed at filling out a lot of boilerplate in previous technologies.So, yes, try it.  But do not become attached to it.  If it works, rejoice in that.  But do not count on it always working.  If it stops for a time, feel free to leave it for a time.reply",
      "Absolutely. \"One size fits all\", doesn't. Doesn't even fit one, all of the time.reply",
      "I have a system for electronics projects where a new project gets a labeled container to store extra parts, papers, spare PCBs, weird cables I might need later...It works pretty well, especially when I want to take a project to a meetup.Unfortunately, I also have a bin labeled \"Projects\".reply",
      "Every day I work on my main project, I clear my desk completely, take out a small notepad and write my overriding goal, and my next step goal, and then make a list of tasks for that next step goal.Then I work.Writing the major goal every day is important to not let sub-goals overshadow it. Writing the immediate goal every day is important because together the two goals create a very clear direction of action with a clear next step.I have my screen mounted on the wall, and have side end-tables for pens, papers and notes I need, etc. so my desk is absolutely clear.My desk is a half circle, but not that deep, because that optimizes the usefulness of the surface for work (not storage).reply",
      "I believe we clutter our workspaces because we suck at keeping iterations short. We always want to add one more feature, tweak one more thing, etc.Eventually, some external pressure (boss, client, IM, whatever) causes us to open a second context simultaneously. Then it happens with a third, a fourth, etcThis is happening because the world is expecting shorter and shorter time to results due to better tooling in the last 10 years, but most have not figured out that all the LLMs and agents in the world won\u2019t shorten the loop, only the person using them can do that.I find that for any given problem, if I don\u2019t see results in 30 minutes, it\u2019s time to stop that problem and likely reshape it. If I don\u2019t actually get the result in 90-120 minutes, I\u2019m doing something wrong.reply",
      "This is my focus protocol. Whenever I find myself having trouble trying started on a task, I create a new desktop and open windows related to the task only. DnD on. Pick a next step. Execute.reply",
      "Every morning I close all work browser tabs from prior day. 99% of them I don't need again/can just reopen if I need. The 1% I'll note on a todo list or keep open somewhere.reply",
      "Meh. This just sounds like all the interface theory stuff we users have to deal with, where useful things are removed in favor of a 'clean' and empty interface that makes you work harder to get your actual work done.reply",
      "Then there's Adobe who remove features to add feature and justify it's next version; or clones it into a separate product so they can justify it's next subscription rise; or moves it into a different product so they can justify it's  subscription expansion.reply"
    ],
    "link": "https://www.vangemert.dev/blog/nothing",
    "first_paragraph": ""
  },
  {
    "title": "Text classification with Python 3.14's ZSTD module (maxhalford.github.io)",
    "points": 90,
    "submitter": "alexmolas",
    "submit_time": "2026-02-09T08:14:28 1770624868",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=46942864",
    "comments": [
      "This looks like a nice rundown of how to do this with Python's zstd module.But, I'm skeptical of using compressors directly for ML/AI/etc. (yes, compression and intelligence are very closely related, but practical compressors and practical classifiers have different goals and different practical constraints).Back in 2023, I wrote two blog-posts [0,1] that refused the results in the 2023 paper referenced here (bad implementation and bad data).[0] https://kenschutte.com/gzip-knn-paper/[1] https://kenschutte.com/gzip-knn-paper2/reply",
      "Concur. Zstandard is a good compressor, but it's not magical; comparing the compressed size of Zstd(A+B) to the common size of Zstd(A) + Zstd(B) is effectively just a complicated way of measuring how many words and phrases the two documents have in common. Which isn't entirely ineffective at judging whether they're about the same topic, but it's an unnecessarily complex and easily confused way of doing so.reply",
      "Good on you for attempting to reproduce the results & writing it up, and reporting the issue to the authors.> It turns out that the classification method used in their code looked at the test label as part of the decision method and thus led to an unfair comparison to the baseline resultsreply",
      "Python's zlib does support incremental compression with the zdict parameter.  gzip has something similar but you have to do some hacky thing to get at it since the regular Python API doesn't expose the entry point.  I did manage to use it from Python a while back, but my memory is hazy about how I got to it.  The entry point may have been exposed in the code module but undocumented in the Python manual.reply",
      "So you just discovered pca in some other form?reply",
      "Ooh, totally. Many years ago I was doing some analysis of parking ticket data using gnuplot and had it output a chart png per-street. Not great, but worked well to get to the next step of that project of sorting the directory by file size. The most dynamic streets were the largest files by far.Another way I've used image compression to identify cops that cover their body cameras while recording -- the filesize to length ratio reflects not much activity going on.reply",
      "Sweet! I love clever information theory things like that.It goes the other way too. Given that LLMs are just lossless compression machines, I do sometimes wonder how much better they are at compressing plain text compared to zstd or similar. Should be easy to calculate...EDIT: lossless when they're used as the probability estimator and paired with something like an arithmetic coder.reply",
      "I do not agree on the \"lossless\" adjective. And even if it is lossless, for sure it is not deterministic.For example I would not want a zip of an encyclopedia that uncompresses to unverified, approximate and sometimes even wrong text. According to this site : https://www.wikiwand.com/en/articles/Size%20of%20Wikipedia a compressed Wikipedia without medias, just text is ~24GB. What's the medium size of an LLM, 10 GB ? 50 GB ? 100 GB ? Even if it's less, it's not an accurate and deterministic way to compress text.Yeah, pretty easy to calculate...reply",
      "(to be clear this is not me arguing for any particular merits of llm-based compression, but) you appear to have conflated one particular nondeterministic llm-based compression scheme that you imagined with all possible such schemes, many of which would easily fit any reasonable definitions of lossless and deterministic by losslessly doing deterministic things using the probability distributions output by an llm at each step along the input sequence to be compressed.reply",
      "https://bellard.org/ts_zip/reply"
    ],
    "link": "https://maxhalford.github.io/blog/text-classification-zstd/",
    "first_paragraph": "Python 3.14 introduced the compression.zstd module. It is a standard library implementation of Facebook\u2019s Zstandard (Zstd) compression algorithm. It was developed a decade ago by Yann Collet, who holds a blog devoted to compression algorithms.I am not a compression expert, but Zstd caught my eye because it supports incremental compression. You can feed it data to compress in chunks, and it will maintain an internal state. It\u2019s particularly well suited for compressing small data. It\u2019s perfect for the classify text via compression trick, which I described in a previous blog post 5 years ago.My previous blog post was based on a suggestion from Artificial Intelligence: A Modern Approach, and is rooted in the idea that compression length approximates Kolmogorov complexity. There\u2019s a 2023 paper called \u201cLow-Resource\u201d Text Classification: A Parameter-Free Classification Method with Compressors that revisits this approach with encouraging results.The problem with this approach is practical: pop"
  },
  {
    "title": "GLM-5: Targeting complex systems engineering and long-horizon agentic tasks (z.ai)",
    "points": 251,
    "submitter": "CuriouslyC",
    "submit_time": "2026-02-11T13:42:16 1770817336",
    "num_comments": 400,
    "comments_url": "https://news.ycombinator.com/item?id=46974853",
    "comments": [
      "Pelican generated via OpenRouter: https://gist.github.com/simonw/cc4ca7815ae82562e89a9fdd99f07...Solid bird, not a great bicycle frame.reply",
      "Thank you for continuing to maintain the only benchmarking system that matters!Context for the unaware: https://simonwillison.net/tags/pelican-riding-a-bicycle/reply",
      "They will start to max this benchmark as well at some point.reply",
      "It's not a benchmark though, right? Because there's no control group or reference.It's just an experiment on how different models interpret a vague prompt. \"Generate an SVG of a pelican riding a bicycle\" is loaded with ambiguity. It's practically designed to generate 'interesting' results because the prompt is not specific.It also happens to be an example of the least practical way to engage with an LLM. It's no more capable of reading your mind than anyone or anything else.I argue that, in the service of AI, there is a lot of flexibility being created around the scientific method.reply",
      "For 2026 SOTA models I think that is fair.For the last generation of models, and for today's flash/mini models, I think there is still a not-unreasonable binary question (\"is this a pelican on a bicycle?\") that you can answer by just looking at the result: https://simonwillison.net/2024/Oct/25/pelicans-on-a-bicycle/reply",
      "So if it can generate exactly what you had in mind based presumably on the most subtle of cues like your personal quirks from a few sentences that could be _terrifying_, right?reply",
      "It's interesting how some features, such as green grass, a blue sky, clouds, and the sun, are ubiquitous among all of these models' responses.reply",
      "It is odd, yeah.I'm guessing both humans and LLMs would tend to get the \"vibe\" from the pelican task, that they're essentially being asked to create something like a child's crayon drawing. And that \"vibe\" then brings with it associations with all the types of things children might normally include in a drawing.reply",
      "If you were a pelican, wouldn't you want to go cycling on a sunny day?Do electric pelicans dream of touching electric grass?reply",
      "This is actually a good benchmark, I use to roll my eyes at it.  Then I decided to apply the same idea and ask the models to generate SVG image of \"something\" not going to put it out there.  There was a strong correlation between how good the models are and the image they generated.  These were also no vision images, so I don't know if you are serious but this is a decent benchmark.reply"
    ],
    "link": "https://z.ai/blog/glm-5",
    "first_paragraph": ""
  },
  {
    "title": "Reports of Telnet's Death Have Been Greatly Exaggerated (terracenetworks.com)",
    "points": 42,
    "submitter": "ericpauley",
    "submit_time": "2026-02-11T20:20:48 1770841248",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=46980355",
    "comments": [
      "Well, that certainly explains why no one in the US telnet BBS community seemed to be discussing having connectivity problems.reply",
      "Glad this one didn\u2019t open with a song parody.reply",
      "Do you have to restart your computer to exit telnet?:)reply",
      "Related: PTT BBS is a popular Telnet-based forum in Taiwan, still actively used these days.https://en.wikipedia.org/wiki/PTT_Bulletin_Board_Systemreply",
      "I think scoffing at plaintext protocols is silly. Contemporary security architecture is a nightmare. It\u2019s like scoffing at keyboards for sending key codes in the open to the HID controller because you\u2019ve failed to secure your machine so badly you have adversaries in your HID controller.If you have a well secured LAN where trust is social SSH gets\nyou nothing. SMTP telnet http being plain were from days when users were able to actually reason about what was happening within their OS. If there\u2019s anything that should be scoffed at its us now with our bloated opaque corporate controlled OSes.reply",
      "Tangentially, I saw an ad the other day for software which purports to encrypt your keystrokes: https://www.keystrokelock.com/ I have no idea what that means.reply",
      "Related:The Day the Telnet Diedhttps://news.ycombinator.com/item?id=46967772reply",
      "The main question is why use Telnet when ssh is available.  Some people mentioned routers, maybe that is why.  But I would think in this day and age routers would now use ssh.I do remember reading a long time ago telnet does/can support encryption. But when I looked at the systems I have access to, the manuals have no mention of that.reply",
      "Probably because ssh ciphers change, telnet doesn\u2019t, and you\u2019re not really supposed to be internet exposing those interfaces anyway.reply",
      "SSH without proper key management offers marginal benefits compared to telnet.reply"
    ],
    "link": "https://www.terracenetworks.com/blog/2026-02-11-telnet-routing",
    "first_paragraph": ""
  },
  {
    "title": "NetNewsWire Turns 23 (netnewswire.blog)",
    "points": 210,
    "submitter": "robin_reala",
    "submit_time": "2026-02-11T18:06:11 1770833171",
    "num_comments": 49,
    "comments_url": "https://news.ycombinator.com/item?id=46978490",
    "comments": [
      "I love the philosophy page: https://netnewswire.com/philosophy.html\"\"\"We believe that apps should never crash. They should be free of bugs. They should be fast \u2014 they should feel lighter-than-air.We believe that quality is more important than just piling on features; we believe that quality is the most important feature. And we believe that high quality is transformative \u2014 it makes for an app you never hesitate to reach for. You can rely on it, and you do, again and again.This makes us slow to add features. We are adding features \u2014 but never at the expense of how it feels. Never at the expense of reliability and speed.reply",
      "Hands down the best RSS reader I've used. It's fast, tiny, built extremely well, and has no flab. It sits in a certain class of application along with Alfred and a handful of others in being a standout example of craftsmanship that's reminiscent of the golden era of OS X. More apps should strive for this standard.reply",
      "I wish it had a more accessible scripting API - I use it locally, and back up saved stories, but I have to directly hit their sqlite database to extract data out of it :/reply",
      "Speaking of Alfred, there\u2019s also a Raycast[0] extension for NetNewsWire allowing one to combine the two[1].Disclaimer: I authored the extension but like most Raycast extensions, it\u2019s open-source[2].[0]: https://raycast.com\n[1]: https://raycast.com/xmok/netnewswire\n[2]: https://github.com/raycast/extensions/tree/main/extensions/n...reply",
      "I love it too, but I would still like some concept of folders, so that I could sort my feeds into eg. programming, design, hobbies, and then have a feed to match the mood.reply",
      "Are you talking about NetNewsWire? There are folders, I have a bunch setup.reply",
      "I'm staying away from macOS Tahoe for now. NetNewsWire has already announced that they will no longer support the earlier 6.x release that I use. I assume that means no bug fixes or back-porting of new features. Sad.reply",
      "I love NNW, especially the new iteration since Brent got it back.  Mac-assed software at its best.The other day I was searching for how to turn a youtube channel into an RSS feed and tried all sorts of convoluted instructions for finding channel IDs, etc.  At some point I thought this is the kind of user-centric thing that NNW has probably already thought of, and sure enough, if you just paste in a youtube channel URL as the feed, NNW sorts it out and creates a feed for you.reply",
      "> if you just paste in a youtube channel URL as the feed, NNW sorts it out and creates a feed for you.While I don't doubt that NNW has great UX, feed auto-discovery is a table stakes feature for any RSS client.reply",
      "I thought YouTube had native RSS feeds for channels?reply"
    ],
    "link": "https://netnewswire.blog/2026/02/11/netnewswire-turns.html",
    "first_paragraph": "NetNewsWire 1.0 for Mac shipped 23 years ago today! \ud83c\udfb8\ud83c\udfa9\ud83d\udd76\ufe0fHere\u2019s where things are on this particular February 11: we just shipped 7.0 for Mac and iOS, and now we\u2019re working on NetNewsWire 7.0.1.After a big release, no matter how careful we are, there are often some regressions to fix and tweaks to make right away, so we\u2019re working on those. Here\u2019s the milestone with the current to-do list.Big picture: we still have a lot of bugs to fix, lots of tech debt to deal with, and lots of polish-needed areas of the app. With Brent\u2019s retirement last year we\u2019ve been able to go way faster on dealing with all this. We plan to keep up the pace.Here are our current plans:For NetNewsWire 7.1 we\u2019re focusing on syncing fixes and improvements.NetNewsWire 7.2 doesn\u2019t have a focus yet. Could end up being UX fixes and polish, could be something else. Could be a potpourri, though we do prefer having a focus when possible.We don\u2019t have a NetNewsWire 7.3 plan yet \u2014\u00a0that\u2019s too far out. Depends on what actually ha"
  },
  {
    "title": "Claude Code is being dumbed down? (symmetrybreak.ing)",
    "points": 747,
    "submitter": "WXLCKNO",
    "submit_time": "2026-02-11T18:23:39 1770834219",
    "num_comments": 517,
    "comments_url": "https://news.ycombinator.com/item?id=46978710",
    "comments": [
      "Hey, Boris from the Claude Code team here. I wanted to take a sec to explain the context for this change.One of the hard things about building a product on an LLM is that the model frequently changes underneath you. Since we introduced Claude Code almost a year ago, Claude has gotten more intelligent, it runs for longer periods of time, and it is able to more agentically use more tools. This is one of the magical things about building on models, and also one of the things that makes it very hard. There's always a feeling that the model is outpacing what any given product is able to offer (ie. product overhang). We try very hard to keep up, and to deliver a UX that lets people experience the model in a way that is raw and low level, and maximally useful at the same time.In particular, as agent trajectories get longer, the average conversation has more and more tool calls. When we released Claude Code, Sonnet 3.5 was able to run unattended for less than 30 seconds at a time before going off the rails; now, Opus 4.6 1-shots much of my code, often running for minutes, hours, and days at a time.The amount of output this generates can quickly become overwhelming in a terminal, and is something we hear often from users. Terminals give us relatively few pixels to play with; they have a single font size; colors are not uniformly supported; in some terminal emulators, rendering is extremely slow. We want to make sure every user has a good experience, no matter what terminal they are using. This is important to us, because we want Claude Code to work everywhere, on any terminal, any OS, any environment.Users give the model a prompt, and don't want to drown in a sea of log output in order to pick out what matters: specific tool calls, file edits, and so on, depending on the use case. From a design POV, this is a balance: we want to show you the most relevant information, while giving you a way to see more details when useful (ie. progressive disclosure). Over time, as the model continues to get more capable -- so trajectories become more correct on average -- and as conversations become even longer, we need to manage the amount of information we present in the default view to keep it from feeling overwhelming.When we started Claude Code, it was just a few of us using it. Now, a large number of engineers rely on Claude Code to get their work done every day. We can no longer design for ourselves, and we rely heavily on community feedback to co-design the right experience. We cannot build the right things without that feedback. Yoshi rightly called out that often this iteration happens in the open. In this case in particular, we approached it intentionally, and dogfooded it internally for over a month to get the UX just right before releasing it; this resulted in an experience that most users preferred.But we missed the mark for a subset of our users. To improve it, I went back and forth in the issue to understand what issues people were hitting with the new design, and shipped multiple rounds of changes to arrive at a good UX. We've built in the open in this way before, eg. when we iterated on the spinner UX, the todos tool UX, and for many other areas. We always want to hear from users so that we can make the product better.The specific remaining issue Yoshi called out is reasonable. PR incoming in the next release to improve subagent output (I should have responded to the issue earlier, that's my miss).Yoshi and others -- please keep the feedback coming. We want to hear it, and we genuinely want to improve the product in a way that gives great defaults for the majority of users, while being extremely hackable and customizable for everyone else.reply",
      "I can\u2019t count how many times I benefitted from seeing the files Claude was reading, to understand how I could interrupt and give it a little more context\u2026 saving thousands of tokens and sparing the context window.  I must be in the minority of users who preferred seeing the actual files.  I love claude code, but some of the recent updates seem like they\u2019re making it harder for me to see what\u2019s happening.. I agree with the author that verbose mode isn\u2019t the answer. Seems to me this should be configurablereply",
      "I think folks might be crossing wires a bit. To make it so you can see full file paths, we repurposed verbose mode to enable the old explicit file output, while hiding more details behind ctrl+o. In effect, we've evolved verbose mode to be multi-state, so that it lets you toggle back to the old behavior while giving you a way to see even more verbose output, while still defaulting everyone else to the condensed view. I hope this solves everyones' needs, while also avoiding overly-specific settings (we wanted to reuse verbose mode for this so it is forwards-compatible going fwd).To try it: /config > verbose, or --verbose.Please keep the feedback coming. If there is anything else we can do to adjust verbose mode to do what you want, I'd love to hear.reply",
      "I'll add a counterpoint that in many situations (especially monorepos for complex businesses), it's easy for any LLM to go down rabbit holes. Files containing the word \"payment\" or \"onboarding\" might be for entirely different DDD domains than the one relevant to the problem. As a CTO touching all sorts of surfaces, I see this problem at least once a day, entirely driven by trying to move too fast with my prompts.And so the very first thing that the LLM does when planning, namely choosing which files to read, are a key point for manual intervention to ensure that the correct domain or business concept is being analyzed.Speaking personally: Once I know that Claude is looking in the right place, I'm on to the next task - often an entirely different Claude session. But those critical first few seconds, to verify that it's looking in the right place, are entirely different from any other kind of verbosity.I don't want verbose mode. I want Claude to tell me what it's reading in the first 3 seconds, so I can switch gears without fear it's going to the wrong part of the codebase. By saying that my use case requires verbose mode, you're saying that I need to see massive levels of babysitting-level output (even if less massive than before) to be able to do this.(To lean into the babysitting analogy, I want Claude to be the babysitter, but I want to make sure the babysitter knows where I left the note before I head out the door.)reply",
      "> I don't want verbose mode. I want Claude to tell me what it's reading in the first 3 seconds, so I can switch gears without fear it's going to the wrong part of the codebase. By saying that my use case requires verbose mode, you're saying that I need to see massive levels of babysitting-level output (even if less massive than before) to be able to do this.To be clear: we re-purposed verbose mode to do exactly what you are asking for. We kept the name \"verbose mode\", but the behavior is what you want, without the other verbose output.reply",
      "This is an interesting and complex ui decision to make.Might it have been better to retire and/or rename the feature, if the underlying action was very different?I work on silly basic stuff compared to Claude Code, but I find that I confuse fewer users if I rename a button instead of just changing the underlying effect.This causes me to have to create new docs, and hopefully triggers affected users to find those docs, when they ask themselves \u201cwhat happened to that button?\u201dreply",
      "Yeah, in hindsight, we probably should have renamed it.reply",
      "It's not too late.reply",
      "> To be clear: we re-purposed verbose mode to do exactly what you are asking for. We kept the name \"verbose mode\", but the behavior is what you want, without the other verbose output.Verbose mode feels far too verbose to handle that. It\u2019s also very hard to \u201ckeep your place\u201d when toggling into verbose mode to see a specific output.reply",
      "I think the point bcherny is making in the last few threads is that, the new verbose mode _default_ is not as verbose as it used to be and so it is not \"too verbose to handle that\". If you want \"too verbose\", that is still available behind a togglereply"
    ],
    "link": "https://symmetrybreak.ing/blog/claude-code-is-being-dumbed-down/",
    "first_paragraph": ""
  },
  {
    "title": "Sekka Zusetsu: A Book of Snowflakes (1832) (publicdomainreview.org)",
    "points": 12,
    "submitter": "prismatic",
    "submit_time": "2026-02-09T03:49:52 1770608992",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://publicdomainreview.org/collection/japanese-snowflake-book/",
    "first_paragraph": "Search The Public Domain ReviewTitled Sekka Zusetsu (\u96ea\u83ef\u56f3\u8aac), this 1832 book of woodblock prints by Doi Toshitsura (1789\u20131848) reflects twenty years of a life devoted to \u201csnow flowers\u201d (sekka). An Edo-era feudal lord (daimy\u014d), who ruled the Koga Domain in what is today\u2019s Shim\u014dsa Province, he was perhaps the first person in Japan to observe ice crystals under a microscope.Sekka Zusetsu contains eighty-six firsthand observations of snowflakes as well as a dozen reproduced from J. F. Martinet\u2019s Katechismus der natur (1779). Doi Toshitsura\u2019s process for making his sketches was simple: on a suitably chilly evening, he would place a black cloth outside to pre-cool it with cold air. Then, gathering freshly fallen snow on the blanket, he transferred each flake individually using tweezers to a lacquerware tray for microscopic observation, being careful not to exhale toward his specimens lest they dissolve. To complete his studies, which he began while still a teenager, he worked closely with Taka"
  },
  {
    "title": "Components will kill pages (bitsandbytes.dev)",
    "points": 40,
    "submitter": "cmsparks",
    "submit_time": "2026-02-11T19:27:46 1770838066",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=46979604",
    "comments": [
      "I think that the inverse thesis is true, you make websites more accessible (a11y, wgac, aria labels etc) for humans, then the interaction heuristics are clearer for agents functioning off browser-use or similar. If a screen reader can understand your site, then an agent can. Reinventing the wheel to facilitate the current state of agents makes the web worse for everyone, it's not a preemptive move, it's actually a decline in almost objective and measurable quality, and potentially one which removes access to the internet by people who just want to.. use the internet.reply",
      "This. The accessibility tree is a superpower for agents when it's good. Screenshots are \"robust\" but low performance. Like so many other things, making stuff better for humans indirectly makes it better for agents.reply",
      "> [Given] a method for chat applications to understand components exposed from the website in question [...] AI chat applications win, and so does the brand that gets to keep ownership of how it tells the story of its product to its users.Never happening. The brand may \"keep ownership of how it tells its story,\" but it loses its users. You have turned your tool into a series of widget in someone else's application, with no control whatsoever over how users interact with you. Want to show your user a notification? (Sure you do\u2014I can't get away from the things.) Too bad. ChatGPT owns your users, and they only see what OpenAI wants them to see, which likely will not include ads for your premium features.Don't mistake this for user freedom, either. Users still won't own their own tools. We're just moving from a model where each vendor separately leases you their tool to a model where every tool is leased via OpenAI, which curates them based on its own monopolistic whims.Tech companies will not surrender control of their users so easily. They may integrate chatbot components into their apps, but they will not permit an inversion of control where their product becomes a component in a chatbot.> You likely won\u2019t expect users 5 years from today to navigate 5 pages deepOf course I do. There's this fallacy that, because chatbots are useful for some things, chatbot interfaces must be the best at everything, and that's just not true. I don't go to ChatGPT to ask it for relevant tech news. I go here and browse the HN frontpage. Chatbots offer zero discoverability; search bars didn't replace page navigation, and chat bots won't either.reply",
      "AI component libraries in your site make your web app even more easily consumed and subsumed by AI chat clients.This not only kills pages, but it kills the concept of a browser where the user agent is a human, rather than making your pages be designed where the user agent is an AI agent.That doesn't make me happy to experience because I'm guessing that after a generation or so, web designers will not only do mobile first designs with stupid amounts of white space and not taking advantage of the desktops greater screen real estate and precise mouse movements, but AI first websites will get so popular that browsing sites manually will look like trying to use a text only browser in the JavaScript world.Easy for me to do a depressing take, but hopefully the bitter lesson of AI will help this particular projected future not come to pass because the AI will get smart enough that it will embed a browser right there in line and just render the window for the user, or it will otherwise gets good enough at screen scraping and UI automation that it can just use an existing browser, just like a human, the sites won't be dumbed down even further for AI consumption.reply",
      "to add to that, it kills the concept of whether the host is human (and not just another soulless megacorp harvesting your data in their walled garden)https://news.ycombinator.com/item?id=46969751 remark that they're taking down their self-hosted projects citing costs associated with AI scraping.at best we have walled garden content; and when those are scraped (either by the host or by more sophisticated bots) those walled gardens will hopefully rot under an inability to drive advertisement revenue.I agree, I think we're at the edge of a paradigmatic shift away from humans navigating TCP-IP itself. What that looks like, I don't know, but given trends (like dynamic pricing, human-futures marketing, surveillance, and consolidation of computing under mega-companies) I can imagine: local beacons screaming AI advertisement components across a geospatial sneakernet. Auditorium-based ticketed podcasting and AR/VR/meatspace events. Thoughtful hackers reminiscing of better times simulating them in web-assembly driven first-person POV \"sites\" and a rolling set of encryption keys for read-access (just send them BTC)without an ecosystem for humans to contribute meaningfully to a feedback loop that allows for free group assembly around like interests, monetary growth for hosts and other participants, and some degree of presence / searchability / permanence, the current text-only web page paradigm is doomed.reply",
      "> AI first websites will get so popular that browsing sites manually will look like trying to use a text only browser in the JavaScript world.That might be great for accessibility, though.reply",
      "> In a world where we can type anything into a text box and get the information back instantly we are circumventing the need to visit websites altogether.This is purely anecdotal, but the only people in my extended circle making this transition (to any extent) are the technically savvy; everyone else is slowly realizing how awful AI tools and \"AI-first experiences\" can be and are actively trying to avoid them.reply",
      "I've noticed this bimodal distribution of perception too, and my hypothesis is that's it's hugely driven by the difference of \"who is in the driver's seat\".Your tech-savvy AI early adopters are discerning between tools, the deployments and environments, and are willing and able to change things to extract the highest output from current capabilities. For instance, re-architecting a codebase to make it easier for agents to contribute to it.The rest are having AI hypeware shoved upon them, often as a cost cutting measure, and lack the agency to influence outcomes. When agents misbehave, they only have the option to \"Press 0 to speak with a Human\" and hope that works.I suspect this is a big factor in the divide we're seeing, and might result in your median adult being ambushed by recent gains in capabilities.reply",
      "AI in apps is garbage. Cheap, low quality models and inflexible interaction patterns.AI agents using frontier models, configured nicely, that interact with programs that have APIs are pure gold.reply",
      "I think the more realistic direction is exposing API / MCP-style interfaces for agents to interact with a product\u2019s functionality, rather than shipping UI components that an AI client would render.The \"AI renders your components inside chat\" idea feels very similar to Facebook\u2019s old canvas apps. That model disappeared for good reasons: abuse, security, and loss of platform control.It seems far more likely that AI platforms will provide their own interaction primitives (forms, pickers, confirmations, etc.) and simply call third-party tools behind the scenes. That lets the platform retain control over UX and safety, and avoids the risks of embedding arbitrary third-party UI.reply"
    ],
    "link": "https://bitsandbytes.dev/posts/components-will-kill-pages",
    "first_paragraph": ""
  },
  {
    "title": "WiFi Could Become an Invisible Mass Surveillance System (scitechdaily.com)",
    "points": 305,
    "submitter": "mgh2",
    "submit_time": "2026-02-07T01:16:42 1770427002",
    "num_comments": 147,
    "comments_url": "https://news.ycombinator.com/item?id=46920315",
    "comments": [
      "This is a VERY controlled environment - and they used 20 passes of each person walking with direct knowledge of each person to train for identity. They did no tests with multiple people walking at the same time, or with any other external moving distortion effects (doors opening, etc) . This is very far from actual 'identification' of people in real public settings - and no doubt the cell phone everyone is carrying with them offers many orders of magnitude better opportunity. In a real crowded environment this would be nearly worthless.The devices that reported BFI information were also stationary, and there were no extra devices transmitting information that would be conflicting.A single camera would be much more effective.reply",
      "Yes, but things could be refined. With more resources and research thrown at it, it could become more versatile, that's why the title of the post says \"could\". And chances are, there are private and government entities already doing this. Research like this has been coming out for at least a decade now.Even Xfinity has motion detection in homes using this technique now:https://www.xfinity.com/hub/smart-home/wifi-motionreply",
      "Yeah, it can and will be refined, but the major limiting factor is resolution. Wi-Fi radio waves are just too big to get a very clear image.reply",
      "like i mentioned in another comment, do you really need good resolution for gait analysis? You also have people carrying their phones inside the house all the time, so you know what bssid is associated with that coarse movement. and if you have access to their ap/router combo, you can tell what IP that device has and what domains it's been visiting.Let's say you visit a friend in a different city, the same ISP controlling their router, can use your mac, but even if you turn off your wifi or leave your phone in your car, your volume profile and gait can betray you. how you sit, how you lean, how you turn. I'd wager, if 6-10 distinct \"points\" can be made out and associated with a person, that's all that's needed to uniquely identify that person after enough analysis of their motion, regardless of where they go in the world.Imagine if they're not using one AP, but using your neighbors AP as well, two neighbor APs and your own can triangulate and refine much better.reply",
      "for now ...reply",
      "No this is fixed by physics. 5ghz waves are about 60mm wavelength.Your resolution limit is about 30mm as a result.reply",
      "There are techniques that can reduce that limit when you have multiple signals, though whether they can be combined with this technique isn\u2019t clear.reply",
      "This has already been an area of research, both publicly, and most likely in private/government defense research. In a targeted situation, i.e. surveillance of a household of 6, this would work easily enough...but I doubt there is enough information to provide reliable (high AUC) tagging of ID in a public scenario oh hundreds to thousands of individuals.reply",
      "https://www.theregister.com/2025/07/22/whofi_wifi_identifier...> Researchers in Italy have developed a way to create a biometric identifier for people based on the way the human body interferes with Wi-Fi signal propagation.. can re-identify a person in other locations most of the time when a Wi-Fi signal can be measured. Observers could therefore track a person as they pass through signals sent by different Wi-Fi networks \u2013 even if they\u2019re not carrying a phone.. their technique makes accurate matches on the public NTU-Fi dataset up to 95.5 percent.reply",
      "I bet there is.Off the top of my head, I bet body composition combined with gait analysis would be enough to uniquely identify an individual.reply"
    ],
    "link": "https://scitechdaily.com/researchers-warn-wifi-could-become-an-invisible-mass-surveillance-system/",
    "first_paragraph": ""
  },
  {
    "title": "Hacking the last Z80 computer \u2013 FOSDEM 2026 [video] (fosdem.org)",
    "points": 24,
    "submitter": "michalpleban",
    "submit_time": "2026-02-07T13:09:12 1770469752",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=46923554",
    "comments": [
      "There were the TI calculators produced through the 2000s, still powered by Z80, just not a \"computer\".reply",
      "That's the first thing mentioned in the video.reply"
    ],
    "link": "https://fosdem.org/2026/schedule/event/FEHLHY-hacking_the_last_z80_computer_ever_made/",
    "first_paragraph": "The Z80 CPU has been extremely popular in home computers of the eighties, but as 16-bit and 32-bit processors became more popular, the only new computers built using the Z80 were continuations of some legacy lines (like the Amstrad PCW).And yet, in 1999 a company named Cidco unveiled a completely new computer line named the MailStation. with a Z80 CPU clocked at 12 MHz and 128 kB of RAM. It was a specialized machine for sending and receiving emails, addressed at people for whom configuring Web access on a PC was too complicated. Yet it was still a computer, with a screen, keyboard, means of communicating with the outside world and possibility of running user apps. Most likely the last new Z80 computer ever designed.In my talk I would like to present this machine, show how it can be hacked to run custom software, and encourage the audience to join me in documenting the machine and writing custom firmware for it.MailStation emulator:\nhttps://github.com/MichalPleban/mailstation-msemuHost "
  },
  {
    "title": "Ireland rolls out basic income scheme for artists (reuters.com)",
    "points": 135,
    "submitter": "abe94",
    "submit_time": "2026-02-11T16:39:11 1770827951",
    "num_comments": 133,
    "comments_url": "https://news.ycombinator.com/item?id=46977175",
    "comments": [
      "I don\u2019t get it. Why are artists more deserving than unemployed insurance salespeople or carpet installers?reply",
      "Agreed. Can just all myself an artist to get other people's tax money?reply",
      "This is admittedly a tangent, but I love that British (and apparently Irish) government programs are commonly called \"schemes\". To American ears, it always sounds like some grand confidence trick is being pulled.reply",
      "In India too, discounts and promotional policies are commonly called 'schemes.' I learned the hard way that in the US, the word has a negative connotation when I asked my rental office about any 'schemes,' they looked at me with total shock.reply",
      "As an Irish person, in normal speech the word \"scheme\" has exactly the same shady connotations as it does for Americans. Calling someone a \"schemer\" is a common insult. I've always assumed the government started using the word in a rare moment of honesty and it stuck.reply",
      "Or perchance it is the other way around. The word started as official term and over time got shady connotation because can't trust Big Government.reply",
      "Growing up in the UK, we would be sent to a \u201cplay scheme\u201d during the school holidays. Weird phrase.reply",
      "They had something like this in the Netherlands during the 80s. Basically everyone was out of a job back then so it didn't really matter. Worst recession since 1929.Artists had to make a buch of art which was then given to the government. The state ended up with entire warehouses filled with crap.reply",
      "There was also the WPA program in the USA:https://en.wikipedia.org/wiki/Works_Progress_AdministrationThe work also included infrastructure projects, and often would create public art to decorate the infrastructure. That is why you'll see far more decorative work when looking at bridges from that era, for example.reply",
      "I remember learning about this in high school, but grew up in a part of a large city that only really developed after the 1940's, I didn't think much of it. However, the name was catchy so I had it stashed in my memory somewhere.As I've gone on to live in a few older cities, I have been surprised the number of times that I have (for example) come across a bridge or tunnel or whatnot and seen a big serif \"WORKS PROGRESS ADMINISTRATION 1936\" plaque on one side of it. It always feels like stepping into an alternate reality where history is more present and real.It feels like a silly way to phrase it, but growing up where only a handful of buildings were older than 40 years, encountering history in a more banal form, like a simple bridge with some engravings, always feels more impactful than seeing some 500-year-old castle, monument or other touristy site.reply"
    ],
    "link": "https://www.reuters.com/world/ireland-rolls-out-pioneering-basic-income-scheme-artists-2026-02-10/",
    "first_paragraph": ""
  },
  {
    "title": "Microwave Oven Failure: Spontaneously turned on by its LED display (2024) (stuffedcow.net)",
    "points": 68,
    "submitter": "arm",
    "submit_time": "2026-02-11T19:50:43 1770839443",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=46979936",
    "comments": [
      "My guess is the LED's suffer reverse bias thermal runaway when they're hot from being in a steamy enclosure and then they get a reverse 5v across them and any leakage current turns into heat accelerating the process.reply",
      "Wouldn't it be more likely to be reverse-bias degradation of the LED junction causing permanently increased leakage current?reply",
      "All LEDs are photodiodes too, certain degredations of parts or poor circuit design could lead to the display turning into a switch.reply",
      "Prev discussion:https://news.ycombinator.com/item?id=41480038reply",
      "Articles like these are great to argue nonconformity which can get you your money back in EU. Even past the warranty period.reply",
      "From what I can find, the guarantee period seems to be two years, after which the burden seems to 'flip'. Given that this microwave is at least five years old, I am not sure what standard one might cite to demonstrate 'non-conformity'. Do you know of a standard which says that a consumer microwave oven must work for more than five years?https://europa.eu/youreurope/citizens/consumers/shopping/gua...reply",
      "168 points and 116 comments at the time:  https://news.ycombinator.com/item?id=41480038reply",
      "This is literally evidence of stuff being designed to fail. An extra diode costs less than a cent at production scale. This was a manufacturing choice, not an error.reply",
      "My microwave mainboard failed because I changed the range light bulb without unplugging the whole microwave first, which I would not have thought necessary.  It seems that, without unplugging the whole microwave, the act of changing a light bulb will cause catastrophic voltage to delicate parts.  Turns out to be a common thing with this brand.I ended up replacing the mainboard with a part from no apparent manufacturer with new features (the blue LEDs dim after inactivity so as not to illuminate the whole room at night) and no connection for the thermistor.  Works like a charm.  It feels very much like the original manufacturer wanted the board to fail and be replaced, while some random Chinese circuit-board maker sold me a better quality board.reply",
      "nah, this is just not something designer would expect to fail like that. The LED has datasheet, the datasheet have leakage current, it has no data on increased leakage over years, you plan for what you have.What would help is not randomly planning for some of the segments to fail (they are multiplexed with other things, you'd have to put more diodes), but to just get slightly better/less cheap LED displayOnly \"choice\" made here was sorting by price when buying components for the cheap device.reply"
    ],
    "link": "https://blog.stuffedcow.net/2024/06/microwave-failure-spontaneously-turns-on/",
    "first_paragraph": "Random stuff\u2026My microwave oven started to malfunction at around five years old. It started to randomly power on the lamp, fan, and turntable. It progressively got worse over several weeks until it was mostly stuck on. The microwave oven is not usable when this happens: It behaves as if the door were open, causing the control panel to ignore button input and to stop cooking if it was cooking.The obvious suspect is a failing door switch, which is a common cause of failure. There are three switches in the door, and a failure of one or more of them can cause strange behaviour when not all three switches agree on whether the door is open or closed. However, all three switches were tested to be in good working condition, so the most obvious reason is not the cause of this failure.My microwave is an Insignia NS-MW09SS8 (a Best Buy brand), which is manufactured by Midea (FCC ID: RSFXM925AYY), with model number EMXAUXX-05-K marked on the circuit board inside. There are many other brands/models "
  },
  {
    "title": "Amazon Ring's lost dog ad sparks backlash amid fears of mass surveillance (theverge.com)",
    "points": 429,
    "submitter": "jedberg",
    "submit_time": "2026-02-11T18:43:01 1770835381",
    "num_comments": 239,
    "comments_url": "https://news.ycombinator.com/item?id=46978966",
    "comments": [
      "The Dark Knight was released in 2008. In that movie, Batman hijacks citizens' cellphones to track down the Joker, and it's presented as a major moral and ethical dilemma as part of the movie's overall themes. The only way Batman remains a \"good guy\" in the eyes of the audience is by destroying the entire thing once he's done.Crazy to think that less than two decades later, an even more powerful surveillance technology is being advertised at the Super Bowl as a great and wonderful thing and you should totally volunteer to upload your Ring footage so it can be analyzed for tracking down the Jok... I mean illegal imm... I mean lost pets.reply",
      "Pulled from IMDB, Morgan Freeman as Lucius Fox voices the consternation perfectly:> Batman: [seeing the wall of monitors for the first time at the Applied Sciences division in Wayne Enterprises] Beautiful, isn't it?> Lucius Fox: Beautiful... unethical... dangerous. You've turned every cellphone in Gotham into a microphone.> Batman: And a high-frequency generator-receiver.> Lucius Fox: You took my sonar concept and applied it to every phone in the city. With half the city feeding you sonar, you can image all of Gotham. This is wrong.> Batman: I've gotta find this man, Lucius.> Lucius Fox: At what cost?> Batman: The database is null-key encrypted. It can only be accessed by one person.> Lucius Fox: This is too much power for one person.> Batman: That's why I gave it to you. Only you can use it.> Lucius Fox: Spying on 30 million people isn't part of my job description.reply",
      "That system is nothing compared to the geolocation databases curated by Apple and Google, with GPS sensors combined with Wi-Fi wardriving, IMEI tracking, cell tower handoffs, and the rest of the insane amount of telemetry they collect collect in real time. And that\u2019s before even considering BLE and the Find My network. Imagine the \u201cGod mode\u201d dashboards they could have in Cupertino (or more likely, in Mountain View).reply",
      "Imagine a Google Maps / Google Earth where you can see everyone\u2019s location and identity in real time, with tagging/targeting/following capabilities and quick links to thorough personal profiles.reply",
      "Go back a little bit further to another Morgan Freeman movie - Se7en (1995) and a big plot point was that it is unthinkable for big brother to be keeping records of what library books people are checking out.  Times sure have changed...reply",
      "Lmao did they really say it's null-key encrypted?Unfortunately a very realistic depiction of how many of the brands advertising their security the strongest often have the most ridiculously broken security (flock)reply",
      "I rewatched recently. That's what he says all right.reply",
      "He also says \"aflongaflongkong\". https://youtu.be/0ukMXA0SJaMreply",
      "I mean it is technobabble but in some way it is also poetic.reply",
      "It's funnier than typical technobabble because they're literally saying its not encrypted. The writers knew what they were doing, I'm surereply"
    ],
    "link": "https://www.theverge.com/tech/876866/ring-search-party-super-bowl-ad-online-backlash",
    "first_paragraph": "Posts from this topic will be added to your daily email digest and your homepage feed.See All TechPosts from this topic will be added to your daily email digest and your homepage feed.See All ReportPosts from this topic will be added to your daily email digest and your homepage feed.See All AnalysisA new AI-powered Search Party feature can scan footage from neighborhood cameras to find lost dogs. Critics worry it could be used to search for people.A new AI-powered Search Party feature can scan footage from neighborhood cameras to find lost dogs. Critics worry it could be used to search for people.Posts from this author will be added to your daily email digest and your homepage feed.See All by Jennifer Pattison TuohyIf you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement.Posts from this author will be added to your daily email digest and your homepage feed.See All by Jennifer Pattison TuohyRing\u2019s new Search Party feature has once again drawn bac"
  },
  {
    "title": "GLM-OCR \u2013 A multimodal OCR model for complex document understanding (github.com/zai-org)",
    "points": 218,
    "submitter": "ms7892",
    "submit_time": "2026-02-07T14:15:36 1770473736",
    "num_comments": 67,
    "comments_url": "https://news.ycombinator.com/item?id=46924075",
    "comments": [
      "There are a bunch of new OCR models.I\u2019ve also heard very good things about these two in particular:- LightOnOCR-2-1B: https://huggingface.co/lightonai/LightOnOCR-2-1B- PaddleOCR-VL-1.5: https://huggingface.co/PaddlePaddle/PaddleOCR-VL-1.5The OCR leaderboards I\u2019ve seen leave a lot to be desired.With the rapid release of so many of these models, I wish there were a better way to know which ones are actually the best.I also feel like most/all of these models don\u2019t handle charts, other than to maybe include a link to a cropped image. It would be nice for the OCR model to also convert charts into markdown tables, but this is obviously challenging.reply",
      "I have been trying to catch up with recent OCR developments too. My documents have enough special requirements that public benchmarks didn't tell me enough to decide. Instead I'm building a small document OCR project with visualization tools for comparing bounding boxes, extracted text, region classification, etc. GLM-OCR is my favorite so far [1]. Apple's VisionKit is very good at text recognition, and fast, but it doesn't do high level layout detection and it only works on Apple hardware. It's another useful source of data for cross-validation if you can run it.This project has been pretty easy to build with agentic coding. It's a Frankenstein monster of glue code and handling my particular domain requirements, so it's not suitable for public release. I'd encourage some rapid prototyping after you've spent an afternoon catching up on what's new. I did a lot of document OCR and post-processing with commercial tools and custom code 15 years ago. The advent of small local VLMs has made it practical to achieve higher accuracy and more domain customization than I would have previously believed.[1] If you're building an advanced document processing workflow, be sure to read the post-processing code in the GLM code repo. They're doing some non-trivial logic to fuse layout areas and transform text for smooth reading. You probably want to store the raw model results and customize your own post-processing for uncommon languages or uncommon domain vocabulary. Layout is also easier to validate if you bypass their post-processing; it can make some combined areas \"disappear\" from the layout data.reply",
      "I'm going to be the obnoxious person who asks you to please create this leaderboard because you care and have a modicum of knowledge in this space.reply",
      "How do these compare to something like Tesseract?I remember that one clearing the scoreboard for many years, and usually it's the one I grab for OCR needs due to its reputation.reply",
      "Tesseract does not understand layout. It\u2019s fine for character recognition, but if I still have to pipe the output to a LLM to make sense of the layout and fix common transcription errors, I might as well use a single model. It\u2019s also easier for a visual LLM to extract figures and tables in one pass.reply",
      "For my workflows, layout extraction has been so inconsistent that I've stopped attempting to use it. It's simpler to just throw everything into postgis and run intersection checks on size-normalized pages.reply",
      "Interesting. What kind of layout do you have?My documents have one or two-column layouts, often inconsistently across pages or even within a page (which tripped older layout detection methods). Most models seem to understand that well enough so they are good enough for my use case.reply",
      "Documents that come from FOIA. So, some scanned, some not. Lots of forms and lots of hand writing to add info that the form format doesn't recognize. Lots of repeated documents, but lots of one-off documents that have high signal.reply",
      "I'd be very curious what works well with FOIA historical documents that have been scanned by hand with redactions by markers & etc.reply",
      "I don't know how, but PyMuPDF4LLM is based on Tessaract and has GNN-based layout detectionreply"
    ],
    "link": "https://github.com/zai-org/GLM-OCR",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        GLM-OCR: Accurate \u00d7 Fast \u00d7 Comprehensive\n      \n    \ud83d\udc4b Join our WeChat and Discord community\n    \n    \ud83d\udccd Use GLM-OCR's API\nGLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoder\u2013decoder architecture. It introduces Multi-Token Prediction (MTP) loss and stable full-task reinforcement learning to improve training efficiency, recognition accuracy, and generalization. The model integrates the CogViT visual encoder pre-trained on large-scale image\u2013text data, a lightweight cross-modal connector with efficient token downsampling, and a GLM-0.5B language decoder. Combined with a two-stage pipeline of layout analysis and parallel recognition based on PP-DocLayout-V3, GLM-OCR delivers robust and high-quality OCR performance across diverse document layouts.Key FeaturesState-of-the-Art Performance: Achieve"
  },
  {
    "title": "Show HN: Agent framework that generates its own topology and evolves at runtime (github.com/adenhq)",
    "points": 38,
    "submitter": "vincentjiang",
    "submit_time": "2026-02-11T19:39:43 1770838783",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=46979781",
    "comments": [
      "Strongly agree on the 'Toy App' ceiling with current DAG-based frameworks. I've been wrestling with LangGraph for similar reasons\u2014once the happy path breaks, the graph essentially halts or loops indefinitely because the error handling is too rigid.The concept of mapping 'exceptions as observations' rather than failures is the right mental shift for production.Question on the 'Homeostasis' metric: Does the agent persist this 'stress' state across sessions? i.e., if an agent fails a specific invoice type 5 times on Monday, does it start Tuesday with a higher verification threshold (or 'High Conscientiousness') for that specific task type? Or is it reset per run?Starred the repo, excited to dig into the OODA implementation.reply",
      "Failures of workflows signal assumption violations that ultimately should percolate up to humans. Also, static dags are more amenable to human understanding than dynamic task decomposition. Robustness in production is good though, if you can bound agent behavior.Best of 3 (or more) tournaments are a good strategy. You can also use them for RL via GRPO if you're running an open weight model.reply",
      "In HNese this means \"very impressive, keep up the good work.\"reply",
      "3. What, or who, is the judge of correctness (accuracy); regardless of the many solutions run in parallel. If I optimize for max accuracy how close can I get to 100% matemathically and how much would that cost?reply",
      "I am of course unqualified to provide useful commentary on it, but I find this concept to be new and interesting, so I will be watching this page carefully.My use case is less so trying to hook this up to be some sort of business workflow ClawdBot alternative, but rather to see if this can be an eventually consistent engine that lets me update state over various documents across the time dimension.could I use it to simulate some tabletop characters and their locations over time?that would perhaps let me remove some bookkeeping how to see where a given NPC would be on a given day after so many days pass between game sessions. Which lets me do game world steps without having to manually do them per character.reply",
      "That's a very interesting use case you brought to the table! I've also dreamt about having an agent as my co-host running the sessions. It's a great PoC idea we might look into soon.reply",
      "I was sort of thinking about a similar idea recently. What if you wrote something like a webserver that was given \"goals\" for a backend, and then told agents what the application was supposed to be and told it to use the backend for meeting them and then generate feedback based on their experience.Then have an agent collate the feedback, combined with telemetry from the server, and iterate on the code to fix it up.In theory you could have the backend write itself and design new features based on what agents try to do with it.I sort of got the idea from a comparison with JITs, you could have stubbed out methods in the server that would do nothing until the \"JIT\" agent writes the code.reply",
      "It\u2019s funny I\u2019ve been pondering something similar. I\u2019ve started by writing an agent first api framework that simplifies the service boundary and relies on code gen for sql stubs and APIs.My next thought was to implement a multi agent workforce on top of this where it\u2019s fully virtuous (like a cycle) and iterative.https://github.com/swetjen/virtuousIf you\u2019re interested in working on this together my personal website and contact info is in my bio.reply",
      "Fascinating concept, you essentially frame the backend not as a static codebase, but as an adaptive organism that evolves based on real-time usage.A few things that come to my mind if I were to build this:The 'Agent-User' Paradox: To make this work, you'd need the initial agents (the ones responding and testing the goals) to be 'chaotic' enough to explore edge cases, but 'structured' enough to provide meaningful feedback to the 'Architect' agent.The Schema Contract: How would you ensure that as the backend \"writes itself,\" it doesn't break the contract with the frontend? You\u2019d almost need a JIT Documentation layer that updates in lockstep.Verification: I wonder if the server should run the 'JIT-ed' code in a sandbox first, using the telemetry to verify the goal was met before promoting the code to the main branch.It\u2019s a massive shift from Code as an Asset to Code as a Runtime Behavior. Have you thought about how you'd handle state/database migrations in a world where the backend is rewriting itself on the fly? It feels to me that you're almost building a lovable for backend services. I've seen a few OS projects like this (e.g. MotiaDev) But none has executed this perfectly yet.reply",
      "The \"JIT\" agent closely aligns with the long-term vision we have for this framework. When the orchestrating agent of the working swarm is confident enough to produce more sub-agents, the agent graph(collection) could potentially extend itself based on the responsibility vacuum that needs to be filled.reply"
    ],
    "link": "https://github.com/adenhq/hive/blob/main/README.md",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          "
  },
  {
    "title": "Kanchipuram Saris and Thinking Machines (altermag.com)",
    "points": 3,
    "submitter": "trojanalert",
    "submit_time": "2026-02-07T06:19:05 1770445145",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://altermag.com/articles/kanchipuram-saris-and-thinking-machines",
    "first_paragraph": ""
  },
  {
    "title": "We rendered and embedded one million CAD files (cad-search-three.vercel.app)",
    "points": 97,
    "submitter": "DavidFerris",
    "submit_time": "2026-02-10T23:12:36 1770765156",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=46968374",
    "comments": [
      "Neat.As a mechanical engineer, I feel the part of my job is safe from AI for the time being. I don't think quality training data for good mechanical design exists.3D CAD is only part of good design. To a tinker-er that is 3D printing simple parts, an STL is fine. But most parts that matter require far more design consideration and detail than simply the geometry data that an STL (or other 3D file) provides.The majority of parts are accompanied with a drawing, and that is where the real design actually is found: Tolerances, GD&T, materials, processing notes...Even then, most of the calculations and considerations to build the model and drawing are not explicit in the design documents: Nothing about a drawing of a stainless steel part tells you WHY it must be a stainless steel part. I don't think there is a large set of well documented designs out there to act as training data for an AI system to design an assembly beyond basic 3D parts.The authors identify this gap, but it's a fundamental problem with the wholesale move to AI in mechanical design.reply",
      "Amusingly, I tried. Your job is safe. ;-) I'm a scientist. I tried using a couple of free 3d modeling tools that are programmable in Python, using Claude within VS Code to design a part according to my prompts. It was a simple plate with some holes for electrical connectors, switches, etc. Of course it let me make some noobie design mistakes, such as making the hole exactly the same diameter as the thing that went into it, plus or minus of course. And I would have had to really scrutinize the Python code to notice that I got one of the diameters wrong altogether.Just judging from this experience, the effort would rise exponentially with the complexity of the part, not to mention assemblies. The designers earn their keep. I get really bad eyestrain with any task that requires staring at the screen while doing fine mouse work, so I just can't use CAD. On the other hand, I can code all day because I'm not closely focused on the screen when I'm typing.reply",
      "Agreed. At the end of the day, manufactured parts are driven by constraints outside of the CAD environment so analyzing 3D data as the foundation of an AI system strikes me as attacking the problem from the wrong direction. i.e. Simple optimization of a part for injection molding can take it from requiring a bunch of side actions and collapsing cores to a simple 2 sided mold and save hundreds of thousands of dollars in tooling. None of that is obvious from 3D data alone.That said, I am excited for AI assisted CAD tools. Things like creating and applying global variables to an existing part, complex assembly analysis for part reduction or just making a starting base part can be incredibly tedious and are low hanging fruit for improving CAD workflows with AI imo.reply",
      "a lot of the why is encoded elsewhere in mechanical engineering at least - the tables, the formulas, textbook problems, engineering reports.one of the challenges to making a good data set might be around bad designs and why they failed. if we get to a mechanical agent, its going to need to understand that brass was a mistake and redesign a part as steel and change the design for the new contraintsunlike code, that kind of train of experiment i think will be a lot more expensive to make, since you might actually want to create those parts along the way and not just pretendreply",
      "I think a real value of AI in Engineering will be when it can act as a reviewer.Reviewing drawings for incompleteness, contradictory informations, deviations from standards, etc and making recommendations would be very valuable.It doesn\u2019t need to be perfect to be useful.reply",
      "Several searches I tried produced terrible results with no objects matching the description on the first page:  M8x30 SHCS\n  Lathe chuck\n  19\" rack mount blank plate\n\nThe thumbnail descriptions are dominated by boilerplate, such as \"The depicted object consists of a wall-mounted shelf unit featuring a rectangular back\"The long descriptions are full of irrelevant fluff and don't answer basic questions like what size it is. For example, on a screw:> The model features a hexagonal socket head cap screw, characterized by its cylindrical shaft and a larger, flared hexagonal head. The head's interior is designed to accommodate a hex key, allowing for easy installation and removal. The shaft tapers slightly towards the tip, providing an elongated profile. The smooth surface indicates precision engineering, suitable for applications requiring a strong and secure fastening. The overall design emphasizes functionality while ensuring compatibility with various tools and mechanical setups.A broad search \"vise\" returns a long list of identical, very eccentric, vises the likes of which I've never seen before.reply",
      "We rendered the one million part ABC dataset from Deep Geometry, and open-sourced the data. We also built a fun demo with the following pipeline: CAD > render > caption > embed.Open-sourced dataset: https://huggingface.co/datasets/daveferbear/3d-model-images-...Blog writeup: https://www.finalrev.com/blog/embedding-one-million-3d-model...reply",
      "The search function doesn\u2019t seem to work at all, it provides nonsensical results.For example if I search \u201csupercolumns\u201d I get regular household furniture.reply",
      "Yeah I think the embedding are describing what can be seen from a picture of the model not what it is or what it is used for.\nsearch some things work like \"Fan\" but others don't so you can search for \n\"plate with 5 holes\" but not for specific engine part cover.reply",
      "Neat, but also hilarious! Searching for \"mug\" gives results where the first item listed (ABC-00008297) is a mug model with a hole not only in the top to pour in your drink, but also in the side and bottom (just in case you wanted more access to your liquid).reply"
    ],
    "link": "https://cad-search-three.vercel.app/",
    "first_paragraph": "Find 3D models using natural languagePowered by the ABC Dataset \u00b7 About"
  }
]