[
  {
    "title": "Sora 2 (openai.com)",
    "points": 539,
    "submitter": "skilled",
    "submit_time": "2025-09-30T16:55:01 1759251301",
    "num_comments": 574,
    "comments_url": "https://news.ycombinator.com/item?id=45427982",
    "comments": [
      "I haven't seen comments regarding a big factor here:It seems like OpenAI is trying to turn Sora into a social network - TikTok but AI.The webapp is heavily geared towards consumption, with a feed as the entry point, liking and commenting for posts, and user profiles having a prominent role.The creation aspect seems about as important as on Instagram, TikTok etc - easily available, but not the primary focus.Generated videos are very short, with minimal controls. The only selectable option is picking between landscape and portrait mode.There is no mention or attempt to move towards long form videos, storylines, advanced editing/controls/etc, like others in this space (eg Google Flow).Seems like they want to turn this into AITok.Edit: regarding accurate physics ... check out these two videos below...To be fair, Veo fails miserably with those prompts also.https://sora.chatgpt.com/p/s_68dc32c7ddb081919e0f38d8e006163...https://sora.chatgpt.com/p/s_68dc3339c26881918e45f61d9312e95...Veo:https://veo-balldrop.wasmer.app/ballroll.mp4https://veo-balldrop.wasmer.app/balldrop.mp4Couldn't help but mock them a little, here is a bit of fun... the prompt adherence is pretty good, at least.NOTE: there are plenty of quite impressive videos being posted, and a lot of horrible ones also.reply",
      "Not to be a downer, but even as someone very optimistic about technology and AI generally, \"TikTok but AI\" sounds like a societally terrible thing to try and create.What's the benefit of this? Curious if anyone has a solid viewpoint steelmanning any positives they can think of.reply",
      "As a social experiment to reveal how senseless and pointless pop entertainment could be.(personal rant) I've been in a mild existential crisis since I read Amusing Ourselves to Death. Can one form of entertainment really be more well-regarded than another? Is fine art fundamentally different from pop art? Are there 'finer' pop cultures amongst all pop cultures? I do still think reading The Song of Ice and Fire is more meaningful than scrolling TikTok. The crisis part is that I can't justify this belief with words.reply",
      "There are two completely distinct differences that jump out to me initially that I think may help justify your feelings:1: Reading a long book demands focus on a longer timespan than scrolling TikTok, and with focusing on a single thing for a long time, we get a sense of accomplishment. I don\u2019t know how to justify this as valuable, but for some reason I feel that it is.2: The Song of Ice and Fire (and GoT) were consumed by a huge proportion of people, and you now have this in common with them. This act of consuming entertainment also grants you a way to connect with other humans - you have so much to talk about. Contrast that with an algorithmic feed, which is unique just for you - no one else sees your exact feed. Of course, there are tons of people that see some of the same snippets of content, if their interests overlap with yours, but it\u2019s not nearly as universal as having read the same series of books (and there\u2019s much less to talk about when you\u2019ve seen the same 17-second short form video than when you\u2019ve both invested dozens of hours in reading the same series of books).I don\u2019t think these thoughts fully justify your belief, but hopefully they provide some support to it.reply",
      "I think the point 2 will rub many people the wrong way (me included) though. That would make reading Fourth Wing or Twilight a more connecting experience than most classics. (Nothing inherently wrong with that, but... you know...)reply",
      "> Can one form of entertainment really be more well-regarded than another? Is fine art fundamentally different from pop art?It depends on what you want to get out of art.Do you want human connection and shared cultural context so you can talk to real friends about things? Do you want virtual friends and connections? Do you want ideas to inspire you to create your own things, or change how you think?Do you just want to distract yourself from how hungry you are, how much inequality is in the world, and how depressed you are, letting death draw closer?All of those are valid things, and different art is more meaningful for different goals.Scrolling tiktok fits into the last one, it's burning time to avoid thinking about things, moving you closer to death. Song of Ice and Fire builds a large coherent world, has bits of morality and human relation, and all of those can spark ideas and be related to your own human suffering, so it indeed feels more valid to me as a way to reflect and change how you think.reply",
      "One analogy is to liken tiktok (and shortform content) as exploring the shallows. Walking around, close to the shoreline, you explore pieces of flotsam that the sea washes your way. You might spend a lifetime on this shore, walking up and down, but most would argue that you've actually never gone anywhere.On the other hand, reading a book is like getting on a boat. You've made certain preparations for acquiring the vessel and set course through unknown territory. A journey away from the shore and away from what's immediately at hand, which can also turn out to be a journey towards self-discovery.reply",
      "I think reading does force more long term focus, even if it's marginal for certain books. Certainly moreso than scrolling TikTok.My personal process of grappling with this led to a focus on agency and intentionality when defining the difference.Scrolling TikTok, much as scrolling Twitter or Facebook or Instagram or YouTube's recommendations would be, is an entirely passive activity. You sit back and you allow the Content to be fed to you.Reading a book requires at least a bare minimum of selecting a book to read, choosing to finish that book, and intentionally choosing at any given time to spend your time reading that particular book. Similar things can be said for selecting movies. The important part in my mind is that you chose it, rather than letting someone or something else pick what they think you'll like.The process of picking things yourself allows you to develop taste and understand what you like and dislike, mentally offloading that to someone or something else removes the opportunity to develop that capability.I think there's arguments to be made against this view: how can you decide what to read or watch without getting recommendations or opinions? If you only engage with popular media isn't it just a slower process of the same issue?But I do believe there is a fundamental difference between passivity and active evaluation of engagement as mental processes, and it's the exact reason why it is harder to do than scrolling is.reply",
      "If society only consisted of the people in a given sector/industry, could it continue and flourish? If we only had engineers, how would society fare versus if we only had influencers? In this paradigm, there's no difference between fine art and pop art.reply",
      "There are probably ways you could explore this quantitatively by trying to measure the amount of novel latent information in the data you are ingesting, or by trying to quantify its effects on cognition.Most short form content would probably score low. It\u2019s short, for one, and it tends to be repetitive and lack anything like plot complexity or nuance.Of course it\u2019s not like trite pop is new. Way back in the dime store novel days it was called pulp. TikTok is just one of the latest iterations. People have always consumed dumb filler.reply"
    ],
    "link": "https://openai.com/index/sora-2/",
    "first_paragraph": ""
  },
  {
    "title": "Drunk CSS (shkspr.mobi)",
    "points": 19,
    "submitter": "FromTheArchives",
    "submit_time": "2025-09-27T13:28:58 1758979738",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=45395549",
    "comments": [
      "The theme switcher in general is really cool. Especially being able to see and compare the \"nude\" version. It's like a modern day CSS Zen Gardenhttps://csszengarden.com/reply",
      "This is not what things look like when you are drunk.(Source: have been drunk many times, and used a computer.)reply",
      "i have been drunk, this is not itreply",
      "now that i got to experience being drunk, id rather not, thanksreply"
    ],
    "link": "https://shkspr.mobi/blog/2025/09/drunk-css/",
    "first_paragraph": "css drunk HTML ui ux webdev  \u00b7 600\u00a0words \u00b7 Viewed\u00a0~2,329\u00a0timesA decade ago, I was writing about how you should test your user interface on drunk people. It was a semi-serious idea.  Some of your users will be drunk when using your app or website. If it is easy for them to use, then it should be easy for sober people to use.Of course, necking a few shots every time you update your website isn't great for your health - so is there another way?Click the \"\ud83e\udd74 Drunk\" button at the top of the page and see what happens!These are a relatively simple set of CSS rules which you can apply to any site in order to simulate inebriation.(I may have changed these since writing the post. Check the source for the latest version.)First, monkey around with the fonts. This sets all the lower-case vowels to be rendered in a different font - as discussed in \"targetting specific characters with CSS rules\":The rest of the characters will be rendered in the system's default Cursive font. Characters will also be s"
  },
  {
    "title": "Diff Algorithms (znkr.io)",
    "points": 124,
    "submitter": "znkr",
    "submit_time": "2025-09-30T20:09:44 1759262984",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=45430604",
    "comments": [
      "The creator of the Myers algorithm is Gene Myers.  He also helped create the BLAST algorithm, one of the fastest and most important DNA and protein search algorithms, and also implemented most of the original human genome assembly done by Celera.  he also helped invent and publish the suffix array.reply",
      "There are at least 3 fundamentally different kinds of diff:* Single-dimensional. Diffs of text lines are just this.* Multi-dimensional. Diffs of words or characters are usually going to be this since lines still matter, but there are multiple approaches (line-first? weighted tokens?).* Tree-based. Unfortunately, these are woefully scarce and poorly documented.For text diffs, it's nontrivial to get the \"missing newline at end of file\" logic working.For tree diffs, consider that for HTML something like `<p>x</p><p>y</p>` should be unmergeable, whereas `<b>x</b><b>y</b>` should be mergeable.(Aside: the blind promotion of `<em>` and `<strong>` did great harm to the notion of semantic HTML. Most things people use italics for (book titles, thoughts, foreign words) are explicitly things that `<em>` should not be used for.)reply",
      "I implemented tree based diff for a JSON superset https://github.com/gritzko/go-rdx\nIt boils down to single dimensional, very much like JSON or DOM tree is represented as a linear text.reply",
      "Another thing I\u2019ve encountered with tree/structured diffs is a concept of identity. diff([{id:1,name:foo}],[{id:2,name:foo}] should show object w/ id:1 removed and id:2 added, not id changed from 1 to 2. Tough because then your diffing algo needs to be aware of the object structure (imo using convention and saying \u201cno objects can contain this key\u201d is pretty tough when you accept any user generated data).reply",
      "Can you explain why the `p` example is unmergeable whereas the `b` one isn't? I can't see any difference between the two examples other than the tag used.reply",
      "The first is:    One paragraph.\n\n    Followed by another.\n\nThe second is two bold letters, one after another in a single word.However if the html is \"an application\" more than it is \"a document\" - a b-tag with two letters, might be meaningfully different from two b-tags in sequence (for example with css:)   b { display: block }\n\nSo, I'd say as a fragment two bold tags might be mergable - but not in the general case?Ed: ie if diffing input from a html input field (rich editor) merging bold tags would probably be what you want - when the first edit bolds first letter, and second edit bolds second letter.reply",
      "I've also worked with probabilistic diff- like tree-based, but tolerant of parsing errors.reply",
      "Mildly related: my favorite tool for viewing .git diffs diff2html - a CLI that with one command opens the diff in your browserhttps://diff2html.xyz/ -- https://github.com/rtfpessoa/diff2htmlreply",
      "A while ago I discovered the lesser known Tichy diff algorithm that seems to preserve more context and is better suited for big code refactors compared to Myers: https://www.researchgate.net/publication/220439403_The_Strin... via https://bryanpendleton.blogspot.com/2010/04/more-study-of-di...reply",
      "Apart from source code versioning what are the other most important real world use cases of diff algorithms ?reply"
    ],
    "link": "https://flo.znkr.io/diff/",
    "first_paragraph": "For software engineers, diffs are a ubiquitous method for representing changes: We use diffs to\ncompare different versions of the same file (e.g., during code review or when trying to understand\nthe history of a file), to visualize the difference of a failing test compared with its\nexpectation, or to apply changes to source files automatically.Every project I worked on professionally or privately eventually needed a diff to visualize a change\nor to apply a patch. However, I have never been satisfied with any of the freely available diff\nlibraries. This was never really a problem professionally, but for private projects, I have copied\nand modified my own library from project to project until I mentioned this to a colleague who set me\non the path to publish my Go library (a port of a previous C++ library I used to copy and modify).\nBoy, did I underestimate how close my library was to publishability!Anyway, I did it and I learned a whole lot about diff algorithms. You can find my library "
  },
  {
    "title": "Introduction to Multi-Armed Bandits (2019) (arxiv.org)",
    "points": 63,
    "submitter": "Anon84",
    "submit_time": "2025-09-30T21:08:28 1759266508",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=45431271",
    "comments": [
      "We employed bandits in a product I worked on. It was selecting which piece of content to show in a certain context, optimizing for clicks. It did a great job, but there were implications that I wish we understood from the start.There was a constant stream of new content (i.e., arms for the bandits) to choose from. Instead of running manual experiments (e.g., A/B tests or other designs), the bandits would sample the new set of options and arrive at a new optimal mix much more quickly.But we did want to run experiments with other things around the content that was managed by the bandits (e.g., UI flow, overall layout, other algorithmic things, etc.). It turns out bandits complicate these experiments significantly. Any changes to the context in which the bandits operate lead them to shift things more towards exploration to find a new optimal mix, hurting performance for some period of time.We had a choice we could make here... treat all traffic, regardless of cohort, as a single universe that the bandits are managing (so they would optimize for the mix of cohorts as a whole). Or we could setup bandit stats for each cohort. If things are combined, then we can't use an experiment design that assumes independence between cohorts (e.g., A/B testing) because the bandits break independence. But the optimal mix will likely look different for one cohort vs. another vs. all of them combined. So it's better for experiment validity to isolate the bandits for each cohort. Now small cohorts can take quite a while to converge before we can measure how well things work. All of this puts a real limit on iteration speed.Things also become very difficult to reason about because their is state in the bandit stats that are being used to optimize things. You can often think of that as a black box, but sometimes you need to look inside and it can be very difficult.Much (all?) of this comes from bandits being feedback loops - these same problems are present in other approaches where feedback loops are used (e.g., control theory based approaches). Feedback mechanisms are incredibly powerful, but they couple things together in ways that can be difficult to tease apart.reply",
      "The real trouble with bandits is that people don't bother to look into what the real potential benefit is as far as the target you're optimizing. Despite theoretically loving bandit techniques, I've convinced multiple teams not to go that path because the real advantage of using them is a slightly more optimal mix of people in experiment than if you ran them manually.At some scales it can make sense, but for the vast majority of products/companies the statistical benefits don't outweigh the added complexity.Bandits work best for relatively simple optimization choices at very large scale. If you care at all about the information gained in an experiment, it's fairly unlikely bandits are a good choice for you (or any reinforcement learning algorithm at that point).reply",
      "Agreed. In the case I was describing above, new arms were constantly being introduced (often several times a day for each of hundreds of thousands of scenarios). Manual experiments weren't an option. This also meant we were in a constant state of partial convergence for most scenarios, but the same would be true with experiments.How to cull arms, so that there are enough samples for any kind of convergence, is another problem in this setup. We eventually built an ML model to select arms and used bandits to pick between them. This proved \"too effective\". The arms were all user-generated content. The bandits on top of the model, both setup to maximize clicks was stupidly good at surfacing inappropriate content because it got a lot of clicks. We ended up having to put more safeties on arm selection for certain categories of our content where we had the most inappropriate submissions.reply",
      "I\u2019ve actually run into the exact same issue. At the time we similarly had to scrap bandits. Since then I\u2019ve had the opportunity to do a fair amount of research into hierarchical dirichelete processes in an unrelated field.On a random day, a light went off in my head that hierarchy perfectly addresses the stratification vs aggregation problems that arise in bandits. Unfortunately I\u2019ve never had a chance to apply this (and thus see the issues) in a relevant setting since.reply",
      "You can do fairly well here with ridge regression as a poor man's hierarchical model. We've used this library's Bayesian ridge regression to support a geo-pricing strategy (and it contains the Dirichlet-Multinomial approach as well): https://github.com/bayesianbandits/bayesianbanditsreply",
      "Ahh, hierarchical dirichlet processes. Sounds like you were reading the literature on Bayesian diffusion modelling / diffusion trees. I studied that stuff almost 20 years ago now, really takes me back.reply",
      "Haha I\u2019ve actually never heard of that field. My work was focused on applying Chinese restaurant process models to text analysis. But very curious what you were working on?reply",
      "\u201c If things are combined, then we can't use an experiment design that assumes independence between cohorts (e.g., A/B testing) because the bandits break independence.\u201dCan you explain, please?reply",
      "Not the OP. I think what they are driving at is that if knowledge is discovered during exploration in cohort A, cohort B can exploit it. Then, the whole A/B test breaks down to which cohort got to benefit more from the bandit learnings.reply",
      "Yes, this is exactly the kind of scenario I was alluding to.For example, cohorts with very light traffic are likely to get undue benefit as a lot of exploration might be done before the smaller cohort needs to select an arm, so things are closer to convergence.Another example is if there are wildly different outcomes between cohorts. More of the exploration will be done in cohorts with more traffic, leading bandit optimizations to fit large cohorts better than lower traffic cohorts.Even if you do manage to make things independent, you have to wait for bandit convergence before you know what converged results will look like. That requires being able to measure convergence, which isn't always easy, especially if you don't know to do that before designing the system.With all of these problems, we kept bandits, and even expanded their application. At least for the 10 years I was still around. They are incredibly powerful. But there was a lot of \"I wish those damned bandits didn't work so well!\"For anyone who is not aware, A/B tests assume cohorts behave independently of each other. The less true that is, the less reliable the results are. This was even worse for us in parts of our system where there were no bandits, but there was direct interactions between individuals.reply"
    ],
    "link": "https://arxiv.org/abs/1904.07272",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Boeing has started working on a 737 MAX replacement (wsj.com)",
    "points": 193,
    "submitter": "bookofjoe",
    "submit_time": "2025-09-30T17:31:34 1759253494",
    "num_comments": 297,
    "comments_url": "https://news.ycombinator.com/item?id=45428482",
    "comments": [
      "It'll be interesting to see if they still can design and build a new ground-up airplane design.  The last all-new design was the 787, initiated in 2003 and launched in 2009, and its design was fraught with problems.  Before then was the 777 in the early 90s (pre-McDonnell takeover), and the 757/767 in the early 80s.There's a phenomena that ofter occurs with large organizations where once their markets mature, everybody who can build a product end-to-end leaves or gets forced out, leaving only people with highly specialized maintenance skillsets.  The former group has no work to do, after all, so why should the company keep them around?  But then if the market ecosystem shifts, and a new product is necessary, they no longer have the capacity to build ground-up new products.  All those people have left, and won't come anywhere near the company.Steve Jobs spoke eloquently about this phenomena in an old interview:https://www.youtube.com/watch?v=K1WrHH-WtaAreply",
      "To add to this & the Jobs interview - an oil industry proverb: a healthy oil company has a geologist in charge, a mature one has an engineer in charge, a declining one has an accountant in charge, and a dying one has a lawyer in charge.reply",
      "A bit ironic though because the CEO of Boing during their best years was William McPherson Allen, a lawyer.reply",
      "One of the most exceptional CEOs I've worked with was a lawyer. I still think the proverb is largely correct, along with the other proverb about the exception proving the rule.reply",
      "There are always people who work out despite common sense saying they shouldn't that doesn't mean common sense is wrong, it just means we don't understand what the real factors are.reply",
      "Or we don't care because it's a rule of thumb not a law of physics.reply",
      "He initially turned down the job because he felt that a lawyer wasn't the right person to run an engineering company, and from reports of people who worked with him he knew his knowledge limits and listened to the engineers. He took serious risks with the 707 and 747 projects because he trusted the people who understood the technology.MBAs and final-gasp lawyers concentrate on making the reported number go up in the short term, they won't take a hit now for a payoff in ten years.reply",
      "Fun fact the 707 had the first implementation of \u201cMCAS\u201d because the plane had a tendency to pitch up in a certain flaps configuration. They added a stick nudger which applied light pressure in said config. Not a stick pusher, as it did not alert the pilots, it simply applied an extra input independent of the pilots. However this was made aware to all pilots of the plane and likely contributed to its certification.Also the 707 tail was extended by 40ft to give it better minimum ground speed control, this was retroactively applied to already built planes. Very interesting to see how this was applied in the past with a lawyer at the helm vs the current ceo during the launch of the 737Maxreply",
      "Assume 40 inches rather than 40ftAdding 12 meters to an aircraft is quite a big change.reply",
      "Mistake on my part they extended the tail to 40ft from 32ft"
    ],
    "link": "https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df",
    "first_paragraph": ""
  },
  {
    "title": "Imgur pulls out of UK as data watchdog threatens fine (express.co.uk)",
    "points": 318,
    "submitter": "ANewbury",
    "submit_time": "2025-09-30T13:01:05 1759237265",
    "num_comments": 332,
    "comments_url": "https://news.ycombinator.com/item?id=45424888",
    "comments": [
      "There's an opportunity for a service like CloudFlare here give people a simple toggle that manages geoblocks on legal liability factors. It's way too much for every organisation to individually track every country's laws day by day in case just by being accessible there you incur a liability. And it sounds like the UK would have just self-selected out of the list of \"safe\" countries.If something like this was in widespread use it would have much more impact since countries would see whole swathes of the internet immediately go dark when they make stupid laws.reply",
      "I wish Wikipedia would take one for the team, and go dark in the UK. (And I'm in the UK).Wouldn't work with somewhere like China, but the UK might still be capable of being shamed.reply",
      "[flagged]",
      "I can't imagine that working for even more than a couple of days. If it did we have much bigger problems than access to Wikipedia and would like be looking at the beginning of civil disobedience and war as I doubt wikipedia would be the only freedom of speech site.reply",
      "https://dash.cloudflare.com/?to=/:account/:zone/security/sec...  (ip.src.country eq \"GB\")\n\nthen take action \"Block\". i know what you mean by a simpler option thoughreply",
      "The point the parent is making is that you don\u2019t have to manually keep track of the countries you need to block. You just tell Cloudflare what your website does / what type of laws may be problematic, and Cloudflare manages the blocklist automatically.Makes a lot of sense actually that it\u2019s surprising they don\u2019t have this yet.reply",
      "So a service like CloudFlare is the Great firewall of the world and CloudFlare can shut you down if you go against their interests as a supranational gatekeeper.Smart thinking Batman.reply",
      "> CloudFlare can shut you down if you go against their interests as a supranational gatekeeperThey already can.> Smart thinking Batman.\u201cBe kind. Don't be snarky. Converse curiously; don't cross-examine. Edit out swipes.\u201dhttps://news.ycombinator.com/newsguidelines.htmlreply",
      "\u201cBe kind. Don't be snarky.\"Sometimes I think you can ignore the \"rules\" a bit - they are really guidelines. Your parent was clearly expressing exasperation and engaging effectively and intelligently.reply",
      "Not really.  It's more like Cloudflare is providing an ipset in your iptables config.  It's not Cloudflare's decision: they're just making it easier for you to do it.reply"
    ],
    "link": "https://www.express.co.uk/news/uk/2115228/image-site-imgur-pulls-out",
    "first_paragraph": "An image hosting platform with more than 130 million users has stopped being available in the UK after regulators signalled their intention to impose penalties over concerns around children\u2019s data.The Information Commissioner\u2019s Office (ICO) said that it has reached provisional findings in an investigation in the parent company of image hosting site, Imgur. Its probe was launched earlier this year, as part of the regulator's Children\u2019s Code strategy, which is intended to set the standards for how online services handle the personal information of young people.In a statement the ICO said: \u201cWe are aware of reports that the social media platform Imgur is currently not available in the UK. Imgur's decision to restrict access in the UK is a commercial decision taken by the company.\u201d Read more:  Shopkeepers warned sharing images of criminals 'may break data protection rules'  Read more:  Stop 'booping' the cows - Fury at 'risky' social media behaviour in countryside  Read more:  Ex-Labour cou"
  },
  {
    "title": "Inflammation now predicts heart disease more strongly than cholesterol (empirical.health)",
    "points": 360,
    "submitter": "brandonb",
    "submit_time": "2025-09-30T20:00:21 1759262421",
    "num_comments": 214,
    "comments_url": "https://news.ycombinator.com/item?id=45430498",
    "comments": [
      "Before you assume that LDL isn\u2019t a good biomarker, read the entire article. Specifically this section:> Why? In some ways, cholesterol has become a victim of its own success. We now screen the whole population for high cholesterol, give statins to those with high LDL (or ApoB), and so then the majority of people who end up having heart attacks have lower cholesterol than they would naturally haveIn other words, in the study population patients who would have had high LDL were likely to be on statins. The had a lower measured LDL value even though they might still be consuming a poor diet and living an unhealthy lifestyle, for example. Statins don't fix everything about poor diet and lifestyle, but they do help with cholesterol.So don\u2019t go throwing LDL out yet. It\u2019s still the best measure we have, though you should obviously know that LDL measured while on statins is lower than it would be normally.The headline, therefore, is somewhat clickbait from a company trying to sell these tests to you outside of your insurance. I recommend checking your insurance to see if the tests would be covered before you go the self-pay route.Edit to add: If your doctor won't order hs-CRP for some reason, you can order it from sites like privatemdlabs.com for $50 (less if you take their 25% off coupon).reply",
      "There have always been people with high LDL who lived to a very old age and finally died of something other than a heart attack (nobody knows why).  Still high LDL after controlling for everything we can think of (cholesterol is cheap to measure so we have a lot of data!) is a strong sign of a future heart attack and so anyone with high LDL should talk to their doctor: there is good reason to think statins will reduce your chance of a heart attack. Which why we measure it and control it.If you have normal cholesterol though - we have long known that people with normal cholesterol also have heart attacks.  It isn't as common as people who have high cholesterol, but it is still very common for someone normal cholesterol to have a heart attack.  We don't really know what to do about this though.  This article is saying we should measure inflammation and if found deal with it. Seems reasonable.What isn't known is if we deal with inflammation will heart attacks go away or if there are more factors.  If there are more factors we don't know what they are or if they are worth measuring/treating (though some researchers may have data they are trying to get out here).  If dealing with inflammation is good, can we start ignoring cholesterol - another unknown (one for researchers to look into, but the rest of us should for now say no cholesterol is independently important - until data says otherwise)reply",
      "LDL is a proxy measure that's cheap and easy to measure. It's widely used for screening despite not being perfect, which confused some into thinking it's the one and only thing measure of CVD risk. It's not, though. Many of the tests we look at are proxies and markers, not actually the sole factor for a disease.More in-depth testing would check LDL-P (particle count) and ApoB along with hsCRP.Though realistically, most people could simply look at their diet and lifestyle and work on improving both before investing in any extra testing. The testing can be useful to catch cases where genetics overwhelm even healthy lifestyles, but in many cases for younger people the testing basically serves as a wake-up call to actually do something about lifestyle and diet problems. It's easier to inspire lifestyle and diet changes when you're staring at bad numbers on the test results and getting a little preview of the consequences of your decisions.reply",
      "> If you have normal cholesterol though - we have long known that people with normal cholesterol also have heart attacks.Just to continue on this line, we also know that people with a genetic disposition for low cholesterol have a significantly lower risk of coronary heart disease than people with ostensibly normal cholesterol levels. It is one of the most proven, obvious correlations (more cholesterol increases the incident of CHD) in medicine.Some snake-oil merchants, usually pitching a book or supplement, have often tried to muddy the waters by pointing out that someone at death's door often has very low cholesterol (they usually aren't eating, and cancer often \"eats\" cholesterol and leads to low levels), trying to then extrapolate this out.reply",
      "(OP here) LDL is still a good biomarker, but ApoB is a better biomarker for the same undelrying risk factor -- each atherogenic particle (LDL, VLDL, IDL) has exactly one ApoB molecule.The reason we offer the tests as cash pay is that it's the only way we can guarantee the price. In the past, when we've gone through insurance, the insurer's \"negotiated rate\" for the same exact panel comes out to $1,400-$1,500. If the insurer later decides to deny coverage for any of the tests, it's more expensive for the patient.The $190 price is negotiated to be pretty low. It includes hs-CRP ($59 by itself online), but also the other major heart health biomarkers: ApoB ($69), Lp(a) ($49), A1c ($39), lipid panel ($59), eGFR ($99), other biomarkers, and a video consultation with a doctor to actually explain the results and what to do about them.For hs-CRP in particular, it's not covered under the ACA as a preventive benefit, so you would usually need to hit your deductible before insurance kicks in at all. (That's assuming they count it as medically necessary at all -- for example, Aetna's current medical policy for hs-CRP requires 2 risk factors, LDL in a specific range, and overall cardiovascular risk to be in a certain range or the claim would simply be denied). It's possible this will change over time as the ACC/AHA recommend universal screening, and I hope it does, but it's a relatively a slow process since it depends on the US Preventive Services Task Force to issue a formal recommendation.reply",
      "Goodlabs prices:https://app.hellogoodlabs.com/book-tests  ApoB  $12\n  LP(a) $20\n  A1c   $ 4\n  Lipid $ 8\n  eGFR  $30 (Under \"Cystatin C with Glomerular Filtration Rate, Estimated (eGFR)\")\n\n  Total: $74\n\nSo no, I wouldn't call $190 \"pretty low\", lol.reply",
      "Their comprehensive men\u2019s panel is $200, which seems like the most comparable package. (The list of biomarkers above is a subset of the ones in our panel\u2014obviously if you remove biomarkers, you\u2019ll end up with a cheaper price.)reply",
      "FYI the above poster is the founder of the company selling these tests (EDIT: He edited his comment to include the \"OP here\" intro after I posted my comment. Thanks!)> In the past, when we've gone through insurance, the insurer's \"negotiated rate\" for the same exact panel comes out to $1,400-$1,500.hs-CRP is not going to be a $1500 negotiated rate under any insurance these days. No sane insurance company is going to pay that.I understand that the negotiated rate you're talking about is for an entire panel of many markers, but most of these are not necessary for a quick screen and many would already be covered by an ACA annual checkup.>  It includes hs-CRP ($59 by itself online),hs-CRP is $50 right now at privatemdlabs.com before the 25% off coupon they're blasting me with in the pop-up. It appears to be in the $40-45 range at a few other direct lab companies.reply",
      "I updated the above post to say (OP here). I thought it was fairly clear from my phrasing of \"we offer the tests as cash pay\", but never hurts to be even more explicit.I checked privatemdlabs.com, and they're asking $249 for a cardiovascular panel that covers the same biomarkers. So I think our pricing compares pretty favorably, especially when you consider that we offer an MD review.> hs-CRP is not going to be a $1500 negotiated rate under any insurance these days.The $1500 insurance price is for the $190 panel. We've actually dealt with this exact situation in the last month, since every so often labs will make a mistake when they process the order (we obviously fix these situations). Perhaps the insurance companies aren't sane (a topic for another day), but this is unfortunately how the system works today.reply",
      "Commenting to add - Insurance negotiated rate may actually be 1500$. If it is and they charge insurance 1500$. They legally cannot charge an individual a different or lower rate. Even if that person doesn\u2019t have insurance and offers to pay cash.This is one of those weird horrible traps health insurance puts you into. OP may charge insurance 1500$, insurance may only pay 20%. But that now means they have to charge individuals the full 1500$ price.So honestly, Cudos to the OP for identifying this trap and then moving to just charging a reasonable flat rate."
    ],
    "link": "https://www.empirical.health/blog/inflammation-and-heart-health/",
    "first_paragraph": " Brandon Ballinger   Sep 29, 2025  Chronic inflammation has long been known to double your risk of heart disease, but prior to now,\ninflammation has never been a SMuRF: standard modifiable risk factor for heart disease.The American College of Cardiology just released recommendations that change that. The ACC is now recommending that everyone\nmeasure inflammation (specifically, hs-CRP) via a blood test:Because clinicians will not treat what they do not measure, universal screening of hsCRP in both primary and secondary prevention patients, in combination with cholesterol, represents a major clinical opportunity and is therefore recommended. American College of CardiologyThere were many interesting bits of evidence that led to this recommendation. The whole article, published in JACC, is worth a read, but this blog post extracts a few of the most interesting parts \u2014 or at\nleast, the parts I thought were most interesting.For decades, LDL cholesterol (or ApoB) has been the main focus of ca"
  },
  {
    "title": "Rio Terminal: A hardware-accelerated GPU terminal emulator (rioterm.com)",
    "points": 14,
    "submitter": "birdculture",
    "submit_time": "2025-10-01T00:29:33 1759278573",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=45432977",
    "comments": [
      "was using it for a month, cool concept, it borrows stuff from alacritty for the base, added iterm2 inline images protocol, kitty image protocol soon\nbut there are quite a lot of issues, mainly just really weird ones\n1. splits dont feel like splits. unlike in something like wezterm or ghostty, rio uses a literal black line for splits, which cannot be controlled. it doesnt properly resize splits, which you can notice if you have like 4 splits active \n2. tabs inherit from the previous tab's zoomed in height and width https://github.com/raphamorim/rio/issues/1196\n3. blur doesnt work on window (not that big of an issue anyways) https://github.com/raphamorim/rio/issues/1174\n4. some characters are broken (no clue how this happens) https://github.com/raphamorim/rio/issues/1192\n5. has issues with iterm2 while using yazi\nits a cool concept like i previously said, but it needs a lot of tweakingreply",
      "Does anyone get anything out of these GPU accelerated terminals? \nI am at a bit of a loss for a use-case where my CPU isn't fast enough to keep up.What I do care about is my bitmap font, which all these new terms don't seem to like supporting.reply"
    ],
    "link": "https://rioterm.com/",
    "first_paragraph": "A modern terminal for the 21st century.The Rio has fast performance, leveraging the latest technologies including Rust and advanced rendering architectures.Regular terminals are limited to just 256 colors, the Rio supports \"true color,\" which means it can display up to 16 million colors.Display images within the terminal using Sixel and iTerm2 image protocol.Rio is a cross-platform app that runs on Windows, macOS, Linux, and FreeBSD.Font ligatures support as a way to improve readability of common expressions or operators.Support to split and manage terminal screens in any platform that you would want to.Rio support configure custom filters and CRT shaders through RetroArch shader files."
  },
  {
    "title": "Economics of sportsbooks and why they ban the best bettors (dopaminemarkets.com)",
    "points": 36,
    "submitter": "_1729",
    "submit_time": "2025-09-30T23:36:23 1759275383",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=45432627",
    "comments": [
      "I had thought bookies set the odds so that half of incoming bets were on either side.  They don\u2019t care which outcome occurs and just take their cut either way.  I guess at low volume it\u2019s hard to find that optimal spot and this is what results in losses.reply",
      "Odds providers have their models on how likely each outcome is. They add a spread to this (for example if it's 50/50, they'll pay out 1.8x instead of 2x). The line can also be affected by other factors, like how likely it is one side is oversubscribed. They don't make money on all markets, just mostBut if someone has a model that is so significantly better than theirs that it beats the line _and_ spread, they will make money in the long run. Haralabos Voulgaris for example is likely one of the most successful sports bettors. Very interesting guy imoreply",
      "They have to compete with others. If the 'good' betters go with you that means you have worse odds that your competition and anyone else who knows goes elsewhere.the real money is in pools where only a few people bet. however that works best if you get people to bet on the unlikely thing you want to take the other side as you can win more often. Often the unlikely thing is only possible because you take a side - so the savey better is taking away your bets.reply",
      "No, it doesn't work that way. Bettors, in this context, aren't price-sensitive so the book is never balanced. There are some tools that bookmakers use to get more balanced - handicap markets - but it still never ends up being balanced if you are a largely retail-facing book.reply",
      "There's more to it. Even if the sports-book functions as an exchange, holding ~zero stake in the outcomes, they still want to ban anyone who consistently wins.That's because if you have a few smart players who consistently win, that means you have a lot of fools who will consistently lose money on the platform - which will drive them away, and impact both betting activity and the spread you collect.Ideally, you want a bunch of uninformed fools generating huge volume gambling against eachother, while you siphon away spread.It's a deplorable, lose-lose industry.reply",
      "This isn't correct.Asian books (not a geographical distinction but a business model) are not exchanges and they effectively pay sharps to bet with them. This is why betting syndicates exist, they set prices, and the business model of Asian books is to make it back on volume from having the best prices (and before legalisation, these were the biggest books in the world, they did billions every week in handle).But the business model is different: retail-facing bookies have to win their customer base back every week so they need to spend more marketing. Asian books also do promos but they tend to be one-time (which are cheaper to run), they don't have a non-sportsbook business typically, and they don't comply with regulators.Exchanges do not drive away sharps either. Their business model is largely about providing an environment for sharps. The only times were that hasn't been true is when a syndicate has owned an exchange (this happened with Matchbook). The exchange provides an incentive to invest for people to invest in price discovery, which happens. There is no way for the business to run without someone being incentivized to discover prices. People randomly gambling against each other isn't a business model.reply",
      "AIUI, two other factors are:the most lucrative (sports) bets are complex and highly unlikely parley bets, betters here naturally tend to all take one sideThey want to offer bets on many sports, matches, and specific events that don't get a lot of bets, so they have to take the riskreply",
      "I mean, yes, obviously being a bookie where you can set a crazy spread on convoluted bets is more profitable than being an agnostic, fair betting exchange - but I was pointing out that even a fair betting exchange has every incentive to kick winners off its platform.reply",
      "The price on most betting exchanges is unbeatable without significant resources (it is set based on Asian books, they will open lines to sharps before betting exchanges list the market).The price on parlays, with the huge spread, is far more vulnerable because there is no large market for these events. Prices for large events are mostly gathered from third parties, parlays will tend to be priced in-house.This is part of the reason why bookmakers control who can bet on what on their site. If they opened parlay line with $100k limit to anyone, they would be bankrupted by the end of the day. The purpose of these bets is to create a product that is exciting for customers and is economic for them to provide...but that requires there to be limit on how much customers can bet.The most profitable product are the handicap markets and point spreads even though the margin for these is usually low single digits. Asian books usually only provide these markets...but the problem is that customers don't like these as much as parlays. The product is entertainment.reply",
      "> It's a deplorable industry.If we start decrying every industry whose business model is based on information asymmetry we are going to have a lot of uncomfortable folk on this forumreply"
    ],
    "link": "https://www.dopaminemarkets.com/p/the-business-of-sports-betting-is",
    "first_paragraph": ""
  },
  {
    "title": "Leaked Apple M5 9 core Geekbench scores (geekbench.com)",
    "points": 197,
    "submitter": "aurareturn",
    "submit_time": "2025-09-30T16:00:36 1759248036",
    "num_comments": 276,
    "comments_url": "https://news.ycombinator.com/item?id=45427197",
    "comments": [
      "Nice.  The iPads generally measure around ~8% slower than the MacBooks, I guess for cooling reasons.  So we should see approximately a 4400 single core Geekbench score for the MacBook series.  This is nice.Single thread MacBook progression on Geekbench:M1: 2350M2: 2600M3: 3100M4: 3850M5: 4400 (estimated)https://browser.geekbench.com/mac-benchmarksreply",
      "Keep in mind that a big part of the huge jump in recent chips was that GB6 added support for SME, and to my knowledge, no app uses SME as of yet. GB5 is a better benchmark for all these chips for this reason.The actual IPC increase and perf/clock of these chips excluding SME specific acceleration is MUCH smaller.reply",
      "I've benchmarked these myself on things like my project's build time on M1, M2 and M3 and I did see similar gains.  So I disagree from experience.reply",
      "SME is just the AMX coprocessor that\u2019s been in Apple chips since 2019. SME made it easier to target the AMX. But it\u2019s been in use and available to developers since 2019.reply",
      "The point stands that virtually no apps used AMX (either directly or through a framework).reply",
      "> The point stands that virtually no apps used AMX (either directly or through a framework).AMX has been present in every M series chip and the A series chips starting with the A13.  If you are comparing M series chip scores in Geekbench 6 they are all using it, not just the latest ones.Any app using Apple's Accelerate framework will take advantage of it.reply",
      "Nice. Lots of people still claim that M1 is super duper fast but now we're at almost twice the performance!reply",
      "M1 is still fast, the speed of its successors does not change that.reply",
      "I use one for software development and it's great. Sometimes rust builds are slow and I'd love to force that to be faster with hardware (optimizing build time would be a huge undertaking with not-so-great returns), otherwise I'm totally content. I also have an M2 Max with 32GB of RAM that still feels like magic. I've never had computers that felt so fast for so long.reply",
      "I dunno, working with M1 daily I struggle with resource contention and slow py/js builds. I'd love something faster when work provides me with updated device.reply"
    ],
    "link": "https://browser.geekbench.com/v6/cpu/14173685",
    "first_paragraph": "\n\nCompare\n\n\n\n\n\n\nCopyright \u00a9 2004-2025 Primate Labs Inc.\n"
  },
  {
    "title": "Software essays that shaped me (refactoringenglish.com)",
    "points": 71,
    "submitter": "mtlynch",
    "submit_time": "2025-09-30T14:01:53 1759240913",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=45425568",
    "comments": [
      "The Grug Brained Developer is one that always sticks in my head, but didn't make the list (to be fair, maybe more because I already agreed than because it transformed my thinking).https://grugbrain.dev/reply",
      "This was a watershed for me: https://stevemcconnell.com/articles/software-quality-at-top-...McConnell isn't especially popular, hereabouts, though.reply",
      "Thanks for sharing! I hadn't read that one, but I read Code Complete early in my career and loved it. It's been on my bookshelf forever, but I've only gone back to it a few times. But every time I do, I think, \"This is really good! I should re-read this.\"I was wondering whatever happened to him because the guys that were popular around his time (Kent Beck, Martin Fowler, Ward Cunningham) all continued writing, even if their popularity waned after the 2000s. But I just never saw anything from McConnell again.It turns out he quit software to be a financial advisor, which is quite surprising.[0][0] https://raindogllc.com/steve-mcconnell-investment-advisor/reply",
      "I used to take classes from his Construx University school.I think he made a pile of money, somehow, and does what he wants.He did a whole bunch of data-mining around COVID: https://stevemcconnell.com/cdc-covid-19-forecast-evaluations...reply",
      "Tangent: It's more than an essay, but I have to mention https://every-layout.dev as an absolute game-changer for my perspective on CSS.reply",
      "The \"Parse, don't validate\" paper is classic IMHO.I disagree with \"Don't put logic in tests\", with the example provided being a problem with using strings where a URI type is needed instead.  Perhaps the source of my disagreement is I hold that test code is production code due to test suite failure(s) during an automated build stops desired deployment.Still, each are definitely worth delving into and determining applicability for oneself.reply",
      ">The \"Parse, don't validate\" paper is classic IMHO.Yeah, I find it so baffling that 90% of programmers I talk to have never heard of it. My circle is more Go/Python/C++ folks, so maybe it's more well-known in functional programming circles.>I disagree with \"Don't put logic in tests\", with the example provided being a problem with using strings where a URI type is needed instead. Perhaps the source of my disagreement is I hold that test code is production code due to test suite failure(s) during an automated build stops desired deployment.Yeah, I think that's a fair criticism. I think the specifics of the example could be better, but I think the important underlying message is that even something that seems simple like a string concatenation is added complexity that can mask a bug in a test.reply",
      ">> I disagree with \"Don't put logic in tests\", with the example provided being a problem with using strings where a URI type is needed instead. Perhaps the source of my disagreement is I hold that test code is production code due to test suite failure(s) during an automated build stops desired deployment.> Yeah, I think that's a fair criticism. I think the specifics of the example could be better, but I think the important underlying message is that even something that seems simple like a string concatenation is added complexity that can mask a bug in a test.I didn't mean to criticize so much as identify why I consider test code to be the same as production code.Continuing with the example provided, the string concatenation is not the problem this test identifies IMHO.  Instead, it is that:  nav.getCurrentUrl()\n\nReturns a `String` instead of a type which disallows the formulation of the problematic `assertEquals` to begin with.In a more general sense, I have found treating test suites the same as one would production code (refactoring, commenting, sometimes testing support logic used to define tests, etc.) has led to tests benefiting the same way.  This approach also has had a twofold benefit of \"keeping the same energy\" when producing all source artifacts along with serving as a great way to onboard new team members.All the usual caveats apply, of course.  YMMV, IMHO, etc.  :-)reply",
      "> Returns a `String` instead of a type which disallows the formulation of the problematic `assertEquals` to begin with.I'm not sure what the best attribution would be but \"Make illegal states unrepresentable\" would be a fantastic addition to this list pairing well with \"parse, don't validate\".A stricter type would force you to parse the URL and would either fix the error (because cleaning trailing/leading slashes might make sense here) or throw a clear error from the parser.It can be slightly more verbose when you just want to write a string in your test for convenience but can (and does) save a lot of debugging pain for less trivial cases.reply",
      ">> Returns a `String` instead of a type which disallows the formulation of the problematic `assertEquals` to begin with.> I'm not sure what the best attribution would be but \"Make illegal states unrepresentable\" would be a fantastic addition to this list pairing well with \"parse, don't validate\".The phrases I have seen describing using types to make illegal states incapable of being represented are \"programming with types\"[0] and \"type level programming\"[1].HTH0 - https://www.manning.com/books/programming-with-types1 - https://rebeccaskinner.net/posts/2021-08-25-introduction-to-...reply"
    ],
    "link": "https://refactoringenglish.com/blog/software-essays-that-shaped-me/",
    "first_paragraph": "by Michael Lynch, published\nSeptember 30, 2025I started reading software blogs before I got my first programming job 20 years ago. At this point, I\u2019ve read thousands of blog posts and essays about software, but only a small handful stuck in my mind and changed the way I think.Joel Spolsky is the greatest software blogger of all time. His essays have informed so much of my approach to software that it was hard to pick out just one, but \u201cThe Joel Test\u201d is my favorite.The Joel Test is a set of 12 questions that employers can ask themselves to see how well they\u2019re investing in their software team:Some of the questions are dated, but the point was never the questions themselves but rather the meta-point of the questions.Joel was really asking employers: do you respect developers?The questions all assess whether an employer prioritizes their developers\u2019 time and focus over things like cheap office space and short-term deadlines.Joel published this article at the height of the dot-com boom, w"
  },
  {
    "title": "Launch HN: Airweave (YC X25) \u2013 Let agents search any app (github.com/airweave-ai)",
    "points": 131,
    "submitter": "lennertjansen",
    "submit_time": "2025-09-30T16:21:09 1759249269",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=45427482",
    "comments": [
      "How do you compare to Onyx? We've used it for some limited use cases, but one of the real challenges - and one I hope to see a lot of innovation on in the space - was permissioning.I see in another comment that you encourage each user to build their own dataset with their own permissions, but often this breaks for founders. If I have a Super Secret Personnel Planning Google Doc at a founder level, how can I be the one to set up the system for our company, but ensure that only files that I've explicitly shared with the company are ingested? What if a file needs to be made anyone-with-link-can-access for sharing with a strategic partner, but that shouldn't be indexed for the entire company?Far too much of the world relies on the security-by-obscurity of public-but-unindexed links, and communications that might look public from a metadata perspective but were carefully designed for a very specific group of people who have verbal/mental context about confidentiality expectations. Being able to categorize by likely confidentiality, and allowing an administrator to partition access on a project and sub-project basis based on that, might be crucial for growth.My recollection is that Onyx had limited support for some security use cases, but very rudimentary. Hoping you can solve this in a thoughtful way!Onyx links for comparison:https://www.onyx.app/https://docs.onyx.app/developers/guides/chat_guidehttps://docs.onyx.app/admin/connectors/official/reply",
      "It\u2019s a good point. It IS hard to map the various \u201coff-market RBACs\u201d onto a unified model and this is part of the reason we delay that - and instead handle it with per-user syncs that include the q=\u201csharedWithMe\u201d parameters.As for intelligently - but probabilistically - determining confidentiality (if I read that correctly), that does sound pretty interesting in scenarios where metadata is just simply insufficient. Also tricky. Sounds like you thought about these problems pretty deeply.reply",
      "Don't mean to hijack (one of the Onyx founders here), but the example you described should be doable with Drive service accounts. Admittedly, our permissioning system is only implemented for a handful of connectors like Drive.Congratulations on the launch Rauf & Lennert! Always great to have more innovation in the open source AI space :D. It looks like Airweave works well with Cursor, something we don't have nailed down yet!reply",
      "Great release,1. How do you decide whether to cache the data into a vector database or fetch it on runtime using a tool call ?2. Slowly all players like Open AI / Claude are trying to provide a somewhat equivalent offering of connecting your workspaces and then providing search on top of it either via direct integrations / mcp servers, how do you see that spanning out ?reply",
      "Airweave always indexes everything. We do not do any direct tool calling currently.reply",
      "Looks great!  It's cool how you are able to unify multiple sources into a single searchable layer. I\u2019m curious how you chose which connectors to support first (e.g. GitHub, Notion, Slack) and how you plan to scale connector coverage?  Thanks!reply",
      "it's currently guided by community feedback, github issues, and user talks. and we rely on private e2e test suites for maintaining quality as we scale coveragereply",
      "Looks good. Curious, how is auth handled? Lot of docs have permissions etc. Can you clarify how this is handled in both indexing side and searching side of things?reply",
      "Great question. We usually sync per user in cases where this matters. That seems inefficient until you realize the following: for most teams, workspace data is pretty small - at least compared to other data workloads (CRMs << 1gb).We plan to implement unified ACL syncs to dedupe the data or even have 1 sync per org, but that\u2019s mostly a cost optimization; Airweave will just scale horizontally until then.reply",
      "Seems like Google Agentspace but without the UI.  Do you folks keep a persistent copy of the data being ingested?  How are you planning on solving RBAC?  IMO, all of these \"search anything\" apps are going to be leaky by design unless you're indexing/gathering on the fly using passthrough credentials...reply"
    ],
    "link": "https://github.com/airweave-ai/airweave",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Airweave lets agents search any app\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\n\n\nAirweave is a tool that lets agents search any app. It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving.Make sure docker and docker-compose are installed, then...That's it! Access the dashboard at http://localhost:8080\nWe welcome contributions! Please check CONTRIBUTING.md for details.Airweave is released under the MIT licen"
  },
  {
    "title": "Mind the encryptionroot: How to save your data when ZFS loses its mind (sambowman.tech)",
    "points": 58,
    "submitter": "6581",
    "submit_time": "2025-09-30T20:58:52 1759265932",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=45431167",
    "comments": [
      "Why zfs freak out is accepted as \"normal\" in a dev environment is beyond me. I use storage spaces on a daily basis in production and dev environment and have for nearly 10 years now and with only marginal use of PowerShell I have been able to restore every array I didn't destroy intentionally. This is the bare minimum I expect out of an redundant array of any type regardless of its speed or scalability promises.reply",
      "It's not accepted and it's not normal.This is the case of user changing password setting and and realizing you can't use them with old backups after  accidentally destroying one dataset. zfs is intented for servers and sysdmins so it is not as friendly as some may expect, but it did not lose anything that user did not destroy. Author  had to use logic to deduct what he did and walk it back.reply",
      "The user has their encrypted data  and two encryption keys. They should be able to decrypt. They don\u2019t care about internal ZFS password settings.I also confirm that people snapshot their data, which is usually child datasets each according to a schedule. If you don\u2019t care about an empty dataset, snapshotting it is not expected.reply",
      "> changing password setting and and realizing you can't use them with old backupsThat's unfair to the author.  The backups were new, post-password change.  And neither old nor new password worked on them.  The thing that was old was an otherwise empty container dataset.reply",
      "This is a case where both sides are completely understandable and no one did anything wrong. ZFS didn't lose its mind. It worked as designed and intended. The author didn't know a critical detail about the implementation. It's a series of unfortunate events. The only failure could be lack of better ZFS documentation.reply",
      "What ZFS did is understandable but wrong.  Sending an incremental snapshot needs to send updates to the encryption parameters, even if they're inherited from another dataset.reply",
      "I'm not sure if anybody is wrong or right. But this should be officially documented, a specific error provided- not \"permission denied\", and a workflow to fix it that doesn't involve patching the driver.reply",
      "Nice write up and website. I should snapshot my empty root!If I\u2019m not wrong, at least some of those sharp edges have been resolved. There was a famous very hard to reproduce bug causing problems with ZFS send receive of encrypted snapshots once in a blue moon, that was hunted down and fixed recently.Still, ZFS needs better tooling. The user has two keys and an encrypted dataset, doesn\u2019t care what is encryption root, and should be able to decrypt. The dataset in question should not be tied to another one.The code for ZFS encryption hasn\u2019t been updated since the original developer left, last I checked.reply",
      "> Lesson: Test backups continuously so you get immediate feedback when they break.This is a very old lesson that should have been learned by now :)But yeah the rest of the points are interesting.FWIW I rarely use ZFS native encryption. Practically always I use it on top of cryptsetup (which is a frontend for LUKS) on Linux, and GELI on FreeBSD. It's a practice from the time ZFS didn't support encryption and these days I just keep doing what I know.reply",
      "ZFS encryption is much more space efficient than dmcrypt+unencrypted ZFS when combined with zstd compression. This is because it can do compress-then-encrypt instead of encrypt-then-(not-really-)compress. It is also much much faster.Source: I work for a backup company that uses ZFS a lot.reply"
    ],
    "link": "https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/",
    "first_paragraph": "\nWhile ZFS has a well-earned reputation for data integrity and reliability, OpenZFS native encryption has some incredibly sharp edges that will cut you if you don't know where to be careful. Unfortunately, I learned this the hard way, standing in a pool of my own blood and tears after thoroughly lacerating myself. I very nearly permanently lost 8.5 TiB of data after performing what should've been a series of simple, routine ZFS operations but resulted in an undecryptable dataset. Time has healed the wound enough that I am no longer filled with anguish just thinking about it, so I will now share my experience in the hope that you may learn from my mistakes. Together, we'll go over the unfortunate series of events that led to this happening and how it could've been avoided, learn how ZFS actually works under the hood, use our newfound knowledge to debug and reproduce the issue at hand, and finally compile a modified version of ZFS to repair the corrupted state and rescue our precious dat"
  },
  {
    "title": "Coding a new BASIC interpreter in 2025 to replace a slow one (nanochess.org)",
    "points": 49,
    "submitter": "nanochess",
    "submit_time": "2025-09-28T18:28:14 1759084094",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=45406645",
    "comments": [
      "For those interested in BASIC, here's \"A curated list of awesome BASIC dialects, IDEs, and tutorials\":https://github.com/JohnBlood/awesome-basic?tab=readme-ov-fil...It's not as popular as Python, obviously, but that lists over fifty implementations of BASIC.reply",
      "I applied for a job about 12 years ago where the company was still using BASIC for some of their software. If I remember correctly it was numbered BASIC, not the more modern stuff. I think the software was doing some type of accounting\u2014stuff that worked and they didn't want to change.reply",
      "I had a job in the early 90s where another team were using Wang 2200s which I'm sure were coded with line-numbered BASIC.Could be one of those with the everlasting ERP type software running on an emulator.reply",
      "Wish I could have found that 35 years ago.reply",
      "> I discovered the pointer to the next line wasn't a good idea, because it needed to move every pointer after a line insertion.Huh? Don't you need to only change the \"next-line-pointer\" for the line that's right before the inserted line?> but the NEXT changed the line, but on the next statement it would lost track and get back to the line following the NEXT. The loops also require their own stack, but including the counter variable address, a pointer to the TO expression, and a pointer to the STEP expression (5 words in total).Mmm. IIRC, usually the compiled NEXT statement would store the pointer to the corresponding FOR statement, so you don't need an additional stack for loop depth during the execution. But you still need it (or some other sort of chaining) during the program input so whatever.> Typing the program was difficult, as the keyboard bounced a lot. This happens when you read too fast the keyboard, so fast you can see that effectively the key contact isn't perfect.Yeah... I've read that keyboard microcontrollers has to deal with contact bounce even today.reply",
      "> Mmm. IIRC, usually the compiled NEXT statement would store the pointer to the corresponding FOR statement, so you don't need an additional stack for loop depth during the executionI think you do. Apart from common sense, nothing forbids one from writing stuff like  100 for i = 1 to 10\n  110 if i = 4 gosub 100\n  120 print i\n  130 next\n  140 return\n\nI think many basics also allowed changing that goto 100 to goto 200 and adding  200 for j = 1 to 4\n  210 print i\n  220 print j\n  230 next\n\nYes, things would likely end badly, but the basic interpreter would not be smart enough to reject such programs. Its editor didn\u2019t even guarantee that a for statement had a corresponding next or vice versa; all it guaranteed was that the program consisted of a list of lines that each in isolation are valid basic code.reply",
      "> I discovered the pointer to the next line wasn't a good idea, because it needed to move every pointer after a line insertion.I get the impression that they were storing everything sequentially in memory, rather than having a linked list of instructions. Why? I can only speculate. Perhaps it is to make memory management simpler (don't have to keep track of which addresses are in use), or to avoid memory fragmentation in system with limited memory (any modification of code would introduce unusable holes). If that's the case, what you want is an offset rather than an absolute address.reply",
      "> I get the impression that they were storing everything sequentially in memory, rather than having a linked list of instructions. Why? I can only speculate.I expect that\u2019s because that is how \u2018every\u2019 homecomputer basic did it. Yes, that makes it slow to insert or remove a line close to the start of a long program, but it allow those offsets to be 8 bits, gaining a precious byte over a 16-bit absolute address.Now, why they initially chose to waste those bytes? I wouldn\u2019t know, but I guess that, because (FTA) \u201cThe CP1610 processor cannot address directly the internal memory in byte terms, instead everything is handled by full word\u201d, they didn\u2019t think of using a single byte.reply",
      "I love basicCan you OOPize it?reply",
      "Yes, meet Visual Basic 6. It has many OO features stapled on.reply"
    ],
    "link": "https://nanochess.org/ecs_basic.html",
    "first_paragraph": ""
  },
  {
    "title": "Designing agentic loops (simonwillison.net)",
    "points": 130,
    "submitter": "simonw",
    "submit_time": "2025-09-30T15:21:23 1759245683",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=45426680",
    "comments": [
      "For lightweight sandboxing on Linux you can use bubblewrap or firejail instead of Docker. They are faster and _simpler_. Here is a bwrap script I wrote to run Claude in a minimal sandbox an hour back:    exec bwrap \\\n      --ro-bind /usr /usr \\\n      --ro-bind /etc /etc \\\n      --ro-bind /run /run \\\n      --ro-bind \"$NODE_PATH\" /node \\\n      --proc /proc \\\n      --dev /dev \\\n      --symlink usr/lib64 /lib64 \\\n      --tmpfs /tmp \\\n      --unshare-all \\\n      --share-net \\\n      --die-with-parent \\\n      --new-session \\\n      --bind \"$HOME/claude\" /claude \\\n      --bind \"$HOME/.claude.json\" /claude/.claude.json \\\n      --bind \"$HOME/.claude\" /claude/.claude \\\n      --setenv HOME /claude \\\n      --setenv PATH \"/node:/claude/bin:/usr/bin\" \\\n      --bind \"$(pwd)\" /work \\\n      --chdir /work \\\n      /claude/bin/claude \"$@\"reply",
      "Nice, thanks for sharing. The lack of an equivalent on macOS (sandbox-exec is similar but mostly undocumented and described as \"deprecated\" by Apple) is really frustrating.reply",
      "There is an equivalent. I played with it for a while before switching to containers. You can just sign an app with sandbox entitlements that starts a subshell and uses security bookmarks to expose folders to it. It's all fully supported by Apple.reply",
      "I would love to be able to use sandbox entitlements for this. I have so far been unable to figure out how.reply",
      "It's not equivalent. You can restrict access but expose select resources, but there's no bind mounting, no overlays, etc. etc.It's a very far cry from bwrap.reply",
      "You don't need bind mounts, you can just pass access rights to directories into the sandbox directly. Also sandboxed apps run inside a (filesystem) container so file writes to $HOME are transparently redirected to a shadow home.reply",
      "Respectfully, it's not enough. You can't treat the inside of the sandbox as a generic macOS system. You can't really install arbitrary things or run arbitrary programs. The wheels fall off extremely quickly.reply",
      "What issues did you hit?The main issue I had is that most dev tools aren't sandbox compatible out of the box and it's Apple specific tech. You can add SBPL exceptions to make more stuff work but why bother. Containers/Linux VMs work everywhere.reply",
      "Would something like dagger.io work for sandboxing? I'm not sure on the security side of things, but I very much liked the presentation they did at the AI Engineering conference (San Fran, earlier this year) about how they can build branching containers to support branching or parallelized development workflows.reply",
      "Yeah, that's definitely an option worth considering. Coincidentally I quoted Dagger founder Solomon Hykes in my article - the \"An AI agent is an LLM wrecking its environment in a loop\" line.reply"
    ],
    "link": "https://simonwillison.net/2025/Sep/30/designing-agentic-loops/",
    "first_paragraph": "30th September 2025Coding agents like Anthropic\u2019s Claude Code and OpenAI\u2019s Codex CLI represent a genuine step change in how useful LLMs can be for producing working code. These agents can now directly exercise the code they are writing, correct errors, dig through existing implementation details, and even run experiments to find effective code solutions to problems.As is so often the case with modern AI, there is a great deal of depth involved in unlocking the full potential of these new tools.A critical new skill to develop is designing agentic loops.One way to think about coding agents is that they are brute force tools for finding solutions to coding problems. If you can reduce your problem to a clear goal and a set of tools that can iterate towards that goal a coding agent can often brute force its way to an effective solution.My preferred definition of an LLM agent is something that runs tools in a loop to achieve a goal. The art of using them well is to carefully design the tools"
  },
  {
    "title": "Atuin Desktop: Runbooks That Run \u2013 Now Open Source (atuin.sh)",
    "points": 130,
    "submitter": "digdugdirk",
    "submit_time": "2025-09-30T20:44:30 1759265070",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=45431001",
    "comments": [
      "This looks really useful. I have a home server with various VMs and containers, but I don't do server admin very often, so whenever something breaks I have to find out how to fix it each time. Having terminals in a document just like you can mix code and documentation in a Python notebook can probably make admin easier for people like me who do it rarely. And of course in a professional setting something like this improves the bus factor in a lot of projects if you can keep things documented.reply",
      "Yes exactly! I have so many systems both personally + professionally, and Desktop has made it so much easier to maintain and work with them allLet us know if you try it and have any feedback or questions at allreply",
      "Not a single mention of AI, LLMs, or Agents. I'm impressed :)reply",
      "(Hi, author here!)I can\u2019t guarantee we never will, but I am only interested in such things if they are genuinely usefulIf we won\u2019t use it internally, and no significant % of users will, then it won\u2019t happenreply",
      "This is The Wayreply",
      "Also see https://runme.dev for a similar approach or https://speedrun.cc if you'd like it to work straight from GitHub markdown.reply",
      "I think this is a potential building block for transforming how MSP organisations manage documentation and workflow.Despite the goal of homogenizing each clients environment to simplify support, there will always be uniqueness amongst them. Having documentation that can be collaboratively edited, versioned, that is also runnable to perform management tasks would be a HUGE deal.I understand this isn't the target audience, but its exciting to imagine the possibilities of other uses.reply",
      "Not directly related to this new Atuin feature, but I need to vent:Last week I was trying to `find` something in some directories, failed, `cd`d to my home directory and instinctively hit up-arrow+return to run the search again. At some time prior to this, Atuin had stopped recording new entries without my notice. Want to guess the last entry that Atuin did record?Go on. Guess.Yep.`rm -rf *`In my home directory.Luckily I have backups of everything important and didn't actually lose anything, and I'm mainly posting this here as a funny anecdote. But - still - after getting myself set up again I have yet to reinstall Atuin.reply",
      "Destructive options shouldn't be allowed to be run from history without confirmation. That seems like a reasonable fix to this issue.reply",
      "\"Destructiveness\" property is undecidable in general. If you ban rm from history, you'll just get false sense of security before you accidentally run some \"aws bla bla drop production cluster\"Behavior of any system should be just one of:\n1. Fully determinate\n2. Have enough latency before confirmation (for example, block input for 1 second after displaying a command)This should apply to history, any fuzzy searching, autocomplete etcreply"
    ],
    "link": "https://blog.atuin.sh/atuin-desktop-open-source/",
    "first_paragraph": "Atuin Desktop looks like a doc, but runs like your terminal. Script blocks, embedded terminals, database clients and prometheus charts - all in one place.Most infrastructure is held together by five commands someone remembers when shit breaks. Docs are out of date, if they exist. The real answers? Buried in Slack threads, rotting in Notion, or trapped in someone's shell history.Atuin CLI\u00a0fixed part of this, with synced, searchable shell history. But history isn\u2019t enough. Teams need workflows they can\u00a0repeat, share, and trust.That\u2019s why we built Atuin Desktop. Runbooks that actually run. Now open beta, and fully\u00a0open source.Atuin Desktop looks like a doc, but runs like your terminal.\u00a0Built to make local developer workflows repeatable, shareable, and reliable.Runbooks should run. Workflows shouldn't live in someone's head. Docs shouldn't rot the moment you write them. Scripts, database queries, HTTP requests and Prometheus charts - all in one place.Back in April we\u00a0launched the closed be"
  },
  {
    "title": "Organize your Slack channels by \"How Often\", not \"What\" (aggressivelyparaphrasing.me)",
    "points": 55,
    "submitter": "todsacerdoti",
    "submit_time": "2025-09-30T20:04:46 1759262686",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=45430542",
    "comments": [
      "Why can't slack let me do something in between muting a channel or notifying me of every new message? Like, perhaps for some channels I want to read every message, but it's not time critical, so it would be nice if it only became \"unread\" once a day if it has new messages since last time.reply",
      "Notify on start of new thread or @mention is the sweet spot.But there are always people who insist on having their entire conversation at the top level of the channel rather than in a thread, so everybody gets notified for every message (unless they mute the whole channel)./me shakes fist at cloud that looks like the face of a past team leadreply",
      "this is the sort of thing i want AI to be solving for me.  i don't need it generating hyper-realistic videos, but when those two people in my office who never thread their messages start having a conversation in #general, i want slack to automatically thread it.reply",
      "That doesn\u2019t generate notifications by default. It does bold the channel though.reply",
      "I still find it utterly mind-boggling that Slack still lacks the ability to batch or \"debounce\" notifications.I don't need (nor want) to react instantaneously to anything and everything that occurs in a channel, thread, or direct-message chat... and especially not for that one senior co-worker who submits each discrete sentence in their indefinitely long stream of consciousness.\"Mute\" is not an answer, because I do need to be notified of activity... But I'd rather get a single minute-delayed \"boop\" about multiple events than a stream of instant and sporadic mad chittering.reply",
      "Or at least give us the discord feature of \"Mute channel for ...\" with some fixed set of durations.reply",
      "Disable notifications for the channel and use the Unreads tab (or \u201cCatch Up\u201d I believe it is now called)reply",
      "This was a game changer for me. The Unreads tab became a bit like email. I'd check it out a few times a day and respond where needed. Greatly cut down on the amount of distraction-by-noise I suffered in a day.reply",
      "I'd sort of roughly approached this technique with my own channel organization over time without thinking about it systematically, but this is a helpful crystallization of what I'd been trying to achieve. I'm glad this was posted.Definitely agree with others that Slack needs a richer selection of notification mechanisms, both for new content in channels and for mentions. For mentions, there's no level between \"I demand immediate attention from this person\" and \"the characters that make up this person's name happen to be in the text of my message.\"reply",
      "I have a bunch of groups from top to bottom:   - Channels incidents I'm in right now\n   - Incidents others are working on. Our Team channel we'll be called out\n   - Channels for my team\n   - Other monitoring and alert channels to keep an eye on \n   - Announcements from my group, diversion, etc\n   - Ongoing Issues. Incident investigation\n   - Ongoing Projects\n   - Issues from earlier this month (move for the top two group once resolved)\n   - 3 groups of issues from 3 previous months\n   - Other teams public channels I read when I have time\n   - Some random internal channels\n   - Other teams channels I ignore but need to join sometimes to ask their help\n\nI'm in an ops team so probably add 5-10 channels per day for new incidents I'm on or others in my team are on.reply"
    ],
    "link": "https://aggressivelyparaphrasing.me/2025/09/30/organize-your-slack-channels-by-how-often-not-what/",
    "first_paragraph": "Aggressively Paraphrasing MeSome of Mark's thoughts and ideasWritten by inA few weeks ago, I changed my Slack channel sections. I\u2019m now more responsive and engaged, while also feeling less stressed. How? By sorting my Slack channels by urgency, or how often I want to read them.VersusSorting by \u201chow often\u201d lets me read my most urgent messages first, focusing my energy on what matters to me. Once I feel tired, I stop reading. By focusing on my most urgent and important channels, I hold confidence that I have already taken care of what I need to, reducing my stress.By framing a channel\u2019s importance through the Eisenhower Matrix, I focus on how I contribute to channels.This framework is flexible. Your needs and availability will change. Projects go from active development to finished. Social channels go through ups and downs. As these changes happen, you can slide the channel between any category, and it\u2019ll still make sense.Misprioritized channels are a source of burnout. Noisy channels in"
  },
  {
    "title": "Washi: The Japanese paper crafted to last 1000 years (4 min video) (bbc.com)",
    "points": 8,
    "submitter": "rmason",
    "submit_time": "2025-09-27T21:43:47 1759009427",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.bbc.com/reel/video/p0m4mg2j/washi-the-japanese-paper-crafted-to-last-1-000-years",
    "first_paragraph": "Japan's washi paper has been crafted by hand for over 1,500 years and is prized for its beauty, strength, and distinct natural textures.\u00a0If kept correctly, some varieties can last more than a thousand years. Ancient documents made with washi are proof of its remarkable longevity. BBC presenter Paul Carter visited\u00a0Echizen to learn about the art of washing paper making.Discover how a Pacific Northwest creator crafts serene viral clips that attract huge audiences worldwide.The Travel Show explores how Rome is planning for ambitious modernisation whilst preserving its ancient roots.A former slaughterhouse district played a surprising role in shaping the United Nations\u2019 permanent home.Canadian start-ups design new tools to forecast wildfires, aiming to prevent disasters before they start.Financial markets are expecting the US Federal Reserve to accelerate the cutting of interest rates.The mysterious 'mermaid' of Cornwall helps to educate future generations about\u00a0cleaner oceans.\u00a0QR codes are"
  },
  {
    "title": "Kagi News (kagi.com)",
    "points": 746,
    "submitter": "grappler",
    "submit_time": "2025-09-30T15:09:00 1759244940",
    "num_comments": 349,
    "comments_url": "https://news.ycombinator.com/item?id=45426490",
    "comments": [
      "Just to be clear I'm understanding correctly:This is pulling the content of the RSS feeds of several news sites into the context window of an LLM and then asking it to summarize news items into articles and fill in the blanks?I'm asking because that is what it looks like, but AI / LLMs are not specifically mentioned in this blog post, they just say news are 'generated' under the 'News in your language' heading, which seems to imply that is what they are doing.I'm a little skeptical towards the approach, when you ask an LLM to point to 'sources' for the information it outputs, as far as I know there is no guarantee that those are correct \u2013 and it does seem like sometimes they just use pure LLM output, as no sources are cited, or it's quoted as 'common knowledge'.reply",
      "Just for concrete confirmation that LLM(s) are being used, there's an open issue on the GitHub repository, on hallucinations with made up information, where a Kagi employee specifically mentions \"an LLM hallucination problem\":https://github.com/kagisearch/kite-public/issues/97There's also a line at the bottom of the about page at https://kite.kagi.com/about that says \"Summaries may contain errors. Please verify important information.\"reply",
      "Love how it only took 8 years to go from \"Fake News!\" to \"News May Be Fake\"reply",
      "There's too much demand for fake news, plenty of subsidy for it, and it's far easier to make.Non fake news is going to be restricted to pay services like Bloomberg terminals.reply",
      "It is getting easier and easier to fake stuff and there are becoming less and less fully trusted institutions. So sadly I think you are right. Its scary but we are likely heading towards a future where you need to pay to get verified information and that itself will likely be segmented to different subscriptions for what information you want.reply",
      "> It is getting easier and easier to fake stuffThis is why the moon landing hoax was revolutionary in the 60's. The size of this project was enormous.reply",
      "As an American, confirmation that the landing was a hoax would make me even prouder than my current belief that it was real.reply",
      "The moon landing was filmed. Problem is, Stanley Kubrick was such a perfectionist that he _demanded_ they film on location.reply",
      "At least we're going from Fake News from certain MAGA leaning sources at 75-90% fake to 99% actual news and 1% hallucinations?reply",
      "Man am I tired of this stuff.reply"
    ],
    "link": "https://blog.kagi.com/kagi-news",
    "first_paragraph": "\n\n\n                    30 Sep, 2025\n                \n\nA comprehensive daily press review with global news. Fully private, with sources openly curated by our community.News is broken. We all know it, but we\u2019ve somehow accepted it as inevitable. The endless notifications. The clickbait headlines designed to trigger rather than inform, driven by relentless ad monetization. The exhausting cycle of checking multiple apps throughout the day, only to feel more anxious and less informed than when we started. This isn\u2019t what news was supposed to be.\nWe can do better, and create what news should have been all along: pure, essential information that respects your intelligence and time.Kagi News operates on a simple principle: understanding the world requires hearing from the world. Every day, our system reads thousands of community curated RSS feeds from publications across different viewpoints and perspectives. We then use AI to distill this massive information into one comprehensive daily brief"
  },
  {
    "title": "Founder sentenced to seven years in prison for fraudulent sale to JPMorgan (cnn.com)",
    "points": 173,
    "submitter": "mandeepj",
    "submit_time": "2025-09-30T12:53:49 1759236829",
    "num_comments": 217,
    "comments_url": "https://news.ycombinator.com/item?id=45424827",
    "comments": [
      "Former federal inmate here who was recently released from prison a month ago today (I did 18 months): The big deal here was the loss amount, which can be construed any number of ways whether we like it or not. This will jack up the points and tilt the scale for the sentencing guidelines, and believe me they are archaic.After all is said and done, Charlie Javice will be hanging out at a prison camp\u2014probably down there with Holmes and Maxwell, because it's cushy\u2014and do no more than 4 years on the 7 assuming she completes all her programming requirements.reply",
      "In reading your blog, it doesn\u2019t really seem like you take accountability for your crimes. It looks like you stole copyrighted content, distributed it for pay, and then tried to claim you were a white hat who should be paid a bug bounty when busted by the copyright holder. It\u2019s hard to take your views on sentencing seriously after having read all of that.https://www.justice.gov/usao-sdny/pr/minnesota-man-sentenced...reply",
      "Reading about you a bit. Would you say I shouldn't go and try to create the replacement for Streameast?reply",
      "did u transition out in corecivic?reply",
      "I got picked up by my partner, thankfully. I was a self-surrender too.I did hear that they yanked out all the valuable assets from Cimarron (OKC's out-of-custody transfer facility) and now all the doors are manually keyed, amongst other things.reply",
      ">> A prosecutor, Micah Fergenson, though, said JPMorgan \u201cdidn\u2019t get a functioning business\u201d in exchange for its investment. \u201cThey acquired a crime scene.\u201dI do not understand how an acquisition this big got thru due diligence without noticing all the fake users. Anyone in corporate M&A know if it is normal to spend this much money without inspecting the goods? Seems like the most basic of OLAP queries and two days of effort would reveal very suspicious userbase.reply",
      "Back in the nineties, Philips was days away from signing a licensing deal for a revolutionary video compression technology that compressed whole movies down to 8KB. The former Philips CTO was a strong believer. And then the inventor died and nothing ever came of it.To be a fly on the wall during due diligence meetings between Philips engineers and management.https://lowendbox.com/blog/the-man-who-was-paid-e113000-for-...reply",
      "Video codec compression scams remained popular even in early 2000s. I worked for a very large public tech company. One of the top 10 in that era. And they fell hard for scammers from Las Vegas that promised revolutionary audio/video compression. We had to sign all sorts of NDA and couldn't look under the hood of what they delivered to us under penalty of breach of contract and all that stuff. I \"accidentally\" ended up looking under the hood and couldn't believe what I found. I reported the findings to my manager and told him to do what he wanted to with that information.Long story short, the whole project got shut down and about 200 people working on project lost their jobs. Myself included. Luckily I quickly landed at a better place working on more meaningful things.reply",
      "> I reported the findings to my manager [...] the whole project got shut down and about 200 people working on project lost their jobs. Myself included.Good for you for reporting the threat.  But I'm a little surprised that they let the messenger get killed along with all the other innocents.I knew someone who whistleblew to C-suite, about misrepresentations they realized, on something that was then an existential threat to one of the top companies in its market.  A series of layoffs and (IIUC) some M&A later, most of the employees were gone, but that one middle-aged engineer who warned C-suite (averting an even worse fate for the company) escaped all the layoffs, and was still there.reply",
      "I\u2019m sure that that one line manager who reported the fraud to the CEO was well rewarded. How he learned what he did? We\u2019ll never know. Too bad his team had to be let go.reply"
    ],
    "link": "https://www.cnn.com/2025/09/30/business/charlie-javice-frank-sentenced-jpmorgan-intl",
    "first_paragraph": "Markets \n\n\nHot Stocks \n\n\nFear & Greed Index \n\n\n\n            Latest Market News \n\n\n\n            Hot Stocks \n\n\n\n            Charlie Javice, the founder of a startup company that promised to revolutionize the way college students apply for financial aid, was sentenced Monday to more than seven years in prison for cheating JPMorgan Chase out of $175 million by greatly exaggerating how many students it served.\n    \n            Javice, 33, was convicted in March of duping the banking giant when it bought her company, called Frank, in the summer of 2021. She made false records that made it seem like Frank had over 4 million customers when it had fewer than 300,000.\n    \n            Addressing the court before she was sentenced, Javice, who was in her mid-20s when she founded the company, said she was \u201a\u00c4\u00fahaunted that my failure has transformed something meaningful into something infamous.\u201a\u00c4\u00f9\n    \n            Sometimes speaking through tears, she said she \u201a\u00c4\u00famade a choice that I will spend my e"
  }
]