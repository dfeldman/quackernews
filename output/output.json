[
  {
    "title": "Middle-aged man trading cards go viral in rural Japan town (tokyoweekender.com)",
    "points": 480,
    "submitter": "PaulHoule",
    "submit_time": "2025-04-07T21:03:16 1744059796",
    "num_comments": 96,
    "comments_url": "https://news.ycombinator.com/item?id=43615912",
    "comments": [
      "I clicked the headline expecting a chuckle and left with an unexpectedly warmed heart.> \u201cWe wanted to strengthen the connection between the children and the older generations in the community. There are so many amazing people here. I thought it was such a shame that no one knew about them,\u201d [...] \u201cSince the card game went viral, so many kids are starting to look up to these men as heroic figures.\u201d \n> Kids have started attending local events and volunteering for community activities \u2014 just for a chance to meet the ojisan from their cards. Participation in town events has reportedly doubled since the game launched.there's so much more I want to comment on--it's not screen-based, increased cross-generational interaction, strengthening community, elders having their stories known--but what I love is that these effects will compound into even greater benefits for the community.\n \nreply",
      "This brings back something we've mostly lost in modern times. Elders had respect because they knew a lot and had contributed a lot, and everyone knew that. But that's not scalable, and we migrate a lot more now.This is an engaging way that brings that back - rather than focusing on fantasy heroes, show kids real life role models.\n \nreply",
      "It's even less that we move around a lot more; technology advanced with the personal computer and Internet such that kids see adults not knowing things about the world that they already do. What is decades of personal lived experience wisdom when there's tiktok and YouTube and chatbots?\n \nreply",
      "I hope the people don't get too much pressure to up their stats.>The rarity of a card isn\u2019t based on fantasy stats \u2014 it\u2019s tied to real-world contributions. The more actively the ojisan engages in volunteer work or community service, the higher the chances of their card being upgraded to a shiny version with a glossy laminated effect.\n \nreply",
      "Reminds me of the fisherman call where you could sign up to have a professional fisherman give you a wake up call. \nhttps://soranews24.com/2017/05/12/japanese-fishermen-start-m...\n \nreply",
      "\u201cGood morning! Are you up?\u201d asks the fisherman in the video, to which the user replies \u201cYes, thanks to you. Are you on your ship?\u201d \u201cYeah, I got up at 3, so I\u2019m already on the sea,\u201d he replies, before adding \u201cI caught a really big fish.\u201d\n\nI\u2019m not sure how much demand there is for this product, but that really brought a smile to my face.\n \nreply",
      "I don\u2019t speak Japanese but I would pay for this. Trying to get up and run before 6 is a chore. Having a fisherman wake up call would be awesome and motivating. Especially for someone who loves fishing.\n \nreply",
      "I want a wake up call from David Goggins.\n \nreply",
      "Last year it was a real fisherman.Next year it will be an AI.\n \nreply",
      "I think AR or augmented reality games like this trading cards is the future of gaming, but this one is offline AR rather than online.One of the best game I ever played is the text based souvenir game shopping game on Windows 3. I can't recall the name of the game now since it's more than 30 years ago, but it's about shopping souvenirs using London Underground Tube. You have a semi realistic time constraints like train schedules, your flight schedules and of course list of souvenirs items to shop. This is totally offline since there is no Internet available at the time but it's very engaging nonetheless.My proposal for the modern version of the game is to use real-time train schedules (with delays, ticket discounts, etc) that are available publicly on the Internet for many metropolitan cities in the world for examples Tokyo, London and Berlin [1],[2],[3].Imagine you can have a real-world realistic in-app in-game items  purchases feature that you personally can buy in the game and delivered to you or anyone you fancy of giving souvenirs except that you only virtually went there.[1] A real-time 3D digital map of Tokyo's public transport system (2023 - 103 comments):https://news.ycombinator.com/item?id=37829061[2] Live map of London's Underground system:https://traintimes.org.uk/map/tube/[3] Show HN: Ub\u00e4hnchen \u2013 Animated subway map of Berlin (2020 - 102 comments):https://news.ycombinator.com/item?id=32647227[4] Berlin train info:https://www.vbb.de/fahrinfo\n \nreply"
    ],
    "link": "https://www.tokyoweekender.com/entertainment/middle-aged-man-trading-cards-go-viral-in-japan/",
    "first_paragraph": ""
  },
  {
    "title": "Framework temporarily pausing some laptop sales in the US due to tariffs (fosstodon.org)",
    "points": 127,
    "submitter": "leotravis10",
    "submit_time": "2025-04-08T00:16:35 1744071395",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=43617207",
    "comments": [
      "They have their entire production in Taiwan, which probably feels like a good move right now. If they did final assembly in the US, US tariffs would be better, but selling outside the US would mean importing parts into the US and exporting computers outside, paying tradewar-level tariffs both ways. The way they are set up now they get hit harder on US imports but the rest of their customers aren't affected at all.\n \nreply",
      "Wouldn\u2019t it make sense then to spin up a separate final assembly within the US? Or do their sales not warrant that kind of investment?\n \nreply",
      "> Wouldn\u2019t it make sense then to spin up a separate final assembly within the US?Like it can just be done?> Or do their sales not warrant that kind of investment?It's not so simple. That's the equivalent of other countries thinking they can just spin up a \"silicon valley\" outside the US. Sales or not it doesn't work like that.\n \nreply",
      "I really hope this won\u2019t affect International/European pricing for their laptops as I was planning on getting my GF (who lives in Poland) a Framework 12.Additionally, I was thinking about purchasing a Framework Desktop to use as a server for my business. (I\u2019d like to use LLMs but due to the sensitive nature of my customer\u2019s data I would strongly prefer doing so locally.) I guess I\u2019d have to send it to my GF and then smuggle it into the US in my carry on.\n \nreply",
      "Dupe https://news.ycombinator.com/item?id=43616447 (50 points)This fractured landscape isn't better - same message on x:https://news.ycombinator.com/item?id=43616389 (10 points)\n \nreply",
      "One obvious benefit of this link and the bsky ones is that it is actually possible to read the comments on them, unlike the last one.\n \nreply",
      "If building hardware products wasn't hard enough.  This could kill a lot of companies pretty quickly.  If you have massive inventory in China or you've invested in a factory to build things at scale and now your US margins are underwater, I'm not sure what you do.Nintendo is facing the same thing.  Nintendo had tried to avoid Trump's first term tariffs in 2019 by spending a ton of money moving Switch production to Vietnam only for Vietnam to get hit massive 46% tariffs this week:https://www.designdevelopmenttoday.com/industries/manufactur...\n \nreply",
      "> If you have massive inventory in China or you've invested in a factory to build things at scale and now your US margins are underwater, I'm not sure what you do.Don\u2019t have massive production in China if you want to sell in the US. The writing has been on the wall for this for almost 20 years. Even Obama was talking about it in 2007-08.\n \nreply",
      "> Don\u2019t have massive production in China if you want to sell in the US.How does this help? Everyone has already tried shifting to Vietnam, India, etc only to be hit by tariffs anyway.\n \nreply"
    ],
    "link": "https://fosstodon.org/@frameworkcomputer/114297967333461078",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Lux \u2013 A luxurious package manager for Lua (mrcjkb.dev)",
    "points": 162,
    "submitter": "mrcjkb",
    "submit_time": "2025-04-07T18:13:23 1744049603",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=43614285",
    "comments": [
      "I know some projects like Koreader[1] use Lua as their primary application language. If you could convince one of them to switch, it would provide some assurances about the maturity and popularity of the idea.[1]: https://github.com/koreader/koreader\n \nreply",
      "That's a neat suggestion, thanks.\nLux will need some time to become mature, but building a large multi-platform project like koreader would definitely be a nice goal.\n \nreply",
      "Execution env is the achille's heel of scripting languages. Personally I don't use Neovim, but had a feeling its adoption would spur development in this area for Lua. Bryan Cantrill called Javascript \"LISP in C's clothes\". In some ways I feel like Lua is the opposite, and love it for those reasons (disclaimer: never had to use it at work).\n \nreply",
      "Javascript is Lisp in C's clothes? On what basis? Also what does Lua have to do with Lisp? It has no Lisp syntax whatsoever.\n \nreply",
      "I have an intuition that the comment you're responding to has a lot of truth to it, but I am going to have to educate myself to give you an answer.One thing I do know is that JS and Lisp both treat functions as first-class citizens, allow some degree of meta-programming, and rely heavily on hierarchical  (e.g., nested objects in JavaScript vs. s-expressions in Lisp).Passing functions by reference enables both LISP and JS to compose higher-order functions and, as suggested in another commented, both Lisp and JavaScript's \"dynamic stack frames\" somehow live updates to running code without requiring a complete restart of the application. The only clear example of this I can find, however, is Bun's --hot mode, which performs a \"soft reload,\" updating its internal module cache and re-evaluates the changed code while preserving global state.I have some vague notion that this is a favorite feature of Lisp, but it's not clear to me that it's unique to these language families.\n \nreply",
      "Fennel has actually convinced me that Lua and lisp have more in common than one might think. I don't know what the above comment was referencing, but I've always found beauty in lisp languages for having a primary datastructure that all others can be abstracted to. Lisp, classically, has list, clojure has sequences, and Lua/fennel has tables.https://fennel-lang.org/\n \nreply",
      "And Tcl has strings.Fennel is more popular than I expected! It's in the official repositories of Arch, Fedora and Debian.\n \nreply",
      "Javascript hotloading development setups are about the closest you can get to the REPL development loop outside of lisp. I'd imagine lua is similar if the embedding is set up for it.\n \nreply",
      "Have you used `bun --hot`?",
      "Also, I do want to point out that despite very recognizable syntax, that's not the only thing that makes lisp lisp. Primary example of lisp-y syntax on a non-lisp would be Janet [0]https://janet-lang.org/\n \nreply"
    ],
    "link": "https://mrcjkb.dev/posts/2025-04-07-lux-announcement.html",
    "first_paragraph": "It\u2019s time Lua got the ecosystem it deserves!For a bit over a year, we have been cooking up Lux,\na new package manager for creating, maintaining and publishing Lua code.\nIt does this through a simple and intuitive CLI inspired by other\nwell-known package managers like cargo.Today, we feel the project has hit a state of \u201cvery usable for everyday tasks\u201d1.While extensive, Luarocks carries with it around 20 years of baggage,\nwhich makes it difficult to make suitable for modern Lua development, while\nretaining backward compatibility.With Lux, we\u2019re pushing for a fresh start:Thanks to our Neovim plugin manager, rocks.nvim,\nand the later addition of Luarocks support to lazy.nvim,\nLuarocks has been steadily gaining popularity in the Neovim space as a way of distributing\nplugins.\nBut it\u2019s been heavily held back by not being fully portable and by being unpredictable\nfrom system to system.\nBecause Luarocks is written in Lua, installing a large number of packages\nand synchronising plugins with rock"
  },
  {
    "title": "Show HN: Browser MCP \u2013 Automate your browser using Cursor, Claude, VS Code (browsermcp.io)",
    "points": 323,
    "submitter": "namukang",
    "submit_time": "2025-04-07T16:25:45 1744043145",
    "num_comments": 122,
    "comments_url": "https://news.ycombinator.com/item?id=43613194",
    "comments": [
      "This may be obvious to most here, but you need Node.js installed for the MCP server to run. This critical detail is not in the set up instructions.\n \nreply",
      "So the website claims:\"Avoids bot detection and CAPTCHAs by using your real browser fingerprint.\"Yeah, not really.I've used a similar system a few weeks back (one I wrote myself), having AI control my browser using my logged in session, and I started to get Captcha's during my human sessions in the browser and eventually I got blocked from a bunch of websites.  Now that I've stopped using my browser session in that way, the blocks eventually went away, but be warned, you'll lose access yourself to websites doing this, it isn't a silver bullet.\n \nreply",
      "The caveat with these things is usually \"when used with high quality proxies\".Also I assume this extension is pretty obvious so it wont take long for CF bot detection to see it the same as playwrite or whatever else.\n \nreply",
      "What do you think they might be looking for that could be detected pretty quickly?  I'm wondering if it is something like they can track mouse movement and calculate when a mouse is moving too cleanly, so adding some more human like noise to the mouse movement can better bypass the system.  Others have mentioned doing too many actions too fast, but what about potential timing between actions.  Even if every click isn't that fast, if they have a very consistent delay that would be another non-human sign.\n \nreply",
      "Modern captchas use a number of tools including many of the approaches you mentioned. This why you might sometimes see a CloudFlare \"I am not a robot\" checkbox that checks itself and moves along before you have much time to even react. It's looking at a number of signals to determine that you're probably human before you've even checked the box.\n \nreply",
      "When I am using keyboard navigation, shortcuts and autofills, I seem to get mistaken for a bot a lot. These Captchas are really bad at detecting bots and really good at falsely labelling humans as bots.\n \nreply",
      "It might depend on the speed with which you click on the elements on the website.\n \nreply",
      "it does, CF bans my own honest to God clicks if I do them too fast.\n \nreply",
      "About five years ago, maybe more, Google started sending me captchas if I ran too many repetitive searches. I could be wrong, but it feel like most large platforms have fairly sophisticated anti-bot/scraping stuff in place.\n \nreply",
      "I use Vimium (Chrome extension for using keyboard control of the browser) and this happens to me as well since the behavior looks \"unnatural\".\n \nreply"
    ],
    "link": "https://browsermcp.io/",
    "first_paragraph": "If you want to automate actions on a website, like repeatedly fill out a form, you normally can't do it with AI apps like Cursor or Claude because they don't have access to a web browser. With Browser MCP, you can connect AI apps to your browser so they can automate tasks on your behalf.Speed up and automate your workflowsEnable your AI editor to automatically test your code end-to-end. Test user flows, validate UI elements, and ensure your application works as expected across different scenarios.Automate repetitive web-based tasks with ease. From data collection and form filling to workflow automation, save time and reduce errors in your day-to-day operations.Automate with speed, security, and convenienceAutomation happens locally on your machine, resulting in better performance without network latency.Since automation happens locally, your browser activity stays on your device and isn't sent to remote servers.Uses your existing browser profile, keeping you logged into all your servic"
  },
  {
    "title": "What Was Quartz? (zachseward.com)",
    "points": 49,
    "submitter": "mooreds",
    "submit_time": "2025-04-07T22:24:16 1744064656",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=43616604",
    "comments": [
      "I remember browsing Quartz back in 2014-15 and being impressed that there was finally a current affairs focused \"digital media\" startup that wasn't styled like the odiously click-chasing Business Insider. The website design felt fresh, with a widescreen layout as opposed to the amateurish, single-column blog-like designs of Gawker and Buzzfeed.But things seemed to change not long after. Paywalls appeared everywhere, so while I could \"see\" the topics they covered, I couldn't read them. Over time I kept coming back to the website, remembering its cool design, and was disappointed to find fewer and fewer new articles.Just took a look at the site now, and sadly it just resembles any of those dime-a-dozen \"content aggregator\" sites like Forbes.com.\n \nreply",
      "> But Quartz never made money. We grew, between 2012 and 2018, to nearly 250 employees and $35 million in annual revenue. The dismal economics of digital media meant losing more than $40 million over that stretch just to grow unsustainably large. Those are laughably terrible numbers.  $35M/year ain't much, and, even at a glance, there's no way to support 250 decently-paid employees on it.  All things considered, even 50 is pushing it.But if they thought outside-the-box a little bit, there might have been a way out:  They could have gone into academic publishing.  Academic publishers make money hand over fist.  Elsevier made $3.5B in profit and >$10B in revenue just last year.Doing social sciences and political science stuff would have been a good fit for Quartz.  You don't need any special permits to become an academic publisher.  Most of your employees (reviewers) do it for free, like jannies.  The industry is ripe for, uh, \"disruption.\"  Oh well.\n \nreply",
      "Zach nails it. He is the reason why I was involved briefly at the start and for where I am today.It was my first time leading a product team. We tried a lot of things that seemed strange at the time; no homepage, no app, native ads (done well imho), a scrolling stream instead of pages. Some of that broke. Some of it worked. A lot of it stuck around.RIP Quartz.\n \nreply",
      "the first publication (that I recall) that offered a chatbot for news too, right?\n \nreply",
      "Yeah, you might be right. I was gone by then so not 100%. Sam Williams was behind that app. I\u2019m not a fan of chat apps but that was done really well and I was surprised by it.\n \nreply",
      "\"It's impossible to kill a media brand,\" \"Still, we also hoped to endure on the scale of centuries\"2012 to nowt.I feel very sorry for Quartz and its staff.  I'm not a fan of private equity firms or parasites as they are generally known.\n \nreply",
      "That's digital media in a nutshell. Either you try to do things the right way with subscribers and in-house advertising, or you chase clicks and Google Ads revenue like Buzzfeed and SPAC your way to riches before the bottom falls out of your valuation.\n \nreply",
      "Expected something about MacOS' Quartz.\n \nreply",
      "I was expecting something about Magma's static timing engine Quartz (EDA world)\n \nreply",
      "Then you dodged a bullet!\n \nreply"
    ],
    "link": "https://www.zachseward.com/what-was-quartz/",
    "first_paragraph": "2012\u20132025\"It's impossible to kill a media brand,\" Jim Spanfeller told me on my first day working for him, as we sat in his corner office. He had just bought the business news organization, Quartz, that I had spent the past decade building and, most recently, trying desperately to save from oblivion. So I was inclined to believe him.But I knew it wasn't true. Jim and his private-equity-backed digital-media conglomerate G/O Media had already destroyed several of their properties, some all at once (Deadspin), most of the others by sapping resources, antagonizing their staff, and undermining the editorial visions that once made them great (Jezebel, The Root). It would take him three years to do the same to Quartz.The end came on Friday, when G/O fired the few remaining writers at Quartz and sold the carcass to a Canadian firm that appears mostly interested in the email list. I thought back to Jim's comment. His cynical view of digital media was good neither for journalism nor business, but"
  },
  {
    "title": "Cardiac arrest deaths among marathoners have decreased, study finds (washingtonpost.com)",
    "points": 25,
    "submitter": "bookofjoe",
    "submit_time": "2025-04-05T19:25:24 1743881124",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=43596133",
    "comments": [
      "Related, the ambulance system where I live has been running a clinical trail using a mobile ECMO unit (Extracorporeal membrane oxygenation) [1,2].  An invasive procedure that oxygenates and pumps blood.  It's boosted survival rate for those who are not responding to CPR (15-20 minutes duration) from 3% to 28%.  The general CPR survival rate is an exponential looking curve: starting around 20%, reducing to 10% at 10 minutes and 3% at 20 minutes [3].[1] https://www.abc.net.au/news/2025-04-07/nsw-ambulance-ecmo-tr...[2] https://www.nsw.gov.au/media-releases/mobile-cardiac-treatme...[3] https://www.bmj.com/content/384/bmj-2023-076019Edit: The numbers in [3] are optimistic, as they are for in-hospital CPR, not out-of-hospital CPR.\n \nreply",
      "Similar program in Melbourne [1]\nhttps://sjtrem.biomedcentral.com/articles/10.1186/s13049-023...(There are similar programs around the world with different models of care, notably in Paris, Albuquerque, the Netherlands and Regensburg, Germany).\n \nreply",
      "I don't know if this is true, but my cardiologist said that everyone who trains hard for a marathon has some level of heart damage from the excessive training. This sounds extreme to me, but I was in to see him for some AFIB episodes that were triggered from pushing myself too hard. It's worth noting that endurance athletes are 2 to 10 times more likely to develop AFIB compared to non-athletes. So this is a bit of warning to apply some level of moderation to your training.\n \nreply",
      "> my cardiologist said that everyone who trains hard for a marathon has some level of heart damage from the excessive training.This strikes me as either your cardiologist being wrong or you misinterpreting what they said.There have been quite a number of studies mentioned in ultramarathon journals that contradict this claim.Additionally, I have run so many marathons and ultramarathons I\u2019ve lost count. I just recently had my own extensive (and expensive, despite insurance) cardiology work where they observed no such damage. I asked them whether endurance sports cause heart damage, because I\u2019m old and thinking of starting up again and the summary of their answer was \u201cas long as you\u2019re training properly, you\u2019re good\u201d.That said, certainly these events put stress on the heart.\n \nreply",
      "\u201cThe most common diagnosis associated with sudden cardiac arrest was coronary artery disease.\u201dI wonder the fitness levels of the people that experienced cardiac arrests. Like were these first time runners that signed up to run a marathon?\n \nreply",
      "Short answer is \"being in shape\" (even for a marathon) doesn't guarantee you'll be free from risks associated with heart disease.\n \nreply",
      "Distance running has, afaik, little to do with coronary artery disease (tho exercise can increase HDL some, it's only moderately protective and I recall that there are some studies that say that Marathon running actually is not healthful).In my understanding of what happens during sudden arrest is that after having filled the artery wall tubing with as much plaque as it could stand, instead of ruptering through the outer wall , it collapses inwards, cutting off blood supply to the heart muscle.\n \nreply",
      "Yeah, it's been a while since anyone died during a marathon I was doing. Back when I started in the early 90s it would be one every couple of years or so.\n \nreply",
      "Can't read the actual study because it's behind a medical journal paywall. It looks like they speculate that improved CPR training and AED use has caused this.My speculation would be that these mega running nerds all wear watches that track their heart rate 24/7 now. I suspect if something is wrong then you get an earlier detection rather than only discovering it on a race day.\n \nreply",
      "https://archive.ph/FDCBz\n \nreply"
    ],
    "link": "https://www.washingtonpost.com/health/2025/04/05/marathon-running-cardiac-arrest-cpr/",
    "first_paragraph": ""
  },
  {
    "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB (arxiv.org)",
    "points": 33,
    "submitter": "PaulHoule",
    "submit_time": "2025-04-07T21:39:33 1744061973",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=43616241",
    "comments": [
      "You could quickly wire up one of the LLM APIs as an application-defined function using SQLite if you wanted to play around with the idea of very slow and expensive queries:https://sqlite.org/appfunc.htmlhttps://learn.microsoft.com/en-us/dotnet/standard/data/sqlit...Maybe stick with the aggregate variety of function at first if you don't want any billing explosions. I'd probably begin with something like LLM_Summary() and LLM_Classify(). The summary could be an aggregate, and the classify could be a scalar. Being able to write a query like:  SELECT LLM_Summary(Comment)\n  FROM Users\n  WHERE datetime(Updated_At) >= datetime('now', '-1 day');\n\nIs more expedient than wiring up the equivalent code pile each time. The aggregation method's internals could handle hierarchical summarization, chunking, etc. Or, throw an error back to the user so they are forced to devise a more rational query.\n \nreply",
      "I tried that a couple of years ago with a CLI tool that uses Python functions called from SQLite - it worked with GPT-3.5: https://simonwillison.net/2023/Apr/29/enriching-data/Example usage:  openai-to-sqlite query database.db \"\n    update messages set sentiment = chatgpt(\n      'Sentiment analysis for this message: ' || message ||\n      ' - ONLY return a lowercase string from: positive, negative, neutral, unknown'\n    )\n    where sentiment not in ('positive', 'negative', 'neutral', 'unknown')\n      or sentiment is null\n  \"\n\nI haven't revisited the idea for fear of the amount it could cost if you ran it against a large database, but given the crashing prices of Gemini Flash, GPT-4o mini etc maybe it's worth another look!\n \nreply",
      "I love that and would maybe even add a model price parameter on each such function call. Perhaps e.g. an number in the range 1-10, with 1 being the cheapest available, and 10 being the best available (whatever the price), and then we'd have environment settings choose the actual models to use for each value. And perhaps have an overload fail-safe to switch all of the queries to cheaper models as a form of throttling.\n \nreply",
      "I'll try this at work tomorrow !\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2504.01157",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Scaffold Level Editor (littlepolygon.com)",
    "points": 60,
    "submitter": "mwkaufma",
    "submit_time": "2025-04-07T20:00:58 1744056058",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=43615322",
    "comments": [
      "I was always inspiring the level editor in Cube 2: Sauerbraten (http://sauerbraten.org). This seems like an alternative to BSP approach which was more popular at the time. But I think this game's octree-based solution has more potential than just the render and raycast logic implemented in the engine. You could potentially make the entire level fully destructible and keep it optimized at the same time. The level building was also fun when I was teenager. No game allowed to edit levels with other players connected to the server and that was working in 2004!\n \nreply",
      "That's a cool reference, thanks for showing me. Similar energy to Q1K3 https://phoboslab.org/log/2021/09/q1k3-making-of\n \nreply",
      "A whistle-stop tour of some game-engine collision/pathfinding background, how modern engines work, how old engines work, and how I'm applying a mixed-approach in my game.\n \nreply",
      "Excellent article, and I appreciate how you gave an overview of solutions that didn't make sense for you but were otherwise important to know about.Could you expand on how your pathfinding works for both ground-based and flying enemies? I've been trying to wrap my head around how to handle this on my own game, and trying not to invent a wholly separate solution for enemies than can avoid obstacles vertically.\n \nreply",
      "The scaffolding is built from convex volumes, and I know which cuboid faces are \"walls\" and which are \"windows\" so I link the windows into a graph where the shared-faces are waypoints and do A*, using straight-line distance as the cost function.Big obstacles are just built from the scaffold itself, and small obstacles are avoidable by just doing \"whisker-traces\" to veer around them (the navigation path is a movement hint used to calculate accelerations, not followed strictly).\n \nreply",
      "Great write up, very interesting. Shame y\u2019all couldn\u2019t get funding even after Day of the Devs! Hang in there, game looks cool.\n \nreply",
      "Last year was rough for indie funding because of interest rates. Luckily, Absolute Nothing Economically Destabilizing Is Happening This Time (O___o).\n \nreply",
      "Isn't this kind of like simply putting the data structures from the first Unreal Engine (which I believe used BSP), into the latest one? For the game you appear to be building, this appears to be rather unnecessary, even for authoring purposes. Even the Nintendo Switch 1, which is being phased out, might not necessarily benefit enough performance-wise to overhaul these core systems.\n \nreply",
      "It's not a BSP, it's volumetric data (this is in the actual article). Just off the bat I get aerial navigation \"for free\" without having to hand-place regions, and a secondary collision-query for bullet-hell that's much faster than Chaos, which was a huge perf regression in UE5 from PhysX (which already wasn't great).Additionally, there's productivity + rapid iteratoin. Trying to build interiors with Unreal's built in drag-out-an-actor-at-time interface is too laborious, and the built-in automations (Construction Scripts, Blutilities, PCG) are impedance-mismatched for constructing interiors quickly.\n \nreply",
      "Thanks, Max!  I've personally never been able to wrap my head around BSP; I appreciate your diagrams explaining everything, and I am optimistic about the new engine.  It's crazy how much faster it can be; really drives home how much extra stuff Unreal is doing that your game may not need.\n \nreply"
    ],
    "link": "https://blog.littlepolygon.com/posts/scaffold/",
    "first_paragraph": "On the latest Nightshift Galaxy weekly dev stream I demonstrated the specialized level editting tool I\u2019m building inside the Unreal Level Editor that I\u2019m calling Scaffold.I gave an impromptu introduction to my motivations and inspirations live, but on reflection the topic deserves a written deep-dive.In designing tools I\u2019m guided by three high-level goals:Scaffold addresses these goals by exposing (i) an interactive interior design tool that prioritizes power-user hotkeys and hackability, (ii) gameplay systems that supplement Unreal\u2019s built-in collision and navigation with opinionated parallel systems, and (iii) data-structures (inspired by 90s game engines) that are efficient by-construction.To explain the details of the system, we need to first establish some background in Convex Geometry that\u2019s relevant to the design. In a nutshell, the convex hull property describes any shape where the straight-line connecting any two points within the shape is also contained in the shape.\nSample C"
  },
  {
    "title": "Fifty Years of Open Source Software Supply Chain Security (acm.org)",
    "points": 103,
    "submitter": "yarapavan",
    "submit_time": "2025-04-07T18:04:22 1744049062",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=43614199",
    "comments": [
      "> The OpenSSH project is careful about not taking on unnecessary dependencies, but Debian was not as careful. That distribution patched sshd to link against libsystemd, which in turn linked against a variety of compression packages, including xz's liblzma. Debian's relaxing of sshd's dependency posture was a key enabler for the attack, as well as the reason its impact was limited to Debian-based systems such as Debian, Ubuntu, and Fedora, avoiding other distributions such as Arch, Gentoo, and NixOS.Does Fedora use Debian's patch set for sshd, or a similar patch set that adds libsystemd?Edit: It looks like Fedora wasn't affected because the backdoor triggered a valgrind test failure, so they shipped it with a flag that disabled the functionality that was backdoored. Seems like they lucked out. https://lists.fedoraproject.org/archives/list/devel@lists.fe...\n \nreply",
      "Great coverage, however it failed to mention code review and artifact signing as well as full source bootstrapping which are fundamental defenses most distros skip.In our distro, Stagex, our threat model assumes at least one maintainer, sysadmin, or computer is compromised at all times.This has resulted in some specific design choices and practices:- 100% deterministic, hermetic, reproducible- full source bootstrapped from 180 bytes of human-auditable machine code- all commits signed by authors- all reviews signed by reviewers- all released artifacts are multi-party reproduced and signed- fully OCI (container) native all the way down \"FROM scratch\"- All packages easily hash-locked to give downstream software easy determinism as wellThis all goes well beyond the tactics used in Nix and Guix.As far as we know, Stagex is the only distro designed to strictly distrust maintainers.https://stagex.tools\n \nreply",
      "Good step.It doesn't distrust the developers of the software though, so does not fix the biggest hole. Multiparty reproduction does not fix it either, that only distrusts the build system.The bigger the project, the higher the chance something slips through, if even an exploitable bug. Maybe it's the developer themselves being compromised, or their maintainer.Reviews are done on what, you have someone reviewing clang code? Binutils?\n \nreply",
      "As the other (dead, but correct) commenter pointed out, job one is proving the released binary artifacts even match source code, as that is the spot that is most opaque to the public where vulns can most easily be injected (and have been in the past over and over and over).Only with this problem solved, can we prove the code humans ideally start spending a lot more time reviewing (working on it) is actually the code that is shipped in compiled artifacts.\n \nreply",
      ">can most easily be injected (and have been in the past over and over and over).In practice this is much more rare then a user downloading and running malware or visiting a site that exploits their browser. Compare the number of 0days chrome has had over the years versus the number of times bad actors have hacked Google and replaced download links with links to malware.\n \nreply",
      "> Reviews are done on what, you have someone reviewing clang code? Binutils?There aren't random developers pushing commits to these codebases: these are used by virtually every Linux distro out there (OK, maybe not the Kubernetes one that ships only 12 binaries, forgot its name).It seems obvious to me that GP is talking about protection against rogue distro maintainers, not fundamental packages being backdoored.You're basically saying: \"GP's work is pointless because Linus could insert a backdoor in the Linux kernel\".In addition to that determinism and 100% reproducibility brings another gigantic benefit: should a backdoor ever be found in clang or one of the binutils tool, it's going to be 100% reproducible. And that is a big thing: being able to reproduce a backdoor is a godsend for security.\n \nreply",
      ">full source bootstrapped from 180 bytes of human-auditable machine codeWhat does this mean?  You have a C-like compiler in 180 bytes of assembler that can compile a C compiler that can then compile GCC?\n \nreply",
      "That\u2019s normally what this means, yes, with a few more intermediate steps. There\u2019s only one bootstrap chain like this that I know of[1,2,3], maintained by Jeremiah Orians and the Guix project; judging from the reference to 180 bytes, that\u2019s what the distro GP describes is using as well.> This is a set of manually created hex programs in a Cthulhu Path to madness fashion. Which only have the goal of creating a bootstrapping path to a C compiler capable of compiling GCC, with only the explicit requirement of a single 1 KByte binary or less.[1] https://guix.gnu.org/en/blog/2023/the-full-source-bootstrap-...[2] https://savannah.nongnu.org/projects/stage0/[3] https://github.com/oriansj/bootstrap-seeds\n \nreply",
      "That's pretty awesome\n \nreply",
      "As per their landing page, yes.> stage0: < 190 byte x86 assembly seed is reproduced on multiple distros> stage1: seed builds up to a tiny c compiler, and ultimately x86 gcc> stage2: x86 gcc bootstraps target architecture cross toolchainsvery impressive, I want to try this out now.\n \nreply"
    ],
    "link": "https://queue.acm.org/detail.cfm?id=3722542",
    "first_paragraph": ""
  },
  {
    "title": "Hasochism: The pleasure and pain of dependently typed Haskell programming [pdf] (2013) (strath.ac.uk)",
    "points": 69,
    "submitter": "fanf2",
    "submit_time": "2025-04-07T17:42:02 1744047722",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=43613994",
    "comments": [
      "Final line is gold.\n \nreply",
      "> we sometimes blur the distinctions between these distinctions> venerable program extraction algorithm> type inference seems a timid virtue> it is a nuisance that the kind-level \u2200 is compulsorily implicitIt sounds fun to do academic research, is that why there's an oversupply of PhD students?\n \nreply",
      "Conor has always had a turn of phrase - every paper has at least one gem.* \"the engineering of coincidence is replaced by the propagation of consequence\" (https://www.cs.ox.ac.uk/projects/utgp/school/conor.pdf)* \"programmers who do not wish the ordering guarantees are entitled not to pay and not to receive\", or \"We could plough on with proof and, coughing, push this definition through, but tough work ought to make us ponder if we might have thought askew.\" (https://strathprints.strath.ac.uk/51678/7/McBride_ICFP_2014_...)* \"be minimally prescriptive, not maximally descriptive\" (https://personal.cis.strath.ac.uk/conor.mcbride/PolyTest.pdf)Their papers are also generally presented very well - easy to follow, as these things go (by which I mean it's actually feasible to work through with a pencil and paper and understand the contents within a few hours). I really recommend PolyTest which is particularly edifying; before I learned the \"avoid the green slime\" principle, dependent types were a battle, but when I read it I was enlightened, and it's super interesting to watch the types evolve as the problem is understood.\n \nreply",
      "Yes, it's phd students there's an oversupply of, not mediocre web devs at ad companies\n \nreply",
      "You can use fun language for any discpline. Even stamp collecting.\n \nreply",
      "twelve years have passed - is DT less painful today?\n \nreply",
      "I am not sure, but as a Typescripter, I think I'd find refinement types easier - https://github.com/ucsd-progsys/liquidhaskellI am not sure if they serve the same purpose or how the venn diagrams overlap on this, but in 2000 I loved the idea of the assersion in Ada, and I love even more the idea the type system can prove your number is between 1 and 10 (etc.).I reckon it occasionally will catch a bug, but more than that is perfect documentation. I don't want delay to be an int, I want it to be a RateLimitBackoffDelaySeconds which is between >0 and <60, for example.\n \nreply",
      "As I understand the nomenclature liquid types are refinement types and they are dependent types, but typically when people say \"dependent types\" without qualification they mean the much more powerful (and difficult) \"full\" dependent types where you can do stuff like returning an int or a string depending on some runtime value.https://goto.ucsd.edu/~ucsdpl-blog/liquidtypes/2015/09/19/li...\n \nreply",
      "David Christiansen did a more recent experience report from 2019 about using Dependently Typed Haskell at Galois.https://davidchristiansen.dk/pubs/dependent-haskell-experien...The video is on YouTube somewhere. Having used Haskell and some dependently typed Haskell around the same time, I thought it was a fair assessment of state of play.\n \nreply",
      "Yes.It could be much better but Dependent Haskell has been stymied by pearl clutching in the community about \u201cpragmatism,\u201d and \u201cscaring away new users.\u201d And a few technical hurdles.I want Dependent Haskell to happen. I think that when you need it, you need it. And when you only have partial support for it you end up with a very confusing set of libraries and conventions to encode dependent types in Haskell that is probably more off-putting than proper language support (singletons, GADTs, type classes and associated type families and.. and.. and..).That being said, Haskell isn\u2019t exactly funded by a company like Microsoft. It\u2019s a small community and the teams driving these projects are small and staffed with keen volunteers for the most part.I don\u2019t think DH will happen but I hope it will get through.\n \nreply"
    ],
    "link": "https://personal.cis.strath.ac.uk/conor.mcbride/pub/hasochism.pdf",
    "first_paragraph": ""
  },
  {
    "title": "20 years of Git (gitbutler.com)",
    "points": 201,
    "submitter": "videlov",
    "submit_time": "2025-04-07T16:37:57 1744043877",
    "num_comments": 128,
    "comments_url": "https://news.ycombinator.com/item?id=43613305",
    "comments": [
      "There\u2019s something that bothers me about these sorts of recollections that make git seem\u2026 inevitable.There\u2019s this whole creation myth of how Git came to be that kind of paints Linus as some prophet reading from golden tablets written by the CS gods themselves.Granted, this particular narrative in the blog post does humanise a bit more, remembering the stumbling steps, how Linus never intended for git itself to be the UI, how there wasn\u2019t even a git commit command in the beginning, but it still paints the whole thing in somewhat romantic tones, as if the blob-tree-commit-ref data structure were the perfect representation of data.One particular aspect that often gets left out of this creation myth, especially by the author of Github is that Mercurial had a prominent role. It was created by Olivia Mackall, another kernel hacker, at the same time as git, for the same purpose as git. Olivia offered Mercurial to Linus, but Linus didn\u2019t look upon favour with it, and stuck to his guns. Unlike git, Mercurial had a UI at the very start. Its UI was very similar to Subversion, which at the time was the dominant VCS, so Mercurial always aimed for familiarity without sacrificing user flexibility. In the beginning, both VCSes had mind share, and even today, the mindshare of Mercurial lives on in hg itself as well as in worthy git successors such as jujutsu.And the git data structure isn\u2019t the only thing that could have ever possibly worked. It falls apart for large files. There are workaround and things you can patch on top, but there are also completely different data structures that would be appropriate for larger bits of data.Git isn\u2019t just plain wonderful, and in my view, it\u2019s not inevitable either. I still look forward to a world beyond git, whether jujutsu or whatever else may come.\n \nreply",
      "The article is written by a co-founder of github and not Linus Torvalds.git is just a tool to do stuff.  It's name (chosen by that Finnish bloke) is remarkably apt - its for gits!It's not Mecurial, nor github, nor is it anything else.  Its git.It wasn't invented for you or you or even you.  It was a hack to do a job: sort out control of the Linux kernel source when Bit Keeper went off the rails as far as the Linux kernel devs were concerned.It seems to have worked out rather well.\n \nreply",
      "I'm curious why you think hg had a prominent role in this. I mean, it did pop up at almost exactly the same time for exactly the same reasons (BK, kernel drama) but I don't see evidence of Matt's benchmarks or development affecting the Git design decisions at all.Here's one of the first threads where Matt (Olivia) introduces the project and benchmarks, but it seems like the list finds it unremarkable enough comparatively to not dig into it much:https://lore.kernel.org/git/Pine.LNX.4.58.0504251859550.1890...I agree that the UI is generally better and some decisions where arguably better (changeset evolution, which came much later, is pretty amazing) but I have a hard time agreeing that hg influenced Git in some fundamental way.\n \nreply",
      "Please don't do that. Don't deadname someone.I'm not saying that hg influenced git, but I'm saying that at the time, both were seen as worthy alternatives. Lots of big projects were using hg at one point: Python, Mozilla, Netbeans, Unity.Sure, you managed to get Github in front of everyone's face and therefore git. For a while, Bitbucket was a viable alternative to many.Were you involved with the decision to sponsor hg-git? I understand that at one point it was hoped that this would help move more people from hg into Github, just like Subversion support for Github would. I think the latter is still there.\n \nreply",
      "\"One particular aspect that often gets left out of this creation myth, especially by the author of Github is that Mercurial had a prominent role.\" implies to me that Hg had a role in the creation of Git, which is why I was reacting to that.For the deadnaming comment, it wasn't out of disrespect, but when referring to an email chain, it could otherwise be confusing if you're not aware of her transition.I wasn't sponsoring hg-git, I wrote it. I also wrote the original Subversion bridge for GitHub, which was actually recently deprecated.https://github.blog/news-insights/product-news/sunsetting-su...\n \nreply",
      "> For the deadnaming comment, it wasn't out of disrespect, but when referring to an email chain, it could otherwise be confusing if you're not aware of her transition.I assumed it was innocent. But the norm when naming a married woman or another person who changed their name is to call them their current name and append the context. Not vice versa. Jane Jones n\u00e9e Smith. Olivia (then Matt).\n \nreply",
      "Another alternative is the patch-theory approach from Darcs and now Pijul. It's a fundamentally different way of thinking about version control\u2014I haven't actually used it myself but, from reading about it, I find thinking in patches matches my natural intuition better than git's model. Darcs had some engineering limitations that could lead to really bad performance in certain cases, but I understand Pijul fixes that.\n \nreply",
      "> There\u2019s this whole creation myth of how Git came to be that kind of paints Linus as some prophet reading from golden tablets written by the CS gods themselves.What?> Git isn\u2019t just plain wonderful, and in my view, it\u2019s not inevitable either.I mean, the proof is in the pudding. So why did we end up with Git? Was it just dumb luck? Maybe. But I was there at the start for both Git and Mercurial (as I comment elsewhere in this post). I used them both equally at first, and as a Python aficionado should've gravitated to Mercurial.But I like to understand how tools work, and I personally found Mercurial harder to understand, slower to use, and much less flexible. It was great for certain workflows, but if those workflows didn't match what you wanted to do, it was rigid (I can't really expound on this; it's been more than a decade). Surprisingly (as I was coding almost entirely in Python at the time), I also found it harder to contribute to than Git.Now, I'm just one random guy, but here we are, with the not plain wonderful stupid (but extremely fast) directory content manager.\n \nreply",
      "In early 2000s I was researching VCSs for work and also helping a little developing arch, bazaar then (less so) bzr.  I trialed Bitkeeper for work.  We went with Subversion eventually. I think I tried Monotone but it was glacially slow.  I looked at Mercurial.  It didn't click.When I first used Git I thought YES! This is it.  This is the one.  The model was so compelling, the speed phenomenal.I never again used anything else unless forced -- typically Subversion, mostly for inertia reasons.\n \nreply",
      "I just wish they'd extend git to have better binary file diffs and moved file tracking.Remembering the real history matters, because preserving history is valuable by itself, but I'm also really glad that VCS is for most people completely solved, there's nothing besides Git you have to pay attention to, you learn it once and use it your whole career.\n \nreply"
    ],
    "link": "https://blog.gitbutler.com/20-years-of-git/",
    "first_paragraph": "\n\t\t\t\t\tTwenty years ago, Git was born. How did this unlikely \"information manager\" take over the world?\n\t\t\t\tTwenty years ago today, Linus Torvalds made the very first commit to Git, the information manager from hell. Over these last 20 years, Git went from a small, simple, personal project to the most massively dominant version control system ever built.I have personally had a hell of a ride on this particular software roller coaster.I started using Git for something you might not imagine it was intended for, only a few months after it\u2019s first commit. I then went on to cofound GitHub, write arguably the most widely read book on Git, build the official website of the project, start the annual developer conference, etc - this little project has changed the world of software development, but more personally, it has massively changed the course of my life.I thought it would be fun today, as the Git project rolls into it\u2019s third decade, to remember the earliest days of Git and explain a bit "
  },
  {
    "title": "A Supermarket Bag and a Truckload of FOMO (julik.nl)",
    "points": 21,
    "submitter": "julik",
    "submit_time": "2025-04-07T21:10:32 1744060232",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=43615986",
    "comments": [
      ">  See, I know that shipping end-user software - like the Tailwind compiler - if that software is written in an interpreted language - is fucking hard.This I don't get. This is interpreted code, most web backends integrate node, why not just ship a node module? Why in gods name ship bun.The whole web development scene has had some of the worst software engineering I've ever seen. With the exception of the Ruby scene, we have Tilt and Nokogiri for these types of things.\n \nreply",
      "I was hesitant to give tailwind a try, like most aging web developers I couldn't stand that it breaks the \"cascading\" part of CSS.But eventually I didn't have choice as I inherited a web app that has all of the newfangled build components that web apps come with.  I love that we're coming full circle back to MVC with server components.After getting used to it, I ended up liking Tailwind, mostly because it breaks the cascading part of CSS.  There are so many unique parts to webpages these days that I think it makes sense to keep the styles close to their components, as opposed to global cascading components.\n \nreply",
      "Yeah, I've been a fairly vocal curmudgeon about Tailwind within my team. Some folks really like it. They're doing the work and I trust them, so I rolled with it.I still have qualms with Tailwind. My classic CSS sensibilities are offended, but whatever. The part that I still don't like is really what this post boils down to: a massively complex build system that creates footguns in the weirdest places.That being said, Tailwind that's set up well in coordination with a complex design system really does feel like it's a win. Seeing that in action was an aha moment where I was able to see value that made some of the tradeoffs worth it.\n \nreply",
      "I have genuinely never understood Tailwind's value proposition, other than as padding for its developers' CVs, at which I assume it excels.We stopped inlining style attributes for a reason - is this just how the next generation needs to learn?\n \nreply",
      "I was a web developer from 1996 until 2023. I jumped ship because of exactly this sort of nonsense. I still do private web development, but I do it all using native vanilla HTML, JS, and CSS. It just works.\n \nreply"
    ],
    "link": "https://blog.julik.nl/2025/03/a-little-adventure-in-modern-frontend",
    "first_paragraph": "The day was nearing to a close. The sun has already set, but that Friday evening in Amsterdam was still warm. Unusually warm, in fact, for those late days in March \u2013 as if spring decided to bless my piligrimage, for that piligrimage was not jovial.I was sitting at a ramen joint, sipping on the broth. To my left, a blue, crinkled supermarket shopping bag was sitting solemnly, inconspicuously.Inside that bag sat a slightly used Mac Studio, which I have just purchased to be able to edit CSS of my own application.By the time that evening descended upon the south of Amsterdam, I have lost 3 days of my life trying to figure out why I was unable to edit CSS.But let me rewind a bit.I tend to listen to people. So when the time came to start a new app - which I haven\u2019t done in quite some time - I knew that I would like it to be modern enough. Modern enough, even, for me to be able to delegate some work on it to others once the time comes.So, while traveling, I started coding on that app - on the"
  },
  {
    "title": "Decoding the 90s: Cryptography in Early Software Development (2023) (botanica.software)",
    "points": 97,
    "submitter": "mu0n",
    "submit_time": "2025-04-07T14:43:17 1744036997",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=43612102",
    "comments": [
      "I have encountered my fair share of in-house RC4 implementations from the 90s.  Every single one of them was vulnerable in some way.  They suffered from all kinds of issues: improper IV initialisation, predictable keystreams, and even partial leakage of plaintext into ciphertext.  RC4's deceptively simple specification made it enticing to implement, giving developers a false sense of confidence and security.As another example, Microsoft Outlook 2003 infamously used CRC32 to \"hash\" the personal folder (.PST) passwords: <https://www.nirsoft.net/articles/pst_password_bug.html>.  Naturally, it was trivial to find a CRC32 checksum collision and open someone else's PST.Thankfully, the industry has come a long way since then.  These days, rolling your own cipher is, quite rightly, considered a red flag!\n \nreply",
      "I've seen far too many IVs statically declared as \"<Product>IV\" in my lifetime.Bonus marks for when the key was also \"<Product>Key\".\n \nreply",
      "Nirsoft saved my ass so many times on different things. I remember when I lived somewhere without (reliable or consistent) internet access, I scraped all the tools to take with me. They still are in my tools folder to this day!\n \nreply",
      "Why waste 2 seconds of my time making your website have a splash screen?\n \nreply",
      "I know this would be less fun, but given that the key space was only 36^4, why not just run the actual decryption functionality in QText? Like, even if it takes 1 second to decrypt, spin up 32 cores and wait a day. They allude to the idea that checking the key derivation is faster, but I wonder by how much.(of course, it\u2019s still interesting to read about 90s encryption, so I appreciate that they did it the fun way)\n \nreply",
      "Love this article. Brings back memories of such a simpler time spending way too much time doing exactly this with IDA pro and Bochs (my favorite tool at the time for these sorts of projects). Bochs plus custom plugins equaled some amazing capabilities for real-time dynamic analysis of DOS, bootcode, and other low-level applications.\n \nreply",
      "And crazy OS debugging up tp i7 processors.\n \nreply",
      "90's crypto was interesting. They would just use naked RSA and block ciphers. Usually, the team would have one guy who was \"smart about crypto\" and he was just left to do his thing and after passing functional tests, it was accepted into the product. There was so much fun stuff to break as was it fun to try to prevent people from breaking your stuff.\n \nreply",
      "Even companies as well resourced as Microsoft made these mistakes well into the 2000s. Remember when they used plain old AES to encrypt the Viewstate for ASP.Net? It was vulnerable to padding oracle attacks: https://en.wikipedia.org/wiki/Padding_oracle_attack#Attacks_...Cryptography is such an esoteric and deep field that it's easy for a fairly smart but inexperience engineer to misjudge the security of a particular implementation or usage of a cryptographic primitive.\n \nreply",
      "> Even companies as well resourced as Microsoft made these mistakes well into the 2000s.Indeed!  As I just wrote in another comment on this page, Microsoft Outlook 2003 used CRC32 to \"hash\" the personal folder (.PST) passwords.  Since CRC32 isn't a cryptographic hash, it was trivial to generate a collision and access someone else's Outlook personal folder.  This flaw persisted until at least 2006!  More details here: <https://www.nirsoft.net/articles/pst_password_bug.html>.\n \nreply"
    ],
    "link": "https://www.botanica.software/post/decoding-the-90s",
    "first_paragraph": "What We DoPortfolioClientsBlogContact UsMoreIntro Getting started - QTextReversing MS-DOS binariesPKZipint 3f - MS-DOS OverlaysKey expansion functionTracing through passcode flowKey derivation functionReversing the key derivationReversing the first stage - 4 bytes to 4 printable charactersReversing the second stage - 16 bytes to \ud835\udfe6\u00d7\ud835\udfe6 byte stringPutting it all togetherIn August 2020, we were commissioned by a client with a cache of locked QText documents from the mid 90s - to which he has lost the passcode.QText was a DOS era Hebrew-English word processor written in Turbo Pascal, released 15 or so odd years before neither I nor @Elisha had laid hands on a reverse engineering tool.In this blog post, we\u2019ll describe the process of analyzing the encrypted documents and reverse engineering a DOS program.Hopefully we\u2019ll be able to provide some insight into the early days of software development in Israel, and more generally into how cryptography was viewed and implemented in the early days of "
  },
  {
    "title": "PiDP-1, or the rebirth of an old machine (hackaday.io)",
    "points": 69,
    "submitter": "ozymandiax",
    "submit_time": "2025-04-05T18:06:09 1743876369",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=43595326",
    "comments": [
      "As one of those guys in the photo I really hope everyone will have a lot of fun with this :)The photos don't show it but you can also have your PiDP-1 in white instead of blue. It will come with both sets of front panels.\n \nreply",
      "Wow, one of these should be at a museum or public institution.\n \nreply",
      "The blue is amazing though!\n \nreply",
      "This looks fascinating! is there a link that would be a better intro to the project?\n \nreply",
      "In case you didn't see, the hackaday project page has two more links on the side, but currently information is a bit sparse, sorry. We're currently on a sprint to finish and polish the software distribution.\n \nreply",
      "There was this link on the page \nhttps://hackaday.io/project/202541-replica-of-the-pdp-1-pidp...\n \nreply",
      "Ah thanks. I didn't scroll down far enough\n \nreply",
      "I have so much fun with my PiDP-11 that this will have to go on the list.Does anyone know if MIDAS has been recreated for it or not?A bit of the hint on the PPT reader, which would be awesome, the rp2xxx chips PIO can handle that with ease.  As the PDP1 doesn't have take-up it is pretty trivial to make at 'historic' speeds.  Obviously punching is much more difficult.I looked into it for quite a bit, but not having access to tapes, puncher, supplies etc... I gave up on the idea as the PiDP-10 will scratch my personal nostalgia itch a bit more right now.\n \nreply",
      "There are some versions of MIDAS around on bitsavers but I have to admit i haven't really looked into them much yet. Gotta leave something to the community, right? (or so i tell myself...). Another thing is that not all PDP-1s were the same and especially the PDP-1/X at MIT was a super hacked up version. Any code for that will not run my emulation (at least for now).A real tape reader would be great and is something we are considering for the future, but without a punch it's not very satisfying. There will be a paper tape visualization though (currently writing the code for that).\n \nreply",
      "Delighted by the resemblance of the HN title to Tracy Kidder's Pulitzer Prize-winning book, \"The Soul of a New Machine.\"https://en.wikipedia.org/wiki/The_Soul_of_a_New_Machine\n \nreply"
    ],
    "link": "https://hackaday.io/project/202541-replica-of-the-pdp-1-pidp-1/log/239666-finished-the-first-test-batch-of-5-machines",
    "first_paragraph": "A project log for Replica of the PDP-1: PiDP-1Blinkenlights and vectordot graphics from 1959In other words, we increased the number of PDP-1s ever produced by 10% in two days :-)Lots of polishing up still two do. We're hiding from the world for two weeks to get it all done. Probably <i>almost</i> all done. <i>Mostly done.</i>\n\n\n\nLog In/Sign up to comment\n\nCreate an account to leave a comment.\nAlready have an account?\nLog In.\n            So cool. \n\n                    \n                    \n                    \u00a0\n                \nAre you sure? yes | no\nTop\n\n                    \n                    \n                    \u00a0\n                \nAre you sure? yes | no\nawesome!\n\n                    \n                    \n                    \u00a0\n                \nAre you sure? yes | no\n\nYes, delete it\nCancel\n\nAbout Us\nContact Hackaday.io\nGive Feedback Terms of Use\nPrivacy Policy\nHackaday API\n\u00a9 2025 Hackaday"
  },
  {
    "title": "Silicon Valley 'nepo baby' publishes scathing first novel about growing up rich (sfstandard.com)",
    "points": 21,
    "submitter": "CalChris",
    "submit_time": "2025-04-08T00:22:15 1744071735",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=43617237",
    "comments": [
      "This is unintentionally a pretty amusing puff piece, and indicates the book is going to be horrible.> His novel is not so much an attempt to break away from that world \u2014 he loves investing and working with entrepreneurs, he says \u2014 as it is an opportunity to skewer it.He started a crypto company and then took a father-sponsored investing job. He's done nothing to show he has the appropriate perspective to do more than lightly self-flagellate himself and his friends.When asked about his responsibilities as an investor:> \u201cI just do the best I can to be like, I really like this opportunity. I think this could be really cool, and a positive thing, and really fun to work on,\u2019\u2019 he says. \u201cBut yes, if I backed a company that turned out to, like, cause horrible harm for people and tons of negative effects, I\u2019d feel fucking terrible.\u201dOh he'd _feel_ terrible. Well good, I thought for a second there wouldn't be appropriate consequences.What an airhead.I don't know anything about the elder Breyer, but this book promotion looks like a great promotion of massive inheritance taxes.\n \nreply",
      "He says he doesn't want to come across as some kind of class-war warrior, but I think this is his future fate, at least until he writes another kind of book.Fitzgerald married into wealth. Parker was born into the fringes.\n \nreply",
      "I don't know why people are so shaky about calling the class war what it is. I remember this being a big issue during the Obama era, Republicans accusing his proposal to raise taxes as \"class warefare\". It's like Warren Buffet said: \"There's class warfare, all right, but it's my class, the rich class, that's making war, and we're winning.\u201d\n \nreply",
      "I don't disagree. I only commented on a thing he was quoted on in the article.An awful lot of the energy in left wing intellectual circles has always come from what Lenin called \"useful idiots\" and this guy is no different.Objectifying things as class war is a mechanism to cast the warrior into the bad set basically. I suspect he said it, because he doesn't want it dismissed by some readers on the basis thats his place. I don't personally think it invalidates the writing.\n \nreply",
      "What would make the story perfect is to find out that the book was Ghost written\n \nreply",
      "Silicon Valley meets Bret Easton Ellis and Less than Zero.https://en.wikipedia.org/wiki/Less_than_Zero_(novel)\n \nreply",
      ">  Asked about his life as an investor, and whether he thinks about the potential harms of companies he invests in \u2014 such as AI startups that require vast quantities of energy to run their chatbots, or automation companies that could replace human jobs \u2014 his answers are slightly less considered. \n\nBeing rich seems to \u2018lessen\u2019 the capacity to feel empathy for other people or responsibility for your actions (the questionable things you invest in).\n \nreply",
      "It's really hard to separate oneself from one's socioeconomic upbringing. For those whom did not grow up at the edge of a great financial cliff, that cliff is merely a metaphor for 'a relative had to bail me out, how embarrassing', while for those whom lives at it's very edge, never slept too easy lest the fall and hope that there exists a social safety net, whilst others learned to live on the cliff face itself, eeking out a small subsistence before the inevitable meth-fueled plunge.\n \nreply",
      "Power doesn't corrupt; it reveals.\n  --Robert Caro\n \nreply",
      "Nearly all men can stand adversity, but if you want to test a man's character, give him power -- Abraham Lincoln\n \nreply"
    ],
    "link": "https://sfstandard.com/2025/04/07/jim-breyer-son-daniel-breyer-novel-smokebirds/",
    "first_paragraph": "In his debut book, Daniel Breyer, son of billionaire VC Jim Breyer, skewers the world of wealth and privilege he grew up in.Daniel Breyer swears he isn\u2019t mad at his dad.\u00a0\u201cHe\u2019s done nothing but give me an incredible life, and he\u2019s a really good dad,\u201d Breyer says of his father, Jim, the billionaire founder and CEO of investment and venture philanthropy firm Breyer Capital. \u201cI care a lot about him.\u201dThis is slightly difficult to believe, given that the younger Breyer\u2019s debut novel, \u201cSmokebirds,\u201d to be released Tuesday by Rare Bird Lit, is a searing indictment of a fabulously wealthy, high-society San Francisco family \u2014 exactly the kind of tribe he grew up in.\u00a0\u201cI would hope that my dad doesn\u2019t take any shit from this book,\u201d Daniel says in an interview at a bustling La Boulangerie in Noe Valley. \u201cHe\u2019s a much better person than all of my characters, I promise.\u201dBreyer\u2019s father is arguably one of the most successful venture capitalists in the country, with early investments in Facebook and Etsy"
  },
  {
    "title": "Show HN: Minimal MCP server in Go showcasing project architecture (github.com/tuankiri)",
    "points": 23,
    "submitter": "TuanKiri",
    "submit_time": "2025-04-07T18:44:17 1744051457",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://github.com/TuanKiri/weather-mcp-server",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A lightweight Model Context Protocol (MCP) server that enables AI assistants like Claude to retrieve and interpret real-time weather data.\n      \n\nReport Bug | Request FeatureA lightweight Model Context Protocol (MCP) server that enables AI assistants like Claude to retrieve and interpret real-time weather data.To use your MCP server with Claude Desktop, add it to your Claude configuration:You can get your API key in your personal account at weatherapi.You can use go to build the binary in the cmd/github-mcp-server directory.current_weather - Gets the current weather for a cityThe project is organized into several key directories:Feel free to open tickets or send pull requests with improvements. Thanks in advance for your help!Please follow the contribution guidelines.This MCP server is licensed under the MIT License.\n        A ligh"
  },
  {
    "title": "Rising odds asteroid that briefly threatened Earth will hit moon (phys.org)",
    "points": 6,
    "submitter": "pseudolus",
    "submit_time": "2025-04-04T10:56:59 1743764219",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://phys.org/news/2025-04-odds-asteroid-briefly-threatened-earth.html",
    "first_paragraph": ""
  },
  {
    "title": "Rsync replaced with openrsync on macOS Sequoia (derflounder.wordpress.com)",
    "points": 522,
    "submitter": "zdw",
    "submit_time": "2025-04-06T21:14:09 1743974049",
    "num_comments": 417,
    "comments_url": "https://news.ycombinator.com/item?id=43605003",
    "comments": [
      "Looking at the sparse documentation of openrsync does not create any confidence for me that it can be an acceptable substitute for rsync.In my opinion, any program that is supposed to copy files, but which is not able to make perfect copies, i.e. copies that do not lose any bit of data or metadata that was present in the original file, is just unusable garbage.Unfortunately, most copying programs available in UNIX-like operating systems (and also many archiving programs) do not make perfect file copies with their default options and many of them are never able to make perfect copies, regardless what options are used.I have not looked recently at the scp command of ssh, but at least until a few years ago it was not possible to make perfect file copies with scp, especially when the copies were done between different operating systems and file systems. That is why I never use scp, but only rsync over ssh.Rsync is the only program that I have seen, which is able (with the right options) to make perfect file copies even between different operating systems and file systems (for instance between FreeBSD with UFS and Linux with XFS), preserving also metadata like extended file attributes, access control lists and high-precision file timestamps (some copying programs and archiving programs truncate high-precision timestamps).The current documentation of openrsync does not make any guarantee that it can make complete file copies, so by default I assume that it cannot, so for now it is a program that I consider useless.Beside rsync for copying, one of the few Linux archiving programs that can archive perfect file copies is bsdtar (when using the pax file format; the ancient tar and cpio file formats cannot store all modern file metadata).(FYI: I always alias rsync to '/usr/bin/rsync --archive --xattrs --acls --hard-links --progress --rsh=\"ssh -p XXX -l YYYYYYY\"')(With the right CLI options, \"cp\" from coreutils can make perfect file copies, but only if it has been compiled with appropriate options; some Linux distributions compile coreutils with wrong options, e.g. without extended file attributes support, in which case \"cp\" makes only partial file copies, without giving any warnings or errors.)\n \nreply",
      "As a contrast to your take - I work for a backup company and I was really surprised to discover most of our customers (big enterprises) really do not care about 99% of metadata restored correctly and are fine with just restoring the data.(We restore everything super carefully but sometimes I feel like we're the only ones who care)\n \nreply",
      "I'm willing to bet a decent number \"don't care\" until they do care because their permissions don't work or their time based script screws up or whatever else nobody thinks about when they're in panic mode about \"I lost my data\".\n \nreply",
      "In case of a complete disaster recovery, the fact that a script or two might fail is super OK. That's why after recovery there's always the cleanup phase where you fix stuff that broke during recovery.\n \nreply",
      "They don't care because you care, so they never experienced the misfortune of not caring.\n \nreply",
      "Nah. Not really. A lot of the useful data out there doesn't need ACL, precise (or any dates at all) etc.Also, a lot of application-specific data formats already don't care about the \"extra\" attributes available in various filesystems because those aren't universally supported and implement them themselves in the file format they operate on. For example, DICOMs, or password-protected PDFs or Zip archives etc.\n \nreply",
      "Extended attributes (and resource forks)  are mostly a liability and anti-pattern because of their non portability. It would be a huge red flag to find something important in there other than cases of backing up entire OS images.\n \nreply",
      "I too have worked at a back company and I can\u2019t recall any of the customers caring or even knowing about the metadata.We would only care if the software our customers were running did. Big enterprise software suits were defined to run in hostile environments, in such they mostly rely on their data formats and don\u2019t care about attributes from the filesystem other than so they have access.\n \nreply",
      "I'm with you on this, I think that data is 99% of what is important and the rest can be recreated or improvised and if in your system you rely too much on file metadata your need more engineering\n \nreply",
      "> in your system you rely too much on file metadata your need more engineeringExcept sometimes it's a 3rd party's app whose data you have to restore, and you don't have control over their engineers.\n \nreply"
    ],
    "link": "https://derflounder.wordpress.com/2025/04/06/rsync-replaced-with-openrsync-on-macos-sequoia/",
    "first_paragraph": "On many Unix-based operating systems, rsync is a command line tool for transferring and synchronizing files on a computer, either between storage attached directly to the computer or between another computer located elsewhere on a network. The rsync command line tool has long been included on macOS, but Apple has provided the last version of rsync 2.x (rsync 2.6.9, released in November 2006) and did not update rsync past that even though rsync 3.x was released. Why not? It has to do with the version of the GNU General Public License (GPL) open source license that rsync 2.x and 3.x were released under, with rsync 2.x being released under the GPLv2 license and rsync 3.x being released under the GPLv3 license. Without going in-depth into the background legal issues, the reason for not providing rsync 3.x is that Apple decided that while it could comply with the terms of GPLv2 license with regards to rsync 2.x, it could not comply with the terms of GPLv3 license with regards to rsync 3.x.W"
  },
  {
    "title": "The Troll Hole Adventure (bluerenga.blog)",
    "points": 30,
    "submitter": "todsacerdoti",
    "submit_time": "2025-04-07T17:52:53 1744048373",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=43614081",
    "comments": [
      "I have a rather sizable collection of 1980s home computers with over a hundred different models (virtually all of the Ataris, Amigas, Apples, \nCommodores, Radio Shacks, Sinclairs, Amstrads, Dragons, MSX machines, etc). I started collecting them in the early 90s when most were less than $5 at thrift stores and I'd stopped collecting by the year 2000 because I basically had them all.Collecting back when they were considered basically worthless junk by most people meant I paid no more than $30 for any one machine and most were actually given to me for free or shipping cost. I do have an Interact but it's one of the few I've never booted up as I never had one back in the day and being so obscure it wasn't one of the computers I lusted after but couldn't afford in the 80s. This article has inspired me to dig it out and fire it up. I heard from someone a few years back that apparently Interacts are quite valuable now due to their rarity (not that I'm in the market to sell any of mine).\n \nreply",
      "I still have the Troll Hole Adventure on tape, along with the Interact that I learned programming on (MS basic! and machine code - we didn't have an assembler).As I recall, you don't flick the lighter you \"flick bic\" as was the marketing slogan at the time. The response would rhyme \"flick bic\" lead to \"lamps lit\". Other exciting things were using the paper tube from the bathroom and the lenses from somewhere else to \"make telescope\" which allowed you to read a distant sign or billboard.... Fun times.Oh right, there's a shovel and if you \"dig frog\" the response was \"I can dig it\" because you know... the 1970's...\n \nreply",
      "The \"I can dig it\" happens if you do DIG on any object other than I think the dirt. Just DIG alone is what tests digging in a room (except it only works in the starting room to find anything).Do you have any good pictures of the tape? There's a Youtube video of someone playing it on hardware so you can see the tape inside of the machine, but no clear pictures of the real tape.(Also, hello, I wrote the post! There's a part 2 where I finish the game, and I'll be doing the other Interact adventure game, Mysterious Mansion, in about a week.)\n \nreply",
      "The Trinity (I'd call them 2nd generation home computers with a screen and keyboard as opposed to 1st gen like the Altair that had a front panel and/or attached to a terminal) were easy to beat in terms of cost by third generation computers (TRS-80 Color Computer, VIC-20, Atari 400, ...) were based on ASIC for glue instead of the discrete logic used in the 2nd gen.Modern attempts to build retrocomputers run into trouble being authentic because production runs are too small to justify an ASIC so they wind up using an FPGA or ESP32 for a display controller.\n \nreply",
      "Fascinating to see there are still video games out there with no prior noted solutions or playthroughs.I would have probably got frustrated and started decompiling it to find all the verbs and nouns :/\n \nreply",
      "Obligatory reference from It's Always Sunny in Philadelphia:https://youtube.com/watch?v=CtOEig1l8SA\n \nreply",
      "\"You got to pay the troll toll if you want to get into this boy's hole!\"\n \nreply",
      "It's really hard not to devolve into a giggling teenager with this headline and how perfectly it conjures up what is perhaps the most memorable Sunny in Philadelphia episode ever made.\n \nreply",
      "If I'm being honest the only reason I clicked to the comments here is because I was expecting this\n \nreply",
      "I read that headline in Danny DeVito\u2019s voice. Always Sunny does musicals better than Broadway.\n \nreply"
    ],
    "link": "https://bluerenga.blog/2025/04/03/the-troll-hole-adventure-1980/",
    "first_paragraph": "When Kenneth Lochner was hired by Dartmouth away from Montana State College as a programmer in 1964, he had been working in computers for four years. Lochner in particular had been teaching FORTRAN and had been having a miserable time, not due to FORTRAN itself, but due to student experiences in using punch cards:Returning to the motivation for this system, let it be noted that anyone who has taught a symbolic system to beginning programmers is aware that syntax and logical errors abound in the programs they produce. One can visualize the standard scene in a [IBM] 1620 installation: a group of students loading the assembler, loading and unloading the punch hopper, entering the object deck, watching the typewriter anxiously, and then staring in increasing bewilderment at a machine which has halted, cleared or is in an infinite loop.Lochner was integral to helping develop Dartmouth\u2019s legendary time-sharing system, where a large computer could have its time divided into slices, and multip"
  },
  {
    "title": "In the 1980s we also downloaded software from TV (newslttrs.com)",
    "points": 18,
    "submitter": "spzb",
    "submit_time": "2025-04-04T21:53:33 1743803613",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=43588032",
    "comments": [
      "For those in the UK at the time, how was the code consumed? It sounds like the BBC Micro was somehow hooked up to the same \"cable\" as a TV. Is that right?Did it decode the data automatically, or did programmers at home have to build something on top of it?It just sounds incredibly ingenious on both ends. First, to invent the process and second, to use the data. I'd appreciate any knowledge that can help with the latter.\n \nreply",
      "The video of the Four Buffs might help you understand this better. [0]There was an extra cable, containing a photo diode, that you just stuck to the screen itself.[0] https://youtu.be/xxo1Gs46ti0?si=fqPIaxaHGFFFJmpF\n \nreply",
      "OMG, I just watched that. Amazing. I get it now, and it is supremely simple but jaw-droppingly so.When would a show like this be on? I don't remember anything like in the States in 1985 (I was rural though).This video is like today's YouTube.\n \nreply",
      "EDIT: Sorry! I answered without first reading the article. What I'm talking about below is different from TFA.You could record the audio to an audio cassette tape. If you had a good enough cassette deck, you could use acoustic coupling (holding up the tape deck to the TV speaker).The BBC Micro had a 7-pin DIN socket for audio in/out and remote control of an external tape deck.https://www.google.com/search?client=ms-android-google&sca_e...\n \nreply",
      "Thanks for that! That makes sense and is very cool. In the US in the 80s we did something similar from the radio (the UK probably did too). So, I assume it was a similar principle.Love it.\n \nreply",
      "Also from the same web site:https://newslttrs.com/yes-in-the-1980s-we-downloaded-games-f...\n \nreply",
      "There was also a backup system for PCs that used a TV interface card, and you connected your computer to a VCR to hold the data.  Data was stored in white scanlines on top of a black background.  Probably not the most efficient way to store data over NTSC (or PAL) video.\n \nreply",
      "Danmere Backer.IIRC I paid \u00a335 for it.\n \nreply",
      "There's more fidelity in the luma (intensity) than in the chroma (color) of NTSC anyway, so it's probably not that bad. Also avoiding color means avoiding crosstalk and fringing.\n \nreply"
    ],
    "link": "https://newslttrs.com/in-the-1980s-we-also-downloaded-software-from-tv/",
    "first_paragraph": "Explore with me two fascinating techniques for encoding software in analogue TV signalsIn the previous newslttr, I wrote about downloading software from radio broadcasts in the 1980s. It sparked some lively conversation on Hacker News which also brought up some other 80s software oddities. One obvious parallel was with software downloads via TV broadcast which is the subject of today's trip down 8k Memory Lane.Join me as once again I lead us back to the 1980s. You'll recall that the 8-bit computer craze was taking the UK by storm. Multiple TV shows were capitalising on the fervour. Most famous were the BBC ones : The Computer Programme, Micro Live and so forth. But the commercial channels (all two of them) were also in on the act. ITV had the soberly titled \"Database\" whilst upstart Channel 4 had the more funky \"Four Computer Buffs\". You'll see from the photo below just how funky and buff they were.These shows were much more \"hands on\" that you'd find today. They taught you how to code"
  }
]