[
  {
    "title": "Claude Sonnet 4.5 (anthropic.com)",
    "points": 1039,
    "submitter": "adocomplete",
    "submit_time": "2025-09-29T16:52:59 1759164779",
    "num_comments": 551,
    "comments_url": "https://news.ycombinator.com/item?id=45415962",
    "comments": [
      "I had access to a preview over the weekend, I published some notes here: https://simonwillison.net/2025/Sep/29/claude-sonnet-4-5/It's very good - I think probably a tiny bit better than GPT-5-Codex, based on vibes more than a comprehensive comparison (there are plenty of benchmarks out there that attempt to be more methodical than vibes).It particularly shines when you try it on https://claude.ai/ using its brand new Python/Node.js code interpreter mode. Try this prompt and see what happens:  Checkout https://github.com/simonw/llm and run the tests with\n  \n  pip install -e '.[test]'\n  pytest\n\nI then had it iterate on a pretty complex database refactoring task, described in my post.reply",
      "To @simonw and all the coding agent and LLM benchmarkers out there: please, always publish the elapsed time for the task to complete successfully! I know this was just a \"it works straight in claude.ai\" post, but still, nowhere in the transcript there's a timestamp of any kind. Durations seem to be COMPLETELY missing from the LLM coding leaderboards everywhere [1] [2] [3]There's a huge difference in time-to-completion from model to model, platform to platform, and if, like me, you are into trial-and-error, rebooting the session over and over to get the prompt right or \"one-shot\", it's important how reasoning efforts, provider's tokens/s, coding agent tooling efficiency, costs and overall model intelligence play together to get the task done. Same thing applies to the coding agent, when applicable.Grok Code Fast and Cerebras Code (qwen) are 2 examples of how models can be very competitive without being the top-notch intelligence. Running inference at 10x speed really allows for a leaner experience in AI-assisted coding and more task completion per day than a sluggish, but more correct AI. Darn, I feel like a corporate butt-head right now.1. https://www.swebench.com/2. https://www.tbench.ai/leaderboard3. https://gosuevals.com/agents.htmlreply",
      "That's a good call, I'll try to remember that for next time.reply",
      "I just wanted to say that I really liked your this comment which just showed professionalism and just learning from your mistakes/improving yourself.I definitely consider you to be an AI influencer, especially in hackernews communities and so I wanted to say that I see influencers who will double down,triple down on things when in reality, people just wanted to help them in the first place.I just wanted to say thanks with all of this in mind, also that your generate me a pelican riding a bicycle has been a fun ride and is always going to be interesting, so thanks for that as well. I just wanted to share my gratitude with ya.reply",
      "Have you thought about benchmarking models a month or two after release to see how it competes vs the day 1 releasereply",
      "For that to be useful I'd need to be running much better benchmarks - anything less than a few hundred numerically scored tasks would be unlikely to reliably identity differences.An organization like Artificial Analysis would be a better fit for that kind of investigation: https://artificialanalysis.ai/reply",
      "Yeah I totally agree, we need time to completion of each step and the number of steps, sizes of prompts, number of tools, ... and better visualization of each run and break down based on the difficulty of the taskreply",
      "Huh. Doesn't work over here.---SYSTEM:Got it. Those are the setup commands for Simon Willison's LLM CLI tool, which lets you interact with language models from the command line.The -e flag installs it in editable mode, and [test] pulls in the test dependencies.\nAre you running into any issues with the installation or tests? What are you working on with the LLM library?USER: Correct. You run them using your code tooling.SYSTEM:Realized inability to execute requested Python commands.The user is telling me to run these commands using my code tooling. They want me to actually execute this. Let me think about what tools I have available:I have the repl tool which runs JavaScript code in a browser environment\nI have artifacts for creating code/documents\nI have web_search and web_fetchThe user wants me to:Checkout https://github.com/simonw/llm\nRun pip install -e '.[test]'\nRun pytestBut I don't have a tool that can:Clone git repositories\nRun shell commands like pip or pytest\nExecute Python code in a command-line environmentThe repl tool is JavaScript-only and runs in a browser environment, not a server environment where I could run shell commands.\nI should clarify that I don't have the capability to run shell commands, install Python packages, or execute pytest. I can only run JavaScript code in a browser environment via the analysis tool.\nI need to be direct and clear about this limitation, following the user's preferences to eliminate filler and be straightforward.I don't have tooling that can run shell commands, clone git repos, install Python packages, or execute pytest. My only code execution environment is a JavaScript browser REPL.\nI can't run pip, pytest, or any command-line tools. I can only execute JavaScript code in a sandboxed browser environment.\nWhat were you hoping to accomplish? Do you need help understanding the test setup, or were you expecting different capabilities?Rreply",
      "When pressed:---My execution environment is a browser-based JavaScript sandbox. I don't have Python, Node.js shell access, pip, git, or pytest capabilities.Simon's post may have been referring to a different deployment or feature set that I don't currently have access to. My only code execution is the JavaScript REPL I just demonstrated.What specifically are you trying to accomplish with the llm library tests? I can help with that in other ways.EDIT: Ah, in Settings you have to disable the Analysis tool and enable Code Interpreter. You cannot have both enabled.reply",
      "Yes! Forgot to mention that, and it's weird that you can have one but not both enabled. I wrote more about that here: https://simonwillison.net/2025/Sep/9/claude-code-interpreter...reply"
    ],
    "link": "https://www.anthropic.com/news/claude-sonnet-4-5",
    "first_paragraph": ""
  },
  {
    "title": "California governor signs AI transparency bill into law (ca.gov)",
    "points": 202,
    "submitter": "raldi",
    "submit_time": "2025-09-29T20:33:14 1759177994",
    "num_comments": 131,
    "comments_url": "https://news.ycombinator.com/item?id=45418428",
    "comments": [
      "Copied from the end of the page:What the law does:\nSB 53 establishes new requirements for frontier AI developers creating stronger:Transparency: Requires large frontier developers to publicly publish a framework on its website describing how the company has incorporated national standards, international standards, and industry-consensus best practices into its frontier AI framework.Innovation: Establishes a new consortium within the Government Operations Agency to develop a framework for creating a public computing cluster. The consortium, called CalCompute, will advance the development and deployment of artificial intelligence that is safe, ethical, equitable, and sustainable by fostering research and innovation.Safety: Creates a new mechanism for frontier AI companies and the public to report potential critical safety incidents to California\u2019s Office of Emergency Services.Accountability: Protects whistleblowers who disclose significant health and safety risks posed by frontier models, and creates a civil penalty for noncompliance, enforceable by the Attorney General\u2019s office.Responsiveness: Directs the California Department of Technology to annually recommend appropriate updates to the law based on multistakeholder input, technological developments, and international standards.reply",
      "What real-world problem does any of this solve? For instance, how does it protect my IP from being vacuumed up and used by LLMs without permission from or payment to me?reply",
      "The problem it solves is providing any sort of baseline framework for lawmakers and the legal system to even discuss AI and its impacts based on actual data instead of feels. That's why so much of it is about requiring tech companies to publish safety plans, transparency reports and incidents, and why the penalty for noncompliance is only $10,000.A comprehensive AI regulatory action is way too premature at this stage, and do note that California is not the sovereign responsible for U.S. copyright law.reply",
      "> A comprehensive AI regulatory action is way too premature at this stageFunny, I think it is overdue.reply",
      ">and why the penalty for noncompliance is only $10,000.Think they were off by an order of magnitude for this fine. The PR for reporting anything bad on AI is probably worth more than the fine for non-compliance. 100k would at least start to dent the bumper.reply",
      "Hint: It's low because the tech companies are already in agreement with the legislation. This is a huge win compared to a blanket regulatory push.reply",
      "I want to hope so. They'll agree, until the bubble is prone to bursting and then suddenly that fine might be better to eat.reply",
      "There is no bubble. Your priors are not serving you well here.reply",
      "Historically speaking, people saying \"history won't repeat this time, it's different\" have a pretty bad track record. Do we remember what the definition of \"insanity\" is?reply",
      "\u201cThe .com economy is a new business paradigm: the rules have changed.\u201d- pioneers in wrongness 25 years ago. Oft copied, but never vindicated.reply"
    ],
    "link": "https://www.gov.ca.gov/2025/09/29/governor-newsom-signs-sb-53-advancing-californias-world-leading-artificial-intelligence-industry/",
    "first_paragraph": ""
  },
  {
    "title": "Claude Code 2.0 (npmjs.com)",
    "points": 508,
    "submitter": "polyrand",
    "submit_time": "2025-09-29T17:12:13 1759165933",
    "num_comments": 188,
    "comments_url": "https://news.ycombinator.com/item?id=45416228",
    "comments": [
      "Actual Changelog[1]* New native VS Code extension* Fresh coat of paint throughout the whole app* /rewind a conversation to undo code changes* /usage command to see plan limits* Tab to toggle thinking (sticky across sessions)* Ctrl-R to search history* Unshipped claude config command* Hooks: Reduced PostToolUse 'tool_use' ids were found without 'tool_result' blocks errors* SDK: The Claude Code SDK is now the Claude Agent SDK\nAdd subagents dynamically with --agents flag[1] https://github.com/anthropics/claude-code/blob/main/CHANGELO...reply",
      "You can find the revamped prompt on github[1], or on twitter summarized by my bot[2].[1] https://github.com/marckrenn/cc-mvp-prompts/compare/v1.0.128...[2] https://x.com/CCpromptChanges/status/1972709093874757976reply",
      "> IMPORTANT: DO NOT ADD *ANY** COMMENTS unless asked*Interesting.  This was in the old 1.x prompt, removed for 2.0.  But CC would pretty much always add comments in 1.x, something I would never request, and would often have to tell it to stop doing (and it would still do it sometimes even after being told to stop).reply",
      "I can't decide if I like this change or not, tbh. I almost always delete the comments Claude adds, to be sure - but at the same time they seem to provide a sort of utility for me as I read through the generated code. They also act, in a funny way, as a kind of checklist as I review changes - I want them all cleaned up (or maybe edited and left in place) before I PR.reply",
      "I am guessing this is an attempt to save computing resources/tokens?reply",
      "Comments in code are instant technical debt. They need to be maintained alongside the code, so you are *programming\" twice. Avoid comments, except when they really explain some obscure, incomprehensible section of code or to prevent explorers from the future getting smacked on the face twice by the same stick. I find myself using the latter often to tell future agents what not to do in the next few lines.reply",
      "I bet you that if you let it comment code, it would produce better code as it acts like an inline rubber-duck basicallyreply",
      "This is why, when you do have comments, they should generally be in the DocBlock and discuss the code in domain-problem terms.reply",
      "imo, comments are quite good for junior developers. docstrings are much preferred though.reply",
      "Avoiding comments is an exercise in thinking how to rename or refactor a function, or a variable in such a way that a junior developer will be able to read it like prose, and immediately understand what's going on.It's cognitively stressing, but is beneficial for juniors, and developers new to the codebase, just as it is for senior developers to reduce the mental overhead for the reader.It's always good to spend an extra minute thinking how to avoid a comment.Of course there are exceptions, but the mental exercise trying to avoid having that exception is always worth it.Comments are instant technical debt.Especially junior developers will be extremely confused and slowed down by having to read both, the comment, and then the code, which was refactored in the meantime and does the opposite of what the comment said.reply"
    ],
    "link": "https://www.npmjs.com/package/@anthropic-ai/claude-code",
    "first_paragraph": ""
  },
  {
    "title": "Voronoi map generation in Civilization VII (2k.com)",
    "points": 93,
    "submitter": "Areibman",
    "submit_time": "2025-09-26T03:28:05 1758857285",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=45382300",
    "comments": [
      "I've been trying to generate my own maps using Voronoi diagrams as well.  I was using Lloyd's algorithm [0] to make strangely shaped regions \"fit\" better, but I like the insight of generating larger regions to define islands, and then smaller regions on top to define terrain.One of the things I like about algorithms like this is the peculiarities created by the algorithm, and trying to remove that seems to take some of the interesting novelty away.- [0] https://en.m.wikipedia.org/wiki/Lloyd%27s_algorithmreply",
      "Related: https://www.redblobgames.com/x/2022-voronoi-maps-tutorial/reply",
      "One of the best webs for gamedev. The a-star/Dijkstra section is legendary.reply",
      "This kind of exploratory/creative programming is bar none the most fun you can have as a software engineer. I love reading write-ups about projects like this because you can practically feel the nerdy joy radiating off the screen.reply",
      "Haven't played any of the new Civ games but find this very interesting.reply"
    ],
    "link": "https://civilization.2k.com/civ-vii/from-the-devs/map-generation/",
    "first_paragraph": "Hello Civ fans! Ken Pruiksma, Senior Graphics Engineer at Firaxis Games, here to share some behind-the-scenes updates on map generation.When Civilization VII first shipped, the map generation technique we used focused on ensuring balanced play while following some new gameplay rules around things like Deep Ocean and Distant Lands. This technique, which has its roots in previous Civ games, created fair maps, but also made them predictable. This caused the exploration phase of the game to be potentially less rewarding (which is a problem when the first X in 4X is Explore!) We heard from you that maps felt too repetitive, and we agreed.\u00a0Something else we heard loud and clear from the community is that the straight-edged coastlines that could occur between the two major halves was hindering the play experience. Part of the fantasy of Civ is that you're playing an alternative history of Earth, and when the maps don't feel plausibly Earth-like, it detracts from the experience (outside some e"
  },
  {
    "title": "Ask HN: What are you working on? (September 2025)",
    "points": 105,
    "submitter": "david927",
    "submit_time": "2025-09-29T20:58:11 1759179491",
    "num_comments": 302,
    "comments_url": "https://news.ycombinator.com/item?id=45418675",
    "comments": [
      "\"Google maps but for old maps\": https://pastmaps.comThis is a solo startup that I've been working on for 2 years now. It's a labor of love and I'm very lucky and thankful that it's big enough to surprisingly pay all of our bills. Still constantly feeling FOMO over all of my startup buddies working with AI and LLMs while I plug away at old maps and GIS .It gets ~80K MAUs and just slowly and consistently is growing organically through word of mouth through history focused communities. I'm currently playing with expanding the coverage internationally as I still only support the US which is a wickedly fun project.reply",
      "Trying to document my current hobby project, but stuck in the analysis phase. I dont even know what it is. When I try to describe its purpose I get blank looks. People tend to need physical demonstrations to understand whats going on. Its not entirely new, or novel, its definitely not revolutionary, but it is a hybrid of so many things, in a very indirect sense, that its just beyond my verbiage. Not a humble brag, I dont think its amazing or anything. I have just failed to describe it. Have been trying to get a phd I know to look at it, and describe it for me, but he just straight up isnt interested.reply",
      "Currently a one-man side project:https://laboratory.loveLast year PlasticList discovered that 86% of food products they tested contain plastic chemicals\u2014including 100% of baby food tested. The EU just lowered their \"safe\" BPA limit by 20,000x. Meanwhile, the FDA allows levels 100x higher than what Europe considers safe.This seemed like a solvable problem.Laboratory.love lets you crowdfund independent testing of specific products you actually buy. Think Consumer Reports meets Kickstarter, but focused on detecting endocrine disruptors in your yogurt, your kid's snacks, whatever you're curious about.Here's how it works: Find a product (or suggest one), contribute to its testing fund, get detailed lab results when testing completes. If a product doesn't reach its funding goal within 365 days, automatic refund. All results are published openly. Laboratory.love uses the same methodology as PlasticList.org, which found plastic chemicals in everything from prenatal vitamins to ice cream. But instead of researchers choosing what to test, you do.The bigger picture: Companies respond to market pressure. Transparency creates that pressure. When consumers have data, supply chains get cleaner.Technical details: Laboratory.love works with ISO 17025-accredited labs, test three samples from different production lots, detect chemicals down to parts per billion. The testing protocol is public.So far a couple dozen products have received some funding, six products have been fully funded (five product results published, the sixth is at the lab as I write this!)You can browse products, add your own, or just follow specific items you're curious about: https://laboratory.lovereply",
      "I love this idea. I imagine it could be extended to other types of testing - for example, I've always wished there was a way to more readily verify whether the contents of vitamins were as specified on the label.reply",
      "Looking at the tofu reports, I really don't know what to make of them. Is there a way to give more meaning to them for the average person? Also, I'd love to see a sort by \"almost funded\" option.reply",
      "Second this, it would be useful to have a \"EU safe\" label or similar to help me understand if 635.8 DEHP is a good thing or bad.reply",
      "What would be a good strategy to prevent companies from cottoning on to this and gaming the system? They could for example change packaging on production runs for a product that\u2019s undergoing laboratory.love funding campaign.reply",
      "My suspicion is if this was gameable, this would be a solved problem by a number of companies. The truth is there is no single simple or even hard step to take, it\u2019s mostly like numerous steps that multiple actors would need to do.reply",
      "Can you talk a bit about the tech stack?reply",
      "This is great. I thought about a different model even before plasticlist: make a subscription and test various products, but people will have a number of upvotes based on their sub streak. They vote for food to test, and then you show results to everyone subbed. Kind of like what examined does, but they do deep dives into medical topics for subs. I think this model will work better than the one you currently have. Awesome project anyways!It is extremely weird to me that countries don't do that on taxpayers money and show the results publicly, this is what they should do.reply"
    ],
    "link": "item?id=45418675",
    "first_paragraph": ""
  },
  {
    "title": "Loadmo.re: design inspiration for unconventional web (loadmo.re)",
    "points": 265,
    "submitter": "surprisetalk",
    "submit_time": "2025-09-29T15:42:46 1759160566",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=45415207",
    "comments": [
      "Oh it's unconventional alright. When I see a link formatted as https://www.example.com/ and click it, the last thing I expect is for it to vanish and become \"Copy Link | Visit website\". Boggles my mind.reply",
      "All this amazing art and stuff on this site and all you could do was find something nitpicky to complain about? Sure says a lot\u2026reply",
      "The website's raison d'\u00eatre is linking to other websites, and that part doesn't work well. It's not a nitpick if the main thing is broken.reply",
      "It says that UX matters even when doing art.reply",
      "UX matters when doing art = pen, paper, paint.But UX does not matter when experiencing art. OP is entitled to feel nit picky and pissy and a critic because of the normal way they experience the web. It does not mean his critique matters.reply",
      "Exactly this. I hate to say it but this is the main reason I stopped coming to HN. I need some kind of, like, Marvin the Robot filter to make it so I can more easily avoid the tiny thoughts.reply",
      "This is not unconventional-- this is the conventional web I want. I want the internet to be a groovy place again.Rave on ravers.reply",
      "Not sure if I'm with you on this one. I see what you mean but I also have a hard time to understand what's going on on these sites. So in my eyes these are quite exactly what the author called it. Unconventional. However I like the idea of getting these concepts shown and shared just to see what could might work or what causes a lot of attention.reply",
      "reminds me of early Flash days where everyone was so excited about not having to use tables/frames/etc and started making \"organic\" sites instead of structured sites. so usability was neat and interesting, others were outright confusing. weird styles have their place, but people like to follow trends and use weird styles when it is not appropriate. these tend to feel like square pegs in a round hole.reply",
      "It does feel a lot like the Flash era.Girbaud, which was once a cool clothing brand, had an all-Flash site for years. It looked like a 3D stack of cubes, and if you clicked on the cubes, interesting things happened. Short videos and pictures would pop up, audio clips would play, and then they'd fold back into their cubes. Very cute.But it was really hard to order stuff. Just finding out where the items for sale were was tough.Their site today is totally vanilla. They have product pages and \"Add to Cart\" buttons, like everybody else.[1][1] https://www.girbaud.com/enreply"
    ],
    "link": "https://loadmo.re",
    "first_paragraph": "loadmo.re is a mobile websites gallery showcasing the best design inspiration for unconventional web. To keep up with updates, follow us on Instagram.\nRead more\nFrom its earliest days, digital design practice has been focused on creating interfaces for computers. Screen-based interactions are now mainly happening through smartphones and mobile-first experiences have become the norm. However, as digital designers, we still use computers as our main working tool and continue to browse desktop websites when searching for references. This process makes it difficult to acknowledge a shift and embrace the fact that the Internet isn\u2019t happening where it used to.loadmo.re showcases distinctive websites for smartphones. Through this archive, we hope to encourage digital designers to take full advantage of the mobile phone\u2019s interface and functionality. We hope that this platform will generate conversation on mobile-first design within our digital communities.\nRead less\nDVTK \u2014 All rights reserve"
  },
  {
    "title": "How to Create an OS from Scratch (github.com/cfenollosa)",
    "points": 21,
    "submitter": "pykello",
    "submit_time": "2025-09-29T23:32:10 1759188730",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=45420173",
    "comments": [
      "While I agree this might be a fun resource and useful example code for various aspects of legacy x86 interfacing, I would urge anyone who hopes to actually get into OS development to ignore this (and in fact every other tutorial I\u2019ve ever seen, including those hosted on the popular sites).For all the reasons stated in the link from the README [1] and agreed by the author, this project should not be followed if one wants to gain an understanding of the design and implementation of operating systems for modern systems. Following it will likely lead only to another abandoned \u201chello world plus shell\u201d that runs only in emulation of decades old hardware.My advice is get the datasheets and programmers\u2019 manuals (which are largely free) and use those to find ways to implement your own ideas.[1] https://github.com/cfenollosa/os-tutorial/issues/269reply",
      "People interested in a \"read the manual and code it up on real hardware\"-type guide should take a look at Stanford's CS140E[1] repo! Students write a bare metal OS for a Raspberry Pi Zero W (ARMv6) in a series of labs, and we open source each year's code.Disclaimer: I'm on the teaching team[1]https://github.com/dddrrreee/cs140e-25winreply",
      "risc-v seems is a clean-sheet design and that should be a good starting point (imho).fwiw, xv-6, the pedagogical os has migrated to it.reply",
      "I don't even know where to _begin_ writing an operating system.If i wanted to learn just so i have a concept of what an os does, what would you recommend?I'm not trying to write operating systems per se. I'm trying to become a better developer by understanding operating systems.reply",
      "Also see https://wiki.osdev.org/Getting_Startedreply",
      "The author dismisses this as out of date, but this is one of the most straightforward examples I\u2019ve seen. At least reading 00 and 01.reply"
    ],
    "link": "https://github.com/cfenollosa/os-tutorial",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        How to create an OS from scratch\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\u26a0\ufe0f Hey! This is an old, abandoned project, with both technical and design issues listed here. Please have fun with this tutorial but do look for more modern and authoritative sources if you want to learn about OS design. \u26a0\ufe0fHow to create an OS from scratch!I have always wanted to learn how to make an OS from scratch. In college I was taught\nhow to implement advanced features (pagination, semaphores, memory management, etc)\nbut:Inspired by this document\nand the OSDev wiki, I'll try to make short step-by-step READMEs and\ncode samples for anybody to follow. Honestly, this tutorial is basically the first document but\nsplit into smaller pieces and without the theory.Updated: more sour"
  },
  {
    "title": "Instant Checkout and the Agentic Commerce Protocol (openai.com)",
    "points": 148,
    "submitter": "meetpateltech",
    "submit_time": "2025-09-29T17:00:42 1759165242",
    "num_comments": 260,
    "comments_url": "https://news.ycombinator.com/item?id=45416080",
    "comments": [
      "This is a new amazon marketplace with trusted reviews. IMO having worked in commerce for a while - this opens up new avenues for merchants to work on gaming reccomendations and the trust quotient delivered.It also opens up\n- ads\n- subscriptions\n- transactionsbased revenue lines for OpenAIreply",
      "> Merchants pay a small fee on completed purchases, but the service is free for usersWhere's the money coming from to pay the fee? From my wallet, of course! At scale, this raises the price of purchases.Another thought -- the text-only interface of ChatGPT is way too limiting. We need images (and video) generated on demand showcasing product suggestions. Showing an image of the user wearing some new shades is going to be pretty compelling...reply",
      "Not really, this is just effectively a new supply side platform for advertisers. Retail companies will just allocate a percentage of their marketing budget to OAI instead of Google/Metareply",
      "This was bound to happen, but they will also likely see a much higher chargeback rate across the board if users are surprised when random comments or agent actions place orders or that orders placed too easily need to be reversed.   Because consumers have less disposable income with all the AI-enabled layoffs, the bigger bonanza will come if OpenAI creates educational pathways via AI to enable more people to make money with AI. Entrepreneurial guides that take you step by step, accounting and other hurdles that AI can walk you through as you grow, brainstorming and exploring new business ideas, training people for a new trade or career path as employees. That will be the true game-changer that beats AGI. Because when you can give the entire society an easy ladder towards the industries that need them most, you will have a society that makes money off of AI to spend via AI shopping experience and gains purpose.reply",
      "> Because consumers have less disposable income with all the AI-enabled layoffs, the bigger bonanza will come if OpenAI creates educational pathways via AI to enable more people to make money with AI.Who do you imagine will be throwing money at all these side-hustle \"make money with AI\" business you envision? No doubt there will be a few- there already are a few- but as the market gets increasingly flooded with AI slop enterprise with very little value add, that well is going to dry up quick.It's not different than all those content creators making videos offering to teach you the secrets to easy money... instead of just making all that easy money themselves.reply",
      "digression:> ... when random comments or agent actions place ordersI believe that the purchase action must be triggered by a user manually in a specific \"Pay <vendor>\" UI element? It doesn't seem to me like any prompt could trigger a purchase directly. Of course this could change, but likely won't for the reasons you said.reply",
      "I realize a lot of the comments here are pessimistic, but this is a pretty obvious monetization path that they just can't not take. This is actually a huge angle IMO. ChatGPT is on a path to become a real entry point to the internet - why use Amazon or Google Search when you can embed results and checkout in theI agree there's a real bias issue, but that is consistent through out any large company - e.g., Google, Facebook, Amazon, etc have sponsored resultsreply",
      "I don\u2019t quite follow this logic. Like there\u2019s too much money involved to not dilute the value of a nascent technology?If you\u2019re willing to torch your credibility as a company, that tends to open up quite a few shorter term business options. The real trick is ensuring enough customer or user lock in that they can\u2019t go anywhere else even when the enshittification is obvious to everyone.The irony here is that ChatGPT could be a credible threat to Google search\u2019s dominance as the entry point to the internet partly because the quality of Google\u2019s search results has degraded so much. For some queries sponsored links push the real results below the fold on mobile, they\u2019ve allowed some content aggregators to take over certain types of results (Pinterests polluting image results with irrelevant content). But that doesn\u2019t matter while you make gobs of money. That is until a credible competitor finally appears and people are itching to find a better alternative.reply",
      "They are already groveling to Nvidia for cash. It\u2019s very likely the options are not \u201cno ads\u201d vs \u201cads\u201d. The options for them are \u201cno free tier\u201d and \u201cfree tier with ads\u201d.reply",
      "> torch your credibility as a companyThat's only so in our little cynical skeptical contrarian hacker bubble. For most people, it's an appreciated convenience.reply"
    ],
    "link": "https://openai.com/index/buy-it-in-chatgpt/",
    "first_paragraph": ""
  },
  {
    "title": "John Jumper: AI is revolutionizing scientific discovery [video] (youtube.com)",
    "points": 64,
    "submitter": "sandslash",
    "submit_time": "2025-09-29T15:20:11 1759159211",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=45414933",
    "comments": [
      "Something else to add is mathematical discovery. There is a team that is very close to solving the Navier-Stokes Millenium Prize problem: https://deepmind.google/discover/blog/discovering-new-soluti...The cynists will comment that I've just been sucked in by the PR. However, I know this team and have been using these techniques for other problems. I know they are so close to a computationally-assisted proof of counterexample that it is virtually inevitable at this point. If they don't do it, I'm pretty sure I could take a handful of people and a few years and do it myself. Mostly a lot of interval arithmetic with a final application of Schauder that remains; tedious and time-consuming, but not overly challenging compared to the parts already done.reply",
      "This is not just PR and is very interesting. However, in my view, (and from a quick read of the paper) this is actually a very classical method in applied math work:- Build a complex intractable mathematical model (here, Navier-Stokes)- Approximate it with a function approximator (here, a Physics Informed Neural Network)- Use the some property of function approximator to search for more solutions to the original model (here, using Gauss-Newton)In a sense, this is actually just the process of model-based science anyway: use a model for the physical world and exploit the mathematics of the model for real-world effects.This is very very good work, but this heritage goes back to polynomial approximation even from Taylor series, and has been the foundation of engineering for literal centuries. Throughout history, the approximator keeps getting better and better and hungrier and hungrier for data (Taylor series, Chebyshev + other orthogonal bases for polynomials, neural networks, RNNs, LSTMs, PINNs, <the future>).You didn't say anything to the contrary, and neither did the original video, but it's very different than what some other people are talking about in this thread (\"run an LLM in a loop to do science the way a person does it\"). Maybe I'm just ranting at the overloading of the term AI to mean \"anything on a GPU\".reply",
      "This is absolutely true, but it still makes use of the advantages and biases of neural networks in a clever way. It has to, because computationally-assisted proofs for PDEs with singularities is incredibly difficult. To me, this is not too similar from using them as heuristics to find counterexamples, or other approaches where the implicit biases pay off. I think we do ourselves a disservice to say that \"LLMs replacing people\" = \"applications of AI in science\".I also wouldn't say this is entirely \"classical\". Old, yes, but still unfamiliar and controversial to a surprising number of people. But I get your point :-).reply",
      "> I know they are so close to a computationally-assisted proof of counterexample that it is virtually inevitable at this point.That's a strong claim. Is it based on more than the linked work on some model problems from fluid mechanics?I will say that I dread the discourse if it works out, since I don't believe enough people will understand that using a PINN to get new solutions of differential equations has substantially no similarity to asking ChatGPT (or AlphaProof etc) for a proof of a conjecture. And there'll be a lot of people trying to hide the difference.reply",
      "It's based on knowledge of the related estimates, applying similar techniques to geometric problems, knowledge of all the prior works that lead to the current work, and speaking with members of the team themselves. They are much further along than it appears at first glance. All of the major bottlenecks have fallen; the only concern was whether double precision accuracy is good enough. The team seems to have estimates that are strong enough for this, but obviously keep them close to their chest.PINNs are different in concept, yes, but clearly no less important, so the additional attention will be appreciated. Asking LLMs for proofs is a different vein of research, often involving Lean. It is much further behind, but still making ground.reply",
      "> PINNs are different in concept, yes, but clearly no less importantIf anything I think they're more important! Whether or not it works out for Navier-Stokes, this kind of thing is an extremely plausible avenue of approach and could yield interesting singularities for other major equations. I am however extremely concerned about public understanding. I know you are well aware that this is worlds away from the speculative technologies like 'mathematical superintelligence' but, if it works out, it'll be like a nuclear bomb of misinformation about AI and math.reply",
      "I think the PR is making it seem that Deepmind is not standing on the shoulder of giants, when in fact it very much is. The paper itself makes this clear. I wish them luck!reply",
      "> The cynists will comment that I've just been sucked in by the PRYou can just ignore them. I see a lot of science-literate folks try to meet the anti-science folks as if they're on equal footing and it's almost always a waste of time. Imagine if every time you talked about biology you had to try to address the young earth creationists in the room and try to pre-rebut their concerns.reply",
      "I'll share something as a former solar researcher.Scientific progress is heavily influenced by how many bodies you can throw at a problem.The more experiments you can run, with more variety and angles the more data you can get, the higher the likelihood of a breakthrough.Several huge scientist are famous not because they are geniuses, but because they are great fundraisers and can have 20/30/50 bodies to throw at problems every year.This is true in virtually any experimental field.If LLMs can be de facto another body then scientific progress is going to sky rocket.Robots also tend to be more precise than humans and could possibly lead to better replication.But given that LLMs cannot interact with the real world I don't see that happening anytime soon.reply",
      "> But given that LLMs cannot interact with the real worldWhat type of interaction do you envision? Could a non-domain-expert, but somewhat trained person provide a bridge? If the LLM comes up with the big ideas and tells a human technical assistant to execute (put the vial here, run the 3D printer with this file, put the object there, drive in a screw), would that help? But dexterous robots are getting more and more advanced, see CoRL demos right now.reply"
    ],
    "link": "https://www.youtube.com/watch?v=2Yguz5U-Nic",
    "first_paragraph": ""
  },
  {
    "title": "Jax: Fast Combinations Calculation (github.com/phoenicyan)",
    "points": 35,
    "submitter": "phoenicyan",
    "submit_time": "2025-09-29T21:17:16 1759180636",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=45418875",
    "comments": [
      "Any idea if the jax-metal package is under active development anymore?reply",
      "This stuff is great; I used this code in one of my projects. Very fast!reply",
      "fun and interesting calculator!reply"
    ],
    "link": "https://github.com/phoenicyan/combinadics",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        fast combinations calculation in jax\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.A fast combinations calculation in jax.Idea of combinadic implementation is from\nhttps://jamesmccaffrey.wordpress.com/2022/06/28/generating-the-mth-lexicographical-element-of-a-combination-using-the-combinadic\nand some useful information can be found here: https://en.wikipedia.org/wiki/Combinatorial_number_system. Below I copied and aggregated some of the details.The following code demostrates the combinations calculation in numpy and via combinadics:And the output from execution of the code is:You can think of a combinadic as an alternate representation of an integer. Consider the integer $859$. It can be represented as the sum of powers of $10$ as$$\n859 = 8 \\times 10^2 + 5"
  },
  {
    "title": "Spending Time with the Material (robinsloan.com)",
    "points": 6,
    "submitter": "thomasjb",
    "submit_time": "2025-09-26T15:43:50 1758901430",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.robinsloan.com/lab/actually-readable/",
    "first_paragraph": "This is a post from Robin Sloan\u2019s lab blog & notebook. You can visit the blog\u2019s homepage, or learn more about me.A wild find at a comic book store yesterday: The Making of Orig\u00adinal Dun\u00adgeons & Dragons, a doorstop volume, devoted almost entirely to repro\u00adduc\u00adtions of early ver\u00adsions of the game, from type\u00adwritten drafts to pub\u00adlished booklets. This isn\u2019t just a breezy review, but a pre\u00adsen\u00adta\u00adtion of deep archival mate\u00adrial. I\u00a0pored over the book for hours, enjoying the edits in pencil, the 1970s paste-up design. I\u00a0read out of order, flipped back and forth, skimmed and scanned, jotted notes on my phone.It occurred to me, deep into a really won\u00adderful experience, of reading and thinking and feeling and pondering, that if Wiz\u00adards of the Coast had pub\u00adlished exactly the same mate\u00adrial online\u200a\u2014\u200aand you can imagine this easily: you can imagine the web\u00adsite, as slick as one of the Google Arts & Culture sites, or the dig\u00adital book from the Steve Jobs Archive\u200a\u2014\u200aI\u00a0would have clicked over; said"
  },
  {
    "title": "A SQL Heuristic: ORs Are Expensive (ethanseal.com)",
    "points": 63,
    "submitter": "ethanseal",
    "submit_time": "2025-09-29T13:29:55 1759152595",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=45413525",
    "comments": [
      "We had a case where a single OR was a massive performance problem on MSSQL, but not at all on Sybase SQLAnywhere we're migrating away from. Which one might consider slightly ironic given the origins of MSSQL...Anyway, the solution was to manually rewrite the query as a UNION ALL of the two cases which was fast on both. I'm still annoyed though by the fact that MSSQL couldn't just have done that for me.reply",
      "If optimization is really as simple as applying De Morgan's laws, surely it could be done within the query planner if that really is the main optimization switch? Or am I misreading this somehow?Edit: I guess the main difference is that it's just calculating separate sets and then merging them, which isn't really DeMorgan's, but a calculation approach.reply",
      "In most cases, the end result is a lot messier than it looks in this minimal example. Consider a case where the query is a 12-way join with a large IN list somewhere (which is essentially an OR); would it still look as attractive to duplicate that 12-way join a thousand times?There _are_ optimizers that try to represent this kind of AND/OR expression as sets of distinct ranges in order to at least be able to consider whether a thousand different index scans are worth it or not, with rather limited success (it doesn't integrate all that well when you get more than one table, and getting cardinalities for that many index scans can be pretty expensive).reply",
      "The core issue the article is pointing to is that most database indexes are B-trees, so if you have a predicate on the form (col_a = 'foo' OR col_b = 'foo'), then it is impossible to use a single B-tree lookup to find all rows that match the predicate. You'd have to do two lookups and then merge the sets. Some query optimizers can do that, or at least things that are similar in spirit (e.g. Postgres bitmap index scan), but it's much more expensive than a regular index lookup.reply",
      "> but it's much more expensive than a regular index lookup.It doesn't have to be, it just \"is\" in some database engines for various historical reasons.I.e.: PostgreSQL 18 is the first version to support \"B-tree Skip Scan\" operators:  https://neon.com/postgresql/postgresql-18/skip-scan-btreeOther database engines are capable of this kind of thing to various degrees.reply",
      "I'm not seeing De Morgan. I am seeing inclusion exclusion, which is just a neat trick all round. Highly recommend remembering it.I imagine negative filters to be a bit inefficient as well, though maybe not for a simple count.reply",
      "The query optimizer knows how many items are in each index, but has no advance idea how many items will be in the result of a JOIN. An \"a OR b\" query on a table with millions of rows might have three hits on A, or millions of hits.\nThe optimal query strategy for the two cases is very different.Has anyone put machine learning in an SQL query optimizer yet?reply",
      "> Has anyone put machine learning in an SQL query optimizer yet?Yes, I think everyone has?  At very least I know that MSSQL has because we semi regularly run into problems with it :).MSSQL keeps track of query statistics and uses those in future planning.  SOMETIMES it just so happens that the optimization for the general case makes the outlier 100x slower which kills general performance.reply",
      "At 100x, it seems like you could run both optimal strategies every time, let them race, and still come out way ahead.reply",
      "Some traditional planners try some different plans on random subsets of the data to see which plan works best.  Don't need machine learning, just Bayes' rule.reply"
    ],
    "link": "https://ethanseal.com/articles/ors-are-expensive",
    "first_paragraph": "Query planning is hard. Sometimes.Queries often have more than one filter (using an and clause).But the developer can often end up in situations requiring an or clause:select count(*)\nfrom application\nwhere submitter_id = :user_id\nor reviewer_id = :user_id;\nBut this is slow! With 1,000,000 applications and 1000 users uniformly distributed on both columns, it takes over 100ms.1If we rewrite it with only andsselect (\n  select count(*)\n  from application a\n  where a.reviewer_id = :user_id\n) + (\n  select count(*)\n  from application a\n  where a.submitter_id = :user_id\n) - (\n  select count(*)\n  from application a\n  where a.submitter_id = :user_id\n  and a.reviewer_id = :user_id\n);\nThis takes less than 1ms; Over 100 times faster! 12This is surprising \u2014 we have indexes on the filtered columns.But why? First, we need to develop some intuition about how and when indexes are used.Basic Query Planning IntuitionSay you have a table with two columns and an index on each column.create table example_ta"
  },
  {
    "title": "Managing Dotfiles with Make (matheusmoreira.com)",
    "points": 14,
    "submitter": "ibobev",
    "submit_time": "2025-09-26T06:15:26 1758867326",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=45383284",
    "comments": [
      "I do this and like it: https://github.com/staticshock/dotfiles/blob/main/MakefileI also tend to put a Makefile into the root of any repo I work in (ignored via .git/info/exclude), so that shell commands relevant to my workflow can accumulate under that Makefile. I've been doing this for at least 10 years and love it. Most repos have some sort of a cli interface, but that doesn't mean that they're any good. Make is nice way to wrap all of them, and to standardize on at least the build/test/run commands across many disparate repos.Here's an example of one of those from a long-abandoned project: https://gist.github.com/staticshock/0e88a3232038d14a2817e105...reply",
      "I've been using Chezmoi for a little while now.  It simplifies what I want to do better than anything else I have tried.reply",
      "I use a combination of stow and make to manage my dotfiles. I added a makefile well after using stow for a decade. The makefile is more for new system setup than day to day management. I might try out replacing stow with make based on this blog, more for fun than anything. I'm a bit reluctant to replace what has been working so well for a decade, but I'm very intrigued by this. Make has always interested me. It seems like it could be incredibly powerful in the right hands.reply",
      "This is very clever, but I'd have to relearn Make's subtleties each time I tried to debug it or add a feature (count me in the \"prefer to avoid it\" crowd).I ended up writing a Go CLI to symlink my dotfiles. It's probably 10x more lines of code, but it uses a \"plan, then execute\" pattern, similar to Terraform that I find easier to test and clearer to use too. And it's a single binary so it's easy to install on Windows too.reply",
      "It\u2019s always fun to see what other people do for dotfile mgmt.I have a simple bash script that does something similar: https://erock-git-dotfiles.pgs.sh/tree/main/item/dotfiles.sh...reply",
      "Been using yadm for a long time and i love it. super simple and has the \"bootstrap\" concept as an escape hatch for when i want to get weeeirdreply"
    ],
    "link": "https://www.matheusmoreira.com/articles/managing-dotfiles-with-make",
    "first_paragraph": "\n          Make is an old tool, an assembly language of sorts for build systems.\n          Many have tried to replace it. Many have tried to reinvent it. Most\n          people prefer to avoid it if at all possible. So why use it to manage\n          dotfiles of all things?\n        \n          There's at least one good reason to do this: make is ubiquitous.\n          Pretty much every machine that has ever compiled software will have a\n          copy of this thing. Using make as a dotfile management tool eliminates\n          the need to install yet another infrequently used program.\n        \n          Another reason to use it is this turned out to be a surprisingly easy\n          task.\n        \n          Make works best when everything is as simple as possible. It doesn't\n          provide much functionality: the few path manipulation functions it\n          includes are of the string matching and substitution variety.\n        \n          The easiest way to achieve that simplicity is to mir"
  },
  {
    "title": "Code golfing a tiny demo using maths and a pinch of insanity (pkh.me)",
    "points": 16,
    "submitter": "ux",
    "submit_time": "2025-09-29T13:34:58 1759152898",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=45413588",
    "comments": [
      "I don't know much about 3d stuff or shaders or this language, but I know golf, and I think I found a byte to save.    w = g-g*exp(-mix(h*3e2,d*2.5,a=h<.001?0.:1.))\n    w = g-g/exp(mix(h*3e2,d*2.5,a=h<.001?0.:1.))reply",
      "Hey, thank you, that's a nice one. I added the change to the Shadertoy version with your credit. I will add a note/update on the blog and demo page later. Thanks again :)reply",
      "Is it actually necessary to specify `a` as a floating-point literal? Will it not be implicitly converted?reply",
      "Yeah. If you want implicit conversion, you need to specify a constructor like float(cond), but that would be longer than the ternary form. I thought about using a vec3() but vec3(...).x is still too long and using the vec3 directly doesn't look viable. Declaring a bool adds more characters as well. There is also step() giving the same number of chars: a=step(.001,h), but it's not more optimal: https://iquilezles.org/articles/gpuconditionals/If you see a way to make it shorter, feel free to share :)reply",
      "The top demo doesn't function on Edge or Chrome on Windows due to too many webgl contexts.> WARNING: Too many active WebGL contexts. Oldest context will be lost.reply",
      "Ah, someone reported this to me today, but I must admit I have no idea how to address the issue. Currently the canvas animations are stopped when out of context, but yeah they have to be loaded.The code on the blog is pretty simple and naive (I'm not a webdev): https://github.com/ubitux/scripts/blob/main/share/blog/shade...Any suggestion on how to address the issue is welcome.Note: I don't have any Windows machine to test withreply",
      "Similar issue on iOS Safari: all examples work except for the very first one.reply"
    ],
    "link": "https://blog.pkh.me/p/45-code-golfing-a-tiny-demo-using-maths-and-a-pinch-of-insanity.html",
    "first_paragraph": "\u2190 indexA few weeks ago, I made a tiny demo that fits into 464 characters:There is no texture, no mesh, no 3D helper: it's simply a procedural\nmathematical formula evaluated at each pixel assigning them a color. Code\ngolfing is about making it as short as possible, and thus is part of the art\nperformance.To put things into perspective, the 853x480 JPEG thumbnail of this article is\n162x larger than this code.You can watch a larger version on its main dedicated page, or a\nportage on Shadertoy (508 chars). If your device is not powerful\nenough (I'm sorry for the lag on this page) or doesn't support WebGL2,\na short preview video (of a slightly outdated variant) can be seen on\nMastodon.I'm guessing the wizardry of the code has confused many people so we're going\nto dive through the making-of together. Overall, this demo is a particularly\ndense and entangled compilation of different techniques, where each aspect could\nmandate a dedicated article. For that reason, some parts will prefer to lin"
  },
  {
    "title": "`std::flip` (morwenn.github.io)",
    "points": 90,
    "submitter": "ashvardanian",
    "submit_time": "2025-09-26T06:29:53 1758868193",
    "num_comments": 77,
    "comments_url": "https://news.ycombinator.com/item?id=45383381",
    "comments": [
      "Very soon after reading this I started to doubt my whole life's work in C++... std::flip? Never heard of it before, and I used to read cppreference for fun. And the name smells sus. I don't think the committee would name it \"flip\" if it did exist. Too short a name for something too niche. I looked at cppreference and saw that yep, this does not exist, so thought maybe this article was some kind of AI hallucination. Finally the author said it was all made up and my mind was at peace once more.reply",
      "> The LISP family of languages generally does not provide such a function by defaultThe Lisp family doesn't provide anything but parentheses, maybe lambda (not necessarily under that name) and a whole lot of heated arguments.Only specific dialects of specific Lisp-family languages provide, or don't provide this and that:TXR Lisp:  1> (mapcar [flipargs -] '(0 1 2) '(10 20 30))\n  (10 19 28)\n\nflip is called flipargs because flip is the name of an operator that mutates a place with a negation of its current value: i.e (flip x) means (set x (not x)) with x evaluated once.reply",
      "C++ is surprisingly close to being a usable functional language.The two missing pieces are -* structural pattern matching* uniform function call syntax that is : a.foo(b) vs foo(a, b) being interchangeable.With the kitchen sink approach of design I\u2019d not be surprised if these get into the language eventually. These ideas have been proposed a few times but haven\u2019t been seriously considered as far as I know.reply",
      "> * uniform function call syntax that is : a.foo(b) vs foo(a, b) being interchangeable.Herb Sutter has proposed this: https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p30... (twice, even, there was an older version of the paper several years ago that didn't pass).I'm resolutely opposed to such a thing because, having had to actually wade through C++'s name lookup and overload resolution rules in the past, they're a dense, confusing morass of logic that you can only stand to stare at for a half-hour or so before your brain turns to mush and you need to do something else, and anything that adds to that complexity--especially in \"this makes things two things nearly equivalent\"--is just a bad idea.(For an example of C++ overload resolution insanity, consider this:)    // Given these overloads...\n    void f(std::float32_t);\n    void f(double);\n    // Which one does this line call? Assume float/double are standard IEEE-754 types.\n    void f((float)1);reply",
      "I don't want implicit conversion to float32_t. The type was introduced because of how annoying implicit conversions are.reply",
      "I'm going to guess f(double) because floats have always been promoted to doubles for function calls since K&R. But I'm not sure by any means. I'd be ready to get more explicit if I needed some specific behaviour.reply",
      "By mentioning K&R you seem to imply that C also promotes floats to doubles in function calls? But that is not the case, floats are passed as floats, as you'd expect.You can try it yourself on godbolt all the way back to GCC 3, test(float x) has always emitted movss and test(double x) will result in movsd/movlpd.reply",
      "Unless you're calling a variable-argument function--floats are promoted to doubles for variable-argument functions:> The arguments are implicitly converted, as if by assignment, to the types of the corresponding parameters, taking the type of each parameter to be the unqualified version of its declared type. The ellipsis notation in a function prototype declarator causes argument type conversion to stop after the last declared parameter, if present. The integer promotions are performed on each trailing argument, and trailing arguments that have type float are promoted to double. These are called the default argument promotions. No other conversions are performed implicitlyreply",
      "It's a difference of whether the function arguments are declared or not. If you declare a `void foo()`, and then call `foo((float)f)`, the `foo()` function is actually passed a `double` as the first argument rather than a `float`. If you instead change the declaration to `void foo(float)` then it gets passed as a `float`.Ex: https://godbolt.org/z/TKjz3Tqqrreply",
      "A misconception on my part, apologies. I had somehow conflated the automatic conversion from float to double that occurs in some circumstances to something that occurs in all circumstances. Or perhaps taken a quirk from a 1980s compiler (Turbo C maybe?) to be standard behaviour. Or something like that.reply"
    ],
    "link": "https://morwenn.github.io//c++/2025/09/25/TSB004-std-flip.html",
    "first_paragraph": "\nSep 25, 2025\n      std::flip is a little-known utility from the C++ standard library header <functional>:\nit is a higher-order function that accepts a Callable and returns an equivalent Callable with the order of its parameters reversed (or \u201cflipped\u201d).To understand how it can be useful, let\u2019s start with a simple example. Consider the following tree node class:Let\u2019s write a function that takes two nodes, and returns whether the first node is a parent of the second one, at any level of ancestry:This is basically walking up the tree from the child node as if it were a linked list.\nThe reverse operation either implies walking through two children nodes, or simply flipping the order of parameters, which is where std::flip intervenes:std::flip finds its roots in functional programming, a domain in which it is extremely prevalent:The LISP family of languages generally does not provide such a function by default, but the need is common enough that I have seen programmers online wonder how oft"
  },
  {
    "title": "Diagnosing a Linux Performance Regression (automattic.com)",
    "points": 32,
    "submitter": "program",
    "submit_time": "2025-09-29T18:46:15 1759171575",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=45417300",
    "comments": [
      "I wonder if this is still a problem if nftables was used instead.nftables has been in mainline linux since 3.13 over a decade ago, and has been in the default in distros for a while (Debian 10 had it as default 6 years ago), but K8s support for it has lagged far behind, with it just recently out of beta in 1.33, and still not the default.reply",
      "(January 2025)Not something recent in the kernel.reply",
      "I suspect using more than two year old (or even older) kernel versions in production is pretty common.Anyone have stats on this?reply"
    ],
    "link": "https://automattic.com/2024/03/14/systems-report-linux-performance-regression/",
    "first_paragraph": "From time to time, our systems engineers write up a case study detailing a notable moment on the infrastructure front lines. This month\u2019s comes from Ale Crismani and Joshua Coughlan,\u00a0systems wranglers who work on WordPress VIP.\u00a0At Automattic, we use Kubernetes to orchestrate the infrastructure running WordPress VIP applications. We have firewall rules that ensure an application cannot connect to resources that are dedicated to other applications, and we monitor those firewall rules in real time.During routine maintenance of our servers, we noticed that our firewall monitoring had started failing. Our ensuing investigation uncovered a regression in the Linux kernel ipset module that resulted in some operations running up to 1,000 times slower. Read on to learn how we went from failure to fix.As mentioned in the introduction, we have monitoring on our Kubernetes hosts to ensure that they conform to our security policies.One of our monitoring scripts checks if the host has the correct IPs"
  },
  {
    "title": "Subtleties of SQLite Indexes (emschwartz.me)",
    "points": 84,
    "submitter": "emschwartz",
    "submit_time": "2025-09-29T15:54:42 1759161282",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=45415332",
    "comments": [
      "Not a great article; I clicked expecting something super technical about SQLite internals and found a mix of rdbms basics and some misconceptions. The limitations in the blog post aren't really specific to SQLite (for the most part), they're just how indexes (indices) and database engines work across the board. And some of the things phrased as \"SQLite [can't] do this\" is stuff that wouldn't make sense to do in the first place.If you understand what (multi-column) indexes are at the lowest level (i.e. what data structure they represent, how they are used by the database, what the code reading them would look like) then all of this makes immediate and natural sense. Indexes aren't magic. They're just a shortcut to help the the db engine find your data more effectively.This doesn't require super complex understanding of data structures, btw. The same limitations and advice would apply if you were trying to, for example, search a stack of resumes sorted by one or more attributes. If you have them sorted by position, years of experience, and applicant's last name; you wouldn't be able to quickly grab all resumes for the HR Manager position that have a last name starting with J - after all, you sorted them by years of experience first! It's physically not possible! You can only use the first part of that index, the position, to jump to HR Manager resumes; then you would need to manually go through them one-by-one to grab only the ones starting with J for each years-of-experience subgroup (if any).reply",
      "> You can only use the first part of that index, the position, to jump to HR Manager resumes; then you would need to manually go through them one-by-one to grab only the ones starting with J for each years-of-experience subgroup (if any).You're phrasing that like this situation \u2014 relying on an index formed by (key 1 that you know, key 2 that you don't know, key 3 that you want to depend on) \u2014 necessarily implies a sequential walk of everything within the HR manager subgroup.But it doesn't; within each key-1 partition of the index, you can binary-search to find the boundaries of the key-2 partitions; and within those, you can binary-search to find the all (or, more commonly, the first) last_name[0] = \"J\" entry/entries. It's actually not overly slow (i.e. is often still a win over a naive index-partial seq scan) if the cardinality ratio of of key-2 : key-3 isn't too extreme (i.e. if you're saving useful amounts of time by eliminating enough key-3 entries from consideration per key-2 partition.)(We use this approach quite a bit at $WORK \u2014 MySQL and Postgres people like to call this a https://wiki.postgresql.org/wiki/Loose_indexscan. [See the \"Making use of a non-leading column of an index\" section.])reply",
      "The fact that it stops at the first range isn't intuitive to me at all, and I've been using sqlite for 20 years now.Given a covering index on (thing_date, thing_color)I would think a scan would not be needed for the query:select thing_date, thing_color where thing_date < date and thing_color = 'blue'I also can't think of a reason this is the case given the underlying data structures.reply",
      "It's a flattened tree, and you've used the index to reach a point where you have multiple child nodes that meet the precondition thing_date < 17. Some, but not all, of these have thing_color = blue, but the important part is that they're non-consecutive (the index can only give you one point-of-entry, so the data has to be consecutive or you need to do some amount of scan).I asked an llm to give me an ascii representation so it'll be easier to see what I mean; consider the case where you want thing_date < 17 and thing_color = blue, the results that match are marked with an asterisk, but note that there's no way to get to them (ALL of them) directly:           Root (internal)\n           +---------------------+\n           | keys: 14     17     |\n           +----+--------+--------+\n                |        |        |\n                v        v        v\n   +----------------+ +----------------+ +----------------+\n   | Leaf P0        | | Leaf P1        | | Leaf P2        |\n   | (dates 10-13)  | | (dates 14-16)  | | (dates 17-20)  |\n   |----------------| |----------------| |----------------|\n   | 10 | red   |101 | | 14 | blue  |141| | 17 | green |171|\n   | 11 | blue  |111 |*| 14 | red   |142| | 18 | blue  |181|\n   | 12 | green |121 | | 15 | yellow|151| | 19 | red   |191|\n   | 13 | red   |131 | | 16 | blue  |161|*| 20 | green |201|\n   +----------------+ +----------------+ +----------------+\n\nGo back to my resume's example: you have the resumes sorted by application timestamp (chronologically) and position (alphabetically). You want to find all resumes received last Tuesday where position was \"Senior Architect\". You literally can't skip to the point where the next n consecutive results will be the results you seek, because you can't temporarily re-order them and within the subset of resumes where date > monday and date < wednesday, there may be multiple for \"Senior Architect\" but they didn't arrive one-after-the-other so they're interspersed with results you don't want. The correct to do here would be (if this is always the shape of the query you expect to get) to sort the resumes by (position, timestamp) instead, at which point you absolutely CAN jump to the position that guarantees the next results are (all the results) of the query (\"Senior Architect\", last Tuesday).One important thing to keep in mind is that a SCAN is not the end of the world. You know your data and you know when it's ok (you expect most of the results to match and a few to be filtered out, in which case you're really not saving much time anyway) and when it's not (the scan will cover a huge range that contains only a few, widely interspersed results and you have no way of terminating early).EDIT: In response to your exact question however, note that with a covering index you might not end up with a SCAN (i.e. you don't hit the table) even though it uses only the partial index (thing_date) and not (thing_date, thing_color)! It's still possible for it to avoid hitting the backing table and might return the results directly from the index - something it wouldn't have been able to do if the index was only (thing_date).reply",
      "The key thing is \"an index is a flattened tree\", and for all us devs who haven't thought about trees in many years or younger folks who might not yet know, that means it's conceptually a bunch of nested maps/objects/dictionaries where the keys at each \"level\" are the columns in that same \"level\" of the index. To use a little bit of python, here's a list of the raw maps in your ascii art DB:    [\n        {\"date\": 10, \"color\": \"red\",    \"id\": 101},\n        {\"date\": 11, \"color\": \"blue\",   \"id\": 111},\n        {\"date\": 12, \"color\": \"green\",  \"id\": 121},\n        {\"date\": 13, \"color\": \"red\",    \"id\": 131},\n        {\"date\": 14, \"color\": \"blue\",   \"id\": 141},\n        {\"date\": 14, \"color\": \"red\",    \"id\": 142},\n        {\"date\": 15, \"color\": \"yellow\", \"id\": 151},\n        {\"date\": 16, \"color\": \"blue\" ,  \"id\": 161},\n        {\"date\": 17, \"color\": \"green\",  \"id\": 171},\n        {\"date\": 18, \"color\": \"blue\",   \"id\": 181},\n        {\"date\": 19, \"color\": \"red\",    \"id\": 191},\n        {\"date\": 20, \"color\": \"green\",  \"id\": 201},\n    ]\n\nAnd here's an example of how we'd represent an \"index\" as a set of nested maps, where that index is (date, color):    {10: {'red':   [101]},\n     11: {'blue':  [111]},\n     12: {'green': [121]},\n     13: {'red':   [131]},\n     14: {'blue':  [141],\n          'red':   [142]},\n     15: {'yellow':[151]},\n     16: {'blue':  [161]},\n     17: {'green': [171]},\n     18: {'blue':  [181]},\n     19: {'red':   [191]},\n     20: {'green': [201]}}\n\nNotice that since this index is built from nested maps, if we want to use this index to find things, we HAVE to first do it by checking the keys in the \"outermost\" map, the 'date' column. It's a compound index but its order matters. This 'order matters' property is true in our map-based index and it's also true in our SQLite based index. It's also true that because this 'date < 17' is a range criteria, it has to check individual keys in the outermost map, which constitutes a SCAN of the index (smaller than a scan of the DB, but a scan nonetheless). To then find everything matching 'color = blue', it has to individually check all the color values in the result of the prior date SCAN in order to get down to only those with a 'color = blue'. As you can see, this index of date -> color isn't super helpful for the query 'WHERE color = blue AND date < 17' query. A query this index would be good for is a query like 'WHERE color = red AND date = 14'. That would not require any scans; if we were writing application code such a query with this index would be like calling `index[14][red]` which as we all know is super fast.A better index for this, which is a different index, comes from swapping the order of the columns when forming the index. Instead of (date, color), this better index would be (color, date). That index looks like this:    {'blue': {11: [111],\n              14: [141],\n              16: [161],\n              18: [181]},\n    'green': {12: [121],\n              17: [171],\n              20: [201]},\n    'red': {10: [101],\n            13: [131],\n            14: [142],\n            19: [191]},\n    'yellow': {15: [151]}}\n\nNow with this new index to fulfill the exact same query of 'WHERE color = blue AND date < 17', we can do a `index[blue]` and then do a single SCAN of that intermediary to find only those `date < 17`. This still means our query does a SCAN, but it's scanning a smaller set of values (only 4 instead of at least 8 with the previous index) and we only do one scan instead of two scans.Anyway, here's a gist of some Python if you want to play with this concept: https://gist.github.com/lelandbatey/d09557fed38c48a797bf1b15...reply",
      "Indexes in general are not flattened trees; they are just trees. Using a Python map as a mental model is fraught with peril since those are implemented as hash tables, which don't have ordering (which most database indexes need to support). So it's the wrong model.For multidimensional indexes, you don't need to do anything fancy about nesting; you just need to have keys that have more than one component. To a first order, string concatenation is a good enough model. So in your case, here's what the index looks like:    ['10 red',   101],\n    ['11 blue',  111],\n    ['12 green', 121],\n    ['13 red',   131],\n    ['14 blue',  141],\n    ['14 red',   142],\n    more values\u2026\n\nwhich is then organized in some sort of tree structure, like (the exact details don't matter):    ['10 red', '12 green',  # min and max values for the tree below\n      [\n        ['10 red',   101],\n        ['11 blue',  111],\n        ['12 green', 121]\n      ]],\n    ['13 red', '14 red',    # similar\n      [\n        ['13 red',   131],\n        ['14 blue',  141],\n        ['14 red',   142]\n      ]],\n    more nodes\u2026reply",
      "Yes, this is true. I chose a nested-maps example because while it is inaccurate for actual DB applications, it's very helpful for explaining the limits of index ordering and the limits that range queries have when interacting with indexes. It's helpful to use maps as a first explanation because they're one of the most used datastructures that nearly all languages have built in (yes, I know, C, I'm talking about the other 9 of top 10 languages), and many work-a-day developers and dabblers will use nearly _only_ lists and maps (and objects/classes/structs) in their language of choice. I could've used Python's OrderedDict but I figured if I was going to stray away from \"the most straightforward possible case using the datastructures everyone is already using daily\", I'd have been better off jumping straight to a custom tree, like you've done.That's a great miniature example of such a tree!reply",
      "I agree, all of these rules aren't the right way to teach about how to reason about this. All of the perf properties described should fall out of the understanding that both tables and indices in SQLite are B-trees. B-trees have the following properties:- can look up a key or key prefix in O(log N) time (\"seek a cursor\" in DB parlance, or maybe \"find/find prefix and return an iterator\" for regular programmers)- can iterate to next row in amortized O(1) time (\"advance a cursor\" in DB parlance, or maybe \"advance an iterator\" for regular programmers). Note that unordered data structures like hash maps don't have this property. So the mental model has to start with thinking that tables/indices are ordered data structures or you're already lost.A table is a b+tree where the key is the rowid and the value is the row (well, except for WITHOUT ROWID tables).An index is a b-tree where the key is the indexed column(s) and the value is a rowid.And SQLite generally only does simple nested loop joins. No hash joins etc. Just the most obvious joining that you could do if you yourself wrote database-like logic using ordered data structures with the same perf properties e.g. std::map.From this it ought to be pretty obvious why column order in an index matters, etc.reply",
      "The non-equivalence of .9 and 0.9 was certainly a surprise to me.reply",
      "Yeah, but who ever writes \"x=0.9\" as a constraint on a partial index?  Really?  Don't you know you aren't suppose to compare floating point quantities for equality?If P is the expression on the partial index and Q is the WHERE clause of the query, then the partial index is only usable if Q implies P for all possible assignments of variables.  A theorem prover is needed to establish this.  Every RDBMS has one.  The one inside SQLite is not terribly bright, true enough.  It leans toward usually little memory and few CPU cycles.  It does not do a good job if P contains \"x=0.9\".  On the other hand, SQLite's theorem prover is decent if P contains \"x IS NOT NULL\", because in actual practice, probably about 90% of partial index WHERE clauses are some variation on \"x IS NOT NULL\".The partial index expression does not always have to be exactly the same as what is in the WHERE clause of the query.  SQLite will always find the match if P is a subset of Q; if Q can be rewritten as \"R AND P\".  But if P is \"x IS NOT NULL\" and Q does anything that restricts x from being NULL, for example if Q contains \"x>0\", then SQLite's theorem prover will find that match too, even if \"IS NOT NULL\" never appears in Q.Will the theorem prover in SQLite get better someday?  Perhaps.  It has gotten better over the years.  The question becomes, how much more code space and query-planning CPU cycles are you willing to spend to get a slightly better query planner?  This trade-off is different for a client/server database engine.  With SQLite being embedded, the trade-off tends to fall more on the side of \"keep it simple\".  If you have followed SQLite over many years, you might have noticed it is shifting toward more complex decision making as memory becomes cheaper and CPUs get faster.  It's a tricky balancing act to find the sweet spot.reply"
    ],
    "link": "https://emschwartz.me/subtleties-of-sqlite-indexes/",
    "first_paragraph": "About Projects Blog Github \ud83d\udc3f\ufe0f Scour\n\n\n                    Sep 29, 2025\n                \n\nIn the last 6 months, Scour has gone from ingesting 330,000 pieces of content per month to over 1.4 million this month. The massive increase in the number of items slowed down the ranking for users' feeds and sent me looking for ways to speed it up again.After spending too many hours trying in vain to squeeze more performance out of my queries and indexes, I dug into how SQLite's query planner uses indexes, learned some of the subtleties that explained why my initial tweaks weren't working, and sped up one of my main queries by ~35%.Scour is a personalized content feed that finds articles, blog posts, etc related to users' interests. For better and for worse, Scour does its ranking on the fly whenever users load their feeds page. Initially, this took 100 milliseconds or less, thanks to binary vector embeddings and the fact that it's using SQLite so there is no network latency to load data.The most "
  },
  {
    "title": "ML on Apple ][+ (mdcramer.github.io)",
    "points": 96,
    "submitter": "mcramer",
    "submit_time": "2025-09-29T16:12:30 1759162350",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=45415510",
    "comments": [
      "One of my early \"this is neat\" programs was a genetic algorithm in Pascal.  You entered a bunch of digits and it \"evolved\" the same sequence of digits.  It started out with 10 random numbers.  Their fitness (lower was better) was the sum the difference.  So if the target was \"123456\" and the test number was \"214365\", it had a fitness of 6.  It took the top 5, and then mutated a random digit by a random +/- 1.  It printed out each row with the full population. and so you could see it scrolling as it converged on the target number.Looking back, I want to say it was probably the July, 1992 issue of Scientific American that inspired me to write that ( https://www.geos.ed.ac.uk/~mscgis/12-13/s1100074/Holland.pdf ) .  And as that was '92, this might have been on a Mac rather than an Apple ][+... it was certainly in Pascal (my first class in C was in August '92) and I had access to both at the time (I don't think it was turbo pascal on a PC as this was a summer thing and I didn't have a IBM PC at home at the time).  Alas, I remember more about the specifics of the program than I do about what desk I was sitting at.reply",
      "I wrote a whole project in pascal around that time.  Analyzing two datasets.  It was running out of memory the night before it was due, so I decided to have it run twice, once for each dataset.That's when I learned a very important principal.  \"When something needs doing quickly, don't force artificial constraints on yourself\"I could have spent three days figuring out how to deal with the memory constraints.  But instead I just cut the data in half and gave it two runs.  The quick solution was the one that was needed.  Kind of an important memory for me that I have thought about quite a bit in the last 30+ years.reply",
      "An Aeon ago in 1984, I wrote a perceptron on the Apple II. It was amazingly slow (20 minutes to complete a recognition pass), but what most impressed me at the time was that it did work. Since that time as a kid I always wondered just how far linear optimization techniques could take us. If I could just tell myself then what I know now...reply",
      "I thought this was going to be about the programming language, and I was wondering how they managed to implement it on a machine that small.reply",
      "That's also what I was thinking. ML predates the Apple II by 4 years, so I think there is definitely a chance of getting it running! If targetting the Apple IIGS I think it would be very achievable; you could fit megabytes of RAM in those.reply",
      "Likely any early implementation of ML would have been on a mainframe or minicomputer, not a 6502. A mainframe/minicomputer would have had oodles of storage (both durable and RAM), as well as a compiler for a high level language (which fits what I can see in https://smlfamily.github.io/history/ML2015-talk.pdf and other locations).reply",
      "Same. What flavor of ML would be the most appropriate for that challenge, do you think?reply",
      "While not exactly ML, David Turner's Miranda system is pretty small, and might be feasible:https://codeberg.org/DATurner/mirandareply",
      "Bit of a weird choice to draw a decision boundary for a clustering algorithm...reply",
      "This motivates me to try this on my Ministrel 4th (21th century Jupiter Ace clone).reply"
    ],
    "link": "https://mdcramer.github.io/apple-2-blog/k-means/",
    "first_paragraph": "\n        ML Product Manager who enjoys hacking around with models and code in his free time, of which there is never, sadly, enough.\n      Wait. Does k-means count as machine learning? Yes. Yes, it does.CS229 is the graduate-level machine learning course I took at Stanford as part of the Graduate Certificate in AI which I received back in 2021. While k-means is my choice as the easiest to understand algorithm in machine learning, it was taught as the introductory clustering algorithm for unsupervised learning. As a TA for XCS229, which I have been doing since 2022 and most recently did this Spring, I know that it is still being taught as part of this seminal course in AI.Unlike previously where I saved the result for the end, let\u2019s start by taking a look at the algorithm in action!Here is a screenshot of the final decision boundary after convergence.The final accuracy is 90% because 1 of the 10 observations is on the incorrect side of the decision boundary.For debugging purposes, to sp"
  },
  {
    "title": "FAA decides it trusts Boeing enough to certify safety of its own planes again (theregister.com)",
    "points": 22,
    "submitter": "sipofwater",
    "submit_time": "2025-09-29T23:56:31 1759190191",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.theregister.com/2025/09/29/faa_decides_it_trusts_boeing/",
    "first_paragraph": ""
  },
  {
    "title": "Not all OCuLink eGPU docks are created equal (jeffgeerling.com)",
    "points": 98,
    "submitter": "speckx",
    "submit_time": "2025-09-29T14:46:11 1759157171",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=45414479",
    "comments": [
      "In my experience, incompatibilities like that have for the most part been due business partnership decisions. Let me give you an example. A company that designs motherboards may state that one board should be compatible with a full range of other hardware in the market, e.g. RAM, PCIe, M.2, etc. The reality is that they usually partner and/or are locked into certain vendors, contractually, that supply them with their products for the board. You can see this in their products spec pages, in a section titled, \"Support List,\" with a warning that should you deviate from this list, they will not guarantee 100% functionality. Basically, the company tests each product that they list, and if does not work properly, they either tweak the board, its firmware, or communicate this to the third-party vendor to see if they can provide a solution. Most companies that sell these boards or devices more often than not, are small, do not have enough R&D funds, or manpower to do the proper research, testing and adjustments to their products, since devices are already 6 months behind current technology when they are released into the wild. Usually, customers can call into the company's support line and report the problem. The company may or may not address the incompatibility of their board with the third-party product if they see that there is potential for increased sales should the third-party device prove to be a trending product, or the customer is a high-volume order customer. More often than not, the incompatibility is not properly addressed.Just my thoughts :|reply",
      "Are they actually extra? Or does the other cable simply route them over coax? Answer here is to grab a couple SFF-???? Breakout boards, a scope, and a signal generator.Another thing to consider is isolating all but the first PCIe lane.reply",
      ">  Answer here is to grab a couple SFF-???? Breakout boards, a scope, and a signal generator.Yeah, I've considered buying an extra adapter and just probing pins \u2014 I don't have a spare M.2 to OCuLink adapter but will probably order a few different models to see if there are any differences there too.> Another thing to consider is isolating all but the first PCIe lane.The Pi nicely does that for me ;)reply",
      "Haha, touch\u00e9- forgot the Pi5 only has a single lane.The comment came out of knowing if the cable's a little dodgy the timing could be off- scrambling the PCIe signals (easier to just tape it off than look up if its supposed to matter and then tape it off anyway), and the fact my first HBA had the \"QC pass\" sticker prevent some lanes from connecting and the whole card was unresponsive as a result.reply",
      "I understand the focus on the cables, and as an owner of several Minisforum devices, IME they don't really do compatibility very well (e.g. linux on V3). Their specialty seems to be taking reference PCB's, down costing them by changing/removing components, and then putting them in very unique enclosures. They're one vendor I would avoid considering - unless it's x86.reply",
      "I can't speak for the V3, but Minisforum's AMD mini PCs (at least from the Zen 4 onwards) all have excellent Linux compatibility. I have the UM780 XTX and it's been a blast using it on Linux (Arch btw). A mate has a UM790, another bought a recent Ryzen AI Max one and they've all reported excellent Linux compatibility (running Fedora-based and Arch-based distros).The only issue is the lack of regular BIOS/firmware updates. My UM780 got around 5 updates post-release, but they haven't released any new updates since last year March last year... which isn't great if you care about firmware security. In my case however it's not a big issue as it's an isolated vlan and I use mine on my for dev/test/homelab stuff.reply",
      "The one AMD x86 box I got from them occasionally randomly segfaulted and would blackscreen and hang if left on on after anywhere from 45 minutes to a few hours.  I read that it might be a driver issue in proxmox debian and tried running windows on it and had the same issue.  Oddly memtest passed on it while running repeatedly for 3 days strait before shutting down unexpectedly.Its kinda made me hesitant to trust any of these cheap minipcs ever again.reply",
      "I just ordered the BD790i X3D mainboard. A while ago Minisforum has been known for their slow BIOS updates, but hope that they have improved their processes since. I'll see\u2026reply",
      "I love the hardware Minisforum makes, but man, this company ongoingly sets consumer expectations lower and lower.I haven't checked in the past year, but historically Minisforum has been awful about having bios updates, doing so seemingly only when truly needed.reply",
      "I've actually been dealing with this myself recently, but only the gpu portion. I've been trying to get a gpu working on the orangepi RV2 just to see if I can. The tools for building the kernel for orangepi are kind of bad though.But no, the 2 old amd gpus I have just don't show up at all in lspci, testing the RTX 2080 and RX 7800 both did show up though.reply"
    ],
    "link": "https://www.jeffgeerling.com/blog/2025/not-all-oculink-egpu-docks-are-created-equal",
    "first_paragraph": "I recently tried using the Minisforum DEG1 GPU Dock with a Raspberry Pi 500+, using an M.2 to OCuLink adapter, and this chenyang SFF-8611 Cable.After figuring out there's a power button on the DEG1 (which needs to be turned on), and after fiddling around with the switches on the PCB (hidden under the large metal plate on the bottom; TGX to OFF was the most important setting), I was able to get the Raspberry Pi's PCIe bus to at least tell the graphics card installed in the eGPU dock to spin up its fans and initialize.But I wasn't able to get any output from the card (using this Linux kernel patch), and lspci did not show it. (Nor were there any logs showing errors in dmesg).I switched back to my JMT eGPU OCuLink dock for the rest of my testing, and uploaded a video detailing some of my struggles, and a blog post detailing the Pi 500+ eGPU testing.A few commenters mentioned they too had issues with the Minisforum DEG1. But a few of them looked closely at the OCuLink cable Minisforum incl"
  }
]