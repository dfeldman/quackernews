[
  {
    "title": "100x defect tolerance: How we solved the yield problem (cerebras.ai)",
    "points": 108,
    "submitter": "jwan584",
    "submit_time": "2025-01-15T21:19:15 1736975955",
    "num_comments": 74,
    "comments_url": "https://news.ycombinator.com/item?id=42717165",
    "comments": [
      "I think this is an important step, but it skips over that 'fault tolerant routing architecture' means you're spending die space on routes vs transistors. This is exactly analogous to using bits in your storage for error correcting vs storing data.That said, I think they do a great job of exploiting this technique to create a \"larger\"[1] chip. And like storage it benefits from every core is the same and you don't need to get to every core directly (pin limiting).In the early 2000's I was looking at a wafer scale startup that had the same idea but they were applying it to an FPGA architecture rather than a set of tensor units for LLMs. Nearly the exact same pitch, \"we don't have to have all of our GLUs[2] work because the built in routing only uses the ones that are qualified.\" Xilinx was still aggressively suing people who put SERDES ports on FPGAs so they were pin limited overall but the idea is sound.While I continue to believe that many people are going to collectively lose trillions of dollars ultimately pursuing \"AI\" at this stage. I appreciate the the amount of money people are willing to put at risk here allow for folks to try these \"out of the box\" kinds of ideas.[1] It is physically more cores on a single die but the overall system is likely smaller, given the integration here.[2] \"Generic Logic Unit\" which was kind of an extended LUT with some block RAM and register support.\n \nreply",
      "\"While I continue to believe that many people are going to collectively lose trillions of dollars ultimately pursuing \"AI\" at this stage\"Can you please explain more why you think so ?Thank you.\n \nreply",
      "Neat. What about power density?An H100 has a TDP of 700 watts (for the SXM5 version). With a die size of 814 mm^2 that's 0.86 W/mm^2. If the cerebras chip has the same power density, that means a cerebras TDP of 37.8 kW.That's a lot. Let's say you cover the whole die area of the chip with water 1 cm deep. How long would it take to boil the water starting from room temperature (20 degrees C)?amount of water = (die area of 46225 mm^2) * (1 cm deep) * (density of water) = 462 gramsenergy needed = (specific heat of water) * (80 kelvin difference) * (462 grams) = 154 kJtime = 154 kJ / 39.8 kW = 3.9 secondsThis thing will boil (!) a centimeter of water in 4 seconds. A typical consumer water cooler radiator would reduce the temperature of the coolant water by only 10-15 C relative to ambient, and wouldn't like it (I presume) if you pass in boiling water. To use water cooling you'd need some extreme flow rate and a big rack of radiators, right? I don't really know. I'm not even sure if that would work. How do you cool a chip at this power density?\n \nreply",
      "The enthalpy of vaporization of water (at standard pressure) is listed by Wikipedia[1] as 2.257 kJ/g, so boiling 462 grams would require an additional 1.04 MJ, adding 26 seconds. Cerebras claims a \"peak sustained system power of 23kW\" for the CS-3 16 Rack Unit system[2], so clearly the power density is lower than for an H100.[1] https://en.wikipedia.org/wiki/Enthalpy_of_vaporization#Other...\n[2] https://cerebras.ai/product-system/\n \nreply",
      "The machine that actually holds one of their wafers is almost as impressive as the chip itself. Tons of water cooling channels and other interesting hardware for cooling.\n \nreply",
      "A Very Fancy cooling engine: https://www.eetimes.com/powering-and-cooling-a-wafer-scale-d...\n \nreply",
      "A good talk on how Cerebras does power & cooling (8min)\nhttps://www.youtube.com/watch?v=wSptSOcO6Vw&ab_channel=Appli...\n \nreply",
      "Minor correction, the keynote video says ~20 kW\n \nreply",
      "If rack mounted, you are ending up with something like a reverse power station.So why not use it as an energy source? Spin a turbine.\n \nreply",
      "There's a bunch of places in Europe that use waste heat from datacenters in district heating systems. Same thing with waste heat from various industrial processes. It's relatively common practice.\n \nreply"
    ],
    "link": "https://cerebras.ai/blog/100x-defect-tolerance-how-cerebras-solved-the-yield-problem",
    "first_paragraph": ""
  },
  {
    "title": "I have made the decision to disband Hindenburg Research (hindenburgresearch.com)",
    "points": 203,
    "submitter": "toomuchtodo",
    "submit_time": "2025-01-15T21:23:55 1736976235",
    "num_comments": 73,
    "comments_url": "https://news.ycombinator.com/item?id=42717234",
    "comments": [
      "I've ironically lost more money the more closely I've paid attention to my investments because I was naively confident in the market's ability (or as I've come to suspect, willingness) to react to evidence of fraud.The amount of deceit put out into the world and gobbled up, on purpose, in business is obscene and seriously depressing. The magnitude of damage to psyches and thus economies that anyone acting in a fraudulent manner in finance creates is far-reaching and immeasurable. Punishment for financial crimes should be calculated based on the average lifetime earnings of a citizen -- if your victims are folks earning at or below the average wage, and you've scammed 100 lifetimes worth of average earnings, it's as if you've murdered 100 people.Hindenburg's reports were a true pleasure to read, and their track record proves their positive contribution to society. Many self-important people online are quick to pounce on short sellers as being evil, and that will forever be a serious red flag to me thanks in no small part to Nate Anderson and the folks at Hindenburg Research.\n \nreply",
      "The market can remain irrational longer than you can remain solvent. The market will tolerate infinite BS for arbitrary periods of time.Which also means being careful of short selling. It can put you at unlimited risk even if you are absolutely right.\n \nreply",
      "I\u2019ll always remember them for exposing the Nikola motors fraud: https://news.ycombinator.com/item?id=24436721\n \nreply",
      "Did he... link the wrong URL at the end of the post? I thought for sure it was going to be some sort of heartfelt speech, or motivational message, or something. But it's an instrumental DJ set which seems totally out of left field\n \nreply",
      "> P.S. If you are chasing something you think you want or need, or are doubting whether you are enough, take a minute and give this a listen. It had a big impact on me at a pivotal time.He shared something that helped him at a pivotal time. Your expectations are your own. :)\n \nreply",
      "The right music at the right time can be transformative.\n \nreply",
      "Ha I guess that's fair. Were you expecting a DJ set?\n \nreply",
      "I did have a guess that it was musical. I did not guess it was of the genre haha.\n \nreply",
      "And the highlighted comment on that video (because it's got a lot of upvotes) is \"Who's here because of Hindenburg?\"...I suppose all the research work, that comment, and the 750+ thumbs-ups, and my cynical meta-comment all brought value to the world. But I'm only sure of one of those things.\n \nreply",
      "I actually skimmed through looking for when the inspirational talk starts.\n \nreply"
    ],
    "link": "https://hindenburgresearch.com/gratitude/",
    "first_paragraph": ""
  },
  {
    "title": "Why is Cloudflare Pages' bandwidth unlimited? (mattsayar.com)",
    "points": 380,
    "submitter": "MattSayar",
    "submit_time": "2025-01-15T15:55:13 1736956513",
    "num_comments": 241,
    "comments_url": "https://news.ycombinator.com/item?id=42712433",
    "comments": [
      "> Additionally, there's plenty of \"Upgrade to Pro\" buttons sprinkled about. It's the freemium model at work.I don't think they care much about few \"Pro\" upgrades here and there. The real money, and their focus as a company, is in enterprise contracts. Note that, Matthew Prince, the CEO, had outlined a few reasons why they have such a generous free tier on an Stack Exchange answer[1]. I think the biggest reason is this:> Bandwidth Chicken & Egg: in order to get the unit economics around bandwidth to offer competitive pricing at acceptable margins you need to have scale, but in order to get scale from paying users you need competitive pricing. Free customers early on helped us solve this chicken & egg problem. Today we continue to see that benefit in regions where our diversity of customers helps convince regional telecoms to peer with us locally, continuing to drive down our unit costs of bandwidth.Cloudflare had decided long ago that they wanted to work at an incredible scale. I would actually be very interested in understanding how this vision came to be. Hope Matthew writes that book someday.[1]: https://webmasters.stackexchange.com/a/88685.\n \nreply",
      "I think there are a few other benefits (even if that was the main benefit/driving force behind the decision).When you have low-paying (or zero-paying) customers, you need to make your system easy. When you're enterprise-only, you can pay for stuff like dedicated support reps. A company is paying you $1M+/year and you hire someone at $75,000 who is dedicated to a few clients. Anything that's confusing is just \"Oh, put in a chat to Joe.\" It isn't the typical support experience: it's someone that knows you and your usage of the system. By contrast, Cloudflare had to make sure that its system was easy enough to use that free customers would be able to easily (cheaply) make sense of it. Even if you're going to give enterprise customers white-glove service, it's always nice for them when systems are easy and pleasant to use.When you're carrying so much free traffic, you have to be efficient. It pushes you to actually make systems that can handle scale and diverse situations without just throwing money at the problem. It's easy for companies to get bloated/lazy when they're fat off enterprise contracts - and that isn't a good recipe for long-term success.Finally, it's a good way to get mindshare. I used Cloudflare for years just proxying my personal blog that got very little traffic. When my employer was thinking about switching CDNs, myself and others who had used Cloudflare personally kinda pushed the \"we should really be looking at Cloudflare.\" Free customers may never give you a dollar - but they might know someone or work for someone who will give you millions. Software engineers love things that they can use for free and that has often paid dividends for companies behind those free things.\n \nreply",
      "I built my website on Cloudflare Pages and ended up using basically their entire suite of tools - Pages, D1, Analytics, Rules, Functions. The DX was pretty good because all of these features worked well together.Cloudflare offered all of this for free because it gets them positive mentions (like the one you\u2019re reading right now) and they\u2019re educating a bunch of developers on their entire product portfolio. And what does it cost to host my blog that 1000-2000 views a month? Literally nothing.\n \nreply",
      "I feel like there might be an additional motivation too, which is that this investment in a better internet (free SSL for everyone before LetsEncrypt came around, generous free tiers for users, etc. etc.) means that Cloudflare builds a reputation of being a steward of the ecosystem while also benefitting indirectly from wider adoption of good, secure practices.In some ways it's analogous to investing in your local community and arguably paying tax: it's rare that you would directly and personally benefit from this, but if the environment you live in improves from it, crime is reduced, more to do, etc. then you can enjoy a better quality of life.\n \nreply",
      "Have they made a better internet?  Many would say that made it worse.\n \nreply",
      "Overall, certainly. There are some negative things people talk about that you might agree with, but look back at what the market was that they disrupted and continue to disrupt. I think that without Cloudflare your registrar would be GoDaddy and your SSL certificates would be from Verisign and your rents would be huge. Backbone wise, that would depend on your region.",
      "I mean, maybe we would have found another solution to DDOS, but as someone who has had a pretty significant attack (on a service which is a clear public good) mitigated for free\u2026 it\u2019s pretty nice being able to keep your services online in a hostile environment.\n \nreply",
      "I don\u2019t know the history here, do you have some examples?My usage is pretty much limited to their DNS.\n \nreply",
      "They're pretty reviled by people who go out of their way to be private via things like VPNs and locked down browsers, because that constantly trips their bot detection and makes using the web miserable.\n \nreply",
      "They\u2019ve got a pretty long history of helping scammers and criminals.https://www.spamhaus.org/resource-hub/service-providers/too-...\n \nreply"
    ],
    "link": "https://mattsayar.com/why-does-cloudflare-pages-have-such-a-generous-free-tier/",
    "first_paragraph": ""
  },
  {
    "title": "OpenAI Fails to Deliver Opt-Out System for Photographers (petapixel.com)",
    "points": 58,
    "submitter": "onetokeoverthe",
    "submit_time": "2025-01-15T23:41:52 1736984512",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=42718850",
    "comments": [
      "No way OpenAI will ever \u201cgood citizen\u201d this. Tools to opt out of training sets will only come if they are legally compelled. Governments will have to make respecting some sort of training preference header on public content mandatory I think.The fact that photographers have to independently submit each piece of work they wanted excluded along with detailed descriptions just shows how much they DONT want anyone excluding content from their training data.\n \nreply",
      "> The fact that photographers have to independently submit each piece of work they wanted excluded along with detailed descriptions just shows how much they DONT want anyone excluding content from their training data.That's bloody brilliant. If you don't want us to scrape your content, please send us your content with all of the training data already provided so we will know not to scrape it if we come across it in the wild. FFS\n \nreply",
      "Insofar as data for diffusion / image / video models are concerned, the rise of synthetic data and data efficiency will mean that none of this really matters anyway. We were just in the bootstrapping phase.You can bolt on new functional modules and train them with very limited data you acquire from Unreal Engine or in the field.\n \nreply",
      "I don\u2019t entirely agree. For example, it\u2019s a very popular scheme on Etsy right now to use LLMs to generate posters in the style of popular artists. Any artist should be able to say hey I don\u2019t want my works to be part of your training set to power derivative generations.And I think it should even apply retroactively so that they have to retrain their models that are already generating works from training data consumed without permission. Of course, OpenAI would fight that tooth & nail but they put themselves in this position with a clear \u201ctake first ask permission later\u201d mentality.\n \nreply",
      "For clarity, I do agree that synthetic data is huge for training AI to do certain tasks or skills. But I don\u2019t think creative work generation is powered by synthetic data and may not be for a quite while.\n \nreply",
      "Maybe the task to implement it was scheduled by ChatGPT...https://news.ycombinator.com/item?id=42716744\n \nreply",
      "Sorry the task failed for unknown reasons.\n \nreply",
      "I don't even understand why it's everyone elses problem to opt-out.Eventually - for how many of these AI companies would a person have to track down their opt-out processes just to protect their work from AI?  That's crazy.OpenAI should be contacting every single one and asking for permission - like everyone has to in order to use a person's work. How they are getting away with this is beyond me.\n \nreply",
      "Eventually the headline will be the first 2 words.The tech is neat, there is value in a sense, LLMs are a fun tech. They are not going to invent AGI with LLMs.\n \nreply",
      "who cares if they do it with LLMs or not? how do you define agi?\n \nreply"
    ],
    "link": "https://petapixel.com/2025/01/06/openai-fails-to-deliver-opt-out-system-for-photographers/",
    "first_paragraph": ""
  },
  {
    "title": "Ropey \u00e2\u20ac\u201c A UTF8 text rope for manipulating and editing large text (github.com/cessen)",
    "points": 188,
    "submitter": "keepamovin",
    "submit_time": "2025-01-15T15:27:55 1736954875",
    "num_comments": 77,
    "comments_url": "https://news.ycombinator.com/item?id=42711966",
    "comments": [
      "From the Readme:\"Unsafe code\nRopey uses unsafe code to help achieve some of its space and performance characteristics. Although effort has been put into keeping the unsafe code compartmentalized and making it correct, please be cautious about using Ropey in software that may face adversarial conditions.Auditing, fuzzing, etc. of the unsafe code in Ropey is extremely welcome. If you find any unsoundness, please file an issue! Also welcome are recommendations for how to remove any of the unsafe code without introducing significant space or performance regressions, or how to compartmentalize the unsafe code even better.\"\n \nreply",
      "Rust is missing an abstraction over non-contiguous chunks of contiguous allocations of data that would make handling ropes seamless and more natural even for smaller sizes.C# has the concept of \u201cSequences\u201d which is basically a generalization of a deque with associated classes and apis such as ReadOnlySequence and SequenceReader to encourage reduced allocations, reuse of existing buffers/slices even for composition, etcKnowing the rust community, I wouldn\u2019t be surprised if there\u2019s already an RFC for something like this.\n \nreply",
      "I think you might be looking for the bytes crate, which is pretty widely used in networking code: https://docs.rs/bytes/latest/bytes/index.htmlIn general this sort of structure is the sort of thing I'd expect to see in an external crate in rust, not the standard library. So it's unlikely there's any RFCs, and more likely there's a few competing implementations lying around.\n \nreply",
      "Bytes is essentially multiple slices over a optimistically single contiguous arc buffer. It's basically the inverse of what the root comment is after (an array of buffers). It's a rather strange crate because network IO doesn't actually need contiguous memory.std does actually have a vague version of what the root comment wants: https://doc.rust-lang.org/std/io/struct.IoSlice.html and its sibling IoSliceMut (slicing, appending, inserting, etc. is out of scope for both - so not usable for rope stuff)\n \nreply",
      "> It's a rather strange crate because network IO doesn't actually need contiguous memory.Network IO doesn't need contiguous memory, no, but each side of the duplex kind of benefits from it in its own way:1. on receive, you can treat a contiguous received network datagram as its own little memory arena \u2014 write code that sends sliced references to the contents of the datagram to other threads to work with, where those references keep the datagram arena itself alive for as long as it's being worked with; and then drop the whole thing when the handling of the datagram is complete.(This is somewhat akin to the Erlang approach \u2014 where the received message is a globally-shared binary; it gets passed by refcount into an actor started just for handling that request; that actor is spawned with its own preallocated memory arena; into that arena, the actor spits any temporaries related to copying/munging the slices of the shared binary, without having to grow the arena; the actor quickly finishes and dies; the arena is deallocated without ever having had to GC, and the refcount of the shared binary goes to zero \u2014 unless non-copied slices of it were async-forwarded to other actors for further processing.)Also note that the whole premise here is zero-copy networking (as the bytes docs say: https://docs.rs/bytes/1.9.0/bytes/#bytes). The \"message\" being received here isn't a copy of the one from the network card, but literally the same physical wired memory the PHY sees as being part of its IO ring-buffer \u2014 just also mapped into your process's memory on (zero-copy) receive. If this data came chunked, you'd need to copy some of it to assemble those chunks into a contiguous string or data structure. But since it arrives contiguously, you can just slice it, and cast the resulting slice into whatever type you like.2. on send \u2014 presuming you're doing non-blocking IO \u2014 it's nice to once again have a preallocated arena into which you can write out byte-sequences before flinging them at the kernel as [vectors of] large, contiguous DMA requests, without having to stop to allocate. (This removes the CPU as a bottleneck from IO performance \u2014 think writev(2).)The ideal design here is that you allocate fixed-sized refcounted buffers; fill them up until the next thing you want to write doesn't fit\u2020; and then intentionally drop the current buffer, switching your write_arena reference to point to a freshly-allocated buffer; and repeating. Each buffer then lives until all its slice-references get consumed. This forms kind of a \"memory-lifetime-managed buffer-persisted message queue\" \u2014 with the backing buffers of your messages living until all the messages held in them get \"ACKed\" [i.e. dropped by the receiving threads.]Also, rather than having the buffers deallocate when you \"use them up\" \u2014 requiring you to allocate the next time you need a buffer \u2014 you can instead have the buffer's destructor release the memory it's holding into a buffer pool; and then have your next-buffer-please logic pull from that pool in preference to allocating. But then you'll want a higher-level \"writable stream that is actually a mempool + current write_arena reference\" type. (Hey, that's BufMut!)\u2020 And at that point, when the next message doesn't fit, you do not split the message. That violates the whole premise of vectorizing the writes. Instead, you leave some of the buffer unused, and push the large message into a fresh buffer, so that the message will still correspond to a single vectorized-write element / io_uring call / DMA request / etc. If the message is so large it won't fit in your default buffer size, you allocate a buffer just for that one message, or better yet, you utilize a special second pool of larger fixed-size buffers. \"Jumbo\" buffers, per se.(Get it yet? Networking hardware is also doing exactly what I'm describing here to pack and unpack your packets into frames. For a NIC or switch, the buffers are the [bodies of the] frames; a jumbo buffer is an Ethernet jumbo frame; and so on.)\n \nreply",
      "> Get it yetI'm not sure if your comment was meant to be condescending, but it really does come across at that. I'm very well versed in this domain.Having a per-request/connection arena isn't the only option. What I have seen/use, which is still zero copy (as far as IO zero copy can be in Rust without resorting to bytemuck/blittable types), is to have a pool of buffers of a specific length - typically page-sized by default and definitely page-aligned. These buffers can come from a single large contiguous allocation. If you run out of space in a buffer you grab a new/reused one from the pool, add it to your vec of buffers, and carry on. At the end of the story you would use vectored IO to submit all of them at once - all the way down to the NIC and everything.This approach is more widespread mainly due to historical reasons: it's really easy to fragment 32bit address space, so allocating jumbo buffers simply wasn't an option if you didn't want your server OOMing with 1GB of available (but non-contiguous) memory.https://man7.org/linux/man-pages/man3/iovec.3type.htmlhttps://learn.microsoft.com/en-us/windows/win32/api/ws2def/n...\n \nreply",
      "> I'm very well versed in this domain.Apologies, I wasn't really responding to you directly; I was just taking the opportunity to write an educational-blog-post-as-comment aimed at the average HN reader (who has likely never considered what an Ethernet frame even is, or how a device that uses what are essentially DSPs does TDM packet scheduling) \u2014 with your comment being the parent because it's the necessary prerequisite reading to motivate the lesson.> Having a per-request/connection arena isn't the only option. What I have seen/use, which is still zero copy (as far as IO zero copy can be in Rust without resorting to bytemuck/blittable types), is to have a pool of buffers of a specific length - typically page-sized by default and definitely page-aligned. These buffers can come from a single large contiguous allocation. If you run out of space in a buffer you grab a new/reused one from the pool, add it to your vec of buffers, and carry on. At the end of the story you would use vectored IO to submit all of them at once - all the way down to the NIC and everything.I think you're focusing too much on the word \"arena\" here, because AFAICT we're both describing the same concept.In your model (closer to the one used in actual switching), there's a single global buffer pool that all concurrent requests lease from; in my model, there's global heap memory, and then a per-thread/actor/buf-object elastic buffer pool that allocates from the global heap every once in a while, but otherwise reuses buffers internally.I would say that your model is probably the one used in most zero-copy networking frameworks like DPDK. While my model is probably the one used in most language runtimes \u2014 especially managed + garbage-collected runtimes, where contending over a global language-exposed pool, can be more expensive than \"allocating\" (especially when the runtime has its own buffer pool and \"allocation\" rarely hits the kernel.)But both models are essentially the same from the perspective of someone using the buffer ADT and trying to understand why it's designed the way it is, what it gets them, etc. :)> it's really easy to fragment 32bit address space, so allocating jumbo buffers simply wasn't an option if you didn't want your server OOMing with 1GB of available (but non-contiguous) memory.Maybe you're imagining something else here, but when I say \"jumbo buffer\", I don't mean custom buffers allocated on demand and right-sized to hold one message; rather, I'm speaking of something very closely resembling actual jumbo frames \u2014 i.e. another pre-allocated pool containing a smaller number of larger, fixed-size MTU-slot buffers.With this kind of jumbo-buffer-pool, when your messages get big, you switch over from filling regular buffers to filling jumbo buffers \u2014 which holds off message fragmentation, but also means new messages go \"out the door\" a bit slower, maybe \"platoon\" a bit and potentially overwhelm the recipient with each burst, etc (which is why you don't just use the larger buffer pool as the only pool.)But if your messages can be bigger than your set jumbo-buffer size, then there's nowhere to go from there; you still need to have a way to split messages across frames.(Luckily, in the case of `bytes`, splitting a message across frames just means the message now needs multiple iovec-list entries to submit, rather than implying a framing protocol / L2 message encoding with a continuation marker / sequence ID / etc.)\n \nreply",
      "How does bytes crate, or anyone else, offer zero copy receive from kernel (as opposed to kernel bypass) sockets?As far as I know that is not possible: there's always a copy.\n \nreply",
      "For network receive, I was assuming kernel-bypass sockets, not kernel sockets.`bytes` can give you \"ring-buffer-like\" one-copy kernel-socket receive by e.g. using the Buf as the target for scheduling io_uring read/recv into.Also, RDMA is technically networking! (Though I think all the Rust RDMA libraries already provide ADTs that work like Buf/MutBuf, rather than just saying \"here's some network-shared memory, build your own ADT on top.\")\n \nreply",
      "Thanks, you mention explicitly kernel networking right below about the send path:> before flinging them at the kernel as [vectors of] large, contiguous DMA requests, without having to stop to allocateSo I had assumed you were taking about kernel networking elsewhere as well.BTW, on the kernel send path, there is again a copy, contiguous or not, regardless of what API you use.When using kernel networking I don't think contiguity matters as you suggest, as there is always a copy. Furthermore \"contiguous\" in userspace doesn't correspond to contiguous in physical address space so in any case the hardware is just often going to see a userspace buffer as a series of discontiguous pages anyway: that's what happens with direct IO disk writes, which _are_ zero copy (huge pages helps).\n \nreply"
    ],
    "link": "https://github.com/cessen/ropey",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A utf8 text rope for manipulating and editing large texts.\n      \n\nRopey is a utf8 text rope for Rust, designed to be the backing text-buffer for\napplications such as text editors.  Ropey is fast, robust, and can handle huge\ntexts and memory-incoherent edits with ease.Ropey is designed and built to be the backing text buffer for applications\nsuch as text editors, and its design trade-offs reflect that.  Ropey is good\nat:On the other hand, Ropey is not good at:Keep this in mind when selecting Ropey for your project.  Ropey is very good\nat what it does, but like all software it is designed with certain\napplications in mind.Ropey's atomic unit of text is\nUnicode scalar values\n(or chars in Rust)\nencoded as utf8.  All of Ropey's editing and slicing operations are done\nin terms of char indices, which prevents accidental creation of invali"
  },
  {
    "title": "dnSpyEx: .NET debugger and assembly editor (github.com/dnspyex)",
    "points": 70,
    "submitter": "unleaded",
    "submit_time": "2025-01-15T19:54:44 1736970884",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=42716072",
    "comments": [
      "Love this tool, used it mod a Unity game and learn all the internals to see how to integrate my mod with the base game. Was an invaluable resource. It also taught me a lot about how they made that game (Lethal Company) and how it was truly made to ship/get the idea out the door and not for perfection/maintainable code.~~IIRC the maintainer was like 15 years old?~~ Edit: My mistake, the maintainer is 18, https://github.com/ElektroKill\n \nreply",
      "DnSpy was great. The author (d4d) did many great things for the world of .net reversing and binary analysis, including dnspy and dnlib. One day, i don't know why, they archived most of their repositories.I'm glad electrokill stepped up and maintains dnspyex now. This also shows how resilient open source can be - as long as someone wants to work on the project, it can go on forever.\n \nreply",
      ">One day, i don't know why, they archived most of their repositories.Sounds like whytheluckystiff.\n \nreply",
      "Used this to debug an application crash in a vendor's proprietary windows app recently, and I was able to file a detailed bug report. (Though they decided it's notabug, womp womp.)You can install it with winget, but it's very particular about whether you're debugging a win32 or win64 app and it's a bit of a pain to get it to install both or just win32. I wonder if it would be possible to have both bundled in the same installer and just automatically relaunch the app if you try to debug a program with a mismatching arch. Or just download from the releases page...\n \nreply",
      "This is great news! Every time I used dnSpy I was blown away by how well made it was.I still don't know why the original dnSpy repo was archived suddenly. Every once in a while I would go check to see if there's been any movement. I didn't realize this project existed so thanks for sharing.\n \nreply",
      "Used dnSpy once to debug runtime IL-emited code as if it was C#. It managed to step into a generated emitted method, decompile it on the fly and set breakpoints on the decompiled C# code for subsequent hits. That was a mind-blowing at the time, nothing else was close and I'm not sure any other tool or IDE supports this even now. Though didn't have a need for that since.\n \nreply",
      "Ahh I remember this. I used this to successfully bypass the registration of a small web scraper back in the day.\n \nreply",
      "Congrats on stealing someone\u2019s software so you could steal someone else\u2019s data!\n \nreply",
      "I laughed out loud lol. We have LLMs for this now.\n \nreply"
    ],
    "link": "https://github.com/dnSpyEx/dnSpy",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Unofficial revival of the well known .NET debugger and assembly editor, dnSpy\n      dnSpyEx is an unofficial continuation of the dnSpy project which is a debugger and .NET assembly editor. You can use it to edit and debug assemblies even if you don't have any source code available. Main features:See below for more featuresLatest stable release: https://github.com/dnSpyEx/dnSpy/releasesIf you like living on the edge you can use the latest \"beta\" builds from:\nTo debug Unity games, you need this repo too: https://github.com/dnSpyEx/dnSpy-Unity-monoClick here if you want to help with translating dnSpy to your native language.See the Wiki for build instructions and other documentation.dnSpy is licensed under GPLv3.\n        Unofficial revival of the well known .NET debugger and assembly editor, dnSpy\n      "
  },
  {
    "title": "Why the weak nuclear force is short range (profmattstrassler.com)",
    "points": 289,
    "submitter": "sohkamyung",
    "submit_time": "2025-01-11T23:43:33 1736639013",
    "num_comments": 138,
    "comments_url": "https://news.ycombinator.com/item?id=42669906",
    "comments": [
      "Doesn't this \"explanation\" just shift the question to what is stiffness?  Like it refactored the question but didn't actually explain it.Previously, we had statement \"the weak force is short range\".  In order to explain it, we had to invent a new concept \"stiffness\" that is treated as a primitive and not explained in terms of other easy primitives, and then we get to \"accurately\" say that the weak force is short due to stiffness.I grant the OP that stiffness might be hard to explain, but then why not just say \"the weak force is short range -- and just take that as an axiom for now\".\n \nreply",
      "I think it's a big improvement. Stiffness is something you can picture directly, so the data -> conclusions inference \"stiffness\" -> \"mass and short range\" follows directly from the facts you know and your model of what they mean. Whereas \"particles have mass\" -> \"short range\" requires someone also telling you how the inference step (the ->) works, and you just memorize this as a fact: \"somebody told me that mass implies short range\". You can't do anything with that (without unpacking it into the math), and it's much harder to pattern-match to other situations, especially non-physical ones.It seems to me like the right criteria for a good model is:* there are as few non-intuitable inferences as possible, so most conclusions can be derived from a small amount of knowledge* and of course, inferences you make with your intuition should not be wrong(I suppose any time you approximate a model with a simpler one---such as the underlying math with a series of atomic notions, as in this case---you have done some simplification and now you might make wrong inferences. But a lot of the wrongness can be \"controlled\" with just a few more atoms. For instance \"you can divide two numbers, unless the denominator is zero\" is such a control: division is intuitive, but there's one special case, so you memorize the general rule plus the case, and that's still a good foundation for doing inference with)\n \nreply",
      "Besides the fact that stiffness shows up as a term in the equations, stiffness is a concept that can be demonstrated via analogy with a rubber sheet, and so lends itself to a somewhat more intuitive understanding.Also, the math section demonstrated how stiffness produces both the short-range effect and the massive particles, so instead of just handwaving \"massive particles is somehow related to the short range\" the stiffness provides a clear answer as to why that's the case.\n \nreply",
      "If you read far enough into the math-y explanations, stiffness is a quantity in the equations.  That makes it more than a hand waving explanation in my book.\n \nreply",
      "In addition to what the sibling comments have said, the \"axiom\" is actually the term in the equations. That is, fundamentally, where this all comes from. \"Stiffness\" is just a word coined to help describe the behavior that arises from a term like this. Everything flows from having that piece in the math, so if you start there and with nothing else, you can reinvent everything else in the article. (Though it will take you a while.)You might also ask where that term comes from. It really is \"axiomatic\": there is no a priori explanation for why anything like that should be in the equations. They just work out if you do that. Finding a good explanation for why things have to be this way and not that way is nothing more and nothing less than the search for the infamous Theory of Everything.\n \nreply",
      "This is often I think a really unsatisfying thing about physics. Usually the qualitative descriptions don't quite make sense if you think very hard about them- and if you dig deeper it's often just \"we found some math that fits our experimental data\" - and ultimately that is as much as we know, and most attempts at explaining it conceptually are conjecture at best.When I was a physics undergrad, most of my professors were fans of the \"shut up and calculate\" interpretation of quantum mechanics.Ultimately, this is probably just a symptom of still not having yet discovered some really important stuff.\n \nreply",
      "I think its more than \"we just found some math that fits the data\" in the sense that its not just a case of adding some terms to match an observed curve - for example like with Rayleigh-Jeans' law vs Wein's Approximation of blackbody radiation and eventually Max Planck's solution by quantizing energy to the curve match experiment, without actually having anything else to say about it.Spiritually it feels more like what happened later, when people took the idea of quantized energy seriously and began finding ways to make it a theoretically consistent theory which also required a radical new approach of disregarding old intuitive assumptions about the way the most fundamental things worked solely to obey a new abstract, esoteric, purely theoretical framework (an approach which was sometimes controversial especially with experimentalists).But of course this new theory of quantum mechanics turned out to be immensely successful in totally unprecedented ways, in a manner similar to Relativity and it's \"theory first\" origin with trying to ensure mathematical consistency of Maxwell's equations and disregarding anything else in the way (and eventually with Einstein's decade long quest to find a totally covariant general theory that folded gravity into the mix).With physics the more I dug into \"why\" it was rarely the case that it was \"just because\", the justification was nearly always some abstract piece of math that I wasn't equipped to understand at the time but was richly rewarded later on when I spent the time studying in order to finally appreciate it.The first time I solved Schrodinger Equation for a hydrogen atom, I couldn't see why anyone could've bothered to try discovering how to untangle such a mess of a differential equation with a thousand stubborn terms and strange substitutions (ylm??) and spherical coordinate transformations - all for a solution I had zero intuition or interest in. After I had a better grasp of the duality between those square integrable complex functions and abstract vector spaces I found classical QM elegant in an way I wasn't able to see before. When basic Lie theory and representations was drilled into my head and I had answered a hundred questions about different matrix representations of the SU(n) and S0(3) groups and their algebras and how they were related, it finally clicked how naturally those ylm angular momentum things I saw before actually arose. It was spooky how group theory had manifested in something as ubiquitous and tangible as the structure of the periodic table. After drudging through the derivation of QFT for the first time, when I finally understood what was meant by \"all particles and fields that exist are nothing more than representations of the Poincare-Spacetime Algebra\", I felt like Neo when everything turned into strings of code. And there's no point describing what it was like when Einstein's field equations clicked, before then I never really got what people meant by the beauty of mathematics or physics.I guess its not really the answer \"why\" things are, but the way our current theories basically constrain nearly everything we see (at least from the bottom up) from a handful of axioms and cherry-picked coupling constants, the rest warped into shape and set in stone only by the self-consistency of mathematics, I feel like that's more of a \"why\" than I would've ever assumed answerable, and maybe more of one than I deserve.\n \nreply",
      "I only got as far as \"solved Schrodinger equation for a hydrogen atom\" and never got to the next stage you describe with physics.In a sense, I think your explanation is consistent with mine, but with the deeper context of math being a language itself, and the math itself being a more satisfactory explanation to someone with a greater intuition for what the equations actually mean. I can pump through all of the major equations in physics and explain almost anything I want with them, but it always just feels like rote application of algebra rules to completely arbitrary seeming formulas- nothing like what you describe. Frankly, I think I was more interested in girls than studying when I was a physics student decades ago, and I could probably get a lot more out of it revisiting this stuff now.However, I do still think there is a real chance that we are missing something big that would fit all of these pieces together with qualitative explanations. Personally, I think Julian Barbour is likely on the right path with his timeless physics, but if so it will need a lot more research and development.\n \nreply",
      "> When I was a physics undergrad, most of my professors were fans of the \"shut up and calculate\" interpretation of quantum mechanics.Well, you build \"intuition\" via \"experience\"--generally lots of experience to get small amounts of intuition.> Usually the qualitative descriptions don't quite make sense if you think very hard about them- and if you dig deeper it's often just \"we found some math that fits our experimental data\"Well, the math needs to fit the data and have predictive power.  That \"predictive\" side is really important and is what sets \"science\" apart from everything else.> Ultimately, this is probably just a symptom of still not having yet discovered some really important stuff.Sure.  But wouldn't the world be incredibly boring if we had it all figured out?\n \nreply",
      "'Stiffness' is a better concept because it can be used to explain behavior of all forces of finite and infinite range and why force mediators can have mass or not.If you need to assume some axiomatic concept it's better to assume one that can used to derive a lot of what is observed.\n \nreply"
    ],
    "link": "https://profmattstrassler.com/articles-and-posts/particle-physics-basics/the-astonishing-standard-model/why-the-weak-nuclear-force-is-short-range/",
    "first_paragraph": "The \u201crange\u201d of a force is a measure of the distance across which it can easily be effective.  Some forces, including electric and magnetic forces and gravity, are long-range, able to cause dramatic effects that can reach across rooms, planets, and even galaxies.  Short-range forces tail off sharply, and are able to make a significant impact only at distances shorter than their \u201crange\u201d.  The weak nuclear force, for instance, dies off at distances ten million times smaller than an atom!  That makes its effects on atoms rather slow and rare, which is why it is called \u201cweak\u201d.   The difference between long-range and short-range is depicted schematically in Fig. 1.  The green object at center is potentially able to create a force on a second object, not shown.  The darkness of the shading at a particular location represents the strength of the force that the second object would be subjected to if it were placed at that location.  A long-range force would be still be rather strong even at the"
  },
  {
    "title": "Lambda Calculus in 383 Bytes (2022) (justine.lol)",
    "points": 230,
    "submitter": "MrBuddyCasino",
    "submit_time": "2025-01-13T01:53:18 1736733198",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=42679191",
    "comments": [
      "I've been attracted to this - along with 2D cellular automata - a bit like a moth to a flame for some time. I find the little machine visualisations mesmerising, the heavily parenthesized Greek representation charming (they look like standing orders written in an alien language, looking for all the world like space invaders) and the tiny code sizes magical.But I can't quite wrap my mind around the core concepts and internalize them into a mental model. It's too different from the simple world of imperative C or scripting languages I guess I call home. So I'm left watching das blinkenlights from the outside, as my attention span chokes on the layers of computer science incorporated into typical explanations. *shrug*I'd be very interested if anyone knows of an ELI5-style alternate path I could walk to break each of the concepts down one at a time. (I ask because I think this is (currently) the kind of thing I think ChatGPT would struggle to present as effectively as a human.)\n \nreply",
      "The best way to wrap your mind around the core concept and internalize them into a mental model is writing an interpreter yourself. It's been abundantly clear to me since young that for anything involving math, you don't internalize it if you merely passively let someone else explain it, whether that's reading a textbook/blog or attending a professor's lecture or watching a YouTube video. You have to do the exercises.Lambda calculus is the same. You can easily define the data structure to represent a program in untyped lambda calculus and then write an interpreter for it. Then go implement some interesting concepts such as the Y combinator or the Omega combinator. If you find lambda calculus too difficult to do things like arithmetic or linked lists, you don't have to stick with Church numerals or Scott encodings. Just introduce regular natural numbers and lists as ground types; when you later have a better understanding, write programs to transform regular numerals from and to Church numerals and bask in the fact that they are isomorphic.\n \nreply",
      "I think the most ELI5 approach is Alligator Eggs [0] which was built for 8-year-olds to play like a game. You can find a lot of the advanced concepts outside of the core also explained in terms of Alligator Eggs and some software visualizers, but there's also something to be said about hands on learning and about printing it out yourself on some cardstock or cardboard paper, cutting it out, personalizing it with crayons, and playing it with a child or at least your inner child.[0] https://worrydream.com/AlligatorEggs/\n \nreply",
      "It's too basic for what you need but the video from eyesomorphic [1], is a wonderful conceptual introduction[1] https://www.youtube.com/watch?v=ViPNHMSUcog\n \nreply",
      "> Whilst it certainly isn't a contender for modern programming languagesYet all that separates the \u03bb-calculus from one modern programming language, Haskell, is a layer of syntactic sugar on top, and a runtime that effectuates\nits pure IO actions. We can in fact compile Haskell programs using just stdin/stdout for IO into terms of the untyped lambda calculus, as wonderfully demonstrated in Ben Lynn's IOCCC entry [1], or equivalently, into BLC programs.[1] https://www.ioccc.org/2019/lynn/index.html\n \nreply",
      "For anyone who's interested - Ben Lynn also has a series of articles that explain the creation of that compiler and add further enhancements:https://crypto.stanford.edu/~blynn/compiler/\n \nreply",
      "> Yet all that separates the \u03bb-calculus from one modern programming language, Haskell, is a layer of syntactic sugar on top, and a runtime that effectuates its pure IO actions. We can in fact compile Haskell programs using just stdin/stdout for IO into terms of the untyped lambda calculus, as wonderfully demonstrated in Ben Lynn's IOCCC entry [1].That's what Turing completeness means, though; you can do the same thing with C, with the same provisos.  (Conal Elliott has an amusing satire on this: http://conal.net/blog/posts/the-c-language-is-purely-functio... .)  It's not that the lambda calculus isn't sufficiently expressive, just that it's not a language in which humans want to write.\n \nreply",
      "I wasn't just claiming Turing completeness of Haskell. I was pointing out that every language construct, every subexpression in Haskell, directly represents a corresponding lambda term, with corresponding semantics (e.g. laziness).\n \nreply",
      "> I wasn't just claiming Turing completeness of Haskell. I was pointing out that every language construct, every subexpression in Haskell, directly represents a corresponding lambda term, with corresponding semantics (e.g. laziness).I was referring to the Turing completeness of the lambda calculus, not of Haskell.  But, again, I think that trying to work directly with lambda expressions everywhere, even if it is possible and, as you say, straightforward for \"vanilla\" Haskell, quickly shows why we put some semantic sugar over it.  That is to say, it's certainly true that, in an obvious sense, the layer of semantic sugar is thinner for Haskell than for C, but it's still \"just\" semantic sugar, and still just as conceptually important, in both cases.\n \nreply",
      "video author is using 3b1b's manim (https://github.com/3b1b/manim).\nwonderful presentation.\n \nreply"
    ],
    "link": "https://justine.lol/lambda/",
    "first_paragraph": "\nFebruary 27th, 2022 @  justine's web page\nLambda Calculus in 383 Bytes\n\n\nThe Lambda Calculus is a programming language with a single keyword.\nIt's the Turing tarpit discovered by Turing's doctoral advisor. This\nblog post introduces a brand new 383 byte implementation of binary\nlambda calculus as an x86-64 Linux ELF executable. Friendly portable C\ncode and prebuilt APE binaries are provided\nfor other platforms too.\n\n\nSectorLambda implements a Church-Krivine-Tromp virtual machine with\nmonadic I/O. In 383 bytes we've managed to achieve garbage collection,\nlazy lists, and tail recursion. The interpreter works by extracting the\nleast significant bit from each stdin byte. Output consists of 0 and 1\nbytes. It's 64-bit however displacement is limited to [0,255] so\nprograms can't be too huge. That makes it a fun tool for learning, but\nfor more industrial scale applications a 520 byte version is provided\ntoo, that overcomes many of those limitations, although it requires\nwriting code at an even"
  },
  {
    "title": "Sweden brings more books and handwriting practice back to its schools (2023) (apnews.com)",
    "points": 291,
    "submitter": "redbell",
    "submit_time": "2025-01-15T19:34:01 1736969641",
    "num_comments": 201,
    "comments_url": "https://news.ycombinator.com/item?id=42715841",
    "comments": [
      "I went fully digital some years ago, gave away most of my printed books and bought ebooks only. Now I have my whole library in Calibre and on my Kindle. Why? Because I have my whole library with me. And I can download my highlights and process them. Into notes in Obsidian, that I can link to in my study notes.Recently I started buying paper based books again. Man, I missed holding physical books in my hands. And I start to regret having gotten rid of my physical library. There were so many memories I had with most of these books. I remember their covers, and instantly my emotions , thoughts, feelings are triggered. I don\u2019t have these emotions when I think of my digital books.My spouse has books that she was gifted when she was a child. Still in our kids shelf. I cannot give her my digital books.I regret the decision having gone fully digital, which can only be a complement to physical books.Printed books are a physical experience. Something that allows me to attach thoughts, emotions, feelings to it. And they can become part of my life. Like a good friend.\n \nreply",
      "And let's be honest, a good book collection is a great addition to a room, aesthetically. People tend not to talk about that aspect, I think they worry about being seen as pretentious showing off their books. But I think a book collection can be a great decoration, just as flowers or a painting can be.And if you have family or friends over and one of them sees something they like, you can lend it to them there and then (if you are so inclined). Some of my earliest reading-related memories are being in an uncle's or neighbour's house and being fascinated by a book on a shelf that they kindly let me take home to read.\n \nreply",
      "While I agree with the sentiment, I have hesitation in letting people see what I read.In a way, you're letting people see the nature of things that you read - from which they might glean the nature of your thoughts, and privacy is something we all value. For that reason (and since I don't have any particular sentimental value for books, only their contents) I've long since preferred a digital library. As a minimalist, having a single Kindle on the table is aesthetics enough for me, which is complementary of the minimalist viewpoint as well.However, I completely agree with the fact that having a physical library is a very conducive environment for kids to grow up with. I remember fun memories of  my childhood reading from the home library, and thinking how pretty and colourful the shelves were too. But I think there should be a distinction between cultivating a library for your kids, versus that for the observation and assessment of strangers.\n \nreply",
      "How do you get along with communicating to others?\n \nreply",
      "And they improve room acoustics a decent amount, making the space that much more pleasant.\n \nreply",
      "Are books like a natural version of those fancy futuristic sound panels in recording studios?\n \nreply",
      "Yeah, diffusers. Smears first reflection time. Probably some low end absorbtion too.\n \nreply",
      "I will not buy DRMed ebooks. I hate the idea that someone can delete a book I bought. Once I have a book, I want to keep it.I have quite a lot of books that belong to be grandfather, and lots that belonged to my parents. A lot of those will last another generation, maybe more. That does not happen with ebooks either.\n \nreply",
      "It is trivially easy to remove DRM with a plugin for Calibre.\n \nreply",
      "Not with the latest encryption. Although you can always screenshot and ocr. Or maybe I've missed something new.\n \nreply"
    ],
    "link": "https://apnews.com/article/sweden-digital-education-backlash-reading-writing-1dd964c628f76361c43dbf3964f7dbf4",
    "first_paragraph": ""
  },
  {
    "title": "Google is making AI in Gmail and Docs free, but raising the price of Workspace (theverge.com)",
    "points": 125,
    "submitter": "lars_francke",
    "submit_time": "2025-01-15T14:15:17 1736950517",
    "num_comments": 169,
    "comments_url": "https://news.ycombinator.com/item?id=42710978",
    "comments": [
      ">Workspace AI includes things like email summaries in Gmail, generated designs for spreadsheets and videos, an automated note-taker for meetings, the powerful NotebookLM research assistant, and writing tools across apps.Maybe I'm just an old curmudgeon stuck in my ways, but I haven't found much compelling value in these use cases in my day-to-day work. For summaries and note-taking specifically, I feel they're solving the wrong problem: it's not that I have all this information that I really want to go through, but it's that I have too much information and it's become all noise.The real solution to too much email is fewer and higher-priority emails. The real solution to too many meetings is fewer and more-focused meetings. These tools paper over the root cause of the problem, which is that people/organizations cannot (or are unwilling to) be clear about communication priorities and say \"maybe this email/meeting isn't a good use of time after all.\"\n \nreply",
      "How is AI in email a good thing?!There's a cartoon going around where in the first frame, one character points to their screen and says to another: \"AI turns this single bullet point list into a long email I can pretend I wrote\".And in the other frame, there are two different characters, one of them presumably the receiver of the email sent in the first frame, who says to their colleague: \"AI makes a single bullet point out of this long email I can pretend I read\".\n \nreply",
      "I think this underrates how many emails are literally just replies of \"sounds good\". Small snippet replies seem to be the vast majority of automatically suggested responses in gmail\n \nreply",
      "It almost can't be a good thing. LLMs are only useful when given all the relevant context. When you write an email, the context is mostly in your head.\n \nreply",
      "It isn't, though; it's in all the meetings that happened beforehand and all the documents around them.The biggest productivity boost I ever managed was using Whisper to convert meetings to text and then a big model to summarize what happened.Then I can chat with the docs and meetings about who decided what, when, and why. It's a superpower that I could only implement because I'm in the C-suite and could tell everyone else to get bent if they didn't like it\u2014and gave babysitters to the rest of the C-suite.Having visibility and ownership for decisions is a huge deal when everyone has access to it.\n \nreply",
      "I like this version of the same joke (unfortunately no idea what the source is): https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fw...\n \nreply",
      "This was literally in the initial gmail demo about AI :D\n \nreply",
      "Really? Wow. And they think if they're pointing it out, it absolves them somehow? Like those companies that used to have Dilbert cartoons pinned on cubicle walls?\n \nreply",
      "Do you happen to have a link for that comic?\n \nreply",
      "Not the person you asked, but I too enjoy good web comics.https://marketoonist.com/2023/03/ai-written-ai-read.html\n \nreply"
    ],
    "link": "https://www.theverge.com/2025/1/15/24343794/google-workspace-ai-features-free",
    "first_paragraph": "By  David Pierce, editor-at-large and Vergecast co-host with over a decade of experience covering consumer tech. Previously, at Protocol, The Wall Street Journal, and Wired.If you wanted to use all of Google\u2019s AI features inside Gmail, Docs, Sheets, Meet, and the rest of the Workspace suite, you previously needed to pony up another $20 per user per month for the Gemini Business plan. As of Tuesday, it\u2019s free. Google is bringing all its AI features to its Workspace app at no extra cost as it continues to race Microsoft, OpenAI, and others to build the AI-powered office suite of the future.There is a catch, though: as it makes this change, Google is increasing the price of all Workspace plans. Jerry Dischler, Google\u2019s president of cloud applications, tells me companies will pay roughly $2 more per month per user for the AI-enabled Workspace than they were paying before. (The numbers aren\u2019t exact, because companies have complicated and varying contracts, but the base subscription price wa"
  },
  {
    "title": "Modern JavaScript for Django developers (saaspegasus.com)",
    "points": 148,
    "submitter": "rob",
    "submit_time": "2025-01-15T14:49:28 1736952568",
    "num_comments": 85,
    "comments_url": "https://news.ycombinator.com/item?id=42711387",
    "comments": [
      "This is an excellent article, and SaaS Pegasus is a great solution for people starting a project.But some of the advice here is dated. The architectural patterns are still valid, but the specifics have changed:* Vite instead of create-react-app (which is unmaintained now) / webpack / babel /  Parcel / etc.* Django-ninja as a lightweight API service.I think these are important to call out because they really simplify the frontend compared to the previous options.\n \nreply",
      "I agree with you on Django Ninja, so refreshingly simple compared to DRF. I think Django core needs to adopt something like it.However, Vite is pretty complicated. I prefer just esbuild if I don't need all the extra features of Vite, which is usually true with Django. I wrote a post[0] with an example repo[1] if anyone wants to see how everything wires up.With Solidjs, the minimum JS payload is around 9kb, and you get access to the whole JS ecosystem if you want it.[0] https://blopker.com/writing/07-django-islands-part-1/\n[1] https://github.com/blopker/typesafedjango\n \nreply",
      "> I agree with you on Django Ninja, so refreshingly simple compared to DRF. I think Django core needs to adopt something like it.I was going to ask about this with respect to DRF, but you answered it.  I am re-learning Django after having been away from it and Python for ~4 years now, and my previous experience was with DRF in a somewhat toxic group so I had less than ideal feelings about it.  I know PTSD is a real thing and I don't mean to sound glib about it, but I think I actually had the beginnings of it from that experience.\n \nreply",
      "What seems to differentiate django-ninja over Flask or FastAPI or any Starlette derivative? You mention lightweight as well, can you expand further?\n \nreply",
      "Ninja lets you use django. There's less config vs DRF\n \nreply",
      "Aside from the obvious that ninja let's you use django.\n \nreply",
      "The ability to use django is the main attractor. The other frameworks are great but make you reinvent Django if you require auth, orm, admin, etc\n \nreply",
      "I've tried several boilerplates like SaaSPegasus and one thing I can't really get around is that I feel like the experience of developing in a docker-compose with two build-and-serve containers (e.g. one with gunicorn auto-reload and the other running something like esbuild for the frontend) is very clunky in VSCode?I feel like I'm doing something crazy, this must be a problem many other people have, but things like language server integration on the JS and Python side separately do not mesh well.If anyone sees this and has a minimal open source boilerplate to recommend I'd love to try it.\n \nreply",
      "So I actually recently dealt with this, sharing this as hopefully it helps you.https://github.com/ospira/docker-django-react-exampleIn essence, you need two instances of VSCode running connected to two separate Docker container instances. As I understand it, it's one remote container per VSCode window. Thus, I found this to be best, even though it isn't strictly speaking necessary, but it ends up feeling that way because as you said the language server integration (intellisense and extensions) will not work properly if not connected to the right container.If you load this up in vs code it should prompt you properly given the presence of the files in `.devcontainter` dir. Having two windows in VSCode is kind of annoying at first, but I found it was actually fine, especially on macOS where tabbing to the other VSCode window (as opposed to ungrouped alt+tab on windows) was painless, and also kept me more organized not having backend and frontend code right next to each other.\n \nreply",
      "Btw, two addendums:1. I fixed some things in that repo, now it should work out of the box. Apologies if the initial version had some bugs, was taking it out of another project, and the first effort at cleaning it up was too hasty. Note it is still however just meant as an example.2. You actually can run more than one container per window - see here https://code.visualstudio.com/remote/advancedcontainers/conn.... However, I opted for the double window method because I found that cleaner than toggling between in one window. In my template I assume the two windows method because it will load up the proper subfolder (django or react) of the workspace/monorepo depending on which dev container you connect to.\n \nreply"
    ],
    "link": "https://www.saaspegasus.com/guides/modern-javascript-for-django-developers/",
    "first_paragraph": ""
  },
  {
    "title": "Code reviews: A success story (blogsystem5.substack.com)",
    "points": 38,
    "submitter": "mu0n",
    "submit_time": "2025-01-13T11:02:32 1736766152",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=42682203",
    "comments": [
      "While I am a proponent of code reviews, what this article actually described is mentoring of a junior engineer by a senior engineer, and requiring effective testing to be implemented alongside the feature.It also shows a broken culture where the other reviewers were not engaged and committed to the success of the feature. When a bug escapes, it's both the author _and_ reviewer at fault.\n \nreply",
      "Disagree. It's only the author's fault. We can't expect engineers to write code and find every last bug in other people's code especially when PRs can be thousands of lines to review.A broken culture would be expecting engineers to find every last bug in other people's code and blaming them for bugs.\n \nreply",
      "He's obviously self-praising for a promotion or looking for another job. I don't think it should be taken seriously on the matter of the effectiveness of code-reviews\n \nreply",
      "They are not antagonistic in nature! Where did they get this idea?\n \nreply",
      "The author describes how his code reviews that he gave others are successful from his own point of view.\n \nreply",
      "But he does back it up with actual facts (as far as we can trust the author to tell the truth) - the feature that the author gave feedback on shipped without any issues. (The article actually doesn't say whether A was fixed-and-bug-free before B shipped, but it certainly sounds like B was less stressful to ship.)\n \nreply",
      "Don't worry, he also asked his own reviewee, who said the reviews were helpful and in no way obnoxious.\n \nreply",
      "My statement still stands true regardless. Not worried.\n \nreply",
      "> I also pushed for breaking large changes into smaller commits, because it is impossible to do a thorough review otherwise.I've found this to be key for both giving and receiving feedback. Even small breaking commits are better in a lot of cases because they invite feedback in a way that 1000+ lines don't.\n \nreply",
      "Breaking down the size of the change is truly important, otherwise it's easy to miss things and to also disregard them as little details when wanting to avoid blocking the whole change on a \"small\" thing (which may only seem small because the PR is now huge)\n \nreply"
    ],
    "link": "https://blogsystem5.substack.com/p/code-reviews-a-success-story",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: I built a fair alternative to Product Hunt for indie makers",
    "points": 97,
    "submitter": "lakshikag",
    "submit_time": "2025-01-15T16:09:07 1736957347",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=42712666",
    "comments": [
      "I really wish Ryan Hoover would take back the integrity of Product Hunt. It's such an amazing product with such currently painful execution, assumingly in the name of site traffic traded for ad dollars.I truly think that the conversion rate for advertisers on PH would go UP if the quality of the site (moderated posts, comments, bot traffic) did the same.\n \nreply",
      "I'm consistently baffled by the rarity of a product owner improving their bottom line by simply improving their users' experience.\n \nreply",
      "I left the site even before Ryan left, but yeah, he was the best person to steward it at forward. It\u2019s been junk for a very long time.\n \nreply",
      "Whatever happened to him that he gave up such a valuable resource for the community? I don't think it can be saved at this point though.\n \nreply",
      "Hey! I am launching my product on your site! Overall I think its really well made, 2 small things:1) In categories it says (1) even for things that don't have a single launch listed when clicked maybe cuz there is an upcoming launch for the category, but none yet? IDK why, but just to let you know\n2) Confirming the launch date (alert) said it was for one day before the one I selected, then on the confirmation page though it had the correct date.I hope your site takes off! GL! ;)\n \nreply",
      "Another related aspect: it\u2019s likely that tech hype sphere will not actually make much of a difference unless you\u2019re selling to those people directly. My app Payload got featured in fastcompany, and I thought that was amazing. It drove traffic to the website and I was just waiting for the users\u2026 that didn\u2019t come. And then a few days later back to normal.On the other hand, the less prestigious tech blogs for regular people (think PC magazines) were much better at driving both real users and also traffic.Anyway, the point is that your customers might not be on product hunt checking out the coolest newest hypiest products. In fact, it\u2019s very unlikely they are. Just a reminder to not take these games so seriously.\n \nreply",
      "This is awesome! Congrats on the metalaunch ;-) I found the site hard to navigate visually as everything was equally prominent (in fact, yesterday's launches pop more than the current ones right now), so I took a stab at a different layout.Screenshot: https://i.imgur.com/4qoY7o2.pngCode here: https://gist.github.com/airstrike/923a7049d5cde7405e60e99e22...\n \nreply",
      "Looks great! Hope to see more submissions to HN from this site! Congrats on the launch!\n \nreply",
      "I really like the direction you\u2019re taking. Product Hunt can feel like it\u2019s run more for the benefit of the maintainers than for the community. It\u2019s their service, so fair enough, but it also means users sometimes lose out. And let\u2019s face it\u2014there\u2019s definitely some gaming of the system going on.Your approach seems promising. Have you considered taking it even further, maybe by making the platform more decentralized or democratized, kind of like a DAO (Decentralized Autonomous Organization)? That might align the incentives more directly with the indie maker community and help keep everything transparent.\nIn any case, I\u2019m glad to see new ideas that give smaller products and teams a fair shot.Keep at it!D\n \nreply",
      "I see a new product-hunt alternative launched every couple months here. Maybe I\u2019m cynical, but I don\u2019t think we\u2019re going to displace product hunt with things like new voting dynamics. They already have the network effects, so I think you\u2019d need to make a relatively large change to stand a real chance.Edit: Here\u2019s a proposal for a bigger change. do some free advertising for the submitted ideas. Run simple Google/youtube/facebook ads for them, just directing people to their page on your platform. Hopefully this doesn\u2019t burn too much cash, since you\u2019re actually advertising for their page on your platform, so it\u2019s good for you in the end. Perhaps submissions have a small fee in the long-term, to monetize the platform.\n \nreply"
    ],
    "link": "item?id=42712666",
    "first_paragraph": ""
  },
  {
    "title": "Empirical Health (YC S23) is hiring interns to build data-driven primary care (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-01-15T21:00:41 1736974841",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/empirical-health/jobs/BQlfWbt-software-engineer-intern-summer-2025",
    "first_paragraph": "Proactive primary care, scaled with AIWe're building the future of primary care. 99% of your health is defined outside the doctor\u2019s office. Empirical helps users manage it.Empirical Health is a primary care provider that uses artificial intelligence and health sensors on wearable devices (Apple Watch, Fitbit, Google Pixel, Samsung watch and more) to scale proactive primary care to everyone. We're building a product that gives every user an AI-powered personal health assistant, backed by a real human doctor who can provide actual diagnosis, treatment, and care. \n\nIn this role you will work with our doctors and interact with real patients to build the tools needed to keep them healthy. Do you want to use your software engineering and design skills to save lives and make healthcare better for everyone? At Empirical Health, we're inventing the future of primary care \u2013 and we want you.Software engineer interns are responsible for a machine learning or full stack project that will have an im"
  },
  {
    "title": "Israel, Hamas reach ceasefire deal to end 15 months of war in Gaza (reuters.com)",
    "points": 139,
    "submitter": "dnsbty",
    "submit_time": "2025-01-15T20:26:49 1736972809",
    "num_comments": 202,
    "comments_url": "https://news.ycombinator.com/item?id=42716440",
    "comments": [
      "Everything else aside, this is an absolutely fantastic development and I really hope the ceasefire holds and all hostages are released.I just fear this will cause western media and politicians to and declare the crisis to be over (after it had began on Oct. 7, of course absolutely out of the blue and without any context...) and go back to pretending everything is back to normal. Never mind that Gaza is still in ruins, the west bank is still being annexed, Israel still has the dual role of \"all authority, no obligations\" over the Palestinians, while  making it pretty clear they have no vision for them at all, apart from \"maybe they just vanish into thin air tomorrow\".And never mind that Israel still has a fundamentalist, authoritarian government that is actively at work undermining democratic structures and civil rights even inside the state - that too with no word of objection from its allies.We'll see where all of that goes.I also found Trump's signalling in the whole issue odd. His base and his cabinet is full of the most hard-line pro-israel figures imaginable, but then he goes forward and quotes Jeffrey Sachs and ostensibly pressures Netanyahu into accepting the ceasefire.Is this just his usual \"appear unpredictable by all means\" spiel or does he have a strategy there?\n \nreply",
      "While this is a good development.  Everything in this part of the world is on a rinse snd repeat cycle ever since the Assyrians and the Babylonians - it hasn't changed much except maybe its actually a little more humane then it was in the past (which says something).  Sorry for the cynical take but this  just does a temporary stop.\n \nreply",
      "> ostensibly pressures Netanyahu into accepting the ceasefireThere is no evidence of this.Every single time Trump has blustered about doing something e.g. turning Canada into a 51st, buying Greenland the parties have been concerned but not particularly worried. Because he doesn't follow through.So the idea that we should credit Trump for his words and ignore the months of diplomacy and pressure from not just the US but Middle Eastern countries is bizarre to me. Ceasefires are always far more complex and nuanced than they look from the outside.\n \nreply",
      "https://www.timesofisrael.com/arab-official-trump-envoy-sway...\n \nreply",
      "Something positive about Trump?  Must be \"Russian disinformation\" or whatever we're saying these days...",
      "I got it from here: https://www.haaretz.com/israel-news/2025-01-13/ty-article/.p...\n \nreply",
      "Diplomacy is a lie, there's only military intelligence.\n \nreply",
      "Multiple sources are crediting months of work by Brett McGurk as the lead in this.   This is Biden admin accomplishment.\n \nreply",
      "One not particularly obscure theory is that Netanyahu was prioritizing Trump coming to power over a peace/hostage deal and now that Trump has power, Netanyahu seeks to benefit from prioritizing the hostages. Trump is claiming credit for it and probably doesn't care about the timing.\n \nreply",
      "Netanyahu was simply pushing his opportunity to do what Israel hardliners have wanted to do for as long as possible (basically aggressively lash out in every direction without consequences and red lines). It was always going to need to be wrapped up, even within Israel there was strong internal pressure. Waiting until is Trump is coming in gives Israel a free golden ticket with him by timing it right and Netanyahu's careers basically over after this anyway, so he has nothing to lose by doing it earlier, absent internal revolt.\n \nreply"
    ],
    "link": "https://www.reuters.com/world/middle-east/gaza-ceasefire-appears-close-us-egyptian-leaders-put-focus-coming-hours-2025-01-14/",
    "first_paragraph": ""
  },
  {
    "title": "First AI-generated and 3D-printed shoe makes its dubious debut (newatlas.com)",
    "points": 4,
    "submitter": "teleforce",
    "submit_time": "2025-01-12T04:11:16 1736655076",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42671221",
    "comments": [
      "That looks awful\n \nreply",
      "Looks about the same as all the other foam slides. I think they're all very ugly as well. Adidas foam runners actually gross me out\n \nreply",
      "Is this an ad?\n \nreply",
      "I'm sure they spent money to have the article written/published. If it were a legit article, it checks a lot of boxes for this crowd with AI and 3D printed.\n \nreply"
    ],
    "link": "https://newatlas.com/lifestyle/syntilay-worlds-first-ai-generated-3d-printed-shoe/",
    "first_paragraph": ""
  },
  {
    "title": "Nobody cares (grantslatton.com)",
    "points": 267,
    "submitter": "fzliu",
    "submit_time": "2025-01-15T04:15:43 1736914543",
    "num_comments": 274,
    "comments_url": "https://news.ycombinator.com/item?id=42707238",
    "comments": [
      "100% of the people around me at work care.I wish they didn't, because they're bad at their job and \"them caring\" puts them as a peer for experts and people who both care AND are competent/experienced via design by committee and inclusion. Their incompetency is explained away as \"unique point of view.\"So perhaps the entire piece is an exercise in overgeneralization, where you assume that everyone has a baseline amount of competency. That curb could have been designed by a very caring intern, who is awful at what they do. They were managed by someone who had 100 other deadlines that are more important. They care about that curb, but they care about 100 other things with more priority.We're in the era of Good Enough.I find it's an impossible thought experiment to judge doing 100 things Good Enough is better/worse than doing 1 thing perfectly and ignoring 99 other things. Add a token / currency to the mix, costs + returns on investment. And now you have something substantial to judge.There is a massive difference between actively not caring and passively omitting attention.Peppered into the diatribe is direct, aggressive, not caring. But that doesn't validate the general stance.Make a consultancy called Caring Company that makes companies/products/projects more efficient at same or less cost.My institution has hired multiple consultancies to fix structures and form new ones... the entropy of pay grade and how to prioritize thousands of tasks in parallel doesn't \"get solved\" because someone finds that some employee is just bad at what they do. And what do you do when you find you can only hire those employees because you don't pay enough for better, because your products' incomes don't match the skill level required?\n \nreply",
      "Is this an AI response? Has the dead internet lured me in, again? Or, more likely, do you just not care as well?Every example in the linked post is either \"not caring\" about the work being done OR aggressively \"not caring\" due to main-character syndrome/individualism of modern American society. AND on top of it, every political fix is a _feel good_ fix instead of actually fixing the fucking problem.An \"era of good enough\" makes no goddamn sense in response to this article. NONE of the things listed are good enough. None of them.\n \nreply",
      "Thanks for the domain name suggestion.TheCaringCompany.com was taken but a good enough variant wasn\u2019t and I got it.Thank you!\n \nreply",
      "I would argue that incompetence is a form of not caring.It means that one just does, maybe even more then necessary because one doesn\u2019t actually understand what their responsibilities are. And to be not detected it\u2019s better to seem very busy and very caring.\n \nreply",
      "> I would argue that incompetence is a form of not caring.It is not.It can be a product of not caring, and what is actually not caring can be mistaken for incompetence, but incompetence can coexist with dedication (the idea that it cannot seems is a face of the \"effort is all that matters, there are no real differences in capabilities\" myth), competence and concern are not at all the same thing or inherently linked such that either necessary implies the other.\n \nreply",
      "One man's incompetence is another man's profound skill. OK maybe not actually, but let's just say that some people are quick to apply a label of \"incompetent\" to people who think a little differently, or who are perhaps only 10% less knowledgeable, or to people they imagine are less knowledgeable.\n \nreply",
      "> One man's incompetence is another man's profound skillOnly when there's no way to measure the results.\n \nreply",
      "Measuring results is notoriously hard in this industry. Any metric can be easily manipulated, and many qualitative aspects of software are not quantifiable. Moreover, the people who get to decide the metrics will tend to choose them in a way that gives an advantage to themselves.\n \nreply",
      "Maybe some fraction of incompetent interns are playing a kind of double game, where they merely pretend to be really caring.But I doubt that\u2019s the norm. There really are a lot of not so smart people of all ages out there in positions way beyond their actual capability.Edit: And in a lot of situations the dumb and hard working are way more dangerous than the smart and lazy.With the dumb and lazy being somewhat better, so I partially agree with the parent.\n \nreply",
      "In my 15 years, I\u2019ve had a lot of interns, and a lot of indirect interaction with other interns. I can usually spot a genuine one in about a day at this point.\n \nreply"
    ],
    "link": "https://grantslatton.com/nobody-cares",
    "first_paragraph": ""
  },
  {
    "title": "Sky-scanning complete for Gaia (esa.int)",
    "points": 142,
    "submitter": "sohkamyung",
    "submit_time": "2025-01-15T09:43:03 1736934183",
    "num_comments": 39,
    "comments_url": "https://news.ycombinator.com/item?id=42709105",
    "comments": [
      "Direct link to some very very nice images and animations: https://www.esa.int/Science_Exploration/Space_Science/GaiaTwo of my favorites:\nhttps://www.esa.int/ESA_Multimedia/Images/2025/The_best_Milk...https://www.esa.int/ESA_Multimedia/Images/2025/01/The_best_M...\n \nreply",
      "I get how Gaia could make the best edge on image, but how could Gaia (or anything man made) get the the \"best\" face on image?\n \nreply",
      "It can't. The galaxy is assumed to be roughly symmetrical, and they fill in the missing data with what we can see on our side of the galaxy. It's \"best\" in the sense that it's the most accurate fiction, I suppose.\n \nreply",
      "The whole purpose of Gaia is to precisely measure the position of stars (and other objects). Once positions are known, a 3D model can be built. But how are the distances measured? The answer is parallax, essentially triangulation. You look for very small changes of position against the background sky. You use the width of the earth's orbit as the baseline and measure at different times of the year.\n \nreply",
      "All of these are \"Artist's Impressions\". My best guess is they run a simulation based on the data from the spacecraft and then can pan the camera around as they see fit\n \nreply",
      "\"The best Milky Way map, by Gaia (edge-on)\"The \"by Gaia\" implies the opposite to me. Unless the \"artist's impressions\" are from someone named Gaia???\n \nreply",
      "From the page:[Image Description: A model image of what our home galaxy, the Milky Way, might look like edge-on, against a pitch-black backdrop. The Milky Way\u2019s disc appears in the centre of the image, as a thin, dark-brown line spanning from left to right, with the hint of a wave in it. The line appears to be etched into a thin glowing layer of silver sand, that makes it look as if it was drawn with a coloured pencil on coarse paper. The bulge of the galaxy sits like a glowing, see-through pearl in the shape of a sphere in the centre of this brown line.]\n \nreply",
      "Gaia has a 1.0 \u00d7 0.5 m focal plane array on which light from both telescopes is projected. This in turn consists of 106 CCDs of 4500 \u00d7 1966 pixels each, for a total of 937.8 megapixels.Neat.\n \nreply",
      "The really neat part is the instrument precision. It's terrifyingly good and I have no idea how it (really) works.- \"Gaia measures their positions to an accuracy of 24 microarcseconds, comparable to measuring the diameter of a human hair at a distance of 1000 km\"https://www.esa.int/Science_Exploration/Space_Science/Gaia/C...\n \nreply",
      "To nitpick with the grammar in the quote: It's capable of measuring to the accuracy of 120 \u03bcm at 1000 km. So it cannot accurately measure the diameter of a human hair (which ranges from around 20 to 200 \u03bcm) at that distance, but only to the accuracy of a human hair.\n \nreply"
    ],
    "link": "https://www.esa.int/ESA_Multimedia/Images/2025/01/Sky-scanning_complete_for_Gaia",
    "first_paragraph": "Thank you for likingYou have already liked this page, you can only like it once!ESA\u2019s Milky Way-mapper Gaia has completed the sky-scanning phase of its mission, racking up more than three trillion observations of about two billion stars and other objects over the last decade to revolutionise our view of our home galaxy and cosmic neighbourhood.Launched on 19 December 2013, Gaia\u2019s fuel tank is now approaching empty \u2013 it uses about a dozen grams of cold gas per day to keep it spinning with pinpoint precision: this amounts to 55 kg of cold gas for 15 300 spacecraft \u2018pirouettes\u2019.Gaia\u2019s catalogue is ever-growing, containing data on stars and other cosmic objects such as asteroids in our Solar System, exoplanets, binary stars and other galaxies.After the raw data are downlinked to Earth, ESA and the Gaia Data Processing and Analysis consortium prepare the data for scientific use, adding crucial information for their usage.Since the publication of the first Gaia data in 2016 and counting up t"
  },
  {
    "title": "Apache DataFusion (apache.org)",
    "points": 22,
    "submitter": "thebuilderjr",
    "submit_time": "2025-01-12T18:53:26 1736708006",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=42675804",
    "comments": [
      "I feel like I'm not the target audience for this. When I have large data, then I directly write SQL queries and run them against the database. It's impossible to improve performance when you have to go out to the DB anyway; might as well have it run the query too. Certainly the server ops and db admins have loads more money to spend on making the DB fast compared with my anti-virus laden corporate laptop.When I have small data that fits on my laptop, Pandas is good enough.Maybe 10% of the time I have stuff that's annoyingly slow to run with Pandas; then I might choose a different library, but needing this is rare. Even then, of that 10% you can solve 9% of that by dropping down to numpy and picking a better algorithm...\n \nreply",
      "You\u2019re right it isn\u2019t for you.It\u2019s largely for companies who can\u2019t put everything in a single database because (a) they don\u2019t control the source schema e.g. it\u2019s a daily export from a SaaS app, (b) the ROI is not high enough to do so and (c) it\u2019s not in a relational format e.g. JSON, Logs, Telemetry etc.And with the trend toward SaaS apps it\u2019s a situation that is becoming more common.\n \nreply",
      "I agree. The main reason I shared it is because I find it interesting as a library. I actually use it behind the scenes to build https://telemetry.sh. Essentially, I ingest JSON, infer a Parquet schema, store the data in S3 with a lookaside cache on disk, and then use DataFusion for querying.\n \nreply",
      "There is a cambrian explosion in data processing engines: DataFusion, Polars, DuckDB, Feldera, Pathway, and more than i can remember.It reminds of 15 years ago where there was JDBC/ODBC for data. Then when data volumes increased, specialized databases became viable - graph, document, json, key-value, etc.I don't see SQL and Spark hammers keeping their ETL monopolies for much longer.\n \nreply",
      "How does this compare/contrast to polars? Seems pretty similar, anybody tried both?\n \nreply",
      "DataFusion and Polars are like two sides of the same Rust coin: DataFusion is built for distributed, SQL-based analytics at scale, serving as the backbone for data systems and enabling complex query execution across clusters. Polars, on the other hand, is laser-focused on blazing-fast, single-node data manipulation, offering a Python-like DataFrame API that feels intuitive for exploratory analysis and in-memory processing.\n \nreply",
      "And the thing is - single node can still scale ridiculously high without the orchestration overheads of distributed stuff.You can do dual AMD 192 core CPU's (384 cores / 768 threads) with 9 TB of memory and a 24 disk SSD array in a 2U box.\n \nreply",
      "Exactly, datafusion is implied batteries included apache bigdata ecosystem.  \nPolars is chasing the Python Pandas crowd and uses python syntax, handy if you're already comfortable with ipython.\n \nreply"
    ],
    "link": "https://datafusion.apache.org/",
    "first_paragraph": "\n\n  ASF Links\n \n\n\n  Links\n \n\n\n  User Guide\n \n\n\n  Library User Guide\n \n\n\n  Contributor Guide\n \n\n\n  DataFusion Subprojects\n \n\n\nStar\n\nFork\nDataFusion is an extensible query engine written in Rust that\nuses Apache Arrow as its in-memory format.The documentation on this site is for the core DataFusion project, which contains\nlibraries and binaries for developers building fast and feature rich database and analytic systems,\ncustomized to particular workloads. See use cases for examples.The following related subprojects target end users and have separate documentation.DataFusion Python offers a Python interface for SQL and DataFrame\nqueries.DataFusion Ray provides a distributed version of DataFusion\nthat scales out on Ray clusters.DataFusion Comet is an accelerator for Apache Spark based on\nDataFusion.\u201cOut of the box,\u201d DataFusion offers SQL\nand Dataframe APIs,\nexcellent performance, built-in support for CSV, Parquet, JSON, and Avro,\nextensive customization, and a great community.\nPython Bindi"
  },
  {
    "title": "TikTok preparing for U.S. shut-off on Sunday (reuters.com)",
    "points": 485,
    "submitter": "xnhbx",
    "submit_time": "2025-01-15T12:57:31 1736945851",
    "num_comments": 1384,
    "comments_url": "https://news.ycombinator.com/item?id=42710339",
    "comments": [
      "\"I would literally write my social security number on a sticky note and stick it to Xi Jinping's forehead than go back to using Instagram Reels\"I saw this yesterday and it's hilarious but this is the feeling right now. TikTok has such a culture of authenticity and realness and Instagram is so phony and overly perfect (not to mention ads and so many bots and spam). It's like shutting down Reddit and telling everyone to go to LinkedIn.\n \nreply",
      "> TikTok has such a culture of authenticity and realnessI must live in another universe because it all feels fake.\n \nreply",
      "The algorithm is genuinely very good. That's why I deleted it.It's very addictive and not always just shoveling slop.I don't know if I can do it justice but there's something genuinely quite fresh about the AI stuff I see every now and again e.g. Anna from the red scare podcast shilling industrial glycine was a meme for a while. Very Land-ian. Neo-china...\n \nreply",
      "https://youtube.com/shorts/3FISFq_sCH8\n \nreply",
      "Can't watch YT shorts because their algorithm is too good also ...\n \nreply",
      "Same reason I never touched prismatic after its first load.https://en.wikipedia.org/wiki/Prismatic_(app)https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que...\n \nreply",
      "Your perception of TikTok likely depends on your TikTok for you page. If you spend time cultivating it, the algorithm will learn you like authenticity and show you more of it.This seems to be less true on YouTube and Reels unfortunately.\n \nreply",
      "The algorithm will spoonfeed you content that you perceive a certain way, whether that's true or not is a different story. Unfortunately for most people, all those hilarious situations that are not-so-obviously staged just fly over their heads as genuine. My wife is smart and well educated, but I even had to keep correcting her when she showed me videos that she believed were genuine.\n \nreply",
      "I think a lot of people fall for the feels authentic and confuse it with authentic.  Also a lot of people cant tell the difference.\n \nreply",
      "A lot of people are simply pessimists and will dismiss real authenticity because they don't have the tools to recognize it."
    ],
    "link": "https://www.reuters.com/technology/tiktok-preparing-us-shut-off-sunday-information-reports-2025-01-15/",
    "first_paragraph": ""
  }
]