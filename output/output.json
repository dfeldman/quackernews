[
  {
    "title": "Nitro: A tiny but flexible init system and process supervisor (vuxu.org)",
    "points": 137,
    "submitter": "todsacerdoti",
    "submit_time": "2025-08-22T19:06:29 1755889589",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=44988530",
    "comments": [
      "I would like a comparison with runit, which is a very minimal but almost full-fledged init system. I see many similarities: control directories, no declarative dependencies, a similar set of scripts, the same approach to logging. The page mentions runit in passing, and even suggests using the chpst utility from it.One contrasting feature is parametrized services: several similar processes (like agetty) can be controlled by one service directory; I find it neat.Another difference is the ability to initiate reboot or shutdown as an action of the same binary (nitroctl).Also, it's a single binary; runit has several.reply",
      "Leah Neukirchen is active member of the Void Linux community, I expect a lot of cross-pollination here. It would be really great if she could write up something how to use it for Void.reply",
      "I've gotten used to runit via Void Linux, and while it does the job of an init system, its UI and documentation leave something to be desired. The way logging is configured in particular was an exercise in frustration the last time I tried to set it up for a service.I wouldn't mind trying something else that is as simple, but has sane defaults, better documentation, and a more intuitive UI.reply",
      "Logging in runit seems simple (I don't remember running into problems), but indeed, the documentation leaves much to be desired. Could be a good thing to contribute to Void Handbook.reply",
      "runit doesn't always take care of services it manages in the same way as a proper init . From the man page:\"If runsvdir receives a TERM signal, it exits with 0 immediately\"reply",
      "This is by design.runsvdir receiving TERM should only happen when stage 2 is triggered to end.Once that happens, the individual runsv processes are still supervising their individual tasks and can be requested to stop through their respective control sockets. It's how standard stage 3 is implemented.reply",
      "I'm always torn when I see anything mentioning running an init system in a container. On one hand, I guess it's good that it's designed with that use case in mind. Mainly, though, I've just seen too many overly complicated things attempted (on greenfield even) inside a single container when they should have instead been designed for kubernetes/cloud/whatever-they-run-on directly and more properly decoupled.It's probably just one of those \"people are going to do it anyway\" things. But I'm not sure if it's better to \"do it better\" and risk spreading the problem, or leave people with older solutions that fail harder.reply",
      "Yes, application containers should stick to the Unix philosophy of, \"do one thing and do it well.\" But if the thing in your docker container forks for _any_ reason, you should have a real init on PID 1.reply",
      "is there any issue besides the potential zombies? also, why can't the real pid1 do it? it sees all the processes after all.reply",
      "Mostly just zombies and signal handlers.And your software can do it, if it's written with the assumption that it will be pid1, but most non-init software isn't.  And rather than write your software to do so, it's easier to just reach for something like tini that does it already with very little overhead.I'd recommend reading the tini readme[0] and its linked discussion for full detail.[0]: https://github.com/krallin/tinireply"
    ],
    "link": "https://git.vuxu.org/nitro/about/",
    "first_paragraph": "Nitro is a tiny process supervisor that also can be used as pid 1 on Linux.There are four main applications it is designed for:Nitro is configured by a directory of scripts, defaulting to\n/etc/nitro (or the first command line argument).Every directory inside /etc/nitro (or your custom service directory)\ncan contain several files:You may find runit\u2019s chpst useful when writing run scripts.Service directories ending in @ are ignored, however you can refer\nto parametrized services by symlinks (either in the service directory\nor as a log symlink), or start them manually using nitroctl.The part after the @, the parameter, is passed to the scripts as\nfirst argument.For example, given you have a script agetty@/run and a symlink\nagetty@tty1 -> agetty@, nitro will spawn agetty@/run tty1.  Upon\nrunning nitroctl up agetty@tty2, nitro will spawn agetty@/run tty2, even if it does not exist in the service directory.The lifecycle of a machine/container/session using nitro consists of\nthree phases.Firs"
  },
  {
    "title": "Show HN: JavaScript-free (X)HTML Includes (github.com/evidlo)",
    "points": 43,
    "submitter": "Evidlo",
    "submit_time": "2025-08-22T18:47:30 1755888450",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=44988271",
    "comments": [
      "Why didn't HTML imports stick around? https://web.dev/articles/importsreply",
      "As of the next version of Chrome, XSLT will be gated behind a flag.Google have also asked for it to be removed from the standard [0].[0] https://github.com/WHATWG/html/issues/11523reply",
      "> As of the next version of Chrome, XSLT will be gated behind a flag.Citation? That would greatly surprise me in its abruptness, and https://chromestatus.com/feature/4709671889534976 gives no such indication.reply",
      "It's kind of too bad XSLT didn't take off. It is quite complex, until you compare it to the complexity of what now solves this problem (e.g. a build step with React and webpack and javascript absolutely required on the client-side). As the OP ably demonstrates, XSLT provides a declarative, non-javascript, non-build way to solve the basic HTML component problem. Perhaps a devastating 0-day in V8 will make us really, really want an alternative to the current best practice.reply",
      "Whilst I can't be certain, I've been hearing that part of Google's want to move away from XSLT is two-fold - and relates to the idea of the security problem.Partly, there's increasing attacks against XML.And also, libxml2 has said \"no\" to security embargoes altogether. [0]They might well consider there to be 0-days waiting in XSLT.[0] https://news.ycombinator.com/item?id=44381093reply",
      "Yeah, I think that was what prompted this submission.All this has also reignited my idea for a compile-to-XSLT templating language, too \u2013 maybe I\u2019ll get to it finally this time; definitely if XSLT 3.0 gets into web standards: https://github.com/whatwg/html/issues/11578, https://news.ycombinator.com/item?id=44987552Also, I\u2019ve put together a simple XSLT playgroung a while ago! https://xsltbin.ale.sh/reply",
      "I find it bizarre that Google can just ask for a feature to be removed from standard and nobody bats an eye.reply",
      "If I understand correctly, Mozilla and Apple don\u2019t really want to support it either. And the reason for that is, the spec is still at XSLT 1.0, which is super old, and current implementations are effectively abandonware. Catch-22?reply",
      "so it's time to use XSLT morereply",
      "I used to do this in the 2000's era, there was a lot to love about it. \nAt the time though the IE engines were far more complete and less buggy than others with various XML features.reply"
    ],
    "link": "https://github.com/Evidlo/xsl-website",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        XSL Rendering Example\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.This is a simple example of using browsers' built in XSL support to build a website with common theming across all pages without any server-side code, static website generators, or Javascript.See the demo siteWhen you browse to index.xml (or any of the other XML files), the browser loads the template file given at the top of the XML.  This template file describes how to render the various custom tags in the XML as HTML.See advanced/ for more advanced examples of defining templates with fields, or using templates inside of other templates.\n        XSL Rendering Example\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page"
  },
  {
    "title": "The First Media over QUIC CDN: Cloudflare (moq.dev)",
    "points": 151,
    "submitter": "kixelated",
    "submit_time": "2025-08-22T18:24:16 1755887056",
    "num_comments": 74,
    "comments_url": "https://news.ycombinator.com/item?id=44987924",
    "comments": [
      "I tested the demo at https://moq.dev/publish/ and it's buttery as hell. Very impressive. Thanks for the great technology!Watching the Big Buck Bunny demo at https://moq.dev/watch/?name=bbb on my mobile phone leaves a lot of horizontal black lines. (Strangely, it is OK on my PC despite using the same Wi-Fi network.) Is it due to buffer size? Can I increase it client-side, or should it be done server-side?Also, thanks for not missing South Kora in your \"global\" CDN map!reply",
      "Horizontal black lines? Dunno what that could be about, we render to a <canvas> element which is resized to match the source video and then resized again to match the window with CSS.reply",
      "Doesn't show up on screen capture, but there's random rolling quickly flickering lines on my phone, kinda like from analog distortion on old tvsreply",
      "I have got same issue with the black linesreply",
      "I don't get the black lines on Android/Chrome but it doesn't respect my aspect ratio when I go full screen. Instead of adding black bars to the sides, it excludes the top and bottom of the video completely.reply",
      "I am bad at CSS.reply",
      "Managing aspect ratios in conjunction with managing a responsive page layout is one of the darker parts of CSS in my experience. You\u2019re not alone.reply",
      "Hi! Cloudflare MoQ dev here, happy to answer questions!Thanks for the award, kixelated. xDreply",
      "Hi, I've got one.Does your team have any concrete plans to reduce the TCP vs. QUIC diff with respect to goodput [1]? The linked paper claims seeing up to a 9.8% video bitrate reduction from HTTP/2 (TCP) to HTTP/3 (QUIC). Obviously, MoQ is based on a slightly different stack, so the results don't exactly generalize. I can imagine the problems are similar, though.(I find this stuff fascinating, as I spent the last few months investigating the AF_XDP datapath for MsQuic as part of my master's thesis. I basically came to the conclusion that GSO/GRO is a better alternative and that QUIC desperately needs more hardware offloads :p)[1]: https://arxiv.org/pdf/2310.09423reply",
      "QUIC implementations are definitely not tuned well in practice for 600Mbps flows on low latency, low loss networks, as the paper attests. But I don\u2019t think almost any uses of video streaming fit that bill. Even streaming 4K video via Netflix or similar is tens of Mbps. In general if you don\u2019t have loss or the need to rapidly establish connections, QUIC performance is not even theoretically better, let alone in practice.P.S. if there\u2019s a public link to your masters thesis - please post it! I\u2019d love to read how that shook out, even if AF_XDP didn\u2019t fit in the end.reply"
    ],
    "link": "https://moq.dev/blog/first-cdn/",
    "first_paragraph": "\npublished 8/21/2025 \ud83d\udea8 It\u2019s finally happening! \ud83d\udea8Cloudflare has just announced their Media over QUIC CDN!\nIt\u2019s an official product, and you can test MoQ on their massive, anycast network.\nTry it out, and convince your boss\u2019 boss that the writing is on the wall.If you\u2019ve been living under a rock, MoQ is an up-and-coming standard for live media, aiming to supplant WebRTC, HLS/DASH, and even RTMP/SRT as the one to rule them all.\nAnd now Cloudflare wins the award for the first CDN offering!\nYour prize is a blog post. You\u2019re welcome mega-corp.Also, while you\u2019re here, some shameless self-promotion: I just soft-launched hang.live.\nCheck it out if you want to see the cringe cool stuff you can do with MoQ.This is a technical preview, so it\u2019s both free and subject to change.Cloudflare is hosting a public relay.cloudflare.mediaoverquic.com endpoint that you can abuse test.\nConnect using my library, Mike\u2019s fork, Lorenzo\u2019s imquic, Meta\u2019s moxygen, or any client that supports this limited subset of dr"
  },
  {
    "title": "Top Secret: Automatically filter sensitive information (thoughtbot.com)",
    "points": 52,
    "submitter": "thunderbong",
    "submit_time": "2025-08-22T04:48:29 1755838109",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=44981088",
    "comments": [
      "I'm no ruby expert, so forgive my ignorance, but it looks like a small \"NER model\" packaged as a string convince wrapper named `filter` that tries to filter out \"sensitive info\" on input strings.I assume the NER model is small enough to run on CPU at less than 1s~ per pass  at the trade off of storage per instance (1s is fast enough in dev, in prod with long convos - that's a lot of inference time), generally a neat idea though.Couple questions;- NER doesn't often perform well in different domains, how accurate is the model?- How do you actually allocate compute/storage for inferring on the NER model?- Are you batching these `filter` calls or is it just sequential 1 by 1 callsreply",
      "Oh hey! Good to see this. I built something similar in python a while ago.Check it out: https://github.com/deepanwadhwa/zinkThe shield functionality fits directly in your LLM workflow.reply",
      "When I had to implement \"deidentification\" for a kind of sensitive safety reporting, an LLM would've been a good way to augment the approaches I used.Today, if I had to do it, I'd probably throw multiple computer approaches at it, including LLM-based one, and take the union of those as the computer result, and check it against a human result.  (If computer and human agree, that's a good sign; if they disagree, see why before the document goes where it needs to be deidentified.)(In some kinds of flight safety reporting, any kind of personnel can submit a report about any observation related to safety.  It gets very seriously handled and analyzed.  There are also multiple ways in which the reporting parties are protected.  There are situations in which some artifacts need to have identifying information redacted.)reply"
    ],
    "link": "https://thoughtbot.com/blog/top-secret",
    "first_paragraph": "Work alongside the thoughtbot team as we collaborate with each other and our clients, live. Ask us anything, we're live right now!Work alongside the thoughtbot team as we collaborate with each other and our clients, live. Ask us anything, we're live right now!We\u2019ve written about how to prevent logging sensitive information when making\nnetwork requests, but that approach only works if you\u2019re dealing with\nparameters.What happens when you\u2019re dealing with free text? Filtering the entire string may\nnot be an option if an external API needs to process the value. Think chatbots or LLMs.You could use a regex to filter sensitive information (such as credit card\nnumbers or emails), but that won\u2019t capture everything, since not all sensitive\ninformation can be captured with a regex.Fortunately, named-entity recognition (NER) can be used to identify and\nclassify real-world objects, such as a person, or location. Tools like MITIE\nRuby make interfacing with NER models trivial.By using a combination o"
  },
  {
    "title": "Computer Fraud Laws Used to Prosecute Leaking Air Crash Footage to CNN (techdirt.com)",
    "points": 44,
    "submitter": "BallsInIt",
    "submit_time": "2025-08-23T00:04:56 1755907496",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.techdirt.com/2025/08/22/investigators-used-terrible-computer-fraud-laws-to-ensure-people-were-punished-for-leaking-air-crash-footage-to-cnn/",
    "first_paragraph": "\n\n\t\t\t\tLegal Issues\t\t\t\nEarlier this year, an Army helicopter collided with a passenger plane over the Potomac River in Washington, DC. All sixty-seven people aboard both vehicles were killed. While the FAA focused its investigation on the failures that led to this mid-air collision, local investigators in Virginia were somehow far more concerned about identifying who had leaked footage of the collision to CNN. The subject matter of the leaked recordings was obviously of public interest. And while the government may have its own interest in controlling dissemination of recording of incidents that involve federal agencies and their oversight, it\u2019s not the sort of government interest most courts consider to be worthy of violating the First Amendment.Fortunately, the government has options. For a very long time, the option federal law enforcement deployed most frequently in cases involving pretty much any sort of technology was the Computer Fraud and Abuse Act (CFAA). This broadly written l"
  },
  {
    "title": "Glyn: Type-safe PubSub and Registry for Gleam actors with distributed clustering (github.com/mbuhot)",
    "points": 22,
    "submitter": "TheWiggles",
    "submit_time": "2025-08-22T22:29:28 1755901768",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44990680",
    "comments": [
      "Really enjoyed my experiments with Gleam! Such a lovely, simple language, and it\u2019s clearly been made with great care and attention to detail.My language of choice is Rust, but I\u2019d go with Gleam in a heartbeat if I:- Were working on a team with junior engineers- Building a web app- On a passion project, or in a business context where the lack of ecosystem etc. wasn\u2019t a concernFor my own projects or with other senior folks, Rust\u2019s complexity is a price you pay once and you reap the rewards forever afterwards. But Gleam\u2019s simplicity would really shine in an organization with a wider range of experience levels.My biggest complaint besides the obvious ecosystem stuff is that the most popular frontend library leaves something to be desired. It\u2019s SPA-first, which seems like a very strange decision to make in 2025.reply",
      "So excited to see this! I hand built a shitty version of this for a small project. Need to switch when I get a chance.Tiny bit of self promotion since it\u2019s easier to link out to my own words than type them again. Typed actors in Gleam are so damn powerful. https://www.tcrez.dev/2025-07-13-gleam-otp-101.htmlreply"
    ],
    "link": "https://github.com/mbuhot/glyn",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Type-safe PubSub and Registry for Gleam actors with distributed clustering support, built on Syn.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\nType-safe PubSub and Registry for Gleam actors with distributed clustering support.Built on the Erlang syn library.Glyn provides two complementary systems for actor communication:Both systems integrate seamlessly with Gleam's actor model using selector composition patterns.First, define your message types and corresponding decoder functions.\nExplicit decoders are required to ensure messages sent between nodes in a cluster are handled with type safety.\nNote the Glyn does not JSON encode messages, they are sent directly as erlang terms and should be decoded from tuples.The real power of Gleam's typed actor system co"
  },
  {
    "title": "FFmpeg 8.0 (ffmpeg.org)",
    "points": 698,
    "submitter": "gyan",
    "submit_time": "2025-08-22T15:22:42 1755876162",
    "num_comments": 169,
    "comments_url": "https://news.ycombinator.com/item?id=44985730",
    "comments": [
      "Thank you FFmpeg developers and contributors!If there's anything that needs audio/video automation, I've always turned to FFmpeg, it's such a crucial and indispensible tool and so many online video tools use it and are generally a UI wrapper around this wonderful tool. TIL - there's FFmpeg.Wasm also [0].In Jan 2024, I had used it to extract frames of 1993 anime movie in 15 minutes video segments, upscaled it using Real-ESRGAN-ncnn-vulkan [1] then recombining the output frames for final 4K upscaled anime [2]. FWIW, if I had built a UI on this workflow it could've become a tool similar to Topaz AI which is quite popular these days.[0]: https://github.com/ffmpegwasm/ffmpeg.wasm[1]: https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan[2]: https://files.horizon.pics/3f6a47d0-429f-4024-a5e0-e85ceb0f6...reply",
      "Even when I don't use directly ffmpeg, I often use tools that embed ffmpeg. For instance, I've recently upscaled an old anime, ripped from a low quality DVD. I used k4yt3x/video2x, which was good enough for what I wanted, and was easy to install. It embedded libffmpeg, so I could use the same arguments for encoding:    Video2X-x86_64.AppImage -i \"$f\" \\\n     -c libvpx-vp9 -e crf=34 -o \"${f/480p/480p_upscale2x}\" \\\n     -p realcugan -s 2 --noise-level 1\n\nTo find the best arguments for upscaling (last line from above), I first used ffmpeg to extract a short scene that I encoded with various parameter sets. Then I used ffmpeg to capture still images so that I could find the best set.reply",
      "About 10-ish years ago, my then employer was talking to some other company about helping them get their software to release. They had what they believed to be a proprietary compression system that would compress and playback 4k video with no loss in quality.They wouldn't let us look into the actual codecs or compression, they just wanted us to build a front-end for it.I got to digging and realized they were just re-encoding the video through FFMpeg with a certain set of flags and options. I was able to replicate their results by just running FFMpeg.They stopped talking to us.reply",
      "One more taking part in a time-honoured tradition of taking someone else's thing, adding your own dipping mustard (if even that), and calling it your own.A new chatbot? Another ChatGPT wrapper. A new Linux Distro. Another Arch with a preinstalled desktop environment. A new video downloader? It's yt-dlp with a GUI.If they were just honest from the get-go, it'd be fine, but some people aren't.reply",
      "Happy to hear that they've introduced video encoders and decoders based on compute shaders. The only video codecs widely supported in hardware are H.264, H.265 and AV1, so cross-platform acceleration for other codecs will be very nice to have, even if it's less efficient than fixed-function hardware. The new ProRes encoder already looks useful for a project I'm working on.> Only codecs specifically designed for parallelised decoding can be implemented in such a way, with more mainstream codecs not being planned for support.It makes sense that most video codecs aren't amenable to compute shader decoding. You need tens of thousands of threads to keep a GPU busy, and you'll struggle to get that much parallelism when you have data dependencies between frames and between tiles in the same frame.I wonder whether encoders might have more flexibility than decoders. Using compute shaders to encode something like VP9 (https://blogs.gnome.org/rbultje/2016/12/13/overview-of-the-v...) would be an interesting challenge.reply",
      "> Happy to hear that they've introduced video encoders and decoders based on compute shaders.This is great news. I remember being laughed at when I initially asked whether the Vulkan enc/dec were generic because at the time it was all just standardising interfaces for the in-silicon acceleration.Having these sorts of improvements available for legacy hardware is brilliant, and hopefully a first route that we can use to introduce new codecs and improve everyone's QOL.reply",
      "I haven't even had a cursory look at decoders state of the art for 10+ years. But my intuition would say that decoding for display could profit a lot from GPU acceleration for later parts of the process when there is already pixel data of some sort involved. Then I imagine thet the initial decompression steps could stay on the CPU and the decompressed, but still (partially) encoded data is streamed to the GPU for the final transformation steps and application to whatever I-frames and other base images there are. Steps like applying motion vectors, iDCT... look embarrassingly parallel at a pixel level to me.When the resulting frame is already in a GPU texture then, displaying it has fairly low overhead.My question is: how wrong am I?reply",
      "I'm not an expert, but in the worst case, you might need to decode dense 4x4-pixel blocks which each depend on fully-decoded neighbouring blocks to their west, northwest, north and northeast. This would limit you to processing `frame_height * 4` pixels in parallel, which seems bad, especially for memory-intensive work. (GPUs rely on massive parallelism to hide the latency of memory accesses.)Motion vectors can be large (for example, 256 pixels for VP8), so you wouldn't get much extra parallelism by decoding multiple frames together.However, even if the worst-case performance is bad, you might see good performance in the average case. For example, you might be able to decode all of a frame's inter blocks in parallel, and that might unlock better parallel processing for intra blocks. It looks like deblocking might be highly parallel. VP9, H.265 and AV1 can optionally split each frame into independently-coded tiles, although I don't know how common that is in practice.reply",
      "Exciting! I am consistently blown away by the talent of the ffmpeg maintainers. This is fairly hard stuff in my opinion and they do it for free.reply",
      "Could you explain more about it? I assumed the maintainers are doing it as part of their jobs for a company (completely baseless assumption)reply"
    ],
    "link": "https://ffmpeg.org/index.html#pr8.0",
    "first_paragraph": "\n  A new major release, FFmpeg 8.0 \"Huffman\",\n  is now available for download.\n  Thanks to several delays, and modernization of our entire infrastructure, this release ended up\n  being one of our largest releases to date. In short, its new features are:\n  \nNative decoders: APV, ProRes RAW, RealVideo 6.0, Sanyo LD-ADPCM, G.728\nVVC decoder improvements: IBC,\n                                  ACT,\n                                  Palette Mode\nVulkan compute-based codecs: FFv1 (encode and decode), ProRes RAW (decode only)\nHardware accelerated decoding: Vulkan VP9, VAAPI VVC, OpenHarmony H264/5\nHardware accelerated encoding: Vulkan AV1, OpenHarmony H264/5\nFormats: MCC, G.728, Whip, APV\nFilters: colordetect, pad_cuda, scale_d3d11, Whisper, and others\n\n\n  A new class of decoders and encoders based on pure Vulkan compute implementation have been added.\n  Vulkan is a cross-platform, open standard set of APIs that allows programs to use GPU hardware in various ways,\n  from drawing on screen, to"
  },
  {
    "title": "70% of Japan smartphone games bypass in-app payment to avoid US tech giants (kyodonews.net)",
    "points": 39,
    "submitter": "anigbrowl",
    "submit_time": "2025-08-22T23:50:13 1755906613",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44991384",
    "comments": [
      "Yes, gacha games are always seeking the most optimal path from the player's wallet to the corporate checking account.reply",
      "Does this take into account that many \"smartphone games\" are playable via multiple platforms, and would need a non-in-app payment process anyway?reply",
      ">A Kyodo News survey found that among the top 30 best-selling game titles in 2024, at least 11 of the 16 offered by domestic companies have introduced payments through external websites.~70% (of the top 16 Japanese Game titles, or, 11 of them)Fuck google and fuck apple, but this isn't exactly a large samplereply",
      "If we count by revenue, I am certain it will be the same.reply",
      "I'm sure it is by revenue.reply"
    ],
    "link": "https://english.kyodonews.net/articles/-/59689",
    "first_paragraph": "TOKYO - Nearly 70 percent of popular Japanese smartphone games have introduced external payment systems for items and services to avoid hefty commission fees from U.S. tech giants Google LLC and Apple Inc., a Kyodo News tally showed.The move comes ahead of a new Japanese law tightening regulations on Google and Apple, which dominate smartphone platforms, set to take full effect in December. The legislation requires the two companies to open their payment systems.Almost all users currently download games through Apple and Google's app stores. When players buy in-game items, software providers pay the tech giants commissions of up to 30 percent.A Kyodo News survey found that among the top 30 best-selling game titles in 2024, at least 11 of the 16 offered by domestic companies have introduced payments through external websites.Although the two tech giants say the fees are necessary to protect user privacy and security, the costs have weighed on game makers.For outside transactions, users "
  },
  {
    "title": "Mail Carriers Pause US Deliveries as Tariff Shift Sows Confusion (bloomberg.com)",
    "points": 53,
    "submitter": "voxadam",
    "submit_time": "2025-08-22T23:09:04 1755904144",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=44991039",
    "comments": [
      "For at least the near term, parcels won't be shipped into the US.  Once things stabilize, they'll figure out who'll pay the tariffs stemming from suspending de minimus.  But it's going to affect small companies outside the US and the cost of goods entering from outside.That's the intent, but it's going to result in some lean months or years until the payola is delivered or the factories are (re)built.  And it's not going to be a fun time for Walmart.reply",
      "https://archive.is/824JTreply",
      "The article says I can't read it unless I pay Bloomberg their $1.99 tariff.reply",
      "Read the fine text> You'll pay $1.99/month beginning today for 1 month. Then your subscription will automatically renew at $39.99 every month after the first month, unless you cancel before the 1 month intro period.lol 20x increase after the first monthreply",
      "They still haven't retracted their lies from the Supermicro hit piece so I say good riddance.reply",
      "Not a tariff. Nice try at a joke though!!reply",
      "> Washington\u2019s long-standing de minimis policy had allowed parcels packed with cheap items to flow into the US from around the world with little interruption or oversight, fueled by consumer demand for bargains and immediacy. Trump\u2019s White House claims it\u2019s a loophole used to evade tariffs and funnel illegal drugs.> Now, postal services, online sellers, consumers and shipping companies are attempting to sort through the costly and complicated process to comply with US rules with little guidance from federal agencies.I wonder what consideration individuals are giving this. . . The article says very little about consumer behavior save for the above two grafs. I very rarely buy directly from abroad and that is by design, with nothing to do re: de minimis. What bargains are people buying?! Especially in this economy.reply",
      "Not American nor stuck in America, but I recently bought a simple kit for building your own clock from a ton of basic chips and resistors (that crappy one Big Clive showed off), for about a dollar.The thing that surprised me most was how on point the shipping emails have been. The kit itself is worth about a dollar and was great for my 8 year old to practice soldering. Though if I skim the Temu site, it\u2019s like 98% absolute trash.reply",
      "I mostly research and analyze retro hardware as a hobby, most of which was made in Japan. All of my research acquisitions at least doubled in price this month, and quite a few sellers have decided to stop shipping to me entirely until the tarrif situation gets sorted.These are 40+ year old consoles and accessories that are no longer being produced anywhere, certainly not in the United States. There will not be a factory built for these items, they're not in high demand. They just got way more expensive.reply",
      "I personally buy things from abroad relatively regularly. Few times a year. Just bought a keyboard from Taiwan and stocked up on Japanese tea in preparation for this. Plenty of  things come from abroad though even if you\u2019re not searching it out. Of course there\u2019s SHEIN, temu, and Alibaba but even Amazon has a good percentage of things that come from abroad. It\u2019s kinda hidden but it\u2019s seamless so until now it was hard to tell.reply"
    ],
    "link": "https://www.bloomberg.com/news/articles/2025-08-21/global-mail-services-halt-us-deliveries-ahead-of-de-minimis-end",
    "first_paragraph": ""
  },
  {
    "title": "Transcribe music in abc with syntax highlighting (fugue-state.io)",
    "points": 8,
    "submitter": "jonzudell",
    "submit_time": "2025-08-22T22:41:20 1755902480",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44990798",
    "comments": [
      "I can\u2019t possibly tell if this is worth it over leading class labeling tools in audacity. There is a sign up with no pricing and no mention of limits.Why would i have a good impression if my first experience is a sunk-cost time-sink dark-pattern?So disrespectful of your potential users time.reply",
      "This just loads with the word processing displayed, nothing elsereply"
    ],
    "link": "https://fugue-state.io/app?project=24024aab-22f1-43cc-abef-c1647cc59597",
    "first_paragraph": "Loading..."
  },
  {
    "title": "Launch HN: BlankBio (YC S25) - Making RNA Programmable",
    "points": 37,
    "submitter": "antichronology",
    "submit_time": "2025-08-22T16:53:30 1755881610",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=44986809",
    "comments": [
      "Fun to see talk of \"a compiler for DNA\"---I've been hoping for that for a long time.I have to admit, at a _glance_ this feels like a promising idea with few results and lots of marketing. I'll try to be clear about my confusion, feel free to explain if I'm off base.- There's not a lot of talk of your \"ground truth\" for evaluations. Are you using mRNABench?- Has you mRNABench paper been peer reviewed? You linked a preprint. (I know paper submission can be touch or stressful, and it's a superficial metric to be judged on!)- Do any of your results suggest that this foundation model might be any good on out of sequence mRNA sequences? If not, then is the (current) model supposed to predict properties of natural mRNA sequences rather than of synthetic mRNA sequences?- Did a lot mRNA sequences have experimental verification of their predicted properties? At a quick glance, I see this 66 number in the paper---but I truly have no idea.I'm super happy to praise both incremental progress and putting forth a vision, I just also want to have a clear understanding of the current state-of-the-art as well!reply",
      "> ground truthHey yes, the ground truth for our evaluations is measured experimental data. Our models are benchmarked using mRNABench, which aggregates results from high-throughput wet lab experiments.Our goal, however, is to move beyond predicting existing experimental outcomes. We intend to design novel sequences and validate their function in our own lab. At that stage, the functional success of the RNA we design will become the ground truth.> peer reviewed?Both mRNA bench and Orthrus are in submission (at a big ML conference and a big name journal) - unfortunately the academic systems move slow but we're working on getting them out there.> synthetic mRNA sequencesI think you're asking on generalizing out of distribution to unnatural sequences. There are two ways that we do this: (1) There are these screens called Massively Parallel Reporter Assays (MPRAs) and we eval for example on https://pubmed.ncbi.nlm.nih.gov/31267113/Here all the sequences are synthetic and randomly designed and we do observe generalization. Ultimately it depends on the problem that we're tackling: some tasks like gene therapy design require endogenous sequences.(2) The other angle is variant effect prediction (VEP). It can be thought of as a counterfactual prediction problem where you ask the model whether a small change in the input predicts a large change in the output. This is a good example of the study (https://www.biorxiv.org/content/10.1101/2025.02.11.637758v2)> experimental verification of their predicted propertiesall our model evaluations are predictions of experimental results! The datasets we use are collections of wet lab measurements, so the model is constantly benchmarked against ground-truth biology.The evaluation method involves fitting a linear probe on the model's learned embeddings to predict the experimental signal. This directly tests whether the model's learned representation of an RNA sequence contains a linear combination of features that can predict its measured biological properties.Thanks for the feedback I understand the caution around pre-prints. We believe a self-supervised learning approach is well-suited for this problem because it allows the model to first learn patterns from millions of unlabeled sequences before being fine-tuned on specific, and often smaller, experimental datasets.reply",
      "Hi, I'm the lead author of the human 5' UTR paper. It was a nice surprise seeing it linked on HN and I'm happy to see that it's providing value for you all. Looking forward to watching your team's progress!reply",
      "Huge fan of the work! I'm a big fan of papers from Seelig lab :)reply",
      "> mRNABenchJust curious, in other areas of ML, I think it's widely acknowledged that benchmarks have pretty limited real world value, just end up getting saturated, and (my view) are all pretty correlated, regardless of their ostensible speciality and don't really tell you that much.Do you think mRNABench is different, or where do you see the limitations? Do you imagine this or any benchmark will be useful for anything beyond comparing how different models do on the benchmark?reply",
      "I watched an interview with one of the co-founders of Anthropic where his point is that although benchmarks saturate they're still an important signal for model development.We think the situation is similar here - one the challenges is aligning the benchmark with the function of the models. Genomic benchmarks for gLMs and RNA foundation models have been very resistant to staturation.I think in NLP the problem is that they are victims of their own success where the models can be overfit to particular benchmarks really fast.In genomics we're a bit behind. A good paper on this is DartEval where they provide levels of complexity https://arxiv.org/abs/2412.05430in RNA the models work much better than DNA prediction but it's key to have benchmarks to measure progress.reply",
      "Here is the link for benchmarks and their utility: https://youtu.be/JdT78t1Offo?t=1444\"We have internal benchmarks. Yeah. But we don't we don't publish them.\"\"we have internal benchmarks that the team focuses on and improving and then we also have a bunch of tasks like I think that accelerating our own engineers is like a top top priority for us\"The equivalent for us would be to ultimate looking to improve experimental results. Benchmarks are a good intermediate point but not the ultimate goalreply",
      "Fascinating platform. I'm fairly new in my bio education but are you effectively finding sequences on NIH and then intelligently chaining them together?I had some fun one evening asking Claude how I could string together sequences for an imaginary therapeutic and it gave me enough to put into alphafold and get a render :) (Worst therapeutic ever: deliver mRNA into macrophages to target those pesky bacteria who happily just choose to reside there)Also: How do you plan to navigate the unfortunate part of our country trying to write mRNA out of the American vocabulary?reply",
      "I am totally onboard with the premise (as a TechBio-adjacent person), and some of the approaches you're taking (focused domain-specific models like Orthrus, rather than massive foundation models like Evo2).I'm curious about what your strategy is for data collection to fuel improved algorithmic design. Are you building out experimental capacity to generate datasets in house, or is that largely farmed out to partners?reply",
      "We think that Orthrus can be applied in a bunch of ways to non-coding and coding RNA sequences but it's definitely fair we're a bit more focused on RNA sequences currently instead of non-coding parts of the genome like promoters and intergenic sequences.For the data - Orthrus is trained on non experimentally collected data so our pre-training dataset is large by biological standards. It adds up to about 45 million unique sequences and assuming 1k tokens per sequence it's about 50b tokens.We're thinking about this as large pre-training run on a bunch of annotation data from Refseq and Gencode in conjunction with more specialized Orthology datasets that are pooling data across 100s of species.Then for specific applications we are fine tuning or doing linear probing for experimental prediction. For example we can predict half life using publicly available data collected by the awesome paper from: https://genomebiology.biomedcentral.com/articles/10.1186/s13...Or translation efficency: https://pubmed.ncbi.nlm.nih.gov/39149337/Eventually as we ramp up out wet lab data generation we're thinking about what does post-training look like? There is an RL analog here that we can use on these generalizable embeddings to demonstrate \"high quality samples\".There are some early attempts at post-training in bio and I think it's a really exciting directionreply"
    ],
    "link": "item?id=44986809",
    "first_paragraph": ""
  },
  {
    "title": "U.S. government takes 10% stake in Intel (cnbc.com)",
    "points": 404,
    "submitter": "givemeethekeys",
    "submit_time": "2025-08-22T21:01:52 1755896512",
    "num_comments": 401,
    "comments_url": "https://news.ycombinator.com/item?id=44989773",
    "comments": [
      "In general I would rather the government take a stake in corporations they're bailing out. I think the \"too big to fail\" bailouts in the past should have come with more of a cost for the business, so on one hand I'm glad this is finally happening.On the other hand, I wish it were a more formalized process rather than this politicized \"our president made a deal to save america!\" / \"Intel is back and the government is investing BUY INTEL SHARES\" media event. These things should follow a strict set of rules and processes so investors and companies know what to expect. These kind of deals should be boring, not a media event.reply",
      "I think it would've been much better to incentivize the likes of Apple and Nvidia to make investments in Intel. They need to have their designs fabbed, they have a good amount of geopolitical risk. They also have a lot of money on hand. Didn't Apple say they were going to invest $600B into the US? (not that that's really going to happen), ok, so why not put $50B into Intel?reply",
      "I\u2019d really rather we didn\u2019t bail out these companies at all. It clearly creates moral hazard and makes it hard for better run companies to enter markets.reply",
      "If shareholders are losing ownership it's less a pure bailout and more a strategic investment and/or takeover.  It also potentially lets the average taxpayer benefit rather than just those its directly propping up.reply",
      "How does the average taxpayer ever actually end up benefitting point blank?reply",
      "Not that I agree with bail-outs, but 2008 financial crisis that resulted in a number of bail outs actually netted the treasury a profit.> In total, U.S. government economic bailouts related to the 2008 financial crisis had federal outflows (expenditures, loans, and investments) of $633.6 billion and inflows (funds returned to the Treasury as interest, dividends, fees, or stock warrant repurchases) of $754.8 billion, for a net profit of $121 billionhttps://en.wikipedia.org/wiki/Troubled_Asset_Relief_Programreply",
      "I don\u2019t think that really counts if there has to be a giant campaign of quantitive easing by printing dollars alongside.reply",
      "Profits from the stake lower taxes that would otherwise be levied on you? Of course that\u2019s moot if the deficit isn\u2019t something being taken seriously.reply",
      "Deficits aren't real and 10% of $0 when they likely go bankrupt is $0reply",
      "Well as much as you don't like it, companies this big failing is terrible for the economy and in this case, national security to a degree. I'm of the thinking that when your company gets to a certain size we'd be well off nationalizing. Apple has more money than some nation states. Something that huge has the potential to affect global politics. There's lots of other reasons too, but this isn't like letting the corner store fail. The repercussions are huge. If we're going to bail out, the people should own some of it.reply"
    ],
    "link": "https://www.cnbc.com/2025/08/22/intel-goverment-equity-stake.html",
    "first_paragraph": ""
  },
  {
    "title": "Leaving Gmail for Mailbox.org (giuliomagnifico.blog)",
    "points": 167,
    "submitter": "giuliomagnifico",
    "submit_time": "2025-08-22T17:41:48 1755884508",
    "num_comments": 201,
    "comments_url": "https://news.ycombinator.com/item?id=44987380",
    "comments": [
      "I\u2019ve been a Fastmail user for years, having left Gmail. It works great and have nothing be but praise for them. I use my own domain with them so if I decide to leave it\u2019s not an issue worrying about updating people with my new email.reply",
      "I'm in the process of switching from Gmail to FastMail. They were the only ones who met one of my requirements: Receive all email for all my domains and deliver it to one inbox with labels.I really like that they offer a Gmail migration, including an initial import and _ongoing Inbox sync_. It only syncs the Inbox though, not spam (which is sometimes legit, especially with Gmail) or mail that gets immediately archived by a rule.I created an alternate domain so I could try them out and perform the switch after a significant evaluation period. Since they have advanced options for figuring out which address to reply to an email with and how, it works seamlessly with gmail and with the catch-all for domains.I could go on and on. The only thing I miss from Gmail is custom notification sounds. I don't like my email notifications having the default OS sound. Oh and you can't migrate stars/icons for emails. I wish I could do that and convert them to labels, but not a big deal.reply",
      "Fastmail is kind of a weird service. If you stop paying they release your email for someone else to take over. Pretty unacceptable this day and age.reply",
      "I really wish all mail providers made it easy and seamless to bring your own domain (or register and manage one in the background for you, without you having to care for the details).  Obviously giving a service-tied email domain to users is a great lock-in strategy.  But it's worrying that so many people have a big part of their online identity tied to Google.(You can even sign up for a Google Account without GMail, using a third-party domain.  And this is distinct from Google Workspace, or whatever they're calling it today.  You get a normal, regular, personal Google Account, just without GMail and using your own non-gmail.com address.)reply",
      "This would be easily solved for customers who care about it by allowing you to pay a one-off fee to reserve the name for ~100 years.Or they could just absorb that.Any idea why it works that way? Have they offered an explanation?I'm a Fastmail customer but I've never noticed this because I use my own domain.reply",
      "Domain names work the same way -- once you stop paying for it, someone else can buy and use it.Do you have the same problem with domain names? If so, how would you propose to fix it?reply",
      "Are you seriously telling me that unless people have a solution for fixing DNS, commercial email should be free to hand out used email addresses? Seriously?reply",
      "It is easier to change MX records for your personal domain.reply",
      "That is 100% unacceptable.reply",
      "I use Fastmail with my own domain. I am not sure of the logic that says paying $60/year for email is fine, but $8/year for a domain is a bridge too far.Do that, it's a non-issue, though I do agree with you that it shouldn't be a thing (or at least have like a multiple year embargo on the address).reply"
    ],
    "link": "https://giuliomagnifico.blog/post/2025-08-18-leaving-gmail/",
    "first_paragraph": "This was a tough decision, having used Gmail since 2007/2008. However, I had to draw the line and stop giving Google my data for free.The problem with email is that everything is transmitted in plain text. Technically, Google can store every message you receive and know everything, and U.S. agencies can request access to that data (this include also EU citizens under the EU-U.S. and Swiss-U.S. Data Privacy Frameworks).For someone like me, who cares about privacy and runs as much as possible on my own home servers, that felt like way too much.So I decided to switch to another provider, one that respects privacy a bit more. Of course, this meant no longer \u201cpaying\u201d with my personal data, but instead paying the actual price of the email service.Let me start by saying: I use email in a very basic way. I send and receive a lot of messages (at least 50 a day), but they\u2019re plain text/html emails with no attachments or fancy features. I couldn\u2019t care less about the rest of the \u201csuite\", like not"
  },
  {
    "title": "Why the Internet Is Turning to Shit (currentaffairs.org)",
    "points": 28,
    "submitter": "Improvement",
    "submit_time": "2025-08-22T23:16:10 1755904570",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=44991117",
    "comments": [
      "I see everything turning to shit.  It's the collapse of society.  We've seen that no one is out there protecting us and \"fighting the good fight\".We had a good run.reply",
      "Society has always been collapsing, at least since the younger generation first started making written notes and letting their brains rot instead of keeping their memory exercised.reply",
      "Most people don't mind enough to put in any effort to move to something less sucky.Some do, and you get things like that one paid search engine that a few people here like to praise, or the various replacements for the big \"social\" sites.reply",
      "Effort or actually pay for services that are good.reply",
      "it's not.but peoples' \"navigation\" or \"traversal\" skills are.it's the fault of the upper classes, starting at journalists and psychologists, if you care for some help with your research. they've been sucking for a while; at their jobs, understanding their true desires, fulfilling them, obviously, too, and they've been really really really (don't use this word as a scalar) bad in general, at developing their ability to get closer to what they want, and what they want to get on the way. they all submitted to shit.and because journalism submitted to shit, it was very very very very ( don't use this word as a scalar) easy for ads people and ads divisions to trash peoples' \"traversal\" or navigation skills via ... well, shit.reply",
      "> \"Instead of better-regulated companies, why shouldn\u2019t the solution be no more companies?\"This is a very weird, even quite childish, criticism.There's an outside chance that some of the stuff Doctorow wants can be implemented.  There's literally zero chance that companies in general are abolished (in favor of what, exactly?) just because a few megacorps, given perverse incentives, have become bad actors.Besides, large firms are, effectively, the most unkillable human-derived things that exist.  If you \"abolish\" them in one place, they'll nevertheless still exist in another, or they'll simply pick up move somewhere else, like high-net-worth individuals.  \"Headquarters\" are often merely a formality.reply",
      "Companies are a legal fiction. They can be abolished as easily as they were created. We just lack any imagination to think of something different. That doesn't mean there isn't a better idea.reply",
      "I'm sick of people who cry \"fire\" but then follow up with  \"but, hey, I'm no fireman\". WHAT  is the alternative? What are you doing to realize it? We don't need anymore more specialists who focus on pointing out the problems.reply",
      "Companies are an acknowledgement that sometimes groups of people do things together.reply",
      "> That doesn't mean there isn't a better idea.I'm genuinely curious, what better idea are you referring to that would replace the concept of companies?reply"
    ],
    "link": "https://www.currentaffairs.org/news/why-the-internet-is-turning-to-shit",
    "first_paragraph": "\nA Magazine of Politics and Culture\nCory Doctorow\u2019s new book is an insightful diagnosis of how Big Tech is making every app and website worse\u2014even if his solutions leave something to be desired.About a year ago, I bought a hardcover thesaurus\u2014Roget\u2019s in dictionary form, the Delacorte edition, printed in 1992. It\u2019s huge, it\u2019s heavy, and it smells faintly of mildew, probably because it was sitting on a shelf in a New Orleans used bookshop for years. But it was a necessary investment, because when I Google synonyms for a word, I can no longer trust that the results I get will be accurate or useful. Or, for that matter, when I Google anything. Instead, some weird AI-generated gunk is likely to come up, or a \u201csponsored\u201d ad, or both. Plenty of people online have noticed this phenomenon. Around the time I hoisted the jolly Roget, a series of viral screenshots showed that when people Googled the question \u201chow many rocks should I eat each day?,\u201d the search engine told them to consume \u201cat least "
  },
  {
    "title": "Writing Micro Compiler in OCaml (2014) (troydm.github.io)",
    "points": 15,
    "submitter": "notagoodidea",
    "submit_time": "2025-08-19T02:00:32 1755568832",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "http://troydm.github.io/blog/2014/03/29/writing-micro-compiler-in-ocaml/",
    "first_paragraph": "\nMar 29th, 2014\nTL;DR Writing micro compiler in OCamlAt one point or another every single software developer in the world comes to a realization in his career when the time is ripe and it\u2019s time\nto write your own super cool programming language.However the subject of creating your own programming language with an compiler is quite a complex one and can\u2019t be tackled without some pre-research.\nThat\u2019s how I\u2019ve started reading Crafting Compiler in C, an aged but\nreally comprehensive book about developing your own compiler for an Ada-like programming language.\nSecond chapter describes writing a really simple micro language targeting pseudo assembly-like output in order to explain the core concepts of developing your\nown compiler and writing an LL(1) parser.Let\u2019s try rewriting this micro compiler in OCaml, a language better suited for writing compilers that is becoming quite popular due to it\u2019s clean syntax and strict evaluation semantics combined\nwith functional and object-oriented programm"
  },
  {
    "title": "LabPlot: Free, open source and cross-platform Data Visualization and Analysis (labplot.org)",
    "points": 193,
    "submitter": "turrini",
    "submit_time": "2025-08-22T09:11:26 1755853886",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=44982409",
    "comments": [
      "It's so interesting to see how much of a commodity charting/graphing has become. When we started building Deltagraph in late 1988, what we made become a kind of standard since we targeted Postscript and Illustrator output, and included almost every kind of chart we could find with ridiculous options for everything, so people used it world wide, especially if targeting print. In the mid-90's, it was sold by the publisher (we just did the dev), and it spent the next 25 years at various owners before dying during the pandemic, all still based on the original source code (C) I started. I can't imagine how bad the code looked by then...reply",
      "And yet it's still not sufficiently commoditized and widespread. The majority of the working force is using proprietary solutions that are out of date - Tableau, JMP in HW engineering, SAS and Excelreply",
      "I used SciDavis a lot and before that tried QtiPlot. When I had a chance to I used Origin.\nSciDavis was clunky and had some issues (liked to crash) but it worked well enough for what I wanted. Had some problems with setting plots styles, maybe it was just me but it wasn't obvious how to copy style between plots.Tried LabPlot recently and had issues with csv import with datetime data not really recognising date and time series format even after using advanced import options and setting it myself manually. Tried to find some solutions, the LabPlot manual website is just a bunch of youtube videos [1]. That is really not helpful, I am not browsing manual to be forced to watch clips of what I already tried. Developers really need to think about making traditional manual.There is also a AlphaPlot, a more or less alive fork of SciDavis. Still have its own issues but still has the same issue with yyyy-MM-dd hh:mm:ss.zzz dates. Other than that it is a useful bit of kit.But when I want to do some batch processing and generate multiple plots, automate and have it reproducible I go with gnuplot. The learning curve is steep, but after writing gnuplot scripts few time you just have a personal template and know relevant parts. It is really good.All in all I am glad there is an opensource movement in this area. It is always better to have more options.1. https://docs.labplot.org/en/2D_plotting/2D_plotting_xycurve....reply",
      "Sure ggplot, for example, is finicky, and you need to fuss over it to get the look you are wanting, but then again, it is very flexible. Most of these solutions get frustrating as soon as you want to do, for example, spaghetti plots of within subject repeated measures using age (not time-point) of accelerated longitudinal design data, with fixed effect plots on top.  e.g. this plot of mine [1]\n[1] https://imgur.com/a/gw2vV7wreply",
      "I just needed to stop and say: as a biostatistician, boy do I love a beautiful complex longitudinal design: I remember my old professor asking us how at this point we would decompose into cross-sectional and longitudinal effects, Lord's paradox, etc... and I still don't fully understand Lord's paradox as well as I should.reply",
      "This is a very important idea. For example, one issue with accelerated longitudinal designs, see image [1], is that while they efficiently cover a larger age range, the fixed effects of age are largely driven by cross-sectional differences between who is samples are younger and older ages. One method that can be used to test whether the pattern seen in the fixed effects represents the pattern within subjects is to decompose within and between effects of age. For example, you can create a non-time-varying variable like age at first visit (starting_age), and then a within subject variable change in age since first visit, which would be zero at the first visit (age1-age1=0, age2-age1 for change of age between visit 2 and visit 1, age3-age1, for change in age between 3rd visit and first visit), calling it dage. Then in the mixed model, test for an interaction between starting_age:dage. If you have an interaction, then you know that the within subject effect of change in age is different depending on how old you were when you started.\nI got this from Lesa Hoffman's freely available lectures [2], particularly [3][4], and now I discovered she recently published [5], which I should read.[1] https://e-m-mccormick.github.io/static/longitudinal-primer/l...\n[2] https://www.lesahoffman.com/\n[3] https://www.lesahoffman.com/PSYC944/944_Lecture11_Alt_Time.p...\n[4] https://www.lesahoffman.com/Workshops/SMiP_Presentation_June...\n[5] https://www.tandfonline.com/doi/full/10.1080/00273171.2025.2...reply",
      "And thank you for the reminder of Lord's paradox. I should refresh myself.reply",
      "Obviously there is a lot of work here, but I am a bit confused. If you already have lab code in Julia, Matlab, R, Python, Excel, etc., what is the motivation to use this tool? Is this hot in a specific community?reply",
      "I suppose this is a FOSS solution for the roughly same space occupied by commercial tools like Origin, that are very popular in some scientific communities.They can be useful if you have other tools (e.g. measurement software) that already produces the data you want, and you just want a GUI tool to create plots, and maybe do some simple things like least squares curve fitting etc.If you already do a lot of data wrangling in something with a programming language and plotting libraries accessible from said language, like the ones you mention, yeah, this is not the tool for you.reply",
      "It is! I remember using this (or SciDavis, a related project) a couple of years back in college. It was not as powerful as Origin 10 years ago, but it ran on Linux.This is great for people who don't know nor want to learn to program.reply"
    ],
    "link": "https://labplot.org/",
    "first_paragraph": "Today we are announcing the availability of the minor patch release 2.12.1. This release contains minor improvements and bug fixes only. The fixes are distributed over many different areas of the application and we recommend everybody update to this patch release which is available from\u2026Welcome LabPlot 2.12! After many months of intense work, we are proud to announce the new release of LabPlot 2.12, a FREE, open source and cross-platform Data Visualization and Analysis software accessible to everyone and trusted by professionals! This latest release introduces a wealth of\u2026In recent weeks we have been working on transferring LabPlot\u2019s documentation to a new format. We decided to move the documentation from the DocBook and MediaWiki format to the Sphinx/reStrcutredText framework. In our perception Sphinx offers a user-friendly and flexible way to create and manage\u2026In many cases, importing data into LabPlot for further analysis and visualization is the first step in the application: LabPl"
  },
  {
    "title": "The issue of anti-cheat on Linux (2024) (tulach.cc)",
    "points": 84,
    "submitter": "todsacerdoti",
    "submit_time": "2025-08-22T01:09:33 1755824973",
    "num_comments": 153,
    "comments_url": "https://news.ycombinator.com/item?id=44980064",
    "comments": [
      "Targeting perfect fairness in a multiplayer video game with arbitrary latency between participants is a waste of energy. A much better target is to make it feel like no one is cheating. I don't really care too much if someone is actually better or worse than me at counterstrike. What I mostly care about is wildly implausible gameplay. No one is going to stop the guy who is getting a 5% gain on his ELO by using a 2nd computer, machine vision and a robot to move his mouse ever so slightly faster than he typically can.However, there are ways to detect when someone is being an absolute madman with the hacks. We're talking head snapping through walls with 100% accuracy and instantaneous displacement across an entire 30 minute match. These people can simply be banned immediately by hardware/steam ID. We can write basic rules to detect stuff like this. There's no \"confidence interval\" for speed hacking through a map and awping the entire CT team in 3 seconds. You certainly don't need kernel drivers.reply",
      "Or entire lobbies filled with bots with the same name that stand around doing nothing while one of them goes full spinbot, and auto kicks anyone who happens to join their lobby. Those bots I see week after week with the same accounts and no bans in sight.reply",
      "No need for a robot to move the mouse:https://www.youtube.com/watch?v=9alJwQG-Wbkreply",
      "This article gave me more appreciation for the stance of the Linux community.So to sum up. Valorant's anti-cheat, which the author sees something like an ideal solution:- starts up and loads its kernel driver on boot.- generates a persistent unique ID based on hardware serial numbers and associates this with my game account.- stays active the entire time the system is up, whether I play the game or not. But don't worry, it only does some unspecified logging.- is somehow not a spyware or data protection risk at all...reply",
      "I also always hear a lot of people complain about cheaters in Valorant, so all of that compromised personal security doesn't actually stop cheaters.Honestly I feel like you should only use kernel anticheat on a dedicated machine that's kept 100% separate from any of your personal data. That's a lot to ask of people, but you really shouldn't have anything you don't consider public data on the same hardware.reply",
      "> you should only use kernel anticheat on a dedicated machine that's kept 100% separate from any of your personal data.Correct. Unfortunately, what you've just described is a gaming console rather than a PC. This problem fundamentally undermines the appeal of PC gaming in a significant way, imo.reply",
      "> This problem fundamentally undermines the appeal of PC gaming in a significant way, imo.Yes, game publishers are trying to turn PCs into a gaming console, which IMO will always be a futile effort, and is quite frankly annoying. I don't game on PC to have a locked down console-like experience.Just embrace the PC for what it is and stop trying to turn it into a trusted execution platform with spyware and rootkits.Look at BF6 - for all the secure boot and TPM required anti-cheat they stuffed it with, there were cheaters day 1, so why abuse your users when it's clearly ineffective anyway.reply",
      "That's what gets me! If these rootkit anti-cheat systems actually stopped cheating then maybe, just maybe, I'd accept them as a necessary evil. But every game that has these things... still has cheaters! So as a user, you're consenting to ripping a security hole through your system, and in return you are still playing games with cheaters.The game companies keep saying these things are necessary, yet they don't fully do the very thing they claim to do on the label.reply",
      "Honestly, if consoles were willing to accept KB+M (and gyro aiming for that matter), I\u2019d be completely proposing that competitive live service titles mostly abandon PC, except for a small \u201cprobably infested with cheaters\u201d base.reply",
      "Somehow Xonotic manages to be both completely free/open software and not have cheating problems like this. It's never been clear to me how they've done that although client-side stuff like these kernel anti-cheat things were obviously never going to work.reply"
    ],
    "link": "https://tulach.cc/the-issue-of-anti-cheat-on-linux/",
    "first_paragraph": "The number of people choosing Linux as their primary operating system to play games has been slowly but steadily going up, at least according to the Steam hardware survey. This is most likely because of the Steam Deck release and the increasingly obnoxious features being added to Windows.The number of people choosing Linux as their primary operating system to play games has been slowly but steadily going up, at least according to the Steam hardware survey. This is most likely because of the Steam Deck release and the increasingly obnoxious features being added to Windows.If you switch to Linux today, you\u2019ll probably be surprised by how many games run out of the box just fine (mostly due to the Windows compatibility layer Proton built right into Steam), except for basically all competitive multiplayer games that utilize any sort of anti-cheat technology.Just to name a few, here is a list sorted by concurrent player count from ProtonDB:Those are just games on Steam. Then there\u2019s also Val"
  },
  {
    "title": "Shader Academy: Learn computer graphics by solving challenges (shaderacademy.com)",
    "points": 3,
    "submitter": "pykello",
    "submit_time": "2025-08-20T11:08:15 1755688095",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://shaderacademy.com/",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Clyp \u2013 Clipboard Manager for Linux (github.com/murat-cileli)",
    "points": 68,
    "submitter": "timeoperator",
    "submit_time": "2025-08-22T16:03:26 1755878606",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=44986205",
    "comments": [
      "I thought wayland had some restrictions on global clipboard access and the last time I tried none of the well known clipboard managers worked as expected. (Also they all looked like shit).This has been one of my pain points switching from macOS to linux or windows. Great job.reply",
      "I actually went looking at the source code to see if this would work on Wayland and it doesn't. The clipboard snooping is implemented by listening for events using gdk.Clipboard, which is not an ext_data_control_v1 implementation. So on Wayland it'll only notice clipboard events if it's in focus (or if the compositor sends clipboard events to unfocused windows, which I'm not sure any do).https://github.com/murat-cileli/clyp/blob/2c0ce6c33813c3f35f...Edit: Yes, tested it now and it doesn't detect clipboard events from Wayland windows when it doesn't have focus. It only detects events from Xwayland windows when unfocused, or if I copy something from a Wayland window and then focus the clyp window then it detects the thing I copied.reply",
      "It's almost as if a Wayland compositor should keep a list of trusted apps to broadcast clipboard events to, somehow similar to how screenshots are handled. (Not that Wayland is well-rounded in this regard.)reply",
      "The ext_data_control_v1 protocol I mentioned is a protocol specifically for clipboard managers. So a client that wants to be a clipboard manager would implement that protocol. There are already implementations of it like wl-clipboard. There is no need for the compositor to broadcast regular clipboard events (wl_data_offer).Now the compositor could certainly keep an additional list of trusted applications that are allowed to be clients of the ext_data_control_v1 protocol.  Though identifying the client to enforce such a thing is a bigger problem than just maintaining a list of applications, because the protocol has no client identification. AFAIK every compositor that supports that protocol has no restrictions on clients requesting it, though something involving the security-context protocol might change this in the future.reply",
      "Odd, why does the readme tout \"Full Wayland support\" then?reply",
      "Because its easy to write it and most wont even verify?reply",
      "That's interesting.. Never ran into this, been using various clipboard managers in wayland (swaywm at first, now niri) for years without issue. copyq is what I use these days and, while not quite as pretty as this one, its great!reply",
      "One thing that I love about Windows (and there aren't many others) is that pressing Super+V (instead of Ctrl+V) shows a list of last N clipboard entries and you can select which one you wish to paste. Simple and very effective.You can also pin some entries so that they are permanently available, but that's a bonus.I haven't seen a clipboard manager behave like that in Linux - can this one be used in a similar way?reply",
      "I configured copyq to work exactly like this, so it's doable.reply",
      "KDE's default clipboard manager lets you summon a list (and you can change what shortcut to invoke it and do things like use a shortcut to move to the next clipboard entry) and edit entries. It doesn't let you pin them though, I think.reply"
    ],
    "link": "https://github.com/murat-cileli/clyp",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Clipboard manager for Linux.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Modern, fast, simple clipboard manager for Linux.ImportantDepends on libgtk-4-devImportantDepends on gtk4Or launch from your application menu.The watcher is a minimal headless Gtk application. It monitors the clipboard and notifies the GUI of database changes via a UNIX socket.Clipboard data is stored in ~/.local/share/bio.murat.clyp/clyp.db using SQLite3. The database includes:Clyp follows XDG Base Directory specifications:\n        Clipboard manager for Linux.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page."
  },
  {
    "title": "What Happened to Egghead Software (homeip.net)",
    "points": 18,
    "submitter": "zdw",
    "submit_time": "2025-08-19T15:18:18 1755616698",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=44952572",
    "comments": [
      "Oh I\u2019m glad this popped up. I was trying to remember the details of a computer store I remembered visiting with my dad when I was maybe 4 or 5, \u201893-\u201894, Chicago.But as soon as I read it all came back and I\u2019m certain this was it. I tried looking but couldn\u2019t find out: does anyone know if there was an Egghead in Chicago at that time? Based on what I could discern about their stores is that seems likely there would have been one there.reply",
      "I remember visiting the Egghead in Lansing, Illinois (just south of Chicago, at River Oaks south) more than once. I think I actually bought Visual Basic there at some point.reply",
      "Back in the 90s, I would go into my local Egghead software store almost every weekend.reply"
    ],
    "link": "https://dfarq.homeip.net/what-happened-to-egghead-software/",
    "first_paragraph": ""
  }
]