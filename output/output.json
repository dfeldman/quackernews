[
  {
    "title": "Launch HN: Airhart Aeronautics (YC S22) \u2013 A modern personal airplane",
    "points": 318,
    "submitter": "n_ermosh",
    "submit_time": "2024-08-05T17:26:43",
    "num_comments": 270,
    "comments_url": "https://news.ycombinator.com/item?id=41163382",
    "comments": [
      "Instrument-rated pilot (and engineer) here.First - congrats on the launch! I think you're working on an interesting set of components that will prove useful to GA aircraft technology. Bringing fly-by-wire, and lowering the cost of maintenance/manufacturing are both great efforts.That being said, my personal view is that stick-and-rudder control is one of the less critical components to improving GA safety. Everything else - flight planning, comms, automation, navigation, weather, inspections, procedures, regs, and most importantly - working in the federal airspace system - are the \"hard\" parts of flying and where problems tend to occur. It's common belief that single-pilot IFR is the most challenging type of flying, because of how much you have to do all at once.It may sound snobby - but I'm not super excited about the idea of lowering the barrier to entry for GA on a foundational skill basis. Like the light-sport rating, it encourages more people to be in the (already congested) airspace system who haven't really gained all the other skills necessary or experience to be there.To be clear - I think improving technology and lowering costs = good. Lowering early-skill requirements for pilots and pushing more people without all the other skills into federal airspace = very bad. In general, I'd frame this effort more as an effort to raise the bar for system technology, not lower the bar to become a pilot in the first place.\n \nreply",
      "Excellent points, it's really these other aspects that get pilots into trouble.For example it's crazy that most pilots are still taught to calculate W&B using printed charts and approximate takeoff performance.I think you could save more general aviation lives with a fairly minimal system.A gas gauge sensor that calculates whether you have enough fuel to get to your destination + reserve.  Avionics where you input your personal minimums like crosswinds and weather and it warns you if you're about to accept a landing or flight-plan that violates those.  Encode that data and send it to ATC via transponder so valuable comm bandwidth is not lost asking for fuel status when emergencies occur.A gear down warning.  It's ridiculous that we still have so many belly landings and consider it a \"good\" to rely on training and human memory to prevent them.  How much cheaper would complex airplanes be if we didn't have the crazy insurance rates due to this?Angle-of-attack and spin warnings.  It's ridiculous that even $1mil+ Cirrus planes can't detect when you're too slow in a base to final turn and sound a warning before you spin.  We have the technology, it's foolish to depend on a decades old stall horn!A system that parses all the hundreds of notams and filters out the important ones.\n \nreply",
      "> For example it's crazy that most pilots are still taught to calculate W&B using printed charts and approximate takeoff performance.We're taught that so we know what we're doing. Then we open our iPhone app (EFB or electronic flightbag) and do it there. In fact, part of the reason we do it there is because most of them phone home to their maker and log that we did it. So if there's an accident people can know \"well they did their W&B\"> the rest of the thingsRegulations have made aviation so expensive that it's ridiculous. A lot of airplanes flying today are flying with their same avionics from 60 years ago because upgrading is expensive. To get the gas calculation you mention would require a certified GPS (on the low end from Garmin that's $6,000) and an engine monitor ($5,500 from Garmin), plus installation costs of another few thousand dollars.Most planes have a gear down warning (ie, 3 indicator lights) but their original \"bitching betty\" is hard to hear because we now wear ear protection when we fly and have noise cancelling headphones. You probably can't even integrate that with a new Garmin system because they've gone full encrypted CANBUS to lock out integrations.re: spin warnings - not sure what to tell you there. Stall speed is based on weight and configuration (flaps) and 99% of GA planes have no idea what they weigh or if their flaps are deployed.But again, it's not that we can't do those things, it's just that they're completely cost prohibitive. Getting anything certified today is a flippin' nightmare and at too high of a cost to ever break even.\n \nreply",
      "Stall speed is a misnomer.Stalls occur due to exceeding the critical angle of attack and can happen at ANY speed.\n \nreply",
      "I think that was his point. To calculate if you are going to stall in a final, you need to know your weight and flap setting to see if you will exceed the critical angle of attack when doing the turn, exactly because the speed isn\u2019t constant.\n \nreply",
      "Whoever is downvoting this: Stop. It's the key point here. Planes don't need to know their weight to produce a stall warning; AoA is a great metric, and GA planes not having an indicator or warning based on it is astonishing.\n \nreply",
      "I thought the point was predicting a stall by knowing the approach speed will be so low that you will stall, not detecting a stall just before it happens\n \nreply",
      "You can't control the weather, if there's a micro burst during landing you are in trouble.\n \nreply",
      "While some base to final stall accidents are as simple as you make it sound a good chunk of them happen when you try to come OUT of the turn at an already slow speed increasing the angle of attack on one wing alone. The stall and subsequent spin catches the pilot entirely surprised unaware of why they\u2019re even stalling and with very little escape bandwidth.The FAA has been trying for better angle of attack instrumentation but what I described above isn\u2019t an easy fix with technology.When you talk to pilots who inadvertently stall spin and lived to tell the tale most of them will tell you they didn\u2019t even recognize that they were in a stall. That\u2019s where the problem starts.\n \nreply",
      "Right but I don't know of any GA stall/spin warning system that takes into account pilot input.  Even simple sensors are lacking, for example accelerometers are nearly free yet GA planes give you no warning that you're in a skid.Similarly given yoke input, bank angle and speed you could warn of an impending stall well before it actually happens with a few position encoder sensors.  As you point out, the current system relies on pilots recognizing a stall which is a foolish thing to rely on and almost all GA stall warning sensors are only on one of the wings and require actual airflow disruption to work.  In many cases that is already too late or the other wing could stall first.  The calculation doesn't even have to be perfect since most pilots want plenty of margin of safety on a base to final turn.  I'd much rather have a false alarm + go-around than an inadvertent spin.The collision thing is also ridiculously irrational.  The FAA requires drones over half a pound to continually transmit their location yet somehow considers it sufficiently safe for planes to fly without a radio nor transponder around most of the airports in the US relying only on pilots looking out the window.It's just disappointing that the vast majority of GA accidents could be completely avoided with slightly better avionics.\n \nreply"
    ],
    "link": "item?id=41163382",
    "first_paragraph": ""
  },
  {
    "title": "Can we stop the decline of monarch butterflies and other pollinators? (wisfarmer.com)",
    "points": 107,
    "submitter": "speckx",
    "submit_time": "2024-08-05T20:30:01",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=41165273",
    "comments": [
      "My wife really loves Monarchs so we have planted a garden of milkweed and butterfly bushes. Monarchs will lay their eggs and then we make sure the caterpillars are doing well and have plenty of food. When they reach 5th instar and look for a place to turn into a chrysalis, we put them in a mesh enclosure to keep them safe and then release them once they emerge as butterflies!It's been such an exciting thing to do every year and the kids love helping out too. It's a fun, satisfying, and easy way to help out! Highly recommend :)\n \nreply",
      "FYI to anyone out there considering this- don't plant tropical milkweed:\"Another problem with tropical milkweed is that it harbors a one-celled parasite, Ophryocystis elektroscirrha, called OE for short. Because tropical milkweed does not die out in winter, the parasite does not die back either. Monarchs with large numbers of this parasite \u2013 which coevolved with monarchs and does not infect other species \u2013 are born with crumpled wings and cannot fly; the less infected are smaller, have shorter lifespans, fly poorly or are unsuccessful at mating. Only the healthiest butterflies reach overwintering areas in Mexico; butterflies with this parasite do not survive long migrations. \"https://www.cambridgeday.com/2024/08/03/more-abut-monarch-bu...\n \nreply",
      "Additionally don't plant butterfly bush, it's considered an invasive noxious weed and illegal in a few US states (at least Washington and Oregon, possibly New York). https://invasivespecies.wa.gov/wp-content/uploads/2019/07/Bu...\n \nreply",
      "We plant both \"swamp\" milkweed (Asclepias incarnata) and common milkweed (Asclepias syriaca) and the monarchs seem to vastly prefer the former for their babies. The one disadvantage, depending on how much you hate bugs, is that the swamp milkweed attracts a large variety of other polinators including various bees, flies, and some scary looking though harmless wasps[1].[1] https://en.wikipedia.org/wiki/Sphex_ichneumoneus\n \nreply",
      "Thank you for doing this.I'd like to do the same. Any suggestions for getting started?\n \nreply",
      "Not who you replied to, but we do this with our kids. The only things are you need are a milkweed patch (there are many varieties besides the big ugly broad-leaf ones you see everywhere) and and a mesh enclosure off Amazon for a few bucks. The process is:You go out, look for the tiny eggs on the milkweed, bring the milkweed leaves in, wait for them to hatch, and bring in fresh milkweed leaves for food once a day. We put them in a paper-towel-lined baking pan so that they have something soft to crawl on if they wander off to taste-test new leaf. They start out rather tiny and grow to into big fat caterpillars. Eventually they stop eating to go on walkabout and anchor themselves somewhere near the top of the enclosure. (Sometimes they are dumb and you have to relocate them with pins or tape.) Once they emerge as butterflies, set them free.We do black swallowtails too. They like dill and parsely.We never get tired of it. We have had 20-something butterflies at a time in a 2-sqft enclosure.\n \nreply",
      "Not sure if the wild milkweed out here in VT is the \"big ugly broad-leaf one\", but I think they are amazing plants.  And I love the alien-looking pods with the almost fractal arrangement of fluff seeds inside. The flowers are interesting too if only because of their brevity, they only last a few days. I love watching the milkweed grow over the summer.  Burdock too.  Incredible plants.\n \nreply",
      "Do you really need to go though all this trouble? We just plant a bunch of swan plants (milkweed) and watch the caterpillar and monarch populations go nuts. Add a bunch of flowers they like too (like zinnias) and that's about all I do.\n \nreply",
      "Obligatory comment to avoid planting Asclepias curassavica (aka tropical milkweed, often found in big box stores), in favor of any of the native species.For the healthiest to butterfly option, your milkweed should die back yearly in whatever climate you plant it.This helps encourage butterflies to migrate at the appropriate time and prevents parasite load from building up.https://www.science.org/content/article/plan-save-monarch-bu...Alternatively, you can cut it back yearly... but safer to just get ahold of a local species.\n \nreply",
      "I'd suggest doing some research before planting stuff.  I recently read that it's suggested to not plant milkweed (and to be ensure you cut it back seasonally if milkweed is appropriate) if you live in certain areas as it may otherwise disrupt their migration.If you're looking to attract butterflies there are other endangered butterflies that can use your help.  E.g. the Misison blue butterfly likes certain species of lupine.  Black swallowtails, while not endangered, love dill.  Don't underestimate how much even just a couple caterpillars will eat.Other fauna seem a lot less picky.  The hummingbirds out here seem to like the natives and \"exotics\" equally.  The leafcutter and carpenter bees too.  If you're in California, Calscape (dot org) is a great resource.  And if you're in the Bay Area there are plenty of nurseries that specialize in native landscaping that can offer guidance.  In the LA area, check out the Theodore Payne Foundation.\n \nreply"
    ],
    "link": "https://www.wisfarmer.com/story/news/2024/08/05/can-we-stop-the-decline-of-monarch-butterflies-and-other-pollinators/74638545007/",
    "first_paragraph": "If you have noticed fewer monarch butterflies fluttering around the yard this summer, you're not alone. Several butterfly aficionados recently shared their concerns during a Facebook discussion on Monarch Madness in Wisconsin.Because just 5% of monarch eggs survive to become butterflies, conservation-minded people like Nina Bottomley of Elkhorn is trying to help. She says the number of monarch butterflies she's raised from eggs and newly hatched caterpillars has plummeted alarmingly.\"I went from 124 down to nearly none!\" she posted. \"What's going on?\"PJ Liesch, director of the University of Wisconsin-Madison's Insect Diagnostics Lab, says he's heard several reports of general pollinator activity and numbers \u2014 including bees \u2014 being down this summer. Unfortunately, reasons for the decline are many.In early fall, monarchs begin their 2,500-mile migration to the overwintering grounds in central Mexico. The fragile butterflies face ever-changing weather conditions along the way and declini"
  },
  {
    "title": "Andy Warhol's lost Amiga art found (homeip.net)",
    "points": 356,
    "submitter": "todsacerdoti",
    "submit_time": "2024-08-05T15:33:30",
    "num_comments": 122,
    "comments_url": "https://news.ycombinator.com/item?id=41162311",
    "comments": [
      "\"The World Premiere of the Amiga (1985, Andy Warhol, Debbie Harry): Possibly wanting to one-up the Apple Macintosh launch in 1984, the Amiga 1000 debuted at a black-tie event held at the Vivian Beaumont Theater at Lincoln Center in New York City on July 23rd, 1985. (...)\"Warhol paints Debbie on stage:https://youtu.be/_QST1ZAJ29o?t=719\n \nreply",
      "Keep in mind that the footage of the computer starting at 13:03 has now been revealed from this article to be from an earlier rehearsal. Now I understand why there's this cheesy fly in effect!\n \nreply",
      "Last year I was really into non-ebay auctions.  Basically traditional auctions that were also online.I got super excited when I saw a Commodore 64 was coming up soon, until I noticed the starting price was around $100,000.   It was actually for a collection of unreleased digital art from Andy Warhol, and they were throwing in the computer for free.  Apparently there is a lot of it that they still haven\u2019t sorted through.I don\u2019t know anything about art, I was just bummed it wasn\u2019t a cheap retro computer.\n \nreply",
      "Yeah I have a couple of commodores in my storage left there for 25 years, probably even more, I can\u2019t even be bothered to make time to go there to throw them away\n \nreply",
      "Although C64 and Amiga are not rare by any means (although the Amiga, if you're in the US, wasn't that much of a success in the US compared to the UK/EU), they're highly thought after items. There's no reason to throw them away: just message a forum/board and say \"free Commodore in XXX, must come pick them\" and some shall gladly give them a second life.\n \nreply",
      "Or take a look at ebay. Filter by already sold.Even in non-working or untested condition, they fetch quite a bit of money.\n \nreply",
      "You might check capacitors and it\u2019s possible the power supply is bad.\n \nreply",
      "If they're Commodore 64's you definitely don't want the use the stock power supplies. Their voltages can drift and can destroy otherwise working hardware.\n \nreply",
      "Every serious retro collector has become an expert on power supplies and capacitors. It is very cool to see and I'm glad it is the power supply and not the more delicate components, at least so far.\n \nreply",
      "Please don't throw them away. Someone will be more than happy to take them off your hands. They are like gold to some people. If you're in the Los Angeles area I'd pay you to \"go there\" to let me take them off your hands.\n \nreply"
    ],
    "link": "https://dfarq.homeip.net/andy-warhols-lost-amiga-art-found/",
    "first_paragraph": ""
  },
  {
    "title": "The Composer Has No Clothes (thebaffler.com)",
    "points": 18,
    "submitter": "tintinnabula",
    "submit_time": "2024-08-05T22:58:06",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://thebaffler.com/latest/the-composer-has-no-clothes-brown",
    "first_paragraph": ""
  },
  {
    "title": "Debugging a rustc segfault on Illumos (sunshowers.io)",
    "points": 74,
    "submitter": "steveklabnik",
    "submit_time": "2024-08-05T19:54:45",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=41164885",
    "comments": [
      "(heavily paraphrasing)> [the core dump is supposed to be in the CWD, and named core, but isn't; what gives?]Followed by,  $ find / -name core -type f\n\nIs a sort of hilarious brute force solution. But it demonstrates a particular kind of problem, where  /-- requires -- evidence\n  |                   ^\n  v                   |\n  answer -- requires -/\n\nThese are pesky. The brute force search is a good idea, in that it breaks that cycle of almost needing to know the answer in order to discover it. (Unless you can surmise that the CWD is the crate dir, but let's assume that we don't want to depend on having such a moment of sheet \"eureka!\".)> But there are also other bits of evidence that this theory doesn\u2019t explain, or even cuts against. (This is what makes post-mortem debugging exciting! There are often contradictory-seeming pieces of information that need to be explained.)I wish more people appreciated this; too many people are apt to sweet such discrepancies under the rug. This post does a good job on not just following through on them, but also showing how figuring some of them out (\"why is our stack weird?\") leads to the key insights: \"oh we're using stacker and \u2026 $the_bug\".I do wonder how the author managed to notice that line in a 1.5k line stack trace, though. The \"abrupt\" address change would have probably gone unnoticed by me. (The only saving grace being a.) it's close to the bottom b.) most of the rest is repetitive, an artifact of a recursive descent parser recursing, and if we just consider that repetition \"one chunk\", it gets a lot smaller. I still dunno if I'd've seen it, though.)\n \nreply",
      "It was actually my coworker Joshua who first noticed that, and then that dredged up a long-forgotten memory in me of having seen stacker on crates.io. Many eyes make bugs shallow!Agree about the power of brute force solutions! When you're struggling to get a foothold, those kinds of approaches are extremely helpful. For the cwd thing specifically, it was clear in hindsight, and I should have known about it (having written nextest which has to handle crate cwds carefully). But I don't beat myself up too much over it, and now I know where to look next time this happens.\n \nreply",
      "I am (obviously?) biased, but this is a great read by Rain, as it takes the reader through not just some of the illumos tooling, but also how compilers need to bootstrap themselves -- and why heterogeneous platforms are important. (As Rain elaborates in the piece, this issue was seen on illumos, but is in fact lurking on other platforms.)\n \nreply",
      "These are easily one of my favourite types of posts (and this one was particularly gratifying). I wish I could go down these rabbit holes every day!> [...] and why heterogeneous platforms are importantThis prompts me to wonder vaguely whether there's any untapped juice in fuzzing approaches that might be relevant here. As in, how much of the platform (including configuration and heuristics, and so on) could be fuzzed as program input?\n \nreply",
      "Thank you, glad you appreciated the post! I love writing up debugging/incident reports and this one was just really fun.Regarding fuzzing... maybe? I've wondered that a couple of times myself but in reality there are really just a finite number of platforms, and so much of this is determined at compile time by library call availability. But I'm probably not thinking as deeply about this as someone could be, and I'd be interested to hear other folks' thoughts.\n \nreply",
      "Thanks for the kind words, Bryan! The illumos debugging tools continue to blow my mind.\n \nreply",
      "It makes me wonder though if illumos is worth it for a relatively small company to maintain. This bug came out of the larger ecosystem not knowing what to do for a niche OS.\n \nreply",
      "What we're doing inherently requires deep integration up and down the stack. We'd still have to be doing OS-level work even if we used another operating system. But then we'd be at the mercy of upstream of accepting patches, or keeping our own fork, and at that point, you're basically at the same spot we are now, but with less overall control.RFD 26 talked about the context around this choice: https://rfd.shared.oxide.computer/rfd/0026There's a lot more to it than just this one thing I mentioned :)\n \nreply",
      "On top of what Steve said, illumos does support all of the required APIs here, but the Rust libc crate was just missing definitions for them. It's not a tremendously exotic platform the way something like Haiku is.Edit: also worth pointing out (again) that the bug actually exists everywhere -- it was just being masked on the other platforms.\n \nreply",
      "Amazing read! This article beautifully guides you through each step and make sure you did get enough context for the next step. Bookmarking this!\n \nreply"
    ],
    "link": "https://sunshowers.io/posts/rustc-segfault-illumos/",
    "first_paragraph": "At Oxide, we use Helios as\nthe base OS for the cloud computers we sell. Helios is a distribution of\nillumos, a Unix-based operating system descended from Solaris.As someone who learned illumos on the job, I\u2019ve been really impressed by the powerful debugging\ntools it provides. I had a chance to use some of them recently to track down a segmentation\nfault in the Rust compiler, with the help of\nseveral of my colleagues. I learned a lot from the process, and I thought I\u2019d write about it!I\u2019m writing this post for an audience of curious technologists who aren\u2019t necessarily familiar with\nsystems work. If you\u2019re an experienced systems developer, parts of it are likely familiar to\nyou\u2014feel free to skip over them.A couple of weeks ago, I wanted to make a change to the Rust standard library on illumos. I logged\ninto my illumos box and cloned the Rust repository (revision\n2d5a628). Following the setup\ninstructions, I configured the rustc build system with the library build profile.When I went to r"
  },
  {
    "title": "Show HN: Iso20022.js \u2013 Create payments in 3 lines of code (iso20022js.com)",
    "points": 149,
    "submitter": "svapnil",
    "submit_time": "2024-08-05T17:55:41",
    "num_comments": 73,
    "comments_url": "https://news.ycombinator.com/item?id=41163645",
    "comments": [
      "Github link: https://github.com/svapnil/iso20022.js",
      "The problem with this standard is all the free text and bank specific fields that banks will use instead of the standard. One bank I integrated with had the equivalent of \"Our fee is 5.65\" in a text field which you had to parse, instead of the field for fees. Of course, the language of that string could also change. Fun times\n \nreply",
      "That does sound really really fun..\nWhat's great about XML is that free text / bank specific fields can be handled elegantly with XML's extensible structure. That is why I think ISO20022 is here to stay.That said, this library is made to be extensible. One day I think it will even be able to encapsulate any type of bank. For example, imagine bofaISO20022.createACHPaymentInitation or something\n \nreply",
      "What XML has to do with bank-specific messages that have to be parsed and processed? It\u2019s just a markup format.\n \nreply",
      "It's the extensible nature of XML that gives it an advantage. You can add custom elements and attributes whilst conforming to the base schema.Granted, XML isn't the only format where this is possible. You can sort of achieve it with JSON, though XML's namespace system helps deal with name collisions. Adding bank-specific messages wouldn't be possible (or would be difficult) with fixed-column formats, for example, unless they had been specifically designed to be extended.\n \nreply",
      "Banks add their own features to the spec - imagine they want to add a new \"Bank only\" attribute that makes their XML schema differentiated and better in some way.ISO20022 / XML allows this to be possible without breaking anything. In the past payment formats used to be fixed width text files - impossible to change or improve functionality for\n \nreply",
      "Custom schema means nothing against improper implementation\n \nreply",
      "You can have extensible structure and fields with JSON Schema, gRPC, Cap\u2019n Proto, etc. There\u2019s nothing XML-specific about that.The only thing XML gives you over any of those formats is unstructured mixing of text and data, which is more a foot-gun than anything. Oh, and of course, being significantly more verbose.\n \nreply",
      "A niche thing maybe, but XML has comments, which I appreciate.\n \nreply",
      "Other than bank specific custom extensions, another problem with this standard is its scope and, consequently, its size \u2013 it is vast. ISO 20022 breaks down into over 700 what they call \u00abmessages\u00bb that cover pretty much everything, from the interbank settlements to bank-to-customer account statements.Another challenge is that different banks may use slightly different versions of the standard messages that are enunciated via the implementation specific concrete XML namespace in the xmlns attribute of the message envelope.Overall, ISO 20022 is an improvement over MT940/MT942 and friends, although it is not easy to use.\n \nreply"
    ],
    "link": "https://www.iso20022js.com/",
    "first_paragraph": "iso20022.js is a low-dependency, open-source node library that helps companies communicate with banks using open ISO20022 standards. Try sending a SWIFT payment in three lines of code.Built with \u2764\ufe0f in NYC by Woodside Labs LLC."
  },
  {
    "title": "A new type of neural network is more interpretable (ieee.org)",
    "points": 184,
    "submitter": "pseudolus",
    "submit_time": "2024-08-05T16:15:19",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=41162676",
    "comments": [
      "I've tried out and written about[1] KANs on some small-scale modeling, comparing them to vanilla neural networks, as previously discussed here: https://news.ycombinator.com/item?id=40855028.My main finding was that KANs are very tricky to train compared to NNs. It's usually possible to get per-parameter loss roughly on par with NNs, but it requires a lot of hyperparameter tuning and extra tricks in the KAN architecture. In comparison, vanilla NNs were much easier to train and worked well under a much broader set of conditions.Some people commented that we've invested an incredible amount of effort into getting really good at training NNs efficiently, and many of the things in ML libraries (optimizers like Adam, for example) are designed and optimized specifically for NNs. For that reason, it's not really a good apples-to-apples comparison.I think there's definitely potential in KANs, but they aren't a magic bullet. I'm also a bit dubious about interpretability claims; the splines that are usually used for KANs don't really offer much more insight to me than just analyzing the output of a neuron in a lower layer of a NN.[1] https://cprimozic.net/blog/trying-out-kans/\n \nreply",
      "Not just the optimizers, but the initialization schemes for neural networks have been explicitly tuned for stable training of neural nets with traditional activation functions. I'm not sure as much work has gone into intialization for KANsI 100% agree with the idea that these won't be any more interpretable and I've never understood the argument that they would be. Sure, if the NN was a single neuron I can see it, but as soon as you start composing these things you lose all interpretability imo\n \nreply",
      "KANs can be modeled as just another activation architecture in normal MLPs, which is of course not surprising, since they are very flexible. I made a chart of different types of architectures here: https://x.com/thomasahle/status/1796902311765434694Curiously KANs are not very efficient when implemented with normal matrix multiplications in Pytorch, say. But with a custom cuda kernel, or using torch.compile they can be very fast: https://x.com/thomasahle/status/1798408687981297844\n \nreply",
      "Side question:Can people this deep in the field read that visualization with all the formulas and actually grok what's going on? I'm trying to understand just how far behind I am from the average math person (obviously very very very far, but quantifiable lol)\n \nreply",
      "The tensor diagrams are not quite standard (yet). That's why I also include more \"classical\" neural network diagrams next to them.I've recently been working on a library for doing automatic manipulation and differentiation of tensor diagrams (https://github.com/thomasahle/tensorgrad), and to me they are clearly a cleaner notation.For a beautiful introduction to tensor networks, see also Jordan Taylor's blog post (https://www.lesswrong.com/posts/BQKKQiBmc63fwjDrj/graphical-...)\n \nreply",
      "These remind me of interaction combinators [1], which are being used in the Bend programming language [1]. I think it'd be good for the standard to also be a valid interaction net.[1]: https://core.ac.uk/download/pdf/81113716.pdf[2]: https://news.ycombinator.com/item?id=40390287\n \nreply",
      "You don't need to be more good in math than in high school. AI is a chain of functions and you derive over those to get to the loss-function (gradient) to tell you which parameters to  change to get a better result (simplified!).Now this structure of functions is different in each implementations, but the type of function is quite similar - even though a large model will combine billions of those nodes and weights. Those visualizations tell you f.e. that some models connect neurons back to ones earlier in the chain to better remember a state. But the activation function is usually a weight and threshold.KAN changes the functions on the edges to more sophisticated ones than just \"multiply by 0.x\" and uses known physical formulas that you can actually explain to a human instead of the result coming from 100x different weights which tell you nothing.The language models we use currently may map how your brain works, but how strong the neurons are connected and to which others does not tell you anything. Instead a computer can chain different functions like you would chain a normal work task and explain each step to you / combine those learned routines on different tasks.I am by no means an expert in this field, but i do a lot of category theory, especially for the reason that i wanted a more explainable neuron network. So take my pov with a grain of salt, but please don't be discouraged to learn this. If you can program a little and remember some calculus you can definitely grasp these concepts after learning the vocabulary!\n \nreply",
      "> You don't need to be more good in math than in high school.I'm very tired of this... it needs to stop as it literally hinders ML progress1) I know one (ONE) person who took multivariate calculus in high school. They did so by going to the local community college. I know zero people who took linear algebra. I just checked the listing of my old high school. Over a decade later neither multivariate calculus nor linear algebra is offered.2) There's something I like to tell my students  You don't need math to train a good model, but you do need to know math to know why your model is wrong.\n\nI'm sure many here recognize the reference[0], but being able to make a model that performs successfully on a test set[1] is not always meaningful. For example, about a year ago I was working a very big tech firm and increased their model's capacity on customer data by over 200% with a model that performed worse on their \"test set\". No additional data was used, nor did I make any changes to the architecture. Figure that out without math. (note, I was able to predict poor generalization performance PRIOR to my changes and accurately predict my model's significantly higher generalization performance)3) Math isn't just writing calculations down. That's part of it -- a big part -- but the concepts are critical. And to truly understand those concepts, you at some point need to do these calculations. Because at the end of the day, math is a language[2].4) Just because the simplified view is not mathematically intensive does not mean math isn't important nor does it mean there isn't extremely complex mathematics under the hood. You're only explaining the mathematics in a simple way that is only about the updating process. There's a lot more to ML. And this should obviously be true since we consider them \"black boxes\"[3]. A lack of interpretability is not due to an immutable law, but due to our lack of understanding of a highly complex system. Yes, maybe each action in that system is simple, but if that meant the system as a whole was simple then I welcome you to develop a TOE for physics. Emergence is useful but also a pain in the ass[4].[0] https://en.wikipedia.org/wiki/All_models_are_wrong[1] For one, this is more accurately called a validation set. Test sets are held out. No more tuning. You're done. This is self-referential to my point.[2] If you want to fight me on this, at least demonstrate to me you have taken an abstract algebra course and understand ideals and rings. Even better if axioms and set theory. I accept other positions, but too many argue from the basis of physics without understanding the difference between a physics and physics. Just because math is the language of physics does not mean math (or even physics) is inherently an objective principle (physics is a model).[3] I hate this term. They are not black, but they are opaque. Which is to say that there is _some_ transparency.[4] I am using the term \"emergence\" in the way a physicist would, not what you've seen in an ML paper. Why? Well read point 4 again starting at footnote [3].\n \nreply",
      "I know many people who did take multivariate calculus, group/ring theory, and thermodynamics in high school, and think this should be the norm. I believe I consider \"high school\" math what most people consider \"undergraduate\", and everything up to linear algebra goes under \"middle school\" in my mental model (ages 12-14). So, I'm probably one of those people propagating, \"ML math is easy, you only need a high school knowledge!\" but I acknowledge that's still more than most people ever learn.\n \nreply",
      "> 1) I know one (ONE) person who took multivariate calculus in high school.Unless you are specifically dealing with intractable Bayesian integral problems, the multivariate calculus involved in NNs are primarily differentiation, not integration. The fun problems like boundary conditions and Stokes/Green that makes up the meat of multivariable calculus  don't truly apply when you are dealing with differentiation only. In other words you only need the parts of calc 2/3 that can be taught in an afternoon, not the truly difficult parts.> I'm sure many here recognize the reference[0], but being able to make a model that performs successfully on a test set[1] is not always meaningful. (sic) ...[2] If you want to fight me on this, at least demonstrate to me you have taken an abstract algebra course and understand ideals and rings. Even better if axioms and set theory.Doesn't matter, if it creates value, it is sufficiently correct for all intents and purposes. Pray tell me how discrete math and abstract algebra has anything to do with day to day ML research. If you want to appeal to physics sure, plenty of Ising models, energy functions, and belief propagation in ML but you have lost all credibility bringing up discrete math.Again those correlation tests you use to fact check your model are primarily linear frequentist models. Most statistics practitioners outside of graduate research will just be plugging formulas, not doing research level proofs.> Just because the simplified view is not mathematically intensive does not mean math isn't important nor does it mean there isn't extremely complex mathematics under the hood. You're only explaining the mathematics in a simple way that is only about the updating process. There's a lot more to ML.Are you sure? The traditional linear algebra (and similar) models never (or rarely) outperformed neural networks, except perhaps on efficiency, absent hardware acceleration and all other things being equal. A flapping bird wing is beautiful from a bioengineering point of view but the aerospace industry is powered by dumb (mostly) static airfoils. Just because something is elegant doesn't mean it solves problems. A scaled up CNN is about as boring a NN can get, yet it beats the pants off all those traditional computer vision algorithms that I am sure contain way more \"discrete math and abstract algebra\".That being said, more knowledge is always a good thing, but I am not naive enough to believe that ML research can only be advanced by people with \"mathematical maturity\". It's still in the highly empirical stage where we experimentation (regardless of whether it's guided by mathematical intuition) dominates. I have seen plenty of interesting ML results from folks who don't know what ELBOs and KL divergences are.\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/kan-neural-network",
    "first_paragraph": "The August 2024 issue of IEEE Spectrum is here!Kolmogorov-Arnold Networks could point physicists to new hypothesesArtificial neural networks\u2014algorithms inspired by biological brains\u2014are at the center of modern artificial intelligence, behind both chatbots and image generators. But with their many neurons, they can be black boxes, their inner workings uninterpretable to users. Researchers have now created a fundamentally new way to make neural networks that in some ways surpasses traditional systems. These new networks are more interpretable and also more accurate, proponents say, even when they\u2019re smaller. Their developers say the way they learn to represent physics data concisely could help scientists uncover new laws of nature. \u201cIt\u2019s great to see that there is a new architecture on the table.\u201d \u2014Brice M\u00e9nard, Johns Hopkins UniversityFor the past decade or more, engineers have mostly tweaked neural-network designs through trial and error, says Brice M\u00e9nard, a physicist at Johns Hopkins"
  },
  {
    "title": "I don't know how CPUs work so I simulated one in code (2019) (djharper.dev)",
    "points": 46,
    "submitter": "azefiel",
    "submit_time": "2024-08-01T13:54:00",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=41129081",
    "comments": [
      "So, what this project misses, which is quite hard to capture if you think of gates being just on off switches, is the fact that signals are not instantaneous, and everything runs in parallel.As the AND gate 4 gates up the chain switches the NOT gate 4 gates down the chain starts to send different and unstable signals which may or may not be interpreted as a 1 or 0 in the downstream gate.That's the reason computers have a clock, to make sure all transistors in a given stage of a CPU reach a steady state before moving on to the next instruction.This is why it's probably a good idea to work with a HDL instead of just trying to wing it.\n \nreply",
      "The thing that got me into CS was building a CPU in Minecraft redstone, which is surprisingly good at being a logic simulator.You only got (back in my day) NOT (the torch) and OR gates (wiring next to each other), and everything was built out of them. Signal repeaters had delay, the gates had delay, so you naturally had to build a clock to synchronise it all.The main benefit that I enjoyed was that you could see the signals physically propagating from one end of your cpu to the other in real time (clock cycles were in the range of 1-4 seconds), flowing from the instructing fetching, decoding, dispatch, logic, writing to back memory, etc. Seeing signals slowly crawl from one end to the other naturally introduced you to pipelining (it even happens naturally if you increase the clock without thinking about what will actually happen: the next instruction starts decoding before the previous one is done, more parts of your cpu start lighting up at once, and oops now you have a pipelined cpu).Even the scales match; many learners are surprised that the actual ALU is the tiny thing in the corner that you can barely see, and all the giant stacks of rows you actually saw is memory and cache. Even in minecraft, most of your CPU is not logic :)Also really taught me how asics are much faster: you could build a tiny compact multiplier that multiplied hundreds of times faster than your giant cpu running a multiplication algorithm.Looks like they community I learnt from is still around actually https://openredstone.org/, even if all the old forum posts seems to be gone now. There were some geniuses on that place building computers with multi-tier caches, full pipelining, out-of-order (albeit primitive) execution, SIMD-capable CPUs, all in redstone.\n \nreply",
      "The game Silicon Zeroes ( https://store.steampowered.com/app/684270/Silicon_Zeroes/ ) teaches this. It starts out with components computing their outputs instantaneously, but then introduces the concept of microticks such that the output of a component is unstable until its inputs are stable enough for some time, so the clock speed must be adjusted according to the largest delay in all circuit paths. The game starts off with simple circuits but very quickly becomes about making a CPU, although the ISA is hard-coded into the game and very small.Another game Turing Complete ( https://store.steampowered.com/app/1444480/Turing_Complete/ , https://news.ycombinator.com/item?id=38925307 ) lets you build a CPU from basic gates and a much larger (and customizable) instruction set. It also has the concept of gate delays, thopugh it doesn't visually show the unstable output as Silicon Zeroes does.\n \nreply",
      "Turing Complete is fantastic! It's a very much one of those games I'd say is like Kerbal Space Program, in the sense that you can be technical and have encountered all the concepts before, but bridging the gap where you actually grok what's happening intuitively isn't quite a leap you can make.\n \nreply",
      "Propagation delay, besides being physically unavoidable, is actually necessary for things like latches (and thus memory) to work, since they rely on feedback loops. The https://en.wikipedia.org/wiki/Ring_oscillator is another example.\n \nreply",
      "Discussed at the time:I don't know how CPUs work so I simulated one in code - https://news.ycombinator.com/item?id=19969321 - May 2019 (172 comments)\n \nreply",
      "I was once shown a dos-based CPU simulator back in the mid/late 90s.From memory it showed instruction decode, execution, cache and memory.Unfortunately I've never been able to find it, because all the google results are about running DOS games and/or DOSBox.\n \nreply",
      "It is probably on one of the SIMTEL shovelware cdroms. https://archive.org/details/SIMTEL_0692A search engine won't help you, but a local llm might.\n \nreply",
      "If, for fun, I wanted to train an ML model on a ton of CPU instructions (which each predicted state/label being the state of the registers), does anyone have any clue how to gather that kind of data?\n \nreply",
      "QEMU isn\u2019t cycle-accurate, but would be a good start (and probably good enough). Just run some benchmarks and whatnot there, and use a tracing tool like Cannoli to capture instructions.If you need real instructions (without an emulator like qemu doing its own translation and messing up timing), you could use a simulator like Gem5. That\u2019s a bit more work and a lot more compute per simulated instruction.\n \nreply"
    ],
    "link": "https://djharper.dev/post/2019/05/21/i-dont-know-how-cpus-work-so-i-simulated-one-in-code/",
    "first_paragraph": ""
  },
  {
    "title": "Starting Hospice (jakeseliger.com)",
    "points": 876,
    "submitter": "jdkee",
    "submit_time": "2024-08-05T03:35:53",
    "num_comments": 117,
    "comments_url": "https://news.ycombinator.com/item?id=41157974",
    "comments": [
      "Hacker News, thank you for all the links and all the great reading. Now I have to say goodbye.I\u2019m with my wife Bess (https://bessstillman.substack.com/) and my brother Sam, and crying, but it is okay. At the end of Lord of the Rings Gandalf says to the hobbits, \"Go in peace! I will not say: do not weep; for not all tears are an evil.\u201d And that is how I feel now. Ending prematurely hurts, but all things must end, and my time to end is upon me.\n \nreply",
      "Thank you for your writing - its taught me a lot about a lot of things. One concrete highlight is how important patient agency is in the patient-doctor relationship - which you've written about a few times.I'm truly deeply sorry about this whole situation. Thank you for sharing all your knowledge.\n \nreply",
      "I hope too to accomplish at least two concrete ends:1. Help and educate other people who are suddenly facing the opaque clinical-trial system: https://bessstillman.substack.com/p/please-be-dying-but-not-...2. Ultimately, reform and speed FDA approval for fatal diseases like recurrent / metastatic head and neck cancers: https://jakeseliger.com/2024/01/29/the-dead-and-dying-at-the.... A drug like petosemtamab (MCLA-158), which I was on from Sept. 27 2023 to March 29 2024, should already be approved, instead of continuing to wander around in clinical trials.\n \nreply",
      "Have you discussed anything about targeted therapies?  For example, how the different genetic makeup of some tumors are used to treat them.  Keytruda comes to mind.https://www.keytruda.com/Antibody drug conjugates also seem to be discussed often:https://www.mdanderson.org/cancerwise/antibody-drug-conjugat...\n \nreply",
      "In his blog, he discusses Keytruda. https://jakeseliger.com/2023/12/19/what-if-things-go-right-w...\n \nreply",
      "I see where he discusses it.  He said it only works for a small group and not for him.He\u2019s mentions trying drugs that target EGFR mutations, which I believe tries to stop the blood supply to the tumor. Targeting specific protein receptors, like HER2 in breast cancer, seems to be promising.https://my.clevelandclinic.org/health/diseases/25213-her2-po...RAS is a target for (some) colon and pancreatic cancers:https://www.cancer.gov/research/key-initiatives/ras#:~:text=....\n \nreply",
      "Thank you and your wife for this. Will keep posting to HN her articles when they come out.https://archive.ph/bessstillman.substack.comWould also like to highlight the possibility of suing the FDA for screwing up clinical trials in general.Here's one case\nhttps://www.theatlantic.com/health/archive/2023/10/xocova-en...\n \nreply",
      "Jake, I am so, so sorry for everything you\u2019ve gone through and wish peace for you and the best for your loved ones. I\u2019ve followed your story here and always been touched by your candor. Thank you for all your contributions. I was rooting for a better outcome and am sorry that it hasn\u2019t arrived. Goodbye.\n \nreply",
      "Reading your updates has been important to me since I started seeing your posts.Thank you for taking the time and energy during the most difficult of circumstances to share your journey with the rest of us. I know it's given me a lot to think about and a lot to be grateful for.Best of luck to you and yours as you come to the end of the journey. You'll be in my thoughts.\n \nreply",
      "Jake, I have read your previous posts and am deeply saddened to see this post; if there are LOTR references to be made then perhaps the part in the Appendix where Aragorn tells Arwen that \"beyond the circles of the world there is more than memory\" can be mentioned.\n \nreply"
    ],
    "link": "https://jakeseliger.com/2024/08/04/starting-hospice-the-end/",
    "first_paragraph": "I\u2019m entering hospice. It\u2019s time, and realistically past time. The squamous cell carcinoma tumors are growing, and the two doses of spot radiation I got on June 10 and 12 have utterly destroyed whatever quality of life I had. This weekend, a nurse came by and did some planning with Bess and me. Our extensive efforts to find and start another clinical trial have turned out to be futile, and I\u2019ve withdrawn from the next-best potential clinical trial, BGB-A3055 in Dallas, at NEXT Oncology, because there\u2019s no feasible way for me to do it (the people at NEXT, however, are and have been amazing: if you\u2019re looking at clinical trials or live in Dallas, schedule a consult). HonorHealth in Scottsdale, where I live, has a TScan slot, but my physical condition remains terrible for essentially the reasons I\u2019ve written about so extensively that there\u2019s no need to belabor them. My days and nights are filled with unrelenting coughing, hacking, and pain. My whole jaw area is numb, likely from tumor grow"
  },
  {
    "title": "Replacing Liquid Metal on an Asus Zephyrus G15's CPU (flemesre.github.io)",
    "points": 36,
    "submitter": "geomaturge",
    "submit_time": "2024-08-05T20:15:21",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=41165117",
    "comments": [
      "If you have to put literal liquid metal in a laptop.... I think I'd rather have something with a lower TDP... Seems like it could be a reliability issue and leak eventually.\n \nreply",
      "I'm concerned that the increased occurrence of gallium (in the liquid state) in consumer devices could represent a sort of warranty time bomb. It's not supposed to leak out, and there isn't much of it, but if it does, you have a serious problem. It's only as good as the seal around it, which could age depending on the type of seal.\n \nreply",
      "Interesting, I have this laptop, and it's got recurrent problems with overheating and with certain keys on the keyboard not working.  Wonder if the liquid metal has escaped and shorted out certain keyboard circuits.\n \nreply",
      "IMHO they should just call it what it is, https://en.wikipedia.org/wiki/Galinstan instead of the vague \"liquid metal\" term, which probably evokes mercury and its negative connotations for a lot of people.Also, it's slightly odd to see the prominent country of origin markings on the CPU --- I've not seen recent Intels marked in the same way.\n \nreply",
      "I bet a solder sucker would work for picking up the droplets around the edge. Same job that they're made for, just at a lower temperature than usual.\n \nreply",
      "Or a syringe with a needle tip. Thats how a lot of the liquid metal TIMs are sold. Works great for picking up excess.\n \nreply",
      "Awesome write up. Thanks for sharing.\n \nreply",
      "Excellent write up! I also have a G15 (GA503RM), and the post made me wonder if I ought to be doing this as well. The temperature improvement makes a strong case. How much of a danger is the (nearly) escaping liquid metal on the CPU? Is this is something that could be frying a lot of Zepheri if it's not addressed?\n \nreply",
      "The LM escaping shouldn't be an issue with silicone over the on-package capacitors and foam around it. If you aren't throttling I wouldn't really bother.If you do end up attempting it removing the LM is easier with a syringe that can suction the larger blobs away. Could even just try to reapply the same LM you removed if it's just a pumpout issue\n \nreply",
      "Asus is the hidden gem in quality laptops.Where else are you getting $800 laptops with Nvidia 6gb vram?I've been using them for 1 decade for professional, personal, and gaming purposes. I only have added RAM/SSDs and changed batteries over the last 10 years.I wonder why they aren't so widespread. I imagine its a marketing thing, they just don't have the connections to US big business/schools like leveno/apple/hp does.\n \nreply"
    ],
    "link": "https://flemesre.github.io/posts/liquid-metal-replacement/",
    "first_paragraph": "I\u2019ve been using the Asus ROG Zephyrus G15 as my daily driver for a few years now. Laptops (gaming laptops especially) pack a lot of heat-generating components in a small and thin space, where the only airflow comes from the small blower fans in the system. In the case of the Zephyrus G15, there\u2019s approximately 120 Watts of output, with 80W-100W going to the GPU and the rest going to the CPU. The cooling systems built into these things are pretty impressive considering the severe lack of thermal mass compared to desktop systems (Noctua\u2019s NH-D15 cooler weighs half of the Zephyrus G15 at 980g without its fan, and that\u2019s just a CPU cooler - the laptop has to cool the GPU as well with that weight, there\u2019s the chassis, battery etc.).A key part of this cooling system is the thermal interface material (TIM), which is added between the heat-generating components and the heatsink to plug in any gaps or scratches between the two surfaces and maximise thermal transfer. In desktops and most laptops"
  },
  {
    "title": "Below the Root: A story, a computer game and my lifelong obsession (2015) (stahlmandesign.com)",
    "points": 46,
    "submitter": "olvy0",
    "submit_time": "2024-08-05T19:29:47",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=41164585",
    "comments": [
      "This was probably the first game I really loved and played to obsession (on an Apple ][)It was also where I discovered my first 'wall hack'. The apple version only loaded the current tile into memory - every time you stepped off screen, it would load the new tile from a floppy and reset your character position to the opposite edge (e.g. walk off the right edge, load the tile to the right and reset your character to the left side at the same height).I found out (largely by accident, initially) that if you removed the floppy, it would try to read a couple times, and then give up. It would still reset your character position to the other side of the screen but not update the actual tile graphics & platform locations. You could use this to cheat past otherwise impassible barriers - find a tile you could walk all the way from one edge of to the other at the same height as the barrier, take out the floppy, walk across the tile the right number of times so you are in the tile 'past' the barrier, then put the floppy back in and walk back so you're now on the other side. :)The version I played also had a charming bug(?) where if you underflowed a stat counter (e.g. by doing a 5 manna action when you only had 4 manna left) the game would print the message \"I have nothing more to give\" and then lock up, leaving it ambiguous whether it was your character that had nothing more to give, or the actual game.\n \nreply",
      "Great article\u2014I love that the author tried to reach out to the original creator of the game \"Below the Root,\" even though they ultimately failed to secure the rights.I'd encourage anyone with a desire to create an homage to a game they have nostalgia for to just reach out to the original authors. You'd be surprised at how often a simple request like this can lead to you being able to breathe new life into an old game.Story time. A favorite game of mine as a young child was an old DOS game called Redhooks' Revenge. It was essentially a graphical naval adventure where each player would take turns rolling dice and moving their ship. Landing on ports would allow you to conquer them Risk-style, and at various times you would be asked a maritime-themed trivia question depending on the square you landed on. Answering correctly would give you bonuses like rum, water, extra cannons, etc.My sister and I used to play it for hours on end to the point where we'd basically both memorized all the quiz questions. Decades later, I was messing around with the files, attempting to figure out a way to add additional trivia questions. I ended up reverse-engineering the weird binary format of the asset file to randomly inject customized trivia into the game every time it was started. You had to be really careful when injecting the questions not to accidentally alter any additional addresses in the file; otherwise, the game would just crash.Out of random curiosity, I looked up the original company, ImagiSOFT, and got in touch with the author, who happily gave me permission to develop an official sequel, Redhook's Revenge II: Call of Booty.https://specularrealms.com/redhook\n \nreply",
      "You called it \"Call of Booty?\" Great title. Funny.\n \nreply",
      "For me, it would be \"Theatre: An interactive night of horror\"[1], the first interactive fiction work I ever saw. I actually did email him 20+ years after it was published, asking for permission to remake it non-commercially, and he was like \"yeah ok whatever\". I haven't really gone anywhere with that, but perhaps one day I will.(What's more shocking is that he came from the same country as me and his company was named after a place I'd actually been to. That doesn't happen very often.)[1] https://ifdb.org/viewgame?id=bv8of8y9xeo7307g\n \nreply",
      "I have also been obsessed with \"Below the Root\" since it came out.Dale Disharoon/DeSharone had an odd insight into textures. Look at the ladders. Look at the vines, both those that can be climbed and those that can be cut with a \"trencher beak\". Look at the way the trees evoke growing wood. The visuals are much more evocative than should be possible at this resolution.Dale also did a Disney-licensed Apple II game based on \"The Jungle Book\" which is similar in character to \"Below the Root\" and a game based on \"Alice in Wonderland\". Both are strange and mystical and full of odd vines that remind me of the patterns left by the cellular automaton \"Langton's Ant\".All Dale's games were unfair and opaque but that was the state of the art.\n \nreply",
      "The author should just port the game to JavaScript and point to the original assets on GoG.Very sorry to hear about the authors passing, I would have loved to hear about what inspired them.\n \nreply",
      "The only link I clicked on in the article was supposed to be for The Dreamsong, and it was broken. But I found the game here: https://www.thedreamsong.com/\n \nreply",
      "I love the game aesthetics but the slippery controls make you feel like you took a bath in bacon fat.\n \nreply",
      "Wow... quite the obsession. BtR was a fun game, and I am looking forward to playing The Dreamsong!\n \nreply",
      "I absolutely loved Below the Root when I was a kid. I definitely shared the author's sense of wonder when playing. The game world felt expansive and mysterious. It was probably the closest to an \"open world game\" I ever played on an Apple II.I'm excited to try out the Commodore 64 version now that I know it was the original.\n \nreply"
    ],
    "link": "https://blog.stahlmandesign.com/below-the-root-a-story-a-computer-game-and-my-lifelong-obsession/",
    "first_paragraph": "The original box cover that contained the diskettes, map and manual. Another great example of how video-game art in the 80s never looked like the game.The back of the box usually showed some real screen shots.I was 10 years old when I discovered Below the Root. It was 1984, and it arrived as some trial educational software in the mail (the other game was T.Rex The Dinosaur Survival Adventure for the Apple II.)I got my friends hooked on it, and it wasn't easy to solve. Even though T.Rex was cool, it was quickly forgotten, while the mysteries in Below the Root just kept unfolding.Below the Root was perfect for kids our age: exploring a giant treehouse world filled with clues and mysteries. What any kid wants to do is climb ever higher, and in the game, it seemed that's where we were supposed to go. And so we went, high in the trees, to a tiny treehouse that was hard to reach. The first time we were turned back by a message that we needed a \"trencher beak\" to cut through the overgrowth.\nT"
  },
  {
    "title": "Google loses antitrust suit over search deals on phones (bloomberg.com)",
    "points": 643,
    "submitter": "rvz",
    "submit_time": "2024-08-05T18:58:16",
    "num_comments": 432,
    "comments_url": "https://news.ycombinator.com/item?id=41164240",
    "comments": [
      "We face a number of challenges simply letting our paying customers change their search engine:1. On iOS the list of allowed search engines is simply baked into OS, we have a fiddly extension that hooks outbound calls to /search and redirects them but I wish we didn't need to.2. On Chrome, we use an extension to change the default search engine and enable search auto-complete etc, but Google has a policy that such an extension can do one thing and one thing only, and recently removed our extension on account of that [1]. We rebuilt it to meet their needs but had a lot of back-and-forth because we included 'search by image' on a context menu item and the first reviewer felt that was a bridge too far. You'll note that Chrome provides such a context menu item for Google Image search out of the box.3. On Chrome for Linux, the default search engine API is not available, so Linux users have to configure it manually through a series of silly steps [2]. This is at least in keeping with most Linux experiences.There are other issues, but I say all this to highlight how surprisingly difficult it is to change this setting in a practical, consumer friendly way. It is most certainly this difficult by design, that's a lot of revenue to protect.1: https://news.ycombinator.com/item?id=410289242: https://github.com/kagisearch/chrome_extension_basic?tab=rea...\n \nreply",
      "I\u2019m one of the paying Kagi customer who wants to make Kagi my default iOS search engine, but cannot. It\u2019s maddening that even though I paid for both my iPhone / iPad and for Kagi, Apple for some reason makes it impossible for me to make this choice (that I already made by paying for Kagi).On Chrome at least this is possible, even if it\u2019s additional steps (I have not used the extension though there.)\n \nreply",
      "> I\u2019m one of the paying Kagi customer who wants to make Kagi my default iOS search engine, but cannotFYI, there is a workaround [1]. It's trash that Apple makes us do this. But you can.Granted, it nudged me to switch to Orion as my main desktop browser.[1] https://help.kagi.com/kagi/getting-started/setting-default.h...\n \nreply",
      "How\u2019s the stability on Orion lately?  I want to use it be plugin compatible was too dicey.\n \nreply",
      "So far (only an occasional user) it's been solid for me.  Though I've only been using it in depth every few weeks, and that's mostly to read manga online while waiting for stuff.\n \nreply",
      "Orion crashed way too often for me. I used Orion for a month or two and switched back to Firefox in early July.\n \nreply",
      "I too am a paying customer.  Surprisingly -- at least to me -- it's easy to do with Microsoft's Edge browser on iOS.  I use it as default on both iOS and macOS.\n \nreply",
      "It's only easy now because Microsoft had a lot of runs in with the US and EU regulators on this specific issue in the past.\n \nreply",
      "And because MS wants to increase its userbase on IOS and macOS. Meanwhile Windows Start Menu is locked to Edge and Bing.\n \nreply",
      "Correct. Minute userbase equals no leverage to extract tolls. If they ended up with the sort of dominant position they had at one point with IE on Mac they'd do this very thing and more.There aren't many shenanigans that MS didn't pioneer in the 90s.\n \nreply"
    ],
    "link": "https://www.bloomberg.com/news/articles/2024-08-05/google-loses-doj-antitrust-suit-over-search",
    "first_paragraph": "To continue, please click the box below to let us know you're not a robot.Please make sure your browser supports JavaScript and cookies and that you are not\n            blocking them from loading.\n            For more information you can review our Terms of\n                Service and Cookie Policy.For inquiries related to this message please contact\n            our support team and provide the reference ID below."
  },
  {
    "title": "Uncovered Euripides fragments are 'kind of a big deal' (colorado.edu)",
    "points": 201,
    "submitter": "caf",
    "submit_time": "2024-08-05T00:11:33",
    "num_comments": 69,
    "comments_url": "https://news.ycombinator.com/item?id=41157192",
    "comments": [
      "As a Classics major in college and with continuing love for that decaying old grande dame of a discipline, this is pretty cool and I hope the identification holds up to scrutiny (because it would be a big deal).Then there\u2019s this:The two scholars have also recently discovered the upper half of a colossal statue of the ancient Egyptian Pharaoh Ramesses II in their joint excavation project at Hermopolis Magna.Percy Bysshe Shelley is practically shouting from the grave.I MET A TRAVELER FROM AN ANTIQUE LAND\u2026\n \nreply",
      "\"Sssh, love, go back to bed.\" ~Mary\n \nreply",
      "This is how I find out they were married. Huh.\n \nreply",
      "Enough opium Percy, time for bed\n \nreply",
      "Obviously not decaying, but alive and well\n \nreply",
      "My favorite play is the Herakles of Euripides, which ends on these lines:    The man who would prefer great wealth or strength\n\n    more than love, more than friends\n\n    is diseased of soul\n \nreply",
      "\"Money can't buy you love but it lets you rent it by the hour\" \u2013 Max Headroom (from memory)\n \nreply",
      "I was born after Max Headroom aired, but for those of you that saw it while it aired, how was it?\n \nreply",
      "Fun fact: the creators of Max Headroom were the creators behind the original 1993 Super Mario Bros movie with Dennis Hopper, Bob Hoskins, and John Leguizamo.The making of that film is a bit crazy. Part of the issue was, Disney bought the distribution rights shortly before filming was supposed to start, and demanded all these rewrites. Probably also demanded that the stripper scenes be cut. :PHoskins claimed that he and Leguizamo started drinking every day before, and between, takes.\n \nreply",
      "Absolutely amazing. American TV was a desolate landscape with occasional stuff so good you couldn\u2019t believe the oasis wasn\u2019t a mirage. Max Headroom was in that category. And of course it didn\u2019t start in the States.Dunno if it would hold up today though\n \nreply"
    ],
    "link": "https://www.colorado.edu/asmagazine/2024/08/01/uncovered-euripides-fragments-are-kind-big-deal",
    "first_paragraph": "\nSkip to Content\nCU Boulder Classics scholars identify previously unknown fragments of two lost tragedies by Greek tragedian EuripidesAfter months of intense scrutiny, two University of Colorado Boulder scholars have deciphered and interpreted what they believe to be the most significant new fragments of works by classical Greek tragedian Euripides in more than half a century.In November 2022, Basem Gehad, an archaeologist with the Egyptian Ministry of Tourism and Antiquities, sent a papyrus unearthed at the ancient site of Philadelphia in Egypt to Yvona Trnka-Amrhein, assistant professor of classics. The two scholars have also recently discovered the upper half of a colossal statue of the ancient Egyptian Pharaoh Ramesses II in their joint excavation project at Hermopolis Magna.She began to pore over the high-resolution photo of the papyrus (Egyptian law prohibits physically removing any artifact from the country), scrutinizing its 98 lines.CU Boulder classicists\u00a0Yvona Trnka-Amrhein\u00a0("
  },
  {
    "title": "Reflex (YC W23) Is Hiring a Staff Infrastructure Engineer (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-08-05T21:00:04",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/reflex/jobs/uBt9ZNP-senior-staff-engineer-infrastructure",
    "first_paragraph": "Web apps in pure Python. Deploy with a single command.We\u2019re looking for a fantastic SF/Bay Area-based engineer comfortable working on a very early product in a quickly changing codebase and role. Our office is in San Francisco and we work in person M-F.This role will involve leading the development of our hosting service. We have an obsession over developer experience so getting things to run reliably, fast, and secure at scale is extremely important.Reflex is an open-source framework to build web apps in pure Python and deploy them with a single command. This can be anything from a small data science/internal app to a large multi-page web app.We launched in December and have seen rapid adoption of our open-source framework, with over 80,000 apps made to date. In parallel with improving our open-source framework, we are now working on a hosting service so users can quickly deploy and scale their apps. We have raised 5M in seed funding in August led by Lux Capital with some fantastic fo"
  },
  {
    "title": "Rosalind Franklin's Methods of Discovery (jstor.org)",
    "points": 21,
    "submitter": "bookofjoe",
    "submit_time": "2024-08-01T14:19:07",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=41129356",
    "comments": [
      "The paper: https://www.jstor.org/stable/10.1086/663241?mag=rosalind-fra...\n \nreply",
      "If you don't have an account there: https://sci-hub.ru/10.1086/663241.\n \nreply"
    ],
    "link": "https://daily.jstor.org/rosalind-franklins-methods-of-discovery/",
    "first_paragraph": "Franklin\u2019s strategy for analyzing images of DNA molecules forces us to reconsider our definition of \u201cscientific discovery,\u201d argues Michelle G. Gibbons.The work of British chemist Rosalind Franklin (1920\u20131958) played an integral role in the discovery of the structure of DNA, but it took many years for Franklin\u2019s contributions to be fully recognized. In 1962, four years after Franklin\u2019s death, James Watson, Francis Crick, and Maurice Wilkins won the Nobel Prize for the discovery of the famous double helix. Wilkins, a colleague of Franklin, gave Watson and Crick several images that she produced before she published them. In his book years later, Watson actively downplayed Franklin\u2019s role.The episode is compelling evidence of sexism in the history of science, writes Michelle G. Gibbons in Philosophy of Science. But she argues that Franklin\u2019s story has additional significance\u2014it forces us to reevaluate our notions of how science works.Gibbons describes how Franklin refined the process of x-"
  },
  {
    "title": "How Postgres stores data on disk \u2013 this one's a page turner (silcock.dev)",
    "points": 412,
    "submitter": "drewsberry",
    "submit_time": "2024-08-05T08:28:25",
    "num_comments": 75,
    "comments_url": "https://news.ycombinator.com/item?id=41159180",
    "comments": [
      "> Then, there\u2019s a random byte like 0x25 or 0x07 followed by the column data \u2013 the rest of the columns are string types so they\u2019re all stored in UTF-8. If you know what these inter-column bytes mean, leave a comment below! I can\u2019t figure it out.Next paragraph mentions TOAST and this byte is related to that. The low order bits (on little endian platforms) determine whether the value is stored inline (00, first 4 bytes are total length), is stored in TOAST table (11) or is shorter than 127 bytes (01 for even length, 10 for odd length, the total length is first byte >> 1). So for 0x25 you get 01, so length is 0x25 >> 1 = 18, which is that byte followed by \"Equatorial Guinea\".Edit: the reason why endianness matters is that the same representation is also used in memory and the whole first word is interpreted as one length value. The toast tag bits have to be in first byte, which is most easily done as two highest order bits of that word on big endian. That means that it is placed in the two highest bits of the byte.\n \nreply",
      "This URL is blocked by my company's network because of a certain substring in the URL lol\n \nreply",
      "A classic case of the Scunthorpe problem: https://en.wikipedia.org/wiki/Scunthorpe_problemIn this case the substring is part of the author's name. Such names are not at all uncommon.\n \nreply",
      "Alistair Cockburn is one of the signatories of the Agile Manifesto.You may have heard of him more recently with the Hexagonal Architecture approach.\n \nreply",
      "I just saw something similar with a user on here dangsux...Apparently might be Dang's UX and not against the Mod. \u00af\\_(\u30c4)_/\u00af\n \nreply",
      "There are dozens of us!\n \nreply",
      "Well if it isn't my arch-nemesis \u2013 my legally designated name. Maybe I should've gone for something with just my first name like drewsexpertblog.dev\n \nreply",
      "I remember us once giving a supplier access to our internal bug tracker for a collaborative project. They were unable to get to the \u201c\u2026/openissue\u201d endpoint.\n \nreply",
      "I was on a mailing list once where messages were blocked because of msexchange being in the headers.\n \nreply",
      "Any relation to ExpertsExchange?\n \nreply"
    ],
    "link": "https://drew.silcock.dev/blog/how-postgres-stores-data-on-disk/",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Visual A* pathfinding and maze generation in Python (github.com/dicklesworthstone)",
    "points": 82,
    "submitter": "eigenvalue",
    "submit_time": "2024-08-05T15:56:01",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=41162505",
    "comments": [
      "Here's a direct link to the YouTube demo video:\n https://www.youtube.com/watch?v=iA6XJRE6CTMAlso, here's the Lisp implementation post that inspired me (and which I based my Python code on):https://news.ycombinator.com/item?id=41145528And here are a few other sample videos using different settings-- I'll add more during the day as they finish generating:https://www.dropbox.com/scl/fo/q13cxuvgy8vxr3ksi06uw/APkL57-...\n \nreply",
      "Ugh, it would be nice to be able to pause the final frame of animation to compare each generated path. Instead YouTube replaces it with their obnoxious \"next video\" suggestions. If you generate another video, I'd suggest artificially \"freezing\" the results frame for about 5 seconds so it can be seen and compared.\n \nreply",
      "Check out the other sample videos I linked to in my comment, those are easy to pause in VLC. I can look into extending the final frame for a few seconds, too.\n \nreply",
      "I prefer this video of A* pathfinding on a real map (Chicago and Rome):https://youtu.be/CgW0HPHqFE8?si=9aw_eHy3IedXY1Ro\n \nreply",
      "That is very cool. Too bad it\u2019s not open source.\n \nreply",
      "I like this..I recently used A* to implement laying out connectors between nodes in a graph. I really like the abstraction of a heuristic function. I was able to add in all sorts of things to make the implementation work the way i want (penalise turns, crossing over lines etc.). This would automatically create \"last resort\" style solutions and minimise ugliness in the diagram.\n \nreply",
      "Yes, doing it that way sort of goes beyond the standard A* and becomes more of a \"build your own custom pathfinder toolbox\" where you can insert any additional considerations you have in your specific problem domain. Sort of like how you can add different factors to a loss function in machine learning (like trying to minimize non-zero parameter count for LASSO in addition to minimizing mean squared error).\n \nreply",
      "I would be interested to hear what fraction of this script and README were generated by large language models. At first glance, the code contains a number of repetitive anti-patterns that 'feel like' they are Copilot-isms (e.g. large stacks of elif statements instead of using appropriate data structures), and the README is very verbose and includes a high fraction of filler words.\n \nreply",
      "Pretty much the entire project.\n \nreply",
      "I think so too. Also, what's the point of using git with commit messages like that? https://github.com/Dicklesworthstone/visual_astar_python/com...LoL.\n \nreply"
    ],
    "link": "https://github.com/Dicklesworthstone/visual_astar_python",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          This project provides a high-performance implementation of the A* (\"A-Star\") pathfinding algorithm (based on this Lisp implementation by Andrew Kravchuck) along with various maze generation techniques to showcase how this algorithm works, as well as an advanced animated visualization of pathfinding in these mazes. The mazes are generated using many diverse approaches, each providing a different visual look and feel and also potential challenges for a pathfinding algorithm. The A* algorithm is designed to efficiently find the shortest path in these mazes, taking into consideration various heuristic functions and neighbor enumerators.The A* algorithm implementation focuses on efficiency and scalability. Key aspects include:Custom Priority Queue: The priority queue is a fundamental component of the A* algorithm, used to manage the open set (fro"
  },
  {
    "title": "A cryptographically secure bootloader for RISC-V in Rust (codethink.co.uk)",
    "points": 129,
    "submitter": "fork-bomber",
    "submit_time": "2024-08-05T14:18:13",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=41161580",
    "comments": [
      "Measured boot > trust chain through signature verification:With measured boot, components in the boot chain tell some trusted component (e.g. a TPM, possibly in FW) about all of their input and only if the hashes at the end match, $something is accessible (in most cases a secret key for data decryption).1. More flexibility (with TPM e.g. you can \"seal\" a secret against different parts independently)2. No need for PKI, which gets very complex once revocations are involved (have fun looking at the \"Secure Boot\" DBX lists and the shim SBAT mechanism)3. More freedom: The system still boots if the measurements don't match, you just don't get access to secrets. You're free to seal your own secrets against your new measurements and whoever did the last sealing has no access anymore. (Unlike on PCs where the Microsoft trust is in most cases not removable).\n \nreply",
      "1. This is interesting. So in a measured boot scenario, you wouldn't be able to boot the main OS, but it would give you access to sort of a minimal initramfs environment for debugging? It's a good idea for personal computers, like a tamper-proofing approach.I assume the TPM in this case would only have a partial decryption key? I think something similar could be accomplished with SSS, no?2. As for this, I can say i've never used DBX with UEFI Secure boot. Instead of revoking keys, I just remake the entire PKI from the top. The PKI is only there to support independent use by OS Vendor/OEM hence the separation of PK/KEK/db.3. Counterpoint: over-reliance on TPMs and such. Whereas the ordinary trust chain only requires signature verification at the start of boot (presumably on-chip), measured boot requires more complex trusted computing hardware (presumably off-chip).Personally, I find that systems that are overly-reliant on complex trusted computing hardware tend to lack in other areas. For example, iphones or google-pixel devices encourage the user to use a low-entropy password like a 4-digit PIN. These systems try often to reconcile \"analog\" passkeys like Biometrics (FaceID, fingerprints) by using trusted computing. Of course, if the trusted computing systems are breached (https://www.404media.co/leaked-docs-show-what-phones-cellebr...), then security is very weak.I suppose the advantage of the measured-boot method is that it is optional. So you can still boot whatever OS you want, just without some TC features.\n \nreply",
      ">1. This is interesting. So in a measured boot scenario, you wouldn't be able to boot the main OS, but it would give you access to sort of a minimal initramfs environment for debugging? It's a good idea for personal computers, like a tamper-proofing approach.If you would like to play around with measured boot and similar functionality of TCG DICE. Thats on a USB stick that open, and have a good team behind it.https://tillitis.se/\n \nreply",
      "> 1. This is interesting. So in a measured boot scenario, you wouldn't be able to boot the main OS, but it would give you access to sort of a minimal initramfs environment for debugging? It's a good idea for personal computers, like a tamper-proofing approach.Depends on how it's set up. Currently most setups that use measured boot (systemd-pcrlock, partially BitLocker) ask for a recovery key if unsealing fails due to measurement mismatches and offer other options.> I assume the TPM in this case would only have a partial decryption key?That's also possible, but so far I haven't seen that. The sealed secret is sent to the TPM which then uses its hidden internal seed to derive the master key for volume decryption and sends it back. (In the case of bitlocker with TPM < 2 that could trivially be sniffed on the LPC bus...)> I think something similar could be accomplished with SSS, no?If you mean Shamir's secret sharing, possibly. Question is what to do with the shares.2. Yeah, for your local machine this is a working approach, if you make sure that really only your own key works. Another reason against PKI is also that the trusted authority can't retroactively sign a backdoored executable to gain access to devices, as the measurements are independent from authority and ideally device specific.3. Signature verification isn't just needed at the start of boot, it's ideally from start of booting until user authentication, which is the part that can be tampered with. I'd argue that the software side for measured boot is simpler, while the hardware side may be more complex.> For example, iphones or google-pixel devices encourage the user to use a low-entropy password like a 4-digit PIN.Using TPM+PIN is actually not that bad: Only if measurements match it's possible to unlock with a PIN and the TPM uses a counter in nonvolatile memory to prevent brute force attacks. It's not unfathomable that some manufacturer screws that up, but it's IMO stronger than relying on multiple parties (CPU, BIOS, OEMs, OS) developing an actually secure trust chain.\n \nreply",
      "That said, it does require more care when you do OS updates or UEFI updates to remember to update the TPM sealed secret with the new measurements. Windows and Linux both have the former automated so it should generally be fine.UEFI updates can also be a problem if they wipe the TPM as part of the update and thus destroy the sealed secret entirely (as my PC mobo does).\n \nreply",
      "> That said, it does require more care when you do OS updates or UEFI updates to remember to update the TPM sealed secret with the new measurements. Windows and Linux both have the former automated so it should generally be fine.Yep, this can be a pain also in regards to firmware bugs (broken TCG event log anyone?). In the worst case you need to enter the recovery key or if you know in advance, exclude some component from measurement temporarily while supervising the next boot. If something goes wrong with the trust chain like a key got revoked but the bootloader didn't update correctly, you end up with an unbootable device and can't even go back easily.> UEFI updates can also be a problem if they wipe the TPM as part of the update and thus destroy the sealed secret entirely (as my PC mobo does).Ouch, that's bad design. The firmware is measured into the TPM on boot so there's no reason to do that..\n \nreply",
      "Yeah, every time I update the UEFI it pops up a warning that the TPM will be cleared and I better have disabled Windows Bootlocker before I did this. The warning also goes away within a fraction of a second because the PC reboots which is not nearly enough time to read it, and I only know what it says because I've updated the UEFI enough times to be able to piece it together. Weird.It might just be a warning to cover their asses; ie it doesn't actually clear the TPM but they don't want to be responsible for your un-unlockable drive in case it does. I don't actually use the TPM for measured boot or anything else so I haven't checked.In any case, UEFI updates are relatively common right now (once every couple of months or so) because it's a relatively new mobo (AM5), and because AMD is about to release new CPUs that requires corresponding AGESA etc updates. It'll probably become less frequent in a few years.\n \nreply",
      "It appears Apple Silicon uses a combination of measured boot and trusted boot concepts: https://support.apple.com/guide/security/boot-process-secac7...\n \nreply",
      "tbh I feel bad for the kid, his thesis supervisor should have helped him more here to scope and direct the work in some sensible way. now it is bit of a mess :(like just doing a review and comparison of existing boot verification mechanisms would have been already good scope for a thesis. Instead they are barely even mentioned as a side-note, which puts this in a awkward position.or if crypto was focus, then putting more work on designing and implementing the crypto scheme would have been relevant. Now they got so tangled with the nitty gritty boot details that the crypto ended up also as questionable side-note.or if rust was focus, then just implementing clean pure-rust bootloader could have been already enough for the thesis, avoiding the stumbling over on misguided crypto bits.or many other ways this could have been more successful. overall it now feels the author ended up biting far more than what they can chew. also they should have imho spent less time coding and more time on editing the actual thesis. the text is all over the place.\n \nreply",
      "I don't get the \"1/10 size of U-Boot\" argument. As it can only boot 3 RISC-V64 boards via TFTP, it also has less than 1/10 of the features and supported hardware of U-Boot. https://github.com/u-boot/u-boot\n \nreply"
    ],
    "link": "https://www.codethink.co.uk/articles/2024/secure_bootloader/",
    "first_paragraph": "SentinelBoot is a demonstrative, cryptographically secure RISC-V bootloader written in Rust. This project forms a final-year project at The University of Manchester sponsored by Codethink.Memory safety is a persistent issue in software, especially system software, such as bootloaders. Implementing some kinds of run-time safety checks can be very computationally expensive, as such, programming languages which employ them to promote memory safety are incompatible with system software due to performance degradation1. That said, exploiting vulnerabilities that arise from a lack of memory safety leads to a myriad of issues, including data leaks, denial-of-service, and arbitrary code execution2. Until recently, there has been no viable memory-safe alternative to C/C++/Assembly for such applications. However, the Rust programming language, which performs static analysis at compile time, has been presented as a viable alternative and has begun being explored for such applications, with project"
  },
  {
    "title": "Show HN: Pie Menu \u2013 a radial menu for macOS (pie-menu.com)",
    "points": 171,
    "submitter": "hauken",
    "submit_time": "2024-08-05T11:38:51",
    "num_comments": 72,
    "comments_url": "https://news.ycombinator.com/item?id=41160268",
    "comments": [
      "That's nice!I wrote a Swift package that does a similar thing (but as an iOS widget): https://github.com/RiftValleySoftware/RVS_SpinnerMy own experience, is that I keep on not using it in my projects. It's too much of an \"in your face\" widget. I was going to do a SwiftUI version of it, but stopped working on it, when I figured out that I probably wouldn't use it.I suspect that MacOS, with the cursor-oriented navigation, is a better home for it.\n \nreply",
      "Nice!\nI\u2019ve noticed that I tend to use Pie Menu myself on apps where I frequently switch tools. Like Figma, Photoshop, Illustrator etc. Or for apps that have different modes: like Calendar (today, week, month), Things (today, inbox++), Obsidian (daily note, graph view, backlinks).For other apps where the keyboard shortcuts acts more as other shortcuts I don\u2019t use it as much.\n \nreply",
      "Have you considered displaying the text representation of the currently-hovered menu item inside the center of the circle? Or if the text wouldn't fit, then above/below the circle?\n \nreply",
      "Always liked the Secret of Mana circular menu, but could never find a really good way of porting that to a computer.\n \nreply",
      "The menu sounds haunt my nightmares\n \nreply",
      "(Disclaimer: I\u2019m on mobile and definitely haven\u2019t used this yet.) I feel like the problem of \u201cI don\u2019t know the shortcut to do this\u201d is solved for my by hitting \u2318? And searching the menus for a keyword, then hitting enter. How do you see this as improving upon that? Is this something you have to set up per app? Because if it doesn\u2019t automatically populate its options with actions I use a lot but don\u2019t use the shortcut for that seems like a miss :)\n \nreply",
      "Like me, you seem to like to drive your computer with the keyboard and use the mouse when necessary.I use the Windows key to open applications on Windows, Cmd+Space on MacOS, and Mod+d on my i3 nixOS machine.Some people like their mouse/trackpad though, and this seems like a useful tool for those that do.\n \nreply",
      "it would be cool to have an options to show the shortcut under the icon in the radial menu so that over time you learn it, and over time can replace items on the menu to ones you still don't have memorized\n \nreply",
      "You set up the frequent shortcuts you want for each app: This way you can quickly switch between different modes or tools in your different apps by only remembering one shortcut.\n \nreply",
      "Hi, I had some initial trouble where I kept selecting all the text on the website when I shift-z-clicked. This is easily fixed by making text unselectable on your site via CSS. I wouldn't recommend everyone use it for their entire site, but I think in your case it will make a better demo.https://stackoverflow.com/questions/826782/how-to-disable-te...\n \nreply"
    ],
    "link": "https://www.pie-menu.com/",
    "first_paragraph": "Whether you're in Figma, Slack or Things, you'll be able to access your most used shortcuts by only remembering one command.One of the best inventions for Macs so far.Customizable shortcuts which can be triggered around the mouse coursor make my life so much easier. Must have for all Mac users!It is growing on me as I figure out how to get more apps to work with it!Pie-menu is a clever way to access menu shortcuts for those of us who can\u2019t remember a miriad of shortcuts.Great app concept and very useful!Nice ideaOmg man, i\u2019m not gonna lie\u2026 This is it!I was dreamin of itI quite like it! I'm not great at remembering shortcuts so this is quite handy, especially for apps that I use less frequentlyStudies show using a mouse instead of keyboard shortcuts wastes 2 seconds per minute. Pie Menu saves the same time without remembering new shortcuts.Find all the shortcuts in our Shortcut-rolodex and add them to Pie Menu with a click!The convenience of shortcuts without the memorization!Are you a "
  },
  {
    "title": "C++'s `noexcept` can sometimes help or hurt performance (16bpp.net)",
    "points": 47,
    "submitter": "def-pri-pub",
    "submit_time": "2024-08-05T16:55:52",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=41163083",
    "comments": [
      "The most common place where noexcept improves performance is on move constructors and move assignments when moving is cheaper than copying. If your type is not nothrow moveable std::vector will copy it instead of moving when resizing, as the move constructor throwing would leave the vector in an invalid state (while the copy constructor throwing leaves the vector unchanged).Platforms with setjmp-longjmp based exceptions benefit greatly from noexcept as there\u2019s setup code required before calling functions which may throw. Those platforms are now mostly gone, though. Modern \u201czero cost\u201d exceptions don\u2019t execute a single instruction related to exception handling if no exceptions are thrown (hence the name), so there just isn\u2019t much room for noexcept to be useful to the optimizer.Outside of those two scenarios there isn\u2019t any reason to expect noexcept to improve performance.\n \nreply",
      "There is another standard library related scenario: hash tables. The std unordered containers will store the hash of each key unless your hash function is noexcept. Analogous to how vector needs noexcept move for fast reserve and resize, unordered containers need noexcept hash to avoid extra memory usage. See https://gcc.gnu.org/onlinedocs/libstdc++/manual/unordered_as...\n \nreply",
      "This is the correct analysis. The article's author could have saved themselves (and the reader) a good amount of blind data diving by learning more about exception processing beforehand.\n \nreply",
      "That's quite interesting and a huge work has been done here, respect for that.Here's what has jumped out at me: `noexcept` qualifier is not free in some cases, particularly, when a qualified function could actually throw, but is marked `noexcept`. In that case, a compiler still must set something up to fulfil the main `noexcept` promise - call `std::terminate()` if an exception is thrown. That means, that putting `noexcept` on each and every function blindly without any regard to whether the function could really throw or not (for example, `std::vector::push_back()` could throw on reallocation failure, hence if a `noexcept` qualified function call it, a compiler must take into account) doesn't actually test/benchmark/prove anything, since as the author correctly said, - you won't ever do this in a real production project.\nIt would be really interesting to take a look into a full code of cases that showed very bad performance, however, here we're approaching the second issue: if that's the core benchmark code: https://github.com/define-private-public/PSRayTracing/blob/a... then unfortunately it's totally invalid since it measures time with the `std::chrono::system_clock` which isn't monotonic. Given how long the code required to run, it's almost certain that the clock has been adjusted several times...\n \nreply",
      "> in that case, a compiler still must set something up to fulfil the main `noexcept` promise - call `std::terminate()`This is actually something that has been more of a problem in clang than gcc due to LLVM IR limitations... but that is being fixed (or maybe is already?)  There was a presentation about it at the 2023 LLVM Developer's meeting which was recently published on their youtube channel https://www.youtube.com/watch?v=DMUeTaIe1CUThe short version (as I understand) is that you don't really need to produce any code to call std::terminate, all you need is tell the linker it needs to leave a hole in the table which maps %rip to the required unwind actions.  If the unwinder doesn't know what to do, it will call std::terminate per the standard.IR didn't have a way of expressing this \"hole\", though, so instead clang was forced to emit an explicit \"handler\" to do the std::terminate call\n \nreply",
      "> then unfortunately it's totally invalid since it measures time with the `std::chrono::system_clock` which isn't monotonic. Given how long the code required to run, it's almost certain that the clock has been adjusted several timesmonotonic clocks are mostly useful for short measurement periods. for long-term timing wall-time clocks (with their adjustments) are more accurate because they will drift less.\n \nreply",
      "> I didn't know std::uniform_int_distribution doesn't actually produce the same results on different compilersI think this is genuinely my biggest complaint about the C++ standard library. There are countless scenarios where you want deterministic random numbers (for testing if nothing else), so std's distributions are unusable. Fortunately you can just plug in Boost's implementation.\n \nreply",
      "It's actually really important that uniform_int_distribution is implementation defined. The 'right' way to do it on one architecture is probably not the right way to do it on a different architecture.For instance, Apple's new CPUs has very fast division. A convenient and useful tool to implement uniform_int_distribution relies on using modulo. So the implementation that runs on Apple's new CPUs ought to use the modulo instructions of the CPU.On other architectures, the ISA might not even have a modulo instruction. In this case, it's very important that you don't try to emulate modulo in software; it's much better to rely other more complicated constructs to give a uniform distribution.C++ is also expected to run on GPUs. NVIDIA's CUDA and AMD's HIP are both implementations of C++. (these implementations are non-compliant given the nature of GPUs, but both they and the C++ standard's committee have a shared goal of narrowing that gap) In general, std::uniform_int_distribution uses loops to eliminate redundancies; the 'happy path' has relatively easily predicted branches, but they can and do have instances where the branch is not easily predicted and will as often as not have to loop in order to complete. Doing this on a GPU might be multiple orders of magnitude slower than another method that's better suited for a GPU.Overzealously dictating an implementation is why C++ ended up with a relatively bad hash table and very bad regex in the standard. It's a mistake that shouldn't be made again.\n \nreply",
      "> There are countless scenarios where you want deterministic random numbers (for testing if nothing else), so std's distributions are unusable. Fortunately you can just plug in Boost's implementation.I don't understand what's your complain. If you're already plugging in alternative implementations,what stops you from actually stubbing these random number generators with any realization at all?\n \nreply",
      "It's a compromised and goofy implementation with lots of warts.  What's the point it in having a /standard/ library then?\n \nreply"
    ],
    "link": "https://16bpp.net/blog/post/noexcept-can-sometimes-help-or-hurt-performance/",
    "first_paragraph": "\n  Software developer in Boston, MA.  I like C++, Qt, UI/UX, graphics, animation and other fun things. \u65e5\u672c\u8a9e\u3092\u8a71\u3057\u307e\u3059\u3002\nOver the course of working on PSRayTracing (PSRT), I've been trying to find all sorts of tricks and techniques to squeeze out more performance from this C++ project. Most of it tends to be alternative algorithms, code rewriting, and adjusting data structures. I thought sprinkling the final keyword like an all purpose seasoning around every class was \"free performance gain\". But... that didn't really turn out to be the case.Back in the early days of this project (2020-2021), I recall hearing about the noexcept keyword for the first time. I was reading through Scott Meyer's works and picked up a copy of \"Effective Modern C++\" and watched a few CppCon talks about exceptions. I don't remember too much, but what I clearly recall:I re-picked up a copy of the aforementioned book whilst writing this. \"Item 14: Declare functions noexcept if they won't emit exceptions\" is the section "
  }
]