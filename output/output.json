[
  {
    "title": "The Grug Brained Developer (2022) (grugbrain.dev)",
    "points": 481,
    "submitter": "smartmic",
    "submit_time": "2025-06-17T20:24:36 1750191876",
    "num_comments": 152,
    "comments_url": "https://news.ycombinator.com/item?id=44303542",
    "comments": [
      "\u201cGood debugger worth weight in shiny rocks, in fact also more\u201dI\u2019ve spent time at small startups and on \u201celite\u201d big tech teams, and I\u2019m usually the only one on my team using a debugger. Almost everyone in the real world (at least in web tech) seems to do print statement debugging. I have tried and failed to get others interested in using my workflow.I generally agree that it\u2019s the best way to start understanding a system. Breaking on an interesting line of code during a test run and studying the call stack that got me there is infinitely easier than trying to run the code forwards in my head.Young grugs: learning this skill is a minor superpower. Take the time to get it working on your codebase, if you can.\n \nreply",
      "There was a good discussion on this topic years ago [0]. The top comment shares this quote from Brian Kernighan and Rob Pike, neither of whom I'd call a young grug:> As personal choice, we tend not to use debuggers beyond getting a stack trace or the value of a variable or two. One reason is that it is easy to get lost in details of complicated data structures and control flow; we find stepping through a program less productive than thinking harder and adding output statements and self-checking code at critical places. Clicking over statements takes longer than scanning the output of judiciously-placed displays. It takes less time to decide where to put print statements than to single-step to the critical section of code, even assuming we know where that is. More important, debugging statements stay with the program; debugging sessions are transient.I tend to agree with them on this. For almost all of the work that I do, this hypothesis-logs-exec loop gets me to the answer substantially faster. I'm not \"trying to run the code forwards in my head\". I already have a working model for the way that the code runs, I know what output I expect to see if the program is behaving according to that model, and I can usually quickly intuit what is actually happening based on the incorrect output from the prints.[0] The unreasonable effectiveness of print debugging (349 points, 354 comments) April 2021 https://news.ycombinator.com/item?id=26925570\n \nreply",
      "Adding these print statements is one of my favorite LLM use cases.Hard to get wrong, tedious to type and a huge speed increase to visually scan the output.\n \nreply",
      "Agreed. Typically my debugger use case is when I'm exploring a potentially unknown range of values at a specific point in time, where I also might not know how to log it out. Having the LLM manage all of that for me and get it 95% correct is the real minor superpower.\n \nreply",
      "I do a lot of print statements as well. I think the greatest value of debuggers comes when I\u2019m working on a codebase where I don\u2019t already have a strong mental model, because it lets me read the code as a living artifact with states and stack traces. Like Rob Pike, I also find single-stepping tedious.\n \nreply",
      "I am also in the camp that has very little use for debuggers.A point that may be pedantic: I don't add (and then remove) \"print\" statements. I add logging code, that stays forever. For a major interface, I'll usually start with INFO level debugging, to document function entry/exit, with param values. I add more detailed logging as I start to use the system and find out what needs extra scrutiny. This approach is very easy to get started with and maintain, and provides powerful insight into problems as they arise.I also put a lot of work into formatting log statements. I once worked on a distributed system, and getting the prefix of each log statement exactly right was very useful -- node id, pid, timestamp, all of it fixed width. I could download logs from across the cluster, sort, and have a single file that interleaved actions from across the cluster.\n \nreply",
      "A log is very different than a debugger though, one tells you what happened, one shows you the entire state and doesn't make you assemble it in your head.\n \nreply",
      "Your framing makes it sound like the log is worse in some way, but what the log gives you that the debugger makes you assemble in your head is a timeline of when things happen. Being able to see time is a pretty big benefit for most types of software.I can always drop an entire state object into the log if I need it, but the only way for a debugger to approximate what a log can give me is for me to step through a bunch of break points and hold the time stream in my head.The one place where a debugger is straight up better is if I know exactly which unit of code is failing and that unit has complicated logic that is worth stepping through line by line. That's what they were designed for, and they're very useful for that, but it's also not the most common kind of troubleshooting I run into.\n \nreply",
      "It's not worse or better, but its not really comparable is all I am really saying, I would not use them for the same things.\n \nreply",
      "I'd love to use a real debugger but as someone who has only ever worked at large companies, this was just never an option. In a microservices mesh architecture, you can't really run anything locally at all, and the test environment is often not configured to allow hooking up a stepping debugger. Print debugging is all you have. If there's a problem with the logging system itself or something that crashes the program before the logs can flush, then not even that.\n \nreply"
    ],
    "link": "https://grugbrain.dev/",
    "first_paragraph": "this collection of thoughts on software development gathered by grug brain developergrug brain developer not so smart, but grug brain developer program many long year and learn some things\nalthough mostly still confusedgrug brain developer try collect learns into small, easily digestible and funny page, not only for you, the young grug, but also for him\nbecause as grug brain developer get older he forget important things, like what had for breakfast or if put pants onbig brained developers are many, and some not expected to like this, make sour faceTHINK they are big brained developers many, many more, and more even definitely probably maybe not like this, many\nsour face (such is internet)(note: grug once think big brained but learn hard way)is fine!is free country sort of and end of day not really matter too much, but grug hope you fun reading and maybe learn from\nmany, many mistake grug make over long program lifeapex predator of grug is complexitycomplexity badsay again:complexity v"
  },
  {
    "title": "Honda conducts successful launch and landing of experimental reusable rocket (global.honda)",
    "points": 820,
    "submitter": "LorenDB",
    "submit_time": "2025-06-17T15:02:12 1750172532",
    "num_comments": 243,
    "comments_url": "https://news.ycombinator.com/item?id=44300102",
    "comments": [
      "Here is the video which they should have put in the post:https://global.honda/content/dam/site/global-en/topics-new/c...\n \nreply",
      "Agreed, it brings the story home. What I most like about this news is that Honda has joined Blue Origin and SpaceX in demonstrating a complete \"hop\" (all though my all time favorite is the \"ring of fire\" video SpaceX did.)But it also illustrates that I've seen in the Bay Area time and time again, which is that once you demonstrate that something is doable (as SpaceX has) It opens the way for other capital to create competitive systems.At Google, where I worked for a few years, it was interesting to see how Google's understanding of search (publicly disclosed), and the infrastructure to host it (kept secret)  kept it comfortably ahead of competitors until the design space was exhausted. At which point Google stopped moving forward and everyone else asymptotically approached their level of understanding and mastery.I see the same thing happening to SpaceX. As other firms master the art of the reusable booster, SpaceX's grasp on the launch services market weakens. Just as Google's grasp of the search market weakens. Or Sun's grasp of the server market weakened. When it becomes possible to buy launch services from another vendor which are comparable (not necessarily cheaper, just comparable) without the baggage of the damage Elon has done, SpaceX will be in a tougher spot.It also helps me to understand just how much SpaceX needs Starship in order to stay on top of the market.Some folks will no doubt see this as casting shade on SpaceX, I assure you it is not. What SpaceX's engineering teams have accomplished remains amazing and they deserve their success. It is just someone who has been through a number of technology curves noting how similar the they play out over their lifetimes.Having witnessed first hand how DEC felt that Sun's \"toy computers\" would never eclipse DEC in the Server business, and watched as United Launch Alliance dismissed Falcon 9 as something that would never seriously challenge their capabilities, it feels almost prophetic to watch SpaceX's competitors emerge.\n \nreply",
      "> What I most like about this news is that Honda has joined Blue Origin and SpaceX in demonstrating a complete \"hopThe list is longer than that! The earliest hop was probably by McDonnell Douglas in 1993 https://www.youtube.com/watch?v=e_QQDY7PYc8\n \nreply",
      "Don't forget Ballmer dissing the iPad.I also won't forget the marketing department at the camera company I worked at, dismissing the iPhone, when it first came out (it ended up eating their lunch).\n \nreply",
      "They should have totally had a Civic in the background and a guy mowing the lawn near the sprinkler.\n \nreply",
      "Generational engine ad\nNeeds some F1\n \nreply",
      "It's interesting how I couldn't tell whether the rocket was 1m tall or 10m tall in this video. Turns out it's actually 6m tall per the link.\n \nreply",
      "In the first shot on the pad, I thought \u201coh, it\u2019s a slightly oversized model rocket\u201d and then when it cut I realized it was quite a bit bigger.\n \nreply",
      "Japan continuing their legacy of minituriazing everything they develop. \\s\n \nreply",
      "You need some manga\n \nreply"
    ],
    "link": "https://global.honda/en/topics/2025/c_2025-06-17ceng.html",
    "first_paragraph": "TOKYO, Japan, June 17, 2025 \u2013 Honda R&D Co., Ltd., a research and development subsidiary of Honda Motor Co., Ltd., today conducted a launch and landing test of an experimental reusable rocket*1 (6.3 m in length, 85 cm in diameter, 900 kg dry weight/1,312 kg wet weight) developed independently by Honda. The test was completed successfully, the first time Honda landed a rocket after reaching an altitude of 300 meters.This test marked the first launch and landing test conducted by Honda with an aim to demonstrate key technologies essential for rocket reusability, such as flight stability during ascent and descent, as well as landing capability. Through this successful test, Honda achieved its intended rocket behaviors for the launch and landing (reaching an altitude of 271.4 m, and landing at 37cm of the target touchdown point, flight duration 56.6 sec), while obtaining data during the ascent and descent.\u30fbPurpose:\u00a0\u00a0\u00a0\u00a0 Establishment of key technologies necessary for a reusable rocket\n\u30fbLoca"
  },
  {
    "title": "Resurrecting a dead torrent tracker and finding 3M peers (kianbradley.com)",
    "points": 362,
    "submitter": "k-ian",
    "submit_time": "2025-06-17T17:40:19 1750182019",
    "num_comments": 109,
    "comments_url": "https://news.ycombinator.com/item?id=44301686",
    "comments": [
      "> Is this legal?Why wouldn't it be? You're not actually hosting a tracker in this case, only looking at incoming connections. And even if you do run a tracker, hard to make the case that the tracker itself is illega. Hosting something like opentrackr is like hosting a search engine, how they respond to legal takedown requests is where the crux is at, and whatever infra sits around the tracker, so police and courts can see/assume the intent. But trackers are pretty stupid coordination server software, would be crazy if they became illegal.\n \nreply",
      "Is this legal isn\u2019t a useful question. The better question is how likely are you to get sued? With civil lawsuits it doesn\u2019t matter if it\u2019s legal you can be sued and harassed by lawyers if you get on their radar.\n \nreply",
      "No need to sue.  Send a cease and desist and your average hacker like OP will take it down in a hurry...\n \nreply",
      "In this case not even a cease-and-desist was needed. Just seeing 1.7M peers crying out in the void for company was enough. Living in a country overly friendly with Hollywood and its money, I do understand him.\n \nreply",
      "I\u2019m not sure if that\u2019s true actually, you might get a takedown notice, but to sue, and maybe I\u2019m wrong but you have to claim damages, all op has to do is not announce out?IE he can see the peer pool but they don\u2019t announce the peer list.\n \nreply",
      "The RIAA doesn't have to sue to make OP's life miserable. They have enough lawyers on the payroll to drown him in perfectly legal demand letters. Go one step further and assume the demand letters are harassment - what's OP going to do, sue the RIAA?\n \nreply",
      "Because knowingly helping people commit crimes generally counts the same as committing the crime yourself. I.e. federally in the U.S. under 18 USC 2a https://www.law.cornell.edu/uscode/text/18/2 The software you're running being \"simple\" isn't a defence for doing illegal things with it - like aiding others commit crimes.There are a few internet/copyright safe harbor provisions (in the US) that might maybe (probably not) make it not a crime, I don't know, I'm not a lawyer. But your general thought when you hear \"helping someone else commit a crime\" ought to be \"that's probably a crime itself\".\n \nreply",
      "Wouldn't particular knowledge be required?  I'm sure Google devs know in the abstract that Google search is used by criminals to help them in committing crimes, but that clearly is not illegal in and of itself.\n \nreply",
      "There's definitely a mens rea requirement here, that you know that a crime is being committed and that you intend to facilitate it. I doubt it requires particularized knowledge that \"this specific request\" is for a crime... I'm still not a lawyer.Running a service primarily for legal purposes that some criminals can take advantage of is pretty different with regards to intent than reviving an old domain name that you know is primarily used by old illegal torrents as a tracker.I spent a few minutes googling, and it seems like that at least as of a decade ago the exact bounds here weren't well defined: https://www.scotusblog.com/2014/03/opinion-analysis-justice-...> Finally, the possible liability for an \u201cincidental facilitator\u201d \u2013 such as a firearms dealer who knows that some customers will use their purchases for crime \u2013 is noted but not resolved.  Thus, thankfully, there is still some fertile ground for hypotheticals with which we practicing law professors can bedevil our students.\n \nreply",
      "IANAL, but I would think you\u2019d also have to have specific mens rea. That is, it\u2019s not illegal to use a torrent or facilitate a torrent, because it\u2019s just a protocol that can be used for good or bad. If you were hosting movies and songs, whatever the protocol, that\u2019s when you\u2019re specifically engaging in piracy. It\u2019s sort of like driving a car isn\u2019t illegal, but being the getaway driver for a bank robbery is, even if you never enter the bank. The car isn\u2019t the problem, it\u2019s what you are using it for. It\u2019s also not illegal to sell a car to a bank robber, even if that\u2019s a possibility, unless you reasonably believe that the particular person you were selling it to is a bank robber and will be using it to commit a crime. The mere fact that somebody could use your tracker for piracy doesn\u2019t loop you into the conspiracy unless you specifically know that they are committing piracy. This is why the telecom companies all have carve outs for this sort of thing. Carrying packets or voice traffic of someone planning a crime doesn\u2019t loop the telecom company into the conspiracy.\n \nreply"
    ],
    "link": "https://kianbradley.com/2025/06/15/resurrecting-a-dead-tracker.html",
    "first_paragraph": "\nJun 15, 2025\n      So I was uh, downloading some linux isos, like usual. It was going slowly, so I opened up the Trackers tab in qBittorrent and saw the following:Most of the trackers were totally dead. Either the hosts were down or the domains weren\u2019t being used.That got me thinking. What if I picked up one of these dead domains? How many clients would try to connect?A tracker is a core component of the BitTorrent protocol. Trackers are the services that point you to other peers for the torrent. Without trackers, there would be no one to share the file with.Obviously this represents a major source of centralization in the torrent protocol. If your trackers aren\u2019t maintained \u2013 or if they get forced offline by certain industry organizations \u2013 you\u2019re out of luck.We have an alternative, called Mainline DHT, which performs a more decentralized lookup of peers based on infohash alone. DHT isn\u2019t perfect, though. It relies on bootstrap nodes and is vulnerable to Sybil attacks. And in the exa"
  },
  {
    "title": "Bzip2 crate switches from C to 100% Rust (trifectatech.org)",
    "points": 149,
    "submitter": "Bogdanp",
    "submit_time": "2025-06-17T20:06:54 1750190814",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=44303361",
    "comments": [
      "How realistic is it for the Trifecta Tech implementation to start displacing the \"official\" implementation used by linux distros, which hasn't seen an upstream release since 2019?Fedora recently swapped the original Adler zlib implementation with zlib-ng, so that sort of thing isn't impossible. You just need to provide a C ABI compatible with the original one.\n \nreply",
      "Ubuntu is using Rust sudo so it's definitely possible.\n \nreply",
      "They do provide a compatible C ABI.  Someone \"just\" needs to do the work to make it happen.\n \nreply",
      "I think that is the goal of uutils.https://uutils.github.io/\n \nreply",
      "> You just need to provide a C ABI compatible with the original one.How does this interact with dynamic linking? Doesn't the current Rust toolchain mandate static linking?\n \nreply",
      "The commenters below are confusing two things - Rust binaries can be dynamically linked, but because Rust doesn\u2019t have a stable ABI you can\u2019t do this across compiler versions the way you would with C. So in practice, everything is statically linked.\n \nreply",
      "Static linking also produces smaller binaries and lets you do link-time-optimisation.\n \nreply",
      "Rust cannot dynamic link to rust. It can dynamic link to C and be dynamicly linked by C - if you combine the two you can cheat but it is still C that you are dealing with not rust even if rust is on both sides.\n \nreply",
      "No. https://doc.rust-lang.org/reference/linkage.html#r-link.dyli...\n \nreply",
      "Rust lets you generate dynamic C-linkage libraries.Use crate-type=[\"cdylib\"]\n \nreply"
    ],
    "link": "https://trifectatech.org/blog/bzip2-crate-switches-from-c-to-rust/",
    "first_paragraph": "Today we published bzip2 version 0.6.0, which uses our rust implementation of the bzip2 algorithm, libbz2-rs-sys, by default. The bzip2 crate is now faster and easier to cross-compile.The libbz2-rs-sys crate can also be built as a C dynamic library, if you have a C project that would benefit from these improvements.Why bother working on this algorithm from the 90s that sees very little use today? The thing is that many protocols and libraries still need to support bzip2 to be compliant with their specification, so many project still, deep down in their dependency tree, depend on bzip2. We've used our experience from zlib-rs to modernize the bzip2  implementation.We've previously written about the implementation details of libbz2-rs-sys in \"Translating bzip2 with c2rust\", now let's look at the benefits of this work.Our rust implementation generally outperforms the C implementation, though there are a couple of cases where we only match C performance. We are not aware of any cases where "
  },
  {
    "title": "3D-printed device splits white noise into an acoustic rainbow without power (phys.org)",
    "points": 60,
    "submitter": "rbanffy",
    "submit_time": "2025-06-15T15:54:54 1750002894",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44282953",
    "comments": [
      "Whoa it's like an ear but for light!\n \nreply",
      "Very neat, this reminds me of the organic shapes of passive demultiplexers in photonics such as https://pubs.acs.org/doi/10.1021/acsphotonics.7b00987\n \nreply",
      "okay who wants to build a musical instrument that works by beaming white noise at a bunch of these things, with some way for the user to rotate them quickly and accurately\n \nreply",
      "I\u2019m wondering if you can change the shape in such a way that rotating one would produce an arpeggio.\n \nreply",
      "One more step toward building the pyramids.\n \nreply"
    ],
    "link": "https://phys.org/news/2025-06-3d-device-white-noise-acoustic.html",
    "first_paragraph": ""
  },
  {
    "title": "Building Effective AI Agents (anthropic.com)",
    "points": 253,
    "submitter": "Anon84",
    "submit_time": "2025-06-17T17:50:05 1750182605",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=44301809",
    "comments": [
      "This article remains one of the better pieces on this topic, especially since it clearly defines which definition of \"AI agents\" they are using at the start! They use: \"systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks\".I also like the way they distinguish between \"agents\" and \"workflows\", and describe a bunch of useful workflow patterns.I published some notes on that article when it first came out: https://simonwillison.net/2024/Dec/20/building-effective-age...A more recent article from Anthropic is https://www.anthropic.com/engineering/built-multi-agent-rese... - \"How we built our multi-agent research system\". I found this one fascinating, I wrote up a bunch of notes on it here: https://simonwillison.net/2025/Jun/14/multi-agent-research-s...\n \nreply",
      "one half of the authors of Building Effective Agents also came by AIE to do a well received talk version of this article: https://www.youtube.com/watch?v=D7_ipDqhtwk\n \nreply",
      "The article on the multi-agent research is awesome.  I do disagree with one statement in the building effective AI agents article - building your initial system without a framework sounds nice as an educational endeavor but the first benefit you get from a good framework is the easy ability to try out different (and cross-vendor) LLMs\n \nreply",
      "This is why you use a library (not a framework) that provides an abstraction over different LLMs.I'm personally a fan of litellm, but I'm sure alternatives exist.\n \nreply",
      "Does anyone know which AI agent framework Anthropic uses? It doesn't seem like they ever released one of their own.\n \nreply",
      "From what it looks like, it's one main LLM (you are sending query to - orchestrator) which calls other LLMs via tool calls. The tools are capable of calling llms too, and can have specific instructions, but mostly just the orchestrator deciding what they should be researching on, and assigns them specific subqueries. There is a limited depth / levels of search queries too, you should see the prompt they use[1]One cool example of this in action is seen when you use claude code and ask it to search something. In a verbose setting, it calls an MCP tool to help with search. The tool returns summary of the results with the relevant links (not the raw search result text). A similar method, albeit more robust, is used when Claude is doing deep research as well.[1]: https://github.com/anthropics/anthropic-cookbook/blob/main/p...\n \nreply",
      "Just write the for loop to react to tool calls? It\u2019s not very much code.\n \nreply",
      "They mentioned hand offs, sub agents, concurrent tool calls, etc. You could write that yourself, but you would be inventing your own framework.\n \nreply",
      "Thank you for the extra notes, this is top of mind for me.\n \nreply",
      "Half a year has passed, and it feels like a long time in the field of AI.\nI read this article repeatedly a few months ago, but now I think the development of Agent has obviously reached a bottleneck.\nEven the latest gemini seems to have regressed.\n \nreply"
    ],
    "link": "https://www.anthropic.com/engineering/building-effective-agents",
    "first_paragraph": ""
  },
  {
    "title": "Dinesh's Mid-Summer Death Valley Walk (1998) (dineshdesai.info)",
    "points": 20,
    "submitter": "wonger_",
    "submit_time": "2025-06-17T23:56:32 1750204592",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=44305271",
    "comments": [
      "Pairs great with the tale of the Death Valley Germans: https://www.otherhand.org/home-page/search-and-rescue/the-hu....\n \nreply",
      "Up until a few years ago, I could've seen doing this as a worthwhile survival exercise, and to know that I can do it.Then, without trying, I overheated simply by exercising in a room that I didn't know was 95F.(Since I've mostly only lived in cold/moderate climates, and had never learned how risky 95F is.)It was highly unpleasant, in an uh-oh, I can see how people die this way, kind of way.Now, I actively avoid anywhere much hotter than about 80F.Just last week, I declined a very interesting recruiting outreach from a CEO in Austin, telling him, sorry, but the weather in Texas is just too hot for me.I'm ready to repurpose the term \"special snowflake\".> A young woman seems to be walking around in a daze. [...] I don't think they believed their guidebooks about how uncomfortably hot it can get in Death Valley.I hope someone helped the dazed person with first aid.  And that other people take the heat seriously.  It's right there in the name: Death Valley.\n \nreply",
      "What really matters is the wet bulb temp. For example, the Death Valley high today was 114\u00b0F (45\u00b0C) but at an RH of only 3%. That gives a wet bulb temp of ~ 65\u00b0F which isn't a problem with acclimation and adequate hydration.Now if it had been 50% RH, the web bulb temp would be > 96\u00b0F which is not survivable by humans for very long because no amount of sweating in that humidity will cool you down.\n \nreply",
      "Your reaction to heat is highly dependent on acclimation. I live in Texas and have to re-acclimate every year. Exercising on the first 85\u00ba+ day of the year is miserable, but a month later 85\u00ba feels quite tolerable, and 95\u00ba is doable, though performance suffers.I grew up playing baseball and tennis in 95-100\u00ba weather with high humidity routinely. It wasn't pleasant, but nobody was getting heatstroke, nobody was cancelling games or practices. But on a visit to Montana a few summers ago, I saw that kids' baseball games had been cancelled because the temperatures had reach a dangerous level: 90\u00ba (in dry mountain air.) Same human beings, different levels of acclimation, very different safety thresholds.I've never been in the temperatures described in this article, though, and I don't know what the physical limits of acclimation are.\n \nreply",
      "Bike riding in the heat once caused me to come close to overheating. Stopping, finding shade, pouring water over my head and laying down finally brought my core temperature down in time.I saw the same things begin to happen to my wife some years later when bike riding in the heat. I did the same for her and all was well.\n \nreply",
      "Interesting that they didn't find the nights too cold for sleeping out. We camped in Racetrack Playa one spring some years back and the nights were bitterly cold with extreme wind.\n \nreply",
      "This was done in Summer, not spring. I'd imagine summer nights wouldn't be as cold as Spring nights...\n \nreply"
    ],
    "link": "https://dineshdesai.info/dv/photos.html",
    "first_paragraph": "The following is an account of our walk through Death Valley National Park during July 1998. We wanted to find out how it feels to walk in extreme heat for days at a time. We had set a goal of walking the entire park, from its northern boundary to its southernmost point, a distance of about 180 miles. Our plan was to walk 15 miles a day and finish the walk in 12 days. Dinesh's caption:Training for the hot walk. Note water-delivery tube, wool pants, wool shirt and heavy-duty thermal top and bottom.Car Talk's caption:\"Jeez, I'm already disoriented -- and I'm still in my driveway.\"This feature appears courtesy of Car Talk. Copyright \u00a9 2004, Dewey, Cheetham and Howe. Used with permission. "
  },
  {
    "title": "AMD's CDNA 4 Architecture Announcement (chipsandcheese.com)",
    "points": 112,
    "submitter": "rbanffy",
    "submit_time": "2025-06-17T17:38:02 1750181882",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=44301660",
    "comments": [
      "Faster small matrix, for AI. Yup, that seems like good fit for what folks want.Supercharging the Local Data Share (LDS) that's shared by threads is really cool to hear about. 64 -> 160KB size. Writes into LDS go from 32B max to 128B, increasing throughout. Transposes, to help get the data in the right shape for its next use.Really really curious to see what the UDNA unified next gen architectures look like, if they really stick to merging Compute and Radeon CDNA and RDNA, as promised. If consumers end up getting multi-die compute solutions that would be neat & also intimidatingly hard (lots of energy spent keeping bits in sync across cores/coherency). After Navi 4X ended up having its flagship cancelled way back now, been wondering. I sort of expect that this won't scale as nicely as Epyc being a bunch of Ryzen dies. https://wccftech.com/amd-enthusiast-radeon-rx-8000-gpus-alle...\n \nreply",
      "When looking at inference is AMD already on par with Nvidia?\n \nreply",
      "Yes, for many applications.Meta, OpenAI, Crusoe, and xAI recently announced large purchases of MI300 chips for inference.MI400, which will be available next year, also looks to be at least on par with Nvidia's roadmap.\n \nreply",
      "(this is also why AMD popped 10% at open yesterday - this is a new development and talks from their 2025 \"Advancing AI\" event were published late last week + over the weekend)\n \nreply",
      "Is the software stack still lacking?\n \nreply",
      "Yeah it's still a few years behind but it's getting better. They are hiring software and tooling engineers like crazy. I keep tabs on some of the job slots companies have in our area and every time I check AMD they always have tons of new slots for software, firmware, and tooling (and this has been the case for ~3 years now).They've been playing catch up after \"the bad old days\" when they had to let a bunch of people go to avoid going under but it looks like they are catching back up to speed. Now it's just a matter of giving all those new engineers a few years to get their software world in order.\n \nreply",
      "They pay hardware rates to software engineers (principal engineer at the salary level of a decent fresh graduate) so I won't be too optimistic about them attracting software people that would propel them forward.\n \nreply",
      "Stock is undervalued. If you get in now and it pops over the next few years, it'll likely make up for lower compensation.\n \nreply",
      "At least where I live (very much not west coast), their SW and HW rates are at or above what we normally see in this area.\n \nreply",
      "FWIW for the first time in 2+ years I managed to compile llama.cpp with ROCm out of the box and run a model with no problems* on Linux (actually under WSL2 as well), with no weirdness or errors.Every time I have tried this previously it has failed with some cryptic errors.So from this very small test it has got way better recently.*Did have problems enabling the WMMA extensions though. So not perfect yet.\n \nreply"
    ],
    "link": "https://chipsandcheese.com/p/amds-cdna-4-architecture-announcement",
    "first_paragraph": ""
  },
  {
    "title": "LLMs pose an interesting problem for DSL designers (kirancodes.me)",
    "points": 111,
    "submitter": "gopiandcode",
    "submit_time": "2025-06-17T19:17:05 1750187825",
    "num_comments": 86,
    "comments_url": "https://news.ycombinator.com/item?id=44302797",
    "comments": [
      "Good to see more people talking about this. I wrote about this about 6 months ago, when I first noticed how LLM usage is pushing a lot of people back towards older programming languages, older frameworks, and more basic designs: https://nathanpeck.com/how-llms-of-today-are-secretly-shapin...To be honest I don't think this is necessarily a bad thing, but it does mean that there is a stifling effect on fresh new DSL's and frameworks. It isn't an unsolvable problem, particularly now that all the most popular coding agents have MCP support that allows you to bring in custom documentation context. However, there will always be a strong force in LLM's pushing users towards the runtimes and frameworks that have the most training data in the LLM.\n \nreply",
      "I think it's healthy, because it creates an undercurrent against building a higher abstraction tower. That's been a major issue: we make the stack deeper and build more of a \"Swiss Army Knife\" language because it lets us address something local to us, and in exchange it creates a Conway's Law problem for someone else later when they have to decipher generational \"lava layers\" as the trends of the marketplace shift and one new thing is abandoned for another.The new way would be to build a disposable jig instead of a Swiss Army Knife: The LLM can be prompted into being enough of a DSL that you can stand up some placeholder code with it, supplemented with key elements that need a senior dev's touch.The resulting code will look primitive and behave in primitive ways, which at the outset creates a myriad of inconsistency, but is OK for maintenance over the long run: primitive code is easy to \"harvest\" into abstract code, the reverse is not so simple.\n \nreply",
      "I think this depends a lot on the stack. for stacks like elixir and Phoenix, imho the extraction layer is about perfect.  For anyone in the Java world, however, what you say is absolutely true. Having worked in a number of different stacks, I think that some ecosystems have a huge tolerance for abstraction layers, which is a net negative for them. I would sure hate to see AI decimate something like elixir and Phoenix though\n \nreply",
      "Oh that's a great blog post and a very interesting point. Yep, I hadn't considered how LLMs would affect frameworks in existing languages, but it makes sense that there's a very similar effect of reinforcing the incumbents and stifling innovation.I'd argue that the problem of solving this effect in DSLs might be a bit harder than for frameworks, because DSLs can have wildly different semantics (imagine for example a logic programming DSL a la prolog, vs a functional DSL a la haskell), so these don't fit as nicely into the framework of MCPs maybe. I agree that it's not unsolvable though, but it definitely needs more research into.\n \nreply",
      "I think there is a lot of overlap between DSL's and frameworks, and most frameworks contain some form of DSL in them.What matters most of all is whether the DSL is written in semantically meaningful tokens. Two extremes as examples:Regex is a DSL that is not written in tokens that have inherent semantic meaning. LLM's can only understand Regex by virtue of the fact that it has been around for a long time and there are millions of examples for the LLM to work from. And even then LLM's still struggle with reading and writing Regex.Tailwind is an example of a DSL is that is very semantically rich. When an LLM sees: `class=\"text-3xl font-bold underline\"` it pretty much knows what that means out of the box, just like a human does.Basically, a fresh new DSL can succeed much faster if it is closer to Tailwind than it is to Regex. The other side of DSL's is that they tend to be concise, and that can actually be a great thing for LLM's: more concise, equals less tokens, equals faster coding agents and faster responses from prompts. But too much conciseness (in the manner of Regex), leads to semantically confusing syntax, and then LLM's struggle.\n \nreply",
      "Knowing just what's going on in the existing text isn't the whole problem in navigating a DSL. You have to be able to predict new things based on the patterns in existing text.Let's say you want to generate differently sized text here. An LLM will have ingested lots of text talking about clothing size and tailwind text sizes vaguely follow that pattern. Maybe it generates text-medium as a guess instead of the irregular text-base, or extends the numeric pattern down into text-2xs.\n \nreply",
      "> there is a lot of overlap between DSL's and frameworksNot just frameworks, but libraries also. Interacting with some of the most expressive libraries is often akin to working with a DSL.In fact, the paradigms of some libraries required such expressiveness that they spawned their own in-language DSLs, like JSX for React, or LINQ expressions in C#. These are arguably the most successful DSLs out there.\n \nreply",
      "Linguistics and history of language folk: isn't there an observed slowdown of evolution of spoken language as the printing press becomes widespread? Also, \"international english\"?Is this an observation of a similar phenomenon?\n \nreply",
      "I think perhaps automatic translators might help mitigate some of this.Even perhaps training a separate new neural network to translate from Python/Java/etc to your new language.\n \nreply",
      "i saw a good post on this earlier today \u2026 https://nurturethevibe.com/blog/teach-llm-to-write-new-progr...\n \nreply"
    ],
    "link": "https://kirancodes.me/posts/log-lang-design-llms.html",
    "first_paragraph": "\nThe most exciting part of Programming Languages (PL) research for me\nhas always been in Programming Language Design.\n\nBy carefully crafting a language with a syntax and semantics tailored\nfor a specific domain, PL designers can provide an interface for end\nusers that seamlessly aligns with the sensibilities and intuitions of\npractitioners, allowing users to focus on the \"interesting\" parts of a\nproblem and tackle larger and more complex problems.\n\nInstead of writing a verbose sequence of API calls to display a dialog\nto a user in a video game:\n\nA DSL instead allows designers to focus on the high-level of what the conversation should be:\n\nBy encoding the \"common sense rules\" of a domain into the language\nitself, we can make writing incorrect programs impossible, and\neliminate cognitive load and minimise the surface area for bugs and exploits. \n\nA DSL for every domain.  When you have eliminated all that is\nincorrect, then whatever remain, however complex, esoteric or\nconvoluted, simply "
  },
  {
    "title": "What Google Translate Can Tell Us About Vibecoding (ingrids.space)",
    "points": 85,
    "submitter": "todsacerdoti",
    "submit_time": "2025-06-17T19:23:10 1750188190",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=44302870",
    "comments": [
      "This article is spot on about a lot of things. One thing I think it fails to address is this:>  I feel confident in asserting that people who say this would not have hired a translator or learned Japanese in a world without Google Translate; they\u2019d have either not gone to Japan at all, or gone anyway and been clueless foreigners as tourists are wont to do.The correlation here would be something like: the people using AI to build apps previously would simply never have created an app, so it\u2019s not affecting software development as a career as much as you first expect.It would be like saying AI art won\u2019t affect artists, because the people who would put in such little effort probably would never have commissioned anyone. Which may be a little true (at least in that it reduces the impact).However, I don\u2019t necessarily know if that\u2019s true for software development. The ability to build software enabled huge business opportunities at very low costs. I think the key difference is this: the people who are now putting in such low effort into commissioning software maybe did hire software engineers before this, and that might throw off a lot of the numbers.\n \nreply",
      "The reasonable concern people have about AI eliminating coder jobs is that they will make existing coders drastically more productive. \"Productivity\" is literally defined as the number X of people required to do Y amount of stuff.I'm not sure how seriously people take the threat of non-coding vibe-coders. Maybe they should! The most important and popular programming environment in the world is the spreadsheet. Before spreadsheets, everything that is today a spreadsheet was a program some programmer had to write.\n \nreply",
      "I'm still optimistic that the net effect of making existing programmers drastically more productive is that our value goes up, because we can produce more value for other people.\n \nreply",
      "The economy has taught us that when there is an excess of worker productivity, it leads to layoffs. It certainly does not lead to raises.\n \nreply",
      "That sounds like the famous lump of labour fallacy. When something's cheaper people often spend more on it (Jevons paradox).\n \nreply",
      "No software company I have ever worked at had an excess of worker productivity. There were always at least 3-5X as much work needing to be done, bugs needing to be fixed, features that needed to be implemented than engineers to do it. Backlogs just grew and grew until you just gave up and mass-closed issues because they were 10 years old.If AI coding improves productivity, it might move us closer to having 2X as much work as we can possibly do instead of 3X.\n \nreply",
      "Do you have a citation for that?\n \nreply",
      "What a strange thing to ask for a citation on when CEO pay, stock buy backs and corporate dividends are at all time highs while worker pay and honestly just affording to live continue to crater.\n \nreply",
      "> everything that is today a spreadsheet was a program some programmer had to writeThat is incorrect, sir.First, because many problems were designed to fit into spreadsheets (human systems designed around a convenient tool). It is much more likely that several spreadsheets were _paper_ before, not custom written programs. For a lot of cases, that paper work was adapted directly to spreadsheets, no one did a custom program intermediate.Second, because many problems we have today could be solved by simple spreadsheets, but they often aren't. Instead, people choose to hire developers instead, for a variety of reasons.\n \nreply",
      "I'm not sure we're really disagreeing about anything here. If you think spreadsheets didn't displace any programmers at all, that's contrary to my intuition, but not necessarily wrong --- especially because of the explosion of extrinsic demand for computer programming.\n \nreply"
    ],
    "link": "https://ingrids.space/posts/what-google-translate-can-tell-us-about-vibecoding/",
    "first_paragraph": "\u03bbnfx.f (n f x)There has been rather a lot of doomsaying (and perhaps astroturfing) lately about LLMs as the end of computer programming. Much of the discussion has been lacking nuance, so I\u2019d like to add mine. I see claims from one side that \u201cI used $LLM_SERVICE_PROVIDER to make a small throwaway tool, so all programmers will be unemployed in $ARBITRARY_TIME_WINDOW\u201d, and from the other side flat-out rejections of the idea that this type of tool can have any utility.1 I think it best sheds light on these claims to examine them in the context of another field that\u2019s been ahead of the curve on this: translation.Google translate has been around for a while, and has gone through some technological iterations; I\u2019m most interested in discussing its recent incarnations since the switch to neural machine translation in 2016. Over the years I\u2019ve heard much made about how this is the end of translation and interpretation as professions. I suspect the people who say such things have never actually"
  },
  {
    "title": "I Wrote a Compiler (singleton.io)",
    "points": 15,
    "submitter": "ingve",
    "submit_time": "2025-06-15T08:28:18 1749976098",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=44281238",
    "comments": [
      "TinyBASIC is fun and beautifully simple. I wrote a 3-part tutorial for making a TinyBASIC-to-C compiler using Python a few years ago.Let\u2019s make a Teeny Tiny compiler\nhttps://austinhenley.com/blog/teenytinycompiler1.html\n \nreply",
      ">The original authors of yacc were Mike Lesk and Eric Schmidt - yes that Eric Schmidt.Incorrect, they were authors of lex. yacc was authored by Stephen Johnson.Surprising to me is all the authors are still around, even though the tools are over 50 years old!. Shows how young computer science field is.\n \nreply",
      ">The original authors of yacc were Mike Lesk and Eric Schmidt - yes that Eric Schmidt.I don't know if it's worth mentioning, but the author of the post is David Singleton, the former CTO of Stripe. I almost hadn't noticed until I saw the domain.\n \nreply",
      "I worked ~4 layers underneath him when he led Android Wear at Google, and every year or two that happens to me, and it puts a smile on my face. Gotta have love of the game to do this at that level.IIRC, and man, maybe I'm making it up, but, lore was he always made time on a regular schedule to hack.Usually 1 layer from the bottom isn't coding so much anymore.(oddly, I didn't realize he was *CTO* of Stripe until a few months back, when his new thing with Hugo Barra was announced)\n \nreply",
      "I thought a compiler, with no adjective or caveat, should turn a HLL into machine language. Isn't what this describes\u2014turning BASIC into Go\u2014more accurately described as a \"pseudocompiler\" or \"Go compiler\" or somesuch? I know Emacs is always said to have a \"bytecode compiler\" that processes Elisp code, not a \"compiler\" per se. Am I mistaken?\n \nreply",
      "Strictly speaking it's a transpiler, but honestly the delta between the target language (Go) and the source language (BASIC) is very fluffy and wooly, from what I remember from my PL theory days the distinction was always fuzzy enough that people used whatever term felt right to them.An example off the top of my head \u2014 Chicken Scheme (call-cc.org) calls itself a compiler but it's target language is C\n \nreply",
      "What would you call TypeScript\u2019s tsc, which translates TS to JS? Microsoft would say it\u2019s a compiler: https://code.visualstudio.com/docs/typescript/typescript-com...\n \nreply",
      "The standard term for this kind of compiler is \"transpiler\", afaik.Here's the Wikipedia page for such things, which also taught me several other names for them:https://en.m.wikipedia.org/wiki/Source-to-source_compiler\n \nreply",
      "So, if he had invoked go for you would it be a compiler?\nAnother definition is that it translates a source language into a target language.\n \nreply"
    ],
    "link": "https://blog.singleton.io/posts/2021-01-31-i-wrote-a-compiler/",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: I made an online Unicode Cuneiform digital clock (oisinmoran.com)",
    "points": 31,
    "submitter": "OisinMoran",
    "submit_time": "2025-06-15T02:38:49 1749955129",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=44280168",
    "comments": [
      "If you like weird clocks, I've got a collection of them here [0] which  includes two others I've made\u2014the QR Code Clock (probably my stupidest design of anything to date), and the vague clock (which is always correct and accurate but as it is just a single rotating \"6\" is only really legible at 6 and 9 o'clock)Currently working on my first physical one![0] https://lynkmi.com/oisin/Clocks\n \nreply",
      "This one is goodhttps://github.com/BarkyTheDog/catclock\n \nreply",
      "It seems that of all the numbers (needed here), the symbol for 20 (\ud808\udf99) is the only one that doesn't render on Android. Very odd. It does seem to be the last used codepoint (U+12399) in the Cuneiform block (U+12000\u2013U+123FF) and they seem to stop rendering from U+1236E (on Android) which leaves 43 symbols un-rendered.Anyone any idea why that might be?See  \nhttps://en.wikipedia.org/wiki/Cuneiform_(Unicode_block)\nand  \nhttps://en.wikipedia.org/wiki/Cuneiform_Numbers_and_Punctuat...sent at \ud808\udf0b\ud809\udc16:\ud809\udc10\ud809\udc15:\ud808\udf0b\ud809\udc17\n \nreply",
      "Okay, in the interim I have a shipped a fix for Android (seems fine on an iPhone emulation) that uses two tens like so \"\ud808\udf0b\ud808\udf0b\" (looks like <<) instead of one twenty \"\ud808\udf99\" (also looks like << but a bit tighter). This is definitely one of the weirdest patches [0] I've ever done\u2014changing how an ancient language is displayed based on the specific type of incomprehensibly advanced technology it's being displayed on\u2014but I guess that's what Sundays are for.[0] https://github.com/OisinMoran/OisinMoran.github.io/commit/15...\n \nreply",
      "https://i.ibb.co/6RBrwZpz/firefox.pngFirefox 139.0.4 on Arch Linux\n \nreply",
      "I guess the Arch in Arch Linux isn't for archaeology then :(\n \nreply",
      "You just don't have the required font.\n \nreply",
      "Well, I can report that \ud808\udf99 isn't rendering on Windows 10 either. Your \"sent at\" renders fine. Whatever it is, it isn't specific to Android.I assume it's mostly down to fonts, but I don't know why a font would implement some of the cuneiform block without doing all of it.\n \nreply",
      "The visual appeal is undeniable ;)\n \nreply",
      "Thank you! I am quite happy with how it turned out and looking at it now reminds me a bit of the clock in Lost when it turns to Egyptian Hieroglyphs.\n \nreply"
    ],
    "link": "https://oisinmoran.com/sumertime",
    "first_paragraph": ""
  },
  {
    "title": "Making 2.5 Flash and 2.5 Pro GA, and introducing Gemini 2.5 Flash-Lite (blog.google)",
    "points": 270,
    "submitter": "meetpateltech",
    "submit_time": "2025-06-17T16:06:05 1750176365",
    "num_comments": 165,
    "comments_url": "https://news.ycombinator.com/item?id=44300717",
    "comments": [
      "They don't mention it in the post, but it looks like this includes a price increase for the Gemini 2.5 Flash model.For 2.5 Flash Preview https://web.archive.org/web/20250616024644/https://ai.google...$0.15/million input text / image / video$1.00/million audioOutput: $0.60/million non-thinking, $3.50/million thinkingThe new prices for Gemini 2.5 Flash ditch the difference between thinking and non-thinking and are now: https://ai.google.dev/gemini-api/docs/pricing$0.30/million input text / image / video (2x more)$1.00/million audio (same)$2.50/million output - significantly more than the old non-thinking price, less than the old thinking price.\n \nreply",
      "The blog post has more info about the pricing changeshttps://developers.googleblog.com/en/gemini-2-5-thinking-mod...\n \nreply",
      "The real news is that non-thinking output is now 4x more expensive, which they of course carefully avoid mentioning in the blog, only comparing the thinking prices.How cute they are with their phrasing:> $2.50 / 1M output tokens (*down from $3.50 output)Which should be \"up from $0.60 (non-thinking)/down from $3.50 (thinking)\"\n \nreply",
      "Is it possible to get non-thinking only now, though? If not, why would that matter, since it's irrelevant?\n \nreply",
      "Yes, by setting the thinking budget to 0. Which is very common when a task doesn't need thinking.In addition, it's also relevant because for the last 3 months people have built things on top of this.\n \nreply",
      "To be fair, the point of preview models and stable releases is so you know what is stable to build on.\n \nreply",
      "The moment you start charging for preview stuff I think you give a tacit agreement that you can expect the price to not increase by a factor of 4.\n \nreply",
      "interesting - why wouldn't you use dynamic thinking? and yeah, sucks when the price changes.\n \nreply",
      "It makes responses much slower with zero benefit for many tasks. Flash with thinking off is very fast.\n \nreply",
      "one example where non-thinking matters would be latency-sensitive workflows, for example voice AI.\n \nreply"
    ],
    "link": "https://blog.google/products/gemini/gemini-2-5-model-family-expands/",
    "first_paragraph": "Jun 17, 2025[[read-time]] min read\n          Gemini 2.5 Flash and Pro are now generally available, and we\u2019re introducing 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet.\n        We designed Gemini 2.5 to be a family of hybrid reasoning models that provide amazing performance, while also being at the Pareto Frontier of cost and speed. Today, we\u2019re taking the next step with our 2.5 Pro and Flash models by releasing them as stable and generally available. And we\u2019re bringing you 2.5 Flash-Lite in preview \u2014 our most cost-efficient and fastest 2.5 model yet.Thanks to all of your feedback, today we\u2019re releasing stable versions of 2.5 Flash and Pro, so you can build production applications with confidence. Developers like Spline and Rooms and organizations like Snap and SmartBear have already been using the latest versions in-production for the last few weeks.We\u2019re also introducing a preview of the new Gemini 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet. "
  },
  {
    "title": "Why JPEGs still rule the web (2024) (ieee.org)",
    "points": 135,
    "submitter": "purpleko",
    "submit_time": "2025-06-17T14:51:10 1750171870",
    "num_comments": 241,
    "comments_url": "https://news.ycombinator.com/item?id=44299970",
    "comments": [
      "How in the world do people store images / photos nowadays?Just as there is a clear winner for video - av1 - there seems to be nothing in the way of \"this is clearly the future, at least for the next few years\" when it comes to encoding images.JPEG is... old, and it shows. The filesizes are a bit bloated, which isn't really a huge problem with modern storage, but the quality isn't great.JPEG-XL seemed like the next logical step until Google took their toys and killed it despite already having the support in Chrome, which pretty much makes it dead in the water (don't you just love monopolies making decisions for you?)HEIC is good, as long as you pinky promise to never ever leave Apple's ecosystem, ie HEIC sucks.AVIF seems computationally expensive and the support is pretty spotty - 8bit yuv420 might work, but 10b or yuv444 often doesn't. Windows 10 also chokes pretty hard on it.Alternatives like WebP might be good for browsers but are nigh-unworkable on desktops, support is very spotty.PNG is cheap and support is ubiquitous but filesizes become sky-high very quick.So what's left? I have a whole bunch of .HEIC photos and I'd really like if Windows Explorer didn't freeze for literal minutes when I open a folder with them. Is jpeg still the only good option? Or is encoding everything in jpeg-xl or avif + praying things get better in the future a reasonable bet?\n \nreply",
      "> JPEG-XL seemed like the next logical step until Google took their toys and killed it despite already having the support in Chrome, which pretty much makes it dead in the water (don't you just love monopolies making decisions for you?)It's worth noting that Firefox is willing to adopt JPEG-XL[1] as soon as the rust implementation[2] is mature And that rust impl is a direct port from the reference C++ implementation[3]. Mac OS and Safari already support JPEG-XL [4]. And recently Windows picked up JPEG-XL support. The only blockers at this point are Firefox, Chromium, and Android. If/when Firefox adopts JPEG-XL, we'll probably see google follow suit if only out of pressure from downstream Chromium platforms wanting to adopt it to maintain parity.So really if you want to see JPEG-XL get adopted, go throw some engineering hours at the rust implementation [2] to help get it up to feature parity with the reference impl.-----1. https://github.com/mozilla/standards-positions/pull/10642. https://github.com/libjxl/jxl-rs3. https://github.com/libjxl/libjxl4. https://www.theregister.com/2023/06/07/apple_safari_jpeg_xl/5. https://www.windowslatest.com/2025/03/05/turn-on-jpeg-xl-jxl...\n \nreply",
      "g**gle is hellbent on killing JPEG-XL support in favor of WebP. assuming they'll capitulate to downstream pressure is a stretch. this article [0] sums it up nicely:What this [removal of support for JPEG-XL in Chromium] really translates to is, \u201cWe\u2019ve created WebP, a competing standard, and want to kill anything that might genuinely compete with it\u201d. This would also partly explain why they adopted AVIF but not JPEG XL. AVIF wasn\u2019t superior in every way and, as such, didn\u2019t threaten to dethrone WebP.[0] https://vale.rocks/posts/jpeg-xl-and-googles-war-against-it\n \nreply",
      "How is Google so intent on webp winning? They don't even support it in their own products besides Chrome.\n \nreply",
      "Chrome is like a different company. They do weird shit.\n \nreply",
      "I'm not assuming they capitulate under just pressure. Rather I'm assuming they'll capitulate if a majority of or even all of the big third party chromium browsers push for adding it to mainline chromium.This is less just blind pressure but rather the risk that google becomes seen as an untrustworthy custodian of chromium and that downstreams start supporting an alternate upstream outside of google's control.Jxl is certainly a hill that google seems intent to stand on but I doubt it's one they'd choose to die on. Doubly so given the ammo it'd give in the ongoing chrome anti-trust lawsuits.\n \nreply",
      "From what I've seen WebP is probably the strongest contender for a JPEG replacement.  It's pretty common in the indie game scene for example to re-encode a JPEG game to WebP for better image quality and often a significant (25% or more) savings on installer size.  Support is coming, albeit somewhat slowly.  It was pretty bad in Ubuntu 22, but several apps have added support in Ubuntu 24.  Windows 11 supports WebP in Photos and Paint for another example.\n \nreply",
      "I hate webp. Not for any legitimate technical reason like, but I often just want to download an image from the web for an image board or drop it in a diagram or ppt or for a joke and nothing works with that format. Nothing.  Google image search is useless because of it.Cmd+shift+4 is now the only way to grab an image out of a browser. Which is annoying.It has made my life needlessly more complicated. I wish it would go away.Maybe if browsers auto converted when you dragged ann image out of the browser window I wouldn\u2019t care, but when I see webp\u2026 I hate.\n \nreply",
      "Total agreement from me, I use this:bin/webp2png:    #!/bin/bash\n    dwebp \"$1\" -o  \"${1%%.webp}\".png\n \nreply",
      "Webp images are right up there with the fake transparent PNGs you come across in Google Images.\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/jpeg-image-format-history",
    "first_paragraph": "30 years ago, the JPEG became the dominant way we share digital photos on the internetErnie Smith is the editor of Tedium, a long-running newsletter that hunts for the end of the long tail.A version of this post originally appeared on Tedium, Ernie Smith\u2019s newsletter, which hunts for the end of the long tail.For roughly three decades, the JPEG has been the World Wide Web\u2019s primary image format. But it wasn\u2019t the one the Web started with. In fact, the first mainstream graphical browser, NCSA Mosaic, didn\u2019t initially support inline JPEG files\u2014just inline GIFs, along with a couple of other formats forgotten to history. However, the JPEG had many advantages over the format it quickly usurped.aspect_ratioDespite not appearing together right away\u2014it first appeared in Netscape in 1995, three years after the image standard was officially published\u2014the JPEG and web browser fit together naturally. JPEG files degraded more gracefully than GIFs, retaining more of the picture\u2019s initial form\u2014and tha"
  },
  {
    "title": "Foundry (YC F24) Hiring Early Engineer to Build Web Agent Infrastructure (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-06-17T21:00:27 1750194027",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/foundry/jobs/azAgJbN-foundry-software-engineer-new-grad-to-mid-level",
    "first_paragraph": "World Model for Browser AgentsLocation: San Francisco, CAWhy Foundry Exists: Most of what people do at work sucks\u2014it's manual, repetitive, and wastes time. Recruiters spend hours every day on LinkedIn, CRMs, email, and tedious data entry tasks instead of focusing on the things humans actually do well: building relationships, strategy, and decision-making.We're building tools so that AI agents can use web browsers exactly like humans, navigating enterprise apps like Salesforce, SAP, or Workday without constant manual intervention. Enterprises like Accenture staff entire teams and charge premium rates just to manage complex platforms like Salesforce, because navigating and operating these systems is so challenging. Right now, browser agents\u2014even those built on GPT-o3\u2014fail most of the time, get stuck on basic UI changes, and require endless manual debugging. This isn't sustainable.Foundry creates the infrastructure to fix this: precise simulations, robust evaluation tools, and direct supp"
  },
  {
    "title": "Time Series Forecasting with Graph Transformers (kumo.ai)",
    "points": 71,
    "submitter": "turntable_pride",
    "submit_time": "2025-06-17T18:05:53 1750183553",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=44301998",
    "comments": [
      "I'm not a fan of this blog post as it tries to pass off a method that's not accepted as a good or standard time series methodology (graph transformers) as though it were a norm. Transformers perform poorly on time series, and graph deep learning performs poorly for tasks that don't have real behaviorial/physical edges (physical space/molecules/social graphs etc), so it's unclear why combining them would produce anything useful for \"business applications\" of time series like sales forecasting.For those interested in transformers with time series, I recommend reading this paper: https://arxiv.org/pdf/2205.13504. There is also plenty of other research showing that transformers-based time series models generally underperform much simpler alternatives like boosted trees.After looking further it seems like this startup is both trying to publish academic research promoting these models as well as selling it to businesses, which seems like a conflict of interest to me.\n \nreply",
      "Recent work like Informer (AAAI'21) and Autoformer (NeurIPS'21) have shown competitive performance against statistical methods by addressing the quadratic complexity and long-range dependency issues that plagued earlier transformer architectures for time series tasks.\n \nreply",
      "Would you be so kind as to recommend some resources on modern, promising methods for time series forecasting? I'm starting a position doing this work soon and would like to learn more about it if you'd be willing to share\n \nreply",
      "thoughts on TimesFM?> After looking further it seems like this startup is both trying to publish academic research promoting these models as well as selling it to businesses, which seems like a conflict of interest to me.is this a general rule of thumb that one should not use the same organization to publish research and pursue commercialization generally?\n \nreply",
      "I disregard anything on time series forecasting from any entity that uses Facebook Prophet as a benchmark.\n \nreply",
      "Prophet is great and we use it for multiple models in production at work. Our industry has tons of weird holidays and seasonality and prophet handles that extremely well.\n \nreply",
      "We also used it at my previous job. Yes it does handle that well, but it was also simply not as correct as we would have liked (often over adjusting based on seasonality) even with tuning. Prophet was probably the right choice initially though just on how easy it is to set up to get decent results.\n \nreply",
      "This is sales research, and after \"CAGR in a GSheet\" FB Prophet is what's going to be most recognizable to the widest base of customers.FWIW seems like the real value add is this relational DB model: https://kumo.ai/research/relational-deep-learning-rdl/ The time-series stuff is them just elaborating the basic model structure a little more to account for time-dependence\n \nreply",
      "For such strong and personal statement I have to ask why.\n \nreply",
      "If you arrived into, say, London and googled \"Best fish and chips\" would you believe that the top result gives you the meal that you're after?\n \nreply"
    ],
    "link": "https://kumo.ai/research/time-series-forecasting/",
    "first_paragraph": "Jan Eric Lenssen, Matthias FeyTime series forecasting is a cornerstone in modern business analytics, whether it is concerned with anticipating market trends, user behavior, optimizing resource allocation, or planning for future growth. As such, a wide range of different approaches have been introduced and investigated for forecasting, lately data-driven approaches using machine learning and generative models.This blog post will dive into forecasting on graph structured entities, e.g., as obtained from a relational database, utilizing not only the individual time series as signal but also related information. As most of the world\u2019s data is stored in relational structures, this topic is of particular interest for real world applications. We describe an end-to-end pipeline to perform forecasting using graph transformers and specifically discuss predictive vs. generative paradigms.Forecasting is the process of making predictions about future events based on historical data and current obse"
  },
  {
    "title": "Now might be the best time to learn software development (substack.com)",
    "points": 126,
    "submitter": "nathanfig",
    "submit_time": "2025-06-17T14:51:40 1750171900",
    "num_comments": 75,
    "comments_url": "https://news.ycombinator.com/item?id=44299979",
    "comments": [
      "\"Great news, boss!  We invented this new tool that allows nontechnical people to write code in English!  Now anyone can deploy applications, and we don't have to hire all those expensive developers!\"\"Wow, show it to me!\"\"OK here it is.  We call it COBOL.\"\n \nreply",
      "Bravo. This is the exact sentiment I have, but you expressed in a way that I could never have.Most people miss the fact that technical improvements increases the pie in a way that was not possible before.When digital cameras became popular, everybody become a photographer. That only made the world better, and we got soo many more good photographers. Same with YouTube & creativity.And same with coding & LLMs. World will have lots more of apps, and programmers.\n \nreply",
      "> That only made the world better, and we got soo many more good photographers.I disagree with the \"only\" part here. Imagine a distribution curve of photos with shitty photos on the left and masterpieces on the right and the height at the curve is how many photos there are to be seen at that quality.The digital camera transition massively increased the height of the curve at all points. And thanks to things like better autofocus, better low light performance, and a radically faster iteration loop, it probably shift the low and middle ends to the right.It even certainly increased the number number of breathtaking, life-changing photos out there. Digital cameras are game-changes for photographic journalists traveling in difficult locations.However... the curve is so high now, the sheer volume of tolerably good photos so overwhelming, that I suspect that average person actually sees fewer great photos than they did twenty years ago. We all spend hours scrolling past nice-but-forgottable sunset shots on Instagram and miss out on the amazing stuff.We are drowning in a sea of \"pretty good\". It is possible for there to be too much media. Ultimately, we all have a finite amount of attention to spend before we die.\n \nreply",
      "Thank you for describing this so eloquently.Meaning no disrespect to photographers, I'm starting to think that a probable outcome of all the AI investment is a sharp uptick in shovelware.If we can get AIs to build \"pretty good\" things - or even just \"pretty average\" things - cheaply, then our app stores, news feeds, ad feeds, company directives, etc, will be continuously swamped with it.\n \nreply",
      "Experts warn that at current production levels, the supply of dick pics may actually outpace demand in a couple decades.\n \nreply",
      "> That only made the world betterDid it?people now stand around on dance floors taking photos and videos of themselves instead of getting on dancing and enjoying the music. to the point where clubs put stickers on phones to stop people from doing it.people taking their phone out and videoing / photographing something awful happening, instead of doing something helpful.people travel to remote areas where the population has been separated from humanity and do stupid things like leave a can of coke there, for view count.it\u2019s not made things better, it just made things different. whether that\u2019s better or worse depends on your individual perspective for a given example.so, i disagree. it hasn\u2019t only made things better. it made some things easier. some things better. some things worse. some things harder.someone always loses, something is always lost. would be good if more people in tech remembered that progress comes at a cost.\n \nreply",
      "> people now stand around on dance floors taking photos and videos of themselves instead of getting on dancing and enjoying the music. to the point where clubs put stickers on phones to stop people from doing it.There are other types of dances where dancers are far more interested in the dance than selfies: Lindy Hop, Blues, Balboa, Tango, Waltz, Jive, Zouk, Contra, and West Coast Swing to name a few. Here are  videos from the Blues dance I help organize where none of the dancers are filming themselves:* https://www.facebook.com/61558260095218/videos/7409340551418...* https://www.facebook.com/reel/3659488930863692\n \nreply",
      "The irony!Though, I'll grant that there's not really a way to argue this without showing videos\n \nreply",
      "I would add one thing though.   The pie definitely gets bigger - but i feel there is a period of \"downsizing\" that happens.  I think this is becuase of lack of ideas.   When you have tool that (say) 10xes your productivity, its not that bosses will have ideas to build 10x the number of things - they will just look to cut costs first (hello lack of imagination and high interest rates).\n \nreply",
      "We\u2019ve had many improvements that increased productivity at least as much as current LLMs, and I don\u2019t think any of them ever temporarily caused downsizing in the total number of programmers.\n \nreply"
    ],
    "link": "https://substack.com/home/post/p-165655726",
    "first_paragraph": ""
  },
  {
    "title": "From SDR to 'Fake HDR': Mario Kart World on Switch 2 (alexandermejia.com)",
    "points": 51,
    "submitter": "ibobev",
    "submit_time": "2025-06-17T19:07:42 1750187262",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=44302704",
    "comments": [
      "I believe the article is based on a wrong assumption. The author argues that everything could look more realistic and that VFX could pop more with stronger HDR, but in my opinion it makes a lot of sense to keep a stylized cartoon game also stylized in its brightness choices.When you drive towards the sun, what is more fun? A realistic HDR brightness that blinds you, or a \u201ewrong\u201c brightness level that helps the background stay in the background without interrupting your flow? Similarly, should eye candy like little sparks grab your attention by being the brightest object on screen? I\u2019d say no.The hardware can handle full HDR and more brightness, but one could argue that the game is more fun with incorrect brightness scaling\u2026\n \nreply",
      "That\u2019s not the problem.The game should look like a normal Mario game at a minimum. It should use its additional color palette available in HDR to look better, and the additional brightness to make make effects pop as you describe.The problem is that\u2019s not what it\u2019s doing. Some things pop better, but it\u2019s not because they\u2019re using extra colors. It may be a little brightness, but mostly it\u2019s that everything else just got toned down so it looks kinda washed out.If they did nothing but use the expanded color palette and did not use the additional brightness at all I would be a lot happier than with what we have right now.I haven\u2019t turned it back to SDR mode but I\u2019m legitimately considering it. Because I suspect the game looks better that way.\n \nreply",
      "Mario Kart World generally looks more saturated than previous games if anything, so it definitely meets that minimum.And the article is about they missed out on the optionality to use the additional gamut, but that additional gamut wouldn't intrinsically make it look better.It's easy enough to edit a screenshot to show us what could have been, but even in that single screenshot there are things that look worse: like the flames gained saturation but lost the depth the smoke was adding, and some reasonable atmospheric haze vanished.(similarly the game in the side-by-side has some downright awful looking elements, like the over-saturated red crystals that punch a hole through my HDR display...)Given Nintendo's track record for stylization over raw image quality, I'm not sure why this isn't just as likely them intentionally prioritizing SDR quality and taking a modest-but-safe approach to HDR... especially when the built-in screen maxes out at 450 nits.\n \nreply",
      "Every game I first start requires a trip to turn off music, in-game VoIP, HDR, bloom, lensflare, screenshake if possible.It's like a keyword bingo for usually poor implementations. I grant that maybe the implementation is good for any specific game you care to mention - but history has shaped my habits.\n \nreply",
      "> But when Gamers in ESA surveys report that the quality of the graphics being the #2 factor in deciding when to purchase a gameSomehow I doubt this survey is representative of the typical Mario Kart player. And to those for whom it is a concern, I don't think SDR is high on the list relative to framerate, pop-in, and general \"see where I'm going and need to go next\" usability.\n \nreply",
      "You are exactly right. I don't care if it's all blocks and squares. As long as I can not lag and see enough to destroy my children at the game.\n \nreply",
      "> HDR is mainstream \u2013 From just a quick browsing of BestBuy, nearly all TVs over 42\u201d are 4K and support HDR. 9th gen consoles are shipping with HDR on by default. The majority of your audience is HDR-equipped.\"Mainstream\" or \"majority\" in context of Nintendo is a $20-40k/yr white collar household with 2 kids. The REAL mainstream. Some would have real ashtrays on a dining table. ~None of them had bought any of TVs over 42\" with 4K resolution and HDR support in past 10 years.Though, I do wonder how globally mainstream is such a household buying Nintendo hardware. Admittedly it could be somewhat of a local phenomenon.\n \nreply",
      "Not sure why you would associate \u201cwhite collar\u201d with poverty. While technically there are poor office workers, this is not the typical association.\n \nreply",
      "I don't know how out of touch you have to be to think a family with two kids making $20k a year is affording a switch 2 on release.\n \nreply",
      "There's someone that did the job to actually figuring out how to make the HDR of the switch work, but it needs your display to support certain features to be correct https://www.youtube.com/watch?v=X84e14oe6gs\n \nreply"
    ],
    "link": "https://www.alexandermejia.com/from-sdr-to-fake-hdr-mario-kart-world-on-switch-2-undermines-modern-display-potential/",
    "first_paragraph": "Nintendo\u2019s Switch 2 launched on June 5th 2025 with Mario Kart World headlining the platform and on paper showcasing its new 4K60 + HDR output pipeline. That promise lands in a market where HDR is the consumer standard: analysts value the global HDR TV segment at \u2248 US $150B in 2024 and project US $250 B by 2033. \u00a0Consumer TVs with HDR functionality started shipping in 2015, and seeing that most consumers replace their TV every 6.4 years this means a high percentage console owners in 2025 now game on HDR capable screens.Yet, as I\u2019ll show in this article, Mario Kart World surfaces an industry-wide problem: SDR-first authoring with a last-minute tonemap hack ruins the experience.\u00a0 I\u2019d figure that SDR games that masquerade as HDR, or \u201cfake HDR\u201d coined by some more incendiary YouTubers, would have been a trend left back in 2020, but here we are in 2025 with a new generation of consoles with a headliner game that still reduces color gamut to SDR, and has no more dynamic range than the SDR pre"
  },
  {
    "title": "Iran asks its people to delete WhatsApp from their devices (apnews.com)",
    "points": 196,
    "submitter": "rdrd",
    "submit_time": "2025-06-17T19:12:23 1750187543",
    "num_comments": 229,
    "comments_url": "https://news.ycombinator.com/item?id=44302752",
    "comments": [
      "I find the wordsmithery on Meta's statement the most interesting:\u201cWe do not track your *PRECISE* location, we don\u2019t keep logs of who everyone is messaging and we do not track the *PERSONAL* messages people are sending one another,\" it added. \u201cWe do not provide *BULK* information to any government.\u201d\n \nreply",
      "If you read around their points, it sounds like they track general location, log group messages, and provide specific information on request to a government.\n \nreply",
      "Meta can also just lie about it. If they were secretly granting backdoor root access to some NSA spooks, like Microsoft did with PRISM or AT&T did with 641A, most likely no one would find out, so, there'd be zero actual downside to simply lying.\n \nreply",
      "> like Microsoft did with PRISM or AT&T did with 641A, most likely no one would find outPeople did find out.\n \nreply",
      "Only because a select few people had the balls to blow the whistle.Imagine if Snowden decided to just do his work and move on? How much longer would it have taken for these facts to be revealed to the public?\n \nreply",
      "Also people found out and nothing happened?So literally no downside to putting a backdoor and lying about it\n \nreply",
      "Even after we found out, nobody cared...\n \nreply",
      "They usually just do a mea culpa:Camera:\nhttps://www.bitdefender.com/en-us/blog/hotforsecurity/facebo...Audio: https://news.ycombinator.com/item?id=41424016Conversations: https://www.vice.com/en/article/facebook-said-it-wasnt-liste...Mass surveillance:\nhttps://thehill.com/video/facebook-spying-on-users-new-repor...Across the web: https://www.wired.com/story/ways-facebook-tracks-you-limit-i...Beacon:\nhttps://www.wired.com/2007/12/facebook-ceo-apologizes-lets-u...Apps: https://www.theguardian.com/news/2018/mar/17/cambridge-analy...People who aren't even on facebook: https://www.vox.com/2018/4/20/17254312/facebook-shadow-profi...Others do it too, e.g. Amazon: https://www.bloomberg.com/news/articles/2019-04-10/is-anyone...But Facebook has always been on a whole other levelhttps://www.theguardian.com/technology/2018/apr/17/facebook-...\n \nreply",
      "I will never understand how anyone in their right mind can use any product owned by Meta\u2026\n \nreply",
      "Because the entire rest of society has wrapped itself around Facebook, Whatsapp, and Instagram. It is easy to be a free software purist until you need to know if your child's school has a snow day. Websites and mailing lists are dead. I cannot be involved in my child's school or any of the informal social networks around the parents and teachers without using Meta's platforms. I cannot volunteer at a non-profit I care deeply about without using Meta's platforms, because that's what they have to coordinate.Are you going to suggest to me that I should force them onto Signal and a pile of other DIY platforms? I dare you. Look a burned out parent in their bloodshot eyes first.\n \nreply"
    ],
    "link": "https://apnews.com/article/iran-whatsapp-meta-israel-d9e6fe43280123c9963802e6f10ac8d1",
    "first_paragraph": ""
  },
  {
    "title": "Proofs Without Words (artofproblemsolving.com)",
    "points": 3,
    "submitter": "squircle",
    "submit_time": "2025-06-14T11:31:49 1749900709",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://artofproblemsolving.com/wiki/index.php/Proofs_without_words",
    "first_paragraph": "The following demonstrate proofs of various identities and theorems using pictures, inspired from this gallery.\nThe sum of the first  odd natural numbers is .\nThe sum of the first  positive integers is .\nThe sum of the first  positive integers is .[1]\nThe alternating sum of the first  odd natural numbers is . (Source)\nNichomauss' Theorem: the sum of the first  cubes can be written as the square of the sum of the first  integers, a statement that can be written as . \nHere, we use the same re-arrangement as the first proof on this page (the sum of first odd integers is a square). Here's another re-arrangement to see this:\n\nThis also suggests the following alternative proof:\n\nThe th pentagonal number is the sum of  and three times the th triangular number.  If  denotes the th pentagonal number, then . \nThe infinite geometric series .\nThe infinite geometric series .\nThe infinite geometric series .\nAnother proof of the identity . \n\nVarignon's theorem: the area of the outer parallelogram is "
  }
]