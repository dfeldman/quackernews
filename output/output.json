[
  {
    "title": "Apple introduces M4 chip (apple.com)",
    "points": 1044,
    "submitter": "excsn",
    "submit_time": "2024-05-07T14:37:32",
    "num_comments": 1233,
    "comments_url": "https://news.ycombinator.com/item?id=40286029",
    "comments": [
      "Together with next-generation ML accelerators in the CPU, the high-performance GPU, and higher-bandwidth unified memory, the Neural Engine makes M4 an outrageously powerful chip for AI.In case it is not abundantly clear by now: Apple's AI strategy is to put inference (and longer term even learning) on edge devices. This is completely coherent with their privacy-first strategy (which would be at odds with sending data up to the cloud for processing).Processing data at the edge also makes for the best possible user experience because of the complete independence of network connectivity and hence minimal latency.If (and that's a big if) they keep their APIs open to run any kind of AI workload on their chips it's a strategy that I personally really really welcome as I don't want the AI future to be centralised in the hands of a few powerful cloud providers.",
      "> In case it is not abundantly clear by now: Apple's AI strategy is to put inference (and longer term even learning) on edge devices. This is completely coherent with their privacy-first strategy (which would be at odds with sending data up to the cloud for processing).Their primary business goal is to sell hardware. Yes, they\u2019ve diversified into services and being a shopping mall for all, but it is about selling luxury hardware.The promise of privacy is one way in which they position themselves, but I would not bet the bank on that being true forever.",
      "> but it is about selling luxury hardware.Somewhat true but things are changing. While there are plenty of \u201cluxury\u201d Apple devices like Vision Pro or fully decked out MacBooks for web browsing we no longer live in a world where tech are just lifestyle gadgets. People spend hours a day on their phones, and often run their life and businesses through it. Even with the $1000+/2-3y price tag, it\u2019s simply not that much given how central role it serves in your life. This is especially true for younger generations who often don't have laptops or desktops at home, and also increasingly in poorer-but-not-poor countries (say eg Eastern Europe). So the iPhone (their best selling product) is far, far, far more a commodity utility than typical luxury consumption like watches, purses, sports cars etc.Even in the higher end products like the MacBooks you see a lot of professionals (engineers included) who choose it because of its price-performance-value, and who don\u2019t give a shit about luxury. Especially since the M1 launched, where performance and battery life took a giant leap.",
      "Engineers use MacBook pros because it\u2019s the best built laptop, the best screen, arguably the best OS and most importantly - they\u2019re not the ones paying for them.",
      "And the M1 chip on mine really alters productivity. Every time we want to update a library, we need some kind of workaround.It's great having a chip that is so much different than what our production infrastructure uses.",
      "And they can typically setup their dev environment without a VM, while also getting commercial app support if they need it.Windows requires a VM, like WSL, for a lot of people, and Linux lacks commercial support. macOS strikes a good balance in the middle that makes it a pretty compelling choice.",
      "I have one and hate it with a passion.  A MacBook Air bought new in the past 3 years should be able to use Teams (alone) without keeling over. Takes over a minute to launch Outlook.My 15 year old Sony laptop can do better.Even if Microsoft on Mac is an unmitigated dumpster fire,  this is ridiculous.I avoid using it whenever possible.  If people email me, it\u2019d better not be urgent.",
      "I avoid using Outlook on any device, but I wouldn't complain about my Surface tablet's performance based on how poorly iTunes performs...",
      "Sounds a bit like my Intel MBP, in particular after they (the company I work for) installed all the lovely bloatware/tracking crap IT thinks we need to be subjected to.  Most of the day the machine runs with the fans blasting away.Still doesn't take a minute to launch Outlook, but I understand your pain.I keep hoping it will die, because it would be replaced with an M-series MBP and they are way, way, WAY faster than even the best Intel MBP.",
      "Is it an Apple silicon or Intel machine? Intel macs are crazy slow - especially since the most recent few versions of macOS. And especially since developers everywhere have upgraded to an M1 or better."
    ],
    "link": "https://www.apple.com/newsroom/2024/05/apple-introduces-m4-chip/",
    "first_paragraph": "Text of this article"
  },
  {
    "title": "Decker: A fantastic reincarnation of HyperCard with 1-bit graphics (beyondloom.com)",
    "points": 52,
    "submitter": "metadat",
    "submit_time": "2024-05-07T22:15:05",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=40292181",
    "comments": [
      "Beautiful!I realize there's a certain aesthetic you're going for here, and what I'm about to propose is a prime example of a slippery slope, but if you're willing to go juuuuust a little further from 1-bit graphics to 2-bit graphics, you might actually get legible photographs.Here's a website that I love that has a similar aesthetic; every image here has at most six colors (although each image has its own color palette): https://solar.lowtechmagazine.com/",
      "Decker's UI is primarily black-and-white, but it actually uses a customizable 16-color palette. On the community forum[0] there's some discussion about dithering and importing colored photos. Users have taken advantage of color to make some really gorgeous projects[1].[0]: https://itch.io/t/2668739/16-color-dithered[1]: https://crowmorbid.itch.io/desker-deckmonth",
      "Folks: Don't forget to try it out yourself!https://www.beyondloom.com/decker/tour.htmlThis triggered amazingly sweet memories for me, am I alone?My only wish would be to have pinch-zooming on mobile.",
      "For some reason that uses about 20% of my iphone screen. Its so tiny I can\u2019t read the text, and they\u2019ve disabled pinch zooming so I can\u2019t get in closer.",
      "Buckle up and put on some glasses B-)Or, shudder, bookmark and visit from a desktop.  Worth it.",
      "Going into landscape helps.",
      "With respect to this project\u2019s choices, I feel like HyperCards greatest flaw was failing to implement color when the Mac had begun to embrace it. But the time I was in high school, all the Macs in my school were color machines. But HyperCard never got true, native, built-color.",
      "Previously on HN [1] (191 points, 3 months ago, 36 comments) [0] (215 points, 2 years ago, 88 comments)[0]: https://news.ycombinator.com/item?id=33377964\n[1]: https://news.ycombinator.com/item?id=38985409",
      "Credit: @smartmichttps://news.ycombinator.com/item?id=40289036"
    ],
    "link": "https://www.beyondloom.com/decker/index.html",
    "first_paragraph": "Decker is a multimedia platform for creating and sharing interactive documents, with sound, images, hypertext, and scripted behavior. You can try it in your web browser right now."
  },
  {
    "title": "Road resurfacing during the daytime without stopping traffic [video] (youtube.com)",
    "points": 500,
    "submitter": "sschueller",
    "submit_time": "2024-05-07T15:36:05",
    "num_comments": 185,
    "comments_url": "https://news.ycombinator.com/item?id=40287020",
    "comments": [
      "There is some more detail on the bridge itself in this video: https://www.youtube.com/watch?v=8tpv6n1ykfAThe bridge is assembled over 2 nights at a motorway exit (so traffic can bypass it by driving off and immediately back on to the road). During night 1 the two end ramps are assembled and attached together to make a short bridge. During night 2 the ramps are driven apart, the central section is built to reach the full length and the entire structure is driven to the final location.The entire length is 236 meters long providing a working length of 100 meters underneath. The assembled bridge can flex slightly at the joins between sections, and has a turning radius of 2 kilometers.",
      "The whole Marti youtube channel is a marvel for engineering geeks like me. If you have the occasion, you should take a look (talks about tunneling, big machines, etc)",
      "This one https://www.youtube.com/watch?v=6AV2NcyX7pk is insane. Elon's Boring Company is a joke in comparison.",
      "They use a tunnel boring machine to bore a tunnel with a 45\u00b0 slope.They do go into the mechanics of how they make this insanely massive machine drive up a grade that steep, and how they ensure it doesn't slide backward.I was glued to the screen more than with most movies.If you like channels like Practical Engineering, you will enjoy this.",
      "I was glued to the screen more than with most moviesi was going to say something similar. this was better than the action movie i saw earlier.",
      "Elon's company is targeting different things - cheap and fast, unlike the company in the video doing technically hard but probably not cheap.",
      "Is fast and cheap always better though.",
      "Move fast and break things runs into problems when \u201cbreak things\u201d translates to collapsed tunnels.",
      "The boring company isn't move fast and break things. They literally just bought a drill and use it like any other construction company. There's no innovation at all.",
      "Fascinating. Thanks for that link."
    ],
    "link": "https://www.youtube.com/watch?v=ymyIEGRw4-U",
    "first_paragraph": ""
  },
  {
    "title": "IBM Granite: A Family of Open Foundation Models for Code Intelligence (github.com/ibm-granite)",
    "points": 71,
    "submitter": "lukhas",
    "submit_time": "2024-05-07T21:16:36",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=40291598",
    "comments": [
      "Does anyone know of other open models available for code intelligence?",
      "WizardCoder, StarCoder, CodeLlama?",
      "Is there an online demo of this somewhere?",
      "I am seeing at least one granite model on ollama, wonder when they will all show up!"
    ],
    "link": "https://github.com/ibm-granite/granite-code-models",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously."
  },
  {
    "title": "LPCAMM2 is a modular, repairable, upgradeable memory standard for laptops (ifixit.com)",
    "points": 230,
    "submitter": "leduyquang753",
    "submit_time": "2024-05-07T15:17:48",
    "num_comments": 104,
    "comments_url": "https://news.ycombinator.com/item?id=40286734",
    "comments": [
      "I\u2019m glad they explained why RAM has become soldered to the board recently. It\u2019s easy to be cynical and assume they were doing it for profit motive purposes (which might be a nice side effect), but it\u2019s good to know that there\u2019s also a technical reason to solder it. Even better to know that it\u2019s been recognized and a solution is being worked on.",
      "I didn't find that a particularly complete explanation - and the slot can't be closer to the CPU because? - I think it must be more about parasitic properties of the card edge connector on DIMMs being problematic at lower voltage (and higher frequencies) or something. Note the solution is a ball grid connection and the whole thing's shielded.I suppose in fairness and to the explanation it does give, the other thing that footprint allows is a shorter path for the pins that would otherwise be near the ends of the daughter board (e.g. on a DIMM), since they can all go roughly straight across (on multiple layers) instead of a longer diagonal according to how far off centre they are. But even if that's it, that's what I mean by it seeming incomplete. :)",
      "> and the slot can't be closer to the CPU because?All the traces going into the slot need to be length-matched to obscene precision, and the physical width of the slot and the room required by the \"wiggles\" made in the middle traces to length-match them restrict how close you can put the slot. Most modern boards are designed to place it as close as possible.LPCAMM2 fixes this by having a lot of the length-matching done in the connector.",
      "Competes with space for VRM's.",
      "Yeah, you can only make the furthest RAM chip in DIMM be so close to the CPU based on the form factor, and the other traces need to match that length. Distance is critical and edge connectors sure don't help.",
      "I didn\u2019t really appreciate the insanity of the electrical engineering involved in high frequency stuff till I tried to design some PCBs. A simplistic mental model of wires and interconnects rapidly falls apart as frequencies increase",
      "They can have their technical fig leaf to hide behind but in practice, how many watts are we really saving between lpddr5 and ddr5? is it worth the ewaste tradeoff to have a laptop we can't modularly upgrade to meet our needs? I would guess not.",
      "> how many watts are we really saving between lpddr5 and ddr5?From what I gathered, it's around a watt per when idling (which is when it's most critical): the sources I found seem to indicate that ddr5 always runs at 1.1V (or more but probably not in laptops), while lpddr5 can be downvolted. That's an extra 10% idle power consumption per.",
      "Yeah, I was actually surprised to learn there was a reason other than \"Apple wants you to buy a new Macbook or overspec your current one\". It's annoying, but at least there's a plausible reason to why they do it.",
      "\"...and they charge 4x what the retail of premium RAM would otherwise be per GB\"do storage next."
    ],
    "link": "https://www.ifixit.com/News/95078/lpcamm2-memory-is-finally-here",
    "first_paragraph": "Article by:\n      Carsten Frauenheim\n@carsten"
  },
  {
    "title": "ScrapeGraphAI: Web scraping using LLM and direct graph logic (onrender.com)",
    "points": 94,
    "submitter": "ulrischa",
    "submit_time": "2024-05-07T19:41:25",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=40290596",
    "comments": [
      "What I'd love to see is scraper builder that uses LLMs/'magic' to generate optimised scraping rules for any page, ie css selectors and processing rules mapped to output keys. So you can run scraping itself at low cost and high performance..",
      "Agreed!Apify's Website Content Crawler[0] does a decent job of this for most websites in my experience. It allows you to \"extract\" content via different built-in methods (e.g. Extractus [1]).We currently use this at Magic Loops[2] and it works _most_ of the time.The long-tail is difficult though, and it's not uncommon for users to back out to raw HTML, and then have our tool write some custom logic to parse the content they want from the scraped results (fun fact: before GPT-4 Turbo, the HTML page was often too large for the context window... and sometimes it still is!).Would love a dedicated tool for this. I know the folks at Reworkd[3] are working on something similar, but not sure how much is public yet.[0] https://apify.com/apify/website-content-crawler[1] https://github.com/extractus/article-extractor[2] https://magicloops.dev/[3] https://reworkd.ai/",
      "Most of the top LLM already do this very well. It's because they've been trained on web data, and also because they're being used for precisely this task internally to grab data.The complicated ops of scraping is running headless browsers, IP ranges, bot bypass, filling captchas, observability and updating selectors, etc. There are a ton of SaaS services that do that part for you.",
      "This is essentially what we're building at https://reworkd.ai (YC S23). We had thousands of users try using AgentGPT (our previous product) for scraping and we learned that using LLMs for web data extraction fundamentally does not work unless you generate code.",
      "It seems also obvious that one would want to simply drag a box around the content you want, and the tool would just provide some examples to help you refine the rule set.Ad blockers have had something very close to this for some time, without any sparkly AI buttons.I\u2019m sure someone would be working on a subscription based model using corporate models in the backend, but it\u2019s something that could easily be implemented with a very small model.",
      "Mozenda does something like that.  I haven't used it in many years, so I'm not up to date on what it currently offers.",
      "That's an interesting take. I've been experimenting with reducing the overall rendered html size to just structure and content and using the LLM to extract content from that. It works quite well. But I think your approach might be more efficient and faster.",
      "I have been working on this. Feel free to DM me.",
      "What is the point of using LLMs for the scrapping itself instead of using them to generate the boring code for mimicking HTTP requests, css/xpath selectors, etc?I get it may be interesting for small tasks combined with a browser extension but for real scrapping just seems to be overkill and expensive.",
      "It is potentially expensive, but here's a different take.Instead of writing a bunch of selectors that break often, imagine just being able to write a paragraph telling the LLM to fetch the top 10 headlines and their links on a news site. Or to fetch the images, titles, and prices off a store front?It abstracts away a lot of manual fragile work."
    ],
    "link": "https://scrapegraph-doc.onrender.com/",
    "first_paragraph": ""
  },
  {
    "title": "The Grateful Dead's Wall of Sound (audioacademy.in)",
    "points": 121,
    "submitter": "1970-01-01",
    "submit_time": "2024-05-07T18:06:29",
    "num_comments": 70,
    "comments_url": "https://news.ycombinator.com/item?id=40289323",
    "comments": [
      "Article doesn't mention one of the more interesting (to me) aspects which was how feedback was avoided. The solution is elegant: each vocal microphone is doubled, meaning there are two at each position. The phase is inverted on one of them, the singer sings into only one, and both are sent to the speakers via their channel's amp.The effect of that setup is that only the difference between the two microphones is amplified; common signal in both (i.e. the sound coming out of the speakers) is nulled out, but the difference signal (the voice) makes it through. It apparently wasn't quite perfect but was absolutely a lot better than wailing feedback.The thing that made it sound so good was that any given speaker only reproduces a single source, but the article touches on that. The mic arrangement I described is simply what makes it possible.",
      "While it's true that they did that and why, I'd ultimately chalk it up as more of a flaw than a feature. Vocals never sounded great on wall of sound shows because they could never sing perfectly into one mic. This can be confirmed by listentng to soundboard tapes of the shows, and comparing them with ones a year or two either side - the full on Wall was only used for about a year.While the WoS laid much of the groundwork for how modern PAs are designed and operated, it was more of a white elephant than anything, and many of it's actual ideas were discarded. It was totally impractical to tour with and they lost money doing so.  The only real technical legacy it has is of using coherent phased line arrays.Really it's whole reason for existence (getting a coherent, in phase, non-canceled signal at an extended distance from the stage) isn't even relevant, as these days secondary speaker arrays with delay lines (to sync them perfectly with the mains) is almost childs play. Literally plug and play. Modern PAs can self-tune the whole system just from playing a short burst of white noise through the system, and listening for the response.",
      "(I assume you're aware, but for the larger audience)... the grateful released an album \"Two From the Vault\" which was a soundboard recording... but the original soundboard had huge phase cancellation errors due to microphone placement.  To recover it, some 20+ years later, with digital tech, the sound engineers could recover the original signal using some clever FFT and phasing very similar to what you describe modern secondary arrays use to self-tune.",
      "Ironically i haven't really listened to most of the official live albums much. I tend to just go straight to the board tapes, which often sound better due to having a few decades of technological advancement - many were transferred in the 90s or 2000s. Of course they didn't have then what we have now, but even consumers by then had access to software for things like mastering that would have made any 70's engineer drool - certain kinds of repairs are much more easily done digitally - back in the day cutting out a spot of stactic or a mic pop involved literal tape and razor blades.",
      "Two From The Vault isn't a \"official live album\", it's a soundboard that was shelved for decades due to the quality of the recording.  I got this album on CD when I was in college (early 90s) and didn't have access to high quality taping equipment, and soundboards from the late 60s were very rare.  The audio quality is absolutely excellent (I am just relistening to it now, there's only tiny background hiss, excellent clarity on all the instruments, decent vocals, and only a bit of high-volume distortion on the guitar and bass).\nIt's also a nice counterpoint to the original of the \"From The Vault\" series, One From The Vault, which was recorded years later under ideal conditions and the band had been practicing extensively.Much has changed from the days when we had to implement balanced binary trees of tapes (analog tape copies were lossy, so you wanted to minimize the total depth of copies).",
      "> Vocals never sounded great at wall of sound showsIt was Donna.",
      "Well, yeah, compared to today it's not great but no one had tried anything like that before. They delayed the sound to distant speakers with tape delay. It's cool as shit and was the groundwork for how we do things today.It's like saying relay computers were dumb... Boolean logic was new and no one had ever attempted stuff like that before.",
      "No, the whole point of the WoS was that there were no distant speakers. Everything was single sourced, to the point where each speaker only carried a single instrument.",
      "I know you probably know, but:    each speaker only carried a single instrument.\n\nEach vertical stack of speakers only carried a single instrument; not each individual speaker.",
      "The routing wasn't nessisarily full spectrum though. There were a lot of crossovers in use.I also believe I heard some of the precussion mics were targetting only one or two speakers.At least in the case of the speakers for Jerry, they had a a seperate McIntosh hi-fi amp for each speaker, being fed out of a Fender-derived preamp and a many-way splitter. Owsley basically bought the every one that model amp that was in stock at dealers on the west coast. Hundreds of thousands of dollars just on those amps - they were something like 2 or 3k a pop even then.The only reason they were even able to afford in the first place was that Owsley (Yeah that Owsley, who was also their primary sound engineer) had so much illegal cash from a decade of making most of the LSD consumed in the United States. Band never even paid for most it. It was more this crazy idea Owsley had and mostly paid for that they kind of rolled with.That sort of thing was more than a bit of a pattern in that camp, and was a large part of the band's downfall. It got to a point where it seemed like half of Marin county was on the payroll, and there was so much money going out that they had to tour constantly, wether they wanted to or not. The heavy touring clearly had a major toll on Jerry both physically and mentally. A two or three year hiatus around '91 or '92 would have done him (and probably some of the other guys) a world of good."
    ],
    "link": "https://audioacademy.in/the-grateful-deads-wall-of-sound/",
    "first_paragraph": "by Audio Academy | Feb 2, 2019 | Audio Legends"
  },
  {
    "title": "SecureDrop Protocol (securedrop.org)",
    "points": 158,
    "submitter": "Zezima",
    "submit_time": "2024-05-07T15:12:56",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=40286632",
    "comments": [
      "This would be very useful for Boeing engineers.",
      "I'm not convinced requirements 3 and 4 are actually needed.3) make internal state not useful to the attacker.4) assuming the ciphertexts won't leak seems silly. Might as well hand them out.Which leads to what they call \"trial decryption\" to be a better solution. If you are that worried about the scalability of your whistleblower protocol at that level, we are trying to solve the wrong problem.",
      "Bitmessage solved this problem a while ago. This just adds extra centralization.",
      "From the Bitmessage website https://wiki.bitmessage.org/> Security audit needed> Bitmessage is in need of an independent audit to verify its security. If you are a researcher capable of reviewing the source code, please email the lead developer. You will be helping to create a great privacy option for people everywhere!"
    ],
    "link": "https://securedrop.org/news/introducing-securedrop-protocol/",
    "first_paragraph": ""
  },
  {
    "title": "Can turning office towers into apartments save downtowns? (newyorker.com)",
    "points": 222,
    "submitter": "pseudolus",
    "submit_time": "2024-05-07T13:29:49",
    "num_comments": 422,
    "comments_url": "https://news.ycombinator.com/item?id=40285211",
    "comments": [
      "It's kind of odd to me (as someone who used to live there at its latest boom time) that nobody talks about Kansas City when it comes to this topic.From the ~70's until the early 2010's Kansas City's downtown was in a similar \"doom loop\" of crime, undevelopment, decaying historic buildings, etc... In that city 75% of the metro lives in suburbs, drives in to downtown for work and promptly leaves. Until about 2012 or so. Urban redevelopment kicked in, adding (free!) transit, boosting retail, arts district events, a new stadium, and crucially - *massive office to housing conversion projects*.There are tons of success stories like the historic Fidelity Tower at 909 Walnut (https://en.wikipedia.org/wiki/909_Walnut), a huge 35-story tower that sat vacant (creepy) for the better part of a decade and is now home to 159 units. Ditto with the Power & Light Building (https://en.wikipedia.org/wiki/Kansas_City_Power_and_Light_Bu...) (36 stories) - largely vacant for the better part of 20 years and now home to nearly 300 units. I could go on, every block has similar projects of 100+ year old buildings of nontrivial sizes that are now super unique apartments. I myself lived in the 30-story Commerce Tower (https://en.wikipedia.org/wiki/Commerce_Tower) for a while and it was incredibly cheap to do so (~$1100/month for 750sqft 1 bed on the 14th floor), I had a 10 minute commute by foot to my office, it was awesome. Even the more squat, broad midsize banking buildings have had major success with residential conversions.These kinds of conversions have been proven out when there is willpower to do so at the city level - people will move in and prices typically get competitive fast if done at scale. I've lived in SF for 4 years now and I'm convinced its a policy problem not an economic problem.",
      "> a similar \"doom loop\" of crime, undevelopment, decaying historic buildings, etc.That's not the doom loop in the OP, which results from office space demand decreasing due to so many working remotely:Urban theorists describe a phenomenon called the \u201cdoom loop\u201d: once workers stop filling up downtown offices, the stores and restaurants that serve them close, which in turn makes the area even emptier. And who wants to work somewhere with no services?> every block has similar projects of 100+ year old buildings of nontrivial sizes that are now super unique apartmentsPer the OP (and I've read elsewhere), older buildings are easier to convert because their floors are smaller, which makes it much easier to give a windows to every apartment (a law in many/most/all places).",
      "It's easy to imagine that the two doom loops are in fact connected. A vacant downtown is essentially what GP described, and the crime seemed to follow and exacerbate the problem",
      "The problem is the differences...IE: Soft on crime policies in large cities.I seriously doubt a lot of these larger cities that are in the \"doom loop\" will have the same results with the current differences between 20-30 years ago and today with simply turning buildings into apartments.Just look at New York where businesses are closing all over because of rampant theft. They aren't closing because people aren't there. They care closing because they can't afford to have half their wares walk out the door because New York is refusing to charge criminals because of \"justice\".The world we live in is vastly different than it was and the doom loops aren't just because of remote workers.",
      "Citation needed on all of that.  It's not just retail closing up in NYC -- the rent is ludicrous, and no one wants to start renting at a lowe rate lest their appraisal goes down and their mortgage lender/city coffers start putting the pressure on the landlord",
      "Corporate real estate is a different beast. Residential real estate and corporate real estate do not mirror each other in the market. One can be in high demand while the other has excess supply.Residential landlords are also much different than dealing with corp real estate owners. The terms, length of lease, laws and many other factors are completely different.",
      "I don't get how any of that is relevant when my claim is that the corporate rental rates is also too high and the financing for rentals shares the same concerns w.r.t rentable price regardless if it's residential or corporate landlords",
      "Fwiw it's almost exclusively international developers running the conversions in Kansas City. I think Greystar might be the one with the largest footprint there.",
      "Perhaps we need to encourage (via taxes?) convertible buildings that can either be corporate or residential with relative ease, similar to how in smaller towns you often have dentists and lawyers operating out of obviously converted houses.",
      "This is primarily a building code issue for residential vs commercial construction.Office generally try to maximize square footage, this tends to result in floor plans that are very awkward to adopt into residential use, primarily because the building code virtually everywhere has some sort of \"natural light\"/window requirement.This means that purpose built residential high rises tend to be \"skinnier\" to have more windows per sq. ft of floor space. Not to mention the very expensive changes (hvac, plumbing, etc.) required to support residential use.If the building code was changed so that the requirements for office and residential use buildings were closer then it would make future buildings more easily convertible between those use cases. It does not solve the problem of the existing buildings however.."
    ],
    "link": "https://www.newyorker.com/magazine/2024/05/06/can-turning-office-towers-into-apartments-save-downtowns",
    "first_paragraph": "Find anything you save across the site in your account"
  },
  {
    "title": "Jolie, the service-oriented programming language (jolie-lang.org)",
    "points": 39,
    "submitter": "todsacerdoti",
    "submit_time": "2024-05-07T21:07:33",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=40291490",
    "comments": [
      "Looking at a Database example[0] doesn't make me want to write this more than using any other framework.[0] https://docs.jolie-lang.org/v1.11.x/language-tools-and-stand...",
      "Maybe it's just a bad example, it doesn't look much different than what I can do in C#.",
      "Why would you showcase a bad example if there was a better one?",
      "Maybe we're just fish in water and don't know any different? GP, is there a better way?",
      "Kinda interesting that there seems to be a few similar ideas with \nhttps://serviceweaver.dev/I'm guess eventually this will be the way but not for a while.",
      "has refinement types. these exist in fstar lang and liquidhaskell.dependant types are not refinement [0]ts and rust have ideas to have them[1], but not so officially[0]https://www.reddit.com/r/dependent_types/comments/ay7d86/wha...[1]https://en.m.wikipedia.org/wiki/Refinement_type",
      "Also see WUFFS for a special purpose language which leans really hard on refinement to deliver safety with socks-blown-off performance in a specific niche.",
      "email:string( regex(\".*@.*\\\\..*\") )\n\nThis is incorrect regex for email. The correct one is https://pdw.ex-parrot.com/Mail-RFC822-Address.html",
      "It's a pragmatic one. It has no false negatives, and there's rarely a reason to care about false positives. (Especially not where a stricter regex would (be the only mechanism to) catch it, but a fake-but-valid address wouldn't trivially bypass it.)",
      "There is a typo on line 17."
    ],
    "link": "https://www.jolie-lang.org/index.html",
    "first_paragraph": "Jolie crystallises the programming concepts of service-oriented computing as linguistic constructs.\n\t\t\t\tThe basic building blocks of software are not objects or functions, but rather\n\t\t\t\tservices that can be relocated and replicated as needed. A composition of services is a\n\t\t\t\tservice."
  },
  {
    "title": "Cold brew coffee in 3 minutes using acoustic cavitation (unsw.edu.au)",
    "points": 358,
    "submitter": "ople",
    "submit_time": "2024-05-07T12:46:14",
    "num_comments": 226,
    "comments_url": "https://news.ycombinator.com/item?id=40284823",
    "comments": [
      "As someone who makes cold brew every day, this is one of the two approaches I've considered to speed things up, the other being one of those magnetic stirrers they have in chemistry lab.\nHowever, after careful consideration, the real low-hanging-fruit here is the time it takes to grind the coffee, load it, fill the water, clean the filter, and rinse the jar.  If a cold brew machine could automate these steps (like some hot coffee machines do) you wouldn't care about making a cup in 3 minutes because you'd always have an automatic jar ready for you from 24 hours ago.",
      "Recently I discovered that many coffee shops, maybe half in my sampling of a couple dozen in different cities, are selling cold coffee (brewed hot, then refrigerated) under the name cold brew, and even the ones that actually cold-brew them seem to be under the impression that it needs to be served cold. I was laughed at in one hipster joint for asking for a steamed or warmed cold-brew, and another one initially refused my request to warm it up saying that would make the coffee extremely sour. (It didn\u2019t) Oh, and at least one other, maybe two, said they couldn\u2019t warm cold brew (in view of both a steamer and microwave) or would have to charge extra (while someone\u2019s cheaper latte was being steamed).Reading the paper, it\u2019s not clear whether their cold brew has lower acidity (higher pH) than the same coffee hot brewed. It does say that the sonic-brew has the same pH as the normal long-steep cold brew. I\u2019m also curious if this cavitation/sonication brewing process is basically agitating the coffee, or doing something different, and how different it is from manually agitating a cold brew compared to letting it sit still for hours.",
      "This is why I love my fully automated <s>luxury</s> mediocre espresso maker (https://www.seattlecoffeegear.com/blogs/scg-blog/jura-a1-rev...). I push a button, it grinds the coffee and does stuff inside that I can't see, then moments later I have a perfectly average cup of something resembling burnt bean soup. I don't know whether it's megasonically brewed or absolute-zero infused or just wet caffeine pills. Sometimes it's OK, other times it's mediocre, but it's never been excellent or terrible. That's the same kind of consistency my code has, so I'm fine with it.It comes out lukewarm, hovering somewhere between room temperature and minutes-old vomit. If I want it hot, I microwave it. If I want it cold, I add ice. If I want a cold latte, I add milk. If I want a hot latte, I'm in the wrong house.It costs less than $1 for a quad shot. It provides caffeine or at least a close-enough placebo effect. What more could an old, washed-out dev ask for?",
      "But how do you waste time in the morning if you\u2019re not concocting some over complicated drink?I feel like my morning coffee is the one part of my day that I can control. You bet I\u2019m gonna take my time and make it enjoyable.",
      "Your ambivalence contributes little to the conversation.",
      "It represents the same position of mediocre acceptance that many espresso affectionadors will arrrive at after spending many months/years mucking about with expensive kit.It\u2019s the natural progression of most hobbies",
      "Thank you for saying this. When cold brew first came out, it was promoted as a brewing process that resulted in smoother (I'm guessing lower acidity) tasting coffee. Heating it up seemed natural, and its use in iced coffee seemed simply opportunistic. (In my experience at least).Then it quickly caught on as a novelty, with nitro et al, and when I tell people I drink cold brew warmed I get looks of confusion or turned up noses.But brew temp and serving temp are orthogonal.",
      "Barismo in Cambridge does (or did) a \"hot draft\" coffee, that is always on tap, is delicious, and is remarkably like hot cold brew. Their method is apparently a secret (although I'm sure more digging could find it), but I wouldn't be surprised if it wasn't basically on-demand heated cold brew. [1]1. https://www.baristamagazine.com/the-function-and-future-of-b...",
      "Thanks for the tip, I'll hit them up tomorrow!",
      "> But brew temp and serving temp are orthogonal.And if anyone doesn't believe this, challenge them to find a truly \"iced\" coffee. :p"
    ],
    "link": "https://www.unsw.edu.au/newsroom/news/2024/05/Ultrasonic_cold_brew_coffee_ready_under_three_minutes",
    "first_paragraph": "Photo: Getty Images"
  },
  {
    "title": "Show HN: Convert your Containerfile to a bootable OS (github.com/containers)",
    "points": 107,
    "submitter": "twelvenmonkeys",
    "submit_time": "2024-05-07T17:50:21",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=40289120",
    "comments": [
      "The 'bootable container' / 'native container' space is getting really exiting, even (and especially) for desktop usecases. Atomic Fedora has had support for so called Ostree Native Containers for a while now, and that will eventually adapt `bootc` as the base layer for building and booting containers (but as of now it's not totally ready yet). VanillaOS is also working on similar things but I don't think it'll use `bootc`.Some awesome community projects have also been born out of this space:- https://universal-blue.org/ provides some neat Fedora images, which have one of the best Nvidia driver experiences on Linux IME, and are over all solid and dependable- https://blue-build.org/ makes it pretty easy to build images like Universal Blue's for personal useThe best part here is really the composability; you can take a solid atomic Linux base, add whatever you like, and ship it over the air to client computers with container registries. All clients see is a diff every day, which they pull and 'apply' to be used after the next boot.",
      "There's also Elemental which is SUSE-oriented but distro agnostic https://github.com/rancher/elemental-toolkitI've been hoping NixOS moves in this direction over time, the distribution/rollout aspect seems under-baked currently.",
      "What's the best way to start with Elemental today? I haven't been able to grasp a start point, unlike RKE/Rancher which is pretty easy to onboard.",
      "It depends what you're trying to do, but I was essentially following this guide: https://rancher.github.io/elemental-toolkit/docs/examples/em... updated to ghcr.io/rancher/elemental-toolkit/elemental-cli:v1.3.0 / registry.suse.com/suse/sle-micro-rancher/5.4The whole project is in major flux now though, with v1.3 -> v2.1 being pre-release and docs haven't been updated, so I'm waiting for dust to settle before picking it back up. But basically `docker build` -> `elemental build-disk` -> qcow2/iso -> deploy / `elemental upgrade` update via OCI registry, or deploy vanilla image and then just update that via registry.",
      "One of my biggest complaints with distros has been the lack of documentation on how to actually build the distro itself, not just an ISO but like build all the packages from source as well. Like as if you were following an LFS book. I have seen VERY few distros that provide this.Do you think this helps that at all?",
      "Universal Blue\u2019s build system (not Blue-Build) is pretty clear, and self documenting. I maintained a personal fork of Bluefin for a while, and it was easy to understand!",
      "Something like this is what I had desired when briefly experimenting with Fedora CoreOS and having to build layered images for ZFS support. I was new to CoreOS and was stuck right after I finished building an OCI image. Eventually I learned that the only way forward was to boot with a base image, then layer what I had built and run `rpm-ostree commit`.I wonder if this project would've served my use case. The OCI images you build when layering FCOS images all build atop the base FCOS image. So I would expect them to be \"bootable\" in some sense.",
      "uCore does this if you wanna check it out: https://github.com/ublue-os/ucore",
      "Keeping an eye on this.  I've been wanting something like this to manage an air-gapped system.  I don't want to worry about keeping on offline apt repository (or what have you) synced, I just want to boot a full new image and mount my home folder.",
      "https://github.com/linka-cloud/d2vm does a similar thing an I\u2018ve used it successfully"
    ],
    "link": "https://github.com/containers/podman-desktop-extension-bootc",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously."
  },
  {
    "title": "Conical Slicing: A different angle of 3D printing (cnckitchen.com)",
    "points": 58,
    "submitter": "fanf2",
    "submit_time": "2024-05-06T10:42:03",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=40273121",
    "comments": [
      "I was thinking that (if possible) adding a sinusoidal vertical element to the layers (in both the x and y axis), similar to a crinkle crankle wall.https://en.wikipedia.org/wiki/Crinkle_crankle_wallThere'll be a limit (some point before the head hits the previous layer), but it would still help to mitigate the weakness of planar printing.",
      "We need a simple file format and or mathematical expression that describes the 3D clearance around the nozzle for a particular printer. This way slicers can automatically slope things as much as possible automatically. I suspect this would be useful on most printers even without modifying the hardware- even a a very small layer slope would improve overhangs a lot.",
      "Very cool project, breaking the assumption that the nozzle must lay down material while moving on the same plane as the bed is a great innovation.One nit though about the website: I found it very distracting that the images kept cycling between slides, and by the time I was halfway down the article, I realized that they were cycling even when off-screen.",
      "I've felt that it's strange that these kinds of things aren't more standard, but I'm guessing it's harder to tune the machines for this?The core idea is basically the same as that of the Nubian vault, so the core idea is very ancient. One can do this with snow too, to build somewhat surprising structures without having to use supports.",
      "Back in the day I implemented this, the trick is that the firmware motion planner is generally by default not designed to handle smooth curves in 3D, only in 2D. We could have adjusted to fix that but the other issue was it makes the printing/slicing process even more unpredictable, which therefore makes it harder to use as an engineering tool.",
      "These days I bet Klipper doesn't care about how many axes are simultaneously moving.",
      "If I recall correctly, 3D printers don't even handle smooth curves in 2D. I've heard CNC people express contempt at how primitive the Gcode used by printers is, how it doesn't support any of the advanced features of machine movement that create smooth curves.",
      "This is correct the last I read about it, but there was a project called ArcWelder that would post-process your gcode and convert the motions to true arcs. The printers today support the arc gcode, but the slicers don't generate them.https://github.com/FormerLurker/ArcWelderLib",
      "This is actually kind of an awesome idea, printing overhang has been a issue with printer adoption, and greatly limits some of the parts you can fabricate. I hope this becomes mainstream in the major slicers soon.",
      "In my experience printing overhangs is rarely an issue.The big issue that I have is that the orientation of the layers determines the strength of the part. Any vertical tension pulls the layers apart, so sometimes I wish I could somehow print a part in two orientations to make it stronger..."
    ],
    "link": "https://www.cnckitchen.com/blog/conical-slicing-a-different-angle-of-3d-printing",
    "first_paragraph": "Current 3D printing slicers are dumb. What I mean by this is that even though they are slicers for 3D printing they simply stack 2-dimensional layers on top of each other to form your final part. There are basically no movements within the GCode instructions where all 3 axes move simultaneously. That\u2019s why current 3D printing slicers are rather 2.5D slicers. But why are they using this approach? Well, simply because it\u2019s mathematically easy and honestly because this approach works remarkably well. Yet we are leaving a ton of potential on the table because 3D printers are easily capable of complex 3-dimensional moves, yet we don\u2019t have any software to take advantage of it. Over the last few years 3D printer slicers didn\u2019t really change a lot besides being way easier to use and much quicker. Yet the general slicing approach always stayed the same. Cut a 3D part into a 2-dimensional slice, draw perimeters around the circumference and fill the rest with one of many infill patterns. The onl"
  },
  {
    "title": "Unix forking the universe by running IBM's free online quantum computer (parel.es)",
    "points": 14,
    "submitter": "andrewp123",
    "submit_time": "2024-05-07T22:52:07",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=40292452",
    "comments": [
      "1 fork per 4 seconds? That's so inefficient. Just observing Geiger counter will fork the universe faster than article's very inefficient implementation.",
      "> The outcome provably does not exist until you measure it.This is not true. It only provably does not exist in local hidden variables.",
      "> I genuinely think it's a fun & potentially meaningful way to make a decision.what difference is there to any 50/50 choice mechanism you chose, other than being horrendously expensive to implement?",
      "If you roll a regular coin without any quantum effects, every version of you will either see only heads, or only tails. You need quantum in order to make the choice nondeterministic.",
      "I keep getting \"500 Internal Server Error\" when trying to login. It's not logging in. I get this error after entering the \"IBM verify code\" which I receive in an email.",
      "There are no other versions of people",
      "I think it's time we had the quantum computing talk: https://www.smbc-comics.com/comic/the-talk-3"
    ],
    "link": "https://parel.es/blog/quantum-dice",
    "first_paragraph": "May 7, 2024"
  },
  {
    "title": "Gradient descent visualization (github.com/lilipads)",
    "points": 137,
    "submitter": "weinzierl",
    "submit_time": "2024-05-07T06:24:59",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=40282923",
    "comments": [
      "These are nice animations. However I've always hesitated to get too enamored with these simple 2D visualizations of gradient descent, because one of the strange takeaways from deep learning is that behavior in high dimensions is very different from behavior in low dimensions.In a 2D problem with many local minima, like the \"eggholder functions\" [1], gradient descent will be hopeless. But neural net optimization in high dimensions really is a similar situation with many local minima, except gradient descent does great.Gradient descent in high dimensions also seems to have the ability to \"step over\" areas of high loss, which you can see by looking at the loss of a linear interpolation between weights at successive steps of gradient descent. This, again, seems like extremely strange behavior with no low-dimensional analogue.[1] https://www.sfu.ca/~ssurjano/egg.html",
      "My understanding of this phenomenon in DL is that its not due to anything intrinsic to gradient descent, the same principles and understanding apply.Rather, it is that with very large dimensionality the probability that you spuriously get all derivatives to be zero is vanishingly small. That is, local minima are less likely because you need a lot of dimensions to agree that df(x_i)/dx_i = 0.I may be wrong though!",
      "if the probability that you get a derivative close to 0 is small, say only 10%, then you need just 3 dimensions to get that multiplicative probability equal to a tenth of a percent. 3 is hardly \u201cvery large dimensionality\u201dyou can assign different numbers, but still you will find you don\u2019t need more than say 10 dimensions for this effect to happen.",
      "Does gradient descent really do well for deep learning when the gradient is computed with respect to the whole dataset? I assumed that the noise in SGD played an important role for escaping local minima.",
      "There aren't really local minima in most deep networks.  When you get into millions/billions of parameters, there will essentially always be some directions that point downwards.  You have to get really really close to the true minimum for there to be no direction to go that improves the loss.Incidentally this same phenomenon is IMO how evolution is able to build things like the eye.  Naively you'd think that since you need so many parts arranged so well, it's impossible to find a step by step path where fitness goes up at every step, i.e. if you just have a retina with no lens or vice-versa, it doesn't work.  However, due to the high dimensionality of DNA, there is essentially guaranteed to be a path with monotonically increasing fitness just because there are so many different possible paths from A to B in the high dimensional space that at least one is bound to work.Now this isn't strictly true for every high dimensional system.  You need to have a degree of symmetry or redundancy in the encoding for it to work.  For example, in convolutional neural networks, you see this phenomenon where some filters get \"stuck\" in training, and those are local minima for that subspace.  What happens though is that if one filter gets stuck, the network will just use another one that had a better initialization.  This is why pruning works, lottery tickets, etc.  Things like residual connections enhance this effect since you can even be stuck in a whole layer and the training process can just bypass it.You see the same thing with life, where you could put a sequence for the same protein in different parts of the genome and it could still be produced, regardless of the position.  There are also many different ways to encode the exact same protein, and many different possible proteins that will have the same shape in the critical areas.  Life finds a way.",
      "If that was the case we would be finding globally optimal solutions for complicated non-convex optimization problems.The reality is different, you need to really explore the space to find the truly global optimal solution.A better explanation is that for ml you don't want a globally optimal solution that overindexes on your training set. You want a suboptimal solution that might also fit an unseen data set.",
      "> There aren't really local minima in most deep networks.How so?If there are no local minima other than the global one there are convex optimization methods that are far faster than SGD or Adam. The only reason these methods exist is because deep networks is a non-convex optimization problem.",
      "There are many nonconvex functions where every local minimum is global, and even many nonconvex functions with a unique local minimum (which is de facto global). Convex methods can fail on those.The reason why GD and friends are a good choice in deep learning is that computing the gradient is cheap (and approximating it even more so). Every descent method relies on solving a subproblem of sorts, typically projecting the current iterate on a sublevel set of an approximation of the function, for some definition of projection. With GD, it's as cheap as it gets, just subtract a shrinked version of the gradient. Subproblems in other algorithms are a lot more expensive computationally, particularly at high dimensions. So more efficient as in requiring fewer function evaluations, yes, but at the cost of doing a lot more work at each step.",
      "Note that such a 2D parameter space gives often the wrong intuition when thinking about applying gradient descent on high-dimensional parameter space.Also, mini-batch stochastic gradient descent behaves more stochastic than just gradient descent.",
      ">gives often the wrong intuition when thinking about applying gradient descent on high-dimensional parameter spaceCan you give some examples?"
    ],
    "link": "https://github.com/lilipads/gradient_descent_viz",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously."
  },
  {
    "title": "Facebook just updated its relationship status with Web Components (mux.com)",
    "points": 90,
    "submitter": "mmcclure",
    "submit_time": "2024-05-07T17:38:32",
    "num_comments": 39,
    "comments_url": "https://news.ycombinator.com/item?id=40288947",
    "comments": [
      "> Web components get imported and registered at the global level. This also means if two different libraries have a component called my-button, you can\u2019t use both of them.In my opinion, fundamental problems like these really should to be addressed before things become part of the web platform.",
      "It seems like Web Components are ideal for libraries shipping pre-built components, which is probably why Mux finds it compelling. From Mux's point of view, they want the highest level of compatibility with the least amount of framework lock-in. For example, they don't want to have to ship a library for React and another one for Vue and another one for bare bones JS/HTML.In terms of building a web app where you control the environment end-to-end, I don't think there's any inherent upside to using Web Components over React.",
      "(author from Mux here) -- that is correct. For the stuff we build for in-house use on mux.com and dashboard.mux.com we have a components library written in React.You nailed it that we are shipping SDKs with visual components (like a video player) that need to be compatible across all kinds of frontends.Instead of N number of SDKs to maintain where N is every front end framework, we have 2: a web component, and a React wrapper around the web component. Maybe in the (near) future we only have to maintain 1.",
      "> I don't think there's any inherent upside to using Web Components over React.The upside is that web components will last longer than react components.There was a great blog post about this very thing and why someone chose them over react components in their use case (making their code migration-proof across updates and stack changes): https://jakelazaroff.com/words/web-components-will-outlive-y...",
      "They won't last longer than a react component (that's not even the claim the linked post is making); you can just use them in a different framework, if you care about that....which is the point the parent comment was making.",
      "I read the point as WebComponents will last longer because \"a standard\" while React is \"a Facebook standard\".",
      "The good: this seems like a necessary step towards standards-based developmentThe worrying: we might be setting up for a scenario where newer developers are bewildered by mixed paradigms within what was (at least once upon a time) a simple libraryFull disclosure I work for Mux (OP), so as a team we're web component fans, but I'm personally still almost exclusively writing React. So...personally, saying this with love, but add this to all the new esoteric hooks in React 19 and it feels like React might be losing the plot.",
      "Interesting. As a not-React user, it feels like a very positive thing that the framework is keeping up with alternatives like Vue (https://vuejs.org/guide/extras/web-components) and Svelte (https://svelte.dev/docs/custom-elements-api) in terms of standards support.",
      "Im not a front end dev but from what Ive read it seems like ES6 and native web components solve a lot of the problems react was originally trying to solve.So is react losing the plot or just naturally it's ideas are being folded into the browser natively?",
      "The number 1 problem React was created to solve is synchronizing state with UI (FB chat originally). Web Components don't provide any solution to this problem."
    ],
    "link": "https://www.mux.com/blog/facebook-just-updated-it-s-relationship-status-with-web-components",
    "first_paragraph": "Published on  May 3, 2024 (4 days ago)"
  },
  {
    "title": "Hackers discover how to reprogram NES Tetris from within the game (arstechnica.com)",
    "points": 161,
    "submitter": "LorenDB",
    "submit_time": "2024-05-07T11:24:08",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=40284291",
    "comments": [
      "Such exploits always remind me of the line from Stross' Accelerando about the ultimate end game for hacking: \"running a timing channel attack on the computational ultrastructure of space-time itself, trying to break through to whatever's underneath\"",
      "I am fairly sure I would not want to be within the lightcone of anyone making a attempt with chance doing anything. thats sounds like a good way to trigger vacuum decay and I would rather that universe not bluescreen.",
      "> I would not want to be within the lightconeIf someone were to find an exploit to run arbitrary code using the computational ultrastructure of the universe, I wouldn't be too sure if in-game restrictions could keep us safe, though!",
      "I would be less concerned with them succeeding and more with them failing and crashing the local shard",
      "I'm sure the sysadmins can restart us. They do have backups, right?",
      "Even if you have backups, if you have never tested your backups, you don't have backups.",
      "Not to worry, the VM we're on has only been running since last Tuesday.",
      "The computational substrate might just be a side-effect of something else happening in higher dimensions.",
      "Being involved in a vacuum decay event would not be bothersome in the slightest.",
      "A very hard sci-fi novel about something like this:https://en.wikipedia.org/wiki/Schild%27s_Ladder  (beware spoilers)"
    ],
    "link": "https://arstechnica.com/gaming/2024/05/hackers-discover-how-to-reprogram-nes-tetris-from-within-the-game/",
    "first_paragraph": "Front page layout"
  },
  {
    "title": "PDEP-13: The Pandas Logical Type System (github.com/pandas-dev)",
    "points": 11,
    "submitter": "hackandthink",
    "submit_time": "2024-05-05T06:58:41",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=40262837",
    "comments": [
      "This actually addresses a huge hole right now in the ecosystem. At the moment, Arrow treats things that are logically the same as different types, for example a dictionary-encoded utf8 is different from a utf8. Really there needs to be a distinction between logical and physical types, and it\u2019s a big source of rough edges in things like Acero. Hacking on my own query engine on the side, I\u2019ve put some thought into how to group physical into logical types in Arrow.I am however worried that the place to define the logical type system might be inside Arrow instead of Arrow consumers. If everyone has their own logical type system, we\u2019re just going to end up with the same incompatibilities that Arrow was trying to solve.",
      "I've opened some issues over the years with Pandas on the transition to Arrow dtypes. They are generally very nice but even today I have many workaround patches in our codebase to avoid bugs (e.g. fillna and dropna working as expected).This PDEP doesn't discuss nullable types at all (i.e. what is currently Float32 instead of float32), which is worth mentioning (though support is pretty good now in Pandas). If any of the core developers reads this, can I suggest `float32?` instead of `Float32`? I think it's more obvious what is intended with the question mark than with capitalization.",
      "I keep on reading this as pdp-13, a la pdp11 computers from history"
    ],
    "link": "https://github.com/pandas-dev/pandas/blob/8a246978e9812a2448f43d0df24a82d51e850d53/web/pandas/pdeps/0013-logical-type-system.md",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously."
  },
  {
    "title": "Array.shift Optimizations in Firefox's JavaScript Engine (2020) (lannonbr.com)",
    "points": 42,
    "submitter": "melvinroest",
    "submit_time": "2024-05-06T00:24:39",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=40269911",
    "comments": [
      "While it's clever, it is also counter intuitive. I would expect an array to be 0(n) when inserting from the beginning. If I wanted something fast from both side, I'd use a double ended queue[1]. This, as the author shows, lead to unexpected behavior.[1] https://en.wikipedia.org/wiki/Double-ended_queue",
      "The problem is, JavaScript doesn't have a rich collections library built in, so the collections that do exist are pushed in all directions. The article you linked actually highlight's JavaScript's array as the double-ended queue implementation:> Javascript's Array prototype & Perl's arrays have native support for both removing (shift and pop) and adding (unshift and push) elements on both ends.Unfortunately, trying to predict O-notation bounds in JavaScript is a fool's errand. The spec usually (always?) doesn't mention expected bounds, so it can depend on the runtime and the heuristics the runtime uses to decide what underlying data structure to use. I was surprised to learn that even implementing isEmpty(ob) in Javascript is an O(n) operation on most runtimes.",
      "> The spec usually (always?) doesn't mention expected boundsActually, I was just reading the other day that there are a few places where the specs do explicitly state expectations, such as performance characsteristics for Map/Set/WeakMap/WeakSet> Maps must be implemented using either hash tables or other mechanisms that, on average, provide access times that are sublinear on the number of elements in the collection. The data structure used in this specification is only intended to describe the required observable semantics of Maps. It is not intended to be a viable implementation model.https://tc39.es/ecma262/multipage/keyed-collections.html#sec...",
      "Bloom filter and gpu",
      "As far as the spec is concerned, JS arrays are actually JS objects. The keys of JS arrays are technically stringified numbers. Because it is essentially a hashtable, there is no guarantee that the array is contiguous.It's not commonly done, but you could add something like `\"myNonNumberKey\"` to your array and things would actually work mostly as expected because the methods in question only work on stringified keys that contain only numbers (numbers like \"01\" aren't counted and the length property doesn't include any of these non-number keys). I'm not completely sure, but I believe these extra properties prevent some optimizations, so use at your own risk.When your array has holes, decent performance simply isn't possible, so giving hard O limitations would be mostly meaningless because the required performance characteristics would be so bad. This is where the JIT magic comes into play. They don't actually stringify your keys normally, but if they detect something requires these keys (eg, calling `Object.keys()` on your array), they can work around this at the expense of some performance.If you use the third function parameter for something like `.map()`, they can handle it, but there are edge cases (eg, what happens when you are halfway through the array then unshift or splice the first item?) that will slow things down.They could make stronger guarantees about arrays in the spec, but that creates a couple headaches. They would have to basically enshrine a second array specification. That specification would have all kinds of weird edge cases with the current array specification (like that `.unshift()` during a `.map()` issue). Those things are currently offloaded to implementations with the handwaving \"you may do things better as long as the user can't tell the difference\").The other issue is that small implementations (eg, QuickJS or Duktape) that have hardware limitations in measured in kb don't have the space to guarantee these advanced features. They are constrained to pick just the one, slow implementation (maybe with a handful of the most important fast optimizations that are easy to implement) so they fit on the MCUs they were designed for.Finally, there is a record/tuple proposal that could allow a lot stricter standards and better performance guarantees for iterating a long tuple in a way that behaves like an immutable array.",
      "> I was surprised to learn that even implementing isEmpty(ob) in Javascript is an O(n) operation on most runtimes.That is surprising. Where `n` is the number of members, yes?Surely that kind of operation should either return false immediately if there are no members, or return true in constant time if there is at least one member?...although, do you have to iterate through the inherited members and check for `hasOwnProperty()`? Is it maybe O(n) where `n` is the number of enumerable inherited properties?",
      "You would think, right?The JS runtime itself must have a constant time way to know this, but it\u2019s not exposed anywhere. The best you can do is iterate over the keys and bail the first time you get a key. But (at least in V8) iterating over the keys does some work up front that is O(n), even if you bail at the first key!",
      "Isn't this kind of negated by the fact that `delete arr[n]` working the way it does has required JavaScript arrays to never have been \"real\" arrays, in the entire history of the language?",
      "I don\u2019t think so. There\u2019s a little bit more nuance to it (because it involves array \u201choles\u201d\u2014almost a third kind of null), but delete on an array index is semantically closer to assigning undefined to the index. It doesn\u2019t actually delete the presence of the index.",
      "> It doesn\u2019t actually delete the presence of the index.Doesn't it? Try:    a = [ 6, 7, 8, 9, 10 ]\n    delete a[2]\n    for (x in a) { console.log(x) }"
    ],
    "link": "https://lannonbr.com/blog/2020-01-27-shift-optimizations/",
    "first_paragraph": "Select a theme. Click on the overlay or the button again to exit"
  },
  {
    "title": "Pyspread \u2013 Pythonic Spreadsheet (pyspread.gitlab.io)",
    "points": 247,
    "submitter": "Qem",
    "submit_time": "2024-05-07T11:08:56",
    "num_comments": 101,
    "comments_url": "https://news.ycombinator.com/item?id=40284219",
    "comments": [
      "I'd like to give praise for the \"Target User Group\" section on the homepage.Not only does it say what users the app is for, but also who it is NOT for. I think this kind of information is invaluable in deciding whether or not to use or suggest an app.I can understand if app developers want EVERYBODY to user their app (whether or not its the best for the job) or if the app developer just doesn't want to take the time to write out who the app is NOT for. But I will praise those who do include that information.",
      "After reading the parent comment - I thought to myself \u201cso what\u201d.But after reading it on the product page - I fully agree: seeing clearly the targeted personas and out of scope usage significantly elevates my trust in the product and the team behind it.Have not used the software, but now I want to try",
      "I like it and I'll take it a step further. I think this is important information for a developer to admit to THEMSELVES at least. Being all things to all people is a route straight to burnout for an open source project.",
      "And yet, it is not available on macOS, presumably THE platform where that target user group lives.",
      "I would really like to see a distribution which puts all the best alternative software together:- pyspread for a spreadsheet- LyX for a word-processor- OpenSCAD for a 3D modeler- TkzEdt (or ipe) for 2D drawing&c.(and I'd be interested in suggestions for similar software for other tasks, esp. presentations and database work)",
      "But why do you even need a distro to begin with just to ship certain software set? Install Arch/Gentoo and install whatever is your preferred software of choice, or hell, you can even do that on any other distro.",
      "The value a dedicated distro provides here is that you don\u2019t have to do the legwork to research and find the best tools.As power users we may want to do that ourselves, but a lot of people place value in having that curation done for them.",
      "I think it would be an interesting thing for a distro to market/focus on, and it might help to find/identify/encourage additional such software.",
      "Isn\u2019t that basically package groups (or whatever the district-specific terms are)?Eg Im pretty sure Ubuntu Desktop has some kind of \u201cProductivity\u201d package group that includes a word processor and spreadsheets and an email client and what not. I\u2019m pretty sure it\u2019s selected by default when you do a full desktop install. I don\u2019t recall what the actual software is, but I would imagine LibreOffice.I would agree with OP that it doesn\u2019t really make sense for a distro, though. People really want to \u201cmake a distro\u201d for some reason so we end up with silly shit like Kubuntu (Ubuntu\u2026 with KDE pre-installed).My general rule of thumb is if I can point the distro\u2019s OS package manager to the distro\u2019s upstream (ie Ubuntu for Kubuntu, or Debian for Ubuntu) and everything works or mostly works, it should be a script or apt repo and not a distro.There are way too many \u201cUbuntu but with a different default DE\u201d distros that could really just be a modified install ISO or post-install script.",
      "A pkg group is enough."
    ],
    "link": "https://pyspread.gitlab.io/",
    "first_paragraph": "pyspread is a non-traditional spreadsheet application that is based on and written in the programming language Python.\nThe goal of pyspread is to be the most pythonic spreadsheet."
  }
]