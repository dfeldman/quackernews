[
  {
    "title": "Visualizing Weather Forecasts Through Landscape Imagery (github.com/lds133)",
    "points": 317,
    "submitter": "lds133",
    "submit_time": "2024-09-20T16:31:00.000000Z",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=41603546",
    "comments": [
      "Cute, and with small adjustments, I'd be legitimately using this. There are just better ways to interpret things:1. Make the bending trees signify wind direction. Have to get creative with north and south, but a tree bent down vs out can do, and the bend or size and clustering of trees should signify magnitude of the wind.2. Put sunrise and sunset as literally sun over the horizon, not the sun and moon.3. Make the night sky shaded differently than day4. Don't start at \"current time\" but rather a fixed point, either morning or midnight, and specify the \"now\" via the location of the house\n \nreply",
      "All good points, agreed. Except #4, would be cute if there's some animal that moves along instead of the house.And perhaps playing with some kind of isometric perspective could help visualize wind directions?\n \nreply",
      "> Would be cute if there's some animal that moves along instead of the house.I can suggest a snail with a shell that has a window and a chimney out the back!It can also be a something man-made but playful like a freight train with a caboose. You\u2019d only see the back end of the train and it would move off screen over the course of the day.Mind you, trains are now ruined for me with John Oliver\u2019s production of Thomas the tank engine.\n \nreply",
      "I noodled with a project a couple of years ago to pick art based on the weather\nhttps://bazzargh.github.io/weather/put it on 'manual filter' and try setting some of the filters, you can see the tagged images it comes up with. I wasn't really interested in this being an accurate weather report, I was thinking more of using it in a photoframe or as a desktop background for mood.the image tags are all in here https://github.com/bazzargh/bazzargh.github.io/blob/master/w...and were largely done manually, I started by picking paintings I liked, then looking for gaps in the tags and trying to find paintings to cover those.\n \nreply",
      "Your page is getting flagged for phishing on Chrome.\n \nreply",
      "In FF as well, Just reported it as \"not suspicious\"\n \nreply",
      "Huh. That's new. I'm guessing it must have been someone who read it here, I don't think I ever even posted it anywhere else. I'm not sure what they could think it's phishing for; there's no links out, no redirection, and nowhere for you to enter any personal information; the only thing it does is pull images from wikimedia, plus the source code is right there for all to see?If it was anyone here who reported it... mind telling us why?\n \nreply",
      "Ha, this is great. I hooked up an old photo frame to OpenAI's DALL-E image generator, which is told to make an image based on the current weather data right now. It updates every few hours.This is what it's showing right now: https://ibb.co/8K5jZ3B\n \nreply",
      "See also: https://github.com/blixt/sol-mate-eink (using city images)\n \nreply",
      "This is really lovely!\n \nreply"
    ],
    "link": "https://github.com/lds133/weather_landscape",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Visualizing Weather Forecasts Through Landscape Imagery\n      Visualizing Weather Forecasts Through Landscape ImageryTraditional weather stations often display sensor readings as raw numerical data. Navigating these dashboards can be overwhelming and stressful, as it requires significant effort to locate, interpret, and visualize specific parameters effectively.Viewing a landscape image feels natural to the human eye. The calming effect of observing landscape elements reduces stress and requires minimal effort, allowing for a more relaxed visual experience.The method below demonstrates how to encode weather information within a landscape image, with no or minimal reliance on numerical data.The landscape depicts a small house in the woods. The horizontal axis of the image represents a 24-hour timeline, starting from the current momen"
  },
  {
    "title": "Docker Desktop Alternative (container-desktop.com)",
    "points": 238,
    "submitter": "istoica",
    "submit_time": "2024-09-20T18:08:22.000000Z",
    "num_comments": 110,
    "comments_url": "https://news.ycombinator.com/item?id=41604262",
    "comments": [
      "Kubernetes is planned - my devops wants me to add it badly!Author note - Most of you guys here are power users, for whom UI is a visual poem that you need or not.\nThis is not a commercial project, it is not following any business goals.\nBut this does not mean concessions to quality, it does try to offer minimal resource usage everywhere, easy experience, good UI/UX.It explains all it does behind the scenes if you enable the developer console.\nIt can help one learn so at a certain moment one understands and automates with scripts and specs.But everyone these days is either seen as too smart or too dumb, I don't consider users like this. I know everyone started somewhere and a gradual learning experience is the best.I broke so many radios and toys when I was a kid and I learned so much, by looking at what was is inside.It is a project done by one dude, after work and when it rains outside (In Belgium it rains a lot).\n \nreply",
      "I don't live my life entirely on the command line either, but GUIs for Docker are just an interesting niche to me, for which I just don't understand what the ven diagram is between people that want Docker containers running locally, know that that's what they want, and know how it all works, but then don't want to do the small handful of commands at the prompt needed to get it running...\n \nreply",
      "Well said. I agree completely.\n \nreply",
      "> But everyone these days is either seen as too smart or too dumbVert succinct and poetic way to describe so much these days in this space.\n \nreply",
      "I never finished it, but I had a lot of fun documenting a basic-ass K8S (well, K3S) setup that costs about 20\u20ac/mo on Hetzner.You don't really learn about sysadmin through it, or even about docker that much, but you get an idea of how you might easily run a few different things on a server while only needing to know YAML, and not some custom DSL like chef or puppet.\n \nreply",
      "> only needing to know YAML, and not some custom DSL like chef or puppet.YAML may be a known syntax, but the use of it still requires domain specific knowledge, and is still a domain specific language expressing those domain specific concepts, as to what the expected keys and values are allowed to be and how they are interpreted.\n \nreply",
      "YAML isn\u2019t the DSL, it\u2019s just the language used to express declarative config because the tooling is ubiquitous and it\u2019s rare that anyone uses it as anything more than a nicer version of JSON.For Kubernetes, it\u2019s CRDs that are written in YAML and they conform to a specification.\n \nreply",
      "I did something similar between jobs\u2014built a k8s \"cluster\" on my home Linux box using kops+qemu. It didn't make me an experienced admin, but it was really enlightening and fun! Projects like these are a great way to learn.\n \nreply",
      "[flagged]",
      "Like everything in tech, it's all about tradeoffs and understanding how you want to scale your business. Of the three startups I've been at, 2 of them adopted k8s early on and 1 of which didn't. Of the two that adopted k8s, one I would say k8s was our key differentiator in terms of our GTM motion and how our platform powered the business. $MM customer needs a setup running as close to their current infra's region for just about any reason, yeah sure we can spin that up in a week. This was often the key bit that allowed us to take customers from our key competitors before the competitor even knew we were in play. The 2nd one..... giant waste of money that gave me a decent pay check.The third startup that opted _not_ to adopt k8s is stuck at $100M in revenue and can't land customers fast enough to offset churn. This is entirely because the COO has held the mentality \"k8s bad, amirite?\" and stuck with a patchwork of ansible scripts to manage configuring VM farms that ran our stack. Years of tech debt piled up and every new $MM customer coming in that needs to run in a specific region for $reasons, takes 6 months to setup and cost so much that we'd lose money on the deal. I genuinely believe this startup would be closer to $500M in revenue in the years since I've left had they invested in migrating from containers running in VM's to k8s. But instead they had to lay off 30% of their staff and get another round of funding, and are stagnating.\n \nreply"
    ],
    "link": "https://container-desktop.com/",
    "first_paragraph": "\n        A cross-platform desktop UI made by the podman team itself, is available at Podman Desktop\n\n      A familiar desktop graphical interface for the free and open container manager -\n      Podman!\n    PublicationsPodman in ActionPodman for DevopsContainer Desktop works on Windows, Mac and Linux providing the same graphical interface.Container Desktop is great for skills improvement and learning features of 'podman'.\n        The dashboard offers just the essential features so that the users can feel right at home.\n        \n        See below what you can manage with Container Desktop.\n      Be aware of all available secrets, define new ones or purge old from existence.Manage shared volumes across containers, limit repetition and also be portable.No popups to annoy you, in-depth details only when needed.\n          \u00a9 2024 |\n          Ionut Stoica\n\n          Webpage by\n          Teodora Pletosu\n"
  },
  {
    "title": "Show HN: Put this touch sensor on a robot and learn super precise tasks (any-skin.github.io)",
    "points": 216,
    "submitter": "raunaqmb",
    "submit_time": "2024-09-20T17:11:48.000000Z",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=41603865",
    "comments": [
      "So you embed magnetic particles in silicon rubber and magnetize them, then use magnometers to detect how the magnetic field is changing from a few different points of reference in order to detect the deformation of the rubber and from that you can analyze the \"pressure points\" on the surface. the innovation here is that you dont have a lengthy re-calibration of your \"input signal\" to the particular magnet-infused silicone interface because the manufacturing makes them consistent enough to be replaceable parts?this makes advanced touch sensors more like machine-cut screws than bespoke hand-forged nails.\n \nreply",
      "Great idea, you could do the same with a capacitive xy sensor.\n \nreply",
      "I'll bet you could open the grippers fully and recalibrate on power up.\n \nreply",
      "One thing that I noticed from watching the first ~minute of the video: rather than simply pressing on the skin, it looks like the finger is mostly pulling the skin down over the edge. My intuition is that this is because there isn't all that much deformation from simply pressing, and so this pulling action triggers a stronger response. But I might be overinterpreting from a few seconds of video.\n \nreply",
      "Very nice, and much easier to manufacture than the old Takktile sensors https://biorobotics.harvard.edu/takktile.html - it also looks like you could use the skins to destructive levels of force, without damaging the circuit boards at all, with a stiff enough layer between the chips and the skin (the Takktile system put the epoxy directly in contact with the pressure sensors, so while you could use protective layers over that, it would necessarily reduce the sensitivity.)How tech-independent is the policy learning part? Do the models end up relying on how the board is giving you direction vectors, rather than contact location? (Nothing wrong with that, I'm just wondering if the directional aspect \"factors out\" certain kinds of change, and thus simplifies the learning process.)\n \nreply",
      "I don't know anything about this space, but damn, this looks impressive!Could it be used to sort trash and recycling? Could it recalibrate if gunk got on it, or as it aged? (I guess silicon is probably pretty resistant to aging.) Can it wash and de-stem a tomato?I think I want a trackpad made out of this. How much resolution could it get? I suppose I wouldn't want to sacrifice a lot of resolution for the pressure, tilt, etc. that I am assuming this would provide.(I said \"think\", because I might find out that it feels like running my finger over skin, and I'm wondering how creepy that might feel. I don't really want my laptop to have a fleshy part.)\n \nreply",
      "I've worked in this space in automation with industrial grade robots and more bespoke end effectors that don't look like mainstream robots, but fulfil specific needs. Responding to some of your questions with how I could see the above touch sensor helping:Trash sort and recycling: Not many robots here, majority of sorting takes advantage of object material properties. Some companies tried to add delta robots to keep up with the high rates required to even approach profitability, but they weren't good enough. Maybe some municipalities or universities that have lots of funding could justify adding robots, but it's just hard to financially justify.Recalibration: I'm curious what the developers have for handling reduced magnetic fields over time along with gunk. Silicone is washdown rated, but anything soft at high throughput with parts will start to wear out and change pickup characteristics.Washing and destemming a tomato is more of a problem to solve now that will need another 10+ years of price reductions in robot+end effector costs and increased efficiency before it beats bulk washing and hand-destemming (or crude machine work). Maybe it'll be a grad-student's project for a theoretical future home-botThe Lenovo TrackPoint is likely already 95% of what you'd need from a trackpad, but this touch sensor is likely not even focused at that market.Things I see useful for this robot touch sensor:* Simpler version that detects part presence, is just a Boolean feedback of \"part detected\" which can stick on existing end effectors. This is often handled by load calculations of the robot to detect if it has a part, but could also detect if a part has substantially \"moved\" while it's been gripped, sending a signal to the robot to pause* Harder to suggest items for food as soft grippers (inflatable fingers) will grip at the precise pressure that they're inflated, reducing the need for sensitive feedback. The application for this touch sensor would be food that needs a combination of different pressures to properly secure something, can't think of a great example* Hard to also suggest places where this sensor would help with fine alignment, as major manufacturers have motor and arm feedback with WAY more sensitivity than the average person would realize, google Fanuc \" Touch Sensing\". But, this could help when the end effector is longer and it's harder for the joints to detect position* Fabric manipulation. Fabric is just a hard problem for robots, adding in more information about the \"part\" should be helpful. Unlocking more automations for shoe manufacturing at reasonable prices is a big wall\n \nreply",
      "This is a very insightful summary, thank you! A few things to add about AnySkin that might be relevant:- AnySkin expressly handles wear and gunk by being replaceable. So if it wears out, and you have a heuristic or learned model for the old skin, it will work pretty well on the new skin! We verify this through an analysis of the raw signal consistency across skins, as well as through visuotactile policies learned using behavior cloning. We found swapping skins to work for some pretty precise tasks like inserting USBs and swiping credit cards.- Could definitely be used for part motion detection- Soft, inflatable grippers are effective, but often passive. AnySkin is not just soft, but also offers contact information from the interaction to actively ensure that blueberry doesn't get squished!- This sensor would be key for robots that seek to use learned ML policies in cluttered environments. Robots are very likely to encounter scenarios where they see an object they must interact with, but the object is occluded either by their own end-effector(s) or by other objects. Touch, and an understanding of touch in relation to vision becomes critical to manipulate objects in these settings.- Industrial robots do have very sensitive motor and arm feedback. However, these systems are bulky and unsafe to integrate into household robotic technologies. Sensors like AnySkin could be used as a powerful, lightweight solution in these scenarios, potentially by integrating with some exciting recent household robotics models like Robot Utility Models.- ReSkin, the predecessor to AnySkin, has previously been used quite effectively for fabric manipulation! (see work from David Held's group at CMU). AnySkin is more reliable as well as more consistent and could potentially improve the performance seen in prior work.\n \nreply",
      "> Washing and destemming a tomato is more of a problem to solve now that will need another 10+ years of price reductions in robot+end effector costs and increased efficiency before it beats bulk washing and hand-destemming (or crude machine work). Maybe it'll be a grad-student's project for a theoretical future home-botHeh, fair. I wasn't thinking of this as a practical usage, it was just the first thing to come to mind when imagining a task requiring a lot of pressure sensitivity and a range of forces.Then again, now that I've said it, I believe the current approach to this is to breed really hard, tasteless tomatoes and then agitate them in a vat. Perhaps we can eventually get tastier produce if robots can handle more fragile things!Hm... or you could invert things and make a glove, then use it as a controller. (VR, or just a richer set of control dimensions for eg photo editing or something.) I guess that needs to generalize across hand shapes and sizes, not just swapping out the glove, but I'd be up for a calibration/training phase.> * Harder to suggest items for food as soft grippers (inflatable fingers) will grip at the precise pressure that they're inflated, reducing the need for sensitive feedback. The application for this touch sensor would be food that needs a combination of different pressures to properly secure something, can't think of a great exampleHow do you know the right pressure without feedback? A lot of foods vary in firmness over time and ripeness. Lemons, for example. I guess most don't, as long as you're sticking to a single type of food.\n \nreply",
      "This seems like it would be really useful for electronic musical instruments.  E.g. Linnstrument (https://www.rogerlinndesign.com/linnstrument) which uses a grid of force sensing resistor strips.  Do these sensors interfere with each other if they're sitting side by side?\n \nreply"
    ],
    "link": "https://any-skin.github.io",
    "first_paragraph": "\n            While tactile sensing is widely accepted as an important and useful sensing modality, its use pales in comparison to other sensory modalities like vision and proprioception. AnySkin addresses the critical challenges of versatility, replaceability, and data reusability, which have so far impeded the development of an effective solution.\n          \n            Building on the simplistic design of ReSkin, and decoupling the sensing electronics from the sensing interface, AnySkin simplifies integration making it as straightforward as putting on a phone case and connecting a charger. Furthermore, AnySkin is the first sensor with cross-instance generalizability of learned manipulation policies.\n          \n            This work makes three key contributions: first, we introduce a streamlined fabrication process and a design tool for creating an adhesive-free, durable and easily replaceable magnetic tactile sensor; second, we characterize slip detection and policy learning with a "
  },
  {
    "title": "CuPy: NumPy and SciPy for GPU (github.com/cupy)",
    "points": 275,
    "submitter": "tanelpoder",
    "submit_time": "2024-09-20T13:18:06.000000Z",
    "num_comments": 82,
    "comments_url": "https://news.ycombinator.com/item?id=41601730",
    "comments": [
      "Cupy was first but at this point you're better of using JAX.Has a much larger community, a big push from Google Research, and unlike PFN's Chainer (of which CuPy is the computational base), is not semi-abandoned.Kind of sad to see CuPy/Chainer eco-system die: not only did they pioneer the PyTorch programming model, but also stuck to Numpy API like JAX does (though the AD is layered on top in Chainer IIRC).\n \nreply",
      "I agree. Though it's good to have options for GPU accelerated Numpy. Especially if Google decides to discontinue Jax at some point.\n \nreply",
      "The idea that this is a drop in replacement for numpy (e.g., `import cupy as np`) is quite nice, though I've gotten similar benefit out of using `pytorch` for this purpose. It's a very popular and well-supported library with a syntax that's similar to numpy.However, the AMD-GPU compatibility for CuPy is quite an attractive feature.\n \nreply",
      "Note that NumPy, CuPy and PyTorch are all involved in the definition of a shared subset of their API:https://data-apis.org/array-api/So it's possible to write array API code that consumes arrays from any of those libraries and delegate computation to them without having to explicitly import any of them in your source code.The only limitation for now is that PyTorch (and to some lower extent cupy as well) array API compliance is still incomplete and in practice one needs to go through this compatibility layer (hopefully temporarily):https://data-apis.org/array-api-compat/\n \nreply",
      "It's interesting to see hardware/software/API co-development in practice again.The last time I think this happen at market-scale was early 3d accelerator APIs? Glide/opengl/directx. Which has been a minute! (To a lesser extent CPU vectorization extensions)Curious how much of Nvidia's successful strategy was driven by people who were there during that period.Powerful first mover flywheel: build high performing hardware that allows you to define an API -> people write useful software that targets your API, because you have the highest performance -> GOTO 10 (because now more software is standardized on your API, so you can build even more performant hardware to optimize its operations)\n \nreply",
      "An excellent example of Array API usage can be found in scikit-learn. Estimators written in NumPy are now operable on various backends courtesy of Array API compatible libraries such as CuPy and PyTorch.https://scikit-learn.org/stable/modules/array_api.htmlDisclosure: I'm a CuPy maintainer.\n \nreply",
      "And of course the native Python solution is memoryview. If you need to inter-operate with libraries like numpy but you cannot import numpy, use memoryview. It is specifically for fast low-level access which is why it has more C documentation than Python documentation: https://docs.python.org/3/c-api/memoryview.html\n \nreply",
      "One could also \"import jax.numpy as jnp\". All those libraries have more or less complete implementations of numpy and scipy (i believe CuPy has the most functions, especially when it comes to scipy) functionality.Also: You can just mix match all those functions and tensors thanks to the __cuda_array_interface__.\n \nreply",
      "Jax variables are immutable.Code written for CuPy looks similar to numpy but very different from Jax.\n \nreply",
      "Ah, well, that's interesting! Does anyone know how cupy manages tensor mutability?\n \nreply"
    ],
    "link": "https://github.com/cupy/cupy",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        NumPy & SciPy for GPU\n      \n\n\n\n\nWebsite\n| Install\n| Tutorial\n| Examples\n| Documentation\n| API Reference\n| ForumCuPy is a NumPy/SciPy-compatible array library for GPU-accelerated computing with Python.\nCuPy acts as a drop-in replacement to run existing NumPy/SciPy code on NVIDIA CUDA or AMD ROCm platforms.CuPy also provides access to low-level CUDA features.\nYou can pass ndarray to existing CUDA C/C++ programs via RawKernels, use Streams for performance, or even call CUDA Runtime APIs directly.Binary packages (wheels) are available for Linux and Windows on PyPI.\nChoose the right package for your platform.NoteTo install pre-releases, append --pre -U -f https://pip.cupy.dev/pre (e.g., pip install cupy-cuda11x --pre -U -f https://pip.cupy.dev/pre).Binary packages are also available for Linux and Windows on Conda-Forge.If you need a sli"
  },
  {
    "title": "Linux/4004: booting Linux on Intel 4004 for fun, art, and no profit (dmitry.gr)",
    "points": 412,
    "submitter": "dmitrygr",
    "submit_time": "2024-09-20T11:04:15.000000Z",
    "num_comments": 88,
    "comments_url": "https://news.ycombinator.com/item?id=41600756",
    "comments": [
      "Wow. And I thought modern NetBSD on a 15 MHz m68030 with a 16 bit memory bus and 10 megabytes of RAM is slow. This is crazy!It illustrates a point I've explained to many people over the years: once computers started coming with persistent storage, open address spaces and MMUs towards the late '80s and early '90s, we basically arrived at modern computing. An Amiga 3000 or i80486 computer can run the same things as a modern computer. Sure, we have ways to run things orders of magnitude faster, and sure, we now have things that didn't exist then (like GPUs that can run code), but there's no functional difference between those machines and new ones.I love that Dmitry shows how loosely \"functional\" can be defined :)\n \nreply",
      "15 MHz m68030 with a 16 bit memory bus and 10 megabytes of RAM -- A Mac LC II, by any chance? :)> towards the late '80s and early '90sBy the late 1960s, really.  It would probably be possible to port Linux to the IBM Model 67 [1]; might even be easy since GCC can already target the instruction set.  The MMU is sufficient.  Maybe a tight squeeze with the max of 2 MB of fast core.  Would be in a similar ballpark, a bit slower, to that 68030 machine.Full virtualization, and hardware enforced memory and IO boundaries, were invented early on, too.  It took a while for such features to trickle down to mini- and then micro- computers.  And then much longer for popular software to take advantage.[1] https://en.wikipedia.org/wiki/IBM_System/360_Model_67\n \nreply",
      "Wonderful look in to our history, and you're undoubtedly correct about being able to target that system.My example was about hardware that was affordable to mere mortals (although it's getting to be more expensive to buy that same hardware now as it was to buy it when it was new), but the idea is the same :)\n \nreply",
      "I have fond memories of the System/360 M67 (and its successors, starting with a System/370 M168) I used from 1970 to the early 1980s. It ran the Michigan Terminal System, and we had all the modern conveniences (terminal operation in the IBM world was klunky, but PDP-10s at the same time did that right). And of course Unix dates from exactly that period.The fact that Linux runs well on a modern zSeries demonstrates that, even with all the historical baggage this 60+-year-old architecture has, it carries with it many of the architectural innovations that current OSes and languages need.\n \nreply",
      "That's basically the concept of Turing Completeness. Any Turing complete system can run anything. It may be very slow, but it will run. ChatGPT could run on a 4004, all you need is time.\n \nreply",
      "That's basically the concept of TuringTarpit,where everything is possible but nothing of interest is easy.https://en.m.wikipedia.org/wiki/Turing_tarpit\n \nreply",
      "A computer is technically not a Turing machine due to the lack of infinite RAM. It is a finite state machine with an absurdly large state space.\n \nreply",
      "I've always interpreted the definition of storage as arbitrarily large, not specifically infinite. The universe, after all, is finite. The \"well, acshually\" arguments aren't interesting, because they're 100% abstract.\n \nreply",
      "It is defined as arbitrarily large but not infinite. That's not because of physical concerns, but because some of the theorems don't work if the memory is actually infinite.\n \nreply",
      "When you're talking about something like neural networks on a 4004, the \"well ackshually\" argument does become very much relevant. The limitations of that kind of platform are hard enough that they do not approximate a Turing machine with respect to modern software.\n \nreply"
    ],
    "link": "https://dmitry.gr/?r=05.Projects&proj=35.%20Linux4004",
    "first_paragraph": "I booted Debian Linux on a 4-bit intel microprocessor from 1971 - the first microprocessor in the world - the 4004. It is not fast, but it is a real Linux kernel with a Debian rootfs on a real board whose only CPU is a real intel 4004 from the 1970s. The video is sped up at variable rates to demonstrate this without boring you. The clock and calendar in the video are accurate. A constant-rate video is linked below.\nIn 2012, I ran real Linux on an 8-bit microcontroller (AVR), setting a new world record for lowest-end-machine to ever run Linux. A natural extension of that project was into something faster and more practical, and I did that. Others also did follow-up work based on the original project. Some exciting work also happened based on my LinuxCard followup, my favourite being this gem. Nobody really tackled the actual record for about eleven years. In 2023, there was this advancement. It is still an AVR, so it is not much lower-end, but it does use an AVR with less RAM, so it cou"
  },
  {
    "title": "How do archivists package things? The battle of the boxes (peelarchivesblog.com)",
    "points": 28,
    "submitter": "bookofjoe",
    "submit_time": "2024-09-20T21:45:35.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://peelarchivesblog.com/2024/09/10/how-do-archivists-package-things-the-battle-of-the-boxes/",
    "first_paragraph": "It\u2019s been a while since we posted one of our articles pulling back the curtains on archival work. To make up for that, here\u2019s a special edition of our popular Archives FAQs and Facts series. For the first time, we\u2019ll compare how archivists in two countries do things a little differently to achieve a common goal.We\u2019ll show you how archivists package (or \u201chouse\u201d) the most common types of physical documents for long-term storage. That is, we\u2019ll talk boxes and files. And we\u2019ll benefit from the experience of special contributors from the United Kingdom to compare how archivists in Canada and the UK commonly do their packaging.To make the comparison easier, we\u2019ll even coin some technical terms that may spark discussions among archives fans and followers.This post came about for a few reasons:Why is this post called the \u201cbattle of the boxes\u201d? First, this is a tongue-in-cheek reference to the friendly rivalry between the two common packaging styles we\u2019ll look at. Archivists themselves sometime"
  },
  {
    "title": "Mitosis in the Gray-Scott model: writing shader-based chemical simulations (pierre-couy.dev)",
    "points": 70,
    "submitter": "thunderbong",
    "submit_time": "2024-09-20T17:22:06.000000Z",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=41603955",
    "comments": [
      "Hey! I'm the author of this article and just noticed the huge traffic spike.This was my first time writing a shader. I was just playing around on shadertoy when I realized I could implement this chemical model that has fascinated me for quite some time. I'm really happy with how well my results align with results from other people who have done this before me.While writing the article, I noticed that the same kind of shader implementation has already been done before.Feel free to ask me anything about the implementation or the chemical model\n \nreply",
      "Great work on the article and video!I've played with RD's especially Gray Scott a lot in Gollygang/Ready, and found some fascinating behaviours that I then brought into Houdini for visualisation, here's a couple of examples:https://youtu.be/4dWJ504FULw?si=lWBrhaL2J12o58e1https://youtu.be/Naj_J8aznyk?si=lb0WrkrUaCDg-RzlOne thing that can help them to look nicer (in my opinion) is a reinterpretation of the reagent values as specific colours before display.. even something as simple as: pow(Uucolor,upow) + pow(Vvcolor,vpow) can be nice with well-picked ucolor&vcolor and powers. Other possibilities like hsv-to-rgb on the values can be pretty interesting too!\n \nreply",
      "Ah damn was meant to be asterisks there (ie multiplication of ucolor by U etc) but it got turned into italics!\n \nreply",
      "This reminds me of this other work[1], which is also a fascinating example of emergence from a dynamical system. Instead of a chemical reaction model, it's a model of slime mold growth, as a system of particles and some update rules. They haven't posted new work in a while, but you can still see many video renderings on their twitter[2] using variants of it that are absolutely mesmerizing.[1] \"Physarum\" by Sage Jenson https://cargocollective.com/sagejenson/physarum[2] https://x.com/mxsage\n \nreply",
      "This is really cool, it looks like it's pretty easy to get started with Shadertoy and your guidance.One nitpick: in section 2.1.1 on implementing the reactions, you refer to \"simulating the reaction A + 2B -> C\"; should this have \"4C\"?\n \nreply",
      "I considered trying to do something like this for CFD (specifically to see if I could show how lift is created by airflow over an airfoil). I got kind of stuck on it though and gave up, too far out of my area. Awesome to see others doing things like this though, feels like it\u2019s an area ripe for exploitation!\n \nreply",
      "If you're into reaction-diffusion there's an open source app called 'Ready' which I think is a great way to explore example patterns or tinker with formulas:https://github.com/GollyGang/readySome fun ones from me can be found in Patterns/Experiments/DanWills\n \nreply"
    ],
    "link": "https://pierre-couy.dev/simulations/2024/09/gray-scott-shader.html",
    "first_paragraph": "\n        08 Sep 2024 \n        \n            in\n                \n                    \n                    \n                        \n                            Simulations\n        \n        .\n        \n            Tags :\n                \n                    shader\n                    ,\n                \n                    simulation\n                    ,\n                \n                    emergence\n                    ,\n                \n                    scientific programming\n\nEdition history on GitHub\nThe Gray Scott Model of Reaction Diffusion\nis an interesting instance of emergence.\nBy simulating a small chemical system that involves only a few components and\nreactions, complex and mesmerizing patterns appear.You can interact with the simulation above by clicking on it to drop some green\nand you can reset it by pressing the previous (\u23ee\ufe0f) button.Although the local rules and the underlying math are quite simple, there is some\nheavy computations involved. For each time step in the simu"
  },
  {
    "title": "Gaining access to anyones Arc browser without them even visiting a website (kibty.town)",
    "points": 1109,
    "submitter": "xyzeva",
    "submit_time": "2024-09-19T23:04:42.000000Z",
    "num_comments": 373,
    "comments_url": "https://news.ycombinator.com/item?id=41597250",
    "comments": [
      "I\u2019m Hursh, cofounder and CTO of The Browser Company (the company that makes Arc). Even though no users were affected and we patched it right away, the hypothetical depth of this vulnerability is unacceptable. We\u2019ve written up some technical details and how we\u2019ll improve in the future (including moving off Firebase and setting up a proper bug bounty program) here: https://arc.net/blog/CVE-2024-45489-incident-response.I'm really sorry about this, both the vuln itself and the delayed comms around it, and really appreciate all the feedback here \u2013 everything from disappointment to outrage to encouragement. It holds us accountable to do better, and makes sure we prioritize this moving forward. Thank you so much.\n \nreply",
      "Was the post written for HN users only? I cannot see it on your blog page (https://arc.net/blog). It\u2019s not posted on your twitter either. Your whole handling seems to be responding only if there is enough noise about it.\n \nreply",
      "Hursh, can you please respond to the above commenter? As an early adopter, I find it fairly troubling to see a company that touts transparency hide the blog post and only publicly \"own up to it\" within the confines of a single HN thread.\n \nreply",
      "We\u2019re working on a proper security bulletin site that will have these front and center! This was a bit of a stopgap for now.\n \nreply",
      "Pretty obvious now that Arc will only share security alerts with the people who \"catch\" them at it - as few as possibleLeaves no choice but for this community to make the rest of the Arc community aware of it as they refuse the transparency\n \nreply",
      "Not a good look it not being on the main page! I personally use [zen browser](https://github.com/zen-browser/desktop); I like the ideas of Arc, but it always seemed sketchy to me, especially it being Chromium-based and closed-source.\n \nreply",
      "Heads up: HN doesn't support link naming markdown and some of the extra characters broke the hyperlink.In case the parent can't fix it in time for the edit window: https://github.com/zen-browser/desktop\n \nreply",
      "I wouldn't be surprised if some HN client apps support markdown.\n \nreply",
      "Hi Hursh, I'm Tom. A couple friends use Arc and they like it, so I had considered switching to it myself. Now, I won't, not really because of this vulnerability itself (startups make mistakes), but because you paid a measly $2k bounty for a bug that owns, in a dangerous way, all of your users. I won't use a browser made by a vendor who takes the security of their users this unseriously.By the way, I don't know for sure, but given the severity I suspect on the black market this bug would have gone for a _lot_ more than $2k.\n \nreply",
      "Selling vulnerability on the black market is immoral and may be illegal. The goal of bug bounty programs was initially to signal \"we won't sue white hat researchers who disclose their findings to us\", when did it evolve into \"pay me more than criminals would, or else\"?\n \nreply"
    ],
    "link": "https://kibty.town/blog/arc/",
    "first_paragraph": "and of course, firebase was the cause (CVE-2024-45489)we start at the homepage of arc. where i first landed when i first heard of it. i snatched a download and started analysing, the first thing i realised was that arc requires an account to use, why do they require an account?so i boot up my mitmproxy instance and i sign up, and i see that they are using firebase for authentication, but no other requests, are they really just using firebase only for authentication?after poking around for a bit, i discovered that there was a arc featured called easels, easels are a whiteboard like interface, and you can share them with people, and they can view them on the web. when i clicked the share button however, there was no requests in my mitmproxy instance, so whats happening here?from previous experience hacking an IOS based app, i immediately had a hunch on what this was, firestore.firestore is a database-as-a-backend service that allows for developers to not care about writing a backend, and"
  },
  {
    "title": "Show HN: Inngest 1.0 \u2013 Open-source durable workflows on every platform (inngest.com)",
    "points": 113,
    "submitter": "tonyhb",
    "submit_time": "2024-09-20T17:33:15.000000Z",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=41604042",
    "comments": [
      "I've been looking into this space since I learned about Temporal recently, and I wonder if it would be better to have this integrated in my application, as a library, instead of an external dependency. This way, all the state would be in the same DB.\n \nreply",
      "Looking at the License - This is not Open Source, but rather Source Available software.Looks great but do not appreciate deceptive marketing\n \nreply",
      "And it seems their misnomer is practically everywhere, not just in the Show HN: their website also mislabels their links as \"Open Source\" - I guess trying to capitalize on SEOSSPLv1 for anyone similarly interested https://github.com/inngest/inngest/blob/v1.0.0/LICENSE.mdSeems they had a change of heart around 2022: https://github.com/inngest/inngest/pull/81 but they actually only started lying about the license in this go-around because their previous Show HN <https://news.ycombinator.com/item?id=36403014> not only didn't mislabel things but they even said \"we're gonna open source in the future\" but I guess the future isn't here yet\n \nreply",
      "And the production infra for running isn't even available, just a pared down \"development server\" via SSPL. This is a long way from OSS.\n \nreply",
      "There might be a bit of misunderstanding on what's in that git repo here.  It actually contains the executor, state store, queue, and our production UI, plus the syncing, registration, and logic for functions.Earlier this year we didn't want folks to roll their own production cloud due to queueing migrations.  It would make your life hard.  We're entirely responsible for that right now, as we discouraged self hosting.That's actually coming to a close, and we'll make it easy to spin up prod clusters using this code and eg. MemoryDB, Dragonfly, or what have you.\n \nreply",
      "Well, my experience has been closer to the \"more eyes make for shallow bugs\" school of thought, so opening the source to contributions would actually help that process, not hinder itI've written quite a lot of CI for projects because it's something I believe in and am willing to roll up my sleeves to get done (as a concrete example). I believe strongly that being able to reference the canonical CI build helps contributors since they can see how it's built for different systems and also ensure they don't submit \"works on my machine\" patches\n \nreply",
      "We'll roll out a change that releases source as GPL after 3-4 years next week, actually.  I do appreciate these comments and points.\n \nreply",
      "Why is it necessary to wait? You've already seen the feedback, if you're going to change the license, why promise to do it later?\n \nreply",
      "It's already planned, and rushing out a legal change on a Friday night ahead of plans is less than ideal.\n \nreply",
      "Agree. This is absolutely deceptive. It's too bad how the OSS moniker is being misused these days...\n \nreply"
    ],
    "link": "https://www.inngest.com/",
    "first_paragraph": "Inngest's durable functions replace queues, state management, and scheduling to enable any developer to write reliable step functions faster without touching infrastructure.Trusted by modern software companies at scale worldwide:Powerful SDKsDrop our SDK into your existing codebase to add durable execution via step functions in seconds. No queues, workers, or additional state management required.Flexible enough for all use cases, powerful enough for advanced requirements.Deploy your Inngest functions to your existing platform or infra. Inngest securely invokes your jobs wherever the code runs.Control exactly how your functions are run with built-in flow control. Forget about queues, workers, and customer logic.Essentials for any type of job or workflow creation.Write functions in any language.TypeScriptPythonGoFunctions run on your own infrastructure: serverless, servers, or edge.VercelNetlifyAWSGCPAzureI wanted to find a solution that would let us just write the code, not manage the i"
  },
  {
    "title": "Show HN: Open-source text classification CLI \u2013 train models with no labeled data (github.com/taylorai)",
    "points": 36,
    "submitter": "andersonbcdefg",
    "submit_time": "2024-09-20T19:22:11.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://github.com/taylorai/aiq",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        ai for jq\n      aiq is a no-frills CLI for embeddings and text classification, inspired by the power of jq. It does 4 things:These commands can operate on text and JSONL files, but they can also read from stdin. This means that you can chain them together: for example, you can use a single command to stream a text file in to be labeled, pipe the labeled data through an embedding model, and finally pipe the embedded, labeled training data through classifier training. (See the Quickstart to learn how!)To use, install with pip. This will install dependencies, and the aiq command-line interface.To use aiq label, you'll also need an OpenAI key. The other commands can be used on their own with no API key, as they run locally on your computer. Set the key as an environment variable:For this quickstart, we include an example dataset and lab"
  },
  {
    "title": "MemoRAG \u2013 Enhance RAG with memory-based knowledge discovery for long contexts (github.com/qhjqhj00)",
    "points": 114,
    "submitter": "taikon",
    "submit_time": "2024-09-20T14:41:16.000000Z",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=41602474",
    "comments": [
      "I don't know how this is different from regular rag yet, but that harry potter example sucks. The \"inferior answer\" seems much more accurate to the prompt with much higher information density, and the \"good answer\" just seems like the type of generic slop any old LLM would produce if you asked it to summarize harry potter.Also, the prompt itself is in semi-broken english and it's not clear what exactly is being asked.\n \nreply",
      "I am naive about LLM technology, in particular the relationship between base models, fine-tuning, and RAG. This particular branch of effort seems aimed at something that is of great interest to me (and I'm sure many others) which is to specialize a more general base model to know a particular domain in great detail and so improve it's responses within that domain. In the past, this might have been called an \"expert system\". For example, you might want to train an LLM on your project codebase and documentation such that subsequent code suggestions prioritize the use of internal libraries or code conventions over those represented by the public sources encoded in the base model.I found the Google Colab notebook of MemoRag[1] to be of great use in understanding roughly the scope and workflow of this work. The interesting step is when you submit your domain as text to encode a new thing that requires a GPU, a process they call \"forming memory\"[2]. Perhaps there is some sort of back-and-forth between the base model and your data that results in new weights added to the base model. As I said, I am naive about LLM technology so I'm not sure about the details or the nomenclature. However, if this is even partially correct I'd like to understand how the \"formed memory\" and the base model cohabitate during inference, because this would create memory pressure on the GPU. If the memory required for the base model is M, and the formed memory is N, it's reasonable to assume you'd need M+N memory to use both.1 - https://colab.research.google.com/drive/1fPMXKyi4AwWSBkC7Xr5...2 - https://colab.research.google.com/drive/1fPMXKyi4AwWSBkC7Xr5...\n \nreply",
      "In the past, this might have been called an \"expert system\". \n\nHeh, it comes full circle... After ~50 years of Expert Systems winter, we're training our new AGIs to become more specialized! This is a memorable lesson that binaries must always be deconstructed, at least to some extent -- kinda like the endless dance we're doing between monoliths and microservices as each new generation of tools runs into the problems inherent in each.  I am naive about LLM technology so I'm not sure about the details or the nomenclature\n\nYou've got all the details right though, so that's pretty impressive :). AFAICT from a quick glance at the code (https://github.com/qhjqhj00/MemoRAG/blob/main/memorag/memora...), it is indeed \"fine tuning\" (jargon!) a model on your chosen book, presumably in the most basic/direct sense: asking it reproduce sections of text at random from the book given their surrounding context, and rewarding/penalizing the neural network based on how well it did. The comment mentions GPU memory in the Colab Notebook merely because this process is expensive -- \"fine tuning\" is the same thing as \"training\", just with a nearly-complete starting point. Thus the call to `AutoModelForCausalLM.from_pretrained()`.To answer your question explicitly: the fine-tuning step creates a modified version of the base model as an \"offline\" step, so the memory requirements during inference (aka \"online\" operation) are unaffected. Both in terms of storage and in terms of GPU VRAM. I'm not the dev tho so obv apologies if I'm off base!I would passionately argue that that step is more of a small addition to the overall pipeline than a core necessity, though. Fine-tuning is really good for teaching a model to recreate style, tone, structure, and other linguistic details, but it's not a very feasible way to teach it facts. That's what \"RAG\" is for: making up for this deficiency in fine-tuning.In other words, this repo is basically like that post from a few weeks back that was advocating for \"modular monoliths\" that employ both strategies (monolith vs. microservices) in a deeply collaborative way. And my reaction is the same: I'm not convinced the details of this meshing will be very revolutionary, but the idea itself is deceptively clever!\n \nreply",
      "> AFAICT from a quick glance at the code (https://github.com/qhjqhj00/MemoRAG/blob/main/memorag/memora...), it is indeed \"fine tuning\" (jargon!) a model on your chosen book, presumably in the most basic/direct sense: asking it reproduce sections of text at random from the book given their surrounding context, and rewarding/penalizing the neural network based on how well it did.Maybe your use of quotes is intentional here, but for posterity's sake there is no actual fine-tuning happening using user input in the code you linked, insofar as the weights of the model aren't being touched at all, nor are they modifying anything else that could impact the original weights (like a LoRA adapter). You touch on this, I think (?), in some of your subsequent language but it read as a little confusing to me at first glance. Or maybe I've been too deep in the ML weeds for too many years at this point.The paper details the actual process, but the TL;DR is that the memory module they use, basically a draft model, does go through a pretraining phase using the redpajama dataset, and then an SFT phase with a different objective. This all happens before and irrespective of the inference-time task (i.e. asking questions about a given text). Also, as has been pointed out in other comments, the draft model could really be any model that supports long context and has decent retrieval performance. So the actual training phases here may be non-essential depending on your infra/cost constraints.\n \nreply",
      "Thanks for the corrections! I\u2019m very much not an expert on LLM usage in the real world. But I\u2019m a bit confused:  does go through a pretraining phase using the redpajama dataset, and then an SFT phase with a different objective\n\nIsn\u2019t that equivalent to what I said, since \u201cSFT\u201d seems to stand for \u201csupervised fine-tuning\u201d? That it starts with a pre trained model, and then modifies that model according to your corpus?Perhaps the confusion here is my ambiguity with \u201cmodel\u201d; I now see that there\u2019s really two models-one for generating a draft + clues and one for constructing the final output\u2014and this library only concerns/modifies the former. Maybe?\n \nreply",
      "I should have quoted you more specifically, my apologies. I was responding to the comment that there was some training of the \"model on your chosen book\".There is no fine-tuning done specific to the corpus you own. I noted this in a sibling comment, but both the pretraining and fine-tuning objective uses a generic dataset (redpajama) which \"aims to maximize the generation probability of the next token given the KV cache of the previous memory tokens\" (quote from section 2.2 of the paper).This is why I noted you could really use any long-context model that also has good retrieval performance. They're training their own draft model in lieu of using an existing model, but you could get similar/better outcomes using something like claude sonnet 3.5.\n \nreply",
      "Thanks for taking the time, that makes sense. This is not the first time I've misunderstood something by having opinions about what it should be doing in my opinion, haha. I absolutely agree with your last point, too.\n \nreply",
      "The overview paragraph needs to be expanded quite a bit. The only operative phrase about how this thing works is \"By recalling query-specific clues\". I think people need a bit more knowledge about what this is and how this works, in an overview, to get them interested in trying it. Surely we can be a bit more specific.\n \nreply",
      "It reads like an LLM wrote it. Word salad that waffles on without any substance. In fact I think an LLM wrote most of the README. There are the telltale bullet points with bold starting words for example.\n \nreply",
      "I think lots of modern writing apps allow people to let AI \"reword\" their own content, into better sentence structures, etc. I'm fine with that actually. Doesn't mean the AI invented the content itself.\n \nreply"
    ],
    "link": "https://github.com/qhjqhj00/MemoRAG",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Empowering RAG with a memory-based data interface for all-purpose applications!\n      Empowering RAG with a memory-based data interface for all-purpose applications!\nQuick-Start |\nRoadmap |\nUsage |\n Demo |\nDataset |\n FAQs\nMemoRAG is an innovative RAG framework built on top of a highly efficient, super-long memory model. Unlike standard RAG, which primarily handles queries with explicit information needs, MemoRAG leverages its memory model to achieve a global understanding of the entire database. By recalling query-specific clues from memory, MemoRAG enhances evidence retrieval, resulting in more accurate and contextually rich response generation.\u200b\n\nWe will provide a toy demo to demonstrate MemoRAG, you can try with the following scripts:Afterwards, you can view the demo as below:[13/09/24] MemoRAG adds Meta-Llama-3.1-8B-Instruct and"
  },
  {
    "title": "Open source maintainers underpaid, swamped by security, and going gray (theregister.com)",
    "points": 86,
    "submitter": "rntn",
    "submit_time": "2024-09-18T13:39:44.000000Z",
    "num_comments": 49,
    "comments_url": "https://news.ycombinator.com/item?id=41579591",
    "comments": [
      "Maybe open-source as the load-bearing infrastructure of the world will never be sustainable, and maybe that's okay. I think open-source is best when it's empowering people to modify and remix their software, and have free alternatives to expensive commercial programs. It seems like open-source has become so captured by corporate interests, that people's main motivation for contributing is to add it as a bullet-point on their resume.\n \nreply",
      "Companies will continue to take advantage of free and cheap labor just as long as those people continue to serve.Perhaps after this generation of LBIP moves on, the next wave won\u2019t be so generous.Also, open source doesn\u2019t have to mean free labor. One could be paid a wage to work on open source.\n \nreply",
      "It would probably be more sustainable if the companies that depend on 100s of FOSS OS'/libraries/applications/etc to generate billions of dollars in profit would contribute more significantly.\n \nreply",
      "It's probably too much to ask corporations to dump money into it as it would not be a legitimate business expense.The people I look to and point my finger at are the Bill Gates, Paul Allen, Steve Jobs, Brin, Page, etc's of the world who know how to create personae for themselves of \"caring about the planet, the future, the next generation\" and yet do dick for the field they actually know something about.billg, mosquito nets for villages in Africa? while you buy all the farmland in the country, claiming you're going give it all away in the future, instead of giving it to your daughter who loves riding horses, each one of which could instead pay to educate villages full of childrean? and the industry you monopolized is just being re-monopolized by surveillance monopolists? you're not a good person, you don't fool me.Fund open source, it would support young people who were just like you were, and whose own open source contributions would multiply in the same way your beneficence is not multiplying.\n \nreply",
      "Absolutely, There\u2019s hope in initiatives like https://osspledge.com/ & https://thanks.dev\n \nreply",
      "The beauty of open-source is that it doesn\u2019t matter what people say, or do, or decide.The rate of new contributions may be dependant on these factors, but not for what\u2019s already out there.\n \nreply",
      "The word underpaid seems too friendly. In the case of the open source developers I know, they don't get any money\n \nreply",
      "The conversation about OSS devs getting paid happens amongst the extremely small slice of OSS devs (probably less than 0.0001%) who have managed to get >$10pm out of it.\n \nreply",
      "I\u2019m a young(-ish) dev who used to care a lot about open source but never managed to break into a community. In recent years my view of the whole thing has soured a lot. There seems to be few compelling incentives to actually develop or participate in open-source software.Open source first felt to me like a way to give power back to ordinary people, and it still is, but it seems like those who get the most benefit from free labor are large corporations. Open source feels increasingly corporate and companies like Microsoft dominate and reap enormous benefits. I\u2019ll work for Joe Neighbor for free but not Sataya Nadella.Open source was always political but in 2010 (around when I started getting into Linux) it felt like dumb arguments over things like programming languages or \u201cthe UNIX philosophy\u201d. Now it feels like a vicious Red vs Blue culture war where not picking a side is just as bad as picking the other side.Contributing to open source is a thankless job and even if your project is really good, most people won\u2019t care and the few that do might still treat you like crap. I\u2019ve submitted a handful of pull requests and I\u2019ve already run into the classic \u201cYour patch works and provides a feature some people might like, but I don\u2019t like it, go away.\u201dI\u2019ve donated plenty to organizations like Mozilla, Wikipedia, and GNOME. I then email them with my opinions on what they\u2019re doing. In nearly every case not only am I ignored completely, I see those projects (Mozilla especially) continue to drift in a direction that I disagree with. So, I stopped donating.For me, the Linux kernel is probably one of the few big open source projects where 1) the project is technically interesting enough that I would learn a lot by contributing, 2) It seems like politics and infighting is kept under control, 3) it actually seems possible to get a patch in while having a 9-5, 4) I use the product myself every day and enjoy it, and 5) the technical direction feels positive in that it is getting regular features & bug fixes that I like\n \nreply",
      "> but it seems like those who get the most benefit from free labor are large corporations.One factor is the lack of adoption of copyleft licenses. The proliferation of permissive licenses turned into a backdoor for corporations to privatize volunteer work. We should adopt copyleft whenever possible. Stallman is right on this.\n \nreply"
    ],
    "link": "https://www.theregister.com/2024/09/18/open_source_maintainers_underpaid/",
    "first_paragraph": ""
  },
  {
    "title": "UpCodes (YC S17) Is Hiring a Marketing Leader (up.codes)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-09-20T21:00:13.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://up.codes/careers?utm_source=HN",
    "first_paragraph": ""
  },
  {
    "title": "CISA boss: Makers of insecure software are the real cyber villains (theregister.com)",
    "points": 17,
    "submitter": "tsujamin",
    "submit_time": "2024-09-21T00:05:57.000000Z",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=41606493",
    "comments": [
      "That is rich coming from a former NSA Tailored Access Operations agent. She had no problems paying companies to release insecure software, including some that have signed the \"secure by design\" pledge.\n \nreply",
      "That is important context, but I still agree with what she's said in this article. It's also rich that Cisco especially -- a company known for hard-coding backdoors into their products for decades -- is \"taking a pledge\" to do better.\n \nreply"
    ],
    "link": "https://www.theregister.com/2024/09/20/cisa_sloppy_vendors_cybercrime_villains/",
    "first_paragraph": ""
  },
  {
    "title": "Reactive Relational Algebra (taylor.town)",
    "points": 107,
    "submitter": "surprisetalk",
    "submit_time": "2024-09-20T13:54:41.000000Z",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=41602056",
    "comments": [
      "Check out the paper \"Dedalus: Datalog in Time and Space\". It formalizes a Datalog to include time, specifically to handle async behavior. It explores exactly what you seem to be doing here.\n \nreply",
      "Peter Alvaro gave a great presentation on Dedalus at Strange Loop 2015.https://www.youtube.com/watch?v=R2Aa4PivG0g\n \nreply",
      "Have you read up on differential data flow?\nMight be what you want?\n \nreply",
      "The blog has random dots as a background. Now I see them on every screen.\n \nreply",
      "document.body.insertAdjacentHTML('afterbegin', '<style>body::before {background-image: none;}</style>');\n \nreply",
      "If you enjoy FRP-related stuff, Missionary[0] (in Clojure) is doing some really great work.Most people discover it through Electric Clojure.[1]The Missionary author has some really good talks he's done up on YouTube about it.[0] https://github.com/leonoel/missionary[1] https://github.com/hyperfiddle/electric\n \nreply",
      "Also in Clojure, and reactive, and relational: https://github.com/wotbrew/relic> Functional relational programming for Clojure(Script).\n \nreply",
      "And someone built a spreadsheet with Electric Clojurehttps://github.com/lumberdev/tesserae\n \nreply",
      "What are the YouTube talks? Did you have a specific one in mind?\n \nreply",
      "This is RBAR by any other name.\n \nreply"
    ],
    "link": "https://taylor.town/reactive-relational-algebra",
    "first_paragraph": ""
  },
  {
    "title": "The Algorithm and the Hippocratic Oath (hedgehogreview.com)",
    "points": 13,
    "submitter": "blueridge",
    "submit_time": "2024-09-20T21:10:09.000000Z",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=41605528",
    "comments": [
      "> Several years ago, I was involved in a case that illuminates the difficult position many doctors today find themselves in. The patient was pregnant, close to delivery, and experiencing dangerous declines in her baby\u2019s heart rate. She had been on a blood thinner, which kept me, the anesthesiologist, from placing an epidural in her back. She also had strange airway anatomy, which would make it a struggle to put her to sleep quickly if an emergency cesarean section became necessary. I advised the obstetrician to perform an elective cesarean section now, in advance, while we had good working conditions, and not to wait for an emergency, where time is of the essence, and where the delay needed to induce general anesthesia might seriously injure the baby.I am a doctor and that scenario scares me. This has a very high likelihood of stuff hitting the fan and you need to think about your plan when it does.You want stuff to hit the fan during daytime when everyone is around. In this case, during the day surgery is around, ENT, around, other anesthesiologists all of these can rush in if needed to help you secure an airway. You also have the neonatologists around.If it happens in the middle of the night, the staffing will be much reduced and you won\u2019t have as many resources available.One of the most important things to learn as a doctor is when algorithms and guidelines actually apply to the current situations.\u201cLife is short, the art long, opportunity fleeting, experiment treacherous, judgment difficult\u201d- Hippocrates\n \nreply",
      "Ho b\u00edos brakh\u00fas,\nh\u0113 d\u00e8 t\u00e9khn\u0113 makr\u1e17,\nho d\u00e8 kair\u00f2s ox\u00fas --- O \u03b2\u03af\u03bf\u03c2 \u03b2\u03c1\u03b1\u03c7\u03cd\u03c2,\n\u03b7 \u03b4\u1f72 \u03c4\u03ad\u03c7\u03bd\u03b7 \u03bc\u03b1\u03ba\u03c1\u03ae,\n\u1f78 \u03b4\u1f72 \u03ba\u03b1\u03b9\u03c1\u1f78\u03c2 \u03bf\u03be\u03cd\u03c2\n \nreply",
      "The difficult edge case presented in the opening is that the standard protocol prohibits a decision that sensibly should be made. I feel the obvious fix is to amend the protocol. The time at which an operation occurs should factor into the protocol, we do prefer complex operations to occur when experts are most available. Rather than abandoning the protocol, it should be updated to reflect this. Of course there will still be cases with the protocol doesn\u2019t handle well, but eventually those will be amended as well.\n \nreply",
      "Factoring everything into the protocol would be to convert it into an exercise in discretion, which was what the protocol was designed to counteract.\n \nreply"
    ],
    "link": "https://hedgehogreview.com/web-features/thr/posts/the-algorithm-and-the-hippocratic-oath",
    "first_paragraph": "Ronald W. Dworkin, a physician and political scientist, is the author of Medical Catastrophe: Confessions of an Anesthesiologist. His writing can be found at RonaldWDworkin.com.Several years ago, I was involved in a case that illuminates the difficult position many doctors today find themselves in. The patient was pregnant, close to delivery, and experiencing dangerous declines in her baby\u2019s heart rate. She had been on a blood thinner, which kept me, the anesthesiologist, from placing an epidural in her back. She also had strange airway anatomy, which would make it a struggle to put her to sleep quickly if an emergency cesarean section became necessary. I advised the obstetrician to perform an elective cesarean section now, in advance, while we had good working conditions, and not to wait for an emergency, where time is of the essence, and where the delay needed to induce general anesthesia might seriously injure the baby.The obstetrician grew quiet. She seemed to descend within hersel"
  },
  {
    "title": "A picture of Earth through time (blog.google)",
    "points": 4,
    "submitter": "throwaway888abc",
    "submit_time": "2024-09-16T04:45:53.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://blog.google/products/earth/a-picture-of-earth-through-time/",
    "first_paragraph": "May 09, 2013[[read-time]] min readToday, we're making it possible for you to go back in time and get a stunning historical perspective on the changes to the Earth\u2019s surface over time. Working with the U.S. Geological Survey (USGS), NASA and TIME, we're releasing more than a quarter-century of images of Earth taken from space, compiled for the first time into an interactive time-lapse experience. We believe this is the most comprehensive picture of our changing planet ever made available to the public.Built from millions of satellite images and trillions of pixels, you can explore this global, zoomable time-lapse map as part of TIME's new\u00a0Timelapse\u00a0project. View stunning phenomena such as the sprouting of Dubai\u2019s artificial Palm Islands, the retreat of Alaska\u2019s Columbia Glacier, the deforestation of the Brazilian Amazon and urban growth in Las Vegas from 1984 to 2012:The images were collected as part of an ongoing joint mission between the USGS and NASA called\u00a0Landsat. Their satellites "
  },
  {
    "title": "Critical Exploit in MediaTek Wi-Fi Chipsets: Zero-Click Vulnerability (sonicwall.com)",
    "points": 9,
    "submitter": "pjf",
    "submit_time": "2024-09-20T21:28:49.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://blog.sonicwall.com/en-us/2024/09/critical-exploit-in-mediatek-wi-fi-chipsets-zero-click-vulnerability-cve-2024-20017-threatens-routers-and-smartphones/",
    "first_paragraph": "The SonicWall Capture Labs threat research team became aware of the threat CVE-2024-20017, assessed its impact and developed mitigation measures for the vulnerability. CVE-2024-20017 is a critical zero-click vulnerability with a CVSS 3.0 score of 9.8, impacting MediaTek Wi-Fi chipsets MT7622/MT7915 and RTxxxx SoftAP driver bundles used in products from various manufacturers, including Ubiquiti, Xiaomi and Netgear. The affected versions include MediaTek SDK versions 7.4.0.1 and earlier, as well as OpenWrt 19.07 and 21.02. This translates to a large variety of vulnerable devices, including routers and smartphones. The flaw allows remote code execution without user interaction due to an out-of-bounds write issue. MediaTek has released patches to mitigate the vulnerability and users should update their devices immediately. While this vulnerability was published and patched back in March, only recently did a public PoC become available making exploitation more likely.The vulnerability resid"
  },
  {
    "title": "T\u014d Reo \u2013 A M\u0101ori Spellchecker (xn--treo-l3a.nz)",
    "points": 117,
    "submitter": "firstbabylonian",
    "submit_time": "2024-09-20T12:30:11.000000Z",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=41601347",
    "comments": [
      "A nice synchronicity here, I was only checking M\u0101ori words today because The Guardian's cryptic crossword was set by \"Pangakupu\" (which means, logically enough, \"crossword\"). This crossword setter always includes a hidden M\u0101ori word or phrase in the puzzle.\n \nreply",
      "I see you've posted about Maori stuff a couple of times. I want to congratulate you, this is really, really great. Thank you for working to preserve a language and culture! You're presenting resources that are tough to find, and that's an amazing thing.\n \nreply",
      "I can't type anything in the text area on Firefox. Works in Chrome (macOS).\n \nreply",
      "Also doesn't work in Firefox on Windows but does in Chrome.\n \nreply",
      "https://www.maoridictionary.co.nz/\nThis is the dictionary I use most often\n \nreply",
      "Wow neat! There's a great collection of M\u0101ori-made technology for te reo M\u0101ori. I'm thinking also of Te Hiku Media's work building a M\u0101ori speech recognition system: https://blogs.nvidia.com/blog/te-hiku-media-maori-speech-ai/\n \nreply",
      "I get \"Sorry, something went wrong. If this error persists, contact us.\" every time I type something.\n \nreply",
      "This is very nice and important. We need more tools for small languages.\n \nreply",
      "excellent use of a Punycode domain\n \nreply",
      "I was going to disagree with you, because most kiwis have no idea how to write the special o (myself included), so they\u2019d end up typing toreo.nz instead.Which as it turns out, redirects to xn--treo-l3a.nz anyway.Nice!\n \nreply"
    ],
    "link": "https://xn--treo-l3a.nz/",
    "first_paragraph": "\ud83c\uddf3\ud83c\uddff Contact: email"
  },
  {
    "title": "Training Language Models to Self-Correct via Reinforcement Learning (arxiv.org)",
    "points": 183,
    "submitter": "weirdcat",
    "submit_time": "2024-09-20T09:19:40.000000Z",
    "num_comments": 82,
    "comments_url": "https://news.ycombinator.com/item?id=41600179",
    "comments": [
      "It's a similar approach to OpenAI's o1 model ( it's not cited, but there's no available paper for o1).I don't see any mention of weight release unfortunately.\n \nreply",
      "I think this submission paper is talking about reinforcement learning as part of/after the main training, then the model does inference as normal.They might have done that for O1, but the bigger change is the \"runtime train of thought\" that once the model received the prompt and before giving a definitive answer, it \"thinks\" with words and readjusts at runtime.At least that's my understanding from these two approaches, and if that's true, then it's not similar.AFAIK, OpenAI been doing reinforcement learning since the first version of ChatGPT for all future models, that's why you can leave feedback in the UI in the first place.\n \nreply",
      "OpenAI stated [1] that one of the breakthroughs needed for o1's train of thought to work was reinforcement learning to teach it to recover from faulty reasoning.> Through reinforcement learning, o1 learns to hone its chain of thought and refine the strategies it uses. It learns to recognize and correct its mistakes. It learns to break down tricky steps into simpler ones. It learns to try a different approach when the current one isn\u2019t working.That's incredibly similar to this paper, which is discusses the difficulty in finding a training method that guides the model to learn a self-correcting technique (in which subsequent attempts learn from and improve on previous attempts), instead of just \"collapsing\" into a mode of trying to get the answer right with the very first try.[1]: https://openai.com/index/learning-to-reason-with-llms/\n \nreply",
      "They are indeed similar and OpenAI did indeed use RL at training time in a way that has not been done before, as does this approach. Yes both also involve some additional inference-time generation, but the problem is that (at least as of now) you can't get standard LLMs to actually do well with extra inference-time generation unless you have a training process that uses RL to teach them to do so effectively. I'm working on a blog post to explain more about this aimed at HN-level audiences. Stay tuned!\n \nreply",
      "Both models generate an answer after multiple turns, where each turn has access to the outputs from a previous turn. Both refer to the chain of outputs as a trace.Since OpenAI did not specify what exactly is in their reasoning trace, it's not clear what if any difference there is between the approaches. They could be vastly different, or they could be slight variations of each other. Without details from OpenAI, it's not currently possible to tell.\n \nreply",
      "you are describing the same thing?sorry as a practitioner i\u2019m having trouble understanding what point/distinction you are trying to make\n \nreply",
      "how is it similar?\n \nreply",
      "https://x.com/karpathy/status/1821277264996352246\n \nreply",
      "I found the paper a tad difficult to understand because it spends a lot of time circling around the main thesis instead of directly describing.  So, to the best of my understanding:We want to improve LLM's abilities to give correct answers to hard problems. One theory is that we can do that by training a \"Self Correcting\" behavior into the models where they can take as input a wrong answer and improve it to a better/correct answer.This has been explored previously, trying to train this behavior using various Reinforcement techniques where the reward is based on how good the \"corrected\" answer is.  So far it hasn't worked well, and the trained behavior doesn't generalize well.The thesis of the paper is that this is because when the model is presented with a training example of `Answer 1, Reasoning, Corrected Answer`, and a signal of \"Make Corrected Answer Better\" it actually has _two_ perfectly viable ways to do that.  One is to improve `Reasoning, Corrected Answer`, which would yield a higher reward and is what we want.  The other, just as valid solution, is to simply improve `Answer 1` and have `Corrected Answer` = `Answer 1`.The latter is what existing research has shown happens, and why so far attempts to train the desired behavior has failed.  The models just try to improve their answers, not their correcting behaviors.  This paper's solution is to change the training regimen slightly to encourage the model to use the former approach.  And thus, hopefully, get the model to actually train the desired behavior of correcting previous answers.This is done by doing two stages of training.  In the first stage, the model is forced (by KL divergence loss) to keep its first answers the same, while being rewarded for improving the second answer.  This helps keep the model's distribution of initial answers the same, avoiding the issue later where the model doesn't see as many \"wrong\" answers because wrong answers were trained out of the model.  But it helps initialize the \"self correcting\" behavior into the model.In the second stage the model is free to change the first answer, but they tweak the reward function to give higher rewards for \"flips\" (where answer 1 was bad, but answer 2 was good).  So in this second stage it can use both strategies, improving its first answer or improving its self correcting, but it gets more rewards for the latter behavior.  This seems to be a kind of refinement on the model, to improve things overall, while still keeping the self correcting behavior intact.Anyway, blah blah blah, metrics showing the technique working better and generalizing better.Seems reasonable to me.  I'd be a bit worried about, in Stage 2, the model learning to write _worse_ answers for Answer 1 so it can maximize the reward for flipping answers.  So you'd need some kind of balancing to ensure Answer 1 doesn't get worse.  Not sure if that's in their reward function or not, or if its even a valid concern in practice.\n \nreply",
      "LLMs have no direct recollection of the qualia of their own training. This is at least a major way that I self-correct myself: if I'm about to talk about something I know, I'll try and figure out how/why I know that thing and in so doing, try to gauge whether I actually know that thing, if I'm hallucinating, or if I actually heard it from a less than reliable source etc.I don't think LLMs can self-correct without remembering their own training in some way.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2409.12917",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  }
]