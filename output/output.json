[
  {
    "title": "Void: Open-source Cursor alternative (github.com/voideditor)",
    "points": 557,
    "submitter": "sharjeelsayed",
    "submit_time": "2025-05-08T16:35:34 1746722134",
    "num_comments": 239,
    "comments_url": "https://news.ycombinator.com/item?id=43927926",
    "comments": [
      "Feedback 1: The README really needs more details. What does it do/not do? Don't assume people have used Cursor. If it is a Cursor alternative, does it support all of Cursor's features?As a non-Cursor user who does AI programming, there is nothing there to make me want to try it out.Feedback 2: I feel any new agentic AI tool for programming should have a comparison against Aider[1] which for me is the tool to benchmark against. Can you give a compelling reason to use this over Aider? Don't just say \"VSCode\" - I'm sure there are extensions for VSCode that work with Aider.As an example of the questions I have:- Does it have something like Aider's repomap (or better)?- To what granularity can I limit the context?[1] https://aider.chat/\n \nreply",
      "I've used both Cursor and Aider but I've always wanted something simple that I have full control on, if not just to understand how they work. So I made a minimal coding agent (with edit capability) that is fully functional using only seven tools: read, write, diff, browse, command, ask, and think.I can just disable `ask` tool for example to have it easily go full autonomous on certain tasks.Have a look at https://github.com/aperoc/toolkami to see if it might be useful for you.\n \nreply",
      "Thanks for the feedback. We'll definitely add a feature list. To answer your question, yes - we support Cursor's features (quick edits, agent mode, chat, inline edits, links to files/folders, fast apply, etc) using open source and openly-available models (for example, we haven't trained our own autocomplete model, but you can bring any autocomplete model or \"FIM\" model).We don't have a repomap or codebase summary - right now we're relying on .voidrules and Gather/Agent mode to look around to implement large edits, and we find that works decently well, although we might add something like an auto-summary or Aider's repomap before exiting Beta.Regarding context - you can customize the context window and reserved amount of token space for each model. You can also use \"@ to mention\" to include entire files and folders, limited to the context window length. (you can also customize the model's reasoning ability, think tags to parse, tool use format (gemini/openai/anthropic), FIM support, etc).\n \nreply",
      "An important Cursor feature that no one else seems to have implemented yet is documentation indexing. You give it a base URL and it crawls and generates embeddings for API documentation, guides, tutorials, specifications, RFCs, etc in a very language agnostic way. That plus an agent tool to do fuzzy or full text search on those same docs would also be nice. Referring to those @docs in the context works really well to ground the LLMs and eliminate API hallucinationsBack in 2023 one of the cursor devs mentioned [1] that they first convert the HTML to markdown then do n-gram deduplication to remove nav, headers, and footers. The state of the art for chunking has probably gotten a lot better though.[1] https://forum.cursor.com/t/how-does-docs-crawling-work/264/3\n \nreply",
      "This is a good point.We've stayed away from documentation assuming that it's more of a browser agent task, and I agree with other commenters that this would make a good MCP integration.I wonder if the next round of models trained on tool-use will be good at looking at documentation. That might solve the problem completely, although OSS and offline models will need another solution. We're definitely open to trying things out here, and will likely add a browser-using docs scraper before exiting Beta.\n \nreply",
      "Just use the Context7 MCP ?  Actually I'm assuming Void supports MCP.\n \nreply",
      "can you elaborate on how context7 handles document indexing or web crawling. If i connect to the mcp server, will it be able to crawl websites fed to it?\n \nreply",
      "Agreed - this is one of the better solutions today.\n \nreply",
      "I agree that on the face of it this is extremely useful. I tried using it for multiple libraries and it was a complete failure though, it failed to crawl fairly standard mkdocs and sphynx sites. I guess it's better for the 'built in' ones that they've pre-indexed\n \nreply",
      "I use it mostly to index stuff like Rust docs on docs.rs and rendered mdbooks. The RAG is hit or miss but I haven\u2019t had trouble getting things indexed.\n \nreply"
    ],
    "link": "https://github.com/voideditor/void",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          Void is the open-source Cursor alternative.Use AI agents on your codebase, checkpoint and visualize changes, and bring any model or host locally. Void sends messages directly to providers without retaining your data.This repo contains the full sourcecode for Void. If you're new, welcome!\ud83d\udc4b Discord\ud83d\ude99 Roadmap\ud83d\udcdd Changelog\ud83e\udded WebsiteTo get started working on Void, see HOW_TO_CONTRIBUTE.Feel free to attend a weekly meeting in our Discord channel!We're open to collaborations and suggestions of all types - just reach out.Void is a fork of the vscode repository. For a guide to the codebase, see VOID_CODEBASE_GUIDE.You can always reach us in our Discord server or contact us via email: hello@voideditor.com."
  },
  {
    "title": "Fui: C library for interacting with the framebuffer in a TTY context (github.com/martinfama)",
    "points": 50,
    "submitter": "Bhulapi",
    "submit_time": "2025-05-08T22:05:37 1746741937",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=43931845",
    "comments": [
      "Awesome! Reminds me of the good old days of QuickBasic and SCREEN 13, when you could write very small programs with fullscreen graphics.I still have not figured out how to do fullscreen graphics on my Mac.\n \nreply",
      ">how to do fullscreen graphics on my MacYou can't, you don't have direct access to the framebuffer. Unless by \"fullscreen\" you just mean spanning from end-to-end in which case you can create an opengl or metal view and just set the fullscreen style mask.\n \nreply",
      "> You can't, you don't have direct access to the framebuffer.Why is this the case? What would be the problem with allowing it?\n \nreply",
      "It fits Apple's modus operandi to enforce things UI/UX wise, I assume in this case they don't want end-apps to be able to bypass the compositor (and e.g. prevent alerts from showing on the screen or whatnot).They used to allow it, but they removed the API after 10.6https://developer.apple.com/library/archive/documentation/Gr...\n \nreply",
      "My first experience with programming was with QuickBasic. You just brought back some memories, wish I still had all of those old programs around.\n \nreply",
      "Can someone explain what \u201cthe framebuffer\u201d is? I\u2019m familiar with OpenGL programming where the OS can provide a framebuffer for an application but I\u2019m confused about whether there is a global framebuffer for the entire desktop. Is this a Linux specific concept?\n \nreply",
      "As far as I know, a framebuffer can mean a lot of things depending on hardware and implementation, but it was used to refer to actual memory that would contain pixel values that would eventually be written to the screen. In Linux, this is abstracted by the framebuffer device, which is hardware independent (you can actually have several fbdevices, which if I'm not mistaken end up referring to different monitors usually). What's convenient about the implementation is that these devices still work as normal memory devices, which means you can read/write as you would any other memory. Some more info: https://www.kernel.org/doc/html/latest/fb/framebuffer.html\n \nreply",
      "On Linux and on other operating systems that have reused the Linux DRM drivers, you can run OpenGL applications from a virtual terminal text console. Examples are kmscube [1] and the glmark2 benchmark suite.[1] https://gitlab.freedesktop.org/mesa/kmscube\n \nreply",
      "What does \"in a TTY\" context mean here?  It doesn't mean in a terminal window, right?\n \nreply",
      "Interesting, I guess you could port LVGL to this and get a full GUI?\n \nreply"
    ],
    "link": "https://github.com/martinfama/fui",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        framebuffer user interface\n      fui, standing for framebuffer user interface, is a C library for interacting with the framebuffer in a tty context. It writes directly to the framebuffer device, using a system of layers for drawing. Some stuff that it does:The actual library is in fui, and can be compiled and installed with:The library can then be used by including -Lfui -l:libfui.a in your compiler flags. The library is statically linked, so you don't need to worry about shared libraries.Since both the video and input access needs root privileges, it is recommended to add the user to these groups, to avoid running any compiled program with sudo. You can do this with:You'll need to logout and log back in after this. If that doesn't work, a reboot should do it.There are some (half-baked for now) examples included, in the examples fol"
  },
  {
    "title": "Reservoir Sampling (samwho.dev)",
    "points": 272,
    "submitter": "chrisdemarco",
    "submit_time": "2025-05-08T17:02:10 1746723730",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=43928315",
    "comments": [
      "We lived in a rural area when I was a kid. My dad told me once that his buddy had to measure the ptarmigan[1] population in the mountains each year as part of his job.He did this by hiking a fixed route, and at fixed intervals scare the birds so they would fly and count.The total count was submitted to some office which used it to estimate the population.One year he had to travel abroad when the counting had to be done, so he recruited a friend and explained in detail how to do it.However when the day of the counting arrived his friend forgot, and it was a huge hassle anyway so he just submitted a number he figured was about right, and that was that.Then one day the following year, the local newspaper had a frontpage headline stating \"record increase in ptarmigan population\".The reason it was big news  was that the population estimate was used to set the hunting quotas, something his friend had not considered...[1]: https://en.wikipedia.org/wiki/Rock_ptarmigan\n \nreply",
      "Never trust statistics.I once worked on a reservation system for some pretty big ski resorts.We were running late, working nights, and one of the last things we had to finish was the official statistics reports about number of guest nights etc that gets published by the government.Lets just say that the statistics that year had little to do with reality.\n \nreply",
      "Hello! o/I\u2019m the author of this post. Happy to answer any questions, and love to get feedback.The code for all of my posts can be found at https://github.com/samwho/visualisations and is MIT licensed, so you\u2019re welcome to use it :)\n \nreply",
      "Very nice post!Another interesting direction you can take reservoir sampling is instead of drawing a random number for each item (to see whether it replaces an existing item and which one), you generate a number from a geometric distribution telling you how many items you can safely skip before the next replacement.That's especially interesting, if you can skip many items cheaply.  Eg because you can fast forward on your tape drive (but you don't know up front how long your tape is), or because you send almost your whole system to sleep during skips.For n items to sample from, this system does about O(k * log (n/k)) samples and skips.Conceptually, I prefer the version of reservoir sampling that has you generate a fixed random 'priority' for each card as it arrives, and then you keep the top k items by priority around.  That brings me to another related interesting algorithmic problem: selecting the top k items out of a stream of elements of unknown length in O(n) time and O(k) space.  Naive approaches to reaching O(k) space will give you O(n log k) time, eg if you keep a min heap around.What you can do instead is keep an unordered buffer of capacity up to 2k.  As each item arrives, you add it to the buffer.  When your buffer is full, you prune it to the top k element in O(k) with eg randomised quickselect or via median-of-medians.  You do that O(2k) work every k elements for n elements total, given you the required O(n) = O(n * 2*k / k) runtime.Another related topic is rendezvous hashing: https://en.wikipedia.org/wiki/Rendezvous_hashingTangentially related: https://www.keithschwarz.com/darts-dice-coins/ is a great write-up on the alias method for sampling from a discrete random distribution.\n \nreply",
      "I actually read that post on the alias method just the other day and was blown away. I think I\u2019d like to try making a post on it. Wouldn\u2019t be able to add anything that link hasn\u2019t already said, but I think I can make it more accessible.\n \nreply",
      "I have a few more topics we could cooperate on, if you are interested.https://claude.ai/public/artifacts/62d0d742-3316-421b-9a7b-d... has a 'very static' visualisation of sorting algorithms.  Basically, we have a 2d plane, and we colour a pixel (x, y) black iff the sorting algorithm compares x with y when it runs.  It's a resurrection (with AI) of an older project I was coding up manually at https://github.com/matthiasgoergens/static-sorting-visualisa...I'm also working on making https://cs.stackexchange.com/q/56643/50292 with its answer https://cs.stackexchange.com/a/171695/50292 more accessible.  It's a little algorithmic problem I've been working on: 'simulate' a heap in O(n) time.  I'm also developing a new, really simple implementation of soft heaps.  And on my write-up for the solution to https://github.com/matthiasgoergens/TwoTimePad/blob/master/d...> I actually read that post on the alias method just the other day and was blown away. I think I\u2019d like to try making a post on it. Wouldn\u2019t be able to add anything that link hasn\u2019t already said, but I think I can make it more accessible.If memory serves right, they don't do much about how you can efficiently support changes to your discrete probability distribution.\n \nreply",
      "That's exactly the blog post that clicked when I put my alias method [0] together. Their other writing is delightful as well.[0] https://github.com/hmusgrave/zalias It's nothing special, just an Array-of-Struct-of-Array implementation so that biases and aliases are always in the same cache line.\n \nreply",
      "Does this method compose with itself? E.g. if I implement reservoir sampling in my service and then the log collector service implements reservoir sampling, is the result the same as if only the log collector implemented it?\n \nreply",
      "Yes\n \nreply",
      "I hadn\u2019t considered this, cool to know it works!\n \nreply"
    ],
    "link": "https://samwho.dev/reservoir-sampling/",
    "first_paragraph": "Reservoir sampling is a technique for selecting a fair random sample when you\ndon't know the size of the set you're sampling from. By the end of this essay\nyou will know:In front of you are 10 playing cards and I ask you to pick 3 at random. How do\nyou do it?The first technique that might come to mind from your childhood is to mix them\nall up in the middle. Then you can straighten them out and pick the first 3.\nYou can see this happen below by clicking \"Shuffle.\"Every time you click \"Shuffle,\" the chart below tracks what the first 3 cards\nwere.At first you'll notice some cards are selected more than others, but if you\nkeep going it will even out. All cards have an equal chance of being selected.\nThis makes it \"fair.\"Click \"Shuffle 100 times\" until the chart evens out.  You can reset the chart if\nyou'd like to start over.This method works fine with 10 cards, but what if you had 1 million cards?\nMixing those up won't be easy. Instead, we could use a random number generator\nto pick 3 indi"
  },
  {
    "title": "From: Steve Jobs. \"Great idea, thank you.\" (hayman.net)",
    "points": 662,
    "submitter": "mattl",
    "submit_time": "2025-05-08T18:40:12 1746729612",
    "num_comments": 184,
    "comments_url": "https://news.ycombinator.com/item?id=43929724",
    "comments": [
      "i love this.   A startup I was at during early COVID times got acquired into Hewlett Packard Enterprise, so we all became HPE employees with HPE addresses.   There was a similar form there to request \"ryancnelson\"@hpe, etc...One of my co-workers got cute and asked for \"root@hpe.com\" .... And boy, there's a lot of cron jobs running at HP.\n \nreply",
      "They must have learned from your experience.  When we were acquired by HPE they did not let us choose and our director of engineering got an email address that misspelled his name... fixing it involved him being locked out of all systems while the people trying to fix it emailed someone else with a similar name about it.  His advice for other team members in the same spot was \"if you don't like your email address, do not attempt to fix it.\"HPE was truly a trip.  I paid $2000 to be able to disparage them online and it was worth every penny.\n \nreply",
      "I\u2019d do that every time I get a chance! Ex-HPE black label on my resume from a startup I used to work in that they bought. That company is a complete horror show.\n \nreply",
      "It was weird.  It makes me sad because the startup I worked at was really gelling despite the HPE interference.  Then they just laid everyone off one day (multiple senior leadership changes later) for no apparent reason.All the code is Apache 2 so I guess if I really cared I could just revive it... and as it turns out, I don't care that much.  Other stuff to do.\n \nreply",
      "What were the details of paying $2000?\n \nreply",
      "Not the commenter, but I would assume forgoing an exit bonus/severance payment that was contingent upon signing a non-disparagement agreement.\n \nreply",
      "Yup exactly.  I got my retention bonus and 2 months pay and all that stuff without agreeing to anything, and they offered a little bit more to agree not to disparage them.  I'm pretty chatty so decided it wasn't worth it ;)\n \nreply",
      "2000 is a pretty low amount. presumably theyd have to spend way more than that to enforce it, so they would NOT spend it, in which case its free money that you shouldnt have turned down because it was way too small for a gag order\n \nreply",
      "Question is, how many zeros would it take to convince you otherwise.\n \nreply",
      "Some number, absolutely.  I don't have that much integrity ;)\n \nreply"
    ],
    "link": "https://blog.hayman.net/2025/05/06/from-steve-jobs-great-idea.html",
    "first_paragraph": "Hey, I'm Steve, I'm the former Musical Dictator of Argonotes, the Until Recently Toronto Argonauts Band.  Freshly retired from that fruit company after 32 years.  Still figuring out micro.blog, but I like it so far.Now that I\u2019ve been retired for a couple of days, I think I can finally tell this story of how I was \u2013 very briefly \u2013 steve@next.comAnd Steve Jobs sent me an email saying \u201cGreat idea, thank you.\"Wait, what? What was the great idea?In October of 1991, I was a new Systems Engineer at NeXT.  NeXT, of course, was the company Steve Jobs had founded after leaving Apple in 1985, and which eventually merged back into Apple in 1996.  I was one of three employees in Canada, and I think NeXT had about 400 people total.Mail on the NeXT Computer was pretty amazing in 1991.  Multimedia!  Fonts!  Attachments!  Sounds!  It\u2019s hard to overstate how cool that was compared to the command line plain-text email everybody was used to.\nEvery NeXT user got this email from Steve when they started up t"
  },
  {
    "title": "Progress toward fusion energy gain as measured against the Lawson criteria (fusionenergybase.com)",
    "points": 154,
    "submitter": "sam",
    "submit_time": "2025-05-08T15:49:37 1746719377",
    "num_comments": 65,
    "comments_url": "https://news.ycombinator.com/item?id=43927337",
    "comments": [
      "This is a great update! I hope the authors continue publishing new versions of their plots as the community builds up towards facility gain. It's hard to keep track of all the experiments going on around the world, and normalizing all the results into the same plot space (even wrt. just triple product / Lawson criteria) is actually tricky for various reasons and takes dedicated time.Somewhat relevant, folks here might also be interested in a whitepaper we recently put up on arXiv that describes what we are doing at Pacific Fusion: https://arxiv.org/abs/2504.10680Section 1 in particular gives some extra high-level context that might be useful to have while reading Sam and Scott's update, and the rest of the paper should also be a good introduction to the various subsystems that make up a high-yield fusion demonstration system (albeit focused on pulser-driven inertial fusion).\n \nreply",
      "I heard that NIF was never intended to be a power plant, not even a prototype of one. It's primarily a nuclear weapon research program. For a power plant you would need much more efficient lasers, you would need a much larger gain in the capsules, you would need lasers that can do many shots per second, some automated reloading system for the capsules, and you would need a heat to electricity conversion system around the fusion spot (which will have an efficiency of ~1/3 or so).Any truth to that?\n \nreply",
      "It's an experimental facility. Yes, a power plant would need much more efficient lasers, but NIF's lasers date back to the 1990s, equivalent modern lasers are about 40X more efficient, and for an experiment it's easy enough to do a multiplication to see what the net result would have been with modern lasers.Modern lasers can also repeat shots much more quickly. Power gain on the capsules appears to scale faster than linear with the input power, so getting to practical gain might not be as far off as it appears at first glance.These are some of the reasons that various fusion startups are pursuing laser fusion for power plants.\n \nreply",
      "I was trying to work out a joke about buying better lasers off of alibaba but it seems that despite being 30 years old they're still orders of magnitude beyond off the shelf options.\n \nreply",
      "partially. The very efficient lasers from alibaba don't have short pulse/high power, so they can potentially be used only as the part of the system - the pumping lasers. The final nanosecond-laser is still a one-off build which though seems to be pretty doable even by a small company if they set their mind to it.Btw, NIF achieved those recent results by adding strong magnetic field around the target (penny-shrinkers knew that tech for 20+ years :).  There are other things like this  around that can potentially be similarly useful. Only if somebody had money and interest ...\n \nreply",
      "There is no need to ask for speculation.  It's the top item in their mission statement.https://lasers.llnl.gov/about/what-is-nif>NIF is a key element of the National Nuclear Security Administration\u2019s science-based Stockpile Stewardship Program to maintain the reliability, security, and safety of the U.S. nuclear deterrent without full-scale testing.\n \nreply",
      "Nothing about the NIF looks like a power plant to me.  It's like the laser weapons guy and the nuclear weapons guy found a way to spend giant piles of money without having to acknowledge the weapons angle.\n \nreply",
      "A lot of people think so, but the US government openly spends way more money on nuclear weapons than on fusion research. We'll spend almost a trillion dollars on nuclear weapons over the next decade.[1] The government's fusion funding was only $1.4 billion for 2023.[2]So it seems more likely to me that some physicists figured out how to get their fusion power research funded under the guise of weapons research, since that's where the money is. NIF's original intent was mostly weapons research but it's turned out to be really useful for both, and these days, various companies are attempting to commercialize the technology for power plants.[3][1] https://theaviationist.com/2025/04/26/us-nuclear-weapons-wil...[2] https://www.fusionindustryassociation.org/congress-provides-...[3] NYTimes: https://archive.is/BCsf5\n \nreply",
      "The primary purpose of the NIF is to maintain the US nuclear stockpile without nuclear tests. The lasers very inefficient (iirc about 2%). The success they claimed is that the energy released by the burning plasma exceeds the laser energy put into the fuel capsule. Since NIF was never intended to be a power plant they don't use the most efficient lasers.\n \nreply",
      "Yes, after the test ban treaties, there was a huge push into exploring mathematical emulations of all aspects of fusion, and all assorted bombs, as well as laser ignition of pellets with these large lasers using inertial confinement of the pellet as the laser impacted it - analysing the fusion by observation of emitted neutrons. xrays etc. They issued reports from time to time(sanitised), and probably used the secret data to fine tune emulated weapons with fact points. The pellets were composed of potential fuels, various Hydrogens and Lithiums, varied in composition to explore the ignition space. A number of pellets performed well in terms of gain, but were far-far from useable fusion when the  LL labs costs were factored in. I think they determined it could not ever work as a fusion energy source, but it provided data. They still mine data from it with various elemental mixes making up the pellets.\n \nreply"
    ],
    "link": "https://www.fusionenergybase.com/articles/continuing-progress-toward-fusion-energy-breakeven-and-gain-as-measured-against-the-lawson-criteria",
    "first_paragraph": ""
  },
  {
    "title": "Notes on rolling out Cursor and Claude Code (ghiculescu.substack.com)",
    "points": 152,
    "submitter": "jermaustin1",
    "submit_time": "2025-05-08T16:34:39 1746722079",
    "num_comments": 67,
    "comments_url": "https://news.ycombinator.com/item?id=43927914",
    "comments": [
      "Changing my old coding behavior aside, biggest limiting factor for me is understanding how and why the coding agent is doing this a certain way, so that I have the confidence to continually sharpen my tools.I want something simple that I have full control on, if not just to understand how they work. So I made a minimal coding agent (with edit capability) that is fully functional using only seven tools: read, write, diff, browse, command, ask, and think.As an example, I can just disable `ask` tool to have it easily go full autonomous on certain tasks. Or, ask it to `think` for refactoring.Have a look at https://github.com/aperoc/toolkami to see if it might be useful for you.\n \nreply",
      "> So far the biggest limiting factor is remembering to use it. Even people I consider power users (based on their Claude token usage) agree with the sentiment that sometimes you just forget to ask Claude to do a task for you, and end up doing it manually. Sometimes you only notice that Claude could have done it, once you are finished. This happens to me an embarrassing amount.Yea, this happens to me too. Does it say something about the tool?It's not like we are talking about luddites who refuse to adopt the technology, but rather a group who is very open to use it. And yet sometimes, we \"forget\".I very rarely regret forgetting. I feel a combination of (a) it's good practice, I don't want my skills to wither and (b) I don't think the AI would've been that much faster, considering the cost of thinking the prompt and that I was probably in flow.\n \nreply",
      "If you're forgetting to use the tool, is the tool really providing benefit in that case? I mean, if a tool truly made something easier or faster that was onerous to accomplish, you should be much less likely to forget there's a better way ...\n \nreply",
      "> is the tool really providing benefit in that case?Yes, much of the time and esp. for tests. I've been writing code for 35 years. It takes a while to break old habits!\n \nreply",
      "Yep! Most tools are there to handle the painful aspects of your tasks. It's not like you are consciously thinking about them, but just the fact on doing them without the tool will get a groan out of you.A lot of current AI tools are toys. Fun to play around, but as soon as you have some real world tasks, you just do it your usual way that get the job done.\n \nreply",
      "Our meat blobs forget things all the time. It's why the Todo apps and reminders even exist. Not using something every time doesn't mean it's not beneficial.\n \nreply",
      "There's a balance to be calculated each time you're presented with the option. It's difficult to predict how much iteration the agent is going to require, how frustrating it might end up being, all the while you lose grip on the code being your own and your head-model of it, vs just going in and doing it and knowing exactly what's going on and simply asking it questions if any unknowns arise. Sometimes it's easier to just not even make the decision, so you disregard firing up the agent in a blink.\n \nreply",
      "You never forgot your reusable grocery bag, umbrella, or sun glasses? You've never reassembled something and found a few \"extra\" screws?\n \nreply",
      "I resonate with your case (b). One reason why I intentionally don't use it is cases where I know exactly what code I want to write, and can thus write it quicker than I can explain it to Claude + have Claude write it.\n \nreply",
      "Many CLI tools that I love using now took some deliberate practice to establish a habit of using them.\n \nreply"
    ],
    "link": "https://ghiculescu.substack.com/p/nobody-codes-here-anymore",
    "first_paragraph": ""
  },
  {
    "title": "Podfox: First Container-Aware Browser (packett.cool)",
    "points": 21,
    "submitter": "pierremenard",
    "submit_time": "2025-05-08T22:19:12 1746742752",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://val.packett.cool/blog/podfox/",
    "first_paragraph": "A port conflict pushed me to abolish container port forwarding once and for all, making my Firefox talk to Podman's whole network. Also: containerizing dev environments for command-line addicts.\n\nMay 5th, 2025\n\t\t\u2726 11 min read\n\tContainers. Containers containers containers. Even if you were reluctant before, it\u2019s likely that you use them at least for running various supporting infrastructure when working on projects that involve that kind of thing. Having a whole-system install of Postgres on a laptop shared between various different projects never actually felt right. Running podman run --rm -it -p 5432:5432 postgres:17 to pop up a temporary instance with no persistent state does.These days, it\u2019s very typical for any web backend or other kind of networked service project to include something like a docker-compose.yml file that makes it possible to just run all the service dependencies with one single command. And so I was running a Compose setup for one project, a serious business clien"
  },
  {
    "title": "Phoenician culture spread mainly through cultural exchange (mpg.de)",
    "points": 42,
    "submitter": "gmays",
    "submit_time": "2025-05-05T21:06:46 1746479206",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=43899456",
    "comments": [
      "> The researchers even found a pair of close relatives (ca. second cousins) bridging the Mediterranean, one buried in a North African Punic site and one in Sicily.This is from over 2500 years ago. How amazing is that, that we have this capacity in DNA analysis now to discover details like this from so long ago?\n \nreply",
      "In the 1700s a ring was found in England, inscribed Silvianus with the name Senicianus scratched into it. In the 1800s a curse tablet was found 80 miles away, complaining that Senicianus stole the ring of Silvianus.\n \nreply",
      "Ah, my precious\u2026\n \nreply"
    ],
    "link": "https://www.mpg.de/24574685/0422-evan-phoenician-culture-spread-mainly-through-cultural-exchange-150495-x",
    "first_paragraph": "Study challenges long-held assumptions about the Mediterranean Phoenician-Punic civilization, one of the most influential maritime cultures in historyPunic Necropolis of Puig des Molins on the island of Ibiza. The new ancient DNA study sequenced human remains from this and other important Phoenician-Punic Archaeological sites.\n          \u00a9 Photo Raymar, MAEF\n        The Phoenician culture emerged in the Bronze Age city-states of the Levant, developing prominent innovations such as the first alphabet (from which many present-day writing systems derive). By the early first millennium BCE, Phoenician cities had established a vast maritime network of trading posts as far as Iberia, spreading their culture, religion, and language throughout the central and western Mediterranean.By the 6th century BCE, Carthage, a Phoenician coastal colony in what is now Tunisia, had risen to dominate this region. These culturally Phoenician communities associated with or ruled by Carthage became known as \u201cPu"
  },
  {
    "title": "When Abandoned Mines Collapse (practical.engineering)",
    "points": 127,
    "submitter": "impish9208",
    "submit_time": "2025-05-06T18:38:14 1746556694",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=43908261",
    "comments": [
      "A great companion piece on a government bureaucrat who solved the problem on how to optimally support the roofs in longwall mines: https://www.washingtonpost.com/opinions/interactive/2024/mic...\n \nreply",
      "That was fascinating. And contained some interesting stories relating to the function profit = f(lives_lost)\n \nreply",
      "The number one cause of mine collapses, in my personal experience, is creepers.\n \nreply",
      "Actually, it is people who come in close contact with creepers.\n \nreply",
      "I was surprised that insurance companies wouldn't cover damage from mine subsidence. I guess the lesson is to never buy property if you can't get insurance to cover something (wildfire, flood, hurricane, etc) at a reasonable rate since you're all but certain to encounter it eventually and be left on the hook for high costs.\n \nreply",
      "You should read your insurance contract carefully so you aren\u2019t hit by any nasty surprises.Earth movement in general - from landslides to sinkholes to shifting foundations - is excluded from most home insurance policies.\n \nreply",
      "This may depend on jurisdiction. UK buildings insurance usually covers subsidence (although it may then be difficult to sell the property, as any buyer will have difficulty getting their own insurance even after a repair)I don't know specifically about mines or sinkholes, but I don't think they are generally excluded. However one difficulty would be that buildings are generally insured up to the value of the rebuilding cost, IE what it would take to put the building back after it was completely destroyed. But in the case of a mine or sinkhole, the land itself may also be unusable. In an expensive city the rebuild cost may be only a fraction of the cost of buying an equivalent home, including the land it sits on.\n \nreply",
      "It's why programs like FEMA are so important.The issue with private insurance when it comes to natural disasters is they don't like losing money (understandable) and the climate is changing.Those two things together mean that this year you could have good insurance that covers freak accidents, but what about next year, or next decade?  An area that may have only seen flooding once a century might be predicted to see it once a decade or even once a year.People still live there.  Some people lived there with the insurance coverage for those natural disasters only to see it slowly go away or to be outright cancelled.  We can't expect that they all migrate.\n \nreply",
      "I feel strongly that we should save every human life it's possible to save during disasters. Fema is pretty great at that.However, that doesn't neccesarily imply that there should be flows of money available to rebuild in vulnerable locations. Insurance becoming unavailable or unaffordable is probably the best signal available that someplace is a bad place to live. If you can't afford the price or the risk ... There are lots of other places in the world.\n \nreply",
      "This assumes you are moving into an area fresh.  But what about someone that's been there, potentially for generations?It's one thing to say \"don't buy beach front property in the Florida everglades\" but what do you do with the millions who already own such property?This came up with hurricane Katrina and Louisiana.  Multigenerational communities were completely obliterated. I really don't find \"the market said you should move\" to be a compelling response.\n \nreply"
    ],
    "link": "https://practical.engineering/blog/2025/5/6/when-abandoned-mines-collapse",
    "first_paragraph": "[Note that this article is a transcript of the video embedded above.]In December of 2024, a huge sinkhole opened up on I-80 near Wharton, New Jersey, creating massive traffic delays as crews worked to figure out what happened and get it fixed. Since then, it happened again in February 2025 and then again in March. Each time, the highway had to be shut down, creating a nightmare for commuters who had to find alternate routes. And it\u2019s a nightmare for the DOT, too, trying to make sure this highway is safe to drive on despite it literally collapsing into the earth. From what we know so far, this is not a natural phenomenon, but one that\u2019s human-made. It looks like all these issues were set in motion more than a century ago when the area had numerous underground iron mines. This is a really complex issue that causes problems around the world, and I built a little model mine in my garage to show you why it\u2019s such a big deal. I\u2019m Grady and this is Practical Engineering.We\u2019ve been extracting "
  },
  {
    "title": "Show HN: Using eBPF to see through encryption without a proxy (github.com/qpoint-io)",
    "points": 197,
    "submitter": "tylerflint",
    "submit_time": "2025-05-08T16:49:11 1746722951",
    "num_comments": 60,
    "comments_url": "https://news.ycombinator.com/item?id=43928118",
    "comments": [
      "Does this work for Go binaries? My understanding is that Go programs do all the encryption \"in the process\" so the data is encrypted before eBPF can intercept it. I'd love to be wrong about that!\n \nreply",
      "We have Go support, but it is not open sourced yet. Go is a bit more complicated but we were able to get it after some cave diving in the ELF formats. To give you a little insight on how this works, because Go is statically linked, we need to pull several different offsets of the functions we are going to hook into.We do this by scanning every version of Go that is released to find offsets in the standard library that won't change. Then when we detect a new Go process, we use an ELF scanner to find some function offsets and hook into those with uprobes. Using both of these, we have all the information we need to see Go pre-encryption content as well as attribute it to connections and processes.\n \nreply",
      "Great approach. I love the choice of practicality over generalization.Are these offsets consistent across compilation targets, and they vary only by version of the Go binary? Or do you need to do this scan for every architecture?\n \nreply",
      "The short answer is that we only have to calculate the offset per go version, no expensive runtime scanning is required.The long answer is that the offsets are the byte alignment offsets for the go structs containing the pointers to the file descriptor and buffers. Fortunately we only have to calculate these for each version where the TLS structs within go actually change, so not even for every version. For instance, if a field is added, removed, or changes type then the location in memory where those pointers will be found changes. We can then calculate the actual offset at runtime where we know which architecture (amd64, arm64, etc) with a simple calculation. Within the eBPF probe, when the function is called, it uses pointer arithmetic to extract the location of the file descriptor and buffer directly.\n \nreply",
      "Ok, that's exciting, and thanks for the insight!\n \nreply",
      "Most programs do encryption without syscalls! eBPF can intercept userspace execution, which they do as mentioned in the post:> The key idea is to hook into common TLS libraries (like OpenSSL) before encryption and after decryption\n \nreply",
      "I saw that, but Go doesn't use dynamically linked libraries for encryption, so I don't think it helps in this particular case.\n \nreply",
      "If I want to do something similar, do you know where the relevant parts of the eBPF docs are?\n \nreply",
      "Qtap scans binaries of processes as well known locations for OpenSSL on startup, then passes the offsets to eBPF where it hooks into the SSL_read and SSL_write to get the content before or after it's been encrypted.This is the eBPF side:\nhttps://github.com/qpoint-io/qtap/blob/main/bpf/tap/openssl....The Go side which indicates what we are scanning for is here:\nhttps://github.com/qpoint-io/qtap/blob/main/pkg/ebpf/tls/ope...For more docs on the topic:\n- https://docs.ebpf.io/ is a must read\n- https://eunomia.dev/en/tutorials/30-sslsniff/ has a tutorial on cracking OpenSSL open and getting the content as well. The tutorials they have are fantastic in general\n \nreply",
      "There's a similiar tool https://github.com/gojue/ecapture\n \nreply"
    ],
    "link": "https://github.com/qpoint-io/qtap",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Qtap: An eBPF agent that captures pre-encrypted network traffic, providing rich context about egress connections and their originating processes.\n      An eBPF agent that captures traffic flowing through the Linux kernel. By attaching to TLS/SSL functions, data is intercepted before and after encryption and then passed to flexible plugins with full visibility along with all of the available context - process/container/host/user/protocol/etc. Qtap makes it possible to understand what's happening with your egress traffic, without modifying apps, installing proxies, or managing certs.Qtap shows you exactly what data is being sent and received in its original, unencrypted form while operating out-of-band with minimal overhead, without adding latency or disrupting application performance.Qtap can augment your existing observability pipli"
  },
  {
    "title": "Stability by Design (potetm.com)",
    "points": 64,
    "submitter": "potetm",
    "submit_time": "2025-05-08T19:51:29 1746733889",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=43930584",
    "comments": [
      "I'm currently struggling with instability in the Rust 3D graphics stack.All this stuff has been around for about five years now, and was mostly working five years ago. \nThe APIs should have settled down long ago. Despite this, there are frequent \"refactorings\" which cause breaking changes to APIs. (I'm tempted to term this \"refuckering\".)Some of this API churn is just renaming types or enum values for consistency, or adding new parameters to functions. Some changes are major, such as turning an event loop inside out. Code using the API must be fixed to compensate.Because of all the breaking changes, the related crates (Wgpu, the interface to Vulkan, etc., Winit, the interface to the operating system's window manager, and Egui, which handles 2D dialog boxes and menus) must advance in lockstep. There's not much coordination between the various development groups on this. Wgpu and Winit both think they're in charge. and others should adapt to them. Egui tries to cope. Users of the stack suffer in silence.When there's a bug, there's no going back to an older version. The refuckering prevents that. Changes due to API breaks are embedded in code that uses these APIs.I'm currently chasing what ought to be a simple bug in egui, and I've been stuck for over a month.\nThe unit tests won't run for some target platforms that used to work, and bug reports are ignored while new\nfeatures are being added. (Users keep demanding more features in Egui, and Egui is growing towards web browser layout complexity.)Most users are giving up. In the last year, three 3D rendering libraries and two major 3D game projects have been abandoned. There's are about two first-rate 3D games in Rust, Tiny Glade and Hydrofoil Generation, and both avoid this graphics stack.The \"Stability by Design\" article is helpful in that it makes it clear what's gone wrong in Rust 3D land.\n \nreply",
      "That sounds like a complete tirefire tbh. The exact thing that I'm hoping to convince people to stop doing.I'm glad the article was helpful though!\n \nreply",
      "All the players think they're doing the right thing. \nEach group is doing a reasonably good job based on their own criteria.\nBut their collective actions create a mess.\n \nreply",
      "As a recent Clojure convert of 3 years or so, I love reading how amazing Clojure is. As a solo Dec there is simply no alternative. It\u2019s so nice to return to a project 2 or 3 years ago and everything is still running and humming along smoothly as it did when I started the project.I previously worked in PHP, Perl-cgi, Java, and Python- webtools mostly based on MySQL and other SQL database flavours.I worked in a Clojure only shop for a while and they taught me the ways after that you don\u2019t go back. Everything can quickly click into place, it\u2019s daunting to start the learning curve is very unsteep, takes long to get anywhere, but as a curiosity it was fun, then I started to hate how everything else was done now I\u2019m sold my soul to the Clojure devil.\n \nreply",
      "The premise feel weird to me, I read the graphs much more as evidence of how scared the devs are to make changes rather than how \"stable\" the libraries are.You add the code, and rather than change it if needed, you just leave it there and add more code.You could argue too that Scala is much safer so changes to the code are not scary and it's easier to be stable even under code changes.\n \nreply",
      "Actually, you can't neither read that or the opposite from the graphs: it doesn't if the new code is for new functionalities or if it's to replace (without deleting) some old code.But you're right: that would be a particularily useful information\n \nreply",
      "I think code retention charts will look similar for any major library in any language. Projects accrete code.You could instead consider:* How many major version releases / rewrites happen in this language? (This might be a sign of ecosystem instability.)* How much new code is replacing old code? (This might imply the language needs more bugfixes.)\n \nreply",
      "Not sure what you mean. The Scala example looks nothing like the Clojure examples.The retention charts show you how much new code is replacing old code, and you can see the releases/rewrites as the code gets replaced.\n \nreply",
      "TLDR.The outcome is the same, statically typed or dynamically. In both cases one need to perform refactoring in case of breaking changes.\n \nreply",
      "> The outcome is the same, statically typed or dynamically. In both cases one need to perform refactoring in case of breaking changes.No. In statically typed languages, failures are usually caught in CI.\nIn dynamically typed languages, they end up in production - https://github.com/pypa/setuptools/issues/4519\n \nreply"
    ],
    "link": "https://potetm.com/devtalk/stability-by-design.html",
    "first_paragraph": "2025-05-08I recently came across the following tweet from OneHappyFellow1: I think I figured out what\u2019s stressing me out about programming in dynamically typed languages: It\u2019s about always being unsure if using a library in a particular way is going to work and if minor version upgrades aren\u2019t going to break your code. I found this tweet interesting because the language I use the most\u2014Clojure\u2014is both dynamic and yet the ecosystem has a very strong reputation for stability. Before diving into why exactly this is the case, allow me to present some evidence to justify this belief.I searched the Clojurians Slack for the word \"stability,\" and out of 20 total posts on the first page, 8 are applauding the stability that Clojure brings. This slack is the main forum for Clojurians, and it includes discussions about various libraries, bugs, fixes, etc, therefore one would reasonably expect stability complaints to dominate the discussion. My search is obviously not a random sampling, but it shoul"
  },
  {
    "title": "How to start a school with your friends (prigoose.substack.com)",
    "points": 58,
    "submitter": "geverett",
    "submit_time": "2025-05-08T19:34:36 1746732876",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=43930397",
    "comments": [
      "I always wondered why no one creates new universities in the US. It seems like in the 1800s every rich guy started their own university, many with unique missions.The existing university model in the US seems like it's ripe for disruption so I'm surprised no one has tried to create their own.\n \nreply",
      "It seems like all we\u2019ve been able to do is churn out diploma mills with dubious (or outright fraudulent) accreditation.\n \nreply",
      "Or predatory/misleading payment schemes, a la Lambda School\n \nreply",
      "Love to see it!Wonder how to reconcile the description of almost-negligible admin overhead with this description of a similar effort that warns, \"We wanted to keep costs extremely low, so we had parent volunteers do all admin for the school. It's going really well, but it's an insane amount of work.\"From my experience both teaching kids and organizing things, that seems like a much more likely outcome.https://x.com/KelseyTuoc/status/1917287461027459239\n \nreply",
      "FractalU isn't a school. It doesn't need to keep records, comply with miles of state regulations (employee and volunteer background checks, record keeping, mandatory exams, ...). It doesn't need to be able to demonstrate to other schools (or universities) what the students achieved. It doesn't need to demonstrate to the state that it's actually teaching the students something. It doesn't handle any money, so it doesn't need an accountant. It doesn't employ anyone. It doesn't need to worry about firing anyone.My kids attended a small co-op school when they were young--5 employees (4 teachers + \"director\" who was mostly a floating assistant/substitute), everything else handled by parent volunteers. There's really an enormous amount of administrative overhead.FractalU doesn't have any of that because it's not actually a school.\n \nreply",
      "The Recurse Center[1] folks (also YC) started an un-school with friends![1] http://recurse.com/\n \nreply",
      "This is such a refreshing inversion of the \u2018edtech\u2019 trend\u2014rather than trying to scale education through software, FractalU scales motivation through community. Makes me wonder: instead of designing better UIs for MOOCs or LLM tutors, maybe the real unlock is designing better social containers for learning.\n \nreply",
      "That looks very cool.As someone that has given a number of classes and seminars, it gets fairly discouraging, how few folks want to learn.I think that establishing a learning-focused community (like this) would probably really get a lot of people engaged.Geeks like learning. Many others don't. It's always fairly demoralizing, when I encounter it.\n \nreply",
      "I love these sort of 'high agency', 'you can just do stuff' posts.\n \nreply",
      "I read \"How to Live Near your Friends\"[1] article linked from this article, and I can't help but be amused by the author's attitude of \"my friends should all move near me because that's the way we can all live near friends\"I mean they're not wrong, but also they could have made friends with their neighbours like the Stoop Coffee[2] author, or moved to be nearer to a friend group also. It's nice to see them really embracing their main character bias though (in this case, in a way that seemed to have successfully built a geographically aligned community)[1]: https://prigoose.substack.com/p/how-to-live-near-your-friend...[2]: https://news.ycombinator.com/item?id=43473618\n \nreply"
    ],
    "link": "https://prigoose.substack.com/p/how-to-start-a-university",
    "first_paragraph": ""
  },
  {
    "title": "How the US Built 5k Ships in WWII (construction-physics.com)",
    "points": 38,
    "submitter": "rbanffy",
    "submit_time": "2025-05-08T20:58:37 1746737917",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=43931285",
    "comments": [
      "A friend and I were at the WW2 museum in New Orleans a couple years ago and he said something that really stuck with me. Amazed at an exhibit on wartime manufacturing, he turned to me and said, \"This is so unbelievable to me. To think what we accomplished when everyone in the country was pulling in the same direction. There's no way that could happen anymore.\" I hardly want to glorify warfare, but he has a point. As a young person in our chaotic and ambiguous present day looking back into the haze of the past, there really is something incredibly romantic about the era of war mobilization. Ordinary people had a purpose simply assigned to them, and if nothing else I think it's still the case that people in all eras crave purpose.\n \nreply",
      "The attitude is a dangerously rose-tinted view of war, the US was operating internment camps for US citizens of Japanese descent you know. In a war, dissent is quashed. That doesn't mean that it isn't there, just that there is a high tolerance for sub-optimal decisions because there isn't time to ruminate.The US isn't getting poor outcomes from their manufacturing sector because people are divided, but because the US has policies tending towards deindustrialisation and there is a broad political consensus to keep them. Ban the smokestacks, ban the smokestack economy and enjoy the clean air.\n \nreply",
      "Yep, purpose.Societies today have immense latent potential. So many people are doing bullshit jobs that tick things over, sitting there wishing to be put to use for some intrinsically motivating purpose. An existential threat - war - is a well known way to bring that out. But war is too destructive for modern tastes.We've seen developing countries get great results by government directing private industry in stronger ways than we're used to in the West. For example China's regularly published national development priorities for the next 5 years. If you hew to these you'll be helped in various ways. Singapore's and South Korea's rises to global powers were helped along by government getting everyone to row in the same direction - among other things, I'm greatly simplifying. But to focus on this one idea, I hope you can agree that providing purpose through top-down leadership is a great way to harness societies' latent potential and mobilize in a given direction..Rudderless, laissez-faire governance got the US a surprisingly long way. But we are seeing the resultant directionlessness leave leaders unable to agree on whether to tear up what's been built, leave it in place, or go some completely random direction.It's not the ships that were built, it's what they represented. That was what got them built.\n \nreply",
      "The latent authoritarianism in in opinions like yours makes it easier to understand why authoritarians keep rising to the top of different societies, so they can destroy lives, squander wealth and crush individual peoples' own perfectly productive capacities for finding their own cooperative purposes in life.\n \nreply",
      "Pretty sure the current crop of politicians that are destroying lives, squandering wealth, and crushing individual people are doing it as banner-bearers, not of any kind of Eastern collectivism, but of the uniquely American brand of 'fuck you, fuck everyone, fuck any responsibilities I may have, don't tread on me, I've got mine'.\n \nreply",
      "In their personal lives.When they have to deign to consider the impact of decisions.In their professional lives, they are Advancing American Independence.",
      "> So many people are doing bullshit jobs that tick things over, sitting there wishing to be put to use for some intrinsically motivating purposeWe're a generation of men raised by Fight Club\u2014I'm wondering if a self-induced mass-culling event is really the answer we need.\n \nreply",
      "I don't see anything romantic about this. The mass mobilization of a society so well over 400,000 members of its youngest and brightest can die grotesquely overseas while industry, society and culture are forcefully synchronized to a single government issued purpose is not usually something to desire.I do understand the needs of that particular war, The Nazis and Imperial Japan were truly invasive evils, big and globally dangerous enough to be worth fighting, even if it meant mass mobilization, but generally, there's no nostalgic beauty to such vast butchery, destruction and creation for the sake of destruction. I prefer finding my own purpose in life, and knowing that my children won't be ripped apart by artillery in some blood-soaked field of mud due to government decree.\n \nreply",
      "I think what the GP is relating to is that we could achieve so so sooo much more, if we didn't have all the opportunistic selfish people in our midst, who will go against any worthy goal, if it means they can enrich themselves. It is about the distribution of resources to reach goals. It would be quite easy for example to ensure, that every school meets some standards, enabling children to learn well. But there are always some lobbyists lobbying against it, and some politicians working against it, because there is no short term gain to be had for their business or for themselves. Also an educated population is maybe not what every politician wants in the first place, even though we all know, that raising the general education level would be beneficial in the long run.Or what we could achieve in terms of renewable energy, if we all were behind the goal. There are many examples that benefit society, but anti-social forces and influences are everywhere, delaying, stopping, and sabotaging our future.\n \nreply",
      "Studs Terkel's collection of interviews with various populations of the USA in The Good War is a good antidote to overromanticization of World War 2 conditions.\n \nreply"
    ],
    "link": "https://www.construction-physics.com/p/how-the-us-built-5000-ships-in-wwii",
    "first_paragraph": ""
  },
  {
    "title": "Using NASA\u2019s SMAP satellite to detect L-band interference (radioandnukes.substack.com)",
    "points": 297,
    "submitter": "c16",
    "submit_time": "2025-05-08T08:52:56 1746694376",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=43924358",
    "comments": [
      "I liked this overview map they posted a few days ago: https://x.com/HamWa07/status/1919763145536463222giammaiot2 on twitter has a long history of trying to use science sensors to detect intentional RF interference, e.g. this post with a map from the Advanced Microwave Scanning Radiometer (AMSR) looking at 7 GHz: https://x.com/giammaiot2/status/1919493425100988490Or this thread from 2023 looking at SMAP: https://x.com/giammaiot2/status/1770815247772729539\n \nreply",
      "Thank you, that map is fascinating. Jamming in and around conflict zones (Ukraine, Myanmar) and in China make sense to my Western brain. But why so much interference in Japan?\n \nreply",
      "It seems it's because Japan allows LTE service on 1427.9Mhz and higher. It looks like Europe starts at 1452Mhz, and the US keeps this entire band clear from consumer applications.\n \nreply",
      "A wonderful example of the useful, sometimes unintentional secondary effects of doing science. SMAP as a mission is firmly in the Earth science category, so very much in the crosshairs of the current administration. The data is used for Earth science and climate research and has many agricultural and water management applications.For example, water management districts can tell if the local soil can accommodate the water from an upcoming storm or if the water will stay on the surface and cause flooding.\n \nreply",
      "The specific allocation is 1400 to 1427 MHz. It is reserved for radio astronomy (the hydrogen line is at 1420.4 MHz), passive (receive only) Earth exploration satellites and passive space research.In the US, 1240 to 1400 MHz is allocated to radar. GNSS downlinks at 1240 to 1300 MHZ are not protected in the US.\n \nreply",
      "Iridium satellites can communicate with ground stations on L-band.This band is extremely useful if you're stuck on a ship in the middle of a typhoon and need to get some help.\n \nreply",
      "L-band signals can penetrate through clouds and rain. This property is why L-band is used for GPS and other applications that require all-weather operation, as it allows for accurate data collection even in the presence of adverse weather.\n \nreply",
      "How does this work? Do they just listen and never transmit, unless they receive a targeted emergency message?\n \nreply",
      "L-band is used for voice and data communications to handheld devices by several satcom providers.At a very high level handheld satellite communications devices work just like regular cellphones, except they also function in the middle of the ocean.\n \nreply",
      "His GitHub site mentions: \"This script processes NASA SMAP L1B .h5 data files\", but he doesn't say how he obtains these data files. Is he using an API, or is he using something like an RTL-SDR to pull the data directly?\n \nreply"
    ],
    "link": "https://radioandnukes.substack.com/p/how-dare-you-transmit-at-14-ghz",
    "first_paragraph": ""
  },
  {
    "title": "First American pope elected and will be known as Pope Leo XIV (cnn.com)",
    "points": 453,
    "submitter": "saikatsg",
    "submit_time": "2025-05-08T16:30:45 1746721845",
    "num_comments": 711,
    "comments_url": "https://news.ycombinator.com/item?id=43927856",
    "comments": [
      "Related: https://catholicreview.org/chicago-native-cardinal-prevost-e...(via https://news.ycombinator.com/item?id=43928742, but we merged that thread hither)",
      "> \"Cardinal George of Chicago, of happy memory, was one of my great mentors, and he said: 'Look, until America goes into political decline, there won't be an American pope.' And his point was, if America is kind of running the world politically, culturally, economically, they don't want America running the world religiously. So, I think there's some truth to that, that we're such a superpower and so dominant, they don't wanna give us, also, control over the church.\"https://www.cbsnews.com/news/new-pope-could-it-be-american-c...\n \nreply",
      "That's an interesting thought but if they're actually that concerned about it then they'd wait longer than four months.  It probably has more to do with America's predominant religion being protestantism by a very wife margin for most of the country's existence.  We didn't have a Catholic president until Kennedy and even then proving to the common American that Catholics aren't insane Vatican mindslaves was considered a hurdle he had to overcome.If there's a political motive in not choosing an American pope until now it's that for most of American history it wouldn't have granted them any influence over American politics.  If there's a personal motive it's that until recently they felt insulted that America went for almost 200 years before finally electing a Catholic president.\n \nreply",
      "OTOH, 6 of the 9 supreme court justices are catholic so there might be some influence there although I think the influence is probably more from the somewhat uniquely American brand of conservative Catholicism.\n \nreply",
      "> That's an interesting thought but if they're actually that concerned about it then they'd wait longer than four months.I don't think they had much control over when Francis died.\n \nreply",
      "The concern is that America would be too powerful if they had this power over Catholicism as well. There's no concern about waiting until it's time to appoint the next one.\n \nreply",
      "We\u2019re four months into political decline.  We haven\u2019t been popeless for four months.\n \nreply",
      "For what it\u2019s worth, I was just reading that Leo wasn\u2019t seen as \u201ccompletely\u201d American due to his many years in Peru \u2014 he\u2019s even a citizen. Take that as you will.\n \nreply",
      "Americans will say they are Italian because their great grandma ate spaghetti once, but God forbid someone is American because he was born there\n \nreply",
      "GP is right, he is not \"completely\" American in the sense that he is both American and Peruvian because of his dual citizenship. He also spent most of his life outside of the USA.Which I think is a great thing as the representative of a worldwide religion. Born in the US, an English-speaking country in North America, lived in Peru, a Spanish-speaking country in the South America, then in Italy, an Italian-speaking country in Europe.\n \nreply"
    ],
    "link": "https://www.cnn.com/world/live-news/new-pope-conclave-day-two-05-08-25",
    "first_paragraph": "Live Updates\n            \u2022 First American pope: Cardinal Robert Prevost has been elected as the first US-born pontiff and will be known as Pope Leo XIV. Leo made his first remarks as pope from the balcony of St. Peter\u2019s Basilica in front of tens of thousands of onlookers, calling for peace and paying tribute to the late Pope Francis.\n    \n            \u2022 About the new pope: Leo, a 69-year-old from Chicago, is a leader with global experience. He spent much of his career as a missionary in South America and holds dual citizenship in the US and Peru, where he served as a bishop. He most recently led a powerful Vatican office for bishop appointments. He is expected to build on Pope Francis\u2019 reforms.\n    \n            \u2022 Global reaction: The pope\u2019s election prompted an outpouring of congratulations from world leaders, who expressed eagerness to work with the pontiff on global issues. US President Donald Trump called the historic selection a great honor for the country.\n    \n            John Pre"
  },
  {
    "title": "Mathematical Problem Solving (lmu.de)",
    "points": 58,
    "submitter": "ibobev",
    "submit_time": "2025-05-05T08:05:35 1746432335",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=43892840",
    "comments": [
      "I hate when mathematicians make math harder to understand.This is an example. Lots of assumptions without any explanations.\n \nreply",
      "> This is an example. Lots of assumptions without any explanations.The course notes are literally filled with examples and explanations. They have discussions on each homework exercise, for instance, not just the answers.\n \nreply",
      "What is an example? This is a course page. Do you have a specific complaint?\n \nreply"
    ],
    "link": "https://www.cip.ifi.lmu.de/~grinberg/t/20f/",
    "first_paragraph": "An introduction to mathematical problem solving.\nWe will learn techniques and tools for solving problems of the kind that appear in mathematical competitions and journals.\nThese techniques (like induction, the Pigeonhole Principle, modular arithmetic or the Cauchy-Schwarz inequality) have uses all over mathematics; we will explore these uses through hands-on problem solving.\n\nEach week will have approx. 50 minutes of video lectures (see the \"Course materials\" below for the links) and 40 minutes of recitations (i.e., collaborative problem-solving sessions over Zoom). Weekly homework sets will reinforce the training.\nPrerequisites: Math 200.  "
  },
  {
    "title": "A flat pricing subscription for Claude Code (anthropic.com)",
    "points": 87,
    "submitter": "namukang",
    "submit_time": "2025-05-08T21:12:32 1746738752",
    "num_comments": 71,
    "comments_url": "https://news.ycombinator.com/item?id=43931409",
    "comments": [
      "The problem is that this is $100/mo with limits. At work I use Cursor, which is pretty good (especially tab completion), and at home I use Copilot in vscode insiders build, which is catching up to Cursor IMO.However, as long as Microsoft is offering copilot at (presumably subsidized) $10/mo, I'm not interested in paying 10x as much and still having limits. It would have to be 10x as useful, and I doubt that.\n \nreply",
      "Doesn\u2019t resonate with me because I\u2019ve spent over $1,000 on Claude Code at this point and the return is worth it. The spend feels cheap compared to output.In contrast - I\u2019m not interested in using cheaper, less-than, services for my livelihood.\n \nreply",
      "> the return is worth itI'm curious, what was the return? What did you do with the 1k?\n \nreply",
      "Produce working code faster => ship faster => paid faster? That's the valu-prop right? So, naturally the $JOB will cover the bill.\n \nreply",
      "Could you anonymize and share your last 5-10 prompts? Just wanna understand how people are using Claude Code.\n \nreply",
      "\"update this pubsub server in java and update the message publishing to reuse the same buffer when calling vertx's write message\"or things to that extent\n \nreply",
      "hey, I'm open to that possibility. Maybe I'll grab $5 in API credit and give it a shot (for 5 minutes or a week depending on who you ask)\n \nreply",
      "i got $100 of credit at the start of the year, and have been using +1$ each month, starting at $2 in january using aider at the time. just switched to claude code this week, since it follows a similar UX. agentic CLI code assist really has been growing in usefulness for me as i get faster at reviewing its output.i use it for very targeted operations where it saves me several roundtrips to code examples and documentation and stack overflow, not spamming it for every task i need to do, i spend about $1/day of focused feature development, and it feels like it saves me about 50% as many hours as i spend coding while using it.\n \nreply",
      "What do you prefer, between Aider and CC? I use Aider for when I want to vibe code (I just give the LLM a high-level description and then don't check the output, because it's so long), and Cursor when I want to AI code (I tell the AI to do low-level stuff and check every one of the five lines it gives me).AI coding saves me a lot of time writing high-quality code, as it takes care of the boilerplate and documentation/API lookups, while I still review every line, and vibe coding lets me quickly do small stuff I couldn't do before (e.g. write a whole app in React Native), but gets really brittle after a certain (small) codebase size.I'm interested to hear whether Claude Code writes less brittle code, or how you use it/what your experience with it is.\n \nreply",
      "Whoever is paying for your time should calculate how much time you\u2019d save between the different products. The actual product price comparison isn\u2019t as important as the impact on output quality and time taken. Could be $1000 a month and still pay for itself in a day, if it generated >$1000 extra value.This might mean the $10/month is the best. Depends entirely on how it works for you.(Caps obviously impact the total benefit so I agree there.)\n \nreply"
    ],
    "link": "https://support.anthropic.com/en/articles/11145838-using-claude-code-with-your-max-plan",
    "first_paragraph": ""
  },
  {
    "title": "Newsreels from the UCLA Film and Television Archive (newsreels.net)",
    "points": 17,
    "submitter": "billfor",
    "submit_time": "2025-05-08T22:32:41 1746743561",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://newsreels.net/",
    "first_paragraph": ""
  },
  {
    "title": "The Rise and Fall of the Visual Telegraph (2017) (parisianfields.com)",
    "points": 24,
    "submitter": "geox",
    "submit_time": "2025-05-08T19:13:29 1746731609",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=43930156",
    "comments": [
      "For the discworld alternative, check the clacks [1].[1] https://wiki.lspace.org/Clacks\n \nreply",
      "There are many places called Telegraph Hill that used to be semaphore towers.\n \nreply",
      "It\u2019s also fascinating this didn\u2019t existed 300 years sooner (with more towers because of poorer telescopes). Communication of information across a kingdom is so important\n \nreply",
      "If i remember correctly the laser optical transmission tech started as a \"laser visual telegraph\" between 2 PARC buildings rooftops in the Stanford Research Park here.\n \nreply",
      "Ah, so that's the origin of sem\u00e1foro in Spanish, which the current meaning is\n\"traffic light\". And a semaphore on programming, too.\n \nreply"
    ],
    "link": "https://parisianfields.com/2017/11/05/the-rise-and-fall-of-the-visual-telegraph/",
    "first_paragraph": "Sometimes we go looking for blog ideas, and sometimes they come along and tap us persistently on the shoulder. This one did \u2013 three times.First, I spotted an \u201cadvertorial\u201d in a 1912 issue of a small American magazine called The Philistine. It was a two-page item on \u201cNapoleon\u2019s Visual Telegraph: The First Long-Distance System\u201d and it was sponsored by American Telephone and Telegraph. The illustration looked vaguely familiar.A few days later, I was hunting for a postcard in our collection and I ran across this one of \u201cLa Tour du T\u00e9l\u00e9graphe\u201d in Montmartre, sitting on top of what appear to be the ruins of a church. But I still wasn\u2019t sure what I was looking at. What was that thing on the tower?Finally, out of the blue, my stepson Alex sent us a link to an article from The Economist\u00a0about \u201cthe world\u2019s first cyber-attack\u201d featuring this same telegraph system.When I mentioned this to Norman, he looked thoughtful and went to find a history of telegraphy called The Victorian Internet by Tom Sta"
  },
  {
    "title": "Static as a Server (overreacted.io)",
    "points": 78,
    "submitter": "danabramov",
    "submit_time": "2025-05-08T17:49:02 1746726542",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=43929054",
    "comments": [
      "I dont get it.  Yes it's a static site, but it's only text and you're sending 100kb+ of JS over the wire.  Is there any reason why you need React?\n \nreply",
      "you don't need to send 100kb+ of JS over the wire to build a static site in react: for example https://vike.dev supports static HTML-only output for a site built with React.as for \"why React\", speaking just for myself it's really nice to just have one tool that can do everything (static HTML-only, static with JS, SPA, SSR) and not have to context switch or potentially even have to split my site into two projects just because I want to hop between one or the other approach. and React has the biggest mindshare and ecosystem.\n \nreply",
      "I think you just proved the point by introducing yet another frontend framework to learn.And you absolutely don't want one tool to do everything.  HTML/CSS is native and understanding it is a requirement for React.  It also doesn't require Node and a build step.\n \nreply",
      "> you don't need to send 100kb+ of JS over the wire to build a static site in reactIt is telling that the blog of the fw creator ships 500kb of JS/CSS/HTML to display text on a screen.\n \nreply",
      "This is also how I build most of my static sites, usually deployed to GitHub Pages. For example, here\u2019s the demo page for a library I recently developed: https://exogen.github.io/turbo-colormap/Or a more complicated app: https://exogen.github.io/t2-model-skinner/Are all of Next.js\u2019 features overkill for such sites? Sure, but the convenience such frameworks provide is worth it. And the reason to prefer it over something like Vite is simply routing, which Vite doesn\u2019t cover out of the box, so as soon as I want to add a second page, I now have another problem to solve.Next.js\u2019 best feature is simply that you\u2019re up and running with `npm i react react-dom next` and step two is just writing the pages.\n \nreply",
      "My exact thoughts[1] when it comes to hosting, things should be static, although I seek joy in Astro and Markdown. \n[1]https://zeropoint.sh/blog/astro-as-static/\n \nreply",
      "I understand that somebody might want to generate static pages from code that generates it dynamically, but I fail to appreciate _why_.  Are people using this for a handful of pages they want to load quickly and whose contents rarely change, or are people building entire static sites using things like React?  If it's the latter... uh... why?  It's been awhile since I was a web developer, so maybe my pain threshold is inappropriately low.  I think Jekyll is fine and use it pretty regularly.\n \nreply",
      "It's because inevitably there comes a time where you want some pages (or even sub-pages) to be static, and other pages (or parts) of your application to be dynamic. If the question is \"why\" the inverse is \"why not\"?Dan's blog is rendered as static, works without javascript and still lets him write complex client-side components when he calls for it. And if one day he decides to add a page that renders content dynamically, he can do so in the same patterns and framework that he's already using.The goal is developer-choice. No need to pick static or dynamic holistically, have access to both when you need them. No need to pick between hydrating the entire website vs. rendering exclusively on the server. No need to pick between writing client-side amenable code or server-only code.How many platforms have a \"marketing\" website for / and a platform website for /dashboard? No need to split them, just use a framework that accommodates both seamlessly. It's more powerful, even though it does come with a learning curve.\n \nreply",
      "To give a concrete example, I\u2019ll probably add some dynamic stuff at some point in the future, like commenting with Bluesky or such. It\u2019s nice not to switch tools for that.\n \nreply",
      "I think the reason to split the marketing page from the dashboard one is that you can deploy one without the other. I would actually prefer to have all the marketing stuff in its own repo away from any dashboard code.\n \nreply"
    ],
    "link": "https://overreacted.io/static-as-a-server/",
    "first_paragraph": ""
  }
]