[
  {
    "title": "IRS Open Sources its Fact Graph (github.com/irs-public)",
    "points": 128,
    "submitter": "ronbenton",
    "submit_time": "2025-10-15T23:24:47 1760570687",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=45599567",
    "comments": [
      "My eyes read Scala but my brain was thinking Clojure, so I was a bit confused on why there weren\u2019t any parentheses for the first couple of seconds looking at the source.reply",
      "Am I being dumb or does this not actually contain the facts about the tax code? Is the /demo/all-facts file supposed to be the \u201creal\u201d facts? Are the XML fact files provided in another location?It\u2019s pretty cool to see the way that the IRS handles defining and maintaining its tax calculations, but also a machine-readable tax code seems cool too.reply",
      "I believe the actual IRS tax code implementation is in a separate repo here: https://github.com/IRS-Public/direct-file while the originally linked repo is the fact graph tooling decoupled from the tax implementation.reply",
      "Look like many of them are specifically the xml files here:https://github.com/IRS-Public/direct-file/tree/e0d5c84451cc5...reply",
      "I was just reading through those! A bit dizzyingreply",
      "specifically here https://github.com/IRS-Public/direct-file/tree/main/direct-f...reply",
      "I\u2019ve had frustrating experiences with TurboTax due to its overly complex interface, aggressive data collection under the guise of saving money (which it doesn\u2019t deliver), and a convoluted pricing structure that rivals the IRS\u2019s own complexity.I hope this initiative is good enough to enable domain experts and good people to build transparent, user-friendly alternatives to challenge TurboTax\u2019s market grip.Has anyone encountered promising tools or approaches that tackle these pain points?reply",
      "I wonder how this can be used with an LLM to provide interesting tax advice? I'd love to regularly ask questions of the tax code...reply",
      "Makes me wonder if someone has already trained a model on the tax code. Would be interesting for sure.reply",
      "Model training data already contains all the text there is[0], so they can already answer questions like this (especially with web search), but they aren't good at tax calculations.https://arxiv.org/abs/2507.16126v1[0] but it's quite possible the conversion from HTML to text is badreply"
    ],
    "link": "https://github.com/IRS-Public/fact-graph",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Fact Graph \n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.No Endorsement or WarrantyThe Internal Revenue Service (IRS) does not endorse, maintain, or guarantee the accuracy, completeness, or functionality of the code in this repository.\nThe IRS assumes no responsibility or liability for any use of the code by external parties, including individuals, developers, or organizations.\nThis includes\u2014but is not limited to\u2014any tax consequences, computation errors, data loss, or other outcomes resulting from the use or modification of this code.Use of the code in this repository is at your own risk. Users of this repository are responsible for complying with any open source or third-party licenses.The Fact Graph is a production-ready knowledge graph for modeling, am"
  },
  {
    "title": "I'm recomming my customers switch to Linux rather that Upgrade to Windows 11 (scottrlarson.com)",
    "points": 15,
    "submitter": "trinsic2",
    "submit_time": "2025-10-16T01:00:17 1760576417",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.scottrlarson.com/publications/publication-windows-move-towards-surveillance/",
    "first_paragraph": "Recently, the Secure Resilient Future Foundation released a newsletter calling for Microsoft to extend Windows 10 support past the October 14th deadline.With the release of Windows 11, the threat to data privacy is the worst it\u2019s ever been. In my recent article, \u201cLooking back at my transition from Windows to Linux in an anti-customer age\u201d, I wrote about my switch to Linux and how it saved me from having to sacrifice my freedom in the name of convenience.Whether you\u2019re a business or a home user, I\u2019m here to tell you that in many cases, Linux is a real alternative to Windows. So instead of pushing the goal post back from the brink of an Orwellian nightmare. I\u2019m suggesting all of us consider switching Linux now.Microsoft\u2019s design of Windows 11 is a concern because:Due to these concerns, I will be recommending Linux as a replacement for new computers I build for my customers. You can still request Windows if Linux doesn\u2019t work for you.Linux Distribution Replacements for Windows\n1. Zorin OS"
  },
  {
    "title": "Apple M5 chip (apple.com)",
    "points": 935,
    "submitter": "mihau",
    "submit_time": "2025-10-15T13:02:53 1760533373",
    "num_comments": 1031,
    "comments_url": "https://news.ycombinator.com/item?id=45591799",
    "comments": [
      "Base models only:- M1 | 5 nm              | 8 (4P+4E)            | GPU 7\u20138      | 16-core Neural | Memory Bandwidth: 68.25 GB/s  | Unified Memory: 16 GB                     | Geekbench6 ~2346 / 8346- M2 | 5 nm (G2)         | 8 (4P+4E)            | GPU 8\u201310     | 16-core Neural | Memory Bandwidth: 100 GB/s    | Unified Memory: 24 GB                     | Geekbench6 ~2586 / 9672- M3 | 3 nm (first-gen)  | 8 (4P+4E)            | GPU 8\u201310     | 16-core Neural | Memory Bandwidth: 100 GB/s    | Unified Memory: 24 GB                     | Geekbench6 ~2965 / 11565- M4 | 3 nm (second-gen) | 10 (4P+6E)           | GPU 8\u201310     | 16-core Neural | Memory Bandwidth: 120 GB/s    | Unified Memory: 32 GB                     | Geekbench6 ~3822 / 15031- M5 | 3 nm (third-gen)  | 10 (4P+6E)           | GPU 10       | 16-core Neural | Memory Bandwidth: 153 GB/s    | Unified Memory: up to 32 GB               | Geekbench6 ~4133 / 15,437 (9-core sample)reply",
      "Let's see if I can turn this into an ASCII table and have it survive HN's reformatting.    +------+------------------+--------------+----------+----------------+-------------------+-------------------+---------------------------+\n    | Chip | Process          | CPU Cores    | GPU      | Neural Engine  | Memory Bandwidth  | Unified Memory    | Geekbench6 (Single/Multi) |\n    +------+------------------+--------------+----------+----------------+-------------------+-------------------+---------------------------+\n    | M1   | 5 nm             | 8 (4P+4E)    | 7\u20138      | 16-core Neural | 68.25 GB/s        | 16 GB             | ~2346 / 8346              |\n    | M2   | 5 nm (G2)        | 8 (4P+4E)    | 8\u201310     | 16-core Neural | 100 GB/s          | 24 GB             | ~2586 / 9672              |\n    | M3   | 3 nm (first-gen) | 8 (4P+4E)    | 8\u201310     | 16-core Neural | 100 GB/s          | 24 GB             | ~2965 / 11565             |\n    | M4   | 3 nm (second-gen)| 10 (4P+6E)   | 8\u201310     | 16-core Neural | 120 GB/s          | 32 GB             | ~3822 / 15031             |\n    | M5   | 3 nm (third-gen) | 10 (4P+6E)   | 10       | 16-core Neural | 153 GB/s          | up to 32 GB       | ~4133 / 15437 (9-core)    |\n    +------+------------------+--------------+----------+----------------+-------------------+-------------------+---------------------------+reply",
      "Or to fit in a narrower window:  Chip | Process | CPU       | GPU  | Neural  | Memory      | Unified | Geekbench6\n       |         | Cores     |      | Engine  | Bandwidth   | Memory  | Single / Multi \n  -----|---------|-----------|------|---------|-------------|---------|----------------------\n  M1   | 5 nm G1 |  8: 4P+4E | 7\u20138  | 16-core |  68.25 GB/s |  16 GB  | 2346 / 8346          \n  M2   | 5 nm G2 |  8: 4P+4E | 8\u201310 | 16-core | 100    GB/s |  24 GB  | 2586 / 9672          \n  M3   | 3 nm G1 |  8: 4P+4E | 8\u201310 | 16-core | 100    GB/s |  24 GB  | 2965 / 11565         \n  M4   | 3 nm G2 | 10: 4P+6E | 8\u201310 | 16-core | 120    GB/s |  32 GB  | 3822 / 15031         \n  M5   | 3 nm G3 | 10: 4P+6E | 10   | 16-core | 153    GB/s | \u226432 GB  | 4133 / 15437 (9 core)reply",
      "doing the lords workreply",
      "You've done yeoman's work, lad.reply",
      "Good idea!reply",
      "The step down from 32GB to 24GB of unified memory is interesting. Theories? Perhaps they decided M4 allowed too much memory in the standard chip and they want to create a larger differential with Pro/Max chips?Update: I am thinking the 24GB for M5 is a typo. I see on Apple's site the 14 inch MBP can be configured optionally with 32GB of RAM.reply",
      "That seems like a typo or incorrect info, the M5 MBP definitely can be configured up to 32 GB, and the Apple page mentions 32 GB explicitly as well.reply",
      "I had the same question, but I can only speculate at the moment.\nThe cynical part of me thinks in a similar line: create an artificial differentiation and push people to upgrade.If anyone has any real clues that they can share pseudonymously, that would be great. Not sure which department drove that change.reply",
      "They definitely do that. You could get 64gb ram without going up to the top spec of the Max tier of CPU in the M1 and M2 generations, but with the M4 Pro you can only do 24 or 48gb, while on the lower spec M4 Max you can only do 36gb and nothing else, only the absolute best CPU can do 64, therefore if you were otherwise going to get the 48gb m4 pro, you'd have to spend another ~$1200 USD to get another 16gb of ram if all you cared about was ram.There may be a technical explanation for it, but incentives are incentives.reply"
    ],
    "link": "https://www.apple.com/newsroom/2025/10/apple-unleashes-m5-the-next-big-leap-in-ai-performance-for-apple-silicon/",
    "first_paragraph": "Text of this articleOctober 15, 2025PRESS RELEASEApple unleashes M5, the\u00a0next big leap in AI performance for Apple\u00a0siliconM5 delivers over 4x the peak GPU compute performance for AI compared to M4, featuring a next-generation GPU with a Neural Accelerator in each core, a more powerful CPU, a faster Neural Engine, and higher unified memory bandwidthCUPERTINO, CALIFORNIA Apple today announced M5, delivering the next big leap in AI performance and advances to nearly every aspect of the chip. Built using third-generation 3-nanometer technology, M5 introduces a next-generation 10-core GPU architecture with a Neural Accelerator in each core, enabling GPU-based AI workloads to run dramatically faster, with over 4x the peak GPU compute performance compared to M4.1 The GPU also offers enhanced graphics capabilities and third-generation ray tracing that combined deliver a graphics performance that is up to 45 percent higher than M4.1 M5 features the world\u2019s fastest performance core, with up to a"
  },
  {
    "title": "Claude Haiku 4.5 (anthropic.com)",
    "points": 426,
    "submitter": "adocomplete",
    "submit_time": "2025-10-15T16:55:06 1760547306",
    "num_comments": 179,
    "comments_url": "https://news.ycombinator.com/item?id=45595403",
    "comments": [
      "Pretty cute pelican on a slightly dodgy bicycle: https://tools.simonwillison.net/svg-render#%3Csvg%20viewBox%...reply",
      "Gemini Pro initially refused (!) but it was quite simple to get a response:> give me the svg of a pelican riding a bicycle> I am sorry, I cannot provide SVG code directly. However, I can generate an image of a pelican riding a bicycle for you!> ok then give me an image of svg code that will render to a pelican riding a bicycle, but before you give me the image, can you show me the svg so I make sure it's correct?> Of course. Here is the SVG code...(it was this in the end: https://tinyurl.com/zpt83vs9)reply",
      "Gemini 3.0 Pro (or what is deemed to be 3.0 Pro - you can get access to it via A/B testing on AI Studio) does a noticeably better jobhttps://x.com/cannn064/status/1972349985405681686https://x.com/whylifeis4/status/1974205929110311134https://x.com/cannn064/status/1976157886175645875reply",
      "How do people trigger A/B testing?reply",
      "There\u2019s obviously no improvement on this metric and hasn\u2019t been in a while.reply",
      "\"create svg code that will create an image of svg code that will create a pelican riding a bicycle\"https://chatgpt.com/share/68f0028b-eb28-800a-858c-d8e1c811b6...(can be rendered using simon's page at your link)reply",
      "I like this workflowreply",
      "What is dada?reply",
      "Context on this cutting-edge benchmark for those unaware:https://simonwillison.net/2025/Jun/6/six-months-in-llms/https://simonwillison.net/tags/pelican-riding-a-bicycle/Full verbose documentation on the methodology: https://news.ycombinator.com/item?id=44217852reply",
      "As added context to ensure no benchmark gaming, here a quite impressive Shitaki Mushroom riding a rowboat: https://imgur.com/Mv4Pi6pPrompt: https://t3.chat/share/ptaadpg5n8Claude 4.5 Haiku (Reasoning High)\n178.98 token/sec\n1691 tokens\nTime-to-First: 0.69 secAs a comparison, here Grok 4 Fast, which is one of worst offenders I have encountered in doing very good with a Pelican Bicycle, yet not with other comparable requests: https://imgur.com/tXgAAkbPrompt: https://t3.chat/share/dcm787gcd3Grok 4 Fast (Reasoning High)\n171.49 token/sec\n1291 tokens\nTime-to-First: 4.5 secAnd GPT-5 for good measure: https://imgur.com/fhn76PbPrompt: https://t3.chat/share/ijf1ujpmurGPT-5 (Reasoning High)\n115.11 tok/sec\n4598 tokens\nTime-to-First: 4.5 secThese are very subjective, naturally, but I personally find Haiku with those spots on the mushroom rather impressive overall. In any case, the delta between publicly known benchmark and modified scenarios evaluating the same basic concepts continues to be smallest with Anthropic models. Heck, sometimes I've seen their models outperform what public benchmarks indicated. Also, seems Time-to-first on Haiku is another notable advantage.reply"
    ],
    "link": "https://www.anthropic.com/news/claude-haiku-4-5",
    "first_paragraph": ""
  },
  {
    "title": "Writing an LLM from scratch, part 22 \u2013 training our LLM (gilesthomas.com)",
    "points": 53,
    "submitter": "gpjt",
    "submit_time": "2025-10-15T23:42:12 1760571732",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.gilesthomas.com/2025/10/llm-from-scratch-22-finally-training-our-llm",
    "first_paragraph": "This post wraps up my notes on chapter 5 of Sebastian Raschka's book\n\"Build a Large Language Model (from Scratch)\".\nUnderstanding cross entropy loss and\nperplexity were the hard bits for\nme in this chapter -- the remaining 28 pages were more a case of plugging bits together and\nrunning the code, to see what happens.The shortness of this post almost feels like a damp squib.  After writing so much\nin the last 22 posts, there's really not all that much to say -- but that hides the fact that\nthis part of the book is probably the most exciting to work through.  All these pieces\ndeveloped with such care, and with so much to learn, over the preceding 140 pages,\nwith not all that much to show -- and suddenly, we have a codebase that we can let\nrip on a training set -- and our model starts talking to us!I trained my model on the sample dataset that we use in the book, the 20,000\ncharacters of \"The Verdict\" by Edith Wharton, and then ran it to predict next tokens after \"Every effort\nmoves you\". "
  },
  {
    "title": "Next Steps for the Caddy Project Maintainership (caddy.community)",
    "points": 97,
    "submitter": "francislavoie",
    "submit_time": "2025-10-15T21:32:19 1760563939",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=45598590",
    "comments": [
      "I've had a really good time with Caddy on a hobby project over the past 7 years on a digital ocean droplet.Automatic HTTPS, multiple domains, proxying specific routes to local services, etc etc, managed by one extremely legible config file.I've had literally one service failure over that period, and it was my own error after running upgrades of the droplet's operating system.Highly recommended.Congrats to Mike on growing the project to the point where he can responsibly take a hand off the wheel now and then. And thank you!reply",
      "It\u2019s nice to see the responsibility spread across more people, open source projects live and die by their maintainers.As a note, Caddy is one of those tools which hits the 80-90% of functionality with 50% of the complexity.For both my homelab and hobby projects it just works. Its configuration is sane and well documented.I highly recommend giving it a try.reply",
      "Free software needs to find a way for people to contribute so that maintainers get paid.Caddy has been great!reply",
      "Caddy is excellent. Great on you, Matt for giving up some control.reply",
      "I like Caddy. Good to see it evolve. Hope it works wellreply",
      "https://caddyserver.com/> The Ultimate Server> makes your sites more secure, more reliable, and more scalable than any other solution.Is this an alternative to nginx or something?reply",
      "It\u2019s an http server like Apache or nginx.A stand-out feature has been ACME support built-in, and it\u2019s a fairly capable reverse proxy. I\u2019ve seen organizations use Caddy to provision certificates for customer domains at scale with very good results.reply",
      "Yes.Personally, I much prefer the way caddy does configuration / plugins (as someone reasonably conversant in how nginx does those things) - comparable to \"sysv init scripts vs systemd unit files\".reply",
      "It is, but I've mostly came across Caddy as a traefik alternative.reply",
      "I still think for Kubernetes ingress controller, traefik is more optimized for this use-case than Caddy. However, sitting in front of containers or a standalone reverse proxy I exclusively use Caddy.reply"
    ],
    "link": "https://caddy.community/t/next-steps-for-the-caddy-project-maintainership/33076",
    "first_paragraph": "tldr: I won\u2019t personally see all comments/issues/PRs anymore; maintainer team is being granted tag+release privileges; community will be more involved with leadership; increase current bus factor of 1; unblock the project where I am the bottleneck; help the project scale better.Caddy is now about 11 years old, and the project has changed a lot over that time, and grown hugely popular! To shed some perspective\u2026For years, my daily-ish routine involved checking my GitHub notifications \u2013 usually around 1-3 \u2013 triaging them and responding to each one of them personally. Most issues were obvious: bugs that needed urgent fixing, features that were a clear yes/no for the project, or questions that had easy answers.Even after the launch of v2, the project was still new and developing, most other people didn\u2019t have a lot of experience with it, and my vision was clear, so it was pretty easy to answer questions, make decisions, review the trickle of pull requests, etc. I wrote most of the code and "
  },
  {
    "title": "ImapGoose (whynothugo.nl)",
    "points": 30,
    "submitter": "xarvatium",
    "submit_time": "2025-10-15T22:28:55 1760567335",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=45599084",
    "comments": [
      "Looks great! Curious what the author and others use for local maildir email reading.reply",
      "I use isync and notmuch! With aerc as my reader.reply",
      "mu4e, but I suspect that a local maildir is a poor choice anyway.It's better to have a local cyrus running and connect to it by imap, with, say, gnus.reply",
      "Well, it says its bidirectional, so perhaps you could run two instances pointed at the same local maildir, but at different IMAP servers?reply",
      "Seems like a clone of https://gitlab.com/Lockywolf/imap-idle-mail-checker/reply",
      "The README for that project mentions IDLE, but not CONDSTORE.So what happens if you lose your connection to the server? How do you get up to date with the current state?reply"
    ],
    "link": "https://whynothugo.nl/journal/2025/10/15/introducing-imapgoose/",
    "first_paragraph": "ImapGoose is a small program to keep local mailboxes in sync with an IMAP\nserver. The wording \u201ckeep [\u2026] in sync\u201d implies that it does so continuously,\nrather than a one-time sync. ImapGoose is designed as a daemon, monitoring both\nthe IMAP server and the local filesystem, and immediately synchronising changes.\nWhen the IMAP server receives an email, it shows up in the filesystem within a\nsecond. When an email is deleted on another email client, it is removed1\nfrom the filesystem within a second.ImapGoose is highly optimised to reduce the amount of network traffic and tasks\nperformed. To do so, it relies on a few modern IMAP extensions and only supports\nmodern email servers. \u201cModern servers\u201d in the context of email means servers\nwhich support extensions which were standardised between 2005 and 2009.ImapGoose uses the CONDSTORE extension (standardised in 2006),\nwhich basically allows it to tell the server \u201cI last saw this mailbox when it\nwas in state XYZ, please tell me what\u2019s new\u201d. This"
  },
  {
    "title": "Blood-Sharing Drug Trend Fuels Global HIV Surge (nytimes.com)",
    "points": 37,
    "submitter": "zahlman",
    "submit_time": "2025-10-11T15:10:05 1760195405",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=45549770",
    "comments": [
      "> One reason the practice hasn\u2019t been more widespread is that it delivers a diminished dose of a drug. It isn\u2019t clear how much of a high secondary users receive, and some medical experts say there is no more than a placebo effect.Yes, this makes sense. People are not actually getting high from this, so I doubt that very many people are trying to get high from injecting other people's blood directly.> Unusual injection practices in Pakistan include selling half-used, blood-infused heroin syringes. (link to https://pubmed.ncbi.nlm.nih.gov/19558668/)This is the headline, I think. Street vendors sell hits of a syringe that may have already been stuck in someone else. Every time the needle enters a blood vessel, some of the blood goes back into the needle. It's not that users are asking for \"blood-infused\" syringes, it's that the vendors are sticking multiple people with a single syringe and now 50% of intravenous drug users in Sargodha have HIV.reply",
      "That's terrifying. There are so many things (that make HIV look like a cakewalk) that can happen, when you inject blood.I guess it is really about the same as \"tranq\" (xylazine), in the US. That stuff is pretty horrifying.Back before I quit, PCP was the drug boogeyman. That's almost quaint, now.reply",
      "https://archive.is/9H2pYreply",
      "whatreply",
      "This has to be a joke on the order of \u201clipstick parties\u201d and \u201cd&d is satanic,\u201d right? I\u2019m not saying the behavior in question _never_ happens, but surely the prevalence has been blown out of proportion.There\u2019s no drug I can think of that is present in the blood in such great concentrations that a transfusion would communicate the high, certainly not without killing the donator or the donatee. Nor does receiving mismatched blood result in anything resembling a high. I concede that the placebo effect can do many remarkable things.However, given that NYT\u2019s citations are decidedly hazy, I\u2019m more inclined to believe this is a rumor that got too big for its britches. Maybe a pharmacist out there can convince me otherwise?reply",
      "Agreed, I'm totally convinced this is a weird media hysteria. This just cannot be true, it makes zero sense from the most basic level.reply",
      "When do we get the article about this article? That's more interesting, in a way.reply",
      "The facts seem to be:1. HIV in Fiji is up 10-fold from 2014 (that's a lot, but if the starting point was small, then it's hard to say)2. 50% of people who contracted HIV said they shared needles (common issue, unrelated to \"Bluetoothing\")3. \"Bluetoothing\" is a meme, and it sounds insane, and in some small sample in South Africa, 18% of drug users claimed to have done it.4. In Pakistan, street vendors are selling pre-used heroin syringes (also insane - but needle sharing, not \"Bluetoothing\")I mean, sure, this is news in the sense that it's insane. But the headline makes you wonder what's going on at the NYT.reply",
      "The article seems to mention that the only effect seems to be placebo.Don\u2019t forget, people were and still are taking horse dewormer for Covid, despite shitting out parasites not really fixing much other than a parasite problem.reply",
      "If you mainline a half gallon of junkie blood, you should get a decent buzz. Just remember to check their blood type first, and to bring a large rug so you can dispose of their desiccated body afterwards.reply"
    ],
    "link": "https://www.nytimes.com/2025/10/08/world/asia/bluetoothing-drug-blood-sharing.html",
    "first_paragraph": ""
  },
  {
    "title": "Bringing NumPy's type-completeness score to nearly 90% (pyrefly.org)",
    "points": 43,
    "submitter": "todsacerdoti",
    "submit_time": "2025-10-07T09:32:12 1759829532",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=45501088",
    "comments": [
      "One of my biggest gripes around typing in python actually revolves around things like numpy arrays and other scientific data structures.  Typing in python is great if you're only using builtins or things that the typing system was designed for. But it wasn't designed with scientific data structures particularly in mind. Being able to denote dtype (e.g. uint8 array vs int array) is certainly helpful, but only one aspect.There's not a good way to say \"Expects a 3D array-like\" (i.e. something convertible into an array with at least 3 dimensions). Similarly, things like \"At least 2 dimensional\" or similar just aren't expressible in the type system and potentially could be. You wind up relying on docstrings. Personally, I think typing in docstrings is great. At least for me, IDE (vim) hinting/autocompletion/etc all work already with standard docstrings and strictly typed interpreters are a completely moot point for most scientific computing. What happens in practice  is that you have the real info in the docstring and a type \"stub\" for typing. However, at the point that all of the relevant information about the expected type is going to have to be the docstring, is the additional typing really adding anything?In short, I'd love to see the ability to indicate expected dimensionality or dimensionality of operation in typing of numpy arrays.But with that said, I worry that typing for these use cases adds relatively little functionality at the significant expense of readability.reply",
      "Have you looked at nptyping? Type hints for ndarray.https://github.com/ramonhagenaars/nptyping/blob/master/USERD...reply",
      "That one's new to me. Thanks!  (With that said, I worry that 3rd party libs are a bad place for types for numpy.)reply",
      "Numpy ships built-in type hints as well as a type for hinting arrays in your own code (numpy.typing.NDArray).The real challenge is denoting what you can accept as input. `NDArray[np.floating] | pd.Series[float] | float` is a start but doesn't cover everything especially if you are a library author trying to provide a good type-hinted API.reply",
      "I also had a very hard time to annotate types in python few years ago. A lot of popular python libraries like pandas, SQLAlchemy, django, and requests, are so flexible it's almost impossible to infer types automatically without parsing the entire code base. I tried several libraries for typing, often created by other people and not maintained well, but after painful experience it was clear our development was much faster without them while the type safety was not improved much at all.reply",
      "Some time ago I created a demo of named dimensions for Pytorch, https://github.com/stared/pytorch-named-dimsIn the same line, I would love to have more Pandas-Pydantic interoperability at the type level.reply",
      "This isn't static, but jaxtyping gives you at least runtime checks and also a standardized form of documenting those types. https://github.com/patrick-kidger/jaxtypingreply",
      "It actually doesn't, as far as I know :) It does get close, though.  I should give it a deeper look than I have previously, though.\"array-like\" has real meaning in the python world and lots of things operate in that world.  A very common need in libraries is indicating that things expect something that's either a numpy array or a subclass of one or something that's _convertible_ into a numpy array.  That last part is key.  E.g. nested lists. Or something with the __array__ interface.In addition to dimensionality that part doesn't translate well.And regardless, if the type representation is not standardized across multiple libraries (i.e. in core numpy), there's little value to it.reply",
      "I wonder if we should standardize on __array__ like how Iterable is standardized on the presence of __iter__, which can just return self if the Iterable is already an Iterator.reply",
      "Would a custom decorator work for you?reply"
    ],
    "link": "https://pyrefly.org/blog/numpy-type-completeness/",
    "first_paragraph": "Because NumPy is one of the most downloaded packages in the Python ecosystem, any incremental improvement can have a large impact on the data science ecosystem. In particular, improvements related to static typing can improve developer experience and help downstream libraries write safer code. We'll tell the story about how we (Quansight Labs, with support from Meta's Pyrefly team) helped bring its type-completeness score to nearly 90% from an initial 33%.TL;DR:Modern IDEs use type annotations to help developers by showing them helpful suggestions and highlighting syntax. Pyright is a popular type-checker which, as well as checking for correctness and consistency, can also measure what percentage of a library's public API has type annotations. We call the percentage of fully-typed symbols exported by a library the type-completeness score.For example, a module which exports functions foo and bar:would have a 50% type-completeness score, because:By changing the foo signature to be def fo"
  },
  {
    "title": "I almost got hacked by a 'job interview' (daviddodda.com)",
    "points": 716,
    "submitter": "DavidDodda",
    "submit_time": "2025-10-15T12:56:12 1760532972",
    "num_comments": 383,
    "comments_url": "https://news.ycombinator.com/item?id=45591707",
    "comments": [
      "This article is so interesting, but I can\u2019t shake the feeling it was written by AI. The writing style has that feel for me.Maybe that shouldn\u2019t bother me? Like, maybe the author would never have had time to write this otherwise, and I would never have learned about his experience.But I can't help wishing he'd just written about it himself. Maybe that's unreasonable--I shouldn't expect people to do extra work for free. But if this happened to me, I would want to write about it myself...reply",
      "It\u2019s incredibly annoying to read. So many super short sentences with the \u201cnot just X. Also Y\u201d format. Little hooks like \u201cThe attack vector?\u201d\u201cNot fancy security tools. Not expensive antivirus software. Just asking my coding assistant\u2026\u201dI actually feel like AI articles are becoming easier to spot. Maybe we\u2019re all just collectively noticing the patterns.reply",
      "I'm regularly asked by coworkers why I don't run my writing through AI tools to clean it up and instead spend a time iterating over it, re-reading, perhaps with a basic spell checker and maybe grammar check.That's because, from what I've seen to date, it'd take away my voice. And my voice -- the style in which I write -- is my value. It's the same as with art... Yes, AI tools can produce passable art, but it feels soulless and generic and bland. It lacks a voice.reply",
      "It also slopifies your work in a way that's immediately obvious. I can tell with high confidence when someone at work runs their email through ChatGPT and it makes me think less of the person now that I have to waste time reading through an overly verbose email with very little substance to it when they could have just sent the prompt and saved us all the time.reply",
      "I agree.  I use Grammarly for finding outright mistakes (spelling and the like, or a misplaced comma or something), but I don't listen to any of the suggestions for writing.I feel like when I try writing through Grammarly, it feels mechanical and really homogeneous. It's not \"bad\" exactly, but it sort of lacks anything interesting about it.I dunno.  I'm hardly some master writer, but I think I'm ok at writing things that interesting to read, and I feel Grammarly takes that away.reply",
      "I manage an employee from another country and speaks English as a second language. The way they learned English gives them a distinct speaking style that I personally find convincing, precise and engaging. I started noticing their writing losing that voice, so I asked if they were using an LLM and they were. It was a tough conversation because as a native English speaker I have it easy, so I tried to frame my side of the conversation as purely my personal observation that I could see the change in tone and missed the old one. They've modified their use of LLMs to restore their previous style, but I still wonder if I was out of line socially for saying anything. English is tough, and as a manager I have a level of authority that is there even when I think it isn't. I don't know the point, except that I'm glad you're keeping your voice.reply",
      "As a non-native English speaker living in AU, I can offer my opinion in case it's helpful.Of course I can't speak to the person you mentioned but if you said what you did with respect and courtesy then they probably would've appreciated it. I know I would have. To me, there's no problem speaking about and approaching these issues and even laughing about cultural issues, as long as it's done with respect.I once had a manager who told me that a certain client finds the way I speak scary. When I asked why, it turns out that they're not expecting the directness in my speech manner. Which is strange to me since we were discussing implementation and requirements and directness and precision are critical and when they're not... well that's how projects fail, in my opinion. On the other hand, there were times when speaking to sales people left me dizzy from all the spin. Several sentences later and I still had no idea if they actually answered the question. I guess that client was expecting more of the latter. Extra strange since that would've made them spend more money than they have to.Now running my own business, I have clients that thank me for my directness. Those are the ones that have had it with sales people that think doing sales is by agreeing to everything the client says and promising delivery of it all and then just walking away leaving the client with a bigger problem than the one they started with.reply",
      "I often ask for ai to give only grammar and spelling corrections, and then only a change set I apply manually. In other words the same functionality as every word processor since\u2026y2k?reply",
      "Why not just use one of those word processors, then? It seems like you'd expend less effort (unless there's an advantage of your approach that I'm missing), since the proof-reading systems built into a Word processor have a built-in queue UI with integrated accept / reject functionality that won't randomly tweak other parts of the paragraph behind your back.reply",
      "Far better at catching some types of mistakes. Word only has this many hardcoded rules past the basic grammar. LLMs operate on semantics, and pick up on errors like \"the sentence is grammatically correct, but uses an obviously wrong term, given the context\".reply"
    ],
    "link": "https://blog.daviddodda.com/how-i-almost-got-hacked-by-a-job-interview",
    "first_paragraph": ""
  },
  {
    "title": "Gerald Sussman - An Electrical Engineering View of a Mechanical Watch (2003) (techtv.mit.edu)",
    "points": 38,
    "submitter": "o4c",
    "submit_time": "2025-10-06T10:46:12 1759747572",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=45489911",
    "comments": [
      "For anyone with a sudden hankering for \"what does that mechanical bit do\", an interactive exhibit:https://ciechanow.ski/mechanical-watch/reply",
      "Valid link: https://videolectures.net/videos/mitworld_sussman_clockreply",
      "I remember watching MIT lectures in OpenCourseWare back in 2007, before I started at university. What an amazing resource it was at the time.reply",
      "I did the same thing, at the same time! I'm pretty self-directed when it comes to learning, so access to the raw materials of a course was great and sufficient. Never missed interactivity. I tried Coursera years later, it was much more like enrolling in a real class virtually.But yeah, great to see OCW still going strong. It's pretty remarkable no administrator has tried to mess with it -- although I wouldn't know if they had.reply",
      "Wrong TLS cert.reply",
      "I don't know about you, but boy, do I feel secure.reply",
      "What a beautiful lecturereply"
    ],
    "link": "https://techtv.mit.edu/videos/15895-an-electrical-engineering-view-of-a-mechanical-watch",
    "first_paragraph": ""
  },
  {
    "title": "Zed is now available on Windows (zed.dev)",
    "points": 173,
    "submitter": "meetpateltech",
    "submit_time": "2025-10-15T16:24:29 1760545469",
    "num_comments": 57,
    "comments_url": "https://news.ycombinator.com/item?id=45594920",
    "comments": [
      "I'm so impressed by how quickly this team can ship new features. It seems like every few weeks there's a new major update!reply",
      "Have they implemented subpixel font rendering by now? I remember that being a sticking point when it came to Linux because they had designed their custom UI renderer around the Macs ubiquitous HiDPI displays, leading to blurry fonts for the much, much larger proportion of Linux (and Windows) users who still use LoDPI displays.reply",
      "Idk about subpixel font rendering, but font rendering on Linux looks massively better after a patch last week: https://github.com/zed-industries/zed/issues/7992#issuecomme...reply",
      "That \"after\" image is still rendered with greyscale AA rather than subpixel, but whatever they changed did make it more legible at least.reply",
      "I'm glad there's finally some progress in that direction. If they actually implement subpixel RGB anti-aliasing, it would definitely be worth considering as an alternative. It's been surprising to see so many people praise Zed when its text rendering (of all things) has been in such a state for so long.reply",
      "I tried it out on macOS and have a 1440p external monitor that the fonts just look horrible on. Looks fine on the laptop's \"retina\" display but blurry enough every else that it actually gave me a headache after a few hours.reply",
      "No lolreply",
      "Nice but too little/too late, already switched to Linux - where Zed already works great!reply",
      "[Window Title]\n  Critical\n\n  [Main Instruction]\n  Unsupported GPU\n\n  [Content]\n  Zed uses DirectX for rendering and requires a compatible GPU.\n\n  Currently you are using a software emulated GPU (Microsoft Basic Render Driver) which\n  will result in awful performance.\n\n  For troubleshooting see: https://zed.dev/docs/windows\n  Set ZED_ALLOW_EMULATED_GPU=1 env var to permanently override.\n\n\n  [Skip] [Troubleshoot and Quit]\n\nAh bummer.reply",
      "That's more of an issue with your system than an issue with Zed, you have to veer pretty far from the beaten path to not have proper DirectX nowadays. Are you running Windows in a VM?reply"
    ],
    "link": "https://zed.dev/blog/zed-for-windows-is-here",
    "first_paragraph": "Max BrunsfeldOctober 15th, 2025Zed is now available on Windows. You can download the stable release here. Or if you prefer to live on the bleeding edge, you can use the preview release, which receives new features one week earlier.Windows is now a fully supported platform for Zed. We'll be shipping updates every week, like we do with Mac and Linux. Several Zed engineers use Windows as their daily driver, and we will maintain a full-time Windows team, including @localcc, our Windows platform lead.Read on to learn about the key Windows features.Zed isn't an Electron app; we integrate directly with the underlying platform for maximal control. The Windows build uses DirectX 11 for rendering, and DirectWrite for text rendering, to match the Windows look and feel.Zed integrates directly with Windows Subsystem for Linux (WSL). From the WSL terminal, you can open a folder in Zed using the zed command-line script. And from within Zed, you can open a folder in any of your WSL distros by clicking"
  },
  {
    "title": "Are hard drives getting better? (backblaze.com)",
    "points": 125,
    "submitter": "HieronymusBosch",
    "submit_time": "2025-10-15T17:18:13 1760548693",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=45595724",
    "comments": [
      "I have a 13 years old NAS with 4x1TB consumer drives with over 10y head flying hours and 600,000 head unloads. Only 1 drive failed at around 7 years. The remaining 3 are still spinning and pass the long self test. I do manually set the hdparm -B and -S to balance head flying vs unloads, and I keep the NAS in my basement so everything is thermally cool.\nI'm kinda of hoping the other drives will fail so I can get a new NAS but no such luck yet :-(reply",
      "I admire the \"use it until it dies\" lifestyle. My NAS is at 7 years and I have no plans to upgrade anytime soon!reply",
      "The problem with setting a nearly maintenance free nas is that you tend to forget about it just running away in the background.Then a drive fails spectacularly.And that's the story of how I thought I lost all our home movies. Luckily the home movies and pictures were backed up.reply",
      "So is that high usage compared to backblaze?Is the 10y head flying for each head? Is it for heads actually reading/writing, or just for spinning drives/aloft heads?I only skimmed the charts, they seemed to just measure time/years, but not necessarily drive use over time.reply",
      "I'm curious what this data would look like collated by drive birth date rather than (or in 3D addition to) age.  I wouldn't use that as the \"primary\" way to look at things, but it could pop some interesting bits.  Maybe one of the manufacturers had a shipload of subpar grease?  Slightly shittier magnets?  Poor quality silicon?  There's all kinds of things that could cause a few months of hard drive manufacture to be slightly less reliable\u2026(Also: \"Accumulated power on time, hours:minutes 37451*:12, Manufactured in week 27 of year 2014\" \u2014 I might want to replace these :D \u2014 * pretty sure that overflowed at 16 bit, they were powered on almost continuously & adding 65536 makes it 11.7 years.)reply",
      "Over the past couple of years, I've been side hustling a project that requires buying ingredients from multiple vendors. The quantities never work out 1:1, so some ingredients from the first order get used with some from a new order from a different vendor. Each item has its own batch number which when used together for the final product yields a batch number on my end. I logged my batch number with the batch number for each of the ingredients in my product. As a solo person, it is a mountain of work, but nerdy me goes to that effort.I'd assume that a drive manufacture does similar knowing which batch from which vendor the magnets, grease, or silicon all comes from. You hope you never need to use these records to do any kind of forensic research, but the one time you do need it makes a huge difference. So many people doing similar products that I do look at me with a tilted head while their eyes go wide and glaze over as if I'm speaking an alien language discussing lineage tracking.reply",
      "I think it's helpful to put on our statistics hats when looking at data like this... We have some observed values and a number of available covariates, which, perhaps, help explain the observed variability. Some legitimate sources of variation (eg, proximity to cooling in the NFS box, whether the hard drive was dropped as a child, stray cosmic rays) will remain obscured to us - we cannot fully explain all the variation. But when we average over more instances, those unexplainable sources of variation are captured as a residual to the explanations we can make, given the avialable covariates. The averaging acts a kind of low-pass filter over the data, which helps reveal meaningful trends.Meanwhile, if we slice the data up three ways to hell and back, /all/ we see is unexplainable variation - every point is unique.This is where PCA is helpful - given our set of covariates, what combination of variables best explain the variation, and how much of the residual remains? If there's a lot of residual, we should look for other covariates. If it's a tiny residual, we don't care, and can work on optimizing the known major axes.reply",
      "Exactly. I used to pore over the Backblaze data but so much of it is in the form of \u201cwe got 1,200 drives four months ago and so far none have failed\u201d. That is a relatively small number over a small amount of time.On top of that it seems like by the time there is a clear winner for reliability, the manufacturer no longer makes that particular model and the newer models are just not a part of the dataset yet. Basically, you can\u2019t just go \u201cHitachi good, Seagate bad\u201d. You have to look at specific models and there are what? Hundreds? Thousands?reply",
      "> On top of that it seems like by the time there is a clear winner for reliability, the manufacturer no longer makes that particular model and the newer models are just not a part of the dataset yet.That's how things work in general. Even if it is the same model, likely parts have changed anyway. For data storage, you can expect all devices to fail, so redundancy and backup plans are key, and once you have that set, reliability is mostly just a input into your cost calculations. (Ideally you do something to mitigate correlated failures from bad manufacturing or bad firmware)reply",
      "I find it more straight forward to just model the failure rate with the variables directly, and look metrics like AUC for out of sample data.reply"
    ],
    "link": "https://www.backblaze.com/blog/are-hard-drives-getting-better-lets-revisit-the-bathtub-curve/",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Halloy \u2013 Modern IRC client (github.com/squidowl)",
    "points": 280,
    "submitter": "culinary-robot",
    "submit_time": "2025-10-15T11:45:47 1760528747",
    "num_comments": 77,
    "comments_url": "https://news.ycombinator.com/item?id=45590949",
    "comments": [
      "I'd love to test this, however it seems to not be accessible with screen readers. I assume this is because of the GUI library not supporting accessibility. I found an open issue about this on the Iced GitHub where in 2024 it was mentioned that the version after next should support it, and the last comment was in february of this year (https://github.com/iced-rs/iced/issues/552)I bookmarked this so hopefully once that effort gets further along I can give it a try!I figured I'd leave this comment so that some folks can see that there are real people even on HN who require these features and that accessibility work is always appreciated. We definitely exist :)reply",
      "Iced also has a roadmap: https://whimsical.com/roadmap-iced-7vhq6R35Lp3TmYH4WeYwLMScreen reader accessibility is at least on it, although not until the release after next.reply",
      "I've tried to use this, but I'm on multiple servers with tons of channels, and it gets a bit unwieldy without tabs. I also can't get it to minimize to tray, and having to \"keep it open\" at all times is somewhat annoying. I'll stick with Quassel for now.Really impressive work though, you should be proud!reply",
      "I added this configuration to make it works more tab-like.    [actions.sidebar]\n    buffer = \"replace-pane\"reply",
      "What channels are still active? I connected to freenode for the first time in years a few months ago and it was a ghost town. Would love to get back into some programming/tech communities on IRC.reply",
      "https://www.vice.com/en/article/major-internet-projects-are-...A lot of users left freenode in 2021.Many moved to Libera.chat instead. While others may have moved on from IRC altogether when they left freenode.reply",
      "Freenode melted down a few years back, there was a lot of drama around what happened, but I don't think it's active anymore and if it is, it's probably not what you remember. The community splintered and moved to other hosts.reply",
      "tabs are the reason i abandoned my halloy experiment. I'm currently following the halloy issue.reply",
      "Agreed on the tabs. Not sure what I'm supposed to do when I have more than 1-2 channels I want to view.reply",
      "to have it act a bit more tabishly, https://halloy.chat/configuration/actions.html#buffer can be set to \"replace-pane\"reply"
    ],
    "link": "https://github.com/squidowl/halloy",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        IRC application written in Rust\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Halloy is an open-source IRC client written in Rust, with the Iced GUI library. It aims to provide a simple and fast client for Mac, Windows, and Linux platforms.Join #halloy on libera.chat if you have questions or looking for help.Installation documentationHalloy is also available from Flathub and Snap Store.Halloy is released under the GPL-3.0 License. For more details, see the LICENSE file.For any questions, suggestions, or issues, please open an issue on the GitHub repository.\n        IRC application written in Rust\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page. There was an error while loading. Ple"
  },
  {
    "title": "Pwning the Nix ecosystem (ptrpa.ws)",
    "points": 236,
    "submitter": "SuperShibe",
    "submit_time": "2025-10-15T13:41:44 1760535704",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=45592401",
    "comments": [
      "This is a great example of why `pull_request_target` is fundamentally insecure, and why GitHub should (IMO) probably just remove it outright: conventional wisdom dictates that `pull_request_target` is \"safe\" as long as branch-controlled code is never executed in the context of the job, but these kinds of argument injections/local file inclusion vectors demonstrate that the vulnerability surface is significantly larger.At the moment, the only legitimate uses of `pull_request_target` are for things like labeling and auto-commenting on third-party PRs. But there's no reason for these actions to have default write access to the repository; GitHub can and should be able to grant fine-grained or (even better) single-use tokens  that enable those exact operations.(This is why zizmor blanket-flags all use of `pull_request_target` and other dangerous triggers[1]).[1]: https://docs.zizmor.sh/audits/#dangerous-triggersreply",
      "I don't disagree... but, there is a use case for orgs that don't allow forks. Some tools do their merging outside of github and thus allow for PRs that cannot be clean from a merge perspective. This won't trigger workflows that are pull_request. Because pull_request requires a clean merge. In those cases pull_request_target is literally the only option.The best move would be for github to have a setting for allowing the automation to run on PRs that don't have clean merges, off by default and intended for use with linters only really. Until that happens though pull_request_target is the only game in town to get around that limitation. Much to my and other SecDevOps engineers sadness.NOTE: with these external tools you absolutely cannot do the merge manually in github unless you want to break the entire thing. It's a whole heap of not fun.reply",
      "That's a fantastic use case that should be supported discretely!reply",
      "Why github didn't is beyond me. Even if something isn't merge clean doesn't mean linters shouldn't be run. I get not running deployments etc. but not even having the option is pain.reply",
      "Inside private repos we use pull_request_target because 1. it runs the workflow as it exists on main and therefore provides a surface where untampered with test suites can run, and 2. provides a deterministic job_workflow_ref in the sub claim in the jwt that can be used for highly fine grained access control in OIDC enabled systems from the workflowreply",
      "Private repos aren't as much of a concern, for obvious reasons.However, it's worth noting that you don't (necessarily) need `pull_request_target` for the OIDC credential in a private repo: all first-party PRs will get it with the `pull_request` event. You can configure the subject for that credential with whatever components you want to make it deterministic.reply",
      "You\u2019re right! I edited my comment to clarify I was talking about good ole job_workflow_ref.reply",
      "This is what GitHub says about it:> This event runs in the context of the base of the pull request, rather than in the context of the merge commit, as the pull_request event does. This prevents execution of unsafe code from the head of the pull request that could alter your repository or steal any secrets you use in your workflow.Which is comical given how easily secrets were exilfiltrated.reply",
      "Yeah, I think that documentation is irresponsibly misleading: it implies that (1) attacker code execution requires the attacker to be able to run code directly (it doesn't, per this post), and (2) that checking out at the base branch somehow stymies the attacker, when all it does is incentivizes people to check out the attacker-controlled branch explicitly.GitHub has written a series of blog posts[1] over the years about \"pwn requests,\" which do a great job of explaining the problem. But the misleading documentation persists, and has led to a lot of user confusion where maintainers mistakenly believe that any use of `pull_request_target` is somehow more secure than `pull_request`, when the exact opposite is true.[1]: https://securitylab.github.com/resources/github-actions-prev...reply",
      "This attack surface is essentially unfixed for almost a year now.Remember the python packages that got pwned with a malicious branch name that contained shellshock like code? Yeah, that incident.I blogged about all vulnerable variables at the time and how the attack works from a pentesting perspective [1].[1] https://cookie.engineer/weblog/articles/malware-insights-git...reply"
    ],
    "link": "https://ptrpa.ws/nixpkgs-actions-abuse",
    "first_paragraph": "last year at nixcon, me and my friend lexi gave a lightning talk about how we found a vulnerability in nixpkgs that would have allowed us to pwn pretty much the entire nix ecosystem and inject malicious code into nixpkgs. it only took us about a day from starting our search to reporting it and getting it fixed. since i unfortunately was too sick to attend this years nixcon, i thought it might be a good time to write up what we found and how we did it.github actions is a ci/cd system by github that can do pretty much anything in a repo. it\u2019s an easy target for attackers because if you have access to a workflow, you can just commit code without authorization and then you have a supply chain attack. plus, it\u2019s all written in yaml \ud83c\uddf3\ud83c\uddf4, which was NEVER meant to be executed !!this is a simple example of a github action. nothing fancy, just running some commands when code is pushed.actions run when a trigger activates them. there are a bunch of different triggers like pushes, commits, or pull "
  },
  {
    "title": "Leaving serverless led to performance improvement and a simplified architecture (unkey.com)",
    "points": 291,
    "submitter": "vednig",
    "submit_time": "2025-10-15T11:20:35 1760527235",
    "num_comments": 181,
    "comments_url": "https://news.ycombinator.com/item?id=45590756",
    "comments": [
      "As someone who worked with serverless for multiple years (mostly amazon lambda but others too) i can absolutly apporove the authors points.While it \"takes away\" some work from you, it adds this work on other points to solve the \"artificial induced problems\".Another example i hit was a hard upload limit. Ported an application to a serverless variant, had an import API for huge customer exports. Shouldnt be a problem right? Just setup an ingest endpoint and some background workers to process the data.Tho than i learned : i cant upload more than 100mb at a time through the \"api gateway\" (basically their proxy to invoke your code) and when asking if i could change it somehow i just was told to tell our customers to upload smaller file chunks.While from a \"technical\" perspective this sounds logical, our customers not gonne start exchanging all their software so we get a \"nicer upload strategy\".For me this is comparable with \"it works in a vacuum\" type of things. Its cool in theory, but as soon it hits reality you will realice quite fast that the time and money you safed on changing from permanent running machines to serverless, you will spend in other ways to solve the serverless specialities.reply",
      "The way to work around this issue is to provide a presigned S3 urlHave the users upload to s3 directly and then they can either POST you what they uploaded or you can find some other means of correlating the input (eg: files in s3 are prefixed with the request id or something)I agree this is annoying and maybe I\u2019ve been in AWS ecosystem for too long.However having an API that accepts an unbounded amount of data is a good recipe for DoS attacks, I suppose the 100MB is outdated as internet has gotten faster but eventually we do need some limitreply",
      "Well i partly agree, and if i would be the one building the counterpart, i prolly had used presigned s3 urls also.In this specific case im getting oldschool file upload request from software that was partly written before the 2000s - noones gonne adjust anything any more.And ye, just accepting giant size uploads is far from good in terms of \"Security\" like DoS - but ye we talking about stupidly somewhere between 100 and 300mb CSV files (called them \"huge\" because in terms of product data 200-300mb text include quite alot) - not great but well we try to satisfy our customers needs.But ye like all the other points - everything is solvable somehow - just needs us to spend more time to solve something that technickly wasn't a real problem in first place.Edit: Another funny example. In a similar process on another provider i downloaded files in a similar size range from S3 to parse them - which died again and again. After contacting the hoster, because their logs litearlly just stopped no error tracing nothing) they told me that basically their setup only allows for 10mb local storing - and the default (in this case aws s3 adapter for PHP) always downloads it even if you tell it to \"stream\". So i build a solution that used HTTP ranged requests to \"fake stream\" the file into memory in smaller chunks so i could process it afterwards without completely download it. Just another example of : yes its solvable, but annoying.reply",
      "I find with these types of customers it\u2019s always easier to just ask them to save files locally and grant me privileges to read the data. Sometimes they\u2019ll be on Google, Dropbox, Microsoft, etc and I also run a SFTP for this in case they want to move them over to my service.Then I either batch/schedule the processing or give them an endpoint to just to trigger it (/data/import?filename=demo.csv)It\u2019s actually so common that I just have the \u201cdata exchange\u201d conversation and let them decide which fits their needs best. Most of it is available for self service configuration.reply",
      "Uploads to an S3 bucket can trigger a lambda\u2026 don\u2019t complicate things. The upload trigger can tell the system about the upload and the client can continue on their day.Uploader on the client uses presigned url. S3 triggers lambda. Lambda function takes file path and tells background workers about it either via queue, mq, rest, gRPC, or doing the lift in workflow etl functions.Easy peasy. /sreply",
      "> Uploads to an S3 bucket can trigger a lambda\u2026 don\u2019t complicate things.I read this and was getting ready to angrily start beating my keyboard. The best satire is hard to detect.reply",
      "I don't really get the joke.  S3 triggering a lambda doesn't sound meaningfully more complicated than using a lambda by itself.  What am I missing?reply",
      "It gets really complex in this workflow to even achieve something like \u201cfile coprocessor successfully\u201d on the client side with this approachhow will your client know if you backend lambda crashed or whatever? All it knows is the upload to s3 succeededBasically you\u2019re turning a synchronous process into asynchronousreply",
      "Solving a serverless limitation with more serverless so you can continue doing serverless when you can\u2019t FormUpload a simple 101mb zip file as an application/octet-stream. Doubling down on it for a triple beat.reply",
      "I wouldn't really call it \"more\" severless to rearrange the order a bit.  Which makes it \"solving a serverless limitation so you can continue doing severless\".  And  that's just a deliberately awkward way of saying \"solving a serverless limitation\" because if you can solve it easily why would you not continue?  Spite?So I still don't see how it's notably worse than the idea of using serverless at all.reply"
    ],
    "link": "https://www.unkey.com/blog/serverless-exit",
    "first_paragraph": ""
  },
  {
    "title": "More About Jumps Than You Wanted to Know (gpfault.net)",
    "points": 7,
    "submitter": "nice_byte",
    "submit_time": "2025-10-09T17:23:25 1760030605",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://gpfault.net/posts/asm-tut-4.html",
    "first_paragraph": "published on Jul 20 2025\n\r\nThis post is a part of a series on x86-64 assembly programming.\r\nCheck out parts 0,\r\n1,\r\n2 and\r\n3!\r\n.\r\n\n\r\nThe header image shows the #2 most-liked\r\npiece of\r\ninternet communication ever made by the author, which \r\nhumorously demonstrates the importance of this post's subject matter.\r\n\n\r\nIt's wild to think that I started this series almost five years ago. Back then,\r\nthe intention was to share my notes from revitalizing my own knowledge of\r\nx86 assembly programming, and provide a starting point for anyone who'd\r\nlike to explore that topic further: an intro to the environment, tools, and a\r\nbrief overview of a subset of the x86-64 ISA. The latter was supposed to include\r\na chapter on conditional instructions and controlling program flow, but I never\r\ngot around to it, until now! \r\n\r\nAs it often happens with these things, I found that simple, naturally arising\r\nquestions demanded rather in-depth answers, which I would prefer to not skip.\r\nAs a result, this post "
  },
  {
    "title": "F5 says hackers stole undisclosed BIG-IP flaws, source code (bleepingcomputer.com)",
    "points": 130,
    "submitter": "WalterSobchak",
    "submit_time": "2025-10-15T13:33:27 1760535207",
    "num_comments": 64,
    "comments_url": "https://news.ycombinator.com/item?id=45592271",
    "comments": [
      ">F5 disclosed that nation-state hackersSomething about this statement screams that companies are setting themselves up for free money from big old gov'ment welfare titties.  I keep seeing it pop up again and again and it only makes sense in that context.Its the boogyman like terrorism.  We need infinite money to fight the bad guys.reply",
      "> I keep seeing it pop up again and again and it only makes sense in that context.Not saying that these companies would turn down corporate welfare given the chance, but I\u2019ll offer an alternative explanation: it shifts accountability away from the company by positing a highly resourced attacker the company could not reasonably be expected to protect against.If you have a physical security program that you\u2019ve spent millions of dollars on, and a random drug addict breaks in and steals your deepest corporate secrets people are going to ask questions.If a foreign spy does the same, you have a bit more room to claim there\u2019s nothing you could have done to prevent the theft.I\u2019ve seen a bunch of incident response reports over the years.  It is extremely common for IR vendors to claim that an attack has some hallmark or another of a nation-state actor.  While these reports get used to fund the security program, I always read those statements as a \u201cget out of jail free\u201d card for the CISOs who got popped.reply",
      ">it shifts accountability awayI agree.  I think what we are split on is purpose/intent.>could not reasonably be expected to protect against.Why not?  If I'm hiring a cybersec thats probably in my top 3 reasons to hire them, if not them then who?   Number one is probably compliance/regulation.> \u201cget out of jail free\u201dThis is one of my red flags I also keep seeing.  Whoops we can't do the thing we say we do.  The entire sec industry seems shady AF.  Which is why I think they are a huge future rent seek lobby.  Once the insurance industry catches on.> these reports get used to fund the security programSo we agree?reply",
      "> I agree. I think what we are split on is purpose/intent.I\u2026 don\u2019t think so?  Your original comment was that companies claim nation state attack as a way to get government funding.  That has nothing to do with assessing blame for an attack.> Why not? If I'm hiring a cybersec thats probably in my top 3 reasons to hire them, if not them then who?If you think you as a private entity can defend against a tier 1 nation state group like the NSA or Unit 8200, you are gravely mistaken.  For one thing, these groups have zero day procurement budgets bigger than most company market caps.That\u2019s why companies reflexively blame nation state actors.  It isn\u2019t to get government funding.  It is to avoid blame for an attack by framing it as something they could not have prevented.> So we agree?No, I don\u2019t believe we do.reply",
      "This is a mean-spirited interpretation of what happens when you claim nation state.Generally the government (as of now) is not paying private (but maybe some Critical Infrastructure companies) companies to secure things. We are in the very early stages of figuring out how to hold companies accountable for security breaches, and part of that is figuring out if they should have stopped it.A lot of that comes down to a few principles:* How resourced is the defender versus the attacker?\n* Who was the attacker (attribution matters - (shoutout @ImposeCost on Twitter/X)\n* Was the victim of the attack performing all reasonable steps to show the cause wasn't some form of gross negligence.Nation state attacker jobs aren't particularly different from many software shops.* You have teams of engineers/analysts whose job it is to analyze nearly every piece of software under the sun and find vulnerabilities.* You have teams whose job it is to build the infrastructure and tooling necessary to run operations* You have teams whose job it is to turn vulnerabilities into exploits and payloads to be deployed along that infrastructure* You have teams of people whose job it is to be hands on keyboard running the operation(s)Depending on the victim organization, if a top-tier country wants what you have, they are going to get it and you'll probably never know.F5 is, at least by q2 revenue[0], we very profitable, well resourced company that has seen some things and been victims of some high profile attacks and vulns over the years. It's likely that they were still outmatched because there's been a team of people who found a weakness and exploited it.When they use verbage like nation-state, it's to give a signal that they were doing most/all the right things and they got popped. The relevant government officials already know what happened, this is a signal to the market that they did what they were supposed to and aren't negligent.[0] -https://www.f5.com/company/news/press-releases/earnings-q2-f...reply",
      "If there was some government program I was previously unaware of that pays organizations that were compromised by nation state hackers then I\u2019m going to be upgrading all my networking infrastructure to F5 products and start reading up on BIG-IP migrations.That is to say, sometimes nation state hackers _were_ behind the compromise. F5 is a very believable and logical target for such groups.reply",
      "Is there an example of a company getting money from the government in response to a statement like this?reply",
      "I don't believe Equifax received money, just a long list of demands to be allowed to continue as a viable business.That it was a nation-state actor may have allowed them some grace, as it didn't result in individuals' details being wholesale sold on the dark web, and the fallout was most-likely a national security issue.It would definitely have helped the CCP target individuals who were vulnerable to recruitment due to their financial status. Especially when combined with the Office of Personnel Management data hack.reply",
      "Nation-states sponsored hackers make up a huge amount of known targeted intrusion groups. This is not some random company tilting at windmills, these are real threats that hit American and American-aligned companies daily.reply",
      "There's huge incentive for nation-state level actors to recruit, train and spend oodles on extremely sophisticated hacking programs with little legal oversight and basically endless resources. I have no idea why you're incredulous about this.If I were running a country practically my highest priority would be cyberattacks and defense. The ability to arbitrarily penetrate even any corporate network, let alone military network, is basically infinite free IP.reply"
    ],
    "link": "https://www.bleepingcomputer.com/news/security/f5-says-hackers-stole-undisclosed-big-ip-flaws-source-code/",
    "first_paragraph": ""
  },
  {
    "title": "Recursive Language Models (RLMs) (alexzhang13.github.io)",
    "points": 67,
    "submitter": "talhof8",
    "submit_time": "2025-10-15T17:43:27 1760550207",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=45596059",
    "comments": [
      "Briefly, an RLM wraps an existing language model (LM) together with an environment that can dynamically manipulate the prompt that will be fed into the LM.The authors use as an environment a Python REPL that itself can call other instances of the LM. The prompt is programmatically manipulated as a Python variable on the REPL.The motivation is for the LM to use Python commands, including commands that call other LM instances, to figure out how best to modify the context at inference time.The results from early testing look impressive at a first glance: An RLM wrapping GPT-5-mini outperforms GPT-5 by a wide margin on long-context tasks, at significant lower cost.I've added this to my reading list.reply",
      "This reminded me of ViperGPT[1] from a couple of years ago, which is similar but specific to vision language models. Both of them have a root llm which given a query produces a python program to decompose the query into separate steps, with  the generated python program calling a sub model. One difference is this model has a mutable environment in the notebook, but I'm not sure how much of a meaningful difference that is.[1] https://viper.cs.columbia.edu/static/viper_paper.pdfreply",
      "My existing project is very similar to this with some other goodies.  I agree with the author that focus on systems versus LLM's is the proper next move. Orchestrating systems that manage multiple different llms and other scripts together can accomplish a lot more then a simple ping pong type of behavior. Though I suspect most people who work with agentic solutions are already quite aware of this. What most in that space haven't cracked yet is the dynamic self modifying and improving system, that should be the ultimate goal for these types of systems.reply",
      "I read the article, and I'm struggling to see what ideas it brings beyond CodeAct (tool use is python) or the \"task\" tool in Claude code (spinning off sub-agents to preserve context).reply",
      "This is old news! Agent-loops are not a model architechturereply",
      "I\u2019m confused over your definition of model architecture.reply",
      "Loops aren\u2019t recursion?reply",
      "Everything old is new again when you are in academiareply",
      "This feels primarily like an issue with machine learning, at least among mathematical subdisciplines. As new people continue to be drawn into the field, they rarely bother to read what has come even a few years prior (nevermind a few decades prior).reply",
      "in today's news: MIT researchers found out about AI agents and rebranded it as RLM for karma.reply"
    ],
    "link": "https://alexzhang13.github.io/blog/2025/rlm/",
    "first_paragraph": "We propose Recursive Language Models (RLMs), an inference strategy where language models can decompose and recursively interact with input context of unbounded length through REPL environments.We explore language models that recursively call themselves or other LLMs before providing a final answer. Our goal is to enable the processing of essentially unbounded input context length and output length and to mitigate degradation \u201ccontext rot\u201d.We propose Recursive Language Models, or RLMs, a general inference strategy where language models can decompose and recursively interact with their input context as a variable. We design a specific instantiation of this where GPT-5 or GPT-5-mini is queried in a Python REPL environment that stores the user\u2019s prompt in a variable.We demonstrate that an RLM using GPT-5-mini outperforms GPT-5 on a split of the most difficult long-context benchmark we got our hands on (OOLONG ) by more than double the number of correct answers, and is cheaper per query on "
  },
  {
    "title": "US Dept of Interior denies canceling largest solar project after axing review (utilitydive.com)",
    "points": 48,
    "submitter": "toomuchtodo",
    "submit_time": "2025-10-15T23:16:07 1760570167",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=45599496",
    "comments": [
      "On top of the fact that they want this cancelled, my guess is that they want to collect bribes (sorry, 'presidential library donations') from each project individually. This also means if they get the bribe and approve then a smaller amount of solar gets built than of they were bribed for project as a whole.reply",
      "> my guess is that they want to collect bribes (sorry, 'presidential library donations')Green account, making an off-topic comment about partisan conspiracies\u2026 and it\u2019s voted to the topreply",
      "Is it a conspiracy if companies are paying it?reply",
      "Are these specific companies paying for said bribes? Or is it still guesses?reply",
      "Previous discussion: https://news.ycombinator.com/item?id=45553487reply",
      "Yeah, ultimately they denied that you could group together all these projects under one environmental review as an exception to normal rules. But the environmentalists are the ones who made rules so onerous that anything worthwhile needs an exception.Environmentalists have stopped nuclear, solar, wind, and geothermal projects in the US. This might make them the number one proponents of climate change here.Remarkable.reply",
      "One article brought quite a lot of complaints from conservationists over the project which i find ironic. As someone else here said if rules exist they should be applied equally to everyone. In this case the biden administration suspended them in order to help the project and now they are being reinstated.reply",
      "You make an argument as if all environmentalists were the same entity. It's really very reductionist, maybe this isn't the right forum for youreply",
      "He literally didn'treply",
      "Oh wait, I can rewrite this for you. Here you go:Yeah, ultimately they denied that you could group together all these projects under one environmental review as an exception to normal rules. But many of the most influential environmental organizations, and a majority of those shaping modern permitting policy, have supported or defended regulatory structures that make large-scale energy projects extremely difficult to approve while also actively using those regulatory structures to kill major projects. Many prominent environmental groups have, at various times, opposed nuclear, solar, wind, or geothermal projects in the US: sometimes over local impacts, sometimes procedural grounds, and sometimes due to broader philosophical objections. Taken together, the net effect of those positions may have been to slow or block clean energy deployment more than almost any other political force, and therefore they may have been the largest policy contributors to climate change in much of the United States.There we go. I think you can probably retune every comment you read with an LLM so that there's lots of hedging in it like this and then you can satisfy your own need for sufficient phatic phrases. But this one is from a human, given to you for free. If you don't like my comments, ask dang to ban me.reply"
    ],
    "link": "https://www.utilitydive.com/news/department-interior-cancels-review-nevada-solar-project-trump/802704/",
    "first_paragraph": ""
  }
]