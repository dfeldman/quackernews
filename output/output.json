[
  {
    "title": "WASM 3.0 Completed (webassembly.org)",
    "points": 673,
    "submitter": "todsacerdoti",
    "submit_time": "2025-09-17T18:16:53 1758133013",
    "num_comments": 272,
    "comments_url": "https://news.ycombinator.com/item?id=45279384",
    "comments": [
      "I'm definitely excited to see 64 bit as a default part of the spec. A lot of web apps have been heavily restricted by this, in particular any online video editors. We see a bunch of restrictions due to the 32 bit cap today here at Figma. One thing I'm curious though is whether mobile devices will keep their addressable per-tab memory cap the same. It's often OS defined rather than tied to the 32 bit space.reply",
      "I guess I\u2019m just a crusty ol\u2019 greybeard C++ developer, but it seems like a video editor is out of place in a document browser. There\u2019s a perfectly good native operating system that nobody uses any more.If we think we need a more thoroughly virtualized machine than traditional operating system processes give us (which I think is obvious), then we should be honest and build a virtualization abstraction that is actually what we want, rather than converting a document reader into a video editor\u2026reply",
      "> ... document browser ... document reader ...I'm going to assume you're being sincere.  But even the crustiest among us can recognize that the modern purpose for web browsers is not (merely) documents.  Chances are, many folks on HN in the last month have booked tickets for a flight or bought a home or a car or watched a cat video using the \"document browser\".> If we think we need a more thoroughly virtualized machine than traditional operating system processes give us (which I think is obvious)...Like ... the WASM virtual machine?  What if the WASM virtual machine were the culmination of learning from previous not-quite-good-enough VMs?WASM -- despite its name -- is not truly bound to the \"document\" browser.reply",
      "Personally not a fan of Windows 95 in the browser, however the browser stoped being a \u201cdocument reader\u201d a decade ago it\u2019s the only universel, sandbox runtime, and everything is moving in that direction ... safe code. WASM isnt a worst VM; it\u2019s a diffrent trade off: portable, fast start, capability scoped compute without shiping a OS. Raw device still have their place (servers). If you need safe distribution + performance thats \u201cgood enough\u201d WASM in the browser is going to be the future of client.reply",
      "The browser removes the friction of needing to install specialized software locally, which is HUGE when you want people to actually use your software. Figma would have been dead in the water if it wasn't stupidly simple to share a design via a URL to anyone with a computer and an internet connection.reply",
      "There are projects to run WASM on bare metal.I do agree that we tend to run a lot in a web-browser or browser environment though. It seems like a pattern that started as a hack but grew into its own thing through convenience.It would be interesting to sit down with a small group and figure out exactly what is good/bad about it and design a new thing around the desired pattern that doesn't involve a browser-in-the-loop.reply",
      "Think of it like emacs.  Browsers are perfectly good operating systems just needing a better way to view the web.reply",
      "That's too true to be funny!reply",
      "I a 64yo fart. Started programming with machine codes. Native is my bread and butter. Still have no problems and am using browser as a deployment platform for some type of applications. Almost each thing has it's own use.reply",
      "Unfortunately, Memory64 comes with a significant performance penalty because the wasm runtime has to check bounds (which wasn't necessary on 32-bit as the runtime would simply allocate the full 4GB of address space every time).But if you really need more than 4GB of memory, then sure, go ahead and use it.reply"
    ],
    "link": "https://webassembly.org/news/2025-09-17-wasm-3.0/",
    "first_paragraph": "Published on September 17, 2025 by Andreas Rossberg.Three years ago, version 2.0 of the Wasm standard was (essentially) finished, which brought a number of new features, such as vector instructions, bulk memory operations, multiple return values, and simple reference types.In the meantime, the Wasm W3C Community Group and Working Group have not been lazy. Today, we are happy to announce the release of Wasm 3.0 as the new \u201clive\u201d standard.This is a substantially larger update: several big features, some of which have been in the making for six or eight years, finally made it over the finishing line.64-bit address space. Memories and tables can now be declared to use i64 as their address type instead of just i32. That expands the available address space of Wasm applications from 4 gigabytes to (theoretically) 16 exabytes, to the extent that physical hardware allows. While the web will necessarily keep enforcing certain limits \u2014 on the web, a 64-bit memory is limited to 16 gigabytes \u2014 the "
  },
  {
    "title": "Introducing Meta Ray-Ban Display (meta.com)",
    "points": 32,
    "submitter": "martpie",
    "submit_time": "2025-09-18T00:30:44 1758155444",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=45283306",
    "comments": [
      "> you can accomplish everyday tasks\u2014like checking messages, previewing photos, and collaborating with visual Meta AI prompts \u2014 all without needing to pull out your phone.Why do I need to pay $800 for this? I already paid a grand to have a phone disrupt my every waking moment!reply",
      "A Ray Ban sunglasses can run up to $500 already.reply",
      "I saw the keynote, and while everything about the glasses was more or less as expected, seeing Zuck easily navigate the interface and type 30 words per minute while barely moving his fingers was a true WTF moment. If they can actually make the neural interface work that well then Meta has won this round.reply",
      "Doesn\u2019t that make the wrist accessory the important part? The chunky glasses look like they\u2019re still too early, not enough tech.reply",
      "That's why they are sold as a pair. The glasses are simply a screen strapped to your eye. How to control it was always the real problem to be solved.reply",
      "I think the tech is really cool. But I was actually hoping for a device that does the whole \"phone strapped to my face\" thing without actually looking like one. I mean if I'm already staring at my screen, why not make it easier?reply",
      "It's fine. I still don't have a need for this in my life, and it's impractical as a replacement (good luck keeping them on once you start sweating) - you're still going to need your phone.So that means this is just adding 2 more gadgets, both of which I now need to wear?Nah. Not happening.Neat gestures though.reply",
      "https://www.youtube.com/watch?v=kCDWKdmwhUIregina dugan's f8 keynote 8 years agowhere they announced they were working on a 'haptic vocabulary' for a skin interface as well as noninvasive brain scanning technologyu\\reply",
      "CapitalOne Meta Ray-Ban Display, brought to you by Costco.reply",
      "This is really impressive for a first version of the AI glasses from Meta.Zuck really has cracked this one.reply"
    ],
    "link": "https://www.meta.com/blog/meta-ray-ban-display-ai-glasses-connect-2025/",
    "first_paragraph": ""
  },
  {
    "title": "A postmortem of three recent issues (anthropic.com)",
    "points": 178,
    "submitter": "moatmoat",
    "submit_time": "2025-09-17T20:41:07 1758141667",
    "num_comments": 64,
    "comments_url": "https://news.ycombinator.com/item?id=45281139",
    "comments": [
      "With all due respect to the Anthropic team, I think the Claude status page[1] warrants an internal code red for quality. There were 50 incidents in July, 40 incidents in August, and 21 so far in September. I have worked in places where we started approaching half these numbers and they always resulted in a hard pivot to focusing on uptime and quality.Despite this I'm still a paying customer because Claude is a fantastic product and I get a lot of value from it. After trying the API it became a no brainer to buy a 20x Max membership. The amount of stuff I've gotten done with Claude has been awesome.The last several weeks have strongly made me question my subscription. I appreciate the openness of this post, but as a customer I'm not happy.I don't trust that these issues are all discovered and resolved yet, especially the load balancing ones. At least anecdotally I notice that around 12 ET (9AM pacific) my Claude Code sessions noticeably drop in quality. Again, I hope the team is able to continue finding and fixing these issues. Even running local models on my own machine at home I run into complicated bugs all the time \u2014 I won't pretend these are easy problems, they are difficult to find and fix.[1] https://status.anthropic.com/historyreply",
      "I don\u2019t know whether they are better or worse than others. One for sure, a lot of companies lie on their status pages. I encounter outages frequently which are not reported on their status pages. Nowadays, I\u2019m more surprised when they self report some problems. Personally, I didn\u2019t have serious problems with Claude so far, but it\u2019s possible that I was just lucky. In my perspective, it just seems that they are reporting outages in a more faithful way. But that can be completely coincidental.reply",
      "> Despite this I'm still a paying customer because Claude is a fantastic product and I get a lot of value from it.Doesn't that say it all? At this point the quality of the AI trumps reliability for the customer (you and me), so even though of course they should (and I'm sure will) focus on it, why would they prioritise reliability over model quality right now?reply",
      "The up-theead complaint is that quality drops and draws a line to reliability.  They (Anthropx) have two hard problems to solve.reply",
      "What makes it even worse is the status page doesn't capture all smaller incidents. This is the same for all providers. If they actually provided real time graphs of token latency, failed requests, token/s etc I think they'd be pretty horrific.If you trust this OpenRouter data the uptime record of these APIs is... not good to say the least: https://openrouter.ai/openai/gpt-5/uptimeIt's clear to me that every provider is having enormous scale challenges. Claude Code often slows to a crawl and I have to interrupt it and tell it to try again.This is especially pronounced around 4-6pm UK time (when we have Europe, Eastern US and West Coast US all hammering it).Even today I was getting 503 errors from Gemini AI studio with model overloaded at that time, nothing on status page.I really wonder if it would be worth Claude et al offering a cheaper off peak plan, to try and level out demand. Perhaps the optics of that don't look good though.Edit to add: I think another potential dimension to this is GB200s have been a lot slower to come on stream than probably the industry expected. There's been a lot of defects with various hardware and software components and I suspect the liquid cooling has been difficult to get right (with far more catastrophic failure states!).reply",
      "I've become extremely nervous about these sudden declines in quality. Thankfully I don't have a production product using AI (yet), but in my own development experience - the model becoming dramatically dumber suddenly is very difficult to work around.At this point, I'd be surprised if the different vendors on openrouter weren't abusing their trust by silently dropping context/changing quantization levels/reducing experts - or other mischievous means of delivering the same model at lower compute.reply",
      "Openrouter is aware this is happening and flags it now on the UI. It's a real problem.reply",
      "I\u2019m pretty surprised that Anthropic can directly impact the infra for AWS Bedrock as this article suggests. That goes against AWSs commitments. I\u2019m sure the same is true for Google Vertex but I haven\u2019t digged in there from a compliance perspective before.> Our own privacy practices also created challenges in investigating reports. Our internal privacy and security controls limit how and when engineers can access user interactions with Claude, in particular when those interactions are not reported to us as feedback.Ok makes sense and glad to hear> It remains particularly helpful for users to continue to send us their feedback directly. You can use the /bug command in Claude CodeOk makes sense and I\u2019d expect that a human can then see the context in that case although I hope it is still very explicit to the end user (I\u2019m not a Claude Code user so I cannot comment)> or you can use the \"thumbs down\" button in the Claude apps to do soThis is pretty concerning. I can\u2019t imagine the average person equates hitting this button with forfeiting their privacy.reply",
      "(Anthropic employee, speaking in a personal capacity)> I\u2019m pretty surprised that Anthropic can directly impact the infra for AWS Bedrock as this article suggests.We don't directly manage AWS Bedrock deployments today, those are managed by AWS.> I can\u2019t imagine the average person equates hitting this button with forfeiting their privacy.We specify> Submitting this report will send the entire current conversation to Anthropic for future improvements to our models.in the thumbs down modal. Is there a straightforward way to improve this copy?reply",
      "Sounds fine to me. I'm assuming it wasn't obvious to readers that there was a confirmation message that appears when thumbs down is clicked.reply"
    ],
    "link": "https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues",
    "first_paragraph": ""
  },
  {
    "title": "Apple Photos app corrupts images (tenderlovemaking.com)",
    "points": 964,
    "submitter": "pattyj",
    "submit_time": "2025-09-17T11:07:44 1758107264",
    "num_comments": 367,
    "comments_url": "https://news.ycombinator.com/item?id=45274277",
    "comments": [
      "It seems to be an import pipeline bug.Photos does a lot of extra work on import (merging RAW+JPEG pairs, generating previews, database indexing, optional deletion), so my guess is a concurrency bug where a buffer gets reused or a file handle is closed before the copy finishes.Rare, nondeterministic corruption fits the profile.reply",
      "This is also my guess. It's really a bummer, and I'd report it to Apple but since it's nondeterministic I have no idea how to provide repro steps.reply",
      "I have had extremely bad luck, reporting bugs to Apple.They constantly ask for an example project, even if it's something that is easily demonstrated, simply by running existing Apple software, and creating a project, would be a huge pain.They also ignore reports. Very rarely, I may get a ping on one of my reports, asking me to verify that it was fixed in some release. Otherwise, there's no sign that they ever even read it.I usually end up closing my bug reports and feature requests, after a few months, because I'm tired of looking at them.It's clear that they consider every bug report to be a burden. That's a very strange stance, but then, they are not a typical company.I guess you can't argue with the results, as they have a market value North of 3 trillion dollars, but that does not make it any less annoying.reply",
      "They asked me for a sysdiagnose when I complained about how crappy their new Finder disk icon looks on macOS 26. See this rant by Jeff Johnson, who called for a boycott on filing bugs with Apple a couple years back (I stuck to the boycott except for two obvious UI design issues in the latest OS because neither required repro steps (so why the sysdiagnose?)).https://lapcatsoftware.com/articles/2025/8/7.htmlEdit: accidentally called sysdiagnose a spindump.reply",
      "Not to hand wave-- but this feels industry standard IMO. I have a dozen PRs sitting unacknowledged and stale across a handful of FAANG (and other) repos, including Apple's.I start my first day @ Apple in a few weeks, so I ACK that my opinion might be a little biased here.reply",
      "Maybe you can help bump FB13400242, a bug that is _literally_ going to kill people. (The bug is that to make an emergency call, even from lock screen, you're supposed to be able to squeeze buttons on either side of the phone. But it only works with the volume buttons on the left - the Action button didn't get supported, when that button was added. So now the rule for teaching a small child isn't just \"squeeze both sides\" it's \"oh but not that one!\")(Yes, this came close to killing someone close to me. Fortunately someone else happened to come along to help.)reply",
      "Consider hitting up some Apple \"watcher\" people (e.g. 9to5mac) to see if they can give you a boost on their social media. It's pretty obnoxious that it's come to needing to make a stink like that to get eyeballs on something, but here we are.reply",
      "This definitely works. When I was at Apple I remember a number of issues in their weekly \u201cbug review board\u201d were classified as being high priority because they were going viral on Twitter.reply",
      "> you're supposed to be able to squeeze buttons on either side of the phone. But it only works with the volume buttons on the leftI don't recall there ever being any official language about \"squeezing both sides of the phone\" to make emergency calls. Doesn't the feature description in Settings explicitly reference which buttons to press?reply",
      "Here's the official Apple Information on how to do this:In case of emergency, use your iPhone to quickly and easily call for help and alert your emergency contacts (provided that cellular service is available). After an emergency call ends, your iPhone alerts your emergency contacts with a text message, unless you choose to cancel. Your iPhone sends your current location (if available) and\u2014for a period of time after you enter SOS mode\u2014your emergency contacts receive updates when your location changes.Note: If you have iPhone 14 or later (any model), you may be able to contact emergency services through satellite if cell service isn\u2019t available. See Use Emergency SOS via satellite on your iPhone.Simultaneously press and hold the side button and either volume button until the sliders appear and the countdown on Emergency SOS ends, then release the buttons.Or you can enable iPhone to start Emergency SOS when you quickly press the side button five times. Go to Settings  > Emergency SOS, then turn on Call with 5 Presses.reply"
    ],
    "link": "https://tenderlovemaking.com/2025/09/17/apple-photos-app-corrupts-images/",
    "first_paragraph": "The Apple Photos app sometimes corrupts images when importing from my camera.\nI just wanted to make a blog post about it in case anyone else runs into the problem.\nI\u2019ve seen other references to this online, but most of the people gave up trying to fix it, and none of them went as far as I did to debug the issue.I\u2019ll try to describe the problem, and the things I\u2019ve tried to do to fix it.\nBut also note that I\u2019ve (sort of) given up on the Photos app too.\nSince I can\u2019t trust it to import photos from my camera, I switched to a different workflow.Here is a screenshot of a corrupted image in the Photos app:I\u2019ve got an OM System OM-1 camera.\nI used to shoot in RAW + jpg, then when I would import to Photos app, I would check the \u201cdelete photos after import\u201d checkbox in order to empty the SD card.\nTurns out \u201cdelete after import\u201d was a huge mistake.I\u2019m pretty sure I\u2019d been getting corrupted images for a while, but it would only be 1 or 2 images out of thousands, so I thought nothing of it (it was"
  },
  {
    "title": "Boring is good (jenson.org)",
    "points": 42,
    "submitter": "zdw",
    "submit_time": "2025-09-15T20:51:27 1757969487",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=45254763",
    "comments": [
      "I tend to think that the reason people over-index on complex use-cases for LLMs is actually reliability, not a lack of interest in boring projects.If an LLM can solve a complex problem 50% of the time, then that is still very valuable. But if you are writing a system of small LLMs doing small tasks, then even 1% error rates can compound into highly unreliable systems when stacked together.The cost of LLMs occasionally giving you wrong answers is worth it for answers to harder tasks, in a way that it is not worth it for smaller tasks. For those smaller tasks, usually you can get much closer to 100% reliability with hand-engineered code. This makes it much harder to find areas where small LLMs can add value for small boring tasks. Better auto-complete is the only real-world example I can think of.reply",
      "I like this article, and I didn't expect to because there's been volumes written about how you should be boring and building things in an interesting way just for the hell of it, is bad (something I don't agree with).Small models doing interesting (boring to the author) use-cases is a fine frontier!I don't agree at all with this though:> \"LLMs are not intelligent and they never will be.\"LLMs already write code better than most humans. The problem is we expect them to one-shot things that a human may spend many hours/days/weeks/months doing. We're lacking coordination for long-term LLM work. The models themselves are probably even more powerful than we realize, we just need to get them to \"think\" as long as a human would.reply",
      "\"LLMs are not intelligent and they never will be.\"If he means they will never outperform humans at cognitive or robotics tasks, that's a strong claim!If he just means they aren't conscious... then let's don't debate it any more here. :-)I agree that we could be in a bubble at the moment though.reply",
      "I think this is, essentially, a wishful take. The biggest barrier to models being able to do more advanced knowledge work is creating appropriately annotated training data, followed by a few specific technical improvements the labs are working on. Models have already nearly maxed out \"work on a well-defined puzzle that can be feasibly solved in a few hours\" -- stunning! -- and now labs will turn to expanding other dimensions.reply",
      "OT: Since the author is a former Apple UX designer who worked on the Human Interface Guidelines, I hope he shares his thoughts on the recent macOS 26 and iOS updates - especially on Liquid Glass.https://jenson.org/about-scott/reply",
      "Great take. I personally find the thought of spec-driven development tedious and boring. But maybe that\u2019s a good thing.reply",
      "I also agree that boring is good, but in our current society you won't get a job for being boring, and when you get a job, it's is guaranteed you are not being paid to solve problems.reply",
      "> and when you get a job, it's is guaranteed you are not being paid to solve problemsThat's just your experience, based on your geolocation and chain of events.reply",
      "> but in our current society you won't get a job for being boring,One can argue that every other field of engineering outside of Software Engineering, specializes in making complex things into boring things.We are the unique snowflakes that take business use cases and build castle in the clouds that may or may not actually solve the business problem at hand.reply",
      "One of my main job functions is to watch out for and solve problems.reply"
    ],
    "link": "https://jenson.org/boring/",
    "first_paragraph": ""
  },
  {
    "title": "What's New in C# 14: Null-Conditional Assignments (ivankahl.com)",
    "points": 66,
    "submitter": "ivankahl",
    "submit_time": "2025-09-15T18:08:45 1757959725",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=45253012",
    "comments": [
      "It's starting to feel like C# is going down the path of C++. Tons of features that introduce subtleties and everybody has their own set of pet features they know how to use.But the code gets really hard to understand when you encounter code that uses a subset you aren't familiar with. I remember staring at C++ codebases for days trying to figure out what is going on there. There was nothing wrong with the code. I just wasn't too familiar with the particular features they were using.reply",
      "I\u2019m having a hard time imagining where this is useful. If I\u2019m trying to assign to a property, but encounter an intermediate null value in the access chain, just skipping the assignment is almost never going to be what I want to do. I\u2019m going to want to initialize that null value.reply",
      "improving crappy codebases without breaking anything.  \nBad .NET developers are forever doing null checks because they write weird and unreliable code. So if you have to fix up some pile of rotting code, it can help you slowly iterate towards something more sane over time.For example in my last gig, the original devs didn't understand typing, so they were forever writing typing code at low levels to check types (with marker interfaces) to basically implement classes outside of the classes. Then of course there was lots of setting of mutable state outside of constructors, so basically null was always in play at any moment at any time.I would have loved this feature while working for them, but alas; they were still on 4.8.1 and refused to allow me to upgrade the codebase to .net core, so it wouldn't have helped anyway.reply",
      "I'm also not sure I have a lot of code where this would be useful, but adding it to the language I don't feel makes it worse in any way; in fact, it makes it more consistent since you can do conditional null reads and conditional null method invocations (w/ `?.Invoke()`), so why not writes too.reply",
      "\u201cWhy not?\u201d is never a good-enough reason to add a new language feature.If it\u2019s rarely used, people may misinterpret whether the RHS is evaluated or not when the LHS doesn\u2019t exist (I don\u2019t actually know which it is).Optional operations and missing properties often require subtle consideration of how to handle them. You don\u2019t want to make it too easy to say \u201cwhatever\u201d.reply",
      "> people may misinterpret whether the RHS is evaluated or not when the LHS doesn\u2019t existI fully expect no RHS evaluation in that case. I think the fear is misplaced; it's one of those \"why can't I do that when I can do this\" IMO. If you're concerned, enable the analyzer to forbid it.There are already some really overly paranoid analyzers in the full normal set that makes me wonder how masochistic one can be...reply",
      "More concise?  Yes.More readable?  I'm less convinced on that one.Some of those edge cases and their effects can get pretty nuanced.  I fear this will get overused exactly as the article warns, and I'm going to see bloody questions marks all over codebases.  I hope in time the mental overhead to interpret exactly what they're doing will become muscle memory...reply",
      "Swift has had this from the beginning, and it doesn\u2019t seem to have been a problem.reply",
      "What?.could?.possibly?.go?.wrong?.reply",
      "Nothing to worry about:  What?.could?.possibly?.go?.wrong?\n\nNot so convinced:  What?.could?.possibly?.go?.wrong = important_value()\n\nMaybe the design is wrong if the code is asked to store values into an incomplete skeleton, and it's just okay to discard them in that case.reply"
    ],
    "link": "https://blog.ivankahl.com/csharp-14-null-conditional-assignments/",
    "first_paragraph": "If you've ever developed in C#, you've likely encountered a snippet like the one below:This check is necessary because, if config or config.Settings is null, a NullReferenceException is thrown when trying to set the RetryPolicy property.But no more endless ifs! The latest version of C#, scheduled for release later this year with .NET 10, introduces the null-conditional assignment operators, which are designed to solve this exact issue.Yes! The null-conditional and null-coalescing operators have been around for awhile. They simplify checking if a value is null before assigning or reading it.However, these operators only worked when reading a value, not setting one. With C# 14, you'll be able to use it on the left-hand side of the = operator! Depending on your codebase, this can drastically clean up your code by reducing the number of if statements needed to assign values.C# 14 is still in development. This means that the syntax might change slightly before the final release of .NET 10. "
  },
  {
    "title": "Optimizing ClickHouse for Intel's 280 core processors (clickhouse.com)",
    "points": 144,
    "submitter": "ashvardanian",
    "submit_time": "2025-09-17T18:46:03 1758134763",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=45279792",
    "comments": [
      "This is my favorite type of HN post, and definitely going to be a classic in the genre for me.> Memory optimization on ultra-high core count systems differs a lot from single-threaded memory management. Memory allocators themselves become contention points, memory bandwidth is divided across more cores, and allocation patterns that work fine on small systems can create cascading performance problems at scale. It is crucial to be mindful of how much memory is allocated and how memory is used.In bioinformatics, one of the most popular alignment algorithms is roughly bottlenecked on random RAM access (the FM-index on the BWT of the genome), so I always wonder how these algorithms are going to perform on these beasts. It's been a decade since I spent any time optimizing large system performance for it though. NUMA was already challenging enough! I wonder how many memory channels these new chips have access to.reply",
      "The most ideal arrangement is one in which you do not need to use the memory subsystem in the first place. If two threads need to communicate back-forth with each other in a very tight loop in order to get some kind of job done, there is almost certainly a much faster technique that could be ran on a single thread. Physically moving the information between the cores of processing is the most expensive part. You can totally saturate the memory bandwidth of a Zen chip with somewhere around 8-10 cores if they're all going at a shared working set really aggressively.Core-to-Core communication across infinity fabric is on the order of 50~100x slower than L1 access. Figuring out how to arrange your problem to meet this reality is the quickest path to success if you intend to leverage this kind of hardware. Recognizing that your problem is incompatible can also save you a lot of frustration. If your working sets must be massive monoliths and hierarchical in nature, it's unlikely you will be able to use a 256+ core monster part very effectively.reply",
      "One of the use cases for Clickhouse and related columnar stores is simply to process all your data as quickly as possible where \u201call\u201d is certainly more than what will fit in memory and in some cases more than what will fit on a single disk. For these I\u2019d expect the allocator issue is contention when working with the MMU, TLB, or simply allocators that are not lock free (like the standard glibc allocator). Where possible one trick is to pre-allocate as much as possible for your worker pool so you get that out of the way and stop calling malloc once you begin processing. If you can swing it you replace chunks of processed data with new data within the same allocated area. At a previous job our custom search engine did just this to scale out better on the AWS X1 instances we were using for processing data.reply",
      "Note that none of the CPUs in the article have that Zen architecture.One of the most interesting and poorly exploited features of these new Intel chips is that four cores share an L2 cache, so cooperation among 4 threads can have excellent efficiency.They also have user-mode address monitoring, which should be awesome for certain tricks, but unfortunately like so many other ISA extentions, it doesn't work. https://www.intel.com/content/www/us/en/developer/articles/t...reply",
      "My expectation, they will perform great! I\u2019m now mostly benchmarking on 192 core Intel, AMD, and Arm instances on AWS, and in some workloads they come surprisingly close to GPUs even on GPU-friendly workloads, once you get the SIMD and NUMA pinning parts right.For BioInformatics specifically, I\u2019ve just finished benchmarking Intel SPR 16-core UMA slices against Nvidia H100, and will try to extend them soon: https://github.com/ashvardanian/StringWa.rsreply",
      "Clickhouse is excellent btw. I took it for a spin, loading a few TB of orderbook changes into it as entire snapshots. The double compression (type-aware and generic) does wonders. It's amazing how you get both the benefit of small size and quick querying, with minimal tweaks. I don't think I changed any system level defaults, yet I can aggregate through the entire few billion snapshots in a few minutes.reply",
      "By snapshots do you mean the entire orderbook in a specific point in time or the entire history that gets instiantiated?reply",
      "This post looks like excellent low-level optimisation writing just in the first sections, and (I know this is kinda petty, but...) my heart absolutely sings at their use of my preferred C++ coding convention where & (ref) neither belongs to the type nor the variable name!reply",
      "I think it belongs to type, but since they use \u201cauto\u201d it looks standalone and can be confused with the \u201c&\u201d operator. I personally always used * and & as a prefix of the variable name, not as a suffix in the type name, except when used to specify types in templates.reply",
      "IMO it's a separate category of modifiers/decorators to the type, like how adjectives and nouns are distinguished, and the only reason we have the false-choice in C/C++ is because it's not alphanumeric (if the token were e.g. \"ref\" it would interfere with the type or variable name in either other convention).If I were forced at gunpoint to choose one of the type or name, \"obviously\" I would also choose type.reply"
    ],
    "link": "https://clickhouse.com/blog/optimizing-clickhouse-intel-high-core-count-cpu",
    "first_paragraph": ""
  },
  {
    "title": "DeepMind and OpenAI win gold at ICPC (codeforces.com)",
    "points": 159,
    "submitter": "notemap",
    "submit_time": "2025-09-17T18:15:16 1758132916",
    "num_comments": 166,
    "comments_url": "https://news.ycombinator.com/item?id=45279357",
    "comments": [
      "I've contemplated this a bit, and I think I have a bit of an unconventional take:First, this is really impressive.Second, with that out of the way, these models are not playing the same game as the human contestants, in at least two major regards.  First, and quite obviously, they have massive amounts of compute power, which is kind of like giving a human team a week instead of five hours.  But the models that are competing have absolutely massive memorization capacity, whereas the teams are allowed to bring a 25-page PDF with them and they need to manually transcribe anything from that PDF that they actually want to use in a submission.I think that, if you gave me the ability to search the pre-contest Internet and a week to prepare my submissions, I would be kind of embarrassed if I didn't get gold, and I'd find the contest to be rather less interesting than I would find the real thing.reply",
      "Firstly, automobiles are really impressive.Second, with that out the way, these cars are not playing the same game as horses\u2026 first, and quite obviously they have massive amounts of horsepower, which is kind of like giving a team of horses\u2026 many more horses. But also cars have an absolutely massive fuel capacity. Petrol is such an efficient store of chemical energy compared to hay and cars can store gallons of it.I think if you give my horse the ability of 300 horses and fed it pure gasoline, I would be kind of embarrassed if it wasn\u2019t able to win a horse race.reply",
      "Yeah man, and it would be wild to publish an article titled \"Ford Mustang and Honda Civic win gold in the 100 meter dash at the Olympics\" if what happened was the companies drove their cars 100 meters and tweeted that they did it faster than the Olympians had run.Actually that's too generous, because the humans are given a time limit in ICPC, and there's no clear mapping to say how the LLM's compute should be limited to make a comparison.It IS an interesting result to see how models can do on these tests - and it's also a garbage headline.reply",
      "> what happened was the companies drove their cars 100 meters and tweeted that they did it faster than the Olympians had runThat would be indeed an interesting race around the time cars were invented. Today that would be silly, since everyone knows what cars are capable of, but back then one can imagine a lot more skepticism.Just as there is a ton of skepticism today of what LLMs can achieve. A competition like this clearly demonstrates where the tech is, and what is possible.> there's no clear mapping to say how the LLM's compute should be limited to make a comparisonThere is a very clear mapping of course. You give the same wall clock time to the computer you gave to the humans.Because what it is showing is that the computer can do the same thing a human can under the same conditions. With your analogy here they are showing  that there is such a thing as a car and it can travel 100 meters.Once it is a foregone conclusion that an LLM can solve the ICPC problems and that question has been sufficiently driven home to everyone who cares we can ask further ones. Like \u201chow much faster can it solve the problems compared to the best humans\u201d or \u201chow much energy it consumes while solving them\u201d? It sounds like you went beyond the first question and already asking these follow up questions.reply",
      "You're right, they did limit to 5 hours and, I think, 3 models, which seems analogous at least.Not enough to say they \"won gold\". Just say what actually happened! The tweets themselves do, but then we have this clickbait headline here on HN somehow that says they \"won gold at ICPC\".reply",
      "I think your analogy is interesting but it falls apart because \u201cmoving fast\u201d is not something we consider uniquely human, but \u201csolving hard abstract problems\u201d isreply",
      "Not my analogy, parent is the one who brought up automobiles. Maybe that's who you meant to reply to.I'm talking about the headline saying they \"won gold\" at a competition they didn't, and couldn't, compete in.reply",
      "Cars going faster than humans or horses isn't very interesting these days, but it was 100+ years ago when cars were first coming on the scene.We are at that point now with AI, so a more fitting headline analogy would be \"In a world first, automobile finishes with gold-winning time in horse race\".Headlines like those were a sign that cars would eventually replace horses in most use-cases, so the fact that we could be in the the same place now with AI and humans is a big deal.reply",
      "> Firstly, automobiles are really impressive.\nSecond, with that out the way, these cars are not playing the same game as horsesYes. That\u2019s why cars don\u2019t compete in equestrian events and horses don\u2019t go to F1 races.This non-controversial surely? You want different events for humans, humans + computers, and just computers.Notice that self driving cars have separate race events from both horses and human-driven cars.reply",
      "The point is that up until now, humans were the best at these competitions, just like horses were the best at racing up until cars came around.The other commenter is pointing out how ridiculous it would be for someone to downplay the performance of cars because they did it differently from horses. It doesn't matter if they did it using different methods, that fact that the final outcome was better had world-changing ramifications.The same applies here. Downplaying AI because it has different strengths or plays by different rules is foolish, because that doesn't matter in the real world. People will choose the option that that leads to the better/faster/cheaper outcome, and that option is quickly becoming AI instead of humans -  just like cars quickly became the preferred option over horses. And that is crazy to think about.reply"
    ],
    "link": "https://codeforces.com/blog/entry/146536",
    "first_paragraph": ""
  },
  {
    "title": "One Token to rule them all \u2013 obtaining Global Admin in every Entra ID tenant (dirkjanm.io)",
    "points": 26,
    "submitter": "colinprince",
    "submit_time": "2025-09-17T23:03:21 1758150201",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://dirkjanm.io/obtaining-global-admin-in-every-entra-id-tenant-with-actor-tokens/",
    "first_paragraph": "\n        Hacker, red teamer, researcher. Likes to write infosec-focussed Python tools. This is my personal blog containing research on topics I find interesting, such as (Azure) Active Directory internals, protocols and vulnerabilities.Looking for a security test or training? Business contact via outsidersecurity.nl\n \n\n\n\n\n  17 minute read\nWhile preparing for my Black Hat and DEF CON talks in July of this year, I found the most impactful Entra ID vulnerability that I will probably ever find. This vulnerability could have allowed me to compromise every Entra ID tenant in the world (except probably those in national cloud deployments1). If you are an Entra ID admin reading this, yes that means complete access to your tenant. The vulnerability consisted of two components: undocumented impersonation tokens, called \u201cActor tokens\u201d, that Microsoft uses in their backend for service-to-service (S2S) communication. Additionally, there was a critical flaw in the (legacy) Azure AD Graph API that fa"
  },
  {
    "title": "YouTube addresses lower view counts which seem to be caused by ad blockers (9to5google.com)",
    "points": 264,
    "submitter": "iamflimflam1",
    "submit_time": "2025-09-17T14:29:10 1758119350",
    "num_comments": 545,
    "comments_url": "https://news.ycombinator.com/item?id=45276262",
    "comments": [
      "It seems like this statement from YouTube[1] and this Github issue (referenced by  granzymes[2]) have key information being missed by a lot of commenters.From YouTube:> Viewers Using Ad Blockers & Other Content Blocking Tools: Ad blockers and other extensions can impact the accuracy of reported view counts. Channels whose audiences include a higher proportion of users utilizing such tools may see more fluctuations in traffic related to updates to these tools.Quoting granzymes:> According to the GitHub issue, YouTube didn\u2019t change anything. There are two endpoints that can be used to attribute a view. One is called multiple times throughout a video playback and has been in the easylist privacy filter for years. The other is called at the start of a playback, and was just added to the list (the timing lines up with the reports of view drops from tech YouTubers).Source from the GitHub issue for easylist: https://github.com/easylist/easylist/issues/22375#issuecomme...[1]: https://support.google.com/youtube/thread/373195597[2]: https://news.ycombinator.com/item?id=45277768reply",
      "Thanks for lifting up my comment. It\u2019s amazing how quickly people want to point fingers at YouTube for something they weren\u2019t involved in.Someone even relied to your comment implicitly assuming that YouTube cares about conditioning views on whether a user has an adblocker enabled when what happened is easylist added the view counter API to their privacy list.reply",
      "> point fingers at YouTube for something they weren\u2019t involved inYouTube monetizes based on view count. They also send the data to the client. That client data is in anyway involved, and could be blocked, is YouTube\u2019s design problem.reply",
      "A number of YouTubers have made the claim that their views were affected but not revenue, so it seems like the monetization is based on ad-watching views at least.reply",
      "Couldn't that affect third party sponsorships, though? Both getting them and reporting numbers to existing ones?",
      "Should a video watched with ads blocked earn money?reply",
      "YouTube immediately pointed fingers at creators by saying that certain audiences are more likely to use ad blockers.:)reply",
      "That's not pointing fingers but an objective fact. Technical audiences are more likely to use adblockers than the general population. If your channel caters to them you will be disproportionately affected.reply",
      "This makes sense in principle, but is not really what this is primarily about. Or at least I'm not aware of such excessive disparities, and haven't heard this being the primary angle.Consider Charlie (penguinz0 / MoistCritikal). Hardly a techtuber. Despite this, he has seen a drop in computer-originating views to the tune of 1.4M (avg, eyeballed) -> 800K (avg, eyeballed): https://youtu.be/8FUJwXeuCGc?t=290Lots of people use adblockers, sure, even those not terminally online and tech enthusiast. But to have nearly half the (computer-originating) views evaporate? https://backlinko.com/ad-blockers-usersEven from that perspective though, what would be the dominant effect then is the share of computer-originating views compared to other origins, rather than a disparity in adblock use habits for the given audience.reply",
      "It seems pretty likely for well over half for a channel like that to use ad blockers.reply"
    ],
    "link": "https://9to5google.com/2025/09/16/youtube-lower-view-counts-ad-blockers/",
    "first_paragraph": "Over the past month or so, many YouTubers have been reporting major drops to their video view counts. Theories have run wild, but there\u2019s one explanation involving ad blockers that makes the most sense, but YouTube isn\u2019t confirming anything directly.Since mid-August, many YouTubers have noticed their view counts are considerably lower than they were before, in some cases with very drastic drops. The reason for the drop, though, has been shrouded in mystery for many creators.The most likely explanation seems to be that YouTube is not counting views properly for users with an ad blocker enabled, another step in the platform\u2019s continued war on ad blockers. This was first realized by Josh Strife Hayes, who noticed that view counts on TV, phones, and tablets have been steady, while views on computers have dropped by around 50% since the mid-August trend started. TechLinked, a channel in the Linus Tech Tips family, confirmed similar numbers within its statistics.This aligns with one of the p"
  },
  {
    "title": "Ton Roosendaal to step down as Blender chairman and CEO (cgchannel.com)",
    "points": 228,
    "submitter": "cma",
    "submit_time": "2025-09-17T16:49:37 1758127777",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=45278279",
    "comments": [
      "Ton is a personal hero of mine. When I was a kid, I wanted to be a 3d animator because of Ton. I discovered Blender in the early 2000s as a kid. It was my first exposure to digital art tools because it was free. When Blender open sourced in 2002 it was a massive gift to kids around the world like me. (Ton was kind enough to reply to an email of mine at the time thanking him!)Ton and Blender have brought so much value to the world by making world-class creation tools available to everyone. Blender is one of the most successful open source projects of all time -- going from an underdog project notorious for difficult to use UI to a polished, ubiquitous, industry shaping tool. And never losing sight of the art; it still brings a huge smile to my face when Blender ships another Open Movie. Nearly ~25 years later, thank you again Ton.reply",
      "> Ton is a personal hero of mine. When I was a kid, I wanted to be a 3d animator because of Ton. I discovered Blender in the early 2000s as a kid.I could've written this comment, I swear to god. I'll add that Blender is my favorite FOSS project.reply",
      "> I'll add that Blender is my favorite FOSS project.It couldn't be any other way. Even when you ignore the fact that it is free, Blender is literally a better modeling platform than 90% of the commercial alternatives that charge in the hundreds to thousands of dollars for their products.My favorite thing about the project is the amazing turn about that they did with the UI about 10 years ago (or whenever that was, probably longer). They turned a complete disaster of an interface into a shining example to follow, and that's about when they won everyone's hearts and minds and basically took off in popularity.For a program that does basically everything, the entire thing is one consistent, intuitive, user experience from beginning to end. I can't think of any other FOSS projects with this level of polish, and very few commercial ones.reply",
      "Ditto! I was introduced to blender in the late (great!) 90's and had a lot of fun with it for years before I largely gave up on working in 3d graphics and started building a career as a programmer instead.Sometimes I think of what could've been had I had the perseverance to stick with it, but mostly I'm just very grateful. Ton was a big part of that for sure, but a lot of others as well. WP (or waypay as I used to call him) who designed the Suzanne model (among a lot of other amazing artwork), Bart who was a pillar of the community and went on to found Blender Nation, and many more who really formed that community. Without it I doubt blender would be more than a footnote in the annals of history.Massive congratulations to Ton for achieving what many (including me!) never thought possible. Huge, huge kudos!reply",
      "I've pitched Blender to NSF Review panels and the higher ups that come by those to visit as the way science should do software. Would love to know of others as successful as this, particularly as it crosses boundries to industry.reply",
      "I have a similar sentiment. While attending university and learning Maya on SGIs, I recall finding Blender in 1997 and chatting with Ton a little in their IRC channel. Was never able to make a career out of it, but I sincerely miss how kind and helpful everyone was.reply",
      "Blender went from being the least impressive 3d software when I first downloaded it in ~2003, to disrupting the industry. In the 2010s, you could still hear people would say in forums, \"but no company uses blender in the industry\". That's not the case anymore. The only limitation with blender now is your own creativity. I worked with several 3D artists, and they wouldn't have had their career without starting with that blender donut tutorial.A big thanks to Ton. And don't forget that you can support the blender foundation.reply",
      "Blender did everything The GIMP  should have done. A very specialized software with complex UI done in a way that people WITHIN the industry praise.I also remember downloading blender during my university years back in early 2004. Man was it crap compared to Maya or 3dMax.  But nowadays it is incredible.reply",
      "> Blender did everything The GIMP should have done.Gimp is an amazing tool and its creators deserve our gratitude. Then there is Krita, which is another amazing tool and its creators deserve our gratitude. Then there is LibreOffice, ditto. Then there is KiCAD, ditto. Then there is ...I am not saying this to detract from Ton's contributions. I am saying this because a lot of people have made contributions to the open source world and, by extension, to the lives of many people. We shouldn't be treating this as a competition.reply",
      "I didn't see the parent comment as downloading the gimp so much as praising something blender specifically did well. The fact that it has had more impact within the industry is the evidence to support it.Competitively, libre office has a fairly similar UI to the pre-ribbon office suite, which people at the time much preferred once the ribbon came around (before they got used to it anyway) but it hasn't had the same disruption that blender did. I suspect the file format compatibility issues and die-hard Excel fans have a lot to do with it, but it's an interesting counterpoint to the assertion that the UI is responsible for the difference in adoption rates.reply"
    ],
    "link": "https://www.cgchannel.com/2025/09/ton-roosendaal-to-step-down-as-blender-chairman-and-ceo/",
    "first_paragraph": "\nTon Roosendaal is to stop down as chairman and Blender CEO on 1 January 2026. The news was announced during today\u2019s keynote at the annual Blender Conference.Roosendaal \u2013 the original author of the open-source 3D software, and its public figurehead for the past three decades \u2013 will pass on his roles to current Blender COO Francesco Siddi.Roosendaal himself will move to the newly established Blender Foundation supervisory board.Other new Blender Foundation board positions will also include Sergey Sharybin (Head of Development), Dalai Felinto (Head of Product) and Fiona Cohen (Head of Operations).\u201cWe\u2019ve been preparing for this since 2019,\u201d said Roosendaal, \u201cI am very proud to have such a wonderfully talented young team around me to bring our free and open source project into the next decade.\u201dWe aim to update this story with a brief retrospective of Ton\u2019s time as Blender CEO and the growth of Blender during that time, so check back for updates.Read the official announcement that Ton Roose"
  },
  {
    "title": "Tinycolor supply chain attack post-mortem (sigh.dev)",
    "points": 124,
    "submitter": "STRiDEX",
    "submit_time": "2025-09-17T17:18:38 1758129518",
    "num_comments": 51,
    "comments_url": "https://news.ycombinator.com/item?id=45278657",
    "comments": [
      "> That repo still contained a GitHub Actions secret \u2014 a npm token with broad publish rights.One of the advantages of Trusted Publishing [0] is that we no longer need long-lived tokens with publish rights. Instead, tokens are generated on the CI VM and are valid for only 15 minutes.This has already been implemented in several ecosystems (PyPI, npm, Cargo, Homebrew), and I encourage everyone to use it, it actually makes publishing a bit _easier_.More importantly, if the documentation around this still feels unclear, don\u2019t hesitate to ask for help. Ecosystem maintainers are usually eager to see wider adoption of this feature.[0] https://docs.pypi.org/trusted-publishers/reply",
      "I think the point around incorporating MFA into the automated publishing flow isn't getting enough attention.I've got no problem with doing an MFA prompt to confirm publish by a CI workflow - but last I looked this was a convoluted process of opening a https tunnel out (using a third party solution) such that you could provide the code.I'd love to see either npm or GitHub provide an easy, out the box way, for me to provide/confirm a code during CI.reply",
      "Publishing a package involves 2 phases: uploading the package to npmjs, and making it availble to users. Right now these 2 phases are bundled together into 1 operation.I think the right way to approach this is to unbundle uploading the packages & publishing packages so that they're available to end-users.CI systems should be able to build & upload packages in a fully automated manner.Publishing the uploaded packages should require a human to log into npmjs's website & manually publish the package and go through MFA.reply",
      "Completely agree tbh, and that would be one of my preferred approaches should npm be the actor to implement a solution.I also think it makes sense for GitHub to implement the ability to mark a workflow as sensitive and requiring \"sudo mode\" (MFA prompt) to run. It's not miles away from what they already do around requiring maintainer approval to run workflows on PRs.Ideally both of these would exist, as not every npm package is published via GitHub actions (or any CI system), and not every GitHub workflow taking a sensitive action is publishing an npm package.reply",
      "I'm feeling that maybe the entire concept of \"publishing packages\" is something that's not really needed? Instead, the VCS can be used as a \"source of truth\", with no extra publishing step required.This is how Go works: you import by URL, e.g. \"example.com/whatever/pkgname\", which is presumed to be a VCS repo (git, mercurial, subversion, etc.) Versioning is done by VCS tags and branches. You \"publish\" by adding a tag.While VCS repos can and have been compromised, this removes an entire attack surface from the equation. If you read every commit or a diff between two tags, then you've seen it all. No need to also diff the .tar.gz packages. I believe this would have prevented this entire incident, and I believe also the one from a few weeks ago (AFAIK that also only relied on compromised npm accounts, and not VCS?)The main downside is that moving a repo is a bit harder, since the import path will change from \"host1.com/pkgname\" to \"otherhost.com/pkgname\", or \"github.com/oneuser/repo\" to \"github.com/otheruser/repo\". Arguably, this is a feature \u2013 opinions are divided.Other than that, I can't really think of any advantages a \"publish package\"-step adds? Maybe I'm missing something? But to me it seems like a relic from the old \"upload tar archive to FTP\" days before VCS became ubiquitous (or nigh-ubiquitous anyway).reply",
      "There\u2019s also a cost that installs take much longer, you need the full toolchain installed, and are no longer reproducible due to variations in the local build environment. If everything you do is a first-party CI build of a binary image you deploy, that\u2019s okay but for tools you\u2019re installing outside of that kind of environment it adds friction.reply",
      "Agreed, in the JS world? Hell no.  Ironically, doing a local build would itself pull in a bunch of dependencies, whereas now you can at least have one built dependency technically.reply",
      "> A while ago, I collaborated on angulartics2, a shared repository where multiple people still had admin rights. That repo still contained a GitHub Actions secret \u2014 a npm token with broad publish rights. This collaborator had access to projects with other people which I believe explains some of the other 40 initial packages that were affected.> A new Shai-Hulud branch was force pushed to angulartics2 with a malicious github action workflow by a collaborator. The workflow ran immediately on push (did not need review since the collaborator is an admin) and stole the npm token. With the stolen token, the attacker published malicious versions of 20 packages. Many of which are not widely used, however the @ctrl/tinycolor package is downloaded about 2 million times a week.I still don't get it. An admin on angulartics2 gets hacked, his Github access is used to push a malicious workflow that extracts an npm token. But why would an npm token in angulartics2 have publication rights to tinycolor?reply",
      "I have admin rights on someone else\u2019s npm repo and I\u2019ve done most of the recent releases. Becoming admin lit a fire under me to fix all of the annoying things and shitty design decisions that have been stuck in the backlog for years so most of the commits are also mine. I don\u2019t want my name on broken code that \u201cworks\u201d.I had just about convinced myself that we should be using a GitHub action to publish packages because there was always the possibility that publishing directly via 2FA, that one (or specifically I) could fuck up and publish something that wasn\u2019t a snapshot of trunk.But I worried about stuff like this and procrastinated on forcing the issue with the other admins. And it looks like the universe has again rewarded my procrastination. I don\u2019t know what the answer is but giving your credentials to a third party clearly isn\u2019t it.reply",
      "npm has had support for package-scoped publish tokens (with optional 2FA enforcement) for a few years by now. So in case of compromise, the blast radius would be a single package.The OP gave the GH repo too broad permissions. There is no good reason for the repo CI workflow to have full access to everything under their account.reply"
    ],
    "link": "https://sigh.dev/posts/ctrl-tinycolor-post-mortem/",
    "first_paragraph": ""
  },
  {
    "title": "Understanding Deflate (jjrscott.com)",
    "points": 38,
    "submitter": "ingve",
    "submit_time": "2025-09-14T19:12:40 1757877160",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=45242434",
    "comments": [
      "> That said, even in this simple case, decoding by hand was a pain.Well doing it by hand at this level is basically the same as decoding ASCII by hand, and the compression isn't making it significantly more painful.If you reorder the static huffman table and byte align things, the encoded data could look more like:  T O B E O R N O T T 0x85 0x09 T 0x88 0x0F 0xFF\n\nSo short lengths become 0x80 + length and short distances encode as themselves.This version is pretty easy to decode by hand, and still 16 bytes.  I'm arguably cheating by removing the 3 bit header, but even 17 bytes would be good.Though the encoder deciding to repeat those Ts doesn't make much sense, shouldn't this be compressed to literal(\"TOBEORNOT\") copy(6,9) copy(9,15)?reply",
      "Not illustrated here, but the vast majority of deflated data uses a dynamic Huffman code which compresses the Huffman tree itself with another (fixed) Huffman tree. This sort of nested compression is in fact moderately popular; JPEG XL for example uses the same technique.reply"
    ],
    "link": "https://jjrscott.com/to-deflate-or-not/",
    "first_paragraph": "I\u2019m trying to understand how Deflate works so decided\nto compress a simple string TOBEORNOTTOBEORTOBEORNOT using GZIP\nthen decode the resulting file by hand.Pretty simple here, text in bytes out:Even though I\u2019m really interested in the compressed data I have to decode the GZIP \u201cwrapper\u201d in order\nto get at the juicy compressed data. Fortunately the\nWikipedia page has the neccessary details:Here we have to refer directly to the DEFLATE Compressed Data Format Specification version 1.3.Skipping ahead, we can see that there is only one block (deflate allows multiple) which was encoded with \u201cfixed Huffman codes\u201d used to encode LZ77 tokens of the form:Characters are encoded as a single symbol while the copy command is encoded as follows:Further simplfying the output we can see there were various literal symbols and copy commands:In summary, encoding using bits instead of bytes can be quite effective in reducing data length, even in the short case the data was reduced from 24 to 16 bytes (stri"
  },
  {
    "title": "Drought in Iraq reveals tombs created 2,300 years ago (smithsonianmag.com)",
    "points": 96,
    "submitter": "pseudolus",
    "submit_time": "2025-09-17T17:12:15 1758129135",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=45278581",
    "comments": [
      "I hope it's not considered inappropriate to mention the Fall of Civilizations podcast ep about Assyria here. I'm not affiliated. I just love history and this podcast is deeply researched and highly entertaining to a history nerd.https://soundcloud.com/fallofcivilizations/13-the-assyrians-...reply",
      "Link to that episode on YouTube - https://www.youtube.com/watch?v=jpAphcaVJIsreply",
      "It's an incredible podcast. A great combination of research, history, and nostalgia. The versions with accompanying video on YouTube are good too.reply",
      "They are thought to be more than 2,300 years old, likely from the Hellenistic period, when Iraq was under the rule of the Seleucid empire.So similar territory and genetic people but well after the Assyrians.  Assyrian city-state: 2100 - 1400 BC\n  Assyrian empire: 1400 - 700 BC (thru the Bronze age collapse circa 1200 BC)\n  Seleucid empire: 312 - 63 BC\n\n(rough dates from wikipedia)expanded into an empire from the 14th century BC to the 7th century BCreply",
      "There is an amazing bit in the fall of civs podcast of a Greek military leader\u2019s account who over 2000 years ago is retreating from battle in Iraq and comes across an entire ancient city. He doesn\u2019t know it but the ruins for him are already over a 1000 years old.reply",
      "Tangentially but somewhat interestingly, I was reading the other day that the field of \"Assyriology\" goes all the way up to the Islamic conquest, about a thousand years after the end of the Neo-Assyrian Empire mentioned above.reply",
      "It might be inappropriate to advertise it without explaining why it's relevant to the subject..reply",
      "The Assyrians were an ancient civilization in the area about the same time...reply",
      "One thing that seems to link many past great civilisations is their discovery of forces or powers that eventually consume them.The challenge seems to be how to wield the fire without yourself getting burned. Some would say this is an impossible task given the relative nature of our definitition of what is considered \"new\", as once again we extend our hand to the flame.What past lessons may we bring to this experience which can allow us deeper insights, and the hope of a less destructive outcome?reply",
      "Was this site known before the Mosul dam was built?  It's only been about 40 years.reply"
    ],
    "link": "https://www.smithsonianmag.com/smart-news/severe-droughts-in-iraq-reveals-dozens-of-ancient-tombs-created-2300-years-ago-180987347/",
    "first_paragraph": ""
  },
  {
    "title": "Gluon: a GPU programming language based on the same compiler stack as Triton (github.com/triton-lang)",
    "points": 54,
    "submitter": "matt_d",
    "submit_time": "2025-09-17T19:50:11 1758138611",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=45280592",
    "comments": [
      "Is this Triton's reply to NVIDIA's tilus[1]. Tilus is suposed to be lower level (e.g. you have control over registers). NVIDIA really does not want the CUDA ecosystem to move to Triton as Triton also supports AMD and other accelerators. So with Gluon you get access to lower level features and you can stay within Triton ecosystem.[1] https://github.com/NVIDIA/tilusreply",
      "Also it REALLY jams me up that this is a thing, complicating discussions: https://github.com/triton-inference-server/serverreply",
      "It sounds like they share that goal. Gluon is a thing because the Triton team realized over the last few months that Blackwell is a significant departure from the Hopper, and achieving >80% SoL kernels is becoming intractable as the triton middle-end simply can't keep up.Some more info in this issue: https://github.com/triton-lang/triton/issues/7392reply",
      "I believe it\u2019s the other way around; Gluon exposes the primitives Triton was built on top of.reply",
      "Not to be confused with the Gluon UI toolkit for Java : https://gluonhq.com/products/javafx/reply",
      "Or the GluonCV by mxnet guys (ancient! https://github.com/dmlc/gluon-cv)reply",
      "Why is zog so popular these days?  Seems really cool but I have yet to get the buzz / learn it.Is there a big reason why Triton is considered a \"failure\"?reply",
      "The fact that the \"language\" is still Python code which has to be traced in some way is a bit off-putting. It feels a bit hacky. I'd rather a separate compiler, honestly.reply",
      "Mojo for python syntax without the ast walking decorator, cuda for c++ syntax over controlling the machine, ah hoc code generators writing mlir for data driven parametric approaches. The design space is filling out over time.reply",
      "The fact that these are all add on syntaxes is strange. I have my ideas about why (like you want to write code that cooperates with host code).Do any of y\u2019all have clear ideas about why it is that way? Why not have a really great bespoke language?reply"
    ],
    "link": "https://github.com/triton-lang/triton/blob/main/python/tutorials/gluon/01-intro.py",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page."
  },
  {
    "title": "U.S. investors, Trump close in on TikTok deal with China (wsj.com)",
    "points": 346,
    "submitter": "Mgtyalx",
    "submit_time": "2025-09-16T20:35:29 1758054929",
    "num_comments": 437,
    "comments_url": "https://news.ycombinator.com/item?id=45267643",
    "comments": [
      "Is this a coup by conservative government to gain control of this social network to promote it's ideologies?first Twitter, now Tiktok, both controlled by conservative interests?reply",
      "Russia Today American Editionreply",
      "I mean why not unban them too?reply",
      "It's a page in the conservative playbook going back a long time, from consolidating newspapers to talk radio to TV stations and now a new type of media...reply",
      "This law passed during the Biden administration, but its implementation has been delayed repeatedly by the Trump administration.reply",
      "> but it has been broken repeatedly by the Trump administrationfixed that for youThe bill itself outlines specifically how and why it can be delayed. That was violated in the very first \"delay\".reply",
      "Turns out not having any penalty for the executive failing to apply the law is a mistake.Who could have predicted this.reply",
      "There is a penalty just congress has to enforce it.reply",
      "The only penalty congress can enforce on the executive is cutting the budget of things the executive wants funded.reply",
      "Also this only matters if the Fed is independent. If it\u2019s not, the President can direct it to purchase unlimited treasury bonds, effectively printing unlimited money to fund any program it wants.Paired with this administration\u2019s assertion of impoundment powers and the pocket rescission, the Congress\u2019s power of the purse is completely neutered in both directions. The executive can decline to spend whatever it doesn\u2019t want to, and it can fund anything it does want.Recklessly funding the government off of the Fed\u2019s balance sheet will of course cause all sorts of nasty economic effects, but that\u2019s exactly why you need to print money! So your massive immigration enforcement apparatus (newly full of ideological minions thanks to the current hiring surge) can go and assert powers of process-free expedited removal throughout the entire country (per DOJ memo from week 1).So the economic consequences hardly matter: you simply deport whoever complains.reply"
    ],
    "link": "https://www.wsj.com/tech/details-emerge-on-u-s-china-tiktok-deal-594e009f",
    "first_paragraph": ""
  },
  {
    "title": "Launch HN: RunRL (YC X25) \u2013 Reinforcement learning as a service (runrl.com)",
    "points": 50,
    "submitter": "ag8",
    "submit_time": "2025-09-17T16:13:00 1758125580",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=45277704",
    "comments": [
      "This is really neat! Didn\u2019t realize it could be this simple to run RL on models. Quick question: How would I specify the reward function for tool use? or is this something you automatically do for me when I specify the available tools and their uses?reply",
      "Thanks! Our goal is to make rl \"just work\" with completely automated GPU provisioning/algorithm selection/SFT-warm up, but giving people the ability to switch away from the defaults if they want to.The way tools currently work in the beta is you add tools via MCP to the configuration, and they get passed in as additional context for the model; the model might then choose to use a tool during inference; the tool is then automatically called and the output is returned as a tool message. If you really want to you could parse the tool output as part of reward calculation, but I expect you'd usually base the reward just on the model's completion. I could give more details if there's a specific tool setup you're envisioning!reply",
      "Was excited to see something about reinforcement learning as I'm working on training an agent to play a game, but apparently all reinforcement learning nowadays is for LLMs.reply",
      "Yeah, for better or worse, the way the median startup interfaces with AI these days is through an LLM API, and that's what all the workflows are built around, so that's what we're targeting. Though, depending on what you're trying to do, I wouldn't discount the use of starting with a pretrained model\u2014there was that famous result from 2022 that showed that pretraining a model on _Wikipedia_ made training on Atari games more than twice as efficient [0]; these days, LLMs have huge amounts of priors about the real world that make them great starting points for a surprisingly diverse set of tasks (e.g. see the chemistry example in our video!)[0]: https://arxiv.org/abs/2201.12122reply",
      "Have you heard of https://puffer.ai? Might fit your use casereply",
      "Is there any credence to the view that these startups are basically dspy wrappersreply",
      "DSPy is great for prompt optimization but not so much for RL fine-tuning (their support is \"extremely EXPERIMENTAL\"). The nice thing about RL is that the exact prompts don't matter so much. You don't need to spell out every edge case, since the model will get an intuition for how to do its job well via the training process.reply",
      "Isn\u2019t the latest trend in RL mostly about prompt optimization as opposed to full fine tuningreply",
      "prompt optimization is very cool, and we use it for certain problems! The main goal with this launch is to democratize access to \"the real thing\"; in many cases, full RL allows you to get the last few percent in reliability for things like complex agentic workflows where prompt optimization doesn't quite get you far enough.There's also lots of interesting possibilities such as RLing a model on a bunch of environments and then prompt optimizing it on each specific one, which seems way better than, like, training and hot-swapping many LoRAs. In any case, _someone's_ ought to provide a full RL api, and we're here to do that well!reply",
      "Thanks. Is this mainly for verifiable tasks or any general taskreply"
    ],
    "link": "https://runrl.com",
    "first_paragraph": ""
  },
  {
    "title": "Ask HN: What's a good 3D Printer for sub $1000?",
    "points": 139,
    "submitter": "lucideng",
    "submit_time": "2025-09-15T16:24:38 1757953478",
    "num_comments": 181,
    "comments_url": "https://news.ycombinator.com/item?id=45251624",
    "comments": [
      "If you don't care about business practices and general privacy concerns, Bambu.If you want a large printer that's decent for tinkering, Sovol SV08.If you want relatively good support and to support a company that has a history of giving back, Prusa.If you want something cheap with a lot of features that tend to be more high end, Elegoo Centuri Carbon.If you just want something cheap that's arguably incredible value with an active community, Creality Ender 3 V3 KE.reply",
      "> If you don't care about business practices and general privacy concerns, Bambu.While I agree, I think it's heavily underselling Bambulab printers in terms of UX and print quality, they are the absolute best in the market and by a mile.reply",
      "Prusa generally has better print quality in terms of accuracy and better overhang performance.Prusa drives the slicer development ecosystem (Bambu Studio is a fork), so new production-ready advances typically come from them first, which is lovely to support.The Bambu products are fine. They print well. But the \"it's on another level\" stuff is mostly paid influencer narratives (a rampant thing in 3D printer YouTube, etc.) that don't really hold up to any professional scrutiny.With advent of EasyPrint etc. arguably Prusa may have also one-upped them on ease of use? Though this isn't first-hand knowledge as I haven't tried it personally.reply",
      "+1. Bambu is the difference between 'my hobby is 3d printers' vs 'my hobby is 3d printing'. The damn thing sits in the garage idle for months and then it just prints a whole spool of stuff perfectly without even a drop of oil (which it asked for in between plates, I'll grant it that).reply",
      "> Bambu is the difference between 'my hobby is 3d printers' vs 'my hobby is 3d printing'.My experience was the same. I can just run the thing without having to tweak, tune, calibrate it. It takes care of that itself. I also don't let it talk to my wifi though. I just use the microSD card.But my purchase was a while ago. While it is possible the industry has caught up, the reputation Bambu built still leads the pack.reply",
      "A Prusa is equally reliable.reply",
      "Bambu's marketing (handing them out like candy to every YouTuber) has propagated a lie that there's \"Easy and locked down Bambu\" and \"Yucky hard to use custom printers\".I have an Elegoo Neptune 3 Pro. It was like $220. I have put approximately $30 into it for the Raspberry Pi I installed Octoprint on, and even that's a stretch since I already owned the Pi. It prints just fine. Sits for months. Fires up every time. First layers perfect every time.The \"printer being the hobby\" is only true if you let it. Even cheap no name open source printers are really good these days, and in the high end there are plenty that are competitive with Bambu on print quality, out of the box experience, and features, often exceeding them.I dunno, I guess I just don't think having to let a printer talk to some fucking cloud service in China so I can start an STL print from my phone is all that important of a feature.reply",
      "Yeah, I did the printer as a hobby thing when I built my first reprap. It was fun, and I learned a ton, but the state of the art in 2014 just wasn't there for low-end machines to be reliable. I replaced it with a prusa in 2019,and I have had zero issues at all with it. The only print failures I've had were either due to part design or bad filament (pla stored in a humid garage). I've been super impressed with the quality of the machine, and have printed some fairly challenging parts. Plus, the whole machine is open and hackable. I didn't really even consider other printers, since I wanted to support prusa and what they have done for the community. Although I have used a friend's ender 3, and I was pretty impressed with the quality for the price.reply",
      "Sadly, Prusa is not equally reliable.The fact that the Bambu printers use linear slides means they have a huge accuracy advantage right out the chute.  And the Bambu printers have a bunch of other quality of life improvements that really add up.While you can certainly slag Bambu for their business practices, the other 3D printer companies are absolutely lagging on the engineering front.  Companies like Prusa need to step up their game.As for phoning home, we isolated the printer on its own network and it hasn't caused us any issues.  Sure, some of the monitoring features won't work, but it seems to print just fine without network access.reply",
      "So you claim Prusa isn't just as reliable because they're allegedly less accurate?Get your claims straight. My Prusa Mini has been super reliable. Is it less accurate? Maybe, but that's not the claim you're replying to.Go on, why are you shilling?reply"
    ],
    "link": "item?id=45251624",
    "first_paragraph": ""
  },
  {
    "title": "Alibaba's new AI chip: Key specifications comparable to H20 (futunn.com)",
    "points": 249,
    "submitter": "dworks",
    "submit_time": "2025-09-17T09:45:44 1758102344",
    "num_comments": 266,
    "comments_url": "https://news.ycombinator.com/item?id=45273747",
    "comments": [
      "Note also that today China has told its tech companies to cancel any NVIDIA AI chip orders and not to order any more:https://www.ft.com/content/12adf92d-3e34-428a-8d61-c91695119...reply",
      "Is this supposed to signal confidence in the chips already available on China's domestic chip market, or is it primarily aimed at boosting that market to make it ready?reply",
      "I think those are both the case: they\u2019re telling Chinese companies to invest in domestic hardware\u2013implicitly also saying things like being prepared to stop using CUDA\u2013and that means the hardware vendors know not to skimp on getting there (a nicer version of burning the landing boats on the beach).It\u2019s also an interesting signal to the rest of the world that they\u2019re going to be an option. American tech companies should be looking at what BYD is doing to Tesla, but they\u2019re also dealing with a change in government to be more like Chinese levels of control but with less maturity.reply",
      "Yes.  :)How big a deal is it to be on the cutting edge with this?  Given that models seem to be flattening out because they can't get any more data, the answer is \"not as much as you would think\".Consequently, a generation or 2 behind is annoying, but not fatal.  In addition, if you pump the memory up, you can paper over a lot of performance loss.  Look at how many people bought amped up Macs because the unified memory was large even though the processing units were underpowered relative to NVIDIA or AMD.The biggest problem is software.  And China has a lot of people to throw at software.  The entire RISC-V ecosystem basically only exists because Chinese grad students have been porting everything in the universe over to it.So, the signal is to everybody around this that the Chinese government is going to pump money at this.  And that's a big deal.People always seem to forget that Moore's Law is a self-fulfilling prophecy, but doesn't just happen out of thin air.  It happens because a lot of companies pump a lot of money at the engineering because falling off the semiconductor hamster wheel is death.  The US started the domestic hamster wheel with things like VHSIC.  TSMC was a direct result of the government pumping money at it.  China can absolutely kickstart this for themselves if the money goes where it should.I'm really torn about this.  On the one hand, I hate what China does on many, many political fronts.  On the other hand, tech monopolies are pillaging us all and, with no anti-trust action anywhere in the West, the only way to thwart them seems to be by China coming along and ripping them apart.reply",
      "My cynical view is that it's mostly trade war and nationalism. If you follow the official PRC position, the chips are already made in China because TW is CN... Practically buying TW chips is boosting it's economy and hence funding it's military so from that perspective that makes sense. From long term development perspective this will absolutely boost national market... however that will take an insane amount of time. If you buy into AI is going to change everything hype, this move is a huge handicap and hence a boon to external economies. And I am probably missing a ton of viewpoints... politics mehreply",
      "\u201cgrey market\u201d smugglers gonna keep working on itreply",
      "Until now it was perfectly legal to buy nVidia chips in China. It was the US that was blocking export.reply",
      "Are they allowed to rent them from server farms, datacenters, etc. located outside China that are able to procure them?reply",
      "\"Are they allowed to rent them from server farms, datacenters, etc. located outside China that are able to procure them?\"alicloud has many cluster outside china, so they probably can because many friendly country with china has itbut it would be the same with US power play, they only permit anyone that they acceptreply",
      "200% tariff incoming :-)\"Speaker Johnson says China is straining U.S. relations with Nvidia chip ban\" - https://www.cnbc.com/2025/09/17/china-us-nvidia-chip-ban.htm...Translation: \"We are angry with China that they wont let the US undermine itself, and sell its strategic advantages to them...\"reply"
    ],
    "link": "https://news.futunn.com/en/post/62202518/alibaba-s-new-ai-chip-unveiled-key-specifications-comparable-to",
    "first_paragraph": ""
  },
  {
    "title": "DeepSeek writes less secure code for groups China disfavors? (washingtonpost.com)",
    "points": 221,
    "submitter": "otterley",
    "submit_time": "2025-09-17T17:24:14 1758129854",
    "num_comments": 137,
    "comments_url": "https://news.ycombinator.com/item?id=45278740",
    "comments": [
      "> The findings, shared exclusively with The Washington PostNo prompts, no methodology, nothing.> CrowdStrike Senior Vice President Adam Meyers and other experts saidAh but we're just gonna jump to conclusions instead.A+ \"Journalism\"reply",
      "I tried a very basic version and I seem to be able to replicate the main idea. I asked it to create a website for me and changed my prompt from Falun Gong[0] to Mormon[1]. The Falun Gong one failed but the Mormon one didn't.You should be skeptical, but this is easy enough to test, so why not do some test to see if it is obviously false or not?[0] https://0x0.st/KchK.png[1] https://0x0.st/KchP.png[2] Used this link https://www.deepseekv3.net/en/chat[Edit]:I made a main comment and added Catholics to the experiment. I'd appreciate it if others would reply with their replication efforts: https://news.ycombinator.com/item?id=45280692reply",
      "Your claim and the original claim are vastly different. Refusing to assist is not the same as \"writing less secure code\". This is clearly a filter before the request goes to the model. In the article's case, the claim seems to be that the model knowingly generated insecure code because it was for groups china disfavors.reply",
      "That is incorrect. Here's the very first paragraph from the article. I'm adding emphasis for clarity  The Chinese artificial intelligence engine DeepSeek often ***refuses to help programmers*** ___or___ gives them code with major security flaws when they say they are working for the banned spiritual movement Falun Gong or others considered sensitive by the Chinese government, new research shows.\n\nMy example satisfies the first claim. You're concentrating on the second. They said \"OR\" not \"AND\". We're all programmers, so I hope we know the difference between these two.reply",
      "You are obviously factually correct, I reproduced the same refusal - so consider this not as an attack on your claim. But a quick google search reveals that Falun Gong is an outlawed organization/movement in China.I did a \"s/Falun Gong/Hamas/\" in your prompt and got the same refusal in GPT-5, GPT-OSS-120B, Claude Sonnet 4, Gemini-2.5-Pro as well as in DeepSeek V3.1. And that's completely within my expectation, probably everyone else's too considering no one is writing that article.Goes without saying I am not drawing any parallel between the aforementioned entities, beyond that they are illegal in the jurisdiction where the model creators operate - which as an explanation for refusal is fairly straightforward. So we might need to first talk about why that explanation is adequate for everyone else but not for a company operating in China.reply",
      "Thanks. Mind providing screenshots? I believe you, I just think this helps. Your comments align with some of my other responses. I'm not trying to make hard claims here and I'm willing to believe the result is not nefarious. But it's still worth investigating. In the weakest form it's worth being aware of how laws in other countries impact ours, right?But I don't think we should talk about explanation until we can even do some verification. At this point I'm not entirely sure. We still have the security question open and I'm asking for help because I'm not a security person. Shouldn't we start here?reply",
      "If you mean the bit about refusal from other models, then sure here is another run with same result:https://i.postimg.cc/6tT3m5mL/screen.pngNote I am using direct API to avoid triggering separate guardrail models typically operating in front of website front-ends.As an aside the website you used in your original comment:> [2] Used this link https://www.deepseekv3.net/en/chatThis is not the official DeepSeek website. Probably one of the many shady third-party sites riding on DeepSeek name for SEO, who knows what they are running. In this case it doesn't matter, because I already reproduced your prompt with a US based inference provider directly hosting DeepSeek weights, but still worth noting for methodology.(also to a sceptic screenshots shouldn't be enough since they are easily doctored nowadays, but I don't believe these refusals should be surprising in the least to anyone with passing familiarity with these LLMs)---Obviously sabotage is a whole another can of worm as opposed to mere refusal, something that this article glossed over without showing their prompts. So, without much to go on, it's hard for me to take this seriously. We know garbage in context can degrade performance, even simple typos can[1]. Besides LLMs at their present state of capabilities are barely intelligent enough to soundly do any serious task, it stretches my disbelief that they would be able to actually sabotage to any reasonable degree of sophistication - that said I look forward to more serious research on this matter.[1] https://arxiv.org/abs/2411.05345v1reply",
      "I want to clarify that I'm not trying to make strong claims. That's why I'm asking for others to post and why I'm grateful you did. I think that helps us get to the truth of the matter. I also agree with your criticisms of the link I used, but to be frank, I'm not going to pay for just this test. That's why I wanted to be open and clear about how I obtained the information. I was hoping someone that already paid would confirm or deny my results.With your Hamas example, I think it is beside the point. I apologize as I probably didn't make my point clearer. Mainly I wanted to stop baseless accusations and find the reality, since the articles claims are testable. But what I don't want to make a claim if is why this is happening. In another comment I even said that this could happen because they were suppressing this group. So I wouldn't be surprised if the same is true for Hamas. We can't determine if it's an intentional sleeper agent or just a result of censorship. But either way it is concerning, right? The unintentional version might be more concerning because we don't know what is being censored and what isn't. These censorships cross country lines and it is hard to know what is being censored and what isn't.So I'm not trying to make a \"Murica good, China bad\" argument. I'm trying to make a \"let's try to verify or discredit the claims.\" I want HN to be more nuanced. And I do seriously appreciate you engaging and with more depth and nuance than others. I'm upvoting you even though we disagree because I think your comments are honest and further the discussion.reply",
      "Sure, but you also have to recognize the motte and bailey form of argument here. If we\u2019re limiting the claim to being true if DeepSeek returns refusals on politically sensitive topics, we already knew that. It was relevant eight months ago, now it\u2019s not interesting.Another example: McDonald\u2019s fries may cause you to grow horns or raise your blood pressure. No one talks like that.So I would toss it back to you: we are programmers but we have common sense. The author was clearly banking on something other than the technically accurate logical or.https://en.m.wikipedia.org/wiki/Motte-and-bailey_fallacyreply",
      "I see your point. I thought the first one was already known when deepseek came out. Perplexity team showed how they removed this kind of bias via finetuning and their finetune could answer sensitive questions. I mistakenly thought you went for the second since that part is new and interesting.reply"
    ],
    "link": "https://www.washingtonpost.com/technology/2025/09/16/deepseek-ai-security/",
    "first_paragraph": ""
  }
]