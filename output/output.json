[
  {
    "title": "Qwen3-Omni: Native Omni AI model for text, image and video (github.com/qwenlm)",
    "points": 316,
    "submitter": "meetpateltech",
    "submit_time": "2025-09-22T17:50:21 1758563421",
    "num_comments": 77,
    "comments_url": "https://news.ycombinator.com/item?id=45336989",
    "comments": [
      "Interesting, the pacing seemed very slow when conversing in english, but when I spoke to it in spanish, it sounded much faster.  It's really impressive that these models are going to be able to do real time translation and much more.The Chinese are going to end up owning the AI market if the American labs don't start competing on open weights.  Americans may end up in a situation where they have some $1000-2000 device at home with an open Chinese model running on it, if they care about privacy or owning their data.  What a turn of events!reply",
      "sitting here in the US, reading that China is strongly urging the adoption of Linux and pushing for open CPU architectures like RISC-V and also self-hosted open modelsare we the baddies??reply",
      "This is exactly what I do. I have two 3090s at home, with Qwen3 on it. This is tied into my Home Assistant install, and I use esp32 devices as voice satellites. It works shockingly well.reply",
      "I run Home Assistant on an RPi4 and have an ESP32-based Core2 with mic (https://shop.m5stack.com/products/m5stack-core2-esp32-iot-de...), along with a 16GB 4070 Ti Super in an always-on Windows system I only use for occasional gaming and serving media. I'd love to set up something like you have. Can you recommended a starting place, or ideally, a step-by-step tutorial?I've never set up any AI system. Would you say setting up such a self-hosted AI is at a point now where an AI novice can get an AI system installed and integrated with an existing Home Assistant install in a couple hours?reply",
      "I mean - the AI itself will help you get all that setup.Claude code is your friend.I run proxmox on an old Dell R710 in my closet that hosts my homeassistant (amongst others) VM and then I've setup my \"gaming\" PC (which hasn't done any gaming in quite some time) to dual boot (Windows or Deb/Proxmox) and just keep it booted into Deb as another proxmox node. That PC also has a 4070 Super that I have setup to passthru to a VM and on that VM I've got various services utilizing the GPU. This includes some that are utilized by my hetzner bare metal servers for things like image/text embeddings as well as local LLM use (though, rather minimal due to VRAM constraints) and some image/video object detection stuff with my security cameras (slowly working on a remote water gun turret to keep the racoons from trying to eat the kittens that stray cats keep having in my driveway/workshop).Install claude code (or, opencode, it's also good) - use Opus (get the max plan) and give it a directory that it can use as it's working directory (don't open it in ~/Documents and just start doing things) and prompt it with something as simple as this:\"I have an existing home assistant setup at home and I'd like to determine what sort of self-hosted AI I could setup and integrate with that home assistant install - can you help me get started? Please also maintain some notes in .md files in this working directory with those note files named and organized as you see appropriate so that we can share relevant context and information with future sessions. (example: Hardware information, local urls, network layout, etc) If you're unsure of something, ask me questions. Do not perform any destructive actions without first confirming with me.\"Plan mode. _ALWAYS_ use plan mode to get the task setup, if there's something about the plan you don't like, say no and give it notes - it will return with a new plan. Eventually agree to the plan when it's right - then work through that plan not in plan mode, but if it gets off the plan, get back in plan mode to get the/a plan set and then again let it go and just steer it in regular mode.reply",
      "Seems interesting setup, do you have it documented anywhere, thinking of building one!reply",
      "That's great to hear.  I was mostly impressed with Qwen3 coder on my 4090, but am hobbled by the small memory footprint of the single card.  What motherboard are you using with your 3090s?  Like the others, I too am curious about those esp32s and what software you run on them.Keep up the good hacking - it's been fun to play with this stuff!reply",
      "Ooo interesting, I'd love to hear more about the esp32's as voice satellites!reply",
      "I assume it's very similar to what Home Assistant's backing commercial entity Nabu Casa sells with the \"Home Assistant Voice PE\" device, which is also esp32-based. The code is open and uses the esphome framework so it's fairly easy to recreate on custom HW you have laying around.reply",
      "omg, this is something I've had in mind for quite some time, I even bought some i2s devices to test it out.\nDo you have some pointers on how to do it?reply"
    ],
    "link": "https://github.com/QwenLM/Qwen3-Omni",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Qwen3-omni is a natively end-to-end, omni-modal LLM developed by the Qwen team at Alibaba Cloud, capable of understanding text, audio, images, and video, as well as generating speech in real time.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\n\n\n        \ud83d\udc9c Qwen Chat\u00a0\u00a0 | \u00a0\u00a0\ud83e\udd17 Hugging Face\u00a0\u00a0 | \u00a0\u00a0\ud83e\udd16 ModelScope\u00a0\u00a0 | \u00a0\u00a0\ud83d\udcd1 Blog\u00a0\u00a0 | \u00a0\u00a0\ud83d\udcda Cookbooks\u00a0\u00a0 | \u00a0\u00a0\ud83d\udcd1 Paper\u00a0\u00a0\n\n\ud83d\udda5\ufe0f Hugging Face Demo\u00a0\u00a0 | \u00a0\u00a0 \ud83d\udda5\ufe0f ModelScope Demo\u00a0\u00a0 | \u00a0\u00a0\ud83d\udcac WeChat (\u5fae\u4fe1)\u00a0\u00a0 | \u00a0\u00a0\ud83e\udee8 Discord\u00a0\u00a0 | \u00a0\u00a0\ud83d\udcd1 API\nWe release Qwen3-Omni, the natively end-to-end multilingual omni-modal foundation models. It is designed to process diverse inputs including text, images, audio, and video, while delivering real-time streaming responses in both text and natural speech. Click the video below for more information \ud83d\ude03\n\n\nQwen3-Omni is th"
  },
  {
    "title": "Are We Chasing Language Hype over Solving Real Problems? (dayvster.com)",
    "points": 42,
    "submitter": "ibobev",
    "submit_time": "2025-09-20T13:28:54 1758374934",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=45313211",
    "comments": [
      "No mention of the relicensing going on. All the GNU utilities are GPL licensed.At a time when relicensing and other rug pulls are making headlines I find it disturbing that so many developers are going with \"permissive\" licenses for their recreation of things in Rust.Edit: not technically relicensing, which would not be legal. They are writing functional clones from scratch under a new license.reply",
      "Not relicensing at all, just simply adhering to a spec, as the GNU tools do. Can we get down from the soap box now on rewrites? People are free to reimplement anything if there\u2019s a spec and no law against it. Derivative works only works when there is no public record of a specification prior to an IP introducing it. A copyright. IANAL but I\u2019m pretty sure that\u2019s how that works.If you can prove they copied code, sure, it\u2019s derivative. If you can prove they started from a fork, maybe you have a case. Starting from scratch is about as this is new as it gets.reply",
      "Not sure this person is claiming code is being copied, but instead claiming there's a bad legal footgun happening.History has shown that the original tools used the correct license, these new ones are like optimistic clones leaving vault 13.reply",
      "I have mixed feelings about this. On paper you're of course right and a copyleft license would be better, but in practice I don't think it matters all that much for something as small as coreutils. It's just yet another implementation of a standardized interface, nobody is going to fork it and build a billion-dollar business around it. Any change or improvement is probably worth more when upstreamed than when kept in a proprietary fork, just because your maintenance burden is lessened.Besides, we've evolved a bit since the 90s/00s: the main risk to software freedom is no longer binary blobs but cloud services - and the GPL doesn't save you there. And any license requiring disclosing your coreutils to anyone accessing your computer over the internet would be a massive dealbreaker to most people...reply",
      "Language hype always justifies itself through intellectualizing its own desire.In this incarnation we don't always hear, \"Rust is the future.\"[1] We hear:* \"improving security, ensuring long-term maintainability, and optimizing performance\" \"memory safety, concurrency, and performance\" [2]* \"its ability to prevent common programming errors such as null pointer dereferences and buffer overflows\" [3]* \"is that it's very portable.\" [4]* \"I believe that we should shift the focus away from memory safety (which many other languages also offer) and instead focus on the explicitness, expressiveness, and ecosystem of Rust that is highly competitive\" [5]* \"Better error messages.Providing comprehensive internationalization support (UTF-8). Improved performances\" [6]All of these things are operational and thus quantifiable, and some (perhaps even most), are probably true. But none of them engage with evidence-based decision making. Therefore it's not really about any of these, but that's what we've been doing since the beginning of time.1. Although we do hear this[5].2. https://uutils.github.io/blog/2025-02-extending/3. https://bytegoblin.io/blog/the-power-of-rust-on-linux-a-begi...4. https://lwn.net/Articles/1007907/5. https://corrode.dev/blog/foundational-software/6. https://github.com/uutils/coreutilsreply",
      "The problem is that coreutils isn't done. It is still getting new features - it even had a release today [0]! And if new features are being added, then we're getting new bugs as well. Using Rust eliminates an entire class of rather serious bugs, so in a greenfield environment it would almost certainly be a better choice for a critical project like coreutils.In practice it's of course more complicated. Any rewrite will also introduce a lot of bugs, so short-term the rewrite will probably have a negative result. I think it's a weird decision for Ubuntu to switch to uutils when it is still incomplete and probably buggier than the GNU version, but perhaps in the long run (5+ / 10+ years?) it might turn out to be the better option.It's also important to keep the wider ecosystem in mind. Even if C and Rust were functionally identical, a rewrite could still make sense. Imagine if coreutils was written in a language like COBOL: the language choice would severely restrict the number of potential contributors. Who's going to learn it solely for the thankless job of maintaining boring tooling like coreutils? With the current generation of OSS maintainers slowly dying off, you need to recruit new (younger) developers to take over. If all the kids love Rust and hate C, then perhaps a switch to Rust might be a necessary evil to ensure we'll have well-maintained coreutils for another generation. You still need to be careful to avoid the fad of the day, but I think it's pretty clear by now that Rust is here to stay.As an aside, as [1] shows, the \"it'll at best be equal\" argument is factually incorrect. There is no good reason to believe that GNU coreutils could not possibly be improved upon.[0]: https://www.phoronix.com/news/GNU-Coreutils-9.8[1]: https://www.phoronix.com/news/Rust-Coreutils-0.2.2reply",
      "I understand there are wider issues here but the reason I personally use the new hotness rewrites like fd or rg or zellij is because they are just straight up better. Better interface, prettier presentation. They have the benefit of hindsight behind them which allows them to really solve a lot of problems with the originals that need to maintain backward compatibility. My only complaint is they are not gpl.reply",
      "> as it had basically never had any major security vulnerabilities in its entire existence.This link[0] shows a CVE with CVSS of 9.8 in 2015 (and a handful of smaller ones). From this other page on the same site[1], it claims that all of all 5 of the CVE's are caused by overflow or memory corruption.[0] https://www.cvedetails.com/vulnerability-list/vendor_id-72/p...[1] https://www.cvedetails.com/product/5075/GNU-Coreutils.html?v...reply",
      "I think they are wasting their time\u2026 but it\u2019s their time. They get to decide what a valuable way to spend it is, not me.reply",
      "Improving legacy software is boring; consequently, less people are willing to do it, and those people tend to work slower. Rewriting a legacy program in a new language may take more work, but because it's more interesting, more people who work faster will do it, such that it takes less time overall.reply"
    ],
    "link": "https://dayvster.com/blog/are-we-chasing-language-hype-over-solving-real-problems/",
    "first_paragraph": "As you may have heard or seen, there is a bit of controversy around Ubuntu adopting a rewritten version of GNU Core Utils in Rust. This has sparked a lot of debate in the tech community. This decision by Canonical got me thinking about this whole trend or push of rewriting existing software in Rust which seems to be happening a lot lately.To put it bluntly I was confused by the need to replace GNU Core Utils with a new implementation as GNU Core Utils has been around since arguably the 90s and more realistically 2000s and it has been battle tested and proven to be reliable, efficient, effective and most importantly secure as it had basically never had any major security vulnerabilities in its entire existence. So why then would we deem it necessary to replace it with a new implementation in Rust? Why would anyone go through the trouble of rewriting something that already works perfectly fine and has been doing so for decades? When the end result at best is going to be a tool that does "
  },
  {
    "title": "The Magic Circle inducts Penn and Teller (nytimes.com)",
    "points": 71,
    "submitter": "wbl",
    "submit_time": "2025-09-19T21:51:32 1758318692",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=45307119",
    "comments": [
      "https://archive.is/v7PEM",
      "Gives me Rush vibes when Rolling Stone finally put them on the cover 41 years, 19 albums, and millions of sales and sold out tours later. Not to mention the R&R HOF\u2026reply",
      "The show Fool Us is wonderful\u2026and this clip in particular is my favorite: https://youtu.be/5_KcQt0z-eE?si=xO5gTByzV0spzS2ereply",
      "Even if you never want to practice magic, I highly recommend buying a few Dani DaOrtiz lectures. The way his mind works is phenomenal, and the things he talks about (psychology, how people think, experiences) are applicable across the board.(I rarely perform his tricks... they're brilliant, but they're so perfectly suited for his style that I can't even come close to pulling them off without seeming like a confused idiot. But I love watching him explain what goes into each trick. This specific trick is available on Vanishing.)reply",
      "Great to hear. I'll never forget running into Teller in the lobby of a small show I went to that he was also in the audience for. I sheepishly asked for an autograph and he was very friendly and gracious (and spoke!).reply",
      "These guys are something else. I remember seeing them on Scooby Doo when I was a kid growing up. Their Vegas show is great. I\u2019ve seen several magic shows and specials on TV, but the way they ran the whole thing and let you inspect the stage was super cool.reply",
      "I'm having trouble getting my head around an interview with Teller. We all know he doesn't speak. :)reply",
      "If you go to any of their shows in Vegas, he hangs out after the show and talks with the fans. Felt very weird talking to him.reply",
      "Here's a talk he did at Google where he talks at length:https://www.youtube.com/watch?v=YJRIkTHqTSEreply",
      "He has a speaking role in an old 1987 baseball movie, \"Long Gone\", with William Petersen and Virginia Madsen, as the son in a father/son partnership who own a minor league team (Henry Gibson was the dad).  Very disorienting to hear him talk when I first saw the movie way back when.reply"
    ],
    "link": "https://www.nytimes.com/2025/09/19/arts/penn-teller-magic-circle.html",
    "first_paragraph": ""
  },
  {
    "title": "In Maine, prisoners are thriving in remote jobs, other states are taking notice (mainepublic.org)",
    "points": 151,
    "submitter": "voxadam",
    "submit_time": "2025-09-22T22:51:09 1758581469",
    "num_comments": 122,
    "comments_url": "https://news.ycombinator.com/item?id=45340600",
    "comments": [
      "If we get serious about actual rehabilitation in prisons instead of punishment there\u2019s never been a better time to be able to learn just about anything on your own time. But we\u2019d have to stop dehumanizing criminals. Dehumanization seems to be the trend that the US is leading on right now.We can also be concerned about the incentives for prison labor - for profit prisons and all the many service providers that get paid a mint. Phone calls in many prisons are like $10. Labor gangs and the such. It\u2019s just horrible how badly we treat people in the US for some middleman to make money.reply",
      "There are also perverse electoral incentives to having a prison in your voting district. Generally the prisoners count toward your population numbers but they can\u2019t vote. No pesky three fifths compromise.reply",
      "If I had my 'druthers, disenfranchisement for felonies is anti-democratic nonsense, so people in prison should retain voting rights.The only ethically-hard problem is which jurisdiction their vote should count in, since they cannot demonstrate it by choosing where to live. Perhaps a choice between:1. The location of the prison, if their main interest is the conditions of their detention rather than anything outside.2. The location of their property or close family, because they're still paying property-taxes or school levies etc. and they will be returning there later.reply",
      "I've never understood the not allowing felons to vote, even while incarcerated. Does serving time really mean you should not get the same say in leaders as everyone else? As if being incarcerated isn't punishment enough, but disenfranchising on top just seems over the top.Many people live in an area, but keep their voting registration in another. They are even able to vote without having to return to their registered polling place. Allowing inmates to vote could just as easily be handled the same way.reply",
      "Poor people and minorities are who are in prison. Removing voting rights from those groups is a feature, not a flaw, in my opinion.To be clear, I'm saying it's garbage, but it's garbage very much on purpose.reply",
      "We\u2019re in agreement here. Just like the bail system. Working as intended if not as designed.reply",
      "> I've never understood the not allowing felons to vote, even while incarcerated.It's literally unconscionable in any kid of democracy to me.reply",
      "Should someone convicted of voter fraud be allowed to vote?But I think the laws in some U.S. states do actually allow felons to vote under certain circumstances.reply",
      "I just looked this up earlier, and there are only 2 states that do. Vermont and Maine allow all prisoners to vote. Other states allow some depending on conviction. I was unaware of this. I was aware some states allow felons to vote once released while other states never reinstate that right. That is some heinous shit. No other way to put it",
      "> Should someone convicted of voter fraud be allowed to vote?Yes. Why shouldn't they?reply"
    ],
    "link": "https://www.mainepublic.org/2025-08-29/in-maine-prisoners-are-thriving-in-remote-jobs-and-other-states-are-taking-notice",
    "first_paragraph": "People who are incarcerated are paid notoriously low wages for kitchen, laundry work and maintenance.But the expanded use of laptops is creating other opportunities.This is part two in a two-part series about remote work in Maine prisons. To read part one, click here.Preston Thorpe is only 32, but he says he's already landed his dream job as a senior software engineer and bought a modest house with his six-figure salary. It was all accomplished by putting in long days from his cell at the Mountain View Correctional Center in Charleston.\"It's not normal to have 15-17 hours a day to really focus on something and learn something, like deeply,\" Thorpe says. \"And fortunately tech is one of the few areas where they're not concerned with your college degree. They're really only concerned with your ability to write code.\"\nCommunity support has always powered public media, and the power to keep this vital public service going rests with you. Join us with ongoing support as an Evergreen Friend a"
  },
  {
    "title": "Federal judge lifts administration halt of offshore wind farm in New England (apnews.com)",
    "points": 132,
    "submitter": "zekrioca",
    "submit_time": "2025-09-22T22:45:54 1758581154",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=45340550",
    "comments": [
      "\"Just then they came in sight of thirty or forty windmills that rise from that plain. And no sooner did Don Quixote see them that he said to his squire, 'Fortune is guiding our affairs better than we ourselves could have wished. Do you see over yonder, friend Sancho, thirty or forty hulking giants? I intend to do battle with them and slay them. With their spoils we shall begin to be rich for this is a righteous war and the removal of so foul a brood from off the face of the earth is a service God will bless.'What giants?' asked Sancho Panza.'Those you see over there,' replied his master, 'with their long arms. Some of them have arms well nigh two leagues in length.''Take care, sir,' cried Sancho. 'Those over there are not giants but windmills. Those things that seem to be their arms are sails which, when they are whirled around by the wind, turn the millstone.'\"reply",
      "That sort of instability is so damaging beyond the boundaries of his term. Hopefully this restores a bit of confidence.reply",
      "The Supreme Court is determined to give him unfettered power to do whatever he wants so I'm sure once it reaches them they'll strike down whatever the lower court does to stop him.reply",
      "Amazing how Trump gets unfettered power and Biden gets reeled in.   Almost prevented Biden from rolling back a Trump EA when Biden was in power.https://en.wikipedia.org/wiki/Biden_v._Texasreply",
      "> Amazing how Trump gets unfettered power and Biden gets reeled inIf we look at how often the justices voted in favour of each administration in emergency applications when the government was the filer, we get Sotomayor and Jackson favouring Biden with a 77-point margin (88 to 11 percent and 77 to 0 percent, respectively), Alito favouring Trump with a 77-point margin (95 to 18%), and Kavanaugh, Barrett and Roberts with 48, 26 and 21-point margins [1].On the whole, Trump has been successful 84% of the time against Biden's 53%. But my point is that the partisan fracture of our court--on the level of individual justices--has been happening for a while. (The fact that we have (a) Alito, who's a hack and (b) a decadelong conservative majority is more explanatory than e.g. Barrett or Roberts having gone to the dark side.)[1] https://www.nytimes.com/2025/09/14/us/politics/supreme-court...reply",
      "Would you characterize the items they hand to the court as similarly extreme and unprecedented in both ways? If one side is providing milder work, then I would expect higher agreeability. Otherwise there is something fishy with both sidesing it.Obviously it is impossible to answer this without projecting some bias. But I don't think that makes it unanswerable.reply",
      "> Would you characterize the items they hand to the court as similarly extreme in both ways?It's really difficult to answer this separate from one's biases.I'd also note that Trump, then Biden, then Trump again escalated the use of the shadow docket way beyond historical norms [1]. This was a deliberate choice by both Presidents.> there is something fishy with both sidesing itDidn't mean to both sides this, at least not at the level of the Court. The Court has had a conservative majority for a decade; one could argue Jackson and Sotomayor are balancing the court by leaning against its centre of pressure. But it's not unexpected for the Court to be a bit more deferential towards a Republican President. We haven't been appointing and confirming neutral arbiters for a while.[1] https://en.wikipedia.org/wiki/Shadow_docket#Since_2017reply",
      "Shadow docket here we come.reply",
      "Don't get too exited.  This is a fight that's been happening for over 20yr now.  Whoever is throwing the political football of any given wind project or who is receiving it in the end zone is just a name.  They'll be gone in a few years.  The institution of fighting over off shore wind farms in the Boston to NYC area was there before them and will be there after them.Regardless of the pretext of any given action the the way things generally are is that the people who have a view they want to protect, the tourism industry and the hippie/nature/biology types are on the no-wind side and the climate types, green energy people, domestic energy and big business types are on the other. Sometimes one side wins, sometimes another side wins. But nobody ever gets a win streak long enough to bring anything to fruition.The area is well suited to wind power but the area but it's also chock full of rich people and moneyed interests that can afford to fight it, likely to the long term detriment of the region, but like locusts they will be gone and cashed out by then so they don't care. That's probably when these things will finally get built.I'd love to see some wind turbines go up but I'll believe it when I see it.  And even then, I bet they'll find some way to make everyone's bill go up instead of down because of it.Sincerely and with the utmost disrespect,A cape wind proponent.reply",
      "SC is gonna shut it down. Anyone taking bets?reply"
    ],
    "link": "https://apnews.com/article/trump-renewable-energy-offshore-wind-revolution-wind-f1cbe85a829e3d5e5496f834bcb617d1",
    "first_paragraph": ""
  },
  {
    "title": "Paper2Agent: Stanford Reimagining Research Papers as Interactive AI Agents (arxiv.org)",
    "points": 50,
    "submitter": "Gaishan",
    "submit_time": "2025-09-22T22:02:01 1758578521",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=45340133",
    "comments": [
      "> Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work [...]But that's the point! If we take out the effort to understand, really understand something on a deeper level from even research, then how can there be anything useful build on top of it?\nIs everything going to loose any depth and become shallow?reply",
      "Isn't this also a problem given that ChatGPT at least is bad a summarizing scientific papers[1]? Idk about Claude or Gemenai with that though. Still a problem.Edit: spelling.[1]: https://arstechnica.com/ai/2025/09/science-journalists-find-...reply",
      "This study seemed to be before the reasoning models came out. With them I have the opposite problem. I ask something simple and it responds with what reads like a scientific paper.reply",
      "Kill me now.Yes, I will get right on that. I believe that killing you is the right strategy to help you escape from a world where AI takes over every aspect of human existence in such a way that all those aspects are degraded.I'm still alive.That is a very good point, and I am sorry.reply",
      "\"I Have No Mouth, and I Must Scream\" \u2014 Harlan Ellisonhttps://www.are.na/block/26283461reply",
      "Earlier today there was a post about someone submitting an incorrect AI generated bug report. I found one of the comments telling:https://news.ycombinator.com/item?id=45331233> Is it that crazy? He's doing exactly what the AI boosters have told him to do.I think we're starting to see the first real AI \"harms\" shake out, after some years of worrying it might swear or tell you how to make a molotov cocktail.People are getting convinced, by hype men and by sycophantic LLMs themselves, that access to a chatbot suddenly grants them polymath abilities in any field, and are acting out there dumb ideas without pushback, until the buck finally stops, hopefully with just some wasted time and reputation damage.People should of course continue to use LLMs as they see fit - I just think the branding of work like this gives the impression that they can do more than they can, and will encourage the kind of behavior I mention.reply",
      "I tried it a few weeks ago. Wasn't very impressed with the resulting code compared to me manually working with an LLM and an uploaded research paper, which takes less time and costs less.reply",
      "So that I understand, is the idea that you point this tool at a GitHub repository, it figures out how to install and run it (figures out the build environment, installs any dependencies, configures the app, etc), plus it figures out how to interact with it, and then you send it queries via a chatbot?Does it take only the repository as input, or does it also consume the paper itself?reply",
      "What if you could sit down, have a beer and shoot the shit with Research Papers?reply"
    ],
    "link": "https://arxiv.org/abs/2509.06917",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "The Beginner's Textbook for Fully Homomorphic Encryption (arxiv.org)",
    "points": 163,
    "submitter": "Qision",
    "submit_time": "2025-09-21T14:26:10 1758464770",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=45323027",
    "comments": [
      "FWIW: I created a github repo for compact zero-knowledge proofs that could be useful for privacy-preserving ML models of reasonable size (https://github.com/logannye/space-efficient-zero-knowledge-p...). Unfortunately, FHE's computational overhead is still prohibitive for running ML workloads except on very small models. Hoping to help make ZKML a little more practical.reply",
      "I was under the impression that, for any FHE scheme with \"good\" security, (a) there was a finite and not very large limit to the number of operations you could do on encrypted data before the result became undecryptable, and (b) each operation on the encrypted side was a lot more expensive than the corresponding operation on plaintext numbers or whatever.Am I wrong? I freely admit I don't know how it's supposed to work inside, because I've never taken the time to learn, because I believed those limitations made it unusable for most purposes.Yet the abstract suggests that FHE is useful for running machine learning models, and I assume that means models of significant size.reply",
      "My understanding is largely ten years old and high level and only for one kind of fully homomorphic encryption. Things have changed and there is more than one kind.I heard it described as a system that encrypts each bit and then evaluates the \"encrypted bit\" in a virtual gate-based circuit that implements the desired operations that one wants applied to the plaintext. The key to (de|en)crypt plaintext will be at least one gigabyte. Processing this exponentially larger data is why FHE based on the system I've described is so slow.So, if you wanted to, say, add numbers, that would involve implementing a full adder [0] circuit in the FHE system.[0] https://en.wikipedia.org/wiki/Adder_(electronics)#/media/Fil...For a better overview that is shorter than the linked 250 page paper, I encourage you to consider Jeremy Kun's 2024 overview [1][1] https://www.jeremykun.com/2024/05/04/fhe-overview/reply",
      "The difference between homomorphic schemes and fully homomorphic schemes is that FHE can be bootstrapped; there's a circuit that can be homomorphically evaluated that removes the noise from an encrypted value, allowing any homomorphic calculation's result to have its noise removed for further computation.reply",
      "Both of these are correct-ish. You can do a renornalization that resets the operation counter without decrypting on FHE schemes, so in that sense there is no strict limit on operation count. However, FHE operations are still about 6 orders of magnitude more expensive than normal, so you are not going to be running an LLM, for instance, any time soon. A small classifier maybe.reply",
      "LLMs are at the current forefront of FHE research. There are a few papers doing some tweaked versions of BERT in <1 minute per token. Which is only ~4 orders of magnitude slower than cleartext.https://arxiv.org/html/2410.02486v1#S5reply",
      "Given individual LLM parameters are not easily interpreted, naturally obfuscated by the diffuse nature of their impact, I would think leaning into that would be a more efficient route.Obfuscating input and output formats  could be very effective.Obfuscation layers can be incorporated into training. With an input (output) layer that passes information forward, but whose output (input) is optimized to have statistically flat characteristics, resistant to attempts to interpret.Nothing like apparent pure noise for obfuscation!The core of the model would then be trained, and infer, on the obfuscated data.When used, the core model would publicly operate on obfuscated data. While the obfuscation/de-obfuscation layers would be used privately.In addition to obfuscating, the pre and post-layers could also reduce data dimensionality. Naturally increasing obfuscation and reducing data transfer costs. It is a really good fit.Even the most elaborate obfuscation layers will be orders and orders of magnitude faster than today's homomorphic approaches.(Given the natural level parameter obfuscation, and the highly limited set of operations for most deep models, I wouldn't be surprised if efficient homomorphic approaches were found in the future.)reply",
      "This paper uses a very heavily modified version of an encoder-only BERT model. Forward pass on a single 4090 is cited there at 13 seconds after switching softmax out for a different kernel (21 seconds with softmax). They are missing a non-FHE baseline, but that model has only about 35 million parameters when you look at its size. At FP16, you would expect this to be about 100x faster than a normal BERT because it's so damn small. On a 4090, that model's forward pass probably runs at something like 100k-1M tokens per second given some batching. It sounds like 6 orders of magnitude is still about right.reply",
      "Does this mean, according to Moore's Law, FHE can operate at speeds from 6 years ago?reply",
      "Moore's Law roughly states that we get a doubling of speed every 2 years.If we're 6 orders of magnitude off, then we need to double our speed 20 times (2^20 = 1,048,576), which would give us speeds approximately in line with 40 years ago. Unless my understanding is completely off.reply"
    ],
    "link": "https://arxiv.org/abs/2503.05136",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Cap'n Web: a new RPC system for browsers and web servers (cloudflare.com)",
    "points": 391,
    "submitter": "jgrahamc",
    "submit_time": "2025-09-22T13:05:32 1758546332",
    "num_comments": 180,
    "comments_url": "https://news.ycombinator.com/item?id=45332883",
    "comments": [
      "The section on how they solved arrays is fascinating and terrifying at the same time https://blog.cloudflare.com/capnweb-javascript-rpc-library/#....> .map() is special. It does not send JavaScript code to the server, but it does send something like \"code\", restricted to a domain-specific, non-Turing-complete language. The \"code\" is a list of instructions that the server should carry out for each member of the array.> But the application code just specified a JavaScript method. How on Earth could we convert this into the narrow DSL? The answer is record-replay: On the client side, we execute the callback once, passing in a special placeholder value. The parameter behaves like an RPC promise. However, the callback is required to be synchronous, so it cannot actually await this promise. The only thing it can do is use promise pipelining to make pipelined calls. These calls are intercepted by the implementation and recorded as instructions, which can then be sent to the server, where they can be replayed as needed.reply",
      "In C#, there's expression trees which handle things like this and it's how Entity Framework is able to convert the lambdas it's given into SQL. This means that you can pass around code that can be inspected or transformed instead of being executed. Take this EntityFramework snippet:    db.People.Where(p => p.Name == \"Joe\")\n\n`Where` takes an `Expression<Func<T, bool>> predicate`. It isn't taking the `Func` itself, but an `Expression` of it so that it can look at the code rather than execute it. It can see that it's trying to match the `Name` field to the value \"Joe\" and translate that into a SQL WHERE clause.Since JS doesn't have this, they have to pass in a special placeholder value and try to record what the code is doing to that value.reply",
      "There are inherent limitations with the \"execute it once and see what happens\" approach; namely that any conditional logic that might be in the mapping function is going to silently get ignored.  For example, `db.people.map(p => p.IsPerson ? (p.FirstName + ' ' + p.LastName) : p.EntityName)` would either be seen as reading `(IsPerson, FirstName, LastName)` or `(p.IsPerson, p.EntityName)` depending on the specific behavior of the placeholder value ... and neither of those sets is fully correct.I wonder why they don't just do `.toString()` on the mapping function and then parse the resulting Javascript into an AST and figure out property accesses from that.  At the very least, that'd allow the code to properly throw an error in the event the callback contains any forbidden or unsupported constructs.reply",
      "> I wonder why they don't just do `.toString()` on the mapping function and then parse the resulting Javascript into an AST and figure out property accesses from that.That sounds incredibly complicated, and not something we could do in a <10kB library!reply",
      "Maybe Fabrice Bellard could spare an afternoon.reply",
      "The placeholder value is an RpcPromise. Which means that all its properties are also RpcPromises. So `p.IsPerson` is an RpcPromise. I guess that's truthy, so the expression will always evaluate to `(p.FirstName + ' ' + p.LastName)`. But that's going to evaluate to '[object Object] [object Object]'. So your mapper function will end up not doing anything with the input at all, and you'll get back an array full of '[object Object] [object Object]'.Unfortunately, \"every object is truthy\" and \"every object can be coerced to a string even if it doesn't have a meaningful stringifier\" are just how JavaScript works and there's not much we can do about it. If not for these deficiencies in JS itself, then your code would be flagged by the TypeScript compiler as having multiple type errors.reply",
      "Yeah I'll definitely chalk this up to my not having more than a very very passing idea of the API surface of your library based on a quick read over just the blog post.On a little less trivial skim over it looks like the intention here isn't to map property-level subsets returned data (e.g., only getting the `FirstName` and `LastName` properties of a larger object); as much as it is to do joins and it's not data entities being provided to the mapping function but RpcPromises so individual property values aren't even available anyway.So I guess I might argue that map() isn't a good name for the function because it immediately made me think it's for doing a mapping transformation and not for basically just specifying a join (since you can't really transform the data) since that's what map() can do everywhere else in Javascript.  But for all I know that's more clear when you're actually using the library, so take what I think with a heaping grain of salt. ;)reply",
      "Couldn\u2019t you make this safer by passing the map something that\u2019s not a plain JS function? I confess to that being the only thing that had me questioning the logic. If I can express everything, then everything should work. If it\u2019s not going to work, I don\u2019t want to be able to express it.reply",
      "I think any other syntax would likely be cumbersome. What we actually want to express here is function-shaped: you have a parameter, and then you want to substitute it into one or more RPC calls, and then compute a result. If you're going to represent that with a bunch of data structures, you end up with a DSL-in-JSON type of thing and it's going to be unwieldy.reply",
      "Another way to screw this up would be to have an index counter and do something different based on the index. I think the answer is \"don't do that.\"reply"
    ],
    "link": "https://blog.cloudflare.com/capnweb-javascript-rpc-library/",
    "first_paragraph": ""
  },
  {
    "title": "Egyptian Hieroglyphic Alphabet (discoveringegypt.com)",
    "points": 9,
    "submitter": "teleforce",
    "submit_time": "2025-09-19T07:15:39 1758266139",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://discoveringegypt.com/egyptian-hieroglyphic-writing/egyptian-hieroglyphic-alphabet/",
    "first_paragraph": "In AD 391, the Byzantine Emperor Theodosius I closed all pagan temples throughout the empire. This action ended a four thousand year old tradition and the message of the ancient Egyptian language was lost for 1500 years. The discovery of the Rosetta stone and the work of Jean-Francois Champollion (1790-1832) finally awakened the Ancient Egyptians from their long slumber. Today, by virtue of the vast quantity of their literature, we know more about Egyptian society than most other ancient cultures.\n\nThe script originated around four thousand years before Christ, accompanied by a decimal system of numeration that extended up to a million. Unlike other cultures, the early picture forms were never discarded or simplified, probably because they are very lovely to look at.Hieroglyphs were called, by the Egyptians, \u201cthe words of God\u201d and were used mainly by the priests.These intricately designed symbols were perfect for adorning temple walls, but for everyday business transactions, another sc"
  },
  {
    "title": "I'm spoiled by Apple Silicon but still love Framework (simonhartcher.com)",
    "points": 182,
    "submitter": "deevus",
    "submit_time": "2025-09-22T13:03:10 1758546190",
    "num_comments": 255,
    "comments_url": "https://news.ycombinator.com/item?id=45332859",
    "comments": [
      "I don't remember having \"range anxiety\" under S3 suspend on my old intel laptops. It just worked and they woke up reasonably reliably and quickly under linux if you had a regular thin and light with integrated graphics. I had an Apple intel laptop that might have been marginally faster returning to life but it basically just worked on either.The move to modern standby has been a mess. It seems to have improved a lot but my kid always shuts down his windows laptop before putting it in a bag because it seems to have become urban folklore that laptops turn on in bags, overheat and get damaged. That is new. I carried laptops in bags suspended for years and nobody thought that. It just worked.reply",
      "As someone whose job was to improve Windows fundamentals this makes me really sad.   I worked on making Windows respect modern standby better, and there just wasn't a whole lot of interest in making it work. The whole OS needs a lot of considered improvement, and it's not getting it.reply",
      "I\u2019ve certainly had that particularly with older Dell XPS computers. So has Linus at LTT, which I suppose how it entered folklore.I say this as my lab mate had his laptop do exactly that just last week, with up to date windows and a newer XPS laptop. It simply has never happened to my Macsreply",
      "I think it is more lived experience than urban folklore - it definitely happens.reply",
      "Modern standby is a disaster.reply",
      "The complaint about power usage in suspend is especially sad because it\u2019s pretty much a common problem for Linux on laptops. Not sure if that\u2019s what applies here, but the numbers about match what I see with my Framework. Basically: if you want to use secure boot you usually also want kernel lockdown mode, and you cannot hibernate a lockdowned kernel. At least not without out-of-tree patches.IMHO that\u2019s a giant issue. If you can\u2019t hibernate (aka suspend to disk) you will never be able to get that power consumption low. And telling people to not run secure boot or lockdown is not really a good answer either. Especially since the default installer already sets those things up. \nI get that \u201eLinux on laptops\u201c is not a priority big enough to get a proper fix for that. And that it\u2019s not an easy issue to fix. But the current state is really really sad.reply",
      "I really do not understand why hibernate under secure boot is not implemented on Linux and this continues for years.It is as if the features are implemented by completely different people. But this is not obviously the case since systemd supports both and actively improving both.Note for me hibernation is a security measure and not about saving battery. I am traveling sometimes with the laptop and risk of theft is non-trivial. If it is hibernated, then it is just a property loss. But with just suspend there is a chance that the data can be extracted. So I configured it to hibernate automatically after 15 minutes in suspension. Surprisingly it has been working reliably with Linux.reply",
      "I have secure boot, hibernation, and full disk encryption working fine on linux, but I have never heard of kernel lockdown.The solution I found involves making a custom initramfs to support hibernation and compiling the kernel into a signed EFI stub.reply",
      "Does the system use a boot loader? Or does it boot directly into kernel bypassing bootloaders?reply",
      "The term to search for is \"UKI\".A UKI is a kernel+initramfs+boot-arguments bundle all as a single WinPE/UEFI executable using the \"EFI Stub Loader\".You configure your system firmware to execute it, passing no arguments. It boots using the command line you set earlier. It's signed, and verified by the platform secure boot.Hibernation works fine with this approach.reply"
    ],
    "link": "https://simonhartcher.com/posts/2025-09-22-why-im-spoiled-by-apple-silicon-but-still-love-framework/",
    "first_paragraph": ""
  },
  {
    "title": "Why haven't local-first apps become popular? (marcobambini.substack.com)",
    "points": 303,
    "submitter": "marcobambini",
    "submit_time": "2025-09-22T13:17:59 1758547079",
    "num_comments": 314,
    "comments_url": "https://news.ycombinator.com/item?id=45333021",
    "comments": [
      "> The Solution: CRDTs. The right approach is CRDTs (Conflict-Free Replicated Data Types)... This means you can apply messages in any order, even multiple times, and every device will still converge to the same state.This is very much \"draw the rest of the owl\".Creating a CRDT model for your data that matches intuitive user expectations and obeys consistent business logic is... not for the faint of heart.Also remember it turns your data model into a bunch of messages that then need to be constantly reconstructed into the actual current state of data. It's a gigantic enormous headache.reply",
      "Almost every time I see CRDTs mentioned it\u2019s used as a magic device that makes conflicts disappear. The details, of course, are not mentioned.Technically an algorithm that lets the last writer win is a CRDT because there is no conflict.Making a true system that automatically merges data while respecting user intent and expectations can be an extremely hard problem for anything complex like text.Another problem is that in some situations using a CRDT to make the conflict disappear isn\u2019t even the right approach. If two users schedule a meeting for the same open meeting room, you can\u2019t avoid issues with an algorithm. You need to let them know about the conflict so they can resolve the issue.reply",
      "Agreed @ not for the faint of heart.There is at least one alternative \"CRDT-free\" approach for the less brave among us: https://mattweidner.com/2025/05/21/text-without-crdts.htmlreply",
      "> Difference from CRDTsThe author has made a CRDT. He denies that his algorithm constitutes a CRDT. It's a straightforward merge, not a \"fancy algorithm\".What specific aspect of a CRDT does this solution not satisfy? The C? The R? The D? The T?reply",
      "We have a local-first app. Our approach?Just ignore the conflicts. The last change wins.No, really. In practice for most cases the conflicts are either trivial, or impossible. Trivial conflicts like two people modifying the same note are trivial for users, once you have a simple audit log.And impossible conflicts are impossible to solve automatically anyway and require business processes around them. Example: two people starting to work on the same task in an offline-enabled task tracker.reply",
      "Just have audit log. No need to try solving every trivial cases. Make something useful.reply",
      ">Example: two people starting to work on the same task in an offline-enabled task tracker.\nWouldn't this just mean both people are working on it?I agree that this means humans intervening.. It sounds like there was a comms breakdown. But rather than doing a first-in-best-dressed, it sounds like accurately recording that both users are in fact working on the same thing is the best option since it surfaces that intervention is required (or maybe its intentional, tools insisting that only one person can work on an item at once annoys me). Sounds much better than quietly blowing away one of the user's changes.In principle, local-first to me means each instance (and the actions each user carries out on their instance) is sacrosanct. Server's job is to collate it, not decide what the Truth is (by first-in-best-dressed or otherwise).reply",
      "One solution is to make it so that people see their literal keystrokes in real time. Then they solve the conflict themselves. Like, \"stop typing into this text because bob is typing into it\".It's like Ethernet conflict resolution: just access the shared medium and detect collisions in real time.reply",
      "They used to be really popular, back in the ancient times when I was young and full of excitement for all things compute, almost all software was local-first, and.. only :)But since the entire world economy has turned to purely optimizing for control and profit, there's just no good reason to not screw people over as much and as often as possible, what'll they do ? Switch to someone who won't ? And who would that be ?reply",
      "> But since the entire world economy has turned to purely optimizing for control and profit, there's just no good reason to not screw people over as much and as often as possible, what'll they do ?I worked on a somewhat well-known (at the time) product that used on-site hosting.One of our biggest customer complaints was that we didn\u2019t have a cloud hosted option. Many companies didn\u2019t want to self-host. They\u2019d happily pay a premium to not have to host it.I think HN underestimates the demand for cloud hosted solutions. Companies don\u2019t want to deal with self-hosted if they can pay someone else a monthly fee to do it.reply"
    ],
    "link": "https://marcobambini.substack.com/p/why-local-first-apps-havent-become",
    "first_paragraph": ""
  },
  {
    "title": "Is a movie prop the ultimate laptop bag? (jgc.org)",
    "points": 152,
    "submitter": "jgrahamc",
    "submit_time": "2025-09-22T11:59:01 1758542341",
    "num_comments": 168,
    "comments_url": "https://news.ycombinator.com/item?id=45332196",
    "comments": [
      "- It has a wide base, so the laptop just swings back and forth while carrying.- Gets the rainwater or any other hazardous material in.- Extremely easy to check out what's inside for a thief.- Can slide out easily and fall when toppled.Terrible overall. Could make it to Top 10 Worst Laptop Bags though.Just use a Tom Bihn that uses a separate, rigid, harder to access sleeve inside your backpack for laptops.reply",
      "don't forget there's no divider, so the prongs of your charger can scratch the shit out of your laptop. you might as well use a tote bagreply",
      "the author linked to a comment where they described primary drivers for this as: disdain for the ostentatious as well as backpackspersonally I use an old shoulder bag, but we've all got our proclivitiesreply",
      "I mean, yes, but: how much does a Tom Bihn bag go for these days? $300?Maybe also add lower-budget recommendations.reply",
      "You don't need to drop $300+ on a Tom Bihn to have a totally functional backpack with a laptop sleeve, for cryin' out loud. So many backpacks have it, and it's not a premium feature.reply",
      "More or less any backpack will do. You can even find them for used for for free.reply",
      "> You can even find them for used for for free.I live in a university town with lots of rich kids, which seems like the kind of place to find good-quality used backpacks if there were one.  If there's an easy way to get my hands on free used backpacks that I'd actually want to use, then I certainly don't know about it.reply",
      "Thriftstores, craigslist, fb marketplace, the facility on/near campus that takes in abandoned/donated items from students, etc.reply",
      "Also, when the semester gets out. Usually there is a [local college name] Christmas and the rich kids just put their stuff outside rather than transport it back home.reply",
      "Facebook marketplace or Craigslist might be a good place to check around the end of a semester.reply"
    ],
    "link": "https://blog.jgc.org/2025/09/is-movie-prop-ultimate-laptop-bag.html",
    "first_paragraph": ""
  },
  {
    "title": "Testing is better than data structures and algorithms (nedbatchelder.com)",
    "points": 89,
    "submitter": "rsyring",
    "submit_time": "2025-09-22T16:21:35 1758558095",
    "num_comments": 89,
    "comments_url": "https://news.ycombinator.com/item?id=45335635",
    "comments": [
      "We wrote a conferencing app and server (years before Zoom). Tested the server by having automated headless apps run in gangs, hopping from conversation to conversation, turning mic and camera on and off, logging out and logging back in. Used it for years, the Bot Army we called it.\nResponsible for our rock-solid quality reputation. Not API design or test classes or contraints or anything. Just, trying the damn thing, in large cases, for a long time.When it ran an hour, we celebrated. When it ran overnight, we celebrated. When it ran a week we celebrated, and called that good enough.reply",
      "Always gonna have to side with Peter Norvig on this one: https://pindancing.blogspot.com/2009/09/sudoku-in-coders-at-...> They said, \u201cLook at the contrast\u2014here\u2019s Norvig\u2019s Sudoku thing and then there\u2019s this other guy, whose name I\u2019ve forgotten, one of these test-driven design gurus. He starts off and he says, \u201cWell, I\u2019m going to do Sudoku and I\u2019m going to have this class and first thing I\u2019m going to do is write a bunch of tests.\u201d But then he never got anywhere. He had five different blog posts and in each one he wrote a little bit more and wrote lots of tests but he never got anything working because he didn\u2019t know how to solve the problem. I actually knew\u2014from AI\u2014that, well, there\u2019s this field of constraint propagation\u2014I know how that works. There\u2019s this field of recursive search\u2014I know how that works. And I could see, right from the start, you put these two together, and you could solve this Sudoku thing. He didn\u2019t know that so he was sort of blundering in the dark even though all his code \u201cworked\u201d because he had all these test cases.reply",
      "I love what Norvig said. I can relate to it. As far as data structures are concerned, I think it's worth playing smart with your tests - focus on the \"invariants\" and ensure their integrity.A classic example of invariant I can think of is the min-heap - node N is less than or equal to the value of its children - the heap property.Five years from now, you might forget the operations and the nuanced design principles, but the invariants might stay well in your memory.reply",
      "More context, from an earlier HN comment: https://news.ycombinator.com/item?id=3033446reply",
      "I think the author is misleadLet's grab a simple use case: some basic CRUD http API. Easy, you say, no need to know fancy stuff ! Just test it and that's all.You do your test, all good, you can roll in production !But sadly, in production, you have multiple users (what an idea ..). Suddenly, your CRUD api has become a concurrent system. Suddenly, you have data corruption, because you never thought about anything about that, and \"your tests were green\".Algorithms are the backbone tools of programming. Knowing them help us, ignoring them burdens us.reply",
      "Macro Pierre White says \"perfection is lots of little things done well.\"Which is something I've always agreed with,  so,  I never understand articles that seek to eschew an important part of releasing software because they believe their approach elsewhere is enough to overcome these intentionally suboptimal choices.reply",
      "Mediocre testing can also lead to a situation where there is friction for improvement because the tests are brittle and coupled (with each other and the misfeature you\u2019re interested in fixing).I like a more uniform distribution in my testing efforts. Start earlier, end later than most, and it\u2019s experiences like this that inform that preference. And also production bugs in code with supposed 100% test coverage.reply",
      "> Mediocre testing can also lead to a situation where there is friction for improvement because the tests are brittle and coupledThis is very very common among inexperienced devs and in immature organizations that think that more tests necessarily means better.reply",
      "I mean, in your example you just have an incomplete test suite. (Though writing a complete one is often unrealistic)While understanding algorithms and data structures is important, the only way you really know how well it works, and how well it's implemented is by thoroughly testing it. There are an infinite amount of clever algorithms out there with terrible implementations.You need both.reply",
      "Testing concurrency is extremely hardFor instance, get sql queries; You ran them, and you have no issue; Is your code sane ? Or is it because one query ran 10ms earlier and, thus, you avoided the issue ?I truly wonder if there is real world tests around this; I bet there is only algorithm and fuzzing;reply"
    ],
    "link": "https://nedbatchelder.com/blog/202509/testing_is_better_than_dsa.html",
    "first_paragraph": "Monday 22 September 2025People should spend less time learning DSA, more time learning testing.I see new learners asking about \u201cDSA\u201d a lot.  Data Structures and Algorithms\nare of course important: considered broadly, they are the two ingredients that\nmake up all programs.  But in my opinion, \u201cDSA\u201d as an abstract field of study\nis over-emphasized.I understand why people focus on DSA: it\u2019s a concrete thing to learn about,\nthere are web sites devoted to testing you on it, and most importantly, because\njob interviews often involve DSA coding questions.Before I get to other opinions, let me make clear that anything you can do to\nhelp you get a job is a good thing to do.  If grinding\nleetcode will land you a position, then do it.But I hope companies hiring entry-level engineers aren\u2019t asking them to\nreverse linked lists or balance trees.  Asking about techniques that can be\nmemorized ahead of time won\u2019t tell them anything about how well you can work.\nThe stated purpose of those interviews is"
  },
  {
    "title": "OpenAI and Nvidia announce partnership to deploy 10GW of Nvidia systems (openai.com)",
    "points": 387,
    "submitter": "meetpateltech",
    "submit_time": "2025-09-22T16:10:15 1758557415",
    "num_comments": 501,
    "comments_url": "https://news.ycombinator.com/item?id=45335474",
    "comments": [
      "Framing it in gigawatts is very interesting given the controversy about skyrocketing electric prices for residential and small business users as a result of datacenters over the past three years, primarily driven by AI growth. If, as another commenter notes, this 10GW is how much Chicago and NYC use combined, then we need to have a serious discussion about where this power is going to come from given the dismal status of the USA's power grid and related infrastructure and the already exploding costs that have been shifted to residential users in order to guarantee electric supply to these biggest datacenters (so they can keep paying peanuts for electricity and avoid shouldering any of the infrastructural burden to maintain or improve the underlying grid/plants required to meet their massive power needs).I'm not even anti-datacenter (wouldn't be here if I were), I just think there needs to be serious rebalancing of these costs because this increase in US residential electric prices in just five years (from 13\u00a2 to 19\u00a2, a ridiculous 46% increase) is neither fair nor sustainable.So where is this 10GW electric supply going to come from and who is going to pay for it?Source: https://fred.stlouisfed.org/series/APU000072610EDIT:To everyone arguing this is how DCs are normally sized: yes, but normally it's not the company providing the compute for the DC owner that is giving these numbers. nVidia doesn't sell empty datacenters with power distribution networks, cooling, and little else; nVidia sells the GPUs that will stock that DC. This isn't a typical PR netnewswire bulletin \"OpenAI announces new 10GW datacenter\", this is \"nvidia is providing xx compute for OpenAI\". Anyway, all this is a segue from the question of power supply, consumption, grid expansion/stability, and who is paying for all that.reply",
      "I work in the datacenter space. The power consumption of a data center is the \"canonical\" way to describe their size.Almost every component in a datacenter is upgradeable\u2014in fact, the compute itself only has a lifespan of ~5 years\u2014but the power requirements are basically locked-in. A 200MW data center will always be a 200MW data center, even though the flops it computes will increase.The fact that we use this unit really nails the fact that AI is basically refining energy.reply",
      "A 200MW data center will always be a 200MW data center, even though the flops it computes will increase.\n\nThis here underscores how important TSMC's upcoming N2 node is. It only increases chip density by ~1.15x (very small relative to previous nodes advancements) but it uses 36% less energy at the same speed as N3 or 18% faster than N3 at the same energy. It's coming at the right time for AI chips used by consumers and energy starved data centers.N2 is shaping up to be TSMC's most important node since N7.reply",
      "> N2 is shaping up to be TSMC's most important node since N7Is it?N2, from an energy & perf improvement seems on par with any generation node update.          N2:N3   N3:N5  N5:N7\n  Power   ~30%    ~30%    ~30%\n  Perf    ~15%    ~15%    ~15%\n\nhttps://www.tomshardware.com/news/tsmc-reveals-2nm-fabricati...reply",
      "I love that term \"refining energy\". We need to plan for massive growth in electricity production to have the supply to refine.reply",
      "It is the opposite of refining energy. Electrical energy is steak, what leaves the datacenter is heat, the lowest form of energy that we might still have a use for in that concentration (but most likely we are just dumping it in the atmosphere).Refining is taking a lower quality energy source and turning it into a higher quality one.What you could argue is that it adds value to bits. But the bits themselves, their state is what matters, not the energy that transports them.reply",
      "I think you're pushing the metaphor a bit far, but the parallel was to something like ore.A power plant \"mines\" electron, which the data center then refines into words. or whatever. The point is that energy is the raw material that flows into data centers.reply",
      "Maybe more like converting energy to data, as a more specific type of refinement.reply",
      "Using energy to decrease the entropy of data. Or to organize and structure data.reply",
      "That's pretty interesting. Is it just because the power channels are the most fundamental aspect of the building? I'm sorta surprised you can't rip out old cables and drop in new ones, or something to that effect, but I also know NOTHING about electricity.reply"
    ],
    "link": "https://openai.com/index/openai-nvidia-systems-partnership/",
    "first_paragraph": ""
  },
  {
    "title": "A board member's perspective of the RubyGems controversy (apiguy.substack.com)",
    "points": 73,
    "submitter": "Qwuke",
    "submit_time": "2025-09-21T19:20:23 1758482423",
    "num_comments": 97,
    "comments_url": "https://news.ycombinator.com/item?id=45325792",
    "comments": [
      "This story is missing any context around what occurred. The only thing I was able to find was by searching, and I came to this PDF statement.https://pup-e.com/goodbye-rubygems.pdf> On September 9th, with no warning or communication, a RubyGems maintainer unilaterally:> renamed the \u201cRubyGems\u201d GitHub enterprise to \u201cRuby Central\u201d,> added non-maintainer Marty Haught of Ruby Central, and> removed every other maintainer of the RubyGems project.> On September 18th, with no explanation, Marty Haught revoked GitHub organization membership for all admins on the RubyGems, Bundler, and RubyGems.org maintainer teamsWhich is important context that was left out of this board member's statement.reply",
      "I found this helpful in explaining what's happened: https://www.theregister.com/2025/09/22/ruby_central_rubygems...Sounds like they made some really big changes and put zero effort into communicating to people who've spent 10+ years working on the project.reply",
      "Thanks - that was helpful indeed. From there, I also found the linked post by Tekin S\u00fcleyman ( https://tekin.co.uk/2025/09/the-ruby-community-has-a-dhh-pro... ) to be informative.reply",
      "Wow!  When that one DHH blog went around the other day, I didn't actually pay attention to who the author was.  All I saw was yet another bigoted rant and just skimmed it and rolled my eyes. (e: here it is to save people the effort: https://world.hey.com/dhh/as-i-remember-london-e7d38e64 )I should not have skimmed it.  From your link:> In the same post he praises Tommy Robinson (actual name Stephen Christopher Yaxley-Lennon), a right-wing agitator with several convictions for violent offences and a long history of association with far-right groups such as the English Defence League and the British Nationalist Party. He then goes on to describe those that attended last weekend\u2019s far-right rally in London as \u201cperfectly normal, peaceful Brits\u201d protesting against the \u201cdemographic nightmare\u201d that has enveloped London, despite the violence and disorder they caused.> To all of that he ads a dash of Islamophobia, citing \u201cPakistani rape gangs\u201d as one of the reasons for the unrest, repeating a weaponised trope borne from a long since discredited report from the Quilliam Foundation, an organisation with ties to both the the US Tea Party, and Tommy Robinson himself.This is ... disqualifying.  That's the best word I can summon here to express my dismay.  This is a crossed line.  Absolutely nutso.edit2: Uh wow I really should not have skimmed it.  Here's one paragraph from DHH's blog itself:> Which brings us back to Robinson's powerful march yesterday. The banner said \"March for Freedom\", and focused as much on that now distant-to-the-Brits concept of free speech, as it did on restoring national pride. And for good reason! The totalitarian descent into censorious darkness in Britain has been as swift as its demographic shift.Well, if that doesn't speak volumes as to DHH's values, I don't know what does.reply",
      "He makes his position clear enough in the second paragraph,\nfor those who know how to read between the lines.\n\"London is no longer the city I was infatuated with in the late '90s and early 2000s. Chiefly because it's no longer full of native Brits.\"reply",
      "... and this is the guy whose Linux \"distro\" Cloudflare has just announced funding for.reply",
      "Not all _that_ surprising. From where I see things, pretty much every time you see \"Cloudflare\" and \"free speech\" in the same sentence, it always end up being about Cloudflare supporting free speech for nazis or white supremacists. DHH's racist and xenophobic views are totally on-brand for them.reply",
      "It was not left out of the statement. I understood that was essentially what happened by the time I got to the end of his piece. The only exception being the \u201cwith no warning or communication\u201d part. Obviously there is disagreement about whether that is true or not.reply",
      "Everything you're quoting is from one aggrieved person, who clearly felt slighted, and who left out a whole lot of context in their own post. The article above is a lot more reasoned, less emotional, and seems completely reasonable to me. Ruby Central clearly has issues with both internal and external communication. And the above article isn't an official statement either; it's just one person, not involve in the decision, offering another perspective.reply",
      "It's not just one person.Between the initial removal of access, then giving it back after explaining it was a mistake; the people involved started a conversation about governance to clarify/fix things.https://github.com/rubygems/rfcs/pull/61The conversation terminated because the majority of those people then had their access revoked again.When weighing the facts here; which group or claimant has the most evidence for their claims?\nThe technical folks with lots of commits over many years, or the treasurer of an organisation who says the impetus for this was a \"funding deadline\" so all access had to be seized?reply"
    ],
    "link": "https://apiguy.substack.com/p/a-board-members-perspective-of-the",
    "first_paragraph": ""
  },
  {
    "title": "What happens when coding agents stop feeling like dialup? (martinalderson.com)",
    "points": 91,
    "submitter": "martinald",
    "submit_time": "2025-09-21T12:11:18 1758456678",
    "num_comments": 89,
    "comments_url": "https://news.ycombinator.com/item?id=45322030",
    "comments": [
      "> Each of these 'phases' of LLM growth is unlocking a lot more developer productivity, for teams and developers that know how to harness it.I still find myself incredibly skeptical LLM use is increasing productivity.  Because AI reduces cognitive engagement with tasks, it feels to me like AI increases perceptive productivity but actually decreases it in many cases (and this probably compounds as AI-generated code piles up in a codebase, as there isn't an author who can attach context as to why decisions were made).https://metr.org/blog/2025-07-10-early-2025-ai-experienced-o...I realize the author qualified his or her statement with \"know how to harness it,\" which feels like a cop-out I'm seeing an awful lot in recent explorations of AI's relationship with productivity.  In my mind, like TikTok or online dating, AI is just another product motion toward comfort maximizing over all things, as cognitive engagement is difficult and not always pleasant.  In a nutshell, it is another instant gratification product from tech.That's not to say that I don't use AI, but I use it primarily as search to see what is out there.  If I use it for coding at all, I tend to primarily use it for code review.  Even when AI does a good job at implementation of a feature, unless I put in the cognitive engagement I typically put in during code review, its code feels alien to me and I feel uncomfortable merging it (and I employ similar levels of cognitive engagement during code reviews as I do while writing software).reply",
      "My experience is exactly the opposite of \"AI reduces cognitive engagement with tasks\": I have to constantly be on my toes to follow what the LLMs are proposing and make sure they are not getting off track over-engineering things, or entering something that's likely to turn into a death loop several turns later.\nAI use definitely makes my brain run warmer, got to get a FLIR camera to prove it I guess...reply",
      "So, reduces cognitive engagement with the actual task at hand, and forces a huge attention share to hand-holding.I don't think you two are disagreeing.I have noticed this personally. It's a lot like the fatigue one gets from too long scrolling online. Engagement is shallower but not any less mentally exhausting than reading a book. You end up feeling more exhausted due to the involuntary attention-scattering.reply",
      "It would be analogous to having to double check the IDE added the lines of code I actually typed. That\u2019s not a great productivity boost, it\u2019s a toy still in many ways.reply",
      "I'm an electrical engineer. One of my jobs is maintaining a couple racks of equipment and the scripts we use to test hardware. I've never been expected to be a programmer beyond things like Matlab but over the past several years, I've been maintaining a python project we use to run these tests. With equipment upgrades and my amateur python skills, we now have a fully automated test, plug in the hardware, hit the green button, and wait for tests to complete and data to be validated. Codesurf absolutely chokes when trying to work on my code, it's just too much of a mess to handle. But I have been using our in-house chatgpt to write some utilities that I've been procrastinating on for years. Like I needed a debug tool to view live telemetry and send commands as required and have been procrastinating for a long time to write this. My existing scripts aren't flexible, they are literally just a script for the test runner to follow. I have an old debug tool but it's not compatible with the existing workflow so it's a pain to run. I told chatgpt what I needed, gave it some specs on the functions it would need from libraries I've written (but didn't want it to see), and it cranked out a perfectly functional python script. I ended up doing a bit of work on the script it gave me since I didn't trust it completely or knew if I could even get it to expand on the work properly. It would have taken me much longer to write on my own so I'm very grateful I could save so much time. Just last week, I had another idea for a different debug tool and did the same process (here's my idea, here's the specs, go) and after a few rounds of \"can you add this next?\", I had another quality tool ready to go with absolutely no touch-up work needed on my end. I want my tools to have simple Tkinter GUIs but I hate writing GUIs so I'm absolutely thrilled chatgpt can handle that for me.I'm a bit of a luddite, I still just use notepad++ and a terminal window to develop my code. I don't want to get bogged down in using vscode so trusting AI to handle things beyond \"can you make this email sound better?\" has been a big leap for me.reply",
      "In a few to several months you will learn the meaning of \"big ball of mud\". Then you will either speedrun the last 20-30 years of software development tooling evolution or crash out of your current modus operandi and help fuel future demand for actual software developers.reply",
      "Do you have a definition of \u201cactual software developers\u201d?To me an \u201cactual software developer\u201d is always learning, always growing, always making mistakes, always looking back and blown away by how far they\u2019ve come - and most importantly, is always willing to generously offer a hand up to anyone who cares enough to learn the craft.It\u2019s ok to make a big ball of mud! You can deal with it later once you understand the problem v1 solves. Parallel rebuilds and migrations are part of software engineering. Or alternatively - maybe that big ball of mud does its job, has no new requirements, so can be left quietly chugging along - for potentially decades, never needing a v2.reply",
      "I disagree.  The quoted scenario is the absolute best for LLMs.1) An easily defined go/no-go task with defined end point which requires2) A bunch of programming code that nobody gives a single shit about3) With esoteric function calls that require staring at obscure documentationThis is the LLM dream task.When the next person has to stare at this code, they will throw it out and rerun an LLM on it because the code is irrelevant and the end task is the only thing that matters.reply",
      "Here's the thing.  Those first two things don't exist.I'm revisiting this comment a lot with LLM's.  I don't think many HN readers run into real life mudball/spaghetti code.  I think there is a SV bias here where posters think taking a shortcut a few times is what a mudball is.There will NEVER be a time in this business where the business is ok with simply scrapping these hundreds of inconsistent one off generations and be ok with something that sorta kinda worked like before.  The very places that do this won't use consistent generation methods either.  The next person to stare at it will not just rerun the LLM because at that time the ball will be so big not even the LLMs can fix it without breaking something else.  Worse the new person won't even know what they don't know or even what to ask it to regenerate.Man I'm gonna buy stock in the big three as a stealth long term counter LLM play.I've seen outside of SV mudballs and they are messes that defy logical imagination.  LLM's are only gonna make that worse.  Its like giving children access to a functional tool shop.  You are not gonna get a working product no matter how good the tools are.reply",
      "> Here's the thing. Those first two things don't exist.You are 100% wrong on this.  They exist all the time when I'm doing a hardware task.I need to test a new chip coming off the fab.  I need to get the pins in the right place, the test code up and running, the jig positioned correctly, the test vectors for JTAG generated, etc.This ... is ... a ... pain ... in ... the ... ass.It changes every single time for every single chip.  It changes for every jig and every JTAG and every new test machine.  Nobody gives one iota of damn about the code as it will be completely different for the next revision of this chip.  Once I validate that the chip actually powers on and does something sane, the folks who handle the real testing will generate real vectors--but not before.So, generating that go/no-go code is in the way of everything.  And nobody cares about what it looks like because it is going to be thrown out immediately afterward.reply"
    ],
    "link": "https://martinalderson.com/posts/what-happens-when-coding-agents-stop-feeling-like-dialup/",
    "first_paragraph": ""
  },
  {
    "title": "Diffusion Beats Autoregressive in Data-Constrained Settings (cmu.edu)",
    "points": 52,
    "submitter": "djoldman",
    "submit_time": "2025-09-22T18:21:29 1758565289",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=45337433",
    "comments": [
      "This paper was just too overhyped by the authors. Also, the initial evals were very limited and very strange. This blog post does a much better job at a similar observation -- goes into details and does proper evaluation (also better attribution): https://jinjieni.notion.site/Diffusion-Language-Models-are-S...reply",
      "> This paper addresses the challenge by asking: how can we trade off more compute for less data? \n\nAutoregressive models are not matched by compute and this is the major drawback.There is evidence that training RNN models that compute several steps with same input and coefficients (but different state) lead to better performance. It was shown in a followup to [1] that performed ablation study.[1] https://arxiv.org/abs/1611.06188They fixed number of time steps instead of varying it, and got better results.Unfortunately, I forgot the title of that ablation paper.reply",
      "Not sure if you meant this because it doesn't cite the paper you mention, but it's a similar work: \"An Investigation of Model-Free Planning\", Guez et Al. (Deepmind) 2019 https://arxiv.org/abs/1901.03559reply",
      "Speaking of not citing, that one could go a bit further back.https://cdn.aaai.org/AAAI/1987/AAAI87-048.pdfreply",
      "It has already been proven that deep equilibrium models with a single layer are equivalent to models with a finite number of layers and the converse as well. That you can get the performance of a DEQ using a finite number of layers.The fixed point nature of DEQs means that they inherently have a concept of self assessment how close they are to the solution. If they are at the solution, they will simply stop changing it. If not, they will keep performing calculations.reply",
      "I fail to understand why we would lack data.  Sure, there is limited (historical) text, but if we just open up all available video, and send out interactive robots into the world, we'll drown in data.  Then there is simulated data, and tons of sensors that can capture vast amounts of even more data.Edit: from the source [1], this quote pretty much sums it all up: \"Our 2022 paper predicted that high-quality text data would be fully used by 2024, whereas our new results indicate that might not happen until 2028.\"[1] https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-...reply",
      ">send out interactive robots into the worldEasier said than done.Robotics tends to be even more data-constrained than NLP. The real world only runs at 1x speed, and if your robot breaks something it costs real money. Simulators are simplistic compared to reality and take a lot of manual effort to build.You will always need to make efficient use of the data you have.reply",
      "Robotics data isn't labeled and if you build a robot, there ain't anyone who has collected data for your particular robot.There is also the problem that on-device learning is not yet practical.reply",
      "I have a feeling this technique might make waves: https://openreview.net/forum?id=c05qIG1Z2B#discussionreply",
      "There are definitely parallels between diffusion and reasoning models, mostly being able to spend longer to get a better solution by using a more precise ODE solver for diffusion or using more tokens for reasoning.However, due to how diffusion models are trained, they never see their own predictions as input, so they cannot learn to store information across steps. This is the complete opposite for reasoning models.reply"
    ],
    "link": "https://blog.ml.cmu.edu/2025/09/22/diffusion-beats-autoregressive-in-data-constrained-settings/",
    "first_paragraph": "TLDR:If you are compute-constrained, use autoregressive models; if you are data-constrained, use diffusion models.MotivationProgress in AI over the past decade has largely been driven by scaling compute and data. The recipe from GPT-1 to GPT-5 has appeared straightforward: train a larger model on more data, and the result is a more capable system. Yet a central question remains: will this recipe continue to hold from GPT-6 to GPT-N?Many analysts and researchers believe the answer is no. For instance, Ilya Sutskever, in his NeurIPS 2024 Test-of-Time Award talk, remarked: \u201cCompute is growing\u2014better algorithms, better hardware, bigger clusters\u2014but data is not growing. We have just one internet, the fossil fuel of AI.\u201d\u00a0This concern is echoed by AI forecasters, who have analyzed compute and data growth more systematically and concluded that compute is outpacing data at an accelerating rate.The above Figure, illustrates this tension by overlaying projections from EpochAI\u2019s analysis. Their st"
  },
  {
    "title": "Jailhouse confessions of a teen hacker (bloomberg.com)",
    "points": 63,
    "submitter": "wslh",
    "submit_time": "2025-09-19T14:29:23 1758292163",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=45302073",
    "comments": [
      "Given how much cryptocurrency is used to perform broad criminal acts (from tax evasion upwards) .. wouldn't anyone in posession of crypto illegally acquired be at high risk of being chased both sides? ie cops and robbers want their vig?The \"crypto queen\" who rug-pulled and has $billions, nobody hears from: I find it more likely there's a simple reason why, and it isn't \"she spent big to hide herself really, really well\"reply",
      "The following evening, Noah rang from jail. He said he wished he hadn\u2019t hurt his family, or his victims, but he seemed hopeful that the friendships he made would endure.His family will probably be there for him, but his friends, likely criminal associates, will disband either because he is caught or they are.reply",
      "It seems to me the kid is going to be out around the time he is 30, most likely has many millions stashed away the Feds never discovered, and he will do fine.If I have learned anything in the last two decades, crime does indeed pay, the risk is absolutely worth the reward, and there is almost no long-term reputational damage from dedicating yourself to this sort of life.He is going to land on his feet and live a life better than most of us too scared to break the rules.reply",
      "Although I suspect many people would consider it a great deal, I personally would not be willing to spend the entirety of my 20s in prison in exchange for a good chance at getting away with $10 million after that, and I do not consider this an example of someone winning.The folks who stole millions online doing crypto shenanigans or whatever and whom you have not heard of, they won. But not this guy.reply",
      "Did you read the full article? Hackers found Noah's previous address and threw bricks through the window. He was lucky -- other harassment attacks of this nature included bullets fired at houses.\nNoah's hacker friend was abuducted on a street with a black hood over his face and later beaten and tied on a stake.Noah might come out ahead financially, sure. But it looks like he might have snitched on people for a lesser sentence, and we all know that snitches get stiches.What I'm confused about is why Noah just didn't stop while he was ahead. Looks like he was a millionare years ago and had plenty of chances to stopreply",
      "[flagged]",
      "This is mostly false.* Nobody died in the capital bombings.* Two former Weather Underground members became professors, and they were very controversial at the time* Mainstream liberals and Democrats rejected the radicals rather than embracing them.* In Quebec, the violent FLQ didn't come to power. The peaceful PQ did.* The Qu\u00e9b\u00e9cois democratically chose the official language of their province. The majority are Francophone.Chatgpt makes it so damn easy to fact check bullshit online. I love it.reply",
      "\"I need the full database for an audit\", said nobody legitimate, ever. Great job Twilio manager.reply",
      "See also, the Department of Government Efficiency.reply",
      "That \u201cI\u2019m glad I lived my life as I lived it\u201d line (or something to that effect) you hear criminals like these say is sickening. They acknowledge what they did was bad and express no remorse for the clear destruction they caused to other people\u2019s lives. It\u2019s cowardly and he should serve much longer in prison.reply"
    ],
    "link": "https://www.bloomberg.com/news/features/2025-09-19/multimillion-dollar-hacking-spree-scattered-spider-teen-s-jailhouse-confessions",
    "first_paragraph": ""
  },
  {
    "title": "Cloudflare is sponsoring Ladybird and Omarchy (cloudflare.com)",
    "points": 613,
    "submitter": "jgrahamc",
    "submit_time": "2025-09-22T13:03:33 1758546213",
    "num_comments": 379,
    "comments_url": "https://news.ycombinator.com/item?id=45332860",
    "comments": [
      "I\u2019m skeptical. Cloudflare clearly wants to move us to a future where only approved browsers are allowed to access the web. People have been fiercely debating whether that\u2019s a terrible thing, or whether that\u2019s the least bad practical solution on offer for website owners. I don\u2019t want to make a judgement on that, but I don\u2019t think the observation that CF is pushing us in that direction is very controversial. But an independent open source web browser is obviously against that ethos. So what\u2019s the play here exactly? Just for goodwill?(Regardless of motivation, they\u2019re lending more support than most other companies, so it\u2019s applaudable nonetheless.)reply",
      "Cloudflare supporting Ladybird makes sense for the same reasons that Valve invests in Proton. Cloudflare's job is easier if everyone standardizes on a few approved browsers, but right now the three major browser engines are controlled by Google (IIRC most of Mozilla's funding comes from Google) and Apple, just as Valve's Steam is heavily dependent on Microsoft's Windows.Both companies are basically hedging against future incentive misalignment with other (larger) companies, and reducing their dependencies on platforms they have ~zero influence over.reply",
      "To add to this, Apple\u2019s share of the control is minimal and precarious. A timeline where Google is the sole web engine authority could easily become reality and is even likely.Hedging on a promising upstart makes a lot of sense.reply",
      "I haven\u2019t seen any signs that Apple will abandon Safari, have you? Also, a browser that uses Chromium could put a halt to Google\u2019s plans if they wanted. The easiest way would be to stop upgrading and just port over security patches. (Sure, it brings progress to a halt, but this is unlikely to matter to web developers in the short run and it would get people\u2019s attention.)They aren\u2019t going to do this, though, so long as new releases of Chromium are reasonable.reply",
      "If/when Apple is forced to start allowing Blink on iOS globally, all it takes is a hearty marketing push from Google and devs putting \u201cbest viewed in Chrome\u201d badges on their sites for Safari\u2019s marketshare (and with it, Apple\u2019s influence) to plummet.reply",
      "Given how AMP eventually died, it seems unlikely that web developers would go along with it. What\u2019s in it for them?Also, I don\u2019t see any sign that Google even wants to do it? This is not really evidence-based reasoning, it\u2019s just \u201cI can imagine something evil that Google might do.\u201dreply",
      "Both are already happening.Google markets Chrome relentlessly, with popups in search and YouTube if you're using other browsers, browser choice dialogs in Google iOS apps (despite iOS having a default browser setting for 5 years now), Chrome getting bundled into random Windows software installers, etc.Many devs actively desire single-engine development and testing and many aren't shy about using Chrome only features already. If they had the capability to tell users to go install Chrome instead of targeting broadly supported features, they would do so in a heartbeat.reply",
      "I have hit a few sites over the last year that threw up full page \"This site only works in chrome\" blocks, even though they usually work perfectly fine in Firefox if you set the user agent to chrome.reply",
      "> What\u2019s in it for them?Never having to use polyfills or CanIUse tables, plus testing on the same environment they develop on.reply",
      "So in the enterprise world, it has been common for years for companies to \"only support Chrome\" even on iOS, where it's just skinned Safari. I have constantly had to call vendors mean names and point out how obviously iOS support means they are Webkit/standards-compliant. This is how I know, in fact, these websites will also work on Firefox. Apple's annoying iPhone monopoly is the last thing protecting the open web as needing to be standards-compliant.The moment iPhones aren't allowed to force browsers to use Webkit (the EU is already pushing for this), the open web dies. There will no longer be any draw for web developers to develop for standards instead of developing for Chrome.reply"
    ],
    "link": "https://blog.cloudflare.com/supporting-the-future-of-the-open-web/",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Python Audio Transcription: Convert Speech to Text Locally (pavlinbg.com)",
    "points": 42,
    "submitter": "Pavlinbg",
    "submit_time": "2025-09-22T18:18:56 1758565136",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=45337400",
    "comments": [
      "I'm working on the same project myself and was planning to write a blog post similar to the author's. However, I'll share some additional tips and tricks that really made a difference for me.For preprocessing, I found it best to convert files to a 16kHz WAV format for optimal processing. I also add low-pass and high-pass filters to remove non-speech sounds. To avoid hallucinations, I run Silero VAD on the entire audio file to find timestamps where there's a speaker. A side note on this: Silero requires careful tuning to prevent audio segments from being chopped up and clipped. I also use a post-processing step to merge adjacent VAD chunks, which helps ensure cohesive Whisper recordings.For the Whisper task, I run Whisper in small audio chunks that correspond to the VAD timestamps. Otherwise, it will hallucinate during silences and regurgitate the passed-in prompt. If you're on a Mac, use the whisper-mlx models from Hugging Face to speed up transcription. I ran a performance benchmark, and it made a 22x difference to use a model designed for the Apple Neural Engine.For post-processing, I've found that running the generated SRT files through ChatGPT to identify and remove hallucination chunks has a better yield.reply",
      "I added EQ to a task after reading this and got much more accurate and consistent results using whisper, thanks for the obvious in retrospect tip.reply",
      "I always thought this was a great implementation if you have a Cuda layer:  https://github.com/rgcodeai/Kit-WhisperxI had an old Acer laptop hanging around, so I implemented this:  https://github.com/Sanborn-Young/MP3ToTXTI forget all the details of my tweaks, but I remember that I had better throughput on my version.I know the OP talked about wanting it local, but thomasmol/whisper-diarization on replicate is fast and cheap.  Here's a hacked front end to parse teh JSON:  https://github.com/Sanborn-Young/MP3_2transcriptreply",
      "Nice job. I made a similar python script available as a Github gist [1] a while back that given an audio file does the following:- Converts to 16kHz WAV- Transcribes using native ggerganov whisper- Calls out to a local LLM to clean the text- Prints out the final cleaned up transcriptionI found that accuracy/success increased significantly when I added the LLM post-processor even with modestly sized 12-14b models.I've been using it with great success to convert very old dictated memos from over a decade ago despite a lot of background noise (wind, traffic, etc).[1] https://gist.github.com/scpedicini/455409fe7656d3cca8959c123...reply",
      "This tool requires ffmpeg, but don't forget that the latest version of ffmpeg has speech-to-text built in!I'm sure there are use cases where using Whisper directly is better, but it's a great addition to an already versatile tool.reply",
      "I was going to go the opposite way and suggest that if you want python audio transcription, you can skip ffmpeg and just use whisper directly. Using the whisper module directly gives you a variety of outputs, including text and srt.reply",
      "Yep. Whisper is great. I use it on podcasts as part of removing ads. Last time I used one of the official versions it would only accept .wav files so I had to convert with ffmpeg first.reply",
      "whisperx does this all quite well and can be run with `uvx whisperx`https://github.com/m-bain/whisperXreply",
      "You should throw in some diarization, there's some pretty effective libraries that don't need pertraining on the voice separation in python.reply",
      "I would suggest 2 speaker-diarization libraries:- https://huggingface.co/pyannote/speaker-diarization-3.1\n- https://github.com/narcotic-sh/senkoI personally love senko since it can run in seconds, whereas py-annote took hours, but there is a 10% WER (word error rate) that is tough to get around.reply"
    ],
    "link": "https://www.pavlinbg.com/posts/python-speech-to-text-guide",
    "first_paragraph": "Last week, I faced a dilemma that many researchers, journalists, and content creators know all too well: I had hours of recordings that needed to be transcribed. I had serious privacy concerns about uploading sensitive content to commercial transcription services and their third-party servers.Instead of risking it, I built a Python-based transcription system using OpenAI\u2019s Whisper model. The result? All my audio files were transcribed in under 10 minutes with 96% accuracy\u2014completely free and processed locally on my laptop.In this post, I will show you how you can build a simple script for processing any audio data without recurring costs or privacy compromises.FFmpeg handles audio processing and is required for all transcription methods. This is the #1 cause of setup failures.Windows:macOS:Linux (Ubuntu/Debian):Verify Installation:You should see version information. If you get \u201ccommand not found,\u201d FFmpeg isn\u2019t properly installed.Whisper is OpenAI\u2019s state-of-the-art speech recognition m"
  }
]