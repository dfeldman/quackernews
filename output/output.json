[
  {
    "title": "Browsing negative content online makes mental health struggles worse (news.mit.edu)",
    "points": 17,
    "submitter": "topato",
    "submit_time": "2024-12-08T00:16:21 1733616981",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42353944",
    "comments": [
      "I just created a regex-based content blocker that literally blacks out text content on any webpage.2 modes - block only matches, or, block surrounding text (too)I entered it into Google Built-in AI hackathon the other day.Uses LLM to scan blocked content to determine if regex was too trigger happy, then lets user unblock.\n \nreply",
      "So impressive the way MIT adds to the depth and breadth of human knowledge.\n \nreply",
      "sounds obvious, yes?\n \nreply",
      "Researchers have developed a web plug-in to help make better/informed online decisions\n \nreply",
      "Ad blocker?\n \nreply"
    ],
    "link": "https://news.mit.edu/2024/study-browsing-negative-content-online-makes-mental-health-struggles-worse-1205",
    "first_paragraph": "Suggestions or feedback?\n\n\n\n\n\n\n\n\n\n\n\n\nPrevious image\nNext image\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeople struggling with their mental health are more likely to browse negative content online, and in turn, that negative content makes their symptoms worse, according to a series of studies by researchers at MIT.The group behind the research has developed a web plug-in tool to help those looking to protect their mental health make more informed decisions about the content they view.The findings were outlined in an open-access paper by Tali Sharot, an adjunct professor of cognitive neurosciences at MIT and professor at University College London, and Christopher A. Kelly, a former visiting PhD student who was a member of Sharot\u2019s Affective Brain Lab when the studies were conducted, who is now a postdoc at Stanford University\u2019s Institute for Human Centered AI. The findings were published Nov. 21 in the journal Nature Human Behavior.\u201cOur study shows a causal, bidirectional relationship between health and what you d"
  },
  {
    "title": "Just: Just a Command Runner (just.systems)",
    "points": 331,
    "submitter": "thunderbong",
    "submit_time": "2024-12-07T17:11:32 1733591492",
    "num_comments": 210,
    "comments_url": "https://news.ycombinator.com/item?id=42351101",
    "comments": [
      "I'm not a fan. It works well for what it is, but what it is is an additional language to know in a place where you probably already have one lying around.Also, like make, it encourages an imperative mode for project tooling and I think we should distance ourselves from that a bit further. It's nice that everybody is on the same page about which verbs are available, but those verbs likely change filesystem state among your .gitignored files. And since they're starting from an unknown state you end up with each Just command prefixed by other commands which prepare to run the actual command, so now you're sort of freestyling a package manager around each command in an ad-hoc way when maybe it's automation that deserves to be handled without depending on unspecified state in the project dir.None of this is Just's fault. This is people using Just poorly. But I do think it (and make) sort of place you on a slippery slope. Wherever possible I'd prefer to reframe whatever needs doing as a build and use something like nix which is less friendly up front, but less surprising later on because you know you're not depending on the outputs of some command that was run once and forgotten about--suddenly a problem because the new guy can't get it to work and nobody else remembers why it works on theirs.\n \nreply",
      "I find declarative build systems end up pretty frustrating in practice. What I want from a build often isn't the artifacts, but the side effects of producing the artifacts like build output or compilation time. You get this \"for free\" from an imperative tool, but represents a significant feature in a declarative system that's usually implemented badly if it's implemented at all. The problem gets worse the smarter your tool is.\n \nreply",
      "Logs emitted during the build, or test results, or metrics captured during the build (such as how long it took)... these can all themselves be build outputs.I've got one where \"deploying\" means updating a few version strings and image reverences in a different repo.  The \"build\" clones that repo and makes the changes in the necessary spots and makes a commit.  Yes, the side effect I want is that the commit gets pushed--which requires my ssh key which is not a build input--but I sort of prefer doing that bit by hand.\n \nreply",
      "The developer time required to learn and properly use nix makes it unattractive to most teams. \nThe benefits don't outweigh the costs of adoption.Instead of debugging code, the team would have to spend significant time maintaining the build system for the build systems sake.  \nDon't get me wrong, I want something nix-like in my toolbox.\nI want to love nix. \nBut I wouldn't dare to argue my team to commit to the world of pain that comes with it.There's a good reason that nix didn't see wide adoption in the industry.\n \nreply",
      "In my experience, Nix is very high leverage. My company has ~5 nix gurus, but Nix is invisibly used by hundreds of engineers. Most engineers know we use Nix and that's about it.\n \nreply",
      ">What I want from a build often isn't the artifacts, but the side effects of producing the artifacts like build output or compilation timeYou frequently build things not to get binaries but to spend time compiling?\n \nreply",
      "The point is that there's often no way way to express \"I want side effects\" in declarative tools, and the number of side effects that might be useful is vast.For example, sometimes I profiling the build times to see where I should focus effort.Sometimes I want to see it to quickly check for issues where adding some dependency header causes build times to explode 100% in downstream dependencies during cold builds.Another common occurrence for is trying to debug a platform, toolchain, or standard library issue and the build system either doesn't detect changes in those components or only makes the components readily accessible in an internal cache that's subject to invalidation issues. You'll usually get the wrong artifact or test results in those cases.Some other systems (e.g. bazel/blaze comes to mind) actively try to hide side effects like stdout.In all of these cases, the only way to actually get these side effects is to reach into the tool's internals by blowing away caches/output folders or reading live log files. That's a failure of the build tool.\n \nreply",
      "Generally my aim with both Nix and Bazel are that, while they are the source of truth, day-to-day development and debugging occurs using language-native tools. So the only touch point for local development is when you are modifying the dependency graph in some way.It's definitely more work (you need to maintain compatibility with two different build systems), but worth it for exactly these reasons.\n \nreply",
      "I haven\u2019t used it, but it sounds like make\u2019s \u2014-assume-new flag does exactly what you want for the first part.  It lets you rebuild everything that would result from a changed file, including all side effects, without needing to first update the file.\n \nreply",
      "--always-make/-B is more in line, but yeah. Make has grown imperative models within its vast declarative morass.\n \nreply"
    ],
    "link": "https://just.systems/",
    "first_paragraph": ""
  },
  {
    "title": "MIT largest open-source car design dataset, incl aerodynamics, to speed design (news.mit.edu)",
    "points": 165,
    "submitter": "toss1",
    "submit_time": "2024-12-06T18:07:35 1733508455",
    "num_comments": 60,
    "comments_url": "https://news.ycombinator.com/item?id=42342346",
    "comments": [
      "Somewhat related: Can someone tell me why all those new electric cars are so poorly designed in terms of size and weight? Most of them weight over 2t and have a large frontal area (VW ID.4 is the prime example). I just want an economical family vehicle (lightweight, hatchback design) and there aren't that many options. Isn't that something loads of people want or am I the exotic one?\n \nreply",
      "Had it for just over a year, but so far happy with the Renault Megane e-Tech[1]. It has a kerb weight of around 1700kg, of which 400kg is the battery[2].It's smaller than the ID.4 so fits in our small garage, great turning radius which is nice in the city, and physical buttons for the most important stuff like climate and media. It can tow up to 900kg and supports a roof rack.YMMV and such.[1]: https://ev-database.org/car/1521/Renault-Megane-E-Tech-EV60-...[2]: https://cdn.group.renault.com/ren/gb/transversal-assets/broc...\n \nreply",
      "Carmakers have been tending towards SUV and crossover designs because they make more money on them.  The ID.4 is a crossover, basically an SUV on a car frame rather than a truck frame.  I too wish they made more compact cars, but it seems that they would rather produce what is more profitable for them rather than what customers want.\n \nreply",
      "I\u2019m not really disagreeing with your broader point, but consumer demand has been tracking toward trucks and SUVs for quite a while now. I would imagine there is a bit of a \u201ctail wagging the dog\u201d situation there, and it\u2019s also probably relevant that the average age of a new car buyer has been rising as wealth distribution has skewed further upward.But that\u2019s great news, if there\u2019s truly a gap in the famously difficult automotive market, maybe a scrappy new player can break through with a simple, small sedan\u2014one that can be made cheaply, and sold affordably\u2014and then their supply chain can be forced into a series of buy outs and hostile takeovers, creating the next generation of wealthy donors to political action groups. It\u2019s the ciiiirrcle of liiiife etc..\n \nreply",
      "Crossovers in that zone are a pretty reasonable compromise of cost, handling, safety, and ergonomics. They have essentially replaced station wagons and are mostly built the same way, just a little taller.\n \nreply",
      "I don't disagree, but I just prefer compact cars.  If I had to buy a new vehicle and there were no compacts available, I'd go for a crossover, but I won't like it.\n \nreply",
      "So are 'most' petrol cars, people want space for a family plus fair amount of luggage and end up at Model 3 size minimum to cover that spec.If you want a small EV what about;Dacia Spring ElectricLeapmotor T03Citroen e-C3Vauxhall FronteraVauxhall CorsaThe list goes on.\nRenault 5 E-Tech\n \nreply",
      "EV Battery packs are very heavy and dense. The ID4 has an aerodynamic Cd score of 0.28 - Mach-E has a Cd of 0.29 and Model Y has a Cd of 0.23, so its in the middle of the pack.\n \nreply",
      "I am pretty sure, that \u201clightweight\u201d and \u201cfamily\u201d are not easy to combine unless you\u2019re family of three without the need for a luggage.Those new electric cars need big batteries just to be somehow usable. Recently consumed 35 kWh for 65 miles ride on model Y with trailer in cold weather. That makes <150 miles total winter range with trailer with fully charged battery.\n \nreply",
      "Most petrol family cars are less than 1500 kg in Europe. Americans are used to overweight cars and that's spreading to here too. Though it's often a problem since most city parking spots are smaller than huge cars like Tesla model Y.\n \nreply"
    ],
    "link": "https://news.mit.edu/2024/design-future-car-with-8000-design-options-1205",
    "first_paragraph": "Suggestions or feedback?\n    Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \n    Creative Commons Attribution Non-Commercial No Derivatives license.\n    You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided \n    below, credit the images to \"MIT.\" \n  \n\n\n\n\n\n\n\n\n\n\n\n\nPrevious image\nNext image\n\n\n\n\n\n\n\n\n\n\n\n\n\nCar design is an iterative and proprietary process. Carmakers can spend several years on the design phase for a car, tweaking 3D forms in simulations before building out the most promising designs for physical testing. The details and specs of these tests, including the aerodynamics of a given car design, are typically not made public. Significant advances in performance, such as in fuel efficiency or electric vehicle range, can therefore be slow and siloed from company to company.MIT engineers say that the sear"
  },
  {
    "title": "Show HN: Countless.dev \u2013 A website to compare every AI model: LLMs, TTSs, STTs (countless.dev)",
    "points": 231,
    "submitter": "ahmetd",
    "submit_time": "2024-12-07T09:42:59 1733564579",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=42348513",
    "comments": [
      "One thing that stands out playing with the sorting is that Google's Gemini claims to have a context window more than 10x that of most of its competition. Has anyone experimented with this to see if its useful context window is actually anything close to that?In my own experiments with the chat models they seem to lose the plot after about 10 replies unless constantly \"refreshed\", which is a tiny fraction of the supposed 128000 token input length that 4o has. Does Gemini actually do something dramatically differently, or is their 3 million token context window pure marketing nonsense?\n \nreply",
      "What tactics do you use to refresh while using them?\n \nreply",
      "I don't have a strategy that I like\u2014it just amounts to having to say \"you forgot about requirement X, try again keeping that in mind\".\n \nreply",
      "I tend to use a sentence along these lines:\n\"Give me a straightforward summary of what we discussed so far, someone who didn't read the above should understand the details. Don't be too verbose.\"Then i just continue from there or simply use this as a seed in another fresh chat.\n \nreply",
      "OP, were you inspired by this LLM comparison tool?https://whatllm.vercel.appThe tables are very similar - though you've added a custom calculator which is a nice touch.Also for the Versus Comparison, it might be nice to have a checkbox that when clicked highlights the superlative fields of each LLM at a glance.\n \nreply",
      "Data in this tool is from https://artificialanalysis.ai/ on October 13 2024 and so is a little of out date.This page has up to date information of all models and providers: https://artificialanalysis.ai/leaderboards/providers\nWe also on other pages cover Speech to Text, Text to Speech, Text to Image, Text to Video.Note I'm one of the creators of Artificial Analysis.\n \nreply",
      "Thanks for sharing. That's a better tool.\n \nreply",
      "Both seem to have great value. Some information is missing from Vercel's tables.\n \nreply",
      "I feel like the number is still a bit lacking, especially since many models made by Chinese companies are not represented, like speech-to-text.As far as I know, there's a volcano engine in China that has impressive text-to-speech capabilities. Many local companies are using this model.\n \nreply",
      "I like the idea of more comparisons of models. Are there plans to add independent analyses of these models or is it only an aggregation of input limits?How do you see this differing from or adding to other analyses such as:https://artificialanalysis.aihttps://huggingface.co/spaces/TTS-AGI/TTS-Arenahttps://huggingface.co/spaces/hf-audio/open_asr_leaderboardhttps://huggingface.co/spaces/TIGER-Lab/GenAI-ArenaGreat work on all the aggregation. The website is nice to navigate.\n \nreply"
    ],
    "link": "https://countless.dev/",
    "first_paragraph": "See and compare every AI model easily. 100% free & open-source."
  },
  {
    "title": "The famed Notre Dame Cathedral in Paris reopens to great fanfare (apnews.com)",
    "points": 49,
    "submitter": "chmaynard",
    "submit_time": "2024-12-07T22:05:15 1733609115",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42353215",
    "comments": [
      "What a beauty!  Its been rebuilt (restored is a tricky concept here) in largely the same materials and in the same form as the original, prior to various restoration efforts through the centuries, with a few knobs on.This means that we get a mediaeval cathedral looking like it did when it was conceived and built (with an extra spire and a few other things).  The colours are amazing.Elderly churches, mosques and temples (int al) have a habit of losing their original colours and \"feel\" across the centuries.  They change - age.  Stone walls age and thanks to modern pollution darken.  Pigments age, disperse and peel off.Notre Dame has been restored.  Not to how it was in 2018, prior the fire ... but to how it was intended when built, with a bit of sympathetic interpretation.Well done!\n \nreply",
      "One of the most spiritual moments of my life was entering Notre Dame, and I was not expecting it at all, wasn't really excited to go there, and am not really religious. Honestly I expected Stonehenge to be more impactful but Notre Dame is a way more worthy visit.\n \nreply",
      "I wonder what would a ceremony like this look like before the era of cameras broadcasting every move to the entire planet simultaneously. It does seem like a lot of this is performative in the \u201cshowtime\u201d sense, and I wonder if the character of an event like this would be more \u201cpractical\u201d in another time.\n \nreply",
      "If cathedrals were practical, they'd look like big cross-shaped barns. To turn the restoration of a city's cathedral into a spectacle befits their nature. They were supposed to inspire awe.\n \nreply",
      "What does a practical opening ceremony even mean? It's in the name, it's a ceremony, a ritual.Even if it wasn't being broadcasted, this is an act of prestige that flows both ways. Power people go these events to lend their gravitas, and power people go to these events to borrow from the gravitas.\n \nreply"
    ],
    "link": "https://apnews.com/article/notre-dame-paris-latest-e50813cf016f08607c20ab115bc4b153",
    "first_paragraph": ""
  },
  {
    "title": "Arctic uses a fan to cool another fan (tomshardware.com)",
    "points": 14,
    "submitter": "LorenDB",
    "submit_time": "2024-12-08T00:28:08 1733617688",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.tomshardware.com/pc-components/case-fans/arctic-uses-a-fan-to-cool-another-fan-s12038-4k-120mm-fan-adds-an-extra-fan-in-its-center-to-cool-its-central-shaft",
    "first_paragraph": "I heard you like fans, so I put a fan in your fan so you can fan while you fan.\nWhen you purchase through links on our site, we may earn an affiliate commission. Here\u2019s how it works.\nArctic has released a new cooling fan archetype for server use, the S12038-4K and -8K. The fan is instantly visually unique thanks to its seven bonus fan blades in its direct center, creating a fan-within-a-fan effect. The fan offers class-leading static pressure, though Arctic seems reticent to advertise its new standout.The S12038's spec sheet is rather impressive on several fronts. Its advertised power consumption at max load, 3.96W, is 12% lower than the next closest server-grade competitor. It generates an insane 11.45 mmH\u2082O of static pressure, around 3X-6X greater than Arctic's stable of standard case fans. This high static pressure and airflow level is near the best in class for server hardware. As the name implies, the S12038 can be had in flavors of 4,000 or 8,000 maximum RPM.The fan achieves this"
  },
  {
    "title": "GroMo (YC W21) Is Hiring (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-12-08T01:01:27 1733619687",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/gromo/jobs/pZHrmBK-senior-executive-finance",
    "first_paragraph": "App for independent agents to sell financial products in IndiaLocation: Gurgaon, India\nExperience: 3-5 years\nIndustry: Fintech/Startup\n\nJob Summary:The Senior Executive - Finance will play a crucial role in managing the company\u2019s financial operations, reporting, and compliance, while contributing to strategic financial planning. You will work closely with the leadership team, providing insights and analysis to drive financial decision-making and ensure optimal use of resources.2 rounds of face to face interview.Building India's largest financial products distribution companyAt GroMo , we deeply understand how financial products such as Demat Account, Saving Account, Loans, Insurance, etc. are sold and we want to empower Millions of agents and financial advisors to sell financial products with the power of technology.  Our ability to understand what customers want, build fast and iterate faster makes us stand far apart from our competitors.It all starts with the right team \u2014 a team that"
  },
  {
    "title": "Economics and Homemakers (thehomefront.substack.com)",
    "points": 26,
    "submitter": "mooreds",
    "submit_time": "2024-12-07T23:02:50 1733612570",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=42353540",
    "comments": [
      "Outside of economics, raising children is in society's interest. We need the next generation to grow up, become productive, and pay taxes. If a family elects to have one parent stay home, that parent is doing valuable work for society and should be compensated. At the very least they should be eligible for unemployment for the first two years of a child's life. I don't care about the externalities.\n \nreply",
      "> We need the next generation to grow up, become productive, and pay taxes.I think it would be even better if the next generation grew up, became productive, and learned to contribute to society in new ways outside of just the obvious monetary one. Overall, we need more non-monetary contributions to be recognized.\n \nreply",
      "SFSU had to declare a financial emergency due to declining enrollment [1].[1]: https://www.sfchronicle.com/sf/article/sf-state-declares-fis...\n \nreply",
      "IMO a government subsidy for homemakers will in reality be a subsidy for corporations who can now pay the working parent even less. What needs to be normalized is a single salary being enough to support a family of four, like it used to be.\n \nreply",
      "> What needs to be normalized is a single salary being enough to support a family of four, like it used to beIs this an American thing or just a rich people thing? I can\u2019t think of a single generation in my family going back to pre-ww2 where this was true. Everyone always worked. At least on the family farm.The farm generations actually had you start working in elementary school. Our school system (Slovenia) even has school holidays aligned so farm kids can help with the harvest. Let alone having adults around who aren\u2019t working.\n \nreply",
      "I was thinking about this the other day. What we need is a tax on businesses that don't pay the area's single-income, livable wage. If companies pay more, they get out of the tax. If they pay less, they pay the tax. That tax then goes into making things like childcare more affordable. It's a way of forcing companies to internalize the externalities of paying below-livable wages.\n \nreply",
      "Why is this downvoted? This is similar to Bernie Sanders\u2019 idea to tax companies whose employees are on welfare\n \nreply",
      "Normalized how?\n \nreply",
      "I think one of the biggest disasters of modern society is to relegate almost every sort of community task such as homemaking, assisting neighbors, etc. to the economic system. The immediate effect of such a relegation is the breaking apart of communities for the sake of economic efficiency: the person who takes care of the children is no longer the parent, the person who helps you fix your porch is no longer your neighbour, and the person you ask for a ride is now an Uber.Yes, it may make for a more convenient society in some ways but I wonder if the cumulative effect is worth it? I don't think so. Economists hail efficiency and comparative advantage as a good thing, but it is also a reduction of true value to a single variable (money) which is a dimensional collapse that strips humanity from human beings.The short-term results seem good, but because in the long-term, the process never stops, the END result is a transformation of human beings from people into cogs in an economic machine that has no soul.\n \nreply",
      "Just FYI, \"TheTrumpet\" article linked to and used as corroborating evidence is a religious site.The post links to a article that ends with this summary:> God created the family to excel by taking advantage of specialization of labor \nbetween husband and wife.> The Trumpet explains the real meaning behind world events. We are a news organization that connects world events with biblical prophecies.Just because household labor is not taxed, does not mean it does not happen.\n \nreply"
    ],
    "link": "https://thehomefront.substack.com/p/economics-and-homemakers",
    "first_paragraph": ""
  },
  {
    "title": "Deepfakes weaponised to target Pakistan's women leaders (france24.com)",
    "points": 17,
    "submitter": "mostcallmeyt",
    "submit_time": "2024-12-08T00:15:13 1733616913",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.france24.com/en/live-news/20241203-deepfakes-weaponised-to-target-pakistan-s-women-leaders",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: I built an HTML5 RTL-SDR application (ea1iti.es)",
    "points": 6,
    "submitter": "jtarrio",
    "submit_time": "2024-12-07T22:36:29 1733610989",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42353390",
    "comments": [
      "Awesome! There's so much you can do with the browser now. Thanks for making an SDR app that's \"gone native.\"\n \nreply",
      "nit: The WebUSB API isn't really a part of \"HTML 5\"https://wicg.github.io/webusb\n \nreply",
      "This is cool!\n \nreply",
      "Excellent work!\n \nreply",
      "Wow that\u2019s cool\n \nreply"
    ],
    "link": "https://radio.ea1iti.es/",
    "first_paragraph": ""
  },
  {
    "title": "Hiroshi Nagai: Japan's Sun-Drenched Americana (tokyocowboy.co)",
    "points": 205,
    "submitter": "neom",
    "submit_time": "2024-12-01T20:53:44 1733086424",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=42290663",
    "comments": [
      "Japanese artistic depictions of America have an interesting way of having an \"accent\" so to speak but in a manner that doesn't mistranslate but rather adds something unique to it. I'm reminded of a series of illustrations from the 1800s explaining the American revolution for a Japanese audience where all the depictions of the important historical characters look like traditional samurai drawings and they take on a sort of mythological character to them. Its like different enough to appreciate that its different while also familiar enough to understand what its saying.https://www.reddit.com/r/history/comments/woqaku/the_fully_s...\n \nreply",
      "Interesting point. I feel the same about the old SNES classic, Earthbound.It was a different perspective on America, making small towns and suburbia (a sometimes looked down upon aspect of the country) look appreciated, cozy, nice.\n \nreply",
      "Hah! If you want to step into a Hiroshi Nagai painting as a 3D world, that's basically what happens in my Ambient Garden project. In fact I was surprised that nobody ever pointed it out despite all the visitors: https://ambient.garden/Editing to respond to multiple replies: Yes, he's painted a series of landscapes with that specific pointillism technique. The best I could find is a pretty random link, but it might be the most relevant painting: https://fortinbrah.wordpress.com/wp-content/uploads/2020/06/...\n \nreply",
      "This is more impressionist pointillist than Nagai\u2019s airbrush style.\n \nreply",
      "Excellent, edit, that painting looks almost exactly like your world to me. Very cool project.\n \nreply",
      "The dots make that site look very pointillist to me, and the colors are pretty impressionistic too. And it's just paths through a hilly landscape with trees as far as I can tell, none of the beaches or architectural and urban settings in Hiroshi Nagai's paintings. Maybe I'm missing something though.\n \nreply",
      "Maybe they call it 1950s, but to me this oozes late 80's video games.Sierra.\n \nreply",
      "Here is the artist\u2019s website http://www.hiroshinagai.com/contents.html\n \nreply",
      "Very interesting website as well. I like to enter throught the home URL:http://hiroshinagai.com/It's funny his BBS service says they were going dark in 2022. I was also curious how he sells things and it took some searching to find:http://hiroshinagai.com/fmcd/collectors/collectors_catalog/c...Or you can go to his gallery in person :)\n \nreply",
      "Nagai's work seems to have been an inspiration for the late 1980's/early 90's Sega aesthetic (OutRun, Sonic, etc.)\n \nreply"
    ],
    "link": "https://www.tokyocowboy.co/articles/uy1r8j003qdvb4ozr4qgplhd3yujyn",
    "first_paragraph": "Hiroshi Nagai, born in the Tokushima countryside in 1947, is known for his prolific output of pop style paintings that came to serve as a backdrop to the City Pop movement of the late 70s and 80s in Japan.Growing up, Nagai watched his father create oil paintings and developed a passion for art himself. He moved to Tokyo in his youth hoping to enter an art school but was unable to gain admission anywhere. Needing money, he initially began work as a set decorator for a Tokyo artist. Eventually, as he made more of a name for himself, he was able to move into creating illustrations.Inspired heavily by the concepts behind American pop art and the styles of British pop artists such as David Hockney, Nagai focused on imaginations of a 1950s Americana landscape. Adapting the deep blue skies, relaxed ocean side settings and sleepy nighttime cityscapes from previous pop artists, Nagai developed his own style throughout the late 1970s. His work finally began to gain traction in Japan around the t"
  },
  {
    "title": "Beekeepers halt honey awards over fraud in global supply chain (theguardian.com)",
    "points": 165,
    "submitter": "a_w",
    "submit_time": "2024-12-01T03:49:12 1733024952",
    "num_comments": 162,
    "comments_url": "https://news.ycombinator.com/item?id=42286030",
    "comments": [
      "Here's a very straightforward guide to buying honey. If you see honey at a supermarket, no you don't, that's sugar syrup. If you see honey online, no you don't, that's sugar syrup. If you find an old dude with a small stand and a bunch of (most likely unlabelled) jars who only accepts cash, that's where you get real honey.\n \nreply",
      "But all the honey labels say the only ingredient is honey. We've always been able to assume that with things like jams, or any other food, when manufacturers adulterate it with things like corn syrup, or even mild poisons, they proudly say so on the label. To not do so is fraud. How can people just claim that an entire industry is committing fraud??? Even this article doesn't mention anything about proof, just suspicions. Why can't they prove it? To make these kind of claims without proof is arguably worse than fraud. This would all be outrageous if it's true, since it becomes impossible to make any rational choices as consumers if the food system has gone fraudulent.\n \nreply",
      "Note this also happens with olive oilhttps://www.europol.europa.eu/media-press/newsroom/news/11-o...https://www.oliveoiltimes.com/business/europe/top-italian-po...\n \nreply",
      "To a first approximation it's accurate to assume any imported olive oil on a grocery store shelf in the US is fraudulent. The only kind I buy now is 100% grown in California and certified by the California Olive Oil Council. And it is very expensive.\n \nreply",
      "I'm sorry you're just hearing this only right now, but unfortunately it's been happening for a while, and we've also known it for a while.I.e. https://food.ec.europa.eu/food-safety/eu-agri-food-fraud-net... found widespread fraud and put measures in place to prevent it, but it continues to be challenging.\n \nreply",
      "The phrasing in the article about \"suspected to be fraudulent\" is over-cautious, probably because of Britain's very generous libel laws covering newspapers.\n \nreply",
      "If you find this hard to believe, I recommend reading this Forbes article [0] which gives some pretty stark numbers. I've included a few select quotes below:> According to the sampling and monitoring work carried out by the Brussels-based body, almost 50% of the honey from non-European countries is cut with sugar syrups made from rice, wheat or sugar beet.> All the 10 honeys entered via the United Kingdom were marked \u201cnon-compliant\u201d and mixed with imports from Mexico, Ukraine and Brazil.> Apart from the main fraudulent addition of sugar syrups, the report also alerts of the presence of additives and colorings and the falsification of traceable information.So yeah, a considerable part of honey contains more than what's on the label and often isn't of the origin written on the label. As for outrageous, it is \u2014 beekeepers have been sounding the alarm on this issue for years \u2014 but nothing has been done to stop this on the policy side.[0] https://www.forbes.com/sites/ceciliarodriguez/2023/03/24/hal...\n \nreply",
      "fyi, \"forbes.com/sites\" is just some blogging platform.In this case, it's mostly right, but the original source is https://food.ec.europa.eu/food-safety/eu-agri-food-fraud-net...\n \nreply",
      "It makes total sense for European farmers to cry foul if cheaper imports get to masquerade as the real thing. This has been a big sticking point for the EU-Mercosur deal that recently concluded.\n \nreply",
      "That\u2019s because honey is sugar syrup with a liberal sprinkling of marketing. The sugar syrup they are blending it with doesn\u2019t have the right marketing and is therefore not honey.\n \nreply"
    ],
    "link": "https://www.theguardian.com/business/2024/nov/30/beekeepers-halt-honey-awards",
    "first_paragraph": "Warnings that genuine products are bulked out with cheaper sugar syrup prompt international congress to withdraw prizesThe World Beekeeping Awards will not award a prize for honey next year after warnings of widespread fraud in the global supply chain.Apimondia, the International Federation of Beekeepers\u2019 Associations, says it will showcase honey from around the world at its congress in Denmark, but for the first time make no awards for the product.The decision came as beekeepers and importers face a mounting crisis over the scale of fraud, with warnings that genuine products are bulked out with cheaper sugar syrup. Some common tests to detect fraud can easily be defeated, and beekeepers say there has been a failure by food watchdogs and the industry to combat the fraudsters.Apimondia said in a statement: \u201cWe will celebrate honey in many ways at the congress, but honey will no longer be a category, and thus no honey judging, in the World Beekeeping Awards. This change to remove honey a"
  },
  {
    "title": "Historically, 4NF explanations are needlessly confusing (minimalmodeling.substack.com)",
    "points": 49,
    "submitter": "thunderbong",
    "submit_time": "2024-12-07T15:36:42 1733585802",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=42350425",
    "comments": [
      "I think the typical presentation of 4NF makes more sense when you consider the ergonomics of data processing systems when relational modeling emerged. Predating computing, data processing was all centered around \"records.\" Relational databases are often also described in terms of \"records,\" relational modeling emerged out of an attempt to formalize data processing practices, the practitioners of the era were learning relational modeling from a data processing background, so it's no surprise that relational concepts are often presented \"as opposed to\" the way things were done in data processing.In traditional data processing systems, a \"record\" was far more independent than we think of in a relational database. Records were hand-punched onto cards or keyed onto magnetic tape to be sorted, summarized, or whatever operation you cared about by a data processing machine. In this environment, joins were extremely expensive operations, often requiring that the operator feed the card stack over and over again (a rather literal O(n^2)). So, the \"weird composed\" schema is absolutely what you would do. And a lot of computer software was built around the exact same concepts, which made sense anyway as computers often continued to use sequential-access storage devices with similar (but less severe) problems around joins. This era famously persisted for a long time, with relational modeling as an academic concept well predating successful implementations of relational databases.One could argue that all of the normal forms are pretty much \"stop doing it the old sequential access (card-stack) way and use random access concepts (keys and joins) instead.\"Of course that leaves the question of whether or not we should teach it that way... we don't tend to tell students about how memory is now random access, so perhaps by the same turn the historical approach isn't useful for teaching here. But it would undoubtedly be more effective if you give a little of the history.\n \nreply",
      "What might be needless is the \"4\" in \"4NF,\" because the first three (and a half) are obsolete. Or rather, as you mention, were only relevant when things were read sequentially. \"Normalization\" is really fine on its own.Things that aren't normalized are \"ad hoc,\" or really just \"input.\"\n \nreply",
      "I\u2019ve seen this design again and again, but not in relational databases. I usually see it in spreadsheets. I would be completely unsurprised to see a three column Excel sheet with Employee, Skill, and Language headers. (https://kerricklong.com/tmp/demo-4nf-need.jpg) The spreadsheet designer\u2019s intuition is that Skill and Language are lists, and Employee is a section header. Smith might have a few rows with his languages and skills listed, one per cell, in arbitrary top-to-bottom order and with no consideration for how language and skill in a row relate to each other\u2014only to Smith. The Employee column has a single merged cell for Smith, a number of rows tall it needs to be to contain the largest of his lists. When migrating this from a spreadsheet to a database, you lose merged cells as a feature and can no longer rely on order\u2014so you copy the value Smith to each cell that had been merged.\n \nreply",
      "It was and is still common to have non experts design databases.  And in many cases,  normal form doesn't make sense.  Tables with hundreds or thousands of columns are commonly the best solution.What is rarely stressed about NF is that update logic must exist someplace and if you don't express the update rules in a database schema, it must be in the application code.  Subtle errors are more likely in that case.In the 1990s I was a certified Novell networking instructor.  Every month their database mysteriously delist me.  The problem was never found but would have been prevented if their db was in normal form.  (Instead I was given the VP of education's direct phone number and he kept my records on his desk. As soon as I saw that I was delist, I would call him and he had someone reenter my data.)Adding fields to tables and trying to update all the application code seems cheaper than redesigning the schema.  At first.  Later, you just need to normalize tables that have evolved \"organically\", so teaching the procedures for that is reasonable even in 2024.\n \nreply",
      ">  The problem was never found but would have been prevented if their db was in normal form.Sorry to be pedantic - if they didn't find the problem, how do we know that was the solution?\n \nreply",
      "The origin of classic explanation probably comes from people attempting to computerize paper forms.  The school would have profiles for each teacher, probably a series of pages in a manila folder stored in a filing cabinet, that was filled out by hand/a typewriter.  A space for the name. A series of blanks to be filled in with a list of skills (that maxes out at however many blanks they could put on the page). A series of blanks for languages.  Woe is the teacher who knows more languages than the available blanks that are on the form!People still create tables today (yes, even in 2024) like (teacher, skill, language) as the row definition.  Someone looks at the list of information they are collecting, conclude that the only axis they need to query on is the teacher, and make wide tables with many columns with disjoint purposes, that are then difficult to query and are inflexible.Consider a library card catalog and the Dewey Decimal Classification system.  The cards are arranged in the drawers by the topics standardized in the DDC.  It looks like the major axis is the DDC numbering and their associated topics.  While a card catalog lets you search for specific books if you know the topic and the author, it can not easily tell you all the books by the same author, or the list of topics that an author covers.  Or find all the books that multiple authors have written together.  But if one was to take the card catalog as it existed and computerize it, the na\u00efve implementation would look like the unnormalized or lower normalization forms. The explanation of the progression of increasing normalization tells you how to incrementally achieve normalization given a data schema that was heavily influenced by the limitations of physical reality, such that a card catalog is, into a system that does not have those constraints.The example of storing the skills and language as a list in a column is grossly inefficient in usage (ignoring the inefficiencies in storage and performance) and ignores that the \"list\" in 4NF is actually the result set, the set of rows returned.  I suppose it could help one to think of the result set as horizontal columns (a list) rather than a vertical set of rows, but that's more of a side effect of the data presentation than the relational algebra underpinnings.  Despite that databases like postgres let you query and index on expressions applied to a JSON-typed column, you end up with something more complex, with different methods of enforcing data hygiene at the row and the individual column level, because you've got sets of data in a single column, when the set is the defined by the rows.  4NF lets you answer way more questions about the data than you might initially anticipate.  I've worked with a number of schemas that someone else created with no or little normalization that literally could not answer the desired questions.  In this example, improper normalization results in finding out the skills of a teacher is easy, but finding out if the school is weak in any areas and what they should hire for is hard. But when the data is 4NF, you can answer questions easily in both directions without jumping through any data conversion hoops and you can have high confidence that the data remains hygienic.\n \nreply",
      "A possible clue is that the weird composed form is, still, the only possible form in which you can actually get results out of your SQL database.Perhaps people worked backwards from sample query results, reasoning that an output that looks like that should come from a table that looks like that. Of course that begs the question of why that ever seemed like a sensible format for output, but apparently the SQL establishment has never seen fit to change it, so either the SQL people are all idiots or there's some advantage to that form.(Yes, I know there is an obsolete meaning of that phrase still recorded in some dictionaries, but I prefer to use live English)\n \nreply",
      "Excellent article.  1NF is part of the problem as the article mentions.I'm seen some articles say that atomic in 1NF means you cannot have things that can be decomposed.  All that matters is that you can perform the relational algebra on the table and get a sensible result.To the why of presenting normalisation from those wacky forms, I think that also comes from data ingestion.  You'll get a list of attributes for an employee from their CV and might naively just load that 'as is' into a table.\n \nreply",
      "\"The existence of the theory implies that the \u201ccomposed\u201d design somehow arises, naively(?) or naturally.\"Given how many first timers come up with really weird schema designs, I'm not necessarily surprised, although I agree presenting it as perhaps a default-but-wrong approach doesn't help much.\n \nreply"
    ],
    "link": "https://minimalmodeling.substack.com/p/historically-4nf-explanations-are",
    "first_paragraph": ""
  },
  {
    "title": "I algorithmically donated $5000 to Open Source (kvinogradov.com)",
    "points": 291,
    "submitter": "lorey",
    "submit_time": "2024-12-03T22:45:05 1733265905",
    "num_comments": 63,
    "comments_url": "https://news.ycombinator.com/item?id=42312469",
    "comments": [
      "> Value increases with # total downloads and LTM downloads on PyPI.While I applaud the OP for the initiative, if this ever takes off it will cause people to exploit the system in the following ways:1. Hammer the package registries with fake downloads, which will increase the financial burden on the registries both in terms of increased resource usage and in employing countermeasures to stop bad actors.2. People spamming the repositories of popular packages with PRs that just so happen to add a dependency on their own personal package, so that they can pick up transitive downloads. This increases the burden on package authors who will need to spend time rejecting all of these.So this approach carries the risk of possibly making things even worse for OSS maintainers.If a metric can be gamed, and there is a financial incentive to game it, it will be gamed. I coin this the \"this is why we can't have nice things\" law.\n \nreply",
      "> While I applaud the OP for the initiative, if this ever takes off it will cause people to exploit the system in the following waysIt's true that the metrics used in this story could lead to being exploited. But the value of the initiative is not in the specific method used to donate, but in the idea of finding worthy yet non-obvious projects to donate and in leading by example.If the initiative catches on, the community can find better, harder-to-exploit methods to find deserving targets, as for example it has happend with NGOs. This idea could create a healthy ecosystem that supports FLOSS software, just like the idea of a stock exchange supported the emergence of public traded corporations in the XVIII and XIX centuries.\n \nreply",
      "Exactly! The idea is to use available data for evaluating the value and risk of OSS and then allocate donations accordingly to the wide algo-based systemic index, not to a narrow set of manually picked projects (usually large or popular ones).The current algorithm is far from being perfect (it's an MVP) and will never be, but with more measurable inputs and after multiple iterations with the help of the community, it can lead to an analogue of \"S&P500\" for OSS, that's worth using for donating to reduce the risk of the global OSS supply chain we all rely on.As with publicly traded companies, having a decentralized set of private donors with skin in the game helps a lot to efficiently evolve the approach and make it harder to exploit in the future. And on the contrary, I would not trust an algorithm created and maintained by some state-owned or simply very large institution.\n \nreply",
      "If everyone use their own idiosyncratic algorithm for choosing OSS to donate to, it's going to be awfully hard to exploit.\n \nreply",
      "There are probably only so many obvious metrics from which to pick and you wouldn't have to game them all, just pick the easiest ones and keep grinding. Fraudsters are usually motivated and not that dumb.\n \nreply",
      "The mechanism is kinda like the Spotify fake songs case: https://www.justice.gov/usao-sdny/pr/north-carolina-musician...In the same way, there was a fixed pot of money available split up by popularity, so making thousands of songs and streaming them as much as possible with bot accounts is profitable, even though each bot account cost a few dollars per month.Here, the bots you use to juice your numbers don\u2019t even need a subscription fee!\n \nreply",
      "Exactly, and Goodhart's law drives the nails in the coffin.https://en.wikipedia.org/wiki/Goodhart%27s_law\n \nreply",
      "Makes me think of the \"cobra effect\", like the Great Hanoi Rat Massacre.[1]Set arbitrary metrics like download count -> bad actors make bots to download their package -> they profit while the registry suffers from very heavy load.[1]: https://en.wikipedia.org/wiki/Great_Hanoi_Rat_Massacre\n \nreply",
      "Rather than trying to donate to the most popular packages, people could try to donate to the packages they use, and then their dependencies (it would be nicer, though, if repos had a way for packages to list their dependencies and automatically propagate donations they received down\u2014which would be a usable by the top level packages but, eh, you have to trust people at some point).\n \nreply",
      "2 is already happening, I have seen this multiple times.\n \nreply"
    ],
    "link": "https://kvinogradov.com/algo-sponsors/",
    "first_paragraph": ""
  },
  {
    "title": "My second year without a job (shilin.ca)",
    "points": 467,
    "submitter": "true_pk",
    "submit_time": "2024-12-06T20:26:05 1733516765",
    "num_comments": 727,
    "comments_url": "https://news.ycombinator.com/item?id=42344002",
    "comments": [
      "Folks, just live your life how it fits you. We're all born into a different set of circumstances that dictate many of the choices we make. We all have different ideas about what life means to us and what levels of risk we're comfortable taking. Listen to your gut and focus on what you'll be smiling about as you lie on your deathbed.\n \nreply",
      "These past three year's markets have been super tough. It's the hardest I've worked in my life to be employed in the field I was interested in actually working in, which is building software. I've had to get two new roles in that time due to layoffs. 1000's of applications, and all that entails. I was ready to be a farmhand this summer before things actually worked out, figured I would get to work a farm on a lake, and fish on my time off, it's pretty much what I want to do when I retire, shouldn't be so bad. Just be flexible. Work as hard as you can to get what you want, but remember McDonalds pays nearly $20 an hour these days, if it keeps the lights on, don't be above it. Better than being homeless.\n \nreply",
      "I fear that we might be in the twilight of the software development job market:https://www.hnhiringtrends.com/Everybody got used to continuous growth, but I fear that for most people the music has stopped.\n \nreply",
      "I think this chart would have been greatly enhanced by adding Fed interest rates.The way things were during the previous decade was not sustainable. The current situation too shall pass and hopefully evolve into something more sustainable.I for one don't miss the churn fuelled by easy VC money.\n \nreply",
      "Our guts are horrific retarded advisors warning us forever about the winter to come to a peasant tribe where nit every member pulks its weight. Meanwhile the most horrific damage is done through blind guts and instincts whorship.\n \nreply",
      "Am I missing something about where this guy is getting money? He posted his bank account, which is essentially empty, and it seems his only current income is $600/mo, which is less than half of his rent. So how is this guy surviving currently?I don't mean to be one of those people that shout \"privilege\" at every turn on the Internet, but most people with no savings and barely any income would be freaking out unless they had some family or support network to lean on, which I noticed any discussion of is suspiciously absent.\n \nreply",
      "Didn't he say he started with $80,000, spent it, and is now out of money?Presumably he now needs to either get a job asap or make some hard choices. But it sounds like the post is supposed to be a retrospective so its not surprising he isn't really talking about the future.> I don't mean to be one of those people that shout \"privilegeHe literally had enough money to blow $80,000 on 2 years of unemployment. Of course he is privleged. Most people in the computer industry are. Most posters to hn are. The average person lives paycheque to paycheque and certainly doesn't have 80k just lying around in their bank account.\n \nreply",
      "> But it sounds like the post is supposed to be a retrospective so its not surprising he isn't really talking about the future.No, my specific point is that he does not sound like he needs to make hard choices, and he is alluding to continuing working on his own projects in the future. He writes:> I made resolutions: to make $1M in revenue in 2025. Well, that's not really happening\u2026 But rest assured, I do everything possible to reach that goal rather sooner than later.and> blymp is the only one generating money \u2014 about $600/month \u2014 and the one I plan to continue next year. Yay!and finishes with> Here's to a promising year 2025. My third year without a job. A year when I give more than I receive. A year of patience. And a year of an even deeper connection with myself. Cheers!And sure, people that make a high income job are privileged, but I was using it in the sense that you frequently see it used online, specifically that he has a backup pool of money/support somewhere, most likely family, that he conspicuously leaves out of his post.\n \nreply",
      "> my specific point is that he does not sound like he needs to make hard choicesSome people are just like that. I've been in similar moments (financially) as that guy, and as weird as it sounds, I was laughing at it. lolI don't think me or him are immune to anxiety by any means, but that our triggers are probably different.",
      "> Here's to a promising year 2025. My third year without a job. A year when I give more than I receive. A year of patience. And a year of an even deeper connection with myself.He does talk about the future and seems to imply he\u2019ll continue to not have a job.If that\u2019s the case, I\u2019m with GP in wondering how he\u2019s going to make it past January.\n \nreply"
    ],
    "link": "https://shilin.ca/my-second-year-without-job/",
    "first_paragraph": ""
  },
  {
    "title": "Melons and Melancholy: \"Eating and Being\" illuminates the dynamics of dietetics (lareviewofbooks.org)",
    "points": 10,
    "submitter": "crescit_eundo",
    "submit_time": "2024-12-06T16:58:39 1733504319",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://lareviewofbooks.org/article/on-melons-and-melancholy/",
    "first_paragraph": ""
  },
  {
    "title": "Biggest shell programs (github.com/oils-for-unix)",
    "points": 192,
    "submitter": "todsacerdoti",
    "submit_time": "2024-12-07T00:59:11 1733533151",
    "num_comments": 102,
    "comments_url": "https://news.ycombinator.com/item?id=42346274",
    "comments": [
      "Okay, so when I worked at Sony about 25 years ago, I got assigned this project to fix our order management system, which was extremely slow, and kept crashing.I jumped in and started digging around, and to my horror, the OMS was a giant set of shell scripts running on an AIX server, which evolved over a decade and was abandoned. It was over 50,000 lines of code! It was horrendous and shit kept timing out everywhere -- orders, payments, and other information were moved from server to server over FTP, parsed with complicated sed/awk, and inventory was tracked in text files (also FTPd around.)At the time, perl seemed like the most practical way for me to migrate the mess -- I rewrote all of the shell piece by piece, starting with the simplest peices and replaced them with small perl modules as part of a larger perl application, refactoring along the way. It took me 3 months and I moved the whole thing to about 5000 lines of perl, and it ran 10-100x faster with almost none of the failures in the original system.As terrible as it was, it's one of the most satisfying things I've ever done. :-)\n \nreply",
      "perl is still the most practical way to mitigate shell script abominations like that. Though tcl's a good option too.\n \nreply",
      "Oils aims to be the absolute best way to migrate shell scripts!  (I created the project, and the wiki page being discussed)https://www.oilshell.org/OSH is the most bash-compatible shell in the world, and YSH is a new language    ls | sort | uniq | wc -l   # this is both OSH and YSH\n\n    var mydict = {foo: 42, bar: ['a', 'b']}   # this is new YSH stuff you can start using\n    json write (mydict)\n\n\nThe difference between OSH and YSH is exactly a set of \"shopt\" options [1], although YSH feels like a brand new language too!  There is a smooth blend.I think it's worth it for 2 things alone- YSH checks all errors - you never lose an exit code- YSH has real arrays and doesn't mangle your variables with word splittingThere's a lot more: modules with namespaces (use mymodule.ysh), buffered I/O that's not slow, etc.Gradually  upgrading -https://github.com/oils-for-unix/oils/wiki/Gradually-Upgradi...   (people are writing new YSH, but not many people have gradually upgraded, so I'd definitely appreciate feedback from people with a big \"shell script problem\")---There is a FAQ here about Perl:Are you reinventing Perl? - https://www.oilshell.org/blog/2021/01/why-a-new-shell.html#a...Not to say that migrating to Perl is worse in any way, i.e. if you already know Perl or your team knows it.But objectively YSH is also a shell, so I think more of the code carries over, and there is a more direct upgrade path.---[1] Unix Shell Should Evolve like Perl 5 - https://www.oilshell.org/blog/2020/07/blog-roadmap.html#the-... - i.e. with compatible upgrade options\n \nreply",
      "Or Ruby, which is essentially Smalltalk for Unix, plus lots of Perl-isms.Haskell (e.g. shh) and Clojure (Babashka) are also a nice for this usecase, but more niche options.\n \nreply",
      "Oh, no, now I have to go dig out some of mine....The first really big one I wrote was the ~7000 line installer for the Enrust CA and directory, which ran on, well, all Unixes at that time. It didn't initially, of course, but it grew with customer demand.The installation itself wasn't especially complicated, but upgrades were, a little, and this was back when every utility on every Unix had slight variations.Much of the script was figuring out and managing those differences, much was error detection and recovery and rollback, some was a very primitive form of package and dependency management....DEC's Unix (the other one, not Ultrix) was the most baffling. It took me days to realize that all command line utilities truncated their output at column width. Every single one. Over 30 years later and that one still stands out.Every release of HP-UX had breaking changes, and we covered 6.5 to 11, IIRC. I barely remember Ultrix or the Novell one or Next, or Sequent. I do remember AIX as being weird but I don't remember why. And of course even Sun's three/four OS's had their differences (SunOS pre 4.1.3; 4.1.3; Solaris pre 2; and 2+) but they had great FMs. The best.\n \nreply",
      "That column truncation sounds bizarre.  Are you sure the terminal didn't have some sort of sideways scroll available?\n \nreply",
      "I think he was meaning that they truncated the lines even when called from a script, with their output going somewhere other than a terminal, not just when run interactively\n \nreply",
      "Yep, but I'm curious enough to quiz it.Weirdly, today I ran wish in MacOS Sequoia (15.1.x) and had the (exception) output truncated at terminal width!\n \nreply",
      "Because macOS closest relative isn't Free/NetBSD, but OSF/1 which, under few different names, was sold by Digital as Unix for Alpha (there were few rare builds for MIPS too).\n \nreply",
      "Interesting, I thought MacOS was basically a FreeBSD variant.But I just tried it again on a resized terminal window and I couldn't reproduce it!\n \nreply"
    ],
    "link": "https://github.com/oils-for-unix/oils/wiki/The-Biggest-Shell-Programs-in-the-World",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          Help me fill out this page!  It's freely editable.What programs should be listed?  I'm using \"biggest\" in the sense of substantial, not necessarily the raw number of lines.OSH \"Wild\" Tests parse over a million lines of shell.  However most of these are small programs and distro package definitions like Alpine PKGBUILD and Gentoo ebuilds, which are repetitive.Shell Programs That Run Under OSHshell script are dangerous The shell is a program to handle your system internally via an interactive console(or not)... It's feature full and extremely dangerous. it's not done to produce applications."
  },
  {
    "title": "Accessing a DRM Framebuffer to display an image (embear.ch)",
    "points": 79,
    "submitter": "compressedgas",
    "submit_time": "2024-11-30T12:29:25 1732969765",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=42281193",
    "comments": [
      "I've worked a lot this year on writing DRM/KMS code while porting my digital signage player (https://info-beamer.com) to support the Raspberry Pi5. Since they moved away from their proprietary Broadcom provided graphical APIs (OMX/dispmanx) the Pi now fully supports DRM and the implementation is really solid by now.There is a ton more to learn: KMS (kernel mode setting) allows fine control over the video mode in case you cannot or do not want to rely on auto-detection.Then there's the atomic API: Unlike in the blog post, all changes applied to the output (from video mode to plane positions or assigned framebuffers...) are gathered and then applied atomically in a single commit. If necessary you can test if your atomic commit is correct and will work by doing a \"Test only commit\" before doing the real commit. Making changes atomically avoids all kinds of race conditions resulting in, for example, screen tearing.Then there's the interaction with video decoding: Using FFmpeg on the Pi allows you to access to hardware decoder. They produce DRM framebuffers for each video frame. You can then directly assign them to planes and position them on the screen. The resulting playback is zero-copy and as fast as it gets on the Pi.Another fun feature is the Writeback connector, which unlike the one ending up as an HDMI signal allows you to write your output to a new DRM framebuffer. This can, for example, be used to take screenshots of your output or even feed the buffer back into a video encoder.One very frustration aspect is that there is basically no real documentation, especially about semantics. I guess it makes sense if you consider that there's probably only a limited number of API consumers (like desktop compositors, special video players).\n \nreply",
      "The drm atomic test is not only to verify if it's \"correct\", it is also used to check if the display controller can scan out that configuration of planes/buffers, given buffer modifiers, plane properties/sizes, etc. and current status of the display controller.  If it can't, you probably need to simplify the configuration via some GPU compositing.This is what we were doing on ChromeOS.\n \nreply",
      "Is an entire pi for signage affordable? I saw someone add VGA to an Arduino but I've seen some holiday cards have a video player in them, bare bones Linux\n \nreply",
      "In case anyone misinterprets it:DRM here is for Direct Rendering Manager (not e.g. interfaces studied to limit access to content).\n \nreply",
      "I had the pleasure to access the framebuffer via the DRM and pull the data with DMA for a vnc server I wrote at work. Learning how to use the api was like half the work, an article like this would\u2018ve certainly helped!\n \nreply",
      "DRM Framebuffers are also the preferred way to interface with Vulkan renderers in GTK. For example, if you wanted to make a game scene editor with gnome, you could render the scene to a DRM Framebuffer and use a GTKGraphicsOffload widget to indicate that it will continue to be updated outside of the event loop.In practice I\u2019ve never been able to get this work. Static images totally fine. Graphics offloading fails and manually refreshing the image causes some sort of memory leak in the GPU\n \nreply",
      "For a bit more complicated application also using DRM directly there is kmscube [1].[1] https://gitlab.freedesktop.org/mesa/kmscube\n \nreply",
      "Really helpful introduction to DRM for the uninitiated, thanks\n \nreply",
      "Excellent, succinct article. Having fought to understand this process years ago I would have loved to find this exact article at the time.\n \nreply",
      "Can this be done on Mac OS?\n \nreply"
    ],
    "link": "https://embear.ch/blog/drm-framebuffer",
    "first_paragraph": "When doing bring-ups on embedded devices a useful way to test if a display interface is working is this command:This shows random data on the display and a lot of gray, red, blue, and green dots appear. This doesn't tell us if everything is working as expected because maybe the resolution is wrong or some timings are too tight but it gives already an indication if the display controller works and if the display gets some understandable data.However, this example only works for framebuffer device drivers under Linux. For the iMX6 this means that the above method only works for the proprietary driver but not for the upstream-based driver. Modern Linux graphics drivers like the iMX6 upstream driver provide a DRM interface to the user space and therefore don't provide a framebuffer device. We can't just simply dump data to a character device to show it on a display. Instead, we need to set up a whole pipeline that tells what framebuffer should be used as input to the CRTC. This image shows"
  },
  {
    "title": "Lies I was told about collab editing, Part 1: Algorithms for offline editing (moment.dev)",
    "points": 274,
    "submitter": "antics",
    "submit_time": "2024-12-06T20:22:07 1733516527",
    "num_comments": 97,
    "comments_url": "https://news.ycombinator.com/item?id=42343953",
    "comments": [
      "Hi! Author of Eg-walker & ShareJS here, both of which are referenced by this post.You write this article like you're disagreeing with me - but I agree completely with what you've said. I've been saying so on HN for years. (Eg, in this comment from 6 years ago[1].)The way I think about it, the realtime collaborative tools we use today make a lot of sense when everyone is online & editing together. But when users edit content offline, or in long lived branches, you probably want the option to add conflict markers & do manual review when merging. (Especially for code.)Luckily, algorithms like egwalker have access to all the information they need to do that. We store character-by-character editing traces from all users. And we store when all changes happened (in causal order, like a git DAG). This is far more information than git has. So it should be very possible to build a CRDT which uses this information to detects & mark conflict ranges when branches are merged. Then we can allow users to manually resolve conflicts.Algorithmically, this is an interesting problem but it should be quite solvable. Just, for some reason, nobody has worked on this yet. So, thanks for writing this post and bringing more attention to this problem!If anyone is interested in making a unique and valuable contribution to the field, I'd love to see some work on this. Its an important piece thats missing in the CRDT ecosystem - simply because (as far as I know) nobody has tried to solve it yet. At least not for text editing.[1] Bottom part of this comment: https://news.ycombinator.com/item?id=19889174\n \nreply",
      "I would simply argue that the \u201coffline\u201d editing is a people-problem and hence van not be solved using automation. People shall find a way to break/bypass the automation/system.The only \u201coffline editing\u201d that I allow on human text documents is having people add comments. So not editing, no automated merging.For \u201coffline editing\u201d that I allow on automation (source code) is GIT which intentionally does not pretend to solve the merge, it just shows revisions. The merge is an action supervised by humans or specialised automation on a \u201cbest guess\u201d effort and still needs reviews and testing to verify success.\n \nreply",
      "Yes I agree. But remember: but git will automatically merge concurrent changes in most cases - since most concurrent changes aren\u2019t in conflict. You\u2019re arguing you want to see & review the merged output anyway - which I agree with.Ideally, I want to be able to replace git with something that is built on CRDTs. When branches have no conflicts, CRDTs already work fine - since you merge, run tests, and push when you\u2019re happy.But right now CRDTs are too optimistic. If two branches edit the same line, we do a best-effort merge and move on. Instead in this case, the algorithm should explicitly mark the region as \u201cin conflict\u201d and needing human review, just like git does. Then humans can manually review the marked ranges (or, as others have suggested, ask an llm to do so or something.). And once they\u2019re happy with the result, clear the conflicting range markers and push.\n \nreply",
      "> Algorithmically, this is an interesting problem but it should be quite solvable. Just, for some reason, nobody has worked on this yet. So, thanks for writing this post and bringing more attention to this problem!I'm skeptical that an algorithmic solution will be possible, but I can see this being handled in a UX layer built on top. For example, a client could detect that there's been a conflict based on the editing traces, and show a conflict resolution dialog that makes a new edit based on the resolution. The tricky part is marking a conflict as resolved. I suspect it could be as simple as adding a field to the crdt, but maybe then it counts as an algorithmic solution?[1] https://josephg.com/blog/crdts-go-brrr/\n \nreply",
      "That is what josephg was suggesting:> it should be very possible to build a CRDT which uses this information to detects & mark conflict ranges when branches are merged\n \nreply",
      "I should have been more clear in my original comment.I don't think that the conflict detection/resolution needs to live inside the CRDT data structure. Ultimately you might want to bake it in out of convenience, but it should be possible to handle separately (of course the resolution will ultimately need to be written to the CRDT, but this can be a regular edit).Keeping the conflict resolution in the application layer allows for CRDT libraries that don't need to be aware of human-in-the-loop conflicts, and can serve a wider range of downstream needs. For example, a note app and a version control system might both be plain text, but conflict resolution needs to be handled completely differently. Another example would be collaborative offline vs. online use cases, as noted above, they are very different use cases.\n \nreply",
      "Do you think that a LLM/\"AI\" can reliably solve the merging problem?\n \nreply",
      "> Do you think that a LLM/\"AI\" can reliablyNo. LLMs definitely have uses where reliability is not a requirement, but that's one requirement which LLMs clearly never meet\n \nreply",
      "The author describes the case of overlapping concurrent splices. It is a known funky corner case, yes.\nIf we speak of editing program code, the rabbit hole is deeper as we ideally expect a valid program as a result of a merge. There was a project at JetBrains trying to solve this problem through AST-based merge. After delving into the rabbit hole much much deeper, the guys decided it is not worth it. This is what I was told.\n \nreply",
      "Hi Joseph! I am sorry, I was not trying to say your work sucks. I was trying to (1) help practitioners understand what they can expect, and (2) motivate problems like the one you mention at the end.(1) might seem stupid but I think just evaluating these systems is a challenging enough technical problem that many teams will struggle with it. I just think they deserve practical advice\u2014I know we would have appreciated it earlier on.\n \nreply"
    ],
    "link": "https://www.moment.dev/blog/lies-i-was-told-pt-1",
    "first_paragraph": ""
  },
  {
    "title": "Tempest Anderson: Pioneer of Volcano Photography (2015) (publicdomainreview.org)",
    "points": 18,
    "submitter": "ulrischa",
    "submit_time": "2024-12-02T20:36:53 1733171813",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://publicdomainreview.org/essay/tempest-anderson-pioneer-of-volcano-photography/",
    "first_paragraph": "Search The Public Domain ReviewPat Hadley, Sarah King,  and Stuart OgilvyTHE YORKSHIRE MUSEUM - Pat Hadley, Sarah King and Stuart Ogilvy present a fascinating selection of photographs from the collection of Tempest Anderson, the pioneering Victorian volcanologist.\nPublishedAugust 6, 2015What could possess a respectable Victorian surgeon from York to spend much of his life travelling to remote and challenging parts of the world to study volcanoes and climb mountains?For Tempest Anderson, pioneering new techniques of ophthalmic surgery and inventing photographic equipment was not enough. He decided that his \u2018limited leisure\u2019 time could not be filled with reading, writing or socialising, he sought to occupy himself with something more exciting: volcanology. For him, it was a branch of science that did not have too much literature and had the \u2018advantage of offering exercise in the open air\u2019: he saw the sides of volcanoes not as dangerous but \u2018picturesque\u2019Vesuvius in Eruption: taken from th"
  }
]