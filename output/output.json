[
  {
    "title": "Signal Protocol and Post-Quantum Ratchets (signal.org)",
    "points": 432,
    "submitter": "pluto_modadic",
    "submit_time": "2025-10-02T16:06:10 1759421170",
    "num_comments": 192,
    "comments_url": "https://news.ycombinator.com/item?id=45451527",
    "comments": [
      "Signal keeps cranking out brilliant crypto papers, but from a product perspective, it feels like they're throwing stuff at the wall to see what sticks. We've got post-quantum handshakes, stories and money transfer experiments, but still no SDK, no APIs, no bots. The official libsignal library is undocumented and incomplete. Large parts of functionality are still buried on clients. Don't get me started on \"but they have published all protocol specs on their website, go on and roll your own library\"! That's not how you run a product. It's borderline negligent for a platform used by millions.Every other major messaging app exposes something to developers, but Signal is allergic to the idea. Makes me wonder if they even have a head of product because whatever they're doing now is a far cry from a coherent product strategy. Signal is basically a pile of hot cryptography duct-taped to a messenger that's more hostile than any product in Apple's walled garden. And that's from a day one user who's been advocating for them the whole way.</rant> thanks to everyone involved in building the product <3reply",
      "Signal don't want you to build 3rd party clients and integrations, they are another fully centralised product meant to capture and lock users into what signal believes is better for them. That's the whole \"we love opensource but we won't merge your PRs and might lock your account out of the network for using forked clients that got rid of features like crypto that you might not like\". I'm still sour for all the bad faith placating \"the ecosystem is moving\" post by Moxie and the lame excuse for not supporting federation. And no, I'm not finding it hard to onboard family and friends onto secure XMPP clients and accounts.reply",
      "I feel you. The \"stories\" feature especially felt like \"throwing stuff at the wall to see what sticks.\" Given that they're a nonprofit founded by an anarchist, I assume  their goals are just different from the typical product-focused company? Which I'm fine with, the app does what it's supposed to do. It would be lovely to have an SDK though.reply",
      "Is this an important feature? I know WhatsApp and iMessage have some kind of API for businesses, but as a regular user, I've never interacted with a legit business using it. Only been harassed by bots a few times.My one serious problem with Signal is that it silently goes out of date then stops sending notifications, so I miss messages entirely. Kind of its one job.reply",
      "Maybe maybe not. I think it is a useful feature for power users. The question is if targeting power users will help mass appeal. I'd argue with an app like Signal, yes it would. The power users are effectively their evangelists. APIs could enable a lot of features that people are asking for like location sharing, bots (e.g. on your IOT devices), and so on. The concern is more that introducing those things creates security risks but I think that's okay. Put a \"developer mode\" type switch like in Android.But there are also other things I'd like to see.For mass appeal I'd like to see them integrating Signal Stickers[0] into the app so people can search stickers. This has been a surprisingly common complaint among people I've converted over.For both groups I'd love to see something like this feature request[1] I like that it could serve as the backbone of a mesh network and AirDrop is a incredibly popular. Would be super cool if you could hold a copy of the APK on your phone and drop it over to others to install that way. I imagine even a rudimentary mesh network could really reduce server loads. My GF and I often sync pictures to each other this way. No reason that needs to go over the network when we're sitting 5 feet from one another.For power users I'd love to see a nuking capability. Bidirectional. I want to know that if I am at a protest or something and get picked up by the Gestapo that either I or a trusted friend can wipe my phone. It's not a cure all, but it greatly reduces the chances of \"incriminating\" evidence being found on my device. But such a feature seems quite unpopular on their forums (I am very much not a fan of their forums and the community there...)[0] https://signalstickers.org/[1] https://community.signalusers.org/t/signal-airdropreply",
      "Counter argument: When the sole reason for existence for Signal is private/secure messaging, it makes sense to resist opening up to third party development.That's a big can of worms that invariably will impact their ability to deliver on their main mission - private IM. Eg of problems: Who gets dev access, how do you vet plugins/aps from deceiving users, would users understand the risks, when an app gets compromised how to fight malicious campaign to discourage using Signal etc. etc.reply",
      "No API and Bots is a feature for mereply",
      "Yeah if I wanted that stuff I'd go to XMPP or matrix.reply",
      "I'm absolutely okay with having none of that. I hope they focus on making a secure and usable messenger above all else.reply",
      "let alone this that drives me nuts: they are playing the ringing sound for the caller without the callee's phone actually ringing.and it's a deliberate choice that they are defending for seceral years now, ever since they removed the submarine sound.reply"
    ],
    "link": "https://signal.org/blog/spqr/",
    "first_paragraph": ""
  },
  {
    "title": "The strangest letter of the alphabet: The rise and fall of yogh (deadlanguagesociety.com)",
    "points": 85,
    "submitter": "penetralium",
    "submit_time": "2025-10-02T21:34:42 1759440882",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=45455882",
    "comments": [
      "Yogh is indeed strange. Wynn is the most irritating loss though; couldn't we have at least kept the name rather than \"double-yoo\"?reply",
      "That would be convenient. Microsoft would have had to rename the \u229e key then lol.reply",
      "Why, isn't that just called 'Super' key?reply",
      "\"English spelling has a reputation. And it\u2019s not a good one.\" - never have i ever agreed with anything moredifferent hill, but one I would die on is: as the letter \"c\" should make the \"ch\" sound, the letter \"c\" serves no purpose not already handled by \"s\" or \"k\" otherwisereply",
      "https://guidetogrammar.org/grammar/twain.htm  For example, in Year 1 that useless letter \"c\" would be dropped to be replased either by \"k\" or \"s\", and likewise \"x\" would no longer be part of the alphabet.\n\n  The only kase in which \"c\" would be retained would be the \"ch\" formation, which will be dealt with later.\n\n  Year 2 might reform \"w\" spelling, so that \"which\" and \"one\" would take the same konsonant, wile Year 3 might well abolish \"y\" replasing it with \"i\" and iear 4 might fiks the \"g/j\" anomali wonse and for all.\n\n  Jenerally, then, the improvement would kontinue iear bai iear with iear 5 doing awai with useless double konsonants, and iears 6-12 or so modifaiing vowlz and the rimeining voist and unvoist konsonants.\n\n  Bai iear 15 or sou, it wud fainali bi posibl tu meik ius ov thi ridandant letez \"c\", \"y\" and \"x\" -- bai now jast a memori in the maindz ov ould doderez -- tu riplais \"ch\", \"sh\", and \"th\" rispektivli.\n\n  Fainali, xen, aafte sam 20 iers ov orxogrefkl riform, wi wud hev a lojikl, kohirnt speling in ius xrewawt xe Ingliy-spiking werld.reply",
      "I remember a version which ends with how we'll end up speaking German.reply",
      "> fiks the \"g/j\" anomali wonse and for allBut we'd still be arguing about how to pronounce \"\u1d79if\"reply",
      "Recommend X for the \u2018sh\u2019 sound, as it is pronounced that way in languages like Portuguese.  Y is a common typographical substitute for theta/thorn, as in \u201cye olde shoppe.\u201dreply",
      "Changing \"cube\" to \"kube\" would just look like it's pronounced \"koob\" (e.g. rube, tube, lube), so we swap a minor spelling aggravation for a minor pronunciation edge case. unless you want to go full kyube but we're not putting that on the table.reply",
      "kyube or kyoob would definitely be the way to go.It's funny you use \"tube\" as an example though, as in my British accent I pronounce that as \"chube\", whereas I believe many Americans would use a \"t\" sound for that word. Not sure how you settle on a spelling in those cases.reply"
    ],
    "link": "https://www.deadlanguagesociety.com/p/history-of-letter-yogh",
    "first_paragraph": ""
  },
  {
    "title": "Apple takes down ICE tracking apps after pressure from Bondi DOJ (foxbusiness.com)",
    "points": 87,
    "submitter": "aspenmayer",
    "submit_time": "2025-10-03T00:34:29 1759451669",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=45457333",
    "comments": [
      "FWIW, everyone who claims that Apple fundamentally needs the centralized ability to control apps on their platforms \"for everyone's safety\" -- despite how that obviously and repeatedly makes them become patsies for governments all over the world to enforce their censorship regimes -- are complicit in this stuff (in addition, of course, to the people who build it at Apple...).reply",
      "It\u2019s a shame too, because Apple has the money and brand wherewithal to fight the government. See the FBI vs Apple stuff that happened years ago. That actually won them some real converts.Capitulating over this is Apple showing their supposed core values have significantly hollowedreply",
      "Isn\u2019t Apple mostly interested in making more money, though, instead of spending money?The way I see it of all the top tech giants, Apple has the most to lose with all the tariff shenanigans, so it\u2019s in their [shareholders] interest to stay friends with the current administration.Apple has never had moral values other than earning money by making great products.And I say this as someone who is deeply embedded in the Apple ecosystem.reply",
      "I wish Steve was still around for these battles. Tim Cook is such a pussy.reply",
      "It was obvious Apple was going to bend the knee with that gold plaque.reply",
      "What exactly do you mean by \u201ccomplicit\u201d in \u201cthis stuff\u201d? What are you accusing?reply",
      "Quite clearly accusing them of reducing public pressure against Apple putting itself in a position where it's doing exactly what it's doing right now.reply",
      "Yep. If you're advocating for a policy that leads directly to something, then you're kind of arguing in favor of that somethingreply",
      "This is exactly what's wrong with Apple's app store exclusivity. It's also what's wrong with mandatory notarization where regulations forbid that, and Google's plan to require developer verification.reply",
      "Methinks this won\u2019t be the last politically-motivated removal from Apple\u2019s App Store; the more apps they remove then the more they weaken their own arguments about how a locked-in walled garden is in consumer interests.reply"
    ],
    "link": "https://www.foxbusiness.com/politics/apple-takes-down-ice-tracking-app-after-pressure-from-ag-bondi",
    "first_paragraph": "\n          Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by\u00a0Factset.\n          Powered and implemented by\u00a0FactSet Digital Solutions.\u00a0\n            Legal Statement.\n          This material may not be published, broadcast, rewritten, or redistributed. \u00a92025 FOX News Network, LLC. All rights reserved. FAQ - New Privacy PolicyActing U.S. Attorney for the Northern District of Texas Nancy Larson discusses dangers posed by I.C.E. tracking apps, a video of law enforcement protecting detainees and the latest on the I.C.E. facility shooting on \u2018The Evening Edit.\u2019FIRST ON FOX:\u00a0Apple dropped ICEBlock, a widely used tracking tool, from its App Store on Thursday after the Department of Justice raised concerns with the big tech giant that the app put law enforcement officers at risk.DOJ officials, at the direction of Attorney General Pam Bondi, asked Apple to take down ICEBlock, a move that comes as Trump administration officials have claimed the tool, which all"
  },
  {
    "title": "Babel is why I keep blogging with Emacs (entropicthoughts.com)",
    "points": 173,
    "submitter": "ibobev",
    "submit_time": "2025-10-02T18:06:41 1759428401",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=45453222",
    "comments": [
      "This works if your code snippets generate relatively static output. Lately I often need to create animations, often interactive ones. See https://akkartik.name/debugUIs.html, for example.In my life I've often switched to more manual tools when I notice that the more automated tool causes me to live within certain limitations. Sometimes it has taken me a decade to notice these limitations. Automation matters when I do something tens of times a day. But I publish a blog post once in tens of days. It feels worth some additional work to get a little more control and break out of ruts.reply",
      "I was in the same boat for many years! Having started using org-mode for my website in 2018 [1] (just add index.org to the path to see the source), it grew into this massive pile of obscure gen and with my limited comfort level with lisp, turned into a scary smelling concoction of dozens of perl/sed/sh scripts that modified the output to fit my needs and have them do something fancy.But then, really, sat down for about 48 hours on a lonely weekend when everyone was away and wrote a simple static site generator [2] that takes exact same files and produces output that I fully understand e2e, becoming the project I'm most proud of.There are so many other generators I tried (hugo, jekyll, rails, asciidoctor, org-publish, astro), rolling up your own gives a sense of a stable foundation. Love your website! So clean. One thing that I'm thinking of adding (though I haven't touched my generator that much, I consider it \"complete\") is the dynamic execution of source code blocks.[1] https://sandyuraz.com[2] https://github.com/thecsw/darknessreply",
      "Babel sounds awesome, I should give it a try.I've been using Pandoc for my static site, which I also used for my PhD thesis. I wrote pandoc-plot[0] to render figures in documents like the author does, inspired by Matplotlib's plot-directive[1]. I feel like this feature is a must for technical writing, but getting the details right is tricky![0]: https://github.com/LaurentRDC/pandoc-plot[1]: https://matplotlib.org/stable/api/sphinxext_plot_directive_a...reply",
      "I write my blog (https://lambdaland.org) entirely in Emacs now. It's Hugo, but I use ox-hugo [1] to convert from org-mode to markdown and Hugo converts it to HTML.What I like about this: everything else I do is in org-mode anyway, so this fits my brain. I also have some nice org-mode tooling to make things like footnotes, margin notes, etc. look really nice.A little convoluted, sure, but I've seen worse. :-P[1]: https://ox-hugo.scripter.co/reply",
      "It looks like Hugo also has native .org file support. Does ox-hugo provide functionality over Hugo\u2019s native org file parsing?reply",
      "I used Hugo's native `.org` file support for a while. It was\u2026 fine. There are some serious issues (for me) that I had with it though. The most salient one is that it didn't convert straight quotes in the source (\"\") to proper curly-quotes. (\u201c\u201d) I don't remember what else but I'm sure there was more than just that. I've spent a lot of time on the typography of my site and I wanted everything to be as perfect as I could make it.reply",
      "Didn't know that Hugo supports org files. But just one example of added functionality is being able to use one big file with a subtree for each post.reply",
      "This is a nice feature of ox-hugo. I still use individual files and it's great.reply",
      "It didn't back when ox-hugo was written.reply",
      "I started using Hugo's built-in org support, but I found it quite limiting. I can't remember the specifics, but it doesn't support everything you can do with Markdown. So I quickly switched to ox-hugo. They can co-exist though so you can try the native support then switch if you run into the same shortcomings as I did.reply"
    ],
    "link": "https://entropicthoughts.com/why-stick-to-emacs-blog",
    "first_paragraph": "\nEvery time I look at someone\u2019s simple static site generation setup for their\nblog, I feel a pang of envy. I\u2019m sure I could make a decent blogging engine in\n2,000 lines of code, and it would be something I\u2019d understand, be proud over,\nable to extend, and willing to share with others.\n\nInstead, I write these articles in Org mode, and use mostly the standard Org\npublishing functions to export them to html. This is sometimes brittle, but\nmost annoyingly, I don\u2019t understand it. I have been asked for details on how my\npublishing flow works, but the truth is I have no idea what happens when I run\nthe org-publish-current-file command.\n\nI could find out by tracing the evaluation of the Lisp code that runs on export,\nbut I won\u2019t, because just the html exporting code (ox-html.el) is 5,000\nlines of complexity. The general exporting framework (ox-publish.el and\nox.el) is 8,000 lines. The framework depends on Org parsing code\n(org-element.el) which is at least another 9,000 lines. This is over 20,0"
  },
  {
    "title": "Why most product planning is bad and what to do about it (railway.com)",
    "points": 96,
    "submitter": "ndneighbor",
    "submit_time": "2025-10-02T19:34:08 1759433648",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=45454374",
    "comments": [
      "Sounds Like you rediscovered the \u201copportunity solution tree\u201d (Teresa Torres) and were skipping a crucial step in product management / UX which is product discovery.\nI would suggest not to generalize your learnings by saying \u201cwhy most product planning is bad\u2026\u201d and rather use a more humble title \u201cwhy our product planning was bad and what we did about it\u201d.reply",
      "Am I getting it wrong? It sounds like they're still doing quarterly planning just with a different ritual?I had hoped they'd realise quarterly planning is a bad premise and asked themselves why they do it.If you have a mature product where you add incremental features, you don't need that plan because it's just an arbitrary block of pretty fungible work.If you're still looking for product market fit, that three month plan wont last a week before becoming obsolete.If you need to build a bigger thing that is only valuable once it's all done, you A: need a project and B: probably don't because it will fail.reply",
      "I have a spicy take! OKRs every quarter don\u2019t keep you focused on the prize, they keep you focused on your own performance for whatever the industry equivalent of publish or perish is called these days.reply",
      "Hello there! Author there, and surprised/delighted with the response. I don't think we had the issue with the cadence, the quarter is arbitrary, but we think it gives us the ability to just go heads down to focus.With that said, one thing we did and I don't why we did it was that we would \"re-justify\" why we would want to work on something every three months which isn't great. There is a world where if we had more eng. resources we could have more people than problems and we could take stuff on board as it arrives, but for us deciding on what to work on is a hard decision.I also agree that market fit is a key factor. I think Railway was lucky that we didn't have to pivot the product 3 to 5 times to get some latch.What would be the post-quarterty planning process that you would like to see?reply",
      "There is no world where you have more people than problems because the more people you hire the more problems they discover.reply",
      "Or create.reply",
      "A man can dream ;-;reply",
      "> There is a world where if we had more eng. resources we could have more people than problems and we could take stuff on board as it arrivesI don't think this is a realistic world. The cavitation surface area that spawns new bubbles of unvetted fresh ideas grows as things are added. Adding eng resources to get more done makes it grow faster and I don't think there is ever such a thing as catching up.tl;dr - deciding what to build (and what it looks like in detail) will always be a critical and fundamental function of product teams.reply",
      "I think quarterly is used as a heartbeat across many teams. If you know your 15 adjacent teams are also quarter planning you can start asking for commitments. It makes things easier to figure out for higher ups too I imagine (never been a higher up though)Why not 6 weeks? Not sure. Gut feeling I like 3 months. Maybe 6 week sprints are good.reply",
      ">> When Platform marks \"Multi-mount volumes\" as P1 and Product marks \"HA DBs\" as P1Hilarious how closely aligned product and engineering are here. There must be essentially no delta in backgrounds at all. Might as well just merge the functions and have engineers talk to customers (who would be developers as well presumably) from time to time.reply"
    ],
    "link": "https://blog.railway.com/p/product-planning-improvement",
    "first_paragraph": "TL;DR: We tried OKRs, they created more ceremony than clarity. Our solution: Problem Driven Development, a 4-day quarterly process focused on identifying problems (not solutions), prioritizing as a team, and committing publicly. It's kept us shipping at velocity even as we've scaled to 1.7M+ users.For most of my friends and colleagues at mature software companies, there are usually three ways for an item of work to get put on the board to eventually be done.Thats not to say that every company is a disorganized mess or a bureaucratic hell scape but, I have never met any engineer who said: \u201cWow, I just love my company planning process.\u201d These words are seldom spoken in the english language. Railway, was fast approaching 1. and 2. at the same time. Despite us using excellent tools (shoutout Linear) planning is much as a cultural phenomena as well as an interesting engineering problem. From our perspective, we would finalize the features and the requirements what we would want to build onc"
  },
  {
    "title": "Playball \u2013 Watch MLB games from a terminal (github.com/paaatrick)",
    "points": 220,
    "submitter": "ohjeez",
    "submit_time": "2025-10-02T16:09:15 1759421355",
    "num_comments": 99,
    "comments_url": "https://news.ycombinator.com/item?id=45451577",
    "comments": [
      "Totally anecdotal, but there are people who literally get paid to watch games and record what happens at every step. I used to have that job. This is how MLB, ESPN etc. have live updates which powers stuff like this.reply",
      "There are also fans who record what happens in baseball games purely for their own enjoyment.https://en.wikipedia.org/wiki/Baseball_scorekeepinghttps://www.reddit.com/r/BaseballScorecards/https://www.reddit.com/r/baseball/comments/1lzpwrq/reasons_i... / https://www.reddit.com/r/baseball/comments/68qdm9/people_who...reply",
      "I helped someone extract data from one of those old DOS personal-database software programs.  He had recorded every scorecard from every game he went to for many years.  Each year had its own floppy disk.reply",
      "I've been using these for about... 20 years or so.. https://www.reisnerscorekeeping.com/reply",
      "Yeah baseball scorekeeping is an interesting part of the game.reply",
      "I see these folks at giants games sometimes, they usually have a little mini golf pencil and a clipboard or something.reply",
      "Futurama had a joke about this hobbyreply",
      "I really feel like this is the only application of AI I would want to support right now. If an LLM can take in these fans commentary and then add a bunch of hallucinations and cultural biases, well, that sounds like pure entertainment.reply",
      "AI Commentary seems fun, especially when you can choose different personalities, biases, etc.It\u201a\u00c4\u00f4ll be a while before it can replace a true play-by-play announcer, but with seven second TV delay it\u201a\u00c4\u00f4s maybe close to feasible.The Finals is a video game with AI voiceover for its commentary, and it\u201a\u00c4\u00f4s pretty engaging. I\u201a\u00c4\u00f4d expect to see this in FIFA soon if it isn\u201a\u00c4\u00f4t there already.reply",
      "I love scoring games when I go to a ballgame. It keeps me engaged, and it's fun to see how I mess up compared to the professional scorers. Did you do MLB scoring? If so, do you do scoring if you see a game now, or are you sick of it? :Dreply"
    ],
    "link": "https://github.com/paaatrick/playball",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Watch MLB games from the comfort of your own terminal\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Watch MLB games from the comfort of your own terminalMLB Gameday and MLB.tv are\ngreat, but sometimes you want to keep an eye on a game a bit more discreetly.\nplayball puts the game in a terminal window.Just want to try it out?Ready for the big leagues? Install the package globallyThen run itPlayball can be configured using the config subcommand. To list the current configuration values run the subcommand with no additional arguments:You should see output similar to:To get the value of a single setting pass the key as an additional argument:To change a setting pass the key and value as arguments:To revert a setting to its default value provide the key and the"
  },
  {
    "title": "We bought the whole GPU, so we're damn well going to use the whole GPU (stanford.edu)",
    "points": 456,
    "submitter": "sydriax",
    "submit_time": "2025-09-28T21:00:18 1759093218",
    "num_comments": 85,
    "comments_url": "https://news.ycombinator.com/item?id=45407953",
    "comments": [
      "Not using the NVDEC and NVJPG units to decompress weights into registers? And you say you're using the whole GPU. There are entire blocks on the silicon going idle!reply",
      "Ha made me chuckle. For those wondering seriously about this, it\u2019s not a viable optimization because weights are not readily compressible via JPEG/DCT, and there are a limited number of these units on the chip which bottlenecks throughout, meaning speed is dwarfed by simply reading uncompressed weights from HBM.reply",
      "It seems like this is indeed possible using video codecs:\nhttps://arxiv.org/abs/2407.00467v1reply",
      "Good fun. Now I wish RT cores would be programmable with some form of PTX, but for now it's Optix or die. Managed to do fun stuff with it but it's like pulling teeth.reply",
      "Yeah, but they could be.I won an GPU hackathon back in 2019 doing something very similar to this; although the other way around, I was compressing weights using hardware modules.reply",
      "Have a link to this?reply",
      "Unfortunately no. I have cool picture, though!reply",
      "I'm reminded of how Carmack talked about the extra efficiencies available when targeting consoles, because you knew exactly what hardware was available.It's great that the efficiencies available can be shown to be extractable. The real, much harder, trick is putting together a sufficiently smart compiler to enable them for heterogeneous compute setups.reply",
      "The demoscene also is an example of how much you can do if you can be absolutely sure exactly what hardware you\u2019re running on.The problem is that even for things like consoles, it's usually more \"cost efficient\" to write normal fast-to-write code that isn't maximally effective, let the compiler do its magic, and call it good enough.Sometimes I dream of what the world would do if we were mystically stuck on exactly the processors we have today, for twenty years.reply",
      "I've wondered sometimes what software would look like if a crisis took out the ability to build new semiconductors and we had to run all our computing infrastructure on chips salvaged from pregnancy tests, shoplifting tags, cars, old PCs, and other consumer electronics.  We'd basically move backwards about 20 years in process technology, and most computers would have speeds roughly equivalent to 90s/00s PCs.But then, this still wouldn't incentivize building directly to the hardware, because of the need to run on a large variety of different hardware.  You're still better off preferencing portability over performance, and then making it up by cutting scope and ease of development.reply"
    ],
    "link": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main",
    "first_paragraph": "Sep 28, 2025 \u00b7 24 min readBenjamin Spector*, Jordan Juravsky*, Stuart Sul*, Dylan Lim, Owen Dugan, Simran Arora, Chris R\u00e9Intro Post | Code | Low-Latency Megakernels | BrrTLDR: We're releasing a throughput-optimized megakernel for tensor-parallel inference with Llama-70B on H100s. Our kernel can aggressively overlap compute, memory, and communication ops in order to simultaneously use the different hardware resources available on a GPU. When integrated into the Tokasaurus inference engine, our megakernel can outperform SGLang by >22% on end-to-end throughput (measured as time to finish 65,536 prompts from the ShareGPT benchmark). We're releasing the code here; please be warned that this really is research code; it is sensitive to compiler versions, GPU setup, and sometimes even being looked at the wrong way, and we have no intention whatsoever of supporting it. We hope you'll find the ideas and results interesting nonetheless!Figure 1: ZoooommmmA few months ago, we showed how we could f"
  },
  {
    "title": "The history of cataract surgery (asimov.press)",
    "points": 194,
    "submitter": "mailyk",
    "submit_time": "2025-09-29T15:04:07 1759158247",
    "num_comments": 120,
    "comments_url": "https://news.ycombinator.com/item?id=45414718",
    "comments": [
      "I did some of the controls software for an automated IOL manufacturing line roughly 8 or 9 years ago. It's very cool technology, I was proud to work on it.One problem at the time (at least from my understanding) was actually that some people needed a second round of surgery since people are living longer and cataracts are getting corrected earlier. Last I heard, the rate of complications for a repeat surgery is significantly higher, but I assume it is improving all the time.A fun fact: UV protection can be put directly into the lens to protect your retinas even without sunglasses (I assume this depends on the polymer type though). Also the lenses are usually tinted a slightly yellow color since our natural lenses shift color as we age. Patients tend to find it jarring if the lens is perfectly clear.reply",
      ">  Last I heard, the rate of complications for a repeat surgery is significantly higher, but I assume it is improving all the time.My complications apeared 2 years after the cataract procedure but were solved through laser capsulotomy, not sure if you're referring to that.  But the capsulotomy is basically trivial compared to the main procedure.reply",
      "> UV protection can be put directly into the lens to protect your retinas even without sunglassesStating the obvious here but just to remind people about basic eye anatomy....The built-in UV protection does not remove the need to wear sunglasses.It does not protect the cornea.  Only sunglasses can do that.reply",
      "My wife had both eyes done over the past year. I would guess that her new lenses did not have UV protection, because she has a most interesting side effect.Basically, certain glass will (to her) essentially fluoresce in sunlight, so to her, it looks bright, bright purple. The glass in question looks like a slight, smoky grey to me, as does the glass vase at home with the same effect. I'd have to look for the link, but essentially, the new lenses filter less UV than the natural ones, and she's got a bit of sensitivity into that range of the spectrum.Similarly, a carved item made from yooperlite has the crystals showing orange to her in sunlight, where the rest of us need to hit it with a UV flashlight.reply",
      "Fascinating - I\u2019d think your brain would compensate pretty quickly for a no-longer-yellow lens? Given a choice I\u2019d be all for the clear one.reply",
      "They might, eventually. But the older you get, the more difficult it is to adapt or accept change.My grandmother was forced (by us) to get her cataract surgery when she mistook a horse for a known person. She assured for the rest of her life that she had better vision before the surgery.reply",
      "One of my best friends is a corneal surgeon at a university hospital, and does cataract surgery when there are other special complications. This is in Western Europe.He told me about a work trip to India he did and how amazed he was by the routine, efficiency and lack of waste there was over there in regular cataract surgery. Literally one doctor would handle 5x to 10x as many surgeries per day as their western counterparts. Where each surgery here requires full sterilization from scratch, there they kept their wrapping etc between surgeries, and had two beds side by side. The surgeon would do one surgery while the other patient would be changed. Then he turned around and did the next cataract surgery.We have a lot of waste in our medical practices.reply",
      "This sounds a bit like the Sunrise Hospital Emergency Room treating patients from the Mandalay Bay shooting in 2017>  I said, \u201cBring all your patients together.\u201d They brought them all towards me, and I was at the head of multiple beds, spiraling out like flower petals around its center. We pushed drugs on all of them, and they all got intubated, transfused, chest tubed, and then shuffled to Station 1.> the respiratory therapist, said, \u201cMenes, we don\u2019t have any more ventilators.\u201d I said, \u201cIt\u2019s fine,\u201d and requested some Y tubing. Dr. Greg Neyman, a resident a year ahead of me in residency, had done a study on the use of ventilators in a mass casualty situation. What he came up with was that if you have two people who are roughly the same size and tidal volume, you can just double the tidal volume and stick them on Y tubing on one ventilator.Groovy in an emergency scenario, but I, a humble non-doctor, like the idea of fewer compromises of sterility as the de rigueur way of doing things for non emergency surgeries.https://epmonthly.com/article/not-heroes-wear-capes-one-las-...reply",
      ">> What he came up with was that if you have two people who are roughly the same size and tidal volume, you can just double the tidal volume and stick them on Y tubing on one ventilator.That is seriously cool!reply",
      "As a patient, I'm not sure if I'd be comfortable with the doctor operating on me doing a speedrun.Full sterilization before each surgery is a good thing. Better safe than sorry. Same for only having one patient in the operating room - reduced risk of contamination and human error.reply"
    ],
    "link": "https://www.asimov.press/p/cataracts",
    "first_paragraph": ""
  },
  {
    "title": "OpenAI's H1 2025: $4.3B in income, $13.5B in loss (techinasia.com)",
    "points": 286,
    "submitter": "breadsniffer",
    "submit_time": "2025-10-02T18:37:28 1759430248",
    "num_comments": 303,
    "comments_url": "https://news.ycombinator.com/item?id=45453586",
    "comments": [
      "I think people are massively underestimating the money they will come from ads in the future.They generated $4.3B in revenue without any advertising program to monetise their 700 million weekly active users, most of whom use the free product.Google earns essentially all of its revenue from ads, $264B in 2024. ChatGPT has more consumer trust than Google at this point, and numerous ways of inserting sponsored results, which they\u2019re starting to experiment with with the recent announcement of direct checkout.The biggest concern IMO is how good the open weight models coming out of China are, on consumer hardware. But as long as OpenAI remains the go-to for the average consumer, they\u2019ll be fine.reply",
      "I also wonder if this will mean that even paid tiers will get ads. Google's ad revenue is only ~$30 per user per year, yet there is no paid, ad-free Google Premium. Why is this? After all, lots of users would gladly pay way more than $30/year have an ad-free experience. There's no Google Premium because Google's ad revenue isn't uniformly distributed across users; it's heavily skewed towards the wealthiest users, exactly the users most likely to purchase an ad-free experience. In order to recoup the lost ad revenue from those wealthy users, Google would have to charge something exorbitant, which nobody would be willing to pay.I fear the same will happen with chatbots. The users paying $20 or $200/month for premium tiers of ChatGPT are precisely the ones you don't want to exclude from generating ad revenue.reply",
      "> But as long as OpenAI remains the go-to for the average consumer, they be fine.This is like the argument of a couple of years ago \"as long as Tesla remains ahead of the Chinese technology...\". OpenAI can definitely become a profitable company but I dont see anything to say they will have a moat and monopoly.reply",
      "They're the only ones making AI with a personality. Yeah, you don't need chocolate flavored protein shakes but if I'm taking it every day, I get sick of the vanilla flavor.reply",
      "I think this is directionally right but to nitpick\u2026Google has way more trust than OpenAI right now and it\u2019s not close.Acceleration is felt, not velocity.reply",
      "I really don't trust either. Google because of what they've already done, OpenAI because it has a guy at the helm who doesn't know how to spell the word 'ethics'.reply",
      "That's mostly because LLMs think in terms of tokens not letters, so spelling is hard.reply",
      "He knows there's no \"I\" in \"ethics\"",
      "This really depends on where you are are. Some countries' populations, especially those known to value privacy, are extremely distrustful of anything associated with Facebook or Google.reply",
      "Google and trust are an oxymoronreply"
    ],
    "link": "https://www.techinasia.com/news/openais-revenue-rises-16-to-4-3b-in-h1-2025",
    "first_paragraph": ""
  },
  {
    "title": "RISC-V Conditional Moves (corsix.org)",
    "points": 50,
    "submitter": "gok",
    "submit_time": "2025-09-29T12:47:45 1759150065",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=45413059",
    "comments": [
      "One piece I would find interesting to see data on, but don't really know how to get meaningful information on against modern, non-academic cores is the fact that reorder buffers I've seen aren't just arrays of single instructions (and dependency metadata), but instead look more like rows of small traces of a few alu/ldst/etc instructions, and one control flow instruction per row.  It kind of ends up looking a lot like a a modern CISC microcode word with a triad/quad/etc of operations and a sequencing op (but in this case the sequencing op is for the main store, not the microprogram store).  That means in some cases, you have to fill the ROB entries with NOPs (that to be fair don't actually hit the backend) in order to account for a control flow op in an inopportune place.The conventional wisdom is that conditional moves mainly uplift in order pipelines, but I feel like there could be a benefit to increased ROB residency on OoOE cores as well with the right architecture.But like I said, I don't have a good way to prove that or not.reply",
      "Surprised an article published on September 28, 2025 does not include any mention of the P (packed integer SIMD) extension.P adds instructions like integer multiply-accumulate, which have a third register read (for rd). So, they're taking the opportunity to add a few forms of 3-register select instructions:   MVM     Move Masked\n           for each bit i:  X(rd)[i] = X(rs2)[i] ? X(rs1)[i] : X(rd)[i]\n\n   MVMN    Move Masked Not\n           for each bit i:  X(rd)[i] = X(rs2)[i] ? X(rd)[i]  : X(rs1)[i]\n\n   MERGE   Merge\n           for each bit i:  X(rd)[i] = X(rd)[i]  ? X(rs2)[i] : X(rs1)[i]\n\nActually I say I'm surprised but given the way the spec is currently spread around different parts of the internet, it's easy to miss if you're not following the mailing lists!reply",
      "This is implemented with instruction fusion. Just need to document properly and publish properly what will end up \"standard instruction fusion patterns\" (like the div/rem one).Adding more instructions is kind of non productive for a R(educed)ISC ISA. It has to be weighted with extreme care. Compressed instructions went thru for the sake of code density (marketing vs arm thumb instructions).In the end, programs will want probably to stay conservative and will implement only the core ISA, at best giving some love to some instruction fusion patterns and that's it, unless being built knowingly for a specific risc-v hardware implementation.reply",
      "> In the end, programs will want probably to stay conservative and will implement only the core ISAThis is probably not the case. The core ISA doesn't include floating point, it doesn't include integer multiply or divide, it doesn't include atomic and fence instructions.What has happened is that most compilers and programs for \"normal desktop/laptop/server/phone class systems\" all have some baseline set of extensions. Today, this is more or less what we call the \"G\" extension collection (which is short-hand for IMAFD_Zicsr_Zifencei). Though what we consider \"baseline\" in \"normal systems\" will obviously evolve over time (just like how SSE is considered a part of \"baseline amd64\" these days but was once a new and exotic extension).Then lower power use cases like MCUs will have fewer instructions. There will be lots of MCUs without stuff like hardware floating point support that won't run binaries compiled for the G extension collection. In MCU use cases, you typically know at the time of compiling exactly what MCU your code will be running on, so passing the right flags to the compiler to make sure it generates only the supported instructions is not an issue.And then HPC use cases will probably assume more exotic extensions.And normal \"desktop/phone/laptop/server\" style use cases will have runtime detection of things like vector instructions in some situations, just like in aarch64/amd64.reply",
      "Its not known as \"G\". The standard that is target by the software ecosystem is RVA20, RVA22, RVA23.https://riscv.org/ecosystem-news/2025/04/risc-v-rva23-a-majo...reply",
      "Instruction fusion still means lower code density. You can go overboard, but the newer ARM instruction set(s) are pretty good.reply",
      "As an aside: it's only relevant on microcontrollers nowadays, but ARM T32 (Thumb) code density is really good. Most instructions are 2 bytes, and it's got some clever ways to represent commonly used 32-bit values in 12 bits:https://developer.arm.com/documentation/ddi0403/d/Applicatio...reply",
      "Not necessarily lower density. On ARM you would often need cmp and csel, which are two instructions, eight bytes.RISC-V has cmp-and-branch in a single instruction, which with c.mv normally makes six bytes. If the cmp-and-branch instruction tests one of x8..x15 against zero then that could also be a compressed instruction: making four bytes in total.reply",
      "ARMv8.7 added some new instructions for int min/max to replace cmp+csel. (I'm surprised it took them so long to add popcnt.)https://www.corsix.org/content/arm-csscreply",
      "> publish properly what will end up \"standard instruction fusion patterns\" (like the div/rem one).The div/rem one is odd because I saw it suggested in the ISA manual, but I have yet to ever see that pattern crop up in compiled code. Usually it's just in library functions like C stdlib `div()` which returns a quotient and remainder, but why on earth are you calling that library function on a processor that has a divide instruction?reply"
    ],
    "link": "https://www.corsix.org/content/riscv-conditional-moves",
    "first_paragraph": "I'm a big fan of aarch64's csel family of instructions. A single instruction can evaluate rd = cond ? rs1 : f(rs2), where cond is any condition code and f is any of f0(x) = x or f1(x) = x+1 or f2(x) = ~x or  or f3(x) = -x. Want to convert a condition to a boolean? Use f1 with rs1 == rs2 == x0. Want to convert a condition to a mask? Use f2 with rs1 == rs2 == x0. Want to compute an absolute value? Use f3 with rs1 == rs2. It is pleasing that the composition of f1 and f2 is f3. I could continue espousing, but hopefully you get the idea.RISC-V is the hot new thing, but it lacks a direct equivalent to csel. Some cases of converting conditions to booleans are possible with the slt family of instructions in the base instruction set. Beyond that, a few special cases are implemented by instruction set extensions: Zbb adds min and max instructions which are a particular pattern of compare and select, and Zicond adds czero.eqz and czero.nez which again are particular patterns of compare and select"
  },
  {
    "title": "Pre-Record Your Demos (steveharrison.dev)",
    "points": 22,
    "submitter": "steveharrison",
    "submit_time": "2025-10-01T09:04:10 1759309450",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=45435720",
    "comments": [
      "I've done a lot of stage demos. I've done pre-recorded ones, I've done live ones.I prefer live ones, by a significant margin, even for non-deterministic AI things. In general I find live ones better, but it does depend on your circumstance. It's worth asking yourself three questions when you're doing a demo:What is the outcome when it goes great?What is the outcome when it goes wrong?What percentage of the time do each of those occur?With a pre-recorded demo, when they go great it's usually a 9/10. Things are solid, but they aren't adaptive to the situation, which means you might have lost an opportunity to insert a quip from something that happened earlier. When they go wrong though, it's a 0/10. You've told people that you don't have enough faith in your product to show it live - why should they have faith in it?. Now you might say that the demo goes right 100% of the time(ish), but there's also a human element to it. Someone might play the demo early, someone might not act in time with the live demo, any of these can give away that it's pre-recorded. I'd put their success rate at 95%.With a live demo, when they go great they're 10/10. You can adapt them, you can show it live, things react instantly instead of slightly-out-of-sync due to scripted content. It's incredible. When they go wrong, it's a 3/10. You've got egg on your face. People make fun of you after. But they know at that point that you ran a live demo. That does count for something. We know Zuck did that demo live. They do fail a bit more, especially in the age of AI. 10% failure rates are pretty good for live demos. To me though, the higher failure rate is worth it for the better PR if you do fail.reply",
      "I'm a sales engineer and demoing is one of the core parts of the job. I think this is applicable if it's an internal demo or a demo to a large audience, i.e a conference where you have a very fixed window of time to present.I would never recommend doing this in a sales context though, where you're selling to prospects. Mainly because it gives you no flexibility to pivot and adjust the content based on what they're interested in, and also because it gives off the impression you don't have a lot of faith in your product to perform live.I'd still 100% recommend having pre-recorded backups however. I was once doing a demo to our largest customer in region and the second we joined the call our core product had a SEV0. I was very relieved to have something to show while the engineers fixed the issue.reply",
      "A company I worked for switched over to pre-recorded demos and everyone talked about how clever it was for the first few larger audiences. Then they made a mistake and replayed a clip during the same session and the audience chat blew up. You could see a  dip in new users for days after the demo.reply",
      "If you pre-record your live demo then it isn't a live demo...What I dislike is presenting something as if it is live when it isn't. Demos are fantastic things and it is even perfectly fine to have imaginative aspects to them. But there's a division line between demoing and lying. Take Rabbit R1 as an example[0]. This whole thing was faked yet it was presented as a live demonstration, not an illustration. In other words, fraud.The difference is if you're trying to show your customers (or potential customers) what you're imagining the device works like vs what the device actually works like. The difference is how you're communicating the difference to them.I'm not sure why this is a contentious opinion, but just don't lie to people?[0] https://www.youtube.com/watch?v=22wlLy7hKP4reply",
      "Absolutely. I don't mind a recording, as long as it's presented as a recording and anything misleading is disclosed. If you say you're going to demo something, demo it. We have enough erosion of truth in this world already -- don't contribute to it!I do live demos at work all the time and the best part is that when someone asks \"can it frobulate with encapulation?\" I can actually press the frobulate button with encapulation enabled and show them. Can't do that with a recording.reply",
      "First time I recorded a demo was when it became too hard to return things to a known good state after every demo. In this case that meant multiple RAID arrays, 3 servers, an application with DB, and monitoring software.All so that I could demonstrate that when I turned off a RAID controller a red light came on and the application just\u2026continued.So much work for something largely unspectacular meant that a recorded demo that could be run anytime from a laptop was far more appealing. My junior colleagues were now doing demos they otherwise wouldn\u2019t, and the seniors turned off the sound and edited the video into snippets they could introduce and talk aroundreply",
      "I agree that you should always have a pre-recorded version as backup, but live demos communicate a confidence in your product, and that can be  worth something. Whenever I see a pre-recorded demo I wonder how many takes it took them, or how many pauses were taken out in editing, etc.Meta took that risk and failed.reply",
      "It helps if you have a master of ceremonies capable of running three rings simultaneously. Jobs could do it. Looking at what Apple is doing, they feel Cook is not. You lean into your strengths and avoid your weaknesses. Meta should realize that theZuck is not Jobs and is much closer to Cookreply",
      "I prerecord all my live demos, and wouldn't have it any other way:https://gitlab.com/stavros/itsaliveI can even pause the replay mid-demo, take over, and resume again later. People are always impressed at how flawless my live demos are, and with some judicious audience participation, the illusion is perfect.reply",
      "I love this idea.  I remember giving a live demonstration of a configuration management tool (SaltStack circa 2012) and some unknown networking conflict between my Virtual Machines and the local wifi subnet caused my commands to timeout.  I was too junior to resolve it live and was left with boring slides explaining what ought to be happening.Luckily it was a small group of devs and not a large venue.  Still embarrassing...reply"
    ],
    "link": "https://www.steveharrison.dev/pre-record-your-demos/",
    "first_paragraph": "Meta was in the news recently for two failed live demos:Mark Zuckerberg's Meta AI glasses demo failed in EPIC fashion today \ud83e\udd23pic.twitter.com/O0UFeoCar7Meta AI's live demo failed for the entire minute \ud83d\ude22 pic.twitter.com/du4roaW0ERWhile there were a lot of people who said they'd rather see a live demo than pre-recorded content such as Apple events nowadays, I disagree: I think most demos should be pre-recorded.Here's one of the reasons the Meta demos failed:So here\u2019s the story behind why yesterdays live #metaconnect demo failed - when the chef said \u201cHey Meta start Live AI\u201d it activated everyone\u2019s Meta AI in the room at once and effectively DDOS\u2019d our servers \ud83e\udd23 That\u2019s what we get for doing it live!There are many reasons why live demos can fail, such as:When Steve Jobs demoed the iPhone 4 at WWDC2010 (I was lucky enough to be in the room!), there were network issues caused by all the MiFi routers in the audience:And if you're demoing something locally, you need to have your environment set "
  },
  {
    "title": "Why I chose Lua for this blog (andregarzia.com)",
    "points": 114,
    "submitter": "nairadithya",
    "submit_time": "2025-10-02T16:58:55 1759424335",
    "num_comments": 68,
    "comments_url": "https://news.ycombinator.com/item?id=45452261",
    "comments": [
      "I miss the days when \"implement your own blogging engine\" was one of the most popular learning projects for engineers.We should bring that back! Its such a great way to play around with client- and server-side development options in an almost zero-risk environment.reply",
      "Still a good project when one learns a new programming language.reply",
      "Everything web-facing, if it's not a static website delivered by a well-tested web server, happens in a high-risk environment. And doubly so, if, like in this case, stuff like custom cgi libraries are involved. One has to be either very confident in their skills to do that or very, very brave.reply",
      "My point here is that if someone breaks your blog, they've broken your blog. The blast radius of that should be strictly limited.Obviously don't go rolling your custom CGI scripts on a server that also hosts your personal email - but these days we are spoiled for choice in terms of isolated hosting strategies for a blog.Heroku, Vercel, Cloudflare Workers, Fly.io, GitHub Pages, a $5/month VPS...reply",
      "Ok. I'll bite. What are the risks? As I see it, if you screw up and someone get root access on your server, it's the worse that may happens and then shut down the whole thing is a click away.reply",
      "What? Are you serious?If you roll your own little cgi-bin perl script behind Apache you're far from vulnerable compared to, say, a WordPress website.reply",
      "In any web application all data that comes from the outside world is potentially hostile. A decent web framework takes care of basic security measures, does input sanitation, provides referer checking and csrf for forms, etc. When you roll your own, your _are_ on your own to do that all properly yourself, if you even know all the potential pitfalls. And if you write your own cgi library like the op even more so. I'm not advocating for using WordPress either. I'm advocating for either having a static blog or using a decent, tested web framework or the the very least cgi module that provides tested implementations of common security features that in my experience are typically missed in self-made cgi scripts.reply",
      "Nah. There are ways to mitigate the blast radius of experiments like this.You can't utterly wrap developers up in cotton wool. Ultimately, people learn strong lessons from screwing up. You can at least make sure they're doing this learning within a sandbox in which the damage is contained. Like, maybe containerising it so it has limited access to anything that could do any real damage. If somebody builds a blogging app and the worst thing that happens is that they learn the value of sanitising your input, preventing SQL injection, that spam is an unfortunate fact of life, and that you should be very careful with how you manage cookies, I would consider that a _very_ big win for that novice developer.Because we're talking about _developers_ here.reply",
      "I am not exactly sure about using containers (most likely you mean docker/podman) as sandboxes...Please correct me wrong but for a better form of sandboxing, I would recommend something like microvm and the bottlefire thing which was recently shared if that might pique your interest as I found it to be interesting and then using it with something like https://github.com/Zouuup/landrunThere is also tinykvm and other projects too which can simply take a binary and run it and I think that maybe developers should also try out all the different sandboxes and different things just for understanding as well I suppose too. To me, I really like playing with different form of sandboxes or such technologies in general.Also do note that I am not sure if bottlefire provides sandbox/isolation by default as they mention Sandbox with Landlock seperately so I am now a bit confused if they provide sandbox by default or not as I previously thought it might have.reply",
      "I love the Lua language, has all the nice to have data structures, no need to think about memory management, is straightforward (keep the whole thing in your head easily) and will go forever without changes. Just stick with v5.1.It\u2019s so cool to take the interpreter source, type make and be using it in one minute.reply"
    ],
    "link": "https://andregarzia.com/2025/03/why-i-choose-lua-for-this-blog.html",
    "first_paragraph": "This blog used to run using with a stack based on Racket using Pollen and lots of hacks on top of it. At some point I realised that my setup was working against me. The moving parts and workflow I created added too much friction to keep my blog active. That happened mostly because it was a static generator trying to behave as if it was dynamic website with an editing interface. That can be done really well \u2014 cue Grav CMS \u2014 but that was not the case for me.Once I decided to rewrite this blog as a simpler system, I faced the dilema of what stack to choose. The obvious choice for me would be Javascript, it is the language I use more often and one that I am quite confortable with. Still, I don't think it is a wise choice for the kind of blog I want to maintain.Talking to some friends recently, I noticed that many people I know that have implemented their own blogging systems face many challenges keeping them running over many years. Not because it is hard to keep software running, but beca"
  },
  {
    "title": "N8n added native persistent storage with DataTables (n8n.io)",
    "points": 131,
    "submitter": "XCSme",
    "submit_time": "2025-10-02T14:26:23 1759415183",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=45450044",
    "comments": [
      "Didn't their change their licensing or something and now folks are leaving? I've seen a few ex-N8n'ers coming over to the Node-RED forum, hence the question.reply",
      "You can also try https://github.com/autokitteh/autokitteh. Python based, completely open source. Got inter workflow storage as wellreply",
      "Hadn't heard of Node-RED but this is really cool. Thanks for mentioning it.reply",
      "Yet more evidence that venture capital is basically incompatible with open source software.  It's just a matter of time before any VC-backed open source company betrays its users.reply",
      "n8n was never open source. They started out using Apache with a non-free clause added, but they called themselves open-source incorrectly. Then when this became more widely known, they stopped calling themselves open-source.reply",
      "Why is it okay to just accept that the hyperscalers are 100% closed, but the minute a smaller player tries to play \"open-ish\" with \"fair source\" we crucify them?Fair source is amazing. You get the code. You can modify the code. You can redistribute the code. You just can't take their business from them and compete with them head-to-head with their product. You can reformulate it into something else, but you can't take their labor and cut their knees off with it. You have literally every other freedom.Why in the hell are so many people against that?Fair source is sustainable and equitable. You get everything except for that one little right with which you could compete with them.Everyone gushes over Obsidian - that's not even \"source available\" or \"fair source\". It's completely closed.I bet if Obsidian went \"fair source\", and you could download the code and compile it yourself, there would be hundreds of voices crawling out of the woodwork to call them the devil for not being pure OSI-approved open. How dare they keep one little freedom to themselves for all the hard work they've done?I'm half convinced the anti-\"fair source\" voices are deep industry folks who want big tech to own the OSI definition. Who can use it strategically as they sit atop hundreds of billions of dollars in cash flows. Open source to the FAANGMAGMA Gods is just a way of outsourcing labor and dangling trinkets in front of underpaid labor.n8n isn't doing a \"VC rug pull.\" They're trying to be sustainable. The only thing that's been \"pulled\" is the wool over the eyes of every engineer satisfied with FAANG OSI-approved serfdom. Some of the pieces are \"open\" all right, but they own pretty much everything the sunlight touches.If n8n can't build a business then it just becomes some side project hobby. What exactly do we think every other company is doing?Please be more pragmatic and realize that \"fair source\" is sustainable. It rewards the innovators and you get almost everything for free. You just can't shoot them with their own gun.Do you not want the ability to download the source and potentially tweak the software? To have a copy you can hold onto forever? To be able to analyze it for telemetry. To be able to potentially submit patches (if you're feeling generous)?Don't you want them to make money? Rather than beg for Github donation scraps?Be especially mindful if you're typing a retort on a Macbook Pro or iPhone and deploying software to GKE or AWS.---https://faircode.io/https://fair.io/reply",
      "I care that software I depend upon survives long term. In consequence, I care to some extent that the principal company or organization building that software also survives... But that finite concern is balanced against the simultaneous concern that the software itself can survive the organization's potential dissolution. I don't want to have to make a sudden, stressful, expensive migration because corporate buy-outs, bankruptcies, or mergers result in rug-pulls for software I use.\"Fair\" source does absolutely nothing to allievate those concerns. The four software freedoms do.It is true that for a quarter century software developers have naively licensed their projects under MIT and BSD style permissive licenses, and have then felt robbed when big companies come in and eat their lunch. Except it was never their lunch because they didn't actually use licenses that would have asserted their rights to that lunch.Thankfully, that naivete is finally, slowly dying but, rather than use robust solutions that have been around for decades (e.g. GPL-or-AGPL/proprietary dual license), some developers have invented a new, largely untested concept and named it by dressing up their own sour grapes and spite as \"fairness\".\"Fair\" source software is a non-solution in search of a problem.Free software and strong copyleft licenses already exist. Dual-licensing already exists. Viable, profitable, healthy, ethical companies built on these strategies already exist.I used to think Stallman was a crank and a fundamentalist, and he absolutely is, and thank goodness for that. I now think \"open-source\" is exactly as diluted as free software advocates initially pointed out, and the emergence of \"fair\" source is part of the damage it has done.reply",
      "Authors using Fair want to share their code while getting protections for themselves. Strong copyleft doesn't care about authors and is all about protecting the end users.So Fair fullfills an actual need or desire not covered elsewhere, thus is not a non-solution.It might be not the appropriate or the best solution to solve the exact concerns of people using it, that's debatable and a different topic, akin to using the wrong tool for the job. Strong copyleft is the wrong tool, too; obviously competitors can just deploy without modifications and offer it as a service.reply",
      "> Authors using Fair want to share their code while getting protections for themselves. Strong copyleft doesn't care about authors and is all about protecting the end users.This is a one-sided assertion of fairness, and therefore an abuse of the concept. Free software offers the same rights to both authors and users. That is fairness.If you want to reserve rights to yourself that are withheld from end-users, that's fine. Arguably even still within a broader conceptual realm of fairness. But naming your personally preferred arrangement of rights \"fair\" and in so doing implying most or all other arrangements are unfair is just plain arrogant.Strong copyleft cares equally about authors and end-users. It doesn't disregard authors. Some authors just want to co-opt the general notion of fairness to mean their own licensing preferences.Call it a non-compete license and be forthright.reply",
      "I don't know/care about \"fair source,\" but the \"open source\" label in particular has ALWAYS had a well-known and specific meaning in the software world. Attaching it to a proprietary product for the marketing and social media good feels is actively deceitful, no matter if the company is USA trillion-dollar Big Tech or a single developer from a marginalized demographic in a developing country.Software authors have the right to choose whatever license they wish for the project/product, just don't try to lie to users about what it really is.reply"
    ],
    "link": "https://community.n8n.io/t/data-tables-are-here/192256",
    "first_paragraph": "Since the very beginning of n8n we\u2019ve heard many of you mention the need for a proper table inside n8n to store data between workflow executions without needing to switch platforms or setting up credentials and now it\u2019s finally here.With data tables you can:Save specific data from your workflow runsKeep data around between multiple executionsAvoid duplicate runs by tracking execution statusStore reusable prompts for different workflowsCollect evaluation data for your AI workflowsDo lookups, merges, enhancements\u2026\u2026and honestly, probably 100 other creative things we haven\u2019t even thought of yet   To make sure your instance stays performant, we\u2019ve set a 50MB limit for everyone. If you\u2019re self-hosting (and know what you\u2019re doing), you can change that via the ENV variable N8N_DATA_TABLES_MAX_SIZE_BYTES Upgrade to 1.113, give data tables a spin, and let us know what you think! What\u2019s missing? What would make it even more useful for you? We\u2019re really curious to hear your ideas and thoughts!  Re"
  },
  {
    "title": "Class Dismissed: Profile of Joe Liemandt and Alpha School (joincolossus.com)",
    "points": 25,
    "submitter": "surprisetalk",
    "submit_time": "2025-09-29T15:41:50 1759160510",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=45415196",
    "comments": [
      "Warning: The article is ~18K words.  Longer than a long-form journalism article; more like novella length.  I started skimming faster when it diverted into an extended YA drama, and eventually gave up.reply",
      "Also see this anonymous entry into an essay-writing contest by one of the parents:https://www.astralcodexten.com/p/your-review-alpha-schoolreply",
      "Read both. This is fascinating. Notes:- 1 on 1 tutoring yields about 2 standard deviations of improvement, but costs too much.- The Alpha system is very effective but costs about $40,000/student.- It's not that complicated. 2-3 hours a day of intense computer-monitored study, about half using off the shelf tools, and the rest of the day is developed to less structured projects. This is a useful insight. Push kids hard for about 2-3 hours a day and then get to the fun stuff.- Works on ordinary kids. Strong claims in this area. Needs outside validation.- Lots of carrot, including cash payments for achievement. Not much stick.- Staff/student ratio about 1:5. No teachers from bottom quartile of teacher population.reply",
      "As noted in the article, for people who won't RTA, the original 2sigma observation of 1 on 1 is due to Bloom more than 40 years ago.reply",
      "Amusingly, $40k is cheap for private school in SF so if they can bring the model here at the same cost...reply",
      "That article mentions family income as possible confounder only once, while in an article of this extent I would expect it to appear dozens of times and be the central point. You simply cannot draw conclusions from comparing general populations and kids in schools that cost $500k for a 13-grade education, or that cost any amount. Students from low income households have so many headwinds. They don't come to school because their transportation is unreliable, because their teeth hurt, because they can't afford sanitary pads, they can't sleep because their neighborhoods are too loud, and so on. When this school opened in Brownsville and preferentially enrolled only the children of employed aeronautical engineers, that precluded any possible comparison of outcomes.reply",
      "But you can draw comparisons with other expensive private schools, which is what the ASC article does.reply",
      "Yes, they are tiny and it has to be considered experimental and unproven as far as scalability goes. They are a long way from trying to educate millions of kids.But it will be interesting to see how far it scales up and into what demographics before it stops working and they reach their limits.So far, it seems they've had success with in-person schools and it didn't work (yet) remotely. Motivation seems like the key factor.reply",
      "Scalability is the issue, though, isn't it? Any number of esoteric pedagogical approaches have been tested, to great success, with the children of people wealthy enough to fulfill all of their scions' material needs and subsequently move onto purchasing immaterial things like \"attention\".You have to figure out something you can apply kids in spite of who their parents are. If not, you have to admit that there's no such thing as a meritocracy divorced from what the parents are willing or able to provide (which opens up questions of how much children deserve to have that deficit filled, it if happens to exist in any individual case).reply",
      "I'd be curious to have adults use their learning program as well, and assess their aptitude at learning with it.reply"
    ],
    "link": "https://joincolossus.com/article/joe-liemandt-class-dismissed/",
    "first_paragraph": ""
  },
  {
    "title": "A simple habit that saves my evenings (alikhil.dev)",
    "points": 116,
    "submitter": "alikhil",
    "submit_time": "2025-09-29T12:29:29 1759148969",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=45412870",
    "comments": [
      "The habit is in the 7th paragraph, after 2 images:> Rather than trying to complete your task in 20 minutes, take this time to write down your thoughts, and a step-by-step action plan of what you think you need to do to finish your task. Then go home. Rest. A feeling of incompleteness will motivate you to come back and finalize your work the next day. Only you will be full of energy, together with a settled plan. No doubt you\u2019ll accomplish your task before lunch.reply",
      "> A feeling of incompleteness will motivate you to come back and finalize your work the next day.Unfortunately, in between there's sleep, which is the great feelings eraser / emotional cache flusher. So all that'll happen is that said \"feeling of incompleteness\" will distract me for the rest of the evening, then disappear at night. Come morning, it won't be there, so I'll have to read the plan to hopefully induce that feeling again.reply",
      "I do the same thing as the author does. Moreover, I keep these notes more frequently, to the project's notebook with my other thoughts during the day.When I return to the task, I get up to speed in ~10 minutes, and things way go smoother because a) I'll be rested, b) My brain would have processed the plan and came up with a refined version of it. When I read the plan I wrote, I automatically recall the refined version most of the time.Then, I get to work and finish what I have started.Interestingly, sleep doesn't erase my emotions, but pause them. I just continue from where I left.reply",
      "sounds like you are more of a morning person,as a non-morning person, it usually takes me about 4-6 hours to get up-to-speed. because there would be some unexpected twists and turns that will distract me from the task.meanwhile, exact opposite. if i cannot \"solve\" an issue, if i just wait till the evening by going for a 100+km bicycle ride, i magically solve the problem in the evening (right about at dusk and after).reply",
      "A good night of sleep induces its own feeling of well-being, excitement, awesomeness. I swear, I have terrible sleep and when I get an 85+ sleep score I'm a different person.reply",
      "This is related to the zeigarnik effect which states that an activity that has been interrupted may be more readily recalled.https://en.wikipedia.org/wiki/Zeigarnik_effectreply",
      "Sounds like the Hemingway method[0], with notes.I do this all the time, quitting when I know what comes next, but noting it down so I don\u2019t forget and become frustrated.[0] https://bk2coady.medium.com/the-hemingway-method-fb56bf93836...reply",
      "a feeling of incompleteness couldn't even motivate me to finish thireply",
      "I see what you\u2019re doing, yet it\u2019s still infuriating me.reply",
      "I'm infuriated treply"
    ],
    "link": "https://alikhil.dev/posts/the-simple-habit-that-saves-my-evenings/",
    "first_paragraph": "As a software engineer, I often work on big tasks that require hours of continuous and focused work. However, we have plenty of meetings, colleagues asking us something in Slack, and lunch breaks. Add a colleague who comes to you and calls you for a cup of coffee if you work from the office.And usually, we don\u2019t really have such a luxury as hours of uninterrupted time.Nevertheless, sometimes we catch the flow of productive and focused work at the end of the workday. Imagine you come up with an elegant solution to a problem you\u2019ve been tackling all day, or maybe even the whole past week. You can\u2019t wait to implement and test your solution. And of course, you are so driven by your idea that you decide to continue working despite your working day being over. \u201c20 minutes more and I will finish it,\u201d you think. Obviously, this is not the case; some edge cases and new issues will inevitably arise. You come to your senses only 2\u20133 hours later\u2014tired, hungry, demotivated, and still struggling wit"
  },
  {
    "title": "How I block all 26M of your curl requests (foxmoss.com)",
    "points": 43,
    "submitter": "foxmoss",
    "submit_time": "2025-09-29T19:37:21 1759174641",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=45417826",
    "comments": [
      "> with tools like Anubis being largely ineffectiveTo the contrary - if someone \"bypasses\" Anubis by setting the user agent to Googlebot (or curl), it means it's effective. Every Anubis installation I've been involved with so far explicitly allowed curl. If you think it's counterproductive, you probably just don't understand why it's there in the first place.reply",
      "If you're installing Anubis, why are you setting it to allow curl to bypass?reply",
      "The problem you usually attempt to alleviate by using Anubis is that you get hit by load generated by aggressive AI scrappers that are otherwise indistinguishable from real users. As soon as the bot is polite enough to identify as some kind of a bot, the problem's gone, as you can apply your regular measures for rate limiting and access control now.(yes, there are also people who use it as an anti-AI statement, but that's not the reason why it's used on the most high-profile installations out there)reply",
      "Those bots would be really naive not to use curl-impersonate. I basically use it for any request I make even if I don\u2019t expect to be blocked because why wouldn\u2019t I.reply",
      "There are plenty of naive bots. That is why tar pits work so great at trapping them in. And this TLS based detection looks just like offline/broken site to bots, it will be harder to spot unless you are trying to scrap only that one single site.reply",
      "I heard about curl-impersonate yesterday when I was hitting a CF page. Did something else to completely bypass it, which has been successful, but should try this.reply",
      "I got exactly this far:    uint8_t *data = (void *)(long)ctx->data;\n\nbefore I stopped reading. I had to go look up the struct xdp_md [1], it is declared like this:    struct xdp_md {\n        __u32 data;\n        __u32 data_end;\n        __u32 data_meta;\n        /* ... further fields elided ... */\n    };\n\nSo clearly the `data` member is already an integer. The sane way to cast it would be to cast to the actual desired destination type, rather than first to some other random integer and then to a `void` pointer.Like so:    uint8_t * const data = (uint8_t *) ctx->data;\n\nI added the `const` since the pointer value is not supposed to change, since we got it from the incoming structure. Note that that `const` does not mean we can't write to `data` if we feel like it, it means the base pointer itself can't change, we can't \"re-point\" the pointer. This is often a nice property, of course.[1]: https://elixir.bootlin.com/linux/v6.17/source/include/uapi/l...reply",
      "Your code emits a compiler warning about casting an integer to a pointer. Changing the cast to void* emits a slightly different warning about the size of integer being cast to a pointer being smaller than the pointer type. Casting to a long and then a void* avoids both of these warnings.reply"
    ],
    "link": "https://foxmoss.com/blog/packet-filtering/",
    "first_paragraph": "All code in this blog post is fully open source at\ngithub.com/FoxMoss/fox-xdp.Bots have always been a problem on the internet. Be it DDOSing, AI scrapers,\netc. It\u2019s a treadmill problem and no solution will ever be perfect, but we can\nblock many minimum effort attempts.I recently went down a coding rabbit hole. Packet filtering and analysis is a fascinating field.\nThere\u2019s just a bone in my body that likes writing software at a scale that\u2019s bigger than it\u2019ll ever\nbe used. So how do you handle network requests fast? Well, write an operating system and custom\nnetwork drivers specifically optimized for speed. We won\u2019t be doing that today, but we can get closer\nto the bare metal fairly easily without sacrificing the Linux ecosystem. XDP - Express Data\nPath is that way of getting closer to your network\ndevice. And according to some benchmarks Wikipedia cites, you can drop 26 million packets per second\non consumer hardware. This is really good load handling, especially for a single server. Bot"
  },
  {
    "title": "Launch HN: Simplex (YC S24) \u2013 Browser automation platform for developers (simplex.sh)",
    "points": 28,
    "submitter": "marcon680",
    "submit_time": "2025-10-02T16:07:23 1759421243",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=45451547",
    "comments": [
      "The Coupa portal is an amazing example because I've had to personally spend ~10+ hours clicking that new invoice button and retyping the exact same info.If you want to set up monthly billing in Coupa, you just manually create 12 invoices and schedule them out. Each time you have to retype all the account information from scratch, and there are a few landmine buttons on the page that will clear all the forms and make you start from scratch. I can't imagine the thousands of human hours lost every year to just filling out fields in Coupa.reply",
      "Yes, it's a huge pain from what we've seen. That portal is actually running in production for some of our AR/AP AI customers, and getting browser agents to properly parse that page was difficult -- Coupa injects all the DOM elements through a JavaScript <script/> tag like 2-5 seconds after the page loads, non-deterministically. :)reply",
      "What about hybrid automations or human-in-the-loop flows? We have automations where the human starts by logging in, then hands over to the agent. Some parts may even be Puppeteer automated. This also means the session may be long running, typically for months at a time and the agent needs to notify the human again if they get logged out. None of the existing browser automation platforms I have tried make this easy or cost effective, so we are currently trying to build our own. Would love to consider Simplex if this is solved.reply",
      "Could I ask why the flow starts with a human logging in? Is it because you're using their credentials and/or have some sensitivity around storing their credentials? Or is it something to do with 2FA (we handle 2FA)? Or are you just storing the session data after they log in so you can re-use it for those few months you mentioned?Re: Puppeteer automation as part of the script -- we have a feature we wrote for one of our customers that we didn't promote to production where you can define a deterministic action in the dashboard that allows you to paste in JavaScript, but we're likely not to push that to prod anytime soon. Could you explain your reasoning for wanting to use Puppeteer still? We've generally seen customers fully switch over to Simplex instead of relying on their original Puppeteer/Playwright scripts -- since we have action caching, the underlying script (click on div locator with this div id, etc.) is pretty similar to what you'd get using Playwright.reply",
      "I can totally see the value of agent driven flows for automating flows that are highly dynamic, poorly specified, error prone, zero shot environments, etc, but that doesn't seem to be at all what you are demonstrating here. Maybe your demos could show something more \"challenging\" to automate?As someone who has spent a LOT of my time in my career working on browser automation and testing, speed and cost was always key. Even with existing programmatic tools like selenium, playwright, cypress, etc speed and headfull hosting costs were already big issues. This seems orders of magnitude slower and more expensive. Curious how you pitch this to potential customers.reply",
      "We've generally seen the \"easy\" flows not actually be easy. Workflows that have complex branching logic (shown when filling out the Aetna form in the first example in the video), structured scraping (the second example we showed in the video), and login/2FA/intelligent multi-agent scraping (shown in the last example in the video), are all things that are difficult to impossible to do with traditional automation libraries.We also have an example of a complex, multi-agent workflow here that might be useful for you to look at: https://www.simplex.sh/blog/context-engineering-for-web-agen...reply",
      "got it, I only looked at the website not the youtube video you posted above, my apologies. On the website, neither the billing platform demo nor the screenshots in the section below convey this value prop very well. Both sections show what appear to be trivial flows without explanation of some of the underlying complexities.I suppose if you are hitting your target demographic dead-on with your marketing efforts, the value prop should be completely obvious to them, but still could be more explicit in your differentiation.reply",
      "replying to myself here... I would be interested to see a more hybrid approach where an AI could step in to help retry / get past failures, or as a way of re-recording automation steps for a flow when something changes, but having AI in the loop for every action all the time feels wasteful at best.reply",
      "Yep we actually cache flows after first run! This makes flows that are closer to traditional RPA pretty much the same as using Playwright/Puppeteer.reply",
      "Great! I see that further down in the website, which I did not see before posting this comment. I think this could be valuable to demonstrate / communicate in the billing platform demo which is the first thing you see, and is what captured all of my attention (i never even scrolled down).Edit: I just re-ran the demo and it seemed way faster this time??? the first time it said GOAL: PRESS_ENTER... (agent proceeds to think about it for 5-8 seconds) which seemed hilarious to me.reply"
    ],
    "link": "https://www.simplex.sh/",
    "first_paragraph": "Simplex provides all the infrastructure needed for modern browser automation.  Remote browsers, steerable web agents, and more.Demo PreviewAgent logs will appear here when demo is runningReliably automate every legacy portal your customers use.Simplex has been used to log into a billing portal and download the list of invoices for a specified customer.Simplex has been used to fill out complex, branching-logic prior authorization forms on medical provider portals.Simplex has been used to automate data entry and download report PDFs across different ERPs.Simplex has been used to search and extract structured information across public government portals.Simplex has been used to log into a TMS portal, create and edit the information for a shipment, then dispatch the shipment.Book a call with us to discuss your specific use case.Reliably automate every legacy portal your customers use.Simplex has been used to log into a billing portal and download the list of invoices for a specified custom"
  },
  {
    "title": "How Israeli actions caused famine in Gaza, visualized (cnn.com)",
    "points": 785,
    "submitter": "nashashmi",
    "submit_time": "2025-10-02T09:23:50 1759397030",
    "num_comments": 698,
    "comments_url": "https://news.ycombinator.com/item?id=45447699",
    "comments": [
      "Israel's response to Oct 7th has been a major blackpill.Their strategy was, I think, as bad as it could possibly be.  In fact, it really seemed, and still seems, like no strategy at all -- they lashed out wildly and extremely destructively, without a clear picture of what the post-war Gaza Strip will look like.Hamas successfully baited Israel into a disproportionate response that killed tens of thousands, if not hundreds of thousands, which played directly into the dynamics of guerrilla warfare where a strong state's extreme actions against a weak opponent undermine its legitimacy.Walking into such a trap tends to be a real world-historical blunder for any nation.Yet, rather than adapting, Israel's network doubled down with censorship campaigns, crackdowns on protests, and weaponizing \"anti-semitism\" accusations to silence critics -- actions that have all backfired.  Now international support is collapsing, the EU is pushing sanctions, and the US is slowly distancing itself.  Israel's best option right now is to end the war as quickly as possible, and devote all of its efforts to repairing damaged relationships and mitigating the war's effects, before isolation accelerates to the level of sanctions similar to those imposed on South Africa.I'll also note that it's interesting how all sides seem to have lost.  Hamas lost the shooting war, the people of Gaza have lost lives and livelihoods which may take more than a decade to rebuild, and Israel lost the information/media war so damn badly that it may genuinely not recover from this.reply",
      "A big question for me continues to be how much of Israel's behavior isn't really about best options for Israel as a state, but power politics for particular political factions internally.reply",
      "> the US is slowly distancing itself.Public opinion in the US has turned against Israel, yes. Trump doesn't care about public opinion. He'll be buddy-buddy to Netanyahu other than symbolic acts of distancing / reprimanding.reply",
      "I think that's mostly accurate, but to be fair he did recently sign an executive order guaranteeing military defense of Qatar, which was clearly a message to Israel that they better not mess with them. It's far from a backpedal on his support for Israel, but it does show he won't let them do anything they want (at least if he is speaking honestly)reply",
      "> Trump doesn't care about public opinionOf course he does. But he's currently most sensitive to Republican voters' opinions, and they're still at 64% net sympathy for Israel and 9% for Palestine. (55% of Trump voters say \"Israel should continue its military campaign until Hamas is fully eliminated, even if it means the civilian casualties in Gaza might continue,\" while only 29% say \"Israel should stop its military campaign in order to protect against civilian casualties, even if Hamas has not been fully eliminated\" [1].)As the midterms come closer, that 26% independent net support for Israel becomes more pertinent, as do the 67% of independents who want Israel to stop its campaign.[1] https://www.nytimes.com/interactive/2025/10/02/polls/times-s...reply",
      "Trump turns on his friends the moment it's convenient to do so.reply",
      "> Israel lost the information/media war so damn badly that it may genuinely not recover from thisIsrael probably came out ahead if all they lost is prestige.They've neutered Iran and become the de facto regional security power. Their weapons and military have been validated, which makes than a desirable trading partner in an increasingly-militarising world. And they're turning into a gas exporter.Worst case, a generational shift occurs and Israel loses its military support from America. (I don't see us sanctioning Israel any time soon, so its economic primacy will remain intact. And we only pay for like 15% of their military budget, so not a disaster.) Do you really think China and India would even hesitate to partner with Tel Aviv on defense?reply",
      "Israel an the US are a single entity when it comes to security matters in the middle east. It was already the de facto regional power.reply",
      "You may not realize it but Israel is slowly becoming Rhodesia/Apartheid South Africa. And i don't mean the word 'apartheid' as a cudgel.During the Rhodesian Bush War, their forces ran circles around the ZIPRA and ZANLA with multiple battles and encounters where they'd routinely record 500:1 KD ratios like Operation Dingo, etc. They had complete freedom of action to bomb any infrastructure obstructing them, reach deep into neighboring countries and slaughter guerillas copiously.Hell, South Africa had a dozen nukes.Once the sanctions came on, it unraveled everything they had.Israel is in such a precarious situation right now. Their economy depends on technology exports to an extreme degree. Cutting off that source of FX would literally half the economy overnight because cash would stop sloshing around internally from its main sources.If that happens, all the smart kids propping up the economy will move out while you're left with extremists who want war but won't fight in the army. In fact, it's ongoing right now with people leaving the country in the midst of a war they're 'winning.'You might think sanctions are a far-off notion, but key Western powers are breaking with America on recognizing Palestine. That's a red line designed to signal to Israel that it's losing ground. People across the world are calling for sanctions and it won't be long before they materialize.And America? Israel's main power base are American boomer evangelicals who're going the way of the dinosaur. Like I said in another comment, their kids are with not religious, don't like bombing kids, have been radicalized by the atrocities they've witnessed, or are aligned with people like Fuentes.I hope they can smell the coffee; if anyone had told South Africa that a nuclear power could be disarmed without a gunshot, they'd never have believed it. But, look what eventually happened.Thanks to the ongoing genocide, America's voting demographic for the next 40 years has begun to see Israel as a genocidal terrorist state. They will be voting for the next 50 years, while the boomer evangelicals die off.reply",
      "The article says on a low period they found people were eating 1400 calories a day at one point. If there was more than that they would have listed it. Israel targets a higher number but that is what it got to in the worst situation the article could find to list.The United States Red Cross sets as a floor 1500 calories a day for people in distress. Is the Red Cross trying to starve Americans in distress?https://emergency.lacity.gov/sites/g/files/wph1791/files/202...The UN just cut food aid in Kenya for refuges from the war in Sudan in half, yet the UN says it is willing and able to provide significantly higher amounts of food (and to not do so would be criminal) to the people of Gaza. Is the UN criminal/genocidal for offering starvation level assistance to one group but significantly more to another? The UN says they are ready and able to provide assistance to one group at the very same time the cut in half/say they can't provide aid that meets the level they say Gaza must receive when it is aid to a differing group.reply"
    ],
    "link": "https://www.cnn.com/2025/10/02/middleeast/gaza-famine-causes-vis-intl",
    "first_paragraph": "\n            Israel\u201a\u00c4\u00f4s nearly two-year war pushed parts of Gaza into \u201a\u00c4\u00faman-made\u201a\u00c4\u00f9 famine, according to a report published in August by a United Nations-backed initiative, deepening the Palestinians\u201a\u00c4\u00f4 struggle for survival under relentless bombing, mass displacement and the spread of disease.\n    \n            The report by the Integrated Food Security Phase Classification (IPC), a UN-backed expert panel that assesses global food insecurity and malnutrition, helped to fuel growing international outcry over Israel\u201a\u00c4\u00f4s campaign in Gaza following the Hamas-led October 7, 2023, attacks \u201a\u00c4\u00ec and was cited by some of countries that recently made moves towards formally recognizing a Palestinian state. The IPC forecast that by the end of September nearly a third of Gaza\u201a\u00c4\u00f4s total population would face famine conditions, although it has not yet provided an update on that forecast.\n    \n            In Gaza governorate alone \u201a\u00c4\u00ec the largest by population of five in the Gaza Strip \u201a\u00c4\u00ec more than h"
  },
  {
    "title": "Liva AI (YC S25) Is Hiring (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-10-02T17:01:16 1759424476",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/liva-ai/jobs/6xM8JYU-founding-operations-lead",
    "first_paragraph": "Scale AI for video and voice data.The mission at Liva AI (YC S25) is to make AI feel truly human.\u00a0AI voices and faces today still feel flat and generic, missing emotion, nuance, and identity. We\u2019re fixing that by building the world\u2019s richest library of human voice and video data, fueling the next generation of realistic AI.We\u2019re hiring an extremely organized and committed operator to take on a full-time role. You\u2019ll manage complex projects and people with efficiency, solve problems in uncertain situations, and help us scale fast. Over time, you\u2019ll also play a key role in building the most automated operations system of any data company, translating the workflows you run today into scalable processes and overseeing the internal systems we\u2019re developing.This is a founding role: your work will directly fuel the next generation of AI in a tangible way, while shaping the foundation of how Liva runs at scale.ABOUT THE ROLEWhat you\u2019ll do:WHAT WE\u2019RE LOOKING FORRequirements:Nice to have:\nBENEFI"
  },
  {
    "title": "Solveit \u2013 A course and platform for solving problems with code (answer.ai)",
    "points": 68,
    "submitter": "eries",
    "submit_time": "2025-10-02T21:21:49 1759440109",
    "num_comments": 68,
    "comments_url": "https://news.ycombinator.com/item?id=45455719",
    "comments": [
      "I had access to GitHub Copilot as a student in early 2022 while learning Haskell and immediately realised that it would hinder my learning if I didn't turn it off and implicitly follow this understand, plan, execute, reflect loop.AI products like Cursor have the notion of an 'autonomy slider' [1] that can fortunately be turned all the way down (disable Cursor Tab) but relying on this discipline seems fickle when with the right agentic loops [2] and context engineering, thousands of lines of code can be churned out with minimal supervision.I've considered always working on two projects over a long timespan, one with no AI assistance, possibly in a separate IDE like Zed, and one in Vibe Kanban (my current daily driver) but this feels like an inefficient proxy to accelerating this four step learning loop with a tool like solveit.Since the solveit product isn't released and seemingly isn't competing with solutions, is there an opportunity to convey how AI product developers should be thinking about amplifying their users and keeping them in the learning loop?So far, I've seen Claude Code's Learning output style [3], and also ChatGPT's study mode but in these cases, the only product change is a prompt and solveit is more than that.[1] https://www.latent.space/i/166191505/part-a-autonomy-sliders\n[2] https://simonwillison.net/2025/Sep/30/designing-agentic-loop...\n[3] https://docs.claude.com/en/docs/claude-code/output-styles#bu...reply",
      "I thought it might be helpful to post a link to one of my favorite writeups from the beta cohort for solveit (last year). It's written by Chris Thomas:https://christhomas.co.uk/blog/2025/09/24/the-human-is-the-a...a few quotes/excerpts:  Being among the first 1000 people to experience SolveIt has felt like witnessing the early days of a significant shift in how we work with AI. As someone who is a seasoned programmer, I have seen many programming paradigms and the advance of AI coding tools. What makes SolveIt different is not just another tool or framework - it is a fundamental rethinking of the human-AI relationship.\n\n  As I look at my experience with SolveIt, I think this is a better more sustainable approach to AI-assisted development. The current trend of ever more powerful models generating ever larger blocks of code feels unsustainable. SolveIt offers a different path. By maintaining human agency, working in comprehensible increments and building genuine understanding at each step, it creates a positive feedback loop where both human and AI capabilities grow stronger over time. This represents a partnership model that builds competence over time rather than creating dependence.\n  \n  The implications extend far beyond programming. Whether I am implementing computer vision algorithms, exploring culinary science, or writing technical articles - the same principles apply. Small steps, continuous understanding, iterative refinement and always keeping the human as the agent in the process.reply",
      "Hey everyone, Eric Ries here. solveit is the AI environment I personally have been using every day for months, not just for code but for writing and research, too.it\u2019s solved all the problems and frustrations I\u2019ve had with both vibecoding and the limitations of the chatbot interface for doing deep work that requires concentration + the ability to understand the artifacts you are producingand, as a special bonus, people in this course will get a sneak preview of the new book I\u2019m working on. we\u2019re going to use it both to teach some of the concepts from it (on how to create mission-driven long-term companies) and how to use solveit for longform writing projectshappy to answer any questions here, for folks that want to learn more,Ericreply",
      "(For those that don't recognise the name, Eric is the creator of The Lean Startup, and also founded the Long Term Stock Exchange. He's the co-founder of Answer.AI, which has built the solveit platform, and which fast.ai is not part of.)reply",
      "do you have a video of someone using it?reply",
      "And this prompted me to record a video showing some of my random non-work usage recently, to give a feeling for what the app looks like :) https://youtu.be/Y2B27hdKMMAreply",
      "Here is a video showing off using solveit for creating a web app. https://youtu.be/DgPr3HVp0eg?t=3120 To reiterate other comments, this is more about the methodology than the tool, but it is fun to see the tool in action too.reply",
      "Thanks Erik - I've added that video to the article now.! :)reply",
      "We also showed it as part of Hamel's course: https://x.com/HamelHusain/status/1956514524628127875 (https://www.youtube.com/watch?v=DgPr3HVp0eg) which is a longer example of the tool in actionreply",
      "Latent Space just released an interview about Solveit:\nhttps://www.youtube.com/watch?v=01ybLOH1fnUreply"
    ],
    "link": "https://www.answer.ai/posts/2025-10-01-solveit-full.html",
    "first_paragraph": "Johno Whitaker October 2, 2025tldr from Jeremy: \u201cHow to Solve it With Code\u201d is a course in iterative problem solving, and a platform (\u2018Solveit\u2019) to make that easier. The course shows how to use AI in small doses to help learn as you build, but doesn\u2019t rely on AI at all \u2013 you can totally avoid AI if you prefer. The approach we teach is based on decades of research and practice from Eric Ries and I, the founders of Answer.AI. It\u2019s basically the opposite of \u201cvibe coding\u201d; it\u2019s all about small steps, deep understanding, and deep reflection. We wrote the platform because we didn\u2019t find anything else sufficient for doing work the \u201csolveit way\u201d, so we made something for ourselves, and then decided to make it available more widely. You can follow the approach without using our platform, although it won\u2019t be as smooth an experience.The platform combines elements of all these: ChatGPT; Jupyter Notebook + nbdev; Bits of vscode/cursor (we embed the same Monaco editor and add similar optional AI an"
  }
]