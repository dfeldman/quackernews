[
  {
    "title": "Bending Spacetime in the Basement (fourmilab.ch)",
    "points": 13,
    "submitter": "wizardforhire",
    "submit_time": "2025-03-11T00:11:33 1741651893",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.fourmilab.ch/gravitation/foobar/",
    "first_paragraph": "\nOne of the things I detested about being a little kid was that every\ntime I thought of something really cool to do, I was invariably\nthwarted by my little brother shouting, \u201cMom!  Kelvin's\nmixing rocket fuel in the bathtub again!\u201d or\n\u201cMom!  Kelvin's making a submarine out of the old\nrefrigerator!\u201d.  Well, middle age has its drawbacks, but at\nleast you can undertake a project like this without fear of getting\nnipped in the bud at the cry, \u201cMom! Kelvin's down in the\nbasement bending spacetime!\u201d.  It's important to recall the\ndistinction between \u201cgrownup\u201d and \u201cgrown up\u201d.\nLet's us grownups head for the basement to bend some serious\nspacetime.\n\nApart from rare and generally regrettable moments of free-fall, we\nspend our entire lives under the influence of the Earth's\ngravity, yet rarely, if ever, do we experience the\nuniversal nature of gravitation.  It's a tremendous\nphilosophical leap from \u201cstuff falls\u201d to \u201ceverything in the universe\nattracts everything else\u201d.  That leap, made by Isaac N"
  },
  {
    "title": "What made the Irish famine so deadly (newyorker.com)",
    "points": 113,
    "submitter": "pepys",
    "submit_time": "2025-03-10T21:24:06 1741641846",
    "num_comments": 109,
    "comments_url": "https://news.ycombinator.com/item?id=43326275",
    "comments": [
      "A crazy fact is that a higher percentage of Irish died in the Great Famine (well over 10% of the population) than in the Bengal famine in India in 1943 (about 3.5%).This is a fascinating point:> In 1837, two years after Alexis de Tocqueville published the first volume of \u201cDemocracy in America,\u201d his lifelong collaborator, Gustave de Beaumont, went to Ireland, a country the two men had previously visited together. The book de Beaumont produced in 1839, \u201cL\u2019Irlande: Sociale, Politique et Religieuse,\u201d was a grim companion piece to his friend\u2019s largely optimistic vision of the future that was taking shape on the far side of the Atlantic. De Beaumont, a grandson by marriage of the Marquis de Lafayette, understood that, while the United States his ancestor had helped to create was a vigorous outgrowth of the British political traditions he and de Tocqueville so admired, Ireland was their poisoned fruit. America, he wrote, was \u201cthe land where destitution is the exception,\u201d Ireland \u201cthe country where misery is the common rule.\u201d\n \nreply",
      "Maybe a closer comparison would be famine of 1876-8, where some estimate go as high as 8.6M fatalities on a population of ~58M.>  In its first full year, 1846, Robert Peel\u2019s Conservative government imported huge quantities of corn, known in Europe as maize, from America to feed the starving. The government insisted that the corn be sold rather than given away (free food would merely reinforce Irish indolence)Compare this to the 1876 response in which \"relief work\" camps had workers doing strenuous labor in order to receive a meager ration of far fewer calories than would have been expended in the work.> ... this 'Temple wage' consisted of 450 grams (1 lb) of grain plus one anna for a man, and a slightly reduced amount for a woman or working child,[12] for a \"long day of hard labour without shade or rest.\"[13] The rationale behind the reduced wage, which was in keeping with a prevailing belief of the time, was that any excessive payment might create 'dependency' ...\n \nreply",
      ">which was in keeping with a prevailing belief of the time, was that any excessive payment might create 'dependency' ...Now where have I heard that recently\n \nreply",
      "Indeed, and ironically, from people who have largely inherited their wealth instead of working for it.\n \nreply",
      "\"No, my father only owned a _share_ in an emerald mine using slave labor under apartheid which is why I'm a self made man\" - Elon Musk in his own mind",
      "From people who want to take us back to feudalism.\n \nreply",
      "The generic term is: perverse incentive.\n \nreply",
      "If only the Irish had been supported and Indian left to fiend for themselves.\n \nreply",
      "Ireland produced plenty of good. The Irish could have supported themselves without the British demanding the Irish export food instead of eating it.\n \nreply",
      "Ireland being left to fend for themselves, without exporting to Britain, would\u2019ve also solved the problem.\n \nreply"
    ],
    "link": "https://www.newyorker.com/magazine/2025/03/17/rot-padraic-x-scanlan-book-review",
    "first_paragraph": "In the first act of the wittiest Irish play of the nineteenth century, Oscar Wilde\u2019s \u201cImportance of Being Earnest,\u201d there is much ado about a shortage of food. The fearsome Aunt Augusta is coming to tea, but we have watched the feckless Algernon eat all the cucumber sandwiches prepared for her by his manservant, Lane. The servant saves the day when the aunt arrives, expecting her sandwiches, by lying: \u201cThere were no cucumbers in the market this morning, sir. I went down twice.\u201d Algy responds with high emotion: \u201cI am greatly distressed, Aunt Augusta, about there being no cucumbers, not even for ready money.\u201dThe play, first performed in 1895, is subtitled \u201cA Trivial Comedy for Serious People,\u201d and this scene is an exquisite exercise in trivialization. Wilde is imagining what a food crisis might look like if it were happening among the English upper classes rather than in his home country. The panic and dread of searching for nourishment and finding none is transformed into an airy nothin"
  },
  {
    "title": "Software-Defined Radio for Engineers (2018) [pdf] (analog.com)",
    "points": 191,
    "submitter": "Tomte",
    "submit_time": "2025-03-10T17:40:40 1741628440",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=43323071",
    "comments": [
      "It's really easy to forget how complex SDR is, and how much expertise it really requires to fully understand if you need to write receivers/processors/filters yourself. There's a TON of great stuff out there for hobbyists that just want to get some cool stuff working though. I tried my hand decoding a soil temp/humidity sensor broadcast and was quickly reminded how complicated it can be for an initiate.I know almost nothing about actual SDR, but I've got a cheap SDR receiver/antenna inside a glass basement door that receives temp/humidity data from ~10 433Mhz transmitters in various rooms in my house + outbuildings, and a weather station that reports temp/humidity/lignthing strikes/rain amount/wind speed+direction, and lux. All that goes to an influx DB instance, and has a set of graphana dashboards built on top of it.  Took me a couple evenings to get set up, and now I've got real time + historical environment data about everything I care about at my house; including high humidity alerts in rooms with dehumidifiers, freeze warnings for a crawl space, and a bunch of other stuff. It has been wildly reliable.\n \nreply",
      "I think as a counter point, Michael Ossmann, the creator of HackRF One has worked really hard to make it a lot more accessible.In one of my favorite YT videos ever he talk about how he design RF PCB. The point of it is kind of like, the field of RF and SDR is incredibly deep and complicated, but at the same time, most filters/mixers/amplifiers and everything else needed for making an SDR are ICs that one can buy from Digikey and put them together (or likely have assembled).So, not denying how deep SDR can be, but it can also be accessible with enough effort!https://www.youtube.com/watch?v=TnRn3Kn_aXg\n \nreply",
      "If you are serious about getting into SDR, having background in DSP can take you very far. Most of the painful SDR dragons are based in stuff that was optional at university time, but is otherwise not that hard to pick up once you know it exists.Understanding the frequency domain, why it's useful, and how to get there & back, is like 80% of the puzzle.\n \nreply",
      "What do you wish you knew about dsp and frequency back then that you know mow?\n \nreply",
      "Ubiquity of the concepts. I'd have paid more attention if I had known. Things like aliasing, kernels and minimum viable sample rate show up pretty much everywhere.\n \nreply",
      "Getting some hands on experience with the signal processing side of SDR, with GNU Radio can help with understanding things like negative frequencies, complex signals, etc.It's open source, and you can just play with your audio ports for starters. Later adding a $40 rtlSDR kit goes a long way. I used mine to build a VOR receiver.\n \nreply",
      "As a pilot with an amateur radio license (though an inexperienced ham) this sounds really interesting. Could you elaborate more on building the VOR receiver?\n \nreply",
      "The transmitter has an electronically switched antenna that virtually moves in a circle at 3600 rpm, causing FM modulation phased with direction. The carrier is also AM modulated with a reference phase signal.I built a gnu radio flowgraph to receive both and display heading to the VOR.\n \nreply",
      "Woah coming from a dsp / synth background that\u2019s wild to hear.\n \nreply",
      "I've played around with Gnu Radio, and even though I've been able to do some very sophisticated things with GRC, I also realize just how little I really know. Part of that is that the math makes my head spin. I'd be lost without those blocks doing a lot of the work for me.\n \nreply"
    ],
    "link": "https://www.analog.com/media/en/training-seminars/design-handbooks/Software-Defined-Radio-for-Engineers-2018/SDR4Engineers.pdf",
    "first_paragraph": ""
  },
  {
    "title": "The shrouded sinister history of the bulldozer (noemamag.com)",
    "points": 57,
    "submitter": "g8oz",
    "submit_time": "2025-03-10T22:45:31 1741646731",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=43326982",
    "comments": [
      "The article has a pull quote: \u201cBulldozers were as important to the Allied victory as the jet engine, the radar or the atomic bomb.\u201dThat's a strange statement since the jet engine had approximately zero impact on the Allied victory; the P-80 Shooting Star was produced too late to be useful.https://en.wikipedia.org/wiki/List_of_jet_aircraft_of_World_...\n \nreply",
      "The P-80 might not have seen any action but the Gloster Meteor was introduced into service in July 1944 and saw a fair bit of combat through to the end of the war in Europe. Not a massive impact and wasn't really that important in the grand scheme of things, but it's still an allied jet powered plane that made some contribution to the war effort.\n \nreply",
      "great catch - deff seems inaccurate of a statement (more like lack of jet engine used against)tangentially, had to dig up https://news.ycombinator.com/item?id=37945006 as it triggered the ol brain\n \nreply",
      "It says \"aircraft engine\" for me\n \nreply",
      "You're both right, it says aircraft engine in the article body but jet engine in the pull quote. They caught the error but forgot to update the latter.\n \nreply",
      "Bulldozers can be used to grade surfaces prior to buildings being constructed... Buildings in which crimes are committed, from financial fraud to outright murder.\n \nreply",
      "Bulldozers themselves have been used to murder. There were examples given in the article? Not sure the point of this comment?\n \nreply",
      "> Bulldozers themselves have been used to murder. There were examples given in the article?https://www.business-humanrights.org/en/latest-news/israelpa...\n \nreply",
      "killdozer? although \"No one else was injured or killed,[1] in part due to timely evacuation orders\"https://en.wikipedia.org/wiki/Marvin_Heemeyer\n \nreply",
      "Here, I got another: https://archive.seattletimes.com/archive/19910912/1305069/ir...\n \nreply"
    ],
    "link": "https://www.noemamag.com/the-shrouded-sinister-history-of-the-bulldozer/",
    "first_paragraph": ""
  },
  {
    "title": "You suck at CSS and that's okay (2022) (rexriepe.com)",
    "points": 25,
    "submitter": "kyleyeats",
    "submit_time": "2025-03-10T23:03:20 1741647800",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=43327155",
    "comments": [
      "And the fix is another framework, because of course it's another framework. Frameworks are to CSS what greige paint is to a house. Trying to implement the missing 5% of a CSS framework without knowing how to implement \"complex\" CSS from the ground up means you are never implementing the missing 5%. Your site or app or whatever is just going to look like every other site that uses that framework.Now that CSS is more or less feature complete and the fact that there's just one web browser means you don't need the clever tricks rolled up in a framework to center a div or to have a grid layout that works without resorting to tables. It's literally part of the CSS spec and has been implemented in every browser for a decade now.\n \nreply",
      "The argument I make in the book is that the last 5% of CSS/design should be written by people who can write CSS. Nobody else should even be writing CSS because it turns into a huge mess when everyone jumps in.I mothballed this project because people were so incredibly cruel about it (a CSS project!). Remember that people who work on this stuff are people, and we're just trying to make things better. Also, you can pry .vertical-center from my cold, dead hands.\n \nreply",
      "My view is that those who designed CSS suck when it comes to designing intuitive systems.I would not advocate constraint solvers, in the future I hope ViTs are so cheap to run that they can infer the right layout of things at any orientation and size in single digit milliseconds, solving the layout problem for good =)\n \nreply",
      "Organic growth leads to vestigial warts - that is the price of success and development. I'm really impressed with so much of CSS and what can be achieved with what it has become.> My view is that those who designed CSS suckI think that is a stink attitude because there is no need to malign people. Don't be a dick. Most successful things \"suck\" because it is easy to feel the compromises when you use something. It is incredible hard to see the warts before we build, and even harder to find consensus solutions. All too often I hear whingers, that lack the ability to deliver working solutions, who are often unrealistically idealistic and too quick to poopoo the work of others. They are the people with second-system syndrome - who often deliver a version 2.0 using technology B and get an outcome all too often far worse than v1.0.\n \nreply",
      "> Don't be a dick.Says the one who is being a royal dick (a short thin one, though)\n \nreply",
      "I detest writing CSS and HTML. I just find it boring fiddly and annoying. I have started doing \"vibe\" coding with LLM's. Giving a decent prompt produces results that are... pretty good.Almost 100% in lighthouse for both mobile and desktop, responsive, reusable components, dark/light mode and a design that was better than I could do in the 2-3 hours I spent doing it (while sipping wine).I know its not a solution for everyone, and probably won't work for the prettier designs out there, but you can go a long way with these tools this day.I know there is a reluctance to not use LLM's for code tasks, and I am one of the largest critics, but for me this solves a real pain point. I don't want to write CSS/HTML anymore and these tools do a good enough job of it that I don't have to.\n \nreply",
      "LLMs are great for building frontends for backend projects and backends for frontend projects\n \nreply",
      "For CRUD I agree. A lot of what I am doing is a bit more complex then that.I actually would be happy to just \"vibe\" code my way through most of the problems I deal with if LLM's were able to do it.That said, they make a great intern or jnr developer you can hand tasks off. You have to review either way, but the LLM does it faster.\n \nreply",
      "I agree. CASS (the library this book was promoting) is actually really great paired with LLMs. If I revisit this project, it'll be along the lines of using it with LLMs.\n \nreply",
      "Tell that to a decent number of people I have worked with over the past 20 years. In fact, I know people to this day who have been writing CSS for decades who don't really know fundamental things like specificity, inheritance, or the cascade. Pattern matching into the sunset, not a single fundamental piece of CSS specifications ever internalized, still getting paid (its okay).https://www.w3.org/Style/CSS/specs.en.htmlYou can read this and know most things about CSS. For a bit anyway. You'll forget things you don't use. You might remember them again one day.\n \nreply"
    ],
    "link": "https://rexriepe.com/yousuckatcss/",
    "first_paragraph": "The official book of California\u00a0Stylesheets(and that's okay)It's a book about writing CSS.And not writing CSS.You suck at......focus...standards...color...naming things...documentation...typography...cargo cults...design...working...architecture...Photoshop...HTML...Pshop-to-HTML...workflows...teamwork...tech debt...ugly...accountability...freak accidents...trends...marketing...accessibility...meetings...user interaction...tracking...estimating...\u201cYou suck at\u201dby Rex RiepeThis book is about writing CSS. It's also about not writing CSS. It's aimed at people who want to learn CSS, people who want to avoid CSS, amateurs, professionals and everyone in-between. The book aims to provide context as to why CSS and the design ecosystem surrounding it are the way they are. It's written to support the California Stylesheets CSS framework. It's all about getting things done fast on the frontend using web technology, with a focus on maintaining that speed in teams.First Chapter: Focus\u00a0\u00bbPay what you "
  },
  {
    "title": "Mathematical Foundations of Reinforcement Learning (github.com/mathfoundationrl)",
    "points": 180,
    "submitter": "ibobev",
    "submit_time": "2025-03-10T18:27:32 1741631252",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=43323946",
    "comments": [
      "6-lecture series on the Foundations of Deep RL by Pieter Abbeel is also very recommended. gives very good overview and intuition\nhttps://youtu.be/2GwBez0D20A\n \nreply",
      "Highly recommended .. even the main contents diagram is a great visual overview of RL in general, as is the 30 minute intro YT video.Im expecting to see a lot of hyper growth startups using RL to solve a realworld problem in engineering / logistics / medicineLLMs currently attract all the hype for good reasons, but Im surprised VCs dont seem to be looking at RL companies specifically.\n \nreply",
      "Another great resource on RL is Mykel Kochenderfer's suite of textbooks: \nhttps://algorithmsbook.com/\n \nreply",
      "These books are all RL? I\u2019ve got the decision one, I didn\u2019t think the other had anything to do with RL.\n \nreply",
      "He (author) has a strong proclivity for policy-based planning, shall we say.\n \nreply",
      "I don't know how to go from understanding this material to having a job in the field. Just stuck as a SWE for now.\n \nreply",
      "- Do you understand the material?\n  - Can you utilize your understanding to build successful models/algorithms? \n\nIf the answer is yes to both, do some projects, put them on your github, and update your resume. You might need to take a job at a lower position first, but you can jump from there. But I want to make sure that the answer is \"yes\" to both and note that it is easy to think you understand something without actually understanding it. Importantly we must recognize that everyone has a different level of sufficient knowledge where they are comfortable saying that they \"understand\" a topic. One person might say they don't and be more knowledgeable than someone that says they do. But demonstration of the knowledge levels is at least a decent proxy for determining this.A way I like to gauge someone's understandings of things is by getting them to explain the limitations. This is often less explicitly stated in learning and a deeper understanding is acquired through experience and most importantly, reflection on that experience. This is often an underutilized tactic but it is very effective. If you can't do this, then the good news is that starting now will only accelerate your understanding :)\n \nreply",
      "Just a random thought:Understanding the limitations is a complicated thing in tech. You can finnangle most systems into doing mostly anything, as inefficient as that may prove to be.The question then becomes up to what point is it a reasonably better than most other solutions. And that's a question of an understanding of a field, not a space in the field.\n \nreply",
      "Awesome resource, in case someone is interested I implemented most of suttons book here https://github.com/ivanbelenky/RL\n \nreply",
      "Also worth mentioning Murphy's WIP textbook[0] focused entirely on RL, which is an outgrowth of his excellent ML textbooks.[0]: https://arxiv.org/abs/2412.05265\n \nreply"
    ],
    "link": "https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        This is the homepage of a new book entitled \"Mathematical Foundations of Reinforcement Learning.\"\n      This textbook has received 5,000+ stars! Glad that it is helpful to many readers.My English open course is online now. You can click the above figure or the link here to jump to our YouTube channel. You can also click the following links to be directed to specific lecture videos.You are warmly welcome to check out the English videos to see if they are helpful!This book aims to provide a mathematical but friendly introduction to the fundamental concepts, basic problems, and classic algorithms in reinforcement learning. Some essential features of this book are highlighted as follows.The book introduces reinforcement learning from a mathematical point of view. Hopefully, readers will not only know the procedure of an algorithm but al"
  },
  {
    "title": "Canon EF and RF Lenses \u2013 All Autofocus Motors (exclusivearchitecture.com)",
    "points": 388,
    "submitter": "ExAr",
    "submit_time": "2025-03-10T13:12:38 1741612358",
    "num_comments": 95,
    "comments_url": "https://news.ycombinator.com/item?id=43320230",
    "comments": [
      "I used to have to manually focus to take photos of my dog. The apparent noise from the early Ring USM AF in the 85mm/1.8 lens drove him mad and he would run out of the room.We did initially wonder if it was some psychological effect of pointing a camera at him, but one night I hid behind a curtain and used the AF motor and he still shot out of the room.Subsequent USM AF motors were silent to him and he was then content to have his photo taken.\n \nreply",
      "Such a cool website.In case anyone else was looking for it, this article from the same author covers the more algorithm-y question of how a camera body decides to actually use these motors when you press the \"focus\" button: https://exclusivearchitecture.com/03-technical-articles-DSLR...I can't find any articles on how a camera body decides what is and isn't a desired subject. I'm guessing there's some amount of machine learning-type stuff involved in that, seeing as how they can detect human (and bird?) faces?\n \nreply",
      "Nowadays they use pre-trained pattern recognition AI models, yes, which has become much more impressive (and CPU-intensive) with mirrorless cameras where the entire resolution of the main image sensor is available for analyzing the scene. Some higher-end traditional DSLRs have a \"high\"-resolution (around 0.1 MPix or so) metering sensor that is used to assist the AF system (eg. what Canon calls iSA and iTR [1]).Traditionally, cameras would just focus using the single focus point the photographer has selected, or if they have selected a larger area focusing mode, the camera would typically pick the closest point of a group of points, assuming that that's usually what the photographer is interested in. (Remember that traditional (D)SLRs have a discrete AF sensor with at most a few dozen focusing points to choose from!)In tracking AF modes (eg. Canon's Servo AF), depending on settings, the camera tries to avoid sudden shifts in focus even if a foreground object momentarily occludes the original target. Tracking AF also has to predict the subject's motion to prevent the focus from lagging behind a fast-moving subject. Higher-end cameras allow configuring the AF behavior in terms of how reactive vs \"sticky\" it should be when tracking a subject, and how linear the subject's motion is expected to be.[1] https://www.canon.com.hk/cpx/en/technical/pa_Overview_of_65-...\n \nreply",
      "With mirrorless cameras the focus switched from specialized sensors to on-CMOS contiuous exposure sensors, so movement is easy to detect. At this point the cameras have specialized AI hardware to run the models, and they also accept user input (on R5 MkII you can register up to ten people to prioritize focus on[1]). The focusing options are now very complex[2][3], and combined with lots of customization options on the camera's buttons you can have very specialized/personalized setups for different types of photography.[1] https://cam.start.canon/en/C017/manual/html/UG-04_AF-Drive_0...\n[2] https://cam.start.canon/en/C017/manual/html/UG-04_AF-Drive_0...\n[3] https://cam.start.canon/en/C017/manual/html/UG-04_AF-Drive_0...\n \nreply",
      "Sure, as I said in the first paragraph, AF is these days very impressive thanks to the large amount of data available (but of course this would have been too much data back in the day, when there wasn't nearly enough CPU power to process it fast enough). I wanted to give more historical context for how AF worked before fancy AI.The AF settings, except those related to face/object recognition, haven't actually changed that much since the 7D Mk II days. The preset system is more general now and allows you to store and recall all AF settings rather than just the three tracking-related variables. The high-end DSLRs used to have six cases for different types of sports that you could modify but not rename.\n \nreply",
      "Sony A9 III even has a configuration setting of whether it should focus on the left or right eye of the person :) It also can remember faces and prioritize them if there other faces. Let's say someone shoots their kid on the football field and wants only them to be in focus.\n \nreply",
      "The left eye/right eye option has been around even on lower end cameras for the better part of a decade, which is kinda wild. My fuji from 2017 has that, and Fuji are specifically known for having worse autofocus performance than Canon or Sony or Nikon. Nikon and Sony specifically seem to be top of the pile right now.The Z8 has a whole separate processor dedicated to autofocus and the viewfinder which, in practice, means it can shoot 20 FPS full quality 45 megapixel RAW files with continuous 120fps autofocus without blacking out the viewfinder for each shot, which is absolutely insane.\n \nreply",
      "The technical term for the \"stickiness\" you're referring to, where a system is resistant to change or has a memory, is hysteresis.\n \nreply",
      "Similarly, promo materials for the Elan 7 talked about how the camera was able to determine exposure based on a database of hundreds of photos.I have no idea how this worked, but would have loved to see the photos they used for \u201ctraining\u201d this system 25 years ago.\n \nreply",
      "As a Canon user since the mid-80's, I found this fascinating reading.Edit: Wow - there's a whole collection of Canon lens technology articles there: https://exclusivearchitecture.com/03-technical-articles-CLT-...\n \nreply"
    ],
    "link": "https://exclusivearchitecture.com/03-technical-articles-CLT-12-autofocus-systems.html",
    "first_paragraph": "The introduction of automatically focusing lenses has changed photography in numerous ways. Autofocus systems have significantly improved the reliability and accuracy with which fast moving objects can be shot. Round four decades ago, the new level of convenience offered by autofocus lenses has made photography more accessible to the masses. Since their first release of an autofocus lens in 1981, Canon has constantly developed their lens technology using different approaches. Over time, Canon has introduced seven different autofocus motor technologies that will be presented here.In optics, focus describes the point at which incoming light rays converge and it is used synonymously as a concept of sharpness. Autofocus, by contrast, is a technology to automatically focus the lens at a desired subject. The autofocus detection system inside the camera determines the ideal focusing lens position and instructs the lenses autofocus drive accordingly. In 1987, Canon has introduced the both the "
  },
  {
    "title": "Performance of the Python 3.14 tail-call interpreter (nelhage.com)",
    "points": 508,
    "submitter": "signa11",
    "submit_time": "2025-03-10T06:44:27 1741589067",
    "num_comments": 144,
    "comments_url": "https://news.ycombinator.com/item?id=43317592",
    "comments": [
      "Hello. I'm the author of the PR that landed the tail-calling interpreter in CPython.First, I want to say thank you to Nelson for spending almost a month to get to the root of this issue.Secondly, I want to say I'm extremely embarrassed and sorry that I made such a huge oversight. I, and probably the rest of the CPython team did not expect the compiler we were using for the baseline to have that bug.I posted an apology blog post here. https://fidget-spinner.github.io/posts/apology-tail-call.htm...\n \nreply",
      "Reading that you are extremely embarrassed and sorry that you made such a huge oversight, I was imagining you had broken something / worsened CPython's performance.But it's nothing like this. You announced a 10-15% perf improvement but that improvement is more like 1-5% on a non buggy compiler. It's not even like that 10-15% figure is wrong, it's just that it's correct only under very specific conditions, unknowingly to you.IIUC, you did your homework: you made an improvement, you measured a 10-15% perf improvement, the PR was reviewed by other people, etc. It just so happens that this 10-15% figure is misleading because of an issue with the version of clang you happened to use to measure. Unless I'm missing something, it looks like a fair mistake anyone could have reasonably made. It even looks like it was hard to not fall into this trap. You could have been more suspicious seeing such a high number, but hindsight is 20/20.Apparently, you still brought significant performance improvements, your work also helped uncover a compiler regression. The wrong number seems quite minor in comparison. I wonder who was actually hurt by this. I only discover the \"case\" right now but at a first glance it doesn't feel like you owe an apology to anyone. Kudos for all this!\n \nreply",
      "> IIUC, you did your homework: you made an improvement, you measured a 10-15% perf improvement, the PR was reviewed by other people, etc. It just so happens that this 10-15% figure is misleading because of an issue with the version of clang you happened to use to measure. Unless I'm missing something, it looks like a fair mistake anyone could have reasonably made. It even looks like it was hard to not fall into this trap. You could have been more suspicious seeing such a high number, but hindsight is 20/20.Hah! Is this a Gettier problem [0]?1. True: The PR improves Python performance 15-20%.\n2. True: Ken believes that the PR improves Python performance 15-20%.\n3. True: Ken is justified in believing that the PR improves Python performance 15-20%.Of course, PR discussions don't generally revolve around whether or not the PR author \"knows\" that the PR does what they claim it does. Still: these sorts of epistemological brain teasers seem to come up in the performance measurement field distressingly often. I wholeheartedly agree that Ken deserves all the kudos he has received; still, I also wonder if some of the strategies used to resolve the Gettier problem might be useful for code reviewers to center themselves every once in a while. Murphy's Law and all that.[0]: https://en.wikipedia.org/wiki/Gettier_problem\n \nreply",
      "In some way, by indirectly helping fix this bug, they led to a ~10% performance increase for everyone who was using that faulty compiler! That's even better than an optional flag that many people won't know about or use.\n \nreply",
      "That performance regression only hit code that was using a very large number of paths with the same table of computed gotos at the end. That's likely to only be relatively complex interpreters that were affected. So it's not a broad performance improvement. But it is nice to have an example of the compiler's new heuristic failing to prove evidence it needs to be tunable.\n \nreply",
      "Well, that includes at least everyone using Python built with that compiler.\n \nreply",
      "FWIW - the fix was merged since you wrote that blog post ;)Beyond that - 3-5% is a lot for something as old as the python interpreter if it holds up.  I would still be highly proud of that.After 30 years, i've learned (like i expect you have) to be suspicious of any significant (IE >1%) performance improvement in a system that has existed a long time.They happen for sure, but are less common.  Often, people are shifting time around, and so it just isn't part of your benchmark anymore[1].  Secondarily, benchmarking is often done in controlled environments, to try to isolate the effect.  Which seems like the right thing to do. But then the software\nis run in non-isolated environments (IE with a million other things on a VM or desktop computer), which isn't what you benchmarked it in.  I've watched plenty of verifiably huge isolated  gains disappear or go negative when put in production environments.You have the particularly hard job that you have to target lots of environments - you can't even do something like say \"okay look if it doesn't actually speed it up in production, it didn't really speed it up\", because you have no single production target.\nThat's a really hard world to live in and try to improve.In the end, performance tuning and measurement is really hard.  You have nothing to be sorry for, except learning that :)Don't let this cause you to be afraid to get it wrong - you will get it wrong anyway.  We all do.  Just do what you are doing here - say 'whoops, i think we screwed this up', try to figure out how to deal with it, and try to figure out how to avoid it in the future (if you can).[1] This is common not just in performance, but in almost anything, including human processes.  For example, to make something up, the team working on the code review tool would say \"we've reduced code review time by 15% and thus sped up everyone's workflow!\". Awesome!  But actually, it turns out they made more work for some other part of the system, so the workflow didn't get any faster, they just moved the 15% into a part of the world they weren't measuring :)\n \nreply",
      "> After 30 years, i've learned (like i expect you have) to be suspicious of any significant (IE >1%) performance improvement in a system that has existed a long time.Laughs in corporate code\n \nreply",
      "Sure, let me amend it to \"i'm suspicious of any significant performance improvement in as system where performance actually matters, and has existed in a state where performance matters for a long time\".\n \nreply",
      "That's a different case. Corporate code is never optimized for performance. Performance as a factor doesn't play any factor.\n \nreply"
    ],
    "link": "https://blog.nelhage.com/post/cpython-tail-call/",
    "first_paragraph": "\n\n      Mar  9, 2025\n    \nAbout a month ago, the CPython project merged a new implementation strategy for their bytecode interpreter. The initial headline results were very impressive, showing a 10-15% performance improvement on average across a wide range of benchmarks across a variety of platforms.Unfortunately, as I will document in this post, these impressive performance gains turned out to be primarily due to inadvertently working around a regression in LLVM 19. When benchmarked against a better baseline (such GCC, clang-18, or LLVM 19 with certain tuning flags), the performance gain drops to 1-5% or so depending on the exact setup.When the tail-call interpreter was announced, I was surprised and impressed by the performance improvements, but also confused: I\u2019m not an expert, but I\u2019m passingly-familiar with modern CPU hardware, compilers, and interpreter design, and I couldn\u2019t explain why this change would be so effective. I became curious \u2013 and perhaps slightly obsessed \u2013 and the"
  },
  {
    "title": "Arranging invisible icons in quadratic time (2021) (randomascii.wordpress.com)",
    "points": 20,
    "submitter": "fanf2",
    "submit_time": "2025-03-07T09:42:03 1741340523",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=43288691",
    "comments": [
      "A few days ago Teams got insanely slow when I was writing a comment on a Planner task. I ran a profiler and it seems like it was running some kind of sanitizer for every key press and that each call took over a second. They also seemed to get slower the more of them were queued up. Typing a sentence (blindly) would freeze the whole app for about five minutes before doing the first draw.What's up with Microsoft these days?\n \nreply",
      "They have a monopoly. The quality of their products has become irrelevant to the short-term well-being of their stock, which is all that matters in our hyper-financialized economy.\n \nreply",
      "People who only learn leetcode are designing everything, it seems\n \nreply",
      "(2021)Pretty sure I've seen this on HN at least once before.\n \nreply",
      "Probably this discussion in 2021? (376 points, 111 comments) https://news.ycombinator.com/item?id=26152335\n \nreply",
      "The article doesn't mention it, and a comment poses the same question, but does this still happen if you have \"Auto arrange icons\" and \"Align icons to grid\" disabled?\n \nreply"
    ],
    "link": "https://randomascii.wordpress.com/2021/02/16/arranging-invisible-icons-in-quadratic-time/",
    "first_paragraph": "Near the end of January I was pointed to a twitter thread where a Windows user with a powerful machine was hitting random hangs in explorer. Lots of unscientific theories were being proposed. I don\u2019t generally do random analysis of strangers\u2019 performance problems but the case sounded interesting so I thought I\u2019d take a look.Update: if you are running Windows 11 then this issue is now fixed for you, according to the status updates on this github issue. If you are running Windows 10 then\u2026 sorry, Microsoft has mostly abandoned that operating system.Freya shared an ETW trace of what was happening on her machine and I took a look using Windows Performance Analyzer (WPA). The first thing I noticed was that the UI Delays graph showed that, as promised, explorer.exe\u2019s thread 7,888 was failing to check for messages for 20.531 seconds. It was hung.Now, explorer.exe has many UI threads so it\u2019s not like the entire process was hung, but one of its windows was definitely hung, it was causing hangs e"
  },
  {
    "title": "Show HN: In-Browser Graph RAG with Kuzu-WASM and WebLLM (kuzudb.com)",
    "points": 107,
    "submitter": "sdht0",
    "submit_time": "2025-03-10T15:12:57 1741619577",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=43321523",
    "comments": [
      "I absolutely love this. I make VR experiences that run on the ICP, which delivers wasm modules as smart contracts - I've been waiting for a combo of node-friendly, wasm deployable tools and webLLM. The ICP essentially facilitates self-hosting of data and provides consensus protocols for secure messaging and transactions.This will make it super easy for me to add LLM functionality to existing webxr spaces, and I'm excited to see how an intelligent avatar or convo between them will play out. This is, very likely, the thing that will make this possible :)If anyone wants to collab, or contribute in some way, I'm open to ideas and support. Search for 'exeud' to find more info\n \nreply",
      "Why the Blockchain there? I don't really see the value. But maybe I misunderstand. It's just that I tend to be pretty dismissive of products mentioning blockchain. Mostly from the time when this tech was severely overhyped. Like Metaverse after it and now of course AI. I do know there's some usecases for it, I just wonder what they are and why you chose it.I think I like the idea but I don't think I fully understand what it is that you're doing :) But I love everything VR.\n \nreply",
      "Take this with a grain of salt, as I run a startup in the industry.Blockchain has taken a weird path. It started with Bitcoin offering something genuinely new - a Byzantine fault-tolerant mechanism for decentralized value exchange without trusted intermediaries. But the industry has drifted toward \"web3\" hype where the technology often isn't necessary.Companies pick tech stacks for all sorts of reasons beyond technical merit - vendor relationships, development velocity, legacy system compatibility, and UX considerations all factor into these decisions.Truth is, most blockchain companies today are solving problems that could be handled just fine with traditional databases and APIs. The industry is shifting toward abstraction layers that hide the consensus mechanisms anyway, focusing on user experience instead.The project mentioned probably doesn't actually need a blockchain backend for what it's doing, except maybe for tradable collectibles on an ERC standard.\n \nreply",
      "It is wise to be suspicious - spending even a small amount of time near the \"web3\" space will make attentive person suspicious of scams and parlour tricks.I use the network to host the webxr experiences, which are bundled wasm files, from unity. All of the code lives on the blockchain so, in this sense, i really couldn't do without it.If you are referring to blockchain-specific functionality, then this is largely true, however i have implemented some demostrations of the consensus mechanism being used from inside an immersive space. This is really to illustrate that it is possible, rather than try to sell you a meme coin.In the near future I expect to be using the ICP for a lot more, however, since it provides some rather interesting technical opportunities. It is wrapped around something called a 'network nervous system' which acts as a sort of administrator for proposals, so your point about abstraction is accurate, but this is the case with any large vendor.I choose to build on the ICP because it is more secure and straightforward for my particular stack, plus it has a lot of potential, despite the noise. I've implemented webrtc messaging to keep the cost of normal multiplayer data transfer as low as possible, because consensus is expensive and the network runs on a pay-per-use compute model.I'm offering a new route, outside of big tech, if you don't consider Unity a menace, which i acknowledge that some do. I am taking an admittedly more radical stance, by including the hosting in this.\n \nreply",
      "Yes, the hype waves have been painful. I am still annoyed by the lack of good tools and content for VR lovers. I am personally trying to do something by offering a toolkit for building VR websites in unity.My website has a few examples on it, including domestic interior, bowling simulator. Recently I helped to make a VR museum, which will be there soon, and I'm working on a flying game with realistic aerodynamics.I've open-sourced both the toolkit and template for self-hosting, essentially providing a no-code route for creating interactive webxr spaces. Putting it on the IC means you can also self-host your creation and keep control of the data, code and costs of running it.You could equally just use the unity toolkit and host it elsewhere, but i wouldn't be able to provide the same level of support, if it broke in unexpected ways.What i like about the original post was the fully in-browser RAG and webLLM, as it looks compatible with the rest, so i could, for example, broadcast responses across webrtc data channels. So many options...\n \nreply",
      "The example is not ideal for showcasing a graph analytics database because they could have used a traditional relational database to answer the same query, Which of my contacts work at Google?\n \nreply",
      "Hi, I work at Kuzu and can offer my thoughts on this.You're making a fair observation here and it's true for any high level query language - SQL and Cypher and interchangeable unless the queries  are recursive, in which case Cypher's graph syntax (e.g., the Kleene star * or shortest paths) has several advantages. One could make the argument that Cypher is easier for LLMs to generate because the joins are less verbose (you simply express the join as a query pattern). This post is not necessarily about graph analytics. It's about demonstrating that it's very simple to develop a relatively complex application using LLMs and a database fully in-browser, which can potentially open up new use cases. I'm sure many people will come up with other creative ways putting these fully in-browser technologies, both graph-specific, and not, e.g., using vector search-based retrieval. In fact, there are already some of our users doing this right now.\n \nreply",
      "This is really cool, but I'm super anxious about entering my personal data, especially LinkedIn connections.Is there some other demo you could do with public graph data? It'd be just as cool of a demo, but with less fear of information misuse.I'm even more anxious about leaking information about my professional connections as I am leaking my own data.\n \nreply",
      "Your concern makes sense, but in the demo we show, all your private data AND the graph database AND the LLM (basically, everything) is confined to your client session in the browser, and no data actually ever leaves your machine. That's the whole point of Wasm!The graph that you build is more for your own exploration and not for sharing with the outside world.\n \nreply",
      "Still, using non-personal example would mean the user wouldn't have to consider whether to trust you on that point (or do the analysis), and would make the technology demo friction-free.imo, privacy shouldn't be the driver but the kicker, because it's so inflammatory.\n \nreply"
    ],
    "link": "https://blog.kuzudb.com/post/kuzu-wasm-rag/",
    "first_paragraph": ""
  },
  {
    "title": "The Einstein AI Model (thomwolf.io)",
    "points": 122,
    "submitter": "9woc",
    "submit_time": "2025-03-08T14:14:22 1741443262",
    "num_comments": 103,
    "comments_url": "https://news.ycombinator.com/item?id=43300414",
    "comments": [
      "I've had some luck instructing AI to \"Don't make up anything. If there's no answer, say I don't know\".Which made me think that AI would be far more useful (for me?) if it was tuned to \"Dutchness\" rather than \"Americanness\".\"Dutch\" famously known for being brutally blunt, rude, honest, and pushing back.Yet we seem to have \"American\" AI, tuned to \"the customer is always right\", inventing stuff just to not let you down, always willing to help even if that makes things worse.Not \"critical thinking\" or \"revolutionary\" yet. Just less polite and less willing to always please you. In human interaction, the Dutch bluntness and honesty can be very off-putting, but It is quite efficient and effective. Two traits I very much prefer my software to have. I don't need my software to be polite or to not hurt my feelings. It's just a tool!\n \nreply",
      "I take my final thoughts out of the LLM and into two other new convos, I give both of them the same convo, but I ask one to steel man and the other to straw man.. I find it's a decent way to look for nuances you're missing.\n \nreply",
      "Today I asked ChatGPT about an old game I was trying to remember the name of, which it immediately identified as Trespasser, an early 3D FPS in the Jurassic Park franchise. But then it got weird. After it identified the game, it started asking me questions like whether I had played the game when it came out (\"oh, awesome!\"), and whether I had managed to finish it or just played it to mess around with the physics engine (which was quite advanced for its time), and then it asked me about specific moments in the game like it was just another gamer bro who was sharing a common passion for video games. I don't know who wants this. It's not something that can even have a real personality, so layering on such a thick layer of friendliness feels wrong to me. I would prefer a \"robot\".\n \nreply",
      "You haven't considered the company offering this service could possibly want its users to be engaged, thus using the service more? I don't have a very hospitable opinion on any of these companies.You may think that kind of interaction is weird, but were in the thick of a loneliness epidemic, and its not a stretch to think some may actually wilfully socialise with an LLM.As an aside...my sister works in medicine, and her boss (specialist surgeon) finishes a $450 consultation which followed him telling my sister \"but deepseek says x,y,z...\"\n \nreply",
      "Your Plastic Pal Who's Fun To Be With\n \nreply",
      "No judgment, honestly wondering. If you didn't want that interaction, why'd you answer the follow up questions? Were you just curious?\n \nreply",
      "Sometimes when I ask ChatGPT and get a perfect answer, I am tempted to say thanks, even though it's not actually a person. So today, when it asked me if this was the right answer, I answered \"yes\", and that's how it got started. I didn't encourage it to be friendly. But yes, I was just curious.\n \nreply",
      "I think I\u2019d be tempted to respond \u201cI\u2019m just a large language model, so I don\u2019t know, but let me ask my little brother Claude.\u201d\n \nreply",
      "You shouldn\u2019t let them think they are a person.\n \nreply",
      "Sounds like the LLM in its own way honestly enjoyed everything in its training data relating to that game and wanted to vicariously experience more about it from your feedback. :D\n \nreply"
    ],
    "link": "https://thomwolf.io/blog/scientific-ai.html",
    "first_paragraph": "\n\t\t\t\t\tI shared a controversial take the other day at an event and I decided to write it down in a longer format: I\u2019m afraid AI won't give us a \"compressed 21st century\".\n\t\t\t\t\n\t\t\t\t\tThe \"compressed 21st century\" comes from Dario's \"Machine of Loving Grace\" and if you haven\u2019t read it, you probably should, it\u2019s a noteworthy essay. In a nutshell the paper claims that, over a year or two, we\u2019ll have a \"country of Einsteins sitting in a data center\u201d, and it will result in a compressed 21st century during which all the scientific discoveries of the 21st century will happen in the span of only 5-10 years.\n\t\t\t\t\n\t\t\t\t\tI read this essay twice. The first time I was totally amazed: AI will change everything in science in 5 years, I thought! A few days later I came back to it and while re-reading I realized that much of it seemed like wishful thinking.\n\t\t\t\t  \n\t\t\t\t\tWhat we'll actually get, in my opinion, is \u201ca country of yes-men on servers\u201d (if we just continue on current trends) but let me explain the"
  },
  {
    "title": "HyperShell X Outdoor PowerSuit Exoskeleton (hypershell.tech)",
    "points": 12,
    "submitter": "danboarder",
    "submit_time": "2025-03-07T08:54:00 1741337640",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=43288410",
    "comments": [
      "That's pretty cool, I didn't know this type of stuff was available to consumers. The reviews on their site seem really enthusiastic, lots of older folks and mobility-restricted folks seem extremely pleased with the product and post action videos. Can't wait to see where this technology goes by the time I need it.\n \nreply",
      "'How do I update the software on my exoskeleton?'  is the fun line of the day I think; I bet that can be a pretty 'interesting' update.\nIt's a lot cheaper than I was expecting.\nAs someone getting achier as I age, I can see that type of thing might come in handy in a few more years.\n \nreply",
      "17.5 km range. So five miles out and five miles back. So like a three hour hike. Anyone should be able to do that without a friction-burn- causing Roboleg attachment.I guess if it meant I could bring my paraplegic neighbor hiking it would be cool.\n \nreply",
      "I don't think you have to be paraplegic to have a condition that prevents you from enjoying an invigorating hike. If this gets more people who aren't in perfect health out and exercising, what an incredible win.\n \nreply",
      "It's supposed to allow you to carry 30kg more than you could without it.   Not many people could carry somebody else for 10 miles, but a lot more could with 30kg of assistance.\n \nreply",
      "Imagine being able to get a 10 mile farther than you could without the roboleg.\n \nreply",
      "But then you\u2019d have to carry it back.\n \nreply",
      "Don\u2019t skip leg day\n \nreply",
      "Battery energy density has come a long way but I have a hard time believing a 400g battery can do much at 800w (even just peak) for long. Moving large masses like human bodies requires a lot of energy.The ergonomics of this design look much less likely to snag and get in the way than other powered exoskeletons I've seen though. Replace the electrical motor and battery with a chemical energy system (internal combustion, or even just hydrogen peroxide decomposition on silver catalyst) and this could go a long way (but with higher response latency, much higher noise, and increased control complexity).\n \nreply",
      "You're only shifting the problem somewhere else!Either your whole body gets capable of walking the distance, or you'll assist a part of it and wear out another part.\n \nreply"
    ],
    "link": "https://hypershell.tech/en-us",
    "first_paragraph": "Order now and get free shipping straight from our LA warehouse  Shop nowNext-GenExoskeleton40%Leg Strength Boosted30%Physical ExertionReductionStep into the future with Hypershell X, where robotics and wearable exoskeleton technology converge to transform every step you take.Break barriers, conquer terrains, and reconnect with the world on your terms.This isn't just a suit\u2014it\u2019s the power to rewrite your story.Step into the future with Hypershell X, where robotics and wearable exoskeleton technology converge to transform every step you take.Break barriers, conquer terrains, and reconnect with the world on your terms.This isn't just a suit\u2014it\u2019s the power to rewrite your story.Combining state-of-the-art robotics, ergonomics, and AI into a compact form, Hypershell X enables you to go further and explore more.The Hypershell X Powersuit packs extraordinary strength into a sleek, ultra-light design\u2014just 1.8kg\u00b2of pure innovation.Experience a new stride with the Hypershell e-assist system, deli"
  },
  {
    "title": "Zero-Downtime Kubernetes Deployments on AWS with EKS (glasskube.dev)",
    "points": 81,
    "submitter": "pmig",
    "submit_time": "2025-03-10T12:48:32 1741610912",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=43320024",
    "comments": [
      "We had to figure this out the hard way, and ended up with this approach (approximately).K8S provides two (well three, now) health checks.How this interacts with ALB is quite important.Liveness should always return 200 OK unless you have hit some fatal condition where your container considers itself dead and wants to be restarted.Readiness should only return 200 OK if you are ready to serve traffic.We configure the ALB to only point to the readiness check.So our application lifecycle looks like this:* Container starts* Application loads* Liveness begins serving 200* Some internal health checks run and set readiness state to True* Readiness checks now return 200* ALB checks begin passing and so pod is added to the target group* Pod starts getting traffic.time passes. Eventually for some reason the pod needs to shut down.* Kube calls the preStop hook* PreStop sends SIGUSR1 to app and waits for N seconds.* App handler for SIGUSR1 tells readiness hook to start failing.* ALB health checks begin failing, and no new requests should be sent.* ALB takes the pod out of the target group.* PreStop hook finishes waiting and returns* Kube sends SIGTERM* App wraps up any remaining in-flight requests and shuts down.This allows the app to do graceful shut down, and ensures the ALB doesn't send traffic to a pod that knows it is being shut down.Oh, and on the Readiness check - your app can use this to (temporarily) signal that it is too busy to serve more traffic. Handy as another signal you can monitor for scaling.e: Formatting was slightly broken.\n \nreply",
      "Racing against an ASG/ALB combo is always a horrifying adrenaline rush.\n \nreply",
      "A few years ago, while helping build a platform on Google Cloud & GKE for a client, we found the same issues.At that point we already had a CRD used by most of out tenant apps, which deployed an opinionated (but generally flexible enough) full app stack (Deployment, Service, PodMonitor, many sane defaults for affinity/anti-affinity, etc, lots of which configurable, and other things).Because we didn't have an opinion on what tenant apps would use in their containers, we needed a way to make the pre-stop sleep small but OS-agnostic.We ended up with a 1 LOC (plus headers) C app that compiled to a tiny static binary. This was put in a ConfigMap, which the controller mounted on the Pod, from where it could be executed natively.Perhaps not the most elegant solution, but a simple enough one that got the job done and was left alone with zero required maintenance for years - it might still be there to this day. It was quite fun to watch the reaction of new platform engineers the first time they'd come across it in the codebase. :D\n \nreply",
      "I know this won't be helpful to folks committed to EKS, but AWS ECS (i.e. running docker containers with AWS controlling) does a really great job on this, we've been running ECS for years (at multiple companies), and basically no hiccups.One of my former co-workers went to a K8S shop, and longs for the simplicity of ECS.No software is a panacea, but ECS seems to be one of those \"it just works\" technologies.\n \nreply",
      "I agree that ECS works great for stateless containerized workloads. But you will need other AWS-managed services for state (RDS), caching (ElastiCache), and queueing (SQS).So your application is now suddenly spread across multiple services, and you'll need an IaC tool like Terraform, etc.The beauty (and the main reason we use K8s) is that everything is inside our cluster. We use cloudnative-pg, Redis pods, and RabbitMQ if needed, so everything is maintained in a GitOps project, and we have no IaC management overhead.(We do manually provision S3 buckets for backups and object storage, though.)\n \nreply",
      "Mentioning \u201cno IaC management overhead\u201d is weird. If you\u2019re not using IaC, you\u2019re doing it wrong.However, GitOps is IaC, just by another name, so you actually do have IaC \u201coverhead\u201d.\n \nreply",
      "Many companies run k8s for compute and use rds/sqs/redis outside of it. For example RDS is not just hosted PG, it has a whole bunch of features that don\u2019t come out of the box (you do pay for it, I\u2019m not giving an opinion as to whether it\u2019s worth the price)\n \nreply",
      "Yea RDS makes your life easy, notifications and easy application of security patches both OS and DB level (minor version upgrades). Easy upgrade of major versions, easy upgrade of storage, RAM and compute (but not so easy to downgrade), easy options for replication, Blue/Green deployments etc to name a few.\n \nreply",
      "You make a great point that when everything is on kube it\u2019s easier to manage.But\u2026 if you are maintaining storage buckets and stuff elsewhere (to avoid accidental deletion etc, a worthy cause) then you are using terraform regardless. So adding RDS etc to the mix is not as tough as you make it sound.I see both sides of the fence and both have their pros and cons.If you have great operational experience with kube though I\u2019d go all in on that. AWS bends you over with management fees\u2026 it\u2019s far more affordable to run a DB, RMQ, etc on your own versus RDS, AMQ\n \nreply",
      "> One of my former co-workers went to a K8S shop, and longs for the simplicity of ECS.I was using K8s previously, and I\u2019m currently using ECS in my current team, and I hate it. I would _much _ rather have K8s back. The UX is all over the place, none of my normal tooling works, deployment configs are so much worse than the K8s equivalent.\n \nreply"
    ],
    "link": "https://glasskube.dev/blog/kubernetes-zero-downtime-deployments-aws-eks/",
    "first_paragraph": "Glasskube is specialized in secure software distribution. Our Open Source Software Distribution Platform (Distr) is built for software vendors distributing their application to customer controlled environments. We are also the authors of a Kubernetes Package Manager.I am Jakob\u2014an engineer working at Glasskube, which helps companies distribute their application to customer-controlled environments.\nWe build an Open Source Software Distribution platform called Distr (github.com/glasskube/distr),\nthe hosted version of which is running on AWS EKS.If you have ever hosted a web-application on EKS, Amazon's managed Kubernetes offering, you are most-likely familiar with the AWS Load Balancer Controller project (formerly AWS ALB Ingress Controller).\nIf you have also upgraded that application, you might also be familiar with reports like this:The upgrade you just triggered has caused downtime for your application!\nYou might think that there is an error your application, but all your logs and metr"
  },
  {
    "title": "How to Implement a Cosine Similarity Function in TypeScript (alexop.dev)",
    "points": 38,
    "submitter": "alexop",
    "submit_time": "2025-03-09T09:20:20 1741512020",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=43307541",
    "comments": [
      "Great post, but what struck me (again, like every time I look at cos similiarity) is how unreasonably well it works. It's just one of those things that's so \"weird\" about our world: why would cosine similarity work in n-dimensional semantic spaces? It's so stupid simple, and it intuitively makes sense, and it works really well. Crazy cool.I'm reminded of that old Eugene Wigner quote: \"The most incomprehensible thing about the universe is that it is comprehensible.\"\n \nreply",
      "It\u2019s a nice post, but \u201cusing array methods\u201d probably shouldn\u2019t be placed in the \u201cEfficient Implementation\u201d section. As often happens with high-level languages, a single plain old loop is faster than three array methods.Similarly, if you plan to query those vectors in search, you should consider continuous `TypedArray` types and smaller scalars than the double precision `number`.I know very little about JS, but some of the amazing HackerNews community members have previously helped port SimSIMD to JavaScript (https://github.com/ashvardanian/SimSIMD), and I wrote a blog post covering some of those JS/TS-specifics, NumJS, and MathJS in 2023 (https://ashvardanian.com/posts/javascript-ai-vector-search/).Hopefully, it should help unlock another 10-100x in performance.\n \nreply",
      "Oh this is awesome! We actually have a really good use case for this, it should improve performance in our case a lot and help us stay away from WASM a bit longer. Thanks for sharing!\n \nreply",
      "I attempted to implement this on the front end of my e-commerce site, which has approximately 20,000 products (see gist [1]). My goal was to enhance search speed by performing as many local operations as possible.Biggest impact in performance was by moving to dot products.Regrettably, the sheer size of the index of embeddings rendered it impractical to achieve the desired results.1. https://gist.github.com/schappim/d4a6f0b29c1ef4279543f6b6740...\n \nreply",
      "This looks nice. I also played on the weekend with Vue and Transformer.js to build the embeddings locally. See https://github.com/alexanderop/vue-vector-search\n \nreply",
      "Those are some janky diagrams. The labels are selectable, and therefore are repeatedly highlighted and un-highlighted while dragging the vector around. The \"direction only\" arrow prevents you from changing the magnitude, but it doesn't prevent said magnitude from changing and it does so often because the inputs are quantized but the magnitude isn't. Multiple conventions for decimals are used within the same diagram. The second diagram doesn't center the angle indicator on the actual angle. Also the \"send me feedback on X\" popup doesn't respond to the close button, but then disappeared when I scrolled again so maybe it did? I'm running Chrome 134.0.6998.36 for Windows 10 Enterprise 22H2 build 19045.5487.This whole thing looks like unreviewed AI. Stylish but fundamentally broken. I haven't had a chance to dig into the meat of the article yet, but unfortunately this is distracting enough that I'm not sure I will.Edit: I'm digging into the meat, and it's better! Fortunately, it appears accurate. Unfortunately, it's rather repetitive. There's two paragraphs discussing the meaning of -1, 0, and +1 interleaved with multiple paragraphs explaining how cosine similarity allows vectors to be compared regardless of magnitude. The motivation is spread throughout the whole thing and repetitive, and the real world examples seem similar though formatted just differently enough to make it hard to tell at a glance.To try to offer suggestions instead of just complaining... Here's my recommended edits:I'd move the simple English explanation to the top after the intro, then remove everything but the diagrams until you reach the example. I'd completely remove the explanation of vectors unless you're going to include an explanation of dot products. I really like the functional approach, but feel like you could combine it with the `Math.hypot` example (leave the full formula as a comment, the rest is identical), and with the full example (although it's missing the `Math.hypot` optimization). Finally, I feel like you could get away with just one real web programming example, though don't know which one I'd choose. The last section about using OpenAI for embedding and it's disclaimer is already great!\n \nreply",
      "Thank you for the good feedback. I tried to improve that. I was writing the blog post for myself to understand Cosine Similarity, which is why it's maybe a bit repetitive, but this is the best way for me to learn something. I get your point. Next time I will write it better. Good feedback - I love that.\n \nreply",
      "Ha, when you put it that way, I can totally see why it read like that!It looks super great now. What you have here leaves an entirely different impression, and a stylish one!Two last suggestions:* Now I'm thinking the Why Cosine Similarity Matters for Modern Web Development section belongs at the top, right after your intro.* The angle indicator is still a bit wonky in the diagram. I might even take direction only mode out entirely, as you point out cosine similarity is invariant to changes in magnitude.\n \nreply",
      "I've just written a tag similarity calculation for a custom set of tagged documents, and if you're going to actually use the metrics, it's probably a good idea to fill a similarity table instead of recalculating it all on the spot.While doing that, the next logical step is precalculating the squared magnitudes for each item, and in my small test case of <1000 items that sped the calculation up almost 300 times. The gains are not exponential, but economy on that constant for each pair considered is not insignificant, especially on small data sets (of course, with large sets a table won't cut it due to memory limitations and requires more sophisticated techniques).\n \nreply",
      "I liked your article. The chart with the vectors on it was cool though kinda hard to use on mobile.I went to the typescript tag and tried to read a few other articles and got 404 errors. Just wanted to let ya know.Nice blog and good work!\n \nreply"
    ],
    "link": "https://alexop.dev/posts/how-to-implement-a-cosine-similarity-function-in-typescript-for-vector-comparison/",
    "first_paragraph": "To understand how an AI can understand that the word \u201ccat\u201d is similar to \u201ckitten,\u201d you must realize cosine similarity. In short, with the help of embeddings, we can represent words as vectors in a high-dimensional space. If the word \u201ccat\u201d is represented as a vector [1, 0, 0], the word \u201ckitten\u201d would be represented as [1, 0, 1]. Now, we can use cosine similarity to measure the similarity between the two vectors. In this blog post, we will break down the concept of cosine similarity and implement it in TypeScript. \ud83d\udca1 Note I won\u2019t explain how embeddings work in this blog post, but only how to use them.The cosine similarity formula measures how similar two vectors are by examining the angle between them, not their sizes. Here\u2019s how it works in plain English:What it does: It tells you if two vectors point in the same direction, opposite directions, or somewhere in between.The calculation:The result:Why it\u2019s useful:In AI applications:When you build applications with any of these features, you"
  },
  {
    "title": "STEPS Toward the Reinvention of Programming (2012) [pdf] (tinlizzie.org)",
    "points": 57,
    "submitter": "pcfwik",
    "submit_time": "2025-03-10T18:22:57 1741630977",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=43323860",
    "comments": [
      "Related:VPRI - https://news.ycombinator.com/item?id=27722969 - July 2021 (1 comment)Viewpoints Research Institute concluded its operations at the beginning of 2018 - https://news.ycombinator.com/item?id=26926660 - April 2021 (36 comments)Final \u201cSTEPS Toward the Reinvention of Programming\u201d Paper [pdf] - https://news.ycombinator.com/item?id=11686325 - May 2016 (64 comments)A computer system in less than 20k LOC progress report - https://news.ycombinator.com/item?id=1942204 - Nov 2010 (3 comments)A compiler made only from PEG-based transformations for all stages - https://news.ycombinator.com/item?id=1819779 - Oct 2010 (2 comments)Steps Toward The Reinvention of Programming - https://news.ycombinator.com/item?id=141492 - March 2008 (12 comments)I feel certain that there were other HN threads related to this project if anyone wants to dig around for some!\n \nreply",
      "I couldn't help but notice that the authors were credited \"In random order\" and am now wondering a) Why not alphabetical? and b) Did they just shuffle the order once or was it \"Random until we found an order that happened to match some other criteria we had in mind\"\n \nreply",
      "With PDF, you can in principle have a different random order every time you open the PDF.\n \nreply",
      "Interesting point! I wonder how that fits in to PDF/A and the idea that an archival format and long-term preservation of the original document...\n \nreply",
      "It's clear that alphabetical order is open to manipulation. Down that path and everybody in the scientific career will be named A.A.\n \nreply",
      "This is obviously an absurd overextrapolation, and it's unlikely that a significant number of people would actually change their name to exploit it, but the principle is accurate: If alphabetical is used consistently then someone with the last name Zelenskyy will consistently end up last in every list of coauthors, while Adams will consistently come near the top. Even if people intuitively understand that alphabetical ordering is used because all coauthors are equal, the citations will still be for Adams et al., and it's not hard to see how that would give an unfair non-merit-based leg up to Adams over Zelenskyy.If applied consistently, random order would be a fairly sound way to ensure that over a whole career no one gets too large a leg up from their surname alone.\n \nreply",
      "This is obviously an absurd overextrapolation, and it's unlikely that a significant number of people would actually change their name to exploit it, but the principle is accurate:That's called a joke.\n \nreply",
      "I wasn't criticizing OP's statement, just elaborating on it. And it wasn't a joke so much as a rhetorical device.\n \nreply",
      "Any updates on this program in the past 13 years?\n \nreply",
      "Lots of later follow-up research has been published.I am proposing to fund a secure parallel operating system, GUI, applications and hardware from scratch in 20 KLOC for the European Community to gain computational independence from the US. I consider it the production version of the STEPS research.We are in the signing up stage of the researchers, programmers and chip designers and have regular meetings and presentations [1].Half a trillion Euro's is the wider funding pool, several hundred million for European chips and operating systems, billions for European chip fabs, dozens of billions for buying the secure EU software and chips for government, schools and military.An unsolved problem is how to program a webbrowser in less than 20 KLOC.I think that the STEPS research was a resounding succes as was proven by the demonstration of the software system in Alan Kay's talks[2] and confirmed by studying the source code. As mentioned before in my earlier HN post, I have a working version of Frank and most other parts of the STEPS research.[1] https://www.youtube.com/watch?v=vbqKClBwFwI[2] https://www.youtube.com/watch?v=ubaX1Smg6pY\n \nreply"
    ],
    "link": "https://tinlizzie.org/VPRIPapers/tr2012001_steps.pdf",
    "first_paragraph": ""
  },
  {
    "title": "Music labels will regret coming for the Internet Archive, sound historian says (arstechnica.com)",
    "points": 337,
    "submitter": "coloneltcb",
    "submit_time": "2025-03-10T16:28:44 1741624124",
    "num_comments": 112,
    "comments_url": "https://news.ycombinator.com/item?id=43322245",
    "comments": [
      "They clarify they mean they think they will regret it historically in the sense of losing that part of history, rather than monetarily, etc.But to me this is like arguing a swarm of locusts are going to one day regret that they destroyed so many fields of crops.\n \nreply",
      "My favourite example of this is that for Babylon 5 the lawyers from Warner Bros insisted that the showrunners not retain any copies of the digital models because they were WB property and WB would archive them.When they started working on B5 The Lost Tales, WB had lost all of the digital files.\n \nreply",
      "This is also a common side effect of corporate policies that favour short term profits rather than retaining staff for the long term.I see this scenario play out constantly:1. A team or staff member has a project's data located on their user account or hardware.2. The team or staff member are made redundant when there is a dip in earnings.3. IT make a backup of the user account/s and wipe the hardware, and because it's older usually move it on.4. Months or years later that data is needed and at this point no one at the company actually knows where the data is - and even if they find something, they don't know if that's the latest or the full version.Now this wouldn't be a problem if there was a decent overlap in staff retention, but that simply isn't the case these days.Writing better storage policies doesn't help - it's the understaffed nature of their businesses which mean that there is no time for staff to keep up with basic data housekeeping.The types of data that clients should have on hand, but nevertheless have asked me to supply, are frankly embarrassing.\n \nreply",
      "What company is allowing employees to have so much data locally? Almost all work is stored in a cloud now. Documents, spreadsheets, design docs, code\u2026 If you really are constantly seeing this then that says a lot about the corporation using severely outdated practices.\n \nreply",
      "> If you really are constantly seeing this then that says a lot about the corporation using severely outdated practices.They probably just used now-outdated practices before those practices were outdated. This happened in the past, remember. Sure, the cloud is a thing today, but was the cloud such a thing 5, 10, 20 years ago? Do you really think it's their fault for not knowing in advance how much of a thing the cloud would one day become? Oh, how outdated. Sheesh.\n \nreply",
      "> Sure, the cloud is a thing today, but was the cloud such a thing 5, 10, 20 years ago?Yes, yes, and no.For example, https://www.explainxkcd.com/wiki/index.php/908:_The_Cloud is from 2011.\n \nreply",
      "Nope. I\u2019ll not list off all the DVCS implementations that existed 20 years ago, you should look it up. That is not an excuse.\n \nreply",
      "Ah yes, git and mercurial \u2013 the perfect place to save 3TB of binary data that changes weekly.\n \nreply",
      "It's sad when requests to official and unofficial archives are lost.\n \nreply",
      "Truly and authentically lost tales\n \nreply"
    ],
    "link": "https://arstechnica.com/tech-policy/2025/03/music-labels-will-regret-coming-for-the-internet-archive-sound-historian-says/",
    "first_paragraph": "\n        Labels push to spike cost of Internet Archive fight over old 78s.\n      On Thursday, music labels sought to add nearly 500 more sound recordings to a lawsuit accusing the Internet Archive (IA) of mass copyright infringement through its Great 78 Project, which seeks to digitize all 3 million three-minute recordings published on 78 revolutions-per-minute (RPM) records from about 1898 to the 1950s.If the labels' proposed second amended complaint is accepted by the court, damages sought in the case\u2014which some already feared could financially ruin IA and shut it down for good\u2014could increase to almost $700 million. (Initially, the labels sought about $400 million in damages.)IA did not respond to Ars' request for comment, but the filing noted that IA has not consented to music labels' motion to amend their complaint.Labels told the court the new complaint was warranted, since these 493 new recordings are evidence of alleged ongoing infringement that they claimed occurred after the c"
  },
  {
    "title": "A technical history of Acorn Computers (mcmordie.co.uk)",
    "points": 84,
    "submitter": "xyzzy3000",
    "submit_time": "2025-03-10T14:35:05 1741617305",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=43321131",
    "comments": [
      "Someone will be along in a minute to tell you to watch Micro Men, an amusing and fairly accurate BBC dramatization of the Sinclair/Acorn rivalry :) but I'm here to recommend that you watch the Computer History Museum's interview with Hermann Hauser the erstwhile director of Acorn - he's very charming: https://m.youtube.com/watch?v=Y0sC3lT313QI'm fairly sure they've got one with Chris Curry too, but I can't spot it just now.\n \nreply",
      "It would appear you can watch Chris Curry and Hermann Hauser watch Micro Men: https://www.youtube.com/watch?v=yaonVYOTSsk -- and then have a post-viewing chat: https://www.youtube.com/watch?v=l4I2ktcWdJM\"I saw the first five minutes and had to run away, because I couldn't bear to see myself portrayed by Martin Freeman\" -- Chris Curry\n \nreply",
      "Which was hosted by the Centre for Computing History!\n \nreply",
      "It's an entertaining watch, although a fair bit of dramatic licence is taken when depicting various anecdotes that would be well-known to readers of The Micro User and similar magazines of the time.\n \nreply",
      "Ah, the delights of a page that covers 26 years of history and hasn't been updated for 21 yearsI can at least say that in the meantime, RISC OS is still alive and now open, available from https://www.riscosopen.org/, and most people will know the ARM company and its architectures went from strength to strength, even if the RISC PC faded away.\n \nreply",
      "Some time when I have some spare time I must fish out a Pi and try it out - I was deeply envious of the Archimedes owner I knew back in the day!\n \nreply",
      "I'm not sure what the state of Pi 5 support is, but RISC OS seems to work on the Pi 1, 2, 3 and 4.ArcEm is a decent emulator for the Archimedes series, and RPCem is the equivalent for the RiscPC which succeeded it. OS ROM images are available from a variety of places.For floppy disk emulation, ADFFS is what you are looking for, and some games have been released in this format with the consent of the copyright holders.\n \nreply",
      "Don't think Pi 5 can run 'supervisor code' (in old ARM language) in 32-bit mode, so RISC OS surely won't run. The bulk of it is written in assembler so reworking it to AArch64 would be an epic task. Probably take no longer to rewrite those bits in a higher-level language, for much of it.\n \nreply",
      "DEC had a similar problem when porting VMS from VAX to Alpha - significant chunks of it were written in VAX assembly.They came up with a rather ingenious solution - a compiler from VAX assembly to Alpha. And that was carried forward into the Itanium and now x86-64 ports, so even latest OpenVMS for x86-64 still contains some VAX assembly code, but it is compiled into LLVM IR and then the LLVM backend converts it to x86-64 ELF binaries.No reason in principle why someone could not do the same thing with the 32-bit ARM assembly code in RISC OS. Likely would be easier than rewriting it all in a high-level language\n \nreply",
      "IIRC there was \"macro32\" which could compile VAX machine code to Alpha, and also VEST, which was a virtual machine.I've used neither.\n \nreply"
    ],
    "link": "https://www.mcmordie.co.uk/acornhistory/index.shtml",
    "first_paragraph": "These pages contain a fairly brief technical history of Acorn Computers. I have decided to focus on the technology because that is where my interests lie. Only very major non-technical events in Acorn's history have been included. If anyone finds any inaccuracies in the following pages please contact me using the link at the bottom of the page, including a correction if possible.Note that the information on these pages is not an offical history of Acorn's computers and has no official support from Acorn Computers.These pages are under construction. I'm very busy at the moment, but I'm trying to sort these pages out as quickly as possible.19th April 2004 - a few days late, but never mind... Happy Birthday, RiscPC! (10 years old on the 16th of April.)There are some gaps in this history. For example, I know that there was a PocketBook II released by October 1996, but I don't know how it fits in. Also, I don't know very much at all about the Acorn Network Computer, Set Top Box 2, Stork or "
  },
  {
    "title": "Show HN: Editable Games (playscl.com)",
    "points": 113,
    "submitter": "sclerek",
    "submit_time": "2025-03-10T15:31:57 1741620717",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=43321688",
    "comments": [
      "\"Not very successful (...) Since it's my passion, I keep trying\"The technology you've built is very impressive, but I suspect the way it's presented won't appeal to your target audience. It's very programmer-art-y.It might be worthwhile hiring a designer and having them go over everything for you, both making sure it looks like a professional tool aimed at an audience that values sophisticated visuals, and also that the examples you have look premium.\n \nreply",
      "I really liked stars and snargs, challenging and fun.\n \nreply",
      "Playpen IRL!\n \nreply",
      "Anyone else misread this is \"Edible Games\" at first?\nGot kinda excited\n \nreply",
      "Slightly OG-offtopic, but there is a very good book about edible games! https://www.amazon.co.uk/Edible-Games-Cookbook-Play-your-ebo...\n \nreply",
      "I misread that too when I got the domain name for EditableGIFs, and I think I even bought EdibleGIFs at the time to just reserve it. I let it expire though.\n \nreply",
      "yup, maybe it's the fasting.\n \nreply",
      "Yeah haha\n \nreply",
      "yes ... I blame my old eyes\n \nreply",
      "PICO-8 is very cool for that matter. You can always view the Lua source code of the cartridge, and the size of the game is inherently limited, so you don't have too much code to dig through.\n \nreply"
    ],
    "link": "https://playscl.com/make",
    "first_paragraph": "\n\t\t\tYou can edit/change the games in the list below. You can upload your own images, and even re-program the game play using\n\t\t\tSCL.\n\t\t\t\n\t\t\tOnce you've made your new game, you can have it activated to run on a domain of your choice by visiting the\n\t\t\tPublishing page. For example, if you have an account a itch.io, you\n\t\t\tcan activate your game for your itch page.\n\t\t\t\n\t\t\tThis is a demo of how editable and customizing games work using Canvas Language. If you choose a game below, you'll be taken to a dev studio where you can edit your game\n\t\t\tand download a project file, for re-upload later.\n\t\t\tCustomizeCustomizeCustomizeCustomizeCustomizeCustomizeCustomizeCustomizeCustomize\u00a9 2025 All rights reserved. | Terms of service | Privacy"
  },
  {
    "title": "People are just as bad as my LLMs (wilsoniumite.com)",
    "points": 133,
    "submitter": "Wilsoniumite",
    "submit_time": "2025-03-10T18:16:26 1741630586",
    "num_comments": 96,
    "comments_url": "https://news.ycombinator.com/item?id=43323755",
    "comments": [
      "> ...a lot of the safeguards and policy we have to manage humans own unreliability may serve us well in managing the unreliability of AI systems too.It seems like an incredibly bad outcome if we accept \"AI\" that's fundamentally flawed in a way similar to if not worse than humans and try to work around it rather than relegating it to unimportant tasks while we work towards a standard of intelligence we'd otherwise expect from a computer.LLMs certainly appear to be the closest to real AI that we've gotten so far.  But I think a lot of that is due to the human bias that language is a sign of intelligence and our measuring stick is unsuited to evaluate software specifically designed to mimic the human ability to string words together.  We now have the unreliability of human language processes without most of the benefits that comes from actual human level intelligence.  Managing that unreliability with systems designed for humans bakes in all the downsides without further pursuing the potential upsides from legitimate computer intelligence.\n \nreply",
      "I don\u2019t disagree. But I also wonder if there even is an objective \u201cright\u201d answer in a lot of cases. If the goal is for computers to replace humans in a task, then the computer can only get the right answer for that task if humans agree what the right answer is. Outside of STEM, where AI is already having a meaningful impact (at least in my opinion), I\u2019m not sure humans actually agree that there is a right answer in many cases, let alone what the right answer is. From that perspective, correctness is in the eye of the beholder (or the metric), and \u201ccorrect\u201d AI is somewhere between poorly defined and a contradiction.Also, I think it\u2019s apparent that the world won\u2019t wait for correct AI, whatever that even is, whether or not it even can exist, before it adopts AI. It sure looks like some employers are hurtling towards replacing (or, at least, reducing) human headcount with AI that performs below average at best, and expecting whoever\u2019s left standing to clean up the mess. This will free up a lot of talent, both the people who are cut and the people who aren\u2019t willing to clean up the resulting mess, for other shops that take a more human-based approach to staffing.I\u2019m looking forward to seeing which side wins. I don\u2019t expect it to be cut-and-dry. But I do expect it to be interesting.\n \nreply",
      "What is your measure of intelligence?\n \nreply",
      "The ability to create novel solutions without a priori knowledge.\n \nreply",
      "I honestly don't have a great one, which is less worrying than it might otherwise be since I'm not sure anyone else does either.  But in a human context, I think intelligence requires some degree of creativity, self-motivation, and improvement through feedback.  Put a bunch of humans on an island with various objects and the means for survival and they're going to do...something.  Over enough time they're likely to do a lot of unpredictable somethings and turn coconuts into rocket ships or whatever.  Put a bunch of LLMs on an equivalent island with equivalent ability to work with their environment and they're going to do precisely nothing at all.On the computer side of things, I think at a minimum I'd want intelligence capable of taking advantage of the fact that it's a deterministic machine capable of unerringly performing various operations with perfect accuracy absent a stray cosmic ray or programming bug.  Star Trek's Data struggled with human emotions and things like that, but at least he typically got the warp core calculations correct.  Accepting LLMs with the accuracy of a particularly lazy intern feels like it misses the point of computers entirely.\n \nreply",
      "I think using the word \u201cintelligence\u201d when speaking of computers, beyond a kind of figure of speech, is anthropomorphizing, and it is a common pseudoscientific habit that must go.What is most characteristic about human intelligence is the ability to abstract from particular, concrete instances of things we experience. This allows us to form general concepts which are the foundation of reason. Analysis requires concepts (as concepts are what are analyzed), inference requires concepts (as we determine logical relations between them).We could say that computers might simulate intelligent behavior in some way or other, but this is observer relative not an objective property of the machine, and it is a category mistake to call computers intelligent in any way that is coherent and not the result of projecting qualities onto things that do not possess them.What makes all of this even more mystifying is that, first, the very founding papers of computer science speak of effective methods, which is by definition about methods that are completely mechanical and formal, and this stripped of the substantive conceptual content it can be applied to. Historically, this practically meant instructions given to human computers who merely completed them without any comprehension of what they were participating in. Second, computers are formal models, not physical machines. Physical machines simulate the computer formalism, but are not identical with the formalism. And as Kripke and Searle showed, there is no way in which you can say that a computer is objectively calculating anything! When we use a computer to add two numbers, you cannot say that the computer is objectively adding two numbers. It isn\u2019t. The addition is merely an interpretation of a totally mechanistic and formal process that has been designed to be interpretable in such ways. It is analogous to reading a book. A book does not objectively contains words. It contains shaped blots of pigment on sheets of cellulose that have been assigned a conventional meaning in a culture and language. In other words, you being the words, the concepts, to the book. You bring the grammar. The book itself doesn\u2019t have them.So we must stop confusing figurative language with literal language. AI, LLMs, whatever can be very useful, but it isn\u2019t even wrong to call them intelligent in any literal sense.\n \nreply",
      "This is the \"anyone can be a mathematician meme\". People who hang around elite circles have no idea how dumb the average human is. The average human hallucinates constantly.\n \nreply",
      "There has been some good research published on this topic of how RLHF, ie aligning to human preferences easily introduces mode collapse and bias into models. For example, with a prompt like: \"Choose a random number\", the base pretrained model can give relatively random answers, but after fine tuning to produce responses humans like, they become very biased towards responding with numbers like \"7\" or \"42\".\n \nreply",
      "I assume 42 is a joke from deep history and The Hitchhiker\u2019s Guide. Pretty amusing to read the Wikipedia entry:https://en.wikipedia.org/wiki/42_(number)\n \nreply",
      "Douglas Adams picked 42 randomly though. :)\n \nreply"
    ],
    "link": "https://wilsoniumite.com/2025/03/10/people-are-just-as-bad-as-my-llms/",
    "first_paragraph": "Wilsons BlogLast year I created a fun little experiment where I asked a bunch of LLMs to rank 97 hackernews users using their comment history based on whether they would be good candidates for the role of \u201csoftware engineer at google\u201d. (yes yes, seems silly I know, you can read part 1 and part 2 but they are long).In it, I had a persistent problem of bias. I had arranged the comments in an interleaved fashion like this: The users aren\u2019t responding to each other, that\u2019s just how I arranged the comments in the prompt. I didn\u2019t give the model the users names for obvious reasons. Then the model says who it prefers, and using many pairwise comparisons we can come up with a ranking (similar to the way chess rankings work). However, I noticed an odd bias. Even though which user was named \u201cPerson one\u201d in the prompt was random, the model still preferred whoever got the name \u201cPerson one\u201d slightly more often than not (or for some models, preferred \u201cPerson Two\u201d). This is dumb. There is no reason t"
  },
  {
    "title": "Sigint in Fiction (siginthistorian.blogspot.com)",
    "points": 31,
    "submitter": "_tk_",
    "submit_time": "2025-03-09T10:29:27 1741516167",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=43307833",
    "comments": [
      "The conflation of SIGINT and cryptology makes me doubt the writer's understanding of the field. While cryptography and cryptanalysis have long been important to the field, they are but one aspect. I won't offer a meaningless percentage as to how important they are, but consider the importance of a) traffic analysis and b) meta-data: There are plenty of historical examples of operational decisions being made on the basis of traffic analysis and there is the oft-quoted Michael Hayden's \"we kill people based on metadata\".(Then there is my favourite WWII nit: The focus on Enigma, which, while undoubtedly important, pales in comparison to Tunny. The fact that Flowers could build essentially a general purpose computer to crack a cipher based only on its apparent properties is heads-and-shoulders and all other superlatives above Enigma, for which GCHQ had working examples throughout the war. I am not downplaying the importance of the Enigma cryptanalysis work, merely suggesting that Tunny was, in the end, and strategically, far more valuable, especially once telephone communications became harder for OKW.)\n \nreply",
      "> The conflation of SIGINT and cryptology makes me doubt the writer's understanding of the field.ELINT is one of the two pillars of SIGINT, the other one being COMINT. Some aspects of COMINT involve crypto analysis. The author seems to be unaware of any of this.\n \nreply",
      "A good example of the importance of traffic analysis and meta-data is:https://slate.com/technology/2013/06/prism-metadata-analysis...\n \nreply",
      "There are others besides the ones that the author mentions. Some that come to mind:1. The Gold Bug (E.A. Poe)\n2. Enigma (R. Harris)\n3. Cryptonomicon (N. Stephenson)In the comments however it appears that the author meant specifically British fiction.\n \nreply",
      "> A key point, I feel, is that it is impossible to describe the process of cryptanalysis in a work of fiction and make it interesting for the general reader.Neal Stephenson pulled it off in Cryptonomicon.\n \nreply",
      "Great example! Although I'm having trouble thinking of other examples that aren't by the same author. An exception to the rule?\n \nreply",
      "Ian McEwan's \"The Innocent,\" and less known, \"The Imitation Game\" have sigint plots, and a few of his novels have spy agency themes without falling into the trap of being genre fiction. Stephenson's \"Cryptonomicon\" was an obvious one as well.when I travelled in some literary circles, they were always on the edge of the bureaucracy via public funding bodies, universities, or foundations, where you were just above the fold, and so you didn't talk about intelligence stuff because everyone had gone to school with someone who was at this or that agency, and making a point of not talking about it was a social signal. I suspect this is why the material on sigint in fiction isn't nearly as rich as they all seemed to affect to know.it was ironic, as actually being a civilian in security where at least half my colleagues had military experience with CV gaps for \"travel,\" and who over drinks might casually recognize someone's loudly striped tie as resembling a burn bag, or whose partners had implausible jobs and hobbies, where literally all of our threat assessment work included what would come to be known via snowden as bullrun- we talked about that stuff all the time.maybe it's more of a plot device these days. I sort of gave up reading fiction when sensitivity readers became a thing because investing time in novels became less appealing. perhaps all these filter bubbles and AI slops will create a new rennaisance in fiction as people search to experience the pleasure of some authentic art again.\n \nreply",
      "In William Gibson's latest novel, \"Agency\", there is an underlying theme of the newly emergent AI being able to evade the eavesdropping of the corporation that spawned her. Later, she cooks up an unbreakable secure comms tech for the human beings in her network.As well, in the part of the novel that takes place in the future, evading SIGINT plays a significant part of the story, as well.Note: \"The Peripheral\" is the novel that precedes \"Agency\", so it would be best to read it first, to better grok the world WG builds. I highly recommend all his books.\n \nreply",
      "Thanks for mentioning these books as I came across \u201cVirtual Light\u201d by William Gibson in jail and found it a very accomplished novel and his craft at a high level. Also it finally clicked to me why the mainframes were called Gibsons in the film \u201cHackers\u201d as I grew up not being exposed to his books (though now I actively have them on my to read list).\n \nreply",
      "Cool, I've been listening to the last 1.5hr or so of Idoru recently (sequel to Virtual Light). I really like Rez's bodyguard Blackwell.Gibson's description, in Idoru, of Slitscan's audience is one of my favorite pieces of Gibson's writing, which is always about \"the now\" of when he wrote it. That particular description happens to be a really, really good description of the modern GOP.I also recommend his collection of non-fiction magazine writings and paid talks, \"Distrust that Particular Flavor\". The important thing about Gibson is that his heart shines through all his work, and -- boy, oh boy! -- does this 2025 world need more heart!Thanks popping up on my radar. Who knows where fate will lead us?Peace be with you, brother. May you be blessed with all healing, health, and prosperity!\n \nreply"
    ],
    "link": "https://siginthistorian.blogspot.com/2025/02/sigint-in-fiction.html",
    "first_paragraph": "\nI had an\narticled published last month in the John Buchan Journal (unsurprisingly,\nthe journal of the John Buchan Society). It is about the way that John Buchan drew\non his First World War experience as a customer of Sigint to use cryptanalysis\nin one of his books and in a short story to advance the narrative, and to develop\ncharacters.A key point,\nI feel, is that it is impossible to describe the process of cryptanalysis in a\nwork of fiction and make it interesting for the general reader. Buchan\u2019s answer\nwas to give a vague idea of an encryption system: Playfair in one, double\ntransposition in the other; make the reader understand that this is something\nreally difficult, and that therefore the practitioners have to be intelligent,\nand not just lucky; and move on, making the story (or the relevant part of the\nstory) about people who break codes or ciphers, not about the process of\nbreaking them.Two of the\nnovels of Dorothy L Sayers have cryptanalytic sub-plots. Lord Peter Wimsey\ncertai"
  }
]