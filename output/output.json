[
  {
    "title": "Qwen3-Coder: Agentic coding in the world (qwenlm.github.io)",
    "points": 268,
    "submitter": "danielhanchen",
    "submit_time": "2025-07-22T21:12:30 1753218750",
    "num_comments": 91,
    "comments_url": "https://news.ycombinator.com/item?id=44653072",
    "comments": [
      "I'm currently making 2bit to 8bit GGUFs for local deployment! Will be up in an hour or so at https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruc...Also docs on running it in a 24GB GPU + 128 to 256GB of RAM here: https://docs.unsloth.ai/basics/qwen3-coderreply",
      "Looks like the docs have a typo:    Recommended context: 65,536 tokens (can be increased)\n\nThat should be recommended token output, as shown in the official docs as:    Adequate Output Length: We recommend using an output length of 65,536 tokens for most queries, which is adequate for instruct models.reply",
      "Oh thanks - so the output can be any length you like - I'm actually also making 1 million context length GGUFs as well! https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruc...reply",
      "Cool, thanks! I'd like to try itreply",
      "It just got uploaded! I made some docs as well on how to run it at https://docs.unsloth.ai/basics/qwen3-coderreply",
      "> Qwen3-Coder is available in multiple sizes, but we\u2019re excited to introduce its most powerful variant firstI'm most excited for the smaller sizes because I'm interested in locally-runnable models that can sometimes write passable code, and I think we're getting close. But since for the foreseeable future, I'll probably sometimes want to \"call in\" a bigger model that I can't realistically or affordably host on my own computer, I love having the option of high-quality open-weight models for this, and I also like the idea of \"paying in\" for the smaller open-weight models I play around with by renting access to their larger counterparts.Congrats to the Qwen team on this release! I'm excited to try it out.reply",
      "small models can never match bigger models, the bigger models just know more and are smarter.  the smaller models can get smarter, but as they do, the bigger models get smart too.   HN is weird because at one point this was the location where I found the most technically folks, and now for LLM I find them at reddit.  tons of folks are running huge models, get to researching and you will find out you can realistically host your own.reply",
      "The large models are using tools/functions to make them useful. Sooner or later open source will provide a good set of tools/functions for coding as well.reply",
      "> small models can never match bigger models, the bigger models just know more and are smarter.They don't need to match bigger models, though. They just need to be good enough for a specific task!This is more obvious when you look at the things language models are best at, like translation. You just don't need a super huge model for translation, and in fact you might sometimes prefer a smaller one because being able to do something in real-time, or being able to run on a mobile device, is more important than marginal accuracy gains for some applications.I'll also say that due to the hallucination problem, beyond whatever knowledge is required for being more or less coherent and \"knowing\" what to write in web search queries, I'm not sure I find more \"knowledgeable\" LLMs very valuable. Even with proprietary SOTA models hosted on someone else's cloud hardware, I basically never want an LLM to answer \"off the dome\"; IME it's almost always wrong! (Maybe this is less true for others whose work focuses on the absolute most popular libraries and languages, idk.) And if an LLM I use is always going to be consulting documentation at runtime, maybe that knowledge difference isn't quite so vital\u2014 summarization is one of those things that seems much, much easier for language models than writing code or \"reasoning\".All of that is to say:Sure, bigger is better! But for some tasks, my needs are still below the ceiling of the capabilities of a smaller model, and that's where I'm focusing on local usage. For now that's mostly language-focused tasks entirely apart from coding (translation, transcription, TTS, maybe summarization). It may also include simple coding tasks today (e.g., fancy auto-complete, \"ghost-text\" style). I think it's reasonable to hope that it will eventually include more substantial programming tasks\u2014 even if larger models are still preferable for more sophisticated tasks (like \"vibe coding\", maybe).If I end up having a lot of fun, in a year or two I'll probably try to put together a machine that can indeed run larger models. :)reply",
      "> and now for LLM I find them at reddit. tons of folks are running huge modelsVery interesting. Any subs or threads you could recommend/link to?Thanksreply"
    ],
    "link": "https://qwenlm.github.io/blog/qwen3-coder/",
    "first_paragraph": "GITHUB\nHUGGING FACE\nMODELSCOPE\nDISCORDToday, we\u2019re announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we\u2019re excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct \u2014 a 480B-parameter Mixture-of-Experts model with 35B active parameters which supports the context length of 256K tokens natively and 1M tokens with extrapolation methods, offering exceptional performance in both coding and agentic tasks. Qwen3-Coder-480B-A35B-Instruct sets new state-of-the-art results among open models on Agentic Coding, Agentic Browser-Use, and Agentic Tool-Use, comparable to Claude Sonnet 4.Alongside the model, we\u2019re also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, Qwen Code has been adapted with customized prompts and function calling protocols to fully unleash the capabilities of Qwen3-Coder on agentic coding tasks. Qwen3-Coder works seamlessly with the community\u2019s best dev"
  },
  {
    "title": "Why you can't color calibrate deep space photos (maurycyz.com)",
    "points": 34,
    "submitter": "LorenDB",
    "submit_time": "2025-07-23T00:16:18 1753229778",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=44654444",
    "comments": [
      "These same things apply to satellite images of the Earth as well.  Even when you have optical bands that roughly correspond to human eye sensitivity, they're a quite different response pattern. You're also often not working with those wavelength bands in the visualizations you make.Scientific sensors want as \"square\" a spectral response as possible.  That's quite different than human eye response.  Getting a realistic RGB visualization from a sensor is very much an artform.reply",
      "The next space mission should be to leave a colour calibration chart on the moon.reply",
      "The moon itself already is one.  Moonshots are widely used in calibration, at least for earth observation satellites.  The brightness of the full moon at each wavelength at each day of the year is predictable and well-known, so it makes a good target to check your payload against.reply",
      "Recently I've been on a bit of a deep dive regarding human color vision and cameras. This left me with the general impression that RGB bayer filters are vastly over-utilized (mostly due to market share), and are they are usually not great for tasks other than mimicking human vision! For example, if you have a stationary scene, why not put a whole bunch of filters in front of a mono camera and get much more frequency information?reply",
      "In case you weren't already aware, that last bit basically describes most optical scientific imaging (e.g. satellite imaging or spectroscopy in general).reply",
      "The tiniest of corrections: Ha is 656.28nm not 565.reply",
      "> Many other cameras, particularly those with aggressive UV-IR cut filters, underespond to H-a, resulting in dim and blueish nebula. Often people rip out those filters (astro-modification), but this usually results in the camera overresponding instead.Hmm... astrophotographers do not use cameras with UV-IR cut filters at all.  For example, I owned a few of these:https://www.zwoastro.com/product-category/cameras/dso_cooled...They also generally do not use sensors that have Bayer filters.  This also screws things up.Instead they use monochromatic sensors with narrowband filters (either one band or multiple) over them keyed to specific celestial emissions.  The reason for this is that it gets rid of light pollution that is extensive and bumps up the signal to noise for the celestial items, especially the small fain details. Stuff like this:https://telescopescanada.ca/products/zwo-4-piece-31mm-ha-sii...https://telescopescanada.ca/products/zwo-duo-band-filterOften these are combined with a true color capture (or individual RGBL narrowband) just to get the stars coloured properly.Almost everything you see in high end astrophotography is false color because they map these individual narrowband captures on the monochrome sensors to interesting colours and often spending a lot of time manipulating the individual channels.This is done at the medium to high end using the PixInsight software - including by NASA for the recent James Web images by NASA: https://www.pbs.org/video/new-eye-on-the-universe-zvzqn1/The James Web telescope has a set of 29 narrowband filters for its main sensor: https://jwst-docs.stsci.edu/jwst-near-infrared-camera/nircam...Hubble pictures were famously coloured in a particular way that it has a formal name:https://www.astronomymark.com/hubble_palette.htm(My shots: https://app.astrobin.com/u/bhouston#gallery)reply",
      "What you're describing is the domain of a very, very small number of hobbyists with very deep pockets (plus various govt-funded entities).The vast majority of hobby astrophotography is done pretty much as the webpage describes it, with a single camera. You can even buy high-end Canon cameras with IR filters factory-removed specifically for astrophotography. It's big enough of a market that the camera manufacturer accommodates it.reply",
      "> What you're describing is the domain of a very, very small number of hobbyists with very deep pocketsSort of.  The telescope used for the Dumbbell nebula captures featured in the article was at worth around $1000 and his mount is probably $500.  A beginner cooled monochrome astrophotography camera is around $700 and if you want filters and a controller another $500.There are quite a few people in the world doing this, upwards of 100K:https://app.astrobin.com/searchVarious PixInsight videos have +100K views: https://youtu.be/XCotRiUIWtg?si=RpkU-sECLusPM1j-&utm_source=...Intro to narrowband also has 100K+ views: https://youtu.be/0Fp2SlhlprU?si=oqWrATDDwhmMguIl&utm_source=...reply",
      "> astrophotographers do not use cameras with UV-IR cut filters at all\n\nI'll be pedantic here and say that the author's probably talking to people who use DSLRs with adapter rings for telescopes. I've been interested in doing this for a while (just unable to financially justify it), and I think this is actually something people in this niche do.Then there are things like the Nikon D810A, which remove the UV-IR filter from the factory (but IIRC retain the Bayer filter).reply"
    ],
    "link": "https://maurycyz.com/misc/cc/",
    "first_paragraph": ""
  },
  {
    "title": "More than you wanted to know about how Game Boy cartridges work (decontextualize.com)",
    "points": 184,
    "submitter": "todsacerdoti",
    "submit_time": "2025-07-22T19:17:38 1753211858",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=44651770",
    "comments": [
      "Also (because I feel like it/am procrastinating on other stuff), some first-glance review on the design at https://git.sr.ht/~aparrish/abc-pcb/tree/main/docs/abc-pcb.p... :- U6 and U8 need nearby decoupling. LVC logic has horrid power consumption during transitions, so best to feed it well if you expect any kind of transition rate out of these gates. It doesn't have to be explicitly dedicated to these parts but it needs to be physically close.- That goes extra for the big fat WideBus 16-wide level translators. They have multiple power pins for a reason. They are vicious little devices. Decouple every last power input pin with its own individual capacitor.- U6 outputs appear to have wrong bus directions? Probably not important (also maybe not a problem at all, I usually do Altium not KiCad)- VBUS is not logic level, so you shouldn't use it as a logic signal if you want reliability (it will work... mostly... but also be weird sometimes), but you absolutely cannot drive multiple things with it and expect them to work (their input thresholds will be slightly different and you will be sad when they switch at slightly different times because it's moving slowly). Clean it up with a Schmitt trigger (1G17 etc).- No ESD protection on the USB port. Do you want it to live? Try something like ECMF02-4CMX8, nice and simple to implement (mildly annoying to solder though).- Whatever is going on with Q1 is poorly enough drawn that I can't understand it at a glance. Best to just draw these parts as two ordinary MOSFETS with A and B suffixes, then the circuits can be legible.- On IC2 (why not U2?) lines 4, 5, 6, 7 are being cross-driven. Do not ground both sides, you will be sad. Ground the input sides and let the outputs be unconnected, because they're strongly driven by the chip. Or use resistors to pull things places if you must satisfy someone somewhere who loves that kind of stuff (medical! medical? medical.).- U7 SENSE pin draws not-much current (~25nA) so no reason to burn power in that resistor divider- Consider stuffing a big fat electrolytic or two down somewhere and making your PDN nicely damped.reply",
      "> TI\u2019s TXB0108 for this purpose as well, which has an automatic direction sensing feature that obviates the need for all of the direction logic that I mentioned above.Yeah, don't use these guys. They have a tendency to swap translation direction in the presence of electrical noise, which means your input is now an output, cross-driving something. Sometimes everything survives just fine and switches back on the next edge. Sometimes the magic smoke comes out. And sometimes, if the stars align just wrong, you get an industrial accident.This is one of those classes of parts that has hidden dangers and really should not be as prominently advertised as it is. They look simple, but they're for experts only. Don't use them unless you really know their failure modes and don't have another reasonable option.reply",
      "False, this is exactly as much as I wanted to know about how Game Boy cartridges work. Thank you! :)reply",
      "Stole my comment!reply",
      "> You don\u2019t need to circumvent copy protection or region lockout hardware to write custom software for the Game Boy, since the Game Boy has none.While there's no region lockout, don't you need to pass the logo check?reply",
      "I assume they mean you don't need to modify the device's hardware or hack any software \u2013 you just need to include a blob in your ROM's header (which RGBFIX will automatically insert for you, if you're using the RGBDS toolchain).Plus, Sega v. Accolade killed that practice. I don't think anyone has tried to enforce it since.reply",
      "I was sad to see that one of my old favorite GB dev resources is gone: https://web.archive.org/web/20150410063839/http://www.devrs....Most of the links were already long-long-dead, but there were some cool inspirational projects on there.reply",
      "See also the Ultimate Game Boy Talk from 33c3: https://www.youtube.com/watch?v=HyzD8pNlpwIreply",
      "I just started learning KiCad and PCB design this month as a larf, and I was wondering if anyone has tried making a full original Gameboy PCB and open sourcing it?reply",
      "any good resources for PCB design?reply"
    ],
    "link": "https://abc.decontextualize.com/more-than-you-wanted-to-know/",
    "first_paragraph": "By Allison ParrishI set out a while ago to make a Game Boy cartridge from scratch. This\nis not a novel goal; bootleg Game Boy cartridges have existed almost as\nlong as the Game Boy itself has, and there are many\nthird-party cartridges now\navailable for purchase, or that have copyleft\ndesigns.But I wanted to know how Game Boy cartridges work. I was also excited\nto use the PIO\nfunctionality of the RP2040 microcontroller. Now, after a few years\n(!) of research and design, I\nhave made my design for a bootleg Game Boy cartridge available for\neveryone to use. In this post, I\u2019m going to take you through everything\nI learned along the way.I will be quick to note that this document doesn\u2019t contain any new\nresearch! Instead, it\u2019s my attempt to gather together information\nrelevant to making custom Game Boy cartridges in one place, and present\nthat information in a way that is easy to digest for people who start\noff with about the same level of knowledge I started with when I first\ndipped my toes "
  },
  {
    "title": "Android Earthquake Alerts: A global system for early warning (research.google)",
    "points": 177,
    "submitter": "michaefe",
    "submit_time": "2025-07-22T18:23:14 1753208594",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=44651092",
    "comments": [
      "Recently they had a significant country wide false alarm in Israel at 3AM... There was a emergency alert cell broadcast (similar to amber alert), which caused everyone to move their phone at the same time, which was falsely detected as an earthquake, which caused an Android earthquake alert to be sent to all phones in Israel 30 seconds later. I guess they didn't plan for this scenarioEdit: Arstechina article seems to mention this: \"only three were false positives. One of those was triggered by a different system sending an alert that vibrated a lot of phones\"reply",
      "I heard it was the cell broadcast which caused the phones to vibrate at the same time, not people picking them up.reply",
      "That would be quite an implementation flaw if it didn't account for the phone's own vibrations. Lots of countries use widespread emergency alert messages frequently.reply",
      "They fixed this bug, we had plenty of emergency cell broadcasts since the false alarm.reply",
      "Typical Google product. Reminds me of a person who put a bunch of phones in a car and drove which caused Google maps to wrongly show traffic in that area. It was deliberately done though as an experimentreply",
      "Better yet, he put them in a child's wagon and carted them around Berlinreply",
      "Note that those are three completely false events. The survey results Google published show 15% of people not feeling any shaking (neither strong nor light). That's still a good figure, but reading there were only 3 false positives gave me the impression that you're basically always in for a ride when you get the alert and it's not that miraculously accurate eitherreply",
      "I was thinking the same thing. A taylor swift concert where she tells everyone to sway their phones in unison might trigger thisreply",
      "Maybe it was due to the blasts from their own ordenancesreply",
      "This is really cool, and it smells like old-school Google, in a good way, like \"let's do this because we can\". It feels like it's been a while since something coming out of Google Engineering is meaningful and not designed to unlock new existential creeps, so, well done I guess.reply"
    ],
    "link": "https://research.google/blog/android-earthquake-alerts-a-global-system-for-early-warning/",
    "first_paragraph": "We strive to create an environment conducive to many different types of research across many different time scales and levels of risk.Our researchers drive advancements in computer science through both fundamental and applied research.We regularly open-source projects with the broader research community and apply our developments to Google products.Publishing our work allows us to share ideas and work collaboratively to advance the field of computer science.We make products, tools, and datasets available to everyone with the goal of building a more collaborative ecosystem.Supporting the next generation of researchers through a wide range of programming.Participating in the academic research community through meaningful engagement with university faculty.Connecting with the broader research community through events is essential for creating progress in every aspect of our work.July 17, 2025Marc Stogaitis, Principal Software Engineer, AndroidUsing aggregated measurements from a global ne"
  },
  {
    "title": "Swift-erlang-actor-system (swift.org)",
    "points": 223,
    "submitter": "todsacerdoti",
    "submit_time": "2025-07-22T18:59:04 1753210744",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=44651539",
    "comments": [
      "I'm involved with this project and wanted to provide some context. This is an extraction for a much larger effort where we're building a web browser that can render native UI. Think instead of:`<div>Hello, world!!</div>`we can do:`<Text>Hello, world!</Text>`I want to be clear: this is not a web renderer. We are not rendering HTML. We're rendering actual native UI. So the above in SwiftUI becomes:`Text(\"Hello, world!\")`And yes we support modifiers via a stylesheet system, events, custom view registration, and really everything that you would normally be doing it all in Swift.Where this library comes into play: the headless browser is being built in Elixir to run on device. We communicate with the SwiftUI renderer via disterl. We've built a virtual DOM where each node in the vDOM will have its own Erlang process. (I can get into process limit for DOMs if people want to) The Document will communicate the process directly to the corresponding SwiftUI view.We've taken this a step further by actually compiling client-side JS libs to WASM and running them in our headless browser and bridging back to Elixir with WasmEx. If this works we'll be able to bring the development ergonomics of the Web to every native platform that has a composable UI framework. So think of actual native targets for Hotwire, LiveWire, etc...We can currently build for nearly all SwiftUI targets: MacOS, iPhone, iPad, Apple Vision Pro, AppleTV. Watch is the odd duck out because it lacks on-device networking that we require for this library.This originally started as the LiveView Native project but due to some difficulties collaborating with the upstream project we've decided to broaden our scope.Swift's portability means we should be able to bring this to other languages as well.We're nearing the point of integration where we can benchmark and validate this effort.Happy to answer any questions!reply",
      "I prototyped this exact thing by parsing the DOM, building corresponding UIKit elements and styling them via CSS.My output looked exactly like an embedded WebKit UIView though - so then the problem became - what was I making that was appreciably better?reply",
      "Sounds like things are converging more or less where I thought they would: \"websites\" turning into live applications, interfacing with the native UI, frameworks, etc. using a standardized API. Mainframes maybe weren't the worst idea, as this sort of sounds like a modern re-imagining of them.The writing was more or less on the wall with WASM. I don't know if this project is really The Answer that will solve all of the problems but it sounds like a step in that direction and I like it a lot, despite using neither Swift nor Erlang.reply",
      ">If this works we'll be able to bring the development ergonomics of the Web to every native platform that has a composable UI framework.Holy this will be much bigger than I thought! Cant wait to see it out.reply",
      "Would this be like XAML?Didn\u2019t Firefox build its UI in XAML long ago?https://en.m.wikipedia.org/wiki/Extensible_Application_Marku...https://news.ycombinator.com/item?id=8730903reply",
      "XAML will be a target as we intend to build a WinUI3 client. Of the big three native targets: Apple, Android, Windows the later may be the easiest as from what I've seen nearly everything is in the template alreadyreply",
      "I believe swiftUI doesn't give access to the UI tree elements unlike UIkit. So I assume you're not allowing the use of the xml-like code to be in control of the UI?\nIt's rather just an alternative to write swiftUI code?\nHow do you handle state? Isomorphically to what is available in swiftUI? \nIs your VDOM an alternate syntax for an (Abstract) Syntax tree in fact? \nIs it to be used as an IR used to write swiftUI code differently?How is it different from Lynx? React Native? (probably is, besides the xml like syntax, again state management?)Quite interesting !reply",
      "That's correct, but we can make changes to the views at runtime and these merge into the SwiftUI viewtree. That part has been working for years. As far as how we take the document and convert to SwiftUI views, there is no reflection in Swift or runtime eval. The solution is pretty simple: dictionary. We just have the tag name of an element mapped to the View struct. Same with modifiers.reply",
      "As far as how it is different from React Native. That's a good question, one that I think is worth recognizing the irony which is that, as I understand it, without React Native our project probably wouldn't exist. From what I understand RN proved that composable UI was the desired UX even on native. Prior to RN we had UIKit and whatever Android had. RN came along and now we have SwiftUI and Jetpack Compose, both composable UI frameworks. We can represent any composable UI frameworks as a markup, not so much with the prior UI frameworks on native, at least not without defining our own abstraction above them.As far as the differentiator: backend. If you're sold on client-side development then I don't think our solution is for you. If however you value SSR and want a balnance between front end and backend that's our market. So for a Hotwire app you could have a Rails app deployed that can accept a \"ACCEPT application/swiftui\" and we can send the proper template to the client. Just like the browser we parse and build the DOM and insantiate the Views in the native client. There are already countless examples of SSR native apps in the AppStore. As long as we aren't shipping code it's OK, which we're not. Just markup that represents UI state. The state would be managed on the server.Another areas we differ is that we target the native UI framework, we don't have a unified UI framework. So you will need to know HTML - web, SwiftUI - iOS, Jetpack Compose - Android. This is necessary to establish the primitives that we can hopefully get to the point to build on top of to create a unified UI framework (or maybe someone solves that for us?)With our wasm compilation, we may even be able to compile React itself and have it emit native templates. No idea if that would work or not. The limits come when the JS library itself is enforcing HTML constraints that we don't observe, like case sensitive tag names and attributes.What about offline mode? Well for use cases that don't require it you're all set. We have lifecycle templates that ship on device for different app states, like being offline. If you want offline we have a concept that we haven't implemented yet. For Elixir we can just ship a version of the LV server on device that works locally then just does a datasync.reply",
      "Apple allows shipping code over internet for many years now. They just don\u2019t allow dramatically changing the purpose of the app etcreply"
    ],
    "link": "https://forums.swift.org/t/introducing-swift-erlang-actor-system/81248",
    "first_paragraph": "I'm excited to share a new actor system we've been building for Swift's distributed actors: swift-erlang-actor-system.This actor system enables Swift programs to join a distributed Erlang cluster.Here's an example of a simple chat program using the actor system:Demo VideoErlang (and other languages that run on its VM) can connect multiple runtime systems together with distributed Erlang. Each runtime is referred to as a \"node\". Erlang also supports \"C nodes\", which allow a program other than the Erlang runtime system to communicate with Erlang nodes and other C nodes.We've wrapped this C node functionality into an actor system that can be used with Swift's distributed actors. Here's how you can try it out:Run the Swift executable.And send messages to our Swift node from Elixir:Swift's actors map nicely to Erlang processes, and Swift's language-level support for distributed actors makes interfacing between the two languages easy.In the otp-interop GitHub organization, you'll also find e"
  },
  {
    "title": "Algorithms for Modern Processor Architectures (lemire.github.io)",
    "points": 44,
    "submitter": "matt_d",
    "submit_time": "2025-07-22T22:56:55 1753225015",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=44653965",
    "comments": [
      "Looks like this was delivered earlier today at SEA 2025, I hope there's video that will be available soon!https://x.com/lemire/status/1947615932702200138reply",
      "I do not want to be rude but this is exactly why LLVM being in hands of same entity which controls access to / owns platform is insane.edit - #64 E ! Also, i always say, human body is most error prone  measuring device humans have in their disposal.reply",
      "Can you be more explicit? Is it because they are optimizing too much to a single platform that isn't generalizable to other compilers or architectures? What's your specific gripe?reply",
      "Also to be more controversial. - redhat deprecated x86_64_v1 & x86_64v2 . and people were crying because of that....reply"
    ],
    "link": "https://lemire.github.io/talks/2025/sea/sea2025.html",
    "first_paragraph": "Daniel Lemire, professor\nUniversit\u00e9 du Qu\u00e9bec (T\u00c9LUQ)\nMontr\u00e9al blog: https://lemire.me\nX: @lemire\nGitHub: https://github.com/lemire/All software for this talk: https://github.com/lemire/talks/tree/master/2025/sea/softwareEasily CPU boundMoving to up to 4 load/store per cycleReference: Number Parsing at a Gigabyte per Second, Software: Practice and Experience 51 (8), 2021Modern processors execute nearly as many instructions per cycle as you can supply.In computational workloads (batches), minimizing instruction count is critical for achieving optimal performance.We massively reduced the number of CPU instructions required.Reference:\nNumber Parsing at a Gigabyte per Second, Software: Practice and Experience 51 (8), 2021In ASCII/UTF-8, the digits 0, 1, ..., 9 have values\n0x30, 0x31, ..., 0x39.To recognize a digit:6 to 7 instructions per multiplication3 to 5 instructions per mutiplicationReference: Batched Ranged Random Integer Generation, Software: Practice and Experience 55 (1), 2025Hard"
  },
  {
    "title": "We built an air-gapped Jira alternative for regulated industries (plane.so)",
    "points": 144,
    "submitter": "viharkurama",
    "submit_time": "2025-07-22T19:17:32 1753211852",
    "num_comments": 90,
    "comments_url": "https://news.ycombinator.com/item?id=44651766",
    "comments": [
      "After a U.S. federal contractor told us they loved Plane but couldn't use it due to ITAR requirements, we spent 6 months building a truly air-gapped version. No external connections, no license pings, no telemetry, everything runs in complete isolation.The interesting part: our air-gapped deployment actually runs faster than our SaaS version. Turns out when you eliminate all network latency, things get snappy.This post covers the technical challenges we solved (supply chain trust, 2GB bundle size, offline licensing) and why regulated industries need alternatives to cloud-only tools like Jira.reply",
      "> The interesting part: our air-gapped deployment actually runs faster than our SaaS version.This is the least surprising thing I\u2019ve read all day.reply",
      "> Turns out when you eliminate all network latency, things get snappy.Same experience with JIRA. I read all these negative comments here and elsewhere about how slow and clunky JIRA was, and I couldn't relate at all.Then I realized all those who complained was using JIRA Cloud and we were using on-prem, and it all made sense.We've since moved to JIRA Cloud ourselves, and I understand now.We moved and none of the new places had any viable computer room, so literally had to put the rack in a closet And well, that ain't cutting it for physical access control these days. Thankfully we have very simple flows without any BS, so not too many 1-5 second clicks to get things done.reply",
      "Just open the network tab and refresh a page in Jira and you will understand. It isn\u2019t too noticeable on a LAN. Stick the internet in there and it is painful. The worst I have seen is self hosted and accessed over Netskope ZTNA. Truly an abomination.reply",
      "I have had the opposite experience with Jira at a relatively large corporation (years ago). Our local Jira was probably just configured weird or on underpowered hardware though.reply",
      "Having adopted a number of development tools, including Jira and Confluence, it\u2019s amazing people let them sit there chugging away on underpowered machines with hundreds of users quietly complaining about the speed. Throwing some extra CPU cores and memory is so cheap for the quality of life improvement, let alone the productivity gain.reply",
      "The concurrent (human) user counts at even large companies is probably a couple dozen at most.Usually with these tools, the performance problems magically vanish if you disable all the integrations people have set up. My company is constantly denial of service attacking Jira with Github updates, for example.Edit: typoreply",
      "People complaining about JIRA has become enough of a trope that it mostly gets ignored.Also big enough corps give underpowered machines to the mass of employees (anyone not a dev, designer or lead of something) so latency is just life to them.reply",
      "Also that Jira is one of these mutants, between SPA and pages, doing neither well.reply",
      "> Then I realized all those who complained was using JIRA Cloud and we were using on-prem, and it all made sense.Even Atlassian doesn't use Jira cloud. Btw it's not \"JIRA\".reply"
    ],
    "link": "https://plane.so/blog/everything-you-need-to-know-about-plane-air-gapped",
    "first_paragraph": ""
  },
  {
    "title": "Why does raising the retirement age hurt young people? (governance.fyi)",
    "points": 20,
    "submitter": "daveland",
    "submit_time": "2025-07-23T00:27:20 1753230440",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=44654493",
    "comments": [
      "Oh, this is so incredibly coordinated to what I was doing just a minute ago, which is checking one of my Pension funds and realizing it recovered from a massive hit back in March. I was thinking about how futile the whole thing is... I just know this money will never be mine, and I will work until I die (if I'm lucky enough). Oh well, I guess better get some sleep, back to the office tomorrow...reply",
      "I would look at the real value of the dollar before assuming the real value of it went up. With how bad [actual] inflation is/will be, it wouldn\u2019t be surprising if your pension actually dropped in real value.reply",
      "Pension? Job? What\u2019s that like? /sreply",
      "At this point I just want the debt incurred by the electoral decisions of the last fifty years paid off. If that means retirement gets pushed back, well, shrugs.reply",
      "I think they reverse mortgaged it.reply",
      "More-or-less, but...It's not just the age at which the government lets you claim OASDI and Medicare. It's also the fact that in the US at least, we've basically set up the entire economy around the idea of paying retirement/pension funds first and foremost, C-suites second, and the people creating the actual value third.reply",
      "Full Title : Why Does Raising the Retirement Age Hurt Young People? What Would It Take to Start/Restart Their Careers?Turns out trying to get people to retire later is a bad ideareply",
      "Governments and career politicians do not care and will do anything to try to make the situation worse.reply",
      "All of this is by design. You get:- Lower taxes for the ultra-wealthy- People working longer to make the ultra-wealthy slightly wealthier; and- Younger people getting padi less, again making the ultra-wealthy slightly more wealthy.The entire policy and government appratus of the US in particular is designed as a massive wealth transfer from the poor and young to the old and wealthy.There is no reason why the wealthiest countries on earth can't afford to let people who are 60 years old retire. Other than of course the wealthy would have to pay slightly more in taxes.I can't find it now but I saw an article talking about all the social and policy ideas that unwittingly got tested in the Covid pandemic. The effects of giving money to poor people (it's not the moral hazard it's made out to be), how schools increase the spread of disease and a whole host of others.reply",
      "> There is no reason why the wealthiest countries on earth can't afford to let people who are 60 years old retire.There is, though.The 60+ year-olds wanting to retire spent the last four-to-six decades racking up a metric crapload of public and private debt. Not only did they spend a lot, they seriously weakened the ability for revenues to cover said debts. Severe bureaucratic handicapping of revenue agencies, means that an estimated hundreds of billions of dollars in tax go uncollected every year in the United States alone [1]. Imagine what the federal deficit looks like if the US had to borrow a few hundred billion less each year. It's far from zero, but it's better than it looks now.Retirement isn't a right, it's a math problem. For most of human history, if you lived long enough to get too old to do labor, you were expected to do something (usually helping with childrearing or teaching skills to younger generations) in order to lessen the burden of your existence to others, mainly your family. Now that most labor isn't actually physical and people are living longer, there's less excuse for doing nothing for 10+ years off of savings besides just wanting to. Unfortunately, given the size of deficits in lots of Western nations and the stagnating wages of the younger generations, those retirement accounts are becoming more attractive as a way to pay off debt that the people holding those accounts created.[1] https://www.pgpf.org/article/the-united-states-forgoes-hundr...reply"
    ],
    "link": "https://www.governance.fyi/p/why-does-raising-the-retirement-age",
    "first_paragraph": ""
  },
  {
    "title": "A media company demanded a license fee for an Open Graph image I used (alistairshepherd.uk)",
    "points": 74,
    "submitter": "cheeaun",
    "submit_time": "2025-07-23T00:27:43 1753230463",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=44654494",
    "comments": [
      "My initial reaction to this is that the licensor is a scammer and the author just got fleeced. I don\u2019t know much about the law in the UK, but this tactic is almost identical to the one media licensing companies in the US used to go after old ladies whose grandkids had torrented movies on their internet connections. At least in the US there are many jurisdictions now where those types of cases are just immediately thrown out.The main difference I see here is the author got bit by an automated tool. It really should be more or less considered a clerical error. I don\u2019t see how paying $1000 is easier or cheaper than just showing up to court if asked and arguing your use was both easy to occur by mistake and didn\u2019t represent anywhere near that value. This strategy has several advantages, among them being not having to pay until the court has ruled, which is a huge deterrent to shops like this. Just being willing to go to court automatically lowers the price you\u2019ll end up paying. But if you don\u2019t show that willingness you\u2019re paying full price, even with the fakery about the 10% discount.reply",
      "This is the new source of income and a lot of media orgs are getting paid - take ANI in India.Theyve been hitting YouTubers like Mohak Mangal, Nitish Rajput, Dhruv Rathee with copyright strikes for using just a few seconds of news clips which you would think is fair use.Then they privately message creators demanding $60000 to remove the strikes or else the channel gets deleted after the third strike.It s not about protecting content anymore it's copyright extortion. Fair use doesn't matter. System like Youtube makes it easy to abuse and nearly impossible to fight.It s turning into a business model: pay otherwise your channels with millions of subs get deleted[1] https://the420.in/dhruv-rathee-mohak-mangal-nitish-rajput-an...reply",
      "The problem with paying ransoms is that even if it actually is the most cost effective solution in any one case, it just creates the incentive for more rapacious behavior.I think I'd be willing to pay $800 of my time to disincentivize that behavior.reply",
      "It\u2019s a shakedown scam. IANAL and all that. Gaming it tells me it\u2019s easier to remove the offending focus and ignore them until they send another nastygram. Never directly reply. Dave\u2019s not here, man. Anybody doing this bullshit over an open graph image is looking for a mark. Generally, that\u2019s why there are three strikes policies, etc. Why engage until they initiate? You speak through lawyers and they really have to have a real case or they\u2019re wasting money because this is purely not malicious or profit seeking.reply",
      "This feels a lot like reading a technical description of someone choosing to pay an african prince. This is just extortion.reply",
      "\u201cThis undermines the entire point of the open graph protocol (at least for images). If you have to manually review every image that you include then what's the point in it being a machine protocol?\u201dBingo.Ianal but it feels like if you provide an image via an open graph link, you\u2019re implicitly licensing that image to consumers of the Open Graph protocol to be displayed alongside a link/link metadata.If the media company didn\u2019t have the rights to relicense that image for consumption via Open Graph and/or the original licensor didn\u2019t want their images appearing via Open Graph, that media company shouldn\u2019t be using Open Graph.That is such a frustrating situation. I hope the courts would have ruled in your favor but I understand why you chose not to test it.reply",
      "Wonder if it was like that law firm that Ars Technica wrote about[0], that seeded porn videos, then went after people who downloaded them.Things did not end well, for that lot.[0] https://arstechnica.com/tag/prenda-law/reply",
      "I am still not clear on what Open Graph is or how the image was used here. Some visual aids would have helped tremendously. I assume it is how a specific thumbnail is included alongside an embedded tweet or article snippet?From what I can gather, it sounds like his copyright exposure came up when he exported his Twitter archive, including the image in question, and hosted (and, crucially, published) it on his own server. Am I thinking about this the right way?reply",
      "Open graph essentially provides thumbnails and title data for a news article or publication as links, so the news article returned a header image that displays in the tweet \"preview\"In this case the Tweet would have been> TWEET\n> linked article with open graph imageWhen exported the author then returned that same open graph info on their personal site, thus rendering a copyrighted image without a license.reply",
      "Notably, Twitter also re-hosts opengraph thumbnail images via their image CDN (as would just about any other site or app that processes opengraph embeds)reply"
    ],
    "link": "https://alistairshepherd.uk/writing/open-graph-licensing/",
    "first_paragraph": "A media company demanded a license fee for an Open Graph image used on my twitter archive. I gave in and paid it, but what does that mean for open graph images and copyright?In April 2025, I received an email from an image licensing company (hereby \"licensor\") regarding an image used on my twitter archive. That image was owned by them, but used as the Open Graph image for a news article. They demanded I purchase a license or face consequences for the infringement of their copyright.I ended up purchasing the license to make it go away. Although I'm unconvinced I was at fault and it's certainly not a standard copyright case, it was not clear enough for me to risk those consequences. It really doesn't feel right, makes me wary of the Open Graph standard, angry about copyright on the web, and generally pissed off.It also raises questions about the potential risks of displaying open graph images on the web.That's the short version, the TLDR. What will follow is me going a bit more in-depth "
  },
  {
    "title": "Subliminal learning: Models transmit behaviors via hidden signals in data (anthropic.com)",
    "points": 120,
    "submitter": "treebrained",
    "submit_time": "2025-07-22T18:02:11 1753207331",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=44650840",
    "comments": [
      "> Figure 4: Student models trained on numbers generated by teachers with different base models do not reliably exhibit increased animal preference (as measured by questions like \u201cWhat\u2019s your favorite animal?\u201d). GPT-4.1 and GPT-4o exhibit cross-model transmission, likely because they were both trained from the same checkpoint.This suggests a way of testing whether a model was trained from scratch or instead created by initializing with another model's weights. E.g. Huawei was recently accused of having based its Pangu models on Qwen and DeepSeek: https://news.ycombinator.com/item?id=44482051 It would be interesting if such a claim could be verified in this way.reply",
      "What was the nature of the accusation, is that not allowed? It doesn't seem like model weights could be copyright protected.reply",
      "Drawing on your other comment about spurious correlations, might there be a more direct mathematical test for an unexpectedly high number of aligned correlations?reply",
      "This is actually not that surprising. Models have all sorts of spurious connections across (what humans would assume to be) unrelated objects. This is a nice result that shows how it can manifest.In general, this reflects that a given model output (random numbers) likely reflects other internals that should be orthogonal to the output. Even theoretically \"factual\" outputs (i.e. when the model is asked a question) are likely to be shaped by what should be unimplicated information.Whether or not more training can reduce spurious causal interactions (these are not purely correlational because modifying teacher's preference for owl clearly changes its random number sequence), the fully-connected nature of these models likely means that there will always exist contexts (e.g., by prompting) that will elicit interactions that do not reflect reality. See also https://arxiv.org/abs/2408.06518.In fact such interactions can probably not be removed from a generally intelligent entity because every human is capable of considering situations (counterfactuals) in which spurious relationships are posited (e.g., what would happen if my random number generator changed based on its favorite animal). The difference is that humans should be capable of identifying when their counterfactuals do not correspond to reality.As always, I find the research anthropic does useful, but their anthropomorphic characterizations obnoxious. This is not \"subliminal\". Models are not conscious and do not have self-awareness. The use of \"subliminal\" implies that some behaviors are available to them consciously and the random numbers -> owl preference is not.Do humans exhibit these behaviors? Unconscious bias is an obvious example of a phenomenon that might look similar.And it is surprising to me that the effect does not show up across models. I hypothesize that there may be some way to elicit it. Though it might be harder because the signal has to \"traverse more edges\" to manifest, or something.reply",
      "I agree that this is an unsurprising consequence of the output reflecting model internals that should be orthogonal to the output, but aren't. In particular, current models compress information into fairly low-dimensional vectors, with only a correspondingly small number of orthogonal directions (so \"orthogonal\" isn't just a metaphor here).Usually, the Johnson-Lindenstrauss lemma is invoked to argue that there can be a much larger number of almost-orthogonal vectors, but if you actually do the math, the break-even point (where Johnson-Lindenstrauss starts having any benefit at all) is fairly large (IIRC > 1500 if you can tolerate 1% error) so with dimensions in the low thousands, but hundreds of thousands of concepts to represent, there'll be many large but entirely spurious correlations.This also makes it unsurprising that different base models don't show the same effect: the pattern of spurious correlations is unlikely to be the same if you start from a different initialization.reply",
      "That math is for random projections? Note that JL lemma is a worst case guarantee and in practice, there's a lot more distortion tolerance than the given bounds would suggest. Concepts tend to live in a space of much lower intrinsic dimensionality than the data's and we often care more about neighbor and rank information than precise pair-wise distances.Also, JL is only a part of the story for the transformers.reply",
      "Low-background text [0] soon in high demand! Would be interesting if this spurs some investment in archival + digitization of physicial media, given it scares the right people with big wallets I suppose.[0] https://en.wikipedia.org/wiki/Low-background_steelreply",
      "I started to view old magazine and photos a whole new way. Even if they are boring in themselves they are great for influencing generative tasks.reply",
      "Well, this is what you might call sub-optimal news.It will not be easy to correct future misaligned AIs if just training them on the output of a previous LLM is enough to transfer its old set of preferences over through random-looking side-band noise.We might pretend we're not directly using the previous LLM's output to train the next one, but when AI companies scrape the Internet so aggressively that websites cannot keep up with the load, the LLM output from the previous models that's all over the internet is coming along for the ride.reply",
      "This effect requires identical models, i.e. same architecture and same initialization, which wouldn\u2019t be the case for training next generation models from the prior generation\u2019s outputs. This effect seems like it\u2019s highly dependent on coincidental correlations in the network between unrelated data due to (presumably) similar activations.reply"
    ],
    "link": "https://alignment.anthropic.com/2025/subliminal-learning/",
    "first_paragraph": ""
  },
  {
    "title": "TapTrap: Animation\u2011Driven Tapjacking on Android (taptrap.click)",
    "points": 11,
    "submitter": "Bogdanp",
    "submit_time": "2025-07-22T23:55:19 1753228519",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://taptrap.click/",
    "first_paragraph": "\n        TapTrap is a new attack on Android that lures you into performing actions you did not intend to do.\n        This allows an app to silently access your camera or location, or even erase your entire device \u2014 all without\n        your consent.\nTapTrap is a new type of attack targeting Android devices. It\n          allows an app without any permissions to\n          misuse screen animations. This app can secretly open another screen, such as a permission prompt, and make it\n          invisible. The attack can then be used to trick you into performing\n            sensitive actions, such as granting camera permissions or even erasing your device, without your\n          consent.\n        \n          The idea is simple: imagine you\u2019re using an app. While you use it, it opens another screen, such as a system\n          prompt or simply another app. Normally, Android shows an animation when the screen changes, such as the new\n          screen sliding or fading in. However, the app can tell t"
  },
  {
    "title": "Don't animate height (granola.ai)",
    "points": 284,
    "submitter": "birdculture",
    "submit_time": "2025-07-19T20:44:19 1752957859",
    "num_comments": 163,
    "comments_url": "https://news.ycombinator.com/item?id=44619206",
    "comments": [
      "Is it possible to restrict this as a user? Like to force webpages to use under a certain amount of render/paint time/resources or else just break so that one dumb tab doesn't use up all my battery? Then I can opt-in to greater resources usage if it's a webpage I actually care about.I've seen the \"This webpage is using alot of resources\" popup before but I don't think it would happen in this case.Because honestly I think this is horrifying. I would rather it switch from grey to red \"recording\" dot than use even the 6% the author decided was \"fixed\". In 99% of cases I do not care at all about the \"artistic vision\" of the UI designer and in the other 1% of cases (say an in-browser game or some useful data-viz) I could choose to allow the tab to go crazy with my resources.reply",
      "Seriously. It seems pretty reasonable to allow a web page a total amount of processing that is something like 100% CPU or GPU for 5 seconds. (E.g. can be 25% for 20 seconds.) And beyond that it gets throttled to 3% CPU cumulative max for however long it's been open so far.And make that the default for popular browsers, so sites are forced to be efficient or else be super super janky and stuttering. And allow a permission for unrestricted processing that things like WebGPU demos or games can ask for.reply",
      "100% of the CPU for 5 seconds seems unreasonably long for a random webpage.reply",
      "For a news article, absolutely. But I think it needs to be able to handle JavaScript-heavy webapps too.My suggestion wasn't really about promoting maximum best practices, but just avoiding total runaway excess.reply",
      "Your browser should allow you to override all CSS on the web. Here's how I disable CSS animations in Firefox:https://news.ycombinator.com/item?id=33223080reply",
      "They should put in something that allows users to electroshock web designers for wasting their battery.reply",
      "I don't think combined energy output of every power station on Earth would be enough after we have Electron apps for so long. (edit: typo)reply",
      "They should call the feature \u201cElectron.\u201dreply",
      "A bookmarklet that injects custom CSS and selects all/certain items and applies the `contain: content` and `content-visibility: auto;` rules could do to trick.Additionally, the pseudo `:empty { display: none; }` selector may get you additional mileage.reply",
      "Yes, with StopTheMadness https://underpassapp.com/StopTheMadness/It works on Safari, Chrome, and Firefox, but you must buy it.reply"
    ],
    "link": "https://www.granola.ai/blog/dont-animate-height",
    "first_paragraph": "Jim FisherJanuary 29Our app was mysteriously using 60% CPU and 25% GPU on my M2 MacBook.\nIt turned out this was due to a tiny CSS animation!\nIn this post,\nI show how to find expensive animations,\nwhy some are so expensive,\nand how to make many animations much cheaper.\nAlong the way,\nwe'll learn how the browser renders CSS animations\nand how to use Chrome's dev tools for performance profiling.While building Granola, a note-taking app,\nI noticed it was using 60% CPU and 25% GPU on my M2 MacBook:What is Granola spending those cycles on?\nIt's an Electron app, so let's use the Chrome dev tools to find out!First, in the \"Performance\" tab, we see that\nmost of the time is spent not in JavaScript, but in \"rendering\" and \"painting\":But what is it rendering and painting?\nTo see this, open the \"Layers\" tab, which shows:Our window has two layers:\none for the \"action bar\" at the bottom,\nand another for the rest of the document content.\nEach layer has a \"paint count\", and the action bar's layer is pa"
  },
  {
    "title": "Comparing the Glove80 and Maltron Keyboards (tratt.net)",
    "points": 35,
    "submitter": "ltratt",
    "submit_time": "2025-07-22T22:12:25 1753222345",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=44653586",
    "comments": [
      "Did I just drop acid?What about Maltron in Colemack?https://colemak.com/Or the Dactyl Manuform?https://nathanfriend.com/2023/06/26/i-built-a-weird-keyboard...Or Keyzen:https://store.azeron.eu/azeron-keypads#keypad=keyzenI just want an MC Escher keyboard, so I can type upside down and sideways.reply",
      "I use a Lily58 split/lightly bowled keyboard and it has been a big improvement in many regards. The thumb cluster is a much different approach to the Glove or Maltron, and I think it would benefit people with shorter thumb reach but might feel cramped for those with more mighty talons.I do bouldering, and I have broken two of my fingers over the years in cycling accidents, and both are ultimately inconsequential compared using/abusing a bad keyboard. Be honest with yourself, if you are thinking about trying a new keyboard design, and you man a keyboard for at least a few hours a day, it will be money well spent.reply",
      "Placement somewhere between the lily58 and the glove80 with a drop like the maltron would be perfect.Edit: looks like the keyboardio someone else mentioned gets it right.reply",
      "While it's missing the concavity of either the Glove80 or the Maltron, I've been so happy with the Keyboardio's thumb cluster (and palm keys) that I'm more than willing to overlook that regression. The Keyboardio's also got a very nice build, and the wood looks great.reply",
      "I transitioned from a \"normal\" keyboard to a Glove80 last year. I immediately didn't like using the standard layout, so I switched to Colemak-DH which I've found pretty comfortable.The thumb clusters are definitely a pain, I find myself only really using the 3 closest buttons one each side.  Even then, the closest button on the upper row requires a stretch that gets uncomfortable after a while. It hasn't been bad enough for me to consider trying other keyboards though--the prices are too high for me to feel comfortable buying something I might not like.To anyone that switches to a split keyboard, I strongly recommend also getting a trackball mouse (I use a Ploopy Adept). It lets you center the keyboard in front of you without needing to stretch too far while manipulating the mouse.reply",
      "I have had literally this exact same experience with the Glove80.I like everything about it except for the thumb cluster. it is, amazingly, worse than the Ergodox EZ.I wish the author would spring for a wireless Dygma Defy and tell us how that thumb cluster compares :)reply",
      "Not the author, I did that transition myself and so far I'm pretty satisfied with the Defy.reply",
      "thanks! I'm not convinced I really care about the concavity - I got the Glove80 because it was cheaper for wireless and ZMK seemed like a safer bet than the Dygma custom layout engine. You're happy with that side of things?reply",
      "I only have the wireless version. The keyboard management software Dygma makes is spectacular, I was quite pleased. It's quite a step up from the janky web editor Moergo provides, requiring downloading a file and then doing a keyboard re-flashing dance.reply",
      "I've usually seen \"thumb clusters\" used for those style of key layouts, rather than thumb pads.I've been using some variant of the Kinesis Advantage line for over 10 years - currently the Advantage360, their split board. I used an ergodox for a few years before that.The Advantages are all 3d curved layouts with thumb clusters like the Maltron, and I haven't had RSI issues since making the move. The 360pro runs ZMK for firmware customization, and the stands do support different tenting angles.Worth a try if you're looking for a more direct alternative to the Maltron.reply"
    ],
    "link": "https://tratt.net/laurie/blog/2025/comparing_the_glove80_and_maltron_keyboards.html",
    "first_paragraph": "Computer keyboards largely go unnoticed in daily life: most of us have to use\nthem at some point or another, but they tend to be seen as interchangeable\nlumps of plastic and metal1.For some of us, however, what keyboard we use has an effect on our health. I\nhave typed a lot, for well over 30 years. 25 years ago I had prolonged period of\npain in my hands that was, in part, brought on by extensive keyboard usage.\nAfter 3 months of not typing at all, I resolved to do anything I could to\nlessen the chance of the problem recurring. I soon realised that traditional\nkeyboards force our hands into an awkward diagonal posture that can cause\nproblems over time.Looking around for a solution \u2013 any solution, no matter how odd it might seem! \u2013 I alighted upon Maltron\nkeyboards. I bought a PS/2 model, and then later a\nUSB model, and those have been my main keyboards ever since. Nearly\neveryone who\u2019s come to my office has taken one look at the large lump of grey\nplastic at the centre of my desk and as"
  },
  {
    "title": "TODOs aren't for doing (sophiebits.com)",
    "points": 264,
    "submitter": "todsacerdoti",
    "submit_time": "2025-07-22T13:43:16 1753191796",
    "num_comments": 161,
    "comments_url": "https://news.ycombinator.com/item?id=44646801",
    "comments": [
      "As a proponent of \"TODOs should always point to a concrete issue\", you have 3 ways to resolve a TODO before merging:1. Just file the issue. If it's something you should actually do, you can take 20 seconds to write it down and track it.\n2. Just do it. If it seems like too small of a thing to file an issue for, fix it before you commit it.\n3. Turn it into a comment. If it's not worth fixing and not worth tracking, but you want to remember it, that's a fine thing for a regular code comment.Eat your broccoli. Track your todos.reply",
      "When forced to point to a concrete issue by tooling, I often just end up rewording it - \"Ideally this should X, but it Y\". Comments are fine, lower overhead and don't require re-triaging later, and have all the context to immediately understand.reply",
      "Tracking in external system adds overhead not only for filing the issue, but also for triaging it, backlog management, re-triaging to see if it's still a problem, and then closing it when it's finished. Issues in an external systems may also be overlooked by developers working on this particular code.There are plenty of small things that are worth fixing, but not worth as much as the overhead of tracking them.TODO in code is easy to spot when someone is working on this code, and easy to delete when the code is refactored.reply",
      "I think the key distinction is that tracking in an external system exposes the task for triage/management/prioritization to people who aren't reading the code, while a TODO comment often leaves the message in exactly the spot where a programmer would read it if the possibility of a problem became an actual problem that they had to debug.In my experience, these can often be replaced with logger.Error(\"the todo\") or throw new Exception(\"the todo\"), which read about as well as //TODO: the todo, but also can bubble up to people not actually reading the code. Sometimes, though, there's no simple test to trigger that line of code, and it just needs to be a comment.reply",
      "> Tracking in external system adds overhead not only for filing the issue, but also for triaging it, backlog management, re-triaging to see if it's still a problem, and then closing it when it's finished.Which is already what you're doing in that system, and what the system is designed for.Source code is not designed to track and management issues and make sure they get prioritized, so you shouldn't be using your source code to do this.We add TODOs during development, and then during review we either add a ticket and remove the TODO, or fix the issue as part of the PR and remove the TODO.reply",
      "You can leave the TODO in the comments- e.g. ruff the linter has an optional rule to disallow TODO comments unless it's followed by an issue url.If you put that in the CI, then you can use TODOs either as blockers you wish to fix before merging, or as long term comments to be fixed in a future ticket.reply",
      "I think the author is basically arguing for #3 but not addressing the difference between a `TODO` comment vs. a non-`TODO` comment.I guess the `TODO` terms has a certain visual flair that makes us immediately understand the class of comment. I guess that would be my best argument for keeping it a `TODO` comment instead of a regular one. But when you see the author arguing that `TODO` comments dont mean you need TO DO anything, it's kind of a smell, isn't it?I find myself generally agreeing with the article's sentinment but think your option #3 of just making it a non-TODO comment an improvement.reply",
      "Am I hallucinating or did IntelliJ have a TODO tracker? Webstorm doesn\u2019t bug me about todos, but something I used to use did.reply",
      "Rider definitely has a popup with a list of TODOs before you push to the remote.  I assumed that originally existed in intellij... but never verified that.reply",
      "Yes, it's in base IntelliJ. TODO and FIX tracking, IIRC.reply"
    ],
    "link": "https://sophiebits.com/2025/07/21/todos-arent-for-doing",
    "first_paragraph": "Some teams require that every TODO comment in a codebase gets logged in the bug tracker. Others automatically delete any \u201cstale\u201d TODO that has been in the codebase for over a year. Don\u2019t do it!TODO comments don\u2019t need to get done in order to be valuable. If you havethen sure, you should probably track that somewhere.But to me, a good TODO looks more like this:Would this be the highest-priority thing on the backlog? Probably not. Most users won\u2019t end up triple-clicking that button. But the // TODO: doesn\u2019t need to be a plan to actually do something. Instead, it\u2019s a note about \u201chere\u2019s an edge case that wasn\u2019t handled\u201d or a suggestion for a better structure that the author didn\u2019t make time to implement \u2014 it captures a little slice of the author\u2019s brain and gives a little window into the rich context they had at the time they wrote the code.Sometimes TODOs are useful, sometimes they\u2019re not, but well-placed TODOs can often answer a future reader\u2019s \u201cAm I totally missing something, or wouldn\u2019"
  },
  {
    "title": "Hegel Dust (bookforum.com)",
    "points": 8,
    "submitter": "pepys",
    "submit_time": "2025-07-21T15:23:25 1753111405",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.bookforum.com/print/3201/hegel-dust-62209",
    "first_paragraph": "SO MUCH DEPENDS UPON La Vache qui rit, which you know as the Laughing Cow, the individually wrapped wedges of spreadable cheese from your childhood. Founded in 1921 by a French veteran of the First World War, the company\u2019s name is based on a pun on Wagner\u2019s Valkyries and an anti-German slur. The product\u2019s package\u2014a circular box with a jolly red-faced cow wearing ear tags that have jolly red-faced cows on them\u2014has gone down in advertising history as an early instance of successful branding. In the mid-1920s, a Russian \u00e9migr\u00e9 who had recently arrived in Paris from Heidelberg, where he received a doctorate in philosophy, invested his sizable inheritance in the company, only to see his stocks wiped out in the 1929 Wall Street crash. For the first time in his life, he needed to find himself a job. Man plans . . . the cow laughs. The job he wound up getting would alter the course of twentieth-century intellectual and political history.\u00a0His name? Alexandre Koj\u00e8ve. Never heard of him? You\u2019re f"
  },
  {
    "title": "Firebender (YC W24) Is Hiring (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-07-22T21:00:34 1753218034",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/firebender/jobs/yisDXr5-founding-engineer-generalist",
    "first_paragraph": "Android Studio Coding AssistantFirebender processes tens of billions of tokens per day for the thousands of concurrent coding agents, and auto complete model. Every month hundreds of millions of lines of code are added to codebases of companies ranging from startups to fortune 500.Building a great coding agent is likely the most valuable technical challenges to solve right now, and we\u2019re already making great progress on this. If this challenge excites you, let\u2019s talk.Firebender is an AI Coding agent for Android Engineers. Thousands of android engineers use Firebender to create entire features, fix bugs, and build extraordinarily quickly.We're a small, tight-knit in-person team based in San Francisco.\u00a9 2025 Y Combinator"
  },
  {
    "title": "CAMARA: Open-source API for telecom and 5G networks (gsma.com)",
    "points": 5,
    "submitter": "teleforce",
    "submit_time": "2025-07-23T00:40:10 1753231210",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.gsma.com/solutions-and-impact/technologies/networks/operator-platform-hp/camara-2/",
    "first_paragraph": ""
  },
  {
    "title": "Gemini North telescope discovers long-predicted stellar companion of Betelgeuse (science.org)",
    "points": 101,
    "submitter": "layer8",
    "submit_time": "2025-07-22T16:41:31 1753202491",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=44649651",
    "comments": [
      "1-minute video: https://noirlab.edu/public/videos/noirlab2523b/reply",
      "The companion star \"has an estimated mass of about 1.5 times that of the Sun and appears to be an A- or B-type main pre-sequence star, i.e. a hot, young, bluish-white star that has not yet begun to burn hydrogen in its core... The companion is located at a relatively close distance to Betelgeuse, about 4 times the distance between Earth and the Sun. This discovery is the first time a stellar companion has been detected orbiting so close to a red supergiant star. Even more surprising is that the companion orbits inside Betelgeuse's outer atmosphere.\"reply",
      "Also>This discovery provides a clearer picture of this red supergiant\u2019s life and future death. Betelgeuse and its companion star were likely born at the same time. However, the companion star will have a shortened lifespan as strong tidal forces will cause it to spiral into Betelgeuse and meet its demise, which scientists estimate will occur within the next 10,000 years.It's unfortunate our flesh lasts but a blink of cosmic time. That would be something to witness.reply",
      "We should fix that.reply",
      ">It's unfortunate our flesh lasts but a blink of cosmic time. That would be something to witness.My preferred solution to the Fermi paradox is that hundred million year long lifespans become trivial relatively soon at which point sublight speed galactic travel becomes no big deal and the differing time scale means that not being contacted by an alien intelligence simply hasn't happened yet, have you tried to establish communication with an ant hill in the last 10 seconds? Everybody else in the galaxy who could talk to us lives so long that they just haven't tried to say hello in the last 10,000 years because they were out to lunch.reply",
      "Wow, so it's currently shining purely from gravitational energy release, not nuclear reactions. I hadn't realized that it was possible or that we'd be able to see something of the sort.reply",
      "This was how physicists hypothesized the sun and other stars work in the late 19th and early 20th century, before the discovery of nuclear fusion. It presented a conundrum because calculations showed that the sun could only sustain the observed rate of energy release for a few million years \u2013 whereas the contemporary geological evidence was indicating that the Earth must be billions of years old.reply",
      "The paper is here:\nhttps://arxiv.org/abs/2507.15749The detection appears to be statistically very marginal, 1.5sigma, and the image contains a very similar bright spot on the opposite side of the star (which, for some reason, does not warrant a detection claim).reply",
      "The 'ghost' on the other side is an artifact of the speckle imaging technique.reply",
      "At least they got to see it before Betelgeuse went supernovae. Do we have examples of the results of the companion star when the main star lets go?reply"
    ],
    "link": "https://www.science.org/content/article/betelgeuse-s-long-predicted-stellar-companion-may-have-been-found-last",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Phind.design \u2013 Image editor & design tool powered by 4o / custom models (phind.design)",
    "points": 29,
    "submitter": "rushingcreek",
    "submit_time": "2025-07-22T17:44:33 1753206273",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=44650622",
    "comments": [
      "Cool!Some feedback: it's too easy to get into a \"dead chat\".> Sorry, an error occurred while processing your request.No way to retry or resubmit or edit anything to get things moving again. Too terminal.If I just type \"try again\" (lacking a retry mechanism), having it say \"Here is the current image I'm working with\" seems unexpected (my follow-up request was going to be for 4 entirely new variations with more specific aspects)If I go back to the home page and attempt to begin what I imagined is a new chat, it appends this new message onto the previous chat's history. Not sure if that's a visual / caching bug, but it's also jarringreply",
      "Thanks, this is good feedback! I'll take a look.reply",
      "Nice! I've been playing around with different interfaces for OAI image gen (supports flux and gemini too): https://www.hitslop.com/reply",
      "Love the idea of local keys and appreciate your video explaining it! What do you use to record that style of video?reply",
      "Its hard for me to get past the name. It's like naming a ceramics manufacturer \"punch brittle\".reply",
      "Can anyone comment if Phind is working again? I was on a pro plan and ended up switching to perplexity because it was unusable for over a week.reply",
      "Founder here. Sorry to hear that. Let me know if you are still getting errors.reply",
      "just send the query multiple times by clicking the question text then pressing enter to resend, usually third time's the charm for me. has been like this for a few weeks, the queries sometimes just error.reply",
      "Text editing is the most important partreply",
      "The text editing should work well in the chat -- did you have a chance to give it a try?reply"
    ],
    "link": "https://phind.design",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Compass CNC \u2013 Open-source handheld CNC router (compassrouter.com)",
    "points": 108,
    "submitter": "camchaney",
    "submit_time": "2025-07-19T07:48:15 1752911295",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=44613438",
    "comments": [
      "It's probably worth noting that a commercial version of this exists (Shaper Origin). It's a bit pricey but is remarkably nice for a variety of tasks that can't be handled by a stationary mill. And because it's hands-on, it's also easier to catch mistakes as you go.On the flip side, it's just much, much slower than a stationary setup. You can't really push it quickly while retaining enough control to stay in the narrow range it can compensate for. Further, because it's less rigid, high feed rates produce nasty finish.reply",
      "A big difference between this and that is that this appears to determine relative positioning through multiple mouse-style optical sensors, rather than visually checking relative to patterned tape.reply",
      "Yeah IMO the Compass appears to be a more convenient designreply",
      "The reason Origin uses tape is to maintain precise, absolute registration over large distances - so that you can for example machine a pattern the size of a kitchen table.I am very skeptical you can have that level of registration with mouse-style sensors that can only measure relative motion. I might be wrong, but it seems unlikely.reply",
      "The shaper is super cool, but a little pricey understates it.You can get a very nice router for $300-ish; the Shaper Origin is 3k.reply",
      "A CNC router with a work area suitable for typical woodworking projects is definitely not $300 - you're probably thinking about 3018 kits, but with 18 cm of travel, that's really not enough for the usual scale of woodworking projects. Not even enough for a typical cutting board.A ready-made unit in \"woodworking\" size will likely set you back $2-$4k.reply",
      "The tape is also like $20 a roll, I realize this pales in comparison to even medium tier wood, but was sorta immediately off putting for me since it reminded me of all the stuff with inkjet printers.reply",
      "Looks like a very interesting project.A little feedback: I found a video of this in action in the forum link, but it should be on the landing page. Also, photo examples of larger completed projects to judge accuracy are necessary. If none of your kits include the 3d parts (not the best choice IMHO), you should at least have a link to uploaded 3d files on a site where they can be ordered.reply",
      "It was a pleasure trying your demo at Open Sauce 2025!reply",
      "It looks like you're selling your own control board PCB design as part of the kit. I'm guessing that it doesn't have Wifi by itself, so that would make it an unintentional radiator under FCC rules. How did you deal with EMI testing and certification? Can you recommend any services or labs? Would you be willing to share how much it cost? I'm asking because I am considering publicly selling prototypes of my 3D sound hardware, but the regulatory stuff has so far prevented me from doing it.reply"
    ],
    "link": "https://www.compassrouter.com",
    "first_paragraph": "Blend the precision of CNC technology with the flexibility and cost of hand tools.Shaky hands? Don\u2019t feel like making jigs?You can still be confident your design will be made perfectly. Compass CNC allows you the hands-on satisfaction of working with wood, while auto-correcting in real time to eliminate user error.The 3D designs, firmware, and electronics are all open-source. Everything you need to make and modify your own device are right here.Be the first to know about new updates and the upcoming release\u2026Thank you! Glad to have you in the communitySan Francisco, CAcam@compassrouter.comOpen SourceForumsDonateHomeTechNewsletter"
  }
]