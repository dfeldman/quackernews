[
  {
    "title": "Mr Browser \u2013 Macintosh Repository file downloader that runs directly on 68k Macs (macintoshrepository.org)",
    "points": 19,
    "submitter": "zdw",
    "submit_time": "2025-07-19T00:20:48 1752884448",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.macintoshrepository.org/44146-mr-browser",
    "first_paragraph": "\n What is MR Browser?MR Browser is a small utility app allowing very old Macs from the 90's that are too old to use a normal web browser, but are new enough to connect to the internet through TCP/IP, to access Macintosh Repository online services, e.g. to directly download files without the help of a modern computer.\u00a0 For now, it's limited to only listing files smaller than 1GB, considering it's only meant to run on mid 90's System 7 environments.\nNote: Before downloading files with MR Browser, to prevent crashes, make sure that the hard drive on which it resides has enough free space to account for your downloaded files.\nGOT SUGGESTIONS YOU'D LIKE IMPLEMENTED IN MR BROWSER? JUST ASK IN THE COMMENTS BOX BELOW! :)\n\u00a0\nTROUBLESHOOTING:\nIf you'd like to use your old Mac's web browser to download MR Browser, then point it to this download link:\u00a0http://macintoshrepository.org/files/direct/MR-Browser-68K-v0.36.bin (only use this link with your old Mac's web browser as modern ones will think th"
  },
  {
    "title": "Asynchrony is not concurrency (kristoff.it)",
    "points": 170,
    "submitter": "kristoff_it",
    "submit_time": "2025-07-18T19:21:38 1752866498",
    "num_comments": 115,
    "comments_url": "https://news.ycombinator.com/item?id=44608754",
    "comments": [
      "\"Asynchrony\" is a very bad word for this and we already have a very well-defined mathematical one: commutativity. Some operations are commutative (order does not matter: addition, multiplication, etc.), while others are non-commutative (order does matter: subtraction, division, etc.).    try io.asyncConcurrent(Server.accept, .{server, io});\n    io.async(Cient.connect, .{client, io});\n\nUsually, ordering of operations in code is indicated by the line number (first line happens before the second line, and so on), but I understand that this might fly out the window in async code. So, my gut tells me this would be better achieved with the (shudder) `.then(...)` paradigm. It sucks, but better the devil you know than the devil you don't.As written, `asyncConcurrent(...)` is confusing as shit, and unless you memorize this blog post, you'll have no idea what this code means. I get that Zig (like Rust, which I really like fwiw) is trying all kinds of new hipster things, but half the time they just end up being unintuitive and confusing. Either implement (async-based) commutativity/operation ordering somehow (like Rust's lifetimes maybe?) or just use what people are already used to.reply",
      "Strictly speaking commutativity is defined over (binary) operations - so if one were to say that two async statements (e.g. connect/accept) are commutative, I would have to ask, \"under what operation?\"Currently my best answer for this is the bind (>>=) operator (including, incidentally, one of its instances, `.then(...)`), but this is just fuzzy intuition if anything at all.reply",
      "`.then()` is ugly, `await` is pretty, but wouldn't the critical part to guarantee commutivity less than guaranteed order (in js) be the `Promise.all([])` part?reply",
      "It's a good intuition. This has been studied extensively, the composition rule that is lax enough to permit arbitrary effects but strict enough to guarantee this class of outcomes is (>>=). We can keep trying to cheat this as long as we want, but it's bind.reply",
      "Commutative operations (all of them I think?) are trivially generalized to n-ary operations (in fact, we do this via \u2211 and \u220f, in the case of addition and multiplication, respectively). You're right that the question of what \"operation\" we're dealing with here is a bit hazy; but I'd wager that it's probably in the family of the increment operation (N++ === N + 1 = 1 + N) since we're constantly evaluating the next line of code, like the head of a Turing machine.Edit: maybe it's actually implication? Since the previous line(s) logically imply the next. L_0 \u2192 L_1 \u2192 L_2 \u2192 L_n? Though this is non-commutative. Not sure, it's been a few years since my last metalogic class :Preply",
      "Generalizing an associative binary op to an n-ary op just requires an identity element Id (which isn't always obvious, e.g. Id_AND=true but Id_OR=false).reply",
      "> \"under what operation?\"You could treat the semicolon as an operator, and just like multiplication over matrices, it's only commutative for a subset of the general type.reply",
      "Right, exactly. It's been said that (>>=) is a programmable semicolon.[0] https://news.ycombinator.com/item?id=21715426reply",
      "Commutativity is a much weaker claim because one is totally before or after the other. e.g. AB may commute with C so ABC=CAB but it is not necessarily the case that this equals ACB. With asynchrony you are guaranteed ABC=ACB=CAB. (There may be an exisiting mathematical term for this but I don't know it)reply",
      "You can prove three-term commutativity from two-term (I did it years ago, I think it looked something like this[1]), so the ordering doesn't matter.[1] https://math.stackexchange.com/questions/785576/prove-the-co...reply"
    ],
    "link": "https://kristoff.it/blog/asynchrony-is-not-concurrency/",
    "first_paragraph": "\nJuly 18, 2025\n    \u2022\n    12\n    min read \u2022 by\n    Loris Cro\n\nThe title of this blog post is not something you hear people say often, if ever. What you do hear people say is \u201cconcurrency is not parallelism\u201d, but that\u2019s not as useful, in my opinion.Let\u2019s see how Wikipedia defines those terms:Concurrency refers to the ability of a system to execute multiple tasks through simultaneous execution or time-sharing (context switching)Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously.What if I told you we\u2019re missing a term to describe another aspect of concurrent programming and, because of it, we\u2019re all collectively missing a key piece of understanding that has shaped our software ecosystems for the worse?Well, I spoiled it in the title: the missing term is \u2018asynchrony\u2019, but why?Say that you have to save two files and order does not matter:A could be saved before B, or B could be saved before A, and all would be fine. You could al"
  },
  {
    "title": "How to write Rust in the Linux kernel: part 3 (lwn.net)",
    "points": 45,
    "submitter": "chmaynard",
    "submit_time": "2025-07-18T22:27:17 1752877637",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://lwn.net/SubscriberLink/1026694/3413f4b43c862629/",
    "first_paragraph": "\n\n\nWelcome to LWN.net\n\nThe following subscription-only content has been made available to you \nby an LWN subscriber.  Thousands of subscribers depend on LWN for the \nbest news from the Linux and free software communities.  If you enjoy this \narticle, please consider subscribing to LWN.  Thank you\nfor visiting LWN.net!\n\n\n\n\n\n\n           By Daroc AldenJuly 18, 2025\n           \nRust in the kernel\n\n\nThe interfaces between C and Rust in the kernel have grown over time; any\nnon-trivial Rust driver will use a number of these. Tasks like allocating memory,\ndealing with immovable structures, and interacting with locks are necessary for\nhandling most devices. There are also many subsystem-specific bindings, but the\nfocus of this third item in our series on writing Rust in the kernel\nwill be on an overview of the bindings that all kernel Rust code\ncan be expected to use.\n\n\nRust code can call C using the\n\nforeign function interface (FFI); given that, one\npotential way to integrate Rust into the ker"
  },
  {
    "title": "Silence Is a Commons by Ivan Illich (1983) (davidtinapple.com)",
    "points": 82,
    "submitter": "entaloneralie",
    "submit_time": "2025-07-18T21:21:50 1752873710",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=44609969",
    "comments": [
      "Why is Ivan Illich so underrated ?He predicted and theorized \nfree software 10 years before it happened in Tools for Conviviality, made the most obvious and needed critic of education and hospitals alone against the Zeitgeist, studied step by step a lot of field of society to find patterns to simplify understanding.He created simple concepts that everyone should know \u2014- counter productivity, vernacular, iaotrogenic, radical monopoly, conviviality, poverty vs. misery etc.He is much more pragmatic than all his leftists colleagues. He might not go very deep in economics but at least he\u2019s not a basic marxist. He might not go as deep as Jacques Ellul in his critics of technology, but at least he is very understandable, anyone can be inspired by his books. I read most of Illich writings at 19 years old and it stayed with me for yearsreply",
      "You might enjoy a newsletter called The Convivial Society, which is heavily influenced by Illich.I'm just starting Tools For Conviviality. I suspect that Illich's ideas are underrated because, at least today, most people want more and Illich does not offer that. He offers freedom, I think, in his definition of conviviality... but it seems to be quite clear that offered freedom or comfort, most of us today (I'm not excluding myself from this) prefer comfort.reply",
      "I agree with you. Is it perhaps because of his religious background (he was a Catholic priest)? For much of the last couple decades, there has been an anti-religious streak in the educational mainstream universities.reply",
      "Half agree.The other half, as a very conservative Catholic, conservative Catholics are neglecting our great teachers like Dorothy Day.reply",
      "Could that perhaps be a reaction to an anti-intellectualism streak in the mainstream religious narrative for the last couple decades?reply",
      "This was a whole cottage industry during the cold war, kind of like it is now that we're in another sort of cold war.The Soviets would fund anyone applying Marxist thought to this or that. There may be some interesting ideas for those willing to sort out the chaff, but for the most part you know exactly what they're going to say if you're already familiar with the propaganda that came before.reply",
      "Well, was it wrong what they said?reply",
      "Yes. In general that's why you resort to propaganda and polemics rather than giving a formal argument that can be disproved.reply",
      "The US funded massive amounts of \"pro-capitalist\", \"pro-liberal\" (etc.) propaganda, including stuff like funding modern art. I don't think we can evaluate scholarly claims or cultural production only by whether similar claims have sometimes been funded by interested parties, or even whether the direct author was being patronized. It's worth tracking the money as part of remaining critical, though.reply",
      "Computers could hardly do anything back then. Mostly backend data processing.Yet this speech could have been written today.Intriguing.reply"
    ],
    "link": "http://www.davidtinapple.com/illich/1983_silence_commons.html",
    "first_paragraph": "\nIvan Illich is doing to computers what he did to education (De-Schooling Society, 1971), to energy (Energy and Equity, 1974), to medicine (Medical Nemesis, 1975), and to sex roles (Vernacular Gender, 1983). Each time it has been radical analysis that changes our perception of what is really going on. Each time, and with growing clarity, it is an economic/historical analysis having to do with the idea of scarcity as a means of exploitation. This article is from Illich's remarks at the \"Asahi Symposium Science and Man - The computer-managed Society,\" Tokyo, Japan, March 21, 1982. The ideas here are part of a book Illich is working on, The History of Scarcity.\n\n\t\t\t\t\t\t\t\t\t- Stewart Brand \n\n\t\t\t\t\t\t\t\t\tThe CoEvolution Quarterly, Winter 1983\n\n\n"
  },
  {
    "title": "Debcraft \u2013 Easiest way to modify and build Debian packages (optimizedbyotto.com)",
    "points": 13,
    "submitter": "pabs3",
    "submit_time": "2025-07-19T00:04:31 1752883471",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://optimizedbyotto.com/post/debcraft-easy-debian-packaging/",
    "first_paragraph": "Search\n\nDebian packaging is notoriously hard. Far too many new contributors give up while trying, and many long-time contributors leave due to burnout from having to do too many thankless maintenance tasks. Some just skip testing their changes properly because it feels like too much toil.Debcraft is my attempt to solve this by automating all the boring stuff, and making it easier to learn the correct practices and helping new and old packagers better track changes in both source code and build artifacts.Unlike how rpm or apk packages are done, the deb package sources by design avoid having one massive procedural packaging recipe. Instead, the packaging is defined in multiple declarative files in the debian/ subdirectory. For example, instead of a script running install -m 755 bin/btop /usr/bin/btop there is a file debian/btop.install containing the line usr/bin/btop.This makes the overall system more robust and reliable, and allows, for example, extensive static analysis to find proble"
  },
  {
    "title": "Ccusage: A CLI tool for analyzing Claude Code usage from local JSONL files (github.com/ryoppippi)",
    "points": 27,
    "submitter": "kristianp",
    "submit_time": "2025-07-18T23:22:49 1752880969",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=44610925",
    "comments": [
      "Have been using this for awhile, I'm on the $100/mo Max plan and have been running $600-800/mo in terms of usage, and I'm hardly pushing it to the limits (missing lots of billing windows).It makes me wonder what Anthropic's true margins are. I could believe they are overcharging via the API, Sonnet is $3/$15/Mtok and Opus at an ABSURD $15/$75/Mtok. But to break even for me, that would mean that they're overcharging by a factor 5x-10x, which doesn't seem possible. Is the music going to stop for Claude Code the same way it did for Cursor? I have to imagine every incentive in the world is pushing them to lower inference cost rather than introduce stricter limits, and unlike Cursor they can actually can reach into their stack and do this. But I'm not sure they're capable of miracles.Regardless, I'm bullish Anthropic. Sonnet and Opus don't benchmark as well as O3/Grok4 at pure coding, and aren't as cheap as Kimi K2 for theoretically similar perf, but as any user knows they are top tier at instruction following, highly reliable and predictable, and have a certain intangible theory of mind that is unique to Anthropic.reply",
      "> but as any user knows they are top tier at instruction following, highly reliable and predictableThis is spot on. Reliability is really the #1 priority for me when it comes to coding agents, and Sonnet, and especially Opus, really deliver on it. It makes such a huge difference when it comes to agents. Anthropic really nailed it on this.My process has become: get Opus to generate a plan, use o3 to help me review the plan, and then get Opus to implement the plan. This works extremely well for me, and is the first time where I've felt AI being actually useful for coding anything more than small prototypes.reply",
      "How do you switch to o3 to review the plan?reply",
      "I have a workflow that tells Claude Code to generate a planning markdown document: https://gist.github.com/Sothatsit/c9fcbcb50445ebb6f367b0a6ca...reply",
      "Personally, I use Repoprompt for this + deeper context integration.reply",
      "Have CC output a plan.md file.reply",
      "What do you mean by music stopping for Cursor? Almost every single developer I run into is transitioning/transitioned to it today. It's stinks like the new VS Code.reply",
      "Their pricing change recently has people reaching for alternatives.reply",
      "There's a predictable journey: people start with Cursor when they are new to AI, and quickly move on to something more powerful.reply",
      "> Sonnet and Opus don't benchmark as well as O3/Grok4 at pure codingDo any of the others have a \"claude code\" local agent?  Seems like a big gap IMO.  Though, it should be pretty easy for them to close that gap.I don't usually take too many moral stances but I feel like I can't use Grok.  It's bad enough Musk did his Nazi salute but his AI product itself is a Nazi too?  It might be good at coding but I really can't stomach using it.reply"
    ],
    "link": "https://github.com/ryoppippi/ccusage",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n        A CLI tool for analyzing Claude Code usage from local JSONL files.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\n\n\n\n\nAnalyze your Claude Code token usage and costs from local JSONL files \u2014 incredibly fast and informative!Thanks to ccusage's incredibly small bundle size (), you can run it directly without installation:\ud83d\udca1 Tip: We recommend using bunx instead of npx for a massive speed improvement!Since ccusage has such a small bundle size, installation is entirely optional:Full documentation is available at ccusage.comCheck out these 47 Claude Code ProTips from Greg Baugues.\n\n\n\n\n\n\n\nMIT \u00a9 @ryoppippi\n        A CLI tool for analyzing Claude Cod"
  },
  {
    "title": "C++: zero-cost static initialization (cofault.com)",
    "points": 13,
    "submitter": "oecumena",
    "submit_time": "2025-07-15T11:39:18 1752579558",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=44570113",
    "comments": [
      "That's a nice trick, but contrary to function statics, it is susceptible to SIOF.\nThis kind of optimization is useful only on extraordinarily hot paths, so I wouldn't generally recommend it.> On ARM, such atomic load incurs a memory barrier---a fairly expensive operation.Not quite, it is just a load-acquire, which is almost as cheap as a normal load. And on x86 there's no difference.One thing where both GCC and Clang seem to be quite bad at is code layout: even in the example in the article, the slow path is largely inlined. It would be much better to have just a load, a compare, and a jump to the slow path in a cold section.\nIn my experience, in some rare cases reimplementing the lazy initialization explicitly (especially when it's possible to use a sentinel value, thus doing a single load for both value and guard) did produce a noticeable win.reply",
      "TIL about encapsulation symbols.Why not just use constinit (iff applicable), construct_at, or lessen the cost with -fno-threadsafe-statics?reply",
      ">Even after the static variable has been initialised, the overhead of accessing it is still considerable: a function call to __cxa_guard_acquire(), plus atomic_load_explicit(&__b_guard, memory_order::acquire) in __cxa_guard_acquire().No. The lock calls are only done during initialization, in case two threads run the initialization concurrently while the guard variable is 0. Once the variable is initialized, this will always be skipped by \"je      .L3\".reply",
      "> For this we need a certain old, but little-known feature of UNIX linkersSTOP WRITING NON-PORTABLE CODE YOU BASTARDS.The correct answer is, as always, \u201cstop using mutable global variables you bastard\u201d.Signed: someone who is endlessly annoyed with people who incorrectly think Unix is the only platform their code will run on. Write standard C/C++ that doesn\u2019t rely on obscure tricks. Your co-workers will hate you less.reply"
    ],
    "link": "https://cofault.com/zero-cost-static.html",
    "first_paragraph": " \"\u0423\u0441\u0435\u0440\u0434\u0438\u0435 \u0432\u0441\u0435 \u043f\u0440\u0435\u0432\u043e\u0437\u043c\u043e\u0433\u0430\u0435\u0442!\" \u041a. \u041f\u0440\u0443\u0442\u043a\u043e\u0432, \u041c\u044b\u0441\u043b\u0438 \u0438 \u0430\u0444\u043e\u0440\u0438\u0437\u043c\u044b, I, 84In C and C++ a static variable can be defined in a function scope:Technically, this defines counter as an object of static storage duration that is allocated not within the function activation frame (which is typically on the stack, but can be on the heap for a coroutine), but as a global object. This is often used to shift computational cost out of the hot path, by precomputing some state and storing it in a static object.When exactly a static object is initialised?For C this question is vacuous, because the initialiser must be a compile-time constant, so the actual value of the static object is embedded in the compiled binary and is always valid.C++ has a bizarrely complicated taxonomy of initialisations. There is static initialisation, which roughly corresponds to C initialisation, subdivided into constant-initialisation and zero-initialisation. Then there is dynamic initialisation, further divided into unordered, parti"
  },
  {
    "title": "Valve confirms credit card companies pressured it to delist certain adult games (pcgamer.com)",
    "points": 194,
    "submitter": "freedomben",
    "submit_time": "2025-07-18T15:54:46 1752854086",
    "num_comments": 205,
    "comments_url": "https://news.ycombinator.com/item?id=44606184",
    "comments": [
      "Why do payment processors do stuff like this? Is there some regulation that requires them to? I get that they don't want to process fraudulent transactions, but I'd think the response to a higher percentage of fraud from some industry would be to charge them more. It doesn't make sense to me why they would be concerned about the content of games, as long as everything is legal and the parties concerned aren't subject to sanctions.Some of these games seem completely abhorrent, and probably illegal in more restrictive jurisdictions, but not the United States. And I've not seen any suggestion they're funding terrorism or something. So I'm perplexed.reply",
      "One factor is the ongoing campaigns from number of moral crusading groups who lobby them to cut off payment processing for things they don't approve of. NCOSE has been working for decades on the project, and targeting credit card companies has been a successful tactic for them for a decade or so.[1] https://www.eff.org/deeplinks/2020/12/visa-and-mastercard-ar...[2] https://www.newsweek.com/why-visa-mastercard-being-blamed-on...[3] https://scholarworks.iu.edu/dspace/bitstreams/761eb6c3-9377-...reply",
      "Another factor is the board members and other investors of the institutions themselves.I have been privy to two specific instances where pressure to either ban or reject providing support for specific content was handed down from beyond the executive level at a major financial network player that my client was doing business with.reply",
      "My guess is it's simply a chargeback risk. It's the reason casinos and adult sites have trouble getting credit card processing and are charged much higher basic rates, even under the best of circumstances when the casino or adult site is operating entirely within the law in the jurisdictions it allows.Punters run a lot of chargebacks on casinos, and people whose spouses catch a XXX video or game on their card statement will lie and run chargebacks too.In the case of Valve, a lot of chargebacks would drastically increase the processing rates demanded by the payment providers for all transactions across the board, not just those related to adult games.There's probably a great market opportunity here for a game store focused on adult games and willing to take on that risk.reply",
      "That's the problem though. The risk means the market for those riskier credit transactions is literally categorically not a great market. You think JP Morgan gives a shit about Japanese titty games? Hah. No. They care that these games get charged back way more often.If there is a market opportunity, it's probably in a processor for debit-based transactions that are harder to reverse. But then that makes fraud harder to combat, and one of the reasons everyone loves credit cards so much is because consumers are far more confident to buy from random shops if they know they can always get their money back if the shop scams them.So - this whole system's lucratively is entirely predicated on easy credit and low risk meaning low fees. Anyone who wants to play in the mud that's leftover by these companies taking the good business are inherently playing a low margin risky game.reply",
      "The USA is extremely litigious, rules are decided not by the legislature usually but instead by people suing each other to establish case law, and anyone with a bone to pick could sink you in legal fees and proceedings at a whim. So probably people who don\u2019t like the idea of adult content can use the courts to make payment processors\u2019 lives painful and they decide to just forgo that business.US courts are too easy to use as a tool of abuse.reply",
      "At least at one time, part of the answer would have been Operation Choke Point: https://en.wikipedia.org/wiki/Operation_Choke_PointHowever, that's clearly not all that's going on -- it doesn't seem like the government is still doing this.reply",
      "If I remember correct from the hot money podcast https://www.ft.com/content/762e4648-06d7-4abd-8d1e-ccefb74b3... part of the problem for the credit card companies is figuring what are the boundaries of legality. Countries have very different laws. Things like representing homosexuality or age of consent are very different and credit cards feel that it is a risky business because of that.reply",
      "This makes little sense with even the tiniest amount of probing.This is a solvable geo regulation issue, solvable like many other geo regulatory issuesreply",
      "I suspect Valve is blaming the credit card companies for something they really wanted for themselves. Steam is a big store open to everyone and you\u2019re going to scare away a big chunk of seniors, Christians, etc with stuff like incest, ageplay, and rape just so that a small minority uses you instead of\u2026itch.io? Better to keep the big safe names like Being a Dik and Eternum on Steam and flush the rest so that you can have the best of both worlds.reply"
    ],
    "link": "https://www.pcgamer.com/software/platforms/valve-confirms-credit-card-companies-pressured-it-to-delist-certain-adult-games-from-steam/",
    "first_paragraph": "It's not a great precedent, that's for sure.\nWhen you purchase through links on our site, we may earn an affiliate commission. Here\u2019s how it works.\nIt's Mastercard's world; we just live in it. That's my understanding based on a recent communiqu\u00e9 from Valve to PC Gamer, which confirmed that, yup, the company sure did recently remove a whole spate of adult games from its storefront because it made payment processors upset.\"We were recently notified that certain games on Steam may violate the rules and standards set forth by our payment processors and their related card networks and banks,\" said Valve. \"As a result, we are retiring those games from being sold on the Steam Store.\"Valve's reaching out to devs impacted by the change \"and issuing app credits should they have another game they\u2019d like to distribute on Steam in the future.\" Just, you know, so long as those games get the seal of approval from Valve's payment processors, I suppose.To be fair to the house that Gabe Newell built, I "
  },
  {
    "title": "Shutting Down Clear Linux OS (clearlinux.org)",
    "points": 82,
    "submitter": "todsacerdoti",
    "submit_time": "2025-07-18T23:45:12 1752882312",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=44611098",
    "comments": [
      "For background: This was Intel's distro and it's likely that most/all of the folks that were maintaining it are a part of the 5,000 layoffs just announced, bringing the total Intel layoffs to 20,000 people.reply",
      ">Effective immediately,>we strongly recommend planning your migrationNot even a brief period of advance notice to do a migration? Just one day no more security patches...wth is going on over at intelreply",
      "> wth is going on over at intelThousands of layoffs.  It's surprising someone even had time to post the notice.reply",
      "no, the message poster keeps his job, so expect more messagesreply",
      "Intel. The Nvidia of the 90's. Oh success, you fickle fickle bride.-reply",
      "How much notice did ppl get when sacked?reply",
      "From my experience, none. \"Utterly unfortunately, today was your last day with the company. A separation agreement has been sent to your personal email. Your corporate access is being revoked. Thank you for your contribution!\"Being fired for poor performance is all about ample warnings, issuing a PIP, etc. The company wants the employee back on track. Being laid off is a situation that an employee cannot fix with their efforts. There's no incentive to work this week if it is already known that you are going to be laid off next week, but some employees might consider a prank or even minor sabotage as a helpless act of protest. It's safest to dismiss the laid-off ASAP.reply",
      "A week, maybe 2 for this last round? They were announced on the 13th I believe, for \"mid July\" layoffs.reply",
      "Mass firingsreply",
      "Is this a guess, or have you heard something?reply"
    ],
    "link": "https://community.clearlinux.org/t/all-good-things-come-to-an-end-shutting-down-clear-linux-os/10716",
    "first_paragraph": "After years of innovation and community collaboration, we\u2019re ending support for Clear Linux OS. Effective immediately, Intel will no longer provide security patches, updates, or maintenance for Clear Linux OS, and the Clear Linux OS GitHub repository will be archived in read-only mode. So, if you\u2019re currently using Clear Linux OS, we strongly recommend planning your migration to another actively maintained Linux distribution as soon as possible to ensure ongoing security and stability.Rest assured that Intel remains deeply invested in the Linux ecosystem, actively supporting and contributing to various open-source projects and Linux distributions to enable and optimize for Intel hardware.A heartfelt thank you to every developer, user, and contributor who helped shape Clear Linux OS over the last 10 years. Your feedback and contributions have been invaluable.Thanks for the wonderful memories.  We definitely miss Clear Linux!gonna miss autospec, godspeedThanks so much, I learned a lot fr"
  },
  {
    "title": "Meta says it wont sign Europe AI agreement, calling it growth stunting overreach (cnbc.com)",
    "points": 109,
    "submitter": "rntn",
    "submit_time": "2025-07-18T17:56:37 1752861397",
    "num_comments": 155,
    "comments_url": "https://news.ycombinator.com/item?id=44607838",
    "comments": [
      "Not just Meta, 40 EU companies urged EU to postpone roll out of the ai act by two years due to it's unclear nature. This code of practice is voluntary and goes beyond what is in the act itself. EU published it in a way to say that there would be less scrutiny if you voluntarily sign up for this code of practice. Meta would anyway face scrutiny on all ends, so does not seem to a plausible case to sign something voluntary.One of the key aspects of the act is how a model provider is responsible if the downstream partners misuse it in any way. For open source, it's a very hard requirement[1].> GPAI model providers need to establish reasonable copyright measures to mitigate the risk that a downstream system or application into which a model is integrated generates copyright-infringing outputs, including through avoiding overfitting of their GPAI model. Where a GPAI model is provided to another entity, providers are encouraged to make the conclusion or validity of the contractual provision of the model dependent upon a promise of that entity to take appropriate measures to avoid the repeated generation of output that is identical or recognisably similar to protected works.[1] https://www.lw.com/en/insights/2024/11/european-commission-r...reply",
      "Lovely when they try to regulate a burgeoning market before we have any idea what the market is going to look like in a couple years.reply",
      "The whole point of regulating it is to shape what it will look like in a couple of years.reply",
      "Regulators often barely grasp how current markets function and they are supposed to be futurists now too? Government regulatory interests almost always end up lining up with protecting entrenched interests, so it's essentially asking for a slow moving group of the same mega companies. Which is very much what Europes market looks like today. Stasis and shifting to a stagnating middle.reply",
      "So the solution is to allow the actual entrenched interests to determine the future of things when they also barely grasp how the current markets function and are currently proclaiming to be futurists?reply",
      "The best way for \"entrenched interests\" to stifle competition is to buy/encourage regulation that keeps everybody else out of their sandbox pre-emptively.For reference, see every highly-regulated industry everywhere.reply",
      "They\u2019re demanding collective conversation. You don\u2019t have to be involved if you prefer to be asocial except to post impotent rage online.Same way the pols aren\u2019t futurists and perfect neither is anyone else. Everyone should sit at the table and discuss this like adults.You want to go live in the hills alone, go for it, Dick Proenneke. Society is people working collectively.reply",
      "Won't somebody please think of the children?reply",
      "You're both right, and that's exactly how early regulation often ends up stifling innovation. Trying to shape a market too soon tends to lock in assumptions that later prove wrong.reply",
      "The experience with other industries like cars (specially EV) shows that the ability of EU regulators to shape global and home markets is a lot more limited than they like to think.reply"
    ],
    "link": "https://www.cnbc.com/2025/07/18/meta-europe-ai-code.html",
    "first_paragraph": ""
  },
  {
    "title": "Broadcom to discontinue free Bitnami Helm charts (github.com/bitnami)",
    "points": 95,
    "submitter": "mmoogle",
    "submit_time": "2025-07-18T19:29:52 1752866992",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=44608856",
    "comments": [
      "Hi, former cofounder of Bitnami here. I left VMware quite a while ago, so not involved with this. The technical team at Bitnami is still top notch and great people. I am quite baffled at this business decision.reply",
      "Broadcom gonna Broadcom.  Don't anthropomorphize the lawnmower.reply",
      "The source of this great quote, from the wonderful Bryan Cantrell: https://youtu.be/-zRN7XLCRhcreply",
      "This announcement is a little hard to read. They make it seem that the current images under docker.io/bitnami/* get deleted on August 28? But individual chart READMEs seem to say that images will move during a period starting on August 28 and ending two weeks later? But looking at https://hub.docker.com/u/bitnamilegacy images have been copied already?From ticket https://github.com/bitnami/charts/issues/35164:> Now \u2013 August 28th, 2025: Plan your migration: Update CI/CD pipelines, Helm repos, and image references> August 28th, 2025: Legacy assets are archived in the Bitnami Legacy repository.From README https://github.com/bitnami/charts/blob/4973fd08dd7e95398ddcc...:> Starting August 28th, over two weeks, all existing container images, including older or versioned tags (e.g., 2.50.0, 10.6), will be migrated from the public catalog (docker.io/bitnami) to the \u201cBitnami Legacy\u201d repository (docker.io/bitnamilegacy), where they will no longer receive updates.What are users expected to do exactly?reply",
      "The complete history of Bitnami container images has been copied to the \"bitnamilegacy\" repository. New tags will continue to be synced there until August 28th. After that date, \"bitnamilegacy\" will no longer receive updates, and images in the mainline \"bitnami\" repository will begin to be removed over a period that may take up to two weeks.Once the cleanup is complete, the mainline \"bitnami\" repository on DockerHub will contain only a limited subset of Bitnami Secure Images (at this moment available at \"bitnamisecure\"). These are hardened, security-enhanced containers intended for development or trial use, providing a preview of the full feature set available in the paid offering.- Bitnami: https://hub.docker.com/u/bitnami\n- Bitnami Legacy: https://hub.docker.com/u/bitnamilegacy\n- Bitnami Secure Images: https://hub.docker.com/u/bitnamisecurereply",
      "Bitnami images have been problematic for a little while, especially given their core focus on security but still resulting in a CVE 9.4 in PgPool recently that ended up being used in the underlying infrastructure for a bunch of cloud hosts:[pgpool] Unauthenticated access to postgres through pgpool \u00b7 Advisory \u00b7 bitnami/charts https://share.google/JcgDCtktG8dE2TZY8reply",
      "Maybe this will finally break me of my habit of using helm charts, period.reply",
      "I\u2019ve never used Helm charts. I learned K8S in a shop in which kustomize is the standard and helm is a permitted exception to the standard, but I just never felt any reason to learn helm. Am I missing out?Sometimes the limitations of kustomize annoy me, but we find ways to live with themreply",
      "Helm gives you more than enough rope to hang yourself with.  At $dayjob we barely use 3rd party helm charts, and when we do we eventually run into problems with clever code.We do package our own helm charts, not in the least because we sign contracts with our customers that we will help them run the software we're selling them.  So we use package docker and helm artifacts that we sell in addition to running locally.So we write some charts that don't use most helm features.  The one useful thing about Helm that I don't want to live without is the packaging story.  We seem to be the only people in the ecosystem that \"burn in\" the Docker image sha into the Helm chart we package, and set our v1.2.3 version only on the chart.  This means we don't have to consider a version matrix between our config and application.  Instead we just change the code and config in the same git sha and it just works.reply",
      "Would you like to count the number of spaces that various items in your manifests are indented and then pass that as an argument to a structure-unaware text file templating engine? Would you like to discover your inevitable yaml file templating errors after submitting those manifests to the cluster? Then yes, you are really missing out!reply"
    ],
    "link": "https://github.com/bitnami/charts/issues/35164",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.As of August 28th, 2025, the Bitnami public catalog will undergo the following changes:Community catalogBitnami Secure ImagesLegacy repository migrationNow \u2013 August 28th, 2025August 28th, 2025Post-August 28th, 2025Q: What is the Bitnami Legacy repository?\nA: The Bitnami Legacy repository is an archive that will host all older container images, including versioned tags (e.g., 2.50.0, 10.6) that are no longer maintained in the main Bitnami catalog. These images will not receive updates, fixes, or support and should only be used as a temporary fallback for migration purposes. Although there are no updates to the container images, the Bitnami project continues to make its source code available at bitnami/containers under the Apac"
  },
  {
    "title": "lsr: ls with io_uring (rockorager.dev)",
    "points": 303,
    "submitter": "mpweiher",
    "submit_time": "2025-07-18T12:40:41 1752842441",
    "num_comments": 152,
    "comments_url": "https://news.ycombinator.com/item?id=44604050",
    "comments": [
      "Author of the project here! I have a little write up on this here: https://rockorager.dev/log/lsr-ls-but-with-io-uringreply",
      "Nice writeup. I suspect you're measuring the cost of abstraction. Specifically, routines that can handle lots of things (like locale based strings and utf8 character) have more things to do before they can produce results. This was something I ran into head on at Sun when we did the I18N[1] project.In my experience there was a direct correlation between the number of different environments where a program would \"just work\" and its speed. The original UNIX ls(1) which had maximum sized filenames, no pesky characters allowed, all representable by 7-bit ASCII characters, and only the 12 bits of meta data that God intended[2] was really quite fast. You add things like a VFS which is mapping the source file system into the parameters of the \"expected\" file system that adds delay. You're mapping different character sets? adds delay. Colors for the display? Adds delay. Small costs that add up.1: The first time I saw a long word like 'internationalization' reduced to first and last letter and the count of letters in between :-).2: Those being Read, Write, and eXecute for user, group, and other, setuid, setgid, and 'sticky' :-)reply",
      "My bfs project also uses io_uring: https://github.com/tavianator/bfs/blob/main/src/ioq.cI'm curious how lsr compares to bfs -ls for example.  bfs only uses io_uring when multiple threads are enabled, but maybe it's worth using it even for bfs -j1reply",
      "Oh that's cool. `find` is another tool I thought could benefit from io_uring like `ls`. I think it's definitely worth enabling io_uring for single threaded applications for the batching benefit. The kernel will still spin up a thread pool to get the work done concurrently, but you don't have to manage that in your codebase.reply",
      "I did try it a while ago and it wasn't profitable, but that was before I added stat() support.  Batching those is probably goodreply",
      "and grep / ripgrep. Or did ripgrep migrate to using io_uring already?reply",
      "No, ripgrep doesn't use io_uring. Idk if it ever will.reply",
      "Curious: Why? Is it not a good fit for what ripgrep does? Isn't the sort of \"streaming\" \"line at a time\" I/O that ripgrep does a good fit for async io?reply",
      "For many workloads, ripgrep spends the vast majority of its time searching through files.But more practically, it would be a terror to implement. ripgrep is built on top of platform specific standard file system APIs. io_uring would mean a whole heap of code to work with a different syscall pattern in addition to the existing code pattern for non-Linux targets.So to even figure out whether it would be worth doing that, you would need to do a whole bunch of work just to test it. And because of my first point above, there is a hard limit on how much of an impact it could even theoretically have.Where I would expect this to help is to batch syscalls during directory tree traversal. But I have nonidea how much it would help, if at all.reply",
      "I believe that io_uring does not support getdents (despite multiple patch series being proposed). So you'd get async stat(), if you need them, but nothing else.reply"
    ],
    "link": "https://rockorager.dev/log/lsr-ls-but-with-io-uring/",
    "first_paragraph": "As an excercise in syscall golf, I wrote an implementation of ls(1) which uses my IO library, ourio to perform as much of the IO as possible. What I ended up with is something that is faster than any version or alternative to ls I tested, and also performs an order of magnitude fewer syscalls. I\u2019m calling it lsr. Let\u2019s start with the benchmarks, then we\u2019ll see how we got there.Data gathered with hyperfine on a directory of n plain files.Data gathered with strace -c on a directory of n plain files. (Lower is better)Let\u2019s start with how lsr works. To list directory contents, we basically have 3 stages to the program:All of the IO involved happens in the second step. Wherever possible, lsr utilizes io_uring to pull in the data it needs. To get to that point, it means that we open the target directory with io_uring, if we need local time, user data, or group data, we open (and read) those files with io_uring. We do all stat calls via io_uring, and as needed we do the equivalent of an lstat"
  },
  {
    "title": "Multiplatform Matrix Multiplication Kernels (burn.dev)",
    "points": 51,
    "submitter": "homarp",
    "submit_time": "2025-07-18T19:59:49 1752868789",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=44609137",
    "comments": [
      "Has there been much research into slightly flawed matrix multiplications?If you have a measure of correctness, and a measure of performance. Is there a maximum value of correctness per some unit of processing that exists below a full matrix multiplyObviously it can be done with precision, since that is what floating point is. But is there anything where you can save x% of computation and have fewer than x% incorrect values in a matrix multiplications?Gradient descent wouldn't really care about a few (Reliably) dud values.reply",
      "Randomized matrix sketching is one way to get at this (see https://arxiv.org/abs/2302.11474), the problem is hardware is heavily optimized for dense multiplies so what you save in flops doesn't translate to real runtime speeds ups.reply",
      "I had bet that matmult would be in transformer-optimized hardware costing a fraction of GPUs first class in torch 2 years ago with no reason to use GPUs any more. Wrong.reply",
      "> matmult would be in transformer-optimized hardwareIt is... it's in GPUs lol> first class in torchIt is> costing a fraction of GPUsWhy would anyone give you this for cheaper than GPUs lol?reply",
      "One of the author here, don't hesitate if you have any question or comment!reply",
      "Very interesting, willing to try burnreply",
      "Could something like this be done in WebGPU?reply",
      "CubeCL supports WebGPU and can be used with wasm!reply",
      "burn is awesomereply",
      "I'm sorry this is a low brow comment but this is the dumbest thing you can do in this space:> Unit (thread in CUDA, invocation in Vulkan/Wgpu): the smallest execution entity performing computations.> Plane (warp in CUDA, subgroup in Vulkan/Wgpu): a group of (typically 32) units executing in lockstep and able to share data efficiently through registers.> Cube (thread block in CUDA, workgroup in Vulkan/Wgpu): a group of units that execute on the same SM, sharing memory and able to synchronizeIt's already bad enough that the vendors themselves insisted on different names but why in the bejesus would you rename these concepts and diverge from literally all existing naming conventions when you're providing middleware. Ie when using your tool I'm still going to reference NVIDIA's or AMD's docs to understand how the hardware actually works. Like do you really think otherwise - that your thing is gonna be end of the line???FYI the word warp isn't random techno babble but is actually a very clever pun that actually fits very well conceptually:https://en.m.wikipedia.org/wiki/Warp_and_weftreply"
    ],
    "link": "https://burn.dev/blog/sota-multiplatform-matmul/",
    "first_paragraph": "home \u00b7 blog \u00b7 sota-multiplatform-matmul\nFew algorithmic problems are as central to modern computing as matrix\n          multiplication. It is fundamental to AI, forming the basis of fully\n          connected layers used throughout neural networks. In transformer\n          architectures, most of the computation is spent performing matrix\n          multiplication. And since compute largely determines capability, faster\n          matrix multiplication algorithms directly translate into more powerful\n          models [1].\n\nNVIDIA probably deserves much of the credit for making matrix\n          multiplication fast. Their early focus on building GPUs for gaming\n          drove major improvements in general-purpose linear algebra. As deep\n          learning took over, they introduced Tensor Cores \u2014 specialized units\n          designed to accelerate matrix multiplication in AI models.\n\nWith hardware evolving to support these new workloads, accelerators\n          have become increasingly comple"
  },
  {
    "title": "Wii U SDBoot1 Exploit \u201cpaid the beak\u201d (consolebytes.com)",
    "points": 81,
    "submitter": "sjuut",
    "submit_time": "2025-07-18T20:30:36 1752870636",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=44609479",
    "comments": [
      "Having spent a while working in embedded and learning that this is not a lesson that's been internalised: this is why you never sign any executable that can boot on shipped hardware unless you'd be ok with everyone running it on shipped hardware. You can not promise it will not leak. You can not promise all copies will be destroyed. If it needs to run on production hardware then you should have some per-device mechanism for one-off signatures, and if it doesn't then it should either be unsigned (if fusing secure boot happens late) or have the signature invalidated as the last thing that happens before the device is put in the box.A lot of companies do not appear to understand this. A lot of devices with silicon-level secure boot can be circumvented with signed images that have just never (officially) been distributed to the public, and anyone relying on their security is actually relying on vendors never accidentally trashing a drive containing one. In this case Nintendo (or a contractor) utterly failed to destroy media in the way they were presumably supposed to, but it would have been better to have never existed in this form in the first place.reply",
      "I think they _might_ have thought a little farther than this; as far as I can tell this tool was _supposed_ to only boot images with the same security checks as the actual fused state of the console, and the issue was that the section header parsing code was vulnerable to a trivial attack which allowed arbitrary execution, which of course could then bypass the lifecycle state checks.I'd extend your thesis to \"you need to audit your recovery tools with the _exact same_ level of scrutiny with which you audit your production secondary bootloader, because they're effectively the same thing,\" which is the same concept but not _quite_ as boneheaded as you suggest.Recently, I see this class of exploit more commonly, too: stuff like \"there's a development bootloader signed with production keys\" has gone away a little, replaced with \"there's a recovery bootloader with signature checking that's broken in some obvious way.\" Baby steps, I guess...reply",
      "I don't like this advice because it seems like it's only useful to people who want to do tivoization in the first place. I hope people who try to do that keep failing at it, because \"success\" is bad for the rest of us.reply",
      "Agreed. I'm rooting for the continued failure of everyone who locks down hardware (and software) to prevent its users from modifying or fully controlling it.reply",
      "This reminds me a lot of the PSP Pandora's Battery: a special factory \"boot from external flash\" system with exploitable vulnerabilities - on PSP, the special Pandora's Battery \"JigKick\" serial number 0xFFFFFFFF or the factory battery challenge/response \"Baryon Sweeper\" on newer consoles, followed by a rather complicated exploit in the \"ipl.bin\" signature checking process on the external hardware. On the Wii U, the \"unstable power\" battery jig followed by a simple overflow in SDBoot1.https://www.psdevwiki.com/psp/Pandorahttps://github.com/khubik2/pysweeperreply",
      "That was super interesting! Are there any details on how/where they found the sd and memory cards? It seems like you\u2019d have to be incredibly lucky to find something like that.reply",
      "I've seen people exploit hardware by messing with the power supply before.  I've never seen it be the intended manufacturer maintenance key.reply",
      "Mirror (site seems down) https://archive.is/92OIxreply",
      "This was an amazing read!reply",
      "Sort of a related tangent:Some of the best gaming time in my life has been on handheld consoles, even when the games were available on PC or TV.I wish there was a modern platform (not just a hobbyist Raspberry Pi kit or something) in the Switch or DS form factor, that boots straight into a coding environment like the legendary Commodore 64 and other \"computer-consoles\" of that era, with a central app store for indie devs to publish to for free. Add in dedicated support from a game engine like Godot, and I think something like that could spark a renaissance of solo devs/buddy teams experimenting with new game ideas and stuff.reply"
    ],
    "link": "https://consolebytes.com/wii-u-sdboot1-exploit-paid-the-beak/",
    "first_paragraph": "ConsoleBytesPreface: I am going to speak mostly in regards to my hand in this as I don\u2019t have solid understanding of the Wii U software and architecture and I don\u2019t want to throw out incorrect information. Where needed I have provided quotes and screenshots so that the information provided is coming straight from the horses mouth. Recently I have had the pleasure to work with an amazing and talented group of people. It starts off like this.WiiCurious contacted me asking if I might possibly be able to recover some data from GameCube Memory Cards and SD Cards that were used in the Nintendo factory setup process for installing the software to the Wii and Wii U systems, respectively. Nintendo tried to destroy these Memory Cards and SD Cards by crushing them and bending them in the middle. By doing so, Nintendo did damage about 50% of the Memory Cards to the point that the main flash memory IC was busted and the die was broken, so no hope. About 25% of the cards were still functional with a"
  },
  {
    "title": "Trying Guix: A Nixer's impressions (tazj.in)",
    "points": 142,
    "submitter": "todsacerdoti",
    "submit_time": "2025-07-15T08:11:19 1752567079",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=44569032",
    "comments": [
      "Its nice read. We need more of comparative posts by user familiar with both nix and guix.We see bias with most discussions.Only cons with Guix I see is, lack of infrastructure and less volunteers to work on guix eco-system.\nIf its solved, I can imagine guix can improve exponentially.reply",
      "> We need more of comparative postsThe article focuses on a comparison between GUIX _system_ and NixOS. It would be interesting to see an equally thoughtful comparison that just focuses on GUIX vs. NIX as package managers used on another Linux distribution (e.g. Debian.)In this case, GUIX might fare better as you won't have to worry about the complexities introduced by binary blobs needed for boot, etc.reply",
      "Guix recently moved from Savannah to Codeberg, which hopefully will help move the needle on infrastructure and volunteer quantity concerns.reply",
      "Would it be possible to automate nix->guix for packages? A lot of nix packages (e.g. python) I think are handled automatically. If guix could build on that huge package repo, it would really help.reply",
      "I suppose an LLM could easily translate the build scripts. It will not be perfect, but probably will save a lot of work.reply",
      "I can\u2019t find the quote right now; but it\u2019s along the lines.\nLLM\u2019s can help with the first 80% of coding which is introducing bugs, but terrible at the other 80% of the time which is removing said bugs.Lean and agile theory says that quality is improved and efficiency gained by going upstream and eliminating the things that are producing poor quality in the first place.Llm code tools may be useful to give syntax tips, but like a thoughtless dev on a team, likely introduce more work than they get done.reply",
      "The major con is in the article, it is super slow to update. Half an hour is just crazy, nobody will move to that if they know.reply",
      "From the article:> My goal was to take my Unchartevice laptop with its strange Zhaoxin x86_64-compatible CPU...> Sure, this is a laptop with a CPU broadly equivalent to old Intel Atom CPUs...Yes, guix pull is slow, but the author is using some old/exotic hardware. The last time I tried guix on a 5th gen dual core i5, the initial pull was not that slow. And as other commenters have pointed out. The first pull is the slowest by far.reply",
      "It doesn't usually take that long, but the first `guix pull` is quite slow, and what should be a no-op of running it a second time immediately afterwards also takes too long.Edit: Just to provide a measurement, on my Framework 13 with AMD Ryzen 5 7640U, a `guix pull` which pulled in 1 nonguix commit and 64 guix commits took 2m10s, and a subsequent no-op `guix pull` took 1m18s.reply",
      "Important question is if its fixable.Nix is pathologically recursive, lazy, and uses fixed points, things that are very apt to changing something that cascades through a bunch of dependents.  Nix's runtime is not magic.  Guile should be able to expose a language and evaluate it in a similar way.For my part, I've not opted into Guix because it's a GNU project, and I've decided to avoid anything in the FSF sphere of influence.  Their orthodoxy turns off contributors and they have a history of taking insular hard-liner approaches that are utopian.  Outside of coreutils that are about to be fully subsumed by rewrite-it-in-Rust (which has a community that is not a fan of the GPL), what has had FSF backing and been successful?  Linus starts two of the most influential pieces of software in human civilization and RMS wants to name the awards.  The pragmatic culture that shifted away from the FSF has I think largely adopted Nix, and it shows.  Nix is open for business, available on lots of platforms, has commercial entities built around its success.reply"
    ],
    "link": "https://tazj.in/blog/trying-guix",
    "first_paragraph": "People occasionally ask me how I think Guix compares to Nix. Let me set the stage: I've been using Nix for many years, have large projects using Nix, used to be very active in the Nix community, and even wrote multiple Nix language interpreters, so I'd say that I'm at least fairly comfortable with Nix.I'm also one of those people who live in Emacs. I'm no stranger to Lisps (although my experience with Scheme is limited) and am very fond of them. It feels natural that people ask me about my views on Guix.The thing is: I haven't actually ever used Guix, so I just don't know. But there's an easy way to find out: let's try it! So that's what I did this weekend.Here are a few things I ran into and found noteworthy. My goal was to take my Unchartevice laptop with its strange Zhaoxin x86_64-compatible CPU and see if I could get all the way to my standard niri desktop. There's no overarching point here, just observations a user of one or the other system might find interesting.Spoiler: I didn'"
  },
  {
    "title": "AI capex is so big that it's affecting economic statistics (paulkedrosky.com)",
    "points": 201,
    "submitter": "throw0101c",
    "submit_time": "2025-07-18T19:59:04 1752868744",
    "num_comments": 219,
    "comments_url": "https://news.ycombinator.com/item?id=44609130",
    "comments": [
      "I don't know... 1.2% of GDP just doesn't seem that extreme to me. Certainly nowhere near \"eating the economy\" level compared to other transformative technologies or programs like:- Apollo program: 4%- Railroads: 6% (mentioned by the author)- Covid stimulus: 27%- WW2 defense: 40%reply",
      "Yeah that's my first reaction to. 1.2% doesn't sound much. It's just people making headlines out of thin air. If it lists the water and energy consumption I might be more concerned.Slightly off-topic, but ~9% of GDP is generated by \"financial services\" in the US. Personally I think it's a more alarming data point.reply",
      "Nearly 20% for healthcare causes some reservations -- considering how little we get for our money in America.reply",
      "Yes, that stat is spirit crushing.reply",
      "Why is 9% for financial services bad? This should cover fees/interest from everything like loans, transactions, mortgages, advice, investing, etc. It doesn't seem that surprising to me that the systems that are the backbone for all the money operations that power the rest of the economy make up about 10%.reply",
      "Ultimately, \"financial services\" is what's downstream of insurance, banking (deposits / money transfers), loans and retirement savings. Also efficient capital allocation and the provision of government services to some extend. Those are things we want, and we want those things to work well.reply",
      "https://youtu.be/HA1YKg_OLBwFinancial services makes the unrealistic consumption of rich countries possible. That\u2019s worth 9%.reply",
      "Nice clip yet it does not make it clear why 9% is the good value of GDP. Why not 7%?reply",
      "Why not 50%?reply",
      "How do they do that?reply"
    ],
    "link": "https://paulkedrosky.com/honey-ai-capex-ate-the-economy/",
    "first_paragraph": "AI capex is so big that it's affecting economic statistics, boosting the economy, and beginning to approach the railroad boomAs ever, here is what's ahead:I previously wrote about the perils of building renovation as a Fed chair, especially given an administration bent on finding a reason to fire you \"for cause.\" As anyone who has renovated anything larger than a dog house knows, no one thinks what you spent on renovation makes sense, not even you. Most spouses wish they could fire someone for cause over most renovations. In that light, this WSJ explainer of the ongoing Federal Reserve renovations is useful reading as various administration officials feign dismay at someone renovating a building that hasn't been renovated in ... ever. Shocked, I am shocked to find renovations going on in here. Say what you will about Powell's monetary policy, either that wallpaper doesn't go\u00b9, or Powell does, apparently. \n    \u00b9 Writer Oscar Wilde's final words may or may not have been: \"This wallpaper "
  },
  {
    "title": "The year of peak might and magic (filfre.net)",
    "points": 80,
    "submitter": "cybersoyuz",
    "submit_time": "2025-07-18T17:21:49 1752859309",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=44607403",
    "comments": [
      "I remember coming across HoMM around 2008, but the published game was nowhere available near where I lived, so I ended up playing a web based clone[0].Been playing it for around 17 years, and it is still quite fun.[0] https://www.lordswm.com/reply",
      "Love to see some Might and Magic articles.Earlier this year I found a boxed copy of Might and Magic IV: Clouds of Xeen on Marketplace. Everything is in the box and it's all pristine. It goes very nicely with my boxed copy of Might and Magic III that I bought (used!) to play on my first computer, my 386.I think I'm going to get these maps framed.reply",
      "I keep a Windows 2000 virtual machine with no network access around just to occasionally play HOMM 3.There aren't many games from that era that are as infinitely replayable. Command and Conquer: Yuri's Revenge and Starcraft come to mind.reply",
      "Yuri's Revenge never gets old - it's just the right balance of RTS with a bit more of an arcadey feel.  Sadly I missed the original Starcraft train (finished Starcraft II), but one of these days I'm going to sit down and work my way through it.reply",
      "Try playing the HD add-on with the HotA (horn of the abyss) espansion!  There are 2 new factions now which are well balanced.And, they look beautiful.Its wonderful what a dedicated community can achieve. Kudos to all of them!TIP: if you get the game on GOG. It will run just fine in any modern Windows setup (even MAC OS i believe)reply",
      "> I keep a Windows 2000 virtual machine with no network access around just to occasionally play HOMM 3.according to wiki there should be an easier way:Platform(s) Windows, Macintosh, Linux (PowerPC/x86), iOS, AndroidRelease March 3, 1999reply",
      "The Windows version is no longer compatible with modern Windows versions.The Mac version (I own both) was for PowerPC Macs.I've already paid for it a third time, as part of a HOMM box set for Windows.Good Old Games has produced a fixed version, so I could pay for it a fourth time, but running it in the VM still works.reply",
      "https://vcmi.eu/There is also a great remake with a new engine, that of course requires the original assets.reply",
      "I would be quite surprised if a modern Linux could run the original binary without gymnastics. Windows is the only OS which prioritizes backwards compatibility.reply",
      "You're probably right about the native version, but the Windows version works fine: https://www.protondb.com/app/297000?device=pcreply"
    ],
    "link": "https://www.filfre.net/2025/07/the-year-of-peak-might-and-magic/",
    "first_paragraph": "  \nSome passions are lifelong, but many others fade away over the course of a life, to be replaced by others. Not long after Jon Van Caneghem, the founder of New World Computing, sold his company to 3DO, it started to become obvious to everyone who worked for him that the torch he carried for computer games no longer burned as brightly as it once had. A younger version of Van Caneghem had spent almost three years designing and programming Might and Magic I, the CRPG that introduced New World to our world, virtually all by himself. Now, he was coming into the office just two or three times a week, leaving even on most of the days when he did make an appearance by shortly after lunchtime. His all-dominating current passion, it was becoming clear, was racing sports cars on tracks all over California and beyond. His colleagues sensed that, in his mind, he had already created his magnum-opus CRPG with Might and Magic IV and\u00a0V, two big games that could be combined into one to form The World "
  },
  {
    "title": "CP/M creator Gary Kildall's memoirs released as free download (ieee.org)",
    "points": 234,
    "submitter": "rbanffy",
    "submit_time": "2025-07-18T10:22:15 1752834135",
    "num_comments": 121,
    "comments_url": "https://news.ycombinator.com/item?id=44603066",
    "comments": [
      "\"\u201cOur father, Gary Kildall, was one of the founders of the personal computer industry, but you probably don\u2019t know his name. Those who have heard of him may recall the myth that he \u2018missed\u2019 the opportunity to become Bill Gates by going flying instead of meeting with IBM. Unfortunately, this tall tale paints Gary as a \u2018could-have-been,\u2019 ignores his deep contributions, and overshadows his role as an inventor of key technologies that define how computer platforms run today.\"Gary viewed computers as learning tools rather than profit engines. His career choices reflect a different definition of success, where innovation means sharing ideas, letting passion drive your work and making source code available for others to build upon. His work ethic during the 1970s resembles that of the open-source community today.\"With this perspective, we offer a portion of our father\u2019s unpublished memoirs so that you can read about his experiences and reflections on the early days of the computer industry, directly in his own voice.\"Sounds really interesting. Thanks for making this available!reply",
      "I just happen to have been reading this past week, the Digital Antiquarian's IBM PC release overview (4 parts).  This covers comparing Gates and Kildall (and includes e.g. the uncertainty of what actually happened with that \"flying instead of meeting with IBM\")Here's the url to part 2 of that 4-parter, where Gary gets mentioned (also covered in parts 3 and 4):\nhttps://www.filfre.net/2012/05/the-ibm-pc-part-2/reply",
      "Let's be frank. Gates was from the WASP elites, old money stuff. IBM would probably find a reason to give him the deal rather than to Gary no matter what.reply",
      "In particular, his mother \u2013 Mary Maxwell Gates \u2013 was on the United Way board along with IBM\u2019s chairman John Opel and reportedly discussed her son\u2019s company with Opel a few weeks before they made the decision to license MS-DOS.https://www.nytimes.com/1994/06/11/obituaries/mary-gates-64-...reply",
      "There's little doubt that Ms Gates suggested that IBM look into Bill Gates, but I seriously doubt that IBM made the major business decision to contract with Gates because of his mother's suggestion.reply",
      "None of us know what was said but I have no reason to doubt it based on the reports of his subsequent conversations with lower-level IBM executives. It probably didn\u2019t seem like an especially consequential decision both because neither Gates nor Kildall were especially proven at that time by the standards of a Goliath like IBM and the mainframe guys were notoriously dismissive of PCs (Opel came up through S/360). I\u2019ve seen enough nepotism not to question the plausibility but it\u2019s especially easy to imagine people high up the management ladder at the biggest mainframe manufacturer thinking it didn\u2019t really matter which of the toy computer operating system vendors they picked. I didn\u2019t work in that world then (that was my dad\u2019s generation) but even in the mid-90s when I started working in tech it was not uncommon to find mainframe people who were dismissive of PC or Unix systems as non-serious.reply",
      "Ms Gates wasn't on the board of IBM, she was on the board of another company. That isn't nepotism.There is no way successful IBM would commit to Microsoft without a thorough vetting.Few remember, but IBM also sold CPM/86 for the PC. Kildall had his chance, and muffed it with the high price.reply",
      "I specified the United Way to avoid confusion on that point. While the word nepotism originated from the Italian word for \u201cnephew\u201d referring to popes appointing their relatives, in modern English usage it more broadly includes friends as well. See for example the OED: \u201cthe practice among those with power or influence of favouring relatives, friends, or associates, especially by giving them jobs\u201d.If it helps, pretend that I wrote \u201ccronyism\u201d instead. My point was simply that it having a friendly voice at the board level is a large potential advantage which was only available to one of the vendors. While we cannot prove anything which wasn\u2019t written down, it seems implausible to say it couldn\u2019t have affected things \u2013 especially in an era where personal relationships carried more weight and there was less scrutiny of these sorts of things.reply",
      "I remember the very high price of CP/M-86. If that was because of DRI's pricing and not something IBM did, then indeed that made the choice simple, in Kildall's disfavour.reply",
      "According to the oral history of Tom Rolander (VP of engineering at DRI, he was in the famous IBM meeting), IBM wanted to call CP/M-86 \"PC-DOS\" and pay a one-time licensing fee, but DRI said they had to keep it as CP/M-86 and pay a per-device royalty. About a month after the meeting, Rolander heard through the grapevine that IBM had licensed QDOS instead of CP/M-86 for their operating system. Kildall informed IBM that he was already aware of QDOS and was preparing a lawsuit against SCP because he believed it to be an illegal CP/M clone. To defuse the situation, IBM promised that they wouldn't bundle an OS with the PC, would offer PC-DOS, CP/M-86, and UCSD P-System alongside the PC, and would pay the royalties up front for some large number of copies of CP/M-86. The condition was that DRI wouldn't sue IBM or Microsoft over the similarities between QDOS and CP/M. When the PC was released, Kildall and Rolander discovered they had been double crossed:> So we got the notice about the rolling out and all the rest\nof that, and so as Gary and I were want to do, we flew up to San Jose and took a cab over to the IBM\nstore, and we came in the store, and sure enough there was the IBM PC sitting there, and here were the\nthree boxes of the operating system. And we looked at this and the IBM PC-DOS was priced at $40, and\nthen over here was CP/M and it was priced at I\u2019m pretty sure it was $260. It was more than $200 above\nPC-DOS, and I don\u2019t even remember what the UCSD P-System was. But we looked at that and I\u2019ve never\nhad my face slapped in my life, but I know what it would feel like to have my face slapped. It was such an\nunexpected thing. I mean we had totally assumed that this was going to be a level playing field, that PC-\nDOS was going to be priced the same as CP/M, the same as the UCSD P-System, and that we were\ngoing to let the market, the users decide which one, which clearly it wasn\u2019t. And Gary described that day\nlater on in his memoirs as kind of the day innocence was gone.Here's a link to the full oral history if you're interested: https://archive.computerhistory.org/resources/access/text/20...reply"
    ],
    "link": "https://spectrum.ieee.org/cpm-creator-gary-kildalls-memoirs-released-as-free-download",
    "first_paragraph": "Tekla S. Perry, a former IEEE Spectrum editor, is now a freelance writer in Palo Alto, Calif.The year before his death in 1994,\u00a0Gary Kildall\u2014inventor\u00a0of the early microcomputer operating system CP/M\u2014wrote a draft of a memoir, \u201cComputer Connections: People, Places, and Events in the Evolution of the Personal Computer Industry.\u201d He distributed copies to family and friends, but died before realizing his plans to release it as a book.This week, the Computer History Museum in Mountain View, with the permission of Kildall\u2019s children, released the first portion of that memoir. You can download it here.Wrote Scott and Kristin Kildall in an introductory letter: \u201cIn this excerpt, you will read how Gary and Dorothy started from modest means as a young married couple, paved a new path for start-up culture, and embraced their idea of success to become leaders in the industry. Our father embodied a definition of success that we can all learn from: one that puts inventions, ideas, and a love of life "
  },
  {
    "title": "Replication of Quantum Factorisation Records with a VIC-20, an Abacus, and a Dog (iacr.org)",
    "points": 61,
    "submitter": "teddyh",
    "submit_time": "2025-07-18T19:09:07 1752865747",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=44608622",
    "comments": [
      "> We verified this by taking a recently-calibrated reference dog, Scribble, depicted in Figure 6, and having him bark three times, thus simultaneously factorising both 15 and 21. This process wasn\u2019t as simple as it first appeared because Scribble is very well behaved and almost never barks.reply",
      "one can always press the door bell button - works like a charm with my Chihuahua. Though he prefers to factorize numbers more like 529 than 21.reply",
      "Previous: https://news.ycombinator.com/item?id=44538693reply",
      "I guess I'll post it here as well. This is my personal take on the whole story: https://gagliardoni.net/#20250714_ludd_grandpasA relevant quote: \"this is your daily reminder that \"How large is the biggest number it can factorize\" is NOT a good measure of progress in quantum computing. If you're still stuck in this mindset, you'll be up for a rude awakening.\"Related: this is from Dan Bernstein: https://blog.cr.yp.to/20250118-flight.html#moonA relevant quote: \"Humans faced with disaster tend to optimistically imagine ways that the disaster will be avoided. Given the reality of more and more user data being encrypted with RSA and ECC, the world will be a better place if every effort to build a quantum computer runs into some insurmountable physical obstacle\"reply",
      "Shor's algorithm still requires O(log(N)\u00b3) qubits and O(log(N)\u00b2log(log(N))log(log(log(N)))) operations to factor N, which is why these satirical \"records\" highlight the absurdity of focusing solely on factorization milestones rather than addressing the exponential scaling challenges.reply",
      "A better measure of progress (valid for cryptanalysis, which is, anyway, a very minor aspect of why QC are interesting IMHO) would be: how far are we from fully error-corrected and interconnected qubits? I don't know the answer, or at least I don't want to give estimates here. But I know that in the last 10 or more years, all objective indicators in progress that point to that cliff have been steadily improving: qubit fidelity, error rate, coherence time, interconnections... At this point I don't think it's wise to keep thrashing the field of quantum security as \"academic paper churning\".I think the problem is that \u201cobjective indicators pointing to the cliff\u201d is pretty handwavy. Could there be a widely agreed-upon function of qubit fidelity, error rate, coherence time, and interconnections that measures, even coarsely, how far we are from the cliff? It seems like the cliff has been ten years away for a very long time, so you might forgive an outsider for believing there has been a lot of motion without progress.reply",
      "That's a cop out.I agree with what you're saying, but what you're also essentially saying is that QCs are so useless at the moment that the granularity of integers is not enough to measure progress on the hardware.reply",
      "Except that factorization is exactly what is needed to break encryption, and so knowing what QC can do in that realm of mathematics and computing is exactly the critical question that needs to be asked.And a reminder that in the world of non-QC computing, right from its very roots, the ability of computers improved in mind boggling large steps every year.QC records, other than the odd statistic about how many bits they can make, have largely not made any strides in being able to solve real world sized problems (with exception of those that use QCs purely as an analog computer to model QC behavior)reply",
      "I beg you to read the full story and to not extrapolate from the quote.Also, in the world of QC, right from its very roots, the ability of QC improved in mind boggling large steps every year. It's only that you cannot see it if you only look at the wrong metric, i.e., factorization records.It's a bit like saying \"classical computing technology has not improved for 50 years, it's only recently that we finally start to have programs that are able to write other programs\".reply",
      "There is a reason QC factorization records haven't shifted much over the past years. Number of qubits by themselves isn't enough. You to be able to do computation on them and for long enough to run Shor's algorithm till it produces a solution. How the qubits are connected, how reliable the logic gates are and how long you can maintain the quantum coherence with enough fidelity to get results is equally important.That no significant factorization milestones have moved is a huge critical black eye to this field. Even worse, that no one has ever even been able to truly run Schors algorithm on even trivial numbers is a shocking indictment of the whole field.reply"
    ],
    "link": "https://eprint.iacr.org/2025/1237",
    "first_paragraph": "This paper presents implementations that match and, where possible, exceed current quantum factorisation records using a VIC-20 8-bit home computer from 1981, an abacus, and a dog.  We hope that this work will inspire future efforts to match any further quantum factorisation records, should they arise.Note: (Minor revisions made to the original to correct a typo, clarify the use of sqrt(n) for factorisation, and add email addresses).BibTeX \nCopy to clipboard"
  },
  {
    "title": "Mango Health (YC W24) Is Hiring (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-07-18T18:39:02 1752863942",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/mango-health/jobs/3bjIHus-founding-engineer",
    "first_paragraph": "AI Therapy for OCDPassion for Mental HealthAI StackAutonomyIf this sounds like you and you are up for the challenge in making a difference \u2014 we\u2019d love to talk to you.We are building the world's leading AI Guide for individuals with OCD. Similar to what Headspace did for meditation apps, we want to be the ultimate resource for individuals looking to better themselves. The status quo for OCD treatment requires an ERP specialist which can run thousands of dollars. We are on a mission to democratize the tools required to enable individuals to be their own therapist with the help of AI.\u00a9 2025 Y Combinator"
  }
]