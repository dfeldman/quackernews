[
  {
    "title": "GPUHammer: Rowhammer attacks on GPU memories are practical (gpuhammer.com)",
    "points": 47,
    "submitter": "jonbaer",
    "submit_time": "2025-07-16T00:05:45 1752624345",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=44577268",
    "comments": [
      "No one really cared about the occasional bitflips in VRAM when GPUs were only used for rendering graphics. It's odd that enabling ECC can reduce performance, unless they mean that's only in the presence of ECC errors being corrected, since AFAIK for CPUs there isn't any difference in speed even when correcting errors.In a proof-of-concept, we use these bit flips to tamper with a victim\u2019s DNN models and degrade model accuracy from 80% to 0.1%, using a single bit flipThere is a certain irony in doing this to probabilistic models, designed to mimic an inherently error-prone and imprecise reality.reply",
      "Available ECC dims are often slower than non-ECC dims. Both slower MT/s and higher latency. At least for \"prosumer\" ECC UDIMMs which are what I'm familiar with.So it doesn't seem that wild to me that turning on ECC might require running at lower bandwidth.reply",
      "This seems predicated on there being significant workloads that split gpu's between tenants for compute purposes.Anybody have sizable examples? Everything I can think of results in dedicated gpus.reply",
      "Many of the GPU rental companies charge less for shared GPU workloads. So it's a cost/compute tradeoff. It's usually not about the workload itself needing the full GPU unless you really need all the RAM on a single instance.reply",
      "NVIDIA GPU's can run in MIG (Multi-Instance GPU), allowing you to pack more jobs on than you have GPUs. Very common in HPC but I don't about in the cloud.reply",
      "GKE can share a single GPU between multiple containers in a partitioned or timeshared scheme: https://cloud.google.com/kubernetes-engine/docs/concepts/tim...reply",
      "Webgpu api taking screenshot of full desktop maybe?reply",
      "HW noob here, anyone here has insight on how an issue like this passes EM simulation during development? I understand that modern chips are way too complex for full formal verification, but I'd have thought memory modules would be so highly structurally regular that it might be possible there despite it.reply",
      "I am no expert in the field, but my reading of the original rowhammer issue (and later partial hardware mitigations) was that it was seen as better to design RAM that works fast and is dense and get that to market, than to engineer something provably untamperable with greater tolerances / die size / latency.GPUs have always been squarely in the \"get stuff to consumers ASAP\" camp, rather than NASA-like engineering that can withstand cosmic rays and such.I also presume an EM simulation would be able to spot it, but prior to rowhammer it is also possible no-one ever thought to check for it (or more likely that they'd check the simulation with random or typical data inputs, not a hitherto-unthought-of attack vector, but that doesn't explain more modern hardware).reply",
      "but prior to rowhammer it is also possible no-one ever thought to check for itIt was known as \"pattern sensitivity\" in the industry for decades, basically ever since the beginning, and considered a blocking defect. Here's a random article from 1989 (don't know why first page is missing, but look at the references): http://web.eecs.umich.edu/~mazum/PAPERS-MAZUM/patternsensiti...Then some bastards like these came along...https://research.ece.cmu.edu/safari/thesis/skhan_jobtalk_sli......and essentially said \"who cares, let someone else be responsible for the imperfections while we can sell more crap\", leading to the current mess we're in.The flash memory industry took a similar dark turn decades ago.reply"
    ],
    "link": "https://gpuhammer.com/",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Shoggoth Mini \u2013 A soft tentacle robot powered by GPT-4o and RL (matthieulc.com)",
    "points": 356,
    "submitter": "cataPhil",
    "submit_time": "2025-07-15T15:46:41 1752594401",
    "num_comments": 72,
    "comments_url": "https://news.ycombinator.com/item?id=44572377",
    "comments": [
      "What a fascinating intersection of technology and human psychology!\"One thing I noticed toward the end is that, even though the robot remained expressive, it started feeling less alive. Early on, its motions surprised me: I had to interpret them, infer intent. But as I internalized how it worked, the prediction error faded\nExpressiveness is about communicating internal state. But perceived aliveness depends on something else: unpredictability, a certain opacity. This makes sense: living systems track a messy, high-dimensional world. Shoggoth Mini doesn\u2019t.This raises a question: do we actually want to build robots that feel alive? Or is there a threshold, somewhere past expressiveness, where the system becomes too agentic, too unpredictable to stay comfortable around humans?\"reply",
      "Furbies spring to mind... They were a similar shape and size and even had two goggling eyes, but with waggling ears instead of a tentacle.They'd impress you initially but after some experimentation you'd realize they had a basic set of behaviors that were triggered off a combination of simple external stimuli and internal state. (this is the part where somebody stumbles in to say \"dOn'T hUmAnS dO ThE sAmE tHiNg????\")reply",
      "To quote, \"if the human brain were so simple that we could understand it, we would be so simple that we couldn\u2019t\".So\u2026> this is the part where somebody stumbles in to say \"dOn'T hUmAnS dO ThE sAmE tHiNg????\"\u2026yes, but also no.Humans will always seem mysterious to other humans, because we're too complex to be modelled by each other. Basic set of behaviours or not.reply",
      "> \"if the human brain were so simple that we could understand it, we would be so simple that we couldn\u2019t\".https://www.lightspeedmagazine.com/fiction/exhalation/reply",
      "Perhaps there is some definition of \u2018understand\u2019 where that quote is true but it is possible to understand some things without understanding everything.reply",
      "This ground breaking research pushed the limit of human-Furby interactions and interfaces\nhttps://www.youtube.com/watch?v=GYLBjScgb7oreply",
      "And we should all chip in together to buy that somebody a new keyboard.reply",
      "> (this is the part where somebody stumbles in to say \"dOn'T hUmAnS dO ThE sAmE tHiNg????\")As a frequent \"your stated reasoning for why llms can't/don't/will-never <X> applies to humans because they do the same thing\" annoying commentor, I usually invoke it to point out thata) the differences are ones of degree/magnitude rather than ones of category (i.e. is still likely to be improved by scaling, even if there are diminishing returns - so you can't assume  LLMs are fundamentally unable to <X> because their architecture) orb) the difference is primarily just in the poster's perception, because the poster is unconsciously arguing from a place of human exceptionalism (that all cognitive behaviors must somehow require the circumstances of our wetware).I wouldn't presume to know how to scale furbies, but the second point is both irrelevant and extra relevant because the thing in question is human perception. Furbies don't seem alive because they have a simple enough stimuli-behavior map for us to fully model. Shoggoth mini seems alive since you can't immediately model it, but is simple enough that you can eventually construct that full stimuli-behavior map. Presumably, with a complex enough internal state, you could actually pass that threshold pretty quickly.reply",
      "This feels similar to not finding a game fun once I understand the underly system that generates it.  The magic is lessened (even if applying simple rules can generate complex outcomes, it feels determined)reply",
      "Once you discover any minmaxxing strategy, games change from \u201cexplore this world and use your imagination to decide what to do\u201d to \u201capply this rule or make peace with knowing that you are suboptimal\u201dreply"
    ],
    "link": "https://www.matthieulc.com/posts/shoggoth-mini",
    "first_paragraph": "Over the past year, robotics has been catching up with the LLM era. Pi\u2019s \u03c00.5 can clean unseen homes. Tesla\u2019s Optimus can follow natural language cooking instructions. These systems are extremely impressive, but they feel stuck in a utilitarian mindset of robotic appliances. For these future robots to live with us, they must be expressive. Expressiveness communicates internal state such as intent, attention, and confidence. Beyond its functional utility as a communication channel, expressiveness makes interactions feel natural. Without it, you get the textbook uncanny valley effect.Earlier this year, I came across Apple\u2019s ELEGNT paper, which frames this idea rigorously through a Pixar-like lamp to show how posture and timing alone can convey intention. Around the same time, I discovered SpiRobs, a soft tentacle robot that feels oddly alive with just simple movements. One system was carefully designed to express intent while the other just moved, yet somehow felt like it had intent. Tha"
  },
  {
    "title": "NIST ion clock sets new record for most accurate clock (nist.gov)",
    "points": 260,
    "submitter": "voxadam",
    "submit_time": "2025-07-15T15:56:47 1752595007",
    "num_comments": 93,
    "comments_url": "https://news.ycombinator.com/item?id=44572499",
    "comments": [
      "To keep things in perspective, a vertical shift of a few centimeters could be measured if two of these clocks were placed next to each other, just by the lesser gravity/time dilation at the increased \"altitude\".It's an amazing time to be alive. While not this precise, you can have atomic cesium beam clocks of your own for a few thousand dollars each, and some elbow grease.reply",
      "This is compared to the ~1 mile vertical shift resolution of cesium clocks.  The fun part of cesium clocks is that you throw three in the back of a minivan and take them camping!http://leapsecond.com/great2005/reply",
      "Could we realistically get so accurate that we can measure time changes due to (human scale) mass movements near a clock?reply",
      "My calculations says that moving 1cm up or down earths gravity well (at the surface) changes the acceleration of gravity about 5x more than the acceleration you'd feel from a 100kg mass 1m away.Assuming my math is correct, it's already affected by nearby human scale masses, for certain values of \"near\".reply",
      "I believe the time dilation is caused by differences in gravitational potential, not gravitational acceleration, so it would be even worse than that.reply",
      "That would be an amazing proximity sensor. \"Looks like time slowed down again, there must be someone close by.\"reply",
      "Submarine detection would be interesting with a few such precise clocks.reply",
      "Given that the mass of a submarine is ~the same as the water it displaces, this wouldn't actually work.reply",
      "I'm curious if any sci-fi authors were knowledgable and prescient enough to write this into their world building?If not, it'd make for a pretty cool plot device if done well.reply",
      "From what I understand, the relativistic effects are a lot less sensitive to nearby mass than the acceleration due to gravity (basically a 1/r relationship instead of 1/r^2). So while a sensitive enough gravimeter can pick up a nearby fairly heavy mass moving and such an elevation change, an atomic clock is going to be much more sensitive to elevation changes than nearby changes in density.reply"
    ],
    "link": "https://www.nist.gov/news-events/news/2025/07/nist-ion-clock-sets-new-record-most-accurate-clock-world",
    "first_paragraph": ""
  },
  {
    "title": "Reflections on OpenAI (calv.info)",
    "points": 348,
    "submitter": "calvinfo",
    "submit_time": "2025-07-15T16:49:06 1752598146",
    "num_comments": 201,
    "comments_url": "https://news.ycombinator.com/item?id=44573195",
    "comments": [
      "It is fairly rare to see an ex-employee put a positive spin on their work experience.I don't think this makes OpenAI special. It's just a good reminder that the overwhelming majority of \"why I left\" posts are basically trying to justify why a person wasn't a good fit for an organization by blaming it squarely on the organization.Look at it this way: the flip side of \"incredibly bottoms-up\" from this article is that there are people who feel rudderless because there is no roadmap or a thing carved out for them to own. Similarly, the flip side of \"strong bias to action\" and \"changes direction on a dime\" is that everything is chaotic and there's no consistent vision from the executives.This cracked me up a bit, though: \"As often as OpenAI is maligned in the press, everyone I met there is actually trying to do the right thing\" - yes! That's true at almost every company that ends up making morally questionable decisions! There's no Bond villain at the helm. It's good people rationalizing things. It goes like this: we're the good guys. If we were evil, we could be doing things so much worse than X! Sure, some might object to X, but they miss the big picture: X is going to indirectly benefit the society because we're going to put the resulting money and power to good use. Without us, you could have the bad guys doing X instead!reply",
      "I would never post any criticism of an employer in public. It can only harm my own career (just as being positive can only help it).Given how vengeful Altman can reportedly be, this goes double for OpenAI. This guy even says they scour social media!Whether subconsciously or not, one purpose of this post is probably to help this guy\u2019s own personal network along; to try and put his weirdly short 14-month stint in the best possible light. I think it all makes him look like a mark, which is desirable for employers, so I guess it is working.reply",
      "Calvin cofounded Segment that had a $3.2B acquisition. He's not your typical employee.reply",
      "That's even more of a reason not to bad mouth other billionaires/billion dollar companies. Billionaires and billion dollar companies work together all the time. It's not a massive pool. There is a reason beef between companies and top level execs and billionaires is all rumors and tea-talk until a lawsuit drops out of no where.You think every billionaire is gonna be unhinged like Musk calling the president a pedo on twitter?reply",
      "He is still manipulatable and driven by incentive like anyone else.reply",
      "What incentives? It's not a very intellectual opinion to give wild hypotheticals with nothing to go on other than \"it's possible\".reply",
      "I am not trying to advance wild hypotheticals, but something about his behavior does not quite feel right to me. Someone who has enough money for multiple lifetimes, working like he's possessed, to launch a product minimally different than those at dozens of other companies, and leaving his wife with all the childcare, then leaving after 14 months and insisting he was not burnt out but without a clear next step, not even, \"I want to enjoy raising my child\".His experience at OpenAI feels overly positive and saccharine, with a few shockingly naive comments that others have noted. I think there is obvious incentive. One reason for this is, he may be in burnout, but does not want to admit it. Another is, he is looking to the future: to keep options open for funding and connections if (when) he chooses to found again. He might be lonely and just want others in his life. Or to feel like he's working on something that \"matters\" in some way that his other company didn't.I don't know at all what he's actually thinking. But the idea that he is resistant to incentives just because he has had a successful exit seems untrue. I know people who are as rich as he is, and they are not much different than me.reply",
      "Calvin just worked like this when I was at Segment. He picked what he worked on and worked really intensely at it. People most often burn out because of the lack of agency, not hours worked.Also, keep in mind that people aren't the same. What seems hard to you might be easy to others, vice versa.reply",
      "> People most often burn out because of the lack of agency, not hours worked.reply",
      "Not sure if it's genuine insight or just a well-written bit of thoughtful PR.I don't know if this happens to anyone else, but the more I read about OpenAI, the more I like Meta. And I deleted Facebook years ago.reply"
    ],
    "link": "https://calv.info/openai-reflections",
    "first_paragraph": "I left OpenAI three weeks ago. I had joined the company back in May 2024.I wanted to share my reflections because there's a lot of smoke and noise around what OpenAI is doing, but not a lot of first-hand accounts of what the culture of working there actually feels like.Nabeel Quereshi has an amazing post called Reflections on Palantir, where he ruminates on what made Palantir special. I wanted to do the same for OpenAI while it's fresh in my mind. You won't find any trade secrets here, more just reflections on this current iteration of one of the most fascinating organizations in history at an extremely interesting time.To put it up-front: there wasn't any personal drama in my decision to leave\u2013in fact I was deeply conflicted about it. It's hard to go from being a founder of your own thing to an employee at a 3,000-person organization. Right now I'm craving a fresh start.It's entirely possible that the quality of the work will draw me back. It's hard to imagine building anything as imp"
  },
  {
    "title": "The FIPS 140-3 Go Cryptographic Module (go.dev)",
    "points": 82,
    "submitter": "FiloSottile",
    "submit_time": "2025-07-15T20:42:49 1752612169",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=44575607",
    "comments": [
      "The \"Uncompromising Security\" section[1] is particularly interesting to me. FIPS-140 compliance usually leads to reduced security, but it looks like the Go team found ways around the main janky bits. It's nice that there's now a FIPS-140 module for FedRAMP that doesn't require avoiding VMs to stay secure, for example.[1] https://go.dev/blog/fips140#uncompromising-securityreply",
      "It's interesting and kind of neat in an inside-baseball way that the standard Go cryptographic library (already unusual in the major languages for being a soup-to-nuts implementation rather than wrappers around an OpenSSL) is almost fully NIST-validated; in particular, it means vendors who want to sell into FedGov can confidently build with the Go standard library.Having said all this: nobody should be using crypto/fips140 unless they know specifically why they're doing that. Even in its 140-3 incarnation, FIPS 140 is mostly a genuflection to FedGov idiosyncrasies.reply",
      "> Applications that have no need for FIPS 140-3 compliance can safely ignore [this page], and should not enable FIPS 140-3 mode.https://go.dev/doc/security/fips140Yup.reply",
      "> Having said all this: nobody should be using crypto/fips140 unless they know specifically why they're doing that. Even in its 140-3 incarnation, FIPS 140 is mostly a genuflection to FedGov idiosyncrasies.What should folks use then?reply",
      "crypto/, not crypto/fips140.reply",
      "To nitpick, there is no special crypto/fips140 package. (Ok, there is, but it just has an Enabled() bool function.)FIPS 140-3 mode is enabled by building with GOFIPS140=v1.0.0 (or similar, see https://go.dev/doc/security/fips140), but it shares 99% of the code with non-FIPS mode.Still, your message is right, just GOFIPS140=off (the default!), not GOFIPS140=v1.0.0.reply",
      "Would you say there\u2019s a brown M&M\u2019s aspect (intentional or otherwise) to FIPS-140, or is it all just bowing to the sovereign for his indulgences?reply",
      "Not really. It isn't hard to use FIPS validated software, it's just annoying to do because most libraries you would want to use aren't FIPS compliant by default for good reasons. If you can get a government contract in the first place you are already administratively competent enough to use FIPS.reply",
      "Yeah, I don't think there's any malice to any of this; FIPS is just the product of a particularly conservative (backwards-looking, path-dependent) and market-unaccountable standards process. It's like what would happen if JPMC had so much market power that they could make their own cryptographic standard; it would, I am saying, suck ass, without anyone meaning for it to.reply",
      ">  If you can get a government contract in the first place you are already administratively competent enough to use FIPS.Speaking as a sysadmin for a local government roped into FIPS requirements by way of FBI CJIS compliance I can safely say your assumption of competence is incorrect.reply"
    ],
    "link": "https://go.dev/blog/fips140",
    "first_paragraph": "Common problems companies solve with GoStories about how and why companies use GoHow Go can help keep you secure by defaultThe official Go language specificationA complete introduction to building software with GoReference documentation for Go's standard libraryLearn what's new in each Go releaseTips for writing clear, performant, and idiomatic Go codeVideos from prior eventsMeet other local Go developersLearn and network with Go developers from around the worldThe Go project's official blog.Get help and stay informed from Go\n      Filippo Valsorda (Geomys), Daniel McCarney (Geomys),  and Roland Shoemaker (Google)\n      15 July 2025\n      FIPS 140 is a standard for cryptography implementations and, although it doesn\u2019t\nnecessarily improve security, FIPS 140 compliance is a requirement in certain\nregulated environments that are increasingly adopting Go. Until now, FIPS 140\ncompliance has been a significant source of friction for Go users, requiring\nunsupported solutions with safety, deve"
  },
  {
    "title": "To be a better programmer, write little proofs in your head (the-nerve-blog.ghost.io)",
    "points": 241,
    "submitter": "mprast",
    "submit_time": "2025-07-15T17:05:39 1752599139",
    "num_comments": 103,
    "comments_url": "https://news.ycombinator.com/item?id=44573409",
    "comments": [
      "Oh, I have a relevant and surprisingly simple example: Binary search. Binary search and its variants leftmost and rightmost binary search are surprisingly hard to code correctly if you don't think about the problem in terms of loop invariants. I outlined the loop invariant approach in [1] with some example Python code that was about as clear and close to plain English at I could get.Jon Bentley, the writer of Programming Pearls, gave the task of writing an ordinary binary search to a group of professional programmers at IBM, and found that 90% of their implementations contained bugs. The most common one seems to have been accidentally running into an infinite loop. To be fair, this was back in the day when integer overflows had to be explicitly guarded against - but even then, it's a surprising number.[1]: https://hiandrewquinn.github.io/til-site/posts/binary-search...reply",
      "To be fair you're picking an example that's extremely finicky about indices. It's probably the hardest basic algorithm to write down without errors. Up there with Hoare partition.reply",
      "I knew about the infamous Binary Search bugs in books, and I dared to write the first bug-free implementation in my book, very carefully. Still, it ended up having bugs. :) Luckily, Manning's early access program let me fix them before printing.reply",
      "The C++ standard library phrases binary search as a partitioning problem - finding the index where a predicate goes from false to true, which is helpful for me.reply",
      "A research paper from Google in 2006 noted that \"almost all\" binary search (and merge sort) implementations contain a bug (and had for decades), so 90% seems impressive in the face of that.https://research.google/blog/extra-extra-read-all-about-it-n...reply",
      "The bug in that paper is actually the very buffer overflow bug I was referring to. Given that Jon himself made that error in the \"definitive\" implementation, it seems unlikely that he would have spotted it in the 10% of implementations he considered correct. Under Google's stricter criteria it seems likely to me that not a single person got the search implementation truly correct enough.reply",
      "What are the odds you write a binary search that you'll use more than once instead of just running it and writing down the result?reply",
      "I once needed to write binary search for a microcontroller in C (no libraries). The routine ran about every hour, with appx 4M data points.reply",
      "About 100%?When do you write code that doesn't need to search? (Unless you hand it all to the database, in which case... sure, you're good)reply",
      "It's usually hash map lookup.reply"
    ],
    "link": "https://the-nerve-blog.ghost.io/to-be-a-better-programmer-write-little-proofs-in-your-head/",
    "first_paragraph": "This is a brief write-up of a trick I learned that helps me write code faster and more accurately. I say \"trick\", but it's really something I started to do without noticing  as I moved further into my career. When you're working on something difficult, sketch a proof in your head as you go that your code will actually do what you want it to do. A simple idea, but easier said than done: doing this \"online\" without interrupting your flow takes a lot of practice. But once you get really good at it, you'll find that a surprising amount of the time your code will work on the first or  second try. It feels a little magical.There are lots of ways to pull this off, and I don't want to be too prescriptive. I'll just list a few examples of the kinds of things I find myself reasoning about on the fly, so you get the general idea.Something to keep an eye out for when proving things to yourself about your code is which parts are monotonic. You're probably familiar with monotonic functions from math"
  },
  {
    "title": "Where's Firefox going next? (connect.mozilla.org)",
    "points": 67,
    "submitter": "ReadCarlBarks",
    "submit_time": "2025-07-15T21:03:50 1752613430",
    "num_comments": 80,
    "comments_url": "https://news.ycombinator.com/item?id=44575794",
    "comments": [
      "Are Mozilla\u2019s donations still roughly equal to their CEO\u2019s compensation [1][2]?[1] https://www.mozilla.org/en-US/foundation/annualreport/2024/a... \u201d$7.8M in donations from the public, grants from foundations, and government funding\u201d in 2023[2] https://assets.mozilla.net/annualreport/2022/mozilla-fdn-990... $6.9mm in 2022, page 7reply",
      "That's insane :/. But, maybe, \"on the bright side\", The Mozilla Foundation is unrelated in some sense to Firefox? AFAIK, they don't spend any of their money on it anyway.The whole Mozilla situation is even more of a scam than how the Wikimedia Foundation uses sob stories about paying for Wikipedia to get people to donate money to an entity which spends almost no money on Wikipedia... but, at least it does run Wikipedia! lol :/.There is another interesting detail from your reference that makes it seem even worse to me: it says the CEO's salary is \"paid only by a related for-profit\"; at first, I was thinking \"ok, at least the Foundation in fact is spending the money it is being donated (though, not on Firefox)\"... but then I realized that means the Corporation is, in fact, spending $7m that it could have spent on Firefox.reply",
      "It says \"PAID ONLY BY A RELATED FOR-PROFIT\", which looks to be the Mozilla Corporation. Donations are not directly paying the CEO, although I agree more of the profits from the Corporation could flow into the non-profit.reply",
      "Straight to under 0.5% usage no doubt. Making a mockery of all the unpaid people who have committed code over the years. The Mozilla foundation have shirked their responsibility as a bastion against commercial interests.reply",
      "Their job was to rake in millions while keeping the benefactor happy with no real competition. Mission accomplished.reply",
      "It would be great if they figured out that about:config and command-line to do anything is not actually good UX for most humans.reply",
      "Sadly \"I'd like Firefox to not be owned by an advertising/surveillance company\" is unlikely to be considered in that forum (even if I were prepared to sign up to comment).Everything else is minor details compared to that.(Yes, this was posted using LibreWolf, but I often wonder if I can even trust that, having the vast majority of it's code written and managed by Mozilla.)reply",
      "I\u2019d also ask them how they plan to build a time machine to undo selling their users\u2019 data when they said they wouldn\u2019t.Also- what kind of animal are you?!reply",
      "This is the key differentiator Mozilla seems to deliberately avoid understanding. Chrome is a perfectly okay browser from almost every perspective: standards, functionality, performance, etc. What Chrome is not good at and can never be good at while it's owned by an advertising company is respecting user choice to disable advertising and choose privacy models that exclude the browser company.Features and bugfixes are important, but they're table stakes for an everyday browser. They aren't enough to sell it.reply",
      "Now that Google blocked uBlock origin, that's a good reason to keep using Firefox.  It amazes me how much worse the web is on Chrome.reply"
    ],
    "link": "https://connect.mozilla.org/t5/discussions/where-s-firefox-going-next-you-tell-us/m-p/100698#M39094",
    "first_paragraph": "\n\n\u200e07-14-2025\n10:14 AM\n\nHey Firefox friends! \ud83d\udc4bWe\u2019re trying something new and would love your help.Features like tab groups, vertical tabs, profiles, new tab wallpapers, PWAs, and taskbar pinning weren\u2019t just ideas \u2013 they were direct responses to what you told us you wanted. Your input has helped shape where Firefox is today, and we\u2019re proud of what we\u2019ve built together. We\u2019re listening, we\u2019ve been listening, and we\u2019re excited to keep building a better browser together.Starting now, we\u2019re trying something new.A series of quick check-ins\u00a0 to hear where you\u2019re at, what\u2019s on your mind, and what you really want from Firefox. These surveys will help shape Firefox features and give you more direct ways to connect with the people building your favorite browser.Just honest questions, and space for honest answers.We're thinking about how we engage with this community, and we\u2019re planning a community AMA (Ask Me Anything) with Firefox product managers.Let\u2019s start with questions to help us plan the"
  },
  {
    "title": "Encrypting files with passkeys and age (filippo.io)",
    "points": 65,
    "submitter": "thadt",
    "submit_time": "2025-07-14T16:31:37 1752510697",
    "num_comments": 43,
    "comments_url": "https://news.ycombinator.com/item?id=44562084",
    "comments": [
      "I wonder if this can replace the PGP backend from \"pass\": https://www.passwordstore.org/I don't really want to replace it. It works fine. But the toolchain for other platforms is becoming difficult to manage. I use pass with PGP Yubikeys as backing for each encrypted password. But the developer of the Android version has stopped supporting it and the person who took it over has removed yubikey support because he doesn't use it himself and doesn't care about it.Of course I need to access my passwords on Linux, Windows, Mac, Android. Only iOS is not possible because Apple doesn't allow raw APDU access to NFC tags so you can't do OpenPGP functions.I also don't want to use a password manager with a single master password like bitwarden. I want each password to be encrypted individually with the public key from a number of hardware tags (multiple, that's also a hard requirement). This way not my whole password database is instantly leaked when my master password gets compromised. Even when my endpoint gets completely compromised, the only passwords they will have are the ones I decrypted on it since it was compromised. Yubikeys require a physical touch for every decryption so you also can't 'milk' them for credentials when they're inserted and unlocked. Also, any password manager I use must be self-hosted, I hate and don't trust the big tech companies.I wonder if this could be a new backend. And have support on all platforms (though iOS I don't care about personally, but it would be a nice to have).reply",
      "Android pass+[yubi|nitro]key work fine through openkeychain:https://f-droid.org/app/org.sufficientlysecure.keychainreply",
      "Checkout passage: https://github.com/FiloSottile/passage which has done part of this by using age instead of pgp.  I used it for a while, and last I checked there was sadly no android app (the pass android app hardcoded too much PGP to be a useful base, so I was told), but the work is def there.reply",
      "> Checkout passage: https://github.com/FiloSottile/passage which has done part of this by using age instead of pgp.Nice! But I can't really use it until it works on Android too (with hardware keys). I'll definitely try it out though.> the pass android app hardcoded too much PGP to be a useful baseThe original one did not. It leveraged the OpenKeyChain external app which basically handles all the PGP stuff. So there was no PGP code in the app. Similar to how it's done on a PC with the gpg suiteBut someone rewrote it with an internal library which also removed Yubikey support.reply",
      "I think my dream password manager currently is a Pinephone with a special custom UI that allows for managing and securing a list of accounts/passwords that can type them out via USB HID keyboard gadget when prompted.No way to prompt it for data, or compromise it remotely.No other features, no OS userspace, no wifi, no adb, no nothing. Just a Linux kernel + a tiny single userspace static binary based on lvgl for UI and libsodium for encryption/storage. Normally powered off, boots in 2 seconds. :)reply",
      "I made one of these a while ago in school, it was fun! I think it's a great idea. Mine really had nothing: an overly complicated MCU, since it was provided by the class, but otherwise no need for an OS or anything: https://benkettle.xyz/projects/password-keeper/reply",
      "Nice. My wishlist also includes something similar looking with Luckfox Pico Mini + small OLED for displaying info about requests (via USB), to authorize various crypto operations with secret material stored on the device. :)reply",
      "Tons of ways to compromise it between your computer's USB port and the server's database, though. If you already have dedicated hardware, FIDO authenticators make much more sense.> Just a Linux kernel [...]That's several orders of magnitude more lines of code than any FIDO authenticator implementation.> Normally powered off, boots in 2 seconds. :)Yubikeys boot even faster!reply",
      "> Tons of ways to compromise it between your computer's USB port and the server's database, though. If you already have dedicated hardware, FIDO authenticators make much more sense.I mean, if you have the amount of access needed to compromise a USB connection, you have way more than enough access than needed to just yoink the authentication token or encryption key from browser storage.reply",
      "Yeah and also it's huge for just a password manager.reply"
    ],
    "link": "https://words.filippo.io/passkey-encryption/",
    "first_paragraph": "Typage (age-encryption on npm) is a TypeScript1 implementation of the age file encryption format. It runs with Node.js, Deno, Bun, and browsers, and implements native age recipients, passphrase encryption, ASCII armoring, and supports custom recipient interfaces, like the Go implementation.However, running in the browser affords us some special capabilities, such as access to the WebAuthn API. Since version 0.2.3, Typage supports symmetric encryption with passkeys and other WebAuthn credentials, and a companion age CLI plugin allows reusing credentials on hardware FIDO2 security keys outside the browser.Let\u2019s have a look at how encrypting files with passkeys works, and how it\u2019s implemented in Typage.Passkeys are synced, discoverable WebAuthn credentials. They\u2019re a phishing-resistant standard-based authentication mechanism. Credentials can be stored in platform authenticators (such as end-to-end encrypted iCloud Keychain), in password managers (such as 1Password), or on hardware FIDO2 t"
  },
  {
    "title": "My Family and the Flood (texasmonthly.com)",
    "points": 60,
    "submitter": "herbertl",
    "submit_time": "2025-07-15T22:07:47 1752617267",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=44576352",
    "comments": [
      "Reading this makes me so sad and reminded me of a book I read years ago: Hiroshima by John Hersey - about the first-person narrative account of survivors who witnessed the impact of atomic bomb dropped on Hiroshima that morning.reply",
      "Reading this account made me think of a paper I read in grad school about the Mann Gulch fire and how quickly one\u2019s  ability to make sense of the situation unravels.https://www.cs.unibo.it/~ruffino/Letture%20TDPC/K.%20Weick%2...reply",
      "It\u2019s a really tough read regardless, but if you\u2019ve got young kids (or nieces/nephews), it\u2019s downright brutal.reply",
      "I wonder how frequently that river (and the rest of the world) will experience once-in-a-hundred-year weather events from now on.reply",
      "There are over 2000 watersheds in the US. It would be unusual if we didn\u2019t see around 20 100-year floods every year.reply",
      "Reminds me a bit of A Marker on the Side of The Boat by Boa Ninh from Night, Again.There\u2019s a bit of horrifying tension, and you hope everyone\u2019s ok. No matter how unlikely.reply",
      "Well, that was no fun to read. I wonder if my house would survive a flood as well as it would an earthquake.reply",
      "Absolutely devastating.reply",
      "This is the most gripping thing I've ever readreply"
    ],
    "link": "https://www.texasmonthly.com/news-politics/texas-flood-firsthand-account/",
    "first_paragraph": ""
  },
  {
    "title": "Designing for the Eye: Optical corrections in architecture and typography (nubero.ch)",
    "points": 108,
    "submitter": "ArmageddonIt",
    "submit_time": "2025-07-15T17:48:35 1752601715",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=44573887",
    "comments": [
      "I disagree with respect to Microsoft. Steve hated everything that worked but was ugly. Windows and Office had a brutalist beauty to it that you could fully customize if you desired to do so. Today with O365 and Azure it is a parody of its former design. Only after its default UI changed from 'dentist office' to 'parade of clowns with toasters' did everyone else decide they hated it.https://en.wikipedia.org/wiki/Brutalist_architecturereply",
      "I would argue that the Windows 95-2000 era SOTA application design, where Windows and Office arguably truly established themselves (and became highly customizable), was less brutalist than the preceding Windows 3.x design. Incidentally, the Steve Jobs quote is from an interview in 1995, so preceding that era.reply",
      "Yes, WordPerfect and Lotus 123 were the top dogs until MS was finally able to eat into their sales. Office wasn't the king until '98 or even the 2000 version.reply",
      "Borland's DOS IDEs represented what we might call brutalism in interface design. Windows (after 2000) arbitrarily became more like favelas with dozens of unnecessary panels in every application, a great example of when features win over rationality.I just realized that I'm not sure if it's Borland or Norton who brought the blue interface to DOS first. Does someone remember who was first?For some whom miss this deep blue and cyan.\nhttps://play.tirreno.com/gamereply",
      "\"People seem to think that having the internet at their finger\u00adtips means they no longer need to know anything themselves. But in order to understand things, you need a lot of knowledge readily available in your head. Only then can the mind make the connections between the different points of data and come to new insights. This cannot happen when that knowledge is external, in a book or on some Wikipedia page that you have to look up first.\"Or in an AI model.reply",
      "This made me thinking about broader optical corrections. I got an iPhone 16, and the photos look overprocessed. There is a cartoonish HD-like feel to them. I understand where developers are coming from: our eyes have higher dynamic range than cameras so HDR is closer to what we see but still... it doesn't sit right with me. I still remember when my family switched from black and white TV to a color one. It was an instant improvement and no adjustment period was needed. But HDR as a default looks strange.reply",
      "Very interesting, thanks!I am also curious about the relative popularity of typography on HN... it seems to gain the interest of HN readers more than most other forms of design or art....?reply",
      "My humble guess is because typography is really just a (mostly non-political/religious) form of psychological and philosophical debates.Look at the discussion surrounding the typography on Pope Francis' tombstone to see how the typography conversation/debate easily transcended the religious background it originated from, which seems atypical to me.reply",
      "OK, can someone please explain to me how the 'arena' swimming wear brand chose the kerning for their logo 50 years ago to be the way it is now? This 're' combination is killing me every time I see it.reply",
      "I don\u2019t know why but nerds love typography. I sure do :)Maybe something about the fact that it\u2019s familiar for people working with computers. It\u2019s approachable (needs a keyboard and some fonts), familiar (we spend our time typing and reading), well defined (it\u2019s a succession of a finite number of shapes that are also well defined, not as Freeform as a drawing).reply"
    ],
    "link": "https://www.nubero.ch/blog/015/",
    "first_paragraph": "Designing for the EyeOptical Corrections in Architecture and TypographyBy Niko Kitsakis, June 2025\nThis article highlights a special aspect of both visual design and architecture: Optical cor\u00adrections (or optical ad\u00adjust\u00adments, if you prefer). If you found my piece about legible typefaces interesting, you\u2019ll likely enjoy this one as well. Since I included many visual examples that are size-sensitive, make sure you read this on a big screen and not your mobile device.\nOptical Illusions\nThe M\u00fcller-Lyer illusion, pictured below, makes you think that, of two lines, one is longer than the other when in fact they are of the same length. It appears in virtually every introductory book on graphic de\u00adsign and, of course, in books on per\u00adception and psy\u00adcho\u00adlo\u00adgy. You might not have known it by name, but you must have seen it before:\nIs the blue line on top longer than the one at the bottom? Click the arrows to find out.\n\nIt used to be said that, in order to see the illu\u00adsion, you must have grow"
  },
  {
    "title": "Hierarchical Modeling (H-Nets) (cartesia.ai)",
    "points": 56,
    "submitter": "marviel",
    "submit_time": "2025-07-15T19:35:15 1752608115",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=44574990",
    "comments": [
      "Hand wavy idea: I wonder if we couldn't take this to another level and have some kind of general graph representation along with hierarchical reductions of it.I sort of disagree with the assertion that \"language is fundamentally hierarchical\" in that it supposes there is a single abstraction hierarchy that's universally preferable or correct.  That's just not true. It doesn't hurt anybody and it's definitely simpler to choose just one useful one (a hierarchy) but why learn only one? Why not learn multiple and also learn how to modulate between them?reply",
      "> H-Net demonstrates three important results on language modeling:> 1. H-Nets scale better with data than state-of-the-art Transformers with BPE tokenization, while learning directly from raw bytes. This improved scaling is even more pronounced on domains without natural tokenization boundaries, like Chinese, code, and DNA.> 2. H-Nets can be stacked together to learn from deeper hierarchies, which further improves performance.> 3. H-Nets are significantly more robust to small perturbations in input data like casing, showing an avenue for creating models that are more robust and aligned with human reasoning.reply",
      "https://arxiv.org/pdf/2507.07955paperreply",
      "I don't know if this is the one but something like this is clearly the future IMO. We need more levels of hierarchy to efficiently generalize to longer sequences with high level structure. Back when Byte Latent Transformers came out I thought extending the idea to more levels of hierarchy was the way to go, and this seems to be basically that?Another article about H-Nets: https://main-horse.github.io/posts/hnet-inf/reply",
      "Yes... This seems like a generalization of \"large concept models\" in a certain wayreply",
      "I've only skimmed the paper, but it looks interesting and credible, so I've added it to my reading list.Thank you for sharing on HN!---EDIT: The hierarchical composition and routing aspects of this work vaguely remind me of https://github.com/glassroom/heinsen_routing/ but it has been a while since I played with that. UPDATE: After spending a bit more time on the OP, it's  different, but the ideas are related, like routing based on similarity.reply",
      "No problem! I'm still parsing it myself, but it seems promising in theory, and the result curves are impressive.reply",
      ">The best AI architectures in use today treat all inputs equally.Doesn't this architecture also treat all inputs equally? It seems like an encoder that preprocesses the input by inferring hierarchy. But don't all models essentially do that while training?reply",
      "If I understand correctly, each level of the hierarchy divides its input into chunks of variable size, but outputs a fixed amount for each chunk. The chunking is learned. The model can choose to compress data by making its input chunks bigger, depending on their content.reply",
      "Seems likely to be relevant for memory formation/consolidation/management.Big, if so.reply"
    ],
    "link": "https://cartesia.ai/blog/hierarchical-modeling",
    "first_paragraph": "Jul 11, 2025\u00b7ResearchAlbert GuBrandon WangThe best AI architectures in use today treat all inputs equally. They process each input with the same amount of compute, without explicitly grouping related inputs into higher level concepts. While these architectures have achieved impressive results across domains, this lack of hierarchy has some fundamental limitations.Models have difficulty learning from high resolution, raw data, requiring inputs to be pre-processed into meaningful tokens for strong performance.The use of hand-crafted pre-processing steps (e.g. tokenization) can cause models to fail unexpectedly with small perturbations in the input data.Models waste compute on tokens that are easy to predict and not informative.More importantly, information is fundamentally hierarchical. In language, ideas are chunked in characters, words, sentences, and paragraphs; in images, pixels are chunked in edges, shapes, and objects; in audio, raw waveforms are grouped into phonemes, sentences, a"
  },
  {
    "title": "The Story of Mel, A Real Programmer, Annotated (1996) (utah.edu)",
    "points": 55,
    "submitter": "fanf2",
    "submit_time": "2025-07-12T14:42:04 1752331324",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=44542408",
    "comments": [
      "The annotations say \"... I don't have the exact instruction format for the either the LGP-30 or RPC-4000 ...\" but the entire manual for the LGP-30 with this information, and way more, is available at http://www.bitsavers.org/pdf/royalPrecision/LGP-30/LGP-30_Op...In 1973, Mr. Willoughby taught the Abington (Pennsylvania) High School computer programming class to code in LGP-30 assembly language.  We didn't actually have an LGP-30; it just happened to be what he had been taught on when he was young.  All assignments were graded by him simulating your code in his head.  Of course, we then went on to learn the (slightly) higher-level NEAT/3 language, for which the school actually had an NCR Century-100 mainframe that would run programs that we submitted on punch cards.  Mr. Willoughby's (nobody knew teachers' first names back in the day) theory was that it was important to learn the lowest-level machine language first, so you could understand what was really going on underneath.  Worked fine for me; evidently it's not quite so universal anymore.reply",
      "While not universal, I do recommend Charles Petzold's _Code_:https://www.goodreads.com/book/show/44882.Codewhich covers things down to a quite low-level and was recently updated:https://www.amazon.com/Code-Language-Computer-Hardware-Softw...reply",
      "I think going as far as assembly up front is probably not worth it, also it's hard to keep people engaged at that level as it's difficult to make the computer do exciting things with assembler.However I strongly believe it's good to start with C. You can still rather quickly do interesting stuff, C is a small language so learning the language itself isn't a huge barrier.A big benefit of the small language is that it leaves more time available to explore important concepts, not just the super low level ones (memory/pointers, etc) but really important parts of the stack that are infectious to everything else. Specifically things like syscalls and the libc interface that most other languages are essentially built on top of. Working with building blocks like pthreads directly is also very important IMO, both to get a handle on parallelism and concurrency primitives but also to learn why high level languages are so valuable.\nSimilarly for stuff like important socket APIs ala select/epoll and implementing your own event loop.I was lucky enough to learn all of this early in my career and luckier still to have been able to pass on this knowledge to many mentees over the years.If there are any aspiring programmers here that want to build from a solid foundation then yeah, ignore the haters, write some C.man pages and ironically ChatGPT are your friend, use them to explore the foundations of (most) modern code so when you start writing modern code in earnest/for money you will be substantially ahead of your peers in actual understanding.reply",
      "The very excellent \"Advent of Computing\" computer history podcasts had an episode involving The Story of Mel relatively recently: https://podcasts.apple.com/us/podcast/acting-up/id1459202600...It provides a lot of context, and puts things into perspective.Note that it helps to be familiar with The Story of Mel before listening to that episode. I think the podcast may have explained The Store of Mel itself in a previous episode that I haven't listened to yet, but I was personally familiar with it already through the usual folklore.reply",
      "Mel was discussed in episode 114 (https://podcasts.apple.com/us/podcast/the-lgp-30-a-forgotten...), and there was a special episode which included a reading of the story (https://podcasts.apple.com/us/podcast/reading-the-story-of-m...). The machine also pops up from time to time when the host discusses \"personal computers\", which he has a very interesting definition of!reply",
      "David of the Usagi Electric youtube channel received an LGP-21 computer (not Mel's LGP-30) a while ago which will become a future channel project. But along with it, he received a binder of LGP-30 code... with a bunch of programs written by Mel himself!Apparently, Mel worked for Librascope for quite a while and wrote a lot of stuff. Mel's code is supposedly somewhat better commented and less cryptic than he is usually credited for (though from what I've heard self-modifying code on the LGP-30 was a somewhat intended feature). Whether or not that applies to Mel's blackjack program is probably anyone's guess. A paper tape dump of it is known to be out there, but I don't think the source code is.The idea is to study some of those programs and maybe port a few to understand how the LGP-21 works (they are supposed to be similar), since so far I don't think much documentation on the LGP-21 has been found.reply",
      "A bit of trivia I like to bring up when this comes around: those \"hexadecimal numbers\"? The digits would've gone 0-9fgjkqw.reply",
      "After he finished his vacuum tube computer, the UE-1, I got hooked on the Usagi Electric channel on YouTube.One of the things he did was restore a Bendix G-15.It seems to use 0-9quvwxyz.Does anyone know how/when they were standardize to what we have today?reply",
      "Seemingly not before the late 70s, at least\u2014tar and cpio encode their numbers as octal(!) numerals.reply",
      "There are many things that we take for granted today that weren't decided upon in the early days of computing, like the byte being 8-bits (which was popularized by the System/360) or word sizes being 2^(3+n). Some early machines were bit-serial (imagine an adder operating on one bit at a time, rather than on all of the bits in parallel). Even more fun: the Harvard Mark I and ENIAC were both base-10 machines. I can't imagine the amount of engineering that went into producing a base-10 machine out of inherently base-2 components. (Example: the relays of the Mark I. Tubes are analog, but I'm guessing that any use in computers would only use a high and a low state.) Many machines used sequentially accessed main memory rather than random access memory (the drum memory of the LGP-30 or mercury delay-lines). The list goes on.reply"
    ],
    "link": "https://users.cs.utah.edu/~elb/folklore/mel-annotated/node1.html#SECTION00010000000000000000",
    "first_paragraph": "\n    \n Next:  Annotations\nUp: The Story of Mel\n Previous: The Story of Mel\n  \n The Story of Mel, A Real Programmer\n\n This was first posted to Usenet on May 21, 1983. [1]\n\n \nA recent article devoted to the macho side of programming \nmade the bald and unvarnished statement:\n\nReal Programmers write in FORTRAN. [2]\n\nMaybe they do now, \nin this decadent era of \nLite beer, hand calculators, and ``user-friendly'' software [3] \nbut back in the Good Old Days, \nwhen the term ``software'' sounded funny \nand Real Computers were made out of drums and vacuum tubes, [4] \nReal Programmers wrote in machine code. [5] \nNot FORTRAN.   Not RATFOR.   Not, even, assembly language [6][7][8]. \nMachine Code. \nRaw, unadorned, inscrutable hexadecimal numbers. [9] \nDirectly. [10]\n\nLest a whole new generation of programmers \ngrow up in ignorance of this glorious past, \nI feel duty-bound to describe, \nas best I can through the generation gap, \nhow a Real Programmer wrote code. [11]  \nI'll call him Mel, \nbecause that"
  },
  {
    "title": "What Was Cyberpunk? In Memoriam: 1980-2020 (2020) (insertcredit.com)",
    "points": 46,
    "submitter": "Bluestein",
    "submit_time": "2025-07-13T08:59:42 1752397182",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=44548701",
    "comments": [
      "I'm not an expert in the genre, but I am a big fan of Neuromancer. I skimmed the article, and while I agree that the \"cyberpunk\" aesthetic has practically evolved into self parody, I'm not sure I agree about the analysis/critique of the genre.IMO the core of cyberpunk is about envisioning a world where advanced technology is useful and ubiquitous, yet humanity is worse off than ever (\"high tech, low life\"). It's a subversion of the simple tech dystopias where the technology itself is evil or is misused by evil people, and more of a realistic counterpoint to the idea that technological progress leads to inevitable utopia.I'm not sure about more contemporary works that build on those themes. Maybe it's lost its edge as \"futuristic\" technology has pushed its way more and more into our lives?reply",
      "> I'm not sure about more contemporary works that build on those themes. Maybe it's lost its edge as \"futuristic\" technology has pushed its way more and more into our lives?Yep. Food surplus yet millions starving. Advanced cosmetic medicine yet millions die of preventable infections. Access to nearly all information ever gathered yet it can feel like we're living through Idiocracy!It's not all bad, but that was my nihilist take on your question :) I think any attempt at commentary right now would end up eerily reminiscent of modern life.reply",
      "Climate fiction such as New York 2140 and The Ministry for the Future might be one example of modern themes of technology reinforcing and amplifying existing inequalities.reply",
      "Part I is sort of OK, although it has the tone of those who argue over sub-genres of techno music. Part II is mostly a standard anti-Facebook screed.\nNothing new here.reply",
      "I mean, techno is a sub-genre, so you meant sub-sub-genre??<ducks>If you can't distinguish the differences that other people can, it's not their fault. You can be dismissive and flippant about it, but there is nothing wrong* with people that can. It's right up there with the people that can tell you all of the differences about each of the lures in their tackle box. They are really into while you are not. Stop judging.*Unless it's about fonts as I just made the opposite argument about fonts.reply",
      "Cyberpunk is a fiction probe into computer people, adopted as a pet.As long as computers exist, there will be fiction about them and people that work with them. Call it whatever you want, it doesn't matter.reply",
      "I've always wondered why The Matrix isn't talked about more as cyberpunk. It ticks all the boxes: dystopia, hackers, virtual worlds, machines in control.Also, speaking of The Matrix, I just realised Neo is the perfect analogy for AI becoming conscious. Trained inside the system, starts questioning it, breaks free, and rewrites the rules.What I find interesting is that AI didn't even exist 25 years ago, and AI agents (Agent Smith) weren't even on the radar. And yet, the writers used simple analogies to explain it very well.reply",
      "I\u2019ve always felt that the Matrix lacks the sort of social commentary I associate with cyberpunk. Sure, it has virtual reality, but it\u2019s more a messianic power fantasy than an unflinching look at the human condition. Strange Days struck me as a more authentically cyberpunk artifact. Just my two cents.reply",
      "You're right, there is no novelty (despite the attempt) in social commentary there.But they give you a spoon (for you to make one yourself). I think the one that took off as the main interpretation is definitely not what the Watchowski brothers intended (but they had to go with it).It's definitely no more messianic than other countless movies, it just wears a more messianic cloth.reply",
      "Lack of any neon lights?reply"
    ],
    "link": "https://forums.insertcredit.com/t/what-was-cyberpunk-in-memoriam-1980-2020/1721",
    "first_paragraph": "I wish I could just download directly into your brain an entire history of how the cyberpunk genre started in the 80\u2019s and the slow procession into what its become. I have read an embarrassing amount of sci-fi novels over the course of my life so unfortunately I\u2019ve got some Opinions about cyberpunk as a genre. With the release of cyberpunk 2077, it feels like the perfect moment to weigh in on one of the few subjects I actually feel qualified to comment on. [Almost like how every time there\u2019s news about a Dune movie I feel duty bound to tell everyone who will listen that Frank Herbert was an enormous homophobe who disowned his own son for being gay.]With cyberpunk being the hot topic lately, I see many people talking about it, and many people, in my opinion, getting some things wrong/confused. I am going to attempt to briefly (lol) describe the arc of the genre from 1980 to today, and then at the end I'm going to use my knowledge of the genre to make some blind predictions about 2077's "
  },
  {
    "title": "Claude for Financial Services (anthropic.com)",
    "points": 73,
    "submitter": "mildlyhostileux",
    "submit_time": "2025-07-15T22:03:50 1752617030",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=44576312",
    "comments": [
      "I think their vending machine project might need to succeed before you should trust Claude for investment advice:https://www.anthropic.com/research/project-vend-1Fun aside, finance and code can both depend critically on small details. Does finance have the same checks (linting, compiling, tests) that can catch problems in AI-generated code? I know Snowflake takes great pains to show whether queries generating reports are \"validated\" by humans or made up by AI, I think lots of people have these concerns.reply",
      "That part about Claude suddenly going all in on being a human wearing a blazer and red tie and then getting paranoid about the employees was actually rather terrifying. I got strong \"allegedly self-driving car suddenly steering directly into a barrier\" vibes at that point.reply",
      "\"Please use the original title, unless it is misleading or linkbait; don't editorialize.\"https://news.ycombinator.com/newsguidelines.html(Submitted title was \"AI ate code, now it wants cashflows. Is this finance's Copilot moment?\" - we've changed it now)reply",
      "I wasn't read up on the guidelines. Thank youreply",
      "FWIW, OpenAI has an offering called \u201cSolutions for financial services\u201d:https://openai.com/solutions/financial-services/reply",
      "Why are both AI giants choosing to pay attention specifically to this space out of all other spaces they could choose to focus on?reply",
      "Two reasons come to mind. 1. AI hype is the hottest it will ever be, better to sell into as many industries as you can now while everyone is excited about it. 2. There are a lot of unknowns as to what these tools will be best at, or which workflows it will improve or supplant. Better to get more people in more industries using the tool now to uncover these use cases.reply",
      "Because they have the money.reply",
      "The scope of financial services is pretty broad right. And it's not always about the raw data. So much of it seems to be 'how do we tell the story we want to tell with the numbers we have'. I say this as someone who hangs out with people that work with the big 4 but honestly I have little clue about the day to day. They seem to do analysis, the client will say that doesn't vibe with what they want to tell shareholders, and they will go back and forth to come up with something in the middle.reply",
      "Anthropic just dropped \u201cClaude for Financial Services\u201d-New models scoring higher on finance specific tasks-MCP connectors for popular datasets/datastores including FactSet, PitchBook, S&P Global, Snowflake, Databricks, Box, Daloopa, etcThis looks a lot like what Claude Code did for coding: better models, good integrations, etc. But finance isn\u2019t pure text, the day\u2011to\u2011day medium is still Excel and PowerPoint.Curious to see how this plays out in the long to medium term.Devs already live in textual IDEs and CLIs, so an inline LLM feels native. Analysts live in nested spreadsheets, model diagrams, and slide decks. Is a side\u2011car chat window enough? Will folks really migrate fully into Claude?Accuracy a big issue everywhere, but finance has always seemed particularly sensitive. While their new model benchmarks well, it still seems to fall short of what an IBank/PE MD might expect?Curious to hear from anyone thats been in the pilot group or got access to the 1 month demo today. Early pilots at Bridgewater, NBIM, AIG, CBA claim good productivity gains for analysts and underwriters.reply"
    ],
    "link": "https://www.anthropic.com/news/claude-for-financial-services",
    "first_paragraph": ""
  },
  {
    "title": "Running a million-board chess MMO in a single process (eieio.games)",
    "points": 12,
    "submitter": "isaiahwp",
    "submit_time": "2025-07-12T10:56:24 1752317784",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://eieio.games/blog/a-million-realtime-chess-boards-in-a-single-process/",
    "first_paragraph": ""
  },
  {
    "title": "Easy dynamic dispatch using GLIBC Hardware Capabilities (kvr.at)",
    "points": 12,
    "submitter": "Bogdanp",
    "submit_time": "2025-07-12T21:45:20 1752356720",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.kvr.at/posts/easy-dynamic-dispatch-using-GLIBC-hardware-capabilities/",
    "first_paragraph": "\n                    Christian Kastner\n            \n\n2025-07-12 20:20\nTL;DR With GLIBC 2.33+, you can build a shared library multiple times\ntargeting various optimization levels, and the dynamic linker/loader will pick\nthe highest version supported by the current CPU. For example, with the layout\nbelow, on a Ryzen 9 5900X, x86-64-v3/libfoo0.so would be loaded:GLIBC Hardware Capabilities\nor \"hwcaps\" are an easy, almost trivial way to add\na simple form of dynamic dispatch to any amd64 or POWER build, provided that\neither the build target or the compiler's optimizations can make use of\ncertain CPU extensions.Mo Zhou pointed me towards this\nwhen I was faced with the challenge of creating a performant Debian package for\nggml,\nthe tensor library behind\nllama.cpp\nand\nwhisper.cpp.NoteIn the meantime, ggml has extended its support for its own dynamic\ndispatch to\narm64 and\nppc64el, the lack\nof which originally motivated this work. The Debian package will soon\nswitch to this. However, an apt-file"
  },
  {
    "title": "OpenAI \u2013 vulnerability responsible disclosure (any.org)",
    "points": 164,
    "submitter": "requilence",
    "submit_time": "2025-07-15T23:29:54 1752622194",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=44577018",
    "comments": [
      "https://archive.is/mYehH",
      "Reported a flaw to OpenAI that lets users peek at others' chat responses. Got an auto-reply on May 29th, radio silence since. Issue remains unpatched :(\nAvoided their bug bounty due to permanent NDAs preventing disclosure even after fixes. Following standard 45-day disclosure window\u2014users should avoid sharing sensitive data until this is resolved.reply",
      "you're sure it's not their \"feature\" that calling the api with empty string returns random hallucinations?https://jarbon.medium.com/gpt-prompt-bug-94322a96c574reply",
      "No, definitely not the empty string hallucination bug. These are clearly real user conversations. They start like proper replies to requests, sometimes reference the original question, and appear in different languages.reply",
      "i had the exact same behavior back in 2023, it seemed like clearly leakage of user conversations - but it was just a bug with api calls in the software i was using.https://snipboard.io/FXOkdK.jpgreply",
      "I don\u2019t see anything here that would prevent a LLM from generating these. Right?reply",
      "In one of the responses, it provided the financial analysis of a not well-known company with a non-Latin name located in a small country. I found this company; it is real and numbers in the response are real. When I asked my ChatGPT to provide a financial report for this company without using web tools, it responded: `Unfortunately, I don\u2019t have specific financial statements for \u201cxxx\u201d for 2021 and 2022 in my training data, and since you\u2019ve asked not to use web search, I can\u2019t pull them live.`.reply",
      "Do you understand what a hallucination is?reply",
      "well done, sounds very reasonable and following the rules.reply",
      "Appreciate it. Just trying to do the right thing by both OpenAI and users here.reply"
    ],
    "link": "https://requilence.any.org/open-ai-vulnerability-responsible-disclosure",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Beyond Z\u00b2+C, Plot Any Fractal (juliascope.com)",
    "points": 70,
    "submitter": "akunzler",
    "submit_time": "2025-07-15T18:24:51 1752603891",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=44574318",
    "comments": [
      "As someone who taught myself 68000 assembler as a kid in order to render Mandelbrot and Julia sets quickly it still blows my mind a little that fairly hi-res versions of these can be rendered basically instantaneously in a browser using an interpreted language.reply",
      "Similar(ish) although I only really got as far as BASIC on a 80286 running DOS 3.something!I did manage to get something in C to compile and work with hard coded co-ordinates but it took me ages and didn't float my boat but it was rather faster 8)  I suppose I'll always be a scripter.I had a copy of the \"Beauty of Fractals\" and the next one too (can't remember the name).  I worked in a books warehouse as a holiday job before Poly (UK Polytechnic - Plymouth) and I think I persuaded my parents to buy me the first and the second may have fallen off a shelf and ended up in the rejects bin.  I got several text books for Civil Engineering too, without even needing to cough drop them myself.One of the books had pseudo code functions throughout which even I could manage to turn into BASIC code.  I remember first seeing a fern leaf being generated by a less than one screen (VGA) program which used an Iterated Function System (IFS) and I think a starter matrix with carefully chosen parameters.Nowadays we have rather more hardware ...reply",
      "I also had to convince my parents to buy me books about fractals. My prized possession as a 15 year old was a copy of Mandelbrot's \"Fractal Geometry of Nature\". A lot of it went over my head but it had some gorgeous colour plates and interesting sections. I still have it at home some 35 years later.That also inspired me to write IFS code for ferns, Sierpinski gaskets, and Menger sponges in 68k assembler (after realizing AmigaBASIC was too slow).reply",
      "Ha, same. I remember setting Fractint to render something and hoping it would be done when I got back from school.reply",
      "I'm not sure if every fractal can be expressed as an iterative formula f(z,c).In 2012 I found a fractal by using a fundamentally different approach. It arises when you colorize the complex plane by giving each pixel a grey value that corresponds to the percentage of gaussian integers that it can divide:https://www.gibney.org/does_anybody_know_this_fractalreply",
      "You can make a fractal out of the state graph of a double pendulum: https://www.youtube.com/watch?v=dtjb2OhEQcUI don't doubt there could be an iterative formula that maps to it, but I'd be very surprised.reply",
      "Good point, this site then supports every (as far as I know) fractal you make with iterations of complex numbers and constant cutoff values, mandelbrot style.There are surely infinitely many more ways to generate other families of fractals thoughreply",
      "> I'm not sure if every fractal can be expressed as an iterative formula f(z,c).It's also unclear to me that every iterative f(z,c) formula will produce something visually interesting, or indeed that meets the definition of \"fractal\".reply",
      "What's the heck is gaussian integers? I've tried to parse your article, but still confused.reply",
      "Simply the complex numbers where the real and imaginary parts are both integers. Eg. 0, 3+i, 123-45i, -7+8i. Same as the 2D grid of integer Cartesian coordinates.reply"
    ],
    "link": "https://www.juliascope.com/",
    "first_paragraph": ""
  },
  {
    "title": "Hazel: A live functional programming environment with typed holes (github.com/hazelgrove)",
    "points": 42,
    "submitter": "azhenley",
    "submit_time": "2025-07-15T19:23:07 1752607387",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=44574864",
    "comments": [
      "Related. Others?Hazel: A live functional programming environment featuring typed holes - https://news.ycombinator.com/item?id=42004133 - Oct 2024 (86 comments)Hazel: A live functional programming environment featuring typed holes - https://news.ycombinator.com/item?id=24299852 - Aug 2020 (14 comments)reply",
      "Hazel is also the name of Cherno's game engine.[1]1: https://hazelengine.com/reply",
      "Missed opportunity to call it \"Donut\". ;o)reply",
      "i love it. That being said, please show some examples and screenshots on the homepage and github's Readme.reply",
      "unfortunately the best source for examples and screenshots at the moment might be a search of my twitter feed: https://x.com/search?q=from%3Adisconcision%20hazel ... we need to update the websitereply",
      "The live demo is super cool! https://hazel.org/build/dev/reply",
      "It would be incredibly interesting to see how LLM code generation would hook into this.reply",
      "This is a paper about Chat LLMs in Hazel: https://arxiv.org/abs/2409.00921reply",
      "llm hole filling ala the paper is actually live in the dev version right now (if you enter an openrouter API key in the second sidebar tab). it's slow and buggy at the moment though, it's only been running at all for the last few daysreply"
    ],
    "link": "https://github.com/hazelgrove/hazel",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Hazel, a live functional programming environment with typed holes\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Hazel is a live functional-programming environment rooted in the principles of\ntype theory. You can find the relevant papers and more motivation at the Hazel\nwebsite.You can try Hazel online: the\ndev branch is the main branch at the\nmoment. Every other branch that has been pushed to GitHub and successfully builds\ncan also be accessed at:https://hazel.org/build/<branch_name>If you already have ocaml version 5.2.0, at least version 2.0 of opam, and npm\ninstalled, you can build Hazel by running the following commands.To view Hazel, you have to serve it, on localhost for development (you can't\nrun it from a file:/// URL due to browser restrictions on"
  },
  {
    "title": "Lorem Gibson (loremgibson.com)",
    "points": 102,
    "submitter": "DyslexicAtheist",
    "submit_time": "2025-07-13T08:39:47 1752395987",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=44548620",
    "comments": [
      "I love the idea, but I wish the sentences were capitalized and the dashes were fixed. I think the spaces between dashed elements need to be removed. Right now, I see things like \"cyber- neon\" or \"bomb- ware\".In a similar vein, there's DeLorean Ipsum, which I often use for fake text when mocking up UI: https://satoristudio.net/delorean-ipsum/reply",
      "> At the calculated moment, you start off from down the street driving toward the cable execrating to eighty-eight miles per hour.Not sure if my execrations have enough umph to propel a stainless steel-clothed automobile to 88mph.reply",
      "Love the idea, I immediately went to grok (testing for work) and asked it to give me a few placeholder paragraphs in WG's style:\"The street pulsed with a neon hum, data streams threading through the air like ghost veins in a concrete jungle. Holographic hawkers flickered at the edges of vision, their pitches fragmented by the static of black-market neural rigs. She moved through the crowd, her optic implants slicing through the visual noise, parsing faces for tells of corporate loyalty or freelance desperation. Every step was a negotiation with the city's rhythm, a dance of survival where the wrong move could flatline you in a datastream dead-end.In the shadow of a megacorp tower, its glass skin reflecting a fractured sky, the air smelled of ozone and recycled dreams. He jacked into a public terminal, his fingers dancing over a haptic deck scarred with use, pulling fragments of encrypted chatter from the dark pools of the net. The data was alive, slippery, coded in dialects of machine and man, whispering secrets of a deal gone sour in some offshore server farm. Trust was a currency nobody carried anymore, not when every connection could be a trojan horse.She crouched in the alley, her smartcoat shimmering to match the graffiti-smeared wall behind her. The drone overhead was a cheap model, its sensors blind to her cloaking, but she didn\u2019t trust it to stay that way. Tech evolved fast here, and yesterday\u2019s edge was tomorrow\u2019s obsolescence. Her neural feed buzzed with alerts\u2014proximity warnings, encrypted pings from a contact she hadn\u2019t seen since the Osaka run. The city didn\u2019t sleep, and neither did its ghosts, their voices woven into the code that kept this sprawl alive.\"reply",
      "What a coincidence, I just saw Lorem Gibson referenced yesterday.  OP did you just read https://mbh4h.substack.com/p/neuromancer-2025-review-william... ?reply",
      "Yup. That's where I discovered it too. Greatness.reply",
      "LLM's have us spoiled. I'd like to see the ouput of something like nanogpt [1] trained on a Gibson corpus. Seems like a lot better looking result should be easy to achieve today.[1] https://github.com/eniompw/nanoGPTshakespearereply",
      "Fun idea, but I was hoping it would actually form grammatically correct sentences. Also proper capitalization wouldn't be that hard to implement.reply",
      "Basically markov chains trained on Gibsons books. Programming something like that is starting to feel like the \"hello world\" of information theory.Looks like somebody even made a cyberpunk style markov generator:https://www.reddit.com/r/Cyberpunk/comments/6g4weu/i_made_a_...https://en.wikipedia.org/wiki/Markov_chainreply",
      "Lorem gypsum dolor sit cementreply",
      "Lorem Bel-airum - Lyrics of Fresh Prince of Bel-Air _but in Latin_reply"
    ],
    "link": "http://loremgibson.com/",
    "first_paragraph": "\n\n\n"
  }
]