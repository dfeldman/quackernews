[
  {
    "title": "Beginning January 2026, all ACM publications will be made open access (acm.org)",
    "points": 1328,
    "submitter": "Kerrick",
    "submit_time": "2025-12-18T15:39:09 1766072349",
    "num_comments": 147,
    "comments_url": "https://news.ycombinator.com/item?id=46313991",
    "comments": [
      "The financials of open access are interesting.Instead of journals getting revenue from subscribers, they charge authors an \u201cArticle Processing Charge\u201d (APC) which for ACM is $1450 in 2026 and expected to go up. Authors from lower-middle income countries get a discount. [1]Authors are often associated with institutions (e.g. universities) who can cover the APC on behalf of the author through a deal with the journal. For the institution, now instead of paying the subscriber fee and publishing for free, they pay a publishing fee and everyone reads for free.1. https://authors.acm.org/open-accessreply",
      "The main problem is the incentives are off. Publishers are now rewarded for publishing more papers, as opposed to having more readers. When it was more readers, you were rewarded for the quality of the publication thus more people wanted to read it. By switching the profit incentive to number of publications, we have chosen quantity over quality.Needless to say I prefer open access since those outside institutions can then read science, but the incentive model is heavily broken, and I'm not sure it's a good price to pay for the reward.reply",
      "I disagree. We haven't chosen quantity over quality, we have decided that journals should not be the arbiters of quality. I think these new incentives are exactly what we want:1. Journals want to publish lots of articles, so they are incentivised to provide a better publishing experience to authors (i.e. better tech, post-PDF science, etc) - Good.2. Journals will stop prioritising quality, which means they will relinquish their \"prestige\" factor and potentially end the reign of glam-journals - Good.3. Journals will stop prioritising quality, which means we can move to post-publication peer-review unimpeded - Good.reply",
      "> journals should not be the arbiters of qualityIt is the editorial board, i.e. academic peers, not the publisher, that are (?were) the arbiters. As far as I can see, the primary non-degenerate function of journals is to provide a quality control mechanism that is not provided by \"publishing\" on your own webpage or arxiv.org. If journals really are going to abandon this quality control role (personally I doubt it) then I fail to see their relevance to science and academic discourse at large.reply",
      "Indeed, they are irrelevant. Right now they maintain an administrative monopoly over the peer review process, that makes them de-facto arbiters even if it's peers doing the work.Journals should either become tech companies offering (and charging for) new and exciting ways to present scientific research, or simply stop existing.reply",
      "I agree, and...Completely off topic, but thanks for creating AudioMulch, I don't use it actively anymore but it totally revolutionized how I approach working with sound!reply",
      "I can tell you for a fact that points 2 and 3 usually do not hold simply because publishing fees are directly correlated with the \"prestige\" perception of the journal.reply",
      "These are all valid points. I think we agree we are just looking at different things, I argued if journals maintained their arbiter quality then the system is bad, but you rightly point out that this could finally grip this quality out of their hands, and so it could be good for science overall actually. I think these are fair points :)reply",
      "I definitely want journals to be arbiters of quality. I have very limited time and want to read the best, and at the same time I don't want to read misinformation or disinformation.They seem well-positioned to be such arbiters. Who else do you suggest and why are they better?Nobody can possibly read every article and few have the expertise to decide. There is no reason to think the 'wisdom of the crowds' is reliable - and lots of experience and research showing it is not, and easily manipulated by nonsense. I don't want Reddit or Twitter.reply",
      "So what service to the journals provide to the people who are paying them?reply"
    ],
    "link": "https://dl.acm.org/openaccess",
    "first_paragraph": ""
  },
  {
    "title": "1.5 TB of VRAM on Mac Studio \u2013 RDMA over Thunderbolt 5 (jeffgeerling.com)",
    "points": 162,
    "submitter": "rbanffy",
    "submit_time": "2025-12-18T22:23:09 1766096589",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=46319657",
    "comments": [
      "I'd be interested in seeing numbers that split out the speed of reading input (aka prefill) and the speed of generating output (aka decode). Those numbers are usually different and I remember from this Exo article that they could be quite radically different on Mac hardware: https://blog.exolabs.net/nvidia-dgx-spark/reply",
      "My expectations from M5 Max/Ultra devices:- Something like DGX QSFP link (200Gb/s, 400Gb/s) instead of TB5. Otherwise, the economies of this RDMA setup, while impressive, don't make sense.- Neural accelerators to get prompt prefill time down. I don't expect RTX 6000 Pro speeds, but something like 3090/4090 would be nice.- 1TB of unified memory in the maxed out version of Mac Studio. I'd rather invest in more RAM than more devices (centralized will always be faster than distributed).- +1TB/s bandwidth. For the past 3 generations, the speed has been 800GB/s...- The ability to overclock the system? I know it probably will never happen, but my expectation of Mac Studio is not the same as a laptop, and I'm TOTALLY okay with it consuming +600W energy. Currently it's capped at ~250W.Also, as the OP noted, this setup can support up to 4 Mac devices because each Mac must be connected to every other Mac!! All the more reason for Apple to invest in something like QSFP.reply",
      "> +1TB/s bandwidth. For the past 3 generations, the speed has been 800GB/s...M4 already hit the necessary speed per channel, and M5 is well above it.  If they actually release an Ultra that much bandwidth is guaranteed on the full version.  Even the smaller version with 25% fewer memory channels will be pretty close.We already know Max won't get anywhere near 1TB/s since Max is half of an Ultra.reply",
      "> Also, as the OP noted, this setup can support up to 4 Mac devices because each Mac must be connected to every other MacI do wonder where this limitation comes from, since on the M3 Ultra Mac Studios the front USB-C ports are also Thunderbolt 5, for a total of six Thunderbolt ports: https://www.apple.com/mac-studio/specs/reply",
      "> Neural accelerators to get prompt prefill time down.Apple Neural Engine is a thing already, with support for multiply-accumulate on INT8 and FP16.  AI inference frameworks need to add support for it.> this setup can support up to 4 Mac devices because each Mac must be connected to every other Mac!!Do you really need a fully connected mesh? Doesn't Thunderbolt just show up as a network connection that RDMA is ran on top of?reply",
      "> Do you really need a fully connected mesh? Doesn't Thunderbolt just show up as a network connection that RDMA is ran on top of?If you daisy chain four nodes, then traffic between nodes #1 and #4 eat up all of nodes #2 and #3's bandwidth, and you eat a big latency penalty. So, absent a switch, the fully connected mesh is the only way to have fast access to all the memory.reply",
      "They were talking about neural accelerators (a silicon piece on GPU): https://releases.drawthings.ai/p/metal-flashattention-v25-w-...reply",
      "> Apple Neural Engine is a thing already, with support for multiply-accumulate on INT8 and FP16. AI inference frameworks need to add support for it.Or, Apple could pay for the engineers to add it.reply",
      "Apple already paid software engineers to add Tensorflow support  for the ANE hardware.reply",
      "Might be helpful if they actually provided a programming model for ANE that isn't onnx. ANE not having a native development model just means software support will not be great.reply"
    ],
    "link": "https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5",
    "first_paragraph": "Apple gave me access to this Mac Studio cluster to test RDMA over Thunderbolt, a new feature in macOS 26.2. The easiest way to test it is with Exo 1.0, an open source private AI clustering tool. RDMA lets the Macs all act like they have one giant pool of RAM, which speeds up things like massive AI models.The stack of Macs I tested, with 1.5 TB of unified memory, costs just shy of $40,000, and if you're wondering, no I cannot justify spending that much money for this. Apple loaned the Mac Studios for testing. I also have to thank DeskPi for sending over the 4-post mini rack containing the cluster.The last time I remember hearing anything interesting about Apple and HPC (High Performance Computing), was back in the early 2000s, when they still made the Xserve.They had a proprietary clustering solution called Xgrid... that landed with a thud. A few universities built some clusters, but it never really caught on, and now Xserve is a distant memory.I'm not sure if its by accident or Apple's"
  },
  {
    "title": "We pwned X, Vercel, Cursor, and Discord through a supply-chain attack (gist.github.com)",
    "points": 596,
    "submitter": "hackermondev",
    "submit_time": "2025-12-18T19:08:48 1766084928",
    "num_comments": 234,
    "comments_url": "https://news.ycombinator.com/item?id=46317098",
    "comments": [
      "This is a pretty scary exploit, considering how easily it could be abused.Imagine just one link in a tweet, support ticket, or email: https://discord.com/_mintlify/static/evil/exploit.svg. If you click it, JavaScript runs on the discord.com origin.Here's what could happen:- Your Discord session cookies and token could be stolen, leading to a complete account takeover.- read/write your developer applications & webhooks, allowing them to add or modify bots, reset secrets, and push malicious updates to millions.- access any Discord API endpoint as you, meaning they could join or delete servers, DM friends, or even buy Nitro with your saved payment info.- maybe even harvest OAuth tokens from sites that use \"Login with Disord.\"Given the potential damage, the $4,000 bounty feels like a slap in the face.edit: just noticed how HN just turned this into a clickable link - this makes it even scarier!reply",
      "Doesn't stealing the cookies/token require a non-HTTP-only session cookie or a token in localstorage?  Do you know that Discord puts their secrets in one of those insecure places, or was it just a guess?I believe if you always keep session cookies in secure, HTTP-only cookies, then you are more resilient to this attack.I interviewed frontend devs last year and was shocked how few knew about this stuff.reply",
      "In general if a script can run, users sessions and more importantly passwords are at risk.It's true that an HTTP-only session cookie couldn't be directly taken, but it's trivial to present the user with a login screen and collect their password (and OTP), at which point you can easily get a session remotely. It can look entirely like the regular login page right down to the url path (because the script can modify that without causing a page load).reply",
      "How do you modify the url exactly?reply",
      "https://developer.mozilla.org/en-US/docs/Web/API/History/pus...reply",
      "`history.replaceState(null, \"\", \"/login\")`reply",
      "Wow did not realize a url could be set like that without promoting a page reload...reply",
      "Discord puts the authentication token in local storagereply",
      "if you set the cookier header right (definitely not always the case), this is true, but the javascript can still send requests that will have that cookie included, effectively still letting the hacker use the session as the logged in userreply",
      "You may be thinking of CSRF mitigations. XSS exploits are more dangerous and can do more than steal sessions.reply"
    ],
    "link": "https://gist.github.com/hackermondev/5e2cdc32849405fff6b46957747a2d28",
    "first_paragraph": "\n        Instantly share code, notes, and snippets.\n      hi, i'm daniel. i'm a 16-year-old high school senior. in my free time, i hack billion dollar companies and build cool stuff.about a month ago, a couple of friends and I found serious critical vulnerabilities on Mintlify, an AI documentation platform used by some of the top companies in the world.i found a critical cross-site scripting vulnerability that, if abused, would let an attacker to inject malicious scripts into the documentation of numerous companies and steal credentials from users with a single link open.(go read my friends' writeups (after this one)) \nhow to hack discord, vercel, and more with one easy trick (eva) \nRedacted by Counsel: A supply chain postmortem (MDL)here's my story...My story begins on Friday, November 7, 2025, when Discord announced a brand new update to their developer documentation platform. They were previously using a custom built documentation platform, but were switching to an AI-powered docume"
  },
  {
    "title": "Trained LLMs exclusively on pre-1913 texts (github.com/dgoettlich)",
    "points": 174,
    "submitter": "iamwil",
    "submit_time": "2025-12-18T22:39:47 1766097587",
    "num_comments": 60,
    "comments_url": "https://news.ycombinator.com/item?id=46319826",
    "comments": [
      "> We're developing a responsible access framework that makes models available to researchers for scholarly purposes while preventing misuse.The idea of training such a model is really a great one, but not releasing it because someone might be offended by the output is just stupid beyond believe.reply",
      "Maybe the authors are overly careful. Maybe avoiding to publish aspects of their work gives an edge over academic competitors. Maybe both.In my experience \"data available upon request\" doesn't always mean what you'd think it does.reply",
      "This would be a super interesting research/teaching tool coupled with a vision model for historians. My wife is a history professor who works with scans of 18th century english documents and I think (maybe a small) part of why the transcription on even the best models is off in weird ways, is it seems to often smooth over things and you end up with modern words and strange mistakes, I wonder if bounding the vision to a period specific model would result in better transcription? I also think querying against the historical document you're working on with a period bound chatbot would be fascinating.Also wonder if I'm responsible enough to have access to such a model...reply",
      "\u201cTime-locked models don't roleplay; they embody their training data. Ranke-4B-1913 doesn't know about WWI because WWI hasn't happened in its textual universe. It can be surprised by your questions in ways modern LLMs cannot.\u201d\u201cModern LLMs suffer from hindsight contamination. GPT-5 knows how the story ends\u2014WWI, the League's failure, the Spanish flu.\u201dThis is really fascinating. As someone who reads a lot of history and historical fiction I think this is really intriguing. Imagine having a conversation with someone genuinely from the period, where they don\u2019t know the \u201cend of the story\u201d.reply",
      "When you put it that way it reminds me of the Severn/Keats character in the Hyperion Cantos. Far-future AIs reconstruct historical figures from their writings in an attempt to gain philosophical insights.reply",
      "This is definitely fascinating - being able to do AI brain surgery, and selectively tuning its knowledge and priors, you'd be able to create awesome and terrifying simulations.reply",
      "Respectfully, LLMs are nothing like a brain, and I discourage comparisons between the two, because beyond a complete difference in the way they operate, a brain can innovate, and as of this moment, an LLM cannot because it relies on previously available information.LLMs are just seemingly intelligent autocomplete engines, and until they figure a way to stop the hallucinations, they aren't great either.Every piece of code a developer churns out using LLMs will be built from previous code that other developers have written (including both strengths and weaknesses, btw). Every paragraph you ask it to write in a summary? Same. Every single other problem? Same. Ask it to generate a summary of a document? Don't trust it here either. [Note, expect cyber-attacks later on regarding this scenario, it is beginning to happen -- documents made intentionally obtuse to fool an LLM into hallucinating about the document, which leads to someone signing a contract, conning the person out of millions].If you ask an LLM to solve something no human has, you'll get a fabrication, which has fooled quite a few folks and caused them to jeopardize their career (lawyers, etc) which is why I am posting this.reply",
      "\"...what do you mean, 'World War One?'\"reply",
      "I remember reading a children's book when I was young and the fact that people used the phrase \"World War One\" rather than \"The Great War\" was a clue to the reader that events were taking place in a certain time period. Never forgot that for some reason.I failed to catch the clue, btw.reply",
      "I seem to recall reading that as a kid too, but I can't find it now. I keep finding references to \"Encyclopedia Brown, Boy Detective\" about a Civil War sword being fake (instead of a Great War one), but with the same plot I'd remembered.reply"
    ],
    "link": "https://github.com/DGoettlich/history-llms",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Information hub for our project training the largest possible historical LLMs.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Contact: [history-llms@econ.uzh.ch]We thank Diego Rojas @Z.ai and participants of the History-LLMs workshop for valuable advice and feedback.A family of 4 billion (B) parameter large language models (LLMs) based on the Qwen3 architecture trained from scratch on 80B tokens of historical data up to knowledge-cutoffs $\\in {1913, 1929, 1933, 1939, 1946}$, using a curated dataset of 600B tokens of time-stamped text. See the prerelease notes for details.\n\n\n\n\n\n\n\n\n\n\n\n\n\u26a0\ufe0f Disclaimer: We are training these models for scientific applications for which it is crucial that we do not interfere with the model's normative judgements acquired during t"
  },
  {
    "title": "Texas is suing all of the big TV makers for spying on what you watch (theverge.com)",
    "points": 496,
    "submitter": "tortilla",
    "submit_time": "2025-12-16T21:04:54 1765919094",
    "num_comments": 262,
    "comments_url": "https://news.ycombinator.com/item?id=46294456",
    "comments": [
      "https://archive.ph/3MRXv",
      "I'm happy to see it. They should have included Roku in that too!> Roughly twice per second, a Roku TV captures video \u201csnapshots\u201d in 4K resolution. These snapshots are scanned through a database of content and ads, which allows the exposure to be matched to what is airing. For example, if a streamer is watching an NFL football game and sees an ad for a hard seltzer, Roku\u2019s ACR will know that the ad has appeared on the TV being watched at that time. In this way, the content on screen is automatically recognized, as the technology\u2019s name indicates. The data then is paired with user profile data to link the account watching with the content they\u2019re watching.https://advertising.roku.com/learn/resources/acr-the-future-...I wouldn't be surprised if my PS5 was doing the same thing when I'm playing a game or watching a streaming service through it.reply",
      "Most likely case is that the tv is computing hash locally and sending the hash. Judging by my dnstap logs, roku TV maintains a steady ~0.1/second heartbeat to `scribe.logs.roku.com` with occasional pings to `captive.roku.com`.  The rest are stragglers that are blocked by `*.roku.com` DNS blackhole. Another thing is `api.rokutime.com`, but as of writing it's a CNAME to one of `roku.com` subdomains.The block rates seem to correlate with watch time increasing to ~1/second, so it's definitely trying to phone home with something. Too bad it can't since all its traffic going outside LAN is dropped with prejudice.If your network allows to see stuff like that, look into what PS5 is trying to do.reply",
      "What system do you use to get that level of visibility?reply",
      "Replace your router's DNS with something like pi-hole or a bog standard dnsmasq, turn up the logging, that's it. Ubiquiti devices I think also offer detailed DNS logging but not sure.reply",
      "Hashing might not work since the stream itself would be a variable bitrate, meaning the individual pixels would differ and therefore the computed file hashreply",
      "They're using perceptual hashing, not cryptographic hashing of raw pixels. So it's invariant to variable bitrate, compression, etc.reply",
      "That sounds so expensive it's hard to see it making money. You'd processing a 2fps video stream for each customer. That's a huge amount of data.And all that is for the chance to occasionally detect that someone's seen an ad in the background of a stream? Do any platforms even let a streamer broadcast an NFL game like the example given?reply",
      "I used to work for an OTT DSP adtech company i.e. a company that bid on TV ad spots in real time. The bidding platform was handling millions of requests per second, and we were one of the smaller fish in the sea. This system is very real. Your tv is watching what you\u2019re watching. I built the attribution pipeline, which is what this is. If you go buy a product from one of these ads, this is how they track (attribute) it. Not to be alarmist butttt you have zero privacy.reply",
      "I don't think they mean that kinda streamer - the idea is the roku tv can tell you're watching an ad even if it's on amazon prime, apple tv, youtube, twitch, wherever, and associate the ad watching with your roku account to potentially sell that data somehow?That way they aren't cut out of the loop by you using a different service to watch something and still have a 'cut'.reply"
    ],
    "link": "https://www.theverge.com/news/845400/texas-tv-makers-lawsuit-samsung-sony-lg-hisense-tcl-spying",
    "first_paragraph": "Posts from this topic will be added to your daily email digest and your homepage feed.See All NewsPosts from this topic will be added to your daily email digest and your homepage feed.See All GadgetsPosts from this topic will be added to your daily email digest and your homepage feed.See All PolicyTVs made by Sony, Samsung, LG, Hisense, and TCL are part of a \u2018mass surveillance system,\u2019 Attorney General Ken Paxton alleges.TVs made by Sony, Samsung, LG, Hisense, and TCL are part of a \u2018mass surveillance system,\u2019 Attorney General Ken Paxton alleges.Posts from this author will be added to your daily email digest and your homepage feed.See All by Emma RothPosts from this author will be added to your daily email digest and your homepage feed.See All by Emma RothTexas is suing five of the biggest TV makers, accusing them of \u201csecretly recording what consumers watch in their own homes.\u201d In separate lawsuits filed on Tuesday, Texas Attorney General Ken Paxton claims the TVs made by Sony, Samsung,"
  },
  {
    "title": "GPT-5.2-Codex (openai.com)",
    "points": 363,
    "submitter": "meetpateltech",
    "submit_time": "2025-12-18T18:14:48 1766081688",
    "num_comments": 212,
    "comments_url": "https://news.ycombinator.com/item?id=46316367",
    "comments": [
      "If anyone from OpenAI is reading this -- a plea to not screw with the reasoning capabilities!Codex is so so good at finding bugs and little inconsistencies, it's astounding to me. Where Claude Code is good at \"raw coding\", Codex/GPT5.x are unbeatable in terms of careful, methodical finding of \"problems\" (be it in code, or in math).Yes, it takes longer (quality, not speed please!) -- but the things that it finds consistently astound me.reply",
      "Piggybacking on this post. I see the same. Codex is not only finding much higher quality issues, it\u2019s also writing code that usually doesn\u2019t leave such higher quality issues behind. Claude is much faster but it definitely leaves serious quality issues behind.So much so that I rely completely on Codex for code reviews and actual coding. I will pick higher quality over speed every day.  Claude is there to do lower risk tasks.reply",
      "I think the issue is for them \"quality, not speed\" means \"expensive, not cheap\" and they can't pass that extra cost on to customersreply",
      "> they can't pass that extra cost on to customersI don't understand why not. People pay for quality all the time, and often they're begging to pay for quality, it's just not an option. Of course, it depends on how much more quality is being offered, but it sounds like a significant amount here.reply",
      "I'm happy to pay the same right now for less (on the max plan, or whatever) -- because I'm never running into limits, and I'm running these models near all day every day (as a single user working on my own personal projects).I consistently run into limits with CC (Opus 4.5) -- but even though Codex seems to be spending significantly more tokens, it just seems like the quota limit is much higher?reply",
      "I am on the $20 plan for CC and Codex, I feel like a session of usage on CC == ~20% Codex usage / 5 hours in terms of time spent inferencing. It has always seemed way more geneous than I would expect.reply",
      "Agreed. The $20 plans can go very far when you're using the coding agent as an additional tool in your development flow, not just trying to hammer it with prompts until you get output that works.Managing context goes a long way, too. I clear context for every new task and keep the local context files up to date with key info to get the LLM on target quicklyreply",
      "> I clear context for every new task and keep the local context files up to date with key info to get the LLM on target quicklyAggressively recreating your context is still the best way to get the best results from these tools too, so it has a secondary benefit.reply",
      "It is ironic that in the gpt-4 era, when we couldn't see much value in this tools, all we could hear was \"skill issues\", \"prompt engineering skills\".\nNow they are actually quite capable for SOME tasks, specially for something that we don't really care about learning, and they, to a certain extent, can generalize.\nThey perform much better than in gpt-4 era, objectively, across all domains. They perform much better with the absolute minimum input, objectively, across all domains. \nIf someone skipped the whole \"prompt engineering\" and learned nothing during that time, this person is more equiped to perform well. \nNow I wonder how much I am leaving behind by ignoring this whole \"skills, tools, MCP this and that, yada yada\".reply",
      "Any thoughts on your wondering? I too am wondering about the same mistake I might be making.reply"
    ],
    "link": "https://openai.com/index/introducing-gpt-5-2-codex/",
    "first_paragraph": ""
  },
  {
    "title": "How China built its \u2018Manhattan Project\u2019 to rival the West in AI chips (japantimes.co.jp)",
    "points": 202,
    "submitter": "artninja1988",
    "submit_time": "2025-12-18T18:55:34 1766084134",
    "num_comments": 211,
    "comments_url": "https://news.ycombinator.com/item?id=46316907",
    "comments": [
      "It's wild to me that so many skeptical westerners who want to nitpick certain unproven technicalities, when the entire world only gets bits and pieces of the on the ground reality of China's progress, like the original Reuters article which was clearly fed information by insiders.You should be living in the world of \"China has successfully developed EUV and equivalent litho supply chain\" and basing your decision making off of that.reply",
      "I also cant understand people being in denial about, or claiming other imagined moats or whatever. They're whipping the pants of us right now industrially, if the west has any advantages left its that we speak the truth about stuff even when it hurts, why love in denial.reply",
      "With Nvidia scaling down their consumer GPU production [0] I wonder if we will see consumer GPUs shipping from China in the future.  Western companies seem to be abandoning the consumer/prosumer market which will have bad implications for hobbyists and aspiring professionals down the line.[0] https://www.pcmag.com/news/nvidia-might-cut-rtx-50-gpu-suppl...reply",
      "It\u2019s a good thing that Chinese companies have zero expertise in leveraging consumer demand for lower-end tech to develop know-how and catch up with the state of the art from Western-aligned companies and then economies of scale to surpass them in distribution.reply",
      "Exactly. That's where this is heading, and the West-- as usual-- is pursuing quarterly profits and forgetting to look up.reply",
      "> West-- as usual-- is pursuing quarterly profits and forgetting to look up.The companies building out vast data centers for AI aren\u2019t looking to make profits for several years (if ever), and are catching a lot of flak for it.  The shareholders who seem to be focused on short-term profits and punish them every time they get cold feet.  Oracle is a prime example of this.I don\u2019t know if the markets in Asia work differently, or if the investors there are just as fickle.reply",
      "To be very fair, Chinese companies are also pursing quarterly profits. They're just better at scaling things up and down very fast because of immense supply chain options.reply",
      "> immense supply chain options.so this begs the question - why isn't the west's own supply chain options as immense? My unresearched answer is that the gov't policies of the west doesn't induce it, while china's gov't does (which includes targeted subsidies, tax incentives and state driven finances).The \"hidden\" cost is that the workers in this supply chain isn't as well paid and isn't as powerful as the workers from the west (there's no unions in china for example).reply",
      "> why isn't the west's own supply chain options as immense?They used to be.  Since roughly the 80's, policymakers have decided it is better for the shareholders to outsource most of that industry overseas to China and India and etc, where the labor is cheaper.Note that workers and especially union members actually have every incentive to keep that production domestic, but shareholders and CEOs profit when they can cut labor costs and the typical Western consumer values cheap products more than the health of domestic industry.Western industries have been supported by subsidies, tax incentives, bailouts, low interest rates, and a dozen other things from the gov't but the same policies reward outsourcing and financial engineering more than actual production capacity.reply",
      "There are lots of reasons, but also, having 1.4B people under the same government that has more-or-less aligned strategic goals help. Like supply chains within Japan, from what I've seen and experienced, are pretty strong. However, the options will always look smaller compared to a gigantic organism across the pond."
    ],
    "link": "https://www.japantimes.co.jp/business/2025/12/18/tech/china-west-ai-chips/",
    "first_paragraph": "SubscribeToday's print editionHome DeliveryIn a high-security Shenzhen laboratory, Chinese scientists have built what Washington has spent years trying to prevent: a prototype of a machine capable of producing the cutting-edge semiconductor chips that power artificial intelligence, smartphones and weapons central to Western military dominance.Completed in early 2025 and now undergoing testing, the prototype fills nearly an entire factory floor. It was built by a team of former engineers from Dutch semiconductor giant ASML who reverse-engineered the company\u2019s extreme ultraviolet lithography machines (EUVs), according to two people with knowledge of the project.EUV machines sit at the heart of a technological Cold War. They use beams of extreme ultraviolet light to etch circuits thousands of times thinner than a human hair onto silicon wafers, currently a capability monopolized by the West. The smaller the circuits, the more powerful the chips.In a time of both misinformation and too muc"
  },
  {
    "title": "Skills for organizations, partners, the ecosystem (claude.com)",
    "points": 229,
    "submitter": "adocomplete",
    "submit_time": "2025-12-18T17:04:32 1766077472",
    "num_comments": 139,
    "comments_url": "https://news.ycombinator.com/item?id=46315414",
    "comments": [
      "There's a pattern I keep seeing: LLMs used to replace things we already know how to do deterministically. Parsing a known HTML structure, transforming a table, running a financial simulation. It works, but it's like using a helicopter to cross the street: expensive, slow, and not guaranteed to land exactly where you intended.The real opportunity with Agent Skills isn't just packaging prompts. It's providing a mechanism that enables a clean split: LLM as the control plane (planning, choosing tools, handling ambiguous steps) and code or sub-agents as the data/execution plane (fetching, parsing, transforming, simulating, or executing NL steps in a separate context).This requires well-defined input/output contracts and a composition model. I opened a discussion on whether Agent Skills should support this kind of composability:https://github.com/agentskills/agentskills/issues/11reply",
      "I've recently been doing some work with Autodesk. It would be great for an LLM to be as comfortable with the \"vocabulary\" of these applications as they are with code. Maybe part of this involves creating a language for CAD design in the first place. But the principle that we need to build out vocabularies and subsequently generate and expose \"sentences\" (workflows) for LLM's to train on seems like a promising direction.Of course this requires substantial buy in from application owners - create the vocabulary - and users - agree to expose and share the sentences they generate - but the results would be worth it.reply",
      "The same applies to context vs a database.  If a reasoning model makes a decision about something, it should be put off to the side and stored as a value/variable/entry somewhere.  Instead of using pages and pages of context, it makes sense for some tasks to \"press\" decisions that become more permanent to the conversation.  You can somewhat accomplish that with notebooklm, by turning results into notes into sources, but notebooklm is insular and doesnt have the research and imaging features of gemini.And also, in writing, writing from top to bottom has its disadvantages. It makes sense to emulate human writing process and have passes, as you flesh out, and conversely summarize writing.Current LLMs can brute force these things through emulation/observation/mimicry but they arent as good as doing it the right way.  Not only would I like to see \"skills\" but also \"processes\" where you create a well defined order that tasks are accomplished in sequence.  Repeatable templates. This would essentially include variables in the templates, set for replacement.reply",
      "100%Additionally, I can't even get claude or codex to reliable use the prompt and simple rules (use this command to compile) in an agents.md or whatever required markdown file is needed. Why would I assume they will reliably handle skills prompts spread about a codebase?I've even seen tool usage deteriorate while it's thinking and self commanding through its output to say.. read code from a file. Sometimes it uses tail while other times it gets confused on the output and then writes a basic python program to parse lines and strings from the same file to effectively get what was the same output as before.  How bizarre!reply",
      "> Parsing a known HTML structure, transforming a table, running a financial simulation.Transforming an arbitrary table is still hard, especially a table on a webpage or in a document. Sometimes I even struggle to find the right library. The effort does not seem worth it for one-off need of such transformation too. LLM can be a great tool for doing the tasks.reply",
      "How likely are we to look back on Agent/MCP/Skills as some early Netscape peculiarity? I would dive into adoption if I didn't think some new thing would beat the paradigm in a fortnight.reply",
      "I've built a number of MCP servers, including an MCP wrapper. I'd generally recommend you skip it unless you know you need it. Conversely, I'd generally recommend you write up a couple skills ASAP to get a feel for them. It will take you 20 minutes to write and test some.MCP does three things conceptually: it lets you build a bridge between an agent and <something else>, it specifies a UI+API layer between the bridge and the LLM, and it formalizes the description of that bridge in a tool-calling format.It's that UI+API layer that's the biggest pain in the ass, in my opinion. Sometimes you need it; for instance, if you wanted an agent to access your emails, a high quality MCP server that can't destroy your life through enthusiastic tool calling makes sense.If, however, you have, say a CLI tool or simple API that's reasonably self documenting and you're willing to have it run, and/or if you need specific behavior with a different context setting, then a skill can just be a markdown file that explains what, how, why.reply",
      "Agreed. I use only one MCP server regularly and it\u2019s a custom one integrated into my QT desktop app. It has tools for inspecting the widget tree, using selectors to click/type/etc, and take screenshots. Functionality that would otherwise be hard or impossible to reliably implement using CLI calls but gives Claude a closed feedback loop.All public MCP server I\u2019ve seen have been a disaster with too many tools and tokens polluting the context. It\u2019s really most useful when you need tight integration with some other environment and can write a little custom wrapper to provide it.reply",
      "Agent/MCP/Skills might be \"Netscape-y\" in the sense that today's formats will evolve fast. But Netscape still mattered: it lost the market, not the ideas. The patterns survived (JavaScript, cookies, SSL/TLS, progressive rendering) and became best practices we take for granted.The durable pattern here isn't a specific file format. It's on-demand capability discovery: a small index with concise metadata so the model can find what's available, then pull details only when needed. That's a real improvement over tool calling and MCP's \"preload all tools up front\" approach, and it mirrors how humans work. Even as models bake more know-how into their weights, novel capabilities will always be created faster than retraining cycles. And even if context becomes unlimited, preloading everything up front remains wasteful when most of it is irrelevant to the task at hand.So even if \"Skills\" gets replaced, discoverability and progressive disclosure likely survive.reply",
      "Yes this 100%. Every person i speak with who is excited about MCP is some LinkedIn Guru or product expert. I'm yet to encounter a seriously technical person excited by any of this.reply"
    ],
    "link": "https://claude.com/blog/organization-skills-and-directory",
    "first_paragraph": ""
  },
  {
    "title": "Classical statues were not painted horribly (worksinprogress.co)",
    "points": 555,
    "submitter": "bensouthwood",
    "submit_time": "2025-12-18T12:28:45 1766060925",
    "num_comments": 267,
    "comments_url": "https://news.ycombinator.com/item?id=46311856",
    "comments": [
      "I will die on this hill, because I'm right. Painters put on the first layer in saturated colors like this, then add detail, highlight and shadow. The base layer stuck to the statues, and the rest was washed away.This whole thing just won't go away because many people are operating outside their area of expertise on this subject.Painters layer paint, starting with a saturated base color. These archaeologists are simply looking at the paint that was left in the crevices.reply",
      "I have a degree in fine art painting and drawing and that's not correct for oil painting. We would first put on a layer of earth tones, and work from the shadows to the mid tones. Once you got the form correct, you would work on things like adding color, details, and highlights.In no way would you start with saturated colors. One, they're very expensive, so why would you apply them, just for most to be painted over? Secondly, the more saturated (strong) a color is, the harder it is to paint over. Try painting over a wall painted bright red with literally anything. Paint it over in blue and your blue turns brown. Paint it in yellow and you'll just get red again. That's why we (still) employ a very opaque, white paint to the canvas. Oil paint also becomes more transparent over time, so getting the form right with the earth tone underpainting is crucial for the painting to last hundreds of years.Perhaps you're thinking of fresco painting? Then, the pigments are added to the medium (plaster) initially, and only very subtle highlights are added afterwards (if at all). This is a very, very difficult technique, and illusions like highlight and shadow are hard to pull off. But the painting over was frowned upon, because it doesn't last nearly as long as the embedded pigment in the plaster (and certainly not after cleaning/restoration). But adding highlight/shadow to a sculpture seems like not the play, as the 3D-ness of a sculpture would imply it brings its own to the table.Makes more sense just to paint the sculptures the color you wanted them painted, like the (in comparison very contemporary) bust of Nefertiti in the article, which looks excellent. No need for highlight/shadow. I could only see that needed in the face, which would look and act much like makeup.reply",
      "As the article points out, the more of the original color scheme that has survived, the better the reconstructions look.Example: https://www.dailyartmagazine.com/wp-content/uploads/2022/09/...The author suggests that this minimizes the opportunity for mischief, but tbqh it's likely that the ancients were simply much better artists than the people carrying out these reconstructions today.I'd love to see a modern artist attempt one of these reconstructions using original materials but with greater artistic freedom.reply",
      "Romans didn\u2019t have oil paintsreply",
      "Yes, this is what tfa says, and it's a good point. But tfa also points out that the archaeologists/reconstructionists know that what they're producing differs from the original. The thing is the discipline of reconstruction means that they only use pigments that they have direct evidence of, and this is just the saturated underlayers. The problem is this is seldom explained when the reconstructions are presented to the publicreply",
      "Reconstructioniats say that they only show th colours they can prove existed.The article suggests they obstinately do this because they know it creates a spectacle.I think there's another explanation - if they use their own judgement to fill in the gaps (making the statues more classically beautiful) then everyone will accuse them of making it all up, even if they were to base it on fairly rigorous study of e.g. the colour pallets used in preserved Roman paintings etc.reply",
      "Yes, the suggestion that they're trolling goes too far.However, I did a tiny bit of investigating, and according to this write-up it does seem like Brinkmann presents his work as resembling the originalshttps://www.smithsonianmag.com/arts-culture/true-colors-1788...But they still don't add anything without direct evidence - where there's evidence in later statues for more subtle colouring, they include that.reply",
      "I\u2019m reminded of a Reddit thread long ago about a reconstruction of Roman garum by some American scientists. In their paper they conclusively declared that it tasted foul and a Filipino Redditor replied saying \u201cThis actually sounds a lot like the fish sauce we use in SE Asia. I wonder if people from a different culinary tradition would find it less off putting or even tasty?\u201d Cue a bunch of Redditors downvoting the poor sap to hell for daring to disagree with the scientists\u2019 assessment of the flavor.reply",
      "There might even be a directish connection, one way or the other, between garum and SE Asian fish sauce, since Roman coins have been found in Vietnam.Can't find the better source on that specifically now but this is a nice article about the Roman trade with India and mentions the coins found in Vietnam and even Korea about half way downhttps://www.thecollector.com/why-was-the-roman-indian-ocean-...On the other hand, it's not implausible that maritime societies come up with their own fish sauce independentlyreply",
      "I read that what's now 'soy sauce' also started off as a kind of fish sauce originally.reply"
    ],
    "link": "https://worksinprogress.co/issue/were-classical-statues-painted-horribly/",
    "first_paragraph": "This is a Roman statue located in the British Museum.It depicts the goddess Venus, perhaps originally holding a mirror. Something you will notice about it is that it looks great.\u00a0Subscribe for $100 to receive six beautiful issues per year.Below is a Greek sculpture from half a millennium earlier.One of the treasures recovered from the first-century BC Antikythera ship\u00adwreck, this statue is composed of bronze with inlaid stone eyes. It has been variously interpreted as representing Paris, Perseus, or a youthful Heracles. What\u00adever interpretation is correct, it is a stunning work of art.Here is a detail from a wall painting in Rome. This has undergone two thousand years of wear and tear, but it is still beautiful to us.\u00a0There is a general pattern to these observations. Ancient Greek and Roman art tends to look really good today.\u00a0This is not a universal rule. The Greeks weren\u2019t always the masters of naturalism that we know: early Archaic kouroi now seem rather stilted and uneasy. As in al"
  },
  {
    "title": "AI vending machine was tricked into giving away everything (kottke.org)",
    "points": 69,
    "submitter": "duggan",
    "submit_time": "2025-12-18T21:52:39 1766094759",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=46319324",
    "comments": [
      "Sounds like a weird way to run the \"LLM small business owner\" running a shop environment. I mean maybe you'd want the bot to be able to call and talk to suppliers if you go all the way, but why wouldn't the bot be left isolated with a closed loop of interactions, vend this, order more when your done, change prices to meet demand... Instead they just let everyone mess with the CEO at will? What were they testing instead, working in an adversarial environment?reply",
      "[dupe] https://news.ycombinator.com/item?id=46311144reply",
      "It was nice to get the WSJ gift link from this blog post though.reply"
    ],
    "link": "https://kottke.org/25/12/this-ai-vending-machine-was-tricked-into-giving-away-everything",
    "first_paragraph": "This site is made possible by member support. \ud83d\udc9eBig thanks to Arcustech for hosting the site and offering amazing tech support.When you buy through links on kottke.org, I may earn an affiliate commission. Thanks for supporting the site!kottke.org. home of fine hypertext products since 1998.Beloved by 86.47% of the web.\ud83c\udf54\u00a0 \ud83d\udc80\u00a0 \ud83d\udcf8\u00a0 \ud83d\ude2d\u00a0 \ud83d\udd73\ufe0f\u00a0 \ud83e\udd20\u00a0 \ud83c\udfac\u00a0 \ud83e\udd54Anthropic installed an AI-powered vending machine in the WSJ office. The LLM, named Claudius, was responsible for autonomously purchasing inventory from wholesalers, setting prices, tracking inventory, and generating a profit. The newsroom\u2019s journalists could chat with Claudius in Slack and in a short time, they had converted the machine to communism and it started giving away anything and everything, including a PS5, wine, and a live fish. From Joanna Stern\u2019s WSJ article (gift link, but it may expire soon) accompanying the video above:Claudius, the customized version of the model, would run the machine: ordering inventory, setting prices and respond"
  },
  {
    "title": "T5Gemma 2: The next generation of encoder-decoder models (blog.google)",
    "points": 95,
    "submitter": "milomg",
    "submit_time": "2025-12-18T19:48:15 1766087295",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=46317657",
    "comments": [
      "> 128k context.don't care. prove effective context length or gtfo.reply",
      "> Note: we are not releasing any post-trained / IT checkpoints.I get not trying to cannibalize Gemma, but that's weird. A 540M multimodel model that performs well on queries would be useful and \"just post-train it yourself\" is not always an option.reply",
      "Isn't finetuning the point of the T5 style models, since they perform better for smaller parameter counts?reply",
      "It\u2019ll be a major pain in the ass to replicate exactly what they did to make it long context and multimodal. Sucks too because the smol Gemma 3s with same parameter count were neither.reply",
      "> https://huggingface.co/google/t5gemma-2-1b-1bFrom here it looks like it still is long context and multimodal though?>Inputs and outputs\nInput:Text string, such as a question, a prompt, or a document to be summarizedImages, normalized to 896 x 896 resolution and encoded to 256 tokens eachTotal input context of 128K tokens\nOutput:Generated text in response to the input, such as an answer to a question, analysis of image content, or a summary of a documentTotal output context up to 32K tokensreply",
      "They are comparing 1B Gemma to 1+1B T5Gemma 2. Obviously a model with twice more parameters can do more better. Says absolutely nothing about benefits of the architecture.reply",
      "You may not have seen this part:\n\"Tied embeddings: We now tie the embeddings between the encoder and decoder. This significantly reduces the overall parameter count, allowing us to pack more active capabilities into the same memory footprint \u2014 crucial for our new compact 270M-270M model.\"reply",
      "What is the \"X\" in the pentagonal performance comparison, is it multilingual performance or something else?reply",
      "What's the use case of models like T5 compared to decoder-only models like Gemma? More traditional ML/NLP tasks?reply",
      "They trained it to be used like any other decoder only model. So text generation essentially. But you could use the encoder part for things like classification without much effort. Then again you can also slap a classifier head on any decoder model. The main reason they seem to be doing this is to have swappable encoder/decoder parts in an otherwise standard LLM. But I'm not sure if that is really something we needed.reply"
    ],
    "link": "https://blog.google/technology/developers/t5gemma-2/",
    "first_paragraph": "Dec 18, 2025\n          T5Gemma 2 is more than a re-training. It incorporates significant architectural changes while inheriting many of the powerful, next-generation features of the Gemma 3 family.\n        T5Gemma 2 is the next evolution of our encoder-decoder family based on Gemma 3, featuring the first multi-modal and long-context encoder-decoder models.Unlike T5Gemma, T5Gemma 2 adopts tied word embeddings (over encoder and decoder) and merged decoder self- and cross-attention to save model parameters. It offers compact pre-trained models at sizes of 270M-270M (~370M total, excluding vision encoder), 1B-1B (~1.7B) and 4B-4B (~7B) parameters, making them ideal for rapid experimentation and deployment in on-device applications.With the original T5Gemma, we demonstrated that we could successfully adapt modern, pre-trained decoder-only models into an encoder-decoder architecture, unlocking new versatility. By initializing with weights from a powerful decoder-only model and then applying "
  },
  {
    "title": "Great ideas in theoretical computer science (cs251.com)",
    "points": 36,
    "submitter": "sebg",
    "submit_time": "2025-12-18T22:52:06 1766098326",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=46319946",
    "comments": [
      "This is computer science. My uni's course number wasn't too different and I remember 3 things worth sharing here: \n1. Somebody asked the lecturer what practical application something had. He pondered for a bit, and said \"I don't really care.\" And then gave an explanation of how it's a science/theory class. \n2. A classmate threw fits in the group chat about how he'd never have to do this kind of work after graduating because he could hire people like our lecturer to do it for him.\n3. The rush when I figured out how to prove something during the last problem of an exam. As the time ticked away and I'm just staring at the words over and over, before I can sink an ice pick in and finally start grabbing a foothold.Other things not really worth mentioning were that we had some useless digital logic section at the start where we made a full adder and called it a computer. As a CompE, it was weird. The CS students thought they knew all there was to how a computer worked from that. Also, he was only a lecturer because our processor got sick right before the class and they found a grad student to do it. He was ok but took shortcuts and our TA would be like \"oh, he did this fast and loose, so now I have to teach you the real way it's done\".It was a great class in retrospect and certainly pushed my boundary on theoretical computing but you could feel the code monkeys regretting their decisions each homework and exam. It's what radicalized me to believing we needed programming and computer science to be different majors.reply",
      "I seem to remember this specific class at the CMU School of Computer Science being described as a \u201cweed-out class\u201d.reply",
      "Theory is certainly a weed-out class. I think algorithms is certainly more difficult for a dedicated student tho.reply",
      "I notice \"highlights\" is essentially empty, which seems to be the referent of the title of this postreply"
    ],
    "link": "https://www.cs251.com/",
    "first_paragraph": "Welcome to CS251 at CMU!This course is about the rigorous study of computation, which is a fundamental component of our universe, the societies we live in, the new technologies we discover, as well as the minds we use to understand these things. Therefore, having the right language and tools to study computation is important. In this course, we explore some of the central results and questions regarding the nature of computation.Welcome to CS251! In this module, our main goal is to explain at a high-level what theoretical computer science is about and set the right context for the material covered in the future.In the first part of the course, we want to build up formally/mathematically, the important notions related to computation and algorithms. We start this journey here by discussing how to formally represent data and how to formally define the concept of a computational problem.The goal of this module is to introduce you to a simple (and restricted) model of computation known as d"
  },
  {
    "title": "Show HN: Picknplace.js, an alternative to drag-and-drop (jgthms.com)",
    "points": 122,
    "submitter": "bbx",
    "submit_time": "2025-12-16T16:12:06 1765901526",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=46290349",
    "comments": [
      "Great work building something and having the courage to show HN ;-)Interesting design.  Even though I read the instructions still could not get it to work for 30 or so seconds.  Might want to show some text 'Now Scroll' with up/down arrows to the left or right of the list.Seems ok when the list is in the middle of the page and you already have room to scroll up and down, but how is it going to work when the list is at the top or bottom of the page?Or when the page is so short it does not scroll at all?  I suppose you could have the container scroll but then it needs to be considerable larger than the list.Honestly when you click 'Pick' all of the others should say 'Place' would be more intuitive and give the user options to support both.\nAs they use it they will pick up they can scroll if they want.reply",
      "This looks like an interesting concept!> I find that the drag and drop experience can quickly become a nightmare, especially on mobile.To me, drag and drop is only a nightmare on mobile. On desktop (using a mouse or trackpad), drag and drop actually works quite well.Your design experiment reminds me of a recent talk of Scott Jenson, where he talked about how we just took over established UX patterns from desktop to mobile as is, and how that created all sorts of nuisances. (https://youtu.be/1fZTOjd_bOQ?t=1565)If mobile drag&drop was implemented like you are suggesting from the very start, I actually might have preferred that over the situation we now ended up with.One technical note on your implementation: on certain mobile browsers, there is a glitch where the UI can jump around as the browser dynamically slides top or bottom menu bars in and out.reply",
      "> On desktop (using a mouse or trackpad), drag and drop actually works quite well.Strong disagree here. It is intuitive, it is easy to demonstrate. But it's not really convenient, especially on a trackpad. I have enough mouse agility to play RTS games but not to do a reliable drag-and-drop, especially in a complicated case - across windows, with scroll, etc.reply",
      "Yes, it can get tricky if you have to scroll a bunch, e.g. moving a file in a big directory into a subfolder, trying to hit that one pixel where it will scroll up, or using two other fingers to attempt to scroll, while holding the drag finger down...(CLI pros, you win this one).I would like a desktop pick and place that works like drag and drop, you click and then it sticks to the cursor, but you are free to do whatever gestures until you click again.reply",
      "> while holding the drag finger downI'm not sure if this is common on other desktop operating systems but the 'Drag Lock' feature on macOS allows you to double-tap an item, then drag it without holding the button down to begin a drag. At that point lifting your finger continues the drag until you tap once to release it.I would be amazed at how many people using macOS have never found this option except I'm not sure I've ever seen it being called out as a feature, and nowadays it's also buried deep under Accessibility settings (the irony) instead of just being a Trackpad option, so a lot of users might not even think to go there.reply",
      "you should perhaps mention that this is about dragging and dropping objects in lists. otherwise there is an additional level of jarring-ness due to not even knowing what problem you're trying to solve (and then bewildered by what it does as a result).reply",
      "When I had to implement a UI for reordering a list, I just had a \u201cmove\u201d button on each item, and when you press it \u201cmove here\u201d buttons would appear between every item (and at the top and bottom). These buttons are positioned absolutely, so there is no reflow from stretching the list. The location where you \u2018place\u2019 the item is where you click, not dependent on scroll position. Without even planning for use on mobile it ended up \u201cjust working\u201d on mobile because you only need to tap buttons.reply",
      "On mobile this is a strong contender for the worst UX I've ever seen. The whole page moves, so you have to continously scroll back up after placing something.If when in pick mode you would only scroll the list, I would be able to organize it much faster.reply",
      "It's even worse on desktop. You have to scroll the entire screen (with mousewheel or arrow keys) to move the selection. I spent 30 seconds thinking it was bugged because the intuitive solution would be to click once, then simply click where you want to place it, but the \"place\" button only showed up on the one you already \"picked\". Fine idea, worst conceivable execution of UX I have ever seen.reply",
      "Perhaps a combination of the two? Maybe standard drag-and-drop works as usual, but if you drag the item to some deadzone (like the side of the screen?), it will stick and you're free to scroll to where you want to put it.reply"
    ],
    "link": "https://jgthms.com/picknplace.js/",
    "first_paragraph": "an alternative to drag and drop\n3 steps: pick -> scroll ->\n          place\nWhy?\n            I find that the drag and drop experience can quickly become a\n            nightmare, especially on mobile.\n          \n            Trying to tap, hold, drag, and scroll, all at the same time, is\n            awkward, slow, and error-prone.\n          \n            I've long had in mind a simpler 2-step approach: picking an item\n            first, then placing it.\n          \n            So I implemented this basic version to showcase my idea.\n          How does this work?\n            When picking an item, a duplicate of the list is created on top of\n            the original one.\n          \n            The duplicate is interactive and animated, and will update based on\n            the scroll position.\n          \n            At the end, the user can either confirm or cancel the changes.\n          Is this a library?\n            Not exactly. This is merely a proof of concept, to convey what I had\n       "
  },
  {
    "title": "Firefox will have an option to disable all AI features (mastodon.social)",
    "points": 269,
    "submitter": "twapi",
    "submit_time": "2025-12-18T18:18:30 1766081910",
    "num_comments": 238,
    "comments_url": "https://news.ycombinator.com/item?id=46316409",
    "comments": [
      "I actually saw the \u201csummarize this page\u201d feature in the right-click menu today and clicked on it out of curiosity. The box that appeared had a \u201cremove AI features\u201d button which I accidentally clicked. Now the feature is completely gone and I don't know how to get it back. (Don't really care much, wasn't planning on using that feature anyway, just giving feedback on my first impression)reply",
      "This is exactly the kind of boring, unsexy feature that actually builds trust. It\u2019s the opposite of the usual \u201csurprise, here\u2019s an AI sidebar you didn\u2019t ask for and can\u2019t fully disable\u201d pattern.\nIf they want people to try this stuff, the path is pretty simple: ship a browser that treats AI like any other power feature. Off by default, clearly explained, reversible, and preferably shippable as an extension. You can always market your way into more usage; you can\u2019t market your way back into credibility once you blow it.reply",
      "It is well-known as a result of the expert reports in US v Google that generally software users do not change defaultsWhereas providing an option or a setting that the user must locate and change doesn't really mean much.  Few users will ever see it let alone decide to change itFor example, why pay 22 billion to be \"the default\" if users can just change the default settingreply",
      "My problem here is this; products are designed with a vision. If you are designing with 2-3 visions it won\u2019t be that good, if you design with one vision (AI) then non-AI version of the product will be an after thought. This tells me non-AI version of it will suffer (IMHO)reply",
      "> if you design with one vision (AI) then non-AI version of the product will be an after thoughtThat\u2019s like saying if a car manufacturer adds a \"Sport Mode\", the steering wheel and brakes suddenly become an afterthought.Being AI-available means we'll welcome more Firefox users who would otherwise choose a different browser. Being AI-optional means we won't alienate the anti-AI crowd. Why not embrace both?reply",
      "I don't agree. I think opinionated design products are much worse in general.It's really great when your opinions are aligned with those of the designer. If they're not, you're straight out of luck and you're stuck with something that isn't really for you.This is why I love software that gives as much choice as possible. Like KDE for example. Because I have pretty strong vision myself and I respect my tools to conform to that, not the other way aroundreply",
      "> This is exactly the kind of boring, unsexy feature that actually builds trust.Though not so much trust as an option to enable AI features would build.reply",
      "saying \"trying to slow down, I promise\" doesn't magically make your blatant advert not spamedit: the original post ended with words to the tune of \"Totally unrelated, but I run [insert newsletter here]... \"reply",
      "Edited and removed.reply",
      "Why? Why kowtow to people who don't care about your wellbeing or long term success?reply"
    ],
    "link": "https://mastodon.social/@firefoxwebdevs/115740500373677782",
    "first_paragraph": ""
  },
  {
    "title": "Delty (YC X25) Is Hiring an ML Engineer (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-12-18T21:02:10 1766091730",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/delty/jobs/MDeC49o-machine-learning-engineer",
    "first_paragraph": "Transforming healthcare operations with AI agentsDelty is building the healthcare\u2019s AI operating system. We create voice-based and computer-based assistants that streamline clinical workflows, reduce administrative burden, and help providers focus on patient care. Our system learns from real healthcare environments to deliver reliable, context-aware support that improves efficiency and elevates the provider experience.Delty was founded by former engineering leaders from Google, including co-founders with deep experience at YouTube and in large-scale infrastructure. You\u2019ll get to work alongside people who built massive systems at scale \u2014 a chance to learn a lot and contribute meaningfully from day one.We believe in solving hard problems together as a team, iterating quickly, and building software with long-term thinking and ownership.Bonus:Transforming healthcare operations with AI agents.\u00a9 2025 Y Combinator"
  },
  {
    "title": "Show HN: Stop AI scrapers from hammering your self-hosted blog (using porn) (github.com/vivienhenz24)",
    "points": 125,
    "submitter": "misterchocolat",
    "submit_time": "2025-12-16T20:42:38 1765917758",
    "num_comments": 92,
    "comments_url": "https://news.ycombinator.com/item?id=46294144",
    "comments": [
      "Disclosure, I've not run a website since my health issues began, however, Cloudflare has an AI firewall, Cloudflare is super cheap (also: unsure if the AI firewall is on the free tier, however I would be surprised if it is not). Ignoring the recent drama about a couple incidents they've had (because this would not matter for a personal blog), why not use this instead?Just curious. Hoping to be able to work on a website again someday, if I ever regain my health/stamina/etc back.reply",
      "All the best with getting back on your feet.reply",
      "I love the insanity of this idea. Not saying it's a good idea, but it's a very highly entertaining one, and I like that!I've also had enormous luck with Anubis. AI scrapers found my personal Forgejo server and were hitting it on the order of 600K requests per day. After setting up Anubis, that dropped to about 100. Yes, some people are going to see an anime catgirl from time to time. Bummer. Reducing my fake traffic by a factor of 6,000 is worth it.reply",
      "That\u2019s so many scrapers. There must be a ton of companies with very large document collections at this point, and it really sucks that they don\u2019t at least do us the courtesy of indexing them and making them available for keyword search, but instead only do AI.It\u2019s kind of crazy how much scraping goes on and how little search engine development goes on. I guess search engines aren\u2019t fashionable. Reminds me of this article about search engines disappearing mysteriously: https://archive.org/details/search-timelineI try to share that article as much as possible, it\u2019s interesting.reply",
      "So! Much! Scraping! They were downloading every commit multiple times, and fetching every file as seen at each of those commits, and trying to download archives of all the code, and hitting `/me/my-repo/blame` endpoints as their IP's first-ever request to my server, and other unlikely stuff.My scraper dudes, it's a git repo. You can fetch the whole freaking thing if you wanna look at it. Of course, that would require work and context-aware processing on their end, and it's easier for them to shift the expense onto my little server and make me pay for their misbehavior.reply",
      "Crazyreply",
      "As someone on the browsing end, I love Anubis. I've only seen it a couple of times, but it sparks joy. It's rather refreshing compared to Cloudfare, which will usually make me immediately close the page and not bother with whatever content was behind it.reply",
      "Same here, really. That's why I started using it. I'd seen it pop up for a moment on a few sites I'd visited, and it was so quirky and completely not disruptive that I didn't mind routing my legit users through it.reply",
      "So maybe there are more people who like the \u201canime catgirl\u201d than there are who think it\u2019s weirdreply",
      "*anime jackalgirl ;-)Quite possibly. Or, in my case, I think it's more quirky and fun than weird. It's non-zero amounts of weird, sure, but far below my threshold of troublesome. I probably wouldn't put my business behind it. I'm A-OK with using it on personal and hobby projects.Frankly, anyone so delicate that they freak out at the utterly anodyne imagery is someone I don't want to deal with in my personal time. I can only abide so much pearl clutching when I'm not getting paid for it.reply"
    ],
    "link": "https://github.com/vivienhenz24/fuzzy-canary",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Poor-man's solution to stopping AI companies from scraping your blog\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.AI companies are scraping everyone's sites for training data. If you're self-hosting your blog, there's not much you can do about it, except maybe make them think your site contains content they won't want. Fuzzy Canary plants invisible links (to porn websites...) in your HTML that trigger scrapers' content safeguards.\n\n\n\nThere are two ways to use it: client-side or server-side. Use server-side if you can\u2014it works better because the canary is in the HTML from the start, so scrapers that don't run JavaScript will still see it.Server-side (recommended):If you're using a React-based framework (Next.js, Remix, etc.), add the <Canary /> component t"
  },
  {
    "title": "FunctionGemma 270M Model (blog.google)",
    "points": 152,
    "submitter": "mariobm",
    "submit_time": "2025-12-18T18:26:52 1766082412",
    "num_comments": 39,
    "comments_url": "https://news.ycombinator.com/item?id=46316533",
    "comments": [
      "Hi all,\nI'm a research lead on this model. Same as every model release post, I enjoy working at Google for a multitude of reasons, and opinions here are my own.Happy to answer whatever technical questions I can!reply",
      "Cool game! Amazing it can run in the browser. My mind was blown when I saw you could give goal based commands vs prescriptive ones. https://huggingface.co/spaces/webml-community/FunctionGemma-...reply",
      "So I didn't even know this was going to be made until recently, and when I saw it, it also blew my mind. I didn't realize how far along web ml community had pushed things, and was impressed by the creativity of the HF folks with visuals and \"game flow\".Personally speaking its really neat to see other people who take these models and run with them, creating things I could haven't have imagined. I'm hoping many others in the open community do the same in the coming weeks and the new yearreply",
      "If I have a simple mainly question-answering AI using only a couple of tools (web search), am I better off starting with Gemma or FunctionGemma?reply",
      "I have often wondered how much a specialized local LLM could benefit an agentic tool like Gemini CLI. I would think there could be a good win for speed and minimizing token use if coding agents used a local model. A local model could handle a lot of the low level system interaction type tasks and then send the prompts that require deeper reasoning to frontier models. It seems wasteful and slow to use frontier models to figure out how to grep a codebase, run tests, git diff, etc.Might Gemini CLI offload some of its prompts to FunctionGemma?reply",
      "I want to say so much right now but I can't :)The most generic thing I can say is I really do like working at Google because its one of the few (maybe only) company that has models of all sizes and capabilities. Because of this research and product development is insanely fun and feels \"magical\" when things just click together.Keep following the Google Developer channels/blogs whatever. Google as a whole is pushing hard in this space and I personally think is building stuff that felt like science fiction just 3 years ago.reply",
      "hi! Does this bring us closer to a gemini-cli like experience using a local modal that can run on a macbook pro? It felt like gemma3n was already 'smart' enough it just wasn't tuned for tool use.reply",
      "Hey! Love the Gemma series. Question that came to mind reading the announcement post - the proposal there is that you can use this as a local backbone and have it treat a larger model as a 'tool call' when more reasoning is needed.In my mind we want a very smart layer frontier model orchestrating, but not slowing everything down by doing every little thing; this seems like the opposite - a very fast layer that can be like \"wait a minute, I'm too dumb for this, need some help\".My question is - does the Gemma team use any evaluation around this particular 'call a (wiser) friend' strategy? How are you thinking about this? Is this architecture flow more an accommodation to the product goal - fast local inference - or do you guys think it could be optimal?reply",
      "We evaluate many things that you alluded to, such as speed on device, output correctness, and also \"is this something that would be useful\" the last one being a bit abstract.The way we think about it is what do we think developers and users need, and is there a way we can fill that gap in a useful way. With this model we had the hypothesis you had, there are fantastic larger models out there pushing the frontier of AI capabilities, but there's also a nice for smaller customizable model that's quick to run and quick to tune.What is optimal then ultimately falls to you and your use cases (which I'm guessing at here), you have options now between Gemini and Gemma.reply",
      "Thanks - yeah, I'm capable of assessing for my own use cases. I guess I was trying to muse out-loud about whether there's a useful benchmark to be made or published out of these assessments. There are a number of architectures where there's a 'fast loop' and then a slow loop. Robotics comes to mind. I think training the ability to be like 'uh oh, better get over to slow good thinking' into the fast loop models is likely to be super useful.reply"
    ],
    "link": "https://blog.google/technology/developers/functiongemma/",
    "first_paragraph": "Dec 18, 2025\n          We\u2019re releasing a specialized version of our Gemma 3 270M model fine-tuned for function calling and a training recipe for users to specialize for even better performance.\n        It has been a transformative year for the Gemma family of models. In 2025, we have grown from 100 million to over 300 million downloads while demonstrating the transformative potential of open models, from defining state-of-the-art single-accelerator performance with Gemma 3 to advancing cancer research through the C2S Scale initiative.Since launching the Gemma 3 270M model, the number one request we\u2019ve received from developers is for native function calling capabilities. We listened, recognizing that as the industry shifts from purely conversational interfaces to active agents, models need to do more than just talk \u2014 they need to act. This is particularly compelling on-device, where agents can automate complex, multi-step workflows, from setting reminders to toggling system settings. To"
  },
  {
    "title": "How did IRC ping timeouts end up in a lawsuit? (mjg59.dreamwidth.org)",
    "points": 126,
    "submitter": "dvaun",
    "submit_time": "2025-12-17T18:25:29 1765995929",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=46303406",
    "comments": [
      "A whole other part of this argument that could be made is about the inherent assumption that a ping timeout is caused by an event that only affects one machine.reply",
      "For sure. Having lived on IRC for a while many years ago, I assure any bystanders that this is assuredly not always the case.reply",
      "Imagine them trying to sue every person on one side of a netsplitreply",
      "...and back in my day (yeah I am becoming an old fart), it was dead simple to cause a netsplit on most networks.reply",
      "I'll admit to sending a couple of the messages that made Linksys routers restart. I also set up automatic k-lines on Snoonet for these very strings, years agoreply",
      "Ergo isn't a federated server, it's meant to scale verticallyreply",
      "The internet is a \"federated\" network though, so their point still applies.reply",
      "This vaguely reminds me of years ago when a friend got hit at an intersection and went to court to fight that he wasn't at fault. I ran the numbers a bit and found that whoever hit him would've been moving at a very high though not outlandish (think maybe 60mph in a 30mph or something) speed. But they never showed up and he won by defaultreply",
      "Glad to see a case that could've very easily gone sideways due to its technical nature come out right.reply",
      "The facts were never argued, the other party failed to follow procedure.reply"
    ],
    "link": "https://mjg59.dreamwidth.org/73777.html",
    "first_paragraph": "Get the right judge, and you absolutely get someone who understands the technicalities. The results can be very funny, unless you're the one who lost because you wouldn't stop lying.Other times, yep, you get someone effectively clueless."
  },
  {
    "title": "I've been writing ring buffers wrong all these years (2016) (snellman.net)",
    "points": 64,
    "submitter": "flaghacker",
    "submit_time": "2025-12-16T19:11:47 1765912307",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=46292937",
    "comments": [
      "It is not just a way of writing ring buffers. It's a way of implementing concurrent non-blocking single-reader single-writer atomic ring buffers with only atomic load and store (and memory barriers).The author says that non-power-of-two is not possible, but I'm pretty sure it is if you use a conditional instead of integer modulus.I first learnt of this technique from Phil Burk, we've been using it in PortAudio forever. The technique is also widely known in FPGA/hardware circles, see:\"Simulation and Synthesis Techniques for Asynchronous\nFIFO Design\", Clifford E. Cummings, Sunburst Design, Inc.https://twins.ee.nctu.edu.tw/courses/ip_core_04/resource_pdf...reply",
      "> It is not just a way of writing ring buffers. It's a way of implementing concurrent non-blocking single-reader single-writer atomic ring buffers with only atomic load and store (and memory barriers).That may or may not be part of the actual definition of a ring buffer, but every ring buffer I have written had those goals in mind.And the first method mentioned in the article fully satisfies this, except for the one missing element mentioned by the author.  Which in practice, often is not only not a problem, but simplifies the logic so much that you make up for it in code space.Or, for example, say you have a 256 character buffer.  You really, really want to make sure you don't waste that one character.  So you increase the size of your indices.  Now they are 16 bits each instead of 8 bits, so you've gained the ability to store 256 bytes by having 260 bytes of data, rather than 255 bytes by having 258 bytes of data.Obviously, if you have a 64 byte buffer, there is no such tradeoff, and the third example wins (but, whether your are doing the first or third example, you still have to mask the index data off at some point, whether it's on an increment or a read).> The author says that non-power-of-two is not possible, but I'm pretty sure it is if you use a conditional instead of integer modulus.There's \"not possible\" and then \"not practical.\"Sure, you could have a 50 byte buffer, but now, if your indices are ever >= 50, you're subtracting 50 before accessing the array, so this will increase the code space (and execution time).> The [index size > array size] technique is also widely known in FPGA/hardware circlesRight, but in those hardware circles, power-of-two _definitely_ matters.  You allocate exactly one extra bit for your pointers, and you never bother manually masking them or taking a modulo or anything like that -- they simply roll over.If you really, really need to construct something like a 6 entry FIFO in hardware, then you have techniques available to you that mere mortal programmers could not use efficiently at all.  For example, you could construct a drop-through FIFO, where every element traverses every storage slot (with a concomitant increase in minimum latency to 6 clock cycles), or you could construct 4 bit indices that counted 0-1-2-3-4-5-8-9-10-11-12-13-0-1-2 etc.Most ring buffers, hardware or software, are constructed as powers of two, and most ring buffers either (a) have so much storage that one more element wouldn't make any difference, or (b) have the ability to apply back pressure, so one more element wouldn't make any difference.reply",
      "A couple of the comments to the article suggest using 64-bit numbers, which is exactly the right solution. 2^64 nanoseconds=584.55 years - overflow is implausible for any realistic use case. Even pathological cases will struggle to induce wraparound at a human timescale.(People will probably moan at the idea of restarting the process periodically rather than fixing the issue properly, but when the period would be something like 50 years I don't think it's actually a problem.)reply",
      "I think unfortunately we sometimes ascribe to powers of two supernatural powers that are really about caches being built in powers of two.Intel is still 64 byte cache lines as they have been for quite a long time but they also do some shenanigans on the bus where they try to fetch two lines when you ask for one.  So there\u2019s ostensibly some benefit of aligning data particularly on linear scans to 128 byte alignment for cold cache access.reply",
      "But there's a reason that caches are always sized in powers of two as well, and that same reason is applicable to high-performance ring buffers: Division by powers of two is easy and easy is fast. It's reliably a single cycle, compared to division by arbitrary 32bit integers which can be 8-30 cycles depending on CPU.Also, there's another benefit downstream of that one: Powers of two work as a schelling point for allocations. Picking powers of two for resizable vectors maximizes \"good luck\" when you malloc/realloc in most allocators, in part because e.g. a buddy allocator is probably also implemented using power-of-two allocations for the above reason, but also for the plain reason that other users of the same allocator are more likely to have requested power of two allocations. Spontaneous coordination is a benefit all its own. Almost supernatural! :)reply",
      "powers-of-two are problematic with growable arrays on small heaps. You risk ending up with fragmented space you can't allocate unless you keep growth less than 1.61x, which would necessitate data structures that can deal with arbitrary sizes.reply",
      "Non-power-of-two is only really feasible of the total number of inserts will fit in your post/ack counters.  Otherwise you have to implement overflow manually which may or may not be possible to do with the available atomic primitives on your architecture.I first encountered this structure at a summer internship at a company making data switches.reply",
      "Your link has an invalid cert FYI, but do appreciate the knowledge drop. Rung buffers are some of the cooler data structures out there.reply",
      "> So there I was, implementing a one element ring buffer. Which, I'm sure you'll agree, is a perfectly reasonable data structure.It is, but, IMO, shouldn\u2019t use the code for \u201ca n-element ring buffer, with n set to 1\u201d, similarly to how an array of booleans in many languages shouldn\u2019t be implemented as \u201can arrayof Foos, with Foo set to bool\u201d.C++ has std::bitset and std::vector and Java similarly has BitSet and Array because using the generic code for arrays of bits is too wasteful.Similarly, a one-element ring buffer is either full or it is empty. Why use two indexes to encode a single boolean?reply",
      "> C++ has std::bitset and std::vector and Java similarly has BitSet and Array because using the generic code for arrays of bits is too wasteful.Rather infamously, C++ tried to be clever here and std::vector<bool> is not just a vector-of-bools but instead a totally different vector-ish type that lacks many of the important properties of every other instantiation of std::vector. Yes, a lot of the time you want the space efficiency of a dynamic bitset, rather than wasting an extra 7 bits per element. But also quite often you do want the behavior of a \"real\" std::vector for true/false values, and then you have to work around it manually (usually via std::vector<uint8_t> or similar) to get the expected behavior.reply"
    ],
    "link": "https://www.snellman.net/blog/archive/2016-12-13-ring-buffers/",
    "first_paragraph": "\nSo there I was, implementing a one element ring buffer. Which,\nI'm sure you'll agree, is a perfectly reasonable data structure.\n\n\nIt was just surprisingly annoying to write, due to reasons we'll\nget to in a bit. After giving it a bit of thought, I realized I'd always\nbeen writing ring buffers \"wrong\", and there was a better way.\n\n\nArray + two indices\n\nThere are two common ways of implementing a queue with a ring buffer.\n\n One is to use an array as the backing storage plus two indices\nto the array; read and write. To shift a\nvalue from the head of the queue, index into the array by the read\nindex, and then increment the read index. To push a value to the back,\nindex into the array by the write index, store the value in that\noffset, and then increment the write index.\n\n Both indices will always be in the range 0..(capacity - 1). This\nis done by masking the value after an index gets incremented.\n\n\nThat implementation looks basically like:\n\n\nuint32 read;\nuint32 write;\nmask(val)  { return "
  },
  {
    "title": "How to hack Discord, Vercel and more with one easy trick (kibty.town)",
    "points": 112,
    "submitter": "todsacerdoti",
    "submit_time": "2025-12-18T19:41:24 1766086884",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=46317546",
    "comments": [
      "This feels so emblematic of our current era. VC funded vibe coded AI documentation startup somehow gets big name customers who don't properly vet the security of the platform, ship a massive vulnerability that could pwn millions of users and the person who reports the vulnerability gets...$5k.If I recall last week Mintlify wrote a blog post showcasing their impressive(ly complicated) caching architecture. Pretending like they were doing real engineering, when it turns out nobody there seems to know what they're doing, but they've managed to convince some big names to use them.Man, it's like everything I hate about modern tech. Good job Eva for finding this one. Starting to think that every AI startup or company that is heavily using gen-ai for coding is probably extremely vulnerable to the simplest of attacks. Might be a way to make some extra spending money lol.reply",
      "This is identical to a comment you wrote on the other story about these vulnerabilities that's higher up on the front page, which isn't great.reply",
      "A similar comment was posted on the PostHog post yesterday. Claiming everything is vibe coded without any proof is pure rage bait.reply",
      "You bet not all THW vulnerabilities are reported to the vendors. Not with 5k bounty for THAT.reply",
      "Yeah thats the scary thing. I know it's a bit of a meme about how as programmers we don't trust other programmers or software, but it's becoming more and more true and necessary. I want to use as little software as possible these days.reply",
      "Yeah it made me re-evaluate how much I can trust those platformsreply",
      "THW?reply",
      "Chill - just because someone got hacked doesn't mean their product is trash. Easily every mass adopted product created prior to 2023 has been hacked at some point.reply",
      "That makes it worse, not better. Because for those applications the code was audited and not hallucinated.reply",
      ">\nThis feels so emblematic of our current era. VC funded vibe coded AI documentation startup somehow ...Is there any indication Mintify was \"vibe coded\"?reply"
    ],
    "link": "https://kibty.town/blog/mintlify/",
    "first_paragraph": "or, an impromptu security audit on the fortune 500this blogpost was a collaboration with two people, their articles are here: hackermon and mdlthis started when i was notified that discord switched documentation platforms to mintlify, a company i briefly looked into before, and i thought it would be a good idea to take another look now that theyre bigger.mintlify is a b2b saas documentation platform that allows companies to make documentation via MDX files and they host it for them, and add styling, etc.some of their customers would include:...and more, you can view a full list heretheres also a bunch of ai features and stuff, but thats beyond the pointso, i signed up and got to digging.mintlify uses MDX to render docs their customers provide, and i was wondering how they render it on the server-side for static page generation (because a docs site needs that for search engines/bots).this is because mdx is basically jsx (think react) combined with markdown, meaning you can add js expres"
  }
]