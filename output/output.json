[
  {
    "title": "FFmpeg by Example (ffmpegbyexample.com)",
    "points": 226,
    "submitter": "piyushsthr",
    "submit_time": "2025-01-14T09:58:15 1736848695",
    "num_comments": 61,
    "comments_url": "https://news.ycombinator.com/item?id=42695547",
    "comments": [
      "I've enjoyed using ffmpeg 1000% more since I was able to stop doing manually the tedious task of Googling for Stack Overflow answers and cobbling them into a command and got Chat GPT to write me commands instead.\n \nreply",
      "I use ffmpeg multiple times a week thanks to LLMs. It's my top use-case for my \"llm cmd\" tool:  uv tool install llm\n  llm install llm-cmd\n\n  llm cmd use ffmpeg to extract audio from myfile.mov and save that as mp3\n\nhttps://github.com/simonw/llm-cmd\n \nreply",
      "I tried this (though with a different tool called aichat) for extremely simple stuff like just \"convert this mov to mp4\" and it generated overly complex commands that failed due to missing libraries. When I removed the \"crap\" from the commands, they worked.So much like code assistance, they still need a fair amount of baby sitting. A good boost for experienced operators but might suck for beginners.\n \nreply",
      "Hate to be that guy, but which LLM was doing the generation? GPT-4 Turbo / Claude 3.x have not really let me down in generating ffmpeg commands - especially for basic requests - with most of their failures resulting from domain-specific vagaries that an expert would need to weigh in on m\n \nreply",
      "\"The future is already here.  It's just not very well distributed\"(honestly, the work you share is very inspiring)\n \nreply",
      ">This will then be displayed in your terminal ready for you to edit it, or hit <enter> to execute the prompt. If the command doesnt't look right, hit Ctrl+C to cancel.I appreciate the UI choice here. I have yet to do anything with AI (consciously and deliberately, anyway) but this sort of thing is exactly what I imagine as a proper use case.\n \nreply",
      "Just like all other code. There will be user-respecting open source code and tools, and there's user-disrespecting profitable closed code that makes too many decisions for you.\n \nreply",
      "For the longest time I had ffmpeg in the same bucket as regex: \"God I really need to learn this but I'm going to hate it so much.\" Then ChatGPT came along and solved both problems!\n \nreply",
      "Not sure about ffmpeg, but you should definitely try memorising regexp. Casual Search&replace that becomes possible is worth it.\n \nreply",
      "in 15 years it never sticks and by the time i need it again i've forgotten it! :D\n \nreply"
    ],
    "link": "https://ffmpegbyexample.com/",
    "first_paragraph": "If you feel like this example could be improved, you may edit this example here.Support us by donating a coffee here! \ud83d\ude4fMade with \ud83d\udc9c by Gariany & the FFmpeg Discord Community"
  },
  {
    "title": "How rqlite is tested (philipotoole.com)",
    "points": 107,
    "submitter": "otoolep",
    "submit_time": "2025-01-14T20:21:47 1736886107",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=42703282",
    "comments": [
      "I love how dedicated you are with this project, Philip. Been watching it for many many years.\n \nreply",
      "can't wait for the https://jepsen.io/ report!\n \nreply",
      "There seems to be some copy pasta in the FAQ on if any node can be contacted for writes or reads, the paragraph on reads mentions writes.\n \nreply",
      "Thanks for flagging -- fixed.\n \nreply",
      "I also enjoyed this in video format https://youtu.be/JLlIAWjvHxM?feature=shared&t=2049Always have been envious of that performance testing setup that is shown here\n \nreply",
      "Anyone using rqlite in prod ?\n \nreply",
      "In case you missed it, OP is also the author of rqlite(I for one didn't notice at first)\n \nreply",
      "Yes, that is right. So here's my disclaimer: I'm the author of rqlite.I really admire the testing that the SQLite team does[1], and the way it allows them to stand behind the statements they make about quality. It's inspiring.IMO there have only been two really big improvements to software development relative to when I started programming professionally 25 years ago: 1) code reviews becoming mainstream, and 2) unit testing. (Perhaps Gen AI will be the third). I believe extensive testing is the only reason that rqlite continues to be developed to this day. It's not just that it helps keep the quality high, it's a key design guide. If a new module cannot be unit tested during development, in a straightforward manner, it's a strong sign one's decomposition of the problem is wrong.[1] https://www.sqlite.org/testing.html\n \nreply",
      "1. Is this a one man project? Why? What if the author dies?2. Why Go? Go is garbage collected, how is this even a good idea for a database engine in the first place?\n \nreply",
      "Making good on the username, for sure!\n \nreply"
    ],
    "link": "https://philipotoole.com/how-is-rqlite-tested/",
    "first_paragraph": "\n\t\t\t\t\tPhilip O'Toole\t\t\t\t\n\t\t\t\t\tPhilip O'Toole\t\t\t\trqlite is a lightweight, open-source, distributed relational database built on SQLite and Raft. With its origins dating back to 2014, its design has always prioritized reliability, and quality. The robustness of rqlite is also a testament to its disciplined testing strategy: after more than 10 years of development and deployments, users have reported fewer than 10 instances of panics in production.Testing a distributed system like rqlite is no small feat. It requires careful consideration of various layers: from individual components to the entire system in operation. Let\u2019s explore how rqlite is tested, following its philosophy of maintaining quality without unnecessary complexity.Testing rqlite adheres to the well-known testing pyramid, which prioritizes unit tests as the foundation, supported by integration tests, and capped with minimal end-to-end (E2E) tests. This strategy reflects decades of software development experience, ensuring "
  },
  {
    "title": "Show HN: Blinkenlights. Bling up your server rack like its 1974 (rodyne.com)",
    "points": 14,
    "submitter": "boznz",
    "submit_time": "2025-01-12T09:39:50 1736674790",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42672366",
    "comments": [
      "I'm planning to do something similar, but using addressable RGB LEDs (like WS2811, also known as neopixel).They are simple to work with. Each LED has 4 connections: GND, +5V, DATA IN and DATA OUT. Each LED grabs the first 24 bits of the data stream (8 bits for Red, Green and Blue) and sends the rest on DATA OUT for the next LED.\n \nreply",
      "> Now you need to remember this is not a metal panel it is a 1.6mm thick fibre-glass one, it does the job, but it wont take much abuse like a metal one will and flexes a little when handling it.That's when you design in a PC board stiffener. These are just pieces of metal, U-channel, L-channel, or solid bar, to add some structural strength. Cheap and easy, but rarely seen in hobbyist work. Any board with buttons or knobs or connectors unsupported for more than a few inches should have some stiffening. You have to allow space for stiffening bars when designing the board, and you need to place screw holes.\n \nreply",
      "This doesn't look like a valid Show HN.  It is an interesting blog post (without the Show HN prefix), though!> Show HN is for something you've made that other people can play with. HN users can try it out, give you feedback, and ask questions in the thread.> Off topic: blog postshttps://news.ycombinator.com/showhn.html\n \nreply",
      "Oh wow, I never read that and thought Show HN was simply, \"Look at this cool thing I did.\"\n \nreply",
      "I can't wait until gaming builds start adopting this trend. I want to see a high end gaming rig but it looks like a server from a 1990s sci-fi movie.\n \nreply"
    ],
    "link": "https://rodyne.com/?p=1674",
    "first_paragraph": ""
  },
  {
    "title": "Don't use cosine similarity carelessly (p.migdal.pl)",
    "points": 77,
    "submitter": "stared",
    "submit_time": "2025-01-14T21:23:21 1736889801",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=42704078",
    "comments": [
      "If you're using cosine similarity when retrieving for a RAG application, a good approach is to then use a \"semantic re-ranker\" or \"L2 re-ranking model\" to re-rank the results to better match the user query.There's an example in the pgvector-python that uses a cross-encoder model for re-ranking: https://github.com/pgvector/pgvector-python/blob/master/exam...You can even use a language model for re-ranking, though it may not be as good as a model trained specifically for re-ranking purposes.In our Azure RAG approaches, we use the AI Search semantic ranker, which uses the same model that Bing uses for re-ranking search results.\n \nreply",
      "Another tip: do NOT store vector embeddings of nothingness, mostly whitespace, a solid image, etc. We've had a few situations with RAG data stores which accidentally ingested mostly-empty content (either text or image), and those dang vectors matched EVERYTHING. WAs I like to think of it, there's a bit of nothing in everything.. so make sure that if you are storing a vector embedding, there is some amount of signal in that embedding.\n \nreply",
      "Interesting. A project I worked on (audio recognition for a voice-command system) we ended up going the other way and explicitly adding an encoding of \"nothingness\" (actually 2, one for \"silence\" and another for \"white noise\") and special casing them (\"if either 'silence' or 'noise' is in the top 3 matches, ignore the input entirely\").This was to avoid the problem where, when we only had vectors for \"valid\" sounds and there was an input that didn't match anything in the training set (a foreign language, garbage truck backing up, a dog barking, ...) the model would still return some word as the closest match (there's always a vector that has the highest similarity) and frequently do so with high confidence i.e. even though the actual input didn't actually match anything in the training set, it would be \"enough\" more like one known vector than any of the others that it would pass most threshold tests, leading to a lot of false positives.\n \nreply",
      "That sounds like a problem for the embedding, would you need to renormalise so that low signal inputs could be well represented. A white square and a red square shouldn't be different levels of details. Depending on the purpose of the vector embedding, there should be a difference between images of mostly white pixels and partial images.Disclaimer, I don't know shit.\n \nreply",
      "I should clarify that I experienced these issues with text-embedding-ada-002 and the Azure AI vision model (based on Florence). I have not tested many other embedding models to see if they'd have the same issue.\n \nreply",
      "FWIW I think you're right, we have very different stacks, and I've observed the same thing, with a much clunkier description thank your elegant way of putting it.I do embeddings on arbitrary websites at runtime, and had a persistent problem with the last chunk of a web page matching more things. In retrospect, its obvious that the smaller the chunk was, the more it was matching everythingFull details: MSMARCO MiniLM L6V3 inferenced using ONNX on iOS/web/android/macos/windows/linux\n \nreply",
      "> So, what can we use instead?> The most powerful approach> The best approach is to directly use LLM query to compare two entries.Cross encoders are a solution I\u2019m quite fond of, high performing and much faster. I recently put an STS cross encoder up on huggingface based on ModernBERT that performs very well.\n \nreply",
      "Cosine similarity and top-k RAG feel so primitive to me, like we are still in the semantic dark ages.The article is right to point out that cosine similarity is more of an accidental property of data than anything in most cases (but IIUC there are newer embedding models that are deliberately trained for cosine similarity as a similarity measure). The author's bootstrapping approach is interesting especially because of it's ability to map relations other than the identity, but it seems like more of a computational optimization or shortcut (you could just run inference on the input) than a way to correlate unstructured data.After trying out some RAG approaches and becoming disillusioned pretty quickly I think we need to solve the problem much deeper by structuring models so that they can perform RAG during training. Prompting typical LLMs with RAG gives them input that is dissimilar from their training data and relies on heuristics (like the data format) and thresholds (like topK) that live outside the model itself. We could probably greatly improve this by having models define the embeddings, formats, and retrieval processes (ie learn its own multi-step or \"agentic\" RAG while it learns everything else) that best help them model their training data.I'm not an AI researcher though and I assume the real problem is that getting the right structure to train properly/efficiently is rather difficult.\n \nreply",
      "Typo: \"When we with vectors\" should be \"When we work with vectors\" I think.\n \nreply",
      "Very interesting article. Is there any model that can generate embeddings given a system prompt? This can be useful not only for similarity searching but also for clustering use cases without having to do too much custom work. Essentially, a zero shot embedding model.\n \nreply"
    ],
    "link": "https://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity/",
    "first_paragraph": "14 Jan 2025 | by Piotr Migda\u0142 Midas turned everything he touched into gold. Data scientists turn everything into vectors.\nWe do it for a reason \u2014 as gold is the language of merchants, vectors are the language of AI1.Just as Midas discovered that turning everything to gold wasn't always helpful, we'll see that blindly applying cosine similarity to vectors can lead us astray. While embeddings do capture similarities, they often reflect the wrong kind - matching questions to questions rather than questions to answers, or getting distracted by superficial patterns like writing style and typos rather than meaning. This post shows you how to be more intentional about similarity and get better results.Embeddings are so captivating that my most popular blog post remains king - man + woman = queen; but why?.\nWe have word2vec, node2vec, food2vec, game2vec, and if you can name it, someone has probably turned it into a vec. If not yet, it's your turn!When we work with raw IDs, we're blind to relat"
  },
  {
    "title": "Home Loss File System (docs.google.com)",
    "points": 120,
    "submitter": "borski",
    "submit_time": "2025-01-14T17:54:51 1736877291",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=42700997",
    "comments": [
      "Thanks Borski for sharing the link for the Home Loss File System - Digital Resource that my family has been working on.Were continuing to work to get the word out about this and the physical file boxes were creating for folks who are not tech savvy (homelossfilesystem.com).We've disseminated 2700 of the physical file boxes to fire survivors over the last 15 years and excited about what the digital resource can become.We welcome contributors/volunteers/suggestions/feedback - feel free to add them here or email us homelossfilesystem@gmail.comThe GoFundMe is 65% the way to its goal. If you wish you contribute you can here:\nhttps://www.gofundme.com/f/help-us-deliver-mor-1500-home-los...Thanks again all! Please continue to share with any fire survivors!\n \nreply",
      "Also here is the link again for the Digital Resource:https://docs.google.com/spreadsheets/d/1TPeJzW5pa-BiJZjuEa1y...\n \nreply",
      "There is no \"File\" menu in the current link because it's been shared in HTML view mode. To make a local copy, use this link to open it in edit mode instead (only the last path of the URL should be different): https://docs.google.com/spreadsheets/d/1TPeJzW5pa-BiJZjuEa1y...\n \nreply",
      "You have to remove \"/htmlview\" from the URL, otherwise the \"File > Make a copy\" interface is not available\n \nreply",
      "This is perhaps only a problem with the niche HN user base, but \u201cFiling System\u201d would be more accurate. Thought this had to do with damaged hard disk recovery or redundant file systems or something technical like that.\n \nreply",
      "A spreadsheet tool with guidance on what to do before and after the catastrophic loss of your house, and what information to collect.\n \nreply",
      "> This tool was created by former California wildfire survivors committed to supporting you through the challenging process of disaster recovery. We hope to provide essential resources, checklists, and organizational tools to help you manage insurance claims, document losses, and track expenses efficiently. By staying organized, you hope you can regain a sense of control during this difficult time. We are truly sorry for your loss and hope this tool offers clarity, support, and empowerment as you move forward on your path to recovery.\n \nreply",
      "Yes, first take a detailed slow panned video of each room, wall by wall, ceiling and carpets. Basement and all tools (open all and spread). Every appliance and fixtures get the medicine cabinet and under all sinks. Same with garage and all in there, same with shed and all vehicles/tools/bikes etc.\nYou can then slowly advance the video, list the books etc, all the kitchen cabinet/freezer/clothes/bedding/frozen foods as well as dry foods etc and amass a fully exhaustive home inventory. An amazing amount of stuff = $$ builds up over the years and few people have such a detailed loss record. You can do the video in 15 minutes and as long as you save in the cloud - in a few places even - you can tabulate that aspect of your loss in detail later - even after the fact if well saved.\n \nreply",
      "This is a useful tool for any homeowner, not just those threatened by wildfire. Good luck and godspeed to those currently in harm's way in California.\n \nreply",
      "[flagged]"
    ],
    "link": "https://docs.google.com/spreadsheets/d/1TPeJzW5pa-BiJZjuEa1yGSFs7ZJetbnxf2gjMvv4tkc/htmlview#gid=1160377357",
    "first_paragraph": ""
  },
  {
    "title": "Virtual Vette \u2013 Racing Simulator Built from Wrecked Corvette (instructables.com)",
    "points": 32,
    "submitter": "starkparker",
    "submit_time": "2025-01-12T08:04:57 1736669097",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42672015",
    "comments": [
      "Related, Ridge Racer Full Size based on an MX-5/Eunos/Miatahttps://arcadeblogger.com/2022/11/20/the-last-ridge-racer/\n \nreply",
      "I'm surprised they built something this involved and went with a belt drive wheel. Direct drive options are very affordable these days and the difference is night and day.\n \nreply",
      "Generally, this is a really cool project but an extremely poor racing simulator. I think the focus was clearly much more on \"let's build something cool\" rather than the sim-racing aspect, and they succeeded at that (I love the choice of donor car - it's very charming shrunken down).The display setup, choice of wheel/pedals, and general ergonomics of the final product are all kind of questionable, and regardless, I'm sure it's still a ton of fun.I'm sure a v.2 could make a lot of improvements.\n \nreply",
      "Yea this was made for a younger kid, so in a few years he\u2019ll grow out of it and probably want real nice gear on a regular metal rig. No need to overdo it! DD is nice but it\u2019s still way more expensive than an entry level offering.\n \nreply",
      "The placement of the monitor is also odd, doubly so because it is curved. Most sims will let you render offset from the driver seat (iRacing has keyboard shortcuts to make all these adjustments easily) but the image is being rendered from a \"first person\" projection a foot to your side. The monitor being tilted isn't ideal either.If using an actual chassis the best way to go would be with a projector mounted as low as it can be to clear the top of the car, ideally onto a screen that is curved out from the driver's perspective.\n \nreply"
    ],
    "link": "https://www.instructables.com/Virtual-Vette-Racing-Simulator-Built-From-Wrecked-/",
    "first_paragraph": "2,9908FeaturedThis project started because I was looking for a way to encourage and reward my son, Daniel, for earning his Eagle Scout rank.\u00a0Daniel had purchased a steering wheel, gear shift and pedal set-up to play games on his computer and had made some sketches of sim-rigs he wanted to build.\u00a0His designs were very similar to the ones you find online in that they are just frames with a seat and mounting brackets for the components.\u00a0I don\u2019t recall which one of us actually said it, but one of said, \u201cWhat if we built this into an actual car?\u201d\u00a0Thus, our journey began and it turned out to be one of the best father son projects ever.I should note at this point that in order to keep this guide manageable and fun, we will skip over many sub-process that are not particularly unique to this project. Explaining how to build a gaming computer, how to upholster automotive seats, lay fiberglass, welding, paint and body work are complete tutorials all their own and there are plenty of them out ther"
  },
  {
    "title": "Show HN: WASM-powered codespaces for Python notebooks on GitHub (marimo.io)",
    "points": 120,
    "submitter": "mscolnick",
    "submit_time": "2025-01-14T17:46:41 1736876801",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=42700852",
    "comments": [
      "Nice ! Is it possible to connect to an in browser DB like WASM DuckDB https://duckdb.org/docs/api/wasm/overview.html or https://github.com/babycommando/entity-db ?That would be most useful imho !\n \nreply",
      "duckdb works \u2014 just import duckdb. We also have built-in SQL cells, powered by duckdb, which should also work.\n \nreply",
      "I absolutely love that this can be hosted on Github Pages. Am I correct in understanding that these notebooks will run independently, and will not need to proxy through marimo.app (in case the app goes down), or is that what the CORS thing is about in note 4, and it will still need to go through this domain?\n \nreply",
      "Yea, this can be hosted on GitHub pages without any vendor infra (no marimo.app)These are two separate features:1) marimo.app + github.com/path/to/nb.ipynb does run on marimo.app infra. this is what the Show HN was about2) separately, you can use the marimo CLI to export assets to deploy to GitHub page: `marimo export html-wasm notebook.py -o output_dir --mode run` which can then can be uploaded to GH pages. This does not find all the data in your repo, so you would need to stick any data you was to access in a /public folder for your site. More docs here: https://docs.marimo.io/guides/exporting/?h=marimo+export+htm...\n \nreply",
      "I love seeing projects like this. When Pyiodide came out I was excited but it was a bit difficult to use, this looks and feels fantastic.I really like Observable as well, but I've found it difficult to find robust and broad numerical libraries in javascript like what Python has.I would love for this type of tool to redefine how we do science. It would be amazing if many scientific papers included both their data and the code in an interactive environment with zero installs and configuration. Plus when discussing a paper you could \"fork\" it and explore different analysis options live which for many fields would be totally feasible to do in the browser.\n \nreply",
      "I feel like pytomls and shared source are becoming standard, but yes-notebooks vs research code are sometimes very separate, very difficult to directly reproduce. A big difficultly with \"working out of the box, shared in browser\" is that weights, training, inference, simulations- are all still very compute intensive.BUT the nice thing about a stateless notebook, is that you can precompute values- and cache them. I've been really excited about expanding marimo's caching system, and would love to get to a point whether sharing a notebook means being able to run the research yourself without some big setup dance.\n \nreply",
      "Super cool to see a real use-case of WASM outside of just game dev and nerding out.\n \nreply",
      "We also have Flash, Java Applets, ActiveX and Silverlight back, running on top of WebAssembly.\n \nreply",
      "Blazor is another example\n \nreply",
      "This is really cool -- going to show it off to my team. I love the fact that you opened it up so that it will work with Jupyter notebooks as well.\n \nreply"
    ],
    "link": "https://docs.marimo.io/guides/publishing/playground/#open-notebooks-hosted-on-github",
    "first_paragraph": "Our online playground lets you\ncreate and share marimo notebooks for free, without creating an account.Playground notebooks are great for embedding in other web pages \u2014 all the\nembedded notebooks in marimo's own docs are playground notebooks. They\nare also great for sharing via links.Try our playground! Just navigate to\nhttps://marimo.new.WebAssembly notebooks onlyCurrently, the online playground only allows the creation of WebAssembly\nnotebooks. These are easy to share and embed in other\nweb pages, but have some limitations in packages and performance.The notebook embedded below is a playground notebook!Playground notebooks run at marimo.app.To create a new playground notebook, visit https://marimo.new.Think of marimo.new as a\nscratchpad for experimenting with code, data, and models and for prototyping\ntools, available to you at all times and on all devices.Saving playground notebooksWhen you save a WASM notebook, a copy of your code is saved to your\nweb browser's local storage. When "
  },
  {
    "title": "Servo vs. steppers: Speed, Torque and Accuracy [video] (youtube.com)",
    "points": 132,
    "submitter": "f1shy",
    "submit_time": "2025-01-14T14:05:40 1736863540",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=42697335",
    "comments": [
      "\"Servo\" is a control method. You can control steppers using servo methods and it's pretty good actually. There are chinese encoder boards that'll do everything for like $10\n \nreply",
      "interesting observation. i found a reference project detailing how this works belowhttps://calischs.pages.cba.mit.edu/step/servo-stepper/\n \nreply",
      "The ones with the magnet? Those are really cool indeed, such a great idea.\n \nreply",
      "That's a unusual servo. Servos usually have a small optical encoder that emits quadrature pulses as it rotates. That's immune to magnetic interference but can potentially miss counts. This servo seems to have an magnetic analog position sensor. Something like this.[1] Can't miss counts but has less noise immunity.[1] https://www.analog.com/en/products/adaf1080.html\n \nreply",
      "Our nomenclature for servos in machine controls is a little different, for example a traditional 3phase servo with a 3phase hall sensor is still called \"servo\" even if it doesn't have an optical encoder. With this setup you can commutate the motor and get smooth motion and variable speed control but not precise position control. We then add on a optical shaft encoder or a optical linear encoder for position control.In the video he has what looks like a magnetic absolute encoder, I have honestly never seen that in industrial applications although I have seen \"absolute\" optical encoders that have a backup battery to store the home point, kind of weird imo.I was surprised when I first encountered servos with just magnetic hall effect sensors but there are actually lots of applications where you want speed control but not accurate position control.\n \nreply",
      "Absolute encoders can be really good for position critical applications that you don't want to re-home all the time. Linear stages, winches - if you don't want to re home it and it needs accurate position control you start looking at an absolute encoder.Even for speed control, hall effect sensors are kind of a poor way to track position. What is nice about hall effect sensors is that you can use them as a signal to perform brushless commutation in your motor controller, and then also use them as a poor-man's encoder. Very useful if you don't need that much accuracy in your application, but you do need brushless motors for some reason. But one of the first things I would go to as a application engineer was recommend customers get an encoder mounted.\n \nreply",
      "It seems like an absolute encoder would immediately lose its advantages if a reduction gear were used though?  Then you would still need to rehome the number of revolutions.\n \nreply",
      "Wouldn't you mount the encoder after the reductor then, where the position actually matters?\n \nreply",
      "Yes, we are discussing servos packaged with an absolute encoder\n \nreply",
      "You can get multi-turn absolute encoders for these applications.\n \nreply"
    ],
    "link": "https://www.youtube.com/watch?v=H-nO1F-AO9I",
    "first_paragraph": ""
  },
  {
    "title": "Cosmos Keyboard: Scan your hand, build a keyboard (ryanis.cool)",
    "points": 165,
    "submitter": "cdata",
    "submit_time": "2025-01-13T17:42:00 1736790120",
    "num_comments": 39,
    "comments_url": "https://news.ycombinator.com/item?id=42686144",
    "comments": [
      "Author here. It\u2019s a surprise seeing this posted while I\u2019m in the middle of traveling. Happy to answer any questions! If you\u2019re curious, the tech stack is static assets bundled with sveltekit/vite and hosted on gitlab pages + a minimal go backend.\n \nreply",
      "What you've built is a work of art, it blew my mind the first time I came across it. I love the completeness of vision (boomboom tss).I've only just realised that you've opensourced it all so I haven't looked at the code yet, but a couple of Qs:- is there anything that the old wave your fingers at the screen pose detect model did better the new screenpalm method? I always wondered if you were detecting the angle of motion (pronation?) for each finger to tilt its keywell, and I guess it's maybe harder to do that with the new method.- have you ever thought about using the measurements for one hand to spot measurement outliers in the other? Would it ever make sense to generate a symmetrical keyboard from the combined measurements of both hands?- have you thought about collaborating with other keyboard designers to add their designs to the app and give them a cut? For example the Cygnus[0] is a stunning keyboard; I'm not sure if it is parametrically generated, but I do remember thinking that I'd absolutely pay money to get one tailored to my hand geometry, and then wondering whether you could ever pull off building a designer marketplace/ecosystem out of the whole thing, maybe with a little sdk. It's one thing that the aliexpress copycat sellers can't copy.Also somehow only just realised that you have a blog. I might go read it.[0]: https://kbd.news/Cygnus-1.0-2307.html\n \nreply",
      "It could really use a way to control the spread of the button around the track ball. Just to make them closer together. The palmrest should also grow wider when columns are added to the keyboard.Also, would be great to have an option of adding a USB hub on the inner edge for plugging in a USB key or adding a USB plug on the outside for a mouse.\n \nreply",
      "On the advanced tab, you can adjust the vertical spacing of the thumb cluster to move keys closer together. As for USB hubs, at some point I need to draw a line and stop adding options. You can download a STEP file of the keyboard and edit it in CAD, or if you\u2019re looking for something easier to learn, I recommend editing the STL file in tinkercad. If you don\u2019t mind plugging in the mouse to the back, you can configure custom-sized connector cutouts in advanced mode.\n \nreply",
      "I used this to build my current keyboard a few months ago. It was my first hand-wired keyboard, and this made it much more approachable. Thanks for creating it!\n \nreply",
      "Great work @rianadon!\n \nreply",
      "I've had so many keyboards that I can't even count them. I've owned five mechanical ones alone. Out of the ergonomic ones, I've only had one - a Microsoft and it was pretty nice. Almost all of them have been replaced because they broke. Either the keys stop working (most often) or the stabilizers start failing.In my opinion, the best keyboards are the ones that are very easy to clean :) Ideally, switches should be chosen based on your hands since everyone has different preferences. I'm currently using Keychron K5 SE ultra-slim with Low Profile Optical hot-swappable \"Banana\" switches, and it's the most comfortable keyboard I've ever had \u2014 and it's not even that expensive (for a mechanical keyboard). Before that, I had SteelSeries' top model, and it broke after about a year.Building custom keyboards is next-level, and I think I'll pass on that. What matters most is that it's comfortable to type on and easy to clean. A piece of advice for beginners: don't buy keyboards from Logitech or Apple. They're overrated and not worth their price.\n \nreply",
      "I feel like I am your hardware destroying cousin. For me it's mice, not keyboards.I've had the same keyboard for like a decade, but I go through mice every 3-6 months. I've tried logitech / corsair / no-name / razor. 90% of the time I replace a mouse because of phantom double clicks or the mouse3 button just ceasing to work.More rarely, the mouse will reconnect cycle over and over, or the scroll wheel will break.I don't THINK I abuse them, but my body count indicates maybe I'm too hard on them and don't know it.Maybe we need hardware that'll give us data on how mean we are to them so we can gain perspective. :p\n \nreply",
      "You can disassemble and replace mouse switches quite easily and very cheaply if you already own a soldering iron and some basic tools - mouse switch failure is the typical reason for misclick double click problem\n \nreply",
      "Maybe it's time you switch to a trackball.\n \nreply"
    ],
    "link": "https://ryanis.cool/cosmos/",
    "first_paragraph": "Custom-Build A Keyboard Fit To YouDon't SettleFor One-Size-Fits-AllSee more keyboards in the showcase\n      Cosmos is the easiest way to design a keyboard around your one-of-a-kind hands. Scan your hand using\n    just your phone camera, then fit a keyboard to the scan. The key positions align to your fingers'\n    lengths and movement.Add a trackball, trackpad, encoder, or OLED display. There's support for MX, Choc, and Alps switches,\n    and almost every type of keycap. Plus with 7 different microcontrollers, you can mix and match all\n    you like.Choose from 3 types of cases, split or unibody, and many customizations.Cosmos catches errors before you print and automatically fixes common model issues.Custom Thumbs mode allows you to drag and drop keys and trackballs in the thumb cluster into\n        place.Your artisans are now ergonomic. Whatever batch of keycaps you decide to use, Cosmos will arrange\n        them to fit your desired curvature.Cosmos has first-class support for Amoeba K"
  },
  {
    "title": "Google\u2019s OAuth login doesn\u2019t protect against purchasing a failed startup domain (trufflesecurity.com)",
    "points": 399,
    "submitter": "simiones",
    "submit_time": "2025-01-14T16:14:01 1736871241",
    "num_comments": 216,
    "comments_url": "https://news.ycombinator.com/item?id=42699099",
    "comments": [
      "It's not 100% clear to me, from reading TFA, what the actual vuln is.Suppose DankStartup folds and I, being a morally-dubious sort of fellow, purchase dankstartup.net which I then use to sign into DankStartup's O365, or DankStartup's ChatGPT as a DankStartup employee.Isn't that a failure on DankStartup's part, to not shut down their business accounts?  And isn't it also a failure on e.g. Microsoft or OpenAI's parts, since they're providing service to a defunct business entity who can't pay its bills?To describe this as a vuln in oauth doesn't really make sense to me.\n \nreply",
      "Well, think about it this way:1. I create DankStartup and my company uses Google workspaces and Google auth for a bunch of stuff, like payroll.2. DankStartup goes under and we close our Google accounts/let our domain lapse.3. Someone else buys DankStartup.com, sets up a Google workspace, and attempts Google auth to log into stuff, and it works.The problem is that the original DankStartup has a Google account that they create in #1, and Google goes around telling other sites (via Auth) \"this is user X from company Y\".Then, the impostors in step #3 create a different google account with the same domain, and Google says \"yeah, these are definitely the same guys as before\", even though Google is fully capable of discerning that that is not the case; these are different people with a different workspace account, different names, different payment information, and so on, but Google is saying that if you're holding the domain you are therefore the same people as far as they're concerned and is asserting that to other companies. They are (or were) refusing to provide any indication to those other companies that these are not, in fact, the same people, so those other companies aren't even capable of doing their due diligence of extra validation if they want to.It's similar to looking at a driver's license and just matching the name rather than the actual ID number; it's possible for someone else to have the same name as you, and identity documents have unique identifiers for specifically that reason.\n \nreply",
      ">Then, the impostors in step #3 create a different google account with the same domain, and Google says \"yeah, these are definitely the same guys as before\", even though Google is fully capable of discerning that that is not the case; these are different people with a different workspace account, different names, different payment information, and so on, but Google is saying that if you're holding the domain you are therefore the same people as far as they're concerned and is asserting that to other companies.Yep I understand the mechanism by which this gets abused; I think we just disagree on the implications.  I don't work for Google but it seems from the outside that they're treating the OIDC subject claim as referencing the domain attached to the workspace account, or something similar.  I've seen implementations where the `sub` claim is more granular, so to me that indicates the field is underspecified.Given all that, I suppose TFAuthor's proposed solution is a good way forward.I still think classifying this as an OAuth vulnerability isn't correct.\n \nreply",
      "Traditionally, SAML / OIDC trust is established using public/private keypairs. Each IdP/SP pair gets a unique combination. In this case, a domain changing hands would not allow the new owner to gain access to the old owner's accounts.In the case of Google OAuth, it's possible to forego this in order to allow any Google user from any Google workspace to login to your application. See the distinction between \"public and internal applications\" here: https://support.google.com/cloud/answer/6158849?hl=en#zippy=...Some applications (e.g. Tailscale) take advantage of the public Google OAuth API to provide private internal corporate accounts. A common misconfiguration here is to use the domain portion of the \"email\" attribute - this can be spoofed by Google Workspace admins. That's not what's happening here.Instead, Google instructs you to look at the \"hd\" parameter, specific to Google, to determine the Google Workspace a given user belongs to for security purposes. This field cannot be overridden by Google Workspace admins. The trust breaks down when the domain changes hands, a new Google Workspace account is opened, but the old \"hd\" value is reused.You can read more about \"hd\" here: https://developers.google.com/identity/openid-connect/openid... (find the table and read the descriptions for both \"email\" and \"hd\".)You can avoid this issue by using a custom Google OIDC IdP configured for internal access only in your applications, rather than using a pre-configured public Google OIDC IdP (be very careful you mark it internal!) A new domain owner would not be able to retrieve the secret key you previously generated.\n \nreply",
      "Yea \"hd\" doesn't work here.Users can make google accounts with corporate domains but without gsuite hosting attached. The same concern applies to just @gmail.com addys. Google has gotten big on nuking inactive accounts now, what stops similar openings where a person's inactive account gets recreated by an attacker and waltz his way into SSO systems? There's some corporate systems (usually benefits) that allow personal emails attached to the same account as work emails for example.",
      "To clarify, any multi-tenant app that has a Google login button that \"just works\" without you having to set up your own OAuth client creds is using the \"public\" app option. Effectively the OAuth client side doesn't care whether you use a public Gmail account or enterprise Workspaces account to login.However, on the Workspace Admin side you can set policies as to whether the org accounts can login to arbitrary public apps by default, or set up an app allowlist, etc. It's definitely a best practice not to let your users login to arbitrary apps, even if it's only for the profile data and not other API scopes.If you do login to a public app with an org account, how that app decides to group/authorize users from (what it perceived as) the same org/domain could be a sensitive procedure with opportunity for exploit, and it seems like maybe Google should offer a more reliable unique org id claim.If you are making your own internal app then client is going to be internal only by default. Note this is managed in a GCP project. If you're using Workspace and GCP then you may want to use GCP services with built-in Google auth like Cloud Functions, App Engine, Identity Aware Proxy, etc.\n \nreply",
      "It seems there are two possible problems.The first is whether taking over a lapsed domain allows you to takeover an existing Google Workspace (or Cloud Identity) organization. This it what houses the corporate email accounts and OAuth client registrations. If Google allows this scenario then the linked account takeover is simply one symptom / side effect among many. TFA is not clear on whether this step actually happened... I assume not, since if it were the case we'd be talking about direct access to the Google account data rather than only linked SP accounts.The second is when an SP doesn't properly use the `sub` claim as a unique identifier. It sounds like some products don't understand this requirement and why it \"seems to change 0.04%\" of the time. I do agree that a unique identifier for the org itself would be a good addition to the token.That said, I'm still not clear how the second problem manifests if the old OAuth client creds (housed in the old Workspace org id) are invalid. Presumably attacker can login to the SP admin account using just email based password recovery, then reconfigure the OAuth integration with new secrets. In that case the SP is failing to do MFA on the email login.Would love to hear if I'm missing something.\n \nreply",
      "> That said, I'm still not clear how the second problem manifests if the old OAuth client creds (housed in the old Workspace org id) are invalid.As far as I understand, this is not a necessary step. The SP is configured to trust Google's public OAuth IdP, not a specific Google Workspace account. So there are no special secrets shared between the old Google Workspace account and, say, Slack. The Slack org trusts any user that Google's public OAuth IdP says is a valid user in the example.com domain. Slack doesn't have to do any MFA for these accounts, they trust Google did that already.Now, you may not be able to access the Slack org admin account in this way, say to add/remove users or delete the org. But you can access all of the other information that any random employee in the org could access back when it was setup, including a list of all other users in the org.\n \nreply",
      "Ah, right, Slack can have their own public oath client which is used for the code grant.So what happens is:\n1. New Workspace org created with same (old) domain name\n2. Same domain name is sent in `hd` property, existing email address sent in the `email` property, new uuid in the `sub` property.If the app is only matching on email instead of sub, then it will grant access to previous user data. Additionally, even if it makes a new user based on the new sub, it may still grant access to other SP resources associated with the existing domain based on the email address or hd value.Instead there needs to be something like `hd` but uniquely identifying the Workspace org entity itself, not just the domain.\n \nreply",
      ">>>The problem is that the original DankStartup has a Google account that they create in #1, and Google goes around telling other sites (via Auth) \"this is user X from company Y\".Google is telling other sites that it's bob@DankStartup.com - isn't that true? Isn't this on DankStartup to close down operations cleanly?\n \nreply"
    ],
    "link": "https://trufflesecurity.com/blog/millions-at-risk-due-to-google-s-oauth-flaw",
    "first_paragraph": ""
  },
  {
    "title": "If You Ever See This Speed Sign, You're Probably Going to Die (theautopian.com)",
    "points": 118,
    "submitter": "colinprince",
    "submit_time": "2025-01-14T21:59:12 1736891952",
    "num_comments": 60,
    "comments_url": "https://news.ycombinator.com/item?id=42704491",
    "comments": [
      "> engineers mused over whether there was anything to be done top stop a torrent of enemy missiles falling across the nation. These superweapons seemed to promise destruction on an overbearing scale, threatening the very existence of human civilization itself.THEY STILL DO!!!!This is what drives me nuts about our politics: so many people seem to think we can flirt with the sort of nationalism that led us into WWI and WWII. But, friends, that road leads to your death in every direction. Mutually Assured Destruction is still a thing. Nuclear peace only works as long as all parties persistently work toward de-escalation, which can be measured by adherence to consensus and norms. Nationalism is antithetical to that posture. The iconoclast leaders of national populism are rooted in rulebreaking. They also tend to embrace strong foreign policy talk as a short term bolster to domestic support, again, antithetical to de-escalation.\n \nreply",
      "MAD precludes total wars of national survival ala WW2. It does not at all preclude the sorts of cabinet wars that were fought between the European powers for centuries before Napoleon.\n \nreply",
      ">nationalism that led us into WWI and WWII.WW1 kicked off due to a royal getting murdered in broad daylight which triggered a cascade of alliances. Nationality had nothing to do with it.WW2 kicked off due to the victors of WW1 raping Germany so bad they had nothing but nationalism to hold on to. American nationalism also initially wanted nothing to do with WW2, begrudgingly started helping behind the curtains once the UK became endangered and then went \"Yeah, fuck this noise. Y'all will be infamous.\" once Japan started playing funny tunes.Nationalism is both good and bad, namely too much of it is a bad thing like everything else in the world while too little of it is also a bad thing akin to malnourishment.\n \nreply",
      "The current government is going to weaken Western alliances. China is going to take Taiwan. Japan and South Korea are going to rapidly develop nuclear weapons programs. North Korea will increase the range of its missiles to reliably hit the entire US. The entire Middle East is going to nuclearize, with Iran first and then Saudi Arabia (the Trump white house was trying to arrange this last term, I suppose they'll succeed this time.) You cannot have a world where every single nation has nuclear weapons mounted on ICBMs and not have a war in the medium term.\n \nreply",
      "By current government I hope you mean Biden administration (since Biden is still in power). And I totally agree that it pushed us towards WW3 by trying to expand NATO to Ukraine, very well knowing [0] that it will leave Russia with no choice but to invade Ukraine.0 https://x.com/ImReadinHere/status/1500782351831662592\n \nreply",
      ">Japan and South Korea are going to rapidly develop nuclear weapons programs.Japan would need to amend their constitution for that, and that is simply not happening. They have failed to amend their constitution for much more mundane objectives, let alone nukes.\n \nreply",
      "Hong Kong was very different than Taiwan would be. Taiwan will be a bloodbath as an armed, diehard anti-communist population fights to the death.And if China just wants to blow it all to bits, what was the point? They want to dominate the country and its people, not own a smoldering ruin.\n \nreply",
      "I doubt there would be any meaningful resistance, TBH.\n \nreply",
      "That notion didn't stop Russia from scorching Ukraine, can't see why it would stop China.\n \nreply",
      "The issue Russia does have with Ukraine is it has to keep it's ships and planes away from Ukrainian areas or they get shot down. That'd be a problem for invading Taiwan.\n \nreply"
    ],
    "link": "https://www.theautopian.com/if-you-ever-see-this-speed-sign-youre-probably-going-to-die/",
    "first_paragraph": "The Manual on Uniform Traffic Control Devices (MUTCD) is widely considered to be a dry and unemotional document. Published by the Department of Transportation, it outlines the basic specifications of all the street signs you could expect to see out on roads and highways across the United States. Most are familiar, but if you dive deeper into its pages, you can find some unsettling relics from darker times.Back in the mid-20th century, America was tangling with the realities of nuclear war. Top generals contemplated targeting strategies, while engineers mused over whether there was anything to be done top stop a torrent of enemy missiles falling across the nation. These superweapons seemed to promise destruction on an overbearing scale, threatening the very existence of human civilization itself.Against this bleak backdrop, government administrators turned to the concept of Civil Defense. The idea was to do whatever could be done to protect the citizens of the nation from the horrors of"
  },
  {
    "title": "Show HN: Value likelihoods for OpenAI structured output (arena-ai.github.io)",
    "points": 65,
    "submitter": "ngrislain",
    "submit_time": "2025-01-14T15:52:43 1736869963",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=42698753",
    "comments": [
      "This looks super valuable!That said, it's concerning to see the reported probability for getting a 4 on a die roll is 65%.Hopefully OpenAI isn't that biased at generating die rolls, so is that number actually giving us information about the accuracy of the probability assessments?\n \nreply",
      "Fair dice rolls is not an objective that cloud LLMs are optimized for. You should assume that LLMs cannot perform this task.This is a problem when people naively use \"give an answer on a scale of 1-10\" in their prompts. LLMs are biased towards particular numbers (like humans!) and cannot linearly map an answer to a scale.It's extremely concerning when teams do this in a context like medicine. Asking an LLM \"how severe is this condition\" on a numeric scale is fraudulent and dangerous.\n \nreply",
      "It'll also give you different results based on logically-irrelevant numbers that might appear elsewhere in the collaborative fiction document.\n \nreply",
      "> That said, it's concerning to see the reported probability for getting a 4 on a die roll is 65%.Finding that an LLM is biased toward inventing die rolls that are the median result rounded to an available result by the most common rounding method is...not particularly surprising. If you want a fair RNG, use an RNG deigned to be fair, not an LLM where that would be, at best, an emergent accidental property.\n \nreply",
      "Thank you! The number is the the sum of the logprobs from the token constituting the individual values. So it does represent the likelihood of seeing this value.\nSo yes OpenAI is super-biased as a random number generator.\nWe sampled other values from OpenAI and got other die roll values, but with much lower probs (5 has 8% chances ).\n \nreply",
      "More precisely it represents the likelihood of seeing this value conditional on the tokens before it.\n \nreply",
      "and i guess includes other possibilities than numbers, like 'f' which could lead to four or five. There's probably a separate probability for 'fi' and 'fo' too.\n \nreply",
      "What about the models they offer would make you think that it wouldn't be biased at generating random die rolls?\n \nreply",
      "I feel like https://xkcd.com/221/ might be heavily influencing what the typical \"random\" die roll looks like on the internet ;)\n \nreply",
      "Based on this comic I've seen unit tests use 4 as replacement for random generated number to ensure non flakiness (of course, only when needed). But it might explain the LLM's bias?\n \nreply"
    ],
    "link": "https://arena-ai.github.io/structured-logprobs/",
    "first_paragraph": "structured-logprobs is an open-source Python library that enhances OpenAI's structured outputs by providing detailed information about token log probabilities.This library is designed to offer valuable insights into the reliability of an LLM's structured outputs. It works with OpenAI's Structured Outputs, a feature that ensures the model consistently generates responses adhering to a supplied JSON Schema. This eliminates concerns about missing required keys or hallucinating invalid values.Simply install with pip install structured-logprobsThen use it this way:For more details, visit Getting Started.The module contains a function for mapping characters to token indices (map_characters_to_token_indices) and two methods for incorporating log probabilities:"
  },
  {
    "title": "Reversible computing escapes the lab (ieee.org)",
    "points": 207,
    "submitter": "jasondavies",
    "submit_time": "2025-01-10T21:42:54 1736545374",
    "num_comments": 51,
    "comments_url": "https://news.ycombinator.com/item?id=42660606",
    "comments": [
      "Nice, these ideas have been around for a long time but never commercialized to my knowledge. I've done some experiments in this area with simulations and am currently designing some test circuitry to be fabbed via Tiny Tapeout.Reversibility isn't actually necessary for most of the energy savings. It saves you an extra maybe 20% beyond what adiabatic techniques can do on their own. Reason being, the energy of the information itself pales in comparison to the resistive losses which dominate the losses in adiabatic circuits, and it's actually a (device-dependent) portion of these resistive losses which the reversible aspect helps to recover, not the energy of information itself.I'm curious why Frank chose to go with a resonance-based power-clock, instead of a switched-capacitor design. In my experience the latter are nearly as efficient (losses are still dominated by resistive losses in the powered circuit itself), and are more flexible as they don't need to be tuned to the resonance of the device. (Not to mention they don't need an inductor.) My guess would be that, despite requiring an on-die inductor, the overall chip area required is much less than that of a switched-capacitor design. (You only need one circuit's worth of capacitance, vs. 3 or more for a switched design, which quadruples your die size....)I'm actually somewhat skeptical of the 4000x claim though.  Adiabatic circuits can typically only provide about a single order of magnitude power savings over traditional CMOS -- they still have resistive losses, they just follow a slightly different equation (f\u00b2RC\u00b2V\u00b2, vs. fCV\u00b2). But RC and C are figures of merit for a given silicon process, and fRC (a dimensionless figure) is constrained by the operational principles of digital logic to the order of 0.1, which in turn constrains the power savings to that order of magnitude regardless of process. Where you can find excess savings though is simply by reducing operating frequency. Adiabatic circuits benefit more from this than traditional CMOS. Which is great if you're building something like a GPU which can trade clock frequency for core count.\n \nreply",
      "Hi, someone pointed me at your comment, so I thought I'd reply.First, the circuit techniques that aren't reversible aren't truly, fully adiabatic either -- they're only quasi-adiabatic. In fact, if you strictly follow the switching rules required for fully adiabatic operation, then (ignoring leakage) you cannot erase information -- none of the allowed operations achieve that.Second, to say reversible operation \"only saves an extra 20%\" over quasi-adiabatic techniques is misleading. Suppose a given quasi-adiabatic technique saves 79% of the energy, and a fully adiabatic, reversible version saves you \"an extra 20%\" -- well, then now that's 99%. But, if you're dissipating 1% of the energy of a conventional circuit, and the quasi-adiabatic technique is dissipating 21%, that's 21x more energy efficient! And so you can achieve 21x greater performance within a given power budget.Next, to say \"resistive losses dominate the losses\" is also misleading. The resistive losses scale down arbitrarily as the transition time is increased. We can actually operate adiabatic circuits all the way down to the regime where resistive losses are about as low as the losses due to leakage. The max energy savings factor is on the order of the square root of the on/off ratio of the devices.Regarding \"adiabatic circuits can typically only provide an order of magnitude power savings\" -- this isn't true for reversible CMOS! Also, \"power\" is not even the right number to look at -- you want to look at power per unit performance, or in other words energy per operation. Reducing operating frequency reduces the power of conventional CMOS, but does not directly reduce energy per operation or improve energy efficiency. (It can allow you to indirectly reduce it though, by using a lower switching voltage.)You are correct that adiabatic circuits can benefit from frequency scaling more than traditional CMOS -- since lowering the frequency actually directly lowers energy dissipation per operation in adiabatic circuits. The specific 4000x number (which includes some benefits from scaling) comes from the analysis outlined in this talk -- see links below - but we have also confirmed energy savings of about this magnitude in detailed (Cadence/Spectre) simulations of test circuits in various processes. Of course, in practice the energy savings is limited by the resonator Q value. And a switched-capacitor design (like a stepped voltage supply) would do much worse, due to the energy required to control the switches.https://www.sandia.gov/app/uploads/sites/210/2023/11/Comet23...\nhttps://www.youtube.com/watch?v=vALCJJs9DtwHappy to answer any questions.\n \nreply",
      "Thanks for the reply, was actually hoping you'd pop over here.I don't think we actually disagree on anything. Yes, without reverse circuits you are limited to quasi-adiabatic operaton. But, at least in the architectures I'm familiar with (mainly PFAL), most of the losses are unarguably resistive. As I understand PFAL, it's only when the operating voltage of a given gate drops below Vth that the (macro) information gets lost and reversibility provides benefit, which is only a fraction of the switching cycle. At least for PFAL the figure is somewhere in the 20% range IIRC. (I say \"macro\" because of course the true energy of information is much smaller than the amounts we're talking about.)The \"20%\" in my comment I meant in the multiplicative sense, not additive. I.e. going from 79% savings to 83.2%, not 99%. (I realize that wasn't clear.)What I find interesting is reversibility isn't actually necessary for true adiabatic operation. All that matters is the information of where charge needs to be recovered from can be derived somehow. This could come from information available elsewhere in the circuit, not necessarily the subsequent computations reversed. (Thankfully, quantum non-duplication does not apply here!)I agree that energy per operation is often more meaningful, BUT one must not lose sight of the lower bounds on clock speed imposed by a particular workload.Ah thanks for the insight into the resonator/switched-cap tradeoff. Yes, capacitative switching designs which are themselves adiabatic I know is a bit of a research topic. In my experience the losses aren't comparable to the resistive losses of the adiabatic circuitry itself though. (I've done SPICE simulations using the sky130 process.)\n \nreply",
      "It's been a while since I looked at it, but I believe PFAL is one of the not-fully-adiabatic techniques that I have a lot of critiques of.There have been studies showing that a truly, fully adiabatic technique in the sense I'm talking about (2LAL was the one they checked) does about 10x better than any of the other \"adiabatic\" techniques. In particular, 2LAL does a lot better than PFAL.> reversibility isn't actually necessaryThat isn't true in the sense of \"reversible\" that I use. Look at the structure of the word -- reverse-able. Able to be reversed. It isn't essential that the very same computation that computed some given data is actually applied in reverse, only that no information is obliviously discarded, implying that the computation always could be reversed. Unwanted information still needs to be decomputed, but in general, it's quite possible to de-compute garbage data using a different process than the reverse of the process that computed it. In fact, this is frequently done in practice in typical pipelined reversible logic styles. But they still count as reversible even though the forwards and reverse computations aren't identical.\nSo, I think we agree here and it's just a question of terminology.Lower bounds on clock speed are indeed important; generally this arises in the form of maximum latency constraints. Fortunately, many workloads today (such as AI) are limited more by bandwidth/throughput than by latency.I'd be interested to know if you can get energy savings factors on the order of 100x or 1000x with the capacitive switching techniques you're looking at. So far, I haven't seen that that's possible. Of course, we have a long way to go to prove out those kinds of numbers in practice using resonant charge transfer as well. Cheers...\n \nreply",
      "PFAL has both a fully adiabatic and quasi-adiabatic configuration. (Essentially, the \"reverse\" half of a PFAL gate can just be tied to the outputs for quasi-adiabatic mode.) I've focused my own research on PFAL because it is (to my knowledge) one of the few fully adiabatic families, and of those, I found it easy to understand.I'll have to check out 2LAL. I haven't heard of it before.No, even with a fully adiabatic switched-capacitance driver I don't think those figures are possible. The maximum efficiency I believe is 1-1/n, n being the number of steps (and requiring n-1 capacitors). But the capacitors themselves must each be an order of magnitude larger than the adiabatic circuit itself. So it's a reasonable performance match for an adiabatic circuit running at \"max\" frequency, with e.g. 8 steps/7 capacitors, but 100x power reduction necessary to match a \"slowed\" adiabatic circuit would require 99 capacitors... which quickly becomes infeasible!\n \nreply",
      "Yeah, 2LAL (and its successor S2LAL) uses a very strict switching discipline to achieve truly, fully adiabatic switching. I haven't studied PFAL carefully but I doubt it's as good as 2LAL even in its more-adiabatic version.For a relatively up-to-date tutorial on what we believe is the \"right\" way to do adiabatic logic (i.e., capable of far more efficiency than competing adiabatic logic families from other research groups), see the below talk which I gave at UTK in 2021. We really do find in our simulations that we can achieve 4 or more orders of magnitude of energy savings in our logic compared to conventional, given ideal waveforms and power-clock delivery. (But of course, the whole challenge in actually getting close to that in practice is doing the resonant energy recovery efficiently enough.)https://www.sandia.gov/app/uploads/sites/210/2022/06/UKy-tal...\nhttps://tinyurl.com/Frank-UKy-2021The simulation results were first presented (in an invited talk to the SRC Decadal Plan committee) a little later that year in this talk (no video of that one, unfortunately):https://www.sandia.gov/app/uploads/sites/210/2022/06/SRC-tal...However, the ComET talk I linked earlier in the thread does review that result also, and has video.\n \nreply",
      "How do the efficiency gains compare to speedups from photonic computing, superconductive computing, and maybe fractional Quantum Hall effect at room temperature computing? Given rough or stated production timelines, for how long will investments in reversible computing justify the relative returns?Also, FWIU from \"Quantum knowledge cools computers\", if the deleted data is still known, deleting bits can effectively thermally cool, bypassing the Landauer limit of electronic computers? Is that reversible or reversibly-knotted or?\"The thermodynamic meaning of negative entropy\" (2011) https://www.nature.com/articles/nature10123 ... https://www.sciencedaily.com/releases/2011/06/110601134300.h... ;> Abstract: ... Here we show that the standard formulation and implications of Landauer\u2019s principle are no longer valid in the presence of quantum information. Our main result is that the work cost of erasure is determined by the entropy of the system, conditioned on the quantum information an observer has about it. In other words, the more an observer knows about the system, the less it costs to erase it. This result gives a direct thermodynamic significance to conditional entropies, originally introduced in information theory. Furthermore, it provides new bounds on the heat generation of computations: because conditional entropies can become negative in the quantum case, an observer who is strongly correlated with a system may gain work while erasing it, thereby cooling the environment.\n \nreply",
      "I have concerns about density & cost for both photonic & superconductive computing. Not sure what one can do with quantum Hall effect.Regarding long-term returns, my view is that reversible computing is really the only way forward for continuing to radically improve the energy efficiency of digital compute, whereas conventional (non-reversible) digital tech will plateau within about a decade. Because of this, within two decades, nearly all digital compute will need to be reversible.Regarding bypassing the Landauer limit, theoretically yes, reversible computing can do this, but not by thermally cooling anything really, but rather by avoiding the conversion of known bits to entropy (and their energy to heat) in the first place. This must be done by \"decomputing\" the known bits, which is a fundamentally different process from just erasing them obliviously (without reference to the known value).For the quantum case, I haven't closely studied the result in the second paper you cited, but it sounds possible.\n \nreply",
      "Can one define the process of an adiabetic circuit goes through like one would do analogusly for the carnot engine? The idea being coming up with a theoretical cieling for the efficiency of such a circuit in terms of circuit parameters?\n \nreply",
      "Yes a similar analysis is where the above expression f\u00b2RC\u00b2V\u00b2 comes from.Essentially -- (and I'm probably missing a factor of 2 or 3 somewhere as I'm on my phone and don't have reference materials) -- in an adiabatic circuit the unavoidable power loss for any individual transistor stems from current (I) flowing through that transistor's channel (a resistor R) on its way to and from another transistor's gate (a capacitor C). So that's I\u00b2R unavoidable power dissipation.I must be sufficient to fill and then discharge the capacitor to/from operating voltage (V) in the time of one cycle (1/f). So I=2fCV. Substituting this gives 4f\u00b2RC\u00b2V\u00b2.Compare to traditional CMOS, wherein the gate capacitance C is charged through R from a voltage source V. It can be shown that this dissipates \u00bdCV\u00b2 of energy though the resistor in the process, and the capacitor is filled with an equal amount of energy. Discharging then dissipates this energy through the same resistor. Repeat this every cycle for a total power usage of fCV\u00b2.Divide these two figures and we find that adiabatic circuits use 4fRC times as much energy as traditional CMOS. However, f must be less than about 1/(5RC) for a CMOS circuit to function at all (else the capacitors don't charge sufficiently during a cycle) so this is always power savings in favor of adiabatics. And notably, decreasing f of an adiabatic circuit from the maximum permissible for CMOS on the same process increases the efficiency gain proportionally.(N.B., I feel like I missed a factor of 2 somewhere as this analysis differs slightly from my memory. I'll return with corrections if I find an error.)\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/reversible-computing",
    "first_paragraph": "This weird information-theory concept has become a power-saving chipMichael Frank has spent his career as an academic researcher working over three decades in a very peculiar niche of computer engineering. According to Frank, that peculiar niche\u2019s time has finally come. \u201cI decided earlier this year that it was the right time to try to commercialize this stuff,\u201d Frank says. In July 2024, he left his position as a senior engineering scientist at Sandia National Laboratories to join a startup, U.S. and U.K.-based Vaire Computing.\n\n\tFrank argues that it\u2019s the right time to bring his life\u2019s work\u2014called \n\treversible computing\u2014out of academia and into the real world because the computing industry is running out of energy. \u201cWe keep getting closer and closer to the end of scaling energy efficiency in conventional chips,\u201d Frank says. According to an IEEE semiconducting industry road map report Frank helped edit, by late in this decade the fundamental energy efficiency of conventional digital log"
  },
  {
    "title": "The rise and fall of the English sentence (2017) (nautil.us)",
    "points": 75,
    "submitter": "cal85",
    "submit_time": "2025-01-14T10:02:56 1736848976",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=42695580",
    "comments": [
      "It's fun to read letters written by children in the 18th century, as it gives you a little glimpse into what it was like to learn to write at this level of complexity, and what aspects of written language children were being taught to master.Here for example is a letter from John Quincy Adams to his father, written when he was ten:>DEAR SIR,\u2014I love to receive letters very well; much better than I love to write them. I make but a poor figure at composition, my head is too fickle, my thoughts are running after birds eggs play and trifles, till I get vexed with myself. Mamma has a troublesome task to keep me steady, and I own I am ashamed of myself. [...] If I can but keep my resolution, I will write again at the end of the week and give a better account of myself. I wish, Sir, you would give me some instructions, with regard to my time, and advise me how to proportion my Studies and my Play, in writing, and I will keep them by me, and endeavor to follow them. I am, dear Sir, with a present determination of growing better, yours.>P. S.\u2014Sir, if you will be so good as to favor me with a Blank Book, I will transcribe the most remarkable occurances I meet with in my reading, which will serve to fix them upon my mind.\n \nreply",
      "Truuuue, but that was John High-I-Quincy Adams, that one study estimated had an IQ around 170: https://www.acsu.buffalo.edu/~jcampbel/documents/SimontonPre...Even if he didn't, there's a lot of evidence that Johnny Boy was exceptionally bright, to say the least. You can't really expect that level of writing skill from a typical 10 year old.\n \nreply",
      "I shall be pleased to thank you for favoring us with this most poignant example; my heart doth flutter a few nanoseconds at the innocence of youth both present and lost to short-form video. I exhale, with a touch of melancholy, but feel gratitude nonetheless.\n \nreply",
      "Another fun format to read is military orders in the Revolutionary War/Napoleonic Wars era, how generals wrote (with a quill!) when actual bullets were whizzing around them. Even Civil War era orders still sound extraordinarily formal, and such orders from all eras are written in beautiful handwriting.\n \nreply",
      "I've read that Washington sent back, unopened, british letters which had been sent to him but without being addressed with all the proper military formalities.Was that his way of ensuring he didn't get labelled as an unlawful combatant?> such orders from all eras are written in beautiful handwritingthe 1876 orders to bring ammunition sent at Greasy Grass (\"Custer's Last Stand\") are an obvious counterexample: https://en.wikipedia.org/wiki/Battle_of_the_Little_Bighorn#/...are you sure you haven't been looking at transcriptions? (as in the upper right of the example above)\n \nreply",
      "That the XVIII, with its love of symmetry: oft observed in contrast as well as in comparison, and with its love of ornament: ascending from initial observation; continuing through main example; and ending upon a final period, is well exemplified by Gibbon, who in this inimitable style filled not just one, nor yet three, but a full six volumes of The History of the Decline and Fall of the Roman Empire (1776), is a fact to which all must acquiesce, yet, even so, the \"short-form\" was also present during this era, perhaps most memorably in the tricolon, as brief as it was lacking in invention, with which Prince William Henry, Duke of Gloucester and Edinburgh, greeted Gibbon's second volume: \"Always scribble, scribble, scribble! Eh, Mr. Gibbon?\"\n \nreply",
      "Constructing elegant, deeply nested sentences is an art in English as well as German (my first language).  But it is an art, more for connoisseurs than those who really need to communicate.An art that I appreciate more is at the opposite end.  Constructing elegant prose out of relatively simple sentences, like Ernest Hemingway.\"He was an old man who fished alone in a skiff in the Gulf Stream and he had gone eighty-four days now without taking a fish. In the first forty days a boy had been with him. But after forty days without a fish the boy's parents had told him that the old man was now definitely and finally salao, which is the worst form of unlucky, and the boy had gone at their orders in another boat which caught three good fish the first week. It made the boy sad to see the old man come in each day with his skiff empty and he always went down to help him carry either the coiled lines or the gaff and harpoon and the sail that was furled around the mast. The sail was patched with flour sacks and, furled, it looked like the flag of permanent defeat.\"Long sentences for sure, but is there any nesting in there at all?  I can't see any.\n \nreply",
      "Yes, using conjunctions to compose small sentences into larger ones is a form of recursive nesting. For example:\"[He was an old man who fished alone in a skiff in the Gulf Stream] and [he had gone eighty-four days now without taking a fish].\"\"[It made the boy sad to see the old man come in each day with his skiff empty] and [he always went down to help him carry either the coiled lines or the gaff and harpoon and the sail that was furled around the mast].\"\n \nreply",
      "The relative clause \"which is the worst form of unlucky\" is embedded, among other complexities here. Obviously this doesn't make the text less readable, so I don't think nesting per se is the problem.\n \nreply",
      "It's been so long since I've read this that I can't even remember the plot.But I remember loving it as a teenager. I must go and reread it soon. That, and For whom the bell tolls, which I can remember slightly clearer.\n \nreply"
    ],
    "link": "https://nautil.us/the-rise-and-fall-of-the-english-sentence-236880/",
    "first_paragraph": "Art+ScienceBiology + BeyondCosmosCultureEarthLifeMindOceanOne QuestionQuanta AbstractionsRewildingScience at the Ballot BoxScience Philanthropy AllianceSpark of ScienceThe Kinship IssueThe PortholeThe Reality IssueThe Rebel IssueWomen in Science & Engineering\nThe surprising forces influencing the complexity of the language we speak and write.\nThe surprising forces influencing the complexity of the language we speak and write.\u201cWhen in the course of human events it becomes necessary for one people [to dissolve the political bands [which have connected them with another]] and [to assume among the powers of the earth, the separate and equal station [to which the laws of Nature and of Nature\u2019s God entitle them]]], a decent respect to the opinions of mankind requires [that they should declare the causes [which impel them to the separation.\u201d\u2014Declaration of Independence, opening sentenceAn iconic sentence, this. But how did it ever make its way into the world? At 71 words, it is composed of ei"
  },
  {
    "title": "Dbt Labs acquires SDF Labs (getdbt.com)",
    "points": 95,
    "submitter": "karakanb",
    "submit_time": "2025-01-14T14:42:54 1736865774",
    "num_comments": 39,
    "comments_url": "https://news.ycombinator.com/item?id=42697764",
    "comments": [
      "You can tell when this deal started to come together by looking at the history of the website on Wayback Machine. In fall of 2024, the website had a checklist comparing SDF to dbt and claiming SDF had a better feature set than dbt Core (page rendering is hit and miss right now for whatever reason):\nhttps://web.archive.org/web/20240919110243/https://www.sdf.c...In December 2024 the page had been updated to now compare \"dbt Core\" against \"SDF with dbt\":\nhttps://web.archive.org/web/20241217172451/https://www.sdf.c...Little marketing switcharoo there to avoid pissing off their future owners.\n \nreply",
      "Congrats to the SDF team for their exit.Alas, dbt Labs has developed a reputation for rug pulling functionality from dbt Core and gating most of their differentiating features behind dbt Cloud. I cannot see this type of consolidation being in the best interest of the dbt community.\n \nreply",
      "dbt Labs is a Series D company with hundreds of millions in funding and a 4.2 billion USD valuation at their last round.Their CEO and founder spoke of an IPO in 2022.Let's not pretend they are still remotely close to their humble beginnings or were able to get this far without credibly demonstrating they have a plan for how to make enterprises bleed through their nose for their product.That's the future.On the flipside, building a dbt adjacent product enhancing or complementing capabilities is basically a sure way of how to get bought.\n \nreply",
      "I agree with you 100%, and we may both be correct!\n \nreply",
      "I've been on the lookout for a lighter, faster version of dbt and I was hoping sdf might be it.For our (https://www.definite.app/) use case, I'd love to have something that compiles client-side, but in general dbt just feels like a lot of work to set up for what most of our customers actually need (simple transform to create tables and views).\n \nreply",
      "A lot of work to set up?I'm quite surprised to hear that.It's literally pip install, a single file for your DB config and that's it. 30-40 seconds.I'm in no way affiliated with dbt but have worked with the tool since 2018.Lighter, faster, sure, but hard to set up?I'm not sure where you'd want to cut corners on setup.\n \nreply",
      "I think they mean setting up in production.\n \nreply",
      "Even that... the beauty of it and why it took off as much as it did is simplicity.Dockerfile, env var injection and you're done.\n \nreply",
      "I'm sure you've heard of SQLMesh but that seems like a potential fit. Or is it still too heavy handed?\n \nreply",
      "Not to mention the sudden pricing change at the end of 2022 that doubled costs for most cloud customers.\n \nreply"
    ],
    "link": "https://www.getdbt.com/blog/dbt-labs-acquires-sdf-labs",
    "first_paragraph": "Jan 14, 2025I am not generally an excitable person. I do not dance. I try to avoid hyperbole.And yet. And yet! It is very, very hard for me to avoid literally jumping up and down as I share this news with you.The TL;DR: today, I have the pleasure of announcing that dbt Labs has acquired SDF Labs. The two teams are already working side-by-side to bring SDF\u2019s SQL comprehension technology into the hands of dbt users everywhere. SDF will be a massive upgrade to the very heart of the dbt user experience moving forward. It will enable faster dbt project compilation (~2 orders of magnitude), amazing developer experience (think: type-ahead in your IDE of choice), the highest-fidelity lineage on the market, and much more.Let me take a sec to share the story of how we got here. Because I think it\u2019s an interesting one.From the very beginning, we wanted writing dbt pipelines to feel as simple as writing SQL. Just write a select statement, in your dialect of choice, and dbt would take care of all o"
  },
  {
    "title": "Rewriting my blog in plain HTML (vijayp.dev)",
    "points": 51,
    "submitter": "arnath",
    "submit_time": "2025-01-14T22:57:19 1736895439",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=42705077",
    "comments": [
      "I've been maintaining my personal website as plain HTML for five years now. I must say, I quite like this method. There's no substitute for practice when it comes to maintaining your skills at editing HTML and CSS.Yes, you must copy and paste content and not having layout page is annoying at times. But the overhead of just doing it yourself is surprisingly small in terms of the time commitment.Typically, I'll draft a post in MS Word then open the git repo for my site, hosted on github pages, duplicate and rename the template.html page that includes the CSS, footer, and header for my site and then copy my content into it. When I'm happy with everything, I'll make my commit and then a minute later it's live at my custom domain. Seeing that it takes only 11KBs and 26ms to load my landing page strangely delightful.\n \nreply",
      "> Yes, you must copy and paste contentMany people who maintain their own sites in vanilla web technologies tend to create reusable functions to handle this for them. It can generate headers and the like dynamically so you don't have to change it on every single page. Though that does kill the \"no javascript required\" aspect a lot of people likeOf course you could simply add a build step to your pure HTML site instead!\n \nreply",
      "I've adopted the idea that a blog post is archived when it's published; I don't want to tinker with it again. Old pages may have an old style, but that's OK, it's an archive. Copy/paste works great for this.The only reason I use a blog engine now (Hugo) is for RSS. I kept messing up or forgetting manual RSS edits.\n \nreply",
      "What I like to do:- have HTML files for the individual pages/posts (I like the freedom that custom HTML provides)- have a script file consisting just of a single array of meta data blocks for the individual posts (headline, description, preview image(s), date, tags, additional assets to load, like additional CSS or JS, if required, restricted visibility, etc. \u2013 most of this is optional) and content for the preview- a server-side template script that generates the chrome around an individual page view and a paginated list view from the feed data (this allows for things like pagination, cross-links, filters per tag, we can generate multiple views), and we can also generate a RRS feed from the feed-index. Moreover, as there is also no external input other than fragments from the request URI, which can be laundered easily (e.g., by discarding all non alpha-numeric characters) and checked for resolving to existing file paths in given constraints, this should be also considerably secure. (This is actually a rather short script.)- a server config (`.htaccess` or similar) that parses parts of the request URL to parameters to be used by the template script.(So, adding a post is as simple as adding a new HTML file, copying an entry in the feed file and modifying it accordingly for the new page. And it can be done all in a text editor. The only thing missing may be a full-text search, as there is no DB involved and no representation of the content as normalized plain text. On the other hand, this also keeps the server load low.)\n \nreply",
      "Did he reinvent a static-site generator? Markdown, pandoc, makefile... Sounds like a job for hugo/eleventy/jekyll/whatever.\n \nreply",
      "I don't maintain a blog so my opinion may not count for much, but I feel like if what you are trying to do doesn't fit neatly into an SSG's existing templates/themes, it may in fact be easier just to use pandoc and some simple tooling around it. Certainly when I looked into a few SSGs for the purpose of making a simple personal website (without a blog) I found I would spend more time trying to bend them to my will than just writing what I want in markdown and running pandoc on it.\n \nreply",
      "Its the the first time someone has reinvented a static site generator...Looks around sheepishly\n \nreply",
      "I started a portfolio website with Netlify (iirc), then I moved to Vue + Gridsome (on GitHub pages), then Next.js with Tailwind CSS, and was about to move to Vite.js over winter break.That's 4 stacks over the course of 5-6 years. Not worth it.Decided to do the sensible thing and use GitHub's README functionality. I prefer this approach and wish more folks in the tech community adopted it: https://github.com/SuboptimalEng\n \nreply",
      "I hate the UI layer, for this reason.  Nothing is ever stable.  I'm looking for \"Boring\" and \"Googleable in the age of AI slop\".  The other alternative are frameworks small enough to easily comprehend.The UI is often tangential to the heavy lifting done by the back end.  It often needs to be \"just good enough\".\n \nreply",
      "GitHub was just down the other day. Why would you want your personal website/portfolio to be tied to GitHub? Crazy \"modern web dev\" stacks are likely overkill, but that's not an argument against self-hosting.\n \nreply"
    ],
    "link": "https://www.vijayp.dev/blog/rewrite-plain-html/",
    "first_paragraph": ""
  },
  {
    "title": "Proof of location for online polls (ip-vote.com)",
    "points": 82,
    "submitter": "c-riq",
    "submit_time": "2025-01-14T20:31:19 1736886679",
    "num_comments": 60,
    "comments_url": "https://news.ycombinator.com/item?id=42703422",
    "comments": [
      "> Latency-based geolocation can help protect poll integrity by:> Detecting when poll responses originate from outside the intended geographic region\n> Identifying attempts to manipulate polls through elevated VPN/proxy usageUnless the user also needs to complete a reaction-time test, couldn't this be defeated by using a remote desktop connection to a machine that is physically located in the other geography?It just shifts which functions need to run on the proxy, from network routing to the browser itself.\n \nreply",
      "I think this is covered on the page\"Successfully manipulating a poll which employs this method would require following efforts and resources:Gaining control over a large number of devices in the target geographic region for submitting votes through those devices\"So yes, it seems like it can be defeated via a remote desktop (or any proxy in the allowed area)\n \nreply",
      "You don\u2019t even need to gain control over a large number of devices in the region.You just need _one_ device in the region, which can connect to the VPN or proxy service you were already using (the assumption seems to be that the attacker has a large number of IPs they can access through such a service).  That device will get some added latency from going through the VPN/proxy, but because it\u2019s physically close, the added latency will be small, probably not enough to reliably detect.\n \nreply",
      "If you're using a proxy, I don't think whether or not the source device is in the region changes anything. The only variance is in the time from where traffic exits the proxy to servers.\n \nreply",
      "> Gaining control over a large number of devices in the target geographic region for submitting votes through those devicesDoes AWS Lambda count as a machine for these purposes?  If so, you can get a nearly infinite number of them just by cycling a config param and casting another vote.\n \nreply",
      "Couldn't the \"test\" add some variety of math challenge, thus making a simple proxy insufficient. Obviously, this method would add more noise to the final calculation, but if the proxy would need to forward its data to the end-user machine to perform the math, then a simple proxy in this case wouldn't be sufficient.\n \nreply",
      "Only a small subset of the IPs has proxies on them, so it would be detectable if a disproportionate amount of traffic is coming from them.\n \nreply",
      "Yes, and also, I'd argue that anonymizing your location is a sacred feature of the internet that anytime someone builds a better mousetrap we WILL build a better mouse. The internet is not a place where requiring proof of location is welcome.For online polls, it should never be necessary, either: My rights to vote somewhere should depend only on my membership status to that somewhere, and not my current physical location.\n \nreply",
      "My state lottery app doesn\u2019t let you play outside the state. It detects screen sharing and VPN configuration and refuses to run if it sees these things.Depending on the importance of the poll, one could definitely apply these other requirements.\n \nreply",
      "That is true, the location proof is only for the hardware whose IP is used for submitting the vote request. However if remote desktop provider / cloud provider / VPN / Tor IPs are already blocked by the voting platform. Then it would require significant effort to acquire hardware in the target geographic region and equip it with a residential IP.\nGenerally the whole setup only makes sense if IP's (or IP ranges) can only vote once per poll.\nThen large scale manipulations should become impractical.\n \nreply"
    ],
    "link": "https://ip-vote.com/geolocation_via_latency.html",
    "first_paragraph": "\n        Chris Rieckmann | January 14, 2025\n    Information about a device's physical location can be inferred by measuring the time it takes for signals to travel between the device and a known server location.\n        As the speed of light cannot be exceeded according to the known laws of physics, a maximum possible distance can be established with certainty, based on the signal latency.\n        Multiple measurements to different servers establish circular areas of possible locations on the earth's surface which can then be intersected.\n    For more context on how this technology enables reliable online polls, see our article on \n        IP-based polls as a proxy for popular opinion.\n    The process relies on the physical limitations of data transmission through the internet infrastructure:Latency-based geolocation can help protect poll integrity by:Successfully manipulating a poll which employs this method would require following efforts and resources:Latency-based geolocation signi"
  },
  {
    "title": "Test if a number is even (ubuntuincident.wordpress.com)",
    "points": 19,
    "submitter": "Fake4d",
    "submit_time": "2025-01-11T21:30:04 1736631004",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=42669065",
    "comments": [
      "The interesting thing about testing values (like testing whether a number is even) is that at the assembly level, the CPU sets flags when the arithmetic happens, rather than needing a separate \"compare\" instruction.gcc likes to use `and edi,1` (logical AND between 32-bit edi register and 1). Meanwhile, clang uses `test dil,1` which is similar, except the result isn't stored back in the register, which isn't relevant in my test case (it could be relevant if you want to return an integer value based on the results of the test).After the logical AND happens, the CPU's ZF (zero) flag is set if the result is zero, and cleared if the result is not zero. You'd then use `jne` (jump if not equal) or maybe `cmovne` (conditional move - move register if not equal). Note again that there is no explicit comparison instruction. If you don't use O3, the compiler does produce an explicit `cmp` instruction, but it's redundant.Now, the question is: Which is more efficient, gcc's `and edi,1` or clang's `test dil,1`? The `dil` register was added for x64; it's the same register as `edi` but only the lower 8 bits. I figured `dil` would be more efficient for this reason, because the `1` operand is implied to be 8 bits and not 32 bits. However, `and edi,1` encodes to 3 bytes while `test dil,1` encodes to 4 bytes. I guess the `and` instruction lets you specify the bit size of the operand regardless of the register size.There is one more option, which neither compiler used: `shr edi,1` will perform a right shift on EDI, which sets the CF (carry) flag if a 1 is shifted out. That instruction only encodes to 2 bytes, so size-wise it's the most efficient.Which of the above is most efficient on CPU cycles? Who knows, there are too many layers of abstraction nowadays to have a definitive answer without benchmarking for a specific use case.I code a lot of Motorola 68000 assembly. On m68k, shifting right by 1 and performing a logical AND both take 8 CPU cycles. But the right-shift is 2 bytes smaller, because it doesn't need an extra 16 bits for the operand. That makes a difference on Amiga, because (other than size) the DMA might be shared with other chips, so you're saving yourself a memory read that could stall the CPU while it's waiting its turn. Therefore, at least on m68k, shifting right is the fastest way to test if a value is even.\n \nreply",
      "Why write redundant code? I just depend on an external is-even library, which depends on an is-odd library ;)(https://news.ycombinator.com/item?id=38791094)\n \nreply",
      "why not use ai? https://github.com/Calvin-LL/is-even-ai\n \nreply",
      "Optimizing compilers have been able to recognize pretty complicated patterns for many years.For instance if you're making a loop to count the bits that are set in a number, the compiler can recognize the entire loop and turn it into a single popcnt instruction (e.g. https://lemire.me/blog/2016/05/23/the-surprising-cleverness-... )\n \nreply",
      "They've been able to recognize this pattern since the 1960s (popcount is a very historically special case and not really a sign of complexity, since traditionally it was \"if you write this exact code from the documentation, you'll get the machine instruction\" and didn't imply any more general cleverness.)\n \nreply",
      "I feel that the compiler is doing too much work here. I know they are thinking about special cases on generated code, but at some point it feels that it just adds compile time for no good reason.Look at this --beauty-- eww, thing, should compilers really spend time trying to figure out how to optimise insane code?    def is_even(n):\n      return str(n)[len(str(n))-1] in [str(2*n) for n in range(5)]\n \nreply",
      "Maybe one day there will be compilers that can choose what to optimize based on their aesthetic judgement of the code.I could see that as a novel feedback mechanism for software engineers.As it stands, I'm glad they design optimizations abstractly, even if that means code I don't like gets the benefits\n \nreply",
      "These optimizations are very useful.  Consider the only slightly less contrived case where you want to mod an index by the size of an array.  And the compiler expands the inline function around a context where the array is a fixed power of two size at compile time.  Poof, no division/modulus needed, magically.  Lots and lots of code looks like this: general algorithms expressed in simple implementation that has a faster implementation in the specific instance that gets generated.\n \nreply",
      "I'm quite surprised that it wasn't recognized by scalar evolution, a common optimization pass to detect induction variables and their relations to other variables. Of course that requires the compiler to reason about `i % 2 == 0` or `(i & 1) == 0` first, but modern compilers do have tons of patterns recognized by that pass...\n \nreply",
      "> I think the optimizer recognizes modulo 2 and converts it to bitwise AND.A quick check in the compiler explorer (godbolt.org) confirms that this is indeed true for GCC on x86_64 and aarch64, but not for clang on the same (clang does optimize it with -O3).\n \nreply"
    ],
    "link": "https://ubuntuincident.wordpress.com/2025/01/11/test-if-a-number-is-even/",
    "first_paragraph": "ProblemYou want to test if a number is even.SolutionWell, the well-known solution is to test the value of n modulo 2. If it\u2019s zero, then the number is even, otherwise it\u2019s odd. But how fast is it?Here is a Pascal snippet:Compile and run:16 seconds! That\u2019s a lot\u2026 Let\u2019s try an alternative approach. Let\u2019s do bitwise operations. You can use the bitwise AND operator and check the least significant bit of the integer. If the least significant bit is 0, the number is even; if it\u2019s 1, the number is odd.Much better :) But what about C? Let\u2019s try it:I tried both versions (modulo 2 and bitwise AND) and got the same result. I think the optimizer recognizes modulo 2 and converts it to bitwise AND."
  },
  {
    "title": "The Missing Nvidia GPU Glossary (modal.com)",
    "points": 142,
    "submitter": "birdculture",
    "submit_time": "2025-01-12T18:22:07 1736706127",
    "num_comments": 48,
    "comments_url": "https://news.ycombinator.com/item?id=42675529",
    "comments": [
      "The weird part of the programming model is that threadblocks don't map 1:1 to warps or SMs. A single threadblock executes on a single SM, but each SM has multiple warps, and the threadblock could be the size of a single warp, or larger than the combined thread count of all warps in the SM.So, how large do you make your threadblocks to get optimal SM/warp scheduling? Well it \"depends\" based on resource usage, divergence, etc. Basically run it, profile, switch the threadblock size, profile again, etc. Repeat on every GPU/platform (if you're programming for multiple GPU platforms and not just CUDA, like games do). It's a huge pain, and very sensitive to code changes.People new to GPU programming ask me \"how big do I make the threadblock size?\" and I tell them go with 64 or 128 to start, and then profile and adjust as needed.Two articles on the AMD side of things:https://gpuopen.com/learn/occupancy-explainedhttps://gpuopen.com/learn/optimizing-gpu-occupancy-resource-...\n \nreply",
      "I was taught that you want, usually, more threads per block than each SM can execute, because SMs context switch between threads (fancy hardware multi threading!) on memory read stalls to achieve super high throughput.There are, ofc, other concerns like register pressure that could affect the calculus, but if an SM is waiting on a memory read to proceed and doesn\u2019t have any other threads available to run, you\u2019re probably leaving perf on the table (iirc).\n \nreply",
      "Pretty sure CUDA will limit your thread count to hardware constraints? You can\u2019t just request a million threads.\n \nreply",
      "You can request up to 1024-2048 threads per block depending on the gpu; each SM can execute between 32 and 128 threads at a time! So you can have a lot more threads assigned to an SM than the SM can run at once\n \nreply",
      "Thread counts per block are limited to 1024 (unless I\u2019ve missed and change and wikipedia is wrong), but total threads per kernel is 1024(2^32-1)65535*65535 ~= 2^74 threadshttps://en.wikipedia.org/wiki/Thread_block_(CUDA_programming...\n \nreply",
      "> I was taught that you want, usually, more threads per block\n> than each SM can execute, because SMs context switch between\n> threads (fancy hardware multi threading!) on memory read \n> stalls to achieve super high throughput.You were taught wrong...First, \"execution\" on an SM is a complex pipelined thing, like on a CPU core (except without branching). If you mean instruction issues, an SM can up to issue up to 4 instructions, one for each of 4 warps per cycle (on NVIDIA hardware for the last 10 years). But - there is no such thing as an SM \"context switch between threads\".Sometimes, more than 432 = 128 threads is a good idea. Sometimes, it's a bad idea. This depends on things like: Amount of shared memory used per warp* Makeup of the instructions to be executed* Register pressure, like you mentioned (because once you exceed 256 threads per block, the number of registers available per thread starts to decrease).\n \nreply",
      "Sorry if I was sloppy with my wording, instruction issuance is what I meant :)I thought that warps weren't issued instructions unless they were ready to execute (ie had all the data they needed to execute the next instruction), and that therefore it was a best practice, in most (not all) cases to have more threads per block than the SM can execute at once so that the warp scheduler can issue instructions to one warp while another waits on a memory read. Is that not true?\n \nreply",
      "100% -- there's basically no substitue for benchmarking! I find the empiricism kind of comforting, coming from a research science background.IIUC, even CuBLAS basically just uses a bunch of heuristics that are mostly derived from benchmarking to decide with kernels to use.\n \nreply",
      "Sounds like the sort of thing that would lend itself to runtime optimization.\n \nreply",
      "I'm not too informed on the details, but iirc drivers _do_ try and optimize shaders in the background, and then when ready swaps in a better version. But I doubt it does stuff like change threadgroup size, the programmer might assume a certain size and their shader would be broken if changed. Also drivers doing background work means unpredictable performance and stuttering, which developers really don't like.Someone correct me if I'm wrong, maybe drivers don't do this anymore.\n \nreply"
    ],
    "link": "https://modal.com/gpu-glossary/readme",
    "first_paragraph": "We wrote this glossary to solve a problem we ran into working with GPUs here at\nModal  : the documentation is fragmented, making it difficult to connect\nconcepts at different levels of the stack, like\nStreaming Multiprocessor Architecture  ,\nCompute Capability  , and\nnvcc compiler flags  .So we've read the\nPDFs from NVIDIA  ,\nlurked in the good Discords  , and even bought\ndead-tree textbooks  \nto put together a glossary that spans the whole stack in one place.This glossary, unlike a PDF or a Discord or a book, is a hypertext document --\nall pages are inter-linked with one another, so you can jump down to read about\nthe Warp Scheduler   so you can\nbetter understand the threads   that you\ncame across in the article on the\nCUDA programming model  .You can also read it linearly. To navigate between pages, use the arrow keys,\nthe arrows at the bottom of each page, or the table of contents (in the sidebar\non desktop or in the hamburger menu on mobile)."
  },
  {
    "title": "Show HN: Simplex: Automate browser workflows using code and natural language (simplex.sh)",
    "points": 9,
    "submitter": "marcon680",
    "submit_time": "2025-01-14T21:30:14 1736890214",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.simplex.sh/playground",
    "first_paragraph": ""
  }
]