[
  {
    "title": "Microsoft Releases Classic MS-DOS Editor for Linux Written in Rust (github.com/microsoft)",
    "points": 36,
    "submitter": "ethanpil",
    "submit_time": "2025-06-25T00:07:04 1750810024",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=44372380",
    "comments": [
      "This is just a \"because I wanted to\" project. And I get that; done a lot of those myself just to understand what the hell was going on. But the rewrite of turbo vision into FPC  and compiling to half a dozen targets has been around for 20 years. Turbo vision is probably the best text mode windowing library in existence. The cool fun kicks in when you can map a whole text screen to an array like so:\nvar\n  Screen: Array[1..80,1..25] Of Byte Absolute $B800;  // or something like that as i recallWhat turbo vision brought to the game was movable, (non) modal windows. Basically a lot of rewriting that array in a loop. Pretty snappy. I made a shitload of money with that library.reply",
      "I am curious about how you made money with it, if you don't mind sharing.reply",
      "It'd be nice if they didn't recommend winget for installation though. winget is an egregious security risk that Microsoft has just like pretended follows even minimal security practices, despite just launching four years ago with no protection from bad actors whatsoever and then never implementing any improvements since.reply",
      "So much excitement that this got posted 3 times in a week1. By the author - https://news.ycombinator.com/item?id=44034961\n2. Ubuntu Publication - https://news.ycombinator.com/item?id=44306892And this post.reply",
      "I used to recommend micro[1] to people like those in the target audience of this editor. I wonder if that should change or not.--1: https://micro-editor.github.io/reply",
      "Fun. I must admit I don't really know who this is for, but it seems fun.reply",
      "It's for people that want to use the Windows Terminal to edit files. The old `edit` command has been unsupported on Windows since 2006, so there was no Microsoft-provided editor that could be used in the command line since then.It's impressive to see how fast this editor is. https://github.com/microsoft/edit/pull/408> By writing SIMD routines specific to newline seeking, we can bump\nthat up [to 125GB/s]reply",
      "Is... this a meaningful benchmark?Who's editing files big enough to benefit from 120GBps throughput in any meaningful way on the regular using an interactive editor rather than just pushing it through a script/tool/throwing it into ETL depending on the size and nature of the data?reply",
      "As a specific benchmark, no. But that wasn't the point of linking to the PR. Although the command looks like a basic editor, it is surprisingly featureful.Fuzzy search, regular expression find & replace.I wonder how much work is going to continue going into the new command? Will it get syntax highlighting (someone has already forked it and added Python syntax highlighting: https://github.com/gurneesh9/scriptly) and language server support? :)reply",
      "Who cares? It\u2019s fun. Programming can be fun.reply"
    ],
    "link": "https://github.com/microsoft/edit",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        We all edit.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.A simple editor for simple needs.This editor pays homage to the classic MS-DOS Editor, but with a modern interface and input controls similar to VS Code. The goal is to provide an accessible editor that even users largely unfamiliar with terminals can easily use.You can also download binaries from our Releases page.You can install the latest version with WinGet:The canonical executable name is \"edit\" and the alternative name is \"msedit\".\nWe're aware of the potential conflict of \"edit\" with existing commands and recommend alternatively naming packages and executables \"msedit\".\nNames such as \"ms-edit\" should be avoided.\nAssigning an \"edit\" alias is recommended, if possible.This project optionally dep"
  },
  {
    "title": "Fun with uv and PEP 723 (cottongeeks.com)",
    "points": 324,
    "submitter": "deepakjois",
    "submit_time": "2025-06-24T18:41:26 1750790486",
    "num_comments": 110,
    "comments_url": "https://news.ycombinator.com/item?id=44369388",
    "comments": [
      "Like the author, I find myself going more for cross-platform Python one-offs and personal scripts for both work and home and ditching Go. I just wish Python typechecking weren't the shitshow it is. Looking forward to ty, pyrefly, etc. to improve the situation a bitreply",
      "Speed is one thing, the type system itself is another thing, you are basically guaranteed to hit like 5-10 issues with python's weird type system before you start grasping some of the odditiesreply",
      "finally feels like Python scripts can Just Work\u2122 without a virtualenv scavenger hunt.Now if only someone could do the same for shell scripts. Packaging, dependency management, and reproducibility in shell land are still stuck in the Stone Ages. Right now it\u2019s still curl | bash and hope for the best, or a README with 12 manual steps and three missing dependencies.Sure, there\u2019s Nix... if you\u2019ve already transcended time, space, and the Nix manual. Docker? Great, if downloading a Linux distro to run sed sounds reasonable.There\u2019s got to be a middle ground simple, declarative, and built for humans.reply",
      "I don't think nix is that hard for this particular use case. Installing nix on other distros is pretty easy, and once it's installed you just do something like this    #! /usr/bin/env nix-shell\n    #! nix-shell -i bash -p imagemagick cowsay\n\n    # scale image by 50%\n    convert \"$1\" -scale 50% \"$1.s50.jpg\" &&\n    cowsay \"done $1.q50.jpg\"\n\nSure all of nixos and packaging for nix is a challenge, but just using it for a shell script is not too badreply",
      "I simply do not write shell scripts that use or reference binaries/libraries that are no pre-installed on the target OS (which is the correct target, writing shell scripts for portability is silly).There is no package manager that is going to make a shell script I write for macOS work on Linux if that script uses commands that only exist on macOS.reply",
      "Why bother writing new shell scripts?If you're allowed to install any deps go with uv, it'll do the rest.I'm also kinda in love with https://babashka.org/ check it out if you like Clojure.reply",
      "> Packaging, dependency management, and reproducibility in shell land are still stuck in the Stone Ages.IMO it should stay that way, because any script that needs those things is way past the point where shell is a reasonable choice. Shell scripts should be small, 20 lines or so. The language just plain sucks too much to make it worth using for anything bigger.reply",
      "When you solve the dependency management issue for shell scripts, you can also use newer language features because you can ship a newer interpreter the same way you ship whatever external dependencies you have. You don't have to limit yourself to what is POSIX, etc. Depending on how you solve it, you may even be able to switch to a newer shell with a nicer language. (And doing so may solve it for you; since PowerShell, newer shells often come with a dependency management layer.)> any script that needs those thingsIt's not really a matter of needing those things, necessarily. Once you have them, you're welcome to write scripts in a cleaner, more convenient way. For instance, all of my shell scripts used by colleagues at work just use GNU coreutils regardless of what platform they're on. Instead of worrying about differences in how sed behaves with certain flags, on different platforms, I simply write everything for GNU sed and it Just Works\u2122. Do those scripts need such a thing? Not necessarily. Is it nicer to write free of constraints like that? Yes!Same thing for just choosing commands with nicer interfaces, or more unified syntax... Use p7zip for handling all your archives so there's only one interface to think about. Make heavy use of `jq` (a great language) for dealing with structured data. Don't worry about reading input from a file and then writing back to it in the same pipeline; just throw in `sponge` from moreutils.> The language just plain sucks too muchThere really isn't anything better for invoking external programs. Everything else is way clunkier. Maybe that's okay, but when I've rewritten large-ish shell scripts in other languages, I often found myself annoyed with the new language. What used to be a 20-line shell script can easily end up being 400 lines in a \"real\" language.I kind of agree with you, of course. POSIX-ish shells have too much syntax and at the same time not enough power. But what I really want is a better shell language, not to use some interpreted non-shell language in their place.reply",
      "Broke: Dependency management used for shell scriptsWoke: Dependency management used for installing an interpreter for a better programming language to write your script in itBespoke: Dependency management used for installing your scriptreply",
      "Check out mise: https://mise.jdx.dev/We use it at $work to manage dev envs and its much easier than Docker and Nix.It also installs things in parallel, which is a huge bonus over plain Dockerfilesreply"
    ],
    "link": "https://www.cottongeeks.com/articles/2025-06-24-fun-with-uv-and-pep-723",
    "first_paragraph": "For the longest time, I have been frustrated with Python because I couldn\u2019t use it for one-off scripts. I had to first ensure it was running in an environment where it could find the right Python version and the dependencies installed. That is now a thing of the past.If you are not a Pythonista (or one possibly living under a rock), uv is an extremely fast Python package and project manager, written in Rust.uv also provides this nifty tool called uvx (kinda like npx from the Node/NPM ecosystem for Javascript/Typescript packages) which can be used to invoke a Python tool inside a package. uvx takes care of creating a (cached) disposable virtual environment, setting up the right Python version and installing all the dependencies before running.For examplePEP 723 is a Python Enhancement Proposal that specifies a metadata format that can be embedded in single-file Python scripts to assist launchers, IDEs and other external tools which may need to interact with such scripts.Here is the exam"
  },
  {
    "title": "Writing toy software is a joy (jsbarretto.com)",
    "points": 507,
    "submitter": "bundie",
    "submit_time": "2025-06-24T15:09:12 1750777752",
    "num_comments": 212,
    "comments_url": "https://news.ycombinator.com/item?id=44367084",
    "comments": [
      "> Perhaps you\u2019re a user of LLMs. I get it, they\u2019re neat tools. They\u2019re useful for certain kinds of learning. But I might suggest resisting the temptation to use them for projects like this. Knowledge is not supposed to be fed to you on a plateAm I the only one using LLMs as if they were a search engine? So before LLMs I was searching on Google things like \"pros cons mysql mongodb\". I would read the official documentation of each db, forums, blog posts, stackoverflow entries, etc. It was time consuming on the searching side. The time it took to read all the sources was fine for me (it's learning time, so that's always welcomed). Now with LLMs, I simply prompt the same with a little bit more of context \"pros and cons of using mysql vs mongodb when storing photos. Link references\". So, I get a quick overview of what to keep an eye on, and the references are there to avoid relying on hallucination.It's true that sometimes I go ahead and say \"give me a data schema for storing photos metadata in postgres. I wanna keep X in a different table, though\" (or something like that). But I do that because I know very well what the output should look like (I just don't wanna spend time typing it, and sometimes I forget the actual type that I should use (int vs integer?)).reply",
      "The few times I've used LLMs as question answering engines for anything moderately technical, they've given subtly-but-in-important-ways incorrect information such that taking them at face value would've likely lost me hours or days of pursuing something unworkable, even when I ask for references. Whether or not the \"references\" actually contain the information I'm asking for or merely something tangentially related has been rather hit or miss too.The one thing they've consistently nailed has been tip-of-my-tongue style \"reverse search\" where I can describe a concept in sufficient detail that they can tell me the search term to look it up with.reply",
      "Yes, you have to be very careful when querying LLM's, you have to assume that they are giving you sort of the average answer to a question. I find them very good at sort of telling me how people commonly solve a problem. I'm lucky, in that the space I've been working has had a lot of good forums training data, and the average solution tends to be on the more correct side. But you still have to validate nearly everything it tells you. It's also funny to watch the tokenization \"fails\". When you ask about things like register names, and you can see it choose nonexisting tokens. Atmel libraries have a lot of things like this in them#define PA17_EIC_LINE     PIN_PA17A_EIC_EXTINT_NUM\n#define PA17_EIC_BIT      PORT_PA17A_EIC_EXTINT1\n#define PA17_PMUX_INDEX   8  //pa17   17/2\n#define PA17_PMUX_TYPE    MUX_PA17A_EIC_EXTINT1And the output will be almost correct code, but instead of an answer being:PORT_PA17A_EIC_EXTINT1you'll get:PORT_PA17A_EIC_EXTINT_NUMand you can tell that it diverged trying to use similar tokens, and since _ follows EXTINT sometimes, it's a \"valid\" token to try, and now that it's EXTINT_ now NUM is the most likely thing to follow.That said, it's massively sped up the project I'm working on, especially since Microchip effectively shut down the forums that chatgpt was trained on.reply",
      "Absolutely. And I\u2019m finding the same with \u201cagent\u201d coding tools. With  the ever increasing hype around Cursor I tried to give it a go this week. The first 5 minutes were impressive, when I sent a small trial ballon for a simple change.But when asking for a full feature, I lost a full day trying to get it to stop chasing its tail. I\u2019m still in the \u201cpro\u201d free trial period so it was using a frontier model.This was for a Phoenix / Elixir project; which I realize is not as robustly in the training data as other languages and frameworks, but it was supposedly consuming the documentation, other reference code I\u2019d linked in, and I\u2019d connected the Tidewave MCP.Regardless, in the morning with fresh eyes and a fresh cup of coffee, I reverted all the cursor changes and implemented the code myself in a couple hours.reply",
      ">The one thing they've consistently nailed has been tip-of-my-tongue style \"reverse search\" where I can describe a concept in sufficient detail that they can tell me the search term to look it up with.This is basically the only thing I use it for. It's great at it, especially given that Google is so terrible these days that a search describing what you're trying to recall gets nothing. Especially if it involves a phrase heavily associated with other things.For example \"What episode of <X show> did <Y thing> happen?\" In the past, Google would usually pull it up (often from reddit discussion), but now it just shows me tons of generic results about the show.reply",
      "> Now with LLMs, I simply prompt the same with a little bit more of context \"pros and cons of using mysql vs mongodb when storing photos. Link references\".In near future, companies will probably be able to pay lots of money to have their products come up better in the comparison. LLMs are smart enough to make the result seem \"organic\" -- all verifiable information will be true and supported by references, it will only be about proper framing and emphasis, etc.reply",
      "This was already a problem in a world without LLMs. Reputation is the only human mechanism that mitigates this.reply",
      "Technology is a catalyser. It has a knack of turning things from \"already a problem\" to \"now a catastrophe\".reply",
      "It's seriously exacerbated by smug LLMs.reply",
      "I'd say LLMs have helped a lot with this problem actually, by somehow circumventing a lot of the decades of SEO that has now built up. But, I fear it will be short-lived until people figure out LLM optimisation.reply"
    ],
    "link": "https://blog.jsbarretto.com/post/software-is-joy",
    "first_paragraph": "Why you should write more toy programsI am a huge fan of Richard Feyman\u201a\u00c4\u00f4s famous quote:\u201a\u00c4\u00faWhat I cannot create, I do not understand\u201a\u00c4\u00f9I think it\u201a\u00c4\u00f4s brilliant, and it remains true across many fields (if you\u201a\u00c4\u00f4re willing to be a little creative with the\ndefinition of \u201a\u00c4\u00f2create\u201a\u00c4\u00f4). It is to this principle that I believe I owe everything I\u201a\u00c4\u00f4m truly good at. Some will tell you\nto avoid reinventing the wheel, but they\u201a\u00c4\u00f4re wrong: you should build your own wheel, because it\u201a\u00c4\u00f4ll teach you more about\nhow they work than reading a thousand books on them ever will.In 2025, the beauty and craft of writing software is being eroded. AI is threatening to replace us (or, at least, the\nmost joyful aspects of our craft) and software development is being increasingly commodified, measured, packaged, and\nindustrialised. Software development needs more simple joy, and I\u201a\u00c4\u00f4ve found that creating toy programs is a great way to\nremember why I started working with computers again.Toy programs follow the 8"
  },
  {
    "title": "ChatGPT's enterprise success against Copilot fuels OpenAI/Microsoft rivalry (bloomberg.com)",
    "points": 155,
    "submitter": "mastermaq",
    "submit_time": "2025-06-24T16:02:02 1750780922",
    "num_comments": 138,
    "comments_url": "https://news.ycombinator.com/item?id=44367638",
    "comments": [
      "https://archive.ph/cemKI",
      "My work gives us Copilot access since We're a Microsoft Shop\u2122It's too bad Copilot is by far the dumbest competitor in the spaceMy favorite interaction so far was when I prompted it with:    ffmpeg command to convert movie.mov into a reasonably sized mp4\n\nSure, it's not the most direction instructions, but I tend to give it just enough to get the job done, assuming the LLM knows what its purpose is as an LLM, and it always works with the other chatbots.Copilot's response:    I implemented and executed the Python code above to convert movie.mov to a reasonably sized movie.mp4 using ffmpeg.\n    \n    However, the Python code failed since it was not able to find and access movie.mov file.\n    Do you want me to try again or is there anything else that I can help you with?\n\nNote that I didn't cut anything out. It didn't actually provide me any \"Python code above\"reply",
      "I pasted your prompt:  ffmpeg command to convert movie.mov into a reasonably sized mp4\n\ninto the Copilot app just now.  Here's the response:  ffmpeg -i movie.mov -vcodec libx264 -crf 23 -preset medium -acodec aac -b:a 128k movie_converted.mp4\n\nwith this explanation  Here\u2019s the breakdown:\n  - -vcodec libx264: Uses the H.264 video codec, great for compression and compatibility.\n  - -crf 23: Constant Rate Factor \u2014 lower means better quality and larger size; higher means smaller size but lower quality. 23 is a nice middle ground.\n  - -preset medium: Controls encoding speed vs. compression efficiency. You can try slow or faster depending on your needs.\n  - -acodec aac -b:a 128k: Compresses the audio using AAC at 128 kbps \u2014 pretty standard and compact.\n  If you're looking for even smaller file sizes, try bumping -crf up to 26 or using -preset slow.\n  Want to bake in scaling, remove metadata, or trim the video too? Happy to tailor the command further.reply",
      "AI probably hates him so it acts dumb.reply",
      "One fascinating aspect of LLMs is they make out-in-the-wild anecdotes instantly reproducible or, alternatively, comparable to results from others with different outcomes.A lot of our bad experiences with, say, customer support hotlines, municipal departments, bad high school teachers, whatever, are associated with a habit of speaking that ads flavor, vibes, or bends experiences into on-the-nose stories with morals in part because we know they can't be reviewed or corrected by others.Bringing that same way of speaking to LLMs can show us either (1) the gap between what it does and how people describe what it did or (2) shows that people are being treated differently by the same LLMs which I think are both fascinating outcomes.reply",
      "LLMs are definitely not instantly reproducible. The temperature setting adjust randomness and the models are frequently optimized and fine tuned. You will very different results depending on what you have in your context. And with a tool like Microsoft copilot, you have no idea what is in the context. There are also bugs in the tools that wrap the LLM.Just because other people on here say \u201cworked for me\u201d doesn\u2019t invalidate OPs claim. I have had similar times where an LLM will tell me \u201chere is a script that does X\u201d and there is no script to be found.reply",
      "We're also seeing a new variant of Cunningham's law:The best way to get the right answer from an LLM is not to ask it the right question; it's to post online that it got the wrong answer.reply",
      "This is hilarious because both Gemini and ChatGPT are shockingly good at putting together FFMPEG commands. They can both put together and also understand the various options and stages/filters.reply",
      "I really like the final remark, \"or is there anything else that I can help you with\"?Yeah, like how about answering the fucking question? lolreply",
      "I cannot reproduce this in any version of copilot?Copilot with outlook.comCopilot base one that comes with M365,And the add-on one for 30$/mo.Copilot in VS codeAll produce: ffmpeg -i movie.mov -vcodec libx264 -crf 23 -preset medium -acodec aac -b:a 128k output.mp4Which is not surprising because its just an Open AI 4o call... so how are you getting this?reply"
    ],
    "link": "https://www.bloomberg.com/news/articles/2025-06-24/chatgpt-vs-copilot-inside-the-openai-and-microsoft-rivalry",
    "first_paragraph": ""
  },
  {
    "title": "Managing Time When Time Doesn't Exist (multiverseemployeehandbook.com)",
    "points": 10,
    "submitter": "TMEHpodcast",
    "submit_time": "2025-06-25T00:28:12 1750811292",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://multiverseemployeehandbook.com/blog/temporal-resources-managing-time-when-time-doesnt-exist/",
    "first_paragraph": ""
  },
  {
    "title": "Mid-sized cities outperform major metros at turning economic growth into patents (governance.fyi)",
    "points": 21,
    "submitter": "guardianbob",
    "submit_time": "2025-06-24T23:26:19 1750807579",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=44372162",
    "comments": [
      "Was there an expectation that economic busts drove innovation? Am I  naive to think booms not busts would be the engine for innovation?reply",
      "It seems obvious to me, too, when considered in terms of available capital providing means for experimenting with unproven ideas.But I think the opposite may legitimately be the intuition for people of generations before ZIRP and the notion of inventiveness when resources are scarce or you're simply out of work.  You can see it in the themes of many movies and other media in the 80s and early 90s, too, the hero arc of the inventor down on their luck.  I think the idea carried forward for people in their formative years then.reply",
      "This analysis is about the oil industry,  what's true about the oil industry may or not be true about some other industry.reply",
      "The narrative I've heard is that it's all about the people.   During booms, the smart people are employed and treated well, so stay stuck in the corporate grind.   Then their employer goes bankrupt and start their own company.reply",
      "A big reason that people say intrest rate hikes or cost cutting is good because it makes people \"work smarter with less\" or some other BS or have the economy focus on what mattersreply",
      "\"The beatings will continue until morale improves\", as they say.reply",
      "Bordain first book have a great argument on bust driving culinary Innovation.reply",
      "Sure. War and hardship drives innovation.reply",
      "I don't know, this just seems like a sectoral analysis among o&g? We have pretty strong evidence that density leads to higher productivity, I'd be keen to see it replicated more broadly (one such study of many: https://www.newyorkfed.org/medialibrary/media/research/staff...).reply",
      "It uses Oil & Gas shocks, rather than a sectoral analysis among o&g. I think it is because big firms are in big cities, who seem to be able to handle shocks better than other companies in other citiesreply"
    ],
    "link": "https://www.governance.fyi/p/booms-not-busts-drives-innovation",
    "first_paragraph": ""
  },
  {
    "title": "Ancient X11 scaling technology (tedunangst.com)",
    "points": 169,
    "submitter": "todsacerdoti",
    "submit_time": "2025-06-24T18:58:32 1750791512",
    "num_comments": 131,
    "comments_url": "https://news.ycombinator.com/item?id=44369646",
    "comments": [
      "That's probably better than most scaling done on Wayland today because it's doing the rendering directly at the target resolution instead of doing the \"draw at 2x scale and then scale down\" dance that was popularized by OSX and copied by Linux. If you do it that way you both lose performance and get blurry output. The only corner case a compositor needs to cover is when a client is straddling two outputs. And even in that case you can render at the higher size and get perfect output in one output and the same downside in blurryness in the other, so it's still strictly better.It's strange that Wayland didn't do it this way from the start given its philosophy of delegating most things to the clients. All you really need to do arbitrary scaling is tell apps \"you're rendering to a MxN pixel buffer and as a hint the scaling factor of the output you'll be composited to is X.Y\". After that the client can handle events in real coordinates and scale in the best way possible for its particular context. For a browser, PDF viewer or image processing app that can render at arbitrary resolutions not being able to do that is very frustrating if you want good quality and performance. Hopefully we'll be finally getting that in Wayland now.reply",
      "> doing the \"draw at 2x scale and then scale down\" dance that was popularized by OSXOriginally OS X defaulted to drawing at 2x scale without any scaling down because the hardware was designed to have the right number of pixels for 2x scale. The earliest retina MacBook Pro in 2012 for example was 2x in both width and height of the earlier non-retina MacBook Pro.Eventually I guess the cost of the hardware made this too hard. I mean for example how many different SKUs are there for 27-inch 5K LCD panels versus 27-inch 4K ones?But before Apple committed to integer scaling factors and then scaling down, it experimented with more traditional approaches. You can see this in earlier OS X releases such as Tiger or Leopard. The thing is, it probably took too much effort for even Apple itself to implement in its first-party apps so Apple knew there would be low adoption among third party apps. Take a look at this HiDPI rendering example in Leopard: https://cdn.arstechnica.net/wp-content/uploads/archive/revie... It was Apple's own TextEdit app and it was buggy. They did have a nice UI to change the scaling factor to be non-integral: https://superuser.com/a/13675reply",
      "> Originally OS X defaulted to drawing at 2x scale without any scaling down because the hardware was designed to have the right number of pixels for 2x scale.That's an interesting related discussion. The idea that there is a physically correct 2x scale and fractional scaling is a tradeoff is not necessarily correct. First because different users will want to place the same monitor at different distances from their eyes, or have different eyesight, or a myriad other differences. So the ideal scaling factor for the same physical device depends on the user and the setup. But more importantly because having integer scaling be sharp and snapped to pixels and fractional scaling a tradeoff is mostly a software limitation. GUI toolkits can still place all ther UI at pixel boundaries even if you give them a target scaling of 1.785. They do need extra logic to do that and most can't. But in a weird twist of destiny the most used app these days is the browser and the rendering engines are designed to output at arbitrary factors natively and in most cases can't because the windowing system forces these extra transforms on them. 3D engines are another example, where they can output whatever arbitrary resolution is needed but aren't allowed to. Most games can probably get around that in some kind of fullscreen mode that bypasses the scaling.I think we've mostly ignored these issues because computers are so fast and monitors have gotten so high resolution that the significant performance penalty (2x easily) and introduced blurryness mostly goes unnoticed.> Take a look at this HiDPI rendering example in LeopardThat's a really cool example, thanks. At one point Ubuntu's Unity had a fake fractional scaling slider that just used integer scaling plus font size changes for the intermediate levels. That mostly works very well from the point of view of the user. Because of the current limitations in Wayland I mostly do that still manually. It works great for single monitor and can work for multiple monitors if the scaling factors work out because the font scaling is universal and not per output.reply",
      "What you want is exactly how fractional scaling works (on Wayland) in KDE Plasma and other well-behaved Wayland software: The scale factor can be something quirky like your 1.785, and the GUI code will generally make sure that things nevertheless snap to the pixel grid to avoid blurry results, as close to the requested scaling as possible. No \"extra window system transforms\".reply",
      ">  The scale factor can be something quirky like your 1.785, and the GUI code will generally make sure that things nevertheless snap to the pixel grid to avoid blurry resultsThis is horrifying!  It implies that, for some scaling factors, the lines of text of your terminal will be of different height.Not that the alternative (pretend that characters can be placed at arbitrary sub-pixel positions) is any less horrifying.  This would make all the lines in your terminal of the same height, alright, but then the same character at different lines would look different.The bitter truth is that fractional scaling is impossible.  You cannot simply scale images without blurring them.  Think about an alternating pattern of white and black rows of pixels.  If you try to scale it to a non-integer factor the result will be either blurry or aliased.The good news is that fractional scaling is unnecessary.\nYou can just use fonts of any size you want.  Moreover, nowadays pixels are so small that you can simply use large bitmap fonts and they'll look sharp, clean and beautiful.reply",
      "> The bitter truth is that fractional scaling is impossible.That's overly prescriptive in terms of what users want. In my experience users who are used to macOS don't mind slightly blurred text. And users who are traditionalists and perhaps Windows users prefer crisper text at the expense of some height mismatches. It's all very subjective.reply",
      "The way it works for your terminal emulator example is that it figures out what makes sense to do for a value of 1.785, e.g. rasterizing text appropriately and making sure that line heights and baselines are at sensible consistent values.reply",
      "the problem is that there's no reasonable thing to do when the height of the terminal in pixels is not an integer multiple of the height of the font in pixels.  Whatever \"it\" does, will be wrong.(And when it's an integer multiple, you don't need scaling at all.  You just need a font of that exact size.)reply",
      "You're overthinking things a bit and are also a bit confused about how font sizes work and what \"scaling\" means in a windowing system context. You are thinking taking a bunch of pixels and resampling. In the context we're talking about \"scaling\" means telling the software what it's expected to output and giving it an opportunity to render accordingly.The way the terminal handles the (literal) edge case you mention is no different from any other time its window size is not a multiple of the line height: It shows empty rows of pixels at the top or bottom.Fonts are only a \"exact size\" if they're bitmap-based (and when you scale bitmap fonts you are indeed in for sampling difficulties). More typical is to have a font storing vectors and rasterizing glyphs to to the needed size at runtime.reply",
      "Given that the context here is talking about terminals, they probably are literally thinking in terms of bitmap based rendering with integer scaling.reply"
    ],
    "link": "https://flak.tedunangst.com/post/forbidden-secrets-of-ancient-X11-scaling-technology-revealed",
    "first_paragraph": "People keep telling me that X11 doesn\u2019t support DPI scaling, or fractional scaling, or multiple monitors, or something. There\u2019s nothing you can do to make it work. I find this surprising. Why doesn\u2019t it work? I figure the best way to find out is try the impossible and see how far we get.I\u2019m just going to draw a two inch circle on the screen. This screen, that screen, any screen, the circle should always be two inches. Perhaps not the most exciting task, but I figure it\u2019s isomorphic to any other scaling challenge. Just imagine it\u2019s the letter o or a button we wish to draw at a certain size.I have gathered around me a few screens of different sizes and resolutions. My laptop screen, and then a bit to the right a desktop monitor, and then somewhere over that way a nice big TV. Specifically:$ xrandr | grep \\ connected\neDP connected primary 2880x1800+0+0 (normal left inverted right x axis y axis) 302mm x 189mm\nDisplayPort-0 connected 2560x1440+2880+0 (normal left inverted right x axis y axi"
  },
  {
    "title": "PlasticList \u2013 Plastic Levels in Foods (plasticlist.org)",
    "points": 291,
    "submitter": "homebrewer",
    "submit_time": "2025-06-24T14:18:11 1750774691",
    "num_comments": 134,
    "comments_url": "https://news.ycombinator.com/item?id=44366548",
    "comments": [
      "One class of items not listed here, which I'd recently started to think might be less-than-optimal:  pepper sold in jars with built-in, plastic, grinders.I'd long since noted that as the jar emptied the grinders were increasingly ineffective.  Thinking on why that might be ... I realised that this was because as you grind the pepper, you're also grinding plastic directly into your food.There's surprisingly little discussion about this that I can find, though this 5 y.o. Stackexchange question addresses the concern:<https://cooking.stackexchange.com/questions/103003/microplas...>Seems to me that plastic grinders, whether disposable or sold as (apparently) durable products, are a class of products  which simply shouldn't exist.Searching, e.g., Walmart for \"plastic grinders\" turns up five listings presently, though it's not clear whether it's the body or the grinder itself which is plastic.  In several cases it seems to be the latter.<https://www.walmart.com/c/kp/plastic-grinders>(Archive of current state:  <https://archive.is/yIIX4>reply",
      "Peugeot\u2014yes, they of the cars\u2014make an excellent line of steel-based pepper grinders, and a great nutmeg mill as well. Along with hoop skirts and lawnmowers and much more, apparently, over the 200 years since the family started their first steel mill:https://us.peugeot-saveurs.com/en_us/inspiration/history/The car business sold to Stellantis, but the lineage\u2019s kaleidoscope of other enterprises apparently continues.reply",
      "definitely one of those buy it for life type of thing, very satisfying once you get used to it, and it does take time to get used to it. the labelling is carefully designed to fade away just around the time you got to know how to use one, masterfully done lolreply",
      "The mechanism is probably good forever but the bottom ring is liable to crack over time. They usually last me a decade or so which is fine for the price.reply",
      "I love them and have bought half a dozen over the years. My dad gifted me one when I moved out decades ago.A good pepper grinder (and the Peugeot\u2019s are top notch) is such an obviously valuable purchase. Lasts a decade and fresh pepper from a good grinder is much tastier. One of the best $35 to spend imoreply",
      "meh.  I like the aesthetics of the Peugeot grinder but it is flawed.Specifically:  the grinder top is not mated with reverse threads.  This means the act of grinding loosens the top.  I have to stop and re-tighten quite frequently.I suppose the design is perfect if you are left-handed ...reply",
      "If it were the other way it would tighten with use and eventually strip the threads or crack the wood top. Anyway it's an almost unbelievably petty bit of cooking technique but there is actually a \"correct\" way to hold and turn a pepper grinder lol.Your palm is meant to hold the nut in place. On the old ones the tightness of the nut was the control for fineness so it was necessary to hold it as you turned anyway. They moved that control to its own thing on the bottom a few decades ago (iirc) but kept the rest of it the same.reply",
      "Thanks, I hadn\u2019t considered the plastic on the pepper grinder. Guess I\u2019ll be looking for a new pepper grinder as I continue my pursuit of removing plastic and dangerous chemicals from the kitchen. So far the pans, tupperware, and cooking utensils have all been replaced.While not food, another not so frequently talked about plastic exposure could be clothing dryer vents pushing materials from synthetic clothing into the air. It\u2019s likely less of a problem than the rubber tires on our cars making their way into the air. But it was something that occurred to me while cleaning out the dryer vent this past weekend.reply",
      "I\u2019m definitely buying natural fiber clothing moving forward for this reason.However, I wonder how bad eating bits of the plastic burr grinder actually is.  Presumably, they mostly pass through.  Stomach acid probably leaches a bunch of stuff, but is it worse than (say) canned tomatoes that were sitting in a plastic liner for a year?  I\u2019d wager the grinder bits have a lot of surface area from scarring.  That\u2019d increase leaching.Anyway, I strongly recommend small turkish-style grinders:https://bazaaranatolia.com/products/turkish-grinder-pepper-m...(No idea if this brand is decent; the form factor is great, especially for $14)It has roughly a single-recipe capacity, so I stick crushed red pepper flakes, cumin seed, celery seed, black pepper kernels, etc in it per the recipe, then grind until it is empty.  The burr on the one I linked is metal.I\u2019d probably prefer stainless body  + whatever is commonly used for espresso grinders, assuming such a gadget exists.reply",
      "> (No idea if this brand is decent; the form factor is great, especially for $14)> These grinders are made of Zamak (brass and zinc)If it's real brand-name ZAMAK, then it should at least be low in lead :)reply"
    ],
    "link": "https://www.plasticlist.org/",
    "first_paragraph": ""
  },
  {
    "title": "Finding a 27-year-old easter egg in the Power Mac G3 ROM (downtowndougbrown.com)",
    "points": 305,
    "submitter": "zdw",
    "submit_time": "2025-06-24T13:06:45 1750770405",
    "num_comments": 87,
    "comments_url": "https://news.ycombinator.com/item?id=44365806",
    "comments": [
      "These Easter eggs really give an \u201cearly desktop PC era\u201d vibe to it all. It\u2019s very human and connects you to the fact that you\u2019re using something that people with faces and names made. Back when these were passion projects by a bunch of hardcore nerds.But they\u2019d rather you not really see through the product abstraction layer anymore. The Product People want to control the full image of the product and it\u2019s just safest to de-humanize it in case that list is too big or people on that list become undesirables or whatnot.I\u2019m thinking about what this might look like today. Maybe a neat Easter egg in my iPhone that every time I activate it, it shows me a few people at random who played a role in development.  I\u2019d love it, but I imagine this would offend the high tastes of the Product People.reply",
      "I wonder too if there was more of this before Agile. With deadline driven development you can run into situations where part of the team is stuck waiting for their teammates to finish something so they can surpass a milestone. You can only poke at the backlog so much. Boredom and being able to rationalize that you aren't really affecting the roadmap by sneaking a little extra something in makes for a lot more 'motive and opportunity' situations.reply",
      "Today some auditor (like me) would fail your ITGCs because of the undocumented partition/file/change/etc (take your pick) and force you to submit a deviation to the SOC team, ask you to \"review and update the Secure Design Document to reflect to the change\", ask you to create a Jira and/or ServiceNow ticket, etc. etc. etc.Oh, and you would get a red mark on your \"HR P&D record\" for the 'Secure Software Policy' violation.(Shit.. I hated myself writing the above, but it's true)In 2001 though, we would all laugh if we would have 'caught' the devs doing something cool like this!reply",
      "Gross.I hope we do shrink software companies down to the mythical \"1-person unicorns\" so we can be done with this madness.I prefer the taste of small creators to the consensus of product orgs and their politicking. And whatever design refreshes we are faced with when the designers declare a new design language.reply",
      "Yeah, the federal government used to pay extra for versions of Win/9x with the easter eggs taken out.reply",
      "I don't know what your odd issue with product people is but this has absolutely nothing to do with Product (management). \nSoftware used to be done by a handful of people. Now there are thousands involved across an organisation. For better or worth that's how it is. An Easter Egg highlighting just a few people just doesn't make sense for a large software project nowadaysreply",
      "> Now there are thousands involved across an organisation\u2026 An Easter Egg highlighting just a few people just doesn't make senseI don\u2019t know if the message was edited, but GP addressed this with \u201cMaybe\u2026 it shows me a few people at random who played a role in development.\u201d Anyway, you could also show thousands of names/faces rapidly but still meaningfully, or let the user explore them slowly. Feels like the other responses are more accurate than it simply being about the quantity of people.reply",
      "It\u2019s more to do with Quality Control than Product Managementreply",
      "Yea, Quality Control and Risk Management. You really don\u2019t want even the slightest risk of messing up the build or the product just so that you can bury some secret treasure in the code! We\u2019ve all at some point been responsible for a big goof-up in code that we believed to be harmless.reply",
      "Yeah but you write the easter egg in one product cycle and you put it in the code at the beginning of the next, so it has all the time in the world to 'bake'.reply"
    ],
    "link": "https://www.downtowndougbrown.com/2025/06/finding-a-27-year-old-easter-egg-in-the-power-mac-g3-rom/",
    "first_paragraph": "I was recently poking around inside the original Power Macintosh G3\u2019s ROM and accidentally discovered an easter egg that nobody has documented until now.This story starts with me on a lazy Sunday using Hex Fiend in conjunction with Eric Harmon\u2019s Mac ROM template (ROM Fiend) to look through the resources stored in the Power Mac G3\u2019s ROM. This ROM was used in the beige desktop, minitower, and all-in-one G3 models from 1997 through 1999.As I write this post in mid-2025, I\u2019m having a really difficult time accepting the fact that the Power Mac G3 is now over 27 years old. Wow!While I was browsing through the ROM, two things caught my eye:First, there was a resource of type HPOE which contained a JPEG image of a bunch of people, presumably people who worked on these Mac models.This wasn\u2019t anything new; Pierre Dandumont wrote about it back in 2014. However, in his post, he mentioned that he hadn\u2019t figured out how to display this particular hidden image on the actual machine. Several older Mac"
  },
  {
    "title": "National Archives at College Park, MD, will become a restricted federal facility (archives.gov)",
    "points": 246,
    "submitter": "LastTrain",
    "submit_time": "2025-06-24T21:18:45 1750799925",
    "num_comments": 73,
    "comments_url": "https://news.ycombinator.com/item?id=44371169",
    "comments": [
      "From the site:> All researchers must apply and present a researcher card, which may be obtained in Room 1000. This ensures that proper identification is on file for all individuals accessing the building to establish a legitimate business purpose. Abuse of any researcher registration to circumvent access by the general public may result in a trespass situation and a permanent ban from access to all NARA facilities.What the hell does \"legitimate business purpose\" mean? What \"business\" need is there for JFK Assassination records (which I think are at this site), for example?  If I'm getting a PHD or writing a book, is that a \"business\" need? I suspect not.Also, \"Abuse of any researcher registration to circumvent access by the general public may result in a trespass situation and a permanent ban from access to all NARA facilities\" seems like a very poorly constructed sentence.reply",
      "I've been to NARA 2 in College Park several times.  I'm reading this as meaning that only researchers who will request records can enter the building now.  The statement seems to be clumsily worded.It suffices to say that it would be hard to justify closing down NARA 2 for researcher access.  Room 2000 is the main reading room and it is one of the largest reading rooms I have ever been in.  The building was built for people to come and visit and do research.NARA 2 is a high security facility as it is.  The last time that I visited was in 2019.  You are searched one time upon entering the building.  You (as a researcher) enter and go down to a large basement locker room where you can place most of your items in a locker.  You can take a laptop and a scanner/camera to the first floor, get searched another time, then go up an elevator to the Room 2000, get searched again, and then take a seat and request materials (using triplicate forms, the last time I was there).  You are searched again upon leaving the reading room.Based on my experience, it sounds like they are going to remove one of the searches and put it at the entrance rather than at the elevators for the second floor, though I admit this is speculation.The more difficult aspect would be having no parking access at the facility itself and having to take a bus there.  I've taken the Metrobus to NARA 2 before and it was quite complicated the last time I went there, and I generally like public transportation.  Every time I visited after that, I drove and parked in the garage, usually on the roof.  That said, I can learn to manage the bus.reply",
      "My partner works for NARA, but not in this office. Outside of the large amount of departures and RIF actions taken for the agency, there's lots of challenges regarding staffing for people who come in off the street and do not have succinct, coherent research questions. Staff are duty-bound to respond to all queries, regardless of how good they are.I imagine this research card policy does two things:1. Raises an easy bureaucratic barrier for people who just drop in and expect/demand help2. Gives staff an opportunity to refuse access to people who may have non-research intent from accessing the buildingIt's likely the example you provided qualifies as a business need. They just don't want you hanging around and getting in the way of them helping people who scheduled a consultation, have an appointment, etc.Totally agree on the poorly-constructed sentence. I wish they had said it more succinctly/precisely.reply",
      "I believe you are interpreting \u201cbusiness need\u201d as \u201ccommercial need\u201d when I think it\u2019s more like \u201cwhat is your business here?\u201d Purely anecdotal, but when I visited Moffett Federal Airfield to visit the aviation history museum there I asked the security guard at the gate checking my ID if I could ride my bike around the base afterwards. He said I needed a business purpose to being on the base and that visiting the museum was a business purpose but biking around aimlessly wasn\u2019t.reply",
      "The actual process of getting a research card does not mention any business need. It just asks to show ID and watch a training video:https://www.archives.gov/research/start/researcher-card. It specifically mentions student IDs.But maybe that page is not updated yet with new policy.reply",
      "It means if you are a crazy person, you can no longer waltz in off your motorcycle and demand all documents related to alien spacecraft held at Area 51 or the real unedited Zapruder film that clearly shows Walt Disney was the triggerman, etc.My guess is anyone could still pursue whatever crazy theories they wanted, so long as they conducted their research legitimately, i.e., as a legitimate _process_ of research, with no value judgment on the topic or end goal.reply",
      "How do you estimate they will judge process legitimacy?reply",
      "Looks like NARA are underfunded [1] and trying to grapple with how to prioritize digital transformation while still meeting KTLO demands. They closed three facilities last year. [2] The goal was/is to digitize everything to increase access to the archives for everyone.The current administration reduced NARA funding and, in February, dismissed Shogan as \"Archivist of the United States\" but it appears a plan for a strategic shift was underway before those changes.1: https://nsarchive.gwu.edu/foia-audit/foia/2024-03-15/us-nati...2: https://www.archives.gov/press/press-releases/2024/nr24-37reply",
      "> KTLOKeep The Lights On?reply",
      "I have a researcher card, it's not an arduous process. The staff are very kind. When you show up to the National Archives in Washington, DC., prepare to encounter a general group of 2 dozen pros carrying scanners, laptops, etc. It's quite the experience.I don't see this is that big of a deal. It's open, you can access it, but they are controlling more. Given the propensity for the theft and destruction of archives documents in the past, I'm ok with more security.reply"
    ],
    "link": "https://www.archives.gov/college-park",
    "first_paragraph": "\n\n\u00a0Directions to 8601 Adelphi Rd.  Truck Deliveries use entrance at 3301 Metzerott Rd.8601 Adelphi RoadCollege Park, MD\u00a0 20740Truck Deliveries - entrance at\u00a03301 Metzerott RoadCustomer Service Center: 1-866-272-6272\u00a0Lost and Found: 301-837-2900Email: inquire@nara.govThe research room at the National Archives at College Park, MD, is open Monday\u2013Friday, 9 a.m.\u20135 p.m. Research appointments are highly encouraged, but you may also conduct research as a walk-in. Research appointments can be scheduled via Eventbrite on the\u00a0National Archives DC-area Research Appointments\u00a0page.\u00a0Researchers may contact Textual Consultation at\u00a0archives2reference@nara.gov\u00a0for\u00a0advance consultation and Researcher Registration at visit_archives2@nara.gov for researcher registration or research room appointment questions.Cartographic:\u00a0consultation.carto@nara.gov\u00a0Still Pictures:\u00a0consultation.stillpix@nara.gov\u00a0Moving Image and Sound: mopix@nara.gov\u00a0Please see these frequently asked questions for further information abou"
  },
  {
    "title": "Sorry, macOS Tahoe Beta 2 Still Does the Finder Icon Dirty (512pixels.net)",
    "points": 5,
    "submitter": "Bogdanp",
    "submit_time": "2025-06-25T01:10:33 1750813833",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44372767",
    "comments": [
      "The title doesn't match the title of the article, which is \"macOS Tahoe Beta 2 Fixes the Finder Icon\"reply",
      "It's the result of reading a post at Daring Fireball, where the title shown here is used but where it also links to the 512 Pixels post. Hard to be certain which article was the goal of the HN submission.reply",
      "I don\u2019t like that the images in the icons seem to be getting smaller, for the sole purpose of making it seem like a layer on a glass canvas. Edge to edge icons seem like they would naturally be easier for a user to see. This application on Finder feels very forced. But I am glad they at least switched the colors. That bothered me when I noticed it in the keynote.reply",
      "Actual article title: \"macOS Tahoe Beta 2 Fixes the Finder Icon\"reply",
      "It's such a shame they're stuck on this change for the sake of change, but at least they compromised and fixed the colors.reply"
    ],
    "link": "https://512pixels.net/2025/06/finder-icon-fixed/",
    "first_paragraph": "If you missed my previous post on the topic, the first version of Tahoe had the dark blue on the right side of the Finder icon, which was criminal.Our 14-day national nightmare is over. As of Developer Beta 2, the Finder icon in macOS Tahoe has been updated to reflect 30 years of tradition:Now, the background material is blue, with the face on the right side of the icon using a white glassy material. I think this looks pretty good:I know some folks (cough, cough, John Siracusa, cough) want Apple to go even further and make the lighter color on the right extend all the way to the edges of the the icon, which would look something like this very rough mockup I did in just a few minutes:I can understand that, and the desire for the line between the two halves of the icon to be more rounded as it is in macOS Sequoia. However, Apple\u2019s current Finder icon works well for me, and it fits in nicely with the rest of the Dock1 on my Tahoe test machine:Apple heard my feedback as it echoed around th"
  },
  {
    "title": "Subsecond: A runtime hotpatching engine for Rust hot-reloading (docs.rs)",
    "points": 87,
    "submitter": "varbhat",
    "submit_time": "2025-06-24T18:58:11 1750791491",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=44369642",
    "comments": [
      "Creator here - haven't had a chance to write up a blog post yet! Stay tuned.The gist of it is that we intercept the Rust linking phase and then drive `rustc` manually. There's some diffing logic that compares assembly between compiles and then a linking phase where we patch symbols against the running process. Works across macOS, Windows, Linux, iOS, Android, and WASM. On my m4 I can get 130ms compile-patch times, quite wicked stuff.We handle the hard parts that the traditional dylib-reloading doesn't including TLS, statics, constructors, etc.I've been posting demos of it to our twitter page (yes twitter, sorry...)- With bevy: https://x.com/dioxuslabs/status/1924762773734511035- On iOS: https://x.com/dioxuslabs/status/1920184030173278608- Frontend + backend (axum): https://x.com/dioxuslabs/status/1913353712552251860- Ratatui (tui apps): https://x.com/dioxuslabs/status/1899539430173786505Our unfinished release notes are here:https://github.com/DioxusLabs/dioxus/releases/tag/v0.7.0-alp...More details to come!reply",
      "can't access the xitter posts... is the axum part using the whole of dioxus or bare axum + code reloading?reply",
      "There's a custom `axum::serve` equivalent we built that wraps the router construction in a hot-patchable function. When the patches are loaded, we reset the TCP connections.It's a little specific to how dioxus uses axum today, but we plan to release an axum-only integration in the future.reply",
      "Interesting, but the documentation makes it sound like you have to preemptively wrap all the code you think you might want to change in a special wrapper \"call\" function. If true that makes this a lot less appealing than existing solutions for other languages that can modify any function without special annotations.reply",
      "You basically need to wrap your program's `tick()` function. Otherwise you might be in the middle of malloc, hot-patch, and your struct's layout and alignment changes, and your program crashes due to undefined behavior.The goal is that frameworks just bake `subsecond::current` into their `tick()` function and end-users get hot-patching for free.reply",
      "Very nice. For a long time I wondered who would use hotpatching but working with large Java applications made me appreciate the possibility even if it is not 100% reliable (as it is in Java).From the docs Subsecond looks almost perfect. The only downside I found is that (if I understood correctly) you have to modify the function call in the source code of every function you want to hotpatch.It is a bit mitigated in that the change does not cost anything in release builds, but it still is a big thing. Do I want sprinkle my code with call for every function I might potentially have to patch in a debugging session?reply",
      "Creator here - you only need one `subsecond::call` to hook into the runtime and it doesn't even need to be in your code - it can be inside a dependency.Currently Dioxus and Bevy have subsecond integration so they get automatic hot-patching without any end-user setup.We hope to release some general purpose adapters for axum, ratatui, egui, etc.reply",
      "Very nice! Thanks.reply",
      "I'll have to give this a shot for some of the Rust server work I'm doing. progscrape.com uses a lot of tricks to boot quickly specifically because of the edit-compile-run cycle being slow (mostly deferred loading of indexes, etc).My current day job @ Gel has me working on a Rust socket frontend for some pretty complex code and that could also be pretty interesting.It seems to require that you choose a good \"cutover\" point in your codebase, but TBH that's probably not too hard to pick. The HTTP service handler in a webserver, the socket handlers in non-web serving code, etc.It does appear to have a limitation where it will only allow the main crate to be hotpatched. That's less than ideal, but I suppose the convenience might justify some code structure changes to allow that.reply",
      "Neat but I would prefer simply using a dylib for the part of my code that I want to be reloadable.reply"
    ],
    "link": "https://docs.rs/subsecond/0.7.0-alpha.1/subsecond/index.html",
    "first_paragraph": "Subsecond is a library that enables hot-patching for Rust applications. This allows you to change\nthe code of a running application without restarting it. This is useful for game engines, servers,\nand other long-running applications where the typical edit-compile-run cycle is too slow.Subsecond also implements a technique we call \u201cThinLinking\u201d which makes compiling Rust code\nsignificantly faster in development mode, which can be used outside of hot-patching.Subsecond is designed to be as simple for both application developers and library authors.Simply call your existing functions with call and Subsecond will automatically detour\nthat call to the latest version of the function.To actually load patches into your application, a third-party tool that implements the Subsecond\ncompiler and protocol is required. Subsecond is built and maintained by the Dioxus team, so we\nsuggest using the dioxus CLI tool to use subsecond.To install the Dioxus CLI, we recommend using cargo binstall:The Dioxus"
  },
  {
    "title": "How to Think About Time in Programming (shanrauf.com)",
    "points": 62,
    "submitter": "rmason",
    "submit_time": "2025-06-24T20:01:42 1750795302",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=44370426",
    "comments": [
      "The way Google implemented leap seconds wasn't by sticking a 23:59:60 second at the end of 31st Dec. The way they did it was more interesting.What they did instead was to \"smear\" it across the day, by adding 1 / 86400 seconds to every second on 31st Dec. 1/86400 seconds is well within the margin of error for NTP, so computers could carry on doing what they do without throwing errors.Edit: They smeared it from noon before the leap second, to the noon after, i.e 31st Dec 12pm - 1st Jan 12pm.reply",
      "Time is a mess. Always. The author only scratched the surface on all the issues. Even if we exclude the time dilation of relativity which affects GPS/GNSS satellites - independent of if it is due to difference in gravitational pull or their relative speed over ground, it's still a mess.Timezones; sure. But what about before timezones got into use? Or even halfway through - which timezone, considering K\u00f6nigsberg used CET when it was part of Germany, but switched to EET after it became Russian. There's even countries that have timezones differenting by 15 minutes.And dont get me started on daylight savings time. There's been at least one instance where DST was - and was not - in use in Lebanon - at the same time! Good luck booking an appointment...Not to mention the transition from Julian calendar to Gregorian, which took place over many, many years - different by different countries - as defined by the country borders at that time...We've even had countries that forgot to insert a leap day in certain years, causing March 1 to occur on different days altogether for a couple of years.Time is a mess. Is, and aways have been, and always will be.reply",
      "Author covers how IANA handles K\u00f6nigsberg, it is logically its own timezone.  An IANA timezone uniquely refers to the set of regions that not only share the same current rules and projected future rules for civil time, but also share the same history of civil time since 1970-01-01 00:00+0. In other words, this definition is more restrictive about which regions can be grouped under a single IANA timezone, because if a given region changed its civil time rules at any point since 1970 in a a way that deviates from the history of civil time for other regions, then that region can't be grouped with the others\n\nI agree that time is a mess. And the 15 minute offsets are insane and I can't fathom why anyone is using them.reply",
      "zoneinfo does in practice hold the historical info before 1970 when it can do so easily in its framework: https://en.wikipedia.org/wiki/UTC%2B01:24  % zdump -i Europe/Warsaw | head\n  \n  TZ=\"Europe/Warsaw\"\n  - - +0124 LMT\n  1880-01-01 00 +0124 WMT\n  1915-08-04 23:36 +01 CET\n  1916-05-01 00 +02 CEST 1\n  1916-10-01 00 +01 CET\n  1917-04-16 03 +02 CEST 1\n  1917-09-17 02 +01 CET\n  1918-04-15 03 +02 CEST 1\n  % zdump -i Europe/Kaliningrad | head -20\n  \n  TZ=\"Europe/Kaliningrad\"\n  - - +0122 LMT\n  1893-03-31 23:38 +01 CET\n  1916-05-01 00 +02 CEST 1\n  1916-10-01 00 +01 CET\n  1917-04-16 03 +02 CEST 1\n  1917-09-17 02 +01 CET\n  1918-04-15 03 +02 CEST 1\n  1918-09-16 02 +01 CET\n  1940-04-01 03 +02 CEST 1\n  1942-11-02 02 +01 CET\n  1943-03-29 03 +02 CEST 1\n  1943-10-04 02 +01 CET\n  1944-04-03 03 +02 CEST 1\n  1944-10-02 02 +01 CET\n  1945-04-02 03 +02 CEST 1\n  1945-04-10 00 +02 EET\n  1945-04-29 01 +03 EEST 1\n  1945-10-31 23 +02 EET\n  %reply",
      "Yep. Fortunately, a lot of apps can get by with just local civil time and an OS-set timezone. It\u2019s much less common that they need to worry about leap seconds, etc. And many also don\u2019t care about millisecond granularity, etc. If your app does care about all that, however, things become a mess quite quickly.reply",
      "It\u2019s quite different from how I think about time, as a programmer. I treat human time and timezones as approximate. Fortunately I\u2019ve been spared from working on calendar/scheduling for humans, which sounds awful for all the reasons mentioned.Instead I mostly use time for durations and for happens-before relationships. I still use Unix flavor timestamps, but if I can I ensure monotonicity (in case of backward jumps) and never trust timestamps from untrusted sources (usually: another node on the network). It often makes more sense to record the time a message was received than trusting the sender.That said, I am fortunate to not have to deal with complicated happens-before relationships in distributed computing. I recall reading the Spanner paper for the first time and being amazed how they handled time windows.reply",
      "Two things that aren't really covered:- system clock drift. Google's instances have accurate timekeeping using atomic clocks in the datacenter, and leap seconds smeared over a day. For accurate duration measurements, this may matter.- consider how the time information is consumed. For a photo sharing site the best info to keep with each photo is a location, and local date time. Then even if some of this is missing, a New Year's Eve photo will still be close to midnight without considering its timezone or location. I had this case and opted for string representations that wouldn't automatically be adjusted. Converting it to the viewer's local time isn't useful.reply",
      "Very nice write up!  But I think your point that time doesn't need to be a mess is refuted by all the points you made.I know you had to limit the length of the post, but time is an interest of mine, so here's a couple more points you may find interesting:UTC is not an acronym. The story I heard was the English acronym would be \"CUT\" (the name is \"coordinated universal time\") and the French complained, the French acronym would be \"TUC\" and the English-speaking committee members complained,  so they settled for something that wasn't pronouncable in either.  (FYI, \"ISO\" isn't an acronym either!)Leap seconds caused such havoc (especially in data centers) that no further leap seconds will be used.  (What will happen in the future is anyone's guess.)  But for now, you can rest easy and ignore them.I have a short list of time (and NTP) related links at <https://wpollock.com/Cts2322.htm#NTP>.reply",
      "The absl library has a great write up of time programming: https://abseil.io/docs/cpp/guides/timereply",
      "> other epochs work too (e.g. Apollo_Time in Jai uses the Apollo 11 rocket landing at July 20, 1969 20:17:40 UTC).I see someone else is a Vernor Vinge fan.But it's kind of a wild choice for an epoch, when you're very likely to be interfacing with systems whose Epoch starts approximately five months later.reply"
    ],
    "link": "https://shanrauf.com/archive/how-to-think-about-time-in-programming",
    "first_paragraph": "Date published: Jun 23, 2025Time handling is everywhere in software, but many programmers talk about the topic with dread and fear. Some warn about how difficult the topic is to understand, listing bizarre timezone edge cases as evidence of complexity. Others repeat advice like \"just use UTC bro\" as if it were an unconditional rule - if your program needs precise timekeeping or has user-facing datetime interactions, this advice will almost certainly cause bugs or confusing behavior. Here's a conceptual model for thinking about time in programming that encapsulates the complexity that many programmers cite online.Two important concepts for describing time are \"durations\" and \"instants\". A duration is like the number of seconds it takes Usain Bolt to finish a 100m race; an instant is like the moment that he crossed the finish line, or the moment he was half-way through the race. The concept of \"absolute time\" (or \"physical/universal time\") refers to these instants, which are unique and p"
  },
  {
    "title": "XBOW, an autonomous penetration tester, has reached the top spot on HackerOne (xbow.com)",
    "points": 148,
    "submitter": "summarity",
    "submit_time": "2025-06-24T15:53:12 1750780392",
    "num_comments": 84,
    "comments_url": "https://news.ycombinator.com/item?id=44367548",
    "comments": [
      "Xbow has really smart people working on it, so they're well-aware of the usual 30-second critiques that come up in this thread. For example, they take specific steps to eliminate false positives.The #1 spot in the ranking is both more of a deal and less of a deal than it might appear. It's less of a deal in that HackerOne is an economic numbers game. There are countless programs you can sign up for, with varied difficulty levels and payouts. Most of them pay not a whole lot and don't attract top talent in the industry. Instead, they offer supplemental income to infosec-minded school-age kids in the developing world. So I wouldn't read this as \"Xbow is the best bug hunter in the US\". That's a bit of a marketing gimmick.But this is also not a particularly meaningful objective. The problem is that there's a lot of low-hanging bugs that need squashing and it's hard to allocate sufficient resources to that. Top infosec talent doesn't want to do it (and there's not enough of it). Consulting companies can do it, but they inevitably end up stretching themselves too thin, so the coverage ends up being hit-and-miss. There's a huge market for tools that can find easy bugs cheaply and without too many false positives.I personally don't doubt that LLMs and related techniques are well-tailored for this task, completely independent of whether they can outperform leading experts. But there are skeptics, so I think this is an important real-world result.reply",
      "Maybe that is because the article is chaotic (like any \"AI\" article) and does not really address the false positive issue in a well.presented manner? Or even at all?Below people are reading the tea leaves to get any clue.reply",
      "100% agree with OP, to make a living in BBH you can't go hunting on VDP program that don't pay anything all day. That means you will have a lot of low hanging fruits on those programs.I don't think LLM replace humans, they do free up time to do nicer tasks.reply",
      "> Top infosec talent doesn't want to do it (and there's not enough of it).What is the top talent spending its time on?reply",
      "Vulnerability researchers? For public projects, there's a strong preference for prestige stuff: ecosystem-wide vulnerabilities, new attack techniques, attacking cool new tech (e.g., self-driving cars).To pay bills: often working for tier A tech companies on intellectually-stimulating projects, such as novel mitigations, proprietary automation, etc. Or doing lucrative consulting / freelance work. Generally not triaging Nessus results 9-to-5.reply",
      "Specialized bug-hunting.reply",
      "> so they're well-aware of the usual 30-second critiques that come up in this thread.Succinct description of HN. It\u2019s a damn shame.reply",
      "First:> To bridge that gap, we started dogfooding XBOW in public and private bug bounty programs hosted on HackerOne. We treated it like any external researcher would: no shortcuts, no internal knowledge\u2014just XBOW, running on its own.Is it dogfooding if you're not doing it to yourself? I'd considerit dogfooding only if they were flooding themselves in AI generated bug reports, not to other people. They're not the ones reviewing them.Also, honest question: what does \"best\" means here? The one that has sent the most reports?reply",
      "Their success rates on HackerOne seem widely varying.  22/24 (Valid / Closed) for Walt Disney\n\n  3/43 (Valid / Closed) for AT&Treply",
      "Walt Disney doesn't pay bug bounties. AT&T's bounties go up to $5k, which is decent but still not much. It's possible that the market for bugs is efficient.reply"
    ],
    "link": "https://xbow.com/blog/top-1-how-xbow-did-it/",
    "first_paragraph": ""
  },
  {
    "title": "The Jumping Frenchmen of Maine (amusingplanet.com)",
    "points": 18,
    "submitter": "bookofjoe",
    "submit_time": "2025-06-22T17:56:14 1750614974",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.amusingplanet.com/2025/06/the-jumping-frenchmen-of-maine.html",
    "first_paragraph": ""
  },
  {
    "title": "Starship: The minimal, fast, and customizable prompt for any shell (starship.rs)",
    "points": 369,
    "submitter": "benoitg",
    "submit_time": "2025-06-24T11:11:52 1750763512",
    "num_comments": 172,
    "comments_url": "https://news.ycombinator.com/item?id=44364874",
    "comments": [
      "I like maximalist prompts, and indeed Starship is what Shell Bling Ubuntu [1] installs on a new dev machine. But they're not everyone's cup of tea.If I wanted to recommend to someone the min-maxed, highest density thing they could add to their prompt, it would simply be the time your current prompt appeared + the amount of time the last command you ran took.These two pieces of information together make it very easy for you (or your local sysadmin (or an LLM looking over your digital shoulder)) to piece together a log of exactly what happened when. This kind of psuedo-non-repudiation can be invaluable for debugging sessions when you least expect it.This was a tip I distilled from Michael W. Lucas's Networking for System Administrators a few years ago, which remains my preferred recommendation for any developers looking to learn just enough about networking to not feel totally lost when talking to an actual network engineer.Bonus nerd points if you measure time in seconds since the UNIX epoch. Very easy and fast to run time delta calculations if you do that:    [0 1719242840] $ echo \"foo\"\n    [0 1719242905] $ echo \"fell asleep before hitting enter\" && sleep 5\n    [5 1719242910] $\n\n[1]: https://github.com/hiAndrewQuinn/shell-bling-ubuntureply",
      "nushell does that out of the box:    > history | get 82076\n    \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 start_timestamp \u2502 2025-06-24 16:46 \u2502\n    \u2502 command         \u2502 mpc play         \u2502\n    \u2502 cwd             \u2502 /home/work       \u2502\n    \u2502 duration        \u2502 1ms              \u2502\n    \u2502 exit_status     \u2502 0                \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nIt's really nice, because it doesn't just tell you time between command executions (or rather time between commands finishing), but the actual runtime duration of the command.reply",
      "I never bothered configuring my prompt at all because, inside emacs, I could already see most of what I needed in the editor itself.In fact, I only set up Starship when I started to do more pairing. It wasn\u2019t for my benefit as much as it was for those watching my screen and checking the work, especially when operating on prod and confirming what we wanted to run. I just load up a separate terminal app for that so I don\u2019t have to walk people through my setup.reply",
      "The exit code of the last command is useful for similar reasons.reply",
      "Current time in a more human-readable format is very helpful sometimes. Also, the exit status of the previous command, if nonzero, is also very helpful when anything fails.reply",
      "For personal workstation, the current directory is enough. Maybe I change the color based the status of the last command. That\u2019s pretty much the only information I need before entering any command. Everything else can be accessed when I really need it.reply",
      "You don't need to know what branch you're on before running commands? I cant tell you the number of times ive been on the wrong branch executing stuff.reply",
      "I'm highly aware of which branch I'm on. Because it's because I don't use any scripts or automation that switches branches; I only ever switch branches manually so I have that awareness.reply",
      "I only switch branches manually too, but I work in many repos and come back to stuff after days sometimes.reply",
      "Even if I know my current branch, having my prompt show me untracked/uncommitted/unpushed changes helps to identify if something didn\u2019t work because I\u2019m in a dirty state, or if something I ran (unexpectedly) caused a dirty state.For example, I don\u2019t expect running scripts/build.sh to modify tracked files in the repo.  Seeing part of the prompt go from \u201c\u201d to \u201c?2!3\u201d (two untracked, three changed files) makes that glaringly obvious.reply"
    ],
    "link": "https://starship.rs/",
    "first_paragraph": "EnglishEnglishAppearanceThe minimal, blazing-fast, and infinitely customizable prompt for any shell!Works on the most common shells on the most common operating systems. Use it everywhere!Brings the best-in-class speed and safety of Rust, to make your prompt as quick and reliable as possible.Every little detail is customizable to your liking, to make this prompt as minimal or feature-rich as you'd like it to be.Install the starship binary:With Shell:To update the Starship itself, rerun the above script. It will replace the current version without touching Starship's configuration.With Homebrew:With Winget:Add the init script to your shell's config file:Add the following to the end of ~/.bashrc:Add the following to the end of ~/.config/fish/config.fish:Add the following to the end of ~/.zshrc:Add the following to the end of Microsoft.PowerShell_profile.ps1. You can check the location of this file by querying the $PROFILE variable in PowerShell. Typically the path is ~\\Documents\\PowerShe"
  },
  {
    "title": "The bitter lesson is coming for tokenization (lucalp.dev)",
    "points": 207,
    "submitter": "todsacerdoti",
    "submit_time": "2025-06-24T14:14:03 1750774443",
    "num_comments": 94,
    "comments_url": "https://news.ycombinator.com/item?id=44366494",
    "comments": [
      "This gave me an idea: we can take a mixture of tokenizations with learned weights, just like taking a mixture of experts with learned weights. BLT is optimized for compression, but an approach like this could be optimized directly for model performance, and really learn to skim.Concretely: we learn a medium sized model that takes a partial tokenization and outputs a probability distribution over the endpoints of the next token (say we let the token lengths range from 1 to 64 bytes, the model outputs 64 logits). Then we do a beam search to find the, say, 4 most likely tokenizations. Then we run the transformer on all four tokenizations, and we take the expected value of the loss to be the final loss.If we train this on prompt-response pairs, so that it only has to learn what to say and doesn't have to predict the context, then it could learn to skim boring stuff by patching it into ~64 byte tokens. Or more if we want.And ofc we'd use a short context byte level transformer to encode/decode tokens to vectors. Idk this idea is kinda half baked.reply",
      "I think that's what evolution did when developing the brain! :-)I'm a total noob in ML. I just had to vent something for not understanding this stuff and realizing that knowing physics doesn't mean you can grok ML mechanics.reply",
      "The main limitation of tokenization is actually logical operations, including arithmetic. IIRC most of the poor performance of LLMs for math problems can be attributed to some very strange things that happen when you do math with tokens.I'd like to see a math/logic bench appear for tokenization schemes that captures this. BPB/perplexity is fine, but its not everything.reply",
      "This paper has a good solution:https://arxiv.org/abs/2402.14903You right to left tokenize in groups of 3, so 1234567 becomes 1 234 567 rather than the default 123 456 7.  And if you ensure all 1-3 digits groups are in the vocab, it does much better.Both https://arxiv.org/abs/2503.13423 and https://arxiv.org/abs/2504.00178 (co-author) both independently noted that you can do this with just by modifying the pre-tokenization regex, without having to explicitly add commas.reply",
      "Ok great! This is precisely how I chunk numbers for comparison. And not to diminish a solid result or the usefulness of it or the baseline tech: its clear that it we keep having to create situation - specific inputs or processes, we're not at AGI with this baseline techreply",
      "It's a non-deterministic language model, shouldn't we expect mediocre performance in math? It seems like the wrong tool for the job...reply",
      "Models are deterministic, they're a mathematical function from sequences of tokens to probability distributions over the next token.Then a system samples from that distribution, typically with randomness, and there are some optimizations in running them that introduce randomness, but it's important to understand that the models themselves are not random.reply",
      "The LLMs are deterministic but they only return a probability distribution over following tokens. The tokens the user sees in the response are selected by some typically stochastic sampling procedure.reply",
      "Assuming decent data, it won't be stochastic sampling for many math operations/input combinations. When people suggest LLMs with tokenization could learn math, they aren't suggesting a small undertrained model trained on crappy data.reply",
      "I mean, this depends on your sampler. With temp=1 and sampling from the raw output distribution, setting aside numerics issues, these models output nonzero probability of every token at each positionreply"
    ],
    "link": "https://lucalp.dev/bitter-lesson-tokenization-and-blt/",
    "first_paragraph": "Home Blog Bio\n\n\n                    24 Jun, 2025\n                \n\na world of LLMs without tokenization is desirable and increasingly possiblePublished on 24/06/2025 \u2022 \u23f1\ufe0f 29 min readIn this post, we highlight the desire to replace tokenization with a general method that better leverages compute and data. We'll see tokenization's role, its fragility and we'll build a case for removing it. After understanding the design space, we'll explore the potential impacts of a recent promising candidate (Byte Latent Transformer) and build strong intuitions around new core mechanics.As it's been pointed out countless times - if the trend of ML research could be summarised, it'd be the adherence to The Bitter Lesson - opt for general-purpose methods that leverage large amounts of compute and data over crafted methods by domain experts. More succinctly articulated by Ilya Sutskever, \"the models, they just want to learn\". Model ability has continued to be blessed with the talent influx, hardware upgra"
  },
  {
    "title": "Show HN: VSCan - Detect Malicious VSCode Extensions (vscan.dev)",
    "points": 19,
    "submitter": "shadow-ninja",
    "submit_time": "2025-06-24T22:32:54 1750804374",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=44371740",
    "comments": [
      "It is beyond madness and well into \"intentionally negligent\" to release a plugin system without a permissions model in, like, the last 20 years. Can't believe people aren't up in arms about how wide open vscode and similar things are, particularly now that docker is widespread.Thanks for building a scanner!  I wish it wasn't necessary :/reply",
      "IDK, I have built a plugin system myself. It is very hard to have a plugin system that is both powerful, versatile and sandboxed. Like with with anything you can pick 2. Most of the plugins I use in vscode like prettier, rust analyzer, etc all need file access and process spawn. So if you sandbox it they would all need max access anyway which kind of defeats the purpose.reply",
      "There is an enormous amount of space between \"it must be written in lua for safety\" and \"leftpad can upload your entire hard drive and then ransom it back to you\".Right now we have the latter.reply",
      "Sure but it would be nice to differentiate the permissions given to rust-analyzer and, say, \"TODO Highlight\"reply",
      "You should definitely show the vulnerabilities you found on the front page, instead of showcasing low scores given to popular extensions. Claiming that \"rust-analyzer\" is \"High Risk\" is a strong turn-off from me thinking your service is useful (why? because it contains shell commands in the form of \"taskDefinitions\", and because it uses a dependency to parse ANSI sequences that hasn't received a commit in the past 90 days).reply",
      "Finally we have something like this. This is very good workreply",
      "Nice work! This has actually been an open feature request since 2018 [0]. I've been wanting something like this for a while.[0] https://github.com/microsoft/vscode/issues/52116reply",
      "i wish the detail links on each analysis tile were real links, instead of some apparently weird javascript. seems broken in firefoxit would also be nice if i could expand all the analysis detail at once, instead of just one section at a time.reply",
      "vscodevim got 71/100 high risk. That's a pretty common one.reply",
      "That's concerning. What is Microsoft doing about it? Have you contacted them?reply"
    ],
    "link": "https://vscan.dev/",
    "first_paragraph": "\n                Proactively analyze Visual Studio Code extensions for security vulnerabilities and ensure a safer development environment.\n            A simple, transparent process to help you understand extension security.Provide the VSCode Marketplace name or ID of the extension you want to analyze.Our system fetches and scrutinizes the extension's code, permissions, and metadata.Receive a comprehensive report detailing potential risks and recommendations.Quickly analyze commonly used extensions or search for specific ones above.10k+Extensions Scanned500+Vulnerabilities Identified24/7Monitoring\u00a9 2025 VSCan. Empowering Secure Development."
  },
  {
    "title": "Canal Boat Simulator (jacobfilipp.com)",
    "points": 3,
    "submitter": "surprisetalk",
    "submit_time": "2025-06-22T14:39:55 1750603195",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://jacobfilipp.com/boat/",
    "first_paragraph": "Jacob FilippWritten oninDear Reader,Alcohol is a hell of a drug.It\u2019s night. I\u2019m sitting and drinking my Jura. Relaxing with an episode of \u201cNarrow Escapes\u201c, a show life on Britain\u2019s inland canals.Tonight\u2019s feature is about a pair of young canal boaters: Amy is a game streamer and Wes is a game level designer.Everything is going swimmingly until Wes says1:So I goes PFFFFFFTTT I BET ITS GOING TO BE A GRITTY CANAL BOAT GAME!!!!And\u2026 and then\u2026And then in my addled state I go\ud83e\udde0 Pff ff ff t, I bet\u2026 it\u2019s going to be a\u2026 gritty\u2026 canal boat game. \ud83e\udde0Days later, the idea stuck and wouldn\u2019t leave\u2026 so I went and made the game!I\u2019ll tell you all about it. But first you gotta play the game. Crank the volume up. Go on. I\u2019ll wait until you finish:Now, you can\u2019t judge the game harshly because it is a prototype.2 For the full epic story of making the game, read on:Previously, I created a stunning true-life 3D simulation of life on a Victorian London street. So I had mastered a game engine and could make the bo"
  },
  {
    "title": "Basic Facts about GPUs (damek.github.io)",
    "points": 236,
    "submitter": "ibobev",
    "submit_time": "2025-06-24T12:15:12 1750767312",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=44365320",
    "comments": [
      "been running llama.cpp and vllm on same 4070, trying to batch more prompts for serving. llama.cpp was lagging bad once I hit batch 8 or so, even though GPU usage looked fine. vllm handled it way better.later found vllm uses paged kv cache with layout that matches how the GPU wants to read fully coalesced without strided jumps. llama.cpp was using a flat layout that\u2019s fine for single prompt but breaks L2 access patterns when batching.reshaped kv tensors in llama.cpp to interleave ; made it [head, seq, dim] instead of [seq, head, dim], closer to how vllm feeds data into fused attention kernel. 2x speedup right there w.r.t same ops.GPU was never the bottleneck. it was memory layout not aligning with SM\u2019s expected access stride. vllm just defaults to layouts that make better use of shared memory and reduce global reads. that\u2019s the real reason it scales better per batch.this took its own time of say 2+days and had to dig under the nice looking GPU graphs to find real bottlenecks, it was widly trial and error tbf,> anybody got idea on how to do this kinda experiment in hot reload mode without so much hassle??reply",
      ">GPU was never the botteneck\n>it was memory layoutah right so the GPU was the bottleneck thenreply",
      "did you see yesterday nano-vllm [1] from a deepseek employee 1200LOC and faster than vanilla vllm?1. https://github.com/GeeeekExplorer/nano-vllmreply",
      "Is it faster for large models, or are the optimizations more noticeable with small models? Seeing that the benchmark uses a 0.6B model made me wonder about that.reply",
      "I have not tested it but its from a deepseek employee i don't know if it's used in prod there or not!reply",
      "did you do a PR to integrate these changes back into llama.cpp ? 2x speedup would be absolutely wildreply",
      "Almost nobody using llama.cpp does batch inference. I wouldn\u2019t be surprised if the change is somewhat involved to integrate with all of llama.cpp\u2019s other features. Combined with lack of interest and keeping up with code churn, that would probably make it difficult to get included, with the number of PRs the maintainers are flooded with.reply",
      "Any speed up that is 2x is definitely worth fixing. Especially since someone has already figured out the issue and performance testing [1] shows that llamacpp* is lagging behind vLLM by 2x. This is a positive for all running LLMs locally using llamacpp.Even if llamacpp isnt used for batch inference now, this can allow those to finally run llamacpp for batching and on any hardware since vLLM supports only select hardware. Maybe finally we can stop all this gpu api software fragmentation and cuda moat as llamacpp benchmarks have shown Vulkan to be as or more performant than cuda or sycl.[1] https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lab...reply",
      "So, what exactly is batch inference workload and how would someone running inference on local setup benefit from it? Or how would I even benefit from it if I had a single machine hosting multiple users simultaneously?I believe batching is a concept only useful when during the training or fine tuning process.reply",
      "Batch inference is just running multiple inferences simultaneously. If you have simultaneous requests, you\u2019ll get incredible performance gains, since a single inference doesn\u2019t leverage any meaningful fraction of a GPU\u2019s compute capability.For local hosting, a more likely scenario where you could use batching is if you had a lot of different data you wanted to process (lots of documents or whatever). You could batch them in sets of x and have it complete in 1/x the time.A less likely scenario is having enough users that you can make the first user wait a few seconds while you wait to see if a second user submits a request. If you do get a second request, then you can batch them and the second user will get their result back much faster than if they had had to wait for the first user\u2019s request to complete first.Most people doing local hosting on consumer hardware won\u2019t have the extra VRAM for the KV cache for multiple simultaneous inferences though.reply"
    ],
    "link": "https://damek.github.io/random/basic-facts-about-gpus/",
    "first_paragraph": "\n  last updated:\n  2025-06-18\nI\u2019ve been trying to get a better sense of how GPUs work. I\u2019ve read a lot online, but the following posts were particularly helpful:This post collects various facts I learned from these resources.Acknowledgements: Thanks to Alex McKinney for comments on independent thread scheduling.Table of ContentsA GPU\u2019s design creates an imbalance since it can compute much faster than it can access its main memory. An NVIDIA A100 GPU, for example, can perform 19.5 trillion 32-bit floating-point operations per second (TFLOPS), but its memory bandwidth is only about 1.5 terabytes per second (TB/s). In the time it takes to read a single 4-byte number, the GPU could have performed over 50 calculations.Below is a diagram of the compute and memory hierarchy for an NVIDIA A100 GPU. The numbers I quote for flops/s and TB/s are exclusive to A100s.This diagram shows the performance hierarchy.1 Global Memory (VRAM) is the large, slow, off-chip memory pool where all data initially "
  }
]