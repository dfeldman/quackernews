[
  {
    "title": "Map of GitHub (github.com/anvaka)",
    "points": 177,
    "submitter": "vortex_ape",
    "submit_time": "2024-12-15T22:02:33 1734300153",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=42426284",
    "comments": [
      "Surprised at how small Rustland is. Barely a province in Clouderra.Also, interesting how both Bevy and Veloren are in Rustland. Probably, the stars come more from the Rust community than the game dev community. Which I guess makes sense: the Rust ecosystem is still relatively small and feels like a lot of people doing X but in Rust.\n \nreply",
      "I'm also shocked how small \"nodelandia\" is and that its not even its own continent. I guess we all overestimate the size of our bubbles\n \nreply",
      "Happy to see bevy between them though! :)\n \nreply",
      "Tangent: not that often to see a fellow Ramon in hn :)\n \nreply",
      "Also, lol at Zig being a suburb of Rust\n \nreply",
      "Somehow torvalds/linux is in Fronterra, next to JS projects, awesome-X lists, and frontend checklists.Either kernel hackers unexpectedly love frontend, or more likely the people that write the code don't overlap much with the people that star Github projects!\n \nreply",
      "Because of react ?\n \nreply",
      "That was my first reaction.\n \nreply",
      "Live link: https://anvaka.github.io/map-of-github/\n \nreply",
      "\"Sussex\" as the name of the Among Us section had me laughing\n \nreply"
    ],
    "link": "https://github.com/anvaka/map-of-github",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Inspirational Mapping\n      This is a map of 400,000+ GitHub projects. Each dot is a project. Dots are close to each other if they have a lot of common\nstargazers.The first step was to fetch who gave stars to which repositories. For this I used a public data set of github activity events on\nGoogle BigQuery, considering only events between Jan 2020 and March 2023. This gave me more than 350 million stars.\n(Side note: Mind blowing to think that Milky Way has more than 100 billion stars)In the second phase I computed exact Jaccard Similarity between each repository.\nFor my home computer's 24GB RAM this was too much, however an AWS EC2 instance with 512GB of RAM chewed through it in a few hours.\n(Side note: I tried other similarities too, but Jaccard gave the most believable results)In the third phase I used a few clustering algorithms "
  },
  {
    "title": "25 Years of Dillo (dillo-browser.github.io)",
    "points": 58,
    "submitter": "todsacerdoti",
    "submit_time": "2024-12-15T23:06:15 1734303975",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=42426589",
    "comments": [
      "I used Dillo when I ran OpenBSD on my laptop in 2003. Running a full desktop environment with text editor and web browser open: 28 MB RAMI remember that Dillo did not support automatic redirects back then. They were everywhere.\n \nreply",
      "I am so delighted to see Dillo is back and being maintained again! I was looking at it in mid 2023 and was sad to see the repo linked from the main site was dead and the general lack of activity. Off the back of this, I started looking at how to extract it and use it as a library (which I ultimately failed at, alas).I've subsequently started a little project I'm calling Freeflow, which is intended to break down pages from 'modern' html to a subset renderable in Dillo, NetSurf, etc - essentially a standard set of xpath queries for tags like `navigation` etc, plus some tinkering with RSS and other things which display pages without styling.Now that Dillo is alive again, I'm inclined to give writing it as a DPI a whirl.So pleased to see this. Keeping these things alive is important.\n \nreply",
      ">On March of 2000, the license was changed to the GPLv3 with the permission of Raph Levien (the main developer of Gzilla/Armadillo).This does not sound right. GPLv3 is much newer than that.\n \nreply"
    ],
    "link": "https://dillo-browser.github.io/25-years/",
    "first_paragraph": "Back in 1999, a group of developers lead by Jorge Arellano Cid forked the\nGzilla/Armadillo code originally\ndeveloped by Raph Levien, and began working on what is now known as the Dillo\nweb browser.\n\nToday, as of December of 2024, Dillo is 25 years old!\nDuring this long journey, the project has seen a lot of changes and has\nstalled several times, but it managed to survive to this day. As a commemoration\nof this milestone, so I wanted to write a short history of the project.\nNotice that Jorge had lead the project from 1999 until 2019, and as of\ntoday, we don't have more news about him anymore. I hope he is okay and just\ndecided to move on to other projects. Since then, I (Rodrigo) decided to\nresurrect the project and bring it back to life. I have no relationship with the\nprevious developers, but I try to do my best to keep the original goals intact.\n\nThis document tries to tells the history as I have observed it through the pieces I was\nable to collect from the\nmailing\n\tlist, the\nchangel"
  },
  {
    "title": "The First 50M Prime Numbers (1975) [pdf] (mpg.de)",
    "points": 35,
    "submitter": "quickfox",
    "submit_time": "2024-12-15T22:11:57 1734300717",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=42426337",
    "comments": [
      "I like how the feeling of the author's excitement about the topic is embedded in the text\"\"\"\nupon looking at these numbers one has the feeling of being in the presence of one of the inexplicable secrets of creation.\n\"\"\"\"\"\"\nI hope that with this and the other pictures I have shown, I have communicated a certain impression of the immense beauty of the prime numbers and of the endless surprises which they have in store for us. \n\"\"\"\n \nreply",
      "The 50 000 000th prime is 982451653, but fun fact: you may have already memorized a prime much larger than this without even realizing you have done so!2^255-19This is where Curve 25519 and the associated cryptography (ed25519, x25519) gets its name from. Written out, 2^255-19=57896044618658097711785492504343953926634992332820282019728792003956564819949.\n \nreply",
      "You could have memorized even large one if you are familiar with the full name of the Mersenne Twister PRNG: MT19937 is named so because its period is 2^19937-1 which is a prime number (in fact, the largest known one at the time of Zigler's writing). In my knowledge any larger prime number hasn't been used for the practical purpose.\n \nreply",
      "Back in the 70s, when I was a teen, we had a set of Encyclopedia Britannica. It came with a service where you could send off for various pamphlets for more focused information. I sent away for a paper listing pi to 10k or maybe 100k digits. By the late 70s/early 80s, that was outdated, as I wrote code to find those for myself (though e to many places was far easier).\n \nreply",
      "That's a very cool accomplishment.What language did you use to write the code?I also have another question, did you witness the transition from punched cards to  terminals?\n \nreply",
      "This would have been assembly code, probably 6809 or 68000 system I had back then.  6809 would have required dumping intermediate data to a disk.  I don't recall just when I first got a hard-disk, which would probably have been a massive 10 megabytes in size.And yes, I saw that transition.  I learned to program using Fortran IV and IBM 11/30 assembly in the mid-70s, using punched cards.  Wrote a MIXAL assembler and simulator for the minicomputer at the local college around 1976; it was about 7000 punched cards in length, all assembly.  Got a Commodore PET in 1978, moved on to SS-50 based 6809 and 68008 systems in the late 70s/early 80s, with a serial terminal.\n \nreply",
      "I wonder what would happen if someone discovered an efficient algorithm for finding or predicting prime numbers or their factors. It would put the fundamentals of internet security at risk and likely much more. Has anyone ever considered a plan B for such a scenario?\n \nreply",
      "Yes. Shor's algorithm on quantum computers represents such a theoretical possibility,\nso the industry is moving to resistant algorithms that aren't based on products of large primes such as elliptic curve cryptography.\n \nreply",
      "Of note that even then, for the seemingly impossibly large prime numbers, they gave dual credit to the researcher and the computer used.  So, credit for the largest prime in 1963 went to Gillies and ILIAC 2.  When that record was broken in 1972, credit went to Tuckerman and IBM 360.\n \nreply",
      "Is this helpful for \u2026 RSA testing ?\n \nreply"
    ],
    "link": "https://people.mpim-bonn.mpg.de/zagier/files/doi/10.1007/BF03039306/fulltext.pdf",
    "first_paragraph": ""
  },
  {
    "title": "Phi-4: Microsoft's Newest Small Language Model Specializing in Complex Reasoning (microsoft.com)",
    "points": 75,
    "submitter": "lappa",
    "submit_time": "2024-12-13T01:54:41 1734054881",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=42405323",
    "comments": [
      "The most interesting thing about this is the way it was trained using synthetic data, which is described in quite a bit of detail in the technical report: https://arxiv.org/abs/2412.08905Microsoft haven't officially released the weights yet but there are unofficial GGUFs up on Hugging Face already. I tried this one: https://huggingface.co/matteogeniaccio/phi-4/tree/mainI got it working with my LLM tool like this:  llm install llm-gguf\n  llm gguf download-model https://huggingface.co/matteogeniaccio/phi-4/resolve/main/phi-4-Q4_K_M.gguf\n  llm chat -m gguf/phi-4-Q4_K_M\n\nHere are some initial transcripts: https://gist.github.com/simonw/0235fd9f8c7809d0ae078495dd630...More of my notes on Phi-4 here: https://simonwillison.net/2024/Dec/15/phi-4-technical-report...\n \nreply",
      "Wow, those responses are better than I expected. Part of me was expecting terrible responses since Phi-3 was amazing on paper too but terrible in practice.\n \nreply",
      "One of the funniest tech subplots in recent memory.TL;DR it was nigh-impossible to get it to emit the proper \"end of message\" token. (IMHO the chat training was too rushed). So all the local LLM apps tried silently hacking around it. The funny thing to me was no one would say it out loud. Field isn't very consumer friendly, yet.\n \nreply",
      ">Microsoft haven't officially released the weightsThought it was official just not on huggingface but rather whatever azure competitor thing they're pushing?\n \nreply",
      "I found their AI Foundry thing so hard to figure out I couldn't tell if they had released weights (as opposed to a way of running it via an API).Since there are GGUFs now someone must have released some weights somewhere.\n \nreply",
      "The SVG created for the first prompt is valid but is a garbage image.\n \nreply",
      "In general I've had poor results with LLMs generating pictures using text instructions (in my case I've tried to get them to generate pictures using plots in KQL).  They work but the pictures are very very basic.I'd be interested for any LLM emitting any kind of text-to-picture instructions to get results that are beyond a kindergartner-cardboard-cutout levels of art.\n \nreply",
      "That's why I use the SVG pelican riding a bicycle thing as a benchmark: it's a deliberately absurd and extremely difficult task.\n \nreply",
      "I'm really glad that I see someone else doing something similar.  I had the epiphany a while ago that if LLMs can interpret textual instructions to draw a picture and output the design in another textual format that this a strong indicator that they're more than just stochastic parrots.My personal test has been \"A horse eating apples next to a tree\" but the deliberate absurdity of your example is a much more useful test.Do you know if this is a recognized technique that people use to study LLMs?",
      "Yeah, it didn't do very well on that one. The best I've had from a local model there was from QwQ: https://simonwillison.net/2024/Nov/27/qwq/\n \nreply"
    ],
    "link": "https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090",
    "first_paragraph": "Today we are introducing Phi-4, our 14B parameter state-of-the-art small language model (SLM) that excels at complex reasoning in areas such as math, in addition to conventional language processing. Phi-4 is the latest member of our Phi family of small language models and demonstrates what\u2019s possible as we continue to probe the boundaries of SLMs. Phi-4 is currently available on Azure AI Foundry under a Microsoft Research License Agreement (MSRLA) and will be available on Hugging Face next week.\u00a0\u00a0Phi-4 Benchmarks\u00a0Phi-4 outperforms comparable and larger models on math related reasoning due to advancements throughout the processes, including the use of high-quality synthetic datasets, curation of high-quality organic data, and post-training innovations. Phi-4 continues to push the frontier of size vs quality.\u00a0\u00a0Phi-4 is particularly good at math problems, for example here are the benchmarks for Phi-4 on math competition problems:\u00a0Phi-4 performance on math competition problems\u00a0To see more "
  },
  {
    "title": "SVC16: Simplest Virtual Computer (github.com/janneuendorf)",
    "points": 153,
    "submitter": "thunderbong",
    "submit_time": "2024-12-15T16:39:26 1734280766",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=42424370",
    "comments": [
      "Funny, I've been working on a similar project, inspired by uxn. Initially, mine was also going to use 16-bit words, but I've since moved to 32-bit signed words. It'll be a bit heftier than this project in other ways, too, but not by much.\n \nreply",
      "It took me about 20 minutes to make a basic FPGA-friendly implementation of the CPU core in Verilog, but the 256KB of ram is pretty expensive (16-bit color seems excessive for such a simple core!).\n \nreply",
      "Ugh, I just noticed it's even worse - the display is double-buffered, so you actually need 384KB of ram (128KB for system RAM and 2 x 128KB display buffers!).\n \nreply",
      "Out of curiosity - why do you care? Do you want to run this thing on a calculator from the 90s or something like that?\n \nreply",
      "Block RAM in FPGAs is very limited and adding something like a DDR3 controller is a hassle - of the dev boards I have lying around, they have between 48KB and 200KB of block RAM. The actual CPU logic is probably <5% of those chips. I have a board with a 16-bit, 16MB PSRAM interface, but then the memory interface is shared between the video generation and the CPU. Going with 4-bit colors or something seems like it would be more in-line with the design philosophy (but it was targeted to be emulated on a modern PC, where memory and color depth are largely free).\n \nreply",
      "I wondered how long it would take for someone to do this. An amount of time approaching zero apparently. Nice.\n \nreply",
      "Implementing a CPU, if you're not trying to hyper-optimize it, is reasonably straightforward. This is a very simple design. It would take a little bit more work to add proper video output and mouse input, but those blocks are easy to find for VGA and PS/2 mice. I think this would run > 100 MHz very easily on anything made in the last decade.\n \nreply",
      "I give you the worlds simplest physical computer:https://en.wikipedia.org/wiki/CARDboard_Illustrative_Aid_to_...\n \nreply",
      "The ISA leaves something to be desired for \"simplest\". Simple, sure, but parameters (and unused ones, at that!)? Memory copy instructions? Multiply and no shifts? Addition _and_ subtraction?Others have mentioned Subleq (Subtract And Branch If Less Than Or Equal To), but there's more useful designs that meet all the design constraints. They state that \"It is also not intended to be as simple and elegant as it could possibly be.\", but it's called \"The Simplest Virtual Computer\" - that kind of name is a challenge.\n \nreply",
      "I like toys like this.  I even have made a few of my own, doodled instruction set ideas on many scraps of paper,  and so forth.It's a bit weird that this particular instruction set annoys me so much.  Perhaps annoys is the wrong word,  maybe irritates is better.   The entire thing I can get behind except for the instruction encoding.   The instructions are too large for the space available.  128k of ram for programs and 128k for screen (and workspace area, given sync),  but at 8 bytes per instruction it eats up the limited resource too quickly.I guess that irritation comes from the efficiency hacks that these constrained environments encourage.  It revives an age of clever tricks to get the most of of a little.  The instruction size pokes at that aesthetic like a lump in a pillow.I'm not sure what would be a better solution while preserving simplicity.   maybe a single extra dereference to turn    opcode arg1 arg2 arg3\n    @(IP) @(IP+1) @(IP+2) @(IP+3)   ;; IP+=4\n\ninto    @(@IP) @(@IP+1) @(@IP+2) @(@IP+3)   ;; IP+=1\n       \nwhich would be one system word per instruction lookup, from a dictionary of instructions in RAM (coders would have to be careful not to lose the ability to generate all constant values if they eliminated certain dictionary entries(Tiny programs would get bigger ,  Larger programs would be smaller when they reuse instructions.If you wanted to work against the idea of 'simplest' and embrace the notion of  weird-features-because-of-technical-debt, you could make an 'upgraded' machine that has a second bank of 128k (64k words) as dedicated program memory, that is initialized at startup with the numbers 0x0000 to 0xffff.    This would make the machine run SVC16 programs unmodified.  You could then either have it use program ROMs or add a single instruction    StoreProgMem @progMem(@arg1+@arg3) =(@arg2+@arg3)\n \nWhich would enable writing the program space.Egads,  I've been nerd sniped!\n \nreply"
    ],
    "link": "https://github.com/JanNeuendorf/SVC16",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Simplest Virtual Computer\n      The goal in one word is simplicity. It should be simple to understand every instruction, to write machine code that runs on it, and to write a compiler for it.The instruction set and the design in general are in no way meant to resemble something that would make sense in real hardware.\nIt is also not intended to be as simple and elegant as it could possibly be.This repo contains an emulator to run games or programs. It can be installed with cargo:You can then run a program from the cli:Use --help to see some basic options.I do not want to provide an assembler, any kind of compiler or even any ideas about things like call conventions.\nThe idea is that you have to build that yourself. You can play a game from the example folder to get an idea of what can be built.\u26a0\ufe0f Warning: Until there are specificatio"
  },
  {
    "title": "I replaced my son's school timetable app with an e-paper (mfasold.net)",
    "points": 59,
    "submitter": "mfld",
    "submit_time": "2024-12-13T14:13:34 1734099214",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=42408546",
    "comments": [
      "Kids are getting scheduling via apps now? The best thing my school ever did for me was give me a paper planner and teach me how to use it. I've finally reached that \"back in my day\" age.It's a neat project though. I wasn't aware Soldered existed so this is a nice find.\n \nreply",
      "> The best thing my school ever did for me was give me a paper planner and teach me how to use it.While in general I'm a fan of \"learn how to do things by hand for didactic value\" (e.g. learning how to do long division before using the calculator to do it), I don't think a paper planner provides any didactic value to learn before replacing it with an electronic calendaring tool.\n \nreply",
      "This is hardly new. My high school did this over fifteen years ago. This app just shows hour+subject+teacher+room number. Quite useful on days the hours/rooms get switched around because a teacher is on leave.Your paper planner won't inform you that your first hour is in another building tomorrow or that the teacher is ill (and yes, the schedules would change less than 12 hours in advance). It's hardly comparable.As for this project, I'm not sure how wise it is to build something that relies on scraping some unknown third party website when it comes to school schedules. I'd risk it for my own schedule, but if your scraper makes a mistake you'll get your kid in trouble. Maybe the teacher will believe \"it's because my parents made this scheduling contraption\" as an excuse, but they'll only accept it once or twice if they do. If there's any app I'd completely unlock in parental controls, it's this one, because it only seems to do the bare minimum anyway.\n \nreply",
      "> Quite useful on days the hours/rooms get switched around because a teacher is on leave.> Your paper planner won't inform you that your first hour is in another building tomorrow or that the teacher is ill (and yes, the schedules would change less than 12 hours in advance).That seems like a really odd way of handling it, and more confusing than it needs to be.  For me the class was assigned to a specific room, not the teacher - so it never moved.  If we had a substitute, they'd just be in the normal room instead of the usual teacher.  If they couldn't get one in time, then either an administrator would bring us to study hall (highschool) or after 10 minutes we'd just leave assuming they couldn't make it (college).\n \nreply",
      "I can\u2019t recall even one time my high school schedule changed except between semesters, not any time a class met in another room. Not once. If a teacher was out sick, there was a sub but the room didn\u2019t change.\n \nreply",
      "Having had a paper planner in high school I don't recall there really being many issues with rooms needing to be swapped or teachers being ill having an impact. If the teacher is ill, the other teacher just takes their room.Having a digital schedule almost gives the school the ability to be more relaxed with the schedule, as opposed to it being built around a schedule that can hardly move. There is an important part of school which is revolved around routine. I remember it only took a few weeks in the first term before I'd be walking to classed without needing to read the timetable at all.It's inevitable that we end up moving down this route, but lets not forget the old solution worked just fine and meant you needed a pretty good reason to change the timetable.\n \nreply",
      "Good points. However unlocking the phone in the morning puts my son on the wrong track: you would be surprised how much time one can spend checking animated gifs by using any text field that will inevitably be unlocked as well.Of course I will take full responsibility in case of bugs... So far it didn't happen.\n \nreply",
      "Imo it should be the school's responsibility to stick a notice on the classroom itself or at least send an email, rather than expect each and every student to check an app every day.  I hadn't been a high school student in a while through.\n \nreply",
      "i don't think there is much value in teaching a paper planner\n \nreply",
      "They still have a paper planner. :) But checking the app each morning is mandatory. And in my son's case, there are changes every couple of days.\n \nreply"
    ],
    "link": "https://mfasold.net/blog/displaying-website-content-on-an-e-ink-display/",
    "first_paragraph": "Or: How I replaced my son's school timetable app with an e-paperRecently, I\u2019ve been looking for a quality-of-life improvement for our family\u2019s morning routine: the daily check of the timetable and substitution plan for the kids\u2019 school. The updated timetable can be accessed either on the school\u2019s website or through a mobile app called \u201cVPmobil\u201d. The problem is that my son\u2019s mobile phone is governed by strict parental controls, making this daily lookup an unpopular chore on my side. Time to do something about this!Essentially, I envision upgrading the classic school timetable on the fridge. I would like to replace the paper note with an e-ink screen that automatically fetches and displays an up-to-date timetable, including daily changes. The display should operate without a direct power connection, and the battery should last for many weeks without a recharge. Critically, this display should show the information without the distraction by interaction with digital devices on either my si"
  },
  {
    "title": "Preferring throwaway code over design docs (softwaredoug.com)",
    "points": 165,
    "submitter": "softwaredoug",
    "submit_time": "2024-12-14T14:52:22 1734187942",
    "num_comments": 86,
    "comments_url": "https://news.ycombinator.com/item?id=42417478",
    "comments": [
      "Opinions sans experience or data can still be helpful when they identify the landscape and issues, e.g.,:A main goal of design is to identify what's not feasible through logic.  So much easier than prototyping.  But still \"can it be done?\" is a far cry from \"can we do it?\".The benefit of prototyping is to show that something is possible.  There the key question is integration: can persons Pn make code C on system S that meets both goal X and constraint Y - i.e., a design as product development.Leaving aside bad-faith behaviors, the real question is socialization: 1. how to gather collective wisdom in some relatively natural and encouraging manner to encourage collective deep consideration of hard problems to avoid waste and get things done.  And 2. the same for motivation: getting everyone aligned, understanding their role and the quality/schedule risks.Given the uncertainties and costs of product development, the prototype has significant advantage of seeming \"real\" relative to some \"ideal\" solution, and thus can bias teams to the concrete, particularly when issues are contended.  Prototype-driven planning likely reflects lack of organizational cohesion (and ironically kicks that can further down the road).So generally I see prototyping as validating designs, and try to make the design discussions lean enough to engage otherwise-prototyping engineers.\n \nreply",
      "This is called prototyping, which is a valuable part of the design process; some people call it \"pathfinding\".These are all inputs into the design. But a design is still needed, of the appropriate size, otherwise you're just making things up as you go. You need to define the problem your are solving and what that solution is. Sometimes that's a 1-page doc without a formal review, sometimes it's many more pages with weeks of reviews and iterations with feedback.Don't forget: \"weeks of coding can save hours of planning\" ;)\n \nreply",
      "Agreed, why not both?  In fact, I\u2019d say, first write up a theory, then demonstrate that it does or does not work using a prototype, then write the actual design doc. At all times, and continuing into the implementation phase, prioritize the disposability of your code. The easier it is to delete, the better.\n \nreply",
      "I think my argument would be that they could be the same artifact. Hence my usage of Draft PRs with a lot of rich documentation. And a willingness to throw it all away if you don't think its ultimately the right approach.The real problem is the industry's inability to throwaway first solutions, so we introduce this (IMO inefficient) 'design doc' step as a safety mechanism\n \nreply",
      "Writing is really beneficial for exploring the problem space.Many times I\u2019ve confidently thought I understood a problem, started writing about it, and come away with new critical questions. These things are typically more visible from the abstract, or might not become apparent in the first few release milestones of work.I\u2019m reminded of a mentor in my career, who had designed an active/active setup retroactively for a payment gateway. He pulls up a Lucidchart and says \u201cthis diagram represents 6 months of my life\u201d.They\u2019re not always necessary or helpful. But when they are you can save weeks of coding with a few days of planning.\n \nreply",
      "I had a boss who had a math degree. He'd map out the flow from start to finish on a whiteboard like you see mathematicians on TV/movies. Always had the smoothest projects because he could foresee problems way in advance. If there was a problem or uncertainty identified, we'd just model that part. Then go back to whiteboard and continue.An analogy is planning a road trip with a map. The way design docs most are built now, it shows the path and you start driving.  Whereas my bosses whiteboard maps \"over-planned\" where you'd stop for fuel, attraction hours, docs required to cross border, budget $ for everything, emergency kit, Plan A, Plan B.Super tedious, but way better than using throwaway code.  Not over-planning feels lazy to me nowSure, everyone has a plan until you get punched in the mouth; however, that saying applies to war, politics, negotiations, but not coding.\n \nreply",
      "> however, that saying applies to war, politics, negotiationsIt\u2019s not even an argument against planning. You\u2019d be a fool to go to war without a plan. The point of the saying is that you\u2019d be a fool not to tear up your plan and start improvising as soon as it stops working.\n \nreply",
      "Yes I have to second that. MLJ.jl is also written by a mathematician and the API is excellent. Truly well thought-out.(If you think \u201cwhy does MLJ.jl have so few stars?\u201d please keep in mind that this library was written for the Julia language and not for Python. I honestly don\u2019t think the library is the cause of low popularity. Just wrong place wrong time.)\n \nreply",
      "First you have to have smart people who will be able to foresee design issues.That\u2019s a bit uncharitable but following this line of thought - you also need those smart people to be confident and communicative.\n \nreply",
      "And for them to be listened to, what is independent on how well they communicate; and for them to be aligned with the most powerful stakeholder, what is almost never the case; and for no big change to happen in an uncontrolled way, what powerful people nowadays seem intent on causing all the time.\n \nreply"
    ],
    "link": "https://softwaredoug.com/blog/2024/12/14/throwaway-prs-not-design-docs",
    "first_paragraph": "We imagine software efforts that go through a clean, neat flow:We write a design doc. Then make small incremental changes in a PR to rollout the functionality. Our git histories look clean and orderly. Like a steady march of progress.\u201cWere it so Easy\u201d\nThe Arbiter, The Halo SeriesIt\u2019s a bit of a delusion that we can take a design doc and go straight to clean gradual rollout of every step. I can guarantee you that once you start coding you\u2019ll eat your design docs words.\u201cNo plan survives contact with the enemy\u201d\n EisenhowerWe need to hack to find the design. We need to make a giant mess of code to make an actual plan. Making a giant mess then figuring out how to pick up the pieces might be the most efficient form of design out there.I prefer a bit of a different design methodology-coding binges. Basically go through the following steps.Important in this methodology is a great deal of maturity. Can you throw away your idea you\u2019ve coded or will you be invested in your first solution? A major"
  },
  {
    "title": "Fast LLM Inference From Scratch (using CUDA) (andrewkchan.dev)",
    "points": 162,
    "submitter": "homarp",
    "submit_time": "2024-12-14T16:02:48 1734192168",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=42417857",
    "comments": [
      "Hi, I'm the author. Thanks for sharing, was great to wake up to my blog post on the front page! Would love to hear any feedback or if I missed anything.\n \nreply",
      "This is great thank you!Does any one know of something similar in python? I want to share with my team something similar to this that goes into  (almost) everything (at least conceptually) needed to efficiently serve an LLM.It doesn\u2019t actually need to be performant mind you (it\u2019s in python) I just need something \u201cconceptually complete\u201d while being more \u201ctutorial style\u201d and concise than vLLM codebase\n \nreply",
      "Excellent, amazing article.To the author, if you're lurking here, I have a tangential question- how long did it take you to write this article? From first line of code to the last line of this post?As someone who works in GPGPU space, I can imagine myself writing an article of this sort. But the huge uncertainty around time needed has deterred me so far.\n \nreply",
      "I don't think this code can make use of the tensor cores, or the wgmma instructions that you typically need to get peak performance out of them.Programming these is a nightmare as you need to have several in flight concurrently for peak performance.Perhaps you don't need the extra flops as you end up bandwidth bound?Regardless the good thing about the code in the blog though is it'll probably work pretty well for other accelerators, if you port it to HIP or similar. If you use wgmma I'm not sure it'll even be portable across Nvidia generations.\n \nreply",
      "For latency-bound inference (i.e. one request) you don't need tensor-cores since all your operations are just matrix vector multiplications.\n \nreply",
      "Good point yes. That explains why he's getting performance similar to the leading frameworks. Those tensor operations are helpful for training or for throughput-optimised batched inference but not really for a batch size of one.\n \nreply",
      "I actually didn't know that. I'm in the space as a hobbyist and I had a vague understanding that tensor cores are essential for reaching peak performance, but can only work for certain operations like dense matrix-matrix multiplication. It was on my list to investigate whether they could be used to further improve single-batch decoding - makes sense that they don't help when it's all matrix-vector.\n \nreply",
      "I wonder how does the perf in tokens/second compares to my version of Mistral: https://github.com/Const-me/Cgml/tree/master/Mistral/Mistral...BTW, see that section of the readme about quantization: https://github.com/Const-me/Cgml/tree/master?tab=readme-ov-f...\n \nreply",
      "Isn\u2019t __shfl_down not recommended these days because of warp synchronization issues?\n \nreply"
    ],
    "link": "https://andrewkchan.dev/posts/yalm.html",
    "first_paragraph": "\n      This post is about building an LLM inference engine using C++ and CUDA from scratch without libraries.\n    \n      Why? In doing so, we can learn about the full stack of LLM inference - which is becoming increasingly importantEspecially as inference compute becomes a new axis with which AI models scale,\n      and models are increasingly deployed locally to devices on the edge. - from CUDA kernels to model architecture, and get a real sense of how different optimizations affect inference speed.\n      And one of the most important use cases is running fast on a single prompt on consumer devices.\n    \n      That's what we'll focus on: building a program that can load weights of common open models and do single-batch inference on them on a single CPU + GPU server, and iteratively \n      improving the token throughput until it surpasses llama.cpp. Readers should have basic familiarity with large language \n      models, attention, and transformers. The full source code is available on "
  },
  {
    "title": "Best-of-N Jailbreaking (arxiv.org)",
    "points": 23,
    "submitter": "flyingpumba",
    "submit_time": "2024-12-14T05:18:52 1734153532",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=42414864",
    "comments": [
      "i've never seen such a complicated author list as far as \"equal contribution\" and \"equal advising\"\n \nreply",
      "best of n paper\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2412.03556",
    "first_paragraph": "The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy.Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Show HN: SmartHome \u2013 An Adventure Game (steviep.xyz)",
    "points": 167,
    "submitter": "scyclow",
    "submit_time": "2024-12-15T17:05:30 1734282330",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=42424508",
    "comments": [
      "This is a really impressive rendition of our corporate hellscape. It reminds me of games like cart life in that I appreciate what it's doing, but even though the frustration is intended it still makes it really unpleasant to play. I lasted until I needed to figure out how to pay the ISP bill before I threw in the towel.\n \nreply",
      "Oh god this is vile.  Phenomenal job.If I can add a recommendation: the app search is far too fast.  Add a random delay to the results after each letter is entered, and let them arrive out of order.\n \nreply",
      "ooo, good idea. I might build that in.\n \nreply",
      "Or build the China version that asks you for an SMS confirmation every time you click anything in the phone.\n \nreply",
      "I had the pleasure of trying this out before it was released. I think it's one of those things I can imagine might have a high bounce rate, but I would really like to encourage anyone checking it out to really give it at least 10-15 minutes. There are so many details and easter eggs and the frustration that's baked into the game design compounds quite nicely given some time.EDIT: it's also more rewarding if you delay looking under the hood for a while.\n \nreply",
      "I stayed at an airbnb with all of this smarthome crap once. The game is accurate at reproducing the experience. I give it a full negative 10 out of 10.\n \nreply",
      "The reminds me a lot of a dark room! Awesome game thanks for the adventure!\n \nreply",
      "I regret there is not a merciful option like \"wait for death in a corner\".Great job! I had fun and got bored to death at the same time.\n \nreply",
      "Try calling 1-800-666-0000. They might be able to help.\n \nreply",
      "This is too realistic. I became annoyed after 5 minutes.\n \nreply"
    ],
    "link": "https://smarthome.steviep.xyz",
    "first_paragraph": ""
  },
  {
    "title": "The Antikythera mechanism \u2013 254:19 ratio (leancrew.com)",
    "points": 90,
    "submitter": "082349872349872",
    "submit_time": "2024-12-15T18:46:48 1734288408",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=42425139",
    "comments": [
      "I\u2019m just here to fulfill the Hacker News rule that any post mentioning the Antikythera Mechanism must have a comment linking the excellent Clickspring build videos.https://youtube.com/playlist?list=PLZioPDnFPNsHnyxfygxA0to4R...\n \nreply",
      "Thank you!  I was waiting for it so I could click on it again. :-)\n \nreply",
      "> My sketch of the Sun-Earth-Moon system assumes a heliocentric solar system, something that wasn\u2019t known to the Greeks of the first century BCE.I am not sure this is entirely accurate. According to Wikipedia:> The notion that the Earth revolves around the Sun had been proposed as early as the 3rd century BC by Aristarchus of Samos,[1] who had been influenced by a concept presented by Philolaus of Croton (c. 470 \u2013 385 BC). In the 5th century BC the Greek philosophers Philolaus and Hicetas had the thought on different occasions that the Earth was spherical and revolving around a \"mystical\" central fire, and that this fire regulated the universe.[2] In medieval Europe, however, Aristarchus' heliocentrism attracted little attention\u2014possibly because of the loss of scientific works of the Hellenistic period.[b]Source: https://en.m.wikipedia.org/wiki/HeliocentrismContrary to the recent revisionist version of the \u201cdark ages\u201d, they were pretty dark if you were a Greek of the Hellenistic period, an Athenian 5th century BC or a Roman the next 500-800 years. Loss of philosophical (locked in monasteries) and technical knowledge is common and Heliocentrism or the blood circulation system (Egyptians during the Ptolemy era had knowledge comparable to the one we will acquire again 18th century) are two prime examples.\n \nreply",
      "You can argue semantics on this, but \"wasn\u2019t known to the Greeks of the first century BCE\" is basically accurate, or certainly \"accurate enough\" for this context. All of the heliocentric writers were basically little more than \"this idea some guy had\", and geocentric views as described by e.g. Aristotle were dominant.\n \nreply",
      "I was living in Athens, and visiting the museums, and I had no idea - I walked around a corner in the National and pow - the Mechanism, THE Mechanism, was there, right in front of me.WOW.Also, Elgin marbles need to be returned.  Parthenon is defaced by their absence.\n \nreply",
      "They\u2019re not putting them outside again; they\u2019d be inside in the Acropolis museum.  The Parthenon (blown up as it is) would still be missing them.\n \nreply",
      "If you're in Athens, the Herakleidon museum has not only an exhibit about this, but about all sorts of other advanced Greek technology: coin-operated vending machines, drink-serving robots, water-powered telegraphs, etc. While this specific device may (or may not) have been a one-off, it's undeniable that ancient Greece was basically the real-life version of a Steampunk-based society. (With the caveat that ancient Persia probably had similar technology at one point also, but most of that has since been destroyed by the British and others throughout history.)\n \nreply",
      "> water-powered telegraphsThis phrasing oversells it a bit too much: The water wasn't a power source and there was no long distance movement of it.They signaled between users with the light of a burning handheld torch, and the duration of the light corresponded to predefined messages.Water was used at each end for independent stopwatches, to measure the duration of the light. It's easy to imagine an equivalent system using sand hourglasses.https://en.m.wikipedia.org/wiki/Hydraulic_telegraph#Greek_hy...\n \nreply",
      "Oh damn it. Missed that one entirely. I will have to come back again.\n \nreply",
      "Same!!!I just asked chatgpt \u201cBased on what you know about me, what do you think I would be interested in seeing in Athens?\u201dThe Herakleidon Museum was third on the list.  May have to try this again in the future.\n \nreply"
    ],
    "link": "https://leancrew.com/all-this/2024/12/the-antikythera-mechanism/",
    "first_paragraph": "\n\nPrevious post December 15, 2024 at  9:45 AM by Dr. DrangA recent episode of the In Our Time podcast is about the Antikythera mechanism, which you\u2019ve probably heard of. It\u2019s a bronze device that dates back to the first century BCE and was found by divers on an ancient shipwreck off the Greek island of Antikythera in the early 1900s. It\u2019s main purpose, as determined by slow study over the past hundred years, was to track the positions of the \u201cplanets,\u201d which in its time were the Sun, Moon, Mercury, Venus, Mars, Jupiter, and Saturn.The show is very good, covering both the mechanism itself and the development of what\u2019s known about it. There\u2019s also some fun speculation on who made it and who owned it. This post expands on a couple of numerical/astronomical/calendrical things mentioned by the guests.At about the 32-minute mark, Mike Edmunds talks about a large wheel in the mechanism that has, he says, 233 teeth, equal to the number of lunar months in the Saros cycle, which is used to predi"
  },
  {
    "title": "Coercing a Magic MIFARE credential into being an iPhone-compatible NFC tag (ewpratten.com)",
    "points": 28,
    "submitter": "ewpratten",
    "submit_time": "2024-12-14T00:44:55 1734137095",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=42413656",
    "comments": [
      "This is cool, but the most interesting part is the part that requires investigation, i.e., what do the compatibility tools write to the card to make it iOS-compatible? I've done some work with iOS NFC, but not enough to have experienced the undocumented quirks.\n \nreply",
      "In my experience, the notification you see at the end of article was used to advertise your brand/site.The number of obnoxious people/guerilla ad companies that printed and programmed NFC tags and stuck them in random places was way too high. In some cases, they would replace the businesses QR code with the NFC tag. In some cases that NFC notification would pop up instead of the business\u2019s menu. Quite annoying.Then there was a case where the person stuck the raw tags _under the table_, so putting your phone down in random places would spam this notification on your screen.\n \nreply",
      "Has anyone managed the opposite feat, i.e. made a piece of hardware that can recognise an iPhone over NFC? I'm trying to design control system that relies on you touching your phone to various hardware readers (with some kind of unchanging key/identity), but off-the-shelf MFRC522 etc don't seem to work, and the 'official' Apple way requires an NDA...\n \nreply",
      "NFC has been opened up more in iOS 16 and 17.Card emulation is a thing now: https://developer.apple.com/documentation/corenfc/cardsessio... (edit: only in the EU)And iOS as well as iPadOS now also support USB smart card readers. iPads can actually use them to access NFC FIDO tokens. (Why iPads don't have native NFC is completely beyond me, there are so many obvious use cases for it)\n \nreply",
      "so cool! anyone have any recs for cheap cards (or hardware) that is scannable on iphones?\n \nreply",
      "The cheapest RFID/NFC \"card\" tag is an expired tap-to-use transit ticket. It has a unique ID that can be used to trigger an iOS Shortcut to take any action, e.g. speak an audio description, open URL, open app, etc.For purchase, there are many form factors: https://store.gototags.com/nfc-tagsOn Amazon, search NTAG215.\n \nreply",
      "If your city doesn't use NFC transit, you can also buy a box of blank cards. They have unique IDs like transit tickets, even without being programmed.\n \nreply",
      "Many of my cards are just repurposed from other things. Lots of hotel keycards can be rewritten to open URLs on phones for example.I actually have a hotel keycard taped to my washing machine to do some laundry-based automation with my phone. Maybe I should write about that sometime..\n \nreply",
      "Have you thought about getting one implanted?\n \nreply",
      "https://www.amazon.com/dp/B0C49SVTCTare the tags I have, and https://apps.apple.com/us/app/nfc-tools/id1252962749 is the app I used (which is the same as the one used in the post).\n \nreply"
    ],
    "link": "https://ewpratten.com/blog/iphone-mifare-magic",
    "first_paragraph": ""
  },
  {
    "title": "Plato: An educational computer system from the '60s shaped the future (arstechnica.com)",
    "points": 7,
    "submitter": "rbanffy",
    "submit_time": "2024-12-14T12:57:12 1734181032",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://arstechnica.com/gadgets/2023/03/plato-how-an-educational-computer-system-from-the-60s-shaped-the-future/",
    "first_paragraph": "\n      Forums, instant messaging, and multiplayer video games all started here.\n    Bright graphics, a touchscreen, a speech synthesizer, messaging apps, games, and educational software\u2014no, it's not your kid's iPad. This is the mid-1970s, and you're using PLATO.Far from its comparatively primitive contemporaries of teletypes and punch cards, PLATO was something else entirely. If you were fortunate enough to be near the University of Illinois Urbana-Champaign (UIUC) around a half-century ago, you just might have gotten a chance to build the future. Many of the computing innovations we treat as commonplace started with this system, and even today, some of PLATO's capabilities have never been precisely duplicated. Today, we'll look back on this influential technological testbed and see how you can experience it now.Don Bitzer was a PhD student in electrical engineering at UIUC in 1959, but his eye was on bigger things than circuitry. \u201cI'd been reading projections that said that 50 percent"
  },
  {
    "title": "Show HN: I built an embeddable Unicode library with MISRA C conformance (railgunlabs.com)",
    "points": 78,
    "submitter": "hgs3",
    "submit_time": "2024-12-15T15:41:27 1734277287",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=42423988",
    "comments": [
      "On https://railgunlabs.com/unicorn/manual/misra-compliance/, I think you will want to fix a typo in  1.2    Required    Compliant (verified by compiling with Clang's -pdentic flag)\n                                                                   ^^^^^^^^\n\nOr am I too pedantic?\n \nreply",
      "This is the most ironic typo I've ever made. Thanks for the catch. I've corrected it.\n \nreply",
      "A couple more suggestions.List the platforms (& compilers) that you've tested on.Compare (pros/cons) against other Unicode libs (like others have done elsewhere in this thread, i.e. https://news.ycombinator.com/item?id=42424637 and https://news.ycombinator.com/item?id=42424638)\n \nreply",
      "Thanks for the suggestions. I think a comparison table would be useful, but I want to make sure I do it right since I'd be comparing my work to someone else's.As for the compilers, I\u2019ve tested the library with GCC, Clang, and MSVC, and with the -pedantic flag like the GP mentioned. The library should build with any standard-compliant C99 compiler.\n \nreply",
      "This is commercial only. Free and small is my safeclib, which does about half of it. ICU is not usable on small devices, and also pretty slow. It's much faster to use precomputed tables per algorithm, such as here or in safeclib. libunistring is also extremely slow. This was tried for grep and failed.\n \nreply",
      "> This is commercial only. Free and small is my safeclibIs it me or does this feel a bit weird? It seems like you're using the comments section here to self-advertise for exposure.I read it like \u2014 \"businesses can't use this without paying the OP, however, if you're a business you can get 50% of the way there by using _my_ library, and you don't even have to pay me!\". It comes off incredibly rude to try to undercut the OP like this.\n \nreply",
      "> This is commercial only.You can use Unicorn for non-commercial use [1], but yes, for commercial use you need to buy a license.> It's much faster to use precomputed tables per algorithmYou're absolutely right about using precomputed tables per algorithm. That is the secret to the library's speed.> Free and small is my safeclib, which does about half of it.I like safeclib! It's nice to hear from the author. It's worth distinguishing that safeclib is a safer string library whereas Unicorn is a Unicode algorithms library, not a string library.[1] https://github.com/railgunlabs/unicorn/blob/master/LICENSE\n \nreply",
      "Well, a string is unicode nowadays. And for sure not just a zero-terminated blob. That would be a buffer. Only the Linux kernel still holds this invalid view.So every string library needs at least a compare function to find strings, with all the variants of same graphemes. Which leads us to NFC normalization for a start. Upcase tables and wordlength tables are also needed.\n \nreply",
      "This is not a comment about open/closed-source software and/or licensing models.Projects like this never fail to impress me vis-a-vis source obfuscation. The 'generate.pyz' is an interesting twist on the usual practice.\n \nreply",
      "#  You may not reverse engineer, decompile, disassemble, or otherwise attempt\n    #  to derive the source code or underlying structure of this script\n\nThis prohibition is void in certain relevant jurisdictions, for any publicly available product.\n \nreply"
    ],
    "link": "https://railgunlabs.com/unicorn/",
    "first_paragraph": "Embeddable Unicode\u00ae algorithms.Unicorn implements the most essential Unicode algorithms:\n\nUnicorn is fully customizable.\nYou can choose which Unicode algorithms and character properties to include.\nYou can even choose which Unicode character blocks to include!\n\nUnicorn does not require an FPU or 64-bit integers.\nIt is written in C99 and only requires a few features from libc which are listed in following table.\n\nUnicorn honors all Required, Mandatory, and most Advisory rules defined by MIRSA C:2012.\nDeviations are documented here.\nYou are encouraged to audit Unicorn and verify its level of conformance is acceptable.\n\nUnicorn is thread-safe except for the following caveats:\n\nAll operations in Unicorn are atomic.\nThat means either an operation occurs or nothing occurs at all.\nThis guarantees errors, such as out-of-memory errors, never corrupt internal state.\nThis also means if an error occurs, like an out of memory error, then you can recover (free up memory) and try the operation again."
  },
  {
    "title": "Maximum likelihood estimation and loss functions (rish-01.github.io)",
    "points": 51,
    "submitter": "snprajwal",
    "submit_time": "2024-12-15T18:07:06 1734286026",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=42424879",
    "comments": [
      "Somehow I've never found an explanation for MLE that's intuitive and suitable for someone who didn't take graduate-level statistics already. I'm 100% on board with the introduction (MSE and cross-entropy make total intuitive sense; you can see how they penalize 'wrongness' to an increasing degree) but in the next paragraph we jump right to:>Let pmodel(x;\u03b8) be a parametric family of distributions over a space of parameters ...and it's straight to the grad-level textbook stuff that breezily assumes familiarity with advanced mathematical notation.One of the reasons I loved Andrew Ng's machine learning course so much is that it eased you into understanding the notation, terminology, and signposted things like \"hey this is really important\" vs. \"hey this is just a weird notational quirk that mathematicians have, don't worry about it too much.\"\n \nreply",
      "This notation doesn't require graduate-level statistics knowledge, it's more like stuff that would be covered in a first mathematical course on probability and statistics. It's totally practical to learn this stuff on your own from videos, books, and PDFs. First you need to get a solid conceptual grasp on probability distributions and their associated concepts like random variables, conditional probability, and joint probability. Then you'll be ready to learn some mathematical statistics and follow along with all the notation.Note that you don't need to go deep into measure-theoretic probability or any of that stuff that requires more advanced prior education in math.\n \nreply",
      "This reminds me that David Mackay\u2019s book and his lectures are so excellent on these topics.\n \nreply",
      "To bring things full circle: the cross-entropy loss is the KL divergence. So intuitively, when you're minimizing cross-entropy loss, you're trying to minimize the \"divergence\" between the true distribution and your model distribution.This intuition really helped me understand CE loss.\n \nreply",
      "Cross-entropy is not the KL divergence. There is an additional term in cross-entropy which is the entropy of the data distribution (i.e., independent of the model). So, you're right in that minimizing one is equivalent to minimizing the other.https://stats.stackexchange.com/questions/357963/what-is-the...\n \nreply",
      "Excellent post\n \nreply",
      "The next step is ELBO \u2014 the evidence lower bound.\n \nreply"
    ],
    "link": "https://rish-01.github.io/blog/posts/ml_estimation/",
    "first_paragraph": "When I started learning about loss functions, I could always understand the intuition behind them. For example, the mean squared error (MSE) for regression seemed logical\u2014penalizing large deviations from the ground-truth makes sense. But one thing always bothered me: I could never come up with those loss functions on my own. Where did they come from? Why do we use these specific formulas and not something else?This frustration led me to dig deeper into the mathematical and probabilistic foundations of loss functions. It turns out, the answers lie in a concept called Maximum Likelihood Estimation (MLE). In this blog, I\u2019ll take you through this journey, showing how these loss functions are not arbitrary but derive naturally from statistical principles. I\u2019ll start by defining what Maximum Likelihood Estimation (MLE) is, followed by the intricate connection between Maximum Likelihood Estimation (MLE) and Kullback-Leibler (KL) divergence. To conclude this article, I show how loss functions "
  },
  {
    "title": "A visual proof that a^2 \u2013 b^2 = (a + b)(a \u2013 b) (futilitycloset.com)",
    "points": 423,
    "submitter": "beardyw",
    "submit_time": "2024-12-15T14:01:37 1734271297",
    "num_comments": 154,
    "comments_url": "https://news.ycombinator.com/item?id=42423409",
    "comments": [
      "If you like this, there is a whole book full of visual proofs [1]. See also wikipedia [2].A few years ago I re-drew a bunch of these in latex with my PhD advisor and another colleague [3]. We planned to print them as posters and hang them for a Pi day event that unfortunately never happened because the pandemic broke out.[1] https://www.amazon.com/Proofs-without-Words-Exercises-Classr...[2] https://en.m.wikipedia.org/wiki/Proof_without_words[3] https://www.antonellaperucca.net/didactics/proof-without-wor...\n \nreply",
      "This reminds me of this video about why we need to be very careful when inspecting visual proofs: https://www.youtube.com/watch?v=VYQVlVoWoPY (it includes a \"proof\" that pi equals exactly 4).In this case, as someone else pointed out below, this proof has unjustified assumptions in it (at least it assumes that b < a).\n \nreply",
      "> we need to be very careful when inspecting visual proofsThis is not a visual proof but a nice visualization, like a written explanation that is not a proof.For an actual novel proof, nobody would imagine that they could eyeball it for a few minutes and conclude it was complete, correct, and consistent - maybe with the exception of professional mathematicians examining for simple proofs. You might eyeball it and follow its logic and not see any immediate flaws, but that's different.\n \nreply",
      "My geometry teacher in 9th grade was adamant that we couldn't assume lengths and angles in diagrams except where explicitly labeled. In particular, he said that we should never assume that a diagram is drawn to scale; if the diagram happened to depict a quadrilateral as a square, unless something stated that it was a square (or we had sufficient information to determine that it was), we should treat it as an unidentified quadrilateral and proceed only with the actual information provided or else he would \"take more points off than the question was worth\" on tests. On one of our quizzes, he did provide a diagram that looked like a kite (two pairs of adjacent sides each with the same length but different length than the other pair) but listed the angles in a way that could only work for a parallelogram that was not a kite, and he made good on his word to take off extra points for people who misidentified it as a kite.\n \nreply",
      "There is no problem with the proof except the assumption that the value in the limit is the same as the value at infinity. If we simply define pi(n) as a function from N U {inf}, which gives the value that \"pi\" takes at the nth step of the process, and pi(inf) as the value that it actually takes for the circle, then we simply have a function where lim n->inf pi(n) \u2260 pi(lim n->inf). For all finite n, it equals 4, and then at infinity it equals 3.1415... .There are ways to reformulate the above so that \"infinity\" isn't involved but this is the clearest way to think of it. It isn't much different than the Kronecker delta function delta(t), which is 1 at t=0 and 0 elsewhere. We have lim t->0 delta(t) \u2260 delta(lim t->0 t).\n \nreply",
      "> at least it assumes that b < aBut you can fairly assume that wlog.\n \nreply",
      "For those thinking this doesn't hold, note that \"wlog\" means without loss of generality (having numerous examples in proofs).\n \nreply",
      "A good example of the shortening of proofs. Like \"Observe that... {unproven thing}\"It is not necessarily bad but explains why every step isn't always proven. To avoid tedium and long papers.\n \nreply",
      "No\n \nreply",
      "Why not? Either b < a, b = a or b > a. If b = a both sides are zero and the result is trivial. If b > a just swap the names of a and b and multiply both sides by -1.\n \nreply"
    ],
    "link": "https://www.futilitycloset.com/2024/12/15/tidy-2/",
    "first_paragraph": "A visual proof that a2 \u2013 b2 = (a + b)(a \u2013 b).Sophie Germain wrote, \u201cIt has been said that algebra is but written geometry and geometry is but diagrammatic algebra.\u201dEnter your email address to receive notifications of new blog posts by email.\n\n\t\t\t\t\t\t\tEmail Address\t\t\t\t\t\t\n\n\n\n\n\n\n \n\t\t\t\t\t\t\tSubscribe\t\t\t\t\t\t\nFutility Closet is a collection of entertaining curiosities in history, literature, language, art, philosophy, and mathematics, designed to help you waste time as enjoyably as possible.You can read Futility Closet on the web, subscribe by RSS, or sign up to receive a free daily email \u2014 see \u201cSubscribe by Email\u201d in the sidebar.\u00a9 Futility Closet 2005-2024"
  },
  {
    "title": "Ask HN: How do you find part time work?",
    "points": 188,
    "submitter": "leros",
    "submit_time": "2024-12-15T19:20:19 1734290419",
    "num_comments": 65,
    "comments_url": "https://news.ycombinator.com/item?id=42425339",
    "comments": [
      "Long time freelancer here (decades). I've gotten 99% of my contracts through word-of-mouth. You will get better long-term results than simply grinding through job boards (which you should still do--you never know). You will want to do two things:1) Cultivate your existing network. Just the other day I reconnected with a friend I haven't seen in several decades. Guess what? He just so happened to be a software dev, and needed another dev to help him with a project. This is just one of many, many stories I have. Call or email people to see how they're doing. Even better, meet up with them for coffee or a meal. This doesn't have to be mercenary; you're probably already doing those things. But start reaching deeper into your network.2) Build your your network by meeting others in your field and/or potential customers in person (e.g. at user groups, meetups, tech talks, etc.). Talk about what you do and love to do. Presumably that involves your skills that you want to get work in.Finally, be consistent and reliable and communicate clearly.As for selling yourself when you're more of a generalist, I wouldn't worry too much about this yet. The key will be when you're presented with a job opportunity that leans in one direction (e.g. 90% dev, 10% management), and you'll have to decide how far you're willing to bend to fit it. But right now you're just getting the word out.\n \nreply",
      "It is truly an extravert's world.What you just described sounds like torture to me.\n \nreply",
      "Networking is not just for extroverts. One on one time with friends, and small group networking is still valuable networking.\n \nreply",
      "I am from a total different field but:  I've gotten 99% of my contracts through word-of-mouth.Yes, no brainer. But how to get the first contract?I exploited myself, my client vastly underpaid but I had my first client.\n \nreply",
      "Networking, honestly. There is no \"go do this, go buy this, etc\" advice. You've never had a friend/colleague who needed contracting work done on their house? Or who was helping another friend develop a Wordpress site for their business? Or a person who knew a lawyer/doctor/plumber that could solve your problem?That's networking. You start doing work for people around you and/or at conventions/social events for networking and then start cultivating long term contacts that continue giving work or referring others.\n \nreply",
      "> Call or email people to see how they're doing. Even better, meet up with them for coffee or a mealI like this idea but it feels inauthentic.  Can you share examples of how you broach this?\n \nreply",
      "The key is to mean it.  Then it's not inauthentic.\n \nreply",
      "\u201cHey it\u2019s been forever, grab a coffee and share what we\u2019re both working on and how we might help each other?\u201d\n \nreply",
      "Software dev, curious about freelancing here. Just recently deleted my LinkedIn profile out of being disappointed with the value it brought to me over several years of using it. Instead, I now note down relevant contacts in a private \u201cCRM\u201d.Do you think it is important to have a public facing profile of yourself and your work history for attracting new clients or is a broad in person network more relevant?\n \nreply",
      "Some new people you meet will prefer to see who you are by looking you up online. At least to me, someone's online presence is a testimony that the person is who they claim to be when it comes to their experience.\n \nreply"
    ],
    "link": "item?id=42425339",
    "first_paragraph": ""
  },
  {
    "title": "Drag and Drop Images into Bevy 0.15 on the web (rustunit.com)",
    "points": 31,
    "submitter": "todsacerdoti",
    "submit_time": "2024-12-12T09:55:41 1733997341",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://rustunit.com/blog/2024/12-10-rust-web-drag-drop-image/",
    "first_paragraph": "In this post, we talk about how to integrate web native APIs via WASM with Bevy.We utilize the recently released bevy_channel_trigger crate, wasm-bindgen and gloo.If you want to play with the code and tinker with it, you can find it on GitHub.In this example, we want to allow the user to drop a PNG image into our Bevy app running in the Browser. The app should load the image into the Bevy Asset machinery and display it like any other image file. See the animation to the right visualizing this.The steps to making this work are:Let's dive into the details of each of these steps.The first thing we need to do is to register the event listeners for dragover and drop on the DOM element we want to receive the drop events. We will be using wasm-bindgen and gloo to be able to do this right from our rust code:You can find the full function here but the important part is that the code is essentially 1:1 from how you would solve this in vanilla javascript translated to rust while looking at the we"
  },
  {
    "title": "Should you ditch Spark for DuckDB or Polars? (milescole.dev)",
    "points": 93,
    "submitter": "RobinL",
    "submit_time": "2024-12-14T20:28:44 1734208124",
    "num_comments": 65,
    "comments_url": "https://news.ycombinator.com/item?id=42419224",
    "comments": [
      "I went through this trade off at my last job. I started off migrating my adhoc queries to duckdb directly from delta tables. Over time, I used duckdb enough to do some performance tuning. I found that migrating from Delta to duckdb's native file format provided substantial speed wins.The author focuses on read/write performance on Delta (makes sense for the scope of the comparison). I think if an engineer is considering switching from spark to duckdb/polars for their data warehouse, they would likely be open to data formats other than Delta, which is tightly coupled to the spark (and even more so to the closed-source Databricks implementation). In my use case, we saw enough speed wins and cost savings that it made sense to fully migrate our data warehouse to a self managed duckdb warehouse using duckdb's native file format.\n \nreply",
      "Thanks for sharing, very intersting!I'm thinking the same wrt dropping Parquet.I don't need concurrent writes, which seems to me about the only true caveat DuckDB would have.Two other questions I am asking myself:1) Is there a an upper file size limit in duckdb's native format where performance might degrade?2) Are there significant performance degradations/ hard limits if I want to consolidate from X DuckDB's into a single one by programmatically attaching them all and pulling data in via a large `UNION ALL` query?Or would you use something like Polars to query over N DuckDB's?\n \nreply",
      "I can offer my experience with respect to your two questions, though my use case is likely atypical.1) I haven't personally run into upper size limits to the point of non linear performance degradation. However, some caveats to that are (a) most my files are in the range of 2-10gb with a few topping out near 100gb. (b) I am running a single r6gd metal as the primary interface with this which has 512 gb of ram. So, essentially, any one of my files can fit into ram.Even given that setup, I will mention that I find myself hand tuning queries a lot more than I was with Spark. Since duckdb is meant to be more lightweight the query optimization engine is less robust.2) I can't speak too much towards this use case. I haven't had any occasion to query across duckdb files. However, I experimented on top of delta lake between duckdb and polars and never really found a true performance case for polars in my (again atypical use case) set of test. But definitely worth doing your own benchmarking on your specific use case :)\n \nreply",
      "Thanks for sharing! :)\n \nreply",
      "Interesting.  So what does that look like on disk? Possibly slightly naively I'm imagining a single massive file?\n \nreply",
      "Great post but it seems like you still rely on Fabric to run Spark NEE. If you're on AWS or GCP, you should probably not ditch Spark but combine both. DuckDB's gotcha is that it can't scale horizontally (multi-node), unlike Databricks. A single node can get you as far as you or can rent 2TB memory + 20TB NVME in AWS, and if you use PySpark, you can run DuckDB until it doesn't scale with its Spark integration (https://duckdb.org/docs/api/python/spark_api.html) and switch to Databricks if you need to scale out. That way, you get the best of the two worlds.DuckDB on AWS EC2's price performance rate is 10x that of Databricks and Snowflake with its native file format, so it's a better deal if you're not processing petabyte-level data. That's unsurprising, given that DuckDB operates in a single node (no need for distributed shuffles) and works primarily with NVME (no use of object stores such as S3 for intermediate data). Thus, it can optimize the workloads much better than the other data warehouses.If you use SQL, another gotcha is that DuckDB doesn't have advanced catalog features in cloud data warehouses. Still, it's possible to combine DuckDB compute and Snowflake Horizon / Databricks Unity Catalog thanks to Apache Iceberg, which enables multi-engine support in the same catalog. I'm experimenting this multi-stack idea with DuckDB <> Snowflake, and it works well so far: https://github.com/buremba/universql\n \nreply",
      "> A single node can get you as far as you or can rent 2TB memory + 20TB NVME in AWSWhat I'm a little curious about with these \"single node\" solutions - is redundancy not a concern with setups like this? Is it assumed that you can just rebuild your data warehouse from some form of \"cold\" storage if you lose your nvme data?\n \nreply",
      "Exactly. In this case, the cold storage is in S3; all the intermediate data (TEMP tables) is written to NVME, and then the final tables are created in S3 & registered in your catalog. AWS recently released S3 Tables, which aims to make the maintenance easier with Apache Iceberg as well: https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tab...You can use spot instances if you can afford more latency or might prefer on-demand instances and keep them warm if you need low latency. Databricks (compute) and Snowflake (warehouse) do that automatically for you for the premium price.\n \nreply",
      "Or just use Clickhouse.\n \nreply",
      "Clickhouse has excellent performance and it scales out indeed. Unfortunately, it's hard to deploy and maintain a Clickhouse cluster and its catalog features are pretty basic compared to Databricks and Snowflake.\n \nreply"
    ],
    "link": "https://milescole.dev/data-engineering/2024/12/12/Should-You-Ditch-Spark-DuckDB-Polars.html",
    "first_paragraph": "\n      \n      \n      December 12, 2024\n    \n  \n\n\n32 minute read\n\nThere\u2019s been a lot of excitement lately about single-machine compute engines like DuckDB and Polars. With the recent release of pure Python Notebooks in Microsoft Fabric, the excitement about these lightweight native engines has risen to a new high. Out with Spark and in with the new and cool animal-themed engines\u2014 is it time to finally migrate your small and medium workloads off of Spark?Before writing this blog post, honestly, I couldn\u2019t have answered with anything besides a gut feeling largely based on having a confirmation bias towards Spark. With recent folks in the community posting their own benchmarks highlighting the power of these lightweight engines, I felt it was finally time to pull up my sleeves and explore whether or not I should abandon everything I know and become a DuckDB and/or Polars convert.While performance can be the most important driver in selecting an engine, the reality is that performance alone"
  },
  {
    "title": "Crystal Ball Trading Game (elmwealth.com)",
    "points": 234,
    "submitter": "EvgeniyZh",
    "submit_time": "2024-12-15T07:22:37 1734247357",
    "num_comments": 143,
    "comments_url": "https://news.ycombinator.com/item?id=42422077",
    "comments": [
      "A couple of things stand right out here: a) There's almost no time horizon here, so it's basically day trading. It's not enough to know what happens, everyone else needs to \"do the right thing\" in a short time horizon. As the article says, that doesn't happen a lot. b) WSJ isn't as good a new source as it used to be, which makes the game harder. Though occasionally, something big will happen and a one day lookback would be helpful. (Say, shorting airline stocks before 9/11...)\n \nreply",
      "> WSJ isn't as good a new source as it used to be, which makes the game harder.You don't think the front page of the WSJ would contain a decent summary of the day's business news?\n \nreply",
      "The WSJ is such a shallow shell of its former self that no, I don\u2019t trust it as far as I can throw a paper airplane made out of it. Somewhere along the line (and I have my hypothesis as many do\u2026) our news sources stopped being objective, stopped seeking truth, stopped being sourced for bringing real information to the consuming public, and instead became a kind of echo chamber for dogmatic ideology of one kind or another. I have no idea what to read any more, because it\u2019s devolved into sick \u201centertainment\u201d rather than insightful reporting. Even my beloved NPR seems on the brink of falling into this as they seek to kowtow, presumably to save their funding.\n \nreply",
      "All you can do now is to go to the source data.If you want long term trend data, it\u2019s easy to get good data for free. It\u2019ll be boring and most trends will be positive (thank our lucky stars we live in the technological era). Unless you have a real passion, you\u2019ll most likely not visit one of these sources twice (World Bank, IMF, FRED in the USA, tradingeconomics.com, the SEC)If you want short term, real-time data, you\u2019ll likely have to pay for it. The downside is that there\u2019s a deluge of data and almost all of it is useless (unless you care about up-to-the-minute prices for beans in China or whatever).The job of journalists is to mine all this info for something sensational or, failing that, spin some short term data bump into a big story.Way back in the 1940\u2019s, there was so little data out there that the WSJ could simply print all the current market events and call it a newspaper. There was so little entertainment out there, that people bought and read that paper!Information dissemination remains an unsolved problem.\n \nreply",
      "I'd have to agree. It doesn't matter WSJ, NYT, WaPo, LAT, Trib, all garbage. \nWhen I started to see grammar mistakes and typos in the NYT, I knew it was over. They were optimizing for clicks not correctness. The infotainment that started on TV and the destruction of revenue by aggregation has created an environment where largescale news as it was is not profitable (enough), but clickbait is. There may be individual journalists who care, but not a single senior editor that does. We have achieved post truth media. The news cycle is literally about UFOs.edit: I will say that's a profit opportunity both for those who can spread fake meme news and for those who can bother to see through it, but for the vast middle it is idocracy.\n \nreply",
      "Yep, in simple terms they all became tabloids. They do still hire a few real journalists to keep up appearances, but sensationalism is what sells so that\u2019s what gets the most real estate.\n \nreply",
      "I wouldn't really say the WSJ is a good summary. If I look at the front page today, most of it is topical, but generic and not really actionable information in any way:- The Drugs Young Bankers Use to Get Through the Day\u2014and Night- CEOs Want Trump to Change Course on Tariffs. He Isn\u2019t Budging.- Untangling America\u2019s Love-Hate Relationship With Corporate PowerEtc, it doesn't really tell you much anything about what's going on in the market. Yahoo Finance on the other hand is a great overview, the upcoming fed meeting is front and center, there's a market overview on the right, highlights of specific big movers, etc.\n \nreply",
      "To be fair it\u2019s also Sunday. Silly season for newspapers generally, the business newspaper in particular.\n \nreply",
      "> You don't think the front page of the WSJ would contain a decent summary of the day's business news?Not anymore?Take front page today, first \"Opinion\" title: \"The Trans Double-Mastectomy Lawsuit\" (not sure what their stance is here: they went full woke but they're toning wokism down now that Trump won). First big headline: \"The drugs young bankers use to get through the day\". \"America's love/hate relationship with corporate power\".\"How an Ivy League Police Commissioner Hunted an Ivy League Murder Suspect\"Facepalm. I mean, sure, if I was reading The Guardian in the UK or something.But how the fuck has anything of that to do with business and/or finance?That's what I see first, front page.Funnily enough the first title related to business or finance is one HN won't like: \"These 5 Wall Street Titans Thought Bitcoin Was a Fad. Here\u2019s What They Say Now\".People are making fun of the WJS, calling it the \"Woke Street Journal\". They've been more interested in pushing the ESG ideology (the one were banks in the US [and the EU] are secretly assigning scores to every US individuals depending on how \"ESG friendly\" they are), including solar (not that there's everything wrong with solar) and most of all running an anti-Trump / anti-Musk campaign, being sure Harris would won, then running actual news about Wall Street and businesses.They just lost the plot.\n \nreply",
      "When I was a subscriber (about 7 years ago) it seemed the news was left-leaning and the opinion section was heavily right-leaning.\n \nreply"
    ],
    "link": "https://elmwealth.com/crystal-ball/",
    "first_paragraph": "info@elmwealth.com+1 (888) 676-314850 S. 16th StreetSuite 1700Philadelphia, PA 19107\u00a9 2024 Elm Partners Management, LLCCookie Policy | Privacy PolicySeptember 26, 2024Featured InsightsBy Victor Haghani, James White and Jerry Bell 1Estimated reading time: 10 min.In the 1989 blockbuster Back to the Future II, time travel enables Michael J. Fox\u2019s nemesis, Biff, to become a gazillionaire by bringing an almanac with sports match outcomes back from the future. We thought it might be instructive, and certainly entertaining, to make a less fanciful version of this dream a reality \u2013 for a few lucky people.In November 2023, we ran an in-person, proctored experiment involving 118 young adults trained in finance. We called the experiment \u201cThe Crystal Ball Challenge.\u201d We gave each participant $50 and the opportunity to grow that stake by trading in the S&P 500 index and 30-year US Treasury bonds with the information on the front page of the Wall Street Journal (WSJ) one day in advance, but with sto"
  }
]