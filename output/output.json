[
  {
    "title": "Sanding UI (jim-nielsen.com)",
    "points": 235,
    "submitter": "roosgit",
    "submit_time": "2024-09-21T19:36:20.000000Z",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=41612154",
    "comments": [
      "On Safari (iPad), type something in the search bar. If you accidentally click outside of your keyboard it will deselect the bar and delete everything you typed.On Spotify the three little dots to do some action to a song have too small of a hitbox. Press even the slightest bit under the button and it will start playing the song. You'd never click there to play the song.When you consider the scale of these apps, there must be so much combined annoyance.\n \nreply",
      "Reading this on Safari on my iPad, the favicon for this tab is the Youtube logo. I genuinely have no idea how this bug came about, but it's been like that for months now.\n \nreply",
      "Safari is really, really bad when it comes to everything around favicons. The biggest annoyance being it doesn\u2019t accept updates, so things like mail status favicons don\u2019t work.\n \nreply",
      "I believe that the iPad behavior you describe is intentional. I\u2019m not sure I\u2019m right here, and there are tradeoffs. I get frustrated with it too. But I think Apple is going for clarity, lack of ambiguity.If your text remained in place but the control were not focused, what would that control then indicate? In Safari now it _always_ indicates (a) the current page, or (b) your current typing. To do otherwise would be to create a third state: \u201cUsed to be typing.\u201d Then it would no longer unambiguously indicate the current state.\n \nreply",
      "This is only tangentially related, but the Safari address bar already does not always indicate the current state, specifically when pages take time loading and when going back and forth in history. There is some kind of broken synchronization between page display and address bar, in conjunction with page loading timeouts.That being said, I would be fine with an \"address bar has been edited but not commited yet\" state. It's how most other (desktop?) browsers work and it's not an issue.\n \nreply",
      "Yep it's just pure jank. Even Apple (especially post-Jobs) can produce jank.\n \nreply",
      "I've never understood the claim that Apple's UIs are better than others'.  That wasn't true while Jobs was around, and it isn't true now.  Apple Photos, for example, loses keystrokes every time I create a new album.  That's been true for years.  And Time Machine randomly drops files from backups.  Linux isn't perfect, but my daily experience using it has been far superior for years.\n \nreply",
      "Well the Time Machine comparison doesn't really belong in a laundry list, it's basically Apple's greatest unmitigated disaster ever.\n \nreply",
      "org chart has been shipped (excel labor cost model actually)\n \nreply",
      "UI/UX standards in general are dogshit in most modern software. It's actually baffling how nobody bothers to do even the most basic polishing of their application as the OP. Like they note, clicking around for 10 or 20 minutes would reveal many imperfections, not to mention actually having testers and experiments and any semblance of a scientific design methodology.\n \nreply"
    ],
    "link": "https://blog.jim-nielsen.com/2024/sanding-ui/",
    "first_paragraph": "\n                    Controls the level of style and functionality of the site, a\n                    lower fidelity meaning less bandwidth, battery, and CPU\n                    usage. Learn more.\n                  One of the ways I like to do development is to build something, click around a ton, make tweaks, click around more, more tweaks, more clicks, etc., until I finally consider it done.The clicking around a ton is the important part. If it\u2019s a page transition, that means going back and forth a ton. Click, back button. Click, right-click context menu, \u201cBack\u201d. Click, in-app navigation to go back (if there is one). Click, keyboard shortcut to go back. Over and over and over. You get the idea.It\u2019s kind of a QA tactic in a sense, just click around and try to break stuff. But I like to think of it as being more akin to woodworking. You have a plank of wood and you run it through the belt sander to get all the big, coarse stuff smoothed down. Then you pull out the hand sander, sand a s"
  },
  {
    "title": "What Happened to the Japanese PC Platforms? (mistys-internet.website)",
    "points": 74,
    "submitter": "zdw",
    "submit_time": "2024-09-21T22:06:01.000000Z",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=41612984",
    "comments": [
      "https://j-core.org/\"What is this processor?\nThe SuperH processor is a Japanese design developed by Hitachi in the late 1990's. As a second generation hybrid RISC design it was easier for compilers to generate good code for than earlier RISC chips, and it recaptured much of the code density of earlier CISC designs by using fixed length 16 bit instructions (with 32 bit register size and address space), using microcoding to allow some instructions to perform multiple clock cycles of work. (Earlier pure risc designs used one instruction per clock cycle even when that served no purpose but to make the code bigger and exhaust the encoding space.)Hitachi developed 4 generations of SuperH. SH2 made it to the United states in the Sega Saturn game console, and SH4 powered the Sega Dreamcast. They were also widely used in areas outside the US cosumer market, such as the japanese automative industry.But during the height of SuperH's development, the 1997 asian economic crisis caused Hitachi to tighten its belt, eventually partnering with Mitsubishi to spin off its microprocessor division into a new company called \"Renesas\". This new company did not inherit the Hitachi engineers who had designed SuperH, and Renesas' own attempts at further development on SuperH didn't even interest enough customers for the result to go ito production. Eventually Renesas moved on to new designs it had developed entirely in-house, and SuperH receded in importance to them... until the patents expired.\"\n \nreply",
      "Interesting point of history\u2014 the H8 processor is the MCU that powers the original Lego Mindstorms RCX. In high school I wrote some assembly language for it when making a robot  that ran on BrickOS:https://en.m.wikipedia.org/wiki/BrickOS\n \nreply",
      "As an aside my recent trip to Japan, I hit up all the crazy gaming stores hoping to find an FM Towns or the even more rare FM Towns Marty.They looked at me like I was a three headed monkey.\n \nreply",
      "I think this somewhat misses an important nuance. Japanese PCs had to be different early on because of the complexities of the written language. All of the important characters could be handled in just a few bits (7 or 8) and low resolution in Western markets, with different fonts and character maps dropped in to support a few different alphabets.But in CJK countries, things were much harder and the entire I/O system had to be significantly more capable than what might pass for usable elsewhere. This meant larger ROMs, larger framebuffers, higher resolution displays, more complex keyboarding systems, the works. Everything was harder and more expensive for a long time. A common add-on was ROMs with Kanji (Chinese derived characters) support in the same way a person in the West might buy a new sound card or get a VGA card. Except this was just so you could use your new $1200 computer (in today's money) to write things on.Back then, given limited memory, you also ended up with a ton of different display modes that offered different tradeoffs between color, resolution, and refresh. Because of the complex character sets, these Japanese systems tended to focus on fewer colors and higher resolution while the west focused on more colors at a lower res in the same or less memory space (any fans of mode 13h?). The first PC-98 (the 9801) shipped in 1982 with 128k of RAM and a 640x400 display with special display hardware. The equivalent IBM-PC shipped with 16KB of RAM and CGA graphics which could give you a display no higher than 640x200 with 1-bit colors but was mostly used in 320x200 with 4 (terrible) colors.Even with similar base architectures, these formative differences meant that lots of the guts of the systems were laid out different to accommodate this -- especially in the memory maps.By the time \"conventional\" PCs were able to handle the character display needs (sometime in the mid-90s), they were selling in the millions of units per anum which drove down their per unit prices.The Japanese market was severely fractured and in a smaller addressable market. Per unit costs were higher, but the software was largely the same. Porting the same businessware to half a dozen platforms cost too much. So now the average user of the Japanese systems had a smaller library of software which was more or less a copy of what was on IBM PCs, on more expensive hardware -- market forces solved the rest.(btw, the FM Towns, IIR, also had specialized graphics hardware to produce arcade-like graphics with tiles and sprites and so on, making it even more different)Some of this history also informs why home computing lagged in Japan compared to the West despite having all of the other prerequisites for it to take off.graphicshttps://www.pc98.org/memory mapshttps://radioc.web.fc2.com/column/pc98bas/pc98memmap_en.htmhttps://wiki.osdev.org/Memory_Map_(x86)\n \nreply",
      "Excellent summary. A few additional comments from personal memory:I have lived in Japan since 1983, and I started working as a freelance Japanese-to-English translator in 1986. I wanted to produce clean-looking text in English for my clients, so after a few months using a manual typewriter I took out a loan and bought a Macintosh with a dot-matrix printer. If I remember correctly, it cost six hundred thousand yen. The Mac could not handle Japanese; when I needed to write Japanese text, such as for notes to clients, I wrote by hand. I eventually bought a dedicated Japanese word processor for writing clean text in Japanese.Around 1992, I bought a modem and went online, first to a local foreign-run BBS and then, a couple of years later, the Internet. Many of the first friends I made online were Japanese-English translators like myself, and some of the most active discussion groups I took part in were about the Japanese language and translation.The display of Japanese characters in our online discussions was a problem for a long time. Even as more and more of the participants became able to type Japanese on their own computers, they were using a variety of OSs and character encodings, and the Japanese parts of their messages, when posted online, would be corrupted more often than not. When discussing a particular Japanese expression, we would have to romanize the Japanese and, sometimes, explain what kanji were used.Here\u2019s are two examples from posts to a translators\u2019 mailing list in 1998:> While this handbook uses \"\u00e5\u00f2\u00e5\u2248\u00ea\u00b4\" for \"robustness\", the systems engineers I work with prefer \"\u00c9\u00e7\u00c9o\u00c9X\u00c9g\u00ea\u00b4\" <robasutosei>.> Ruth, the kanji for taikou are tai (as in taishi - Crown Prince) and kou (as in kugurido - the radical is mon with gou inside (gou = au/awasersu).  Does this help? The dictionary meaning obviously does not make sense here.This made it impractical to discuss longer texts or to have our discussions in both English and Japanese.It was a great relief when, around 2000 or so, the encoding issues were gradually resolved and we became able to write Japanese freely in our online discussions.(Addendum: I am still in touch with some of the people on that mailing list, including the Ruth mentioned above. In fact, last month I attended a party in Yokohama in honor of her and her husband\u2019s 55th wedding anniversary. Several other friends I first met online in the mid-1990s were there, too.)\n \nreply",
      "Even in the larger commercial computer space, Japan always liked to sorta do their own thing. Aside from a couple other companies, they were always big Itanium backers for example.I was an analyst during that period and Japan was always something of an outlier. (Europe was to some degree as well. But less so.)\n \nreply",
      "Very interesting! Thanks for posting this!\n \nreply",
      "I never thought about this before, but product competition is basically evolution in action. Entities with more desirable traits that adapt better to a given ecosystem survive, the rest don't. (In addition to things like a pre-existing dominant species having advantages over new ones)(fwiw, Windows won out because it had better business strategy. Apple wanted to be in everyone's homes; Microsoft wanted to be in everyone's business. One of those is easier to sell to in bulk, and easier to charge more money. In addition, Windows being more hardware-agnostic, and encouraging an ecosystem of competing hardware manufacturers, allowed them to invest less in hardware themselves, while creating an industry that would vie for business on Microsoft's behalf. This is of course different than the \"workstation\" market of uber-high-powered individual computers, which sort-of still exists, though with PC hardware)\n \nreply",
      "Yes! And it's very interesting to consider two additional things:1. how seemingly \"less capable\" technologies win out in this evolutionary environment2. how plentiful VC (and to some extent government funding for R&D) distorts normal \"evolutionary\" forces in a market\n \nreply",
      "Windows also won by parasitizing a previously bigger host (Bill Gates\u2019 mother was on IBM\u2019s board), and shutting out competition by forcing vendors not to offer other companies\u2019 software if they wanted Microsoft licenses at better than retail pricing.\n \nreply"
    ],
    "link": "https://www.mistys-internet.website/blog/blog/2024/09/21/what-happened-to-the-japanese-pc-platforms/",
    "first_paragraph": "\nSep 21st, 2024 2:01 pm\n(This was originally posted on a social media site; I\u2019ve revised and updated it for my blog.)The other day a friend asked me a pretty interesting question: what happened to all those companies who made those Japanese computer platforms that were never released outside Japan? I thought it\u2019d be worth expanding that answer into a full-size post.It\u2019s hard to remember these days, but there there used to be an incredible amount of variety in the computer space. There were a lot of different computer platforms, pretty much all of them totally incompatible with each other. North America settled on the IBM PC/Mac duopoly pretty early1, but Europe still had plenty of other computers popular well into the 90s, and Japan had its own computers that essentially didn\u2019t exist anywhere else.So who were they? By the 16-bit computer era, there\u2019s three I\u2019m going to talk about today2: NEC\u2019s PC-98, Fujitsu\u2019s FM Towns, and Sharp\u2019s X68000. The PC-98 was far and away the biggest of thos"
  },
  {
    "title": "AI Companions Reduce Loneliness (arxiv.org)",
    "points": 37,
    "submitter": "Dowwie",
    "submit_time": "2024-09-21T23:52:32.000000Z",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=41613513",
    "comments": [
      "I would rather kill myself than talk to an AI to \u201creduce loneliness.\u201d People make life, without others we are nothing. Loneliness is how we realize we\u2019re spending too much time by ourselves; it encourages us to be social.I don\u2019t mind being lonely, because when I\u2019m lonely I figure out ways to be around people. If AI \u201cfixes\u201d loneliness without needing other people our entire world will fall apart.\n \nreply",
      "> I would rather kill myself than talk to an AI to \u201creduce loneliness.\u201dSeems like you're not alone!https://www.american.edu/spa/news/generation-z-and-deaths-of...\n \nreply",
      "> I would rather kill myself than talk to an AI to \u201creduce loneliness.\u201d People make life, without others we are nothing. Loneliness is how we realize we\u2019re spending too much time by ourselves; it encourages us to be social.I think like you but my friends had already locked themselves behind screens in the last 2 decades - with twitch, social media, chats, and lives occurring exclusively online. The offline life is for rent and RTO where we physically locate ourselves and then plug in online as soon as we can.\n \nreply",
      "Ibuprofen reduces pain. It is also not a solution, especially when that pain has a solvable underlying cause.What worries me is that many people are becoming lonely because we\u2019re losing the habits and skill sets needed to form and maintain meaningful relationships.Given everything we know about the importance of social connection to many facets of human well-being, unless something like this is used explicitly to rebuild those skills and to subsequently form real connections, tools like this seem likely to worsen the problem in the long run.\n \nreply",
      "I think your view comes from right place. But no doctor would recommend ibuprofen as sole treatment in all causes of pain. Similarly, no doctor or therapist would recommend everyone replace meaningful human relationships with AI companions. It's another tool.I personally am fortunate to have many rich meaningful relationships in my life. But I've also gotten additional value from voice chatbots. Using voice mode and having them act as therapists, coaches, or experts giving feedback are very helpful.\n \nreply",
      "But companies will merrily \"prescribe\" AI companions as _a_ treatment, not as a sole treatment, but having even less of an obligation, and not even an unenforced ethical code to act on behalf of the patient's best interests, they'll just let people use them unsupervised and not accompanied with a method to stop relying on the bots.\n \nreply",
      "Until AI companions actively argue, tease or are straight up disagreeing with you, no, we won't. It's part of the learning to converge all of our reality tunnels together. How can I learn that when I'm stuck in my echo box ?\n \nreply",
      "What makes you think they can't do that?\n \nreply",
      "Problem with this imo is that AI companion will never tell you that what you are doing/thinking is not acceptable in a society, like how your friend or any other person you interact with would, because with real relationship it\u2019s a two way street.This may create a bunch of people who are even more distant from the social interaction than they were in a beginning\n \nreply",
      "AI mostly does whatever you ask it to. You can easily make a chatbot that will question your choices or act like a hater on twitter.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2407.19096",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "100x Faster CPUs from Finland's New Startup (ieee.org)",
    "points": 81,
    "submitter": "rbanffy",
    "submit_time": "2024-09-21T21:04:27.000000Z",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=41612665",
    "comments": [
      "Seems like a nice idea \u2014 instead of the stark CPU/GPU divide we have today, this would fit somewhere in the middle.Reminds me slightly of the Cell processor, with its dedicated SPUs for fast processing, orchestrated by a traditional CPU. But we all saw how successful that was (:  And that had some pretty big backing.Overcoming the inertia of the current computing hardware landscape is such a huge task. Maybe they can find some niche(s).\n \nreply",
      "I\u2019m still waiting for a clockless core\u2026 some day\n \nreply",
      "Does anyone know what they mean by \"wave synchronization\"? That's supposedly their trick to prevent all those parallel CPUs from blocking waiting for data. Found a reference to something called that for transputers, from 1994.[1] May be something else.Historically, this has been a dead end. Most problems are hard to cut up into pieces for such machines. But now that there's much interest in neural nets, there's more potential for highly parallel computers. Neural net operations are very regular.\nThe inner loop for backpropagation is about a page of code. This is a niche, but it seems to be a trillion dollar niche.Neural net operations are so regular they belong on purpose-built hardware. Something even more specialized than a GPU. We're starting to see \"AI chips\" in that space. It's not clear that something highly parallel and more general purpose than a GPU has a market niche. What problem is it good for?[1] https://www.sciencedirect.com/science/article/abs/pii/014193...\n \nreply",
      "GPUs have wavefronts so I assume it is similar?  Here is a page that explains it:https://gpuopen.com/learn/occupancy-explained/\n \nreply",
      "We're starting to see \"AI chips\" in that space.\"Positronic\" came to my mind.\n \nreply",
      "Discussion (28 points, 3 months ago, 32 comments) https://news.ycombinator.com/item?id=40650662\n \nreply",
      "Does anyone have any knowledge/understanding on how this is (or isn't?) fundamentally different from Intel's Xeon Phi?https://en.wikipedia.org/wiki/Xeon_Phi\n \nreply",
      "How is this different from an integrated gpu other than it presumably doesn't do graphics.\n \nreply",
      "When will we get the \u201cMill\u201d cpu?\n \nreply",
      "I've been following that saga for a long time. Seems mostly like vapourware sadly.\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/parallel-processing-unit",
    "first_paragraph": "Flow Computing aims to boost central processing units with their \u2018parallel processing units\u2019Dina Genkina is the computing and hardware editor at IEEE SpectrumIn an era of fast-evolving AI accelerators, general purpose CPUs don\u2019t get a lot of love. \u201cIf you look at the CPU generation by generation, you see incremental improvements,\u201d says Timo Valtonen, CEO and co-founder of Finland-based Flow Computing.Valtonen\u2019s goal is to put CPUs back in their rightful, \u2018central\u2019 role. In order to do that, he and his team are proposing a new paradigm. Instead of trying to speed up computation by putting 16 identical CPU cores into, say, a laptop, a manufacturer could put 4 standard CPU cores and 64 of Flow Computing\u2019s so-called parallel processing unit (PPU) cores into the same footprint, and achieve up to 100 times better performance. Valtonen and his collaborators laid out their case at the Hot Chips conference in August.The PPU provides a speed-up in cases where the computing task is parallelizable"
  },
  {
    "title": "Tetris Game Shows Promise in Reducing PTSD Symptoms (legalreader.com)",
    "points": 77,
    "submitter": "giuliomagnifico",
    "submit_time": "2024-09-21T20:09:54.000000Z",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=41612367",
    "comments": [
      "https://tetris.com/play-tetris/\n \nreply",
      "More HN-esque:  ssh playnetris.com\n\nhttps://playnetris.com\n \nreply",
      "Video games are an excellent distraction to break up invasive thought patterns, too. There are several methods to use distractions that therapists tailor to their clients.This is part of why I work in the games industry, grandstanding as it may sound. Games have helped me deal with complex PTSD at a point in my life. It is important to do the other work and not just rely on distraction as forever.But distractions give you breathing room and some space away from invasive thoughts. The trauma can then begin to heal.\n \nreply",
      "That's a highly compassionate reason to get into an industry, bravo.Is there a specific type of game you found healing, or especially one that you prefer to create for such a purpose?I personally strangely find a mix of 'brain turn off' games such as ARPGs healing but then can also find great peace in crushing my brain through another Factorio run.\n \nreply",
      "I have no PTSD, but I recently lost my job, and while I am looking for another one I decided to use the time off in order to stop smoking, so far it's been a bit more than 3 weeks, and I think it's been the period I've been playing the most videogames since high school, it is really useful to overcome temporary cravings by doing something that doesn't require much mental efforts but still keeps the mind busy to don't think about smoking, every day I am suffering less and less, and I think without videogames it would have been much harder.Keep in mind, I'm 37 years old and have smoked since I was 13\n \nreply",
      "Don\u2019t worry about grandstanding, it\u2019s not.It\u2019s good to have found an angle that you care about and can apply yourself to.This is a really nice way of explaining it and I didn\u2019t consider it before despite being a very heavy gamer at one point.\n \nreply",
      "My reading of the research is quite a bit of positive impact of video game usage on mental health, and the negatives come up when they take away from healthy habits due to extreme use (exercise, socialization, education).And it\u2019s hard to tell causality of the negative side (maybe video games are being used to cope with something  terrible)https://www.charliehealth.com/post/video-games-and-mental-he...\n \nreply",
      "As someone living with PTSD, distraction is absolutely the best tool for dealing with the effects. I'm lucky enough to have a career that does that job for me, I get really absorbed in what I'm working on and then I don't have to think about the awful things that happened to me.\n \nreply",
      "This is a newer application of an existing approachhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2607539/\n \nreply",
      "Some of what they're saying reminds me of EMDR therapy which is also used (with mixed success) to treat PTSD, and is briefly mentioned in the underlying paper: https://en.wikipedia.org/wiki/Eye_movement_desensitization_a...\n \nreply"
    ],
    "link": "https://www.legalreader.com/tetris-game-shows-promise-in-reducing-ptsd-symptoms/",
    "first_paragraph": "The 1980s video game proves to make a notable difference with intrusive memories.A recent study led by researchers at Uppsala University has uncovered promising results for a simple yet effective intervention in alleviating symptoms of post-traumatic stress disorder (PTSD). The research, published in BMC Medicine, focuses on the use of video games, particularly the well-known Tetris game that has been around for decades, to help reduce intrusive memories, a core and sometimes debilitating symptom of the condition.PTSD is often marked by intrusive, distressing memories or flashbacks, where individuals vividly recall traumatic events as if reliving them. These flashbacks can be debilitating, affecting a person\u2019s overall mental health, sleep, concentration, and ability to engage in daily activities. Traditionally, treatments for PTSD involve therapy sessions, including cognitive behavioral therapy (CBT) or exposure therapy, often requiring multiple appointments with a trained clinician. H"
  },
  {
    "title": "What Is a Particle? (2020) (quantamagazine.org)",
    "points": 94,
    "submitter": "sblank",
    "submit_time": "2024-09-21T19:20:34.000000Z",
    "num_comments": 71,
    "comments_url": "https://news.ycombinator.com/item?id=41612049",
    "comments": [
      "Discussed at the time:What Is a Particle? - https://news.ycombinator.com/item?id=25085286 - Nov 2020 (37 comments)\n \nreply",
      "I'm reading \"The Big Picture\" (Sean Carroll) right now.I'd love to have a real physicist explain this, but:When we think of what a particle IS, we often think as though it were dirt, or a billiard ball, or something. As though there were some other substance of which it's made. At least I do.But the definition is as low as you can go. It's hard to wrap your head around that. Unless you're trained to do so, I guess.\n \nreply",
      "Particle spin explained:Imagine a ball that\u2019s rotating,Except it\u2019s not a ball, andIt\u2019s not rotating.(popular particle physics meme)From what I understand of QFT, the Universe is made of fields of different types, and a \u201cfundamental particle\u201d is just an excitation (wave) in the corresponding field.For example, a photon is a wave in the universal electromagnetic field,\nA charm quark is a wave in the universal charm quark field, etc.I\u2019m not a trained physicist, so I might be wildly wrong.\n \nreply",
      "I get it but I still think these sorts of concepts are also just another level of mathematical abstraction that isn\u2019t necessarily \u201creally what it is\u201d any more than a rotating ball or a math equation or any of the other ideas are \u201creally what it is\u201d\n \nreply",
      "> we often think as though it were dirt, or a billiard ball, or somethingThe problem lies that it is hard to imagine something that does have zero dimensions. You can get the example of ant walking into 2D and it is unaware of third dimension to explain we are have something similar for space-time 4D (although not the same picture exactly as time is different from spatial dimensions). But we don't have an idea how to approximate a mental picture of what a zero dimension could be. So you have something that does not occupy a volume in space (Talking strictly about elementary particles here) in the classical sense.This does not mean they are abstract concept. According to QFT -Quantum field theory- you would think (by training) of particles are excitations or quanta of their respective fields. Fields are there always (vacuum is just filled with fields) and particle appears when they are excited (more complex processes occurs). So you would think of each particle as a manifestation of a quantum field that permeates the universe. What is interesting (and probably confusing to most people) is that these fields are not zero-dimensional, instead, they exist everywhere in space and time. But the quanta (particles themselves) are considered point-like with no spatial extension.In practice physicists will think about particles properties (i.e charge, mass, interactions, spin) ..etc instead of what this particle actually is from that point of view. This is often for practical reasons. You are a working physicist and you learned from your training that you shut up and calculate (or implement if you are doing experimental particle physics as you spend most of your time coding) by this stage.\n \nreply",
      "> The problem lies that it is hard to imagine something that does have zero dimensions.Do you really think so? It\u2019s not hard to picture the real number line, with the point zero (or any other single point) distinguished. Sure \u2014 if you draw it in the standard schematic way you have to give it some area, but it still seems quite intuitive that it\u2019s \u2018zero-dimensional\u2019. Especially if you play around with converging sequences and open sets and stuff; you quickly develop intuition for what it means to be a point rather than something higher dimensional.\n \nreply",
      "Richard Feynman gave what I consider to be the best possible answer to questions like this:https://www.youtube.com/watch?v=Q1lL-hXO27Q\n \nreply",
      "At first I was impressed with that video. Then I felt he does not have an answer and unnecessarily gets edgy with it, because question is valid.\n \nreply",
      "I just watched it. I don't think he's edgy.You can't explain it in terms of anything else, which was sorta my original point. Maybe he could have been more touchy-feely in his answer, but that wasn't his nature.\n \nreply",
      "> he does not have an answerWell, yeah.  That's the whole point.\n \nreply"
    ],
    "link": "https://www.quantamagazine.org/what-is-a-particle-20201112/",
    "first_paragraph": "\nAn editorially independent publication supported by the Simons Foundation.\n\nGet the latest news delivered to your inbox.\nCreate a reading list by clicking the Read Later icon next to the articles you wish to save.Type search term(s) and press enterPopular\n                                    SearchesNovember 12, 2020Elementary particles are the basic stuff of the universe. They are also deeply strange.Illustrations by Ashley Mackenzie for Quanta MagazineSenior EditorNovember 12, 2020Given that everything in the universe reduces to particles, a question presents itself: What are particles?The easy answer quickly shows itself to be unsatisfying. Namely, electrons, photons, quarks and other \u201cfundamental\u201d particles supposedly lack substructure or physical extent. \u201cWe basically think of a particle as a pointlike object,\u201d said Mary Gaillard, a particle theorist at the University of California, Berkeley who predicted the masses of two types of quarks in the 1970s. And yet particles have disti"
  },
  {
    "title": "LHC experiments at CERN observe quantum entanglement at the highest energy yet (home.cern)",
    "points": 84,
    "submitter": "gmays",
    "submit_time": "2024-09-21T18:09:05.000000Z",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=41611613",
    "comments": [
      "\u00ab test the Standard Model of particle physics in new ways and look for signs of new physics that may lie beyond it \u00bb\"Surely we're just a teensy bit away from that new physics, and if we can just a little bit more money^Wenergy into the system, we'll find that new physics for sure!\"\n \nreply",
      "They know that it's a long way and a lot more money. Fundamental physics has a habit of paying off in utterly unexpected ways, but that's not really why we do this. It's pure curiosity.They are grateful that the public seems willing to pay for curiosity. I don't know how long it will last. Though I can say that it's a rounding error in national budgets.\n \nreply",
      "Imagine how much money is spent on defence. And then imagine the tiny proportion that is spend on basic research that might result in offensive or defensive weapons.\n \nreply",
      "Personally I can\u2019t imagine not understanding that the defence funding is one of the most essential factors in enabling our modern way of life. It\u2019s been so effective at providing the security needed to produce globalisation and highly democratic societies, that a lot of people have forgotten the reason we need it in the first place. Certainly worth the minuscule portion of GDP we allocate to it.\n \nreply",
      "The US GDP is about $25 trillion. Our military costs about $1 trillion, even if you leave out things like the Veterans Administration or the funding of Ukraine's war. That's 4%, a number I'd be hard pressed to describe that as \"minuscule\". And it's close to the amount everybody else spends, combined.The military is certainly necessary. But even a 25% cut would be a lot of money you could spend on other things, and still be by far the most advanced and expensive military in the world.\n \nreply",
      ">enabling our modern way of lifeThat's kind of their point, though? They're suggesting our \"way of life\" is backwards in some ways, which I think is a fair assessment.We find ourselves supporting financially things like what's happening in Israel or Gaza or happened in Iraq or Afghanistan or Sudan or Armenia or Haiti or insert the rest of the list of countries we've coup'ed here. While we fight over whether poor people deserve healthcare, or if school kids deserve not to starve. Over if public education is good or not. Or if women deserve medical autonomy...None of that is particularly democratic. Globalization happened because rich people wanted cheaper labor in the 'global south' not because of military bases.\nI don't know that it's obvious military is what's \"produced globalization\" and \"highly democratic\" societies. That seems a fairly large keep of faith.\n \nreply",
      "The fusion breakthrough in the National Ignition Facility was said to be essentially a program claiming to be for nuclear weapons research but instead they did fundamental research.\n \nreply",
      "Particle accelerators are pinging the deepest layers of reality that we can possibly reach. Deeper than anyone could have imagined just a few generations ago. That anyone can be cynical about that is hard to understand.\n \nreply",
      "I am willing to pay for curiosity, I'm however, not willing to pay for thousands of bureaucrats of the system who are only employed because they somehow got hired into an unnecessary position.\n \nreply",
      "I don\u2019t think it\u2019s pure curiosity. Maybe physicists are driven by passion, but everybody understands that new physics has historically unlocked a lot of new technology.I also don\u2019t think that the public really has much of a say in what gets tax payer funding. Governments are just bureaucratic behemoths and once they start paying for something, they can almost never find an acceptable way to stop. Additionally I\u2019ve never met a tax funding recipient that was grateful to be getting it, if anything the perspective that they should be entitled to even more is a lot more common.\n \nreply"
    ],
    "link": "https://home.cern/news/press-release/physics/lhc-experiments-cern-observe-quantum-entanglement-highest-energy-yet",
    "first_paragraph": "At CERN, we probe the fundamental structure of particles that make up everything around us. We do so using the world's largest and most complex scientific instruments.Know more\nWho we are\n\nOur Mission\n\nOur Governance\n\nOur Member States\n\nOur History\n\nOur People\n\nWhat we do\n\nFundamental research\n\nContribute to society\n\nEnvironmentally responsible research\n\nBring nations together\n\nInspire and educate\n\nFast facts and FAQs\n\nKey Achievements\nKey achievements submenuThe Higgs BosonThe W bosonThe Z bosonThe Large Hadron ColliderThe Birth of the webAntimatterLatest news\nNews\n\nAccelerators\n\nAt CERN\n\nComputing\n\nEngineering\n\nExperiments\n\nKnowledge sharing\n\nPhysics\n\nEvents\n\nCERN Community\n\nNews and announcements\n\nOfficial communications\n\nEvents\n\nScientists\n\nNews\n\nPress Room\nPress Room submenuMedia NewsResourcesContactThe research programme at CERN covers topics from kaons to cosmic rays, and from the Standard Model to supersymmetryKnow more\nPhysics\n\nAntimatter\n\nDark matter\n\nThe early universe\n\nThe "
  },
  {
    "title": "Applied Mathematical Programming (web.mit.edu)",
    "points": 89,
    "submitter": "ibobev",
    "submit_time": "2024-09-21T18:00:57.000000Z",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=41611571",
    "comments": [
      "This is from 1977. I suppose it's ok for fundamentals but you can probably do better going with a modern text like  Model Building in Mathematical Programming by H. Paul Williams (5th Edition)\n \nreply",
      "H. P. Williams' book is often recommended for learning MIP formulations.However, I've found a cheat sheet to be much more helpful and practical for MIPs (not LPs but MIPs):https://msi-jp.com/xpress/learning/square/10-mipformref.pdfThis contains the primitives you typically use in MIPs.\n \nreply",
      "convex optimization by boyd vandenberghe should be mentioned https://web.stanford.edu/~boyd/cvxbook/\n \nreply",
      "Has this been updated since 1977? Because the field and tools and even the view points have changed a ton.\n \nreply",
      "The tools probably have changed but the fundamental language is the same. The same way that you need to wire your brain to see how a problem can be casted as a dynamic programming one, you also need to learn how to formulate problems as integer/linear programming ones.For example all of the \"hard\" leetcode problems can be casted as math programming ones. But the interviewers will not appreciate this solution approach lol.Once you conquer the logic/language then learning the tools is the easy part.\n \nreply",
      "> But the interviewers will not appreciate this solution approach lol.I once witnessed a programmer with a PhD in Maths find closed form formulas for a lot of questions where it was expected to write some code with loops building/accumulating a result. As a simple example, to explain what was going on, if the question would be \"calculate the 100th fibonacci number\", she would just use Binet's formula to do so (as opposed to using a loop). I was rather impressed how often that happened.\n \nreply",
      "Is Binet's formula really that practical a way to calculate the Fibonacci numbers (except asymptotically)? The problem is, you have this nice clean expression, but you'd still have to implement a bunch of fancy arbitrary-precision arithmetic to approximate the golden ratio through Newton's method. In other words, the formula gives much more information about the structure of the Fibonacci numbers than their actual values.For evaluating the Fibonacci numbers (as with any other integer linear recurrence), I'd generally prefer the matrix-exponentiation-by-squaring approach, or one of the simplified formulas based on it. Those don't need anything more complicated than bigint multiplication. [And from there, taking the ratio between two values gives you a quick way to approximate the golden ratio!]\n \nreply",
      "Once you have postulated BigInt as available, the mathematician is going to make a rational approximation for phi using the continued fraction expansion that they know by heart (because of its \u201csimplicity\u201d).\n \nreply",
      "TBF Binet's formula is astonishing\n \nreply",
      "If you think it is you should read up on linear algebra, specifically its use in finite difference equations and how that relates to linear differential equations.The astonishment doesn't get less, but it shifts from Binet's single formula to the exponential map, and maybe the fundamental theorem of algebra (or generalisations).\n \nreply"
    ],
    "link": "https://web.mit.edu/15.053/www/AMP.htm",
    "first_paragraph": "Applied Mathematical Programming by Bradley, Hax,\nand Magnanti (Addison-Wesley, 1977) \n\nThis\nbook is a reference book for 15.053, Optimization Methods in\n\u00a0\u00a0\u00a0 Business Analytics, taught at MIT. To make the book available online, most chapters have been re-typeset.\nChapters 6, 7 and 10 were not, but are still available (as direct scans of the original\nchapters). \n\nDownloads of the\nbook and its chapters \u00a0\u00a0\u00a0\u00a0\u00a0 Some\nExcel exercises.Remark:\u00a0 These exercises (and\nExcel) were developed after the book was published.\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise 3.1\u00a0\u00a0 \u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise 3.2 \u00a0\u00a0\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise 3.7\u00a0\u00a0 \u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise\n3.9 \u00a0\u00a0\u00a0\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise 3.10\u00a0 \u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise 3.11 \u00a0\u00a0\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise 3.12\u00a0 \u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise 3.14\u00a0\u00a0 \u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise 3.15\u00a0\u00a0 \u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise 3.16\u00a0\u00a0 \u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise 3.18\u00a0\u00a0 \u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise 3.19\u00a0\u00a0 \u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise 3.25\u00a0\u00a0 \u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nExercise 3.28\u00a0\u00a0 \u00a0Other\nspreadsheets\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nSection\n1.3.\u00a0 Blast furnace example\u00a0\u00a0 \u00b7\u00a0"
  },
  {
    "title": "Ultra high-resolution image of The Night Watch (2022) (rijksmuseum.nl)",
    "points": 466,
    "submitter": "lhoff",
    "submit_time": "2024-09-21T09:08:20.000000Z",
    "num_comments": 122,
    "comments_url": "https://news.ycombinator.com/item?id=41608648",
    "comments": [
      "A colleague of mine made this very nice way to explore the (often) high resolution images from their collection:https://rijkscollection.net/Highly recommended and easy to fall into a \u201crijkscollection hole\u201d for a bit :)\n \nreply",
      "This is really nice to use. Is this how this wing of the gallery actually looks?\n \nreply",
      "No, it looks different from this.\n \nreply",
      "This is super cool. Thanks for sharing the link\n \nreply",
      "Technically it is an interesting project.But anyone who has visited the museum will find it weird. It is very different. The building architecture is very different, there are thousands more works in the exposition, and the order of the works is very different, ...\n \nreply",
      "Works better than the one mentioned in the title, this one let you zoom in and out with scroll weel.\n \nreply",
      "*scroll wheel\n \nreply",
      "I worked at this museum a few decades ago on a contract job, it was cool to walk around among so much history. Though I never really could appreciate the \"old masters\" from the Dutch Golden Age. Their work was part art and part record-keeping for which nowadays we have photography and video. The subject of many of these works are stuffy rich people posing for the \"family album\". Artfully done yes but boring subjects in my personal opinion.I did like some of the landscape views though. But overall I'm more into modern art where the art and the message is the only goal.One of the things special to me about the night watch is that it's huge in real life which I never really appreciated before I saw it. In contrast, the Mona Lisa at the Louvre was disappointingly tiny.\n \nreply",
      "> One of the things special to me about the night watch is that it's huge in real life which I never really appreciated before I saw it.Famous art that's stunningly bigger in person than I expected:   - The Raft of the Medusa (G\u00e9ricault)\n   - Guernica (Picasso)\n   - The Hallucinogenic Toreador (Dal\u00ed)\n\nCannot recommend seeing art in person enough.Aside from the scale, it's also impossible to fully capture color or translucency in screen/page-presented imaging.And so much of the European painting mastery in the 1400s+ is the manipulation of non-opaque paint to create a desired effect.\n \nreply",
      "Aside from color and translucency, an original artwork shows also the relief. It can tell much about the creation process of a painting and adds additional texture. \nFurthermore, some pigments were expensive and hard to work with prior to the 19th century such that artists used it very sparingly.\n \nreply"
    ],
    "link": "https://www.rijksmuseum.nl/en/stories/operation-night-watch/story/ultra-high-resolution-image-of-the-night-watch",
    "first_paragraph": "The Rijksmuseum uses cookies. A cookie is a small text file that a website stores on your computer or mobile device when you visit our site. Would you like to know more? Read our cookie policy.\r\n            5 min. reading time\r\n              -\r\nHow did the team create this image?          10/01/2022 - RijksmuseumThe new high-resolution image of The Night Watch represents a major advance in the state of the art for imaging paintings, setting records for both the resolution and the total size of the image. The sampling resolution is 5 \u00b5m (0.005 mm), meaning that each pixel covers an area of the painting that is smaller than a human red blood cell. Given the large size of The Night Watch, this results in a truly enormous image: it\u2019s 925,000 by 775,000 pixels \u2013 717 gigapixels \u2013 with a file size of 5.6 TB!To create this huge image, the painting was photographed in a grid with 97 rows and 87 columns with our 100-megapixel Hasselblad H6D 400 MS camera. Each of these 8,439 separate photos was "
  },
  {
    "title": "WP Engine is not WordPress (wordpress.org)",
    "points": 49,
    "submitter": "pentagrama",
    "submit_time": "2024-09-22T00:16:25.000000Z",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=41613628",
    "comments": [
      "I have a long rant about how open source is wage theft and value extraction by unscrupulous third parties is built in and unavoidable. But setting that aside, I learned of WP Engine when they launched here, on hacker news. At the time their value proposition was \"bullet proof and secure wordpress for people who just want to publish and not learn devops\".Over the years, I've watched them through a progression of management changes move from value of service to value extraction. Chipping away at costs while holding the price constant or raising it and extracting the difference for themselves. This isn't in and of itself a \"bad\" thing, it is what business does, however I find the integrity around value extraction varies tremendously. From zero integrity Mackenzie type MBAs to high(er) integrity owner operators.It is rare when a management team says, \"this is enough money\" and that is sad.\n \nreply",
      "> WordPress is a content management system, and the content is sacred. Every change you make to every page, every post, is tracked in a revision systemI'm not sure I see how the absence of a revision tracking system rises to a violation of sacred principles.\n \nreply",
      "The revision system is not (not is it claimed to be) a sacred principle of life, morality, or society at large. It's a core tenet of WordPress.\n \nreply",
      "You can also just turn it on by emailing support...\n \nreply",
      "You can turn on an extremely limited version. Did you read the post?\n \nreply",
      "It might not be WordPress, but then WordPress is often an inefficient, error-prone piece of dung anyway. WP Engine is the only painless WordPress hosting solution I've seen. It's fast and has a great backend panel for debugging. Hard to do all that yourself for that price, unless you're hosting WP in bulk.\n \nreply",
      "This feels like a private which has reached the public shorn of the context that would be necessary to understand it.I use Wpengine and have enjoyed it. They have some aggressive upsells and you learn you can ignore them. Actually had no idea about revisions, hadn't used them before I hosted with Wpengine.I very much like using Wordpress and Wpengine has helped make it easy to do so. I'm sure they have some things to work put between them but I feel this needs more info. At a certain level it's open source software and if it isn't a trademark violation and is allowed by the license terms then Wordpress has only moral suasion to work with.Very happy with the work Wordpress has done to make an amazing ecosystem. If they need something from WPEngine to keep things going I think it's fair to ask and perhaps they did but we're a bit in the dark here.\n \nreply",
      "I've been out of the WP community/scene for a number of years now, but this post is kinda weird. Revisions definitely aren't the hill I'd die on when choosing a hosting provider at all. Especially considering WPEngine does a lot of things very well:1. Dead-simple staging environments2. Support for Local, which makes WP development an absolute breeze because I don't need to maintain docker, vagrant, or a LAMP stack, etc. And it makes deployments quick/easy.3. Dead-simple backup/restore features4. Simplified cache-managementAnd yeah, I've got the technical know-how to handle all of that myself directly on a proper server and all that devops-y goodness. And yes, $5/mo shared hosting cPanel provider would be comparable (and let's be real, it's good enough for most people using WP)....But man is it nice to just charge/pay a little more for a host that just does that crap for me with a nice interface.I like revisions as a feature. Hell, I made reference to them a lot in the training material and sessions I put together for clients way-back-when as a way to give clients the confidence to tweak copy without fear of completely ruining their site. But this blog post seems to pretend it's the heart of WP and without it, it's an entirely different piece of software all together, which is absurd.\n \nreply",
      "Amazed that this got to the point where an official post was deemed necessary.\n \nreply",
      "I mean, yeah. Isn't that what they're supposed to be doing? Because an option exists to store every revision doesn't mean you necessarily have to do it for free. It is a costly feature. They're free to chop up WordPress as much as they like and monetize features that elsewhere are default. They're not hiding it and users can choose to go elsewhere. Otherwise every WordPress host would be the same, just hosting vanilla WordPresses... and while the WordPress people may not like it, ain't nothing wrong with it.\n \nreply"
    ],
    "link": "https://wordpress.org/news/2024/09/wp-engine/",
    "first_paragraph": "It has to be said and repeated: WP Engine is not WordPress. My own mother was confused and thought WP Engine was an official thing. Their branding, marketing, advertising, and entire promise to customers is that they\u2019re giving you WordPress, but they\u2019re not. And they\u2019re profiting off of the confusion.I spoke yesterday at WordCamp about how Lee Wittlinger at Silver Lake, a private equity firm with $102B assets under management, can hollow out an open source community. Today, I would like to offer a specific, technical example of how they break the trust and sanctity of our software\u2019s promise to users to save themselves money so they can extract more profits from you.WordPress is a content management system, and the content is sacred. Every change you make to every page, every post, is tracked in a revision system, just like the Wikipedia. This means if you make a mistake, you can always undo it. It also means if you\u2019re trying to figure out why something is on a page, you can see precise"
  },
  {
    "title": "Logging all C++ destructors, poor mans run-time tracing (raymii.org)",
    "points": 29,
    "submitter": "jandeboevrie",
    "submit_time": "2024-09-21T19:06:31.000000Z",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=41611948",
    "comments": [
      "I consider Tracy the state of the art for profiling C++ applications.  It\u2019s straightforward to integrate, toggle, gather data, analyze, and respond. It\u2019s also open source, but rivals any product you\u2019d have to pay for:https://github.com/wolfpld/tracy\n \nreply",
      "Looks fine, but it does not look like there is a automatic full function entry/exit trace, just sampling. The real benefit is when you do not even need to insert manual instrumentation points, you just hit run and you get a full system trace.How well does the visualizer handle multi-TB traces? Usually pretty uncommon, but a 10-100 GB is not that hard to produce when doing full tracing.\n \nreply",
      "Of note is that tracy is aimed at games, where sampling is often too expensive and not fine-grained enough. Hence the manual instrumenting.For the Bevy game engine, we automatically insert tracy spans for each ECS system. In practice, users can just compile with the tracy feature enabled, and get a rough but very usable overview of which part of their game is taking a long time on the CPU.\n \nreply",
      "I was talking about automatic instrumentation of every single function call by default. No manual instrumentation needed because everything is already instrumented.To be fair, you do still want some manual instrumentation to correlate higher level things, but full trace everywhere answers most questions. You also want to be able to manually suppress calls if performance relevant, but the point is \u201cdefault on, manual off\u201d over \u201cdefault off, manual on\u201d.",
      "But what was the shutdown bug you were trying to identify?  Was this destructor logging actually useful?  The article teases the problem and provides detailed instructions for reproducing the logging, but doesn't actually describe solving the problem.\n \nreply",
      "Address/MemorySanitizer are also meant for this kind of problem. https://github.com/google/sanitizers/wiki/AddressSanitizer https://github.com/google/sanitizers/wiki/MemorySanitizerAlso valgrind, but I'm more familiar  with the first two.\n \nreply",
      "I did this a long time ago with macros. It helped me to find a ton of leaks in a huge video codec codebase.I still don't understand the hate for the C preprocessor. It enables doing this like this without any overhead. Set a flag and you get constructor/destructor logging and whatever else you want. Don't set it and you get the regular behavior. Zero overhead.\n \nreply",
      "The hate might have to do with it being such a primitive and blunt tool; doing anything moderately complex becomes extremely complicated and fragile.\n \nreply",
      "Yeah, this very primitive tool easily creates the programming equivalent of the \"iwizard problem\".[You replace straight forwardly \"mage\" with \"wizard\" and oops, now your images are \"iwizards\" and your \"magenta\" is \"wizardnta\"]\n \nreply",
      "do you have a write-up how you did it? I'm interested, thanks.\n \nreply"
    ],
    "link": "https://raymii.org/s/software/Logging_all_Cpp_destructors_poor_mans_run-time_tracing.html",
    "first_paragraph": "\n\n\n\n\nPublished: 21-09-2024 23:59 | Author: Remy van Elst | Text only version of this article\nI recently faced a challenging issue with an application that wasn't shutting down correctly, either segfaulting or terminating without an active exception. Running the program via valgrind to check for memory leaks wasn't possible because the program couldn\u00e2\u20ac\u2122t perform its cleanup if it didn't shut down correctly. This article covers adding runtime instrumentation provided by gcc to log destructors. This helped me figure out what was still left over from the closed-source framework in use preventing correct shutdowns or causing segfaults. It includes example code, setup instructions and insights into handling shutdown issues in large, multi-threaded codebases. Recently I removed all Google Ads from this site due to their invasive tracking, as well as Google Analytics. Please, if you found this content useful, consider a small donation using any of the options below: I'm developing an open sour"
  },
  {
    "title": "Scaling up linear programming with PDLP (research.google)",
    "points": 153,
    "submitter": "bookofjoe",
    "submit_time": "2024-09-21T12:57:11.000000Z",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=41609670",
    "comments": [
      "In this post, I don\u2019t see any comparisons of their solver to other LP solvers on benchmarks, so it\u2019s difficult to know how useful this is.\n \nreply",
      "I think it's partially excusable. Most LP solvers target large-scale instances, but instances that still fit in RAM. Think single-digit millions of variables and constraints, maybe a billion nonzeros at most. PDLP is not designed for this type of instances and gets trounced by the best solvers at this game [1]: more than 15x slower (shifted geometric mean) while being 100x less accurate (1e-4 tolerances when other solvers work with 1e-6).PDLP is targeted at instances for which factorizations won't fit in memory. I think their idea for now is to give acceptable solutions for gigantic instances when other solvers crash.[1] https://plato.asu.edu/ftp/lpfeas.html\n \nreply",
      "Hans D Mittlemann, you are my top dude when it comes to web design. A salute your aesthetic.[1] https://plato.asu.edu/\n \nreply",
      "FYI the table does not include the commercial top dogs (ibm cplex, gurobi, Fico Xpress), since due to politics they all withdrew\n \nreply",
      "Indeed those are the \"big four\" solver businesses in the West, and also probably the most reliably-good solvers. But by the time Gurobi withdrew from the benchmarks (a few weeks ago), COpt was handily beating them in the LP benchmarks, and closing down on them in MIP benchmarks. Solver devs like to accuse each other of gaming benchmarks, but I'm not convinced anyone is outright cheating right now. Plus, all solver companies have poached quite a bit from each other since cplex lost all its devs, which probably equalizes the playing field. So overall, I think Mittelmann benchmarks still provide a good rough estimate of where the SOTA is.\n \nreply",
      "Their numerical results with GPUs, compared to Gurobi, are quite impressive [1]. In my opinion (unless I'm missing something), the key benefits of their algorithms lie in the ability to leverage GPUs and the fact that there\u2019s no need to store factorization in memory. However, if the goal is to solve a small problem on a CPU that fits comfortably in memory, there may be no need to use this approach.[1] https://arxiv.org/pdf/2311.12180\n \nreply",
      "I agree that their results are impressive. Just to be clear, however:1. They compare their solver with a 1e-4 error tolerance to Gurobi with 1e-6. This may seem like a detail, but in the context of how typical LPs are formulated, this is a big difference. They have to do things this way because their solver simply isn't able to reach better accuracy (meanwhile, you can ask Gurobi for 1e-9, and it will happily comply in most cases).2. They disable presolve, which is 100% reasonable in a scientific paper (makes things more reproducible, gives a better idea of what the solver actually does). If you look at their results to evaluate which solver you should use, though, the results will be misleading, because presolve is a huge part of what makes SOTA solvers fast.\n \nreply",
      "hmm... I am reading [1] right now. When looking at their Table 7 and Table 11 in [1], they report comparison results with Gurobi presolve enabled and 1e-8 error. Do I miss anything?Their performance isn't quite as good as Gurobi's barrier method, but it's still within a reasonable factor, which is impressive.\n \nreply",
      "Regarding presolve: When they test their solver \"with presolve\", they use Gurobi's presolve as a preprocessing step, then run their solver on the output. To be clear, this is perfectly fair, but from the perspective of \"can I switch over from the solver I'm currently using\", this is a big caveat.They indeed report being 5x slower than Gurobi at 1e-8 precision on Mittelmann instances, which is great. Then again, Mittelmann himself reports them as 15x off COpt, even when allowed to do 1e-4. This is perfectly explainable (COpt is great at benchmarks; there is the presolve issue above; the Mittelmann instance set is a moving target), but I would regard the latter number as more useful from a practitioner's perspective.This is not to diminish PDLP's usefulness. If you have a huge instance, it may be your only option!\n \nreply",
      "They link to three of their papers that have more details.\n \nreply"
    ],
    "link": "https://research.google/blog/scaling-up-linear-programming-with-pdlp/",
    "first_paragraph": "We strive to create an environment conducive to many different types of research across many different time scales and levels of risk.Our researchers drive advancements in computer science through both fundamental and applied research.Our research teams have the opportunity to impact technology used by billions of people every day.We regularly open-source projects with the broader research community and apply our developments to Google products.Publishing our work allows us to share ideas and work collaboratively to advance the field of computer science.We make products, tools, and datasets available to everyone with the goal of building a more collaborative ecosystem.Supporting the next generation of researchers through a wide range of programming.Participating in the academic research community through meaningful engagement with university faculty.Connecting with the broader research community through events is essential for creating progress in every aspect of our work.September 20,"
  },
  {
    "title": "Infineon's CO2 Sensor Monitors Indoor Air Quality (allaboutcircuits.com)",
    "points": 56,
    "submitter": "WaitWaitWha",
    "submit_time": "2024-09-21T19:08:50.000000Z",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=41611965",
    "comments": [
      "We did some extensive testing of photo-acoustic and optical NDIR CO2 sensors indoors as well as outdoors and also in comparison to scientific reference instruments.I just updated the blog post that I wrote last year [1] with the specs of this new Infineon sensor and also our experiences with doing CO2 measurement outdoors.In summary we can say that under normal indoor conditions (e.g. small range of temperature, relative high range of CO2 etc.), the photo-acoustic NDIR sensors perform well.However, outdoors they significantly underperform optical NDIR sensors to the extent that they are barely usable. See for example this chart [2].It's hard to say why exactly they perform so poorly outdoors but I believe that they rely on quite complex internal algorithms that probably have been only developed with typical indoor conditions in mind and as soon as some parameters like temperature are out of the typical range, they significantly drop in their performance.Furthermore, we also detected that they are sensitive to interference from low frequency noise which could for example come from ventilation systems, refrigerators etc. This is not surprising as they rely on their internal microphone to do the photo-acoustic measurements.All of this and the rock-solid performance we see with optical NDIR sensors made us quite wary about the photo-acoustic sensors and not use them in our open-source hardware monitors. As I said, I believe they work quite well in general but because we saw also some really strange and unexpected behavior of these photo-acoustic sensors, I'd now always prefer optical NDIR sensors if possible.[1] https://www.airgradient.com/blog/co2-sensors-photo-acoustic-...[2] https://www.airgradient.com/blog/co2-sensors-photo-acoustic-...\n \nreply",
      "Seems similar to the SCD40 photoacoustic approach.I used that for an open-source CO2 monitor I designed:https://bitclock.io/https://github.com/goat-hill/bitclock\n \nreply",
      "Nice to see this miniaturization of photoacoustic spectroscopy - something I've done a bit of in the past.  It is an underappreciated technique.  Ordinarily one measures the difference in optical throughput with and without a sample.  If it is a weak absorber, it is a difference between two large numbers.  PAS is zero background.  No absorption, no pressure wave, no signal.  Any absorption stands out clearly against that zero background.\n \nreply",
      "Finally an actually working (cheap?) CO2 sensor?So many of those actually measure humidity, temp and VOCs and try to derive some sort of CO2 reading out of those.\n \nreply",
      "True. The cheap ones are trying to guess CO2. Those are called \"indoor air quality sensors\".Small CO2 sensors have been available for years, for about $50. Compare [1].Life of this new device is only 10 years, which is short for HVAC systems. A hotel might have a thousand of these.\nOlder devices say \"15+\" years.All these devices have a calibration problem. They drift. They try to correct by treating the lowest value they ever see as \"normal\" (that's about 400 ppm CO2 today, vs 300 PPM in 1950) and recalibrating. So they're not useful for observing a general increase in CO2. They're also not useful for greenhouses, where CO2 levels may drop below ambient CO2 due to photosynthesis.\nManual recalibration is possible but requires feeding in pure nitrogen and a known nitrogen/CO2 mixture.[2]Devices which don't need that re-calibration exist.[3] They're more complicated. Also don't seem to be stocked by the usual distributors.[1] https://rmtplusstoragesenseair.blob.core.windows.net/docs/pu...[2] https://www.co2meter.com/blogs/news/7512282-co2-sensor-calib...[3] https://www.murata.com/en-us/products/sensor/co2/overview/te...\n \nreply",
      "It's not really \"finally\". They introduced a similar sensor, the PASCO2V01, a couple years ago. That one has been available on a breakout board with the necessary support hardware from SparkFun for over a year [1].Comparing the datasheets for the PASCO2V01 and the new PASCO2V15 the old one actually seems a little better as far as CO2 measuring performance goes. They are the same on most things, but the old one has slightly better accuracy.The new one is \u00b1(50 ppm + 5%) between 400 ppm and 3000 ppm.The old one is \u00b1(30 ppm + 3%) between 400 ppm and 5000 ppm.The big difference is this:> Infineon has recently introduced the PASCO2V15, a new 5 V sensor to improve air quality monitoring in building environments.Both of them require a dual voltage power supply. They both want 3.3 V for their digital components and a higher voltage for their IR emitter.For the older one that higher voltage is 12 V. For the newer it is 5 V.[1] https://www.sparkfun.com/products/22956\n \nreply",
      "Iirc they were merged into esphome last year too.\n \nreply",
      "Sensirion has the SCD40, which appears to be based on the same principle. It's much cheaper than the SCD30.\n \nreply",
      "Yes, SCD30 series are optical while SCD40 series are photoacoustic. STC series are thermal conductivity based. STC and SCD40 are smaller than SCD30 but less accurate if memory serves (check datasheet).\n \nreply",
      "This was my thought exactly. I used sensors that were about $25/each in the past and those worked well but this would be seemingly way easier to integrate and get ahold of.\n \nreply"
    ],
    "link": "https://www.allaboutcircuits.com/news/infineons-co2-sensor-precisely-monitors-indoor-air-quality/",
    "first_paragraph": ""
  },
  {
    "title": "Fable at 20: a uniquely British video game with a complex legacy (theguardian.com)",
    "points": 14,
    "submitter": "n1b0m",
    "submit_time": "2024-09-18T10:17:10.000000Z",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=41577916",
    "comments": [
      "Related: One of the main designers of Fable recently released a free, highly imaginative Ultima-4-inspired game called Moonring. Check it out!https://store.steampowered.com/app/2373630/Moonring/\n \nreply",
      "Huh, what this whole article actually led me to find is that the winner of Curiosity never actually got any price. The price they were supposed to get was never even developed. I guess that\u2019s some way to get out of your contractual agreements. Wonder how that would have gone for them if the winner wasn\u2019t a clueless 18 year old.\n \nreply",
      "Molyneux is obviously infamous for over promises and under delivering, but Fable (Fable, Fable The Lost Chapters, etc.) had such a special place in my heart.An amazing game with raunchy wit, moving story, and great fantasy world building.  The music still rattles through my brain and I still occasionally say, \"Do you chase chickens, Chicken Chaser?\".I really enjoyed the games, but especially Fable and Fable TLC (which was an extended chapter at the end which changes the ending and takes you to a talked about continent that the base game did not).\n \nreply",
      "I vaguely recall playing Fable on the Xbox and being satisfied, though not blown away by it. It was certainly one of the most fleshed-out house-buying/marriage-allowing games I recall playing (not the first, though) and probably paved the way for more complete systems.I love Peter Molyneux and he's built amazing things, but he is certainly a hype factory.\n \nreply",
      "IIRC, II was considered superior, where a lot of the ideas were supposed to have matured and gelled to create a compelling experience. I didn't own a 360, so I never got to see for myself, but I did get to watch a few hours of gameplay while hanging out with my boyfriend at the time.A notable moment: he'd unlocked a Demon Door and was enthusiastically laying out how the game's real estate system worked, and his plans for the idyllic winter lodge he'd just found, when he walked his character inside and... well, I won't spoil the surprise. Suffice it to say, someone on the design team had a very good handle on what they were doing. It's always stuck out to me as an excellent example of how deeply game designers understand their systems, how those systems influence gamer behavior and expectations, and how to play on those expectations for emotional impact.\n \nreply"
    ],
    "link": "https://www.theguardian.com/games/2024/sep/18/fable-at-20-a-uniquely-british-video-game-with-a-complex-legacy",
    "first_paragraph": "In 2004, Fable was as famous for what it didn\u2019t deliver as for what it did. But this Python-esque fantasy game deserves to be remembered for more than thatIn 1985, brothers Dene and Simon Carter vowed to each other that they would one day start their own development studio together. The game they imagined was ambitious, as Simon outlined in a developer diary: a fantasy role-playing game, \u201cpopulated with compelling and convincing characters with real personality, people who actually reacted to what you did \u2026 We wanted each and every person who played our game to have a unique experience, to have their own stories to tell.\u201d The idea of a living, reactive game world was an obsession for many game creators (and players) at the time, largely because it had never yet been done. In the 1980s, a virtual fantasy world like this was far beyond the realms of technological possibility.Thirteen years later, they got the opportunity to make the game of their dreams, at their own studio Big Blue Box."
  },
  {
    "title": "Substack (YC W18) Is Hiring Machine Learning Engineers (grnh.se)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-09-21T21:00:41.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://grnh.se/d034f1ba5us",
    "first_paragraph": "San FranciscoOverviewAs an ML engineer at Substack, you will play a crucial role in developing and implementing cutting-edge machine learning solutions to enhance our product offerings. You will be part of a dynamic team, collaborating closely with software engineers and data scientists, to bring machine learning models into our codebase and integrate them seamlessly into our products. This role offers an exciting opportunity to shape the future of our technology stack and make a significant impact.\u00a0Substack\u2019s compensation package includes a market-competitive salary, equity for all full-time roles, and exceptional benefits. Our cash compensation salary range for this role is $185,000 - $240,000. Final offer amounts are determined by multiple factors including candidate experience and expertise and may vary from the amounts listed above.ResponsibilitiesRequirementsNice to haveSubstack is an equal opportunity employer. All applicants will be considered for employment without regard to r"
  },
  {
    "title": "Analyzing the OpenAPI Tooling Ecosystem (modern-json-schema.com)",
    "points": 65,
    "submitter": "handrews",
    "submit_time": "2024-09-21T19:13:41.000000Z",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=41612002",
    "comments": [
      "I find it striking that in the same day I saw a video about how someone \"Made an API in 20 minutes with one prompt\" and this. The two approaches seem very divergent. One that is almost cavalier about things like security, standards, etc and another that is (almost) over engineered.One observation, is that I there are two trains of thought. Using OAD (Open API Descriptions) as a source of truth and generating code from there or treating OAD as an artifact that comes out of some other tools.I personally see OpenAPI as kind of a glue that can allow different tooling to be able to speak the same language.Overall I found the linked Moonwalk[1] document to be more interesting. But there is some interesting analysis to be found in this article as well.[1] https://www.openapis.org/blog/2023/12/06/openapi-moonwalk-20...\n \nreply",
      "Yeah this article is more about how we (the OpenAPI Initiative) are designing the next versions of the OpenAPI Specification than it is about how to use it.  The diagram does include both an OAD generator and editor, intended to encompass both code-first and description-first (which doesn't make too much difference for this blog post).  The Moonwalk article is definitely more general purpose!  This is \"OK Moonwalk has a great vision, but how do we actually make it a real spec?\"  I've been using variations of this diagram in the weekly Moonwalk calls for the past month or two.\n \nreply",
      "> OK Moonwalk has a great vision, but how do we actually make it a real spec?I'm not sure the article really succeeds if that was the goal. I suspect that there might be some aspects of the discussion that are taking place that are missing from the article making it a little difficult for someone who wasn't in those discussions to connect the dots.Don't get me wrong, I think the article had some useful pieces in it, I just think if that was the goal of the article it could possibly use some additional framing for people who don't have the full context.With that said, I really appreciate transparency into the thought process!\n \nreply",
      "> I just think if that was the goal of the article it could possibly use some additional framing for people who don't have the full context.It's always a struggle to figure out how much explanation to put in before people see something like \"20 minute read\" and just refuse to read it.  (BTW I don't mind the critical feedback at all- I'm just glad you found something useful in it).But keep in mind that _we_ haven't answered \"how do we actually make it a real spec?\" either!  This is a snapshot of our efforts at this particular moment. \n Also, there's a reason that this is \"part one in a series\" :-)\n \nreply",
      "Thanks for writing this!  This nicely breaks it down into boxes that OpenAPI deals with.I still think OpenAPI usage is a bit confusing in general. For example, I am still waiting for a better explanation of this diagram with relation to a choice of backend (Python WSGI) + frontend (JS) combinations. Perhaps someone here has a pointer for me to read?\n \nreply",
      "If you haven\u2019t yet, I highly recommend to check FastAPI for your python backend: https://fastapi.tiangolo.com/. OpenAPI is a core part of it, making it simple to integrate with other tools such as docs and clients generator\n \nreply",
      "Please, people, just use GRPC or Thrift. This stuff makes me want to vomit.\n \nreply",
      "Excellent article, I really like the diagrams\n \nreply"
    ],
    "link": "https://modern-json-schema.com/analyzing-the-openapi-tooling-ecosystem",
    "first_paragraph": "12 min readWelcome to a new series of posts that will take you on a visual journey through the OpenAPI Specification (OAS) and its tooling ecosystem!As part of the efforts to design OAS 3.2 and 4.0 \u201cMoonwalk\u201d, I wanted to figure out how different sorts of tools work with the OAS. Moonwalk is an opportunity to re-think everything, and I want that re-thinking to make it easier to design, implement and maintain tools. We also want 3.2 to be an incremental step towards Moonwalk, so figuring out what improvements we can make to align with the needs of tools while maintaining strict compatibility with OAS 3.1 is also important.To do that, we need to understand how the OAS-based tooling ecosystem works, and how the various tools relate to the current OAS version, 3.1. This eventually led me to create two groups of diagrams: One about the architecture of OAS-based tools, and one about the many objects and fields defined by the OAS and how they relate to what tools need to do. But this was not "
  },
  {
    "title": "Social Initiation (truman.edu)",
    "points": 53,
    "submitter": "yamrzou",
    "submit_time": "2024-09-21T13:01:07.000000Z",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=41609694",
    "comments": [
      "Some good advice, but a lot of stuff is just weird or robotic and it has some surprisingly judgmental comments.The gender page is just strange, most of those sounds so american or old fashioned.edit: sources are mostly old, there are no sources from less than 10 years which is bad if you're trying to describe current social behaviors, the average source on the gender page is from 2002...\n \nreply",
      "The rules are indeed stated in a robotic, and somewhat patronizing way, as if written by a mild aspie to teach a stronger aspie the rules.As you note, most of them are generally true though, and some just kind of obvious to a high empathy person.\n \nreply",
      "> The rules are indeed stated in a robotic, and somewhat patronizing way, as if written by a mild aspie to teach a stronger aspie the rules.Doesn't is kind of have to be? That \"curse of knowledge\" thing makes it kind of hard to explain things to people who's skill on whatever topic is more than a level or two below your own.\n \nreply",
      "> As you note, most of them are generally true thoughNot really, it's so mixed that  I wouldnt advise a neurodivergent person to follow them, how would you know which one is good?I don't think the author is sympathetic to autistic people:\"If you engage in less socially acceptable self-stimulatory behaviors that involve clenched muscles, quick jerky movements, rocking, or vocalizations, strangers will likely be afraid to talk to you, and even people you already know may be embarrassed to be with you in public.\"You shouldn't be with people that are embarrassed to be with you, as those behaviors are usually not controllable, this is terrible.\n \nreply",
      "Sometimes you can't help it, sometimes you are related to those people, and sometimes they also can't help it. If I'm out with a friend who has a severe disorder that means he can't help but make a loud \"whoop\" sound every minute or so, am I a bad person for feeling embarrassment, even if that feeling is uncontrollable? People don't usually choose to feel embarrassed. It's as helpful to tell somebody to not feel embarrassed as it is to tell somebody with verbal tics to simply not have them.\n \nreply",
      "Reality is often disappointing.That doesn't make ignoring it a good idea.\n \nreply",
      "I\u2019m under the impression that most of those kinds of actions are involuntary. If anything I\u2019m pretty sure people are wildly aware of the fact that their ticks make people uncomfortable, having lived with them their whole lives.\n \nreply",
      "I think this could be helpful for those who are especially shy or on the spectrum. It is not optimized for viewing on a phone.\n \nreply",
      "Use Firefox and press the button next to the URL to get a clean format.\n \nreply",
      "That removes a lot of the content. Much of this is inside clickable menus that reader mode doesn't capture.For a site that is apparently for people with disorders, the accessibility is somewhat appalling.\n \nreply"
    ],
    "link": "https://socialcommunication.truman.edu/attitudes-emotions/social-initiation/",
    "first_paragraph": " Getting Started in Social Situations\u00a0\u00a0\u00a0There are times when you can\u2019t avoid meeting new people: when you start a new job (or a new worker starts at your workplace), when you start a new class, when you meet new neighbors, when someone you already know introduces you to someone else, etc. It does matter what these people think of you, even if you don\u2019t become close friends. If your co-workers or boss don\u2019t like you, it will make your work more difficult. If your classmates or teacher don\u2019t like you, it will make the classroom experience very difficult. If your friend\u2019s friend thinks you\u2019re a jerk, your friend will be embarrassed by you. For most people, then, small talk with strangers is a way to practice the skills they will need to make a good first impression on those whose opinions of you do matter.You should be aware, however, that men are much more likely to interpret a woman striking up a conversation as an attempt to flirt than women are apt to do when the roles are reversed, s"
  },
  {
    "title": "A Simple Spaced Repetition Algo (In Ugly SQL) (taylor.town)",
    "points": 17,
    "submitter": "surprisetalk",
    "submit_time": "2024-09-17T14:21:47.000000Z",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=41567862",
    "comments": [
      "Why wouldn\u2019t you use something like this instead?WITH episode_ AS (...), -- stale episodes\ns_ AS (\n  SELECT\n    e.feed_id,\n    s.card_id,\n    MAX(e.created_at) AS last_created_at,\n    AVG(COALESCE(s.score::INT::REAL, 0.5)) AS _avg,\n    COUNT(s.score) AS _count\n  FROM score s\n  INNER JOIN episode e ON s.episode_id = e.episode_id\n  INNER JOIN feed f ON e.feed_id = f.feed_id\n  WHERE f.usr_id IS NOT NULL\n  GROUP BY e.feed_id, s.card_id\n)\nINSERT INTO episode (episode_id, feed_id, card_ids)\nSELECT LEFT(MD5(RANDOM()::TEXT), 16), feed_id, ARRAY_AGG(card_id)\nFROM (\n  SELECT feed_id, card_id\n  FROM (\n    SELECT\n      feed_id,\n      card_id,\n      per_episode,\n      ROW_NUMBER() OVER (\n        PARTITION BY feed_id\n        ORDER BY\n          COALESCE(s_.last_created_at, c.created_at, NOW())\n          + f.every * RANDOM() * COALESCE(fd.chaos, 0.3)\n          + f.every * COALESCE(s_._count, 7) ^ COALESCE(fd.deceleration, 0.7) * s_._avg ^ COALESCE(fd.feedback, 1.2)\n          + f.every * LN(1 + c.bytes_b) * COALESCE(fd.ease, 1.0),\n          RANDOM()\n      ) AS n\n    FROM episode_ e_\n    INNER JOIN feed f USING (feed_id)\n    INNER JOIN feed_deck fd USING (feed_id)\n    INNER JOIN card c USING (deck_id)\n    LEFT JOIN s_ ON s_.feed_id = e_.feed_id AND s_.card_id = c.card_id\n    WHERE NOT c.card_id = ANY(fd.hidden_card_ids)\n  ) a\n  WHERE per_episode >= n\n) b\nGROUP BY feed_id;\n \nreply",
      "So weird how some minds think alike in more than one aspect.I already loved your writing, but I never would have predicted that anyone else (besides me) was trying to put spaced repetition algorithms into SQL.I've been in touch with the guys making Open Spaced Repetition:\nhttps://github.com/open-spaced-repetition... specifically the Free Spaced Repetition Scheduler (FSRS), of which they recommended if we're going to copy one of the implementations it should be the TypeScript one, since that's the best documented:\nhttps://github.com/open-spaced-repetition/ts-fsrs\n + https://open-spaced-repetition.github.io/ts-fsrs/I've been working on it the last few days and was working on it this morning, then I saw this/your post on HN. Jaw dropped.I always enjoy making things from scratch as a way of getting to know them, but in this case it seems wise (and a bit of a new experience for me) to copy someone else's open source work and try to do a real implementation of the FSRS-5 scheduler in PostgreSQL PL/pgSQL functions.Then cards and their data-logic can stay together in one place, and anyone can use whatever language they want for UI/API.\n \nreply"
    ],
    "link": "https://taylor.town/flashcasts-srs-algo",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Parse your Postgres queries into a fully-typed AST in TypeScript (github.com/pg-nano)",
    "points": 44,
    "submitter": "aleclarsoniv",
    "submit_time": "2024-09-18T07:49:34.000000Z",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=41576956",
    "comments": [
      "Hey, this is really cool.Suggestion: check out the Slonik library for Postgres. It encourages writing raw SQL using string template literals (i.e. sql`select foo from bar where zed = ${someParam}`). It also supports strong typing of results, but the programmer needs to create their own Zod validators manually to check this and get strong typing support.Seems like this tool could essentially pre-create the types for any raw Postgres SQL statement?I think this approach of using raw SQL is much better than a custom query builder like kysely, where half the time I'm just trying to translate the SQL that I know in my head to some custom, library-specific API. But the benefit of using kysely is the automatic typing support. Being able to use raw SQL and get query types \"for free\" would be amazing.\n \nreply",
      "This is interesting. You seem to provide extra functionality besides the typescript types over libpg-query, like the walk function, right? I assume that's the reason these changes can't be easily merged into the main library and you chose to fork entirely.As an aside, do you think it's possible to use your libraries to get the return type of an arbitrary postgres query, even if it's dynamic? I have a library that requires users to write the return type of queries manually always, and I'd like to automate that step.\n \nreply",
      "The main reason I didn't contribute my changes via PR is I wanted a package without \"deparse\" support, which adds considerably to the install size. I also didn't want pre-compiled binaries for every supported platform to be published to NPM, preferring a postinstall script that only downloads the necessary binary. I don't know how the walk function would be received by the original maintainers, as I didn't ask.> do you think it's possible to use your libraries to get the return type of an arbitrary postgres query, even if it's dynamic?Yes it is. I've solved that exact problem in pg-nano. I use the `describePrepared` feature of libpq: https://github.com/pg-nano/pg-nano/blob/4cca3dbe6be609c479e4...\n \nreply",
      "Well the question you saw coming (hopefully) - how does it compare to Prisma use cases?One thing I really like about Prisma is only updating my schema and having migrations generated as the \"diff\".\n \nreply",
      "Are those migrations still editable, out of curiosity? Oftentimes I'll want to have migrations add things that simply aren't possible with some ORMs (e.g. generated columns, which can't be schema'd in most ORMs).The main thing holding me back from Prisma is precisely what you like about it - if the migrations are auto-generated and I can't edit them afterward, I won't be able to do what I need to.\n \nreply",
      "You can edit Prisma migrations. They are plain SQL files with no magic.\n \nreply",
      "question: we are using kysely.dev as postgresql query builder and porsager's postgres.js for high performance.. is this something that can complement our stack or something to replace it entirely?\n \nreply"
    ],
    "link": "https://github.com/pg-nano/pg-parser",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Parse your Postgres queries into a 100% type-safe AST (powered by libpg_query)\n      A fork of libpg-query with best-in-class type definitions and AST utilities.The major and minor version of this package is meant to be aligned with the supported PostgreSQL major and minor version. Older and newer versions may be compatible, but this is not guaranteed.Upon install, the pre-compiled binary for your operating system and architecture will be pulled from GitHub Releases.This package exports the following functions:Note: There is no deparse function (for turning an AST back into a string) included, as this isn't needed for my use case.I've added a walk function for easy AST traversal. You can pass a callback or a visitor object. You can return false to not walk into the children of the current node.Each node passed to your visitor is wra"
  },
  {
    "title": "Emacs Speaks Statistics (r-project.org)",
    "points": 29,
    "submitter": "smartmic",
    "submit_time": "2024-09-21T18:20:26.000000Z",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=41611681",
    "comments": [
      "My colleagues and I have been discussing recently if we should use Positron IDE as a common environment for teaching R and Python. Emacs is another option, of course.\n \nreply",
      "This semester is my first using Posit Cloud. It removes the headache whether you're teaching R or Python. The only thing I've found annoying is the need for students to install their own packages.I ran my own RStudio Server on Digital Ocean for years. That worked very well, with students not needing to install packages, but maintaining things myself created more work for me.Before that I had to deal with an unending string of support for the students using their own computers. I would never go back to that again.I used Cocalc once during the pandemic. It has a great set of features, but the students found the interface confusing.\n \nreply",
      "I think it depends on how new your students are to programming. If they are already coding and have their favorite IDE then it would be nice to force them to try something new to see a different workflow (even if they end up going back to vscode later).On the other hand for newer programmers you want to focus less on the tools, installation and workflow and more on just getting them programming. In that situation the batteries included environments like RStudio would be preferred.Another practical thing to think about is how much you and your TAs want to wrestle with installation issues vs just giving them a web based notebook environment like Colab.\n \nreply",
      "I love Emacs, but I think it would be challenging to teach an intro programming course using it these days. (We landed on VSCode for teaching Python.)\n \nreply",
      "The basics of Emacs are not that hard. One of my introductory freshman courses taught us XEmacs in parallel, as we went through Lex & Yacc.Basically you need to learn a few basic movements, which are also really useful on any terminal thanks to GNU Readline, and a some basic concepts like the minibuffer, interactive commands, etc. From there onward, things are quite easy to discover little by little, especially with newer packages like vertico, marginalia or which-key.Obviously, its much harder than VSCode, but investing on Emacs is IMHO worth the effort if one values stability. It will be probably still relevant when most competing solutions are gone. Plus, it offers great support for lots of niche languages and workflows, like Org.\n \nreply",
      "> Basically you need to learn a few basic movements, which are also really useful on any terminal thanks to GNU ReadlineThis is one of the biggest reasons why I\u2019m slowly switching to Emacs for most of my editing needs.  Those movement keyboard shortcuts are ubiquitous on UNIX family operating systems.  They even work in most native text editing fields in macOS and iOS.\n \nreply",
      "I agree, having a subset of Emacs/Readline keybindings available everywhere on macOS is really convenient and made Emacs quite natural to use for me.GNOME used to have an Emacs GTK key theme, but it is no longer available as of GTK 4.0?\n \nreply"
    ],
    "link": "https://ess.r-project.org/index.php?Section=home",
    "first_paragraph": "Welcome to the home page of the ESS project"
  }
]