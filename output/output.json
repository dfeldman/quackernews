[
  {
    "title": "Nano Banana can be prompt engineered for nuanced AI image generation (minimaxir.com)",
    "points": 438,
    "submitter": "minimaxir",
    "submit_time": "2025-11-13T17:39:13 1763055553",
    "num_comments": 114,
    "comments_url": "https://news.ycombinator.com/item?id=45917875",
    "comments": [
      "I have been generating a few dozen images per day for storyboarding purposes. The more I try to perfect it, the easier it becomes to control these outputs and even keep the entire visual story as well as their characters consistent over a few dozen different scenes; while even controlling the time of day throughout the story. I am currently working with 7 layers prompts to control for environment, camera, subject, composition, light, colors and overall quality (it might be overkill, but it\u2019s also experimenting).I also created a small editing suite for myself where I can draw bounding boxes on images when they aren\u2019t perfect, and have them fixed. Either just with a prompt or feeding them to Claude as image and then having it write the prompt to fix the issue for me (as a workflow on the api). It\u2019s been quite a lot of fun to figure out what works. I am incredibly impressed by where this is all going.Once you do have good storyboards. You can easily do start-to-end GenAI video generation (hopping from scene to scene) and bring them to life and build your own small visual animated universes.reply",
      "We use nano banana extensively to build video storyboards, which we then turn into full motion video with a combination of img2vid models. It sounds like we're doing similar things, trying to keep images/characters/setting/style consistent across ~dozens of images (~minutes of video). You might like the product depending on what you're doing with the outputs! https://hypernatural.aireply",
      "Your \"Dracula\" character is possibly the least vampiric Dracula I've ever seen tbhreply",
      "I agree. Bruhcula? Something like that. He's a vampire, but also models and does stunts for Baywatch - too much color and vitality. Joan of Arc is way more pale.Maybe a little mode collapse away from pale ugliness, not quite getting to the hints of unnatural and corpse-like features of a vampire - interesting what the limitations are. You'd probably have to spend quite a lot of time zeroing in, but Google's image models are supposed to have allowed smooth traversal of those feature spaces generally.reply",
      "That looks exactly like the photos on a Spirit Halloween costume.reply",
      "People pay consulting firms good money to be told their ideal customer so plainly!reply",
      "Having a Statue of Liberty character available is for some reason so funny to me.reply",
      "lol you can make your own Dracula if you want him to look different: https://hypernatural.ai/charactersreply",
      "Yes we are definitely doing the same! For now I\u2019m just familiarizing myself in this space technically and conceptually. https://edwin.genego.io/blogreply",
      "> The more I try to perfect it, the easier it becomes\nI have the opposite experience, once it goes off track, its nearly impossible to bring it back on messagereply"
    ],
    "link": "https://minimaxir.com/2025/11/nano-banana-prompts/",
    "first_paragraph": "You may not have heard about new AI image generation models as much lately, but that doesn\u2019t mean that innovation in the field has stagnated: it\u2019s quite the opposite. FLUX.1-dev immediately overshadowed the famous Stable Diffusion line of image generation models, while leading AI labs have released models such as Seedream, Ideogram, and Qwen-Image. Google also joined the action with Imagen 4. But all of those image models are vastly overshadowed by ChatGPT\u2019s free image generation support in March 2025. After going organically viral on social media with the Make me into Studio Ghibli prompt, ChatGPT became the new benchmark for how most people perceive AI-generated images, for better or for worse. The model has its own image \u201cstyle\u201d for common use cases, which make it easy to identify that ChatGPT made it.Two sample generations from ChatGPT. ChatGPT image generations often have a yellow hue in their images. Additionally, cartoons and text often have the same linework and typography.Of n"
  },
  {
    "title": "Zed is our office (zed.dev)",
    "points": 467,
    "submitter": "sagacity",
    "submit_time": "2025-11-13T15:41:26 1763048486",
    "num_comments": 233,
    "comments_url": "https://news.ycombinator.com/item?id=45916196",
    "comments": [
      "I generally like what Zed is trying to become. However, all of these features and blog posts are frustraing when they struggle to keep basic editor features stable. Edit a file outside of the editor? It's not going to show up in the project pane or the git diff. Need to work inside a container because it's 2025 and we don't need to clutter our local machine with 100s of dependencies and env managers... well now all the AI stuff is broken. ACP sounds cool until you realize every single CLI in existence works better.My wish is that Zed gets the core working correctly 100% of the time before moving on to expanding feature sets. For now I'm back in NeoVIM because it always works the first time....https://github.com/zed-industries/zed/issues/38109Hopefully soon I can give it another shot at full time usage.reply",
      "My favorite bug was Claude Code kept editing some template file. Zed kept auto-formatting the file, which was BREAKING the template itself. It took me like 30 minutes of watching Claude implode, and literally using \"ed\" to edit the line, before I realized, then I started asking Claude how to turn off the Zed formatters to which it was like AH THAT MAKES MORE SENSE, which I thought was hilarious after it tried everything from editorconfig onwards.reply",
      "> Need to work inside a container because it's 2025 and we don't need to clutter our local machine with 100s of dependencies and env managers...\u201cCan I tell you about our lord and saviour Nix?\u201dKidding, but seriously though I\u2019ve found having to work in a container to be a bit clumsy, even with good tooling around it. As you said it\u2019s 2025, and there are other ways to have reproducible toolchains that don\u2019t pollute the rest of your system environment Nix or otherwise.reply",
      "In the case of AI tools it is for security reasons, not just reproducible toolchains.reply",
      "We need more selinux and sandbox-exec in daily dev lives. There's no reason to bring a whole new system along just to restrict some access.reply",
      "It says they're targeting Spring 2026 for their 1.0 release, so I'll treat as beta and put a calendar entry in for April 2026 to check back in on it.reply",
      "With the AI stuff, it feels like they invested a bit prematurely. When the Agentic editing demo came out (6 months, 10 months ago? It\u2019s a blur), it felt right. Accepting and reviewing edits, live tracking ..etc., felt like pair programming. The ACP addition felt like a natural evolution .With the continuous improvement in CLI tools and people\u2019s experience with them, it feels like doing a live review or edit-by-edit approvals all feel like a drag. I personally have come to avoid using the IDE/Editor. I just kick up Claude code - plan mode, auto-accept edits. Once the session is done, switch to the editor and make necessary adjustments. I suspect people with Max subscription and \u201cdangerously-skip-permissions\u201d \u2026etc won\u2019t even care if an editor has AI integration or not.reply",
      "I used to only use JetBrains for AI stuff, now I just open everything in Zed because of its Claude Code integration. Especially with the linters and other nice to haves. I am insanely close to cancelling JetBrains.reply",
      "The only editor integration I think is semi useful is wiring it into Diagnostics/Problems data that the editor has from extensions. Speeds up the agents flow quite a bit when it leans on that to check its work vs always executing (say) \u201ceslint\u201d directly.But that can be done easily enough with an MCP extension for your editor/IDE of choicereply",
      "Claude hooks are great for this.reply"
    ],
    "link": "https://zed.dev/blog/zed-is-our-office",
    "first_paragraph": "Joseph LyonsNovember 13th, 2025It's Monday, 12 PM ET, and the entire Zed Industries team is piled into our weekly all-hands meeting.\nSome teammates jot down their schedule deviations, while others detail what they intend to focus on for the week.\nNathan just wrapped up top-of-mind announcements and Morgan is sharing trends from our metrics and covering operational updates.\nMeanwhile I'm preparing user quotes from the last week to share out, and others add topics to the Discussions section.Throughout the meeting, screens are being shared, various voices are popping in and out of the conversation, and our notes are growing rapidly as dozens of cursors are concurrently editing the same file in real-time.This entire meeting is taking place inside Zed.Our mission from the beginning has been to engineer an editor that will be:Setting the first two properties aside, let's focus on collaboration.We've been dreaming of building the ultimate collaborative editor for years.\nThe roots of this visi"
  },
  {
    "title": "Apple Mini Apps Partner Program (developer.apple.com)",
    "points": 19,
    "submitter": "soheilpro",
    "submit_time": "2025-11-14T00:41:08 1763080868",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=45922550",
    "comments": [
      "This article explains this new program for those (like me) who had no idea what a \"mini app\" was and why it matters: https://www.macrumors.com/2025/11/13/apple-announces-mini-ap...tldr: it will let Apple charge a commission (although at 15%, it's half the normal 30% rate for the app store) on popular web app games embedded in to WeChat for the Chinese marketreply",
      "> although at 15%, it's half the normal 30% rate for the app store15% is the normal rate for the App Store. Only developers earning above $1MM/yr through the App Store have to pay 30%, the vast majority of developers only pay 15%.reply",
      "Fuck Apple and fuck being forced to pay them for their shitty App Store.reply",
      "Haha what? Just buy a different phone, mate.reply",
      "Industry wide tax, even the mafia gave you a better vig.reply",
      "I really detest Walmart too. OMG and Starbucks too. Amazon is the worst. And man oh man do I hate McDonald\u2019s.You know what I do?I don\u2019t give any of them a cent. Ever.It\u2019s cool we can choose what is and is not part of our lives.reply",
      "Don\u2019t buy iOS devices then.I look at mobile devices, especially iOS, as \u201cconsoles\u201d akin to a Nintendo Switch. It\u2019s not a \u201creal computer,\u201d the definition of which requires the ability to run any code I want. It does whar it does pretty well and it is what it is.reply",
      "FWIW this is how most informed consumers think as well. People buy iOS, consoles, etc because they want a walled garden. I think the real way out is getting consumers to see and value the benefits of leaving it.reply",
      "And where has such retreat led us? Rootable Androids are vanishing, Google is set to prevent side-loading entirely, and countless apps refuse to work on rooted devices.You either force the companies to stop, to restore your control over your devices, or be dragged by the uninformed consumer masses into slavery.reply",
      "They are not forcing you to make apps and put them on the store. If you want their services, then pay them. If not, then good luck to ya.reply"
    ],
    "link": "https://developer.apple.com/programs/mini-apps-partner/",
    "first_paragraph": "View in EnglishSince 2017, the App\u00a0Store has supported mini apps and games \u2014 self-contained experiences built with web technologies offered within a native host app. The Mini\u00a0Apps Partner\u00a0Program provides an improved customer experience for mini app users while helping developers who host mini apps and games grow their business on the App\u00a0Store.This program is designed for developers who host mini apps and games, which are experiences that are built using web technologies like HTML5 or JavaScript and distributed within a larger, native app. Participating apps are required to support certain App\u00a0Store technologies, including the Declared Age Range API and the Advanced Commerce API in order to provide a safe and seamless experience for customers. As a result, program members earn 85% of qualifying In\u2011App\u00a0Purchase sales within qualifying mini apps.To be eligible for the Mini\u00a0Apps Partner\u00a0Program:If you\u2019d like to participate in the Mini\u00a0Apps Partner\u00a0Program, submit a request form. Please n"
  },
  {
    "title": "650GB of Data (Delta Lake on S3). Polars vs. DuckDB vs. Daft vs. Spark (dataengineeringcentral.substack.com)",
    "points": 66,
    "submitter": "tanelpoder",
    "submit_time": "2025-11-13T21:33:26 1763069606",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=45920881",
    "comments": [
      "Awk? https://adamdrake.com/command-line-tools-can-be-235x-faster-...reply",
      "Honestly this benchmark feels completely dominated by the instance's NIC capacity.They used a c5.4xlarge that has peak 10Gbps bandwidth, which at a constant 100% saturation would take in the ballpark of 9 minutes to load those 650GB from S3, making those 9 minutes your best case scenario for pulling the data (without even considering writing it back!)Minute differences in how these query engines schedule IO would have drastic effects in the benchmark outcomes, and I doubt the query engine itself was constantly fed during this workload, especially when evaluating DuckDB and Polars.The irony of workloads like this is that it might be cheaper to pay for a gigantic instance to run the query and finish it quicker, than to pay for a cheaper instance taking several times longer.reply",
      "It would be amusing to run this on a regular desktop computer or even a moderately nice laptop (with a fan - give it a chance!) and see how it does. 650GB will stream in quite quickly from any decent NVMe device, and those 8-16 cores might well be considerably faster than whatever cores the cloud machines are giving you.S3 is an amazingly engineered product, operates at truly impressive scale, is quite reasonably priced if you think of it as warm-to-very-cold storage with excellent durability properties, and has performance that barely holds a candle to any decent modern local storage device.reply",
      "Absolutely. I recently reworked a bunch of tests and found my desktop to outcompete our (larger, custom) Github Action runner by roughly 5x. And I expect this delta to increase a lot as you lean on the local I/O harder.It really is shocking how much you're paying given how little you get. I certainly don't want to run a data center and handle all the scaling and complexity of such an endeavour. But wow, the tax you pay to have someone manage all that is staggering.reply",
      "Everyone wants a data lake when what they have a is a data pond.reply",
      "10Gbps only? At Google where this type of processing would automatically be distributed, machines had 400Gbps NICs, not to mention other innovations like better TCP congestion control algorithms. No wonder people are tired of distributed computing.reply",
      "\"At Google\" is doing all the heavy lifting in your comment here, with all due respect. There is but one Google but remain millions of us who are not \"At Google\".reply",
      "I'm kind of suprised they didn't choose an ec2 instance with higher throughput. S3 can totally eek out 100s of Gibps with the right setup.BUT the author did say this is the simple stupid naive take, in which case DuckDB and Polars really shined.reply",
      "If I understand correctly, polars relies on delta-rs for Delta Lake support, and that is what does not support Deletion vectors: https://github.com/delta-io/delta-rs/issues/1094It seems like these single-node libraries can process a terabyte on a typical machine, and you'd have have over 10TB before moving to Spark.reply",
      "> It seems like these single-node libraries can process a terabyte on a typical machine, and you'd have have over 10TB before moving to Spark.I'm surprised by how often people jump to Spark because \"it's (highly) parallelizable!\" and \"you can throw more nodes at it easy-peasy!\" And yet, there are so many cases where you can just do things with better tools.Like the time a junior engineer asked for help processing 100s of ~5GB files of JSON data which turned out to be doing crazy amounts of string concatenation in Python (don't ask). It was taking something like 18 hours to run, IIRC, and writing a simple console tool to do the heavy lifting and letting Python's multiprocessing tackle it dropped the time to like 35 minutes.Right cool for the right job, people.reply"
    ],
    "link": "https://dataengineeringcentral.substack.com/p/650gb-of-data-delta-lake-on-s3-polars",
    "first_paragraph": ""
  },
  {
    "title": "OpenMANET Wi-Fi HaLow open-source project for Raspberry Pi\u2013based MANET radios (openmanet.net)",
    "points": 66,
    "submitter": "hexmiles",
    "submit_time": "2025-11-13T21:18:14 1763068694",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=45920677",
    "comments": [
      "MANET is one of the protocols I was involved in implementing for a certain network protocol suite back around 2012.  Mesh routing protocols only work for the most limited of use cases.  They don't know about the capacity of the underlying wireless network and basically fall apart when things are congested or there are radios with poor reception.  QoS is implemented far better in modern cell phone networks, and if the routing protocol doesn't take QoS into account, it's gonna suck.reply",
      "In case anybody is like me and didn't know what Wi-Fi HaLow is: https://en.wikipedia.org/wiki/IEEE_802.11ahreply",
      "This guy has been promoting and hacking hardware around this project heavily the last few months: https://www.youtube.com/watch?v=550fh2n5rUsreply",
      "That guy is not contributing to the open source project.reply",
      "Which is why OP says \u201daround\u201d the project? Never claimed he is a contributor to the code.reply",
      "MANETs: back from the dead!? The problem is not the hardware, but the software; apparently, nobody can think of a killer application.reply",
      "The killer application in this case is ATAK.https://en.wikipedia.org/wiki/Android_Team_Awareness_Kitreply",
      "Is ATAK even useful to civilians? Is it trustworthy?reply",
      "Yes and yes, we've used it for civilian Search & Rescue in tandem with CalTopo.reply",
      "Meshtastic has been popular, but relies upon a terrible implementation of a mesh -- and it's vastly oversold on its capabilities.I understand some hams run a meshtastic repeater primarily to convince meshtastic users to become hams.reply"
    ],
    "link": "https://openmanet.net/",
    "first_paragraph": "\n        OpenMANET is an open-source project for building Raspberry Pi\u2013based MANET radios on Wi-Fi HaLow (915 MHz) using Morse Micro chipsets. \n        A MANET (Mobile Ad-Hoc Network) is a self-forming wireless mesh where each node connects directly without centralized infrastructure. \n        This technology is especially useful in the civilian space for search and rescue, disaster response, airsoft events, and any disconnected communications scenario. \n        Designed to be budget-friendly with excellent long-range performance.\n\u00a9  OpenMANET \u2022 Open source, field-ready mesh radios"
  },
  {
    "title": "Launch HN: Tweeks (YC W25) \u2013 Browser extension to deshittify the web (tweeks.io)",
    "points": 170,
    "submitter": "jmadeano",
    "submit_time": "2025-11-13T16:03:04 1763049784",
    "num_comments": 136,
    "comments_url": "https://news.ycombinator.com/item?id=45916525",
    "comments": [
      "This looks cool and could be a much needed step towards fixing the web.Some questions:[Tech]1. How deep does the modification go? If I request a tweek to the YouTube homepage, do I need to re-specify or reload the tweek to have it persist across the entire site (deeply nested pages, iframes, etc.)2. What is your test and eval setup? How confident are you that the model is performing the requested change without being overly aggressive and eliminating important content?3. What is your upkeep strategy? How will you ensure that your system continues to WAI after site owners update their content in potentially adversarial ways? In my experience LLMs do a fairly poor job at website understanding when the original author is intentionally trying to mess with the model, or has overly complex CSS and JS.4. Can I prompt changes that I want to see globally applied across all sites (or a category of sites)? For example, I may want a persistent toolbar for quick actions across all pages -- essentially becoming a generic extension builder.[Privacy]5. Where and how are results being cached? For example, if I apply tweeks to a banking website, what content is being scraped and sent to an LLM? When I reload a site, is content being pulled purely from a local cache on my machine?[Business]6. Is this (or will it be) open source? IMO a large component of empowering the user against enshittification is open source. As compute commoditizes it will likely be open source that is the best hope for protection against the overlords.7. What is your revenue model? If your product essentially wrestles control from site owners and reduces their optionality for revenue, your arbitrage is likely to be equal or less than the sum of site owners' loss (a potentially massive amount to be sure). It's unclear to me how you'd capture this value though, if open source.8. Interested in the cost and latency. If this essentially requires an LLM call for every website I visit, this will start to add up. Also curious if this means that my cost will scale with the efficiency of the sites I visit (i.e. do my costs scale with the size of the site's content).Very cool.Cheersreply",
      "> 1. How deep does the modification go? If I request a tweek to the YouTube homepage, do I need to re-specify or reload the tweek to have it persist across the entire site (deeply nested pages, iframes, etc.)If you're familiar with Greasemonkey, we work similar to the @match metadata. A given script could have a specific domain like (https://www.youtube.com/watch?v=cD5Ei8bMmUk) or all videos (https://www.youtube.com/watch*) or all of youtube (https://www.youtube.com/*) or all domains (https:///). During generation, we try to infer your intent based on your request (and you can also manually override with a dropdown.> 2. What is your test and eval setup? How confident are you that the model is performing the requested change without being overly aggressive and eliminating important content?Oh boy, don't get me started. We have not found a way to automate eval yet. We can automate \"is there an error?\", \"does it target the right selectors\", etc. But the request are open ended so there are 1M \"correct\" answers. We have a growing set of \"tough\" requests and when we are shipping a major change, we sit down, generate them all, and click through and manually check pass/fail. We built tooling around this so it is actually pretty quick but definitely thinking about better automation.This is also where more users comes in. Hopefully you complain to us if it doesn't work and we get a better sense of what to improve!> 3. What is your upkeep strategy? How will you ensure that your system continues to WAI after site owners update their content in potentially adversarial ways? In my experience LLMs do a fairly poor job at website understanding when the original author is intentionally trying to mess with the model, or has overly complex CSS and JS.Great question. The good news is that there are things like aria labels that are pretty consistent. If the model picks the right selectors, it can be pretty robust to change. Beyond that, hopefully it is as easy as one update request (\"this script doesn't work anymore, please update the selectors\"). Though we can't really expect each user to do that, so we are thinking of an update system where e.g. if you install/copy script A, and then the original script A is updated, you can pull that new update. The final stage of this is an intelligent system where the script can heals itself (every so often, it assess the site, sees if selectors have changed and fixes itself) -> that is more long-term.> 4. Can I prompt changes that I want to see globally applied across all sites (or a category of sites)? For example, I may want a persistent toolbar for quick actions across all pages -- essentially becoming a generic extension builder.\nYes, if domain is https:/// it applies to all sites so you can think of this as a meta-extension builder. E.g. I have a timer script that applies across reddit, linkedin, twitter, etc. and keeps me focused.> 5. Where and how are results being cached? For example, if I apply tweeks to a banking website, what content is being scraped and sent to an LLM? When I reload a site, is content being pulled purely from a local cache on my machine?There is a distinction. When you generate a tweek, the page is captured and sent to an LLM. There is no way around this. You can't generate a modification for a site you cannot see.The result of a generation is a static script that applies to the page across reloads (unless you disable it). When you apply a tweek, everything is local, there is no dynamic server communication.Hopefully that is all helpful! I need to get to other replies, but I will try to return to finish up your business questions (those are the most boring anyway)-- Edit: I'm back! --> 6. Is this (or will it be) open source? IMO a large component of empowering the user against enshittification is open source. As compute commoditizes it will likely be open source that is the best hope for protection against the overlords.It is very important to me that people trust us. I can say that we don't do X, Y, Z with your data and that using our product is safe, but trust is not freely given (nor should it be). We have a privacy policy, we have SOC II, and in theory, you could even download the extension and dig into the code yourself.Open-source is one way to build trust. However, I also recognize that many of these \"overlords\" you speak of are happy to abuse their power. Who's to say that we don't open our code, only to have e.g. OpenAI fork it for their own browser? Of course, we could put restrictive licenses, but lawsuits haven't been particularly protective of copyright lately. I am interested in open-sourcing parts of our code (and there certainly is hunger for it in this post), but I am cognizant that there is a lot that goes into that decision.> 7. What is your revenue model? If your product essentially wrestles control from site owners and reduces their optionality for revenue, your arbitrage is likely to be equal or less than the sum of site owners' loss (a potentially massive amount to be sure). It's unclear to me how you'd capture this value though, if open source.The honest answer is TBD. I would push back on your claim that we wrestle control from site owners and reduce their optionality for revenue. While there likely will be users who say \"hide this ad\" (costing the site revenue) there are also users who say \"move this sidbebar from left to right\" or \"I use {x} button all the time but it is hidden three menus in, place it prominently for easy access\". I'd argue the latter cases are not negative for the site owners, they could be positive sum. Maybe we even see a trend that 80% of users make this UX modification on Z site. We could go to Z site and say, \"Hey, you could probably make your users happy if you made this change\". Maybe they'd even pay us for that insight?Again, the honest answer is that I'm not certain about the business model. I am a lover of positive sum games. And in the moment, I am building something that I enjoy using and hopefully also provides value to others.> 8. Interested in the cost and latency. If this essentially requires an LLM call for every website I visit, this will start to add up. Also curious if this means that my cost will scale with the efficiency of the sites I visit (i.e. do my costs scale with the size of the site's content).As I noted above, this does not require an LLM call for every website you visit. You are correct that that would bankrupt us very quickly! An LLM is only involved when you actively start a generation/update request. There is still a cost and it does scale with the complexity of the site/request, but it is infinitely more feasible than running on every site.In the future, we may extend functionality so that the core script that is generated can itself dynamically call LLMs on new page loads. That would enable you to do things like \"filter political content from my feed\" which requires test time LLM compute to dynamically categorize on each load (can't be hard-coded in a once-generated static script). That would likely have to be done locally (e.g. Google actually packages Gemini nano into the browser) for both cost and latency reasons. We're not there yet, and there is a lot you can do with the extension today, but there are definitely opportunities to build really cool stuff, way beyond Greasemonkey.Wow, you really put me to work with this comment. Appreciate all the great questions!reply",
      "> We could go to Z site and say, \"Hey, you could probably make your users happy if you made this change\". Maybe they'd even pay us for that insight?My honest opinion:1. No site would pay for that insight2. Every site should pay for that insightPart of the problem is that a lot of companies fall into one of two categories:1. Small companies that don't have the time/energy/inclination to make changes, even if they're simple; often they're not even the ones making the website itself and they aren't going to way to pay the company who made the site originally to come back and tweak it based on what a small, self-selecting group of users decided to change.2. Large companies who, even if they did care about what that small, self-selecting group of users wanted to change, have so many layers between A and Z that it's nearly impossible to get anything done without a tangible business need. No manager is going to sign off on developer and engineer time and testing because 40% of 1% of their audience moves the sidebar from one side to the other.Also:1. Designers are opinionated and don't want some clanker telling them what they're doing wrong, regardless of the data.2. Your subset of users may have different goals or values; maybe the users more likely to install this extension and generate tweaks don't want to see recommended articles or video articles or 'you may like...' or whatever, but most of their users do and the change would turn out to be a bad one. Maybe it would reduce accessibility in some way that most users don't care about, etc.If I had to pick a 'what's the value of all this', I would say that it's less about \"what users want from this site\" vs. \"what users want from sites\". For example, if you did the following:1. Record all the prompts that people create that result in tweaks that people actually use, along with the category of site (banking, blogs, news, shopping, social media, forums); this gives you a general selection of things that people want. Promote these to other users to see how much mass appeal they have2. Record all the prompts that people create that result in tweaks that people don't actually use; this gives you a selection of things that people think they want but it turns out they don't.3. Summarize those changes into reports.Now you could produce a 'web trend report' where you can say:1. 80% of users are making changes to reduce clutter on sites2. 40% of users are disabling or hiding auto-play videos3. 40% of People in countries which use right-to-left languages swap sidebars from one side to another even on left-to-right-language websites4. The top 'changed' sites in your industry are ... and the changes people make are ...5. The top changes that people make to sites in your industry are ... and users who make those changes have a 40% lower bounce rate / 30% longer time-on-site / etc. than users who don't make those changes.On top of that, you could build a model trained on those user prompts that companies could then pay for (somehow?) to run their sites through to provide suggestions of what changes they could make to their sites to satisfy these apparent user needs or preferences without sacrificing their own goals for the websites - e.g. users want to remove auto-playing videos because they're obnoxious, but the company is trying to promote their video content so maybe this model could find a middle-ground to present the video to users in a way that's less obnoxious but generates user engagement.That's what I think anyway, but I'm not in marketing or whatever.reply",
      "Can you answer question 7?reply",
      "I doubt that they know. It's too early to figure something like that out.reply",
      "Seems to me that the obvious business model here is that they will need to have their AI inject their own ads into the DOM. Overall though, this feels like a feature, not a business.reply",
      "To me the more obvious option is additional features that people pay for, i.e. freemium. But what do I know.reply",
      "As a user, I'll never pay for software. Adblock for SaaS and pirated downloads for everything else is all I need.reply",
      "Or, they do know and don't want to say. This project does seem to have funding so I assume there is a plan.reply",
      "The \"share\" part is interesting.I am imagining something slightly different perhaps? In the same way Pi Hole has a kind of global list of (ad) URLs to block, I am looking for an extension where all these edits to deshittify a site are applied for me automatically when I visit a site.That is, if someone has already stripped out banners, etc. (deshittified a site) and (somehow?) \"submitted\" the edits, I just want to pull those in when I visit the same site.I understand 1) one person's deshittifying might go too far 2) there will be multiple ways to deshittify a site depending on who does it, and 3) sites change and a deshittify strategy that worked earlier could break.I have no good answers for the above issues.reply"
    ],
    "link": "https://www.tweeks.io/onboarding",
    "first_paragraph": "Follow a focused walkthrough to connect the extension, enable script access, and run your first tweek.Get Set UpLooking for the extension...Pin the extension so it's ready on every site.Finish installing the extension before unlocking script powers.Learn the BasicsInstall the rainbow heading script to see it in action.Generate a script that solves an actual problem on this page.Open the extension, tweek your generated script, and make it your own.Install the extension to unlock the full setup walkthroughStep 1We'll check whether tweeks is already installed and ready. If you haven't yet, install it from the Chrome Web Store and then use \"Check again\" once installed.Chrome is verifying the extension connection. This usually takes just a moment.Install tweeks from the Chrome Web Store below. After installing, come back and press \"Check again.\"Preview what you can do. Once the extension is installed, you can add any of these tweeks with a single click.Strip away distractions like sidebars,"
  },
  {
    "title": "Blue Origin lands New Glenn rocket booster on second try (techcrunch.com)",
    "points": 205,
    "submitter": "perihelions",
    "submit_time": "2025-11-13T21:24:25 1763069065",
    "num_comments": 90,
    "comments_url": "https://news.ycombinator.com/item?id=45920748",
    "comments": [
      "Congrats to the Blue Origin team! That's a heck of a milestone (landing it on the second attempt). It will compete more with Falcon Heavy than Starship[1] but it certainly could handle all of the current GEO satellite designs. I'm sure that the NRO will appreciate the larger payload volume as well. Really super glad to see they have hardware that has successfully done all the things. The first step to making it as reliable as other launch platforms. And having a choice for launch services is always a good thing for people buying said launch services.Notably, from a US policy standpoint, if they successfully become 'lift capability #2' then it's going to be difficult to ULA to continue on.[1] Although if Starship's lift capacity keeps getting knocked back that might change.reply",
      "Doesn't ULA use Blue Origin's rocket engines?reply",
      "Yes, for Vulcan [1].[1] https://spacenews.com/evolution-of-a-plan-ula-execs-spell-ou...reply",
      "Yes, which makes it even harder for ULA to compete.reply",
      "[flagged]",
      "> Starship is vaporwareVaporware is \"late, never actually manufactured, or officially canceled\" [1].Starship is late, so you're pedantically correct. But so is New Glenn, and it started being developed when Falcon 9 made its first trip to the ISS. (2012.)[1] https://en.wikipedia.org/wiki/Vaporwarereply",
      "And Blue Origin was incorporated a few years prior to SpaceX. They\u2019ve been working on this problem significantly longer than SpaceX, so they were more confident in their approach.reply",
      "https://www.merriam-webster.com/dictionary/vaporware\"a computer-related product that has been widely advertised but has not and may never become available\"It's not available and it's going to be the same as all products coming from their CEO - it maybe one day available, but only thing it'll share with original announced product is a name. Nowhere close on the cost/features/scale/etc.Only things that were shown so far are prototypes that are many iterations away from being anywhere close to a product.New Glenn is actual product that's just going through final validation steps.reply",
      "> It's not available and it's going to be the same as all products coming from their CEO - it maybe one day availableDid you miss Falcon 9 and Heavy? (New Glenn competes with them, not Starship. Falcon Heavy can launch more mass than New Glenn, currently, for cheaper.)> New Glenn is actual product that's just going through final validation stepsThis is literally the first time they've successfully recovered New Glenn. Recovered. No reuse. It's the second time they've every flown the damn thing. It's impressive. But it's not \"just going through final validation.\"I have a background in aerospace engineering, specifically astronautics. It's wild to see armchair engineers shoot shit at major accomplishments like this.reply",
      "I'm reading this thread and there are a few things that come to mind.My sense is that SpaceX's goals with Starship are significantly more ambitious than what is being tried with New Glenn.  I don't mean to underplay the difficulty of what Blue is facing with New Glenn, but if we take that \"rapid reusability\" goal seriously the problem set seems significantly larger and not so \"been there, done that\".  This makes the development programs much more difficult to compare.... certainly on the surface of the public optics at the very least.While it's one thing to talk about rockets, it's another altogether to look and the engineering and practices going into the manufacture process of those rockets.  I'm not an engineer, but I do work in manufacturing and, at least looking from the outside, SpaceX seems to be dedicating some significant amount of effort into building a scalable manufacturing process.  Many other efforts have always appeared to be more about \"bespoke\" production even if the designs of each unit produced are constant. I could be wrong and maybe it's just SpaceX is a lot more transparent (willingly or otherwise)... but looking in from the outside, they seem to be developing a very mass-production oriented rocket factory.And if New Glenn is just finalizing things and Starship is just vaporware... well New Glenn still has to land a couple more boosters and re-fly one (or two?) to catch up to those vaporware numbers.  :-)  Sure, New Glenn has now flown a paying customer... but I think we'll see Starlink launches on Starship pretty soon... well before it gets to \"final validation\".reply"
    ],
    "link": "https://techcrunch.com/2025/11/13/blue-origin-lands-new-glenn-rocket-booster-on-second-try/",
    "first_paragraph": "\n\n\t\tLatest\t\n\n\n\t\tAI\t\n\n\n\t\tAmazon\t\n\n\n\t\tApps\t\n\n\n\t\tBiotech & Health\t\n\n\n\t\tClimate\t\n\n\n\t\tCloud Computing\t\n\n\n\t\tCommerce\t\n\n\n\t\tCrypto\t\n\n\n\t\tEnterprise\t\n\n\n\t\tEVs\t\n\n\n\t\tFintech\t\n\n\n\t\tFundraising\t\n\n\n\t\tGadgets\t\n\n\n\t\tGaming\t\n\n\n\t\tGoogle\t\n\n\n\t\tGovernment & Policy\t\n\n\n\t\tHardware\t\n\n\n\t\tInstagram\t\n\n\n\t\tLayoffs\t\n\n\n\t\tMedia & Entertainment\t\n\n\n\t\tMeta\t\n\n\n\t\tMicrosoft\t\n\n\n\t\tPrivacy\t\n\n\n\t\tRobotics\t\n\n\n\t\tSecurity\t\n\n\n\t\tSocial\t\n\n\n\t\tSpace\t\n\n\n\t\tStartups\t\n\n\n\t\tTikTok\t\n\n\n\t\tTransportation\t\n\n\n\t\tVenture\t\n\n\n\t\tStaff\t\n\n\n\t\tEvents\t\n\n\n\t\tStartup Battlefield\t\n\n\n\t\tStrictlyVC\t\n\n\n\t\tNewsletters\t\n\n\n\t\tPodcasts\t\n\n\n\t\tVideos\t\n\n\n\t\tPartner Content\t\n\n\n\t\tTechCrunch Brand Studio\t\n\n\n\t\tCrunchboard\t\n\n\n\t\tContact Us\t\nJeff Bezos\u2019 Blue Origin has landed the booster of its New Glenn mega-rocket on a drone ship in the Atlantic Ocean on just its second attempt \u2014 making it the second company to perform such a feat, following Elon Musk\u2019s SpaceX.It\u2019s an accomplishment that will help the new rocket system become an option to send larger payloads to space, the moon, and be"
  },
  {
    "title": "How to fix subsystem request failed on channel 0 (x-way.org)",
    "points": 8,
    "submitter": "speckx",
    "submit_time": "2025-11-06T20:50:52 1762462252",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://blog.x-way.org/Linux/2025/11/06/How-to-fix-subsystem-request-failed-on-channel-0.html",
    "first_paragraph": "\n\tToday I got greeted by the following error when I was trying to scp some file.\n\n\tThis cryptic error message comes from a default config change that OpenSSH introduced in version 9.\n\tBy default it now uses the sftp protocol for transfering files and no longer the original scp protocol.\n\tThus for hosts which do not support the sftp subsystem, but only the original scp, the file transfer fails with the above error.\n\n\tLuckily there is the -O parameter which can be used to select the original scp protocol, and the transfer then succeeds:\n"
  },
  {
    "title": "SIMA 2: An agent that plays, reasons, and learns with you in virtual 3D worlds (deepmind.google)",
    "points": 169,
    "submitter": "meetpateltech",
    "submit_time": "2025-11-13T15:29:38 1763047778",
    "num_comments": 63,
    "comments_url": "https://news.ycombinator.com/item?id=45916037",
    "comments": [
      "OK, AI playing video games is cool. But you know what's really really cool? It looks like SIMA 2 is controlling the mouse and reading the screen at something approaching 30+fps. WANT. Computer use agents are so slow right now, this is really something. I wonder what the architecture is for this.reply",
      "Its even cooler if humans find something to be excited about in this world, since AI is replacing everything we do.reply",
      "I desperately want an AI agent that can use my phone for me. Just something that takes instructs for each screen and execute it.\"Open Chrome\"\"Go to xyz.com\"\"open hamburger menu\"\"Click login\"etc. etc.reply",
      "Isn't that what the voice a11y tools have been doing for years. Why do you need AI for that.https://support.google.com/accessibility/android/answer/6151...https://support.apple.com/en-us/111778reply",
      "My friend that is AI. However, it can get a lot better: be more aware of screen content, follow multiple instructions at once, keep context in mind throughout the conversation and from past interactionsreply",
      "AI commonly means LLM. Where are you determining this is using a LLM for proccessing?reply",
      "AI has existed for several decades before the first LLM was ever created.[1][2][3]And that's not even considering machine learning and deep learning which also have existed for many years before LLMs.Even if you consider the current usage of the word AI in popular culture, it includes things that are not an LLM like Stable Diffusion and Suno[1] https://en.wikipedia.org/wiki/Expert_system[2] https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)[3] https://en.wikipedia.org/wiki/Lisp_machine#Historical_contex...reply",
      "Droidrun did a Show HN recently. It's exactly that.reply",
      "The gap between high level and low level control of robots is closing.  Right now thousands of hours of task specific training data is being collected and trained on to create models that can control robots to execute specific tasks in specific contexts. This essentially turns the operation of a robot into a kind of video game, where inputs are only needed a in low-dimensional abstract form, such as \"empty the dishwasher\" or \"repeat what I do\" or \"put your finger in the loop and pull the string\".\nThis will be combined with high-level control agents like SIMA 2 to create useful real-world robots.reply",
      "I work on a much easier problem (physics-based character animation) after spending a few years in motion planning, and I haven\u2019t really seen anything to suggest that the problem is going to be solved any time soon by collecting more data.reply"
    ],
    "link": "https://deepmind.google/blog/sima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds/",
    "first_paragraph": "SIMA TeamLast year, we introduced SIMA (Scalable Instructable Multiworld Agent), a generalist AI that could follow basic instructions across a wide range of virtual environments. SIMA was a crucial first step in teaching AI to translate language into meaningful action in rich, 3D worlds.Today we\u2019re introducing SIMA 2, the next milestone in our research creating general and helpful AI agents. By integrating the advanced capabilities of our Gemini models, SIMA is evolving from an instruction-follower into an interactive gaming companion. Not only can SIMA 2 follow human-language instructions in virtual worlds, it can now also think about its goals, converse with users, and improve itself over time.This is a significant step in the direction of Artificial General Intelligence (AGI), with important implications for the future of robotics and AI-embodiment in general.The first version of SIMA learned to perform over 600 language-following skills, like \u201cturn left,\u201d \u201cclimb the ladder,\u201d and \u201co"
  },
  {
    "title": "Show HN: DBOS Java \u2013 Postgres-Backed Durable Workflows (github.com/dbos-inc)",
    "points": 47,
    "submitter": "KraftyOne",
    "submit_time": "2025-11-13T20:33:43 1763066023",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=45920156",
    "comments": [
      "We are increasingly using Temporal with Temporal Cloud and soon Nexus to manage numerous workflows. I'm curious what type of observability is avialable for DBOS and how much of that you get for \"free\". The reason we ended up in Temporal was not that previous job-systems were unreliable, it was simply that nobody wanted to go dig through a database to find out what happened with their job, and nobody has time/energy to build a UI just for that purpose.reply",
      "There's an observability and workflow management UI: https://docs.dbos.dev/java/tutorials/workflow-managementYou can view your workflows and queues, search/filter them by any number of criteria, visualize graphs of workflow steps, cancel workflows, resume workflows, restart workflows from a specific step--everything you'd want.Currently, this is available as a managed offering (Conductor - https://docs.dbos.dev/production/self-hosting/conductor), but we're also releasing a self-hostable version of it soon.reply",
      "Recently added DBOS python to one of my FOSS projects. It's awesome, looking forward to seeing what that self hostable observability would look like. I have been looking in the DB and OTEL logs to debug development.reply",
      "Thanks Peter, but I wonder if there will there be .NET support? Since Temporal includes one, so it is a hard selling point for us .NET developers when MassTransit went commercial.reply",
      ".NET support is something we're considering, but aren't actively building right now.reply",
      "glad to see the java sdk released, i've been following it for a while.one of the rough edges i've noticed w/DBOS is for workflows that span multiple services. all of the examples are contained in a single application and thus use a single dbos 'system db' instance. if you have multiple services (as you often do in the real world) that need to participate in a workflow.. you really can't. you need to break them into multiple workflows and enqueue them in each service by creating an instance of the dbos client pointed at the other services system db. aside from the obvious overhead from fragmenting a workflow into multiple (and that you have to push to the service instead of a worker pulling the step), that means that every service needs to be aware of and have access to, every other services system db. also worth noting that sharing a single system db between services was not advised when i asked.(docs for the above: https://docs.dbos.dev/architecture#using-dbos-in-a-distribut...)reply",
      "The pattern I would recommend in such cases is having one service be responsible for the overall workflow and then call the other services as steps.So if you were for example running a website and wanted to have a \"cancellation\" flow, you'd have the cancellation service with the workflow inside of it, which would have all the steps defined, like1) disable user account2) mark user data as archived3) cancel recurring paymentsAnd then each step would call the service that actually does that work, using an idempotency key.  Each service might have its own internal workflows to accomplish each task.  In this case step 1 would call the accounts service, step two would call the storage service, and step three would call the payment service.But then you have a clean reusable interface in each service, as well as a single service responsible for the workflow.reply",
      "the OP wasn't clear but that's effectively what i settled on by launching workflows (within steps) via the dbos client. keeping that an implementation detail in each service though is probably better + solves the db awareness, just need to do the endpoint/rpc plumbing. thanks!reply",
      "Thanks for the great feedback! Yeah, for isolation we recommend each service have its own system database and communicate via clients (so service A starts a workflow in service B by creating a client and calling \"client.enqueue\").How could we make this experience better while keeping DBOS a simple library? One improvement that comes to mind is to add an \"application name\" field to the workflows table so that multiple applications could share a system database. Then one application could directly enqueue a workflow to another application by specifying its name, and workflow observability tooling would work cross-application.reply",
      "yeah i think that's a step in the right direction, but ultimately as long as the workflow executor needs to know 'who runs this step' there will always be some friction compared to other systems like temporal.reply"
    ],
    "link": "https://github.com/dbos-inc/dbos-transact-java",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Lightweight Durable Java Workflows\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.DBOS provides lightweight durable workflows built on top of Postgres.\nEssentially, it helps you write long-lived, reliable code that can survive crashes, restarts, and failures without losing state or duplicating work.As your workflows run, DBOS checkpoints each step they take in a Postgres database.\nWhen a process stops (fails, intentionally suspends, or a machine dies), your program can recover from those checkpoints to restore its exact state and continue from where it left off, as if nothing happened.In practice, this makes it easier to build reliable systems for use cases like AI agents, data synchronization, payments, or anything that takes minutes, days, or weeks to com"
  },
  {
    "title": "Think in math, write in code (2019) (jmeiners.com)",
    "points": 106,
    "submitter": "alabhyajindal",
    "submit_time": "2025-11-09T12:03:15 1762689795",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=45864954",
    "comments": [
      "I think the author makes a good point about understanding structure over symbol manipulation, but there's a slippery slope here that bothers me.In practice, I find it much more productive to start with a computational solution - write the algorithm, make it work, understand the procedure. Then, if there's elegant mathematical structure hiding in there, it reveals itself naturally. You optimize where it matters.The problem is math purists will look at this approach and dismiss it as \"inelegant\" or \"brute force\" thinking. But that's backwards. A closed-form solution you've memorized but don't deeply understand is worse than an iterative algorithm you've built from scratch and can reason about clearly.Most real problems have perfectly good computational solutions. The computational perspective often forces you to think through edge cases, termination conditions, and the actual mechanics of what's happening - which builds genuine intuition. The \"elegant\" closed-form solution often obscures that structure.I'm not against finding mathematical elegance. I'm against the cultural bias that treats computation as second-class thinking. Start with what works. Optimize when the structure becomes obvious. That's how you actually solve problems.reply",
      "Some people like Peter Norvig prefer top-down, hackers like me and you prefer bottom-up.  Many problems can be solved either way.  But for some problems, if you use the wrong approach, you're gonna have a bad time.  See Ron Jeffries' attempt to solve sudoku.The top-down (mathematical) approach can also fail, in cases where there's not an existing math solution, or when a perfectly spherical cow isn't an adequate representation of reality. See Minix vs Linux, or OSI vs TCP/IP.reply",
      "Fair point about problem-fit - some problems do naturally lend themselves to one approach over the other.But I think the Sudoku example is less about top-down vs bottom-up and more about dogmatic adherence to abstractions (OOP in that case). Jeffries wasn't just using a 'hacker' approach - he was forcing everything through an OOP lens that fundamentally didn't fit the problem structure.But yes, same issue can happen with the 'mathematical' approach - forcing \"elegant\" closed-form thinking onto problems that are inherently messy or iterative.reply",
      "I'd argue that everyone solves problems bottoms up.  It's just that some people have done the problem before (or a variant of it) so they have already constructed a top-down schema for it.reply",
      "I really enjoyed the book Mathematica by David Bessis, who writes about his creative process as a mathematician.  He makes a case that formal math is usually the last step to refine/optimize an idea, not the starting point as is often assumed.  His point is to push against the cultural idea that math == symbols.  Sounds similar to some of what you're describing.reply",
      "3Blue1Brown has a great video which frames this as a cultural problem that also exists in mathematics pedagogy:https://www.youtube.com/watch?v=ltLUadnCyi0Personally, I find a mix of all three approaches (programming, pen and paper, and \"pure\" mathematical structural thought) to be best.reply",
      "I have math papers in top journals and that's exactly how I did math;Just get a proof of the open problem no matter how sketchy. Then iterate and refine.But people love to reinvent the wheel without caring about abstractions, resulting in languages like Python being the defacto standard for machine learningreply",
      "I agree with the thrust of the article but my conclusion is slightly different.In my experience the issue is sometimes that Step 1 doesn't even take place in a clear cut way. A lot of what I see is:  1. Design algorithms and data structures\n  2. Implement and test them\n\nOr even:  1. Program algorithms and data structures\n  2. Implement and test them\n\nOr even:  1. Implement\n  2. Test\n\nOr even:  1. Test\n  2. Implement\n\n:-(IMO, this last popular approach gets things completely backwards. It assumes there is no need to think about the problem before hand, to identify it, to spend any amount of time thinking about what needs to happen on a computer for that problem to be solved... you just write down some observable behaviors and begin reactively trying to implement them. Huge waste of time.The point also about \"C-style languages being more appealing\" is well taken. It's not so much about the language in particular. If you are able to sit down and clearly articulate what you're trying to do, understand the design tradeoffs, which algorithms and data structures are available, which need to be invented... you could do it in assembly if it was necessary, it's just a matter of how much time and energy you're willing to spend. The goal becomes clear and you just go there.I have an extensive mathematical background and find this training invaluable. On the other hand, I rarely need to go so far as carefully putting down theorems and definitions to understand what I'm doing. Most of this happens subliminally somewhere in my mind during the design phase. But there's no doubt that without this training I'd be much worse at my job.reply",
      "Reminds me of the attempt to TDD a way to a sudoku solver.  Agreed that it is a bit of a crazy path.Not that Implement/Test can't work.  As frustrating as it is, \"just do something\" works far better than many alternatives.  In particular, with enough places doing it, somebody may succeed.reply",
      "> Or even: 1. Test 2. Implement \nIMO, this last popular approach gets things completely backwards. It assumes there is no need to think about the problem before handI think you misunderstand this approach.The point of writing the tests is to think about the desired behaviour of the system/module you are implementing, before your mind gets lost in all the complexities which necessarily happens during the implementation.When you write code, and hit a wall, it\u2019s super easy to get hyper-focused on solving that one problem, and while doing so: lose the big picture.Writing tests first can be a way to avoid this, by thinking of the tests as a specification you think you should adhere to later, without having to worry about how you get there.For some problems, this works really well. For others, it might not. Just don\u2019t dismiss the idea completely :)reply"
    ],
    "link": "https://www.jmeiners.com/think-in-math/",
    "first_paragraph": "\nHome\nGitHub\nRss\n6/8/19Programmers love to discuss programming languages.\nWe not only debate their technical merits and aesthetic qualities,\nbut they become integrated into our personal identities,\nalong with the values and traits that we associate with them.\nSome even defend a form of Linguistic Determinism that thinking is confined to what the language\nmakes typable.Since we spend so much time writing code, a keen interest in language design is justified.\nHowever, the character of these discussions suggests that we think of them as much more,\nand have perhaps forgotten their primary role.\nProgramming languages are implementation tools for instructing machines, not thinking tools\nfor expressing ideas.\nThey are strict formal systems riddled with design compromises and practical limitations.\nAt the end of the day, we hope they make controlling computers bearable for humans.\nIn contrast, thoughts are best expressed through a medium which is free and flexible.The natural language which ha"
  },
  {
    "title": "Why do we need dithering? (typefully.com)",
    "points": 44,
    "submitter": "ibobev",
    "submit_time": "2025-11-04T19:27:55 1762284475",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=45814939",
    "comments": [
      "This is going to be an odd comment, but  I immediately recognised the parrot in the test images.  It's the scarlet macaw from 2004 which is often used in many Wikipedia articles about colour graphics.I think this is the original, photographed and contributed by Adrian Pingstone: https://commons.wikimedia.org/wiki/File:Parrot.red.macaw.1.a...But this particular derivative is the one that appears most often in the Wikipedia articles: https://commons.wikimedia.org/wiki/File:RGB_24bits_palette_s...This parrot has occurred in several articles on the web.  For example, here's one article from a decade or so ago: https://retroshowcase.gr/index.php?p=paletteParrots are often used in articles and research papers about computer graphics and I think I know almost all the parrots that have ever appeared in computing literature.  This particular one must be the oldest computing literature parrot I know!By the way, I've always been fascinated by dithering ever since I first noticed it in newspapers as a child. Here was a clever human invention that could produce rich images with so little, something I could see every day and instinctively understand how it creates the optical illusion of smooth gradients, long before I knew what it was called.reply",
      "This also used to be a really common test image: https://en.wikipedia.org/wiki/LennaBut its apparently a cropped centerfold from Playboyreply",
      "The original Lenna is controversial, but I'm delighted to share the \"ethically sourced Lenna\":\nhttps://mortenhannemose.github.io/lena/reply",
      "This was recently shared on HN: https://visualrambling.space/dithering-part-1/For anyone interested in seeing how dithering can be pushed to the limits, play 'Return of the Obra Dinn'. Dithering will always remind you of this game after that.- https://visualrambling.space/dithering-part-1- https://store.steampowered.com/app/653530/Return_of_the_Obra...reply",
      "A related bit of tech trivia is that digital audio also often involves dithering, and not just decimated or compressed audio. Even very high-quality studio mastered audio benefits from an audio specific kind of dithering called noise shaping. Depending on the content, studio mixing engineers may choose different noise shaping algorithms.https://en.wikipedia.org/wiki/Noise_shapingreply",
      "We really don't anymore.Back in the late 90s maybe. Gifs and other paletted image formats were popular.I even experimented with them. I designed various formats for The Palace. The most popular was 20-bit (6,6,6,2:RGBA, also 5,5,5,5; but the lack of color was intense, 15 bits versus 18 is quite a difference). This allowed fairly high color with anti-aliasing -edges that were semi transparent.reply",
      "The figures in this article are really great. How where they made? If I was to try and recreate them I might render things individually and then lay it out in Illustrator to get that 3D isomorphic look, but I assume there's a better way.reply",
      "You still see dithering from time to time as a cheap transparency, it's been a few years since Mario Odyssey but that's when last I recall it really stood out: https://xcancel.com/chriswade__/status/924071608976924673reply",
      "Also by the author: https://www.makingsoftware.com/Recent discussions:Making Software - https://news.ycombinator.com/item?id=43678144How does a screen work? - https://news.ycombinator.com/item?id=44550572What is a color space? - https://news.ycombinator.com/item?id=45013154reply",
      "We've had a couple of other recent discussions on dithering: https://news.ycombinator.com/item?id=45750954 and https://news.ycombinator.com/item?id=45698323.  I commented specifically about the history of blue-noise dithering at https://news.ycombinator.com/item?id=45728231.The article points out that, historically, RAM limitations were a major incentive for dithering on computer hardware.  (It's the reason Heckbert discussed in his dissertation, too.)  Palettizing your framebuffer is clearly one solution to this problem, but I wonder if chroma subsampling hardware might have been a better idea?The ZX Spectrum did something vaguely like this: the screen was 256\u00d7192 pixels, and you could set the pixels independently to foreground and background colors, but the colors were provided by \"attribute bytes\" which each provided the color pairs for an 8\u00d78 region http://www.breakintoprogram.co.uk/hardware/computers/zx-spec....  This gave you a  pretty decent simulation of a 16-color gaming experience while using only 1.125 bits per pixel instead of the 4 you would need on an EGA.  So you got a near-EGA-color experience on half the RAM budget of a CGA, and you could move things around the screen much faster than on even the CGA.  (The EGA, however, had a customizable palette, so the ZX Spectrum game colors tend to be a lot more garish.  The EGA also had 4.6\u00d7 as many pixels.)Occasionally in ZX Spectrum game videos like https://www.youtube.com/watch?v=Nx_RJLpWu98 you will see color-bleeding artifacts where two sprites overlap or a sprite crosses a boundary between two background colors.  For applications like CAD the problem would have been significantly worse, and for reproducing photos it would have been awful.The Nintendo did something similar, but I think had four colors per tile instead of two.So, suppose it was 01987 and your hardware budget permitted 8 bits per pixel.  The common approach at the time was to set a palette and dither to it.  But suppose that, instead, you statically allocated five of those bits to brightness (a Y channel providing 32 levels of grayscale before dithering) and the other three to a 4:2:0 subsampled chroma (https://www.rtings.com/tv/learn/chroma-subsampling has nice illustrations).  Each 2\u00d72 4-pixel block on the display would have one sample of chroma, which could be a 12-bit sample: 6 bits of U and 6 bits of V.  Moreover, you can interpolate the U and V values from one 2\u00d72 block to the next.  As long as you're careful to avoid drawing text on backgrounds that differ only in chroma (as in the examples in that web page) you'd get full resolution for antialiased text and near-photo-quality images.That wouldn't liberate you completely from the need for dithering, but I think you could have produced much higher quality images that way than we in fact did with MCGA and VGA GIFs.reply"
    ],
    "link": "https://typefully.com/DanHollick/why-do-we-need-dithering-Ut7oD4k",
    "first_paragraph": "Share17 days agoView on XDan Hollick@DanHollickdesign engineer @tailwindcss. writing a book about software at makingsoftware.com\nprev: @raycastapp"
  },
  {
    "title": "Blender Lab (blender.org)",
    "points": 197,
    "submitter": "radeeyate",
    "submit_time": "2025-11-13T13:38:47 1763041127",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=45914761",
    "comments": [
      "If anyone else is having trouble loading the CSS for the page, go to https://www.blender.org/wp-content/themes/bthree/style.css?x... to be Captcha'd by Cloudflare - it should load fine after that.reply",
      "Yeah the page is currently loading very quickly and is very easy to read due to an issue with Cloudflare.Edit: If I'm honest, I find this specific css fairly innocuous. I have a more general grudge that raw HTML is easy, free and accessible but we collectively insisted that it's not OK to use it.reply",
      "It's actually funny, the lack of styling actually makes consuming the info alot easier.reply",
      "The page is objectively and substantially better in every aspect of legibility with CSS loaded.reply",
      "Hey, take it easy! It's not nice to pick on those poor developers who can only read text that has been piped into their preferred TUI.reply",
      "Yea, \"Minimalism is better!\" is like a reflex for some peoplereply",
      "> objectivelyI do not think that word means what you think it meansreply",
      "All art is a combination of objective and subjective aspects.The objective improvements from css here include: shorter lines are easier to read (per multiple legibility studies), the styling distinguishes navigation and secondary site elements from the main content (without css you get a half screen of navigation links), and the visual importance of in-page anchor links is reduced.reply",
      "It is way more readable with CSS, I don't know what you're all on about.The font is bigger, the lines are shorter, the navigation doesn't take half the page. The only thing worse would be the contrast but it's not that bad.reply",
      "On mobile it's actually super readable and lines are a perfect length, and having to scroll past the top navigation doesn't affect readability muchreply"
    ],
    "link": "https://www.blender.org/news/introducing-blender-lab/",
    "first_paragraph": "\n          If Blender helped you with your projects this year, we're asking you to help us back.For the past year, you've had access to professional 3D software, completely free. No subscriptions, no licenses, no limits. Blender has been there for your projects, your learning, your art, your business.Today, we're asking for $5.That's it. If every active Blender user contributed $5 this month, Blender Foundation would meet its entire yearly funding goal for 2026.\n          If Blender helped you with your projects this year, we're asking you to help us back.For the past year, you've had access to professional 3D software, completely free. No subscriptions, no licenses, no limits. Blender has been there for your projects, your learning, your art, your business.Today, we're asking for $5.That's it. If every active Blender user contributed $5 this month, Blender Foundation would meet its entire yearly funding goal for 2026.Introducing an innovation space within the Blender project, where de"
  },
  {
    "title": "SlopStop: Community-driven AI slop detection in Kagi Search (kagi.com)",
    "points": 276,
    "submitter": "msub2",
    "submit_time": "2025-11-13T19:03:26 1763060606",
    "num_comments": 129,
    "comments_url": "https://news.ycombinator.com/item?id=45919067",
    "comments": [
      "This is so, so exciting. I hope HN takes inspiration and adds a similar flag. :)reply",
      "I just requested access to the database @freediver so hopefully it should be integrated into https://hcker.news soon.I appreciate Kagi's community-driven approach. The open Small Web list[0] is invaluable. Applying a smallweb filter[1] on HN brings a breath of fresh air to the frontpage.0: https://github.com/kagisearch/smallweb1: https://hcker.news/?smallweb=truereply",
      "Indeed.reply",
      "HN could use some of this. It'd be nice if there was a safe having from the equivalent of high grade junk mail.reply",
      "we just need human attestation. A vial of blood per commentreply",
      "I can live with that ;)reply",
      "Definitely anecdata but an eye opener for me:I've been using Anthropic's models with gptel on Emacs for the past few months. It has been amazing for overviews and literature review on topics I am less familiar with.Surprisingly (for me) just slightly playing with system prompts immediately creates a writing style and voice that matches what _I_ would expect from a flesh agent.We're naturally biased to believe our intuition 'classifier' is able to spot slop. But perhaps we are only able to stop the typical ChatGPTesque 'voice' and the rest of slop is left to roam free in the wild.Perhaps we need some form of double blind test to get a sense of false negative rates using this approach.reply",
      "Nice. This is needed at every place where user-generated content is commented and voted on. Any forum that offers the option to report something as abuse or spam should add \"AI slop\" as an additional option.reply",
      "Nice. This is needed at every place where user-generated content gets commented and voted on. Any forum that offers the option to report something as abuse or spam should add \"AI slop\" as an additional option.reply",
      "So we have two universes. One is pushing generated content up our throats - from social media to operating systems - and another universe where people actively decide not to have anything to do with it.I wonder where the obstinacy on the part of certain CEOs come from. It's clear that although such content does have its fans (mostly grouped in communities), people at large just hate arificially-generated content. We had our moment, it was fun, it is no more, but these guys seem obsessed in promoting it.reply"
    ],
    "link": "https://blog.kagi.com/slopstop",
    "first_paragraph": "\n\n\n                    12 Nov, 2025\n                \n\nWe made it our mission to prevent the web from becoming useless and a harmful space. That\u2019s why today, Kagi Search introduces the first community-driven system to detect and downrank deceptive AI-generated text, images, and video inside search results.It\u2019s 2025, and the internet we loved is drowning in AI-generated noise. Content farms exploiting AI for profit are manipulating search results in this attention economy\u2019s race to the bottom.This makes us wonder: who are we building the web for?AI slop is deceptive or low-value AI-generated content, created to manipulate ranking or attention rather than help the reader.Per our AI integration philosophy, we\u2019re not against AI tools that enhance human creativity. But when it includes fake reviews, fabricated expertise, misinformation, content farms designed purely for profit rather than value, and systems that seek to replace genuine human insight and connection, we know it\u2019s hurting us, a"
  },
  {
    "title": "Piramidal (YC W24) Hiring: Front End Engineer (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-11-13T21:00:27 1763067627",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/piramidal/jobs/i9yNX5s-front-end-engineer-user-interface",
    "first_paragraph": "Foundation Model for the BrainWe are looking for a software engineer to help us build Piramidal\u2019s flagship product. You'll be responsible for building and maintaining the system that enables performant web experiences for our users.We are building a first-of-its-kind foundation model for electrophysiological brain data. Our goal is to decode neural syntax in order to bridge the gap between biological and artificial intelligence.We are dedicated to redirecting technology to maximise human potential. At the heart of our mission is support for cognitive liberty - the fundamental right to freedom of thought, mental privacy, and self-determination.\u00a9 2025 Y Combinator"
  },
  {
    "title": "GitHub Partial Outage (githubstatus.com)",
    "points": 180,
    "submitter": "danfritz",
    "submit_time": "2025-11-13T15:04:39 1763046279",
    "num_comments": 76,
    "comments_url": "https://news.ycombinator.com/item?id=45915731",
    "comments": [
      "I really do feel for those hubbers that are still working on this years into the Microsoft era. GitHub was an excellent product and from what I hear it was an excellent culture too. I can only assume the culture has eroded similarly to the product itself as Microsoft has finally begun integrating the org into the slower moving machine that is MS.reply",
      "I was getting crazy thinking that there was something wrong with my SSH keys all of a sudden. Thanks $DEITY it's just GitHub.reply",
      "Same. I reflex replaced mine thinking it needed to be. Glad its working now thoughreply",
      "I\u2019m old enough to remember when GitHub was on main page due to a cool feature they added, now they just end up here when it stops workingreply",
      "If I remember it well, every once in a while a new cool feature was also breaking stuff, doubling the chances of getting to the top page here. But truth being told, GitHub was fixing those at light speed too and it was very interesting to follow their progress. Their delivery pipeline (per branch, deliver when ready, etc.) sounded very much innovative by then and I think inspired many people.reply",
      "Must be a day ending in Y.reply",
      "Anyone using GitLab have any insight on how well their operations are running these days?We originally left GitLab for GitHub after being bit by a major outage that resulted in data loss. Our code was saved, but we lost everything else.But that was almost 10 years ago at this point.reply",
      "We use GitLab on the daily. Roughly 200 repos pushing to ~20 on any given day. There have been a few small, unpublished outages that we determined were server side since we have a geo-distributed team, but as a platform seems far more stable than 5-6 years ago.My only real current complaint is that the webhooks that are supposed to fire in repo activity have been a little flaky for us over the past 6-8 months. We have a pretty robust chatops system in play, so these things are highly noticeable to our team. It\u2019s generally consistent, but we\u2019ve had hooks fail to post to our systems on a few different occasions which forced us to chase up threads until we determined our operator ingestion service never even received the hooks.That aside, we\u2019re relatively happy customers.reply",
      "FWIW, GitHub is also unreliable with webhooks. Many recent GH outages have affected webhooks.They are pretty good, in my experience, at *eventually* delivering all updates. The outages take the form of a \"pause\" in delivery, every so often... maybe once every 5 weeks?Usually the outages are pretty brief but sometimes it can be up to a few hours. Basically I'm unaware of any provider whose webhooks are as reliable as their primary API. If you're obsessive about maintaining SLAs around timely state, you can't really get around maintaining some sort of fall-back poll.reply",
      "> you can't really get around maintaining some sort of fall-back poll.This has been my experience with GitHub Actions as well, which I imagine rely on the same underlying event system as webhooks.Every so often, an Action will not be triggered or otherwise go into the void. So for Actions that trigger on push, I usually just add a cron schedule to them as well.reply"
    ],
    "link": "https://www.githubstatus.com/incidents/1jw8ltnr1qrj",
    "first_paragraph": "Resend OTP in:  seconds \n                    Didn't receive the OTP?\n                    Resend OTP \nResend OTP in: 30 seconds \n                      Didn't receive the OTP?\n                      Resend OTP \nThe URL we should send the webhooks toWe'll send you email if your endpoint failsGet tips, technical guides, and best practices. Twice a month. Right in your\r\n          inbox.\n          Subscribe to updates for Some users may experience failing git push and pull operations. via email and/or text message. You'll receive email notifications when incidents are updated, and text message notifications whenever GitHub creates or resolves an incident.\n        "
  },
  {
    "title": "Itiner-E \u2013 The Digital Atlas of Ancient Roads (itiner-e.org)",
    "points": 13,
    "submitter": "beatthatflight",
    "submit_time": "2025-11-06T20:13:35 1762460015",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://itiner-e.org/",
    "first_paragraph": "Itiner-e aims to host the most detailed open digital dataset of roads in the entire Roman Empire. The data creation is a collaborative ongoing\n                project edited by a scholarly community. Itiner-e allows you to view, query and download roads."
  },
  {
    "title": "The emergence and diversification of dog morphology (science.org)",
    "points": 15,
    "submitter": "Marshferm",
    "submit_time": "2025-11-13T23:01:21 1763074881",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=45921861",
    "comments": [
      "Every child should have a dog. It gives them valuable life lessons about responsibility, fidelity, unconditional love and to always turn around three times before you lie down.reply",
      "The dogs that win prizes often disturb me. I feel like there must be a bunch of inbreeding going on, and maybe for features that look interesting but aren't necessarily good for them.Is this concern misplaced? Not a dog show expert so maybe I'm just revealing my ignorance.reply",
      "My first dog taught me how to love animals, and that is a gift for which I'll forever be grateful.reply"
    ],
    "link": "https://www.science.org/doi/10.1126/science.adt0995",
    "first_paragraph": ""
  },
  {
    "title": "Rust in Android: move fast and fix things (googleblog.com)",
    "points": 285,
    "submitter": "abraham",
    "submit_time": "2025-11-13T18:32:36 1763058756",
    "num_comments": 203,
    "comments_url": "https://news.ycombinator.com/item?id=45918616",
    "comments": [
      "5 million Rust LOC \n    One potential memory safety vulnerability found \n    Rust is 0.2 vuln per 1 MLOC.\n\n    Compared to \n    C and C++ : 1,000 memory safety vulnerabilities per MLOC. \n\n\nKey take.reply",
      "There are certain places on the internet where any mention of rewriting in Rust is met with scorn and ire. And while, like any technical decision, there are pros and cons, I cannot see why in the face of astounding evidence like this, you would completely dismiss it.And I say this as someone who has never written a line of Rust in their life (some day I'll find the time).reply",
      "An earlier Google blog post from the same series (link in the first sentence) pointed out why: new code tend to have more vulnerabilities than established code. So it makes more sense to write new code in Rust than to rewrite old code in Rust. After all new features are still being added and new code needs to be written; it\u2019s not like the codebase is done with features.reply",
      "According to that blog post (https://security.googleblog.com/2024/09/eliminating-memory-s...), the vulnerability density for 5 year old code in Android is 7.4x lower than for new code. If Rust has a 5000 times lower vulnerability density, and if you imagine that 7.4x reduction to repeat itself every 5 years, you would have to \"wait\" (work on the code) for... about 21 years to get down to the same vulnerability density as new Rust code has. 21 years ago was 2004. Android (2008) didn't even exist yet.reply",
      ">So it makes more sense to write new code in Rust than to rewrite old code in Rust.This is a general result regardless of what language you're talking about (unless you're really downgrading to assembly or something crazy). This of course presumes that the overall Rust (or other new language) situation is better than the existing one. It's not generally.reply",
      "I think that it would be foolish for any software engineer to completely dismiss any technology. There is a time and place for any tool, and it is a job of a competent engineer to determine what the appropriate combination of these is that would solve a certain problem within specific constraints.That said, memory safety is one criterion out of many that could be used to make that decision. For a large number of software projects, memory safety simply isn't a major concern. Ease of use, iteration speed, developer familiarity, availability of specific libraries, and so on, are often equal or greater concerns than memory safety.So, sure, if you're writing a kernel, operating system, or a mission-critical piece of software, then Rust might be worth considering. Otherwise, you might be better served by other languages.reply",
      "[flagged]",
      "But every language has those, even C.You can just ignore those people. I\u2019d hate to miss out on a positive technical choice because some people were being annoying about it.reply",
      "Generally speaking, the purpose of a program is not to minimize the number of memory safety bugs. All other things being equal, yes, having fewer memory safety bugs is better than having more. But perhaps you're trading legible bugs for illegible bugs? The rust implementation is most likely going to be more complex than the c implementation (which is fair since it almost eliminated a whole class of bugs), and in that complexity there is extra room for non-memory safety related bugs.There's probably also 500x more people who know c to a given level then know rust to a given level.If we have an analyzer that can find memory safety bugs in C, we could also just put that in the CI pipeline, or as a pre-submit hook before you're allowed to add code to a code base.reply",
      "This idea that if Rust doesn't have all those memory safety bugs it must somehow have loads of other bugs we haven't discovered reminds me of Americans insisting that countries which don't have their lousy gun safety problems must have the same effects by some other means they haven't detected - Like, OK England doesn't have lots of gun murders like America, but surely loads of English people are just dropping dead because someone threw a yoghurt at them, or used harsh language, and we just missed them off our statistics ?No man, it is possible to just do better, and this is an example of just doing better. The Rust is just better software. We can and should learn from this sort of thing, not insist that better is impossible and the evidence suggesting otherwise must be a mirage.reply"
    ],
    "link": "https://security.googleblog.com/2025/11/rust-in-android-move-fast-fix-things.html",
    "first_paragraph": "\nLast year, we wrote about why a memory safety strategy that focuses on vulnerability prevention in new code quickly yields durable and compounding gains. This year we look at how this approach isn\u2019t just fixing things, but helping us move faster.\n\nThe 2025 data continues to validate the approach, with memory safety vulnerabilities falling below 20% of total vulnerabilities for the first time.\n\nUpdated data for 2025. This data covers first-party and third-party (open source) code changes to the Android platform across C, C++, Java, Kotlin, and Rust. This post is published a couple of months before the end of 2025, but Android\u2019s industry-standard 90-day patch window means that these results are very likely close to final. We can and will accelerate patching when necessary.\n\nWe adopted Rust for its security and are seeing a 1000x reduction in memory safety vulnerability density compared to Android\u2019s C and C++ code. But the biggest surprise was Rust's impact on software delivery. With Rus"
  },
  {
    "title": "I Built a One File Edge Probe to Tell Me When Time Is Lying (physical-ai.ghost.io)",
    "points": 4,
    "submitter": "boulevard",
    "submit_time": "2025-11-03T13:51:21 1762177881",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://physical-ai.ghost.io/a-one-file-pwa-to-tell-you-when-time-is-lying/",
    "first_paragraph": "I have lost more hours than I want to admit to problems that turned out not to be the network or the app or the API, but just bad time.We already have NTP, PTP (IEEE-1588), linuxptp, real grandmasters, fancy NICs. That\u2019s not what this is about. I wanted a super cheap way for a person holding an Android/iPad/laptop to answer exactly one question:So I built a single-file web app for that.It\u2019s just one HTML file you can stick in GitHub, serve from nginx on the local gateway, and tell a tech:This is not a PTP/IEEE-1588 monitor and it will not give you sub-\u00b5s accuracy. It also won\u2019t prove the server is telling the truth.It is just a browser-level sanity probe. It compares Date.now() (client) to /time (server), draws a tiny sparkline, and goes green/red. Because it\u2019s a PWA you can \"Add to Home Screen\" and keep it around.In short, it tells you:That\u2019s enough to stop chasing ghosts.Field people already have a browser. They can hit http://192.168.10.1/time. They can bookmark a page. This can run"
  }
]