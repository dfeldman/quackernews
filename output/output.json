[
  {
    "title": "Data Exfiltration from Slack AI via indirect prompt injection (promptarmor.substack.com)",
    "points": 338,
    "submitter": "tprow50",
    "submit_time": "2024-08-20T18:27:45",
    "num_comments": 87,
    "comments_url": "https://news.ycombinator.com/item?id=41302597",
    "comments": [
      "The key thing to understand here is the exfiltration vector.Slack can render Markdown links, where the URL is hidden behind the text of that link.In this case the attacker tricks Slack AI into showing a user a link that says something like \"click here to reauthenticate\" - the URL attached to that link goes to the attacker's server, with a query string that includes private information that was visible to Slack AI as part of the context it has access to.If the user falls for the trick and clicks the link, the data will be exfiltrated to the attacker's server logs.Here's my attempt at explaining this attack: https://simonwillison.net/2024/Aug/20/data-exfiltration-from...\n \nreply",
      "It gets even worse when platforms blindly render img tags or the equivalent. Then no user interaction is required to exfil - just showing the image in the UI is enough.\n \nreply",
      "Yup - all the basic HTML injection and xss attacks apply. All the OWASP webdev 101 security issues that have been mostly solved by web frameworks are back in force with AI.\n \nreply",
      "These attacks aren't quite the same as HTML injection and XSS.LLM-based chatbots rarely have XSS holes. They allow a very strict subset of HTML to be displayed.The problem is that just supporting images and links is enough to open up a private data exfiltration vector, due to the nature of prompt injection attacks.\n \nreply",
      "More like xxe I'd say.\n \nreply",
      "Can\u2019t upvote you enough on this point. It\u2019s like everyone lost their collective mind and forgot the lessons of the past twenty years.\n \nreply",
      "> It\u2019s like everyone lost their collective mind and forgot the lessons of the past twenty years.I think this has it backwards, and actually applies to every safety and security procedure in any field.Only the experts ever cared about or learned the lessons. The CEOs never learned anything about security; it's someone else's problem. So there was nothing for AI peddlers to forget, they just found a gap in the armor of the \"burdensome regulations\" and are currently cramming as much as possible through it before it's closed up.\n \nreply",
      "Some (all) CEOs learned that offering a free month coupon/voucher for Future Security Services to secure your information against a breach like the one that just happened on the platform that's offering you a free voucher to secure your data that sits on the platform that was compromised and leaked your data, is a nifty-clean way to handle such legal inconveniences.Oh, and some supposed financial penalty is claimed, but never really followed up on to see where that money went, or what it accomplished/paid for - and nobody talks about the amount of money that's made by the Legal-man & Machine-owitz LLP Esq. that handles these situations, in a completely opaque manner (such as how much are the legal teams on both sides of the matter making on the 'scandal')?\n \nreply",
      "Techies aren't immune either, before we all follow the \"blame management\" bandwagon for the 2^101-tieth time.CEOs aren't the reason supply chain attacks are absolutely rife with problems right now. That's entirely on the technical experts who created all of those pinnacle achievements in tech ranging from tech-led orgs and open source community built package ecosystems. Arbitrary code execution in homebrew, scoop, chocolatey, npm, expo, cocoapods, pip... you name it, it's got infected.The LastPass data breach happened because _the_ alpha-geek in that building got sloppy and kept the keys to prod on their laptop _and_ got phised.\n \nreply",
      "Yeah supply chain stuff is scary and still very open. This ranges from the easy stuff like typo-squatting pip packages or hacktavists changing their npm packages to wreck all computers in Russia up to the advanced backdoors like the xz hack.Another big still mostly open category is speculative execution data leaks or other \"abstraction breaks\" like Rowhammer.At least in theory things like Passkeys and ubiquitous password manager use should eventually start to cut down on simple phishing attacks.\n \nreply"
    ],
    "link": "https://promptarmor.substack.com/p/data-exfiltration-from-slack-ai-via",
    "first_paragraph": ""
  },
  {
    "title": "Bug squash: An underrated interview question (jez.io)",
    "points": 87,
    "submitter": "jez",
    "submit_time": "2024-08-18T15:07:51",
    "num_comments": 57,
    "comments_url": "https://news.ycombinator.com/item?id=41282807",
    "comments": [
      "I like this approach far, far more than coding tests!> It\u2019s fun. It\u2019s fun in the same way an escape room is fun. It\u2019s fun because of the dopamine you get when the test suite blinks green.Keep in mind that for lots of people in a job interview setting (even excellent candidates) this is not fun, this is stressful. It may be fun on the job or at home, but the stress of a job interview both eliminates any \"fun\" aspect and makes even simple tasks more difficult. Just mentioning that to encourage some amount of empathy and understanding for the candidate.\n \nreply",
      "This approach then selects those who are relaxed. The candidates that have five more interviews at advanced stages, and likely to receive several offers, as opposed to candidates who badly need a job.The former kind of candidate may be more desirable for the employer :-/\n \nreply",
      "Depends very much on the company.  Fishing in the unemployed-and-desperate end of the pool is a real tactic.  The odds are good but the goods are odd, as they say.\n \nreply",
      "Even if I have no other offers I\u2019m pretty good at maintaining a calm demeanor in an interview. I think it\u2019s served me well over the years.\n \nreply",
      "The discomfort and stress is a given in any interview. What I think is nice about this is it doesn\u2019t require you to emanate fully-formed, nicely designed code from your brain on the spot. Instead you just have to navigate the codebase, read it and understand it.It\u2019s a better demonstration of knowing how software works imo. But I am biased because this sort of thing is one of my strengths.\n \nreply",
      "At one place there was a bug squash interview like this. Rough idea was we wrote a very small version of a system we had in our app (some data-syncing-then-displaying service), with a handful of bugs and a very simple feature request.It was very helpful for sanity checking if a person was able to be in front of a computer. There's a bit of a challenge because I think there's a pretty low ceiling of performance (strace on a Python program is fun, but our bugs are not that deep!), but we could use it to also talk about new features on this mini-system and whatnot.General feeling on it was \"this is a great filter\", because ultimately we needed people to just pass above a minimum skill bar, not be maximally good at coding. And having the exercise really helped to communicate the day-to-day (which can be tedious!)\n \nreply",
      "I've done the equivalent of this by asking them to describe an interesting bug they've encountered in past lives; how it came up, how they hunted it, how they fixed it.  By listening to them describe the work, asking questions, and following their thought processes, you can come to a fairly good hire/no from this single walk-through.I know, it's short and humane, so not a good fit for current Sillycon Valley culture.\n \nreply",
      "Triplebyte (one of the best hiring experiences I ever had) gave me this during their initial interview.They dropped an archive of a medium-ish codebase to my machine, that had failed unit tests. My task was to simply fix the code so the tests passed.Not only did I feel engaged with the interview because I could speak aloud as I debugged, I also found it fun!\n \nreply",
      "I've only had one \"find the bug\" interview, and it was awful:- Didn't set you up with a way to reliably run/test the code, nor a step-through-debugger which, jeez is it 1980? Like setting up a repo in a language you aren't familiar with (say getting your py-env exactly right) can take a whole hour if it's not your main language.- Didn't have standardized questions, which is hugely problematic (some bugs are 2 orders of magnitude harder than others)- It also just seems like there's a huge random element, like am I debugging code written by somebody who thinks at the same level of abstraction as me? Did they write comments I understand?\n \nreply",
      "> - Didn't set you up with a way to reliably run/test the code, nor a step-through-debugger which, jeez is it 1980? Like setting up a repo in a language you aren't familiar with (say getting your py-env exactly right) can take a whole hour if it's not your main language.How frequently do people interview in a language other than their \"main\" one(s)?> - Didn't have standardized questions, which is hugely problematic (some bugs are 2 orders of magnitude harder than others)How do you know they're not standardized? (You say in another comment where it was, and indeed that's where the blog post author works. It's described as a pretty standardized process by my reading of it.) You can pass the interview without actually solving the bug, but I get it's easier to blame the problem than admit you struggled with it.\n \nreply"
    ],
    "link": "https://blog.jez.io/bugsquash/",
    "first_paragraph": ""
  },
  {
    "title": "Basic Mechanisms In Fire Control Computers (1953) [video] (youtube.com)",
    "points": 286,
    "submitter": "teqsun",
    "submit_time": "2024-08-20T12:02:00",
    "num_comments": 103,
    "comments_url": "https://news.ycombinator.com/item?id=41299211",
    "comments": [
      "In 1989 I was a data systems tech on a Destroyer  going through some overhaul at the shipyard in Pascagoula Mississippi. Moored right next to us was the battleship Wisconsin. Huge relic from WW2 but still going through modernization. A bunch of us that worked on combat systems got invited for a tour of their fire control systems.Wow. Just wow. All mechanical computers calculating fire control solutions for the big 16 inch guns. The guys giving the tour were well beyond the age for regular military retirement. Come to find out, they were all reactivated because practical knowledge of the mechanical computers had since left the navy. That was a very cool day.\n \nreply",
      "By the end of WW II American torpedoes were automatically programmed (direction, speed, fusing) before firing.  The heavy calculations would be done by the shipboard firing computer while the parameters set would be used by the simple computer on the torpedo (which had inertial guidance). I struggle to imagine how people managed to design such things with just pencils and slide rules.\n \nreply",
      "BTW these computers were mostly very functional, in the modern sense. They took inputs from instruments and controls, and computed functions, all usually continuous, smooth, real- or complex-valued. These functions' values, computed as voltages, frequencies, angles, etc were directly controlling some actuators, rudders, throttles, etc.It's also highly compositional, as in applying relatively simple functions to results of other such functions, etc., which you can reason about analytically, and can plot on paper or an oscilloscope as a part of development and testing loop.Disclaimer: all my hands-on experience with analog computers is from a one-semester course decades ago, using analog electronic, not mechanical devices.\n \nreply",
      "Not exclusive to the US though, check out the \"Torpedovorhalterechner\" ;)http://www.tvre.org/en/torpedo-fire-control-system-on-german...Scrolling down there's a nice photo with removed cover.\n \nreply",
      "Yes, in fact even a Harrier Jump Jet was designed with pencils and a slide rule!\n \nreply",
      "I.suspect they used tabulators and other such mechanical calculation devices, with a higher precision and faster speed than a slide rule.\n \nreply",
      "WW II American torpedoes didn't have inertial guidance. They used gyros for directional control and just ran in a straight line after making a single turn onto the set course. Occasionally the torpedo would get stuck in that turn and run in a circle. Towards the end of the war the Navy also started introducing homing torpedoes, but those didn't use inertial guidance either.\n \nreply",
      "> Occasionally the torpedo would get stuck in that turn and run in a circle.Well that's not a great failure mode, if it can come right back at vessel which launched it ... imagine trying to implement a self-destruct failsafe with that tech back then ...\n \nreply",
      "At least two US Navy submarines were sunk by their own torpedoes making circular runs. The main failsafe mechanism disabled the detonators until the weapon had run out a certain minimum distance but obviously that wasn't effective in circular runs.https://www.usni.org/magazines/naval-history-magazine/2011/j...\n \nreply",
      "A gyro by definition IS inertial guidance.\n \nreply"
    ],
    "link": "https://www.youtube.com/watch?v=gwf5mAlI7Ug",
    "first_paragraph": ""
  },
  {
    "title": "The first mass-produced DRAM of the Soviet Union (cpushack.com)",
    "points": 19,
    "submitter": "segasaturn",
    "submit_time": "2024-08-16T20:10:21",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.cpushack.com/2023/12/21/the-first-mass-produced-dram-of-the-soviet-union/",
    "first_paragraph": ""
  },
  {
    "title": "Toasts are bad UX (maxschmitt.me)",
    "points": 450,
    "submitter": "Mackser",
    "submit_time": "2024-08-20T10:57:27",
    "num_comments": 283,
    "comments_url": "https://news.ycombinator.com/item?id=41298794",
    "comments": [
      "https://en.wikipedia.org/w/index.php?title=Toast_(computing)",
      "I'm not convinced. Most of the argument seems to be that redundant UX is bad UX:> But by archiving the email, the email disappears from the list, which already implies the action was successful.> In this example, the button already includes a confirmation so the toast is entirely unnecessary.I vehemently disagree with the idea that just because you're already communicating something one way it's bad UX to include another way of communicating the same thing at the same time. Redundancy in communication is a feature, not a bug, and it's present in all human languages. It ensures that even in less than ideal conditions the message still gets through.In the case of toasts, having a single, standardized way of communicating the status of all actions (and if possible providing the undo) allows a user to quickly pick up on the pattern. Extra indicators closer to the action can be valuable too, but it's when they're paired with the toast that their meaning becomes entirely clear.  To remove the toast in favor of a bunch of specific indicators is to force your user to learn several different ways of saying \"it's done now\" entirely from context (many of which will be small and subtle as in the examples given). This might work fine for you and me but isn't great for, say, the elderly or the vision impaired or children.Unless they're actually getting in the way, toasts aren't bad UX, they're redundant UX, and a UX designer shouldn't be striving to optimize away redundancy.\n \nreply",
      "The unfortunate thing is they aren\u2019t communicating the same thing.Taking the YouTube example, the checkboxes are 100% optimistic while the toast notification indicates that the request to the backend that was fired off asynchronously was successful. With the archive message example, it is the same thing. The message is removed from the list optimistically and the toast message is representing that the message was actually archived.I would much rather only get the toast if there is a failure to commit the change. Generally, them flashing up is a distraction from what I\u2019m trying to accomplish. And being far on the screen from where I\u2019m taking an action makes them even more of a distraction.\n \nreply",
      "> I would much rather only get the toast if there is a failure to commit the change ... And being far on the screen from where I\u2019m taking an action makes them even more of a distraction.But wouldn't this situation be even worse with a failure-only toast?  A request timeout could happen 30 seconds after the fact.  You're likely in a very different UI state at that point, and unless the error message is very specific, you'll have no idea what even failed if you are quickly performing lots of actions.\n \nreply",
      "Toasts showing up far from where the action is take also makes them super annoying for people (like me) who use screen magnifiers. I'm oftne using a site while zoomed in, and will completely miss a toast, because it never enters the \"viewport\" on the screen I'm looking at.\n \nreply",
      "What kind of design choices do you find helpful with using a magnifier like that? It's not something I'd ever considered before, sounds tricky to design for but I'll try to keep it in mind now.\n \nreply",
      "The two main things for me are:- Put cause and effect close to eachother\n- Don't block my view based on mouse position. I hate video players that ofverlay the pause button when the mouse is over the video, or images that get obscured by some overlay when hovered. My zoom follows the mouse, so I can't move what I'm looking at and where my mouse is pointing independently.\n \nreply",
      "Oh yeah those video players are awful for anyone on mobile too, always ends up somehow getting stuck active and the only way to dismiss it is tap the video, which of course is usually bound to some other disruptive action like pausing or exiting full-screen mode.\n \nreply",
      "Good questions - also note that fixes that would help magnifier people also benefit users who have overlapping windows and/or windows partially off-screen.  (This is also an example of accessibility features helping people who are \"fully-abled\")\n \nreply",
      "I disagree on that\u2014in the YouTube example specifically this isn't necessarily a problem, but the toast serves a valuable purpose in the archive in that it tells you again which button it was that you pressed. There have been countless times in cases like that where the toast has saved me and allowed me to undo a misclick.I can see the argument that there are certain places where people use toasts that are unnecessary and provide information that the user doesn't need. But that's not the same thing as toasts being bad UX in the general case.\n \nreply"
    ],
    "link": "https://maxschmitt.me/posts/toasts-bad-ux",
    "first_paragraph": "Max SchmittJuly 17 2024The core problem is that toasts always show up far away from the user's attention.Take a look at this example from YouTube:In this particular example, the entire interaction is quite jarring:And there are a few more problems in this particular example:Here is a simple redesign of the \"Save\" interaction that solves all the problems above:When archiving an email in Gmail, a toast appears showing confirmation. But by archiving the email, the email disappears from the list, which already implies the action was successful.NoteWe do have to consider the undo-functionality and that the toast feedback can be useful when using keyboard\nshortcuts.A toast is shown after something was copied to the clipboard. In this example, the button already includes a confirmation so the toast is entirely unnecessary.What's worse than a toast? No feedback at all.So if you don't have time to design or build a better feedback mechanism, a toast is better than nothing.Hey, my name is Max!I'"
  },
  {
    "title": "Migrating from DokuWiki to Obsidian (kaeruct.github.io)",
    "points": 29,
    "submitter": "kaeruct",
    "submit_time": "2024-08-19T07:16:28",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=41288579",
    "comments": [
      "I decided to migrate from DokuWiki to Markdown some time ago too. I write a python script for the conversion if anyone is interested https://github.com/michalmiddleton/Dokuwiki2Markdown\n \nreply",
      "Intended color scheme? https://i.ibb.co/741mzfh/image.png\n \nreply",
      "I got dark on more dark. Hope this was awesome, but I can't read it.Chrome.\n \nreply",
      "Thats what i get on firefox on ios\n \nreply",
      "Looks correct now on Firefox on macOS (light mode, probably)\n \nreply",
      "The screenshot doesn't load for me, but I see a very dark color, maybe purple, on black background. Absolutely no way to read anything, just about able to make out that there is some text.Firefox on android, no dark mode.\n \nreply",
      "This website is a good test of who actually clicked the link, not able to read a thing, coming back to the comments to report the bad experience, and who didn't click the link/is a bot, talking about the article as if nothing happened.\n \nreply",
      "The styling isn't broken for everyone (looks fine to me, Firefox on Fedora in dark mode), so I'm afraid the test may have some false positives.\n \nreply",
      "Yes, it's only broken in \"light mode\"; looks like there's a bug/omission in the CSS it doesn't correctly set the background colour (but does set the text colour).\n \nreply",
      "I simply flicked on reader mode (Firefox/Android). But to the OP yes, please do fix your styles!\n \nreply"
    ],
    "link": "https://kaeruct.github.io/posts/2024/08/18/migrating-from-dokuwiki-to-obsidian/",
    "first_paragraph": "\udb80\udced 2024-08-18About a year ago, I decided to move all my personal notes from DokuWiki to Obsidian.DokuWiki is great software and has served me well, but I never utilized it to its full capacity. It\u2019s biggest strength is that it\u2019s a\u2026 wiki, and I was the sole user.It made little sense for me to keep a service running when I could just have some Markdown files locally. And Obsidian provides a really nice UI on top of those Markdown files.One reason I chose DokuWiki in the first place, is that it doesn\u2019t need a database. All the content is kept in plaintext files. So if I ever wanted to move my notes elsewhere, it would hopefully be a painless process. In the end, I think it was a good choice, because the migration was quite simple.These are the steps I followed:First, I needed to get the source text and image files from DokuWiki to my local machine. All of it is located at /var/www/dokuwiki/data/ for a common installation. This directory contains all the text files and images that comprise "
  },
  {
    "title": "Making database systems usable (muratbuffalo.blogspot.com)",
    "points": 63,
    "submitter": "jamesblonde",
    "submit_time": "2024-08-20T18:13:21",
    "num_comments": 53,
    "comments_url": "https://news.ycombinator.com/item?id=41302453",
    "comments": [
      ">They care less about impressive benchmarks or clever algorithms, and more about whether they can operate and use a database efficiently to query, update, analyze, and persist their data with minimal headache.Hugely important, and I would add \"backup-and-restore\" to that list. At risk of sounding conceited, ease of use is a primary goal of rqlite[1] -- because in the real world databases must be operated[2]. I never add a feature if it's going to measurably decrease how easy it is to operate the database.[1] https://www.rqlite.io[2] https://docs.google.com/presentation/d/1Q8lQgCaODlecHa2hS-Oe...Disclaimer: I'm the creator of rqlite.\n \nreply",
      "Or, I should say, I don't add the feature until I can figure out how it can made be easy and intuitive to use. That's assuming the feature is even coherent with the existing feature set of the database.Of course, it's easy for me to do this. I am not developing the database for commercial reasons, so can just say \"no\" to an idea if I want. That said, I've found that many ideas which didn't seem interesting to me when an end-user first proposed them become compelling once I think more about the operational pain (and it's almost always operational) they are experiencing.Automatic backups to S3[1] was such a feature. I was sceptical -- \"just run a script, call the backup API, and upload yourself\" was my attitude. But the built-in support has become popular.[1] https://www.philipotoole.com/adding-automatic-s3-backups-to-...\n \nreply",
      "The distance between a large effort and a moderate effort is not very long; from the user's perspective, both things are in the realm of the Hassle.The chasm between a small required effort and zero effort is vast, from the user's perspective.Any product person will tell you that. Hitting the right zero-effort target is what separates a runaway success from a tepid reaction.\n \nreply",
      "yes, definitely.  I find it helpful to think of usability and friction as an inverse square law relation [0].  Small increases in friction (x-axis) cause dramatic drop-off in usability (y-axis) to begin with, then correspondingly less so.  Specific user tolerance will vary, but adoption broadly follows a similar path - exponential drop off.I've never seen any data to back this up in a quantitative sense (though interested if anyone has?).  Nevertheless, I've still found it useful as a qualitative rule of thumb in a positive sense: shaving off small edges of friction can have non-linear return in adoption and satisfaction.[0]: https://en.wikipedia.org/wiki/Inverse-square_law\n \nreply",
      "Add schema migrations and bulk loads to this. So many systems crap out doing things like schema migrations at scale. Query latencies degrade over time due to internal structures keeping track of things, stuff runs out of buffer / log space in memory or on disk, you have a traffic spike but you can't pause or throttle a 40 hour long ALTER, things that should never lock do lock for indeterminate times during cutover, stuff craps out after cutover due to surprising behavior but there is no rollback, or even worse I've seen things just flat out crash with some random assert fail or segfault.It's a world of pain, and there are so much scar tissue of third party tooling doing crazy stuff dealing with this problem among large companies that really should be the DB vendor's problem.\n \nreply",
      "If you spend time reading docs \u2013 I mean really reading them, you\u2019ll probably find out why something locked. For example, MySQL 8.? can extend metadata locks to cover foreign key relationships. This seemingly innocuous line in docs can cause the entire DB to grind to a halt under load.I\u2019m not saying this is good, only that RDBMS management is very much still in the land of arcane incantations, and anyone saying otherwise is selling something. It\u2019s also why I have a job, so from my perspective it isn\u2019t all bad.\n \nreply",
      "Something thats always bugged me about relational database modelling is how you have to use table relationships for everything. Humans have a special category for ownership (eg Order owns DeliveryAddress), which works differently from other kinds of relationships. Eg Order references Products.This problem is heightened by the fact that a SQL table typically can't store composite types. Like, you can't make a column of lists-of-strings. Every modelling problem is solved by using more tables. And any nontrivial application ends up with a table explosion. You have to study your database very closely, then write complex, slow, join-heavy queries, just to get the information about simple stuff - like an order.Solving every problem with tables might seem clever from a technical perspective, but its just not how humans think. There's a reason document databases like mongodb are popular. Even if they are worse technically, they're much easier to use and reason about.There's no reason SQL databases couldn't support nested data. Postgres sort of does already via JSON fields. But it feels like you have to fight the database to do it.\n \nreply",
      "Your argument is a variant of the object\u2013relational impedance mismatch problem[1]. It's easier for us to reason about objects (or functions and types) than it is to think in terms of SQL tables, so it is much easier to develop complex logic or domain models in general purpose programming languages. Yet data storage and retrieval is much, much more efficient with relational databases, and it is generally a good practice to logically separate the data storage layer from the rest of the system anyway. But implementing that layer to translate from the relational model to the object model frequently involves tons of finicky, manually crafted SQL statements embedded in general purpose code where the linting/type checking/static analysis tooling often isn't great for the embedded SQL. The only other alternatives are using an ORM that automagically handles most if not all of the relational-object mapping or a NoSQL solution like Mongo that avoids the mismatch altogether. Both those alternatives typically involve sacrificing performance and scalability, however, and the limitations and quirks of each frequently contribute to bugs (e.g., schema drift in document DBs, or implementation details of the ORM that make it hard to map nested relationships).[1] https://en.wikipedia.org/wiki/Object%E2%80%93relational_impe...\n \nreply",
      "I don't think it's just about storage and retrievel being efficient. Sometimes it _is_ easier to think in terms of tables.I have a bunch of posts, written by various authors, on various sites. Store that in some OOP way, with ownership going in some direction (For example, sites own posts). Now look up the most recent posts for authors. Look up the most recent posts per site. Look up the most prolific authors. Do the authors work cross-site? Maybe they don't but people use the same name across sites.There are plenty of times I have put things into a database not for perf reasons but simply because it's easier to do queries that way than to write a bunch of bespoke logic at each variant of certain queries, simply due to ownership arrows being all over the place\n \nreply",
      "I think this is a poor understanding and laziness. SQL is type and schema first and people hate that because it makes things hard and complicated up front. Table explosions are rare if you know what you are doing. Many people don't any more.As for join heavy, complexity, this is not necessarily a problem in reality. It's incredibly easy to scale this out to huge systems (relatively) cheaply.Believe me as they scale up, they look way less hard and less complicated than arbitrary and poorly enforced schemas in document databases. I could write an essay on how to fuck up MongoDB (or any document store) because I spent nearly 2 years unfucking one.\n \nreply"
    ],
    "link": "http://muratbuffalo.blogspot.com/2024/08/making-database-systems-usable.html",
    "first_paragraph": "\nOn distributed systems broadly defined and other curiosities. The opinions on this site are my own.\nC. J. Date's Sigmod 1983 keynote, \"Database Usability\", was prescient. Usability is the most important thing to the customers. They care less about impressive benchmarks or clever algorithms, and more about whether they can operate and use a database efficiently to query, update, analyze, and persist their data with minimal headache. (BTW, does anyone have a link to the contents of this Sigmod'83 talk? There is no transcript around.)The paper we cover today is from Sigmod 2007. It takes on the database usability problem raised in that 1983 keynote head-on, and calls out that the king is still naked.\u00a0Let's give some context for the year 2007. Yes, XML format was still popular then. The use-case in the paper is XQuery. The paper does not contain any \u00a0reference to json. MongoDB would be released in 2009 with the document model; and that seems to be great timing for some of the usability pa"
  },
  {
    "title": "Launch HN: MinusX (YC S24) \u2013 AI assistant for data tools like Jupyter/Metabase",
    "points": 65,
    "submitter": "nuwandavek",
    "submit_time": "2024-08-20T16:24:02",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=41301448",
    "comments": [
      "This is impressive! We use Metabase and I've been wanting this exact user experience for quite some time. So far, I've been dumping our Postgres schema into a Claude project and asking it to generate queries. This works surprisingly well, save for the tedious copy-paste between the two tabs. The Chrome extension workflow makes perfect sense.Is there a way to select which model is being used? Anecdotally, I've found that Claude 3.5 Sonnet works incredibly well with even the most complex queries in one shot, which is not something I've seen with GPT-4o.\n \nreply",
      "Haha, yes! We were doing the exact same thing. Also, there is so much context you can't capture with just table schema that you can if you integrate the extension deep into the tool. It also unlocks cross-app contexts (we're working on a way to import context from a doc to a metabase query, or from a sheet/dashboard to a jupyter notebook etc.> Is there a way to select which model is being used?\nNot at the moment, but this is in our pipeline! We will enable this (and the ability to edit the prompts, etc.) very soon.Do try it out and let me know what you think!\n \nreply",
      "I love that you can take a screenshot and it starts to explain what it sees!While this is clearly an ai analytics assistant your \"retrofit\" approach certainly differentiates you from existing approaches:\nhttps://github.com/Snowboard-Software/awesome-ai-analyticsNot quite sure if this should be a seperate category? It's more similar to the web automation agents like https://www.multion.ai/ than to https://www.getdot.ai/.\n \nreply",
      "We love that feature too and use it quite a bit ourselves!> Not quite sure if this should be a separate category?We see ourselves at the intersection of generic browser-automation agents and generic coding agents. MinusX integrates deeply into jupyter/metabase (we had to do a lot of shenanigans to get the entire jupyter app context) and has more context than RPA agents do today. It is possible that eventually all these apps will converge, but we think MinusX will be more useful for anything data related than any of them for the foreseeable future.To paraphrase geohot, we think that the path to advanced agents runs through specialized, useful intermediaries.\n \nreply",
      "How does the AI know about things like other tables? Does it have some basic knowledge of Metabase\u2019s link structure so it can navigate to a listing of all tables, then pulls context from there for in-context learning while writing the query?Anecdotally, my hardest problems w/ nl2sql are finding the right tables and adding the right filters.\n \nreply",
      "Yep! MinusX uses Metabase APIs to pull relevant tables, schema, & dashboards to construct the context for your instruction.> Anecdotally, my hardest problems w/ nl2sql are finding the right tables and adding the right filters.Totally! especially in large orgs with thousands of tables. Using your existing dashboards and queries, gives useful context on picking the right tables for the query.\n \nreply",
      "In your demo, you seemed to have performed everything on a small dataset.How\u2019s the performance on doing the same analysis on a dataset with 1 billion rows for instance?Also does this work with self hosted Metabase or Metabase Cloud? Or both?\n \nreply",
      "> How\u2019s the performance on doing the same analysis on a dataset with 1 billion rows for instance?This really depends on whether your tool can handle the scale. We only use a sample of the outputs when constructing the context for your instruction so it should be independent of the scale of the data. We mostly use metadata such as table names, fields, schemas etc to construct the context.> Also does this work with self hosted Metabase or Metabase Cloud? Or both?Yep, it should work on both :) We have users across both\n \nreply",
      "This is very interesting. Can we bring our own API keys? Is that in the roadmap?\n \nreply",
      "Yes! Both bring-your-own-keys and local models are on the roadmap. The ETA for both is ~1-2 weeks.\n \nreply"
    ],
    "link": "item?id=41301448",
    "first_paragraph": ""
  },
  {
    "title": "Nasir Ahmed's digital-compression breakthrough helped make JPEGs/MPEGs possible (ieee.org)",
    "points": 112,
    "submitter": "Brajeshwar",
    "submit_time": "2024-08-20T13:52:21",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=41300071",
    "comments": [
      "Wikipedia writes: \"Ahmed developed a practical DCT algorithm with his PhD students T. Raj Natarajan, Wills Dietrich, and Jeremy Fries, and his friend Dr. K. R. Rao at the University of Texas at Arlington in 1973.\" [1]So perhaps it would fair to give due credit to the co-workers as well.[1] https://en.wikipedia.org/wiki/Discrete_cosine_transform\n \nreply",
      "The DCT is really neat, but the actual compression magic comes from a combination of side effects that occur after you apply it:1. The DCT (II) packs lower frequency coefficients into the top-left corner of the block.2. Quantization helps to zero out many higher frequency coefficients (toward bottom-right corner). This is where your information loss occurs.3. Clever zig-zag scanning of the quantized coefficients means that you wind up with long runs of zeroes.4. Zig-zag scanned blocks are RLE coded. This is the first form of actual compression.5. RLE coded blocks are sent through huffman or arithmetic coding. This is the final form of actual compression (for intra-frame-only/JPEG considerations). Additional compression occurs in MPEG, et. al. with interframe techniques.\n \nreply",
      "The \"actual compression magic\" has been used before DCT in other codecs, but applied directly to pixels gave lousy results.You can also look at 90's software video codecs developed when DCT was still too expensive for video. They had all kinds of approaches to quantization and entropy coding, and they all were a pixelated mess.DCT is the key ingredient that enabled compression of photographic content.\n \nreply",
      "What's so special about DCT for image compression?The main idea of lossy image compression is throwing away file detail, which means converting to frequency domain and throwing away high frequency coefficients. Conceptually FFT would work fine for this, so use of DCT instead seems more like an optimization rather than a key component.\n \nreply",
      "DCT is now replaced by Hadamard Transform which can be implemented by additions/subtractions and don't have the drift problem of DCT. HT was considered before DCT, but during that time DCT was picked because of better perceptual quality. Later during H.264 standardization, HT replaced DCT and is now used in all video codecs instead of DCT.\n \nreply",
      "Interestingly enough, JPEG XR used a form of the Hadamard Transformation, but JPEG XL (which is newer) uses DCT and Haar transforms.[edit]Combined with the information from sibling comments, it seems that the Hadamard transform was something used in standards developed in the '00s but not since.\n \nreply",
      "probably because multiplication got really fast\n \nreply",
      "Nope.X265/HEVC https://en.m.wikipedia.org/wiki/High_Efficiency_Video_CodingAlso not true for X266/VVC.\n \nreply",
      "AV1 also uses DCT and DST, but not Hadamard.\n \nreply",
      "correct, it is integer DCT. Lot of techniques adopted from the integer transform of H.264. That's what I meant, not the floating point DCT proposed in 70s.\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/compression-algorithms",
    "first_paragraph": "His digital-compression breakthrough helped make JPEGs and MPEGs possibleWillie Jones covers transportation for IEEE Spectrum and the history of technology for The Institute.Nasir Ahmed\u2019s compression algorithms helped result in the standards that underpin much of the modern Internet.Stop for a second and think about the Internet without digital images or video. There would be no faces on Facebook. Instagram and TikTok probably wouldn\u2019t exist. Those Zoom meetings that took the place of in-person gatherings for school or work during the height of the COVID-19 pandemic? Not an option.Digital audio\u2019s place in our Internet-connected world is just as important as still images and video. It has changed the music business\u2014from production to distribution to the way fans buy, collect, and store their favorite songs.What do those millions of profiles on LinkedIn, dating apps, and social media platforms (and the inexhaustible selection of music available for download online) have in common? They r"
  },
  {
    "title": "Reflecting on transducers in Scheme (2023) (thatgeoguy.ca)",
    "points": 54,
    "submitter": "fanf2",
    "submit_time": "2024-08-20T17:42:03",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=41302150",
    "comments": [
      "I wrote the transducers srfi.I don't really mind schemes monomorphic nature and chose to adhere to it and leave a generic transduce form for whenever there is a good way to implement it in a portable way. There is really nothing stopping anyone from extending SRFI-171 with a generic transduce form (and some more transducers that I left out because didn't really think it through). Such a project has my blessing and I would be happy to mark the SRFI as superseded.A vector-reduce form would be trivial but icky, and I chose not to do it to not have to have the continuation safety discussion. I have an idea to make thread and continuation safe transducers with immutable and visible state, but the first PoC was pretty slow. (I am going to say it... I miss c++ move semantics. Ouch)Anyway, if I read things correctly the complaint that srfi-171 has delete dupes and delete neighbor dupes forgets that transducers are not always used to or from a data structure. They are oblivious to context. That is why both are necessary.The SRFI document was written for someone who already knows what a transducer is, and specifies an API that implementers are to follow. I did not intend for it to be user documentation. User documentation is severely lacking. I was hoping for it to make it into r7rs-large (hubris. I know) and then I would make some kind of push to document it better. As it is now I have very little computer time.Regarding why transducers are faster I am still pretty certain it has to do with mutation and boxing. Looking at the assembly generated by srfi-171 in chez I don't really see that much aggressive inlining - and I don't think chez would fare much worse with srfi-158.  Generators and accumulators use set! everywhere, meaning chez (and guile) doesn't really try to keep the values unboxed or typed. That incurs quite a slowdown. It does use more state though.Sorry about the messy response. Typing this while walking home.In short: his library looks fine. Use it. From what I can see the only differences are ordering of clauses to make the transduce form generic and naming conventions. His library shadows a bunch of bindings in a non-compatible way. The transduce form is still not generic but moves the list-, vector-, generator- part of transduce into a \"folder\". Which is fine. But a generic dispatch would be nicer.Ask me anything I guess.\n \nreply",
      "Author of the post here, hi! Funny to see this resurface again, I have made a number of changes to the transducers library since this blog post (see: https://wiki.call-cc.org/eggref/5/transducers).A vector-reduce form would be trivial but icky, and I chose not to do it to not have to have the continuation safety discussion.I am not sure what \"continuation safety\" refers to in this context but I wanted a library that would give me a good out-of-the-box experience and have support for commonly used Scheme types. I have not yet added any folders/collectors/transducers specific to some types (like anything related to streams or SRFI-69), but I think a broad swath of types and patterns are currently covered.I think in particular my griped regarding vectors were that collectors such as `collect-vector`, `collect-u8vector`, etc. were not implemented. There is a chance to break out of these collectors using continuations but that's not really a good argument to not have them (I hope this is not what you're referring to!).Anyway, if I read things correctly the complaint that srfi-171 has delete dupes and delete neighbor dupes forgets that transducers are not always used to or from a data structure. They are oblivious to context. That is why both are necessary.I think this is exactly my argument: they are oblivious to context and actually do the wrong thing by default. I've seen this happen in Rust with users preferring `dedup` or `dedup_by` (from the Itertools crate) rather than just constructing a HashSet or BTreeSet. It almost always is used as a shortcut to save on a data structure, and time and again I've seen it break workflows because it requires that the chain of items is first sorted.I think this is is particularly damning for a library that means to be general purpose. If users want to implement this themselves and maintain it within their own code-bases, they're certainly welcome to; however, I don't personally think making this kind of deduping \"easy\" helps folks in the general sense. You'd be better off collecting into a set or bag of some kind, and then transducing a second time.From what I can see the only differences are ordering of clauses to make the transduce form generic and naming conventions. His library shadows a bunch of bindings in a non-compatible way. The transduce form is still not generic but moves the list-, vector-, generator- part of transduce into a \"folder\". Which is fine. But a generic dispatch would be nicer.Shadowing bindings in a \"non-compatible\" way can be bad, but it also helps to make programs more clean. If you're using transducers across your codebase, you almost certainly aren't also using e.g. SRFI-1's filter.As for generic dispatch: I agree wholeheartedly. I wish we had something like Clojure protocols that didn't suck. I've looked into ways to (ab)use variant records for this sort of thing, but you run into an open/closed problem on extending the API. This is really something that needs to be solved at the language level and something like COOPS / GOOPS incurs a degree of both conceptual and actual performance overhead that makes them somewhat unsatisfying :(And also: thank you for SRFI-171. I disagree with some of the design decisions but had it not been written I probably wouldn't have even considered transducers as something worth having.\n \nreply",
      "Fun article.  I'm somewhat still in the camp of loving Common Lisp's LOOP over many of the newer tools that are for looping over things.  Articles like this do a good job of shining light on a lot of the concerns.Quick nit/question.  For the fold method, I don't think I've seen it called sentinel value.  Usually it is seed or initial?Now, my main question.  Transducer?  I'm curious on the etymology of that word.  By itself, I don't think I could ever guess what it was referencing. :(\n \nreply",
      "AFAIK, the term was popularized by Rich Hickey in Clojure's implementation [1]. His talk introducing the concept goes into the etymology specifically. If I remember correctly it's something like \"to carry across.\"[1] https://clojure.org/reference/transducers\n \nreply",
      "Quick nit/question. For the fold method, I don't think I've seen it called sentinel value. Usually it is seed or initial?I think in Scheme it is common to call it knil, mirroring how lists use nil as the \"sentinel\" value which marks the end of a proper list. I opted to name it sentinel in that article (and in the docs) for two reasons:1. Sentinel values are a common topic in many languages https://en.wikipedia.org/wiki/Sentinel_value2. Transducers necessarily abstract across a lot more than loops / lists. Lisps do a lot of really cool (and optimized) stuff with lists alone and Scheme is no different in this regard. However, because of how Scheme primarily exports list operations in (scheme base) is really easy to run into a situation where lists are used in an ad-hoc way where another data structure is more appropriate. This includes vectors, sets, mappings, hashmaps, etc. Transducers-the-library is meant to be general across operations that work on all of these types, so I chose language that intentionally departs from thinking in a list-focused way.Now, my main question. Transducer? I'm curious on the etymology of that word. By itself, I don't think I could ever guess what it was referencing. :(This is from Rich Hickey's presentation: https://www.youtube.com/watch?v=6mTbuzafcIIIt's not a reducer, because they serve as higher-order functions that operate on reducers. Instead, the values they accept are transient through the function(s), so they transduce. You should watch the video, I think Rich explains the origins of his language very well.\n \nreply",
      "Transducers are not loops. You can create transducers and pass them as arguments to functions, that in their turn can prepend or append then.They are composable algorithmic transformations.\n \nreply",
      "composable algorithmic transformation in the streetsbut mostly an alternative to LOOP in the sheets\n \nreply",
      "Sure. But in that case it is somewhere between map(car)-and-friends and LOOP.Which is a situation where they add very little. Being able to use them as an intermediate step wherever data flows is probably the only place I use them myself. In channels, in file-readers etc. in places where you really need speed you should of course reach for whatever loop construct you prefer.\n \nreply",
      "You are correct with your question, SRFI 1[0] describes the knil argument as the \"seed\" or fold state (the latter for a recursive implementation). A sentinel value usually refers to a final value.Regarding the etymology: transform + reduce = transduce[0] https://srfi.schemers.org/srfi-1/srfi-1.html#fold\n \nreply",
      "I can't be alone in thinking that LOOP usage produces nearly unreadable code. It's extremely difficult to figure out what the intended behavior is if you don't have decades of reading them under your belt.\n \nreply"
    ],
    "link": "https://www.thatgeoguy.ca/blog/2023/01/04/reflections-on-transducers/",
    "first_paragraph": "On 2023-01-04 by ThatGeoGuySo over the holidays I have had something of a little bug in my brain:\ntransducers. It started when I was thinking of working on\nan unrelated side-project, but I more or less got frustrated and asked myself \u201cwhy doesn\u2019t Scheme\nhave a library that is as good as Rust\u2019s Iterator trait?\u201d I am a strong proponent of Rust both at\nwork and outside of work; however, I don\u2019t always want to use Rust. Sometimes I\u2019m scripting\nsomething fairly quick-and-dirty, but largely I\u2019m part of the Lisp Cult1 and I have fun\nwriting Scheme. This isn\u2019t about picking languages or favourites, it\u2019s about finding out why, in\nthe case of my question above.Needless to say, this eventually led me to writing my own egg (module) for transducers in Scheme. If\nall has gone well, it should already be available to download and install by running:I only note this because I think this blog post will serve as a good companion piece of\ndocumentation for the egg. I think the rationale is especially wort"
  },
  {
    "title": "MIT leaders describe the experience of not renewing Elsevier contract (sparcopen.org)",
    "points": 320,
    "submitter": "nabla9",
    "submit_time": "2024-08-20T19:27:52",
    "num_comments": 114,
    "comments_url": "https://news.ycombinator.com/item?id=41303159",
    "comments": [
      "Some random bits of context.This conflict goes back a long time. In the early 1990's, with online journals just getting started, MIT was able to insist on principles like \"anyone physically in the library has full access, even if they are not otherwise affiliated with MIT\". A couple of years later, power shifted, and Elsevier could \"our terms, take it or leave it\". Then three decades, a human generation, of Elsevier rent-seeking, and so many people working towards Open Access, unbundling, google scholar, arXiv, sci-hub, and so on. Societal change can be so very slow, nonmonotonic, and profoundly discouraging. And yet here we are, making progress.For anyone unfamiliar with \"author [...] required to relinquish copyright [...] generous reuse rights\", journals would require authors to completely sign over copyright, so authors' subsequent other-than-fair-use usage of fragments would be a copyright violation. Rarely enforced, but legally you'd have to obtain permission. While some institutions sign contracts easily, and struggle with the fallout later, MIT legal culture has been pain-up-front careful with contracts. Which is sometimes painful. But IIRC, we're today using X Windows instead of CMU's Andrew, because MIT could say \"sure!\" while CMU was \"sure, err,... we'll get back to you... some mess to clean up first\".\n \nreply",
      "> we're today using X Windows instead of CMU's AndrewAnd in that world, TypeScript would have to find another name! Now, good luck googling Andrew Typescript :)Edit: Found it... https://www.cs.cmu.edu/~AUIS/ljdocs/mkmost/fig4.gif\n \nreply",
      "> \"our terms, take it or leave it\".So many monopolies do this and they should all be disrupted. This is our world, and anyone telling us they get to gatekeep information exchange is not an ally. Google with Chrome, Apple with iPhone.> Societal change can be so very slow, nonmonotonic, and profoundly discouraging. And yet here we are, making progress.Encouraging, but we lost Aaron.\n \nreply",
      "Aaron Swartz would have liked some of those principles.  This is a small, \"better late than never\" step for most of us, but for Aaron...it's more like \"too little, too late.\"\n \nreply",
      "Surely MIT made this move at least partially due to Aaron's legacy. I hope and believe he'd be happy to see this.\n \nreply",
      "Something I thought of too, second to only a few months ago, where I remember an article coming to the top of HN stating that JSTOR was now free in a ton of prisons. Stopped me dead in my tracks, with potent thought of Aaron - may he rest.\n \nreply",
      "My first thought as well\n \nreply",
      "I wonder how far California could go on its own in destroying the journal cartels. They really are a parasitic racket, and I don't think the average voter understand how bad it really is.I feel like one basic ballot measure to the effect of \"All state funded research shall be released to the public domain, and all prior contract terms to the contrary are hereby void\" might be enough to knock over the first domino, hopefully culminating in it being made a federal regulation.\n \nreply",
      ">> I don't think the average voter understand how bad it really is.Unless you're in academia, you'd have no idea. I spent a few years in academia while pursuing my Masters. Dropped out for various reasons and never had any idea this was such a huge thing - mainly because as a university student, you just always had access.I got a small glimpse when I got a job a few years later working for a rather large legal publisher and how locked down they kept all of their online materials for anybody outside of academics. Its when I really understood there was a massive war raging about access to this stuff and the publishers trying to eek out every penny from granting access to materials and research that should have been just in the public domain. I didn't stay there long and had almost forgotten about all that stuff until 2011 when it broke into the news cycle again.You're 100% right, this should really be a much larger issue and covered more regularly, it has massive implications on research and copyrights. Remember how long the issues of Napster and piracy have been in the media for so long (there was just another HN topic this week), but this? Not much is ever really talked about it - it just seems to linger in the shadows, which is depressing.\n \nreply",
      "I think it's because not a lot of professions access these research journals for their day to day outside of medicine, engineering, and academia.It's just not an issue for most people's everyday. So it's just not an issue.\n \nreply"
    ],
    "link": "https://sparcopen.org/our-work/big-deal-knowledge-base/unbundling-profiles/mit-libraries/",
    "first_paragraph": ""
  },
  {
    "title": "On finishing projects (alexreichert.com)",
    "points": 84,
    "submitter": "reichertjalex",
    "submit_time": "2024-08-19T18:03:21",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=41293219",
    "comments": [
      "Over the years I\u2019ve noticed that I\u2019m often operating in various \u201cmodes\u201d. Some of this is related to depressive episodes, but often it\u2019s just a more neutral ebb and flow. I\u2019ll have a week where ideas are flowing and connections form easily. I\u2019ll have a week where it feels like I can\u2019t engage so deeply - like my system is cooling off after exertion.And this is one of the reasons writing things down has been such a useful tool, and starting with a spec is something I now do religiously after spending the first half of my life just diving in haphazardly (and often not getting very far).Those ultra productive days are often enough to get a pretty decent spec written; something I can flesh out in far greater detail than I could with a productive coding day. Those down days are now bolstered by the thinking of a version of myself that was firing on all cylinders, and solved many of the problems I probably don\u2019t have the brain power to tackle on the fly when I\u2019m tired and stressed for life reasons.I feel like I\u2019ve squandered really good trains of thought by telling myself \u201cI\u2019ll remember this and write it down later\u201d. So I always start with writing. Future me always appreciates it.\n \nreply",
      "Your comment resonates with me.I totally agree about writing things down being useful. I also like the author\u2019s timebox plan as well. I\u2019m great at writing specs to define what I want to build, but my hobby project timelines are nonexistent. I think I\u2019ll give this a try!\n \nreply",
      "I resonate strongly with this. I found something pretty simple thats been helping me lately though. I uhh, i just force myself to keep working on it until its done. Grind it out. Work on it especially when i dont want to. Ditch those little relaxing moments for it. Wake up early to find time for it. Its torture.And also therapeutic. I see the incompleteness of my original idea, or plan. I see that the things i declare important feel less so when i have to keep working on it. I start to permanently discard essays, apps, ideas, as i appreciate what it would really take to do them. I say no to myself far more often. I become more disciplined in my work. And more disciplined in my play. I start making more realistic plans and exit points because im not a masochist.Ultimately i let it change me. I obviously wont grind like this forever. But the grind teaches me, and changes me, into the kind of person that finishes things. Which is mostly about learning to be more selective and less impulsive, and knowing when to play and when to put things down and truly let them go.\n \nreply",
      "+1 to being selective and ruthlessly deleting time-draining ideas.I'm not sure I can force myself to keep working, though. Self-imposed deadlines don't help me either. It seems like a willpower problem, like going to the gym.What's helped me is making projects alongside a small group of fellow programmers. Seems like the author has done something similar with a hackers co-op chat and with Recurse. External accountability is my most successful motivator.\n \nreply",
      "> I start making more realistic plans and exit points because im not a masochist.Or, you start to enjoy the pain, because you enjoy the result. But then your mind does its illogical associative thing and you start to just enjoy the pain on its own. Now you're a masochist!I jest, but only some. Ask me how I know ...\n \nreply",
      "Yeah, definitely. I know this is a somewhat controversial opinion, but I do think there's value in developing the muscle around \"grinding it out\". I think the tricky part (depending on your personality type) is knowing when it makes sense to give up and move on to something else.\n \nreply",
      "I genuinely consider two of my greatest accomplishments to be building a game and composing a song.Neither are very impressive, but in both cases, I ended up having to simply decide I was done and stop working on them. I could continue making changes indefinitely, adding little improvements and such every day, but they were also complete as they were. Once I learned to be okay with that and move on, I learned what finishing a project really meant (or at least one form of it).\n \nreply",
      "\u2018Art is never finished, only abandoned\u2019 - Leonardo Da Vinci\n \nreply",
      "Can you post game link ?\n \nreply",
      "Reposting an old Reddit comment of mine that I think agrees with much of this article:I'm in my 40s and for most of my life I had an unending series of unfinished projects that I felt guilty about. Today, I am able to finish some stuff, including some pretty large, hard projects. Better, I don't feel guilty about the stuff I don't finish.The trick for me is to be deliberate and mindful about why I'm embarking on a particular project.If it's because I want to feel good by:* Sharing something with others.* Accomplishing a difficult, challenging task.* Proving to myself that I can do something.* Getting the social cachet of being a creative, productive person. (Maybe this is shallow, but who doesn't like to feel impressive in the eyes of their peers?)Then the goal of the project is the product it and I focus my attention and discipline on it. I try to have as few of these as possible\u2014like only one at a time\u2014so that my willpower is not diluted.If it's because I want to feel good by:* Improving a skill.* Exploring an unfamiliar domain or learning something new.* Relaxing by tinkering on something I like.Then the goal of the project is the process and I don't feel bad about not finishing. The real treasure is all the stuff I learned and did along the way and there is no real destination. I can have as many of these as I want because there's no real failure mode here. They're all recreation.Once I got more honest and clear with myself about my goals for each project, I started to be able to finish the ones where that mattered and stopped feeling bad about the ones where it doesn't.\n \nreply"
    ],
    "link": "https://www.alexreichert.com/blog/finishing",
    "first_paragraph": "Finishing projects is surprisingly difficult for me.I've started and restarted this essay several times over the course of the last three months, trying to figure out what I want to say, trying to make it a little less crappy. The only reason I've persisted is because the unbearable irony of not finishing would probably kill me.Similarly, I've been hacking on Booper on and off for over a year now. I started it because I wanted to dip my toes into mobile development and learn more about push notifications, but I keep uncovering new issues that prevent me from launching.Part of the difficulty stems from the fact that it's not always obvious what it means to be \"finished\". When is an essay, or a piece of software, truly finished? An essay can always be edited more; an app can always be further refactored, redesigned, enhanced.Maybe it's safe to say a project is \"finished\" when it's good or useful enough that I'm comfortable sharing it with other people. This makes it simple to test if som"
  },
  {
    "title": "Emerge Tools (YC W21) is hiring a senior front end engineer (emergetools.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-08-20T21:00:26",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.emergetools.com/careers/jobs/senior-frontend-engineer",
    "first_paragraph": "Never miss a post or product update"
  },
  {
    "title": "The U.S. Navy's $100M checkbox (2019) (adrian3.com)",
    "points": 119,
    "submitter": "davidbarker",
    "submit_time": "2024-08-20T07:09:36",
    "num_comments": 93,
    "comments_url": "https://news.ycombinator.com/item?id=41297563",
    "comments": [
      ">We create rules enforcing mandatory sleep requirements stupidly believing that we can eliminate the potential for a user of the system to be drowsy while at the controls.>stupidly believingDick move by author to reveal his level of ignorance of USN operation tempo around McCain collision until the last few paragraphs. There were lol 4 fucking surface ship collisions and a grounding in westpac in 2017 because sailors were ran ragged, leading to operation pause. UX wasn't the primary problem. Sailors weren't \"drowsy\", they were sleep deprived, hopped up on stimulants etc due to manning shortages and long deployments, and likely lax training (due to shortages), which caused USS Connecticut accident a few years later. I'm sure you can improve UI for audience subsisting on 3-5 hours of sleep, but maybe the more pressing thing to try is to get them more sleep. IIRC there was study on navy sleep hyigene and like 100% of sailors in bottom quartile experienced bewilderment/confusion.https://news.usni.org/2017/09/18/admiral-captain-removed-par...\n \nreply",
      "Having spent 10 years as a Navy Nuclear Propulsion Operator on two different submarines operating the reactor plant, I can tell you I am not surprised by incidents like these. In order of importance:\n1) Lack of sleep (it wasn't unusual to operate on no sleep over a 36-48 hour period)\n2) Poor or insufficient training. Just because you are \"trained or qualified\" doesn't mean you know how to operate.\n3) Poor or missing procedures (let's call it UI/UX for today's lingo). Many procedures were vague, and drawings were hard to understand. The Navy has a feedback system for this, but it often takes months/years to resolve.Having said all that, the issues pointed out in the comments and the article, including my comments, have existed for decades in the Navy. At some point, it comes down to the command's leadership and superiors to ensure these issues don't happen. A poorly designed checkbox is the last thing that caused this issue.\n \nreply",
      "It's good to see all the Navy nukes comments on this article.  MM1/SS, one tour on an SSN, and one on an SSBN!\n \nreply",
      "E: since many are quoting authors preface about not knowing much, but doing their own researchMy beef is, given disclaimer, I read piece to end thinking author made good faith effort at research, only to see author characterize, near conclusion, sailor/operator severe lack of sleep hygiene as \"drowsiness\" which can be designed around. That expecting enforcement of better operational conditions is \"stupid\", which may feel true in military context. But 7th fleet went from 4 accidents in one year to none after brief operational pause for a month, I dont think the result is because USN bureaucracy figured out a way to improve UXUI on Arleigh Burkes software. Also note the other 6 fleets with... more relaxed tasking relative to west pac weren't suffering from same level of dysfunction. UXUI is important yes, but sometimes operations are ran so badly that you should prioritize improving the way it's run instead of pretending it can be bandaid over with a better checkbox.\n \nreply",
      "The Navy has the stupidest possible ideas of sleep hygiene, boiling down to \"it sucked for me so it should suck for the next guy, too\". I had friends in departments who worked 6 hours on, 6 hours off, 6 hours on, 6 hours off repeatedly. In that pattern you never get a solid 8 hours of uninterrupted sleep, ever. Yeah, it's possible that emergencies will arise where you have to work 24-48-more hours straight without relief because the ship is under attack, or on fire, or barely afloat. That's not a reason to try to kill sailors with sleep deprivation the rest of the time.\n \nreply",
      "> Dick move by author to reveal his level of ignorance of USN operation tempo around McCain collision until the last few paragraphs. There were lol 4 fucking surface ship collisions and a grounding in westpac in 2017 because sailors were ran raggedI read it as \"mandatory sleep requirements don't actually mean people don't show up to a shift without enough sleep\".Basically acknowledging the difference between how the world is on paper and how the world is in reality. Even if there's rules about people getting enough sleep, designing a system that assumes everyone who works it will get enough will get people killed.\n \nreply",
      "I didn't read it necessarily like that. It can also mean that even with fully rested sailors, the same confusion can still happen again, because the interface is inherently confusing.In a sudden life-and-death situation combined with information overload, a bad interface can be what tips the scale into disaster.\n \nreply",
      "This is also very important for lorry drivers, to the extent that there's all sorts of tracking and enforcement for how long they're driving. But in this case it sounds like poor staff management: this isn't a convenience store running on zero-hour contracts, they should have a shift plan in place that provides adequate cover before even leaving port.\n \nreply",
      "Bingo. Problem is USnavy has large + increasing at sea staff/billet shortages, but at the same time has to (or insist on) on doing more missions with less sailors. You can build a better checkbox, but can you build a good enough checkbox to allow a lorry driver to drive 20 hours a day?\n \nreply",
      "The shift (watch) plan is set by the ship's officers but they have to work with the personnel that they're assigned. There is a constant shortage, especially for experienced sailors on surface vessels. But they have to put to sea anyway to accomplish the mission, which causes personnel burnout due to overwork thus worsening the shortage in a vicious cycle.The only real solutions would be for Congress to either significantly increase personnel funding, or trim back the mission set to make it sustainable at current personnel levels. There is no political support for either solution and so the problem will persist.\n \nreply"
    ],
    "link": "https://adrian3.com/blog/2019/2019-09-28-The-US-Navys-100-million-dollar-checkbox.php",
    "first_paragraph": "Two years ago a Navy destroyer was ripped open by the nose of a Liberian tanker. Ten sailors were crushed or drowned as their sleeping quarters filled with water after the collision. At the heart of the tragedy is a single checkbox on a touchscreen. This is the untold story of how bad design caused a crew to lose control of the $1.8 billion John S McCain destroyer and the mystery around how designers manage to avoid blame when their creations cause death and destruction. Before going any further, I want to make it clear that I am just a civilian piecing together this story from whatever information I can glean from the internet. I have trudged through hundreds of pages of technical documents and dense reports. Much of this documentation is intentionally vague, some of it has been redacted, and all of it has undoubtedly been scrubbed by lawyers and PR professionals. As a result, there are probably errors in my analysis and conjecture. However, with that disclaimer in mind, I believe the"
  },
  {
    "title": "Good refactoring vs. bad refactoring (builder.io)",
    "points": 168,
    "submitter": "steve8708",
    "submit_time": "2024-08-19T23:01:51",
    "num_comments": 104,
    "comments_url": "https://news.ycombinator.com/item?id=41295296",
    "comments": [
      "Reading this I realised I've kind of drifted away from the idea of refactoring for the point of it.The example with the for-loop vs. map/filter in particular - it's such a micro-function that whichever the original author chose is probably fine. (And I would be suspicious of a developer who claimed that one is 'objectively' better than the other in a codebase that doesn't have an established style one way or the other).Refactor when you need to when adding new features if you can reuse other work, and when doing so try to make minimal changes! Otherwise it kind of seems more like a matter of your taste at the time.There's a limit of course, but it's usually when it's extremely obvious - e.g. looong functions and functions with too many parameters are obvious candidates. Even then I'd say only touch it when you're adding new features - it should have been caught in code review in the first place, and proactively refactoring seems like a potential waste of time if the code isn't touched again.The (over) consolidation of duplicated code example was probably the most appealing refactor for me.\n \nreply",
      "Agreed with everything except the following:>Remember, consistency in your codebase is key. If you need to introduce a new pattern, consider refactoring the entire codebase to use this new pattern, rather than creating one-off inconsistencies.It's often times not practical (or even allowed by management due to \"time constraints\") to refactor a pattern out of an entire codebase if it's large enough. New patterns can be applied to new features with large scopes. This can work especially in the cases of old code that's almost never changed.\n \nreply",
      "To me it\u2019s less about \u201cconsistency\u201d as some nebulous, subjective thing. If you want to set the new standard for $thing, whole-ass it and set the new standard for $thing. I fully support this, but within reason of course. The point at which I duck out of this is when numerous replacement operations require significant non-trivial changes to highly depended on and/or untested code. Otherwise if it\u2019s a simple task that a good IDE and a couple hours of hard work can solve\u2026 just do it!\n \nreply",
      "The key word is consider. If you wouldn't apply the pattern to the whole codebase, maybe you don't actually want to introduce it in just this one new place.\n \nreply",
      "It's not that it wouldn't be applied to the whole codebase, it's that it wouldn't be applied to the whole codebase __at once__. You have to start somewhere and new features are a good place to start new patterns. Older code can be refactored piece by piece.\n \nreply",
      "I've had success with strategies at introducing some abstractions/patterns at my current place(doing this alone for a enterprise SaaS company with 200-ish devs). It's weird that we don't teach these or talk about them in software engineering(AFAIK). I see them being re-invented all the time.To borrow from medicine: First step is to always stop the `stop the hemorrhage`, then clean the wound, and then protect the wound(or wounds).- Add a deprecation marker. In python this can be a decorator, context-manager, or even a magic comment string. This i ideally try to do while first introducing the pattern. It makes searching easier next time.- Create a linter, with an escape hatch. If you can static analyse, type hint your way; great! In python i will create AST, semgrep or custom ones to catch these but provide a magic string similar to `type: noqa` to ignore existing code. Then there's a way to track and solve offending place. You can make a metric out of it.- Everything in the system as to have a owner(person, squad, team or dept). Endpoints have owners, async tasks have owners, kafka consumers might have owners, test cases might have owners. So if anything fails you can somehow make these visible into their corresponding SLO dashboards.  The other alternative to this last step is \"if possible\" some platform squad can take over and do this as zero-cost refactor for the other product squad. Ofcourse the product squads have to help test/approve etc. It's an easier way to get people to adopt a pattern if you do it for them. But the ROI on the pattern has to be there, and the platform squad does get stuck doing cruft thankless work sometimes. If you do this judiciously the win might be thanks enough, like more robust systems, better observability/traces, less flaky tests etc. etc.\n \nreply",
      "In the majority of places I've worked, nobody who started refactoring older code piece by piece ever finished it. The exception is people who documented the scope of the work, got leadership buy-in, and then worked on it continuously like any other project.The problem is that sometimes the new pattern gets overridden by an even newer pattern, and so on, until you've got three different implementations from 2016, 2019, 2021, and then you find that in 2024 you're working on implementation number four and all the people who did the first three have left the company without writing any documentation or finishing their work.\n \nreply",
      "Refactoring isn't an end on its own, and it shouldn't ever be considered a standalone project.The easiest way to accomplish a real goal like fix a bug, or add a feature, may very well be to first refactor the code.  And yes maybe you want to merge that in as its own commit because of the risk of conflicts over time.  But just having the code look nice (to who exactly?) isn't valuable, and it encourages the addition of useless abstractions.  It may even make later real-work harder.  Never refactor outside the context of real-work.The cartoon with the PM also gets at a ridiculous pattern: engineers negotiating when to do various parts of their job with non-technical people who have no idea how do any part of their job.  The PM doesn't know what a refactor is, the EM probably doesn't either.  It doesn't make the organization function any better to tell these people about something they don't understand, and then ask them when it should be done.  Budget it as part of the estimate for real-work.\n \nreply",
      "Good refactoring should significantly reduce the size or complexity of a codebase.These two metrics are interrelated, but as a general rule if the gzipped size of the codebase (ignoring comments) does not go down, it's probably not a good refactoring.\n \nreply",
      "I'm going to disagree and see what other people say.I don't think that reduction of size is of any relevance. I admit my own refractors tend to make things smaller but it's only a tendency. Most definitely some increase the size overall. I'm currently refactoring a code base \u2013 for each item there used to be one class. Each object was examined after creation then a runtime flag was set: Rejected or Accepted. As the code crew I found I was wasting a lot of time around this Accepted/Rejected stuff. Now I'm refactoring so I have two classes for each item, one for when it's Accepted and one for when it's Rejected. The amount of boilerplate has definitely bulked up the code but it will be worth it.As for complexity, I don't know.The only thing I refactor for is human comprehensibility. That is the final goal. What other goal can there be?\n \nreply"
    ],
    "link": "https://www.builder.io/blog/good-vs-bad-refactoring",
    "first_paragraph": "Get the guide: Ship 10x faster with visual development + AIAnnouncing Visual Copilot - Figma to production in half the timeBuilderMitosisQwikPartytownFigma to HTMLBlogHomeResourcesBlogForumGithubLoginSignup\u00d7Visual CMSDrag-and-drop visual editor and headless CMS for any tech stackTheme Studio for ShopifyBuild and optimize your Shopify-hosted storefront, no coding requiredResourcesBlog\u2630I've hired a lot of developers over the years. More than a few of them have come in with a strong belief that our code needed heavy refactoring. But here's the thing: in almost every case, their newly refactored code was found by the other developers to be harder to understand and maintain. It also was generally slower and buggier too.Now, don't get me wrong. Refactoring isn't inherently bad. It's a crucial part of keeping a codebase healthy. The problem is that bad refactoring is, well, bad. And it's surprisingly easy to fall into the trap of making things worse while trying to make them better.So, let's "
  },
  {
    "title": "A road safety plan that will lead to cars communicating with each other (engadget.com)",
    "points": 44,
    "submitter": "Brajeshwar",
    "submit_time": "2024-08-18T15:29:33",
    "num_comments": 134,
    "comments_url": "https://news.ycombinator.com/item?id=41282948",
    "comments": [
      "https://www.its.dot.gov/research_areas/emerging_tech/pdf/Acc...",
      "I see nothing about the humans outside of the metal clown cars. Meanwhile e-bikes are growing, and pedestrians and other road user die needlessly to oversized vehicles driven by anyone with a pulse due to lax licensing rules.Fix licensing, make vehicles safe for those walking/biking not just those in other clown cars.\n \nreply",
      "The article doesn't make it clear to me when the DOT talks about \"V2X being deployed\", what the full scope of that is \u2013 does it refer to just the physical technologies, or the lowest layers of the OSI model? Or does \"V2X deployment\" here mean more application-level stuff, i.e. a series of minimum requirements about what information classes of devices will broadcast to other classes of devices, with what limitations?Without that clarification, I think the first thing readers of HN will think, justifiably, is \"is all of my car's information being broadcast all the time to everything\", for plenty of reasons \u2013 dragnet surveillance, disruptive attacks ranging from Flipper pranks to state actors, etc.? It's not clear whether that's true or expected of this V2X initiative.After some quick digging, it looks like so far, it looks like only very domain-specific features have been \"implemented with V2X\", and will be for the forseeable future (see p7+ in [1]) \u2013 oversize vehicle complaince, pedestrian in crosswalk, blind spot warnings. How that's implemented will probably need a lot more digging.[1] https://rosap.ntl.bts.gov/view/dot/68128\n \nreply",
      "I wish that the safety of the people outside of motor vehicles received at least as many resources (funding, research, legal) as the safety of motorists -- who are cause of these risks in the first place.Motorists already have strong incentives to make their vehicles safer for themselves, but they have very little incentive to make things safer for people outside of their vehicle. For that reason we need better regulations and infrastructure that account for those externalities.\n \nreply",
      "The solution is one that is unpopular to a good number of consumers, it's to make cars smaller, lighter, and slower.While I'm sure it's happened, death via golfcart is a pretty rare occurrence.  Death via a Dodge ram, on the other hand, happens all the time. [1]Giant trucks are super popular and super deadly.  I was nearly killed by one myself (driver ran a red light while I was in the cross walk).  While I wouldn't outright ban them, I definitely would be up to something like requiring a CDL before you can buy one.[1] https://www.autoblog.com/article/most-deadly-cars-other-driv...\n \nreply",
      "I completely agree on all accounts. Heavy vehicles such as pickup trucks should require a CDL... and drivers should lose it when they are found driving recklessly.I have no interest in unproven high-tech approaches when we haven't even implemented very basic proven pedestrian safety measures like eliminating street-level parking around pedestrian crossings to increase visibility, or mandating pedestrian safety tests for motor vehicles.\n \nreply",
      "> Heavy vehicles such as pickup trucksWhat about electric vehicles?\n \nreply",
      "> Heavy vehicles such as pickup trucks should require a CDL...I'm guessing you don't live in the south. Pickup trucks are a major way of life for a lot of people. They certainly wouldn't be happy about increased regulatory burden.Attitudes on roads vary wildly based on the community in question. There's a large surface area of this country that doesn't care to have non-vehicular traffic sharing the roadways.Urban communities will prioritize different needs than suburban and rural communities. The two ends of the spectrum aren't really compatible because these are wholly different lifestyles that are geographically separable.\n \nreply",
      "Okay, then they should require CDL when entering a town or any other urban location.\n \nreply",
      "We don\u2019t let people carry around lethal weapons just because some people think it\u2019s a \u201cway of life\u201d, so why should it be diff\u2026\u2026 oh wait\n \nreply"
    ],
    "link": "https://www.engadget.com/transportation/the-us-lays-out-a-road-safety-plan-that-will-see-cars-talk-to-each-other-170043265.html",
    "first_paragraph": "The US Department of Transportation has laid out a nationwide road safety plan [PDF] that will lead to cars communicating with each other. The agency is hoping that broadly deploying vehicle-to-everything (V2X) tech will boost its \"commitment to pursue a comprehensive approach to reduce the number of roadway fatalities to zero.\" The National Highway Traffic Safety Administration estimates that 40,990 people died in motor vehicle crashes last year.V2X enables vehicles to stay in touch with each other as well as pedestrians, cyclists, other road users and roadside infrastructure. It lets them share information such as their position and speed, as well as road conditions. They'd be able to do so in situations with poor visibility, such as around corners and in dense fog, NPR notes.A US-wide rollout will require an array of mobile, in-vehicle and roadside tech that can communicate efficiently and securely while protecting people's personal information, the DoT said in its National V2X Depl"
  },
  {
    "title": "Artificial intelligence is losing hype (economist.com)",
    "points": 192,
    "submitter": "bx376",
    "submit_time": "2024-08-20T01:13:23",
    "num_comments": 304,
    "comments_url": "https://news.ycombinator.com/item?id=41295923",
    "comments": [
      "https://archive.ph/PFmWw",
      "I tried to do some AI database clean up this weekend - simple stuff like zip lookup and standardizing spacing, and caps - and ChatGPT managed to screw it ip over and over. It\u2019s the sort of thing there a little error means the answer is totally wrong so I spent an hour refining the query and then addressing edge cases etc. I could have just done it all in excel in less with less chance of random (hard to catch) errors.\n \nreply",
      "If the SQL took you an hour to just clean up and you're an expert that is some pretty complex SQL.  I could understand how it could get it wrong.\n \nreply",
      "Similar experience.In fields I have less experience with it seems feasible. In fields I am an expert in, I know it's dangerous. That makes me worry about the applicability of the former and people's critical evaluation ability of the whole idea.I err on the side of \"run away\".\n \nreply",
      "AI (specifically Claude Sonnet via Cursor) has completely transformed my workflow. It's changed my job description as a programmer. (And I've been doing this for 13y \u2013 no greenhorn!)This wasn't the case with GPT-4/o. This capability is very new.When I spoke to a colleague at Microsoft about these changes, they were floored. Microsoft has made themselves synonymous with AI, yet their company is barely even leveraging it. The big cos have put in the biggest investments, but also will be the slowest to change their processes and workflows to realize the shift.Feels like one of those \"future is here, not evenly distributed yet\" moments. When a tool like Sonnet is released, it's not like big tech cos are going to transform over night. There's a massive capability overhang that will take some time to work itself through these (now) slow-moving companies.I assume it was the same with the internet/dot-com crash.\n \nreply",
      "Does your work not depend on existing code bases, product architectures and nontrivial domain contexts the LLM knows nothing about?Every thread like this over the past year or so has had comments similar to yours, and it always remains quite vague, or when examples are given, it\u2019s about self-contained tasks that require little contextual knowledge and are confined to widely publicly-documented technologies.What exactly floored your colleague at Microsoft?\n \nreply",
      "Context is the most challenging bit. FWIW, the codebases I'm working on are still small enough to where I rarely need to include more than 12 files into context. And I find as I make the context bigger beyond that, results degrade significantly.So I don't know how this would go in a much larger codebase.What floored him was simply how much of my programming I was doing with an LLM / how little I write line-by-line (vs edit line-by-line).If you're really curious, I recorded some work for a friend. The first video has terrible audio, unfortunately. This second one I think gives a very realistic demonstration \u2013 you'll see the model struggle a bit at the beginning:https://www.loom.com/share/20d967be827141578c64074735eb84a8\n \nreply",
      "I think that Greptile is on the right track.  I made a repo containing the c# source code for the godot game engine, and it's \"how to do X\", where X is some obscure technical feature (like how to create a collision query using the godot internal physics api) is much better than all the other ai solutions which use general training data.However there are some very frustrating limitations to greptle, so severe that I basically only use it to ask implementation questions on existing codebases, not for anything like general R&D:  \n1) answers are limited to about 150 lines.\n2) it doesn't re-analyze a repo after you link it in a conversation (you need to start a new conversation, and re-link the repo, then wait 20+ min for it to parse your code)\n3) it is very slow (maybe 30 seconds to answer a question)\n4) there's no prompt engineeringI think it's a bit strange that no other ai solution lets you ask questions about existing codebases.   I hope that will be more widespread soon.\n \nreply",
      "I work at Greptile and agree on all three criticisms. 1) is a bug we haven't been able to fix, 2) has to do with the high cost of re-indexing, we will likely start auto-updating the index when LLM costs come down a little, and 3) has to do with LLM speed. We pushed some changes to cut time-to-first-token by about half but long way to go.Re: prompt engineering, we have a prompt guide if that helps, was that what you are getting at?https://docs.greptile.com/prompt-guide\n \nreply",
      "Solid questions and comments, layer8.I notice that the person you replied to has not replied to you yet.It may be that they will, of course.But your points are good.\n \nreply"
    ],
    "link": "https://www.economist.com/finance-and-economics/2024/08/19/artificial-intelligence-is-losing-hype",
    "first_paragraph": ""
  },
  {
    "title": "Transformers for Ruby (github.com/ankane)",
    "points": 275,
    "submitter": "felipemesquita",
    "submit_time": "2024-08-20T11:54:07",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=41299148",
    "comments": [
      "I'm sure it's not why he does it, but I just nominated ankane for the Rails 2024 Luminary award, if anyone else wanted to express their gratitude.. https://rubyonrails.org/2024/8/2/nominations-open-for-2024-l...\n \nreply",
      "Seconded. The sheer volume of useful work Ankane puts out for the community is unreal.\n \nreply",
      "Ankane's Onnx runtime for ruby is so easy to use that makes you wonder why the official repo for js is so difficult to understand. This guy's a hero, although I'm only scratching the surface for what he has done.\n \nreply",
      "Seriously, is this guy human? I'd invest a billion dollars in any individual so talented...\n \nreply",
      "I'm 100th as talented and would consider accepting a $10M investment.\n \nreply",
      "Ankane single handedly is contributing so much back to the industry, in so many ways. Wild!\n \nreply",
      "Thanks for creating this - it looks interesting. Contributions like this are really needed in the Ruby community\n \nreply",
      "They really are.The lack of such contributions -in general- or the speed at which they appear, is what leads me to conclude that the Ruby community is slowing down.In this case, suddenly there's an awesome library for Ruby. Which is fantastic. An achievement to be very thankful for.\nBut \"the community\" \"produced\" this months or years after such libs landed for Python, JS, TS, Rust, Go and so on.Not just ML/AI, same happens for \"gems\" (Ruby libs) that deal with any new tech. It used to be that any SAAS or startup would offer official Rubygems from the get-go. Often before offering other platforms or languages. Today, when I want or need to integrate something, from notion to slack to cloudflare: no Ruby option or at least no official one.This saddens me. Ruby is so much more than Rails (for which I can understand the reluctance or \"hate\"). Ruby is so much nicer to work in than Python and certainly than JavaScript. Ruby could easily have been what Python is today and tens of thousands of developers would be just a little happier than they are now, I am certain.\n \nreply",
      "I find the ruby gem ecosystem significantly richer than any another. Maybe not by sheer quality, which quickly translates to millions of abandoned libraries (I\u2019m looking at you nom), but certainly quality.It\u2019s also recently on the rise in general.\n \nreply",
      "This is such an odd comment to me.On the one hand, you praise Ruby, and lament that it gains such libraries so much later than other languages.On the other hand...if you were paying attention to the \"Python, JS, TS, Rust, Go and so on\" ecosystems, and noticed the ML/AI work, why didn't you create one for Ruby yourself?I guarantee that whatever answer you give doesn't matter, because every other Rubyist has their own reply. A \"community\" begins with one person doing the thing.\n \nreply"
    ],
    "link": "https://github.com/ankane/transformers-ruby",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        State-of-the-art transformers for Ruby\n      \ud83d\ude42 State-of-the-art transformers for RubyFirst, install Torch.rb.Then add this line to your application\u2019s Gemfile:DocsDocsDocsDocsNamed-entity recognitionSentiment analysisQuestion answeringFeature extractionImage classificationImage feature extractionThis library follows the Transformers Python API. Only a few model architectures are currently supported:View the changelogEveryone is encouraged to help improve this project. Here are a few ways you can help:To get started with development:\n        State-of-the-art transformers for Ruby\n      "
  },
  {
    "title": "Pragtical: Practical and pragmatic code editor (pragtical.dev)",
    "points": 228,
    "submitter": "rd07",
    "submit_time": "2024-08-20T07:15:34",
    "num_comments": 94,
    "comments_url": "https://news.ycombinator.com/item?id=41297609",
    "comments": [
      "Not criticism, just thinking out loud:This editor claims to be lightweight, citing that it uses 30MB of RAM. But I assume that's without any extensions loaded.Back in the day, though, one joke about Emacs was that it's an acronym for Eight Megabytes All Continuously Swapping. This was meant to highlight Emacs's reputation for bloat. Right now when I run Emacs it's using a lot more than 30, let alone eight. I'm pretty sure most of that is all the modes I have installed for every language I might ever use, regardless of whether I'm actually using it right now.About 15 years back Visual Studio had a reputation for bloat, but my experience was that it was actually quite lightweight and snappy, especially compared to Eclipse and IntelliJ. Until you install ReSharper, which transformed it into 50 tons of molasses.At work, Visual Studio Code currently consumes about 1GB of RAM and takes 5+ minutes to start up. On my personal computer, a 2013 MacBook, it uses more like 50MB and starts darn near instantaneously. But they're very different beasts; on my MacBook I've got it configured to only load the plugins I need for each project. At work we've got a whole dang Devcontainer that includes configuration to load I-don't-know-how-many extensions, basically anything anyone on the team has ever wanted. The devcontainer extension makes you put the list of extensions to load into a file that needs to be checked into source control. So the only way for someone to get this tool they want is to make everyone else get it, too. All to sling a relatively modest volume of Python code.And of course if I try to opt out of all of that I make my life even harder. Trying to get by without that pile of crap is just spitting in the wind. Run-time requirements aren't documented; they're shoved into an undocumented and ever-growing list of Bash commands in the Dockerfile. Coding standards aren't documented or managed with something straightforward like Git hooks; they're enforced through a plugin and its configuration.I do remember when vscode was lightweight. It happened to be a time when not many plugins were available. That put a hard limit on just how much bloat you could accomplish. But, of course, as soon as it got popular people started creating plugins for darn near everything.Perhaps the problem isn't the editors. Perhaps it's us.\n \nreply",
      "It's a cycle, and we've been around the bend multiple times already.https://www.xkcd.com/2044/Installing multiple programs on my computer or a server is complicated, and slows things down, and it's insecure and hard to replicate. So we created VMs. And for a while VMs were great. But then we started putting everything we needed in the VMs, and they also became complicated, and slow, and insecure and whatnot. So we have containers. And containers are now slowly getting bloated too. Kubernetes simplified some things, but now we need Helm to deal with K8s, and Helm itself is now quite complicated.Editors start lightweight and fast, then get bloated with features. So does productivity software. Programming languages start simple and easy to use and understand, and progressively get more features, each of which seems nice in isolation, but soon the codebases use everything, and it interacts, and you need decades of experience to use it all proficiently.Same for libraries. For network protocols. For standards of all kinds.It's most definitely us.\n \nreply",
      "> On my personal computer, a 2013 MacBook, it uses more like 50 and starts darn near instantaneously.Just for clarification, do you mean 50 GiB or 50 MiB? I'm assuming MiB in this scenario, since allocating 50 GiB doesn't mix with an instantaneous startup.\n \nreply",
      "Yeah, mib\n \nreply",
      "Yup, it is your works crazy policy of making sure everyone gets a one size fit all VS code Configuration. I\u2019ve loaded VS code on a 10 year-old Lenovo and it runs just fine with only basic Python plug-ins. Five minute startup time is crazy.\n \nreply",
      "This is exactly why I switched from containers to nix flakes and from Vs code to neovim or Helix. The difference is night and day. It's so nice to have my editor open instantly and to be able to have multiple instances open at once. The LSP is by far the most memory hungry. It's definitely worth the effort.\n \nreply",
      "Direnv + nix packages is way better than Dev container development in my experience.\n \nreply",
      "> Perhaps the problem isn't the editors. Perhaps it's us.For coming up on a decade I've used Vim with a minimal .vimrc and no plugins. The only time I deviate from this is when I am writing in an s-expression based language. I would probably deviate from this to write Java or C#, but I haven't written either in a while.There are upsides and downsides. The biggest upside is simply that I haven't spent ANY time learning new editors or new editor features; I'll occasionally learn about a feature of Vim that I didn't know existed, but that's very oriented toward solving immediate problems, because it tends to happen when I run into something that feels like there's probably an easier way to do it, and I'll do a quick internet search. I think a lot of devs spend a lot of time learning tools with the sense that the time spent will be paid back by time savings from using the tools, but the reality is way more hit-and-miss, and I think a lot of people could benefit from being more selective in what they spend their learning time on.The thing that Vim completely misses is being able to jump to the appropriate file where a class/function is defined. This is more of a tradeoff than IDE folks recognize: when I was using PyCharm/IntelliJ/ReSharper, I found that being able to jump around easily would hide the fact that my projects were growing in size and complexity. The tooling makes this less painful up front, but eventually, you still feel the pain, because eventually there's some bug that cascades through a bunch of files, and you still have to reason about all of them. Finding definitions isn't the core issue with having a lot of definitions, reasoning about how they interact is the core issue, and the IDE tooling doesn't solve that. Being in Vim and having to deal with my project's file structure directly and explicitly means I feel the pain of complexity earlier, when it's easier to fix.If I'm being honest, I'm not sure that the tradeoffs comes out in Vim's favor here. I don't think we get to have a conclusive answer because there's simply nobody who uses both vanilla Vim and the best IDEs at a high enough level to have an informed opinion about which is better. I'd say I am close because I have used both extensively, but my IDE knowledge is outdated by about a decade.But, I've said before and I'll say again that entering text into files isn't usually the limiting factor of software development speed. If I'm mentoring a new programmer I'd rather see them learn TDD and/or how to leverage type systems and write code in Notepad, than see them write untested, unchecked code in The Best IDE/Editor Ever. Of course, there's no reason to go to those extremes.\n \nreply",
      "Your 3rd and last paragraphs reminded me of a feature I really like about F#: all source files in an assembly have an explicit, sequential compilation order. And you can only have references to things that had been defined earlier, either in the current file or in a file that comes earlier in the compilation order.It makes learning and navigating a new codebase much easier. So much so that it doesn't really require IDE tooling the way it does with most mainstream languages. It's harder to get lost when you always know which way is up. Consciously thinking about whether you're doing top-down or bottom-up design also flows naturally from this, for the same reason, and that seems to encourage more thoughtful, readable code design.Is it more work? Up-front, yes, absolutely. In the long run, though? By the time I finished my first year of CS education I had already been exposed to many many examples of cases where greedy algorithms consistently produce sub-optimal results. Perhaps they aren't teaching people about that in school anymore.\n \nreply",
      "That sounds like a nightmare when an IDE presents files alphabetically but has a strict logical order that the UI doesn't understand. Talk about jumping around.\n \nreply"
    ],
    "link": "https://pragtical.dev/",
    "first_paragraph": "The practical and pragmatic code editor30 MB of RAM, 5 MB of disk space. Pragtical runs on many devices without performance issues.Syntax highlighting, mulitple cursors, command palette and many more. LSP and other features are available as plugins.Pragtical allows you to extend the editor via Lua and its C API. Documentation is available for many parts of the editor.Built on SDL, C and Lua, Pragtical runs on Windows, Linux and macOS. Porting to other systems is trivial.Easily change your editor settings, color theme, key bindings and installed plugins configuration using the graphical settings manager.Pragtical is licensed under the MIT license. No telemetry or data collection."
  },
  {
    "title": "Sourcegraph went dark (eric-fritz.com)",
    "points": 381,
    "submitter": "kaycebasques",
    "submit_time": "2024-08-20T03:30:27",
    "num_comments": 153,
    "comments_url": "https://news.ycombinator.com/item?id=41296481",
    "comments": [
      "Sourcegraph CEO here. We made our main internal codebase (for our code search product) private. We did this to focus. It added a lot of extra work and risk to have stuff be open source and public. We gotta stay focused on building a great code search/intelligence product for our customers.That's what ultimately lets us still do plenty of things for devs and the OSS community:(1) Our super popular public code search is at https://sourcegraph.com/search, which is the same product customers use internally on their own codebases. We spend millions of dollars annually on this public instance with almost 1M OSS repositories to help out everyone using OSS (and we love when they like it so much they bring it into their company :-).(2) We also have still have a ton of open-source code, like https://sourcegraph.com/github.com/sourcegraph/cody (our code AI tool).BTW, if any founders out there are wondering whether they should make their own code open-source or public, happy to chat! Email in profile. I think it could make sense for a lot of companies, but more so for infrastructure products or client tools, not so much for full server-side end-user applications.\n \nreply",
      "Been a fan of sourcegraph since 2016 or so, it's been exciting to watch the pivots along the way. That being said, the loss of transparency here is pretty sad, speaking as a large FOSS repo owner. What were the main factors apart from risk that went into the decision?\n \nreply",
      "Thanks for being a fan. And I understand it's a bummer to not have our code be public and open-source anymore. Sorry.It's a bunch of reasons that add up. I'll give some more details for anyone curious.(And I know that despite these reasons, lots of HNers probably wish it was not so. I agree! I too wish for a world where all companies could have their code be public and open source.)- We have a lot of tech around large-scale code graph, indexing, etc., stuff that is very differentiated and hard to build. We were starting to put some of this in separate private repositories and link them in at build time, but that was complex. It added a lot of code complexity, risked bugs, and slowed us down, and if a lot of the awesome stuff was private anyway, what was the point?- As we've been building Cody (https://cody.dev), our code AI tool, we've seen a LOT more abuse. That's what happens when you offer any free tier of a product with LLM inference. We had to move a lot more of our internal backend abuse logic to private repositories, and it added code complexity to incorporate that private stuff in at build time.- It confused devs and customers to have 2 releases: an open-source release with less scaley/enterprisey features, and an enterprise release. It was a pain to migrate from one to the other (GitLab also felt this pain with their product) because the open-source build had a subset of the DB schema and other things. It was confusing to have a free tier on the enterprise release (lots of people got that mixed up with the open-source release), and it made our pricing and packaging complex so that lots of our time was spent helping customers understand what is paid and what isn't.- There were actually very very few companies that were going to pay but then decided to use the open-source version and not pay us. A lot of people probably assume that's why we made this move, but it's not. I think this is because people like the product and see value in it, including all the large-scale code nav/search features that are in our enterprise version.- Although very very few companies used our open-source version to avoid paying us, we did see it cause a lot of annoyance for devs who were asked by their management to try cloning our product or to research our codebase to give their procurement team ammunition to negotiate down our price. This honestly was just a waste of everyone's time.- If we got a ton of contributions (we never really solicited any), then it might've changed the calculus. Sourcegraph is an end-user application that you use at work (and when fun-coding, but the primary revenue model is for us to charge companies). For various reason, end-user server-side applications just don't get nearly as many contributions. Maybe it's because you'd need to redeploy your build for a bunch of other users at your company, not just yourself. Maybe it's because they necessarily entail UX, frontend, and scaling stuff, in addition to just adding new features.- We heard from people who left GitHub that people at GitHub were frequently monitoring our repository to get wind of our upcoming features and launches. Someone from GitHub told me his \"job is to clone Sourcegraph\". Since then, they obviously deprioritized their code search to re-found GitHub on AI, so we're not seeing this threat anymore. But I didn't love giving Microsoft an unfair advantage, especially since GitHub products are not open source either.- Since we made our code non-open-source, we've been able to pursue a lot more big partnerships (e.g., with cloud providers and other distribution partners and resellers). This is a valuable revenue stream that helps us make a better product overall. Again, because Sourcegraph is an end-user application with a UI that devs constantly use and care about, we never really had the MongoDB/Redis/CockroachDB risk of AWS/GCP/Azure just deploying our stuff and cutting us out. We're not protecting from downside here, but we are enjoying the upside because now those kinds of distribution partnerships are viable for us. To give a specific example, within ~2 months of making our code non-open-source last year, we signed a $1M+ ARR deal through a distribution partner that would not have happened if our code was open source. This is not our biggest annual deal, but it's still really nice!We are totally focused on building the best code search/intelligence and appreciate all our customers and all the feedback here. Hope this helps explain a bit more where we're coming from!\n \nreply",
      ">Although very very few companies used our open-source version to avoid paying us, we did see it cause a lot of annoyance for devs who were asked by their management to try cloning our product or to research our codebase to give their procurement team ammunition to negotiate down our price. This honestly was just a waste of everyone's time.Trying to spin that it was \"for the devs\" is really stretching the bounds of incredulity.  We get it, its fine, you have investors to answer to, but come on don't pee on our shoes and tell us its raining.\n \nreply",
      "Actually this one I get completely. There\u2019s plenty of places or managers with dev orgs that will check if they can install something complex in house with open source. Nothing wrong with it. But it\u2019s usually a huge waste of time.\n \nreply",
      "Why? Getting operational experience with the product that you might then pay a lot for seems very important. Especially if you end up liking the product/service but not the pricing changes that might then happen, so doing some exploratory fact finding for a backup plan doesn't seem to be waste of time.For example when we used Jira on-prem and it was snappy and we were happy ... and it was a rather important point of difference compared to the slow shitocumulus version.Also, when people are using GitHub issues to ask questions the problem is usually a lack of clear documentation. (And if spending time to link FAQ answers to potential customers is a waste of time ... then maybe it's not surprising that Sourcegraph CEO is doing damage control on HN instead of focusing on focusing or whatever.)\n \nreply",
      "> But it\u2019s usually a huge waste of time.Is it? I think at this point my company has probably saved millions of dollars by not paying for subscriptions, but hosting everything in-house. The price point of a lot of these services makes perfect sense when you are small, but paying 1M/year in subscription fees when you can host the same thing for 10k/year is just bonkers. I appreciate that someone has to pay for it for them to continue making the product, but there\u2019s a point where it makes more sense for me to spend a year setting it up (and really only costs two weeks).\n \nreply",
      "Well that obviously doesn\u2019t apply to Sourcegraph because their self-host offering requires paying a subscription. You can\u2019t use any form of Sourcegraph on private code, (at least not without all the important features being nobbled) without paying a subscription. So there\u2019s no saving to be made from self-hosting sourcegraph\n \nreply",
      "> So there\u2019s no saving to be made from self-hosting sourcegraphThat may have been true in the time before LLMs, but I'd argue that any sourcecode exfiltration nowadays runs the very real risk of \"oh, sure, we won't use your code for training our model ... wink, wink\"\n \nreply",
      "There's enough open source code available that the hassle of other code probably isn't worth it. LLMs aren't insightful enough to benefit from any differences you'd see between open and closed source code corpii.\n \nreply"
    ],
    "link": "https://eric-fritz.com/articles/sourcegraph-went-dark/",
    "first_paragraph": "Towards the end of my mid-2019 job search, I was down to joining the Google Go team or Sourcegraph. Sourcegraph ultimately won due to cultural factors - the most important of which was the ability to build 100% in the open. All documents were public by default. Technical and product RFCs (and later PR/FAQs) were drafted, reviewed, and catalogued in a public Google Drive folder. All product implementation was done in public GitHub repositories.Today, the sourcegraph/sourcegraph repository went private. This is the final cleaving blow, following many other smaller chops, on the culture that made Sourcegraph an attractive place to work. It\u2019s a decision for a business from which I resigned, and therefore have no voice. But I still lament the rocky accessibility of artifacts showing four years of genuine effort into a product that I loved (and miss the use of daily in my current role).On the bright side, I\u2019ve cemented my place on the insights leaderboard for the remainder of time.Over my te"
  }
]