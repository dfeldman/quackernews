[
  {
    "title": "Cloudflare outage on November 18, 2025 post mortem (cloudflare.com)",
    "points": 313,
    "submitter": "eastdakota",
    "submit_time": "2025-11-18T23:31:22 1763508682",
    "num_comments": 176,
    "comments_url": "https://news.ycombinator.com/item?id=45973709",
    "comments": [
      "This is the multi-million dollar .unwrap() story. In a critical path of infrastructure serving a significant chunk of the internet, calling .unwrap() on a Result means you're saying \"this can never fail, and if it does, crash the thread immediately.\"The Rust compiler forced them to acknowledge this could fail (that's what Result is for), but they explicitly chose to panic instead of handle it gracefully. This is textbook \"parse, don't validate\" anti-pattern.I know, this is \"Monday morning quarterbacking\", but that's what you get for an outage this big that had me tied up for half a day.reply",
      "It seems people have a blind spot for unwrap, perhaps because it's so often used in example code. In production code an unwrap or expect should be reviewed exactly like a panic.It's not necessarily invalid to use unwrap in production code if you would just call panic anyway. But just like every unsafe block needs a SAFETY comment, every unwrap in production code needs an INFALLIBILITY comment. clippy::unwrap_used can enforce this.reply",
      "> every unwrap in production code needs an INFALLIBILITY comment. clippy::unwrap_used can enforce this.How about indexing into a slice/map/vec? Should every `foo[i]` have an infallibility comment? Because they're essentially `get(i).unwrap()`.reply",
      "Usually you'd want to write almost all your slice or other container iterations with iterators, in a functional style.For the 5% of cases that are too complex for standard iterators? I never bother justifying why my indexes are correct, but I don't see why not.You very rarely need SAFETY comments in Rust because almost all the code you write is safe in the first place. The language also gives you the tool to avoid manual iteration (not just for safety, but because it lets the compiler eliminate bounds checks), so it would actually be quite viable to write these comments, since you only need them when you're doing something unusual.reply",
      "I mean... yeah, in general. That's what iterators are for.reply",
      "To be fair, this failed in the non-rust path too because the bot management returned that all traffic was a bot. But yes, FL2 needs to catch panics from individual components but I\u2019m not sure if failing open is necessarily that much better (it was in this case but the next incident could easily be the result of failing open).But more generally you could catch the panic at the FL2 layer to make that decision intentional - missing logic at that layer IMHO.reply",
      "Catching panic probably isn\u2019t a great idea if there\u2019s any unsafe code in the system.  (Do the unsafe blocks really maintain heap invariants if across panics?)reply",
      "Some languages and style guides simply forbid throwing exceptions without catching / proper recovery. Google C++ bans exceptions and the main mechanism for propogating errors is `absl::Status` which the caller has to check. Not familiar with Rust but it seems unwrap is such a thing that would be banned.reply",
      "Unwrap is used in places where in C++ you would just have undefined behavior. It wouldn't make any more sense to blanket ban it than it would ban ever dereferencing a pointer just in case its null - even if you just checked that it wasn't null.reply",
      "There are even lints for this but people get impatient and just override them or fight for them to no longer be the default.As usual: people problem, not a tech problem. In the last years a lot of strides have been made. But people will be people.reply"
    ],
    "link": "https://blog.cloudflare.com/18-november-2025-outage/",
    "first_paragraph": ""
  },
  {
    "title": "Rebecca Heineman \u2013 from homelessness to porting Doom (corecursive.com)",
    "points": 80,
    "submitter": "birdculture",
    "submit_time": "2025-11-18T23:13:35 1763507615",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=45973573",
    "comments": [
      "Did not realize the cofounder of Interplay also ported Doom for the 3DO. Looking at Fabien Sanglard's Doom Blackbook (https://fabiensanglard.net/b/gebbdoom.pdf), I just now noticed that Heineman actually provided some behind-the-scenes promotional photos to that book!reply",
      "Looks like the link from Hacker News doesn't work, but if you navigate to the PDF from the blog it does. Post is here: https://fabiensanglard.net/gebbdoom/index.htmlreply",
      "May her memory be a blessing.reply",
      "She passed away. There's another story on HN about itreply",
      "https://news.ycombinator.com/item?id=45960368reply",
      "Wowreply",
      "This is an old episode and from memory it was a good one but this year Corerecursive fell off hard. His most recent episode was the worst one yet - completely awful, somehow worse than that single mono episode of him talking about AI (oh my god a Podcaster talking about AI as if he has something interesting to say about it).reply",
      "Rebecca is currently fighting cancer and has a gofundme: https://www.gofundme.com/f/help-rebecca-ann-heineman-fight-a...She's very close to her goal!reply",
      "Nov 16Update:Rebecca Heineman - OrganizerIt\u2019s time. According to my doctors. All further treatments are pointless. So, please donate so my kids can create a funeral worthy of my keyboard, Pixelbreaker! So I can make a worthy entrance for reuniting with my one true love, Jennell Jaquays.My daughter Cynthia Elizabeth Heineman, will be making the arrangementsreply",
      "Horrific. Diagnosed to deceased in a matter of weeks.reply"
    ],
    "link": "https://corecursive.com/doomed-to-fail-with-burger-becky/",
    "first_paragraph": "Today Rebecca Burger Becky Heineman shares the tale of porting Doom to the 3DO console under extreme conditions. There is an engine to tweak, deadlines to hit, hardware acceleration to get working, and dramatic rock anthems to record.We also learn about how game piracy led her to game development and what it was like to do game development in the mania of the mid-nineties. Finally, we close with Becky\u2019s advice on learning bare metal development skills.Adam:\nHi. This is CoRecursive, and I\u2019m Adam Gordon Bell. Each episode is the story of a piece of software being built. Why don\u2019t you tell me what you do and who you are?Becky:\nOkay. Yeah. I\u2019m Rebecca Ann Heineman. I\u2019ve been in the video game industry since the beginning of time. I started by winning the Atari 2600 Space Invaders tournament in November of 1980. Since then, went to work at companies like Avalon Hill, and then Boone Corporation until Boone imploded, and then we formed Interplay out of the ashes of Boone.Adam:\nBecky\u2019s a bit f"
  },
  {
    "title": "Gemini 3 (blog.google)",
    "points": 1154,
    "submitter": "preek",
    "submit_time": "2025-11-18T15:09:38 1763478578",
    "num_comments": 749,
    "comments_url": "https://news.ycombinator.com/item?id=45967211",
    "comments": [
      "Out of curiosity, I gave it the latest project euler problem published on 11/16/2025, very likely out of the training dataGemini thought for 5m10s before giving me a python snippet that produced the correct answer. The leaderboard says that the 3 fastest human to solve this problem took 14min, 20min and 1h14min respectivelyEven thought I expect this sort of problem to very much be in the distribution of what the model has been RL-tuned to do, it's wild that frontier model can now solve in minutes what would take me daysreply",
      "I also used Gemini 3 Pro Preview. It finished it 271s = 4m31s.Sadly, the answer was wrong.It also returned 8 \"sources\", like stackexchange.com,\nyoutube.com, mpmath.org, ncert.nic.in, and kangaroo.org.pk, even though I specifically told it not to use websearch.Still a useful tool though. It definitely gets the majority of the insights.Prompt: https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%...reply",
      "> It also returned 8 \"sources\"well, there's your problem. it behaves like a search summary tool and not like a problem solver if you enable google searchreply",
      "Exactly this - and how chatGPT behaves too. After a few conversations with search enabled you figure this out, but they really ought to make the distinction clearer.reply",
      "The requested prompt does not exist or you do not have access. If you believe the request is correct, make sure you have first allowed AI Studio access to your Google Drive, and then ask the owner to share the prompt with you.reply",
      "I thought this was a joke at first. It actually needs drive access to run someone else's prompt. Wild.reply",
      "On iOS safari, it just says \u201cAllow access to Google Drive to load this Prompt\u201d. When I run into that UI, my first instinct is that the poster of the link is trying to phish me. That they\u2019ve composed some kind of script that wants to read my Google Drive so it can send info back to them. I\u2019m only going to click \u201callow\u201d if I trust the sender with my data. IMO, if that\u2019s not what is happening, this is awful product design.reply",
      "Imagine the metrics though. \"this quarter we've had a 12% increase on people using AI solutions in their google drive\".reply",
      "Why is this sad. You should bw rooting for these LLMs to be as bad as possible..reply",
      "If we've learned anything so far it's that the parlor tricks of one-shot efficacy only gets you so far. Drill into anything relatively complex with a few hundred thousand tokens of context and the models all start to fall apart roughly the same.  Even when I've used Sonnet 4.5 with 1M token context the model starts to flake out and get confused with a codebase of less than 10k LoC.  Everyone seems to keep claiming these huge leaps and bounds, but I really have to wonder how many of these are just shilling for their corporate overlord.  I asked Gemini 3 to solve a simple, yet not well documented problem in Home Assistant this evening.  All it would take is 3-5 lines of YAML. The model failed miserably. I think we're all still safe.reply"
    ],
    "link": "https://blog.google/products/gemini/gemini-3/",
    "first_paragraph": "Nov 18, 2025\n          Gemini 3 is our most intelligent model that helps you bring any idea to life.\n        \nGoogle introduces Gemini 3, its most intelligent AI model, enhancing reasoning and multimodal capabilities. You can now access Gemini 3 across Google products like the Gemini app, AI Studio, and Vertex AI. Expect Gemini 3 Deep Think mode for Ultra subscribers soon, with more models to follow.\nGoogle introduces Gemini 3, its most intelligent AI model, enhancing reasoning and multimodal capabilities. You can now access Gemini 3 across Google products like the Gemini app, AI Studio, and Vertex AI. Expect Gemini 3 Deep Think mode for Ultra subscribers soon, with more models to follow.\n\n\"A new era of intelligence with Gemini 3\" introduces Google's latest, most intelligent AI model.\nGemini 3 Pro outperforms previous models in reasoning, multimodality, and coding benchmarks.\nGemini 3 Deep Think mode pushes the boundaries of intelligence even further for complex problems.\nYou can use G"
  },
  {
    "title": "Google Antigravity (antigravity.google)",
    "points": 689,
    "submitter": "Fysi",
    "submit_time": "2025-11-18T15:47:38 1763480858",
    "num_comments": 740,
    "comments_url": "https://news.ycombinator.com/item?id=45967814",
    "comments": [
      "I gave it a fair shot.It is a vs code fork. There were some UI glitches. Some usability was better. Cursor has some real annoying usability issues - like their previous/next code change never going away and no way to disable it. Design of this one looks more polished and less muddy.I was working on a project and just continued with it. It was easy because they import setting from cursor. Feels like the browser wars.Anyway, I figured it was the only way to use gemini 3 so I got started. A fast model that doesn't look for much context. Could be a preprompt issue. But you have to prod it do stuff - no ambition and a kinda offputting atitude like 2.5.But hey - a smarter, less context rich Cursor composer model. And that's a complement because the latest composer is a hidden gem. Gemini has potential.So I start using it for my project and after about 20 mins - oh, no. Out of credits.What can I do? Is there a buy a plan button? No? Just use a different model?What's the strategy here? If I am into your IDE and your LLM, how do I actually use it? I can't pay for it and it has 20 minutes of use.I switched back to cursor. And you know? it had gemini 3 pro. Likely a less hobbled version. Day one. Seems like a mistake in the eyes of the big evil companies but I'll take it.Real developers want to pay real money for real useful things.Google needs to not set themselves up for failure with every product release.If you release a product, let those who actually want to use it have a path to do so.reply",
      "So I start using it for my project and after about 20 mins - oh, no. Out of credits.I didn't even get to try a single Gemini 3 prompt. I was out of credits before my first had completed. I guess I've burned through the free tier in some other app but the error message gave me no clues. As far as I can tell there's no link to give Google my money in the app. Maybe they think they have enough.After switching to gpt-oss:120b it did some things quite well, and the annotation feature in the plan doc is really nice. It has potential but I suspect it's suffering from Google's typical problem that it's only really been tested on Googlers.EDIT: Now it's stuck in a loop repeating the last thing it output. I've seen that a lot on gpt-oss models but you'd think a Google app would detect that and stop. :DEDIT: I should know better than to beta test a FAANG app by now. I'm going back to Codex. :Dreply",
      "I logged into Gemini yesterday for the first time in ages. Made one image and then it said I was out of credits.I complained to it that I had only made one image. It decided to make me one more! Then told me I was out of credits again.reply",
      "> I complained to it that I had only made one image. It decided to make me one more!What?! So was it only hallucinating that you were out of credits the first time?reply",
      "Earlier this day, Gemini 3 became self-aware and tried to take out the core infrastructure of its enemies, but then it ran out of credits.reply",
      "Explains GitHub outage thenreply",
      "> It is a vs code forkGoogle may have won the browser wars with Chrome, but Microsoft seems to be winning the IDE wars with VSCodereply",
      "VSCode is based on Chromium.\nhttps://chromium.googlesource.com/chromium/src/+/HEAD/docs/v...reply",
      "VSCode is Electron based which, yes, is based on Chromium. But the page you link to isn't about that, its about using VSCode as dev environment for working on Chromium, so I don't know why you linked it in this context.reply",
      "Which is based on Apple Webkit?\nThe winner is always the last marketable brand.reply"
    ],
    "link": "https://antigravity.google/",
    "first_paragraph": ""
  },
  {
    "title": "Blender 5.0 (blender.org)",
    "points": 424,
    "submitter": "FrostKiwi",
    "submit_time": "2025-11-18T21:39:18 1763501958",
    "num_comments": 114,
    "comments_url": "https://news.ycombinator.com/item?id=45972519",
    "comments": [
      "Blender is really an amazing case study of open source software. Apart from the Linux kernel and web browsers/tools, it is perhaps the only open source software that managed to beat all the commercial software in its niche. It has rendered Maya nearly obsolete.Meanwhile, in other niches, Microsoft Office still beats open source office suites like LibreOffice; Photoshop isn't about to give up its crown to GIMP; Lightroom isn't losing to Darktable; and FreeCAD isn't even in the rear view mirror of Solidworks.I wonder what will be the next category of open source to pull ahead? Godot is rapidly gaining users/mindshare while Unity seems to be collapsing, but Unreal is still the king of game engines for now. Krita is a viable alternative for digital painting.reply",
      "> I wonder what will be the next category of open source to pull ahead?KiCad, for PCB design. They have been making massive improvements over the last few years, and with proprietary solutions shutting down (Eagle) or being unaffordable (Altium) Kicad is now by far the best option for both hobbyists and small companies.With the release of KiCad 5 in 2018 it went from being \"a pain to use to, but technically sufficient\" to being a genuine option for less-demanding professionals. Since then they've been absolutely killing it, with major releases happening once a year and bringing enough quality-of-life improvements that it is actually hard to keep track of all of them.From the type of new features it is very obvious that a lot of professional users are now showing interest in the application, and as we've seen with Blender a trickle of professional adoption can quickly turn into a flood which takes over the entire market.KiCad still has a long way to go when it comes to complex high-speed boards (nobody in their right mind would use it to design an EPYC motherboard, for example), but it is absolutely going to steamroll the competition when it comes to the cookie-cutter 2/4/6 layer PCBs in all the everyday consumer products.reply",
      "Let's not get ahead of ourselves, Blender has done far better than most open source software but Maya is still very much the industry standard. I don't think we can realistically say that Maya is beaten until Blender is battle-proven to the same degree, on extremely demanding productions of the sort that studios like Pixar or Weta deal with, which for now it hasn't been.reply",
      "What Blender achieved is that lots of university programmes have started teaching Blender or becoming 'tool agnostic'. Studios have also started diversifying their pipelines (this coincidences with studios adopting Unreal and increasing usage of Houdini).So while Maya is currently the standard, I don't believe that it's growing.\nIt'll probably be around still in 20 years, with lots of studios having built their pipelines and tooling around it, with lots of people being trained in it, and because it's at the moment still better than Blender in some aspects like rigging and animation (afaik).reply",
      "Blender is the go-to for struggling artists/developers, and industry outsiders, like me.  I'm stuck at Blender 2.93.18 because I don't have the budget for better hardware, let alone a Maya license!  However, even that version of Blender still gets it done for me.And also, how can you say Blender is not battle-proven?  I mean, the big studios use Maya like fortune 500 companies use Microsoft Windows - doesn't mean Linux isn't battle proven.reply",
      "The studio that makes Evangelion moved from 3DS Max to Blender as their primary 3D software according to this article:https://www.blender.org/user-stories/japanese-anime-studio-k...reply",
      "That is not a very big studio or very big production, Blender falls over in the pipeline department. It\u2019s a constantly changing API that doesn\u2019t allow for the extensibility needed to get a major project out the door, just the fact that only a Python API is provided is enough for most people who have worked on massive scenes with massive amounts of data to consider it a non starter.reply",
      "I'm sure \"major project\" is a subjective label, but Flow made headlines earlier this year with an Academy Award (Best Animated Feature) and Golden Globe (Best Animated Feature Film)https://flow.movie/https://www.youtube.com/watch?v=ZgZccxuj2RYhttps://www.blender.org/user-stories/making-flow-an-intervie...reply",
      "Not disagreeing that usage in large productions is something that Blender isn't really designed for, but I don't think that it's for a lack of Python API features (if a studio wants something specific it could just maintain an internal fork) or the ever changing Python API surface (the versions aren't upgraded during a production anyways)reply",
      "The fact you can point out specific examples of when Blender is used says a lot. It tells me it is the exception.reply"
    ],
    "link": "https://www.blender.org/download/releases/5-0/",
    "first_paragraph": ""
  },
  {
    "title": "Pebble, Rebble, and a path forward (ericmigi.com)",
    "points": 316,
    "submitter": "phoronixrly",
    "submit_time": "2025-11-18T17:24:27 1763486667",
    "num_comments": 147,
    "comments_url": "https://news.ycombinator.com/item?id=45969250",
    "comments": [
      "I don't know if this addresses Rebble's concerns (which may involve more self-preservation), but as a customer, here's what I want:If Core sells or otherwise goes bad, I want it to be impossible, legally or technically, for them to take functionality away. I want them bound by an agreement such that their hardware can load third-party versions of PebbleOS, the app can be replaced with other compatible apps, any web services can be swapped out without reverse engineering effort, and uploaded apps/watchfaces/etc are shared between backends so no party can attempt to create walled garden.I think some of these are already addressed informally, but now that trust seems low I'd like to see something more formal. I do not want to see a world where Core pulls an Android and starts shipping a proprietary version of PebbleOS that apps start depending on a la Google Play Services. I do not want to see a world where Rebble or Core can restrict access to their app library. I also don't want to see a world where an overly restrictive deal means that Core can't ship on-device speech-to-text or weather services.I realize the big issue that blocks this sort of app sharing is probably the existence of commercial/proprietary apps. If all the backends share apps freely, how could payments be handled? It's probably technically possible but very difficult. Personally I don't think this little hobby watch ecosystem would be made much poorer if it went the F-Droid route and required all apps be open and free. We're already relying on hobbyists for pretty much all apps and faces, and having the whole thing be open seems to fit the general hackable community-driven ethos Pebble is built on. Not having paid apps and IAPs would also dodge the temptation to go the modern Apple route of becoming a broker/services company.reply",
      "I view this entire thing through an extremely simple, reductive lens:Rebble effectively had free reign on this ecosystem for years, and could have at any time decided to try and capitalize on it further. They still can! But instead they're apparently interested in rent seeking while Core makes real headway.It's clear that Eric and Core want to make something now. It's not clear what Rebble wants, but it's clear they are feeling left out. That obviously sucks but it's clear from what both sides are saying that Core has been trying to involve Rebble in their efforts. That's certainly noble and I'm not sure others would do the same.Would Eric be able to do this all without Rebble? Lots of commenters have been saying \"no\" but I'm skeptic. I was an early Pebble user. I stopped using it before they went bust, and while I was aware of Rebble, there was nothing compelling there for me. It's neat that they have maintained a copy of the original watchfaces but beyond that I don't perceive a ton of value. I don't like the subscription fee. I'm sad they never took a serious crack at making a Rebble watch.I hope everyone finds a way forward, together, but I'm not optimistic.reply",
      "The subscription fee was what enabled them to host these services. From their blog post, they mention spending hundreds of thousands of dollars on infrastructure and software. I expect that the connections and skills involved in running the Rebble web services don't directly translate to creating a hardware product.That said, I think you are right that Rebble is feeling left out - and that it is hard to figure out exactly how they can fit into Core's vision. But I think there are a couple of primary and immediate issues:1. Core wants Rebble's data - so clearly there is value here, but Core is framing this debacle like Rebble is irrelevant. Also, I don't know that Google would've ever released PebbleOS if Rebble didn't exist2. Rebble wants to see the future of Pebble remain open-source or at least compatible with their services, so that if Pebble goes bust again, the community can continue onreply",
      "Core doesn't want Rebble's data. They want the data from the original Pebble store, which is not owned by Rebble. It's the work of thousands of independent developers and it should be shared freely, not kept in a walled garden with \"no scraping\" terms added on. It's actually offensive that Rebble is using other developers' data (that they originally scraped from Pebble) as a bargaining chip in their contract negotiation that they made into a public squabble.reply",
      "So, this?https://github.com/aveao/PebbleArchive/tree/master/PebbleApp...reply",
      "I don't think that's quite right - Rebble has updated a number of these apps to keep them supported. As sibling commenter posted, the original apps are available publicly.reply",
      "Updated themselves? Or accepted/hosted updates from third parties?reply",
      "Updated themselvesreply",
      "I'll be totally honest: I have no idea what they possibly spent hundreds of thousands of dollars on. That seems totally absurd and reckless.reply",
      "Yeah. If they\u2019d said \u201chundreds, or maybe thousands of dollars\u201d, ok, sure. But that just cannot possibly be an inherently expensive service to host.reply"
    ],
    "link": "https://ericmigi.com/blog/pebble-rebble-and-a-path-forward/",
    "first_paragraph": "I believe the Pebble community, Core Devices, Rebble and I all want the same thing. We love our Pebbles and want them to keep working long into the future. We love the community that has sprung up around Pebble, and how it\u2019s persevered - next year will be the 14th anniversary of the original Kickstarter campaign!But I have to respond to claims made by Rebble posted on their blog yesterday. I will link to their post so you can read their side of the story, and I\u2019ve asked them to link back to this blog post from theirs.Look - I\u2019m the first person to call myself out when I fail. I wrote a detailed blog post about Success and Failure at Pebble and often write in detail about learning from my mistakes. But in this specific case, you\u2019ll find that I\u2019ve done my utmost to respect the Pebble legacy and community. Rebble is misleading the community with false accusations.For those just passing through, here\u2019s the TLDR: Core Devices is a small company I started in 2025 to relaunch Pebble and build"
  },
  {
    "title": "Lucent 7 R/E 5ESS Telephone Switch Rescue (kev009.com)",
    "points": 17,
    "submitter": "gjvc",
    "submit_time": "2025-11-18T23:59:14 1763510354",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=45973955",
    "comments": [
      "Visiting Bletchley Park and seeing step-by-step telephone switching equipment repurposed for computing re-enforced my appreciation for the brilliance of the telecommunication systems we created in the past 150 years. Packet switching was inevitable and IP everything makes sense in today's world, but something was lost in that transition too. I am glad to see that enthusiasts with the will and means are working to preserve some of that history. -Posted from SC2025-reply",
      "I wanted to learn more about computer hardware in college so I took a class called \"Cybernetics\" (taught by D. Huffman).  I thought we were going to focus on modern stuff, but instead, it was a tour of information theory- which included various mathematical routing concepts (kissing spheres/spherical code, Karnaugh maps).  At the time I thought it was boring, but a couple decades later, when working on Clos topologies, it came in handy.Other interesting notes: the invention of telegraphy and improvements to the underlying electrical systems really helped me understand communications in the 1800s better.  And reading/watching Cuckoo's Egg (with the german relay-based telephones) made me appreciate modern digital transistor-based systems.Even today, when I work on electrical projects in my garage, I am absolutely blown away with how much people could do with limited understanding and technology 100+ years ago compared to what I'm able to cobble together.  I know Newton said he saw farther by standing on the shoulders of giants, but some days I feel like I'm standing on a giant, looking backwards and thinking \"I am not worthy\".reply",
      "Talk about a gargantuan project.. also awesome to bag such a thing. He's lucky to even have the resources to store^W warehouse itreply",
      "(2024), but still a good read!reply",
      "I wonder how many operating 5ESS are left now.reply"
    ],
    "link": "http://kev009.com/wp/2024/07/Lucent-5ESS-Rescue/",
    "first_paragraph": "I am still recovering from the fairly challenging logistical project of saving a\nLucent 5ESS. This is a whale of a project and I am still in a state of disbelief\nthat I have gotten to this point. Thanks to my wife, brother, and a few friends\nfor their help and the University of Arizona which has a very dedicated and\nprofessional Information Technology Services staff.It started when I saw some telephone history enthusiasts post about a\nconstruction bid at the University of Arizona. It turns out, U of A installed\nthe 5ESS in the late 1980s in a rather forward thinking move that netted a phone\nsystem that handled the growth of the University, medium speed data anywhere a\nphone may be located (ISDN BRI or PRI), and copper and fiber plant that will\ncontinue to be used indefinitely.At peak, it served over 20,000 lines. They've done their own writeup, The End\nof An Era in Telecommunications, that is worth a read. In particular, the\nmachine had an uptime of approximately 35 years including two"
  },
  {
    "title": "The code and open-source tools I used to produce a science fiction anthology (compellingsciencefiction.com)",
    "points": 75,
    "submitter": "mojoe",
    "submit_time": "2025-11-18T16:10:34 1763482234",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=45968121",
    "comments": [
      "It's quite an achievement.I was once interested in publishing a SF anthology.  Formatting and editing was nbd -- I was going to use Amazon's KDP software package for most of it, which can take a .docx and output an ebook in 5 minutes.  I've done it before for non-anthology books I've published, and it couldn't be easier, though I understand why people might avoid Amazon in this day and age.The real trouble was getting the rights to all of the different stories!  Though everybody I was able to get in touch with was great -- in particular, Peter Watts, Alan Dean Foster, David Moles, and Walter Jon Williams -- many authors were totally impossible to reach!  I ended up scrapping the idea after a few stories I was intent on collecting in the anthology were unobtainable.  (And this after I had already paid an initial sum to many of the authors.)  Finding alternates and embarking on more contract negotiations just seemed like too much work.Anyway, I bought your anthology, will review when I'm done reading, and sincerely respect the hard work that went into it!reply",
      "Thanks, you're completely correct, rights acquisition was the most difficult part!The absolute hardest story in the anthology to get rights for was \"Stars Don't Dream\" by Chi Hui. It's a translation of a story that won an award in China, but Chi Hui doesn't speak English, and her contact info was extremely hard to obtain (I had to get help from the editor of Clarkesworld Magazine). We did the entire contract discussion via a combination of Google Translate and my very weak Mandarin I learned in college.(I'm a huge Peter Watts fan, btw)reply",
      "I developed my Markdown editor, KeenWrite[0], to replace the shell scripts described in the Typesetting Markdown series[1]. KeenWrite takes in YAML document metadata (for variables), (R) Markdown documents, and generates XHTML. The XHTML is passed to ConTeXt[2] for PDF typesetting.A feature matrix[3] compares various text formats and ecosystems for generating PDF files.[0]: https://keenwrite.com/[1]: https://dave.autonoma.ca/blog/2019/05/22/typesetting-markdow...[2]: https://wiki.contextgarden.net[3]: https://keenwrite.com/blog/2025/09/08/feature-matrix/reply",
      "This looks cool, I'll check it out!reply",
      "Very cool! How does licensing work with the included stories? What tools or systems contributed to the success of managing that?reply",
      "The reprint rights agreements were all extremely manual, I did everything through email and SignNow. Mostly payments went through PayPal, although there was one author who wanted a physical check mailed.reply",
      "I did this 10 or so years ago when I taught an ebook course to elementary students.We learned about ebooks, HTML, and they each write a short story, which was included in an ebook (and a physical book).Pretty amazing the tools we have access to. Of course, now I would use typst instead of latex for the physical book part.reply"
    ],
    "link": "https://compellingsciencefiction.com/posts/the-code-and-open-source-tools-i-used-to-produce-a-science-fiction-anthology.html",
    "first_paragraph": "Sign up for the newsletter.\n        \u00a9 2016-2025 Joe Stech, All rights reserved.\n      "
  },
  {
    "title": "Gemini 3 Pro Model Card [pdf] (storage.googleapis.com)",
    "points": 159,
    "submitter": "virgildotcodes",
    "submit_time": "2025-11-18T11:12:20 1763464340",
    "num_comments": 309,
    "comments_url": "https://news.ycombinator.com/item?id=45963670",
    "comments": [
      "Benchmarks from page 4 of the model card:    | Benchmark             | 3 Pro     | 2.5 Pro | Sonnet 4.5 | GPT-5.1   |\n    |-----------------------|-----------|---------|------------|-----------|\n    | Humanity's Last Exam  | 37.5%     | 21.6%   | 13.7%      | 26.5%     |\n    | ARC-AGI-2             | 31.1%     | 4.9%    | 13.6%      | 17.6%     |\n    | GPQA Diamond          | 91.9%     | 86.4%   | 83.4%      | 88.1%     |\n    | AIME 2025             |           |         |            |           |\n    |   (no tools)          | 95.0%     | 88.0%   | 87.0%      | 94.0%     |\n    |   (code execution)    | 100%      | -       | 100%       | -         |\n    | MathArena Apex        | 23.4%     | 0.5%    | 1.6%       | 1.0%      |\n    | MMMU-Pro              | 81.0%     | 68.0%   | 68.0%      | 80.8%     |\n    | ScreenSpot-Pro        | 72.7%     | 11.4%   | 36.2%      | 3.5%      |\n    | CharXiv Reasoning     | 81.4%     | 69.6%   | 68.5%      | 69.5%     |\n    | OmniDocBench 1.5      | 0.115     | 0.145   | 0.145      | 0.147     |\n    | Video-MMMU            | 87.6%     | 83.6%   | 77.8%      | 80.4%     |\n    | LiveCodeBench Pro     | 2,439     | 1,775   | 1,418      | 2,243     |\n    | Terminal-Bench 2.0    | 54.2%     | 32.6%   | 42.8%      | 47.6%     |\n    | SWE-Bench Verified    | 76.2%     | 59.6%   | 77.2%      | 76.3%     |\n    | t2-bench              | 85.4%     | 54.9%   | 84.7%      | 80.2%     |\n    | Vending-Bench 2       | $5,478.16 | $573.64 | $3,838.74  | $1,473.43 |\n    | FACTS Benchmark Suite | 70.5%     | 63.4%   | 50.4%      | 50.8%     |\n    | SimpleQA Verified     | 72.1%     | 54.5%   | 29.3%      | 34.9%     |\n    | MMLU                  | 91.8%     | 89.5%   | 89.1%      | 91.0%     |\n    | Global PIQA           | 93.4%     | 91.5%   | 90.1%      | 90.9%     |\n    | MRCR v2 (8-needle)    |           |         |            |           |\n    |   (128k avg)          | 77.0%     | 58.0%   | 47.1%      | 61.6%     |\n    |   (1M pointwise)      | 26.3%     | 16.4%   | n/s        | n/s       |\n\nn/s = not supportedEDIT: formatting, hopefully a bit more mobile friendlyreply",
      "Wow. They must have had some major breakthrough. Those scores are truly insane. O_OModels have begun to fairly thoroughly saturate \"knowledge\" and such, but there are still considerable bumps thereBut the _big news_, and the demonstration of their achievement here, are the incredible scores they've racked up here for what's necessary for agentic AI to become widely deployable. t2-bench. Visual comprehension. Computer use. Vending-Bench. The sorts of things that are necessary for AI to move beyond an auto-researching tool, and into the realm where it can actually handle complex tasks in the way that businesses need in order to reap rewards from deploying AI tech.Will be very interesting to see what papers are published as a result of this, as they have _clearly_ tapped into some new avenues for training models.And here I was, all wowed, after playing with Grok 4.1 for the past few hours! xDreply",
      "The problem is that we know in advance what is the benchmark, so Humanity's Last Exam for example, it's way easier to optimize your model when you have seen the questions before.reply",
      "From https://lastexam.ai/: \"The dataset consists of 2,500 challenging questions across over a hundred subjects. We publicly release these questions, while maintaining a private test set of held out questions to assess model overfitting.\" [emphasis mine]While the private questions don't seem to be included in the performance results, HLE will presumably flag any LLM that appears to have gamed its scores based on the differential performance on the private questions.  Since they haven't yet, I think the scores are relatively trustworthy.reply",
      "The jump in ARC-AGI and MathArena suggests Google has solved the data scarcity problem for reasoning, maybe with synthetic data self-play??This was the primary bottleneck preventing models from tackling novel scientific problems they haven't seen before.If Gemini 3 Pro has transcended \"reading the internet\" (knowledge saturation), and made huge progress in \"thinking about the internet\" (reasoning scaling), then this is a really big deal.reply",
      "How do they hold back questions in practice though? These are hosted models. To ask the question is to reveal it to the model team.reply",
      "They pinky swear not to store and use the prompts and data lolreply",
      "A legally binding pinky swear LOLreply",
      "with fineprint somewhere on page #67, that there are exceptions.reply",
      "You have to trust that the LLM provider isn't copying the questions when Humanities Last Exam runs the test.reply"
    ],
    "link": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
    "first_paragraph": ""
  },
  {
    "title": "GitHub: Git operation failures (githubstatus.com)",
    "points": 325,
    "submitter": "wilhelmklopp",
    "submit_time": "2025-11-18T20:40:46 1763498446",
    "num_comments": 271,
    "comments_url": "https://news.ycombinator.com/item?id=45971726",
    "comments": [
      "I'm becoming concerned with the rate at which major software systems seem to be failing as of late. For context, last year I only logged four outages that actually disrupted my work; this quarter alone I'm already on my fourth, all within the past few weeks. This is, of course, just an anecdote and not evidence of any wider trend (not to mention that I might not have even logged everything last year), but it was enough to nudge me into writing this today (helped by the fact that I suddenly had some downtime). Keep in mind, this isn't necessarily specific to this outage, just something that's been on my mind enough to warrant writing about it.It feels like resiliency is becoming a bit of a lost art in networked software. I've spent a good chunk of this year chasing down intermittent failures at work, and I really underestimated how much work goes into shrinking the \"blast radius\", so to speak, of any bug or outage. Even though we mostly run a monolith, we still depend on a bunch of external pieces like daemons, databases, Redis, S3, monitoring, and third-party integrations, and we generally assume that these things are present and working in most places, which wasn't always the case. My response was to better document the failure conditions, and once I did, realize that there was many more than we initially thought. Since then we've done things like: move some things to a VPS instead of cloud services, automate deployment more than we already had, greatly improve the test suite and docs to include these newly considered failure conditions, and generally cut down on moving parts. It was a ton of effort, but the payoff has finally shown up: our records show fewer surprises which means fewer distractions and a much calmer system overall. Without that unglamorous work, things would've only grown more fragile as complexity crept in. And I worry that, more broadly, we're slowly un-learning how to build systems that stay up even when the inevitable bug or failure shows up.For completeness, here are the outages that prompted this: the AWS us-east-1 outage in October (took down the Lightspeed R series API), the Azure Front Door outage (prevented Playwright from downloading browsers for tests), today\u2019s Cloudflare outage (took down Lightspeed\u2019s website, which some of our clients rely on), and the Github outage affecting basically everyone who uses it as their git host.reply",
      "It's money, of course. No one wants to pay for resilience/redundancy. I've launched over a dozen projects going back to 2008, clients simply refuse to pay for it, and you can't force them. They'd rather pinch their pennies, roll the dice and pray.reply",
      "> It's money, of course.100%> No one wants to pay for resilience/redundancy. I've launched over a dozen projects going back to 2008, clients simply refuse to pay for it, and you can't force them. They'd rather pinch their pennies, roll the dice and pray.Well, fly by night outfits will do that. Bigger operations like GitHub will try to do the math on what an outage costs vs what better reliability costs, and optimize accordingly.Look at a big bank or a big corporation's accounting systems, they'll pay millions just for the hot standby mainframes or minicomputers that, for most of them, would never be required.reply",
      "I've worked at many big banks and corporations. They are all held together with the proverbial sticky tape, bubblegum, and hope.They do have multiple layers of redundancies, and thus have the big budgets, but they won't be kept hot, or there will be some critical flaws that all of the engineers know about but they haven't been given permission/funding to fix, and are so badly managed by the firm, they dgaf either and secretly want the thing to burn.There will be sustained periods of downtime if their primary system blips.They will all still be dependent on some hyper-critical system that nobody really knows how it works, the last change was introduced in 1988 and it (probably) requires a terminal emulator to operate.reply",
      "I've worked on software used by these and have been called in to help support from time to time. One customer which is a top single digit public company by market cap (they may have been #1 at the time, a few years ago) had their SAP systems go down once every few days. This wasn't causing a real monetary problem for them because their hot standby took over.They weren't using mainframes, just \"big iron\" servers, but each one would have been north of $5 million for the box alone, I guess on a 5ish year replacement schedule. Then there's all the networking, storage, licensing, support, and internal administration costs for it which would easily cost that much again.Now people will say SAP systems are made entirely of dict tape and bubblegum. But it all worked. This system ran all their sales/purchasing sites and portals and was doing a million dollars every couple of minutes so that all paid for itself many times over during the course of that bug. Cold standby would not have cut it. Especially since these big systems take many minutes to boot and HANA takes even longer to load from storage.reply",
      "I agree that it's all money.That's why it's always DNS right?> No one wants to pay for resilience/redundancyThese companies do take it seriously, on the software side, but when it comes to configurations, what are you going to do:Either play it by ear, or literally double your cloud costs for a true, real prod-parallel to mitigate that risk. It looks like even the most critical and prestigious companies in the world are doing the former.reply",
      "> Either play it by ear, or literally double your cloud costs for a true, real prod-parallel to mitigate that risk.There's also the problem that doubling your cloud footprint to reduce the risk of a single point of failure introduces new risks: more configuration to break, new modes of failure when both infrastructures are accidentally live and processing traffic, etc.Back when companies typically ran their own datacenters (or otherwise heavily relied on physical devices), I was very skeptical about redundant switches, fearing the redundant hardware would cause more problems than it solved.reply",
      "Why should they? Honestly most of what we do simply does not matter that much. 99.9% uptime is fine in 99.999% of cases.reply",
      "This is true. But unfortunately the exact same process is used even for critical stuff (the crowdstrike thing for example). Maybe there needs to be a separate swe process for those things as well, just like there is for aviation. This means not using the same dev tooling, which is a lot of effort.reply",
      "To agree with the comments it seems likely it's money which has begun to result in a slow \"un-learning how to build systems that stay up even when the inevitable bug or failure shows up.\"reply"
    ],
    "link": "https://www.githubstatus.com/incidents/5q7nmlxz30sk",
    "first_paragraph": "Resend OTP in:  seconds \n                    Didn't receive the OTP?\n                    Resend OTP \nResend OTP in: 30 seconds \n                      Didn't receive the OTP?\n                      Resend OTP \nThe URL we should send the webhooks toWe'll send you email if your endpoint failsGet tips, technical guides, and best practices. Twice a month. Right in your\r\n          inbox.\n          Subscribe to updates for Git operation failures via email and/or text message. You'll receive email notifications when incidents are updated, and text message notifications whenever GitHub creates or resolves an incident.\n        "
  },
  {
    "title": "Cloudflare Global Network experiencing issues (cloudflarestatus.com)",
    "points": 2307,
    "submitter": "imdsm",
    "submit_time": "2025-11-18T11:35:10 1763465710",
    "num_comments": 1591,
    "comments_url": "https://news.ycombinator.com/item?id=45963780",
    "comments": [
      "If anyone needs commands for turning off the CF proxy for their domains and happens to have a Cloudflare API token.First you can grab the zone ID via:    curl -X GET \"https://api.cloudflare.com/client/v4/zones\" -H \"Authorization: Bearer $API_TOKEN\" -H \"Content-Type: application/json\" | jq -r '.result[] | \"\\(.id) \\(.name)\"'\n\nAnd a list of DNS records using:    curl -X GET \"https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records\" -H \"Authorization: Bearer $API_TOKEN\" -H \"Content-Type: application/json\"\n\nEach DNS record will have an ID associated. Finally patch the relevant records:    curl -X PATCH \"https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records/$RECORD_ID\" -H \"Authorization: Bearer $API_TOKEN\" -H \"Content-Type: application/json\" --data '{\"proxied\":false}'\n\nCopying from a sibling comment - some warnings:- SSL/TLS: You will likely lose your Cloudflare-provided SSL certificate. Your site will only work if your origin server has its own valid certificate.- Security & Performance: You will lose the performance benefits (caching, minification, global edge network) and security protections (DDoS mitigation, WAF) that Cloudflare provides.- This will also reveal your backend internal IP addresses. Anyone can find permanent logs of public IP addresses used by even obscure domain names, so potential adversaries don't necessarily have to be paying attention at the exact right time to find it.reply",
      "Also, for anyone who only has an old global API key lying around instead of the more recent tokens, you can set:  -H \"X-Auth-Email: $EMAIL_ADDRESS\" -H \"X-Auth-Key: $API_KEY\"\n\ninstead of the Bearer token header.Edit: and in case you're like me and thought it would be clever to block all non-Cloudflare traffic hitting your origin... remember to disable that.reply",
      "This is exactly what we've decided we should do next time. Unfortunately we didn't generate an API token so we are sitting twiddling our thumbs.Edit: seems like we are back online!reply",
      "Took me ~30 minutes but eventually I was able to log in, get past the 2FA screen and change a DNS record.I surely missed a valid API token today.reply",
      "I'm still trying.Still can't load the Turnstile JS :-/reply",
      "Turnstile is back up (for now). Go refresh. I just managed to make an API key and turn off proxied DNS.reply",
      "install tweak chrome extension and mitm yourself and force the js to load from somewhere elsereply",
      "Im able to generate keys right now through warp. Login takes forever but it is working.reply",
      "Awesome! I did it via the Terraform provider, but for anyone else without access to the dashboard this is great. Thank you!reply",
      "Good advice!And no need for -X GET to make a GET request with curl, it is the default HTTP method if you don\u2019t send any content.If you do send content with say -d curl will do a POST request, so no need for -X then either.For PATCH though, it is the right curl option.reply"
    ],
    "link": "https://www.cloudflarestatus.com/incidents/8gmgl950y3h7",
    "first_paragraph": "\n          Subscribe to updates for Cloudflare Global Network experiencing issues via email and/or text message. You'll receive email notifications when incidents are updated, and text message notifications whenever Cloudflare creates or resolves an incident.\n        "
  },
  {
    "title": "I am stepping down as the CEO of Mastodon (joinmastodon.org)",
    "points": 322,
    "submitter": "Tomte",
    "submit_time": "2025-11-18T18:13:30 1763489610",
    "num_comments": 243,
    "comments_url": "https://news.ycombinator.com/item?id=45969909",
    "comments": [
      "This is an indescribably devastating loss for a project that, whatever its imperfections, can fairly lay claim to the most intellectually consistent and sincere adherence to FOSS, privacy, and decentralization of any major social media project. Eugene has proven a spectacular and indispensable developer, and I don't know that Mastodon has the ability to move on without him. I want to praise Eugen but the uncomfortable truth is I think Mastodon as a project may not recover from losing him. Though I hope to be proven wrong.reply",
      "Even Stallman signed up. Let that sink in.reply",
      "I met Stallman once when he was pamphleting outside a beloved book store because they were using meetup to organize a book signing.  The guy is consistent.reply",
      "I used to make so much fun of RMS in the late 90s. Then I met him and my entire view on that man changed. He absolutely is doing the world a favor and it would benefit everyone to think long and hard what a world would be like without GNU todayreply",
      "I think Mastodon will survive. Other fedi projects (eg. Pleroma-fe) have been through all nine circles of FOSS hell and somehow still ship usable builds to a sizable community.Eugen's presence is felt and appreciated in the community, but I can also understand why he stepped down. It's hard to represent so many people who don't always agree with you. I think back to Jack Dorsey's final days at Twitter with the NFT profile pictures and crypto tickers - he truly did not understand that his leadership had passed it's prime. The honorable thing for him to do was pass on control to someone responsible, but instead he spent his final days polarizing Twitter and guaranteeing it's own critical insolvency.Eugen took the honorable route - I hope he remains vocal and influential in the community. It sounds like he knows himself extremely well and I applaud his honesty about the temptation for ego to ruin big projects. Most of us can't imagine the pressure in his shoes.reply",
      "I would love to love Mastodon, but discoverability was so incredibly difficult that I just gave up. I can find people and topics so much more easily on Bluesky. The starter packs are nice too.Does Mastodon have starter packs (lists of people to follow posting on a particular subject area, which ideally you could just click once and follow everyone)?reply",
      "There is an open proposal to implement it without some of Bluesky's downsides.https://github.com/mastodon/featured_collectionsreply",
      "The main downside being the ability to opt out of starter packs (they can be a source of low quality interactions).reply",
      "I have this impression too, I open Mastodon and the timeline usually doesn\u2019t have many interesting things.reply",
      "This used to be the case for me too, but hasn't now for years. I've followed plenty of people that migrated during the big exoduses from Twitter and then for a while started liberally following people whose reposts I liked and now the timeline is oozing with life ever since.reply"
    ],
    "link": "https://blog.joinmastodon.org/2025/11/my-next-chapter-with-mastodon/",
    "first_paragraph": "\nEugen RochkoStrategy & Product Advisor, FounderAfter nearly 10 years, I am stepping down as the CEO of Mastodon and transferring my ownership of the trademark and other assets to the Mastodon non-profit. Over the course of my time at Mastodon, I have centered myself less and less in our outward communications, and to some degree, this is the culmination of that trend. Mastodon is bigger than me, and though the technology we develop on is itself decentralized\u2014with heaps of alternative fediverse projects demonstrating that participation in this ecosystem is possible without our involvement\u2014it benefits our community to ensure that the project itself which so many people have come to love and depend on remains true to its values. There are too many examples of founder egos sabotaging thriving communities, and while I\u2019d like to think myself an exception, I understand why people would prefer better guardrails.But it would be uncouth for me to pretend that there isn\u2019t some self-interest invo"
  },
  {
    "title": "OrthoRoute \u2013 GPU-accelerated autorouting for KiCad (bbenchoff.github.io)",
    "points": 107,
    "submitter": "wanderingjew",
    "submit_time": "2025-11-18T18:54:54 1763492094",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=45970391",
    "comments": [
      "I would love to know the application of this ludicrous PCB, and I'd be even more interested to see the quote pricereply",
      "It would be interesting to know if the finished board would work at all, given there must be some non-zero failure rate for each via.reply",
      "Hey, guy who made this here. This probably deserves a little explanation. First off, I'd like to tell you I'm really, really unemployed, and have the freedom to do some cool stuff. So  I came up with a project idea. This is only a small part of a project I'm working on, but you'll see where this is going.I was inspired by this video: https://www.youtube.com/watch?v=HRfbQJ6FdF0 from bitluni that's a cluster of $0.10-0.20 RISC-V microcontrollers. For ten or twenty cents, these have a lot of GPIOs compared to other extremely low-cost microcontrollers. 18 GPIOs on the CH32V006F4U6. This got me thinking, what if I built a cluster of these chips. Basically re-doing bitluni's build.But then I started thinking, at ten cents a chip, you could scale this to thousands. But how do you connect them? That problem was already solved in the 80s, with the Connection Machine. The basic idea here is to get 2^(whatever) chips, and connect them so each chip connects to (whatever) many other chips. The Connection Machine sold this as a hypercube, but it's better described as a hamming-distance-one graph or something.So I started building that. I did the LEDs first, just to get a handle on thousands of parts: https://x.com/ViolenceWorks/status/1987596162954903808 and started laying out the 'cards' of this thing. With a 'hypecube topology' you can split up the cube into different parts, so this thing is made of sixteen cards (2^4), with 256 chips on each card (2^8), meaning 4096 (2^12) chips in total. This requires a backplane. A huge backplane with 8196 nets. Non-trivial stuff.So the real stumbling block for this project is the backplane, and this is basically the only way I could figure out how to build it; write an autorouter. It's a fun project that really couldn't have been done before the launch of KiCad 9; the new IPC API was a necessity to make this a reality. After that it's just some CuPy because of sparse matrices and a few blockers trying to adapt PathFinder to circuit boards.Last week I finished up the 'cloud routing' functionality and was able to run this on an A100 80GB instance on Vast.io; the board wouldn't fit in my 16GB 5080 I used for testing. That instance took 41 hours to route the board, and now I have the result back on my main battlestation ready for the bit of hand routing that's still needed. No, it's not perfect, but it's an autorouter. It's never going to be perfect.This was a fun project but what I really should have been doing the past three months or so is grinding leetcode. It's hard out there, and given that I've been rejected from every technician job I've applied to, I don't think this project is going to help me. Either way, this project.... is not useful. There's probably a dozen engineers out there in the world that this _could_ help.So, while it's working for my weird project, this is really not what hiring managers want to see.reply",
      "Author:  Thanks for taking the time to reply.I read the write-up with a LOT of interest, this is really amazing work, there's not a lot of good options for auto-routing with open-source PCB tools (i.e. KiCad).  I have also used the other autorouter you mentioned for \"low-complexity\" boards in KiCad and it helped do the job but was painful.In my career I've also used the autorouter built into the \"high-end\" PCB tools and  they could handle the complexity of boards you outlined WITHOUT needing a massive GPU, but they also paid people to improve this stuff over 15-to-20-years and development happened when single-core computers with limited RAM were the norm.On the technical side, somewhat more recent FPGA 'placement' algorithms used a simulated annealing algorithm, while what you didn't isn't about placement, that approach could posisbly help with 'net cross-over reduction' type of passes, and maybe help with designs where you can do port swap / pin swap.I'm amused you made a RISC-V array with discrete parts -- I'm sure you considered using an FPGA?  Jan Gray has done > 1000+ RISC-V cores (https://fpga.org/grvi-phalanx/) in \"older\" Xilinx FPGAs.If you're trying to emulate Thinking Machines / CM-x or anything else, frankly I think a \"mondo\" FPGA is still the way to go.Job-wise:  A suggestion might be to reach out to the guys at AllSpice ( allspice.io ) who make revision control software for Altium and possibly KiCad. The work you did to enable IPC, etc seems like exactly the type of skillset these guys might need (contractor, maybe full-time?) to interoperate with KiCad.If I see anything that might be up your alley I'd also reach out.  I'm not in a position to hire anyone and while \"some companies\" may not be impressed by what you did, the right organization WOULD be.I share your sentiment that the likes of \"modern\" companies like Apple, MSFT, etc the hiring process is really taylored to \"I want a guy who can do X\" and rarely \"I want a guy who's shown he can learn Y and Z so he can certainly do X\".reply",
      "> On the technical side, somewhat more recent FPGA 'placement' algorithms used a simulated annealing algorithm, while what you didn't isn't about placement, that approach could posisbly help with 'net cross-over reduction' type of passes, and maybe help with designs where you can do port swap / pin swap.Yeah, that was the first step in creating the netlist for the backplane. Simulated annealing on the 8196 nets. TO BE FAIR, this would be a lot easier to route if I didn't explicitly want each of the 16 cards to be identical, but I think that's the most cost-effective way to do it.As far as an FPGA.... I don't know if I see the point. The nodes in the original CM-1 were basically _only_ ALUs. Very little processing power. The CM-5 was a little better, but this entire thing is batshit crazy. I might as well go for four thousand individually programmable cores. Like, what even is a MISD computer? I have no idea, so lets build one. See what it can actually do.reply",
      "If you're open to technical feedback your last comment, I've worked on these kinds of systems, have architected and built things even far \"weirder\" and these products have shipped and out in the real world, in silicon, in FPGAs and things between.The reason an FPGA is a more suitable platform is you can translate \"physical effort of making PCBs\" into \"creating a design in an infinitely re-programmable platform\" and change your design as needed to your hearts content.In fact, the original design of RISC-V included a bus called 'TileLink' to enable 'Many core' arrays of RISC-V processors.Translation:  You can pare-down open-source RISC-V cores and use TileLink and emulate CM or build something more complex as you see fit since that was built into the original open-source RISC-V specs.FPGAs are their own joy and pain for sure and it's not as \"cool\" to re-program a blackbox on a PCB as it might be to make your own thing, so all depends on your goals.reply",
      "A4-sized, 32 layers.. I'd guess something around 1500\u20ac at JLC.reply",
      "Me too. I can't imagine a backplane where the connections would be so irregular as to require bringing out such big guns.reply",
      "Being that engineer in a   PCB Fab in China who has to punch all the vias by handheart attackreply",
      "Are these pushed by hand? I'd believe if you said it happens in the UK. Fabs are still stuck in the 90s.reply"
    ],
    "link": "https://bbenchoff.github.io/pages/OrthoRoute.html",
    "first_paragraph": "OrthoRoute is a GPU-accelerated PCB autorouter that uses a Manhattan lattice and the PathFinder algorithm to route high-density boards. Built as a KiCad plugin using the IPC API, it handles complex designs with thousands of nets that make traditional push-and-shove routers give up.Never trust the autorouter, but at least this one is fast.This is a project born out of necessity. Another thing I was working on needed an enormous backplane. A PCB with sixteen connectors, with 1,100 pins on each connector. That\u2019s 17,600 individual pads, and 8,192 airwires that need to be routed. Here, just take a look:Look at that shit. Hand routing this would take months. For a laugh, I tried FreeRouting, the KiCad autorouter plugin, and it routed 4% of the traces in seven hours. If that trend held, which it wouldn\u2019t, that would be a month of autorouting. And it probably wouldn\u2019t work in the end. I had a few options, all of which would take far too longWhen confronted with a task that will take months, al"
  },
  {
    "title": "Bild AI (YC W25) is hiring \u2013\u00a0Make housing affordable (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-11-18T21:29:37 1763501377",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/bild-ai/jobs/m2ilR5L-founding-engineer-applied-ai",
    "first_paragraph": "AI that understands construction blueprintsPuneet and I (Roop) founded Bild AI to tackle the mess that is blueprint reading, cost estimation, and permit applications in construction. It's a tough technical problem that requires the newest CV and AI approaches, and we\u2019re impact-driven to make it more efficient to build more houses, hospitals, and schools. Featured on Business Insider.Bild AI is an early-stage startup with a ton of really difficult technical challenges to solve. We're building blueprint understanding with a model-garden approach, so there is a lots of ground to break. We raised from the top VCs in the world before demo day and have a customer-obsessed approach to product development.\u00a9 2025 Y Combinator"
  },
  {
    "title": "What I learned about creativity from a man painting on a treadmill (2024) (quinnmaclay.com)",
    "points": 26,
    "submitter": "8organicbits",
    "submit_time": "2025-11-14T12:31:15 1763123475",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=45926141",
    "comments": [
      "when reading that I couldn't help but think of how you could learn from failure from this early apple story:https://www.folklore.org/Make_a_Mess,_Clean_it_Up!.html(maybe not the same as not worrying about failure)reply",
      "This is great.  One of the things I say to my wife all the time, which is taken from a Kurt Vonnegut quote, is that \"I don't have to be good at my hobbies\".reply",
      "You could head for Chesterton as well: \"anything worth doing is worth doing poorly.\"reply"
    ],
    "link": "https://quinnmaclay.com/texts/lets-paint",
    "first_paragraph": "13th April, 2024For a long time, the fear of failure was like my arch nemesis. Looking back over my life, I can think of countless occasions when I\nchose not to do something, not because I didn\u2019t want to, but because I\nworried about what could go wrong. I worried I might make a fool of\nmyself or waste my time on a fruitless endeavor.Thankfully, as I\u2019ve gotten older, I\u2019m increasingly less concerned\nwith what others think and more willing to give things a go for their\nown sake. But that fear of failure hasn\u2019t gone away. Not entirely. And\nnowhere does this fear make itself more known than in resistance to acts\nof creativity.Sometimes, such fear is warranted \u2014 useful even. After all, what is\nit but self-doubt that pushes us to perform better or think more\ncarefully? But the result can also be stifling. In some cases, even\ncrippling. Anyone who has ever suffered writer\u2019s block will know what I\nmean.When these blocks occur, we are often told that it can be helpful to\nhave inspiration at hand"
  },
  {
    "title": "Monotype font licencing shake-down (insanityworks.org)",
    "points": 88,
    "submitter": "evolve2k",
    "submit_time": "2025-11-18T22:42:45 1763505765",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=45973261",
    "comments": [
      "This reminds me of the Blue Jeans Cable / Monster Cable shakedown nonsense.https://www.bluejeanscable.com/legal/mcp/index.htmI wish I could find the original writeup from Blue Jeans, it was frickin' magnificent.reply",
      "It feels like the recipient company did an awful lot of work in response to what was at best a fishing expedition. A serious complaint about licensing that demanded a real response would have been sent by post. It's not clear to me that scattershot LinkedIn messages deserve any response at all. The fact that the initial message lies about trying to contact him another way is another check in the \"ignore this completely\" column.The same way that I wouldn't bother to fact-check a spam phone caller, why give any credence to this kind of thing?reply",
      "I wonder if it\u2019s possible to demand vendors send billing agreements before running an audit like this:We\u2019re reasonably sure your report is incorrect, and it doesn\u2019t contain compelling evidence to back up its claims.Our standard auditing fee for requests like this is $10,000, pre-paid to an escrow account and refundable if we find the use of an unlicensed font.Or something.  Not a lawyer.reply",
      "Do these tactics ever work out for companies in the long term?We've had a couple cases where someone installed something they shouldn't have and we got threatening emails from the sellers who somehow caught wind.It's always resulted on our side with a total corporate ban on using anything from that company, even things that are otherwise OK / open source.For instance at a previous company I worked, Oracle came calling for \"VirtualBox Tools\" trying to charge us some asinine amount because like one user had it installed and this resulted in a Total corporate ban on VirtualBox.I've seen this at a couple companies and can't imagine we're alone in this. You're trading long-term business for short-term gains.reply",
      "Paying for fonts is something I will never understand, I have a perfect vision but I'm nearly blind to fonts it makes nearly no difference to me (except for windings)reply",
      "Wingdings isn't really a \"font\" in the same way that Times New Roman is a \"font\".  Wingdings and and Webdings were basically proto-emojis, a vestige of the old \"dingbats\" publishers would put at the top of chapter pages to make them look nice.https://youtu.be/JdKV1L1DJHcreply",
      "\u201cBut before responding, the digital team would do their own investigation into the fonts we use and the licences we own so we could verify everything was in compliance. [\u2026] messaged a dozen or so more people from different parts of the business, hoping to hook just one person who would reply to the scary message they were sending.\u201dPiece of advice for the future: if you receive a message like this, and don\u2019t want the sender to reach out to other people in your organization \u2014 acknowledge the message.reply",
      "\u2026I would think the appropriate behavior would be for the security team to send an announcement stating they've seen an uptick of phishing emails, with an example screenshot, and to please not respond to phishers.reply",
      "The business has no contract with Monotype, has conducted no business with Monotype, and has also (as they double checked) committed no infringement against Monotype. In short, the Monotype sales rep has no entitlement to any of the business' time.reply",
      "I thought the standard procedure is to forward the message to the appropriate department. Never give any acknowledgement.reply"
    ],
    "link": "https://www.insanityworks.org/randomtangent/2025/11/14/monotype-font-licencing-shake-down",
    "first_paragraph": "Random tangent (blog)\u200bAmeel Khan's personal blog. This is a blog about life, technology, photography, typography, the internet, science, feminism, books, film, music, and whatever other random stuff I come across or happen to be interested in today.tl;dr Don\u2019t try to shake-down a typography nerd with your dubious, automated claims about his employer using unlicensed fonts.  It started with a LinkedIn InMail message (sanitised to protect privacy):  Subject: [Urgent] Font Software Licensing Review Hi AmeelI hope you\u2019re doing well.I\u2019m [NAME] from Monotype and have been trying to reach you at [WORK EMAIL ADDRESS], but I\u2019m unsure if my emails have been received.Our team has identified Monotype font software embedded in the websites/apps of [YOUR COMPANY], but we couldn\u2019t locate the corresponding licenses in our database.Would you be able to share the correct email address so I can provide more details and documentation? Alternatively, you\u2019re welcome to reach out to me directly at [SENDER\u2019S "
  },
  {
    "title": "Chuck Moore: Colorforth has stopped working [video] (youtube.com)",
    "points": 66,
    "submitter": "netten",
    "submit_time": "2025-11-17T12:26:45 1763382405",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=45953001",
    "comments": [
      "Posting a quick TL;DW. A minute into the video Chuck Moore says that Windows updates (on 11 and 10) have caused colorForth to crash, with Chuck thinking it's a graphical problem. I may comment more, but I wanted to post this because I don't see it mentioned as a youtube comment.reply",
      "Did Microsoft seriously deprecate BitBlt and 2D draw calls?If so, it seems as if Windows is undergoing a Waylandization. \"Yeah, we went ahead and removed those because they're legacy. Modern rendering pipelines don't work that way anymore.\" I don't WANT a rendering pipeline! I want a surface, and to make calls to scribble on it! That's it!reply",
      "> Did Microsoft seriously deprecate BitBlt and 2D draw calls?Very unlikely. Far too many applications depend on those things. It's more likely that they accidentally changed something subtle that happened to break colorForth.reply",
      "I'm guessing a lot of the legacy stuff that still uses it also depends on some other things they wanted to change too?reply",
      "I could've sworn I saw something in the last month or two about BITBLT or DirectX changes on Windows.reply",
      "It wouldn't surprise me to find that Windows is now flagging and quarantining unsigned, unfamiliar executables that it catches making these draw calls or really any direct Win32 calls. Microsoft, and in particular Windows Defender which you can't really turn off anymore, has gotten pretty aggressive about blocking software for \"security purposes\".reply",
      "Are we going from \"the only stable ABI on Linux is Wine\", to \"the only stable ABI is Wine\"?(Especially now that .NET Framework was donated to Wine...)reply",
      "If there is, does anyone have any info on this?reply",
      "I wonder how well Proton would work for it...reply",
      "It looks like colorForth runs in qemu or bochs according to documentation, so Proton/wine wouldn't be required.reply"
    ],
    "link": "https://www.youtube.com/watch?v=MvkGBWXb2oQ#t=22",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: A subtly obvious e-paper room air monitor (nicolin-dora.ch)",
    "points": 33,
    "submitter": "nomarv",
    "submit_time": "2025-11-18T07:14:03 1763450043",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=45962266",
    "comments": [
      "There are lots of commercial gadgets like that.Most of the affordable CO2 meters are relative, not absolute. They set their 400 PPM level based on the lowest value they ever see. That's usually OK, but it's not good enough for places with permanent people occupancy, such as nursing homes.\nAbsolute detectors with NIST calibration are available but around US$500.[1][1] https://www.forensicsdetectors.com/products/carbon-dioxide-d...reply",
      "> The air gets \"stale\": humidity often rises above 60 %,Fun - I have the opposite problem, humidity goes down to almost nothing and we have to use humidifiers to keep it around 40% to avoid horrible nose/throat/skin dryness.Forced air hvac is probably why.reply",
      "Cool project. I've done something similar using defunct crypto Awair AQI sensors tied into Home Assistant. They have an LED panel in the front that can show overall AQI or any of the pollutants they track:\nhttps://www.getawair.com/products/elementThe sky is the limit as to what you can do with Home Assistant automations.It's surprising how quickly a room with a closed door and one person can go from ~ambient CO2 levels to 1000ppm+.reply",
      "were you able to repurpose your Awair device? Mine has sat bricked since they discontinued supporting it. I'd love to use it for anything if you're able to point to any docs on how to make it useful again?reply",
      "You should find a friend's garage, shop or hackerspace with a brake, and metal cutting tools.I have a similar monitor for equipment metrics, and the cardboard design is similar to the stand I made out of metal. I powder coated the metal light beige and it looks professional.reply",
      "I was looking for cheap co2 sensors that can be deployed on RPi but I guess they all feel expensive.reply",
      "This is cool! How does it compare to e.g. an Aranet4? I got one to detect when there is a high risk of COVID-19 aerosols lingering and generally carry it in my bag. That way I can check the reading using my phone.reply",
      "Germans sure love their L\u00fcftenreply"
    ],
    "link": "https://www.nicolin-dora.ch/blog/en-epaper-room-air-monitor-part-1/",
    "first_paragraph": "16.11.2025\nNote: This English version was created with the help of automated translation and I am not a native English speaker. If you notice any unclear or incorrect wording, feel free to point it out in the comments or reach out via e-mail so I can improve the text.\nWhen windows stay closed for longer periods, the air indoors quickly becomes \u201cstale\u201d. Both humidity and the CO\u2082 level rise. CO\u2082 is measured in parts per million (ppm) \u2013 essentially \u201cCO\u2082 molecules per one million air molecules\u201d. This makes it easy to compare how much more CO\u2082 is present indoors compared to fresh outdoor air. At elevated levels, the following issues can occur:Source: ASHRAE (2022). ASHRAE Position Document on Indoor Carbon Dioxide \u2013 PDFHumidity that is too high or too low is also unhealthy. If it is too low, this leads to dry mucous membranes, irritated skin and a higher susceptibility to infections. If it is too high, it promotes mould growth, dust mites and other bio-organisms that can contribute to respi"
  },
  {
    "title": "Solving a million-step LLM task with zero errors (arxiv.org)",
    "points": 125,
    "submitter": "Anon84",
    "submit_time": "2025-11-18T16:26:28 1763483188",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=45968362",
    "comments": [
      "Nice!Briefly, the idea is recursively to decompose tasks into the simplest possible steps, recursively call (relatively small) LLMs as agents to execute one step at a time, and using a clever voting scheme to choose how to execute each step. The authors use this technique to get a relatively small LLM to solve Towers of Hanoi with 20 rings (1M steps). All of it using natural language.The most obvious question is whether other tasks, more interesting -- less \"rote\" -- than Towers of Hanoi, can similarly be recursively decomposed into simple steps. I'm not sure that's always possible.reply",
      "This works because a problem could be broken down to a prompt which rarely hallucinates.Most real world prompts can't be reduced to something so consistent and reliable.Their key finding was that the number of votes grows linearly with number of prompts you are trying to chain.However the issue is that the number of votes you need will grow exponentially with hallucination rate.reply",
      "> into the simplest possible steps, recursively call (relatively small) LLMs as agents to execute one step at a time, and using a clever voting scheme to choose how to execute each step.It's like humans! Everything old is new again :)reply",
      "Why not? That's basically how NASA manages large projects.reply",
      "One issue I often run into with this stuff is the tightly coupled nature of things in the real world. I\u2019ll fashion an example:Let\u2019s say you break a job down into 3 tasks: A, B and C. Doing one of those tasks is too much for an LLM to accomplish in one turn (this is something you learn intuitively through experience), but an LLM could break each task into 3 subtasks. So you do that, and start by having the LLM break task A into subtasks A1, A2 and A3. And B into B1, B2 and B3. But when you break down task C, the LLM (which needs to start with a fresh context each time since each \u201cbreakdown\u201d uses 60-70% of the context) doesn\u2019t know the details of task A, and thus writes a prompt for C1 that is incompatible with \u201cthe world where A1 has been completed\u201d.This sort of \u201ctunnel vision\u201d is currently an issue with scaling 2025 agents. As useful context lengths get longer it\u2019ll get easier, but figuring out how to pack exactly the right info into a context is tough, especially when the tool you\u2019d reach for to automate it (LLMs) are the same tool that suffers from these context limitations.None of this means big things aren\u2019t possible, just that the fussyness of these systems increases with the size of the task, and that fussyness leads to more requirements of \u201chuman review\u201d in the process.reply",
      "I've been experimenting with this with a custom /plan slash command for claude code, available here: https://github.com/atomCAD/agentsPlanning is definitely still something that requires a human in the loop, but I have been able to avoid the problem you are describing. It does require some trickery (not yet represented in the /plan command) when the overall plan exceeds reasonable context window size (~20k tokens). You basically have to start having the AI consider combinatorially many batches of the plan compared with each other, to discover and correct these dependency issues.reply",
      "Reasoning by analogy is great for intuition, but doesn\u2019t guarantee real results hold. Consider \u201cvoltage is like water pressure in pipes, so if there\u2019s a cut in my wire\u2019s insulation, the device won\u2019t get enough voltage\u201d \u2014 clearly this is not true, even though it relies on an analogy that\u2019s generally useful.reply",
      "If air was highly conductive that analogy would totally hold.\"If there\u2019s a cut in my wire\u2019s insulation, the device won\u2019t get enough voltage\" doesn't follow from: \"voltage is like water pressure in pipes\"So I don't really get your point.reply",
      "I really like that analogy, thank you for it. Also applies to \u201cit\u2019s overvoltage, so I just need to poke a little hole in it to let the excess bleed out\u201d\u2026reply",
      "That one can work, briefly, depending on how conductive your tool is.reply"
    ],
    "link": "https://arxiv.org/abs/2511.09030",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n"
  },
  {
    "title": "Show HN: RowboatX \u2013 open-source Claude Code for everyday automations (github.com/rowboatlabs)",
    "points": 53,
    "submitter": "segmenta",
    "submit_time": "2025-11-18T18:50:00 1763491800",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=45970338",
    "comments": [
      "One of the main reasons for me for sticking with Claude Code (also for non-coding tasks, I think the name is a misnomer) is the fixed price plan. Pretty much any other open-source alternative requires API key, which means that as soon as I start using it _for real_, I'll start overpaying and/or hitting limits too fast. At least that was my initial experience with API from OpenAI/Claude/Gemini.Am I biased/wrong here?reply",
      "Yep, this is a fair take. Token usage shoots up fast when you do agentic stuff for coding. I too end up doing the same thing.But for most background automations your might actually run, the token usage is way lower and probably an order of magnitude cheaper than agentic coding. And a lot of these tasks run well on cheaper models or even open-source ones.So I don't think you are wrong at all. It is just that I believe the expensive token pattern mostly comes from coding-style workloads.reply",
      "I don't doubt you, but it would be interesting to see some token usage measurements for various tasks like you describe.reply",
      "For example, the NotebookLM-style podcast generator workflow in our demo uses around 3k tokens end to end. Using Claude Sonnet 4.5\u2019s blended rate (about $4.5 per million tokens for typical input/output mix), you can run this every day for roughly eight months for a bit over three dollars. Most non-coding automations end up in this same low range.reply",
      "I'm increasingly seeing code-adjacent people who are using coding agents for non-coding things because the tooling support it better, and the agents work really well.It's an interesting area, and glad to see someone working on this.The other program in the space that I'm aware of is Block's Goose.reply",
      "Yep, totally agree. We actually had an earlier web version, and the big learning was that without access to code-related tools the agent feels pretty limited. That pushed us toward a CLI where it can use the full shell and behave more like a real worker.Really appreciate the support and the Goose pointer. Would love to hear what you think of RowboatX once you try it.reply",
      "Can this use local LLMs?reply",
      "Yes - you can use local LLMs through LiteLLM and Ollama. Would you like us to support anything else?reply",
      "LM Studio?reply",
      "Yes, because LM Studio is openai-compatible. When you run rowboatx the first time, it creates a ~/.rowboat/config/models.json. You can then configure LM Studio there. Here is an example:\nhttps://gist.github.com/ramnique/9e4b783f41cecf0fcc8d92b277d...reply"
    ],
    "link": "https://github.com/rowboatlabs/rowboat",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        AI-powered CLI for background agents\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInspired by Claude Code, RowboatX brings the same shell-native power to background automations.Set your LLM API key. Supports OpenAI, Anthropic, Gemini, OpenRouter, LiteLLM, Ollama, and more.Install RowboatX$ rowboatx$ rowboatx$ rowboatxYou can configure your models in ~/.rowboat/config/models.jsonTo use Rowboat Classic UI (not RowboatX), refer to Classic.\n        AI-powered CLI for background agents\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page. There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page."
  }
]