[
  {
    "title": "Convert photos to Atkinson dithering (gazs.github.io)",
    "points": 214,
    "submitter": "nvahalik",
    "submit_time": "2025-06-07T20:33:18 1749328398",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=44212446",
    "comments": [
      "Still my favorite B&W dither algorithm.The university had a B&W flatbed scanner attached to a Mac running ... a Hypercard stack? that allowed you to scan an image and get a B&W image.A clipart book I picked up from the college bookstore and a quick scan and I had a \"logo\" for the Mac shareware games I started writing in 1988 or so.At the time I didn't;t realize how really ... nice .. Atkinson's algorithm is. But when, later, I tried dithering with other algos I saw how nice the diffusion was in Bill's code.More recently I was playing with an eInk calendar project and wanted an \"Atkinson-esque\" series of images of the Moon in various phases. So I found a site very like the linked one to Atkinson-dither the moon photos I found [1].[1] see the moon in screenshot: https://github.com/EngineersNeedArt/SystemSix/blob/10f2332b5...\n \nreply",
      "The implementation is excellent, and could be slightly improved by giving a default name and .png extension to the downloaded file, by passing a value to the \"download\" property on the anchor. See https://developer.mozilla.org/en-US/docs/Web/API/HTMLAnchorE...\n \nreply",
      "In his defence, that attribute has been available in browsers since March 2017 according to your link [1], whereas the most recent commit in the repo for the dithering tool was in March 2016 by the looks of it.https://github.com/gazs/canvas-atkinson-ditherHe\u2019s still active on GitHub though, in other repos. Maybe he will accept a pull request? :)[1]: https://developer.mozilla.org/en-US/docs/Web/API/HTMLAnchorE...\n \nreply",
      "Oh, I assumed it had been recently built and probably posted today by its author given the news and the lack of a year in the title. I'll open a PR.edit: I might open a PR. 'CoffeeScript...now there's a name I've not heard in a long time. A long time...'\n \nreply",
      "> CoffeeScriptIt was acceptable in the 2010sIt was acceptable at the time:phttps://www.youtube.com/watch?v=dOV5WXISM24\n \nreply",
      "Nor have I said there is anything wrong with it, only that it's been a long time. So reflexively to equate calling something old with calling it bad seems like a young man's game, but it has been some time since I had close experience of being one of those, also.\n \nreply",
      "It\u2019s a reference to the linked song. One of my favorite songs :D\n \nreply",
      "Don't click the \"as follows\" in the info dialog. Looks like this wasn't updated in a while and since then the link became NSFW.\n \nreply",
      "Here's one I've been working on and off that lets you convert multiple images to MacPaint in a 400k MFS formatted disk image.https://github.com/minorbug/mfsjsI've had this project gathering a light layer of dust in my home directory for a couple months now. I used Gemini Deep Research to help produce the library, and I included the LLM-generated markdown for anyone who wishes to reproduce on other languages, improve upon it, etc.\n \nreply",
      "If you want to do this in Python, there's:https://github.com/tgray/hyperdither\n \nreply"
    ],
    "link": "https://gazs.github.io/canvas-atkinson-dither/",
    "first_paragraph": "\n\nChoose Image...\nFit picture in:\n\n50x50\n320x240\n512x384\n640x480\n800x600\n1024x768\nOther...\n\nSave to Desktop\nThis is an implementation of the classic Macintosh 1-bit filter, as used by Hyperdither and HyperScan originally.It compares every pixel to 50% grey, then changes them to either black or white. The difference between the input and the output is then distributed to the neighbouring pixels as follows (X is the current pixel):The rendered image can be rightclicked-saved. (Due to limitations of the browsers(?) you cannot drag it to the desktop)\n \nThis code uses Canvas, Drag and Drop events, WebWorkers and the FileReader API so you'll need the newest and shiniest browser to try it.This code uses Canvas, Drag and Drop events, WebWorkers and the FileReader API so you'll need the newest and shiniest browser to try it.Atkinson dithering demo appwritten by G\u00e1sp\u00e1r K\u00f6rtesi<gazs@bergengocia.net>"
  },
  {
    "title": "Joining Apple Computer (2018) (folklore.org)",
    "points": 128,
    "submitter": "tosh",
    "submit_time": "2025-06-07T20:32:54 1749328374",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=44212441",
    "comments": [
      "I'm always blown away by the vision behind stuff like HyperCard. It was all about giving non-techies the keys to the kingdom.But looking at today's tech landscape, with its walled gardens and app stores, I can't help but feel we've gone backwards.\n \nreply",
      "What's worse, in context here, is Apple's distinguished primary role in bringing this about.\n \nreply",
      "It's really hard to extract computing from the capitalistic, consumerist cradle within which it was born.Every other human creative practice and media (poetry, theater, writing, music, painting, etc) have existed in a wide variety of cultures, societies, and economic contexts.But computing has never existed outside of the immensely expensive and complex factories & supply chains required to produce computing components; and corporations producing software and selling it to other corporations, or to the large consumer class with disposable income that industrialization created.In that sense the momentum of computing has always been in favor of the corporations manufacturing the computers dictating what can be done with them. We've been lucky to have had a few blips like the free software movement here and there (and the outsized effect they've had on the industry speaks to how much value there is to be found there), but the hard reality that's hard to fight is that if you control the chip factories, you control what can be done with the chips - Apple being the strongest example of this.We're in dire need of movements pushing back against that. To name one, I'm a big fan of the uxn approach, which is to write software for a lightweight virtual machine that can run on the cheap, abundant, less/non locked down chips of yesteryear that will probably still be available and understandable a century from now.\n \nreply",
      "Part of the problem trying to isolate computing is that it's fundamentally material. Even cloud resources are a flimsy abstraction over a more complex business model. That materialism is part of the issue, too. You can't ever escape the churn, bit rot gets your drives and Hetzner doesn't sell a lifetime plan. If you're not computing for the short-term, you're arguably wasting your time.I'm not against the idea of a disasterproof runtime, but you're not \"pushing back\" against the consumerist machine by outlasting it. When high-quality software becomes inaccessible to support some sort of longtermist runtime, low-quality software everywhere sees a rise in popularity.\n \nreply",
      "I totally agree\n \nreply",
      "Apparently we need to be doing more LSD\n \nreply",
      "Surprised he was only at Apple for 12 years. A wild ride, I'm sure.When I moved out to \"the Valley\" in 1995, the apartment I picked out turned out to be right next to General Magic (on Mary Ave.).I knew it as a \"spin off\" of Apple but at the time did not know the luminaries that were there. It was just a cute rabbit in a hat logo \u2014 lit up when I got home late and was turning off to my apartment.\n \nreply",
      "> Inspired by a mind-expanding LSD journey in 1985, I designed the HyperCard authoring system that enabled non-programmers to make their own interactive media.Watching some YouTube about the Beatles and, of course, their LSD trips. More recently the history of Robert Crumb \u2014 on his big acid trip he more or less created a large part of his stable of comic characters.Somewhere along the way, someone said that LSD alters your mind permanently....It caused me to wonder if we'll never get the genius of Beatles music, Crumb art without the artist taking something conscious-altering like LSD. Of course then I have to consider all the artists before LSD was \"invented\" \u2014 the Edvard Munch's, T.S. Eliot's, William Blake's, etc.(Tried acid once in college. That was enough of that.)\n \nreply",
      "Survivorship bias? Plenty of brilliant people smoked tobacco. I didn't think more smoking will produce more brilliance.\n \nreply",
      "Neither does smoking alter your conscioudness in any remarkable way further than irritability or cravings due to whitdrawal symtpomat least acid doesnt make sense to consume daily because it stops having the same effects the more you consume it\n \nreply"
    ],
    "link": "https://www.folklore.org/Joining_Apple_Computer.html",
    "first_paragraph": "40 years ago today, I joined Apple Computer on April 27, 1978. It was a big turning point in my life and I am glad I said \"Yes\"."
  },
  {
    "title": "Bill Atkinson has died (daringfireball.net)",
    "points": 982,
    "submitter": "romanhn",
    "submit_time": "2025-06-07T16:19:58 1749313198",
    "num_comments": 191,
    "comments_url": "https://news.ycombinator.com/item?id=44210606",
    "comments": [
      "When I was on the ColorSync team at Apple we, the engineers, got an invite to his place-in-the-woods one day.I knew who he was at the time, but for some reason I felt I was more or less beholden to conversing only about color-related issues and how they applied to a computer workflow. Having retired, I have been kicking myself for some time not just chatting with him about ... whatever.He was at the time I met him very in to a kind of digital photography. My recollection was that he had a high-end drum scanner and was in fact scanning film negatives (medium format camera?) and then going with a digital workflow from that point on. I remember he was excited about the way that \"darks\" could be captured (with the scanner?). A straight analog workflow would, according to him, cause the darks to roll off (guessing the film was not the culprit then, perhaps the analog printing process).He excitedly showed us on his computer photos he took along the Pacific ocean of large rock outcroppings against the ocean \u2014 pointing out the detail that you could see in the shadow of the rocks. He was putting together a coffee table book of his photos at the time.I have to say that I mused at the time about a wealthy, retired, engineer who throws money at high end photo gear and suddenly thinks they're a photographer. I think I was weighing his \"technical\" approach to photography vs. a strictly artistic one. Although, having learned more about Ansel Adams technical chops, perhaps for the best photographers there is overlap.\n \nreply",
      "> I have been kicking myself for some time not just chatting with him about ... whatever.Maybe I should show some initiative! See, for a little while now I've wanted to just chat with you about whatever.At this moment I'm working on a little research project about the advent of color on the Macintosh, specifically the color picker. Would you be interested in a casual convo that touches on that? If so, I can create a BlueSky account and reach out to you over there. :)https://merveilles.town/deck/@rezmason/114586460712518867\n \nreply",
      "There probably still isn't a good way to get that kind of dynamic range entirely in the digital domain. Oh, I'm sure the shortfall today is smaller, say maybe four or five stops versus probably eight or twelve back then. Nonetheless, I've done enough work in monochrome to recognize an occasional need to work around the same limitations he was, even though very few of my subjects are as demanding.\n \nreply",
      "I wish a good monochrome digital camera didn't cost a small fortune. And I'm too scared to try to remove the Bayer grid from a \"color\" CCD.Seems that, without the color/Bayer thing, you could get an extra stop or two for low-light.I had a crazy notion to make a camera around an astronomical CCD (often monochrome) but they're not cheap either \u2014 at least one with a good pixel count.\n \nreply",
      "You would also remove the microlenses, which increase sensitivity.\n \nreply",
      "I've replaced my D5300's viewfinder focusing screen a couple of times, back before I outgrew the need for focusing aids. I also wouldn't try debayering its sensor! But that sort of thing is what cheap beater bodies off your friendly local camera store's used counter, or eBay, were made for. Pixel count isn't everything, and how better to find out whether the depth of your interest would reward serious investment, than to see whether and how soon it outgrows unserious? Indeed, my own entire interest in photography has developed just so, out of a simple annoyance at having begun to discover what a 2016 phone camera couldn't do.\n \nreply",
      "I like that idea. I should start watching eBay.\n \nreply",
      ":) Color in the computer is a good \u201cwhatever\u201d topic.Sometimes it\u2019s just nice to talk about the progress of humanity. Nothing better than being a part of it, the gears that make the world turn.\n \nreply",
      "Ha ha, but it's also \"talking shop\". I'm sure Bill preferred it to talking about his Quickdraw days.\n \nreply",
      "You always lose something when doing optical printing - you can often gain things too, but its not 1:1.I adore this hybrid workflow, because I can pick how the photo will look, color palate, grain, whatever by picking my film, then I can use digital to fix (most if not all of) the inherent limitations in analog film.Sadly, film is too much of a pain today, photography has long been about composition for me, not cameras or process - I liked film because I got a consistent result, but I can use digital too, and I do today.\n \nreply"
    ],
    "link": "https://daringfireball.net/linked/2025/06/07/bill-atkinson-rip",
    "first_paragraph": "By John\u00a0GruberWorkOS powers authentication and authorization for secure, scalable AI agents.From his family, on Atkinson\u2019s Facebook page:We regret to write that our beloved husband, father, and\nstepfather Bill Atkinson passed away on the night of Thursday,\nJune 5th, 2025, due to pancreatic cancer. He was at home in\nPortola Valley in his bed, surrounded by family. We will miss him\ngreatly, and he will be missed by many of you, too. He was a\nremarkable person, and the world will be forever different because\nhe lived in it. He was fascinated by consciousness, and as he has\npassed on to a different level of consciousness, we wish him a\njourney as meaningful as the one it has been to have him in our\nlives. He is survived by his wife, two daughters, stepson,\nstepdaughter, two brothers, four sisters, and dog, Poppy.One of the great heroes in not just Apple history, but computer history. If you want to cheer yourself up, go to Andy Hertzfeld\u2019s Folklore.org site and (re-)read all the entries ab"
  },
  {
    "title": "Self-Host and Tech Independence: The Joy of Building Your Own (ssp.sh)",
    "points": 129,
    "submitter": "articsputnik",
    "submit_time": "2025-06-07T17:51:51 1749318711",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=44211273",
    "comments": [
      "I self-host most of what I need but I recently faced the ultimate test when my Internet went down intermittently.It raised some interesting questions:- How long can I be productive without the Internet?- What am I missing?The answer for me was I should archive more documentation and NixOS is unusable offline if you do not host a cache (so that is pretty bad).Ultimately I also found out self-hosting most of what I need and being offline really improve my productivity.\n \nreply",
      "Each downtime is an opportunity to learn the weaknesses of your own system.There are certain scenarios you have no control over (upstream problems), but others have contingencies. I enjoy working out these contingencies and determining whether the costs are worth the likelihoods - and even if they're not, that doesn't necessarily mean I won't cater for it.\n \nreply",
      "I find that self hosting \"devdocs\" [1] and having zeal (on linux) [2] solves a lot of these problems with the offline docs.[1] https://github.com/freeCodeCamp/devdocs[2]  https://zealdocs.org/\n \nreply",
      "For offline documentation, I use these in order of preference:\u2022 Info\u00b9 documentation, which I read directly in Emacs. (If you have ever used the terminal-based standalone \u201cinfo\u201d program, please try to forget all about it. Use Emacs to read Info documentation, and preferably use a graphical Emacs instead of a terminal-based one; Info documentation occasionally has images.)\u2022 Gnome Devhelp\u00b2.\u2022 Zeal\u00b3\u2022 RFC archive\u2074 dumps provided by the Debian \u201cdoc-rfc\u201c package\u2075.1. https://www.gnu.org/software/emacs/manual/html_node/info/2. https://wiki.gnome.org/Apps/Devhelp3. https://zealdocs.org/4. https://www.rfc-editor.org/5. https://tracker.debian.org/pkg/doc-rfc\n \nreply",
      "I've taken this as far as I can. I love being disconnected from the internet for extended periods - they're my most productive timesI have a bash alias to use wget to recursively save full websitesyt-dlp will download videos you want to watchKiwix will give you a full offline copy of WikipediaMy email is saved locally. I can queue up drafts offlineSingleFile extension will allow you to save single pages really effectivelyZeal is a great open source documentation browser\n \nreply",
      "Could you share the bash alias? I would love this too.\n \nreply",
      "https://srcb.in/nPU2jIU5CaUnfortunately it doesn't work well on single page apps. Let me know if anyone has a good way of saving those\n \nreply",
      "I get why you want to self host, although I also get why you don\u2019t want.Selfhosting is a pain in the ass, it needs updating docker, things break sometimes, sometimes it\u2019s only you and not anyone else so you\u2019re left alone searching the solution, and even when it works it\u2019s often a bit clunky.I have a extremely limited list of self hosted tool that just work and are saving me time (first one on that list would be firefly) but god knows i wasted quite a bit of my time setting up stuffs that eventually broke and that i just abandoned.Today I\u2019m very happy with paying for stuff if the company is respecting privacy and has descent pricing.\n \nreply",
      "> dockerThere's your problem.  Docker adds indirection on storage, networking, etc., and also makes upgrades difficult as you have to either rebuild the container, or rely on others to do so to get security and other updates.If you stick to things that can be deployed as an upstream OS vendor package, or as a single binary (go-based projects frequently do this), you'll likely have a better time in the long run.\n \nreply",
      "Maybe.  There are pros and cons.  Docker means you can run two+ different things on the same machine and update them separately.  This is sometimes important when one project releases a feature you really want, while a different one just did a major update that broke something you care about.  Running on the OS often means you have to update both.Single binary sometimes works, but means you need more memory and disk space.  (granted much less a concern today than it was back in 1996 when I first started self hosting, but it still can be an issue)\n \nreply"
    ],
    "link": "https://www.ssp.sh/blog/self-host-self-independence/",
    "first_paragraph": "After watching the two PewDiePie videos where he learned about installing Arch (something considered quite hard, even for Linux enthusiasts) and building three products (camera for the dog, weather/drinking/meditation device, and who knows what comes next) based on open-source, 3D-printed parts, I started wondering about building things yourself, self-hosting, and tech independence. Something dear to my heart for a while.If people ask me how they should start writing or how to get a job, I always say to buy a domain first. Secondly, host your own blog website if you have the technical skills (although it\u2019s not so hard anymore). Because all of this compounds over time. Of course, you can start with a ready-made blog and a URL not yours, but if you want to do it long term, I saw many people changing from WordPress to Medium to Substack to Ghost, so what\u2019s next? Over that time, sometimes they didn\u2019t migrate their long-effort blog posts but started new.Every time they had a new domain. To "
  },
  {
    "title": "BorgBackup 2 has no server-side append-only anymore (github.com/borgbackup)",
    "points": 114,
    "submitter": "jaegerma",
    "submit_time": "2025-06-07T18:39:21 1749321561",
    "num_comments": 67,
    "comments_url": "https://news.ycombinator.com/item?id=44211612",
    "comments": [
      "This has been replaced with a permissions feature that still provides both delete and overwrite protections. The difference is the underlying store needs to implement it rather than running a server that understands the permission differences. You can read more about this change here: https://github.com/borgbackup/borg/issues/8823#issuecomment-...\n \nreply",
      "This comment needs to be pinned, alongside what the developers say [0] since the change is very misunderstood.> The \"no-delete\" permission disallows deleting objects as well as overwriting existing objects.[0]: https://github.com/borgbackup/borg/pull/8798#issuecomment-29...\n \nreply",
      "Isn't this \"no-delete permission\" just a made-up mode for testing the borg storage layer while simulating a lack of permissions for deleting and overwriting? In actual deployment, whatever backing store is used must have the access control primitives to implement such a restriction. I don't know how to do this on a posix filesystem, for example. Gemini gave me a convoluted solution that requires the client to change permissions after creating the files.\n \nreply",
      "at first it was implemented to easily test permission restricted storages (can't easily test on all sorts of cloud storages).it was implemented for \"file:\" (which is also used for \"ssh://\" repos) and there are automated tests for how borg behaves on such restricted permissions repos.after the last beta I also added cli flags to \"borg serve\", so it now also can be used via .ssh/authorized_keys more easily.so it can now also be used for practical applications, not just for testing.not for production yet though, borg2 is still in beta.help with testing is very welcome though!\n \nreply",
      "Currently, you can either provide the `BORG_REPO_PERMISSIONS` env var to borg [0] or `--permissions` flag to `borg serve` [1]. You can then enforce this as part of your `authorized_keys` command, for example.[0] https://github.com/borgbackup/borg/blob/3cf8d7cf2f36246ded75...[1] https://github.com/borgbackup/borg/blob/3cf8d7cf2f36246ded75...\n \nreply",
      "Ah, I was searching borgstore for no-delete, but it gets exploded into itemized permissions in borg. Documentation seems to be non-existent, as the only mention seems to be the changelog where it suggests this only exists for testing. But I suppose it's not released yet.\n \nreply",
      "Thanks for that link.\nThat issue somehow didn't come up when I researched the removal of append-only.\nThe only hint I had was the vague \"remove remainders of append-only and quota support\" in the change log without any further information.\n \nreply",
      "For anyone looking to migrate off borg because of this, append-only is available in restic, but only with the rest-server backend:https://github.com/restic/restichttps://github.com/restic/rest-serverwhich has to be started with --append-only. I use this systemd unit:  [Unit]\n  After=network-online.target\n\n  [Install]\n  WantedBy=multi-user.target\n\n  [Service]\n  ExecStart=/usr/local/bin/rest-server --path /mnt/backups --append-only --private-repos\n  WorkingDirectory=/mnt/backups\n  User=restic\n  Restart=on-failure\n  ProtectSystem=strict\n  ReadWritePaths=/mnt/backups\n\nI also use nginx with HTTPS + HTTP authentication in front of it, with a separate username/password combination for each server. This makes rest-server completely inaccessible to the rest of the internet and you don't have to trust it to be properly protected against being hammered by malicious traffic.Been using this for about five years, it saved my bacon a few times, no problems so far.\n \nreply",
      "You can achieve append-only without exposing a rest server provided that 'rclone' can be called on the remote end:  rclone serve restic --stdio\n\nYou add something like this to ~/.ssh/authorized_keys:  restrict,command=\"rclone serve restic --stdio --append-only backups/my-restic-repo\" ssh-rsa ...\n\n... and then run a command like this:  ssh user@rsync.net rclone serve restic --stdio ...\n\nWe just started deploying this on rsync.net servers - which is to say, we maintain an arguments allowlist for every binary you can execute here and we never allowed 'rclone serve' ... but now we do, IFF it is accompanied by --stdio.\n \nreply",
      "If you want to use some object storage instead of local disk, rclone can be a restic server: https://rclone.org/commands/rclone_serve_restic/\n \nreply"
    ],
    "link": "https://github.com/borgbackup/borg/pull/8798",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page. There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\nHave a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.\n  By clicking \u201cSign up for GitHub\u201d, you agree to our terms of service and\n  privacy statement. We\u2019ll occasionally send you account related emails.\n    Already on GitHub?\n    Sign in\n    to your account\n  Some features like append-only repositories rely on a server-side component that enforces them (because that shall only be controllable server-side, not client-side).So, that can only work, if such a server-side component exists, which is the case for borg 1.x ssh: repositories"
  },
  {
    "title": "My experiment living in a tent in Hong Kong's jungle (trebaol.com)",
    "points": 165,
    "submitter": "5mv2",
    "submit_time": "2025-06-07T16:40:09 1749314409",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=44210736",
    "comments": [
      "I really appreciate the level of detail in this post. Not too little. Not too much.It does seem that being in school made this experiment distinctly different from just living in a tent. In a sense, tuition was rent. It paid for showers, electricity, and a living room with air conditioning (the library). It also provided a supportive community. School and even society at large is more inclined to help a poor student than an adult trying to cut rent.I make this observation not to diminish the experiment's value. I am just putting it in context to arrange its utility in my mind.(edit: I can't imagine why this is flagged. It is def life- hacking if not tech hacking.)\n \nreply",
      "> edit: I can't imagine why this is flagged.Flagging seems to be one of the big vulnerabilities of HN.Maybe flaggers should be required to state the reason for flagging, and this reason should be exposed.Flagging means \"no one should even see this on HN\", and random people shouldn't get arrogant or cavalier about swinging around that power.\n \nreply",
      "I have my account set to show flagged comments. A lot of flagged comments are simply some form of \"wrongthink\" but not violating any guidelines. So I've used the function often to \"save\" a flagged thing but it seemed to have stopped working for me at some point. I can only speculate why, but I think I saw some other commenters saying that happens if you unflag too much.. wrongthink. I want to give the site admin the benefit of the doubt though. Maybe it's simply an automated process that notices you unflagged too many things that were flagged by others too much?\n \nreply",
      "I also have that setting, and occasionally vouch for an inexplicably flagged comment I notice.There's definitely wrongthink/ideological flagging and downvoting going on.(On some comments I make, I know when I make it that it's going to get downvoted, because it pushes against an opinion of the kinds of people who will downvote to suppress criticism.  It used to be that criticizing cryptocurrency would get downvotes, but now it's popular to criticize.  I can get reliably downvoted any time that I suggest that adding a fee for some basic public infrastructure (e.g., to drive on street in a city), in a \"market-based\" way, is a handout of the basic public infrastructure to the wealthy.  Also, suggestions that there's still any bias against women, in anything, somewhere, seems to reliably get downvotes, no matter how relevant; I don't know why, but I'd guess it's because the topic has a lot of general angry sentiment, and people who are angry the other direction aren't represented as much on HN.)I'd distinguish wrongthink from something being off-topic and done-to-death or a flamewar magnet.  Maybe one mental exercise test for this is whether the same person would also still downvote as \"topic\" if the opinion of the post/comment were flipped.\n \nreply",
      "It was flagged because it originally had a totally different (and inappropriate) title.\n \nreply",
      "Appreciate the feedback!100%. It's a lot easier when you live next to a Google campus. And it sorts all the menial matters that make a huge difference, like access to washing machines.About the flagging, you seem to have been here for a while, any hint? I get the word usage can comes across as disrespectful now that people mention it, but didn't think a link would get flagged for that.\n \nreply",
      "I have been here for years. Most things that get flagged are extremely objectionable or touch a political nerve.I could see conservatives disliking that it questions capitalism's viability post AI. I could see liberals thinking you are making light of folks experiencing homelessness.I think those are absurd, but with a low vote count, your post may only need a few absurd people to flag you.Naturally, there could be other reasons things get flagged, but I never see them because they disappear too fast.You could always ask @dang to weigh in. He might see something which violates the guidelines.\n \nreply",
      "Makes sense. Thanks for sharing!Looks like it might have to do with the title, or at least the title was changed before it got unflagged. Good learning!\n \nreply",
      "I lived in the woods during my undergrad too: https://medium.com/@caydenpierce4/the-homeless-hippie-cyborg...This was in between two stints living and working in a mobile RV hacker lab: https://www.youtube.com/watch?v=AT1gPmQQkxII'm in SF now and we'd probably be best friends.\n \nreply",
      "The ROI calculation is way too short sighted to be meaningful. To start you are already paying college tuition, and the expectation is to get an education that will help you pay off the loans (and then some). Going a few hundred deeper in the hole every month to have a roof over your head (you know, the most basic requirment for humans after water and food) is a no brainer and will massively increase your education ROI. A couple months of \"homeless man\" cosplay is probably fun and games but start to face the heat, cold, humidity, animals, police, theft, physical danger and more and those As aren't going to remain As for long.\n \nreply"
    ],
    "link": "https://corentin.trebaol.com/Blog/8.+The+Homelessness+Experiment",
    "first_paragraph": ""
  },
  {
    "title": "What was Radiant AI, anyway? (paavo.me)",
    "points": 144,
    "submitter": "paavohtl",
    "submit_time": "2025-06-07T13:22:53 1749302573",
    "num_comments": 91,
    "comments_url": "https://news.ycombinator.com/item?id=44209497",
    "comments": [
      "Regarding debunking the skooma merchant murder anecdote:> The addicts live in a locked cabin, so it\u2019s unlikely for the player to enter it unless they are specifically looking for it.This is overlooking a crucial, obscure, and unintentionally hilarious detail: not all the skooma addicts are in the cabin! Out in the world are two NPCs who make a monthly inter-city trip to the den to get their fix. However, due to a bug where these NPCs are assigned to the wrong faction, they can't actually get through the locked door of the den, so they'll stand outside the door drinking skooma forever, unable to progress to the step of the AI package that would eventually return them home to their usual schedules, unless the player unlocks the door for them. https://en.uesp.net/wiki/Oblivion:Trenus_Duronius\n \nreply",
      "The closest thing we got to the idea of Radiant AI is probably Dwarf Fortress.But entirely goal-driven (and thus unpredictable) game AI systems like this are usually at odds with story-driven gameplay where the outcome needs to be deterministic (or at least \"winnable\") and the player is the hero which the story is built around (while games like Dwarf Fortress don't have a pre-defined story, and also no player character to take care of, and the whole fortress being wiped out because of comically unpredictable events is a large part of the \"fun\").\n \nreply",
      "That was also my thought. How does the world behave 100 hours into the simulation? If half the town residents have managed to get themselves killed by guards and some of the shopkeepers are gone, it's a bad outcome. Complex sims have emergent behaviors that are hard to tune.The other thing is a bit more subtle. It's a big open world and all NPCs need to be active continuously for that sim to work. So you have a big N to squeeze into a tight per frame CPU budget. Also, things like path planning or object interaction only work if some information like object positions and pathfinding maps are kept in memory the whole time for the entire world. This sounds very challenging on a 2005 era PC.\n \nreply",
      "In time since Oblivion we got games like Divinity: Original sin 1/2 where you can kill pretty much every character in the game and it will still be finishable.The essential NPCs could also be flagged essential, or maybe have a variation of that flag where only way given character dies is if say 1/4 of the damage dealt to character is from player (so NPC can't accidentally kill important NPC basically).Also, radiant AI can also just... not run on the plot significant NPCs.Finally, Bethesda games aren't known from main story being the main selling point.\n \nreply",
      "Some of the Ultima games (and I think Morrowind) had a kind of simulated life routine: sleeping, opening shop, visiting family, exploring, etc.\n \nreply",
      "Some games in the Ultima series did, but Morrowind didn't, which is why Radiant AI was developed in the first place. The first chapter of the article is about that.\n \nreply",
      "There's probably some mathematical way to express that... it'd be interesting to look at Todd's mythical \"Radiant Economy\", create a dynamical system model/game-theoretic mode, and try to prove that in the long run everyone doesn't end up broke or a millionaire.\n \nreply",
      "I think Veloren has a sort of dynamic economy where NPCs trade in and consume goods. Well, maybe not NPCs, but at least settlements as a collective, or something like that. I'm unsure of the details, but I remember prices being different between settlements, and prices changing based on local NPC inventories.\n \nreply",
      "> create a dynamical system model/game-theoretic mode, and try to prove that in the long run everyone doesn't end up broke or a millionaire.Simply ask yourself which factors in the real world lead or don't lead (depending on your political stance) to this outcome, and you likely have found the relevant factors that you have to include.\n \nreply",
      "To be fair, the velocity of money can be be significantly higher in a video game, and you're much less likely to have innovations reshuffling the market. It seems inevitable that extremal states would be more prevalent than real life.\n \nreply"
    ],
    "link": "https://blog.paavo.me/radiant-ai/",
    "first_paragraph": "A ridiculously deep dive into Oblivion's controversial AI system and its legacyThe recent release of The Elder Scrolls IV: Oblivion Remastered has rekindled the public\u2019s interest in the 19-year-old RPG classic, making it once again \u2014 at least temporarily \u2014 one of the most played and talked about Bethesda Game Studios titles. The remaster is a comprehensive upgrade of the original, featuring fully remade yet mostly faithful graphics, select gameplay improvements and a redesigned user interface. But it is still Oblivion, and I don\u2019t mean that in the sense that it \u201cmaintains the spirit of the original game\u201d or something like that. The remaster is quite literally built on top of the original game; the Oblivion from 2006, including its game engine and content, is still there, just with an Unreal Engine 5 wrapper handling the audio-visual side of things.And that is interesting because Oblivion was a very ambitious game for its time. The world was huge, the graphics were cutting edge, but per"
  },
  {
    "title": "Updates to Advanced Voice Mode for paid users (help.openai.com)",
    "points": 36,
    "submitter": "mfiguiere",
    "submit_time": "2025-06-07T20:27:38 1749328058",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=44212413",
    "comments": [
      "They absolutely destroyed Sol. I\u2019m not sure what it is now. the disinterest, the umms, the inability to speak directly to question, a new inflection but I am pretty mad. I am an avid voice user. I love to use the advanced voice while I\u2019m doing tasks to explore new projects I want to work on and to get a basics understanding of home renovation tasks, etc. I had to finally change the voice to Maple but ran out of time to see if I could stand it. So disappointing.At least know I know i\u2019m not crazy and there were in-fact changes rolled out.\n \nreply",
      "> Additionally, rare hallucinations in Voice Mode persist with this update, resulting in unintended sounds resembling ads, gibberish, or background music. We are actively investigating these issues and working toward a solution.Would be cool to hear some samples of this. I remember there was some hallucinated background music during the meditation demo in the original reveal livestream but haven't seen much beyond that. Artifact of training on podcasts to get natural intonation.\n \nreply",
      "If anyone's wondering, here's a short sample. It quietly updated last night, and I ended up chatting for like an hour. It sounds as smart as before, but like 10x more emotionally intelligent. Laughter is the biggest giveaway, but the serious/empathetic tones for more therapy-like conversations are noticeable, too.\nhttps://drive.google.com/file/d/16kiJ2hQW3KF4IfwYaPHdNXC-rsU...\n \nreply",
      "Did it really say partwheel or is it garbled?\n \nreply",
      "I use advanced voice a lot and have come across many weird bugs.1) Every response would be normal except end with a \u201cwhoosh\u201d like one of those sound effects some mail clients use when an message is sent, and the model itself either couldn\u2019t or wouldn\u2019t acknowledge it.2) The same except with someone knocking on a door. Like someone would play on a soundboard.3) The entire history in the conversation disappearing after several minutes of back and forth, leading to the model having no idea what I\u2019m talking about and acting as if it\u2019s a fresh conversation.4) Advanced voice mode stuttering because it hears its own voice and thinks it\u2019s me interrupting (on a brand new iPhone 16 Pro, medium-low built in speaker volume and built-in mic).5) Really weird changes in pronunciation or randomly saying certain words high-pitched, or suddenly using a weird accent.And all of this was prior to these most recent changes.It also stutters and repeats sometimes and says poor connection even though I know the connection is near-ideal.\n \nreply",
      "I may know why that first one happens! They\u2019re not correctly padding the latent in their decoder (by default torch pads with zeros, they should pad with whatever their latent\u2019s representation of silence is). You can hear the same effect in songs generated with our music model: https://sonauto.ai/Yeah we\u2019re too lazy to fix it too\n \nreply",
      "I\u2019m super curious now, how does padding lead to repeatedly ending tts replies with what seem to be an actual non-speech sound effect?\n \nreply",
      "If you pad your output with something that doesn't represent silence, then any outputs that happen to have a non-standard length (i.e. nearly all outputs) will end with whatever sound your padding bits represent in the model's embedding space. if \"0000\" represents \"Whoosh,\" then most of your outputs will end in \"whoosh.\"Here's a non-AI example: If all HN comments had to be some multiple of 50 characters long and comments were padded with the letter \"A,\" then most HN comments would look like the user was screaming at the end. AAAAAAAAAAAAAAAAAA\n \nreply",
      "In addition to what Centigonal said, even if the autoencoder was trained on only speech data, an all zero vector is probably just be out of distribution (decoder has never seen it before) and causes weird sounds. However, given the hallucinations we're seeing, the AE has (maybe unintentionally) likely seen a bunch of non-speech data like music and sound effects too.\n \nreply",
      "they still need to post-train out the emissions of all the trapped souls\n \nreply"
    ],
    "link": "https://help.openai.com/en/articles/6825453-chatgpt-release-notes",
    "first_paragraph": ""
  },
  {
    "title": "The time bomb in the tax code that's fueling mass tech layoffs (qz.com)",
    "points": 1313,
    "submitter": "booleanbetrayal",
    "submit_time": "2025-06-04T13:30:21 1749043821",
    "num_comments": 828,
    "comments_url": "https://news.ycombinator.com/item?id=44180533",
    "comments": [
      "There are some misunderstandings in the comments that seem to stem from not having read the section, so I thought it was worth referencing the actual text [0]. It's quite short and easy to read.The most important bits:* Subsection (a) requires amortizing \"Specified research or experimental expenditures\" over 5 years (paragraph (2)) instead of deducting them (paragraph (1))* Paragraph (c)(3) is a Special Rule that requires that all software development expenses be counted as a \"research or experimental expenditure\".That's it. All software expenses must be treated as research and experimental expenses, and no research and experimental expense can be deducted instead of amortized. Ergo, all software expenses must be amortized over 5 years.I strongly recommend reading the section before forming an opinion. It really is quite unambiguous and is unambiguously bad for anyone who builds software and especially for companies that aren't yet thoroughly established in their space (i.e. startups).Also note that this makes Software a special case of R&D. It's the only form of R&D that Section 174 requires you to categorize as such and therefore amortize.[0] https://www.law.cornell.edu/uscode/text/26/174\n \nreply",
      "It's pretty bad.It had a huge impact on my personally, I'm a small R&D shop and basically I have had to end all risky long-term research projects.In addition to the research costs, I'd also have to pay taxes on the research costs mostly up-front. Significantly, if the project doesn't work out, I'm still out of pocket for the tax money. It's a penalty for taking a risk, and it kneecaps American innovators in a globally competitive technology race.The rules are even worse than the article notes because it double-dings open source developers. See Section 6.4 of https://www.irs.gov/pub/irs-drop/n-23-63.pdf. The relevant bit is here:> \"However, even if the research provider does not bear financial risk under the terms of the contract with the research recipient, if the research provider has a right to use any resulting SRE product ... costs paid or incurred by the research provider that are incident to the SRE activities performed by the research provider under the contract are SRE expenditures of the research provider for which no deduction is allowed ...\"The rule as written means contractors who write Windows drivers could deduct their expenses (as they would have no residual rights to a closed-source work product), but contractors who write Linux drivers may not (as they would have some rights to open-source Linux drivers).\n \nreply",
      "> In addition to the research costs, I'd also have to pay taxes on the research costs mostly up-front. Significantly, if the project doesn't work out, I'm still out of pocket for the tax money.That\u2019s how it works for every business! If Jim Bean builds a distillation facility it has to amortize the investment in that over time. If the distillation facility doesn\u2019t pan out, then it doesn\u2019t get a refund for the taxes paid.\n \nreply",
      "It doesn\u2019t matter. Jim Bean doesn\u2019t compete with an R&D software company. The R&D software company does compete with other companies in different jurisdictions with better regulations.\n \nreply",
      "\u201cThe power to tax is the power to destroy.\u201d\n \nreply",
      "> The rule as written means contractors who write Windows drivers could deduct their expenses (as they would have no residual rights to a closed-source work product), but contractors who write Linux drivers may not (as they would have some rights to open-source Linux drivers).Is it just me or are you conflating two orthogonal things?An open-source Windows driver would have the same issue, no? And a closed-source proprietary Linux driver privately written for some company wouldn't have this issue either, right?\n \nreply",
      "You're right, the law text doesn't specifically call out the Windows operating system or the Linux operating system.  The example you gave of Open Source Windows drivers is valid.The Grandparent's point about that \"it double-dings open source developers\" is still correct and poignant even with this clarification.\n \nreply",
      "> The Grandparent's point about that \"it double-dings open source developers\" is still correct and poignant even with this clarification.I feel like I'm missing what subset of people this is, exactly. We're talking about businesses here that would struggle with these tax rules. Which I guess is, mainly, contractors or startups. How common is it for such businesses to release their software as open-source, vs. as closed-source? I would've (naively) expected most paid OSS developers to be funded by large organizations/businesses that have plenty of money to fund them, not small businesses/contractors that would be severely impacted by this law. Is this actually a large set of people?\n \nreply",
      "There are lots of small OSS businesses that are contractors to the big companies you mention. My go-to example is Igalia, who work on web browser and other core OSS tech, but there lots of others, some mentioned on the FOSSjobs wiki.https://www.igalia.com/\nhttps://github.com/fossjobs/fossjobs/wiki/resources\n \nreply",
      "I could see it being inferred that way but, the way I read it, they are not meant as unilateral facts. Rather, they serve as rhetorical examples of where you might find contractors doing similar work, but where the one more in service of \"public good\" is taxed higher because it's open source. Strictly speaking, Windows bits are not all closed source and there exist closed source Linux bits. But it's not a point that really matters in the context of the conversation.I think it's fair to use Windows and Linux as stand-ins for closed vs open source because it's a very accessible example. And knowing the technicalities clearly doesn't undermine the argument.\n \nreply"
    ],
    "link": "https://qz.com/tech-layoffs-tax-code-trump-section-174-microsoft-meta-1851783502",
    "first_paragraph": ""
  },
  {
    "title": "Field Notes from Shipping Real Code with Claude (diwank.space)",
    "points": 61,
    "submitter": "diwank",
    "submit_time": "2025-06-07T18:11:25 1749319885",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=44211417",
    "comments": [
      "Author here: To be honest, I know there are like a bajillion Claude code posts out there these days.But, there are a few nuggets we figured are worth sharing, like Anchor Comments [1], which have really made a difference:\u2014\u2014  # CLAUDE.md\n\n  ### Anchor comments\n\n  Add specially formatted comments throughout the codebase, where appropriate, for yourself as inline knowledge that can be easily `grep`ped for.\n\n  - Use `AIDEV-NOTE:`, `AIDEV-TODO:`, or `AIDEV-QUESTION:` as prefix as appropriate.\n\n  - *Important:* Before scanning files, always first try to grep for existing `AIDEV-\u2026`.\n\n  - Update relevant anchors, after finishing any task.\n\n  - Make sure to add relevant anchor comments, whenever a file or piece of code is:\n\n  * too complex, or  \n  * very important, or  \n  * could have a bug \n\n\u2014\u2014[1]: https://diwank.space/field-notes-from-shipping-real-code-wit...\n \nreply",
      "Q: How do you ensure tests are only written by humans? Basically just the honor system?\n \nreply",
      "You can:1. Add instructions in CLAUDE.md to not touch tests.2. Disallow the Edit tool for test directories in the project\u2019s .claude/settings.json file\n \nreply",
      "Honest question: approx what percent of the post was human vs machine written?\n \nreply",
      "I\u2019d say around ~40% me, the ideating, editing, citations, and images are all mine; rest Opus 4 :)I typically try to also include the original Claude chat\u2019s link in the post but it seems like Claude doesn\u2019t allow sharing chats with deep research used in them.Update: here\u2019s an older chatgpt conversation while preparing this: https://chatgpt.com/share/6844eaae-07d0-8001-a7f7-e532d63bf8...\n \nreply",
      "One of the exciting things to me about the ai agents is how they push and allow you to build processes that we\u2019ve always known were important but were frequently not prioritized in the face of shipping the system.You can use how uncomfortable you are with the ai doing something as a signal that you need to invest in systematic verification of that something. As a for instance in the link, the team could build a system for verifying and validating their data migrations. That would move a whole class of changes into the ai relm.This is usually much easier to quantify and explain externally than nebulous talk about tech debt in that system.\n \nreply",
      "Pretty disingenuous to emphasize \"building a culture of transparency\" while simultaneously not disclosing how heavily AI was [very evidently] used in writing this post.\n \nreply",
      "I\u2019d say around ~40% me, the ideating, editing, citations, and images are all mine; rest Opus 4 :)I typically try to also include the original Claude chat\u2019s link in the post but it seems like Claude doesn\u2019t allow sharing chats with deep research used in them.See this series of posts for example, I have included the link right at the beginning: https://diwank.space/juleps-vision-levels-of-intelligence-pt...I completely get the critique and I already talked about it earlier: https://news.ycombinator.com/item?id=44213823Update: here\u2019s an older chatgpt conversation while preparing this: https://chatgpt.com/share/6844eaae-07d0-8001-a7f7-e532d63bf8...\n \nreply",
      "Coming up next on Hackernews: Field Notes from Eating Real Pizza Made with Claude:\"The glue adds a flavor and texture profile that traditionalists may not be used to on their cheese pizza. But I've had Michelin-star quality pizzas without glue in them that weren't half as delicious as this one was with. AI-mediated glue-za is the future of pizza, no doubt about it.\"\n \nreply",
      "The whole point seems to be how to get the most out of today's tooling without \"glue getting in your pizza\". It's a little flag-wavy (probably because of the author's company) but overall seemed like a pretty candid peek into how it's being used. Did you have a specific critique?\n \nreply"
    ],
    "link": "https://diwank.space/field-notes-from-shipping-real-code-with-claude",
    "first_paragraph": ""
  },
  {
    "title": "Discovering a JDK Race Condition, and Debugging It in 30 Minutes with Fray (aoli.al)",
    "points": 56,
    "submitter": "aoli-al",
    "submit_time": "2025-06-07T19:01:17 1749322877",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=44211779",
    "comments": [
      "Neat to see sleep calls artificially introduced to reliably recreate the deadlock. [0]Looks like fixing the underlying bug is still in-progress, [1] I wonder how many lines of code it will take.[0] https://github.com/aoli-al/jdk/commit/625420ba82d2b0ebac24d9...[1] https://bugs.openjdk.org/browse/JDK-8358601\n \nreply",
      "Impressive! Can't wait to try Fray out at work.\n \nreply",
      "Maybe it is just me, but I can't read the text in the code because the font is nearly white on white.\n \nreply",
      "The light mode is fine, but you're right the dark mode is truly awful, the code blocks are unreadable.edit: for some reason the author overrode the background color on code blocks via an inline style of    background-color:#f0f0f0\n\nfrom    var(--code-background-color) = #f2f2f2\n\nto make the background nigh imperceptibly darker, but then while the stylesheet properly switches the to #01242e in dark mode the inline override stays and blows it to bit.Not that it's amazing if you remove the inline stle, on account of operators and method names being styled pretty dark (#666 and #4070a0).\n \nreply",
      "Thanks for pointing it out! Just did a quick fix using Claude :)\n \nreply",
      "On mobile (Safari), the lines in the code blocks have different font sizes. They also have different fonts. Some are like 3-4x the size of other lines. No idea what could be going wrong, but it does unfortunately make the code blocks difficult to follow along.\n \nreply",
      "should be fixed as well :)\n \nreply",
      "any chance you can make light/dark mode switch a UI button?\n \nreply"
    ],
    "link": "https://aoli.al/blogs/jdk-bug/",
    "first_paragraph": ""
  },
  {
    "title": "You need much less memory than time (computationalcomplexity.org)",
    "points": 56,
    "submitter": "jonbaer",
    "submit_time": "2025-06-07T21:38:48 1749332328",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=44212855",
    "comments": [
      "See Kelsey Houston-Edwards's exceptional breakdown of Williams' paper, & Scott Aranson's thoughts on the topic.[1] https://www.youtube.com/watch?v=8JuWdXrCmWg[2] https://scottaaronson.blog/?p=8680\n \nreply",
      "I think the summary at the beginning of your first video is misleading; it's not a way to \"trade space for time\", at least not in an arbitrary program. The real statement is a bit odder to wrap one's head around -- \"every problem solvable in t time on a multitape Turing machine is also solvable in close to \u221at space\".For a Turing machine that already solves a problem in n time and \u221an space (in other words, a lot of them!), it doesn't say anything.\n \nreply",
      "When you convert a generic Turing machine into a Tree Evaluation instance, you end up with square-root space with respect to the original runtime t, but the new runtime will be far, far slower. IME, with these types of circuit reductions, the runtime typically becomes exponential in the space required, which is just about 'as long as possible'.If we're being pedantic, it's trading time for the space guarantee.\n \nreply",
      "Related. Others?For algorithms, a little memory outweighs a lot of time - https://news.ycombinator.com/item?id=44055347 - May 2025 (139 comments)\n \nreply",
      "Dupe of today's https://news.ycombinator.com/item?id=44212861 ?\n \nreply",
      "From Februray 2025 fwiw.  Same result there have been multiple articles here about.  I wonder how it would work for Haskell programs (no mutable memory).\n \nreply",
      "I'd view \"no mutable memory\" as misleading, because immutable languages can still create a new variable and forget an old one which has the same memory footprint as mutating one variable.Obvious example: the flickering stack frame of tail call elimination.\n \nreply"
    ],
    "link": "https://blog.computationalcomplexity.org/2025/02/you-need-much-less-memory-than-time.html",
    "first_paragraph": "Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill GasarchJust as I was complaining that we haven't seen many surprising breakthroughs in complexity recently, we get an earthquake of a result to start the year, showing that all algorithms can be simulated using considerable less memory than the time of the original algorithm. You can reuse space (memory) but you can't reuse time, and this new result from Ryan Williams in an upcoming\u00a0STOC paper\u00a0provides the first stark difference.DTIME(\\(t(n)\\)) \\(\\subseteq\\) DSPACE(\\(\\sqrt{t(n)\\log t(n)}\\))This is a vast improvement on the previous best known simulation, the classic 1977 Hopcroft-Paul-Valiant paper\u00a0showingDTIME(\\(t(n)\\)) \\(\\subseteq\\) DSPACE(\\(t(n)/\\log t(n)\\))only slightly lower than the trivial \\(t(n)\\) bound. Williams gets a huge near quadratic improvement that will go down as a true classic complexity theorem. Note that the space simulation does not maintain the time bound.Williams"
  },
  {
    "title": "Why Understanding Software Cycle Time Is Messy, Not Magic (arxiv.org)",
    "points": 12,
    "submitter": "SiempreViernes",
    "submit_time": "2025-06-07T21:03:28 1749330208",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://arxiv.org/abs/2503.05040",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Washington Post's Privacy Tip: Stop Using Chrome, Delete Meta Apps (and Yandex) (slashdot.org)",
    "points": 270,
    "submitter": "miles",
    "submit_time": "2025-06-07T16:33:13 1749313993",
    "num_comments": 142,
    "comments_url": "https://news.ycombinator.com/item?id=44210689",
    "comments": [
      "Without the suggestion to install an adblocker, this is not credible advice.\n \nreply",
      "A media outlet which depends on ad revenue as a primary income source is unlikely to suggest this.Ditching these deeply invasive products remains a good idea, independent on any decision to use ad blockers or not.The Meta/Yandex incident in particular is straight-up malware and everyone should remove their apps.\n \nreply",
      "Getting privacy advice from an adtech funded outlet sounds like reading democracy advice from the Chinese ruling party or vegetarianism advice from lions to be honest.It might be correct-and-incomplete but they just have no credibility on the topic.\n \nreply",
      "Many HN commenters work for \"adtech funded outlets\".  Do they have any credibility on the issue of privacy.\n \nreply",
      "Individually they might, but I wouldn't take advice from their employers.\n \nreply",
      "WaPo is dependent on subscription revenue, not ads. They limit the number of articles non subscribers can read.They're also owned by one of the richest men in the world...\n \nreply",
      "Maybe, but they they refused to offer an ad-free subscription tier last time I asked. NYT and Chicago Sun Times also refused.\n \nreply",
      ">  which depends on ad revenueThey're more tightly bound than that.  They're dependent on Google Display Ads.  Which really makes their whole diatribe that much more pathetic.Any media company that decided to traffic the ads themselves,  from their own servers,  and inline with their own content,  would effectively be immune from ad blocking.> Ditching these deeply invasive products remains a good ideaWhile still allowing random third party javascript to run unchecked on a parent website.\n \nreply",
      "> While still allowing random third party javascript to run unchecked on a parent website.Lol, why are you commenting as if somehow allowing it to run negates the other good ideas in some way? Obviously some is better than none, and all is better than some, but each step takes more effort.\n \nreply",
      "lol,  because ads pay for the content you're reading.  it pays salaries.what I _don't_ want is to be _tracked_.  show me ads all day if you want.\n \nreply"
    ],
    "link": "https://tech.slashdot.org/story/25/06/07/035249/washington-posts-privacy-tip-stop-using-chrome-delete-metas-apps-and-yandex",
    "first_paragraph": "\n\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\tFollow Slashdot stories on Twitter\n\nNickname:\n\n\nPassword:\n\n\nNickname:\n\n\nPassword:\n\n\nThe Fine Print: The following comments are owned by whoever posted them.  We are not responsible for them in any way.\nYou are all \"dumb fucks\" according to Zuck.Mod parent funny?However i think the biggest joke may be that the sharks might be going after each other. Not the only reason, but one of the requirements for becoming so stinking rich is that they ALWAYS want MORE money, even though they already have more money than makes any human sense. Up to now, they have mostly been content to squeeze blood out of the impoverished cabbages like you and me, but if they are sincerely attacking each other, then maybe they've realized there isn't any more cash = blood ava Well, he should know.It works (for the most part) by establishing one-to-one connections between clients (your device) and servers (what you're looking at) where by construction the client and the server both know eachothe"
  },
  {
    "title": "Low-Level Optimization with Zig (alloc.dev)",
    "points": 235,
    "submitter": "Retro_Dev",
    "submit_time": "2025-06-07T07:26:24 1749281184",
    "num_comments": 116,
    "comments_url": "https://news.ycombinator.com/item?id=44208060",
    "comments": [
      "What interests me most by zig is the ease of the build system, cross compilation, and the goal of high iteration speed. I'm a gamedev, so I have performance requirements but I think most languages have sufficient performance for most of my requirements so it's not the #1 consideration for language choice for me.I feel like I can write powerful code in any language, but the goal is to write code for a framework that is most future proof, so that you can maintain modular stuff for decades.C/C++ has been the default answer for its omnipresent support. It feels like zig will be able to match that.\n \nreply",
      "I recently, for fun, tried running zig on an ancient kindle device running stripped down Linux 4.1.15.It was an interesting experience and I was pleasantly surprised by the maturity of Zig. Many things worked out of the box and I could even debug a strange bug using ancient GDB. Like you, I\u2019m sold on Zig too.I wrote about it here: https://news.ycombinator.com/item?id=44211041\n \nreply",
      "> I feel like I can write powerful code in any language, but the goal is to write code for a framework that is most future proof, so that you can maintain modular stuff for decades.I like Zig a lot, but long-term maintainability and modularity is one of its weakest points IMHO.Zig is hostile to encapsulation.  You cannot make struct members private: https://github.com/ziglang/zig/issues/9909#issuecomment-9426...Key quote:> The idea of private fields and getter/setter methods was popularized by Java, but it is an anti-pattern. Fields are there; they exist. They are the data that underpins any abstraction. My recommendation is to name fields carefully and leave them as part of the public API, carefully documenting what they do.You cannot reasonably form API contracts (which are the foundation of software modularity) unless you can hide the internal representation.  You need to be able to change the internal representation without breaking users.Zig's position is that there should be no such thing as internal representation; you should publicly expose, document, and guarantee the behavior of your representation to all users.I hope Zig reverses this decision someday and supports private fields.\n \nreply",
      "Some years ago I started to just not care about setting things to \"private\" (in any language).  And I care _a lot_ about long term maintainability and breakage. I haven't regretted it since.> You cannot reasonably form API contracts (...) unless you can hide the internal representation.Yes you can, by communicating the intended use can be made with comments/docstrings, examples etc.One thing I learned from the Clojure world, is to have a separate namespace/package or just section of code, that represents an API that is well documented, nice to use and more importantly stable. That's really all that is needed.(Also, there are cases where you actually need to use a thing in a way that was not intended. That obviously comes with risk, but when you need it, you're _extremely_ glad that you can.)\n \nreply",
      "Just prefix internal fields with underscore and be a big boy and don't access them from the outside.If you really need to you can always use opaque pointers for the REALLY critical public APIs.\n \nreply",
      "I am not the only user of my API, and I cannot control what users do.My experience is that users who are trying to get work done will bypass every speed bump you put in the way and just access your internals directly.If you \"just\" rely on them not to do that, then your internals will effectively be frozen forever.\n \nreply",
      "Or you change it and respond with \u201cYou were warned\u201d.I seriously do not get this take. People use reflection and all kinds of hacks to get at internals, this should not stop you from changing said internals.There will always be users who do the wrong thing.\n \nreply",
      "Let's say I'm in a large company. Someone on some other team decided to rely on my implementation internals for a key revenue driver, and snuck it through code review.I can't break their app without them complaining to my boss's boss's boss who will take their side because their app creates money for the company.Having actual private fields doesn't 100% prevent this scenario, but it makes it less likely to sneak through code review before it becomes business-critical.\n \nreply",
      "> If you \"just\" rely on them not to do that, then your internals will effectively be frozen forever.Or they will be broken when you change them and they upgrade. The JavaScript ecosystem uses this convention and generally if a field is prefixed by an underscore and/or documented as being non-public then you can expect to break in future versions (and this happens frequently in practice).Not necessarily saying that's better, but it is another choice that's available.\n \nreply",
      "[flagged]\n \nreply"
    ],
    "link": "https://alloc.dev/2025/06/07/zig_optimization",
    "first_paragraph": ""
  },
  {
    "title": "Why We're Moving on from Nix (railway.com)",
    "points": 197,
    "submitter": "mooreds",
    "submit_time": "2025-06-07T11:36:13 1749296173",
    "num_comments": 96,
    "comments_url": "https://news.ycombinator.com/item?id=44208968",
    "comments": [
      "Okay I'm a Nix enthusiast but you'll have to trust me when I say that I'm not criticizing them for moving away from Nix; it isn't that strong of an emotional attachment. However, I'm not really sure I understand some of these complaints and they really could use more explanation. For example:> The biggest problem with Nix is its commit-based package versioning. Only the latest major version of each package is available, with versions tied to specific commits in the nixpkgs repo.While Nixpkgs is an amazing resource, Nix != Nixpkgs. Nixpkgs is highly unideal for cases where you want to be able to pull arbitrary versions of toolchains, but it is not the only way to go. For example, there is amazingly good Nix tooling for pulling an arbitrary version of Rust. Other Nix-based developer tools have shown how you can do this well.> no way of splitting up the Nix dependencies into separate layersThat doesn't make any sense. You can literally just split them into separate layers in whatever arbitrary fashion you'd like. The built-in Nixpkgs docker tooling has some support for this even.> We also changed the codebase from Rust to Go because of the Buildkit libraries.This part is not related to Nix, but I find it interesting anyways. Obviously most people don't transition programming languages on a whim, it's generally something you do when you're already planning on building from scratch anyways. To me it almost sounds like different people worked on Railpacks vs Nixpacks.(I've definitely seen what happens when people not familiar with Nix wind up having to deal with unfinished Nix solutions within an organization. It is not pretty, as most people are unwilling to try to figure out Nix. I don't generally use Nix at work out of fear of causing this situation.)\n \nreply",
      "I don't use Nix, however this seems dismissive:> While Nixpkgs is an amazing resource, Nix != Nixpkgs.If Nixpkgs is the default and alternatives require additional research and effort then for most users it _is_ Nix.> That doesn't make any sense. You can literally just split them into separate layers in whatever arbitrary fashion you'd like. The built-in Nixpkgs docker tooling has some support for this even.Is this obvious, simple, and default behaviour?\n \nreply",
      "Nixpkgs isn't Nix and in production you rarely just use Nixpkgs verbatim. It's trivial to overlay whatever versions you want (including forks), and I'd say it's expected for any company in production to manage their package set.We are talking about a company full of professionals. If they need something obvious, simple, and default to manage their build - the core business function that turns their text into deployable artifacts - maybe there is a skill culture issue.The industry is full of ineptitude though.\n \nreply",
      "> The industry is full of ineptitude though.While I disagree with the person you're replying to, I find your reply dismissive.I don't know the behind-the-scnene reasons for this, but I can very very easily apply a very similar situation to this from my experience.Nix is a full blown functional programming language along with a very rich (and poorly documented, niche, only second to C++ template in error comprehensibility[1]) ecosystem in itself. It's not like \"docker\" or \"kubernetes\" where you're mostly dealing with \"data\" files like yaml, json or Dockerfile. You're dealing with a complex programming project.With that in mind:- You have a core team with 1 or 2 people with Nix passion/expertise.- Those people do most of the heavy lifting in implementation.- They onboarding the team on to Nix- They evangelize Nix through the org/company- They mod and answer all the \"#nix-discussions\" channel questionsInitially the system is fairly successful and everything is good. over the next 5-6 years it would accumulate a lot of feature asks. The original \"Nix person\" has long left. Most of the original people have moved either to other projects or not particularly that passionate about Nix. In fact, the \"best\" developer you have who has inherited the whole Nix thing has only really had to deal with all the shit parts of Nix and the system. They are they ones fixing issues, dealing with bugs, etc. All while maintaining 3 stacks, a Nix stack, a Go stack, and a Rust stack.Eventually that person/team that's annoyed by maintaining the Nix project wins. They want to own that code. They don't want to use Nix any more. They know what's needed, they want to implement it as part of their main Go stack that they are actively working on. They can optimize things for their special case without having to worry about \"implementing it the Nix way\" or \"doing it upstream\".They promise you (the management who is open to the idea, but trying to understand the ROI) feature parity + top 5 feature asks for the initial release. You trust the team enough to let them do what they think is best.[1]: LLMs are really good at suggesting a solution given an error message. Nix errors bring them to their knees. It's always \"Hmmm.... it appears that there is an error in your configuration... have you tried a `git revert`?\"\n \nreply",
      "> maintaining 3 stacks, a Nix stack, a Go stack, and a Rust stackNix is a package management system with a little bit of programming tucked onto the side.  \"Nix stack\" is not the same type of thing as \"Go/Rust stack\".  If you try to move things over to Go/Rust, you'll spend a little bit of time rewriting the code and a whole lot of time reinventing the wheel on everything else involved.  You're not moving between implementations, you're building your own implementation.  That's almost always a bad idea, and it's a much higher cost than learning the syntax.Moving from a Nix stack to a Go stack only makes slightly more sense than moving from a docker stack to a Go stack.  Which is to say, very little sense.\n \nreply",
      "I'm not being dismissive. Well, I am dismissing a lot of industry people's opinions. Because they're bad.Just because people decide stuff for money doesn't mean I can't call them bad. Not everyone is equally skilled.And your parable is exactly the issue. The unskilled and loud and whiny do often win, and it's a shame. I see it all the time.(Also you're way overstating Nix as a \"full blown FP language.\" It isn't hard to learn. I learned it just be existing on a project with it. Within 6mo, now I'm apparently a \"Nix expert\" and people now point at me as one of the people who \"knows it\" and \"you can't expect everyone to know it like you do.\" idk maybe I'm some genius but I think it's more that I just don't have a bad personality.)\n \nreply",
      "So you are being dismissive. That's what you're doing. You're dismissing more than just \"stuff for money\". You're dismissing anything that doesn't fall under the \"skill\" or \"technical\" category. All software projects contain a human element. I was showing an example from my experience on how something like that could happen.> A perfectly capably (but perhaps a bit esoteric) technology is picked by a smart passionate person for a project.> The novel technology is in 1 isolated module that's mostly feature complete for the first 1-3 years.> People in the team/company deal with that \"thing\" as a blackbox more and more> 5-10 years later, mostly new team maintaining the project. They hate the weird choice of tech. \"Why is only this one component different???\"> People understand the contract with the \"black box\" very well, but HATE the \"black box\". People think \"We can implement the black box contract very easily\"\n \nreply",
      "Yes I am dismissing. People don't have a right to not be dismissed if I judge them poorly. People are allowed to have bad professional opinions of others.And I am dismissing the types you describe specifically. I dismiss them (privately amongst the likeminded) at work all the time too. I just put them on a list in my head when they start spouting these sorts of bad values.\n \nreply",
      "This isn\u2019t \u201cmost users\u201d, this is a large company building a product on top of Nix. I\u2019m pretty sure most orgs using Nix at a minimum have a custom overlay.If you identify these things as an issue, any competent engineer should find a variety of solutions with search and/or LLM assistance within an hour, since they\u2019re not super obscure requirements.I\u2019m not saying Railway didn\u2019t do this and realize that these common solutions weren\u2019t viable for them, but it\u2019s odd to not mention anything they tried to get around it.\n \nreply",
      "To emphasize this point: dleslie's comment is valid on a blog post \"we tried Nix for a while to manage our dependencies, we are just building an app and want builds to work, and we decided to move on\". For an end user, it is absolutely understandable to assume \"nix = nixpkgs\".But as kfajdsl points out: that's not what TFA is. This is a company building a product on top of Nix. Package management is their expertise. Anyone using Nix in that capacity understands the distinction between nix and nixpkgs. Which they certainly do--GP only remarked it was odd they didn't explain it, not that they didn't know.\n \nreply"
    ],
    "link": "https://blog.railway.com/p/introducing-railpack",
    "first_paragraph": "Today we\u2019re excited to release Railpack \u2014 the next iteration of the Railway builder, developed from the ground up and based on everything we\u2019ve learned from building over 14 million apps with Nixpacks.We first announced Nixpacks nearly 3 years ago and it quickly became the default way to build images from user code on Railway. While Nixpacks works great for 80% of users, that still left us with 200k Railway users who might encounter limitations daily. It became clear we needed a major builder upgrade to scale our user base from 1M to 100M.Cumulative builds with Nixpacks over timeHere are the highlights of Railpack:You can opt-in to using Railpack for your builds today. It is already powering builds for railway.com and central station.The biggest problem with Nix is its commit-based package versioning. Only the latest major version of each package is available, with versions tied to specific commits in the nixpkgs repo. We tried to support every patch version, but it looked like this:Th"
  },
  {
    "title": "Researchers develop \u2018transparent paper\u2019 as alternative to plastics (yomiuri.co.jp)",
    "points": 373,
    "submitter": "anigbrowl",
    "submit_time": "2025-06-06T21:43:10 1749246190",
    "num_comments": 231,
    "comments_url": "https://news.ycombinator.com/item?id=44205282",
    "comments": [
      "Transparency isn't the reason we use so much plastic. We like plastic because it is lightweight and not biodegradable. We like it because it lasts thousands of years. Because if it lasts thousands of years it will do a good job of storing your food products. Or it will stick around in various components without needing to worry about rain and such.What we need to develop is something that doesn't degrade at all under most human living conditions, but does degrade rapidly if we expose it to some sort of not-common trigger, whether that is another chemical or temperature or pressure or whatever.\n \nreply",
      "We also use it because it's super-easy to mold, and is incredibly suited to mass production. The ease with which it can be shaped might even be the single most compelling reason to go plastic.Plastic takes the best aspects of wood (lightweight, cheap), ceramics (easy to shape, watertight), and metal (casual resiliency); and dodges some of the biggest issues with each (wood requires a lot of finishing and is very slow to shape industrially, ceramics tend to shatter, metal is comparatively expensive, prone to rust, and also electrically conductive). They're not perfect, but if you add up the stat points it's obvious why they're so prevalent.\n \nreply",
      "Let's not forget it's strength to weight ratio and how incredibly cheap it is. A polythene bag having few grams of weight can easily carry a load of 5kg or more while costing only a few cents.\n \nreply",
      "What's clear to me, at least, is that a few cents doesn't represent the actual cost. It's a shortcoming of our economics that we consider such a great and long lasting material so disposable.\n \nreply",
      "I like to put it as all the damage we're causing is just taking out a huge loan, and either we repay it on our own terms or mother nature is going to debt collect for us...\n \nreply",
      "This is probably the most important comment ITTThe tricky part is how do we even begin to model that with a somewhat comprehensible parameter? Without near perfect traceability across all nations in the world, we can only use sledgehammer methods like a \u201cplastic tax\u201d - which you\u2019ll find very difficult to pass outside of more developed jurisdictions like the EU\n \nreply",
      "Collecting, sorting and burning is not that expensive\n \nreply",
      "Burning is much worse than burying plastic - as it releases much of its mass as CO2 and other greenhouse gasses, and likely other pollutants as well.\n \nreply",
      "For CO2 purposes it's no different than burning oil. You can burn trash to generate electricity too.At 5 grams per bag it's also hard to get any real volume of the emissions.One of my pet theories is that we vastly overestimate the environmentally impact of things we personally touch. People lose sleep over their single use Starbucks cups, while things many orders of magnitude worse happen out of sight.\n \nreply",
      "In 2021 there were 51 Million tons of plastic waste produced in the US [0], which is about 150kg per person.Burning that is creating between 264 and 750kg of CO2 per person and year, definitely not insignificant.I'm not saying that big corporations are not responsible for a huge chunk of the emissions, but getting away from using so much plastic is not hurting.[0]: https://www.statista.com/statistics/1339439/plastic-waste-ma...\n \nreply"
    ],
    "link": "https://japannews.yomiuri.co.jp/science-nature/technology/20250605-259501/",
    "first_paragraph": "Please disable the ad blocking feature.To use this site, please disable the ad blocking feature and reload the page.This website uses cookies to collect information about your visit for purposes such as showing you personalized ads and content, and analyzing our website traffic. By clicking \u201cAccept all,\u201d you will allow the use of these cookies.Users accessing this site from EEA countries and UK are unable to view this site without your consent. We apologize for any inconvenience caused.The Yomiuri Shimbun14:47 JST,\u2002June 5, 2025A team of researchers with the Japan Agency for Marine-Earth Science and Technology (JAMSTEC) and other entities have developed thick sheets of transparent paper using cellulose, a material made from plant biomass.The transparent paper sheets can be broken down by microbes into water and carbon dioxide. Also, they can be used to make containers because they are thicker than conventional cellulose-based materials. The new material is expected to replace plastics f"
  },
  {
    "title": "The US is turning into a mass techno-surveillance state (elpais.com)",
    "points": 45,
    "submitter": "geox",
    "submit_time": "2025-06-07T23:08:51 1749337731",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=44213301",
    "comments": [
      "The subhead says \"have accelerated\", which is bigger than the headline's \"is turning into\".\n \nreply",
      "Turning into one?It's been one for at least the last twenty years.\n \nreply",
      "The problem is that people become used to the conditions they are living in, so the disaster is always in the future. People expect some sharp, intolerable transition.\n \nreply",
      "Traveling abroad to asian countries it becomes immediately and shockingly clear how intolerable surveillance really can be. Between having your face and fingerprints scanned on a pirated windows xp pc to being fined heavily for going 51 kph in 50 zone it can be hard to imagine how much worse it can get from there, but it can.\n \nreply",
      "I\u2019m interested in how strong your reaction is to those two examples. Could you explain why those are such terrible things?\n \nreply",
      "Come on. It's blindingly obvious to anyone not used to these things why it's inherently bad. To people who live in a police state you eventually get used to such things. Since police states aren't inherently bad for all their citizens they don't get called out enough.For example i really believe that traffic enforcement cameras are state oppression. They create more human suffering then they prevent. it's just that people are used to it so they don't protest.",
      "I read that article this morning, that was also my reaction.Do they not remember Snowden?Although the level of accessing \"social media\" posts, and internal government docs, for use in persecution of an individual is rising to new levels with The Cheato administration.\n \nreply",
      "Although, Cheatos are the wrong shade of orange. I feel like the Orange Immigration Man is something more like a Kraft Single.\n \nreply",
      "it was all downhill since the red scare\n \nreply",
      "Ever since McCarthy where friends, neighbors and co-workers were pressed into narcing and ratting on each other.\n \nreply"
    ],
    "link": "https://english.elpais.com/usa/2025-06-05/how-the-us-is-turning-into-a-mass-techno-surveillance-state.html",
    "first_paragraph": "Massive unauthorized scanning of social media. Analysis of biometric, income, health, and Social Security data. Interception of telephone communications. Geolocation via mobile devices. Tracking of car journeys using license plate readers. Since Donald Trump returned to the White House, the U.S. government has been using these and other tools based on artificial intelligence (AI) to monitor and persecute thousands of people without judicial authorization \u2014 mostly immigrants, foreigners passing through, or students. In the last four months, Trump and his former star advisor, the tech tycoon Elon Musk, have, along with the private sector, accelerated the deployment of a massive techno-surveillance state. And for the first time in history, Washington is boasting about it rather than denying its existence.\u201cSurveillance in the U.S. didn\u2019t begin with Trump, nor will it end when he leaves the White House. The foundations for the current state of techno-surveillance were laid over decades, wit"
  },
  {
    "title": "A tool for burning visible pictures on a compact disc surface (github.com/arduinocelentano)",
    "points": 132,
    "submitter": "carlesfe",
    "submit_time": "2025-06-07T08:30:23 1749285023",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=44208283",
    "comments": [
      "Back in the day, there was a Yamaha burner with a feature called \"DiscT@2\". It could burn images and text onto the unused area of a CD-ROM. I just had to get it and did so, and I had a bit of fun with it.\n \nreply",
      "It seemed especially badass when the model number was the CRW-F1, released in 2002.It was also cool because the activity would blink purple (orange + blue) during writing. This set it apart when blue LEDs were all the rage.\n \nreply",
      "I still have mine (in a firewire enclosure)!  Last tested the DiscT@2 feature about four years ago, at the time qpxtool had a utility for burning the imagery under Linux.\n \nreply",
      "Same. I had one of these in \u201898/\u201899. The disc didn\u2019t even go into a standard tray\u2014-you had to use a caddy that completely enveloped the disc.\n \nreply",
      "any idea what the caddy did?some sort of feedback for rotation angle maybe?\n \nreply",
      "The caddies were just a simple loading mechanism, with a spring door like a floppy disc. I suspect they had the life they did because someone was hoping that we would all buy ultra-expensive caddies for our collections instead of moving discs in and out of cases.\n \nreply",
      "Caddies were fairly common in early CD-ROM drives. Tray-loading (and, even later, slot-loading) drives were a later development.One theory I've seen is that caddies were developed in part to protect valuable data CDs from accidental damage, and faded in popularity as software became more affordable. Early multimedia software could be quite expensive, with some titles running into the hundreds of dollars.\n \nreply",
      "I fondly remember LightScribe, that was a pretty awesome technology.\n \nreply",
      "I was going to say, I still have a 5 pack of Lightscribe DVDs unopened in a box specifically to save something \"special\" but obviously nothing has ever been special enough to warrant using them. And now that they aren't made anymore it would feel downright sacrilegious to use them, not to mention 4.7GB of capacity is just not enough for anything nowadays really.\n \nreply",
      "4.7GB is quite enough for a standalone Linux DVD (for devices that still have DVD drives). Plus some cool art.Might be a good idea to preserve a known-working distro for some old PC, especially for discontinued or less-used architectures. Just saw a discussion the other day about finding 32-bit Debian for an old laptop.\n \nreply"
    ],
    "link": "https://github.com/arduinocelentano/cdimage",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A tool for burning visible pictures on a compact disc surfase\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.I know of at least two successful attempts to implement a similar technique. One was accomplished about 15 years ago by argon Instructables user. Another attempt was made by a user with nickname [unDEFER] (no English documentation unfortunately). These two projects inspired me some time ago. And in fact my coordinate conversion code is mostly based on [unDEFER]\u2019s implementation. I also used geometric parameters of some compact discs from that project. I acknowledge and am grateful to these developers for their contributions.I played with color shades and different compact discs with moderate success and created a GUI with visual preview mode. I tried"
  },
  {
    "title": "An innovative superfamily of fonts for code (githubnext.com)",
    "points": 10,
    "submitter": "laex",
    "submit_time": "2025-06-08T00:15:11 1749341711",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://monaspace.githubnext.com/",
    "first_paragraph": "An innovative superfamily of fonts for codeSince the earliest days of the teletype machine, code has been set in monospaced type\u2009\u2014\u2009letters, on a grid. Monaspace is a new type system that advances the state of the art for the display of code on screen.Every advancement in the technology of computing has been accompanied by advancements to the display and editing of code. CRTs made screen editors possible. The advent of graphical user interfaces gave rise to integrated development environments.Even today, we still have limited options when we want to layer additional meaning on top of code. Syntax highlighting was invented in 1982 to help children to code in BASIC. But beyond colors, most editors must communicate with developers through their interfaces\u2009\u2014\u2009hovers, underlines, and other graphical decorations.Monaspace offers a more expressive palette for code and the tools we use to work with it.Monospaced fonts are generally incompatible with one another. Each one uses different metrics, "
  }
]