[
  {
    "title": "Sunsetting Whois (icann.org)",
    "points": 20,
    "submitter": "radeeyate",
    "submit_time": "2025-03-17T00:48:48 1742172528",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=43384069",
    "comments": [
      "RDAP replaces WHOIS, offering a more technologically advanced way to discover the domain is protected by privacy services.\n \nreply",
      "Wow. I never noticed how much how I used the internet changed. I haven\u2019t done a WHOIS in a decade.When I started using the internet, it\u2019s how I contacted people. If I liked their site or their blog, I\u2019d check who was behind it and get an email address I could contact.Now\u2026 humans don\u2019t really own domains anymore. Content is so centralized. I obviously noticed this shift, but I had forgotten how I used to be able to interact with the internet.\n \nreply",
      "To be replaced with a system providing a standardized method to give law enforcement easier \"secure access\" to your redacted personal information.\n \nreply",
      "These days how can one register a domain anonymously, using crypto as payment, and without KYC?\n \nreply",
      "I havent had a successful use of whois in probably over a decade. What was once a useful tool was destroyed by spammers harvesting email addresses and privacy oriented registrars.I won't even notice its gone\n \nreply"
    ],
    "link": "https://www.icann.org/en/announcements/details/icann-update-launching-rdap-sunsetting-whois-27-01-2025-en",
    "first_paragraph": ""
  },
  {
    "title": "Docs \u2013 Open source alternative to Notion or Outline (github.com/suitenumerique)",
    "points": 1130,
    "submitter": "maelito",
    "submit_time": "2025-03-16T11:38:52 1742125132",
    "num_comments": 334,
    "comments_url": "https://news.ycombinator.com/item?id=43378239",
    "comments": [
      "This is a really great project from both the French and German governments.I think state-funded open source solutions to digital platforms is a fantastic opportunity to get away from the big tech walled gardens. Of course, there is always the risk that this becomes unmaintained in the future, but the community at least can take over. But until then, it's a nice platform and a nice contribution to the community.\n \nreply",
      "Personally, even if this software wouldn't be 1:1 capable of replacing the established players, it still feels like a good idea. With how much people (rightfully) complain about how open source is underfunded and with how often we're forced into borderline exploitative dealings with the established players in the market (the likes of MS Office, Adobe products, Atlassian products, even some Oracle stuff), funding the development of open alternatives (even if done with some comparatively small amount of taxes) seems like a good idea, as long as everyone in the government isn't incompetent.For example, if we had governments with strong tech departments that could fund helping the development of LibreOffice, then suddenly even if someone wants to use MS Office, that's still a bargaining chip to get a better deal because there's a viable alternative. Or to develop something like OpenProject, Kanboard etc., alternatives to the likes of Jira, that might be enough for many out there, while also possibly benefitting from community contributions. People love to complain about how Jira supposedly sucks, so that'd be a good opportunity to step up and make something \"better\". Or using open source technologies like PostgreSQL or MariaDB/MySQL for developing their own internal systems instead of always forking over a bunch of cash for Oracle or MS SQL by default.If you want a government that's cost efficient, then invest in making it be so, treat the software landscape as an investment opportunity - spend some money now to save a bunch of money later. The same way how an app can be a home cooked meal, some software could be a public utility.\n \nreply",
      "Notion is not an example of delightful software and it is very much one of the most reproducible apps ever. I don't know how they managed to make it fashionable amongst startups, but it's certainly not because it's an innovative product.\n \nreply",
      "I also have to disagree here.What Notion has built is amazing.When leadership tells us our job is to replace Microsoft Office. I say it's notThis is Libre Office's job. While I truly admire this community\u2019s work. \nIf I ever get anywhere close to their level I\u2019ll consider myself lucky. They do important work and I hope they continue for may years  .I\u2019m not trying to replace Microsoft Office because work has changed.As it came online, it became collaborative.What\u2019s replacing Microsoft isn\u2019t perfectly similar alternatives to text editing, spreadsheets and slides which are tools that were made for formatting more than content editing.These were meant to be printed to be shared.What\u2019s actually replacing Microsoft Office are tools like Notion.Nowadays content is created in real time with 4, 6 or more pair of hands typing at the same time. \u2328The way we actually replace Microsoft Office is by building products that follow the change in usage like Notion has been doing.That\u2019s what we need to do as an opensource community.Adopting Notion won't do in times like we're living as states (hell, all of us!) we need strategic digital autonomy.The product of our collaborative work is knowledge, we can't have it siphoned because it's sitting on an American server.Notion has been leading the content over form revolution for a while now.But revolutions are our thing  right ?We like to start them, but it's way more fun when they spread to the whole continentWant to join us or support us with a little GitHub  \nhttps://github.com/suitenumerique/docs\n \nreply",
      "Notion is some kind of Kanban board, isn't it? I think the point of the parent is that such boards were not invented by Notion, and writing a webapp that essentially allows you to move post-its between columns is not exactly innovative. Which is fine, if it works. We don't always need innovation (actually most of the time we don't). Notion just seems to be super popular for just being a webapp of post-its.\n \nreply",
      "> Notion is some kind of Kanban board, isn't it?No, it\u2019s not. But I wouldn\u2019t be surprised if it has a Kanban board.It\u2019s a proprietary cloud-based wiki with support for every more or less mainstream feature (multimedia, databases, AI integration, collaboration, etc.). It\u2019s a bit sluggish and doesn\u2019t have a good mobile story, but if you don\u2019t mind the proprietary aspect, it\u2019s otherwise a polished product.\n \nreply",
      "Notion is a wiki. It\u2019s like Confluence but good.Maybe they have tickets and boards as well? Even better!\n \nreply",
      "Notion is good, maybe great for some but small things holding it back from becoming ubiquitous.Does notion work offline-first yet?The only copy of my data should not exist solely in an app\u2019s cloud, and I should not need to manually export anything.Collaboration is nice, still has nothing to do with being offline-first.Docs looks very promising.  Congrats to the remnant he launch.Another effort that might be a subset of Docs is Anytype - another French effort that seems to be very promising.\n \nreply",
      "> Does notion work offline-first yet?It does via a desktop application.\n \nreply",
      "Y Combinator. They gave it away to all of them. This is how you become popular with startups.\n \nreply"
    ],
    "link": "https://github.com/suitenumerique/docs",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A collaborative note taking, wiki and documentation platform that scales. Built with Django and React. Opensource alternative to Notion or Outline.\n      \n\n\n\n\nWelcome to Docs! The open source document editor where your notes can become knowledge through live collaboration\n\n\n    Chat on Matrix\n   - \n    Documentation\n   - \n    Getting started\n   - \n    Reach out\n  \nDocs is a collaborative text editor designed to address common challenges in knowledge building and sharing.Test Docs on your browser by logging in on this environment\u26a0\ufe0f Running Docs locally using the methods described below is for testing purposes only.  It is based on building Docs using Minio as the S3 storage solution but you can choose any S3 compatible object storage of your choice.PrerequisiteMake sure you have a recent version of Docker and Docker Compose installed"
  },
  {
    "title": "Going down the rabbit hole of Git's new bundle-URI (gitbutler.com)",
    "points": 115,
    "submitter": "chmaynard",
    "submit_time": "2025-03-13T13:38:26 1741873106",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=43353223",
    "comments": [
      "This is super interesting, as I maintain a 1M commits / 10GB size repo at work, and I'm researching ways to have it cloned by the users faster. Basically for now I do a very similar thing manually, storing a \"seed\" repo in S3 and having a custom script to fetch from S3 instead of doing `git clone`. (It's faster than cloning from GitHub, as apart from not having to enumerate millions of objects, S3 doesn't throttle the download, while GH seem to throttle at 16MiB/s.)Semi-related: I always wondered but never got time to dig into what exactly are the contents of the exchange between server and client; I sometimes notice that when creating a new branch off \nmain (still talking the 1M commits repo), with just one new tiny commit, the amount of data the client sends is way bigger than I expected (tens of MBs). I always assumed the client somehow established with the server that it has a certain sha, and only uploads missing commit, but it seems it's not exactly the case when creating a new branch.\n \nreply",
      "Funny you say this. At my last job I managed a 1.5TB perforce depot with hundreds of thousands of files and had the problem of \u201chow can we speed up CI\u201d. We were on AWS, so I synced the repo, created an ebs snapshot and used that to make a volume, with the intention of reusing it (as we could shove build intermediates in there too.It was faster to just sync the workspace over the internet than it was to create the volume from the snapshot, and a clean build  was quicker from the just sync\u2019ed workspace than the snapshotted one, presumably to do with however EBS volumes work internally.We just moved our build machines to the same VPC as the server and our download speeds were no longer an issue.\n \nreply",
      "I used to use fuse and overlayfs for this, I\u2019m not sure it still works well as I\u2019m not a build engineer and I did it for myself.Its a lot faster in my case (little over 3TiB for latest revision only).\n \nreply",
      "VMware?\n \nreply",
      "The linux kernel does the same thing, and publishes bundle files over CDN[0] for CI systems using a script called linux-bundle-clone[1][0]: https://www.kernel.org/best-way-to-do-linux-clones-for-your-...[1]: https://web.git.kernel.org/pub/scm/linux/kernel/git/mricon/k...\n \nreply",
      "Have you tried downloading the .zip archive of the repo? Or does that run into similar throttling?\n \nreply",
      "How much bandwidth and time is wasted cloning the entire history of large projects when people only need single snapshot in a single branch?According to SO, newer versions of git can do,  git init\n  git remote add origin <url>\n  git fetch --depth 1 origin <sha1>\n  git checkout FETCH_HEAD\n \nreply",
      "I believe there is a bit of a footgun here because if you don't git clone then you don't fetch all branches, just the default. Can be very confusing and annoying if you know a branch exists on remote but don't have it locally (the first time you hit it, at least).\n \nreply",
      "I have a vague recollection that GitHub is optimized for whole repo cloning and they were asking projects not to do shallow fetching automatically, for performance reasons\n \nreply",
      "I don't know if that applies anymore or if it doesn't apply on GitHub Actions, but shallow clones is the default there. See `actions/checkout`\n \nreply"
    ],
    "link": "https://blog.gitbutler.com/going-down-the-rabbit-hole-of-gits-new-bundle-uri/",
    "first_paragraph": "\n\t\t\t\t\tGit's new bundle-uri could help significantly speed up clones, but what bugs lurk within?\n\t\t\t\tThis is a story about me trying to write up a somewhat obscure Git feature and falling down a long and winding rabbit hole that ended in the worlds tiniest patch to the Git codebase. Strap in.This all started with me trying to figure out the state of a feature that I had heard about from GitLab's Gitaly project called bundle-uri, which allows you to, in theory, speed up a clone by downloading a cached file that seeds your project's data before doing a server CPU-intensive fetch.In a nutshell, when you clone a Git repository, the git clone command will start a conversation with the Git server you're connecting to in order to figure out what all is available on the server and what it would like to get.Most clones for a repository end up with very similar data transfer, but every single time, the git client and server are doing this (sometimes quite complex and expensive) negotiation dance."
  },
  {
    "title": "Study finds 46 percent of U.S. counties have pharmacy deserts (ncpa.org)",
    "points": 32,
    "submitter": "toomuchtodo",
    "submit_time": "2025-03-16T23:59:44 1742169584",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=43383802",
    "comments": [
      "I live in a county bigger than Connecticut with zero pharmacies. The clinic has a big medicine pantry, but if they don't have it your next best bet is to drive to the next state over, about two hours from the clinic.  The clinic was staffed by a nurse practitioner only on my last visit. Rural America is not a convenient place to be sick.\n \nreply",
      "Isn't this like 1/2 the appeal of living in rural America though?Way out there, on your own. Fewer creature comforts, more getting stuff done on your own. No paying the connivence fee associated with cities, having everything nearby but paying double the rent.Yes this sounds like an obscene edge case but it's part of the deal\n \nreply",
      "Very few completely eschew the option to get medical services when they decide they need them.\n \nreply",
      "The point is that being far away from critical services is part of the rural-urban tradeoff. It's like being upset that there isn't a level 1 trauma center within a 30 minute drive, because you live in the middle of nowhere. Nobody is going to deny the importance of living near a level 1 trauma center, but you kinda signed up for it when you decided to live in the middle of nowhere.\n \nreply",
      "Some people do not decide.",
      "Rural America certainly votes that way. I\u2019d be curious if the people in these counties view this report as more than bashing.\n \nreply",
      "Similar story here. Our insurance likes to mail things, but gets the prescription wrong or it doesn\u2019t show up 60% of the time.We spend a lot of time arguing on the phone.\n \nreply",
      "Not just likes to mail things, they basically try to force you to use the mail pharmacy service that they own.My Aetna policy will deny all prescriptions for anything greater than 30 days supply or multiple refills unless you go through their mail pharmacy - it's the only way to get a 90 day supply.And yup. Their online service often is \"unable to determine status\" of a prescription that they are filling, which means I can't even order a refill through them.\n \nreply",
      "So let\u2019s vote for the party who isn\u2019t for healthcare for everyone. Obviously business will sort this out and put pharmacies in these places. I cannot help but remain cynical of the world that we\u2019ve inherited.\n \nreply",
      "Putting pharmacies in all those places when door to door delivery exists would be insanely wasteful. Just make it easier for pharmacists to mail you your medicines.\n \nreply"
    ],
    "link": "https://www.ncpa.org/newsroom/qam/2024/08/28/study-finds-46-percent-us-counties-have-pharmacy-deserts",
    "first_paragraph": ""
  },
  {
    "title": "Amiga 600: From the Amiga No One Wanted to Retro Favorite (homeip.net)",
    "points": 94,
    "submitter": "rbanffy",
    "submit_time": "2025-03-16T17:28:55 1742146135",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=43380649",
    "comments": [
      "If anyone was interested in the Amiga but has not kept up with recent developments, I suggest looking up the Vampire V4. It is mentioned in the article but I thought I'd add a few more details.It has a reimplementation of the ECS and AGA chipset. It includes custom extensions to the chipset to 'SAGA' which is an attempt at extending the registers to more modern standards.It also has a reimplementation of the 680x0 CPU, which is using more modern design techniques. The developer used to work on Power.Anyway putting it all together its a great system in the vein of the Amiga. Of course it is not as fast as a modern ASIC, being consumer low end FPGA based. Still it is great fun.Relevant to the Amiga 600? Well there is a standalone version but there is also a version called 'Manticore' that fits into the Amiga 600.Many people will say you can get similar performance with emulation. This is of course true though, as someone who studied microelectronics, I see the value in real hardware. Both in future potential for making an ASIC and for more precise sub-microsecond level timing.There is an alternative semi-emulation approach. i.e. emulating the CPU with a raspberry pi and using the rest of the original hardware. This is known as PiStorm and connects the GPIOs from PI onto the 68K to replace the original CPU.\n \nreply",
      "Vampire is real hardware alright, but is basically just emulation in that layer instead. The hardware has nothing to do with an Amiga. So I don\u2019t see much being won over traditional emulation in this case, other than perhaps improved input latency and performance.If I were to go there, I\u2019d go MiSTer or a clone instead, and save a load of cash in the process. You\u2019ll get an A1200 level + AGA performance, and this ought to cover by far most of the content from the Amiga scene.\n \nreply",
      "And for anyone who doesn't mind going down a massive rabbithole there are many variants of Minimig, the original FPGA-based recreation of the Amiga - most of which are on more affordable hardware than the Vampire - and open-source to boot.The versions which have a soft-CPU are significantly slower than the Vampire - but for my money feel more authentic as a result. The versions which use a real 68000 CPU are slower still - but I think the version which combines an FPGA for chipset and an actual PiStorm for CPU is currently the fastest nearly-an-Amiga...\n \nreply",
      "The Vampire is way overpriced and the extensions it adds are useless because they only work on other Vamprire machines. The PiStorm is much, much cheaper, and emulates a standard 68k while continuing to use the Amiga chipset.\n \nreply",
      "The vampire v4 is too proprietary, has too many incompatibilities and there's way too much drama around it, besides costing an arm and a leg.For a modern FPGA setup, I'd look at miSTer. There's an excellent miniMig core for it, complete with AGA support, and it is open source hardware.\n \nreply",
      "I mean, sure, but, as the Vampire makes clear in its name... it's not an Amiga, it's its own thing, wearing the Amiga's skin as a hat. You can also stick a Raspberry Pi running E-UAE inside an Amiga case for pretty much the same thing (a thing that's definitely not an Amiga, emulating being an Amiga, in the shell of an Amiga)\n \nreply",
      "older accelerators still work and are great compatibility-wise of course, there's also terrible fire which is great. I'd put Vampire the last actually, in-part due to compatibility and as you've said it.. you can pretty much do the thing with RPI and heck, you can even have rpi as an accelerator within an amiga - PiStorm.I have three A1200s, two A600s, and an A500+. In each A1200 I have in order of least to most powerful: Blizzard 1230 with 030+FPU, Blizzard 1260 with 060, and TF1260 with 060. If I'm after most compatibility and games, it's 1230 that gets out. B1230 is just plug and play, you put it into Amiga and it's faster. That's it. With 1260 both Blizzard and TF you have to install stuff, play with it etc. It's good for demos and demo coding where target always is anyways Blizzard 1260. In A600 I have one stock and one with Furia with 030. A500+ is stock (actually A500 reworked to be A500+). Technically that's the full circle of it. There's no new stuff made for it or advantage of more power than this. Demos are made for 060 anyways. I also have PCMCIA cards with WiFi on them so that I can move stuff to amigas over the air, or even remote execute code.Aside from ALL OF THAT (including a bunch of 1084s monitors), I have RPI 400 on which I have installed an emulator. It gets most use out of all of the above. I boot it up, it's an amiga, has all the crap on it and most of it works immediately. What's also cool is it's a computer in a keyboard with a mouse, just like amiga was. That's what I'd recommend anyone wanting to get a bit into it outside of WinUAE on their own comp.\n \nreply",
      "> ...I have RPI 400 on which I have installed an emulator. It gets most use out of all of the above. ... What's also cool is it's a computer in a keyboard with a mouse, just like amiga was.Worth noting that the RPi 500 is also out now, with much improved performance.  It's the most \"home computer\"-like thingy on the market, so it goes quite well with that kind of usage.\n \nreply",
      "Worth looking into pimiga and amiberry, if using raspberry pi for Amiga emulation.Of course, retroarch has an Amiga emulator as well.\n \nreply",
      "I think the article is a little hard on the 600 in its stock configuration. (That's not to say that Commodore did a great job with it, in terms of product-line placement. Oh, Commodore... >sigh<)The 600 isn't a \"repackaged Amiga 1000\" or a \"cut-down version of the Amiga 500\". It didn't make sense in the line-up when Commodore introduced it, but it's definitely a step-up from the A1000 and A500. Having the Enhanced Chip Set[0] meant getting the Productivity video modes on a multisync monitor.The internal ATA controller was also a big enhancement as compared to the A500 and A1000, too. Laptop IDE/ATA drives were getting pretty common. When equipped with a hard disk it was vastly more portable than an A500 (w/ a \"sidecar\" hard disk drive module).Having the built-in color composite video output was a \"win\" for just plugging it into a TV or VCR. Again, that helped with portability as compared to an A500.As a retro machine it is a cute little machine. The community has definitely stepped-up with enhancements for the A600, though.[0] https://en.wikipedia.org/wiki/Amiga_Enhanced_Chip_Set\n \nreply"
    ],
    "link": "https://dfarq.homeip.net/amiga-600-the-amiga-no-one-wanted/",
    "first_paragraph": "\ufffd6\u0007\b@\u0007\u0017\ufffd\ufffd\u0333\ufffd\ufffd\ufffdh\ufffd.zsE\nx\u001c\ufffd\ufffd\n\ufffd\ufffdSQ\ufffdQC\ufffd(\u0000\ufffd\ufffdA\ufffd\ufffd\ufffdk\t\ufffd\ufffd\ufffd\u00043\ufffd\ufffd\f\ufffd\u04e6^G^\ufffdU\u0002\nh\ufffd\ufffd\ufffde\ufffd#d\u0017\u0011\ufffdh2\ufffd\ufffd\u000f\\\ufffd8\u001e\ufffd\u001fd\ufffdkrsp\ufffdap\u000eh\ufffd\f\ufffd\r\ufffd\u0014\ufffd\ufffd\ufffd/\ufffd\ufffdX\ufffd\ufffd\ufffd\u001b\ufffd,\u0013\ufffd@>\ufffd\ufffd$,`\ufffd\n\ufffd\ufffd\u0016\ufffdP\ufffdc\u0371UT-\ufffd\u0012tx \ufffd\ufffd\ufffd\b\ufffdy\ufffd\ufffd\ufffd\ufffd\ufffd\u0012\ufffdtZ\ufffd\ufffd\ufffd\u0018A.\ufffd\ufffd\ufffd@\ufffd\ufffd9\ufffdz\u0156\u007f\u0002\ufffd\ufffd\u0013c\u0017\"\u038b\ufffd\u0013\u001br\ufffd\ufffd'\ufffdeL\ufffd-P\ufffd\ufffd\ufffd\ufffdo%\ufffd\ufffd\u001bR\ufffd\u0002 \ufffd\ufffd\ufffd.>\ufffd)\ufffd\ufffd\n\ufffd\u0013C\ufffd\ufffd\ufffd\u0013Bb\ufffd\ufffd\u0004\ufffd\u0011RI\ufffd\ufffd\ufffd,\ufffd=\ufffd\ufffd\u0004\ufffd\ufffd\ufffd\u0000$@\ufffd\ufffdA:D\ufffd3\ufffd/\ufffd\b:\ufffd\ufffd\fZJ\ufffdie\ufffd\ufffd\ufffd\ufffd\ufffd\u0017\ufffdQ\ufffd\u0018\u0012\ufffd;L\ufffd/\ufffd\u0633\ufffd\ufffd\ufffd\ufffd-b-\ufffdf\ufffd5\ufffd\ufffd%\ufffd\ufffdc\u0752\ufffd\ufffd\ufffdK\ufffd\ufffd\u0217J\u0007b\ufffdH\u0010\u001b(z\ufffd\u0217_\ufffd\ufffdXH\ufffd\u0000S\ufffd,D\ufffd\u0016\ufffd\ufffdT\ufffd\ufffd\ufffdmc\u0002W\ufffd@\u0014\ufffd\u0004\ufffdZ\ufffd\ufffdY\ufffd\ufffdp10\ufffd\ufffd\ufffd\u0012\ufffd\u0001*D!\u0202\ufffd\ufffd\ufffd\ufffd\ufffd(\u000bA\ufffd\ufffdDDD\tn^R\ufffd0\ufffd\ufffdV\ufffd`S\u001c\u0760\ufffd'\ufffd(\u000e\ufffd\ufffd\ufffd\u0005\ufffdY\u00a0\u001c:c*\u0004\ufffd\u0003\ufffd\u000b)\u001c\u0001\u0014\ufffd\ufffd\ufffd\u0013\ufffd\u0011\u001dm@\ufffdYl\ufffd\ufffd\ufffd!P\ufffdam\ufffd)D~\ufffd]\ufffdf\u061fk32U\u0001G\ufffd\ue994\u0014\ufffd\n\ufffd21j8\ufffd\ufffdU\ufffdFU&\ufffdx\ufffd\ufffd\ufffd\u001c\ufffd\u0594\ufffd!\ufffd\u05ab\ufffdNQ\u00181\ufffdk4\u001cb\u001f\ufffdH\ufffd\u000f\ufffd\ufffd\f:o\u000ezM\ufffd|\ufffdl\ufffd#\ufffd\f\ufffd\ufffd\u0230dEY#\ufffdi\ufffd iO\ufffd\u001dA \ufffdlyt\ufffd\ufffd\ufffdi*\ufffd\ufffdH\u0004\ufffd\ufffd\ufffd\ufffd\ufffd\u0014W\u0019-O\ufffd\ufffd\u000bE\ufffd\ufffdd+fZ\u0014\u0003\t/m\ud76e\ufffd\ufffdTp \u0798\ufffdT\ufffdI\ufffd \ufffd\ufffdT\ufffd9]\u0004\u0001\ufffd\ufffd \u000e\u0019X\ufffdn\t\ufffd\ufffd\u0014QJ\ufffd\u0000\ufffd\u0001\u000e\u0011\ufffd1\ufffd\ufffd\ufffd[\u001d$vG;l\ufffd\u001e\ufffd\ufffd\ufffdDj\ufffdy/hA\ufffd\ufffd\u0016\ufffd%P\u033c\ufffd>\ufffd\ufffd,<\ufffd\u0004=\ufffd\ufffd O\ufffd\ud835\udca2CC\u1a2b\ufffd\u0016pH\ufffd1h\u0018\ufffdF\ufffdr\ufffd\ufffd\u001bOP\ufffd\ufffd\u0001\u007f\ufffdi,\ufffd\u0012\ufffd\ufffd\ufffdj\ufffd[X\ufffd\ufffdJzE\"\ufffd\r&P\ufffd\ud92f\ude3e \ufffd\u000e\\\ufffd\ufffd2\ufffd\ufffdM\ufffd\ufffdl\u000b\ufffd`\u0015\u0017\ufffd^\ufffdI\ufffdD\ufffd \u00006K\ufffd\u0012Y\u0001\ufffd\ufffd\ufffd\u0000\ufffd+\u000fEXQ\ufffd\ufffd\ufffd\ufffdqU\ufffd\ufffd\u0016\ufffdD\u079dP\ufffd\ufffd\u00072#aD9\u0003%\ufffd\ufffde\ufffd\ufffd\ufffd\u001d\ufffd\u0000\u001e\ufffdK&\ufffdse\t\u05a9\bf\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd+\ufffd4Rg\ufffd'\u0011d\r\ufffd/\ued9a\u0017\ufffd\ufffd\ufffd\u0007\ufffdS8Y\ufffdO\u0012F$\ufffd\ufffd\u0005_\u000f\ufffdBR9\ufffdb\ufffd\ufffd_\"/\ufffd\u001cU\ufffd\n\u001f\u000e\ufffdS\ufffd\ufffdb)z\u000fo\ufffd\u001e\ufffd\u0018\ufffd\u0003j\u000b\u0013\ufffd\ufffd\u0002\ufffdR\ufffd,\u0001\ufffd(\u007f\ufffd\ufffd\bg\u000b\ufffd8\ufffds\ufffd\ufffd%z\ufffdk\ufffd\ufffd>\ufffd\u001c(\ufffdz\ufffdFL\ufffd\ufffdT\ufffd\ufffd\ufffd\ufffd\ufffd&\ufffd#\ufffdV\ufffd\ufffdD\ufffd\ufffdWtk\ufffd\ufffd\ufffd\ufffd\ufffd?\ufffd?\ufffd`\ufffd\ufffd\ufffdg\ufffd3\ufffd(Y\ufffd\ufffdm\ufffd7\u0016z\ufffd\ufffdG\u0157\ufffd\ufffd\ufffdP\ufffd&c0o\ufffd\u0010P.\ufffd\u057cU2\n\ufffdY\u001bT\ufffd\ufffd\u001b\r$y\ufffdL\ufffd\ufffd< %\ufffd,\ufffd\ufffdoQu\ufffd;p\u0010k"
  },
  {
    "title": "Show HN: 10 teams are racing to build a pivotal tracker replacement (bye-tracker.net)",
    "points": 73,
    "submitter": "jFriedensreich",
    "submit_time": "2025-03-16T13:30:36 1742131836",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=43378925",
    "comments": [
      "Make that 11 teams! :)I've been quietly working on oknext.io since late last year. If you're looking for an opinionated task manager for yourself and your team, then I'd love for you to check it out!One thing I love about it is that it sort of takes care of sprint planning for me. No more figuring out how much I can fit into a week - it does that for me, and seeing my progress over the past few weeks motivates me to keep my momentum up.It isn't just like Pivotal tracker - will likely never be - the estimation is in hours, rather than points. I plan on explaining the thought process behind this decision soon. And it isn't built specifically for software teams - I manage personal and dev and non-dev work tasks with it.If you do try it, I'd love to hear from you (vishal@oknext.io).\n \nreply",
      "I\u2019ve been a happy redmine user for about 15 years. It may not look as flashy as newer systems but I appreciate the consistent UI which hasn\u2019t broken, and someone is doing the hard work of making non flashy but extremely useful things like e-mail workflows, export as PDF, git integration, etc stay working\n \nreply",
      "Too bad its UI is ugly and its update procedures make me nervous. It needs a rewrite.\n \nreply",
      "Related: \"Pivotal tracker will shut down\" - https://news.ycombinator.com/item?id=41591622\n \nreply",
      "What made Pivotal Tracker so popular? It had story points front and center, was that it?\n \nreply",
      "The UI had very few states/pages. 99% of your time was just on the project page to with every card action 1-click away.It also had an optional feature to cap the number of story-point cards in the sprint column based on an upper estimate of the team's story-point velocity. I.e. if the team can only do at max 10pts of cards per week, that's all you are allowed to queue up in sprint. This could be overwritten to force more cards into the sprint, but in practice it provided a nice safety guard from over flooding a sprint's delivery expectations.\n \nreply",
      "I think, roughly, Tracker was a ticketing tool for developers, whereas the others are for project managers. Hence they have endless configurability, millions of views, forms, workflows, and whatnot, all of which gives project managers something to do that feels like work, but end up weighing down the people doing actual work. Tracker has the bare minimum, with no configurability, so it can't be used in that way.As a developer, i actually wished Tracker had a tiny bit more complexity (eg i want to be able to track items past \"accepted\" and into \"in production\" and \"validated with users\"). But i would rather have a bit too little than JIRA!\n \nreply",
      "I haven't used it in a decade, but I always liked it.  It was the first issue tracker that I had used that had strong Git integration, and it made very satisfying noises when you would would push a commit.It also just felt slicker than Jira; not the horrible laggy mess that Jira was for most of its history.\n \nreply",
      "Huh, I totally forgot about Pivotal Tracker. There was a phase where every project I was on used it.The killer feature was the UI. When the alternative was basically JIRA, Pivotal was so quick and easy to use. That front page view that let you see every task in a compact list, and easily drag things around to reprioritise was pretty revolutionary.Now it seems kind of pass\u00e9 but it really was pretty excellent at the time.\n \nreply",
      "Can you at least link to some of the efforts? This post is light on details.I\u2019m already seeing parallels to the MeetUp situation: MeetUp hasn\u2019t shut down, but it\u2019s becoming disliked enough that multiple teams are working on replacements. Unfortunately it seems they\u2019re all more interested in doing greenfield work of developing the replacement themselves when it would be more effective in the most dedicated contributors to the various teams were willing to join up and work on one promising replacement.\n \nreply"
    ],
    "link": "https://bye-tracker.net",
    "first_paragraph": ""
  },
  {
    "title": "Microsoft's 1986 IPO (homeip.net)",
    "points": 72,
    "submitter": "giuliomagnifico",
    "submit_time": "2025-03-16T18:41:30 1742150490",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=43381141",
    "comments": [
      "Interesting article but I'm not sure some of the statements chime with how things looked at the time (IME ofc).I got involved in IT as a profession in 1995 and even then Lotus 1-2-3 and Wordperfect were going pretty well, so the idea stated in the article that :-\"a monopoly in operating systems and office suites was inevitable before 1990 was over.\"seems a little overdone.My memory of what really tipped it for Microsoft in the office suite market (at least in the UK) was that they started giving away a full office suite with every new PC. So even though competitors might have better functionality, it was a tough sell to get past the price of \"free\".Definitely in the SMEs I worked in this was the reason that Office became the defacto option for office applications.\n \nreply",
      "I started my career in '95, too, and yeah, as difficult as it is to believe now, Microsoft's monopoly wasn't yet assured. The OS market was pretty much locked up (OS/2 Warp???), but as I recall, Wordperfect was the big obstacle in the office-suite space. It had a massive userbase, especially in markets that were not friendly to change (law offices).IMHO, the big boost for Microsoft was when they adopted the Lotus 1-2-3 and Wordperfect file formats. Now there was a product that costs less (free, with a new PC), and was compatible with all the existing docs/data. Game over.\n \nreply",
      "1995 was the tipping point.  Windows 95 came out and WordPerfect was late coming up with a good Windows app.\n \nreply",
      "In my memory (was young at the time), 95 coming out was such a huge event. Something about its emphasis on multimedia experience really changed the whole feel of the PC, at least for me. 3.1 was like this arcane puzzle, but 95 felt personable.\n \nreply",
      "It was a very big deal.  It was introduced with the help of Jay Leno.https://m.youtube.com/watch?v=Dv0PxINy2dsPeople lined up for it.\n \nreply",
      "Don\u2019t forget the Rolling Stones Start Me Up was the marketing song. We \u2018leet hackers all joked that it was entirely appropriate for \u201cWindoze\u2019s\u201d theme song to include the lyrics \u201cYou make a grown man cry.\u201d Those were the days!\n \nreply",
      "By 95 it was game over. WP for Windows sucked. Quatro Pro was dying. Office 95 was the final nail.\n \nreply",
      "I was only starting working with computers as an unpaid IT help at that time, but I distinctly remember that MS Office was just better. More streamlined, and well-integrated.Back then, pretty much everybody in my country just pirated the software, so the cost was not a consideration at all.\n \nreply",
      "MSO 97 was the pinnacle of usability and customizability per megabyte installed.I only wish it were not so buggy when handling large documents.\n \nreply",
      "People play up Bill Gates\u2019 connections, sure, but next to no 16-year-olds could program BASIC in assembly.Microsoft\u2019s success was as much to do with them being a programming languages company first. DOS, Windows 3.1, and even Windows 95 shipped with an interpreter and compiler for their BASIC and C, respectively. This empowered developers to use and write code for the OS out-of-the-box.\n \nreply"
    ],
    "link": "https://dfarq.homeip.net/microsofts-1986-ipo/",
    "first_paragraph": ""
  },
  {
    "title": "Tcl Tutorial (tcl-lang.org)",
    "points": 84,
    "submitter": "Tomte",
    "submit_time": "2025-03-16T18:48:39 1742150919",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=43381195",
    "comments": [
      "If you want to play with reading/recreating a very small Tcl interpreter, recently I put Picol (a 500 lines of C code Tcl interpreter) on Github. It was still on the web, but a bit more \"hidden\". I had a chance to re-read the code, and it is not in the category of code I regret writing :D Still relatively useful for newcomers, I hope.https://github.com/antirez/picol\n \nreply",
      "Thanks for that effort. Some time ago, I got Picol working on an Arduino-compatible microcontroller board with minimal effort. I was working towards making my own programmable calculator, and test-drove a number of simple interpreters, including yours and a couple others.Project languished due to my attention span running out, but I still have my adaptations of those codes on my drive, in case I ever pick it up again.\n \nreply",
      "The once-ubiquitous open-source package manager for macOS, MacPorts, is basically a Tcl app.That is to say, its packages are Tcl.I haven't used it in many years, as is has been largely replaced by Homebrew, which uses Ruby.(I once maintained a MacOS port of a good-sized scientific analysis package. Hundreds of MacPorts packages, I have debugged.)https://www.macports.org/\n \nreply",
      "In the 1990s, an awful lot of Linux apps were Tk/Tcl. People complained then about them not being \"native\", but they were slimmer and better performing than the typical Java or Electron app of today.\n \nreply",
      "It\u2019s only because TK was so butt-ugly\n \nreply",
      "I don't think homebrew has replaced MacPorts. I suspect that MacPorts has more ports.\n \nreply",
      "MacPorts has far more packages than Homebrew and was implemented by the creator of the original FreeBSD ports system, who was also an employee on Apple\u2019s UNIX team. MacPorts is the standard macOS package manager.\n \nreply",
      "> MacPorts is the standard macOS package manager.No, it's not.  macOS does not have a standard package manager.\n \nreply",
      "https://www.gnu.org/fun/jokes/ed-msg.en.html\n \nreply",
      "What do you mean by \u201cstandard\u201d?I like MacPorts, but most macOS using developers I\u2019ve conversed with over the past 12+ years never heard of it, while all of them used Homebrew. Maybe it depends on what developer circles one inhabits.\n \nreply"
    ],
    "link": "https://www.tcl-lang.org/man/tcl8.5/tutorial/tcltutorial.html",
    "first_paragraph": ""
  },
  {
    "title": "Teach, Don't Tell (2013) (stevelosh.com)",
    "points": 90,
    "submitter": "Tomte",
    "submit_time": "2025-03-16T17:55:42 1742147742",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=43380833",
    "comments": [
      "When I come across bad documentation (which is often), by far the most common problem I find is that the information I needed just plain isn't present.Articles like this one really aren't helping. If you wrote a piece of software I'm using that doesn't make you my teacher. It makes you someone offering a contract, and what I need to know is what that contract says.The first duty of your documentation is to be complete and correct. Unless you've got that sorted, no amount of \"putting yourself in the student's place\" is going to give adequate results.\n \nreply",
      "I actually like re-writing documentation, but if I can not reorganize the documentation too I am discouraged from re-writing. I believe a lot of contextual information is lost when people use confluence as a dumping ground and don't plan the structure of the documents.I've emotionally moved to git for documentation, but I can not get people to follow or transition to better documentation methods once someone is emotionally tied to tribal knowledge and communication.\n \nreply",
      "There are 3 types of documentation:1. Why/what\n2. API spec\n3. TutorialYou need all 3. They are distinct, use different styles, and exist for different purposes and audiences.\n \nreply",
      "You actually need four https://docs.divio.com/documentation-system/Explanation (why, what)reference (API spec)tutorialsHow to guides\n \nreply",
      "There's another more thorough version from the same author, for what it's worth. Just moved on from that company but the overall points are the same https://diataxis.fr/\n \nreply",
      "Right! The good old days when the software used to come in a box with several varied width books containing all these Four, and one named Getting Started.This same set could be easily \"transposed\" to the contemporary world of web. With all the proper indexing. Why is this \"art\" \"lost\" for most of the software :-( ...BTW, one Excellent incarnation of this documentation art is on the front page right now:https://news.ycombinator.com/item?id=43381627\n \nreply",
      "> Why is this \"art\" \"lost\" for most of the software.Before the internet, the printed book was all you really got.  That meant the company distributing the software had to hire technical writers who'd work with the software devs to create all of this, send it to an editor, and ultimately get published.We no longer live in an era where tech companies hire tech writers.  Software documentation lacking is something that can and limp along with jira cases and support services sold rather than trying to put in the upfront effort to fix everything.Now, for open source software, hate to say it but the docs have always been pretty crap.  Certainly some stands out (usually when the business model was around providing services on top of open source software), but nobody is really paying anyone and few people really want to do that sort of free labor.\n \nreply",
      "Thanks! Knew I was missing something. I guess in my mind tutorials and howtos merged :D\n \nreply",
      "Making these 3 from static websites generated from docstrings is a multi-billion dollar industry called LLMs.\n \nreply",
      "A docstring won't contain the necessary context for \"why\", and is something I see coding assistants get consistently wrong without human data.\n \nreply"
    ],
    "link": "https://stevelosh.com/blog/2013/09/teach-dont-tell/",
    "first_paragraph": "Posted on September 3rd, 2013.This post is about writing technical documentation.  More specifically: it's\nabout writing documentation for programming languages and libraries.I love reading great documentation.  When I have a question and the\ndocumentations explains the answer almost as if the author has magically\nanticipated my problem, I get a warm, fuzzy feeling inside.  I feel a connection\nwith the writer that makes me smile.I also love writing documentation.  Being able to rewire the neurons in\nsomeone's brain so that they understand something they didn't understand before\nis extremely satisfying.  Seeing (or hearing about) the \"click\" when a bunch of\nconcepts suddenly fall together and make sense never fails to make my day.This post is going to be about what I think good documentation is and how\nI think you should go about writing it.  I'm not perfect, so you should take\neverything with a grain of salt, but I hope you'll find it useful and\nthought-provoking even if you don't agre"
  },
  {
    "title": "Everything Picolisp can do, and more (picolisp.com)",
    "points": 51,
    "submitter": "damir",
    "submit_time": "2025-03-16T19:49:02 1742154542",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=43381627",
    "comments": [
      "Gotta admit, the author has a nice sense of eDSL https://picolisp.com/wiki/?taskDB\n \nreply",
      "I initially confused this with Lisp for microcontrollers [1][1] http://www.ulisp.com/show?3J\n \nreply",
      "PicoLisp is like an ancient technology or species which somehow survived to this day. It always felt to me like on the cusp to mainstream acceptance.\n \nreply",
      "\"No arrays nor floating point numbers\" and \"mainstream acceptance\" don't live in the same world, in my eyes.Fexprs are cool, though.\n \nreply",
      "Agreed, at least on the floats. I used PicoLisp in the past some, but I had to call C functions for floats.\n \nreply"
    ],
    "link": "https://picolisp.com/wiki/?Documentation",
    "first_paragraph": "https://picolisp.com/wiki/?documentation\n\n31jan25\n\u00a0\u00a0\u00a0llawrence\n\n\n"
  },
  {
    "title": "That Time I Recreated Photoshop in C++ (f055.net)",
    "points": 196,
    "submitter": "f055",
    "submit_time": "2025-03-15T18:22:15 1742062935",
    "num_comments": 81,
    "comments_url": "https://news.ycombinator.com/item?id=43374278",
    "comments": [
      ">  \"It followed a set of five rules to benefit the end user: no installers, no archives, no registry keys, no additional runtimes and a single executable file.\"Reading this sentence made me feel warm and happy. I get why the registry exists and things work the way they do today around the Windows software ecosystem but... damn, I really miss the days when most desktop software was more like this. These days I try to use portable installs whenever they're available, I just wish it was more common. The time, inconvenience and uncertainty I'll be able to fully restore all my preferences makes me actively avoid reinstalling Windows.\n \nreply",
      "There are lots of useful actions an installer does that (some, if not most) end users actually want, including but not limited to adding shortcuts to start menu so you can find it or search for it, associating file formats, registering to Windows' program list, etc.I was a big fan of \"portable\" software, but nowadays if a software offers both ways, I actually prefer using installer. Otherwise I have to manually add them to Start Menu to be able to search for it, to begin with.I do hate registry keys, simply for the fact they are often lost after reinstalling the OS. Please just keep all the settings in %appdata%!\n \nreply",
      "I\u2019m really fond of the way macOS does this, which is to stick this information in the executable.Back in the day, this worked via a flag in the metadata for each file. When you got a new executable, the flag was unset. The OS would see a file with an unset flag, ask \u201cIs this an executable? What files can it open?\u201d and then add this information to a database called the desktop database.This is why file associations work on a Mac without an installer, and without writing any code (you just have to write the code for \u201copen file in response to OS request\u201d).\n \nreply",
      "Furthermore, the OS can pick up the extensions, services, etc the app carries within its app bundle and offer them to the user to enable this way. No need to copy files to obscure directories (be it manually or with an installer), and when you trash the app bundle they\u2019re all gone.This is why merely having a copy of an app is enough to make QuickLook able to preview the filetypes the app is capable of opening. The system picks up the QuickLook extension in the app bundle and enables it.It\u2019s not perfect since small .plist config files get left behind, but it\u2019s a whole lot more clean by default compared to the Windows approach.\n \nreply",
      "> which is to stick this information in the executableThis is neat, shame they went completely the other way with file metadata. It forces a hidden .ds_store file in every single folder a mac touches just to store info like access time. Every other system stores this in the file itself.\n \nreply",
      "Not to be too annoying, but that file is how it supports the feature on non-posix-compliant file systems (usually FAT). On filesystems that support file-streams (or the original HFS), that metadata is stored on the file itself\n \nreply",
      "You're thinking of resource forks. .DS_Store is stored as \".DS_Store\" even on APFS.\n \nreply",
      "Considering how pushy Apple normally are when it comes to forcing their customers to use new things so they can obselete old ones, I am very surprised where Apple have drawn the line in supporting older systems.\n \nreply",
      "Windows does the same thing with 'Thumbs.db' files. They're just hidden from you on Windows systems, just as .DS_Store files are hidden on Unix-like systems; but you'll see them if you use a thumb drive with both types of machines.\n \nreply",
      "So why are the smb shares on my linux file servers full of millions .ds_store files but zero thumbs.db files? I have to run automated scripts to remove them regularly, and try to run the commands on the macs to stop them writing ds_store files to network shares [0]. But still it seems to magically reenable itself every now and then. This is a problem I have never had with windows machines and 'thumbs.db' files.[0] https://web.archive.org/web/20190714230437/https://support.a...\n \nreply"
    ],
    "link": "https://f055.net/technology/that-time-i/that-time-i-recreated-photoshop-in-c/",
    "first_paragraph": ""
  },
  {
    "title": "When the Dotcom Bubble Burst (homeip.net)",
    "points": 95,
    "submitter": "rbanffy",
    "submit_time": "2025-03-16T17:02:34 1742144554",
    "num_comments": 68,
    "comments_url": "https://news.ycombinator.com/item?id=43380453",
    "comments": [
      "To me the most sobering cautionary tale from the dotcom bubble is the story of Cisco. Cisco manufactured, in a very real sense, the physical infrastructure of the internet: the routers, switches, modems, etc. that directed the IP packets to their destinations. (To a significant extent they still do, though nowadays they have more competition in that area.)Savvy investors piled in to the stock, reasoning that, while internet startups might come and go, the internet itself was surely here to stay. It was popular to observe that, in the California gold rush of the mid-1800s, the purveyors of mining equipment made it rich more reliably than the prospectors for gold.Anyway the Cisco stock price peaked in March 2000, and to this day it still has not reached that level again. The savvy investors were of course correct in their belief that the internet would continue to be important, and that Cisco would continue to be an important manufacturer of internet networking equipment. But they lost money anyway, because once the euphoria had worn off the market consensus was that the stock just wasn\u2019t worth as much as the price it had been selling for at the height of the mania.Any parallels to hot contemporary stocks are left as an exercise for the reader \u2014 and I do not mean to suggest that history must always repeat exactly.\n \nreply",
      "Obviously you don't mean NVIDIA, because 80% margins on matrix multiplication will last foreverrrrr.\n \nreply",
      "And I can't name several customers with very deep pockets working on their own chips to squeeze/compete with NVDA.\n \nreply",
      "I think you're not naming entire nation states, given the way the Chinese are splashing money around there it looks like there is about to be a flood of chipmaking capacity hitting the market and a lot of that will end up competing in matrix multiplying.Although I hear VCs are working on an algorithm where they burn money directly and the smoke patterns represent the solution to the multiplication. That might keep the margins high.\n \nreply",
      "Those Chinese companies will have to fight our western champions, TSMC in a green trenchcoat, TSMC in a red trenchcoat, TSMC in a blue trenchcoat, TSMC in a purple trenchcoat, and TSMC in a yellow trenchcoat.\n \nreply",
      "Is this after the same Spidermen TSMCs got down from all sitting on top of each other?\n \nreply",
      "Microsoft, Meta and Oracle already spend billions on supposedly completely inferior(according to the Nvidia cultists) AMD chips.  Nvidia is the biggest bubble in human history.\n \nreply",
      "> And I can't name several customers with very deep pockets working on their own chips to squeeze/compete with NVDA.Google, Amazon, and Microsoft all have custom ASIC and TPU projects in their pipeline, and what holds true today might not hold true in 5 years.A major reason Nvidia was able to do so well was because of technical outreach by donating their GPUs to programs all over the US, building a strong albeit self serving OSS relationship, the CUDA ecosystem, and the acquisition of Infiniband.Much of these advantages can be nullified by competitive margins and pricing for hyperscalers designed and owned hardware.Cisco was hit by this same situation as server vendors like Dell began integrating their own in-house networking functionality within their servers, and Dell itself was outcompeted by cloud vendors.\n \nreply",
      "Agreed, the only thing I'd add would be AMD's self-sabotage. Pretending to have compute capability (see also \"right around the corner\") for over a decade when it was actually so broken as to be sub-viable was an enormous own-goal that they doubled down on when they exited desperation mode without rehydrating the teams and tripled down on with the RDNA/CDNA split.By rights they should have been fast followers, but they stacked the critical mistakes so high that frankly I think NVIDIA's subsidized GPGPU classes are the smaller part of this story. If the people struggling to get OpenCL to work had been able to get it to work (on other than NVIDIA GPUs lol) the situation would look very different today.\n \nreply",
      "You see different versions of the four horsemen of the internet but the one I remember (and I can find lots of references to on the internet) is: Cisco, Sun, EMC, and Oracle. And, indeed, Oracle is the only one continuing to perform whether you like them or not and whether startups use them at this point.With respect to Cisco specifically (and Intel) there was also a huge optical networking bubble.\n \nreply"
    ],
    "link": "https://dfarq.homeip.net/when-the-dotcom-bubble-burst/",
    "first_paragraph": ""
  },
  {
    "title": "Local-First and Ejectable (thymer.com)",
    "points": 76,
    "submitter": "yamrzou",
    "submit_time": "2025-03-14T14:00:34 1741960834",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=43362725",
    "comments": [
      "I run an offline-first app[0] that has a sync mechanism (over Yjs). To solve server longevity we just sell you a lifetime license of the app and allow for file-based sync.While I agree with the sentiment of the article most users do not even know what a server is, much less capable of self-hosting. Syncing a folder over Dropbox or Google Drive, though, is simple enough.[0]: https://nestful.app\n \nreply",
      "This pretty much matches the way I think it should be done ideally. I'm still pretty new to local-first though. Are there data formats or types of apps that don't lend themselves well to file-based sync?For your app, how do notifications of changes get propagated? Does it depend on the backend (Google Drive, etc) supporting that, or does it just do polling or something?\n \nreply",
      "For the desktop version just watching the directory for changes is enough to achieve that. Nestful is not aware the files are syncing, it just reads files as they are.\n \nreply",
      "Sure but not all backend operations can always be mapped to flat files on Dropbox, like when you have certain real-time collaboration features, syncing on graph/tree-like data structures, or perhaps permissions in case of a company tool. That's why longevity of the \"data\" aspect is usually easy to solve when a copy of the data is already local, but then you still risk losing a part of what it is that makes the app when there is no way to run the app's syncing backend yourself.\n \nreply",
      "It serves more use cases than you might think, especially with CRDTs. Nestful is a tree structure and uses Yjs for sync. Permissions can be done in the file system level.Yes, I agree this is not an end-all be all solution, but the tradeoff is often worth it.\n \nreply",
      "I'm currently building an offline-first app that has a custom sync between a local SQLite and Postgres (Supabase). The \"ejectable\" idea here is so good and I will definitely implement something that turns all your saved data into a spreadsheet with a few tabs.\n \nreply",
      "Not related to the topic, but I first came across Thymer about (I want to say) 2 years ago when I looked for a todo.txt GUI. I found the blog they used to document building it: https://80daystartup.com/Somehow, this thing is still not available for testing yet. Not hating but isn\u2019t this supposed to be the category where new solutions launch every single day? While I am still very much interested, I\u2019ve lost any hope that this will become a real thing to try out any time soon.\n \nreply",
      "It is possible that life got in the way. Thymer did seem a great idea.\n \nreply",
      "I think this is a subset of the fifth (of the seven) ideals of local-first software https://www.inkandswitch.com/local-first/#5-the-long-now> Local-first software enables greater longevity because your data, and the software that is needed to read and modify your data, are all stored locally on your computer.There are many ways to achieve this goal - open document standards, open source servers, a escrowed release of the server, or this idea of a bail out system for taking the current version and self hosting it. All are commendable.Actually achieving all seven ideals is hard, and there isn't much modern software that does it. But in my view anyone trying to achieve even a few of the ideals is making strides towards a worthy goal.There is a lot of exciting stuff happening in the local-first software movement at the moment, and a lot of that is related the sync engines that are being built (disclaimer: I work for ElectricSQL, we are building one). These sync engine don't inherently make your software local-first, fitting all the ideals, but they do make it a lot easer to do so. They are an important building block. But there is more needed, we need more open document standards - Automerge, Yjs, Loro and the other CRDT data structures are perfect lower level data structures to build these higher level abstractions on. Martin Kleppmann has talked quite a bit about sync engines that are disconnected from the underlying application, essentially pluggable sync engine, you choose who to use to sync you copy of a document or application - I'm excited to see where this goes.But, we also need to free up the application distribution platform. The app stores are walled gardens that prevent some business models, native apps (while performant) are tied to a specific platform (or even version!), and the web is inherently tied to servers and browsers. The web platform though, i.e. JS, HTML, CSS, is perfect for building high longevity software that runs anywhere, on any device, the issue is the distribution. We need a middle ground, an application package that built on web standards, but isn't tied to a server. I want to download a bundled app to my machine and have a copy, email to a family member, or even open and hack on it. Thats the final missing piece.A downloadedable app, with an open and pluggable sync engine would achieve the same goles of this ejectable idea.\n \nreply",
      "If you use a local, browser-based database, and allow your app to be installed as a PWA, then you are ejectable? The only \"server\" is just a host for HTML+JS that your browser automatically downloads when you visit the site. The app runs locally, like a desktop app, inside your browser. No need for the items listed in the article.\n \nreply"
    ],
    "link": "https://thymer.com/local-first-ejectable",
    "first_paragraph": ""
  },
  {
    "title": "From Languages to Language Sets (gist.github.com)",
    "points": 14,
    "submitter": "whatever3",
    "submit_time": "2025-03-14T07:12:43 1741936363",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=43360287",
    "comments": [
      "One thing I'll note is we tend to use languages from different levels in different settings (front end, back end, systems) and we spend an awful lot of time writing glue code to get them to talk to each other.A major advantage of the proposed approach is automated FFI and serialization/deserialization between languages in the same language set. RustScript would be able to accept a struct or enum from Rust or RustGC, and vice-versa. You could have a channel with different languages on either end.You can also see that we _want_ something like this, e.g. we bolt TypeScript on top of JavaScript, and types onto Python. If JavaScript (or python) were designed so they could be more easily compiled (notably, no monkey patching) then they would support level 2 as well.I have been thinking of level 2 or 1 languages that support the higher levels. This is a really good framing. (The problem with going the other way is the implementation decisions in the interpretter often constrain how the compiler can work, e.g. CPython is dominant because all the libraries that make use of the CPython FFI, and similarly for NodeJS. It is easier to interpret a constrained language than to compile a dynamic language designed with an interpretter in mind).\n \nreply",
      "Back when I did some high perf Python, I\u2019d define my data at C structs and bump allocate those structs in a list using the cffi.It is not unlike defining your data model for SQL so that you can have sane data access.\n \nreply",
      "2022https://hn.algolia.com/?query=From%20Languages%20to%20Langua...\n \nreply",
      "I\u2019m a strong supporter of adding an automatic GC to Rust. Although it seems difficult to justify as RustGC code wouldn\u2019t be trivial to convert to traditional Rust. But going in the opposite direction should be trivial.\n \nreply"
    ],
    "link": "https://gist.github.com/xixixao/8e363dbd3663b6729cd5b6d74dbbf9d4",
    "first_paragraph": "\n        Instantly share code, notes, and snippets.\n      After working with a lot of languages, writing my own, this is currently what I consider the most useful classification of programming languages, into 4 levels:There is a 0th level, assembly, but it\u2019s not a practical choice for most programmers today.Now every language trades off \u201cease of use\u201d with \u201cperformance\u201d. On this hierarchy the higher numbered, \u201chigher level\u201d, languages are easier to use, while the lower numbered, \u201clower level\u201d, languages are more performant.I postulate that for most programming, the \u201cbusiness logic\u201d kind of programming, we want to use a language that sits right in the middle of that hierarchy. Looking at the languages listed that\u2019s no revelation. One language could combine the 2nd and 3rd level though. A language that can be interpreted during development for fast iteration cycle, but compiled for better performance for deployment. There isn\u2019t such a language popular today though.Now let\u2019s address level "
  },
  {
    "title": "Kastle (YC S24) Is Hiring Engineer #3 (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-03-16T21:00:22 1742158822",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/kastle/jobs/XSq5nJT-founding-applied-ai-engineer-at-kastle",
    "first_paragraph": "AI agents for mortgage servicingKastle is an AI platform serving America\u2019s largest mortgage lenders, helping them collect payments, resolve disputes, and streamline loan processing. Backed by $2.3M from Y Combinator, Commerce Ventures, and executives from Snapdocs, Google, and WePay, we\u2019re redefining loan servicing with AI.As a Founding Applied AI Engineer, you will play a crucial role in integrating advanced AI technologies into Kastle\u2019s platform. You\u2019ll work on fine-tuning large language models (LLMs), designing AI workflows for highly regulated enterprises, and ensuring our AI agents deliver precise, compliant, and effective interactions.This role is ideal for an engineer who is passionate about applied AI, eager to solve real-world challenges, and excited to build the technical foundation of an early-stage AI startup.If you\u2019re excited to push the boundaries of applied AI and help scale a category-defining company, we\u2019d love to hear from you. Apply now to join Kastle as a Founding A"
  },
  {
    "title": "A Guide to Undefined Behavior in C and C++ (2010) (regehr.org)",
    "points": 40,
    "submitter": "GarethX",
    "submit_time": "2025-03-13T12:16:33 1741868193",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=43352503",
    "comments": [
      "This is an area the newer languages get right - I don\u2019t think Rust or Go has any undefined behavior? I wish they would have some kind of super strict mode for C or C++ where compilation fails unless you somehow fix things at the call sites to tell the compiler the behavior you want explicitly.\n \nreply",
      "I think data races can cause undefined behavior in Go, which can cause memory safety to break down. See https://research.swtch.com/gorace for details.\n \nreply",
      "> I wish they would have some kind of super strict mode for C or C++ where compilation fails unless you somehow fix things at the call sites to tell the compiler the behavior you want explicitly.The C++ language committee _does not_ want to add more annotations to increase memory safety.\n \nreply",
      "Several programming languages can testify to the fact than a Benevolent Dictator For Life is not a panacea. Several more than testify that having a Committee to design the language is likewise not a panacea. Perhaps uniquely C++ can clarify for us that both is in fact worse than either alone.\n \nreply",
      "Safe Rust has no undefined behavior. Unsafe Rust does.\n \nreply",
      "The first example (signed integer overflow) is no longer valid in newer standards of C. Now it should use the two-complement semantics and no UB.\n \nreply",
      "I believe they only standardized the two's-complement representation (so casts to unsigned have a more specific behavior, for example) but they did not make overflow defined.\n \nreply",
      "my opinion as a very experienced C system programmer:there must be better sources to guide people than a poorly written and infantilizing article from 15 years ago.\n \nreply",
      "Being a very experienced programmer, I'm sure you know many such sources. Can you share any?\n \nreply",
      "Perhaps you could draw on your wealth of experience to write one. I\u2019d love to read it!\n \nreply"
    ],
    "link": "https://blog.regehr.org/archives/213",
    "first_paragraph": "Also see Part 2 and Part 3.Programming languages typically make a distinction between normal program actions and erroneous actions. For Turing-complete languages we cannot reliably decide offline whether a program has the potential to execute an error; we have to just run it and see.In a safe programming language, errors are trapped as they happen. Java, for example, is largely safe via its exception system. In an unsafe programming language, errors are not trapped. Rather, after executing an erroneous operation the program keeps going, but in a silently faulty way that may have observable consequences later on. Luca Cardelli\u2019s article on type systems has a nice clear introduction to these issues. C and C++ are unsafe in a strong sense: executing an erroneous operation causes the entire program to be meaningless, as opposed to just the erroneous operation having an unpredictable result. In these languages erroneous operations are said to have undefined behavior.The C FAQ defines \u201cundef"
  },
  {
    "title": "Laplacian Mesh Smoothing by Throwing Vertices (nosferalatu.com)",
    "points": 23,
    "submitter": "nosferalatu123",
    "submit_time": "2025-03-13T00:30:19 1741825819",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=43349204",
    "comments": [
      "Isn't this a fancy name for just iteratively moving a vertex towards the center of mass of its connected vertices?\n \nreply",
      "It\u2019s a shorter name for that.\n \nreply"
    ],
    "link": "https://nosferalatu.com/./LaplacianMeshSmoothing.html",
    "first_paragraph": "In this blog post, I\u2019ll talk about smoothing and blurring 3D meshes using Laplacian mesh smoothing. A good example of where this is useful is Adobe Substance 3D Modeler\u2019s smooth tool which I implemented using Laplacian mesh smoothing. Laplacian mesh smoothing works by iteratively shifting each vertex towards the average position of that vertex\u2019s neighbors. The math formula for this\u00a0is:which we can implement in code\u00a0as:Run that a few times, and the mesh smooths out like magic. It\u2019s important to note that this process only changes the vertex data of the mesh; the connectivity (triangle indices) are\u00a0unchanged.The above is straightforward except that we need the neighbors() and numNeighbors() functions. While one approach is to build a half-edge mesh data structure, which gives us information about the connectivity of the mesh, this blog post will show two alternative methods that can be used when the mesh is two-manifold (where each edge connects exactly two\u00a0triangles).Forget about neighb"
  },
  {
    "title": "Who Is Free Software For? (tante.cc)",
    "points": 20,
    "submitter": "NotInOurNames",
    "submit_time": "2025-03-16T21:09:46 1742159386",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=43382344",
    "comments": [
      "> With Open Source/Free Software as well as Creative Commons we have build pipelines to contribute to the commons (great!) but we never thought about how to defend those commons against appropriation.Perhaps the CC people have not thought about this, but it is precisely why GPL exists, and later AGPL.If courts find that derivative work comes from AI systems, the AGPL will become a very interesting legal landscape for even hosted models.\n \nreply",
      "AI tools are removing coding barriers so everyone benefits except those who depend on restrictions, but even they benefit in the long run.A couple minor issues: the AGPL and similar licenses exist and have been effective at stopping classes of appropriation, and the cake quote is the common misconception. It's referring to the nutritious leftovers usually discarded, and free software means the corporate code deemed nonessential enough to release under free licenses gets spread around, so let's eat that delicious free software cake.\n \nreply",
      "There's a reason why FOSS is by hackers for hackers\u2014it's software written by people to solve their own needs. They release that software as Free/Libre so that others can benefit from it if they would like to, but first and foremost it's written because the author themselves saw a problem and wanted a solution. What's changed is that as startup culture infected hacker culture a few subcultures developed within FOSS that thought that that wasn't good enough.One of those subcultures decided that it was an imperative that they make more money off of their FOSS projects than anyone else does\u2014this is how you get the Matt Mulenwegs of the FOSS world, who diss on other FOSS developers because \"most of the value\" of their projects was \"captured by others\" [0]. They see FOSS as a revenue stream and if it's not a revenue stream (or if it ends up being a bigger revenue stream for someone else than it is for you) it's a failure.Another subculture thinks that FOSS should be mainstream. That's the idea typified here: by \"hackers for hackers\" is selfish and we should instead \"reshape our thinking towards more political goals and values\".The fact is that both of these perspectives are fundamentally not about adapting FOSS to the times\u2014they're both about building two entirely different philosophies. FOSS is not a business model, and what the Matt Mulenwegs of the world actually want is to be a tech startup CEO. And FOSS is not and cannot be mainstream because to become mainstream requires a bucketload of UX design and user support work that no one in their right mind wants to take on for free as a hobbyist.FOSS is by hackers for hackers because it must be. If you want to make more profit than anyone else does off your project or if you want to pursue \"political goals and values\" you're looking for an entirely different type of organization.[0] http://web.archive.org/web/20241014235025/https://ma.tt/2024...\n \nreply",
      "Free Software isn't for hackers. It's for users. Hackers are also users, but they're not special.The purpose of Free Software isn't to teach people how to program. It's not a scholarship program for gifted kids to generate startups. It's to institutionalize the right to access and change that software that defines the behavior of the machines that control our lives. You may say that only a programmer can take advantage of that right, so that means that Free Software is meant to create hackers. But I can hire a hacker just like I hire a plumber, or like my condo association hires a roofer, or my town hires a construction firm.There should not be machines in your life that run on secret commands that you are not allowed to know about. They are not working for you.edit: it is best not to talk about Free Software Licenses and Open Source/Creative Commons licenses at the same time if you're not talking as a consumer. They don't have much to do with other, other than that their development models are similar and that Open Source software will always be available over the same channels as Free Software.Copyleft licenses like the GPL are very restrictive licenses that try to make sure that access to the code that they cover will never be restricted to a user of that code. They are an attempt to grant the rights that people should have to all code to enough code that one can accomplish one's goals without having to touch a locked-down black box, and to forbid the makers of locked-down black boxes from taking advantage of, embracing, extending, or extinguishing that code. The copyright holders of copyleft code are granting you that right of access and modification instead of the government. Open Source/Creative Commons has zero interest in that.Open Source/Creative Commons are liberal licenses that allow anyone to do what they want, and make no (or very few and trivial) demands. You can take it and lock it behind any license. They are only compatible with copyleft licenses because copyleft licenses are part of the class \"any license,\" not for any other reason.\n \nreply",
      "While I mostly agree with this, it's inaccurate to say that Open Source Software is more liberal than Free Software. The issue of copyleft vs. permissive licenses is orthogonal to that classification. The difference between Free Software and Open Source is mostly that of intent: Free Software is primarily political, with Freedom being the ultimate goal, while Open Source is primarily practical, with high quality software being the ultimate goal. In practice, almost all software that meets the definition of one meets the definition of the other.\n \nreply"
    ],
    "link": "https://tante.cc/2025/03/03/who-is-free-software-for/",
    "first_paragraph": "Smashing Frames\n\n\n\n\n\n\n\n \n\t\t\t\t\t\t\t\tSubscribe\t\t\t\t\t\t\t\n\u2014byFor a while I have been arguing that maybe there are some issues with the whole \u201cOpen*\u201d movements, their founding myths and ideologies (see for example my talk at Fluconf). This criticism comes from a place of love. All the writing on this blog is licensed CC-BY-SA to allow others to take the texts and do something with it: I release my work under those conditions because I believe that we need strong and rich commons to flourish as a society but also as communities, groups and individuals. I\u2019ve also been running my own personal systems (servers, my own laptops and a few other systems) on Linux for more than 20 years now. I am deeply embedded in the space of open culture but also Free and Open Source Software.This morning I made a bit of an off-hand remark that summarized a few thoughts going on in my head: People within the Open* movements have done the impossible, have created whole encyclopedias, the most successful and most used "
  },
  {
    "title": "\u201cQWERTY wasn't designed to solve type bar jamming\u201d [pdf] (kyoto-u.ac.jp)",
    "points": 61,
    "submitter": "vishnuharidas",
    "submit_time": "2025-03-16T18:34:37 1742150077",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=43381088",
    "comments": [
      "So, to summarize, the original typewriter layout (1868) was alphabetical, with the top row (A-N) going left-to-right and the bottom row (O-Z) going right-to-left.  Then (1870), the vowels (including Y) were pulled out and put in a separate top row.  After that (by 1872), changes were made in order to better support the use case of people receiving Morse code, and that's when we finally start to see something that looks like QWERTY.  Additional changes later got it to the modern form, but by 1872 something QWERTY-like was in place.And yeah -- if you look at the bottom two rows of a QWERTY keyboard, you can still see what remains of that alphabetical ordering, being left-to-right on one row followed by right-to-left on the row below!\n \nreply",
      "In the next century, researchers will discover that the GUI wasn't designed to make computing harder by forcing people to find cryptic little symbols, randomly arranged on the screen, and break routine operations into tiny sequences of manual steps. And it wasn't called a \"personal computer\" because it turned each person into a computer.\n \nreply",
      "This reminds me of Motel of the Mysteries :^)\n \nreply",
      "\"The legend was referred by Prof. James V. Wertsch,[22, 23] a professor of the Department of Psychology, Clark University, then it was regarded as an established theory in the field of psychology. \"The reproducibility crisis struck early, it seems.\n \nreply",
      "Ha. so the reason that I is next to 8 is that early typewriters used the I as a 1(no independent 1 key) and the morse transcription company wanted to type years(1871) quickly. I love it.\n \nreply",
      "Whatever its intent, QWERTY definitely hasn't impeded the fastest typists, who can regularly exceed 200wpm these days.Odd to see no mention of the Linotype layout, also known as the \"Etaoin Shrdlu\", given that was also a common competing keyboard layout in that era.\n \nreply",
      "Humans do not have fins, but Micheal Phelps can still cut through water. That elites can thrive is not a compelling argument when most people just want technology to get out of the way.An alternative layout with commonly used symbols on the home row makes the QWERTY deficiencies immediately apparent. Significantly less effort required for writing prose when using something like DVORAK.\n \nreply",
      "Dvorak is way more comfortable and is very fast for me, but its emphasis on alternating hands causes me to frequently invert pairs of letters when typing fast, probably because my left and right brain isn't perfectly coordinated.I have considered switching to Colemak, which is supposed to have less of that, but as a 100+ wpm typer of Dvorak it really is diminishing returns.\n \nreply",
      "I really don't think people who type slowly do so because of QWERTY. Anecdotally, my dad basically isn't able to develop muscle memory for the key locations and will frequently revert to the \"scan and then press with one finger\" method. You could give him any layout and he would still type slowly. Pretty sure even an alphabetical order would trip him up, because it'd need to be broken into multiple rows, so he'd need muscle memory again.And while this is speculative, given how close typing speeds seem on a cursory search between layouts, this suggests to me that the vast majority of the performance comes not from the layout, but from touch typing and effective use of multiple hands and fingers at the same time. All layout agnostic skills.This is not to say that on an input method level, things cannot be further improved. I sometimes see stenography [0] related software and demonstrations on YouTube for example. It also isn't to say that there cannot be a benefit health wise (i.e. ergonomically) to alternative layouts. It's just that for speed I'm not convinced it affects much, and so I think it's the wrong thing to try and change. Especially considering that sometimes things that are suboptimal can be better by being the standard.[0] https://en.wikipedia.org/wiki/Stenotype\n \nreply",
      "I got into alternate keyboard layouts and developed my own (roughly an optimized NIRO). When I tried using it on my small Surface Go I found that my fingers would 'jam' typing letters close together, so I leave that in QWERTY so it happens much less.\n \nreply"
    ],
    "link": "https://repository.kulib.kyoto-u.ac.jp/dspace/bitstream/2433/139379/1/42_161.pdf",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Quickly connect to WiFi by scanning text, no typing needed (github.com/yilinjuang)",
    "points": 35,
    "submitter": "ylj",
    "submit_time": "2025-03-16T13:58:15 1742133495",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=43379129",
    "comments": [
      "That's a nice idea. I wish stuff like this was builtin. Small suggestion: API level 29 (Android 10) introduced WifiNetworkSuggestion[1], which allows an app to prompt the user to add WiFi credentials to the system. The suggestion has to be sent as an extra in an ACTION_WIFI_ADD_NETWORKS intent[2] (which was added in API 30 for some reason, not sure right now what happens in between).[1] https://developer.android.com/develop/connectivity/wifi/wifi...[2] https://developer.android.com/reference/android/provider/Set...\n \nreply",
      "Spending compute to encode a string just to decode it with more compute, because the whole purpose was to print the decoded string seems... a bit wasteful at best. I don't understand.Sure, the excuse here might be that the generated QR code can be used to connect other devices as well, but if that was the reason, it would have been mentioned i guess. It seems like the QR code is only generated & displayed to be read from screen... It seems to me\u2013 a judgmental moron\u2013 almost as if chatgpt came up with this...Sorry if this sounds harsh, most likely i am wrong and don't get something here. And usually i wouldn't have commented because my comment doesn't bring something positive to the table... But i really so much don't get it here, i had to comment in the hope of being enlightened why this is smart and not the opposite...But either way: if it solves an issue for you the way you want it to: perfect. Congratulations on finishing an app as you imagined it. That is really great, regardless of opinions like mine.\n \nreply",
      "I think the intended purpose are cases like hotels where they really should be offering QR codes but instead just display text on the TV.  If you control the display device then definitely just use the QR code but the problem this solves is when someone else has made bad choices.\n \nreply",
      "I do like people building things, but isn't this what the share network button is in android? It creates a qr code with ssid and password, and when scanned with lens it gives you a \"join\" button.Edit - ah is the point taking a photo of credentials and joining from that?\n \nreply",
      "Yeah, from the demo video, it looks like this OCRs a photo of text and turns it into one of those QR codes. Then you can use Google Lens against the QR code onscreen to get the \"join\" button.\n \nreply",
      "From this state, adding a button to join the network shouldn't be too complicated, at least not on Android. There's an API for offering a WiFi connection to the user. I don't know about iOS but I presume there's a similar API there.\n \nreply",
      "Yep, `NEHotspotConfigurationManager`: https://developer.apple.com/documentation/NetworkExtension/N...\n \nreply",
      "For anyone interested, the wifi QR code format is described here: https://www.wi-fi.org/system/files/WPA3%20Specification%20v3...These QR codes usually work with your device's default camera app -- point at QR code and get prompted to join the network.\n \nreply",
      "I created a few stickers with my guest network WiFi details QR code to put on my fridge and around the house for when I have guests. I also recommend writing the SSID and password on it for those times the QR code won\u2019t work with someone\u2019s device (usually my mom\u2019s ancient Android phone).\n \nreply",
      "While this is def neat, it shows signs of a classical computer scientist fallacy: \"There's a recurring problem I occasionally run into, and each time it takes me about 3 minutes to solve it. No more! Instead, I'm going to spend a few weeks programming an app that solves the problem for me!\"\n \nreply"
    ],
    "link": "https://github.com/yilinjuang/wify",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          A React Native application that scans WiFi text to extract WiFi credentials, matches them with nearby networks, and connects to them.Clone the repository:Install dependencies:Run the app:For development on physical devices:The app requires the following permissions:MIT"
  }
]