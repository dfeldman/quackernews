[
  {
    "title": "You Didn't Notice MP3 Is Now Free (idiallo.com)",
    "points": 44,
    "submitter": "foxfired",
    "submit_time": "2025-02-06T00:41:43 1738802503",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=42957517",
    "comments": [
      "I don't know why this article says \"now\". It was over 7, almost 8 years ago that the final patents on MP3 expired. There was even HN discussion at the time:https://news.ycombinator.com/item?id=14240645More notable is that many H.264 patents are expiring this year:https://meta.wikimedia.org/wiki/Have_the_patents_for_H.264_M...\n \nreply",
      "I noticed. The MP3 patents expired in 2017.https://www.iis.fraunhofer.de/en/ff/amm/consumer-electronics...\n \nreply",
      "The reason why no one noticed is that everyone has been trivially circumnavigating the patents for decades (the usual model being a converter that doesn't come with its own lame.dll but asks you to put one into its folder) and Frauenhofer hasn't been caring much when it was private users as opposed to hardware manufacturers. If not for this, something like Ogg Vorbis would have taken its place.As to files, I'm sure they start mattering to you when your train goes through a tunnel or your wifi is down. The fileless world is the leakiest abstraction of them all.\n \nreply",
      "MP3 is still far from niche as this article tries to hammer in repeatedly.\n \nreply",
      "MP3 is still more widely supported in most devices. Taking existing MP3's, which is going to be most people that deal with MP3, and converting them to FLAC is not going to have a benefit (along with taking time, and needing to deal with tagging, of which I may be wrong but believe MP3 has better support and more tags - on top of being able to create your own key:value tags).\n \nreply",
      "I still rip every audio CD (including audiobooks) into MP3 and I have about 8 GB of MP3 audio files in my library (a lot of children's audio books in there for the kids).The format plays on nearly every device that plays any kind of music ever made, files are tiny, and they sound amazing still.It's so much different than like 480i videos from old VHS and DV tape imports from the MP3 era.\n \nreply",
      "MP3 is still the standard lossy audio format. Plays on everything, patent-free. OK sound quality (some stuff falls apart even at 320 if you have golden ears).\n \nreply",
      "Realistically, what modern device won't play OPUS files? According to xiph.Org[0] it's transparent at 120\n \nreply",
      "> Fast forward to today, and internet speeds have grown exponentially. A song in a more modern format like AAC or FLAC might be double or triple the size of an MP3, but who notices? You can stream a full album in lossless quality without buffering.You'd think so, but somehow this doesn't stop Jellyfin from choking whenever it starts streaming a FLAC.With no actual knowledge, I speculate that they don't bother starting to decode the upcoming track until the current track has already finished.\n \nreply",
      "Not sure I'd call FLAC \"more modern.\" There are college graduates younger than FLAC. I mean, sure, it's a decade younger than MP3, so you're technically correct, but it's not exactly the latest hot format.\n \nreply"
    ],
    "link": "https://idiallo.com/blog/listen-mp3-is-free",
    "first_paragraph": "The MP3 format, once the gold standard for digital audio files, is now free. The licensing and patents on MP3 encoders have expired, meaning you can now include them in your applications without paying royalties. For software developers and audio enthusiasts, this might seem like a big deal. But, surprisingly, almost no one noticed. Why? Because the world of technology has changed so drastically that MP3's significance has faded into the background.I noticed the change because of my habit of downloading Audacity, the open-source audio editing software. For years, Audacity required users to download an external MP3 encoder, like LAME, because the MP3 format was proprietary. This extra step was a constant reminder of the legal and technical restrictions surrounding MP3. But now, that step is unnecessary. And yet, no one seems to care.  The reality is that MP3, while still relevant in certain niche areas, has largely been eclipsed by a combination of faster internet speeds, changing softw"
  },
  {
    "title": "Ingesting PDFs and why Gemini 2.0 changes everything (sergey.fyi)",
    "points": 572,
    "submitter": "serjester",
    "submit_time": "2025-02-05T18:05:28 1738778728",
    "num_comments": 201,
    "comments_url": "https://news.ycombinator.com/item?id=42952605",
    "comments": [
      "I work in fintech and we replaced an OCR vendor with Gemini at work for ingesting some PDFs. After trial and error with different models Gemini won because it was so darn easy to use and it worked with minimal effort. I think one shouldn't underestimate that multi-modal, large context window model in terms of ease-of-use. Ironically this vendor is the best known and most successful vendor for OCR'ing this specific type of PDF but many of our requests failed over to their human-in-the-loop process. Despite it not being their specialization switching to Gemini was a no-brainer after our testing. Processing time went from something like 12 minutes on average to 6s on average, accuracy was like 96% of that of the vendor and price was significantly cheaper. For the 4% inaccuracies a lot of them are things like the text \"LLC\" handwritten would get OCR'd as \"IIC\" which I would say is somewhat \"fair\". We probably could improve our prompt to clean up this data even further. Our prompt is currently very simple: \"OCR this PDF into this format as specified by this json schema\" and didn't require some fancy \"prompt engineering\" to contort out a result.Gemini developer experience was stupidly easy. Easy to add a file \"part\" to a prompt. Easy to focus on the main problem with weirdly high context window. Multi-modal so it handles a lot of issues for you (PDF image vs. PDF with data), etc. I can recommend it for the use case presented in this blog (ignoring the bounding boxes part)!\n \nreply",
      "This is spot on, any legacy vendor focusing on a specific type of PDF is going to get obliterated by LLMs. The problem with using an off-the-shelf provider like this is, you get stuck with their data schema. With an LLM, you have full control over the schema meaning you can parse and extract much more unique data.The problem then shifts from \"can we extract this data from the PDF\" to \"how do we teach an LLM to extract the data we need, validate its performance, and deploy it with confidence into prod?\"You could improve your accuracy further by adding some chain-of-thought to your prompt btw. e.g. Make each field in your json schema have a `reasoning` field beforehand so the model can CoT how it got to its answer. If you want to take it to the next level, `citations` in our experience also improves performance (and when combined with bounding boxes, is powerful for human-in-the-loop tooling).Disclaimer: I started an LLM doc processing infra company (https://extend.app/)\n \nreply",
      "> After trial and error with different modelsAs a mere occasional customer I've been scanning 4 to 5 pages of the same document layout every week in gemini for half a year, and every single week the results were slightly different.To note the docs are bilingual so it could affect the results, but what stroke me is the lack of consistency, and even with the same model, running it two or three times in a row gives different results.That's fine for my usage, but that sounds like a nightmare if everytime Google tweaks their model, companies have to reajust their whole process to deal with the discrepancies.And sticking with the same model for multiple years also sound like a captive situation where you'd have to pay premium for Google to keep it available for your use.\n \nreply",
      "At temperature zero, if you're using the same API/model, this really should not be the case. None of the big players update their APIs without some name / version change.\n \nreply",
      "Consider turning down the temperature in the configuration? LLMs have a bit of randomness in them.Gemini 2.0 Flash seems better than 1.5 - https://deepmind.google/technologies/gemini/flash/\n \nreply",
      "Wait isn't there atleast a two step process here one is semantic segmentation followed by a method like texttract for text - to avoid hallucinations?One cannot possibly say that \"Text extracted by a multimodal model cannot hallucinate\"?> accuracy was like 96% of that of the vendor and price was significantly cheaper.I would like to know how this 96% was tested. If you use a human to do random sample based testing, well how do you adjust the random sample for variations in distribution of errors that vary like a small set of documents could have 90% of the errors and yet they are only 1% of the docs?\n \nreply",
      "One thing people always forget about traditional OCR providers (azure, tesseract, aws textract, etc.) is that they're ~85% accurate.They are all probabilistic. You literally get back characters + confidence intervals. So when textract gives you back incorrect characters, is that a hallucination?\n \nreply",
      "It\u2019s a question of scale. When a traditional OCR system makes an error, it\u2019s confined to a relatively small part of the overall text. (Think of \u201cPlastics\u201d becoming \u201cPIastics\u201d.) When a LLM hallucinates, there is no limit to how much text can be made up. Entire sentences can be rewritten because the model thinks they\u2019re more plausible than the sentences that were actually printed. And because the bias is always toward plausibility, it\u2019s an especially insidious problem.\n \nreply",
      "It's a bit of a pick your poison situation. You're right that traditional OCR mistakes are usually easy to catch (except when you get $30.28 vs $80.23). Compared to LLM hallucinations that are always plausibly correct.But on the flip side, layout is often times the biggest determinant of accuracy, and that's something LLMs do a way better job on. It doesn't matter if you have 100% accurate text from a table, but all that text is balled into one big paragraph.Also the \"pick the most plausible\" approach is a blessing and a curse. A good example is the handwritten form here [1]. GPT 4o gets the all the email addresses correct because it can reasonably guess these people are all from the same company. Whereas AWS treats them all independently and returns three different emails.[1] https://getomni.ai/ocr-demo\n \nreply",
      "I'm the founder of https://doctly.ai, also pdf extraction.The hallucination in LLM extraction is much more subtle as it will rewrite full sentences sometimes. It is much harder to spot when reading the document and sounds very plausible.We're currently working on a version where we send the document to two different LLMs, and use a 3rd if they don't match to increase confidence. That way you have the option of trading compute and cost for accuracy.\n \nreply"
    ],
    "link": "https://www.sergey.fyi/articles/gemini-flash-2",
    "first_paragraph": "Stay connected and receive new blog posts in your inbox.I\u2019d love to hear from you.\u00a9 2025 Sergey Filimonov. All rights reserved."
  },
  {
    "title": "Okta Bcrypt incident lessons for designing better APIs (n0rdy.foo)",
    "points": 123,
    "submitter": "n0rdy",
    "submit_time": "2025-02-05T21:10:25 1738789825",
    "num_comments": 57,
    "comments_url": "https://news.ycombinator.com/item?id=42955176",
    "comments": [
      "I can see the incident was a jumping off point to talk about bad APIs (bcrypt probably should error >72) but it sounds like the actual bug was they weren't checking the value in the cache matched the data they used in the hash for the key. The authentication cache check should survive any arbitrarily bad hashing algorithm because all of them are going to have collisions (pigeonhole principal). Even an arbitrarily 'strong' hash function with no input truncation, as long as it has a fixed width result, will have this property. Thus, any arguing in the comments here about different hash functions with different truncation properties is moot.The analogy is something like creating a hash map whose insert function computes the slot for the key and unconditionally puts the value there instead of checking if the keys are the same during a collision. No amount of tinkering with the hash function fixes this problem. The algorithm is wrong. A hashmap should survive and be correct even giving it a hash function that always returns 4.\n \nreply",
      "I would guess that they felt comfortable that the bcrypt output (192 bits) is enough that collisions are very unlikely. If these were already partitioned by customer, rather than being a single cache for the entire Okta userbase that seems fine. You're going to have weird cosmic ray bugs more often than a natural collision.Now, the data structure they're using for a cache will use some sort of hash table, likely in memory, so maybe they've got the 192-bit bcrypt \"key\" and then that's hashed again, perhaps well or perhaps badly [e.g. C++ really likes using the identity function so hash(12345) = 12345] but then a modulo function is applied to find the key in an index and then we go groping about to look for the Key + Value pair. That part the API probably took care of, so even if the hash has size 6-bits, the full 192-bit key was checked. But the original data (userid:username:password) is not compared, only that 192-bit cache key.\n \nreply",
      "Bcrypt is a password hash, not a KDF, which is the way it was used in this API. It's super unclear to me why they wanted a string-based KDF here at all; does anyone have more context?I've in the past been annoying about saying I think we should just call all password hashes \"KDFs\", but here's a really good illustration of why I was definitely wrong about that. A KDF is a generally-useful bit of cryptography joinery; a password hash has exactly one job.\n \nreply",
      "The value is the combination of userid, username, and password, so in threads on other platforms people have hypothesised that the developer tried to play it safe and use a password hash because of the password's presence.Also I'm not sure the average developer understands the distinction.\n \nreply",
      "They didn't want a KDF, as far as I know, but they wanted a hash function with unlimited input size.Including the username in the hash input gives you guaranteed domain separation between users that you don't get from salts/nonces. Its a generally good idea if you have a hash function with unlimited input size (all modern cryptographic hash functions except bcrypt have unlimited input size).\n \nreply",
      "They clearly wanted something stronger than \"a hash function\" or they'd have reached for weaker cryptographic hashes.\n \nreply",
      "They wanted a hard-to-compute cryptographic hash function. Today, that means bcrypt or something with a KDF construction. However, they needed one with unlimited input size, which rules out bcrypt.\n \nreply",
      "Or just a hash of the bcrypt hash, for the password!I don't like using thought-stopping cliches any more than anybody else does, but this design feels a little cargo-culted. All this stuff follows the more fundamental question of \"why is the password mixed into a cache key\"?\n \nreply",
      "Yeah, I think both of the following would have worked if they wanted the password involved in a cache key and they wanted bcrypt to be used:* bcrypt(SHA-512(PW || stuff))* SHA(stuff || bcrypt(PW))Disclaimer: Not cryptography advice.It's still unclear to me why the password is in there.\n \nreply",
      "For 'unlimited' input size it should be SHA-3-512. Maybe too slow, but Bcrypt is slower, right? Less things to go wrong too.\n \nreply"
    ],
    "link": "https://n0rdy.foo/posts/20250121/okta-bcrypt-lessons-for-better-apis/",
    "first_paragraph": "\n    Exploring software engineering and tech\n    \npowered by Hugo | themed with poison\n\n    \u00a9 2025 n0rdy personal blog. All rights reserved.\nHello there! If you follow tech news, you might have heard about the Okta security incident that was reported on 1st of November. The TLDR of the incident was this:The Bcrypt algorithm was used to generate the cache key where we hash a  combined string of userId + username + password. Under a specific set of conditions, listed below, this could allow users to authenticate by  providing the username with the stored cache key of a previous  successful authentication.This means that if the user had a username above 52 chars, any password would suffice to log in. Also, if the username is, let\u2019s say, 50 chars long, it means that the bad actor needs to guess only 3 first chars to get in, which is quite a trivial task for the computers these days. Too bad, isn\u2019t it?On the other hand, such long usernames are not very usual, which I agree with. However, so"
  },
  {
    "title": "The Language Construction Kit (zompist.com)",
    "points": 26,
    "submitter": "pzrsa",
    "submit_time": "2025-02-03T12:31:15 1738585875",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.zompist.com/kit.html",
    "first_paragraph": "  Caja de herramientas para construir idiomas (en espa\u00f1ol; traducido por Renato Montes)\n  O Kit de Constru\u00e7\u00e3o de L\u00ednguas (em portugu\u00eas; traduzido por Gustavo Pereira)    Il Kit di Costruzione di Linguaggi (in italiano; tradotto per Daniele \"MadMage\" Calisi)  Der Sprachbaukasten (auf Deutsch; \u00fcbersetzt von Carsten Becker)     \r\u00a0\r\rThe LCK is also available in a print edition, four times the length, published by Yonagu Books on Amazon.\r\rWhen you're done with that, you\u2019ll want  Advanced Language Construction!  Does the LCK seem pretty basic to you, and you want to delve into morphosyntax, logographic languages, predicate logic, and more?  Get the sequel today! \n\nNext is The Conlanger\u2019s Lexipedia: everything you know to create words that aren\u2019t a simple copy of English.  Creating the lexicon is the part of conlanging that takes the most time and requires the most real-world knowledge; you need this book to help.\n\nMy latest book is The Syntax Construction Kit: all about modern Syntax, from C"
  },
  {
    "title": "Servo's progress in 2024 (servo.org)",
    "points": 304,
    "submitter": "brson",
    "submit_time": "2025-02-05T15:03:09 1738767789",
    "num_comments": 106,
    "comments_url": "https://news.ycombinator.com/item?id=42949390",
    "comments": [
      "I'm convinced that using an embedded browser engine to render app UI is the future. Browser rendering engines are so powerful and versatile and universal I don't see how they can lose.\"But Electron!\" Yes, Electron hasn't taken the world by storm because it has two huge deficiencies: 1. It takes an enormous amount of resources including ram and disk. 2. It has no native DOM API bindings so it takes even more ram and cpu to compile and run JS quickly.I'm excited for the new crop of browser engines because they could fix those deficiencies, opening up browser tech to app developers without hugely compromising the experience.\n \nreply",
      "The Bloomberg Terminal is rendered using Chromium and has been for many years. You don\u2019t need to wait for new browser engines to use the existing ones, but you do need resources to contribute, maintain, bugfix,  etc. because there is no such thing as a free lunch.\n \nreply",
      "Numerous video games have been doing precisely that for the past decade using CEF: https://en.wikipedia.org/wiki/Chromium_Embedded_Framework\n \nreply",
      "Minecraft Bedrock uses React + a \"web-like platform\" (I assume this means they've just implemented the bits they needed) for their UI: https://github.com/Mojang/ore-ui\n \nreply",
      "FiveM uses CEF and let's you replace much of the built in GTA V scaleform with it.It's incredible what can be done but can be far less responsive than the scaleform it replaces, but this may partially be due to it being a third party mod to the game.\n \nreply",
      "s/o to CefGlue, the C# bindings: https://github.com/space-wizards/cefglue\n \nreply",
      "and before that Scaleform!\n \nreply",
      "I mean, most OSes already ship with a WebView component that you can use instead of shipping an entire browser runtime.Wails does that: https://wails.io/Tauri also does that: https://tauri.app/That does help with the app sizes quite a bit: https://github.com/Elanis/web-to-desktop-framework-compariso...Sadly it doesn\u2019t change the memory usage much so the technology is still inherently wasteful, but on a certain level it feels like a lost battle - because web technologies often feel like the choice of least resistance when you want GUI software that will run on a bunch of platforms while not being annoying to develop (from the perspective of your run of the mill dev).\n \nreply",
      "Correct me if I'm wrong, but isn't the problem with tauri and wails that they are still dependent on the client's native OS webview?  I know Tauri uses WRY which essentially takes in your request, finds the webview present in the client and calls a corresponding function.  The differences between these webviews are vast and you end up with different UIs or cobbled together adjustments for each OS to split the differences.  Fully embedding a browser is extremely storage inefficient but it does guarantee apps look identical regardless of platform.\n \nreply",
      "Depends on what you care more about: the package size or that sort of consistency in exactly how things look. For example, when I was using Wails to quickly throw together a UI for managing some other client software, small package sizes were great and since whatever I do inside of a WebView/browser will always be different than the native OS GUI solutions, slight differences were inconsequential. Someone else might have vastly different requirements.\n \nreply"
    ],
    "link": "https://servo.org/blog/2025/01/31/servo-in-2024/",
    "first_paragraph": ""
  },
  {
    "title": "Tell HN: Cloudflare is blocking Pale Moon and other non-mainstream browsers",
    "points": 551,
    "submitter": "Hold-And-Modify",
    "submit_time": "2025-02-05T19:08:12 1738782492",
    "num_comments": 192,
    "comments_url": "https://news.ycombinator.com/item?id=42953508",
    "comments": [
      "I'm using chrome on linux and noticed that this year cloudflare is very agressive in showing the \"Verify you are a human\" box. Now a lot of sites that use cloudflare show it and once you solve the challenge it shows it again after 30 minutes!What are you protecting cloudflare?Also they show those captchas when going to robots.txt... unbelievable.\n \nreply",
      "We're on Chrome on Linux, mostly we don't see those.\n \nreply",
      "Cloudflare has been even worse for me on Linux + Firefox. On a number of sites I get the \"Verify\" challenge and after solving it immediately get a message saying \"You have been blocked\" every time. Clearing cookies, disabling UBO, and other changes make no difference. Reporting the issue to them does nothing.This hostility to normal browsing behavior makes me extremely reluctant to ever use Cloudflare on any projects.\n \nreply",
      "I'm a Cloudflare customer, even their own dashboard does not work with linux+slightly older firefox. I mean one click and it is ooops, please report the error to dev null\n \nreply",
      "At least you can get past the challenge. For me, every-single-time it is an endless loop of \"select all bikes/cars/trains\". I've given up even trying to solve the challenge anymore and just close the page when it shows up.\n \nreply",
      "I run a few Linux desktop VMs and Cloudflare's Turnstile verification (their auto/non-input based verification) fails for the couple sites I've tried that use it for logins, on latest Chromium and Firefox browsers. Doesn't matter that I'm even connecting from the same IP.I'd presumed it was just the VM they're heuristically detecting but sounds like some are experiencing issues on Linux in general.\n \nreply",
      "Check that you are allowing webworker scripts, that did the trick for me. I still have issues on slower computers (Raspberry pies and the like) as they seem to be to slow to do whatever Cloudflare wants as a verification in the allotted time, however.\n \nreply",
      "Does it still apply if you change the UA to something more common (Chrome on Windows or something)?\n \nreply",
      "Sounds like my experience browsing internet while connected to the VPN provided by my employer: tons of captcha and everything is defaulted to German (IP is from Frankfurt).\n \nreply",
      "Happens after updating the browsers too. \"Click on images with fire hydrants\". \"Click on images with cars\". Oh we are doing this again? Sigh...\n \nreply"
    ],
    "link": "item?id=42953508",
    "first_paragraph": ""
  },
  {
    "title": "S1: A $6 R1 competitor? (timkellogg.me)",
    "points": 589,
    "submitter": "tkellogg",
    "submit_time": "2025-02-05T11:05:40 1738753540",
    "num_comments": 227,
    "comments_url": "https://news.ycombinator.com/item?id=42946854",
    "comments": [
      "From the S1 paper:> Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending \"Wait\" multiple times to the model's generation when it tries to endI'm feeling proud of myself that I had the crux of the same idea almost 6 months ago before reasoning models came out (and a bit disappointed that I didn't take this idea further!). Basically during inference time, you have to choose the next token to sample. Usually people just try to sample the distribution using the same sampling rules at each step.... but you don't have to! you can selectively insert words into the the LLM's mouth based on what it said previously or what it wants to say, and decide \"nah, say this instead\". I wrote a library so that you could sample an LLM using llama.cpp in swift and you could write rules to sample tokens and force tokens into the sequence depending on what was sampled. https://github.com/prashanthsadasivan/LlamaKit/blob/main/Tes...Here, I wrote a test that asks Phi-3 instruct \"how are you\" and it if it tried to say \"as an AI I don't have feelings\" or \"I'm doing \" I forced it to say \"I'm doing poorly\" and refuse to help since it was always so dang positive. It sorta worked, though the instruction tuned models REALLY want to help. But at the time I just didn't have a great use case for it - I had thought about a more conditional extension to llama.cpp's grammar sampling (you could imagine changing the grammar based on previously sampled text), or even just making it go down certain paths, but I just lost steam because I couldn't describe a killer use case for it.This is that killer use case! forcing it to think more is such a great usecase for inserting ideas into the LLM's mouth, and I feel like there must be more to this idea to explore.\n \nreply",
      "I found the discussion around inference scaling with the 'Wait' hack so surreal. The fact such an ingeniously simple method can impact performance makes me wonder how many low-hanging fruit we're still missing. So weird to think that improvements on a branch of computer science is boiling down to conjuring the right incantation words, how you even change your mindset to start thinking this way?\n \nreply",
      "I think the fact alone that distillation and quantization are techniques that can produce substantial improvements is a strong sign that we still have no real comprehensive understanding how the models work.If we had, there would be no reason to train a model with more parameters than are strictly necessary to represent the space's semantic structure. But then it should be impossible for distilled models with less parameters to come close to the performance of the original model.Yet this is what happens -  the distilled or quantized models often come very close to the original model.So I think there are still many low-hanging fruits to pick.\n \nreply",
      "We have a partial understanding of why distillation works\u2014it is explained by The Lottery Ticket Hypothesis (https://arxiv.org/abs/1803.03635). But if I am understanding correctly, that doesn't mean you can train a smaller network from scratch. You need a lot of randomness in the initial large network, for some neurons to have \"winning\" states. Then you can distill those winning subsystems to a smaller network.Note that similar process happens with human brain, it is called Synaptic pruning (https://en.wikipedia.org/wiki/Synaptic_pruning). Relevant quote from Wikipedia (https://en.wikipedia.org/wiki/Neuron#Connectivity):\n\"It has been estimated that the brain of a three-year-old child has about 10^15 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 10^14 to 5x10^14 synapses (100 to 500 trillion).\"\n \nreply",
      "> still have no real comprehensive understanding how the models work.We do understand how they work, we just have not optimised their usage.For example someone who has a good general understanding of how an ICE or EV car works. Even if the user interface is very unfamiliar, they can figure out how to drive any car within a couple of minutes.But that does not mean they can race a car, drift a car or drive a car on challenging terrain even if the car is physically capable of all these things.\n \nreply",
      "Your example is somewhat inadequate. We _fundamentally_ don\u2019t understand how deep learning systems works in the sense that they are more or less black boxes that we train and evaluate. Innovations in ML are a whole bunch of wizards with big stacks of money changing \u201cHmm\u201d to \u201cWait\u201d and seeing what happens.Would a different sampler help you? I dunno, try it. Would a smaller dataset help? I dunno, try it. Would training the model for 5000 days help? I dunno, try it.Car technology is the opposite of that - it\u2019s a white box. It\u2019s composed of very well defined elements whose interactions are defined and explained by laws of thermodynamics and whatnot.\n \nreply",
      "We know how the next token is selected, but not why doing that repeatedly brings all the capabilities it does. We really don't understand how the emergent behaviours emerge.\n \nreply",
      "For quantization I don't think that's really true. Quantization is just making more efficient use of bits in memory to represent numbers.\n \nreply",
      "It feels like we're back in 1900 when anyone's clever idea (and implementation) can give huge performance improvements, such as Ford's assembly line and Taylor's scientific management of optimizing shovel sizes for coal.\n \nreply",
      "yes, it also feels like we are going to lose our just-in-time global shipments of anything to anywhere any day now. It will soon feel like 1900 in other ways.\n \nreply"
    ],
    "link": "https://timkellogg.me/blog/2025/02/03/s1",
    "first_paragraph": "\n\n\t\t\tMon February 03, 2025\n\t\t\nA new paper released on Friday is making waves in the AI community, not because of the model \nit describes, but because it shows how close we are to some very large breakthroughs in AI. The model\nis just below state of the art, but it can run on my laptop. More important, it sheds light on how all\nthis stuff works, and it\u2019s not complicated.OpenAI were the first to claim the inference-time scaling laws. Basically, an LLM can get higher performance\nif it can \u201cthink\u201d longer before answering. But, like, how do you do it? How do you make it think longer?OpenAI and R1 had cool graphs showing performance scaling with average thinking time (this from\nthe s1 paper):But how do they control the length of an LLM response? Everyone skipped over that part, but s1\nshows us details, and it is fun.Context: When an LLM \u201cthinks\u201d at inference time, it puts it\u2019s thoughts inside <think> and \n</think> XML tags. Once it gets past the end tag the model is taught to change voice in"
  },
  {
    "title": "Minimum effective dose (winnielim.org)",
    "points": 53,
    "submitter": "surprisetalk",
    "submit_time": "2025-02-02T04:43:00 1738471380",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=42905900",
    "comments": [
      "> Nobody says we have to be good at everything we doThis is advice I have to push on my kids constantly, because they are obsessed with finding that one thing they are better than everyone else in the world.\"Do some\" is not advice I got as a kid, but my mom eventually figured it out and told me that when was in my mid 20s or something.Her words (from Malayalam) are best translated as \"For whom a little is not enough, nothing is ever enough\".I think that's true for everything from money to self-worth. Enough is too hard to have.\n \nreply",
      "Can you help me understand your comment? Being the best at one thing is very different from being good at everything, so it's unclear how that advice applies to your children.Also, what does \"do some\" mean?\n \nreply",
      "Not OP but I read it as: It's ok to just do some. People stress about their abandoned side projects, their career progression, being the best, this world has become hyper competitive, hyper \"do\" - this wasn't and isn't the default for a lot of people and a lot of history, there is a lot of joy to be found in just \"do some\" - but it's a dying art.\n \nreply",
      "I like the philosophy and employ it myself.Regarding the gym, specifically lifting -- progressive overload ( thus muscle growth) can definitely be accomplished in a minimal amount of time. BUT, it requires a high to maximum amount of effort to be effective, so not sure if that counts.\n \nreply",
      "It absolutely applies to lifting and progressive overload. A measure of progressive overload can be applied to amount of time spend lifting, also knows as time-under-tension, it can also apply inversely with shortened rest periods. Whether or not these methods align with your fitness goals, is a separate issue.Effort, or what the fitness industry calls Rating of Perceived Exertion (RPE), is another lever to pull with progressive overload, and is necessary in programming for all experience levels. Beginners start their programs using minimal weights, slowly increasing the weight each week as their skill improves and through that process their strength and muscles grow.A more advanced lifter, will need a higher minimum effective dose due to what is needed to trigger adaptation, will manipulate RPE throughout their programming to manage fatigue accrued throughout the program.A great personal trainer is someone that will help their clients find their own personal mimimum dose of training that aligns with their fitness goals.\n \nreply",
      "You can be in and out of the gym in 30-40 minutes using a PPL.\n \nreply",
      "Notoriously difficult to find one with a landing strip inside, however.\n \nreply",
      "\"240 minutes a month which is enough to finish 1-2 books\"this number stands out to me ! maybe i am slow reader but .5-1 seems more realistic\n \nreply",
      "Judging by length of audiobooks, four hours is the length of the novella long essay A Room of One\u2019s Own by Virginia Woolf, Terry Pratchett\u2019s Discworld adaptations all come in around 9-10 hours and those are short books that require my full attention due to the wordplay and setup for jokes sometimes being longish. One would have to double the speed and sacrifice comprehension to meet this goal. I had to listen to first hour of some Greg Egan book and the glossary about ten times before I grokked it.\n \nreply",
      "I read light prose several times faster than an audio book; 60-100 pages in an hour for a typical paperback.  That works out to 240-400 pages for the author's 240 minutes.I read Going Postal by Pratchett in under 4 hours.[edit]Also, consumers of audiobooks may turn the speed up.  My wife listens at 1.25-1.5x speed, depending on the narrator's pace.\n \nreply"
    ],
    "link": "https://winnielim.org/journal/minimum-effective-dose/",
    "first_paragraph": "I am still recovering from my failed root canal (and still have one visit to complete the procedure), so I have been hesitant in taking up my regular exercise again because I don\u2019t want to distract my immune system from my tooth\u2019s healing. So when I went to the gym today for the first time in weeks I tried to do only the bare minimum required \u2013 also known as the minimum effective dose. It may come as a surprise how little time we truly need at the gym to gain strength and muscle. I think it is all about sending our bodies the right signals. With the body it is almost always use it or lose it. So even though we are stressing our bodies for merely a short while, we are still sending a signal that our muscles are still needed.I find the concept of the minimum effective dose fascinating. It can be applied to many areas in life, especially when it comes to learning. There are many people who tend to believe that it is all or nothing when it comes to practising things. It is either we commit"
  },
  {
    "title": "It's unlikely that there will be any further releases of mt32-pi (github.com/dwhinham)",
    "points": 109,
    "submitter": "nickt",
    "submit_time": "2025-02-05T21:42:20 1738791740",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=42955613",
    "comments": [
      "The social media-izing of open source has been a massive blow.  Projects and contributors across the board are randomly bombarded with mob harassment.Return to Cathedral.\n \nreply",
      "> Return to Cathedral.What does this mean?\n \nreply",
      "It's a reference to Eric S. Raymond's famous article \"The Cathedral and the Bazaar\", where he compares the rather top-down, leader driven culture of Unix development to the free-for-all style of Linux.Of course, I always like to point out the foolishness of this metaphor: Bazaars in the Near East were usually run in a fairly regimented fashion by merchant guilds and their elected or appointed leaders.\n \nreply",
      "FWIW: https://en.wikipedia.org/wiki/The_Cathedral_and_the_Bazaar\n \nreply",
      "A closed-off development team releases what they want, and you take it or leave it, like sqlite.\n \nreply",
      "'DOS\" retro pseudo-hackers [crackers] always have been a cancer like this, sadly. Elitism, tons of gatekeeping that would even blush most *BSD users, code stealing, non-acreditation... basically the opposite of the public FOSS/Nix/Lisp world.They were like that in the 90's, and tons of them never acknowleckged their mediocrity against, well, serious systems.Amiga diehard fanboys and microcomputer owners were like that with tons of 'secretive' software, too.In my country (Spain) tons of people tolk about the 'Golden Age' of the Spanish software, were 90% of it was pure crap against what the French and the Brits where producing even for the ZX, not to mention what the Japanese did with the MSX.A community built on snobbism and spoiled kids. Then when the international scene (the serious one) compares their IF 'work' against The Hobbit or, worse, isometric games (not the ones made with an infamous engine, the ones with amazing artwork and free roaming), the whole deck of cards collapses.\n \nreply",
      "It's an intentionally edgy statement, got it...The major problem: With any idea around decreasing all that social media circus, and coming back to the actual things a bit more, you lose entire generations of human beings nowadays whose only reason to exist, whose only driver, whose only source of energy, is their smartphone with the right social media silo apps installed.It's at least a decade too late to stop that apocalypse. It will not get better before a big boom suddenly forces them from outside. Our societies are obviously not strong and not resilient enough for more than a few decades of existance before it makes boom.\n \nreply",
      "mt32-pi has been, and will continue to be, a source of great joy for many retro PC enthusiasts. For those of us with a particular interest in DOS music, it is huge.Some modern projects generously give enthusiasts access to the power and functionality of (what might otherwise be $1000+ in) rare retro hardware, and in this hobby, mt32-pi ranks high among them.\n \nreply",
      "I did find the community around those projects (MisterFPGA) to be rather weird, so I understand a little where he's coming from.\n \nreply",
      "In my experience, their description sounds exactly like the greater emulation and retrogaming scene in general.\n \nreply"
    ],
    "link": "https://github.com/dwhinham/mt32-pi/blob/075b52809e77420c6e80828825fe42430336b369/README.md",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          It's unlikely that there will be any further releases of mt32-pi.I have endured a sustained campaign of abuse from members of the VOGONS forum, been labelled a \"clout-chaser\", had threats sent to my personal email address, code been used in other projects without proper accreditation, my 3D print designs stolen and sold by faceless eBay/Etsy sellers, personal attacks made towards me when people don't get their feature request... the list goes on and on.There is only so much I can take.My mental health has been in decline as a direct result of this behavior; the joy of working on this project has pretty much gone. There is nothing to be gained from putting time and hard work into it any more. There is no gratitude, no encouragement - just entitled behavior and grift.To those who supported this project in the past, especially whilst I was a st"
  },
  {
    "title": "Why is Warner Bros. Discovery putting old movies on YouTube? (tedium.co)",
    "points": 422,
    "submitter": "shortformblog",
    "submit_time": "2025-02-05T14:47:32 1738766852",
    "num_comments": 331,
    "comments_url": "https://news.ycombinator.com/item?id=42949181",
    "comments": [
      "Old movies have been available on various \"free ad-supported streaming television\" for a while now, so I'm actually more surprised it took copyright holders that long to realize that Youtube also shows ads and doesn't require people to install some wonky app that might or might not be available for their platform.Of course, region-specific copyright deals are incredibly complex etc. etc., so I could imagine it was just a matter of waiting out until the last person putting up a veto retired or moved on to other things.\n \nreply",
      "I assume that bandwidth is by far the biggest cost for running your own streaming service, so letting Google take that hit makes a lot of sense.\n \nreply",
      ">> I assume that bandwidth is by far the biggest cost for running your own streaming service, so letting Google take that hit makes a lot of sense.Judging from the clunky, buggy, nonsensical experiences on 2nd tier streaming services (i.e., everything except Netflix, Amazon Prime, YouTube, Disney+, Max), I'd say the biggest cost is probably hiring a decent Engineering+Product+Test team. There are complexities here, like making these things work on different TV brands, versions, older models, etc.Pushing all the complexity to YT seems like a total no-brainer.\n \nreply",
      "> Judging from the clunky, buggy, nonsensical experiences on 2nd tier streaming services (i.e., everything except Netflix, Amazon Prime, YouTube, Disney+, Max)With the exception of Netflix, these other companies' apps are similarly buggy and painful to use. I run into an at least issue daily (usually multiple times a day) in every streaming app I use except Netflix.\n \nreply",
      "I LOATHE peacock. I don't know what checks they do at the start of the stream, but they always peg me at 720p or lower resolution despite having over 300mb. Its not an issue on any other streaming app and they give you no option to set it manually. Streams look like a dog's breakfast on my 4k TVs.\n \nreply",
      "Are you behind a CG-NAT?  Not all companies have caught up to the fact that one IP is used by multiple customers now.Things like throttling by IP Address which used to be a viable option is not effective anymore.\n \nreply",
      "Its just them being cheap. They probably set every one to a max of 720p, hope most people do not realise (cutting down bandwidth costs) and let them set max quality themselves.\n \nreply",
      "They just need to look at their stock price vs NFLX to realize that people do indeed realize the difference across the stack.\n \nreply",
      "Could be a DRM thing.  You might not have a trusted display/decoding device, so it gives you the low res.\n \nreply",
      ">Could be a DRM thing. You might not have a trusted display/decoding device, so it gives you the low res.True, but that is why this is a hard engineering challenge -- there are a lot of variations on client-side devices which need to be supported well. Upgrade cycles for TVs is 3x that of phones, is my guess.\n \nreply"
    ],
    "link": "https://tedium.co/2025/02/05/warner-bros-youtube-full-movie-releases/",
    "first_paragraph": ""
  },
  {
    "title": "The FAA\u2019s Hiring Scandal (tracingwoodgrains.com)",
    "points": 522,
    "submitter": "firebaze",
    "submit_time": "2025-02-05T05:25:19 1738733119",
    "num_comments": 464,
    "comments_url": "https://news.ycombinator.com/item?id=42944203",
    "comments": [
      "This is a fascinating read, but the thing that bugs me about this whole affair is that when this came to light many years ago it was treated as a cheating and recruitment scandal. But only recently has it been reframed as a DEI issue.Taking old, resolved scandals - slapping a coat of culture war paint on it - and then selling it as a new scandal is already a popular MO for state-sponsored propoganda, so we should be extra wary of stories like this being massaged.\n \nreply",
      "> when this came to light many years ago it was treated as a cheating and recruitment scandal. But only recently has it been reframed as a DEI issue.Respectfully, thats not accurate.The article actually shows that dei considerations were central to the original changes, not just recent framing. The FOIA requests show explicit discussions about \"diversity vs performance tradeoffs\" from the beginning. The NBCFAE role and the \"barrier analysis\" were both explicitly focused on diversity outcomes in 2013.The article provides primary sources (internal FAA documents, recorded messages, investigation reports) showing that racial considerations were explicitly part of the decision making process from the start. This is documented in realtime communications.The scandal involved both improper hiring practices (cheating) AND questionable DEI implementation. These aren't mutually exclusive; they're interrelated aspects of the same event.> Taking old, resolved scandalsIn what way do you consider this resolved?The class action lawsuit hasn't even gone to trial yet (2026).The FAA is still dealing with controller shortages. (facilities are operating understaffed,controllers are working 6-day weeks due to staffing shortages, training pipelines remain backed up)The relationship between the FAA and CTI schools remains damaged, applicant numbers have declined significantly since 2014.\n \nreply",
      "Was deeply aware of it at the time - was not really a DEI issue even then - it was pure cronyism.\n \nreply",
      "The source article includes primary material that strongly contradicts your anecdote. The policy change arrived in 2013, and there are materials from that same year indicating DEI.For example, here's an FAA slide from 2013 which explicitly publishes the ambition to place DEI as the core issue (\"- How much of a change in jo performance is acceptable to achieve what diversity goals?\"):https://archive.ph/Qgjy5The evidence in this source does not discuss cronyism, although I believe you that it could have been relevant to your personal experience; it's just false to claim the issue as a whole was unrelated to DEI.\n \nreply",
      "The answer to the question you've quoted is important, since it could be \"none\", \"a little bit\", \"a lot\", \"any amount\", each of which has very different ramifications. There is no answer on the slide ...\n \nreply",
      "They decided that at least some amount was acceptable - the minimum score on the AT-SAT was changed so that 95% of test takers would pass because the original threshold where 60% passed excluded too many black applicants. This was despite previous studies showing that a higher score on the AT-SAT was correlated with better job performance.\n \nreply",
      "No, that's not an answer to that specific question.Performance on the AT-SAT is not job performance.If you have a qualification test that feels useful but also turns out to be highly non-predictive of job performance (as, for example, most college entrance exams turn out to be for college performance), you could change the qualification threshold for the test without any particular expectation of losing job performance.In fact, it is precisely this logic that led many universities to stop using admissions tests - they just failed to predict actual performance very well at all.",
      "I found one thing odd, which was outside of the scope over the zero sum game being fought here.If you are understaffed, AND you are hiring traditionally, it would make sense that recruiting people would go up. That would mean diverse hires anyway - based on the article, it seems that even increasing diversity was not between undeserving candidates and ideal candidates (the second band section of the article)Is the third variable at play here a lack of funding from congress for recruitment?\n \nreply",
      "If you are trying to reach race/gender based quotas, you simply cannot hire white men anymore when they are 90% of the applicants. Or at least, you must attempt to minimize it as much as possible. Math.\n \nreply",
      "Yeah but thats not how any quota based system works. Thats the strawman of quota systems. The article itself showed that the quota is some fraction of total applicants that results in minimal impact to performance.Also I heard \"math\" with a youtube overlay.\n \nreply"
    ],
    "link": "https://www.tracingwoodgrains.com/p/the-full-story-of-the-faas-hiring",
    "first_paragraph": ""
  },
  {
    "title": "A hexagonal-tiled cartogram for U.S. counties (jordanroga.com)",
    "points": 31,
    "submitter": "jordanhroga",
    "submit_time": "2025-02-05T03:18:22 1738725502",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=42943271",
    "comments": [
      "I am not a cartographer or any type of a data visualization expert, but isn't the advantage of the hexagon map - the fact that it \"highlights trends that are often masked by the geographical quirks of actual state boundaries\" per the article - negated by hexagon county maps?For example, California has 58 counties, but Kentucky has 120. That means that the \"resolution\" of the data in California is half as accurate as the data in Kentucky despite having ten times as many inhabitants and 4 times the land area.Basically, by switching to hexagon county maps, aren't we actually amplifying the problem that hexagon state maps try to solve and making it worse, not better?(A good article and good examples OP but I just don't personally see how the hexagon county map is an improvement, but again I am not an expert in this field)\n \nreply",
      "You're correct that increasing resolution to the county level can inflate the size of states with more counties. This kind of map wouldn't be appropriate for a state by state comparison at all; instead, it's more useful for a county by county comparison. It's just a different resolution of administrative boundary and there are limits to that.A lot of U.S. data is reported at the county level and comparing across regions within or across states without worrying about the geography or shapes of different counties can sometimes be helpful. Adding population as a size dimension and filtering to a single state can also help mitigate the distortions.\n \nreply",
      "> For example, California has 58 counties, but Kentucky has 120. That means that the \"resolution\" of the data in California is half as accurate as the data in Kentucky despite having ten times as many inhabitants and 4 times the land area.Kentucky also simply looks larger, which over-indexes its importance.  I can't really think of any data that I would want visualized by county directly, especially data about people.  In California alone, there is a 9000:1 population variance between the largest and smallest county.\n \nreply",
      "It helps if you have a dataset that goes by county, and a lot of data in the US has it's highest resolution at the county level.\n \nreply",
      "> A county like San Francisco or New York, which packs thousands of residents into a small area, now stands out with an intense color saturation, drawing the eye immediately to where the people actually are. Conversely, a vast county with a sparse population adopts a lighter tone, accurately reflecting its lower density without the distraction of an oversized area.That sounds great, but this colour scheme doesn't seem to be what was actually used on the map.If I know anything about the US population density, the gradient used is dark blue -> light blue -> light pink -> red, so the \"vast counties with sparse population\" actually show up as saturated as the dense urban ones.The most visually attractive areas on that map are the large dark blue areas of West Texas and the Rockies, the opposite of the intention described in the text.I would recommend using a single colour gradient instead.\n \nreply",
      "side note Uber wrote about hexagons\nhttps://www.uber.com/blog/h3/I also remember a star link map with hexagons\n \nreply",
      "Starlink's availability map uses hexagons, but you likely mean https://satellitemap.space/\n \nreply",
      "That map does seem to be using H3 hexagons: https://h3geo.org/\n \nreply",
      "> Hexagons have a unique property: they tile a plane without gaps, and each cell maintains a consistent shape and size.This is not a unique feature of hexagons; squares do this (and square-tiled cartograms are common, and to the extent any thing in this article is a real trait of hex-tiled cartograms it also is of square-tiled ones) as do triangles (though you need two different orientations with triangles.)> Compared to square grids, hexagonal tiling reduces the \u201cedge effect\u201d where corners in squares can mislead the interpretation of adjacency.This is true if you are drawing a grid over a true map, but when you make a cartogram that reduces irregularly sized and shaped geographic units (counties) to each be represented by any regular polygon, you are going to \"mislead the interpretation of adjacency\" pretty significantly in any case, so this seems largely irrelevant to the presented use case.\n \nreply",
      "The first image jumped out at me because the author chose to use blue and red which, when used on a map of the United States always indicates political affiliation, but the areas that were marked \"red\" and \"blue\" were reversed from what my expectation was (coasts and cities red, midwest + great plains blue).> A county like San Francisco or New York, which packs thousands of residents into a small area, now stands out with an intense color saturation, drawing the eye immediately to where the people actually are. Conversely, a vast county with a sparse population adopts a lighter tone, accurately reflecting its lower density without the distraction of an oversized area.(Emphasis on \"saturation\" added.)  This isn't quite right, the most-densely populated hexes (large cities) are a pretty saturated red color yes, but the least-densely populated hexes (west Texas) are a pretty saturated blue color.  In fact, this color palette makes it really difficult to see what the author intends.> Using the size of counties as a dimension to represent population can also showcase population centers and free up the color dimension to show another dimension like population density.Ok yeah that's a cartogram, I know how to expect that to look... wait, no, the image has the same center points for every county, a hexagon outline drawn around that, and then a confusingly-colored solid circle painted over each.  The colors are confusing because the overwhelming effect is that the hex outlines dominate the perception of the dark blue nearly-point-sized \"circles\" in the majority of the map.Look at Nevada.  On the first map Clark County (where Las Vegas is) is a light blue colored hex in a state of mostly dark blue hexes and Washoe County (where Reno is) is light red.  On the second map Clark is the largest light blue circle in the state and Washoe is a smaller light blue circle, which seems to be the reverse of what the first map says.  ...Also, Washoe appears to be in a different place maybe?  It seems to have moved north a hex.I'm not just picking nits, this is difficult stuff, but an extremely powerful way to convey a ton of densely-packed (no pun intended) information, when done right.  See, for instance [0][1][2][3] all by Edward Tufte.[0] \"The Visual Display of Quantitative Information\"  https://www.amazon.com/Visual-Display-Quantitative-Informati...[1] \"Visual Explanations: Images and Quantities, Evidence and Narrative\" https://www.amazon.com/dp/0961392126[2] \"Beautiful Evidence\" https://www.amazon.com/dp/0961392177[3] \"Envisioning Information\" https://www.amazon.com/Envisioning-Information-Edward-R-Tuft...\n \nreply"
    ],
    "link": "https://www.jordanroga.com/blog/introducing-a-hexagonal-tiled-cartogram-for-u-s-counties",
    "first_paragraph": "Mapping data isn\u2019t just about geography - it\u2019s about telling stories hidden in numbers. Recently, I created a cartogram that tiles all U.S. counties into a uniform grid of hexagons. This approach transforms our traditional view of the nation, offering fresh insights into population, density, and other socio-economic variables that are often obscured on conventional maps.\u00a0Mapping data isn\u2019t just about geography - it\u2019s about telling stories hidden in numbers. Recently, I created a cartogram that tiles all U.S. counties into a uniform grid of hexagons. This approach transforms our traditional view of the nation, offering fresh insights into population, density, and other socio-economic variables that are often obscured on conventional maps.\u00a0Hexagons have a unique property: they tile a plane without gaps, and each cell maintains a consistent shape and size. Unlike traditional maps where county areas can vary dramatically, a hexagonally tiled cartogram normalizes visual weight. Every county"
  },
  {
    "title": "Discord client that works on Win95*, Win98 and above (github.com/discordmessenger)",
    "points": 80,
    "submitter": "Tiberium",
    "submit_time": "2025-02-03T11:49:49 1738583389",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=42917268",
    "comments": [
      "Someone needs to get around to doing this for Microsoft Teams.\n \nreply",
      "You'd need to somehow get it to work on Windows 10/11 first.\n \nreply",
      "According to the author's bluesky posts, there's a few features missing from Win32s compared to regular Win32 that prevent this from being ported even further back to windows 3.1 easily.\n \nreply",
      "\"General limitations under Win32s (131896)\", surviving technical note.https://ftp.zx.net.nz/pub/archive/ftp.microsoft.com/MISC/KB/...\n \nreply",
      "The \u2018s\u2019 does stand for \u2018subset\u2019 after all :)\n \nreply",
      "How long until they get slapped with a trademark claim?\n \nreply",
      "Ripcord is a third party discord client that's been around for quite a while, as far as I'm aware they haven't run into any trouble. And they actually might get used as opposed to someone seriously trying to run discord on windows 95\n \nreply",
      "I think the root of the problem in this one is that it's named \"Discord Messenger\", Ripcord is reminiscent but not confusing.\n \nreply",
      "That's fair\n \nreply",
      "Speaking as a ripcord user, it helps your unofficial client avoid being blocked when you don't update for years and fall behind on feature parity (though I personally don't miss most any of said features)\n \nreply"
    ],
    "link": "https://github.com/DiscordMessenger/dm",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Discord Messenger is a free Discord-compatible messaging client that works on almost 30 years of Windows.\n      Discord Messenger is a messenger application designed to be compatible with Discord, while being\nbackwards compatible with down to Windows 2000 (although support for even older versions has been\nattempted).Its motto: It's time to ditch MSN and Yahoo.NOTE: This is beta software, so there may be issues which need to be fixed!The project is licensed under the MIT license.Using third party clients is against Discord's TOS! Although the risk to get banned is low, the\nrisk is there! The author of this software is not responsible for the status of your Discord\naccount.See https://twitter.com/discord/status/1229357198918197248.A Discord server about this client can be joined here: https://discord.gg/cEDjgDbxJj\nWindows 2000 or newe"
  },
  {
    "title": "Running ArchiveTeam's Warrior in Kubernetes (gabrielsimmer.com)",
    "points": 65,
    "submitter": "gmemstr",
    "submit_time": "2025-02-05T18:04:20 1738778660",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=42952584",
    "comments": [
      "For anyone else interested in running this, it only took a couple seconds to launch their docker-compose.ymlhttps://github.com/ArchiveTeam/warrior-dockerfile/blob/maste...\n \nreply",
      "I noticed from the docker overlay filesystem that the container was spraying files all over the disk. (Ephemeral, destroyed on container shutdown, sure, but I wanted to reduce write-wear on my ssd...)I tried setting it up with /tmp as a tmpfs (ramdisk) but it then refused to start...Anyone know any broad-spectrum docker incantations to force all overlay writes to RAM, for a container?\n \nreply",
      "You can put the entire docker directory in a ramdisk. Same as you would when trying to move it to a secondary harddisk. Risky though as a reboot would wipe everything\n \nreply",
      "I think you'll just need to mount it at the right place, with right permissions.Demonstrated here https://stackoverflow.com/questions/39193419/docker-in-memor...\n \nreply",
      "> the container was spraying files all over the diskRight, that's basically the point...the Warrior downloads files, compresses them, and uploads them for archival. This necessarily requires staging the files somewhere between download and upload.> Anyone know any broad-spectrum docker incantations to force all overlay writes to RAM, for a container?Why would you want this? This sounds like a terrible footgun.\n \nreply",
      "They say exactly why they want it... \"I wanted to reduce write-wear on my ssd\"\n \nreply",
      "The Warrior doesn't resume old jobs after a power cycle, so what's the point of committing anything at all to non-volatile storage?\n \nreply",
      "Many of these sites are already captured and archived by proper entities as required by federal law. More is better, I guess, except when it isn't. Duplication of effort is a huge problem in the humanities in general and with archiving in particular.The whole concept needs to be rethought. Captures from these tools show up under \"ArchiveTeam\" which is currently pumping thousands of copies of the Google Home Page into the Wayback Machine every week. Or at least trying to.https://web.archive.org/web/20250122000033/www.google.comLike so many things about archive.org, when you dig in you start to find wonder and craziness at every turn.\n \nreply",
      "> by proper entities as required by federal law.What federal law do you suppose is guiding the mass deletions? That doesn't look like archiving to me. Now that the foxes are running the henhouse, how reliable do you suppose their own archives are?\n \nreply",
      "Some of the mass deletions are merely a new administration setting up shop. Policies from the previous administration don't belong on the current whitehouse.gov. They wind up here instead https://bidenwhitehouse.archives.gov/We pay half a billion in tax dollars for the National Archives, and nearly a billion to the Library of Congress to preserve these records. Others are managed as part of Presidential Libraries.Thousands of employees, dozens of facilities, billions of dollars.Meanwhile archive.org doesn't have air conditioning and preserves physical material within the blast radius of an oil refinery. They let vagrants sleep on their steps yet seem surprised when they set the utility pole outsides on fire.I didn't say it didn't need to be done. I said the whole process needs to be rethought with professional supervision. Setting up more volunteer K8 clusters so that more copies of the Google Home Page can be captured with the wrong user agent isn't going to save democracy.\n \nreply"
    ],
    "link": "https://gabrielsimmer.com/blog/archiveteam-warrior-kubernetes",
    "first_paragraph": ""
  },
  {
    "title": "20k federal workers take \"buyout\" so far, official says (axios.com)",
    "points": 210,
    "submitter": "djoldman",
    "submit_time": "2025-02-05T16:23:10 1738772590",
    "num_comments": 350,
    "comments_url": "https://news.ycombinator.com/item?id=42950790",
    "comments": [
      "I think the main issue for anyone wanting to take the offer is simply: this was never authorized by congress, so the money to pay people to September is questionable if it exists at best. Meanwhile, there's a government funding deadline on March 14, 2025. So there's a very real chance at this deal offering something closer to ~1 month of pay before it suddenly gets dropped due to budget negotiations.It would be an incredibly generous and nice buyout package, but obviously if it gets torn up after a month it's not that great of a deal.\n \nreply",
      "Aren't Twitter workers still trying to get their severances and they took the offer when Twitter actually had the money to pay.\n \nreply",
      "The senior executives still are.Case: https://www.courtlistener.com/docket/68307087/agrawal-v-musk...The only substantive order in the case to date appears to be one denying Elon Musk et al.'s motion to dismiss one of the claims: https://storage.courtlistener.com/recap/gov.uscourts.cand.42...\n \nreply",
      "To my knowledge, no. Former employees sued to get Twitter's old pre-acquisition severance package and the court dismissed it. [1]1. https://techcrunch.com/2024/07/10/elon-musk-does-not-owe-ex-...\n \nreply",
      "This is not true. There are thousands of cases at various stages still in progress. This ruling was specific to one specific avenue being pursued. Source: the body of the article you posted.\n \nreply",
      "How about this: https://finance.yahoo.com/news/elon-musk-loses-battle-dismis...\n \nreply",
      "This one is about executives not regular employees.\n \nreply",
      "Aren't Twitter workers still trying to get their severances and they took the offer when Twitter actually had the money to pay.Considering that SpaceX is so far behind on its bills that dozens and dozens of companies in Texas have had to place liens against the company, my guess is that neither the Twitter people, nor the SpaceX people, nor the federal buyout people will ever see a dime.For some reason, links to stories about the leins and SpaceX becoming notorious for not paying its bills are hard to come by, but it's in the printed newspapers regularly; as recently as yesterday. Here's and older link I could find: https://www.chron.com/culture/article/spacex-overdue-bills-t...\n \nreply",
      "The article mentions $2.5MM liens which is drastically less than 1% of expenses of $1445MM (\"[SpaceX] generated $55 million in profit on $1.5 billion in revenue during the first quarter of 2023\")It don't appear to be because SpaceX is having trouble paying.I would guess SpaceX are delaying payment as much as possible because it is cheap lending and because it's run as an extremely mercenary company.Their costs of deliquent payment are likely below their lending costs. So optimally don't pay until the cost of deliquency exceeds lending costs (maybe \u2248 junk bond rate per year).\n \nreply",
      "While interesting, I don't see how it might make any difference to any federal workers' decisions?Musk doesn't pay \"because he can get away with it\" isn't better or worse than \"because there's a money shortage\"; and in any case, the federal government being delinquent is a very different kind of catastrophe, as is it making promises it refuses to keep.\n \nreply"
    ],
    "link": "https://www.axios.com/2025/02/04/trump-buyout-federal-workers-20000",
    "first_paragraph": ""
  },
  {
    "title": "News from Scroll 5 (scrollprize.substack.com)",
    "points": 21,
    "submitter": "diodorus",
    "submit_time": "2025-02-05T21:23:09 1738790589",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://scrollprize.substack.com/p/exciting-news-from-scroll-5",
    "first_paragraph": ""
  },
  {
    "title": "Avoiding outrage fatigue while staying informed (scientificamerican.com)",
    "points": 453,
    "submitter": "headalgorithm",
    "submit_time": "2025-02-05T14:55:03 1738767303",
    "num_comments": 441,
    "comments_url": "https://news.ycombinator.com/item?id=42949277",
    "comments": [
      "One thing to consider for those of us who are more sensitive to online outrage is to just quit social media all together. I\u2019m technically gen z and I\u2019ve been off of social media (aside from HN, WhatsApp and discord) for years and you wouldn\u2019t believe how great it\u2019s been for my overall state of mind.Reddit, instagram, X, Facebook, TikTok, LinkedIn, Threads, etc are all the equivalent of digital junk food and I\u2019d argue that we\u2019re all a lot more negatively affected by it than we think. There\u2019s a reason \u2018brain rot\u2019 was word of the year.\n \nreply",
      "I quit reddit too recently, I still look at it for info but I'm not logged in/scrolling through itI find myself reaching for something when I have YouTube/chilling at my desk at the end of the day, can't code anymore/make something just on till I sleep. Sometimes have the desire to play a video game (I have a gaming rig too funny how that works)I've been trying to read HN or IEEE, TechCrunch stuff like that as my \"lazy fun\"I will miss posting stuff like \"what is this car\" or being part of the car talk for a sporty car I drive but idk kind of want to just live tooIt's unfortunate people expect you to have social media like a girl asks me if I have Instagram and I'm weird to not have one, I get it they can scope you out too for safety but when I tried using that stuff I felt this pressure to post about somethingAnyway my main goal in life right now is getting out of debt/staying fit and work on projects\n \nreply",
      "I checked reddit recently for the first time in a while, and I was shocked by how radicalized its become. An echo chamber of hateful people and perhaps GPTs that are agitating the big subreddits. The contrast is stark with all the \"no place for hate\" in the rules and endless banning of microaggressions.I saw dozens of death threats. Even an explicit death threat thread with over 40,000 upvotes before reddit stepped in and shut the whole subreddit down.It reminded me of Ghostbusters 2 with all the aggressively angry people and the ooze pouring out of the sewers, all building upon itself.\n \nreply",
      "This is just the consequence of the API protests. Despite people claiming it had no lasting impact, admins coming in and making sweeping changes to mod teams replacing them with loyalists, alongside ramping up centralized feeds to serve more ads onto meant content quality took a nosedive. This is obvious in most subs if you actually look at who is submitting the threads (something the app and All/Popular pages hides in several views), most of these subs are dominated by a handful of accounts. It's a cycle too, because often they'll continue spamming subs in order to get on All/Popular, or make up weird stories to do so, effectively karma farming taken very seriously, with mods encouraging it because of the aforementioned loyalists.It's all just driveby anger and reposts. Maybe some smaller subs with good communities here and there, but that often requires a mod team putting in substantial hours and remaining under the radar from All/Popular in any shape.Forgot to mention, Reddit also started paying these accounts for posting. So a literal financial incentive to ragebait. It' called the \"Contributor Program\".\n \nreply",
      "That particular subreddit isn't shut down, it was temporarily suspended as the moderators simply got overwhelmed. There's no indication of bad faith from either the mod team nor the reddit admins, the floodgate was just too much for them to handle. It pretty much says so in the ban message, admins are gonna help them take back control and it will be up within a couple of days.\n \nreply",
      "Reddit, by far, is one of the worst echo chambers on the internet. I've seen hundreds of death threats at one political group on there, but if any veiled threat is made against the \"reddit approved party\" it is instantly removed or accounts suspended. This really peaked during 2020, when open calls for violence  stayed up, some with reddit admin approvals.It used to be a good site, but that was many years ago.\n \nreply",
      "Did people forget the_donald?What do people think others will do, when they see that the_donald behavior gets rewarded by electoral and political support?If its not clear, everyone is going to radicalize, because its getting success.\n \nreply",
      "> I checked reddit recently for the first time in a while, and I was shocked by how radicalized its become.Reddit has always had these elements, but they were previously isolated to certain subreddits.I noticed the biggest change when the app and website became aggressive about getting people to join other subreddits and inserting posts from other subreddits into people's feeds. Suddenly the isolated subreddits I followed were full of low effort content and angry comments.Reddit's front page is shockingly bad. The amount of misinformation and ragebait that gets upvoted to the front page is almost hard to believe.It's also interesting that many subreddits have embraced the ragebait. Subreddits like /r/AITA have been clear about how they don't care if stories are real or not, but legions of Redditors engage with obvious ChatGPT spam as if it was a real situation they need to weigh in on.\n \nreply",
      "I just stick to the niche subreddits (games, interests, whatever). The main subreddits have been especially aggressive echo chambers for a long time now.\n \nreply",
      "/r/worldnews is one of the most astroturfed places on the internet. Some of those commenters are so nationalist and bloodthirsty they unnerve me. The ban hammer is extremely active on this sub, and for saying completely innocuous political statements about personal preference. I'm absolutely sure this is broader than just that sub but I've probably heard this specific complaint from probably a dozen other people too.I will say, the subreddit system does a decent job of quarantining the dysfunction to that sub. The mod quality is everything and the mod drama is an absolute dumpster fire. (Extremely curiously, Ghislaine Maxwell seems to have been one of the most prolific of the mods, and one of her suspected accounts may be one of the most successful (karma-wise) posters of all reddit.) But on the flipside, /r/askhistorians is still one of the best resources on the internet. Many of the specialty subreddits I frequent (Aviation, UkraineRussiaReport, video game subs, several miscellaneous african subs) are still functioning fine.\n \nreply"
    ],
    "link": "https://www.scientificamerican.com/podcast/episode/how-to-avoid-outrage-fatigue-and-tune-in-without-burning-out/",
    "first_paragraph": "February 4, 2025Avoiding Outrage Fatigue while Staying InformedOutrage fatigue can wear us down\u2014but we can take care of ourselves in an onslaught of overwhelming news.By Rachel Feltman, Tanya Lewis, Madison Goldberg & Fonda Mwangi Anaissa Ruiz Tejada/Scientific American[CLIP: Theme music]Rachel Feltman: For Scientific American\u2019s Science Quickly, this is Rachel Feltman.No matter what you believe, I\u2019m willing to bet you\u2019ve been feeling a lot of outrage lately. To me personally, it feels unavoidable: I can\u2019t look down at my phone or glance up at a TV without seeing something that makes me upset. And that\u2019s really exhausting. But when outrage is everywhere, what can we do to keep it from getting to us?If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.Here to talk to us about fighting so-called outrage "
  },
  {
    "title": "Gemini 2.0 is now available to everyone (blog.google)",
    "points": 398,
    "submitter": "meetpateltech",
    "submit_time": "2025-02-05T16:03:18 1738771398",
    "num_comments": 179,
    "comments_url": "https://news.ycombinator.com/item?id=42950454",
    "comments": [
      "What is the model I get at gemini.google.com (i.e. through my Workspace subscription)? It says \"Gemini Advanced\" but there are no other details. No model selection option.I find the lack of clarity very frustrating. If I want to try Google's \"best\" model, should I be purchasing something? AI Studio seems focused around building an LLM wrapper app, but I just want something to answer my questions.Edit: what I've learned through Googling: (1) if you search \"is gemini advanced included with workspace\" you get an AI overview answer that seems to be incorrect, since they now include Gemini Advanced (?) with every workspace subscription.(2) a page exists telling you to buy the add-on (Gemini for Google Workspace), but clicking on it says this is no longer available because of the above. (3) gemini.google.com says \"Gemini Advanced\" (no idea which model) at the top, but gemini.google.com/advanced redirects me to what I have deduced is the consumer site (?) which tells me that Gemini Advanced is another $20/monthThe problem, Google PMs if you're reading this, is that the gemini.google.com page does not have ANY information about what is going on. What model is this? What are the limits? Do I get access to \"Deep Research\"? Does this subscription give me something in aistudio? What about code artifacts? The settings option tells me I can change to dark mode (thanks!).Edit 2: I decided to use aistudio.google.com since it has a dropdown for me on my workspace plan.\n \nreply",
      "changes must be rolling out now, I can see 3 Gemini 2.0 models in the dropdown, with blue \"new\" badges.screenshot: https://beeimg.com/images/g25051981724.png\n \nreply",
      "This works on my personal Google account, but not on my workspace one. So I guess there's no access to 2.0 Pro then? I'm ok trying out Flash for now and see if it fixes the mistakes I ran into yesterday.Edit: it does not. It continues to miss the fact that I'm (incorrectly) passing in a scaled query tensor to scaled_dot_product_attention. o3-mini-high gets this right.\n \nreply",
      "As someone with over a decade of Google Apps management history, my experiences is Workspace customers are practically always the last to get the shiny new features. Quite frustrating.\n \nreply",
      "Isn't that generally how it goes?  Windows Vista was tested on consumers to make 7 Enterprise appropriate?\n \nreply",
      "If you subscribe to Gemini the menu looks like this, with the addition of 2.0 Pro.https://imgur.com/a/xZ7hzag\n \nreply",
      "It doesn't for workspace users. No dropdown appears.\n \nreply",
      "This is funny how bad UI is on some of websites which are considered the best. Today I tried to find prices for Mistral models but I couldn\u2019t. Their prices page leads to 404\u2026\n \nreply",
      "Just in case you're still interested in their pricing, it's towards the bottom of [1], section \"How to buy\", when changing the selection from \"Self-hosted\" to \"Mistral Cloud\".[1] https://mistral.ai/en/products/la-plateforme\n \nreply",
      "if only these models were good at web development and could be used in agentic frameworks to build high quality website... wait...\n \nreply"
    ],
    "link": "https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/",
    "first_paragraph": "Feb 05, 2025[[read-time]] min readIn December, we kicked off the agentic era by releasing an experimental version of Gemini 2.0 Flash \u2014 our highly efficient workhorse model for developers with low latency and enhanced performance. Earlier this year, we updated 2.0 Flash Thinking Experimental in Google AI Studio, which improved its performance by combining Flash\u2019s speed with the ability to reason through more complex problems.And last week, we made an updated 2.0 Flash available to all users of the Gemini app on desktop and mobile, helping everyone discover new ways to create, interact and collaborate with Gemini.Today, we\u2019re making the updated Gemini 2.0 Flash generally available via the Gemini API in Google AI Studio and Vertex AI. Developers can now build production applications with 2.0 Flash.We\u2019re also releasing an experimental version of Gemini 2.0 Pro, our best model yet for coding performance and complex prompts. It is available in Google AI Studio and Vertex AI, and in the Gemi"
  },
  {
    "title": "Andrej Karpathy: Deep Dive into LLMs Like ChatGPT [video] (youtube.com)",
    "points": 295,
    "submitter": "leroman",
    "submit_time": "2025-02-05T18:29:30 1738780170",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=42952960",
    "comments": [
      "I tell all my friends that Andrej was the best instructor I had in grad school, even though I didn't even go to Stanford--I just watched his CS321n videos on YouTube. Really thrilled that he's still making videos.\n \nreply",
      "He's made more than 5 videos covering basically the same topic, of transformer architecture and training. Wonder whats different about this one?\n \nreply",
      "My YouTube videos fall into two tracks:1. technical track (all the GPT repro series)2. general audience trackFor (2), I had a 1hr video from 1 year ago, but I didn't actually expect that video to be some kind of authoritative introduction to LLMs. The history is that I was invited to give an LLM talk (to general audience), prepared some random slides for a day, gave the talk, and then re-recorded the talk in my hotel room later in a single take, and that become the video. It was quite random and haphazard. So I wanted to loop back around more formally and do a more comprehensive intro to LLMs for general audience; Something I could for example give to my parents, or a friend who uses ChatGPT all the time and is interested in it, but doesn't have the technical background to go through my videos in (1). That's this video.\n \nreply",
      "Great work! I love your videos; they've taught me so much. Any plans for a Mixture of Experts (MoE) video? My understanding is that starting from GPT4 most advance models use MoE to some extent. For example, can I take the model from your GPT2 video and just change the feed forward layer to an MoE layer like the one found here (1)? I guess I can just try it myself but I enjoy the expert guidance you provide in your videos. Please don't stop! great content!1. https://github.com/mistralai/mistral-inference/blob/main/src...\n \nreply",
      "I haven\u2019t watched this video yet, but do you plan to create any technical videos in the (1) series on RL in LLMs?\n \nreply",
      "His intro to RL (not for LLM) blog post is a great read FYIhttps://karpathy.github.io/2016/05/31/rl/\n \nreply",
      "This would be very welcome as it brings us closer to understanding the secret sauce behind training a real, practical LLM.\n \nreply",
      "I watched that one after finding it randomly! Will give the new one a watch too\n \nreply",
      "From the description:    I have one \"Intro to LLMs\" video already from ~year ago, but that is just a re-recording of a random talk, so I wanted to loop around and do a lot more comprehensive version.\n\nI think he has videos on building GPT2 from scratch, but this seems more high-level.\n \nreply",
      "When he drops a vid, you don't ask questions. You watch first and then ask questions :)\n \nreply"
    ],
    "link": "https://www.youtube.com/watch?v=7xTGNNLPyMI",
    "first_paragraph": ""
  },
  {
    "title": "Software development topics I've changed my mind on (chriskiehl.com)",
    "points": 602,
    "submitter": "belter",
    "submit_time": "2025-02-05T09:50:30 1738749030",
    "num_comments": 593,
    "comments_url": "https://news.ycombinator.com/item?id=42946281",
    "comments": [
      "> Most won't care about the craft. Cherish the ones that do, meet the rest where they are> (\u2026)> People who stress over code style, linting rules, or other minutia remain insane weirdos to me. Focus on more important things.What you call \u201cstressing over minutiae\u201d others might call \u201ccaring for the craft\u201d. Revered artisans are precisely the ones who care for the details. \u201cStressing\u201d is your value judgement, not necessarily the ground truth.What you\u2019re essentially saying is \u201ccherish the people who care up to the level I personally and subjectively think is right, and dismiss everyone who cares more as insane weirdos who cannot prioritise\u201d.\n \nreply",
      "There's another way to look at this: if you consider the school of thought that says that the code is the design, and compilation is the construction process, then stressing over code style is equivalent to stressing over the formatting and conventions of the blueprint (to use a civil engineering metaphor), instead of stressing over load bearing, material costs and utility of the space.I'm fond of saying that anything that doesn't survive the compilation process is not design but code organization. Design would be: which data structures to use (list, map, array etc.), which data to keep in memory, which data to load/save and when, which algorithms to use, how to handle concurrency etc. Keeping the code organized is useful and is a part of basic hygiene, but it's far from the defining characteristic of the craft.\n \nreply",
      "> the formatting and conventions of the blueprintSome of those formatting conventions are written in blood.  The clarity of a blueprint is a big deal when people are using it to convey safety critical information.I don\u2019t think code formatting rises anywhere close to that level, but it\u2019s also trying to reduce cognitive load which is a big deal in software development.  Nobody wants to look at multiple lines concatenated together, how far beyond that you take things runs into diminishing returns.  However at a minimum formatting changes shouldn\u2019t regularly complicate doing a diff.\n \nreply",
      "I 100% agree. The problem is that after a half a century, software engineering discipline has been unable to agree on global conventions and standards. I recently had an experience where a repair crew was worried about the odd looking placement of a concrete beam in my house. I brought over the blueprints, and the technician found the schedule of beams and columns within seconds, pinpointed the beam and said, \"Ah, that's why. We just need to <solution I won't go into>\". Just then it struck me how we can't do this in software engineering, even when the project is basically a bog-standard business app: CRUD API backed by an RDBMS.\n \nreply",
      "That\u2019s because the few hard rules you have to comply with have workarounds and matters rarely. In house construction, you have to care about weight, material degradation, the code, etc\u2026 there\u2019s no such limitation on software so you can get something to work even if it\u2019s born out of a LSD trip.But we do have some common concepts. But they\u2019re theoretical, so only the people that read the books knows the jargon.\n \nreply",
      "I mean why should we expect software to have hard rules? Flexibility is the point, no?\n \nreply",
      "No. Flexibility is a side effect. Sometimes it's a useful side effect, other times it bites you in the ass.\n \nreply",
      "The rules are what make it flexible. The rules let me understand what the heck is going on in the code you wrote so I can change it. Code that is faster to rewrite from scratch isn\u2019t flexible.\n \nreply",
      "Construction and civil engineering have been unable to agree on global conventions and standards, and they have a multi-millenia head start over software engineering. The US may claim to follow the \"International Building Code\", but it's just called that because a couple of small countries on the Americas have adopted it. For all intents and purposes it's a national standard. Globally we can't even agree on a system of units and measurements, never mind anything more consequential than that.\n \nreply",
      "I\u2019d say that globally we have agreed on a system of units and measurements. It\u2019s just the US and a handful of third world countries that don\u2019t follow that system.\n \nreply"
    ],
    "link": "https://chriskiehl.com/article/thoughts-after-10-years",
    "first_paragraph": "Published 2025-02-03 Four years ago I posted about the same topic.  A kind email reminded me its time for another check in.Things I now believe, which past me would've squabbled with:We'll see which of these have flipped at year 15. "
  }
]