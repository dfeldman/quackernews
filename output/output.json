[
  {
    "title": "Intel and AMD standardise ChkTag to bring Memory Safety to x86 (intel.com)",
    "points": 143,
    "submitter": "ashvardanian",
    "submit_time": "2025-10-14T18:04:01 1760465041",
    "num_comments": 61,
    "comments_url": "https://news.ycombinator.com/item?id=45582958",
    "comments": [
      "With all the negative comments here: This is existing technology on ARM64 (MTE) and on modern iPhones (https://security.apple.com/blog/memory-integrity-enforcement...).For a good intuition why this (coupled with instrumenting all allocators accordingly) is a game-changer for exploitation, check https://docs.google.com/presentation/d/1V_4ZO9fFOO1PZQTNODu2...In general, having this come to x86 is long-overdue and very welcome.reply",
      "But wait, how do you know that's what this is?The reason I'm negative is the entire article has zero detail on WTF this instruction set is or does.  The best you can do is guess from the name of the instruction set.Compare the linked iPhone article to this blog and you'll quickly see the difference.  There's very real discussion in the MTE article of how the instructions work and what they do.  This article just says \"Memory safety is hard and we'll fix it with these new instructions that fix memory safety!\"reply",
      "Probably because it's very likely that both AMD and Intel have had engineers working on this sort of thing for a long time, and they're now deciding to collectively hash out whatever the solution is going to be for both of them.reply",
      "> This is a general data security concern, including for AI data, which governments have urged companies to defend against.Had to find some way to use \"AI\" in a press release, less the stock gods get angry and vengeful.reply",
      "lest*: https://en.wiktionary.org/wiki/lestreply",
      "As your citation says, lest is an Old English contraction of 'less [pronoun that starts with th]'.reply",
      "Then write \"less that the\" if you prefer not to use the contraction.reply",
      "Or, I can use less without using a word after it that lest would be a contraction for.The way I've used it is fine.reply",
      "Sparse on details.Presumably will be based on the existing Linear Address Masking/Upper Address Ignore specs, which are equivalent, and will be similar to CHERI.If so it needs to be opt-in or at least opt-out per process, because many language runtimes use these pointers bits to optimize dynamic types, and would suffer a big performance hit if they were unable to use them.reply",
      "Dynamic types have classically used the lower bits freed by alignment constraints. If I know a cons cell is 16 bytes then I can use the low 4 bits of an address to store enough type info to disambiguate.reply"
    ],
    "link": "https://community.intel.com/t5/Blogs/Tech-Innovation/open-intel/ChkTag-x86-Memory-Safety/post/1721490",
    "first_paragraph": "\n\n                            Success!  Subscription added.\n                        \n\n\n                            Success!  Subscription removed.\n                        \n\n\n                            Sorry, you must verify to complete this action. Please click the verification link in your email. You may re-send via your\n                            profile.\n                        \nChkTag: x86 Memory Safety\u00a0Memory safety violations due to programming errors have long afflicted software. Industry and academia have been searching for solutions to this problem. As first noted in August 2025 posts by Intel and AMD x86 Ecosystem Advisory Group (EAG) leaders [1, 2], Intel and AMD are working together, along with their ecosystem partners in the EAG, to address the need for memory safety. They are creating a unified specification for a universal x86 memory tagging instruction set architecture, code named ChkTag (pronounced \u201cCheck Tag\u201d). This will help ensure that x86 continues to meet the ev"
  },
  {
    "title": "Building a message queue with only two UNIX signals (leandronsp.com)",
    "points": 57,
    "submitter": "SchwKatze",
    "submit_time": "2025-10-20T22:22:45 1760998965",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=45650178",
    "comments": [
      "Before y'all go nuts with the criticisms...\"Yes, we built a message broker using nothing but UNIX signals and a bit of Ruby magic. Sure, it\u2019s not production-ready, and you definitely shouldn\u2019t use this in your next startup (please don\u2019t), but that was never the point.\"The real takeaway here isn\u2019t the broker itself: it\u2019s understanding how the fundamentals work. We explored binary operations, UNIX signals, and IPC in a hands-on way that most people never bother with.\"We took something \u201cuseless\u201d and made it work, just for fun. So next time someone asks you about message brokers, you can casually mention that you once built (or saw) one using just two signals. And if they look at you weird, well, that\u2019s their problem. Now go build something equally useless and amazing. The world needs more hackers who experiment just for the fun of it.\"reply",
      "Unfortunately I bet that 90% won't even reach at that part and just ragebait based on the title. The golden rule of modern age is always do the disclaimer as soon as possible.reply",
      "Or you could skip the rage bait title entirely?reply",
      "Maybe, but I know Leandro, it was more a joke than anything else. People just don't chill, the post is coolreply",
      "That's fine, but then you shouldn't be surprised and complain when people respond to the rage bait.reply",
      "\"Its just a joke, bro\" is always a terrible defense for rude behaviour.reply",
      "The later the disclaimer, the funnier.reply",
      "UNIX signals *do not* queue.  If two or more signals with the same number are sent faster than the receiving thread handles them (due to the signal being blocked and/or the thread not being scheduled), all but the last will be lost irrevocably.  There is no mechanism to prevent this.https://ldpreload.com/blog/signalfd-is-uselessreply",
      "RT signals do get queued... that is one of the major differences (and yes, the essay is not using them, so your point stands as it is written, but using RT signals is a mechanism to prevent it).https://davmac.org/davpage/linux/rtsignals.htmlreply",
      "Naively asking: what prevents RT Unix/Linux from being used in place of non-RT mainstream versions? Seems like a superset.reply"
    ],
    "link": "https://leandronsp.com/articles/you-dont-need-kafka-building-a-message-queue-with-only-two-unix-signals",
    "first_paragraph": "Have you ever asked yourself what if we could replace any message broker with a very simple one using only two UNIX signals? Well, I\u2019m not surprised if you didn\u2019t. But I did. And I want to share my journey of how I achieved it.If you want to learn about UNIX signals, binary operations the easy way, how a message broker works under the hood, and a bit of Ruby, this post is for you.And if you came here just because of the clickbait title, I apologize and invite you to keep reading. It\u2019ll be fun, I promise.A few days ago, I saw some discussion on the internet about how we could send messages between processes. Many people think of sockets, which are the most common way to send messages, even allowing communication across different machines and networks. Some don\u2019t even realize that pipes are another way to send messages between processes:Here\u2019s what\u2019s happening:Note the word \u201csend\u201d. Yes, anonymous pipes are a form of IPC (Inter-process communication). Other forms of IPC in UNIX include:Ac"
  },
  {
    "title": "Claude Code on the web (anthropic.com)",
    "points": 357,
    "submitter": "adocomplete",
    "submit_time": "2025-10-20T18:12:23 1760983943",
    "num_comments": 217,
    "comments_url": "https://news.ycombinator.com/item?id=45647166",
    "comments": [
      "We were heavy users of Claude Code ($70K+ spend per year) and have almost completely switched to codex CLI. I'm doing massive lifts with it on software that would never before have been feasible for me personally, or any team I've ever run. I'll use Claude Code maybe once every two weeks as a second set of eyes to inspect code and document a bug, with mixed success. But my experience has been that initially Claude Code was amazing and a \"just take my frikkin money\" product. Then Codex overtook CC and is much better at longer runs on hard problems. I've seen Claude Code literally just give up on a hard problem and tell me to buy something off the shelf. Whereas Codex's ability to profoundly increase the capabilities of a software org is a secret that's slowly getting out.I don't have any relationship with any AI company, and honestly I was rooting for Anthropic, but Codex CLI is just way way better.Also Codex CLI is cheaper than Claude Code.I think Anthropic are going to have to somehow leapfrog OpenAI to regain the position they were in around June of this year. But right now they're being handed their hat.reply",
      "I find Codex CLI to be very good too, but it\u2019s missing tons of features that I use in Claude Code daily that keep me from switching full time.- Good bash command permission system- Rollbacks coupled with conversation and code- Easy switching between approval modes (Claude had a keybind that makes this easy)- Ability to send messages while it\u2019s working (Codex just queues them up for after it\u2019s done, Claude injects them into the current task)- Codex is very frustrating when I have to keep allowing it to run the same commands over and over, Claude this works well when I approve it to run a command for the session- Agents (these are very useful for controlling context)- A real plan mode (crucial)- Skills (these are basically just lazy loaded context and are amazing)- The sandboxing in codex is so confusing, commands fail all the time because they try to log to some system directory or use internet access which is blocked by default and hard to figure out- Codex prefers python snippets to bash commands which is very hard to permission and auditWhen Codex gets to feature parity, I\u2019ll seriously look at switching, but until then it\u2019s just a really good model wrapped in an okay harnessreply",
      "I don't think anyone can reasonably argue against Claude Code being the most full-featured and pleasant to use of the CLI coding agent tools. Maybe some people like the Codex user experience for idiosyncratic reasons, but it (like Gemini CLI) still feels to me rather thrown together - a Claude Clone with a lot of rough edges.But these CLI tools are still fairly thin wrappers around an LLM. Remember: they're \"just an LLM in a while loop with access to tool calls.\" (I exaggerate, and I love Claude Code's more advanced features like \"skills\" as much as anyone, but at the core, that's what they are.) The real issue at stake is what is the better LLM behind the agent: is GPT-5 or Sonnet 4.5 better at coding. On that I think opinion is split.Incidentally, you can run Claude Code with GPT-5 if you want a fair(er) comparison. You need a proxy like LiteLLM and you will have to use the OpenAI api and pay per-token, but it's not hard to do and quite interesting. I haven't used it enough to make a good comparison, however.reply",
      "You are absolutely right that Claude Code is significantly more feature complete, has a better UX, and is faster than codex, but in my experience this is secondary to a better model. For me, 90% of the time Codex just does a better job. You can work around everything else, but it's hard to work around a worse model.reply",
      "Yeah I think the argument is the tooling vs agent. Maybe the OpenAI agent is performing better now, but the tooling is significantly better from anthropic.The anthropic (ClaudeCode) tooling is best-in-class to me. You listed many features that I have become so reliant on now, that I consider them the Ante that other competitors need to even be considered.I have been very impressed with the Anthropic agent for code generation and review. I have found the OpenAI agent to be significantly lacking by comparison. But to be fair, the last time I used OpenAI's agent for code was about a month ago, so maybe it has improved recently (not at all unreasonable in this space). But at least a month ago when using them side-by-side the codex CLI was VERY basic compared to the wealth of features and UI in the ClaudeCode CLI. The agents for Claude were also so much better than OpenAI, that it wasn't even close. OpenAI has always delivered me improper code (non-working or invalid) at a very high rate, whereas Claude is generally valid code, the debate is just whether it is the desired way to build something.reply",
      "I totally agree. I remember the June magic as well - almost overnight my abilities and throughput were profoundly increased, I had many weeks of late nights in awe and wonder trying things that were beyond my ability to implement technically but within the bounds of my conceptual understanding.Initially, I found Codex CLI with GPT-5 to be a substitute for Claude Code -  now GPT-5 Codex materially surpasses it in my line of work, with a huge asterisk. I work in a niche industry, and Codex has generally poor domain understanding of many of the critical attributes and concepts. Claude happens to have better background knowledge for my tasks, so I've found that Sonnet 4.5 with Claude Code generally does a better job at scaffolding any given new feature. Then, I call in Codex to implement actual functionality since Codex does not have the \"You're absolutely right\" and mocked/placeholder implementation issues of CC, and just generally writes clean, maintainable, well-planned code. It's the first time I've ever really felt the whole \"it's as good as a senior engineer\" hype - I think, in most cases, GPT5-Codex finally is as good as a senior engineer for my specific use case.I think Codex is a generally better product with better pricing, typically 40-50% cheaper for about the same level of daily usage for me compared to CC. I agree that it will take a genuinely novel and material advancement to dethrone Codex now. I think the next frontier for coding agents is speed. I would use CC over Codex if it was 2x or 3x as fast, even at the same quality level. Otherwise, Codex will remain my workhorse.reply",
      "> trying things that were beyond my ability to implement technically but within the bounds of my conceptual understandingThis is a really neat way of describing the phenomenon I've been experiencing and trying to articulate, cheers!reply",
      ">I think, in most cases, GPT5-Codex finally is as good as a senior engineer for my specific use case.This is beyond bananas to me given that I regularly see codex high and Gpt-5-high both fail to create basic react code slightly off the normal distribution.reply",
      "That might say something about the understandability of the react framework/paradigm ;)Quality varies a lot based on what you're doing, how you prompt it, how you orchestrate it, and how you babysit and correct it. I haven't seen anything I'd call senior, but I have seen it, for some classes of tasks, turn this particular engineer into many seniors. I still have to supply all the heavy lifting (here's the concurrency model, how you'll ensure exactly-once-delivery, particular functions and classes you definitely want, a few common pitfalls to avoid, etc), but then it can flesh out the details extremely well.reply",
      "It makes me waaayyyy faster but, like you, that\u2019s because I already know what has to be done.reply"
    ],
    "link": "https://www.anthropic.com/news/claude-code-on-the-web",
    "first_paragraph": ""
  },
  {
    "title": "A laser pointer at 2B FPS [video] (youtube.com)",
    "points": 201,
    "submitter": "thunderbong",
    "submit_time": "2025-10-19T06:42:19 1760856139",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=45632429",
    "comments": [
      "Tl:dw for how this works:He scans one line at a time with a mirror into a photomultiplier tube which can detect single photon events. This is captured continually at 2MSample/s (2 billion times per second: 2B FPS) with an oscilloscope and a clever hack.The laser is actually pulsing at 30KHz, and the oscilloscope capture is synchronized to the laser pulse.So we consider each 30KHz pulse a single event in a single pixel (even though the mirror is rotating continuously). So he runs the experiment 30,000 times per second, each one recording a single pixel at 2B FPS for a few microseconds. Each pixel-sized video is then tiled into a cohesive imagereply",
      "The triggering scheme is completely brilliant. One of those cases where not knowing too much made it possible, because someone who does analog debug would never do that (because they would have a 50k$ scope!.reply",
      "Hmm, it's a clever hack, but they would use an oscilloscope with an \"External trigger\" input, like most of the older Rigols. That would let you use the full sample rate without needing to trigger from CH2reply",
      "Does anyone have a $50,000 scope they could just give to this dude? He seems like he would make great use of it.reply",
      "The original MIT video from 2011: \"Visualizing video at the speed of light \u2014 one trillion frames per second\" https://youtu.be/EtsXgODHMWk (project site: https://web.media.mit.edu/~raskar/trillionfps/)He mentions this as the inspiration in his previous video (https://youtu.be/IaXdSGkh8Ww).reply",
      "As I understand it, this is sort of simulating what it would be like to capture this, by recreating the laser pulse and capturing different phases of it each time, then assembling them; so what is represented in the final composite is not a single pulse of the laser beam.Would an upgraded version of this that was actually capable of capturing the progress of a single laser pulse through the smoke be a way of getting around the one-way speed of light limitation [0]? It seems like if you could measure the pulse's propagation in one direction, and the other (as measured by when it scatters of the smoke at various positions in both directions), this seems like it would get around it?But it's been a while since I read an explanation for why we have the one-way limitation in the first place, so I could be forgetting something.[0] https://en.wikipedia.org/wiki/One-way_speed_of_lightreply",
      "No, as he explains in the video, this is not a stroboscopic technique, the camera _does_ capture at 2 billion fps. But it is only a single pixel! He actually scans the scene horizontally then vertically and sends a pulse then captures pixel by pixel.reply",
      "Think of it more like \"IRL raytracing\", where a ray (the beam) is cast and the result for a single pixel from the point of view is captured, and then it is repeated millions of times.Even if you had a clock and camera for every pixel, the sync is dependent on the path of the signal taken. Even if you sent a signal along every possible route and had a clock for each route for each pixel (a dizzingly large number) it still isn't clear that this would represent a single inertial frame. As I understand it even if you used quantum entanglement for sync, the path of the measurement would still be an issue. I suggest not thinking about this at all, it seems like an effective way to go mad https://arxiv.org/pdf/gr-qc/0202031E: Do not trust my math under any circumstances but I believe the number of signal paths would be something like 10^873,555? That's a disgustingly large number. This would reveal whether the system is in a single inertial frame (consistency around loops), but it does not automatically imply a single inertial frame. It's easy to forget that the earth, galaxy, etc are also still rotating while this happens.reply",
      "No, you cannot escape the conclusion of the limitations on measuring the one-way speed of light.While the video doesn't touch on this explicitly, the discussion of the different path lengths around 25:00 in is about the trigonometric effect of the different distances of the beam from the camera.  Needing to worry about that is the same grappling with the limitation on the one-way speed.reply",
      ">As I understand it, this is sort of simulating what it would be like to capture this, by recreating the laser pulse and capturing different phases of it each time, then assembling them; so what is represented in the final composite is not a single pulse of the laser beam.It is not different phases, but it is a composite! On his second channel he describes the process[0]. Basically, it's a photomultiplier tube (PMT) attached to a precise motion control rig and a 2B sample/second oscilloscope. So he ends up capturing the actual signal from the PMT over that timespan at a resolution of 2B samples/s, and then repeating the experiment for the next pixel over. Then after some DSP and mosaicing, you get the video.>It seems like if you could measure the pulse's propagation in one direction, and the other (as measured by when it scatters of the smoke at various positions in both directions), this seems like it would get around it?The point here isn't to measure the speed of light, and my general response when someone asks \"can I get around physics with this trick\" by answer is no. But I'd be lying if I said I totally understood your question.[0] https://www.youtube.com/watch?v=-KOFbvW2A-oreply"
    ],
    "link": "https://www.youtube.com/watch?v=o4TdHrMi6do",
    "first_paragraph": ""
  },
  {
    "title": "My trick for getting consistent classification from LLMs (verdik.substack.com)",
    "points": 98,
    "submitter": "frenchmajesty",
    "submit_time": "2025-10-13T18:01:07 1760378467",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=45571423",
    "comments": [
      "There is a flaw with the base problem: each tweet only has one label, while a tweet is often about many different things and can't be delinated so cleanly. Here's an alternate approach that both allows for multiple labels and lower marginal costs (albeit higher initial cost) for each tweet classified.1. Curate a large representative subsample of tweets.2. Feed all of them to an LLM in a single call with the prompt along the lines of \"generate N unique labels and their descriptions for the tweets provided\". This bounds the problem space.3. For each tweet, feed them to a LLM along with the prompt \"Here are labels and their corresponding descriptions: classify this tweet with up to X of those labels\". This creates a synthetic dataset for training.4. Encode each tweet as a vector as normal.5. Then train a bespoke small model (e.g. a MLP) using tweet embeddings as input to create a multilabel classification model, where the model predicts the probability for each label that it is the correct one.The small MLP will be super fast and cost effectively nothing above what it takes to create the embedding. It saves time/cost from performing a vector search or even maintaining a live vector database.reply",
      "I just built a logistic regression classifier for emails and agreeJust using embeddings you can get really good classifiers for very cheapYou can use small embeddings models too, and can engineer different features to be embedded as wellAdditionally, with email at least, depending on the categories you need, you only need about 50-100 examples for 95-100% accuracyAnd if you build a simple CLI tool to fetch/label emails, it\u2019s pretty easy/fast to get the datareply",
      "I am doing a similar thing for technical documentation, basically i want to recommend some docs at the end of each document.\nI wanted to use the same approach you outlined to generate labels for each document and thus easily find some \u201cfurther reading\u201d to recommend for each.How big should my sample size be to be representative ? It\u2019s a fairly large list of docs across several products and deployment options. I wanted to pick a number of docs per product. \nMaybe I\u2019ll skip the steps 4/5 as I only need to repeat it occasionally once I labelled everything oncereply",
      "Under-discussed superpower of LLMs is open-set labeling, which I sort of consider to be inverse classification. Instead of using a static set of pre-determined labels, you're using the LLM to find the semantic clusters within a corpus of unstructured data. It feels like \"data mining\" in the truest sense.reply",
      "OP here. This is exactly right! You perfectly encapsulated the idea I stumbled up so beautifully.reply",
      "Dunno if this passes the bootstrapping test.This is sensitive to the initial candidate set of labels that the LLM generates.Meaning if you ran this a few times over the same corpus, you\u2019ll probably get different performance depending upon the order of the way you input the data and the classification tag the LLM ultimately decided upon.Here\u2019s an idea that is order invariant: embed first, take samples from clusters, and ask the LLM to label the 5 or so samples you\u2019ve taken. The clusters are serving as soft candidate labels and the LLM turns them into actual interpretable explicit labels.reply",
      "Am I understanding it right that for each new text (tweet) you generate its embedding first, try to match across existing vector embeddings for all other text (full text or bag of words), and then send the text to the LLM for tag classification only if no match is found or otherwise classify it to the same tag for which a match was found.Will it be any better if you sent a list of existing tags with each new text to the LLM, and asked it to classify to one of them or generate a new tag? Possibly even skipping embeddings and vector search altogether.reply",
      "This is very similar to how I've approached classifying RSS articles by topic on my personal project[1].  However to generate the embedding vector for each topic, I take the average vector of the top N articles tagged with that topic when sorted by similarity to the topic vector itself.  Since I only consider topics created in the last few months, it helps adjust topics to account for semantic changes over time.  It also helps with flagging topics that are \"too similar\" and merging them when clusters sufficiently overlap.There's certainly more tweaking that needs to be done but I've been pretty happy with the results so far.1: jesterengine.comreply",
      "Nice!So the cache check tries to find if a previously existing text embedding has >0.8 match with the current text.If you get a cache hit here, iiuc, you return that matched' text label right away. But do you also insert a text embedding of the current text in the text embeddings table? Or do you only insert it in case of cache miss?From reading the GitHub readme it seems you only \"store text embedding for future lookups\" in the case of cache miss. This is by design to keep the text embedding table not too big?reply",
      "Op here. Yes that's right. We do also insert the current text embedding on misses to expand the boundaries of the cluster.For instance: I love McDonalds (1). I love burgers. (0.99) I love cheeseburgers with ketchup (?).This is a bad example but in this case the last text could end up right at the boundary of the similarity to that 1st label if we did not store the 2nd, which could cause a cluster miss we don't want.We only store the text on cache misses, though you could do both. I had not considered that idea but it make sense. I'm not very concerned about the dataset size because vector storage is generally cheap (~ $2/mo for 1M vectors) and the savings in $$$ not spend generating tokens covers for that expense generously.reply"
    ],
    "link": "https://verdik.substack.com/p/how-to-get-consistent-classification",
    "first_paragraph": ""
  },
  {
    "title": "Production RAG: what I learned from processing 5M+ documents (abdellatif.io)",
    "points": 294,
    "submitter": "tifa2up",
    "submit_time": "2025-10-20T15:55:36 1760975736",
    "num_comments": 81,
    "comments_url": "https://news.ycombinator.com/item?id=45645349",
    "comments": [
      "Does anyone know how to do versioning for embeddings? Let\u2019s say I want to update/upsert my data and deliver v6 of domain data instead of v1 or filter for data within a specified date range. I am thinking of exploring context prepending to chunks.reply",
      "I must be missing something, this says it can be self-hosted. But the first page of the self-hosting docs say you need accounts with no less than 6 (!) other third-party hosted services.We have very different ideas about the meaning of self-hosted.reply",
      "You can self-host their code. I don't think there is any official definition of \"self hosted\" that this violates.For example - if a \"self hosted\" service supports off-site backups is it self hosted or just well designed?reply",
      "In that case I\u2019m self hosting every web page on the internet because I installed Firefox.reply",
      "That was my observation as well. To be fair their business is to sell a hosted version, they\u2019re under no obligation to release a truly self hosted version.reply",
      "I\u2019ve never worked in such a space where the deployed environment had unfettered internet access, no access at all actually.I\u2019ve probably missed a huge wave of programming technology because of this, and I\u2019ve figured out a way to make it work for a consistent paycheck over these past 20 years.I\u2019m also not a great example, I think I\u2019ve watched 7 whole hours of YouTube videos ever, and those were all for car repair help.I shy away from tech that needs to be online/connected/whatever.reply",
      "I consider this to be good open source and I'm a happy user of their OSS offering. Want no hosted dependencies? Then go write it all in Rust.reply",
      "that's a stupid take and shows lack of engineering experiencereply",
      "The point about synthetic query generation is good. We found users had very poor queries, so we initially had the LLM generate synthetic queries. But then we found that the results could vary widely based on the specific synthetic query it generated, so we had it create three variants (all in one LLM call, so that you can prompt it to generate a wide variety, instead of getting three very similar ones back), do parallel search, and then use reciprocal rank fusion to combine the list into a set of broadly strong performers. For the searches we use hybrid dense + sparse bm25, since dense doesn't work well for technical words.This, combined with a subsequent reranker, basically eliminated any of our issues on search.reply",
      "Boy, that should not be the concern of the end user (developer) but those implementing RAG solutions as a service at Amazon, Microsoft, Openai and so on.reply"
    ],
    "link": "https://blog.abdellatif.io/production-rag-processing-5m-documents",
    "first_paragraph": "October 20, 2025 \u2022 3 min readI've spent the last 8 months in the RAG trenches, I want to share what actually worked vs. wasted our time. We built RAG for Usul AI (9M pages) and an unnamed legal AI enterprise (4M pages).We started out with youtube tutorials. First Langchain \u2192 Llamaindex. Got to a working prototype in a couple of days and were optimistic with the progress. We run tests on subset of the data (100 documents) and the results looked great. We spent the next few days running the pipeline on the production dataset and got everything working in a week \u2014 incredible.Except it wasn't, the results were subpar and only the end users could tell. We spent the following few months rewriting pieces of the system, one at a time, until the performance was at the level we wanted. Here are things we did ranked by ROI.We put all our learning into an open-source project: agentset-ai/agentset under an MIT license. Feel free to reach out if you have any questions.twitterlinkedincontactmy startu"
  },
  {
    "title": "BERT is just a single text diffusion step (nathan.rs)",
    "points": 344,
    "submitter": "nathan-barry",
    "submit_time": "2025-10-20T14:31:16 1760970676",
    "num_comments": 83,
    "comments_url": "https://news.ycombinator.com/item?id=45644328",
    "comments": [
      "Back when BERT came out, everyone was trying to get it to generate text. These attempts generally didn't work, here's one for reference though: https://arxiv.org/abs/1902.04094This doesn't have an explicit diffusion tie in, but Savinov et al. at DeepMind figured out that doing two steps at training time and randomizing the masking probability is enough to get it to work reasonably well.reply",
      "Im just learning this from your text, after spending last week trying to get a BERT model to talk.https://joecooper.me/blog/crosstalk/I\u2019ve still got a few ideas to try though so I\u2019m not done having fun with it.reply",
      "Interesting as I was in the (very large) camp that never considered it for generation, and saw it as a pure encoder for things like semantic similarity with an easy jump to classification, etcreply",
      "To my knowledge this connection was first noted in 2021 in https://arxiv.org/abs/2107.03006 (page 5). We wanted to do text diffusion where you\u2019d corrupt words to semantically similar words (like \u201cquick brown fox\u201d -> \u201cspeedy black dog\u201d) but kept finding that masking was easier for the model to uncover. Historically this goes back even further to https://arxiv.org/abs/1904.09324, which made a generative MLM without framing it in diffusion math.reply",
      "It goes further back than that. In 2014, Li Yao et al (https://arxiv.org/abs/1409.0585) drew an equivalence between autoregressive (next token prediction, roughly) generative models and generative stochastic networks (denoising autoencoders, the predecessor to difussion models). They argued that the parallel sampling style correctly approximates sequential sampling.In my own work circa 2016 I used this approach in Counterpoint by Convolution (https://arxiv.org/abs/1903.07227), where we in turn argued that despite being an approximation, it leads to better results. Sadly being dressed up as an application paper, we weren't able to draw enough attention to get those sweet diffusion citations.Pretty sure it goes further back than that still.reply",
      "Yeah, that's the first formal reference I remember as well (although, BERT is probably the first thing NLP folks will think of after reading about diffusion).I collected a few other text-diffusion early references here about 3 years ago: https://github.com/madaan/minimal-text-diffusion?tab=readme-....reply",
      "Also relevent - https://arxiv.org/pdf/1902.04094reply",
      "To me, the diffusion-based approach \"feels\" more akin to whats going on in an animal brain than the token-at-a-time approach of the in-vogue LLMs. Speaking for myself, I don't generate words one a time based on previously spoken words; I start by having some fuzzy idea in my head and the challenge is in serializing it into language coherently.reply",
      "> the token-at-a-time approach of the in-vogue LLMs. Speaking for myself, I don't generate words one a time based on previously spoken wordsAutoregressive LLMs don't do that either actually. Sure with one forward pass you only get one token at a time, but looking at what is happening in the latent space there are clear signs of long term planning and reasoning that go beyond just the next token.So I don't think it's necessarily more or less similar to us than diffusion, we do say one word at a time sequentially, even if we have the bigger picture in mind.reply",
      "If a process is necessary for performing a task, (sufficiently-large) neural networks trained on that task will approximate that process. That doesn't mean they're doing it anything resembling efficiently, or that a different architecture / algorithm wouldn't produce a better result.reply"
    ],
    "link": "https://nathan.rs/posts/roberta-diffusion/",
    "first_paragraph": "A while back, Google DeepMind unveiled Gemini Diffusion, an experimental language model that generates text using diffusion. Unlike traditional GPT-style models that generate one word at a time, Gemini Diffusion creates whole blocks of text by refining random noise step-by-step.I read the paper Large Language Diffusion Models and was surprised to find that discrete language diffusion is just a generalization of masked language modeling (MLM), something we\u2019ve been doing since 2018.\nThe first thought I had was, \u201ccan we finetune a BERT-like model to do text generation?\u201d I decided to try a quick proof of concept out of curiosity.NOTE: After I wrote the article I stumbled upon the paper DiffusionBERT which does essentially the same thing but with more rigorous testing! Check it out if this post interested you.The original Transformer architecture, introduced in 2017, was an encoder-decoder model. In 2018, researchers realized that the encoder and decoder components of the model could be sep"
  },
  {
    "title": "Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system (tomshardware.com)",
    "points": 334,
    "submitter": "hd4",
    "submit_time": "2025-10-20T12:31:22 1760963482",
    "num_comments": 229,
    "comments_url": "https://news.ycombinator.com/item?id=45643163",
    "comments": [
      "Alibaba Cloud claims to reduce Nvidia GPU used for serving unpopular models by 82% (emphasis mine)> 17.7 per cent of GPUs allocated to serve only 1.35 per cent of requests in Alibaba Cloud\u2019s marketplace, the researchers foundInstead of 1192 GPUs they now use 213 for serving those requests.reply",
      "I\u2019m slightly confuse as to how all this works. Do the GPUs just sit there with the models on them when the models are not in use?I guess I\u2019d assumed this sort of thing would be allocated dynamically. Of course, there\u2019s a benefit to minimizing the number of times you load a model. But surely if a GPU+model is idle for more than a couple minutes it could be freed?(I\u2019m not an AI guy, though\u2014actually I\u2019m used to asking SLURM for new nodes with every run I do!)reply",
      "Models take a lot of VRAM which is tightly coupled to the GPU so yeah, it's basically sitting there with the model waiting for use. I'm sure they probably do idle out but a few minutes of idle time is a lot of waste--possibly the full 82% mentioned. In this case they optimized by letting the GPUs load multiple models and sharing the load out by token.reply",
      "They definitely won't idle out- if they idle out, it'll take on the order of up to 60 seconds to load the model back into VRAM, depending on the model.That's an eternity for a request. I highly doubt they will timeout any model they serve.reply",
      "How does this work with anything but trivially small context sizes!?reply",
      "Tensor parallelism, so you only need to store a fraction of kv cache per gpu.reply",
      "> I guess I\u2019d assumed this sort of thing would be allocated dynamicallyAt the scale of a hyperscaler I think Alibaba is the one that would be doing that. AWS, Azure and I assume Alibaba do lease/rent data centers, but someone has to own the servers / GPU racks. I know there are specialized companies like nscale (and more further down the chain) in the mix, but I always assumed they only lease out fixed capacity.reply",
      ">Do the GPUs just sit there with the models on them when the models are not in useI've assumed that as well.  It makes sense to me since loading up a model locally takes a while. I wonder if there is some sort of better way I'm not in the know about.  That or too GPU poor to know about.reply",
      "The paper is about techniques to do that dynamic allocation to maximize utilization without incurring unacceptable latencies. If you let a GPU sit idle for several minutes after serving a single request, you're setting money on fire. So they reuse it for a different model as soon as possible, starting even before the first request is finished, because: If you don't have a dedicated GPU for a model, are you going to wait for a multi-gigabyte transfer before each request? So they have a dedicated GPU (or two, one for prefill, one for decode) for a group of models that are processed in an interleaved fashion, scheduled such that they stay within the latency budget.reply",
      "the models are huge, so not a single (latest gen) one can fit on a single GPU.It's likely that these are small unpopular (non flagship) models, or that they only pack eg one layer of each model.reply"
    ],
    "link": "https://www.tomshardware.com/tech-industry/semiconductors/alibaba-says-new-pooling-system-cut-nvidia-gpu-use-by-82-percent",
    "first_paragraph": "A paper presented at SOSP 2025 details how token-level scheduling helped one GPU serve multiple LLMs, reducing demand from 1,192 to 213 H20s.\nWhen you purchase through links on our site, we may earn an affiliate commission. Here\u2019s how it works.\nAlibaba Cloud claims its new Aegaeon pooling system reduces the number of Nvidia GPUs required to serve large language models by 82% during a multi-month beta test inside its Model Studio marketplace. The result, published in a peer-reviewed paper presented at the 2025 ACM Symposium on Operating Systems (SOSP) in Seoul, suggests that cloud providers may be able to extract significantly more inference capacity from existing silicon, especially in constrained markets like China, where the supply of Nvidia's latest H20s remains limited.Unlike training-time breakthroughs that chase model quality or speed, Aegaeon is an inference-time scheduler designed to maximize GPU utilization across many models with bursty or unpredictable demand. Instead of pin"
  },
  {
    "title": "Postman which I thought worked locally on my computer, is down (postman.com)",
    "points": 199,
    "submitter": "helloguillecl",
    "submit_time": "2025-10-20T15:40:40 1760974840",
    "num_comments": 87,
    "comments_url": "https://news.ycombinator.com/item?id=45645172",
    "comments": [
      "I remember when one of the \"Core Goals\" of Postman was \"Complete control over your data - Keep simple JSON based data formats which can be exported and shared as per user needs\".https://web.archive.org/web/20140604204111/http://www.getpos...reply",
      "This is exactly why I made Yaak [1]. It's fully offline, no telemetry, open source, and can even sync with Git.https://yaak.appreply",
      "Curious to know more about the commercial licensing scheme for Yaak: if i\u2019ve read correctly, purchasing a pro license if based on \u00ab good faith \u00bb as the features are exactly the same as the MIT licensed Hobby version?Sincere question, been studying lots of OSS commercial licensing and always wonder what works in which contextreply",
      "This is a conscious bet I'm making.Yes, it's a good-faith license. The license doesn't even apply to the OSS version (only prebuilt binaries).The bet is that super fans will pay for it in the early days and, as it gets adopted by larger companies, they will pay in order to comply with the legalities of commercial use. So far, it's working! The largest company so far is 34 seats, with a couple more in the pipe!reply",
      "Having often thought this is how I would attempt to monetize if I built a developer tool, I'm glad to hear that it's working.It makes good sense because companies actually have an absurd amount of liability to you if they violate your agreement.reply",
      "Without telemetry, how will you know that anyone at all is using your software let alone only within the agreement of any licensing terms?reply",
      "You don't - ergo good faith.You can be an Oracle and audit your customers and develop that adversarial relationship. The idea is that that sort of thing makes you rot in the long run.reply",
      "How's that been going for Oracle so far?reply",
      "Excellent work! Looking forward your post about some milestone ARR boundary, the gory details of how you got there.reply",
      "My runway reaches infinity around $10k MRR so I'll likely do a post around then. Currently 11% of the way there!reply"
    ],
    "link": "https://status.postman.com",
    "first_paragraph": "Resend OTP in:  seconds \n                    Didn't receive the OTP?\n                    Resend OTP \nResend OTP in: 30 seconds \n                      Didn't receive the OTP?\n                      Resend OTP \n\n          Subscribe to updates for Users may encounter issues with accessing or using Postman via email and/or text message. You'll receive email notifications when incidents are updated, and text message notifications whenever Postman creates or resolves an incident.\n        No incidents or maintenance related to this downtime.Unresolved incident: Users may encounter issues with accessing or using Postman.No incidents reported.No incidents reported.No incidents reported.No incidents reported.No incidents reported.No incidents reported.No incidents reported.No incidents reported.No incidents reported.No incidents reported.No incidents reported.No incidents reported."
  },
  {
    "title": "Show HN: I created a cross-platform GUI for the JJ VCS (Git compatible) (judojj.com)",
    "points": 64,
    "submitter": "bitpatch",
    "submit_time": "2025-10-20T15:35:19 1760974519",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=45645120",
    "comments": [
      "Thanks for working on this project!It was mentioned on the JJ Discord server that there doesn't seem to be any information available about who you are. Especially since the project seems to be closed-source, perhaps you could share some information about who you are in order to build trust.I hope you understand. I think users may hesitate to download and install the application without knowing anything about its publisher.reply",
      "Totally understandable! I'm a solo dev out of LA who's mostly consulted on various apps and sites over the years, and out of my own personal need I began working on the project earlier this year.The project is indeed closed-source. Personally I've had experiences where I found the project moved forward towards the roadmap much faster - specifically when considering smaller-scale projects that are UI heavy and therefore take in many UI changes in parallel - when I instead implemented feature requests and focused most of my energy on listening to user feedback rather than reviewing code. UI polish can be quite tricky when it comes from many sources :)That said, nothing is set in stone! If at some point there are enough feature requests to where I am the bottleneck, this is something I would revisit.I'd be happy to hop on the Discord as well!reply",
      "I haven't managed to try this because the \"select repo\" does nothing on Ubuntu 24.04. It doesn't seem I can actually perform any action at all.reply",
      "I wish this was open source. Even if you kept comments etc disabled.Or if you shipped the source code alongside the binary. Or just had a zip of the source.reply",
      "Awesome would love to follow this journeyreply",
      "It's difficult to go back to a separate VCS app after using IDE git support (specifically VSCode & the 'git graph' extension which is sadly permanently abandoned).reply",
      "VS Code now has an OKish git graph built-in, in case you didn't know. This feature is a few months old.reply",
      "I'm confused -- what would you be going back from, given your preferred VSCode extension is abandoned?reply"
    ],
    "link": "https://judojj.com",
    "first_paragraph": "(works with Git repos too!)"
  },
  {
    "title": "Why UUIDs won't protect your secrets (alexsci.com)",
    "points": 12,
    "submitter": "8organicbits",
    "submit_time": "2025-10-20T23:16:41 1761002201",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=45650617",
    "comments": [
      "Great piece, but worth mentioning that you generally can't use a presigned URL with CDN endpoints. So great for sensitive content, but if you rly want the thing to be widely and quickly accessible there's more work to be donereply",
      "Why would you use UUIDv7 rather than UUIDv4 though?reply",
      "UUIDv4 is much more scattered (i.e., uniformly distributed), which heavily degrades indexing performance in databases.reply"
    ],
    "link": "https://alexsci.com/blog/uuids-and-idor/",
    "first_paragraph": "This post is part of a collection on UUIDs.Indirect Object Reference (IDOR) occurs when a resource can be accessed directly by its ID even when the user does not have proper authorization to access it.\nIDOR is a common mistake when using a separate service for storing files, such as a publicly readable Amazon S3 bucket.\nThe web application may perform access control checks correctly, but the storage service does not.Here\u2019s vulnerable Django code which allows a user to view their latest billing statement:While Django ensures the user is logged in and only provides them with bills they own, S3 has no concept of Django users, and performs no such authorization checks.A simple attack would start from a known URL and increment the bill ID:The attacker can keep trying bill IDs, potentially accessing the entire collection of bills.What if we changed the Django model to use UUIDs for the primary key instead of an auto-increment?\nThe new URLs will look like: my-bucket.us-east-1.s3.amazonaws.com"
  },
  {
    "title": "Today is when the Amazon brain drain sent AWS down the spout (theregister.com)",
    "points": 313,
    "submitter": "raw_anon_1111",
    "submit_time": "2025-10-20T20:50:03 1760993403",
    "num_comments": 128,
    "comments_url": "https://news.ycombinator.com/item?id=45649178",
    "comments": [
      "Between the engineering staff and the warehouse workers, I wonder how long it will be until they have already fired everyone who ever would have been willing to work there.Even with candidate pools of hundreds of thousands of H1-B engineers and tens of millions of illegal immigrant warehouse workers, there still comes a point where such a big company firing so many people so quickly exhausts all their options.It reminds me of the Robot Chicken Sketch where Imperial Officers aboard the Death Star all pretend to be force choked to death by Darth Vader so they can avoid getting killed by lightsaber, then come back in under different names in different jobs. It's worse though for Amazon: nobody wants to come back.https://www.youtube.com/watch?v=fFihTRIxCkgreply",
      "Seriously.  I don\u2019t know any half way decent engineer that would ever work there twice.reply",
      "I like to think I'm halfway decent at my job, and I wouldn't work there once. During undergrad, my landlord working for AMZN on the opposite end of the country offered me an interview, but it was during final exam week.I asked if I could schedule the interview after my final exams, and his arrogance really showed when not only did he refuse, but then insisted my exams are not don't even register on the same scale of importance as the opportunity to work for Amazon.Somewhat related: a recruiter at Google cold-called me a couple months into my first job out of undergrad back in 2016 and was similarly condescending about \"the chance\" to work for Google compared to everything else. I already had a low opinion of them when they gave my then-girlfriend an introductory O'Reilly book on Java after she failed their interview.I regret being born too late to work somewhere like Bell Labs, SGI, or Sun. I had a ton of graybeard wizard coworkers from these places, and they were all a pleasure to learn from and even better friends. For the first 2 years of my first job, every day of work was like walking into the Shire and talking magic spells with 20 Gandalfs.That job was great until I got put on a team with a guy who was a former middle manager at some IBM-like company and went from being surrounded by people lightyears ahead of me to being surrounded by Dilbert characters. The messed-up part was that it wasn't even punishment. I was rewarded after completing a project with my choice of which team I joined next, and I joined the wrong one. I assumed that joining a new team to utilize this newfangled \"cloud computing\" thing would be trailblazing, and I didn't do any diligence on who I would work with.To this day, I still regret not rejoining the first team I worked for, otherwise I would still be at that company and happy about it. Then again, the boredom and discontent while being on that sucky team is the reason I started investing, and now I can buy a house in cash and fund myself to do whatever I want for at least a decade. Hard to complain about the way things turned out.reply",
      "> I regret being born too late to work somewhere like Bell Labs, SGI, or Sun.I'm not even out of college, and I feel the same way. Especially for Sun, everything they did was so cool. \"The network is the computer\" and all that.reply",
      "I'd note that a huge amount of the work at those companies was hardware (and a lot of theory in the case of Bell Labs)--though there was, of course important software as well, a lot of it related to Unix.Doesn't mean it might not have been a blast but not hacking on software and playing in the open source world as is the case at at least some companies today.reply",
      "Well I still think that software like DTrace, ZFS, NFS, IRIX and Solaris, IrisGL, and the like are cool, even if there was a lot of hardware engineering. I realize that there are disadvantages to it, but the variances in ISAs (MIPS, SPARC, Alpha, etc) seems like it could have posed challenges for software people.I don't know; I'm not young enough to remember.reply",
      "Sun did a lot of great software too and I know a lot of the folks involved. I just think many people look at the innovation through the lens of software (especially open source) hacking which a great deal of it wasn't.When I was in the minicomputer business, it was maybe 50/50 hardware and software (and that mostly assumes you considered software to include low-level things like microcode). And software people weren't mostly paid more than those in hardware--which is to say generally a good middle class professional wage.reply",
      "The coolness peaked before the \u201cthe network is the computer\u201d phase, IMO. Late 80s vs mid 90s.reply",
      "> to being surrounded by Dilbert characters.As a real life Wally I appreciate this comment.reply",
      "Wally is the one Dilbert character I can tolerate in the workplace. He's honest about who he is and what he does. When you know you're in a bloated company run by buffoons, all you can do for your sanity is work to rule and not upset the apple cart.I was Wally for the last 2 1/2 years of that previous job until I started to realize I'm becoming more and more like a Dilbert character myself. Something in my brain just told me it wasn't sustainable, call it fear of God or paranoia, but letting my skills atrophy in a place like that for 20 years didn't seem like it would end well for me.The only problem was that I stayed so long, and it made me hate software engineering so much that I didn't even want to be a software engineer anymore.I put up with it just long enough so I could avoid selling stock and drawing cash out of my portfolio, and now I'm back at square one as a post-bacc student getting my applications in order for MD and PhD programs where I'll most certainly wind up drawing hundreds of thousands out of my portfolio to pay rent and eat dinner for about a decade.It's sad, I really enjoyed systems programming, but it seems like finding interesting systems programming and distributed computing projects that have significant economic value is like squeezing blood out of a stone. Maybe LLMs or future progress in bioinformatics will change that, now that finding ways to shovel a lot of data into and out of GPUs is valuable, but I'm so far into physiology, genetics/proteomics, and cell biology that I'm not sure I would even want to go back.reply"
    ],
    "link": "https://www.theregister.com/2025/10/20/aws_outage_amazon_brain_drain_corey_quinn/",
    "first_paragraph": "column \"It's always DNS\" is a long-standing sysadmin saw, and with good reason: a disproportionate number of outages are at their heart DNS issues. And so today, as AWS is still repairing its downed cloud as this article goes to press, it becomes clear that the culprit is once again DNS. But if you or I know this, AWS certainly does.And so, a quiet suspicion starts to circulate: where have the senior AWS engineers who've been to this dance before gone? And the answer increasingly is that they've left the building \u2014 taking decades of hard-won institutional knowledge about how AWS's systems work at scale right along with them.AWS reports that on October 20, at 12:11 AM PDT, it began investigating \u201cincreased error rates and latencies for multiple AWS services in the US-EAST-1 Region.\u201d About an hour later, at 1:26 AM, the company confirmed \u201csignificant error rates for requests made to the DynamoDB endpoint\u201d in that region. By 2:01 AM, engineers had identified DNS resolution of the DynamoDB"
  },
  {
    "title": "The scariest \"user support\" email I've ever received (devas.life)",
    "points": 153,
    "submitter": "hervic",
    "submit_time": "2025-10-15T08:47:32 1760518052",
    "num_comments": 113,
    "comments_url": "https://news.ycombinator.com/item?id=45589715",
    "comments": [
      "> as ChatGPT confirmed when I asked it to analyze itlol we are so cookedreply",
      "I just pasted the blob in my terminal without the pipe to bash, felt smart, then realized if they had snuck `aaa;some-bad-cmd;balblabla` in there I'd have cooked myself.Not so smart, after all.reply",
      "Better yet - ChatGPT didn't actually decode the blob accurately.It nails the URL, but manages somehow to get the temporary filename completely wrong (the actual filename is /tmp/pjKmMUFEYv8AlfKR, but ChatGPT says /tmp/lRghl71wClxAGs).It's possible the screenshot is from a different payload, but I'm more inclined to believe that ChatGPT just squinted and made up a plausible /tmp/ filename.In this case it doesn't matter what the filename is, but it's not hard to imagine a scenario where it did (e.g. it was a key to unlock the malware, an actually relevant filename, etc.).reply",
      "Very common for these sorts of things to give different payloads to different user agents.reply",
      "just feed the thing to any base64 decoder like cyberchef:https://cyberchef.org/#recipe=From_Base64('A-Za-z0-9%2B/%3D'...Isn't it just basic problem solving skill? We gonna let AI do the thinky bit for us now?reply",
      "Isn't analysing and writing bits of code one of the few things LLMs are actually good at and useful for (as opposed to creative writing or whatever).Before LLMs if someone wasn't familiar with deobfuscation they would have no easy way to analyse the attack string as they were able to do here.reply",
      "> Isn't analysing and writing bits of code one of the few things LLMs are actually good at and useful forAbsolutely not.I just wasted 4 hours trying to debug an issue because a developer decided they would shortcut things and use an LLM to add just one more feature to an existing project. The LLM had changed the code in a non-obvious way to refer to things by ID, but the data source doesn't have IDs in it which broke everything.I had to instrument everything to find where the problem actually was.As soon as I saw it was referring to things that don't exist I realised it was created by an LLM instead of a developer.LLMs can only create convincing looking code. They don't actually understand what they are writing, they are just mimicking what they've seen before.If they did have the capacity to understand, I wouldn't have lost those 4 hours debugging its approximation of code.Now I'm trying to figure out if I should hash each chunk of data into an ID and bolt it onto the data chunk, or if I should just rip out the feature and make it myself.reply",
      "The \"old fashioned\" way was to post on an internet message board or internet chatroom and let someone else decode it.reply",
      "Providing some analysis? sure. Confirming anything? no.reply",
      "LLMs are just as bad at code as \"creative writing or whatever\". It's just that fewer people know how to write/smell code at the same level as prose, so we get drowned out as \"anti-AI\" cynics and the lie continues.reply"
    ],
    "link": "https://www.devas.life/the-scariest-user-support-email-ive-ever-received/",
    "first_paragraph": "Hi, it's Takuya. As your app grows in popularity, you occasionally start to attract attacks aimed directly at you\u2014the developer or site owner. Just the other day, I got one that was honestly terrifying, so I'd like to share it.In short, they\u2019re saying:Weird already \u2014 because my app\u2019s website, https://www.inkdrop.app/, doesn\u2019t even show a cookie consent dialog. I don\u2019t track or serve ads, so there\u2019s no need for that.Still, I replied politely:A bit later, I got this reply (which Gmail had automatically placed in the spam folder):At first glance, it looked perfectly normal. But notice \u2014 they never actually told me which page was causing the issue. Instead, they sent a link claiming to contain a screenshot. It looked like a Google Drive link, but it was actually a Google Sites page. Without thinking, I clicked it. (You should never do this!)It showed a Captcha screen.I clicked it\u2026 and got this:It said something like \u201cverification step\u201d \u2014 telling me to open a terminal, paste a command, and "
  },
  {
    "title": "x86-64 Playground \u2013 An online assembly editor and GDB-like debugger (halb.it)",
    "points": 106,
    "submitter": "modinfo",
    "submit_time": "2025-10-20T17:55:18 1760982918",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=45646958",
    "comments": [
      "Nice to see my playground trending! All of this is essentially made possible by the blink engine by @jart: https://github.com/jart/blink/\nWhich is an x86-64-linux emulator written in a few kb of c code.There is no Backend server, everything runs locally in the browser in a runtime that weights less than a screenshot of the website itself!To implement it I modified the blink emulator to run as a C library, and compiled it into a Typescript + WASM module that exposes an emulator API. Then I built a regular web app on top of it.reply",
      "Neat site!This is a level lower than https://godbolt.org/ (the \"Compiler Explorer\") -- think of that site as turning C into assembly, and this site as watching the machine code actually run on virtual hardware.reply",
      "Is that the same as x86 emulation or there are subtle distinction?reply",
      "Is there a version of this for a simpler ISA like for the 6502 chip, the Gameboy chip, or straight up ARM?reply",
      "Very nice and approachable! Almost makes me want to come back to assembly again ;-)reply",
      "Very cool project! Thank You For Making And Sharing :)reply",
      "Is there GPU assembly (PTX) version available anywhere?reply",
      "got \"wut\" for trying to call popcnt, oh well.reply"
    ],
    "link": "https://x64.halb.it/",
    "first_paragraph": "x86-64 Playground is a web app for experimenting and learning \n          x86-64 assembly.\n\nThe Playground web app provides an online code editor where you\n        can write, compile, and share assembly code for a wide\n        range of popular assemblers such as GNU As, Fasm and Nasm.\n\nUnlike traditional onlide editors, this playground\n          allows you to follow the execution of your program step by step,\n        inspecting memory and registers of the running process from a GDB-like interface.\n\nYou can bring your own programs!\n        Drag and drop into the app any x86-64-Linux static executable to run and \n        debug it in the same sandboxed environment, without having to install anything.\nThe app is for anyone that wants to run amd64 assembly snippets or\n          inspect the inner workings of simple Linux ELF files.It has been designed with the academic world of binary exploitation in mind;\n            The debugger interface offers visualizations similar to the GDB+PwnGDB debu"
  },
  {
    "title": "AWS Multiple Services Down in us-east-1 (amazon.com)",
    "points": 1584,
    "submitter": "kondro",
    "submit_time": "2025-10-20T07:22:28 1760944948",
    "num_comments": 1777,
    "comments_url": "https://news.ycombinator.com/item?id=45640838",
    "comments": [
      "Interesting day. I've been on an incident bridge since 3AM. Our systems have mostly recovered now with a few back office stragglers fighting for compute.The biggest miss on our side is that, although we designed a multi-region capable application, we could not run the failover process because our security org migrated us to Identity Center and only put it in us-east-1, hard locking the entire company out of the AWS control plane. By the time we'd gotten the root credentials out of the vault, things were coming back up.Good reminder that you are only as strong as your weakest link.reply",
      "This reminds me of the time that Google\u2019s Paris data center flooded and caught on fire a few years ago. We weren\u2019t actually hosting compute there, but we were hosting compute in AWS EU datacenter nearby and it just so happened that the dns resolver for our Google services elsewhere happened to be hosted in Paris (or more accurately it routed to Paris first because it was the closest). The temp fix was pretty fun, that was the day I found out that /etc/hosts of deployments can be globally modified in Kubernetes easily AND it was compelling enough to want to do that. Normally you would never want to have an /etc/hosts entry controlling routing in kube like this but this temporary kludge shim was the perfect level of abstraction for the problem at hand.reply",
      "> temporary kludge shim was the perfect level of abstraction for the problem at hand.Thats some nice manager deactivating jargon.reply",
      "Manager deactivating jargon is a great phrase - it\u2019s broadly applicable and also specific.reply",
      "Yeah that sentence betrays my BigCorp experience it\u2019s pulling from the corporate bullshit generator for surereply",
      "+1...hee heereply",
      "I remember Facebook had a similar story when they botched their BGP update and couldn't even access the vault. If you have circular auth, you don't have anything when somebody breaks DNS.reply",
      "Wasn't there an issue where they required physical access to the data center to fix the network, which meant having to tap in with a keycard to get in, which didn't work because the keycard server was down, due to the network being down?reply",
      "Wishful thinking, but I hope an engineer somewhere got to ram a door down to fix a global outage. For the stories.reply",
      "Way back when I worked at eBay, we once had a major outage and needed datacenter access.  The datacenter process normally took about 5 minutes per person to verify identity and employment, and then scan past the biometric scanners.On that day, the VP showed up and told the security staff, \"just open all the doors!\".  So they did.  If you knew where the datacenter was, you could just walk-in in mess with eBay servers.  But since we were still a small ops team, we pretty much knew everyone who was supposed to be there.  So security was basically \"does someone else recognize you?\".reply"
    ],
    "link": "https://health.aws.amazon.com/health/status?ts=20251020",
    "first_paragraph": ""
  },
  {
    "title": "Code from MIT's 1986 SICP video lectures (github.com/felipap)",
    "points": 85,
    "submitter": "felipap",
    "submit_time": "2025-10-17T20:18:54 1760732334",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=45621557",
    "comments": [
      "If you are into SICP, you would probably like a nicely formatted html version of the book:https://sarabander.github.io/sicp/html/index.xhtml#SEC_Conte...And also this:https://eli.thegreenplace.net/tag/sicpreply",
      "The SICP video lectures with Gerald Sussman and Harold Abelson got me into Scheme and from there on Lisp. Although now I'm wondering if this would be better as a 'Show HN' submission.reply",
      "In the first lecture, Abelson says Computer Science is neither a science nor is it really about computers. Considering the current ML paradigm, maybe CS has finally earned its name as a science.reply",
      "quite the oppositereply",
      "i watched the lecture series during the pandemic and commented on many of the youtube videos. in at least one instance, a library function is used on the board that is not compatible with the current function signature in mit scheme.reply",
      "This is such a fun class!reply"
    ],
    "link": "https://github.com/felipap/sicp-code",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Digitized code from MIT's 1986 SICP video lectures.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.MIT lectures on Structure and Interpretation of Computer Programs, as taught in 1986 by Gerald Sussman and Harold Abelson, are available online in youtube and MIT OCW's website. The videos are available in 240p/360p: very poor quality for visualizing the code in the slides. In addition to that, the camera won't fixate on the board for long, making it extremely difficult to follow.This project's intent is to make the lectures' code and content readable (unlike the slide below) and available in digital format.I tried to maintain consistency of notation, comments and indentation throughout the lectures. Much of the original indentation on the slides was maintaine"
  },
  {
    "title": "TernFS \u2013 an exabyte scale, multi-region distributed filesystem (xtxmarkets.com)",
    "points": 91,
    "submitter": "kirlev",
    "submit_time": "2025-10-20T17:36:16 1760981776",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=45646691",
    "comments": [
      "Could anybody with applicable experience tell me how this filesystem compares in the real world to Lustre?If it is decisively better than Lustre, I am happy to make the switch over at my sector in Argonne National Lab where we currently keep about 0.7 PB of image data and eventually intend to hold 3-5 PB once we switch over all 3 of our beamlines to using Dectris X-Ray detectors.Contrary to what the non-computer scientists insist, we only need about 20Gb/s of throughput in either direction, so robustness and simplicity are the only concerns we have.reply",
      "There are several other systems I would recommend before TernFS for your environment. If you're looking at Lustre versus this in particular, Lustre has been through the wringer, and ANL/DOE has plenty of people who understand it enough to run it well and fix it when it breaks.However, you are right. Your bandwidth needs don't really require Lustre.reply",
      "Previously: https://news.ycombinator.com/item?id=45290245reply",
      "Isn't this literally what ZFS is designed for? What is ZFS lacking that this is needed.reply",
      "ZFS is not distributed. So probably closer to ceph or lustre. I have to admit, on my first pass through the page it failed to explain why it was better than ceph.reply",
      "Given all the good work ZFS does locally, it does make you wonder what it would take to extend the concepts of ARC caching and RAID redundancy to a distributed system, one where all the nodes are joined together by RDMA rather than ethernet; one where reliability can be taken for granted (short of a rat chewing cables).It would make for one heck of a FreeBSD development project grant, considering how superb their ZFS and their networking stack are separately.P.S. Glad someone pointed this out tactfully. A lot of people would have pounced on the chance to mock the poor commenter who just didn't know what he didn't know. The culture associated with software development falsely equates being opinionated with being knowledgeable, so hopefully we get a lot more people reducing the stigma of not knowing and reducing the stigma of saying \"I don't know\".reply",
      "This is hobby project I\u2019ve been thinking about for quite a while. It\u2019s way larger than a hobby project, though.I think the key to making it horizontally scalable is to allow each writable dataset to be managed by a single node at a time. Writes would go to blocks reserved for use by a particular node, but at least some of those blocks will be on remote drives via nvmeof or similar. All writes would be treated as sync writes so another node could have lossless takeover via ZIL replay.Read-only datasets (via property or snapshot, including clone origins) could be read directly from any node. Repair of blocks would be handled by a specific node that is responsible for that dataset.A primary node would be responsible for managing association between nodes and datasets, including balancing load and handling failover. It would probably be responsible for metadata changes(datasets, properties, nodes, devs, etc., not posix fs metadata) and the coordination required across nodes.I don\u2019t feel like I have a good handle on how TXG syncs would happen, but I don\u2019t think that is insurmountable.reply",
      "Even if you were build a ZFS mega-machine with an Exabyte of storage with RDMA (the latencies of \"normal\" Ethernet in the datacenters would probably not be good enough), wouldn't you still have the problem that ZFS is fundamentally designed to be managed by and accessed on one machine? All data in and out of it would have to flow through that machine, which would be quite the bottleneck.reply",
      "If your entire system is connected via RDMA networks (rather common in HPC) I would not worry at all about latency. If you are buying NICs and switches that are capable of 100Gb or better, there\u2019s a reasonable chance they support RoCE.reply",
      "Great default license.reply"
    ],
    "link": "https://www.xtxmarkets.com/tech/2025-ternfs/#posix-shaped",
    "first_paragraph": "About XTX\nCareersSeptember 2025XTX is an algorithmic trading firm: it builds statistical models that produce price forecasts for over 50,000 financial instruments worldwide. We use those forecasts to make trades. As XTX's research efforts to build better models ramped up, the demand for resources kept increasing.The firm started out with a couple of desktops and an NFS server, and 10 years later ended up with tens of thousands of high-end GPUs, hundreds of thousands of CPUs, and hundreds of petabytes of storage.As compute grew, storage struggled to keep up. We rapidly outgrew NFS first and existing open-source and commercial filesystems later. After evaluating a variety of third-party solutions, we made the decision to implement our own filesystem, which we called TernFS[1].We have decided to open source our efforts: TernFS is available as free software on our public GitHub. This post motivates TernFS, explains its high-level architecture, and then explores some key implementation deta"
  },
  {
    "title": "How to stop Linux threads cleanly (mazzo.li)",
    "points": 167,
    "submitter": "signa11",
    "submit_time": "2025-10-15T07:28:45 1760513325",
    "num_comments": 64,
    "comments_url": "https://news.ycombinator.com/item?id=45589156",
    "comments": [
      "I'm reminded of Raymond Chen's many many blogs[1][2][3](there are a lot more) on why TerminateThread is a bad idea. Not surprised at all the same is true elsewhere. I will say in my own code this is why I tend to prefer cancellable system calls that are alertable. That way the thread can wake up, check if it needs to die and then GTFO.[1] https://devblogs.microsoft.com/oldnewthing/20150814-00/?p=91...[2] https://devblogs.microsoft.com/oldnewthing/20191101-00/?p=10...[3] https://devblogs.microsoft.com/oldnewthing/20140808-00/?p=29...there are a lot more, I'm not linking them all here.reply",
      "One of my more annoying gotchas on Windows is that despite this advice being very reasonable sounding, the runtime itself (I believe it actually happens in the kernel) essentially calls TerminateThread on all child threads before running global destructors and atexit hooks. Good luck following this advice when the kernel actively fights you when it come time to shutdownreply",
      "This article does a nice job of explaining why pthread cancellation is hopeless.> If we could know that no signal handler is ran between the flag check and the syscall, then we\u2019d be safe.If you're willing to write assembly, you can accomplish this without rseq. I got it working many years ago on a bunch of platforms. [1] It's similar to what they did in this article: define a \"critical region\" between the initial flag check and the actual syscall. If the signal happens here, ensure the instruction pointer gets adjusted in such a way that the syscall is bypassed and EINTR returned immediately. But it doesn't need any special kernel support that's Linux-only and didn't exist at the time, just async signal handlers.(rseq is a very cool facility, btw, just not necessary for this.)[1] Here's the Linux/x86_64 syscall wrapper: https://github.com/scottlamb/sigsafe/blob/master/src/x86_64-... and the signal handler: https://github.com/scottlamb/sigsafe/blob/master/src/x86_64-...reply",
      "> Well, since thread cancellation is implemented using exceptions, and thread cancellation can happen in arbitrary placesNo, thread cancelation cannot happen in arbitrary places. Or doesn't have to.There are two kinds of cancelation: asynchronous and deferred.POSIX provides an API to configure this for a thread, dynamically: pthread_setcanceltype.Furthermore, cancelation can be enabled and disabled also.  int pthread_setcancelstate(int state, int *oldstate); // PTHREAD_CANCEL_ENABLE, PTHREAD_CANCEL_DISABLE\n  int pthread_setcanceltype(int type, int *oldtype);    // PTHREAD_CANCEL_DEFERRED, PTHREAD_CANCEL_ASYNCHRONOUS\n\nNeedless to say, a thread would only turn on asynchronous cancelation over some code where it is safe to do so, where it won't be caught in the middle of allocating resources, or manipulating data structures that will be in a bad state, and such.reply",
      "I talk about the cancelability state and how it can help us shortly after that statement: https://mazzo.li/posts/stopping-linux-threads.html#controlle... . In hindsight I should have made a forward reference to that section when talking about C++. My broad point was that combining C++ exceptions and thread cancellation is fraught with danger and imo best avoided.reply",
      "I regret to be informed that they still haven't figured this out. I was very active in thread on Linux over 20 years ago, working on glibc and whatnot. That was before C++ had threads, needless to say. There was a time when cancelation didn't do C++ unwinding, only the PTHREAD_CLEANUP_PUSH handlers. So of course cancellation and C++ exceptions was a bad cocktail then.reply",
      "Ah, the eternal problem of asynch unwind!  (without-interrupts \n    (acquire-resource)\n    (unwind-protect\n        (with-local-interrupts\n          (do-jobs-might-block-or-whatever))\n      (release-resource)))\n\n... and to cancel:  (interrupt-thread thread (lambda () (abort-thread)))\n\nI think what is really needed is just exception (unwind cleanup) mechanism and a cheap way to mask interrupts. Signal deferral mechanism does exactly that -- so that with(out)-interrupts just simply set a variable and don't need to go through syscall.reply",
      "For interrupting long-running syscalls there is another solution:Install an empty SIGINT signal handler (without SA_RESTART), then run the loop.When the thread should stop:* Set stop flag* Send a SIGINT to the thread, using pthread_kill or tgkill* Syscalls will fail with EINTR* check for EINTR & stop flag , then we know we have to clean up and stopOf course a lot of code will just retry on EINTR, so that requires having control over all the code that does syscalls, which isn't really feasible when using any libraries.EDIT: The post describes exactly this method, and what the problem with it is, I just missed it.reply",
      "This option is described in detail in the blog posts, with its associated problems, see this section: https://mazzo.li/posts/stopping-linux-threads.html#homegrown... .reply",
      "Ah, fair, I missed it when reading the post because the approach seemed more complicated.reply"
    ],
    "link": "https://mazzo.li/posts/stopping-linux-threads.html",
    "first_paragraph": "Let\u2019s say you\u2019re writing a long running multi-threaded application, on Linux. Maybe it\u2019s a database or a server of some sort. Let\u2019s also imagine that you\u2019re not running on some managed runtime (maybe the JVM, Go, or BEAM), but rather managing threads spawned using the clone syscall. Think of threads created in C with pthread_create, or using C++\u2019s std::thread.1Once you get into the business of starting threads, you\u2019re probably also in the business of stopping them. However the former is much easier than the latter. With \u201cstopping\u201d I mean stopping the thread while giving it a chance to run some cleanup operations before fully terminating. Or in other words, we want to terminate a thread while ensuring that memory is freed, locks are released, logs are flushed, and so on.2This task is sadly not as straightforward as it should be, and there definitely isn\u2019t a one-size-fits-all solution. This blog post aims to give an overview of the problem space and to highlight some pitfalls in an area "
  },
  {
    "title": "Art Must Act (aeon.co)",
    "points": 22,
    "submitter": "tintinnabula",
    "submit_time": "2025-10-17T00:28:01 1760660881",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://aeon.co/essays/harold-rosenberg-exhorted-artists-to-take-action-and-resist-cliche",
    "first_paragraph": "by Blake Smith\u00a0+ BIODetail from Onement I (1948, full image below) by Barnett Newman. Courtesy MOMA. Gift of Annalee Newman \u00a9 The Barnett Newman Foundation. Artists Rights Society (ARS), New York/Copyright Agency, 2025is a historian and translator. He lives in Chicago.Edited bySam HaselbyThe public has, rightly, lost faith in politicians, experts and the media. Progress seems impossible in politics or culture. Massive impersonal bureaucracies, the beguilements of the capitalist market, and ideologies propounded by parties, intellectuals and institutions fill us with disorienting clich\u00e9s and false identities. We are unable to discern the truth, communicate it effectively to each other, or find an authentic role through which to connect with others and escape the forces that divert our potential for genuine action into unthinking conformity or delusional grandstanding.So argued, from the crisis of the Second World War until their deaths in the 1970s, two of the most important intellectua"
  },
  {
    "title": "Old Computer Challenge \u2013 Modern Web for the ZX Spectrum (0x00.cl)",
    "points": 5,
    "submitter": "0x00cl",
    "submit_time": "2025-10-20T23:40:25 1761003625",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://0x00.cl/blog/2025/occ-2025/",
    "first_paragraph": "2025/10/19Table of contentsLast year I participated in the OCC challenge and between work and personal life I totally forgot about it, luckily I saw a post online of someone doing the challenge and remembered about it. I didn\u2019t want to miss this year challenge as it was fun learning something new last year, so hopefully its not too late.This years challenge it was a DIY, you could create your own OCC challenge. So I thought it\u2019d be fun to try to recreate a website and navigation for the ZX Spectrum using BASIC and the limited graphics capabilities of the computer.Well, in this case the hardware is the ZX Spectrum, though I must say that I used an emulator (Fuse) to test and run my code.The ZX Spectrum image resolution is 256x192 pixels, so the space is very limited. The colour palette is made up of 8 colors, with a \u201cbrighter\u201d variation.With this in mind I had to design a website for the ZX Spectrum and how you\u2019d navigate with it.So given the hardware and limitations I thought that I\u2019d "
  }
]