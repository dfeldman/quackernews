[
  {
    "title": "What Is the Fourier Transform? (quantamagazine.org)",
    "points": 108,
    "submitter": "rbanffy",
    "submit_time": "2025-09-04T22:11:09 1757023869",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=45132810",
    "comments": [
      "Shameless plug: If you are interested in Fourier Transform and signal processing you might enjoy my somewhat artistic 3D visualisation of the fourier transform as well as the fractional fourier transform [1](Fractional fourier transform on the top face of the cube)And for short time fourier transform showing how a filter kernel is shiftes across the signal. [2][1]: https://static.laszlokorte.de/frft-cube/[2]: https://static.laszlokorte.de/time-frequency/reply",
      "If I might also plug \u2018the Atlas of Fourier Transforms\u2019. If your interested in understanding building intuition of symmetry and phase in fourier space, the book illustrates many structures.reply",
      "I love the visualization! Thanks for sharing.How do you compute the fractional FT? My guess is by interpolating the DFT matrix (via matrix logarithm & exponential) -- is that right, or do you use some other method?reply",
      "I am glad you like it!Yes the simplest way to think of it is to exponentiate the dft matrix to an exponent between 0 and 1 (1 being the classic dft).\nBut then the runtime complexity is O(n^2) (vector multiplied with precomputed matrix) or O(n^3) opposed to the O(n log n) of fast fourier transform.\nThere are tricks to do a fast fractional fourier transform by multiplying and convolving with a chirp signal. My implementation is in rust [1] compiled to web assembly, but it is based on the matlab of [2] who gladly answered all my mails asking many questions despite already being retired.[1]: https://github.com/laszlokorte/svelte-rust-fft/tree/master/s...[2]: https://nalag.cs.kuleuven.be/research/software/FRFT/reply",
      "I made a cool rust fft tui a long time ago toohttps://github.com/lquinn2015/FFT-tuireply",
      "If you like Fourier, you're going to love Laplace (or its discrete counterpart, the z transform).This took me down a very fascinating and intricate rabbit hole years ago, and is still one of my favorite hobbies. Application of Fourier, Laplace, and z transforms is (famously) useful in an incredibly wide variety of fields. I mostly use it for signal processing and analog electronics.reply",
      ">one man\u2019s mathematical obsession gave way to a calculation that now underpins much of mathematics and physicsUnderpins much of mathematics, science and engineeringreply",
      "Kind of a tangent but why are there so many articles and videos popularizing the Fourier transform and practically none for the Laplace transform?  The first person to do a well done 3Blue1Brown style video that focuses on intuition and visualization would probably be an overnight sensation (well, among engineers at least).reply",
      "I think the best explanation and understanding I received about Fourier Transform was from studying linear algebra.The Fourier Transform equation essentially maps a signal from the time domain onto orthogonal complex sinusoidal basis functions through projection.And the article does not even mention this. =)reply",
      "Almost every paragraph of this article says the same thing but in different ways.reply"
    ],
    "link": "https://www.quantamagazine.org/what-is-the-fourier-transform-20250903/",
    "first_paragraph": "\nAn editorially independent publication supported by the Simons Foundation.\n\nGet the latest news delivered to your inbox.\nCreate a reading list by clicking the Read Later icon next to the articles you wish to save.Type search term(s) and press enterPopular\n                                    SearchesSeptember 3, 2025The Fourier transform breaks a function down into wavelike building blocks.Sierra Boucher; Samuel Velasco/Quanta MagazineContributing WriterSeptember 3, 2025As we listen to a piece of music, our ears perform a calculation. The high-pitched flutter of the flute, the middle tones of the violin, and the low hum of the double bass fill the air with pressure waves of many different frequencies. When the combined sound wave descends through the ear canal and into the spiral-shaped cochlea, hairs of different lengths resonate to the different pitches, separating the messy signal into buckets of elemental sounds.It took mathematicians until the 19th century to master this same calc"
  },
  {
    "title": "Stripe Launches L1 Blockchain: Tempo (tempo.xyz)",
    "points": 506,
    "submitter": "_nvs",
    "submit_time": "2025-09-04T16:32:24 1757003544",
    "num_comments": 646,
    "comments_url": "https://news.ycombinator.com/item?id=45129085",
    "comments": [
      "There are lots of crypto skeptics on HN (and we ourselves were disappointed with crypto's payments utility for much of the past decade), so it might be interesting to share what changed our mind over the past couple of years: we started to notice a lot of real-world businesses finding utility in stablecoins. For example, Bridge (a stablecoin orchestration platform that Stripe acquired) is used by SpaceX for managing money in long-tail markets. Another big customer, DolarApp, is providing banking services to customers in Latin America. We're currently adding stablecoin functionality to the Stripe dashboard, and the first user is an Argentinian bike importer that finds transacting with their suppliers to be challenging.Importantly, none of these businesses are using crypto because it's crypto or for any speculative benefit. They're performing real-world financial activity, and they've found that crypto (via stablecoins) is easier/faster/better than the status quo ante.reply",
      "It sounds great, but every time I see this argument, I end up going down the rabbit hole of actually studying how stablecoins operate. And every time, I come to the same conclusion: they always rely on trust in an off-chain oracle or custodian. At that point, a shared ledger implemented with traditional databases / protocols would be faster, easier, and more transparent.Bitcoin (and possibly a few others) is one of the few uses of blockchain that actually makes sense. The blockchain serves the currency, and the currency serves the blockchain. The blockchain exists to provide consensus without needing to trust any off-chain entity, but the blockchain relies on computing infrastructure that has real-world costs. The scarcity of Bitcoin (the currency) and arguably-fictitious reward for participation in mining is the incentive for people in the real world to contribute resources required for the blockchain to function.Any real-world value given to Bitcoin is secondary and only a result of the fact that (1) mining infrastructure has a cost, and (2) people who understand the system have realized that, unlike fiat, stablecoins, or 1000 other crypto products, Bitcoin has no reliance on trusted, off-chain entities who could manipulate it.You trust your stablecoin's issuer that they hold enough fiat in reserve to match the coin? You might as well trust your bank, but while you're at it, remind them that they don't have to take days to process a transaction - they could process transactions as fast as (actually faster than) a blockchain. But I imagine most banks would point to regulation as a reason for the delays, and they might be right.So what are stablecoins really trying to do? Circumvent regulation? Implement something the banks just aren't willing to do themselves?reply",
      "> a shared ledger implemented with traditional databases / protocols would be faster, easier, and more transparent.Stablecoin is not a technology. It's an excuse. An excuse to do what banks do while not being regulated like a bank or using the infrastructure banks use. Similar to how Airbnb is not a technology but an excuse to do what hotels do without hotel's license.So it makes no sense to compare it to database, a technology.Will this excuse work? Banking is a heavily regulated field so it's less likely than Airbnb, but it's ultimately up to lawmakers.reply",
      "Large banks like JPMorgan Chase are also looking into launching their own stablecoins, just because it has less regulation than normal banking. In fact Jamie Dimon himself says so. The idea is really simple:  creating stablecoin deposit accounts for customers allows  banks to skip existing customer protections that are normally afforded to traditional deposit accounts.reply",
      "Venmo is essentially a stable coinreply",
      "You are correct but with Venmo or PayPal there\u2019s a middleman charging fees who can lock your funds.  A decentralised PayPal is appealing.reply",
      "PayPal also has their own stablecoin, PyUSD\nhttps://www.paypalobjects.com/devdoc/community/PYUSD-Solana-...reply",
      "What fees?reply",
      "That middleman can be compelled by the government to return your funds. A foreigner who empties your wallet on a decentralized PayPal cannot.reply",
      "Stablecoins will end subject to just as much regulation as a normal bank, maybe even more.JPMorgan Chase, BofA, and their ilk have R&D budgets large enough to have already launched a dozen stablecoins by now. They haven't, not because they can't (on a technical level) but because they don't actually see the value to it (on a business level). They're simply paying lip service to crypto because it pumps up share value, the same way every business was bragging about their AI investments just a few months ago.reply"
    ],
    "link": "https://tempo.xyz",
    "first_paragraph": "// Incubated by Stripe & ParadigmTempo is a purpose-built, layer 1 blockchain for payments, developed in partnership with leading fintechs and Fortune 500s. With support for all major stablecoins, Tempo enables high-throughput, low-cost global transactions for any business use case.Stablecoins enable instant, borderless, programmable transactions, but current blockchain infrastructure isn\u2019t designed for them: existing systems are either fully general or trading-focused. Tempo is a blockchain designed and built for real-world payments.// Multifaceted and borderlessDesigned to meet the needs of modern payment processing, enhancing speed, efficiency, and reliability.Tempo was started by Stripe and Paradigm, with design input from Anthropic, Coupang, Deutsche Bank, DoorDash, Lead Bank, Mercury, Nubank, OpenAI, Revolut, Shopify, Standard Chartered, Visa, and more.If you\u2019re a company with large, real-world economic flows and would like to help shape the future of Tempo, get in touch.// Purpo"
  },
  {
    "title": "LLM Visualization (bbycroft.net)",
    "points": 178,
    "submitter": "gmays",
    "submit_time": "2025-09-04T18:06:05 1757009165",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=45130260",
    "comments": [
      "Related. Others?LLM Visualization - https://news.ycombinator.com/item?id=38505211 - Dec 2023 (131 comments)reply",
      "Wow, this is tremendously intricate and very impressive! What an awesome way to visualize the process.reply",
      "Here is another take on visualizing transformers from Georgia Tech researchers: https://poloclub.github.io/transformer-explainer/The Illustrated Transformer: https://jalammar.github.io/illustrated-transformer/Sebastian Raschka, PhD has a post on the architectures: https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-os...This HN comment has numerous resources: https://news.ycombinator.com/item?id=35712334reply",
      "This is awesome! Would be cool if these LLM visualizations were turned into teaching tools, like showing how attention moves during generation or how prompts shift the model\u2019s output. Feels like that kind of interactive view could really help people get what\u2019s going on under the hood.reply",
      "I always liked this visualization from a while ago https://alphacode.deepmind.com/\n(Press play, zoom all the way out and scroll down if on mobile)reply"
    ],
    "link": "https://bbycroft.net/llm",
    "first_paragraph": ""
  },
  {
    "title": "What If OpenDocument Used SQLite? (sqlite.org)",
    "points": 68,
    "submitter": "whatisabcdefgh",
    "submit_time": "2025-09-04T21:36:50 1757021810",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=45132498",
    "comments": [
      "If you're going to use SQLite as an application file format, you should:1. Enable the secure_delete pragma <https://antonz.org/sqlite-secure-delete/> so that when your user deletes something, the data is actually erased.  Otherwise, when a user shares one of your application's files with someone else, the recipient could recover information that the sender thought they had deleted.2. Enable the options described at <https://www.sqlite.org/security.html#untrusted_sqlite_databa...> under \"Untrusted SQLite Database Files\" to make it safer to open files from untrusted sources.  No one wants to get pwned when they open an email attachment.3. Be aware that when it comes to handling security vulnerabilities, the SQLite developers consider this use case to be niche (\"few real-world applications\" open SQLite database files from untrusted sources, they say) and they seem to get annoyed that people run fuzzers against SQLite, even though application file formats should definitely be fuzzed. https://www.sqlite.org/cves.htmlThey fail to mention any of this on their marketing pages about how you should use SQLite as an application file format.reply",
      "One thing I would call out, if you use SQLite as an application format:BLOB type is limited to 2GiB in size (int32). Depending on your use cases, that might seem high, or not.People would argue that if you store that much of binary data in a SQLite database, it is not really appropriate. But, application format usually has this requirement to bundle large binary data in one nice file, rather than many files that you need to copy together to make it work.reply",
      "You can split your data up across multiple blobsreply",
      "Dr. Hipp occasionally gets on a soapbox and extolls the virtue of sqlite databases for use as an application file format. He also preaches about the superiority of Fossil over Git. His arguments generally make sense. I tolerate his sermons because he is one of the truly great software developers of our time, and a personal hero of mine.reply",
      "An interesting skim, but it would have been more meaningful if it had tackled text documents or spreadsheets to show what additional functionality would be enabled with those beyond \"versioning\".Maybe it's just me, but I see the presentation functionality as one of the less used aspects of the OpenOffice family.reply",
      "It seems like it would be relatively straightforward to make an sqlite based file format and just have users add a plugin if for some reason they couldn't upgrade their older version of LibreOffice etc. I agree with the other commenter who mentioned that the benefits for text and spreadsheet files needs more explanation. But it seems like a good enough idea to have a LibreOffice working group perform a more in depth study. If significant memory reduction is real and that would translate to fewer crashes, it would be a huge boost even if it had no other benefits, IMHO.reply",
      "Didn\u2019t Apple actually move to SQLite for their Pages/Numbers format? I remember reading years ago that it was rocky (the transition), but was maybe eventually smoothed out?reply",
      "If I remember correctly Mendix project file format is simply a sqlite db. I thought the designer was lazy but it turns out it's a reasonable decision.Recently, DuckDB team raise similar question on DataLake catalog format. Why not just use SQL database for that ? It's simpler and more efficient as well.reply",
      "I've being trying out SQLite for a side project of mine, a virtual whiteboard, I haven't quite got my head around it, but it seems to be much less of a bother than interacting with file system APIs so far. The problem I haven't really solved is how sync and maybe collaboration is going to interact with it, so far I have:1. Plaintext format (JSON or similar) or SQLite dump files versioned by git2. Some sort of modern local first CRDT thing (Turso, libsql, Electric SQL)3. Server/Client architecture that can also be run locallyHas anyone had any success in this department?reply",
      "SQLite has a builtin session extension that can be used to record and replay groups of changes, with all the necessary handling.  I don't necessarily recommend session as your solution, but it is at least a good idea to see how it compares to others.https://sqlite.org/sessionintro.htmlThat provides a C level API.  If you know Python and want to do some prototyping and exploration then you may find my SQLite wrapper useful as it supports the session extension.  This is the example giving a feel for what it is like to use:https://rogerbinns.github.io/apsw/example-session.htmlreply"
    ],
    "link": "https://www.sqlite.org/affcase1.html",
    "first_paragraph": "Suppose the\nOpenDocument file format,\nand specifically the \"ODP\" OpenDocument Presentation format, were\nbuilt around SQLite.  Benefits would include:\n\nSmaller documents\nFaster File/Save times\nFaster startup times\nLess memory used\nDocument versioning\nA better user experience\n\n\nNote that this is only a thought experiment.\nWe are not suggesting that OpenDocument be changed.\nNor is this article a criticism of the current OpenDocument\ndesign.  The point of this essay is to suggest ways to improve\nfuture file format designs.\n\nAbout OpenDocument And OpenDocument Presentation\n\nThe OpenDocument file format is used for office applications:\nword processors, spreadsheets, and presentations.  It was originally\ndesigned for the OpenOffice suite but has since been incorporated into\nother desktop application suites.  The OpenOffice application has been\nforked and renamed a few times.  This author's primary use for OpenDocument is \nbuilding slide presentations with either \nNeoOffice on Mac, or\nLibreOff"
  },
  {
    "title": "Classic 8\u00d78-pixel B&W Mac patterns (pauladamsmith.com)",
    "points": 94,
    "submitter": "todsacerdoti",
    "submit_time": "2025-09-04T19:53:42 1757015622",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=45131538",
    "comments": [
      "As an aside, you could also have gotten this data from the source code:https://github.com/historicalsource/supermario/blob/master/b...:)reply",
      "Interesting - what's the provenance of this? Is this a leaked copy of the System source, or a disassembly/decompile?reply",
      "It's a copy of the Toolbox ROM code for the Quadra 660AV. It's a real leak, not reverse engineered. You can see comments on the code from the early 80s by the original Macintosh team. It was published online (I think on Applefritter or something) about 20 years ago by a former Apple employee who was having some sort of mental breakdown. He believed it contained proof that Apple was complicit in subjecting its employees to MKULTRA mind control and ran underground maglev trains to other dimensions.reply",
      "Holy crap. I was not aware of that context - that's wild.reply",
      "It's a leaked copy of the System 7.1 source code - just the System itself; it doesn't include any applications, control panels, or extensions. (So code for things like the Finder or QuickTime aren't included.)I'm not certain where it came from precisely; from what I understand, it's been circulating online for ages. Apple hasn't authorized its release, but, as far as I'm aware, they haven't made any attempt to suppress it either.reply",
      "It's a leaked version of the Mac toolbox ROM source code circa very early 1994.reply",
      "Or a screenshot! But what\u2019s the fun in that?reply",
      "Exactly. One of the cool things about doing this the hard way was discovering that Apple still hosts old system and programmers manuals like the one for QuickDraw on its website.reply",
      "That's an incredible hit of nostalgia. I haven't smiled like this in days.It's still hard to believe that some of these effects are accomplished in 8x8 pixels \u2014 in a single integer's worth of space, on modern architectures.reply",
      "Really glad to see I'm not the only one out there who appreciates these patterns.In my LisaGUI project I've added not just the ones from the Lisa Office System and Mac System 1, but also a few I found in betas of the Mac OS, as well as some from Windows 3.x and 9x. These kinds of patterns popped up in all sorts of places in the 80s and 90s. I'm continually surprised at how much you can fit in an 8x8 monochrome grid whenever I come across a clever pattern I haven't seen before.Edit: https://alpha.lisagui.com/\nAt the desktop click the preferences icon and go to the Decorate Desktop panereply"
    ],
    "link": "https://www.pauladamsmith.com/blog/2025/09/classic-mac-patterns.html",
    "first_paragraph": "\nSeptember 03, 2025\n        \nTL;DR: I made a website for the original classic Mac patternsI was working on something and thought it would be fun to use one of the\nclassic Mac black-and-white patterns in the project. I'm talking about the\noriginal 8\u00d78-pixel ones that were in the original Control Panel for setting the\ndesktop background and in MacPaint as fill patterns.Screenshots via to Marcin's awesome interactive\nhistoryI figured there'd must be clean, pixel-perfect GIFs or PNGs of them somewhere\non the web. And perhaps there are, but after poking around a bit, I ran out of\nenergy for that, but by then had a head of steam for extracting the patterns en\nmasse from the original source, somehow. Then I could produce whatever format I\nneeded for them.There are 38 patterns, introduced in the original System 1.0 in the 1984 debut\nof the Macintosh. They were unchanged in later versions, so I decided to get\nthem from a System 6 disk, since that's a little easier with access to utility\nprogram"
  },
  {
    "title": "WiFi signals can measure heart rate (ucsc.edu)",
    "points": 260,
    "submitter": "bookofjoe",
    "submit_time": "2025-09-04T14:53:50 1756997630",
    "num_comments": 147,
    "comments_url": "https://news.ycombinator.com/item?id=45127983",
    "comments": [
      "Hey, I heard about how utility pole inspecting helicopters are able to tell the good/rotten state of wooden telephone poles by the reverb pattern of sound waves coming off the poles from the rotors -- it seems to me the whole field of non-invasive sensing (and using existing/ambient emission sources) is getting pretty impressive.reply",
      "In telecommunications construction we are taught to make ample use of the \"hammer test\" when working on and around poles.  The difference in sound between a good pole, a marginal pole and a completely rotten pole is quite significant.reply",
      "You could of course just bury your lines.reply",
      "Too expensive where I live.  Rocks, hills and trees: the natural enemies of buried fibre and wireless networks.  One of my competitors took 6 months to bury a cable in granite that would've been a 5 day aerial job.reply",
      "Source article (2001!): https://electricenergyonline.com/energy/magazine/4/article/n...I can\u2019t tell if this ever became a reality; I know of more modern approaches attempting to use thermal and multi spectral imaging to achieve the same goal.reply",
      "What kind of helicopter of what size are we talking about here that can actually get close to a utility pole with wires going across?reply",
      "I live near a helicopter factory and when the spinning towers are in use, you hear all sorts of auditory patterns as you move around the town.  When they are test flying - similar and the Police have one and there is an air ambulance too.  My Dad's other staff car in the '80s was a Gazelle and in the '70s he whizzed around in a Sioux.  I've seen and heard a lot of helos!I have absolutely no doubt that with some funky signal processing you can do all sorts of things.reply",
      "HV transmission line inspection routinely has the linesmen crawl out of helicopters onto the lines and back. Granted, as far as I know its the highest skill and most difficult helicopter job.reply",
      "I've seen that in person while in Canada and it is most impressive. The moment they discharge the differential between the helicopter and the line is just awesome. The firebreak clearing operations are also something to behold. From a very safe distance.reply",
      "Look up \"Helicopter tree trimming\" and prepare to be amazed.reply"
    ],
    "link": "https://news.ucsc.edu/2025/09/pulse-fi-wifi-heart-rate/",
    "first_paragraph": "News\n\t\t\t\t\t\tHealth\t\t\t\t\t\n\t\t\t\t\tEngineers prove their technique is effective even with the lowest-cost WiFi devices\t\t\t\t\n\t\t\t\t\tBy Emily Cerf Computer Science and Engineering Ph.D. student Nayan Bhatia demonstrates Pulse-Fi, technology that uses WiFi signals to measure a person's heart rate.\n\n\n\n\n\n\t\t\t\t\t\tPhotos by Erika Cardema/UC Santa Cruz\t\t\t\t\t\n\n\n\n\n\n\t\t\t\t\t\t\t\t\t\tecerf@ucsc.edu\t\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n\n\n\n\t\t\t\t\t\t\t\tAccess Paper\t\t\t\t\t\t\t\nHeart rate is one of the most basic and important indicators of health, providing a snapshot into a person\u2019s physical activity, stress and anxiety, hydration level, and more.Traditionally, measuring heart rate requires some sort of wearable device, whether that be a smart watch or hospital-grade machinery. But new research from engineers at the University of California, Santa Cruz, shows how the signal from a household WiFi device can be used for this crucial health monitoring with state-of-the-art accuracy\u2014without the need for a wearable.Their proof of concept work demonstr"
  },
  {
    "title": "Memory is slow, Disk is fast \u2013 Part 2 (bitflux.ai)",
    "points": 41,
    "submitter": "ghuntley",
    "submit_time": "2025-09-04T22:01:11 1757023271",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=45132710",
    "comments": [
      "Shouldn't you also compare to mmap with huge page option? My understanding is its presicely meant for this circumstance. I don't think its a fair comparison without it.Respectfully, the title feels a little clickbaity to me. Both methods are still ultimately reading out of memory, they are just using different i/o methods.reply",
      "The original blog post title is intentionally clickbaity.  You know, to bait people into clicking.  Also I do want to challenge people to really think here.Seeing if the cached file data can be accessed quickly is the point of the experiment.  I can't get mmap() to open a file with huge pages.void* buffer = mmap(NULL, size_bytes, PROT_READ, (MAP_HUGETLB | MAP_HUGE_1GB), fd, 0); doesn't work.You can can see my code here https://github.com/bitflux-ai/blog_notes.  Any ideas?reply",
      "Shouldn't this be \"io_uring is faster than mmap\"?I guess that would not get much engagement though!That said, cool write up and experiment.reply",
      "Lol.  Thanks.reply",
      "...for sufficiently solid values of \"disk\" ;-)reply",
      "I worked on SSDs for years.  Too many people are suffering from insufficiently solid values of \"disk\" IMHO.reply",
      "Very interesting article, thanks for publishing these tests!Is the manual loop unrolling really necessary to get vectorized machine code? I would have guessed that the highest optimization levels in LLVM would be able to figure it out from the basic code. That's a very uneducated guess, though.Also, curious if you tried using the MAP_POPULATE option with mmap. Could that improve the bandwidth of the naive in-memory solution?> humanity doesn't have the silicon fabs or the power plants to support this for every moron vibe coder out there making an app.lol. I bet if someone took the time to make a high-quality well-documented fast-IO library based on your io_uring solution, it would get use.reply",
      "Nice write-up with good information, but not the best. Comments below.Are you using linux? I assume so since stating use of mmap() and mention using EPYC hardware (which counts out macOS). I suppose you could use any other *nix though.> We'll use a 50GB dataset for most benchmarking here, because when I started this I thought the test system only had 64GB and it stuck.*So the OS will (or could) prefetch the file into memory. OK.> Our expectation is that the second run will be faster because the data is already in memory and as everyone knows, memory is fast.*Indeed.> We're gonna make it very obvious to the compiler that it's safe to use vector instructions which could process our integers up to 8x faster.There are even-wider vector instructions by the way. But, you mention another page down:> NOTE: These are 128-bit vector instructions, but I expected 256-bit. I dug deeper here and found claims that Gen1 EPYC had unoptimized 256-bit instructions. I forced the compiler to use 256-bit instructions and found it was actually slower. Looks like the compiler was smart enough to know that here.Yup, indeed :)Also note that AVX2 and/or AVX512 instructions are notorious for causing thermal throttling on certain (older by now?) CPUs.> Consider how the default mmap() mechanism works, it is a background IO pipeline to transparently fetch the data from disk. When you read the empty buffer from userspace it triggers a fault, the kernel handles the fault by reading the data from the filesystem, which then queues up IO from disk. Unfortunately these legacy mechanisms just aren't set up for serious high performance IO. Note that at 610MB/s it's faster than what a disk SATA can do. On the other hand, it only managed 10% of our disk's potential. Clearly we're going to have to do something else.In the worst case, that's true. But you can also get the kernel to prefetch the data.See several of the flags, but if you're doing sequential reading you can use MAP_POPULATE [0] which tells the OS to start prefetching pages.You also mention 4K page table entries. Page table entries can get to be very expensive in CPU to look up. I had that happen at a previous employer with an 800GB file; most of the CPU was walking page tables. I fixed it by using (MAP_HUGETLB | MAP_HUGE_1GB) [0] which drastically reduces the number of page tables needed to memory map huge files.Importantly: when the OS realizes that you're accessing the same file a lot, it will just keep that file in memory cache. If you're only mapping it with PROT_READ and PROT_SHARED, then it won't even need to duplicate the physical memory to a new page: it can just re-use existing physical memory with a new process-specific page table entry. This often ends up caching the file on first-access.I had done some DNA calculations with fairly trivial 4-bit-wide data, each bit representing one of DNA basepairs (ACGT). The calculation was pure bitwise operations: or, and, shift, etc. When I reached the memory bus throughput limit, I decided I was done optimizing. The system had 1.5TB of RAM, so I'd cache the file just by reading it upon boot. Initially caching the file would take 10-15 minutes, but then the calculations would run across the whole 800GB file in about 30 seconds. There were about 2000-4000 DNA samples to calculate three or four times a day. Before all of this was optimized, the daily inputs would take close to 10-16 hours to run. By the time I was done, the server was mostly idle.[0]: https://www.man7.org/linux/man-pages/man2/mmap.2.htmlreply",
      "int fd = open(filename, O_RDONLY);\nvoid* buffer = mmap(NULL, size_bytes, PROT_READ, (MAP_HUGETLB | MAP_HUGE_1GB), fd, 0);This doesn't work with a file on my ext4 volume. What am I missing?reply",
      "Cool.  Original author here.  AMA.reply"
    ],
    "link": "https://www.bitflux.ai/blog/memory-is-slow-part2/",
    "first_paragraph": "Sourcing data directly from disk IS faster than caching in memory.  I brought receipts.\nBecause hardware got wider but not faster, the old methods don't get you there.  You need new tools to use what is scaling and avoid what isn't.In part 1 I showed how some computer performance factors are scaling exponentially while others have been stagnant for decades.  I then asserted, without proof, that sourcing data from disk can be faster than from memory.  What follows is the proof.Computer Science dogma says that unused memory should be used to cache things from the filesystem because the disk is slow and memory is fast.  Given that disk bandwidth is growing exponentially and memory access latency has stagnated this isn't always true anymore.We need data and something straight forward to do with the data.  I used my free will or the illusion thereof to create a benchmark I cleverly call \"counting 10s\".  I write some pseudo random integers between 0 and 20 to a buffer and then count how many"
  },
  {
    "title": "Atlassian is acquiring The Browser Company (cnbc.com)",
    "points": 385,
    "submitter": "kevinyew",
    "submit_time": "2025-09-04T12:12:31 1756987951",
    "num_comments": 398,
    "comments_url": "https://news.ycombinator.com/item?id=45126358",
    "comments": [
      "The strategic insight behind Arc was perfect \u2013 your browser IS the Operating System, and so we should build a browser that can function as that platform.Arc had pretty good market validation with early adopters, they say that growth was flattened out but IMO that's normal for most products, and it's up to the company to find out WHY growth flattened and then solve that problem. Not kill the product and chase some entirely new idea about AI.I wouldn't be surprised if the investors were fed up with the business and wanted out, pretty good exit all things considered.reply",
      "> The strategic insight behind Arc was perfect \u2013 your browser IS the Operating System, and so we should build a browser that can function as that platform.Marc Andreessen said famously (or at least is paragraphed as saying) in 1994 that the \"Browser is the Operating System\" and people have been doing riffs on that since then.https://www.forbes.com/sites/anthonykosner/2012/04/22/always...https://www.theguardian.com/technology/2014/apr/09/software-...This was also the idea behind Chromebooks:https://en.wikipedia.org/wiki/ChromeOSreply",
      "> Marc Andreessen said famously (or at least is paragraphed as saying) in 1994 that the \"Browser is the Operating System\" and people have been doing riffs on that since then.Isn't that downstream of Sun Microsystems\u2019 old slogan: The Network is the Computer?https://en.wikipedia.org/wiki/The_Network_is_the_Computerreply",
      "AFAICT, Sun\u2019s underlying vision was more on the side of pervasive RPC and/or downloadable code, i.e. closer to DCOM or NeWS than HTTP.(We have in fact ultimately ended up layering downloadable code on top of HTTP. I don\u2019t think I like the results, yet some of the things I don\u2019t like seem inherent to downloadable code in general.)reply",
      "It was more than this. The Sun Ray thin clients were so frigging impressive.The problem wasn't the tech, the problem was it was SUN. It ran on Sun Hardware, with Sun Software and all at Sun Prices.  Metaframe was just so much cheaper (it was also hot garbage but thats another story).reply",
      "In 1994 the browser was not an operating system, was an hyperlink media app. JavaScript was born in 1995 and for years was \u201conly\u201d used for modifying the colors of HTML buttons on a mouse-over.reply",
      "Sometimes there isn't a reason why a product fails to find broad product adoption. If you take VC money, you need a mega hit. Sometimes, all you find is a niche.reply",
      "Which is why tactics are so important. I would say no one has actually got the experience right yet, 'browser is the OS' has been true for a long time, and no one has delivered it yet.Similar to ambient computing and augmented reality.reply",
      "One might suggest Cromebooks have done so well because Google got it more or less right.reply",
      "I think Chromebooks have done so well because they're cheap and are purchased for locked down environments (education and people who really don't want complexity). Even then, I think they kind of demonstrate that the browser is NOT the OS because users and Google still felt the need to break out of the browser box, with both Android apps and Linux application support.reply"
    ],
    "link": "https://www.cnbc.com/2025/09/04/atlassian-the-browser-company-deal.html",
    "first_paragraph": ""
  },
  {
    "title": "ICPC 2025 World Finals Results (icpc.global)",
    "points": 63,
    "submitter": "pykello",
    "submit_time": "2025-09-04T20:33:53 1757018033",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=45131921",
    "comments": [
      "The \"Diamond Sponsors\" of the event are Huawei and OpenAI. I found the welcome note from OpenAI [1] quite curious:\"[...] Eventually, AI will be able to solve even the hardest contest problems that we\u2019ve seen yet. It will work alongside us and help drive the discovery of new knowledge. What you take from this week - the sense of being stuck, the thrill of progress, and the practice of building together - will remain critical as you shape your community and the future you build. [...]\"By Chief Scientist of OpenAI, Jakub Pachocki. Who happens to be an incredibly accomplished Competitive Programmer (2nd in ICPC World Finals, Winner of Code Jam, 2nd in Hacker Cup).[1] https://icpc.global/community/history/brochures/world-finals...reply",
      "That curious statement comes across as inappropriate. These talented students are among the best in the world. Yet, openai chose to make it about and praise themselves. That was not classy.reply",
      "Surely it's meant to reassure the contestants that they're not wasting their time. They'll obviously know AI can do some of this stuff so it's a reminder that the true purpose of the competition isn't to be good at programming.People still compete in playing musical instruments, riding horses, painting pictures, etc. All redundant because of technology but still they do it for other reasons, not the practical utility of the product of their work.reply",
      "Lol and the Huawei note wrote otherwise. AI still hallucinates a lot so pursue mastery over algorithms and data structures to improve the technologyreply",
      "Chinese century incoming.reply",
      "How many chief scientists (aka career climbers) does OpenAI have at this point?reply",
      "I was onsite today watching the contest live, and great atmosphere all around. One surprising outcome: the team in 17th place solved the same number of problems as the team that won gold in 4th place. Hopefully that isn't too demotivating to any team and we can see better separation in the future. After all, it can only mean that the problemsetters underestimated the contestants ;)Congratulations to all the teams!reply",
      "> the problemsetters underestimated the contestantsExcept for problem C, which was only submitted by 4 teams, all unsuccessfully.reply",
      "I don't blame them.  That problem statement seems to be deliberately confusing.reply",
      "Yes, I meant to imply that the problemsetters are to blame.reply"
    ],
    "link": "https://worldfinals.icpc.global/scoreboard/2025/index.html",
    "first_paragraph": ""
  },
  {
    "title": "Le Chat: Custom MCP Connectors, Memories (mistral.ai)",
    "points": 364,
    "submitter": "Anon84",
    "submit_time": "2025-09-04T11:04:36 1756983876",
    "num_comments": 149,
    "comments_url": "https://news.ycombinator.com/item?id=45125859",
    "comments": [
      "I recently upgraded a large portion of my pipeline from gpt-4.1-mini to gpt-5-mini. The performance was horrible - after some research I decided to move everything to mistral-medium-0525.Same price, but dramatically better results, way more reliable, and 10x faster. The only downside is when it does fail, it seems to fail much harder. Where gpt-5-mini would disregard the formatting in the prompt 70% of the time, mistral-medium follows it 99% of the time, but the other 1% of the time inserts random characters (for whatever reason, normally backticks... which then causes it's own formatting issues).Still, very happy with Mistral so far!reply",
      "It is such a common pattern for LLMs to surround generated JSON with ```json \u2026 ``` that I check for this at the application level and fix it. Ten years ago I would do the same sort of sanity checks on formatting when I used LSTMs to generate synthetic data.reply",
      "Some LLM APIs let you give a schema or regex for the answer. I think it works because LLMs give a probability for every possible next token, and you can filter that list by what the schema/regex allows next.reply",
      "Interestingly, that gives a different response distribution from simply regenerating while the output doesn't match the schema.reply",
      "This is true, but there are methods to greatly reduce the effect of this and generate results that match or even improve overall output accuracy:e.g. DOMINO https://arxiv.org/html/2403.06988v1reply",
      "It sounds like they are describing a regex filter being applied to the model's beam search. LLMs generate the most probable words, but they are frequently tracking several candidate phrases at a time and revising their combined probability. It lets them self correct if a high probability word leads to a low probability phrase.I think they are saying that if highest probability phrase fails the regex, the LLM is able to substitute the next most likely candidate.reply",
      "You're actually applying a grammar to the token. If you're outputting, for example, JSON, you know what characters are valid next (because of the grammar), so you just filter out the tokens that don't fit the grammar.reply",
      "Very common struggle, but a great way to prevent that is prefilling the assistant response with \"{\" or as much JSON output as you're going to know ahead of time like '{\"response\": ['reply",
      "Just to be clear for anyone reading this, the optimal way to do this is schema enforced inference. You can only get a parsable response. There are failure modes, but you don't have to mess with parsing at all.reply",
      "Haven\u2019t tried this.  Does it mix well with tool calls? Or does it force a response where you might have expected a tool call?reply"
    ],
    "link": "https://mistral.ai/news/le-chat-mcp-connectors-memories",
    "first_paragraph": "Le Chat now integrates with 20+ enterprise platforms\u2014powered by MCP\u2014and remembers what matters with Memories.The widest enterprise-ready connector directory (beta), with custom extensibility, making it easy to bring workflows into your AI assistant.Directory of 20+ secure connectors\u2014spanning data, productivity, development, automation, commerce, and custom integrations. Search, summarize, and act in tools like Databricks, Snowflake, GitHub, Atlassian, Asana, Outlook, Box, Stripe, Zapier, and more.Custom extensibility: Add your own MCP connectors to broaden coverage and drive more precise actions and insights.Flexible deployment: run on mobile, in your browser, or deploy on-premises or in your cloud.Context that carries: introducing Memories (beta).Highly-personalized responses based on your preferences and facts.Careful and reliable memory handling: saves what matters, slips sensitive or fleeting info.Complete control over what to store, edit, or delete.And\u2026 fast import of your memorie"
  },
  {
    "title": "Coalition for Metabolic Health Launches with $50M (coalitionformetabolichealth.org)",
    "points": 4,
    "submitter": "brandonb",
    "submit_time": "2025-09-05T00:44:11 1757033051",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://coalitionformetabolichealth.org/news/coalition-for-metabolic-health-launches-as-part-of-50-million-investment-to-tackle-americas-health-crisis/",
    "first_paragraph": ""
  },
  {
    "title": "Melvyn Bragg steps down from presenting In Our Time (bbc.co.uk)",
    "points": 266,
    "submitter": "aways",
    "submit_time": "2025-09-04T06:18:13 1756966693",
    "num_comments": 153,
    "comments_url": "https://news.ycombinator.com/item?id=45124143",
    "comments": [
      "In Our Time has been my favorite podcast to listen. It made me appreciate how well moderated a discussion among experts can be and how poorly most moderators on other radio programs or conference panels do their jobs.My complaint with In Our Time is that BBC started inserting the \"this program is  supported by ads outside of the U.K.\" ads in the middle of the discussions. The ads start and end with an extremely annoying loud chime that just blows out the speakers if I have the volume turned up to understand a guest that's speaking in a more soft voice.reply",
      "The thing that makes it work (aside from Melvyn's excellent hosting) is that they have an unspoken but fundamental assumption about the audience, which is that the listeners are intelligent. Like, it's ok to have nuance, to dig deep into topics, it's even ok for listeners not to follow every point precisely. But the listeners are smart people that appreciate hearing from people who know what they're talking about.That's a very rare assumption in modern media, when most mainstream things seemed to be aimed at some sort of lowest common denominator.reply",
      "Imo this works well, especially in the podcast format, because it doesn't feel like they're trying to make every episode for every listener. You dip in on the espisodes that catch your curiosity so if you're listening there's a decent chance you're interested in the topic and are paying attention.reply",
      "> how well moderated a discussion among experts can beI've been listening to the podcast for years. I don't think the format counts as a 'discussion among experts'. It's more like Melvin asks prepared questions individually to each expert--a hub and spoke model. The experts rarely talk to or debate each other, although they often agree which each other. Melvin largely controls the narrative and direction, which I think works better. Guests do get to free-wheel at the end of the podcast, however.reply",
      "He\u2019s really, really good. Whenever I listen to the show I\u2019m amazed at how precise his questions are - he\u2019s perfect in the role of the intelligent non-expert, often asking about the exact thing that was confusing to me in the expert\u2019s previous reply.reply",
      "For what it's worth, the BBC has a premium podcasts subscription for the US and Canada, with no ads.https://www.bbcselect.com/select-more/bbc-podcasts/I believe it's currently Apple-exclusive, but if you enjoy BBC content, it's worth it.reply",
      "VPN to the UK and pull the back catalogue from the BBC using get_iplayer, don't get it from the podcast feedreply",
      "You don't even need VPN, you can just straight download the mp3 file of each episode https://www.bbc.co.uk/programmes/b006qykl/episodes/downloadsOnly the streaming options (iPlayer and Sounds) are geolockedreply",
      "Very nice! I'd also want to run a script to remove the BBC Sounds ident at the startThe audio is so much better when it begins with \"<Melvyn inhales> Hello, 400 years ago in the Sahel region of Africa...\"reply",
      "Those introductions are iconic! There is no delay and no doubt about what is about to happen. I have the full back catalogue downloaded but never considered that I'd need to start trimming.Audacity? Are there automations possible?reply"
    ],
    "link": "https://www.bbc.co.uk/mediacentre/2025/melvyn-bragg-decides-to-step-down-from-presenting-in-our-time/",
    "first_paragraph": "After 26 years on the programme, the legendary presenter bids farewell to the seriesHaving presented well over 1,000 episodes of the much-loved BBC Radio 4 series, Melvyn Bragg has made the decision to step down from In Our Time following the series which aired earlier this year. Melvyn has presented every episode of In Our Time since the series first launched in 1998.In Our Time is regularly one of the BBC\u2019s most listened to on-demand programmes around the world, its appeal spanning generations. It is one of BBC Sounds' most popular podcasts amongst under 35s.Over the last quarter of a century, Melvyn has skilfully led conversations about everything from the age of the Universe to \u2018Zenobia\u2019, Queen of the Palmyrene Empire. He has welcomed the company of the brightest and best academics in their fields, sharing their passion and knowledge with a fascinated audience right around the globe.While he will be much missed on In Our Time, Melvyn will continue to be a friend of Radio 4 with mor"
  },
  {
    "title": "Wikipedia survives while the rest of the internet breaks (theverge.com)",
    "points": 184,
    "submitter": "leotravis10",
    "submit_time": "2025-09-04T15:30:20 1756999820",
    "num_comments": 147,
    "comments_url": "https://news.ycombinator.com/item?id=45128391",
    "comments": [
      "https://archive.is/rXgsz",
      "There has been this trend recently of calling Wikipedia the last good thing on the internet.And i agree its great, i spend an inordinate amount of my time on Wikimedia related things.But i think there is a danger here with all these articles putting Wikipedia too much on a pedestal. It isn't perfect. It isn't perfectly neutral or perfectly reliable. It has flaws.The true best part of Wikipedia is that its a work in progress and people are working to make it a little better everyday. We shouldn't lose sight of the fact we aren't there yet. We'll never be \"there\". But hopefully we'll continue to be a little bit closer every day. And that is what makes Wikipedia great.reply",
      "I would say this is all we really should reasonably expect from our knowledge consensus systems. In fact it\u2019s the same values that \u201cscience\u201d stands on: do our best everyday and continue to try improving.It\u2019s a bit hard for me to imagine something better (in practice). It\u2019s easy to want more or feel like reality doesn\u2019t live up to one\u2019s idealism.But we live here and now in the messiness of the present.Viva la Wikipedia!reply",
      "Indeed, Wikipedia really is worth celebrating. While I sympathize with the GP, we should avoid devolving into purity spirals or we'll never have moments of joy.reply",
      "It\u2019s possible to both criticise Wikipedia and celebrate it.reply",
      "We don't always have to do both at once though. Sometimes we can just enjoy things.reply",
      "> In fact it\u2019s the same values that \u201cscience\u201d stands on: do our best everyday and continue to try improving.Scientists realized there is no \"Truth\", only a series of better and better models approximating it. But philosophers still talk about Truth, they didn't get the message. As long as we are using leaky abstractions - which means all the time - we can't capture Truth. There is no view from nowhere.reply",
      "It's not a coincidence that somebody might insult philosophy as a discipline and then drop some freshman dorm room level epistemology as evidence. If you don't know anything about a topic, it is very easy to dismiss it.reply",
      "Yeah sure, all scientists have the same opinion on that matter, while all philosophers have a different obsolete dogmatic view, both camp are perfectly disjoint, and only the first one is acquired this fundamental truth^W continuously improving model always closer to truth^W something relative to something else and disconnected of any permanent absolute.reply",
      "> Scientists realized there is no \"Truth\", only a series of better and better models approximating it.I don't quite agree with this, unless what you mean is that there's no procedure we can follow which generates knowledge without the possibility of error. This doesn't mean that there's no such thing as truth, or that we can't generate knowledge. It just means that we can never guarantee that our knowledge doesn't contain errors. Another way to put this (for the philosophers among us) is that there is no way to justify a belief (such as a scientific theory) and as such there is no such thing as \"justified true belief.\" But again, this doesn't mean that we cannot generate knowledge about the world.reply"
    ],
    "link": "https://www.theverge.com/cs/features/717322/wikipedia-attacks-neutrality-history-jimmy-wales",
    "first_paragraph": ""
  },
  {
    "title": "Type-safe and user-friendly error handling in Swift 6 (theswiftdev.com)",
    "points": 5,
    "submitter": "TheWiggles",
    "submit_time": "2025-09-01T02:54:19 1756695259",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://theswiftdev.com/2025/type-safe-and-user-friendly-error-handling-in-swift-6/",
    "first_paragraph": "Learn how to implement user-friendly, type-safe error handling in Swift 6 with structured diagnostics and a hierarchical error model.Swift 6 brings an exciting new feature to the language: typed throws. This change makes error handling in Swift much more type-safe, allowing us to define exactly what kinds of errors a function can throw. It\u2019s a small change on the surface, but it opens the door to writing cleaner, more reliable code.Now, you might be wondering \u2014 how do we actually use this in practice? The idea I\u2019m going to share with you came up during a conversation with my wife. She came up with this user-friendly layered error message model, and I turned it into a technique we even ended up using in Toucan, our Swift-based static site generator at Binary Birds.In this post, I\u2019ll show you how this approach works and how you can use it to improve your own Swift projects.First of all, I was never satisfied with the built-in LocalizedError protocol. Sure, its errorDescription property c"
  },
  {
    "title": "Evolving the OCaml Programming Language (2025) (kcsrk.info)",
    "points": 6,
    "submitter": "matt_d",
    "submit_time": "2025-09-05T00:05:50 1757030750",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://kcsrk.info/talks#Evolution_Ashoka_2025",
    "first_paragraph": "KC Sivaramakrishnan\n      \u00a9 2008 - 2025. All rights reserved.\n    Evolving the OCaml Programming Language\n   CS Colloquium, Ashoka University, Sep 2025\n[slides (pdf)] [slides (key)]Automatically Verifying Replicated Data Types\nSchool of Computing, National University of Singapore, Aug 2025\n[slides (pdf)] [slides (key)]Superpowers for the Curious: Using AI tools in your research journey\nIIT Madras, July 2025\n[slides (pdf)] [slides (key)]OCaml\u2019s Parallel Runtime System\nGuest Lecture, Parallel Functional Programming\nChalmers University of Technology, May 2025\n[slides (pdf)] [slides (key)]Automatically Verifying Replicated Data Types\nWG 2.8 2025, May 2025\n[slides (pdf)] [slides (key)]Concurrent and Parallel Programming with OCaml 5\nBloomberg, Mar 2025\n[slides (pdf)] [slides (key)]Concurrent and Parallel Programming with OCaml 5\nIIT Gandhinagar, Mar 2025\n[slides (pdf)] [slides (key)]Concurrent and Parallel Programming with OCaml 5 (part 1)\nPACE Lab Research Huddle, Feb 2025\n[slides (pdf)] ["
  },
  {
    "title": "Age Simulation Suit (age-simulation-suit.com)",
    "points": 130,
    "submitter": "throwup238",
    "submit_time": "2025-09-04T16:41:24 1757004084",
    "num_comments": 90,
    "comments_url": "https://news.ycombinator.com/item?id=45129190",
    "comments": [
      "The site is slow so I can't see it. I'm 68, eat well, lost 20 pounds, work out twice a week. Everything is working fine. But I live in a place surrounded by people in walkers, wheelchairs, or using canes. Some of them have had strokes or accidents making improvement hard, but many simply chose to not do anything to avoid the aging. You don't ordinarily wind up with a walker at a single point; it often starts many years or even decades earlier when you failed to keep in decent physical shape. I almost started too late (last couple of years), I can see how easy it is to not notice your physical being slowly going down. But assuming no major injury or disease, you can improve your body at almost any age, a little at a time, and avoid or at least postpone physical aging for quite a while.I also write code daily, read the same things I read when I worked, thus keep my brain going too. You can't ignore body or mind, you have to keep both in tune.I am still getting older, but I am in better shape than I was before I retired. The last time I felt as fit was when I was still playing basketball 30+ years ago.Don't wait, it's easier to do a little for decades than wait until it's almost too late.reply",
      "My dad is in his mid-70s now, and still swims 3-5 miles a week in the ocean currents and waves. He\u2019s so active and so healthy, I literally have a hard time imagining what he\u2019ll be like when he\u2019s feeble.15 years ago, we went for a hike at elevation and he actually kicked my ass despite being around 35 years older than me. Crazy stuff. That alone was enough to kick my ass into gear. Now I do sprints and lifting, and I actually enjoy it now that my goal is just \u201cdo something for health\u201d rather than \u201creach a half-ton total across my big-3\u201d or something like that.reply",
      ">but many simply chose to not do anything to avoid the agingThank you for saying this. A depressingly large proportion of people are seemingly resigned to the fact that once you hit 40-50, you'll inevitably turn into an achy tub of lard and it's rapidly and irreversibly downhill from there.Barring injuries that are truly irreversible (e.g. severe damage to joints/cartilage), with the correct diet and fitness regime, it's entirely possible to remain lean (\u226420% bodyfat) and muscular (\u226580th percentile in strength standards [0]) well into what most consider \"old age.\" So many people have no idea just how poorly they eat or how inactive and physically weak they are, and consider the result to just be a normal part of life.>I also write code daily, read the same things I read when I worked, thus keep my brain going too. You can't ignore body or mind, you have to keep both in tune.Thanks for saying this too. So much cognitive decline is due to inactivity of the mind. My mom was whip smart until she retired in her mid-60s to a life of idle leisure, and her mental faculties noticeably deteriorated within a few months. Thankfully, she noticed this and deliberately re-engaged with more intense intellectual pursuits (including consulting part-time in the professional field that she loved), and the improvement was night-and-day.[0] https://strengthlevel.com/strength-standardsreply",
      "I've noticed that the difference between 30 and 40 isn't the level of performance I have, but how quickly performance drops when I stop exercising. In my 30s, I could just not go to the gym for months, and I'd be fine. Now, if I don't go for a few weeks, stuff starts aching.reply",
      "This is very true, which is why consistency is so key. I think the reason so many people perceive their health falling off in their 40s is that this is when the cumulative weight of increasing life responsibilities (kids, career advancement, caring for elder relatives, etc.) really hits hard, making it more and more difficult to find time/energy for regular exercise.reply",
      "My grandma is 90 and still lives alone, walks around her neighborhood daily, and swims in a community pool outside her back door. She attributes it (I think rightfully) to a lot of walking and activity throughout her life.Probably some good genes too (her brother is 100, her sister just passed at 104)reply",
      "Yes and: Maintain your balance. Get tested (assessed). Do the exercises. (For anyone who hasn't heard.)My mom and her bf were hard core. Swimming, biking, running, the works.They served as one of the hosts for BBC's program Are You Fitter Than a Pensioner? [2010] https://www.bbc.co.uk/programmes/b00tyr5n My mom was 70 at the time. Spoiler: The seniors smoked the youths.Alas, as with so many: falls -> injury -> idleness -> decline.Some balance stuff can't be helped. Mom's bf got spells of vertigo; apparently the little balancing sensor bone inside the ear gets loose with age.reply",
      "My dad is 85 and this article hits hard about what he fights going on in his body. What sucks is how much of a downward, self reinforcing spiral it all is. It's so hard to see the curbs to walk over or how to get to a thing himself, so he just naturally chooses to do fewer and fewer things. Watching TV is safer and kinder and becomes the default to anything. Which just makes his brain less and less stimulated and active, and you can imagine the drag that adds to keep figuring out life.But like the empathy found in this article, it's caused me to be incredibly more patient with anyone struggling to walk in front of me on a crowded or narrow sidewalk.Aging is rough. Thank you to everyone working on accessibility and aging related tech and science.reply",
      "My grandma is 83 and I could\u2019ve written this exact same post.I know it comes for everyone, but the pace of said spiral is frightening.Wish we were in a timeframe with more alternatives for rapid loss of mobility and muscle.reply",
      "While it is challenging, looked at one a life time scale it is kind of a neat thing. It isn't a purely linear decline and that means while the later years kind of suck, you get a lot of decent time before then.Yes, we should try and work against this but I am just looking at the silver lining.reply"
    ],
    "link": "https://www.age-simulation-suit.com/",
    "first_paragraph": "The age simulation suit GERT offers the opportunity to experience the impairments of older persons even for younger people.\r\n\r\nThe age-related impairments are:\u25a0\u00a0\u00a0opacity of the eye lens\u25a0\u00a0\u00a0narrowing of the visual field\u25a0\u00a0\u00a0high-frequency hearing loss\u25a0\u00a0\u00a0head mobility restrictions\u25a0\u00a0\u00a0joint stiffness\u25a0\u00a0\u00a0loss of strength\u25a0\u00a0\u00a0reduced grip ability\u25a0\u00a0\u00a0reduced coordination skillscomplete as pictured, plus shipping and VAT if applicable\r\nNew: now with 2 pairs of glasses instead of the model shown\n\n\n\n\n\n7 ratings\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u202f\u202f4,9\u202fSend\u202f\u202fus\u202f\u202fyour\u202f\u202fratingDue to the significant increase in the time and effort required to process orders, in particular as a result of incomplete or incorrect information provided with orders, and the fact that we increasingly have to send reminders for invoices for smaller amounts, we can only accept orders with a value of at least 300 euros or pounds.Customer reviews:The quality is great and it works how it is supposed to. I\u0092m happy with my purchase.Great way to teach about elde"
  },
  {
    "title": "Action was the best 8-bit programming language (goto10retro.com)",
    "points": 41,
    "submitter": "rbanffy",
    "submit_time": "2025-09-04T19:25:54 1757013954",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=45131243",
    "comments": [
      "One limitation not mentioned is that Action! didn't support recursion.  This had to do with how local variables were stored.Whether it was the best language for 8-bit programming, it certainly was a great fit for the 6502, as the language targeted the peculiarities of that chip.  Accessing hardware-specific features of the 8-bit Atari's was a snap, which was necessary in order to do anything more interesting than sieves or print loops.Action! probably could've been ported to the Apple line, but 8-bits were winding down by the time it was released.  Porting to 16-bit machines like the IBM PC or Mac (or even the Atari ST) would have been a tougher sell, since Pascal and C were better established by that point, and worked well on those machines.Two bad things about Action!: Charging a license fee to distribute the runtime, and that dumb bang in the name.reply",
      "The OP says that 8-bit CPUs couldn't handle Pascal well, and that Action! (release in 1983) was the first IDE for 8-bit machines.But Apple Pascal was released for the Apple II in 1979.  Based on UCSD Pascal, the Apple Pascal system was basically an OS that simply was an IDE; and it worked perfectly well on 8-bit hardware.  I had quite a lot of fun with it back in the day.reply",
      "Around 1979 or 1980 I was working for an 8080-based CRT terminal manufacturer and ported UCSD Pascal to our 8080 system, which worked flawlessly.  I don't remember the details, but I believe all I had to do was implement a few BIOS-style routines.  I got hung up for a few days because I had inited the heap pointer to a byte boundary instead of a word boundary, but after that everything booted and ran as advertised.reply",
      "I learnt Pascal on a Beeb. It had a compiler, an editor, and a runtime in two 16KB ROMS.reply",
      "Apple Pascal was a UCSD Pascal descendant, which means it was a P-Code interpreter.The article is broadly correct.  The 6502 had what amounts to a mixed-performance address space.  All indirect addressing had to be done via pairs of registers in the zero page at addresses 0-255.  Essentially all the \"pointers\" in your application wanted naturally to live as one of these 128 \"pointer registers\".  But that's not the way natural code generation wants to work, where the pointers get stored in the data memory along with everything else.So compiled languages need to have some kind of trampoline for every pointer to copy it into the memory where it needed to live, which was a really tall order for the optimizers of 1983.Or they could just cheat and compile to a virtualized instruction set and feed that to an interpreter.  Apple chose this, twice: once with Woz's sweet16 in which Integer BASIC was written, and again with the port of the P-Code interpreter for Pascal.reply",
      "If I'm not mistaken Apple Pascal ran a virtual machine which executed \"p-Code\" and the compiler emitted that.Because, yeah, the 6502 is a difficult target for high level languages.reply",
      "The OP says that 8-bit CPUs couldn't handle Pascal wellThe 6502 might not have been able to handle Pascal well, but Borland Turbo Pascal for CP/M (z80, 8080, etc) worked very, very well.  It was also released in 1983 or so, but dunno whether it or Factor was 'first'.reply",
      "I have this theory that Go tickles people because like Basic or something like Action it has all of these sort of abstraction ceilings that lead to \"straight down the middle\" procedural code.Definitely leads to a feeling of velocity. I don't like the language that much but I do get the fun from that feeling!reply",
      "An excellent interview with the creator of Action! - Clinton Parker.https://ataripodcast.libsyn.com/antic-interview-111-clinton-...reply",
      "Another 8-bit \"better than BASIC\" language was COMAL. Similar to the language in the article, it also had structured programming constructs, and the C64 version had built-in turtle graphics, sprite, and sound commands. I remember picking a version up at a mall kiosk that sold PD disks and it expanded my horizons!reply"
    ],
    "link": "https://www.goto10retro.com/p/action-was-the-best-8-bit-programming",
    "first_paragraph": ""
  },
  {
    "title": "Updating restrictions of sales to unsupported regions (anthropic.com)",
    "points": 10,
    "submitter": "yurivish",
    "submit_time": "2025-09-04T23:20:43 1757028043",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=45133344",
    "comments": [
      "And people complain the TSA is \"security theater\"...Yeah, I'm sure whatever restrictions they put in place will make it so hard for Chinese entities to access their APIs.This is simply an attempt by Anthropic to pretend to play nice with the current administration (and to be clear, I don't blame them - let's just not pretend that this actually prevents China getting access to Anthropic).reply",
      "My understanding is Anthropic is closed source and has been unavailable for Chinese companies for a long time, and if they wanted to use offshore entities to use closed-source LLMs, they would have gone to OpenAI...?reply",
      "I have a feeling that China will be OK. Wouldn't these restriction protect the native LLM ecosystem and will give them room and revenues to grow? Strong internal-only competition will still make them competitive.reply",
      "The Chinese who famously don\u2019t have vpns\u2026reply"
    ],
    "link": "https://www.anthropic.com/news/updating-restrictions-of-sales-to-unsupported-regions",
    "first_paragraph": ""
  },
  {
    "title": "I ditched Spotify and set up my own music stack (leshicodes.github.io)",
    "points": 93,
    "submitter": "starkparker",
    "submit_time": "2025-09-04T22:47:45 1757026065",
    "num_comments": 73,
    "comments_url": "https://news.ycombinator.com/item?id=45133109",
    "comments": [
      "So the author talks about how little money per stream artists make... but how much SHOULD they be making? What is fair compensation for writing a song?In the old days, artists would join a label and put out an album. The artist would earn about 10% of sales or so (varies of course, but on average). So a $15 CD would earn an artist $1.50.The article lists the 'price per stream' as about $0.005. So it would take about 300 streams of a song to earn the same amount as selling a CD used to make.I feel like that isn't categorically less money than artists used to make per song listen? There are many albums I own that I have listened to way more than 30 times, which is what it would take for a 10 song album to get 300 song 'streams'Is that a fair compensation? Why or why not?I think artists should be able to earn money from creating music, but I don't know how we decide how much they actually deserve if we aren't just going based on the price the market sets.reply",
      "> how little money per stream artists make ... What is fair compensation for writing a song?Those are two different things. Recording artist does not always equal songwriter. So how much should the songwriter make? The recording studio? The audio engineer? All the other people involved in creating the recorded song? Now that it's made, how do you get people to know the song exists and want to listen to it, much less purchase it?The reason compensation isn't a settled thing is it's a very complex thing to answer.The simplest possible answer is \"the artist sets their own price\" - assuming they just DIY'd the entire production, advertising, distribution, etc themselves. But that is so much work that they would need to already have an income stream to give them the time to do it all, not to mention all the non-music skills if they're not paying professionals to do the rest.If they're not just going to play at the local coffee shop, or bus from city to city barely making enough for gas and beer, they need some way to professionally produce, mass-market, and mass-distribute their songs. It's not feasible for most musicians to do this themselves, so there exists a music industry to do it... which gives them all the cards... letting them set the price, and contract terms... which are often unfair. That's what happens when an industry is given the power to exploit people: they do.reply",
      "> Those are two different things. Recording artist does not always equal songwriter. So how much should the songwriter make? The recording studio? The audio engineer? All the other people involved in creating the recorded song? Now that it's made, how do you get people to know the song exists and want to listen to it, much less purchase it?Why are any of these the distribution medium's (or better, listener's) problem? The songwriter, recording studio, audio engineer, marketing firm, etc should be paid for their services at their standard rates at the time the service is performed. The artist is the one who should accept this risk. Just like.. basically everything else in the world. The plumber who installed an office sink is not entitled to some fraction of the occupying organization's revenue, right?> But that is so much work that they would need to already have an income stream to give them the time to do it allWhich is why labels exist. They take the risk on, and pre-pay for (everything), in exchange for the lion's share of potential revenue. Artists are, of course, welcome to stay unsigned and handle all the risk and rewards themselves, but that typically isn't a good value prop.IMO everything here is working as designed, including Spotify. The author just doesn't understand that \"artists getting paid fractions of pennies per stream\" is exactly what should happen.reply",
      "> should be paid for their services at their standard rates at the time the service is performedBecause by and large they don't want that. They are creatives who would prefer to be invested in their work: Charge less now, putting more into their work in the hope and belief that it will pay off over time. Sometimes it does.reply",
      "Without giving specific numbers, I think the following situation is inherently unfair:I pay Spotify $20. They take their cut (say, 50%) and there's $10 left for the artists. I've only listened to one small artist throughout the entire month. The artist does not get $10 but much less despite Spotify knowing precisely which artists I listened to.reply",
      "They on average pass approximately 70% on, but the record labels also eat heavily into that before the artists get their share.I'm reminded of an effort a few years ago to legislate the creators getting 50% - which of course meant the \"platforms\" and the \"labels\" would collectively share only the other 50%.  Which is presumably why the initiative failed.> The three major labels - Sony, Universal and Warner Music - faced some of the toughest questioning of the inquiry, and were accused of a \"lack of clarity\" by MPs.> They largely argued to maintain the status quo, saying any disruption could damage investment in new music, and resisted the idea that streaming was comparable to radio - where artists receive a 50/50 royalty split.> \"It is a narrow-margin business, so it wouldn't actually take that much to upset the so-called apple cart,\" said Apple Music's Elena Segal.https://www.bbc.com/news/entertainment-arts-57838473These days Spotify has hundreds of millions for Joe Rogan and podcast investments, and Apple reports a 75% profit margin on services, so I guess it is quite profitable for everyone except the actual artists.reply",
      "I\u2019m not sure about this accounting. I know some artists with very successful songs and they made nothing substantial from millions of streamsCould it be that the streaming platform pays 0.005 which then gets divided amongst the whole band, and then the label takes their cut for producing and marketing it?Whereas before, the label was simply giving 10%?reply",
      "I managed a few artists in the past. Usually Spotify paid something like $0.0035 per stream but it ranges based on where the listen took place. One artist owned part of their catalog so earned the 100% on those streams. The rest of their catalog was owned by a major label where they were credited 15% of the streaming take (which was slightly higher than the direct rate) towards their unrecouped major label account.I'd say overall though, streaming can be good for artists. It helps keep them fresh in fans ears (via auto-generated & editorial playlists) and provides a revenue stream for the older stuff that would never be selling in stores or iTunes now.reply",
      "The article says they purchase from bandcamp which takes less than 20%, and support them on patreon.reply",
      "I can't say whether the music industry fairly compensates artists or not. I can say that the film industry, for example, has leveraged each subsequent evolution in distribution technology as on opportunity to shift profits towards distributors and away from those involved in production.reply"
    ],
    "link": "https://leshicodes.github.io/blog/spotify-migration/",
    "first_paragraph": "For years, I relied on Spotify like millions of others. The convenience was undeniable stream anything, anywhere, discover new music through algorithms, and share playlists with friends. But over time, several issues became impossible to ignore: artists getting paid fractions of pennies per stream, fake Artists and ghost Tracks, AI music and impersonation, creepy age verification complicity and the fact that despite paying monthly, I never actually owned anything.\nSo I decided to take back control of my music experience. Here's how I built my own self-hosted music streaming setup that gives me everything Spotify offered and more.There are components of this post which may be improved if you zoom in with your device. The mermaid diagram and code blocks in particular may be hard to read on smaller screens.At the core of my setup is Navidrome, an open-source music server that handles streaming your personal music collection.To access my music from anywhere, I expose Navidrome via a CloudF"
  },
  {
    "title": "We Investigated Tesla's Autopilot. It's Scarier Than You Think [video] (youtube.com)",
    "points": 27,
    "submitter": "mgh2",
    "submit_time": "2025-09-04T23:58:09 1757030289",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=45133607",
    "comments": [
      "A great mystery is how Tesla is avoiding culpability and liability for gross lying and mortal harms caused by the product, and instead being rewarded with fantastic amounts of money!Personal computer appliance users truly blame themselves for the failures of the devices, no matter the degree of wreckage and injury caused by bad design, while cheering the men (unbearable a*holes) who foist malfunctional and dangerous tech upon them.The captains of this industry are notorious for refusing to take responsibility for their mistakes and actively planning to deploy tech they themselves claim is hazardous, while being continually cheered by investors hungry to make a killing.Tesla is a case study for the world about the hazards of California Ideology libertarianism and the precedence of greed over personal responsibility and justice.Since Ronald Reagan, personal responsibility has never been a libertarian (Republican) trait. It's always \"Oops I did it again!\" and \"I forgot!\"No surprise the \"trolley problem\" is the signature thought experiment for the industry as its technocrats constantly hunt for ways to escape responsibility and seek unearned profits.With the woeful performance of Musk's cars and robots, DOGE fiasco, Federal schedule drug habit, goonerism, and inability to maintain personal relationships, Musk's plans for a mars adventure are psychotic.But what fun to watch!reply",
      "> A great mystery is how Tesla is avoiding culpability and liabilityThe civil justice system is slow (and COVID made the whole justice system even slower for quite a while), but aside from that, maybe they're not? A quarter billion dollar verdict for a 2019 crash was recently returned against them.https://www.reuters.com/legal/litigation/tesla-rejected-60-m...reply",
      "Clearly the \"Union\" isnt playing into the \"Hate Elon\" campaign.reply",
      "@5:30 fake demo video like Nikolareply",
      "Yep, it's quite incredible that they released this faked video, quite a lot of people died in various accidents involving Autopilot / FSD where they clearly felt for the marketing, then Tesla guy who made it admitted it was faked and even crashed at some point while filming, and now instead of being in jail, he is ... the head of Tesla Autopilot program.reply",
      "1) Nikola didn't kill anyone2) Tesla's fraud eclipses Nikola's few billion amateur hour.reply",
      "To be fair, it was \u201cstaged\u201d, not completely fake; but yes, the false advertising killed manyreply",
      "Don't get me wrong, I love Tesla design.  I just never understood why anyone would deliberately inject additional possible critical safety faults into their driving experience.I was glad when they started charging for it, 'cuz it just meant fewer dangerous Teslas on the road.I have no doubt we'll get to full autopilot...eventually, and we've \"gotten there\" already with adaptive cruise control, BUT in the interim, if you can't pay full attention while driving you shouldn't be driving.reply"
    ],
    "link": "https://www.youtube.com/watch?v=6ltU9q1pKKM",
    "first_paragraph": ""
  }
]