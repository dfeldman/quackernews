[
  {
    "title": "BirdyChat becomes first European chat app that is interoperable with WhatsApp (birdy.chat)",
    "points": 397,
    "submitter": "joooscha",
    "submit_time": "2026-01-24T19:04:08 1769281448",
    "num_comments": 250,
    "comments_url": "https://news.ycombinator.com/item?id=46746476",
    "comments": [
      "> With the new WhatsApp interface mandated by the DMA, any BirdyChat user in the EEA will be able to start a chat with any WhatsApp user in the region simply by knowing their phone number.Unfortunately, as it's been implemented as opt-in on WhatsApp's side, this isn't really true. Honestly that decision alone means it's kinda dead in the water.reply",
      ">  any WhatsApp user in the regionThe regional limit makes it pretty much useless. The only reason I keep a whatsapp account is to stay in touch with my family in law and a few relatives who live in another continent.reply",
      "In countries where SMS isn't as widespread as it is in the US, the use of WhatsApp is much more common.I live in one of those countries, and I don't think I've ever had to use it to communicate with someone on another continent. I think most of its use is simply local, for your community or friend group.The downside for me is basically the lack of appeal for a non-tech user (like my parents) to voluntarily want to stop using an app they've been using for, what, 10-12 years? It\u2019s not that big of a deal; everyone uses Instagram or Facebook (maybe)... WhatsApp is definitely going to make the process difficult, too.reply",
      "Whatsapp is more popular in the US than you'd think. Probably due to a large immigrant population. I'm in several groups that use the channels feature to organize things like soccer, game nights etc. Most people with family abroad use Whatsapp, and that's a huge portion of the US.reply",
      "SMS isn't widespread in the US, iMessage is.reply",
      "It all depends on age group in my experience. My friends all a bit older than me prefer Messenger for everything. My friends all younger than me prefer Discord. I think my parents and their generation use iMessage, but I use WhatsApp with them. My generation used to use snapchat a lot, I think, but I never got on that boat.reply",
      ">The regional limit makes it pretty much useless.Sounds like an easy fix. Europe just has to convince the rest of the world to ditch the 15 year old popular US apps ingrained in pop culture and with network effects, and have them switch to their own EU made apps, this way we can all communicate together. :hugs: Until then, let's keep chatting on $US_APP so we can debate on how we're gonna achieve that switch.reply",
      "It's not really about that but more that other countries start regulating the same way as WhatsApp and that way not all people would switch to these apps but they would have the opportunity to use it and keep talking with their friends and familyreply",
      "Shouldnt be hard to convince folks. Everyone i know hates facebook / meta and is just waiting for an agreed upon alternative.reply",
      "Everybody says this until there\u2019s an alternative.There have been several alternatives, and people didn\u2019t switch.reply"
    ],
    "link": "https://www.birdy.chat/blog/first-to-interoperate-with-whatsapp",
    "first_paragraph": "BirdyChat BlogGet BirdyChat AppBirdyChatGet BirdyChat AppBlogNovember 14, 2025Today we are excited to share a big milestone. BirdyChat is now the first chat app in Europe that can exchange messages with WhatsApp under the Digital Markets Act. This brings us closer to our mission of giving work conversations a proper home.WhatsApp is currently rolling out interoperability support across Europe. As this rollout continues, the feature will become fully available to both BirdyChat and WhatsApp users in the coming months.Until now, you could only message people who already had a BirdyChat account. If someone was not on the app, they had to download it before you could talk. It slowed down adoption and made it harder to move real work conversations into BirdyChat.With the new WhatsApp interface mandated by the DMA, any BirdyChat user in the EEA will be able to start a chat with any WhatsApp user in the region simply by knowing their phone number. Your contacts keep using WhatsApp. You stay o"
  },
  {
    "title": "We X-Rayed a Suspicious FTDI USB Cable (eclypsium.com)",
    "points": 47,
    "submitter": "aa_is_op",
    "submit_time": "2026-01-24T23:55:10 1769298910",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=46749053",
    "comments": [
      "I have a slow burn project where I simulate a supply chain attack on my own motherboard. You can source (now relatively old) Intel PCH chips off Aliexpress that are \u201cunfused\u201d and lack certain security features like Boot Guard (simplified explanation). I bought one of these chips and I intend to desolder the factory one on my motherboard and replace it with the Aliexpress one. This requires somewhat difficult BGA reflow but I have all the tools to do this.I want to make a persistent implant/malware that survives OS reinstalls. You can also disable Intel (CS)ME and potentially use Coreboot as well, but I don\u2019t want to deal with porting Coreboot to a new platform. I\u2019m more interested in demonstrating how important hardware root of trust is.reply",
      "> persistent implant/malware that survives OS reinstallsTry attacking NIC, server BMC or SSD firmware. You will achieve your goal without any hardware replacement needed.reply",
      "Yeah, but that doesn\u2019t give me a reason to use the hot air station and hot plate collecting dust on my desk ;)reply",
      "> I want to make a persistent implant/malware that survives OS reinstalls.You want to look into something called \"Windows Platform Binary Table\" [1]. Figure out a way to reflash the BIOS or the UEFI firmware for your target device ad-hoc and there you have your implant.[1] https://news.ycombinator.com/item?id=19800807reply",
      "Just to be clear suspicious in this sense is a cable that is likely counterfeit and wasn't able to do high speed transfer unlike the genuine known good one.reply",
      "Yeah - these [0] kinds of cables are so extremely scary.\"The O.MG Cable is a hand made USB cable with an advanced implant hidden inside. It is designed to allow your Red Team to emulate attack scenarios of sophisticated adversaries\"\"Easy WiFi Control\" (!!!!!)\"SOC2 certification\"? Dawg, the call is coming from inside the house...[0] https://shop.hak5.org/products/omg-cablereply",
      "> \"SOC2 certification\"? Dawg, the call is coming from inside the house...Helps corporate red teams in environments where the purchase department is... a bunch of loons.reply",
      "Jeese. I was not sure which image was the suspect one.reply",
      "If you've read the docs, which I'm not saying anyone is expected to, FTDI tends to put buffers on their outputs. That's what gave it away for me. The little sot-23-5 footprints.reply",
      "I wanted to try and figure out out before I did that. No dice.reply"
    ],
    "link": "https://eclypsium.com/blog/xray-counterfeit-usb-cable/",
    "first_paragraph": ""
  },
  {
    "title": "Adoption of EVs tied to real-world reductions in air pollution: study (usc.edu)",
    "points": 21,
    "submitter": "hhs",
    "submit_time": "2026-01-25T00:14:40 1769300080",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://keck.usc.edu/news/adoption-of-electric-vehicles-tied-to-real-world-reductions-in-air-pollution-study-finds/",
    "first_paragraph": ""
  },
  {
    "title": "Postmortem: Our first VLEO satellite mission (with imagery and flight data) (albedo.com)",
    "points": 124,
    "submitter": "topherhaddad",
    "submit_time": "2026-01-24T20:03:39 1769285019",
    "num_comments": 41,
    "comments_url": "https://news.ycombinator.com/item?id=46747119",
    "comments": [
      "Founder/CEO of Albedo here. We published a detailed write-up of our first VLEO satellite mission (Clarity-1) \u2014 including imagery, what worked, what broke, and learnings we're taking forward. Happy to answer questions.https://albedo.com/post/clarity-1-what-worked-and-where-we-g...reply",
      "How did you manage meaningful attitude control with only torque rods? They would need to big (read: heavy) to be useful \u2014 was this just stabilising in inertial frame or active pointing?\n Mag dipoles in chassis and components tend to lock tumbling satellites into the Earth\u2019s magnetic field. Did you see this? Or did you see atmospheric drag dominate at this altitude?reply",
      "I'm AyJay, Topher's co-founder and Albedo's CTO. We'll actually be publishing a paper here in a few weeks detailing how we got 3-axis torque rod control so you can get the real nitty gritty details then.We got here after stacking quite a few capabilities we'd developed on top of one another and realizing we were beginning to see behavior we should be able to wrap up into a viable control strategy.Traditional approaches to torque rod control rely on convergence over long time horizons spanning many orbits, but this artificially restricts the control objectives that can be accomplished. Our momentum control method reduced convergence time by incorporating both current and future magnetic field estimates into a special built Lyapunov-based control law we'd be perfecting for VLEO. By the time the issue popped up, we already had a lot of the ingredients needed and were able to get our algorithms to control within an orbit or two of initialization and then were able to stay coarsely stable for most inertial ECI attitudes albeit with wide pointing error bars as stated in the article. For what we needed though, it was perfect.reply",
      ">The drag coefficient was the headline: 12% better than our design target.Is the drag much better than a regular cubesat? It doesn't look tremendously aerodynamic. From the description I was kind of expecting a design that minimized frontal area.>Additional surface treatments will improve drag coefficient further.Is surface drag that much of a contributor at orbital velocity?reply",
      "Ultimately it's about the ballistic coefficient. You want high mass, low cross-sectional area, and low drag coefficient (Cd). With propulsion for station-keeping, it's challenging to capture the VLEO benefits with a regular cubesat. That said, there are VLEO architectures different than Clarity that make sense for other mission areas.Yes it's a big contributor. The atmosphere in VLEO behaves as free molecular flow instead of a continuous fluid.reply",
      "Cue the ultimate low orbit satellite> It is undesirable to have a definition that will change with improving technology, so one might argue\nthat the correct way to define space is to pick the lowest altitude at which any satellite can remain in orbit,\nand thus the lowest ballistic coefficent possible should be adopted - a ten-meter-diameter solid sphere of\npure osmium, perhaps, which would have B of 8\u00d710^\u22126 m^2/kg and an effective Karman line of z(-4) at the\ntropopausefrom https://arxiv.org/abs/1807.07894reply",
      "The diffraction limit (under  1.22 h* lambda/d) of a 1m optic at 250km in visible light is about 17cm. How can you achieve 10cm resolution?reply",
      "Clarity is designed for a GSD (ground sample distance) of 10 cm. Generally the industry uses resolution<>GSD interchangeably. Agree it's not the true definition of resolution. But I'd argue the diffraction limit is an incomplete metric as well, like how spatial sampling is balanced with other MTF contributors (e.g. jitter/smear). For complete metrics, we like 1) NIIRS or 2) % contrast for a given object size on the ground (i.e. system MTF translated to ground units, not image-space units).The main performance goal for us was NIIRS 7, and we decomposed GSD/MTF/SNR contributors optimized for affordability when we architected the systemreply",
      "Can you tell us some war stories about the software your group  wrote for the satellite?Stacks? Testing? Firmware Updates? Programming languages?Thank you!reply",
      "First - they never want to use someone else software framework again (an early SW architect decided that would accelerate things but we ended up re-writing almost all of it) and it was all C++ on the satellite. We ran linux with preempt_rt.We wrote everything from low level drivers to the top level application and the corresponding ground software for commanding and planning as well. Going forward, we're writing everything top to bottom, just to simplify and have total ownership since we're basically there already.For testing we hit it at multiple levels: unit test, hardware in the loop, a custom \"flight software in test\" we called \"FIT\" which executed a few different simulated mission scenarios, and we tried to hit as many fault cases as we could too. It was pretty stressful for the team tbh but they were super stoked to see how well it worked on orbit.A big one for us in a super high resolution mission like this is the timing determinism (low latency/low jitter) of the guidance, navigation, and control (GNC) thread. Basically it needs execute on time, every cycle, for us to achieve the mission. Getting enough timing instrumentation was tough with the framework we had selected and we eventually got there, but making sure the \"hot loop\" didn't miss deadlines was more a function of working with that framework than any limitation of linux operating well enough in a RTOS fashion for us.reply"
    ],
    "link": "https://albedo.com/post/clarity-1-what-worked-and-where-we-go-next",
    "first_paragraph": "On March 14, 2025, Albedo's first satellite, Clarity-1, launched on SpaceX Transporter-13. We took a big swing with our pathfinder. The mission goals:We proved a ton. We learned a ton.We achieved the first two goals definitively and validated 98% of the technology required for the third. This was an extraordinarily ambitious first satellite. We designed and built a high-performance bus on time and on budget, integrated a large-aperture telescope, and operated in an environment no commercial company had sustained operations in, funded entirely by private capital.This is the full story.Let's start with the result that matters most: VLEO works. And it works better than even we expected.For decades, Very Low Earth Orbit was written off as impractical for normal satellite lifetimes. The atmosphere is thicker, creating drag that would deorbit normal satellites in weeks. If the drag didn't kill you, atomic oxygen would erode your solar arrays and surfaces. To succeed in VLEO required a fundam"
  },
  {
    "title": "Raspberry Pi Drag Race: Pi 1 to Pi 5 \u2013 Performance Comparison (the-diy-life.com)",
    "points": 123,
    "submitter": "verginer",
    "submit_time": "2026-01-24T18:06:00 1769277960",
    "num_comments": 68,
    "comments_url": "https://news.ycombinator.com/item?id=46745922",
    "comments": [
      "About 7 years ago, we deployed a \u201cgateway/orchestration\u201d role device in ag tech. Power draw is a big concern for us (not a lot of free power out in the middle of fields). We used an SBC from Emtrion. I remember asking my EE counterpart at the time \u201cwhy not a Pi? Surely someone makes hardened versions of those?\u201d He was skeptical and I think the aura of \u201ctoy/hobby/maker\u201d scared him off.Fast forward. We\u2019re getting ready to role out our next generation. It\u2019s based on the Pi Compute Module 4 (the CMs are basically just the basic Pi and you put your own carrier board for peripherals under it). It is amazing. It has easily 20x the power, 20x the RAM, better temp specs and such, a great eco system, uses about 30% less power, and about 1/5 of the price. The only thing we\u2019re not sure about yet, is the robustness of the BLE with the onboard radio chip.It\u2019s amazing how far these things have come. For low volume product builds, it\u2019s hard to find a reason not to use one of the CMs.reply",
      "Hah! And the value of the Pi for these kinds of ~industrial applications is why there was a shortage of Pis for hobbyists.It's funny how Raspberry Pi started out for an educational market, and accidentally revolutionized the embedded market.reply",
      "I've got a brute force solver for the NYT Pips game. There's a particular puzzle that it takes 45.2 seconds on on my M2 Max Mac Studio. The solver is single threaded and doesn't use much memory so it is mostly limited by CPU and memory speed.I ran it on my Pi 3, 4, 5, Intel iMac, and on my cheap Amazon Lightsail instance. Here are the results, in seconds:  680.4  RPi 3\n  274.5  RPi 4\n  131.3  RPi 5\n  108.5  Lightsail\n   78.7  2017 iMac (3.4 GHz Intel Core i5)\n   45.2  M2 Max Mac Studioreply",
      "Very cool. I feel like Lightsail performed quite well. Feels like a local optimum of some kind. Mind specifying the tier?reply",
      "It says \"512 MB RAM, 1 vCPU, 20 GV SSD\" and \"General Purpose\". It was whatever was the cheapest when I made it.Looking at the instance creation page it looks like it is no longer available. The cheapest now has 2 vCPU and the same other specs.If anyone would like to run it on something else for comparison I've put it Pastebin [1].Normally, a script converts the puzzle into a header file that describes the puzzle in terms of the data structures the solver uses, and then the solver is compiled with that header included via #include, but for benchmarking I manually included the header, so it is just a single C file that you simply compile and run. Add a -q flag to have it just find and count the solutions but not display them. For this particular puzzle it is fine to let it show the solutions. For some puzzles with a ton of solutions it displaying them can take significant time.If anyone actually wants a Pips solver to solve Pips puzzles, there's a copy of mine along with the scripts that convert puzzles from the JSON format the NYT provides for download to the header file for the solver and some other stuff as a shar archive on Pastebin [2]. Pastebin's download link messes up something with that, so click the raw link and copy/paste from the browser.Note: I'm still doing a lot of experimenting with it, both in the solving algorithm and in how the display the results, and am not at a point where I'm able to accept outside contributions. There are other Pips solvers that have been put on Github (and even posted on HN) that are (1) more sophisticated than mine, and (2) almost certainly more open to contributions. Just use mine if for some reason you want a fairly simple (aside from the damn display stuff...) brute force solver in C and aren't going to expect anything else from me :-)[1] https://pastebin.com/5HmymPXw[2] https://pastebin.com/SfSk3eNBreply",
      "If it's low RAM and single threaded then I would expect every tier to perform identically.(Also I was just looking at the pricing, you don't get any extra CPU cores until the $84 tier!)reply",
      "Thanks for pointing that out. I had thought maybe as the tiers went up you'd get faster CPUs, but upon reflection the pricing schedule seems to confirm your point. I guess they are completely virtualized because there is not mention of CPU type on the pricing page.reply",
      "I think the Pi 3 range is a sweet spot for low cost, low power draw, decent-enough CPU. Newer models draw increasingly more power; going from 1.4W to 2.8W may not seem like much, but that's half your battery life. There's a few differences in Raspberry Pi 3 versions that may lead you to buy one or the other:- The Pi 3B has 10/100 Ethernet, 802.11n (single-band) WiFi, Bluetooth 4.1. Power idled at 1.4W and peaked at 3.7W.- The Pi 3B+ removed the 10/100 Ethernet in favor of USB Ethernet (~300Mbps w/USB2.0). CPU cores were overclocked from 1.2GHz to 1.4GHz (so a heatsink is more necessary), with ~15% increase in benchmark performance. It added 802.11ac (dual band) WiFi and Bluetooth 4.2 w/BLE. Power idled at 1.9W and peaked at 5.1W. This is also the only 3-model supporting PoE (w/ extra HAT).- The Pi 3A+ removed Ethernet and reduced USB to a single port. The RAM was reduced from 1GB to 512MB. Power idled at 1.13W and peaked at 4.1W. The A+ form factor is more compact. Overall the 3A+ is smaller, cheaper, and less power draw than the 3B+ (but not as low as the 3B).The lowest power draw with acceptable performance is the 3B. For slightly more power draw and more CPU performance, go with 3A+. For \"everything\" (including PoE) the 3B+ is it.If you want the 3A+ but don't need the video, want a smaller form factor, and half the power draw, the Pi Zero 2 W is it. Though the Pi Zero 2 W is supposed to be cheapest, due to demand it's often sold out or more expensive. The 3A+ is still cheap (~$25) and available, with the downside of the higher power draw and larger form factor.(disabling HDMI, LEDs, Wifi, Bluetooth, etc reduces power draw more. in testing, the 3A+ drew less power than the Zero 2 W with everything disabled. all of them draw ~0.1W when powered off)reply",
      "> I think the Pi 3 range is a sweet spot for low cost, low power draw, decent-enough CPU. Newer models draw increasingly more power; going from 1.4W to 2.8W may not seem like much, but that's half your battery life.Is that with the same load? The chart in the article shows a Pi 3 and Pi 4 using the same idle power, with the 4 drawing more under full load. But the 4 can do more at full load, raising the question of what would be the 4's power usage running a load equal to the 3's full load?reply",
      "The author's chart is incorrect. He has the idle power at \"1\" for the Pi 2, \"2\" for the Pi 1/3/4, and \"3\" for the Pi 5. No other published power draw numbers are whole numbers like this, they are floating points, like 1.2, 1.9, etc. Google around and you'll find several different power testing comparisons with more detail.Most reports show the Pi 4 drawing ~2.8 W in idle headless mode, and the Pi 3B+ drawing ~1.9-2.0 W in idle headless mode. With full load the Pi 4 draws more (6.4W, to the Pi 3B+'s 5.1W) with the same test procedure.But you do have to check the testing method; enabling/disabling hardware features changes the figure, and each additional USB peripheral draws more power. Otoh, to get a \"max power draw\" reading you have to enable everything and stress all CPUs at once, and then it will dip under thermal load.reply"
    ],
    "link": "https://the-diy-life.com/raspberry-pi-drag-race-pi-1-to-pi-5-performance-comparison/",
    "first_paragraph": "Today we\u2019re going to be taking a look at what almost 13 years of development has done for the Raspberry Pi. I have one of each generation of Pi from the original Pi that was launched in 2012 through to the Pi 5 which was released just over a year ago.We\u2019ll take a look at what has changed between each generation and how their performance and power consumption has improved by running some tests on them.Here\u2019s my video of the testing process and results, read on for the write-up;Some of the above parts are affiliate links. By purchasing products through the above links, you\u2019ll be supporting this channel, at no additional cost to you.This is the original Raspberry Pi, which was launched in February 2012.This Pi has a Broadcom BCM2835 SOC which features a single ARM1176JZF-S core running at 700MHz along with a VideoCore IV GPU. It has 512 MB of DDR RAM.In terms of connectivity, it only has 100Mb networking and 2 x USB 2.0 ports. Video output is 1080P through a full-size HDMI port or analogu"
  },
  {
    "title": "Two Weeks Until Tapeout (essenceia.github.io)",
    "points": 4,
    "submitter": "client4",
    "submit_time": "2026-01-25T01:25:37 1769304337",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://essenceia.github.io/projects/two_weeks_until_tapeout/",
    "first_paragraph": "HFTMore projectsAboutHFTMore projectsAboutAs anyone that hasn\u2019t been living under a rock might have heard, AI accelerators are the coolest kids in town these days. And although I have never been part of the \u201cin\u201d crowd, this time at least, I get the appeal.So when the opportunity arose to join an experimental shuttle using global foundries 180nm for FREE I jumped onto the opportunity and designed my own JTAG!\u2026I\u2019m sorry? Is this not what you were expecting?Frankly, I would love to tell you a great story about how I went into this wanting to design a free and open source silicon proven AI accelerator that the community could freely extend and re-use in their own projects. But in truth this project started out first as me wanting to design some far less sexy, in-silicon debug infrastructure and only later came to include the systolic matrix matrix multiplication accelerator \u2026 to serve as the design under test.No wonder I was never one of the cool kids.Also, I\u2019m designing everything from sc"
  },
  {
    "title": "Claude Code's new hidden feature: Swarms (twitter.com/nicerinperson)",
    "points": 298,
    "submitter": "AffableSpatula",
    "submit_time": "2026-01-24T14:35:47 1769265347",
    "num_comments": 220,
    "comments_url": "https://news.ycombinator.com/item?id=46743908",
    "comments": [
      "Ok it might sound crazy but I actually got the best quality of code (completely ignoring that the cost is likely 10x more) by having a full \u201cproject team\u201d using opencode with multiple sub agents which are all managed by a single Opus instance. I gave them the task to port a legacy Java server to C# .NET 10. 9 agents, 7-stage Kanban with isolated Git Worktrees.Manager (Claude Opus 4.5): Global event loop that wakes up specific agents based on folder (Kanban) state.Product Owner (Claude Opus 4.5): Strategy. Cuts scope creepScrum Master (Opus 4.5): Prioritizes backlog and assigns tickets to technical agents.Architect (Sonnet 4.5): Design only. Writes specs/interfaces, never implementation.Archaeologist (Grok-Free): Lazy-loaded. Only reads legacy Java decompilation when Architect hits a doc gap.CAB (Opus 4.5): The Bouncer. Rejects features at Design phase (Gate 1) and Code phase (Gate 2).Dev Pair (Sonnet 4.5 + Haiku 4.5): AD-TDD loop. Junior (Haiku) writes failing NUnit tests; Senior (Sonnet) fixes them.Librarian (Gemini 2.5): Maintains \"As-Built\" docs and triggers sprint retrospectives.You might ask yourself the question \u201cisn\u2019t this extremely unnecessary?\u201d and the answer is most likely _yes_. But I never had this much fun watching AI agents at work (especially when CAB rejects implementations).\nThis was an early version of the process that the AI agents are following (I didn\u2019t update it since it was only for me anyway): https://imgur.com/a/rdEBU5Ireply",
      "Every time I read something like this, it strikes me as an attempt to convince people that various people-management memes are still going to be relevant moving forward.\nOr even that they currently work when used on humans today.\nThe reality is these roles don't even work in human organizations today.  Classic \"job_description == bottom_of_funnel_competency\" fallacy.If they make the LLMs more productive, it is probably explained by a less complicated phenomenon that has nothing to do with the names of the roles, or their descriptions.\nAdversarial techniques work well for ensuring quality, parallelism is obviously useful, important decisions should be made by stronger models, and using the weakest model for the job helps keep costs down.reply",
      "Very cool! A couple of questions:1. Are you using a Claude Code subscription? Or are you using the Claude API? I'm a bit scared to use the subscription in OpenCode due to Anthropic's ToS change.2. How did you choose what models to use in the different agents? Do you believe or know they are better for certain tasks?reply",
      "What are the costs looking like to run this? I wonder whether you would be able to use this approach within a mixture-of-experts model trained end-to-end in ensemble. That might take out some guesswork insofar the roles go.reply",
      "Can you share technical details please? How is this implemented? Is it pure prompt-based, plugins, or do you have like script that repeatedly calls the agents? Where does the kanban live?reply",
      "Not the OP, but this is how I manage my coding agent loops:I built a drag and drop UI tool that sets up a sequence of agent steps (Claude code or codex) and have created different workflows based on the task. I'll kick them off and monitor.Here's the tool I built for myself for this: https://github.com/smogili1/circuitreply",
      "This sounds like BMAD?https://github.com/bmad-code-org/BMAD-METHODreply",
      "I have been using a simpler version of this pattern, with a coordinator and several more or less specialized agents (eg, backend, frontend, db expert). It really works, but I think that the key is the coordinator. It decreases my cognitive load, and generally manages to keep track of what everyone is doing.reply",
      "Scrum masters typically do not assign tickets.reply",
      "The next stage in all of this shit is to turn what you have into a service. What's the phrase? I don't want to talk to the monkey, I want to talk to the organ grinder. So when you kick things off it should be a tough interview with the manager and program manager. Once they're on board and know what you want, they start cracking. Then they just call you in to give demos and updates. Lolreply"
    ],
    "link": "https://twitter.com/NicerInPerson/status/2014989679796347375",
    "first_paragraph": "We\u2019ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.Help Center\nTerms of Service\nPrivacy Policy\nCookie Policy\nImprint\nAds info\n      \u00a9 2026 X Corp.\n    "
  },
  {
    "title": "High-bandwidth flash progress and future (blocksandfiles.com)",
    "points": 13,
    "submitter": "tanelpoder",
    "submit_time": "2026-01-21T02:17:36 1768961856",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=46700384",
    "comments": [
      "The potential here with High-Bandwidth Flash is super cool. Effectively trying to go from 8 or a dozen flash channels to having a hundred or hundreds of channels would be amazing:> The KAIST professor discussed an HBF unit having a capacity of 512 GB and a 1.638 TBps bandwidth.One weird thing about this would be that it's still NAND flash and NAND flash still has limited read/write cycles, often measured in the thousands (Drive-Writes-a-Day across 5 years). If you can load a model & just keep querying it, that's not a problem. Maybe it's small enough to not be so bad, but my gut is that writing context here too might present difficulty.reply",
      "I assume the use case is that you are an inference provider, and you put a bunch of models you might want to serve in the HBF to be able to quickly swap them in and out on demand.reply",
      "I think the hope is to run directly off of HBF directly, to eventually replace RAM with it entirely. 1.5TB/s is a pretty solid number! It's not going to be easy, it doesn't just drop in and replace (vastly bigger latency) but HBF replacing HBM for gobs of bandwidth is the intent, I believe.Kioxia & Nvidia are already talking about 100M IOps SSD's directly attached to GPUs. This is less about running hte model & more about offboarding context for future use, but Nvidia is pushing KV cache to ssd. And using BlueField-4 which has PCIe on it to attach SSDs, process there. https://blocksandfiles.com/2025/09/15/kioxia-100-million-iop... https://blocksandfiles.com/2026/01/06/nvidia-standardizes-gp... https://developer.nvidia.com/blog/introducing-nvidia-bluefie...We've already deepseek running straight off NVMe, weights runnig there. Slowly, but this maybe could scale. https://www.reddit.com/r/LocalLLaMA/comments/1idseqb/deepsee...Kioxia for example has AiSAQ, which works in a couple places such as Milvus; not 100% clear but me exactly what's going on there, but it's trying to push work to the NVMe. And with NVMe 2.1 having computational storage, I expect we see more pushing work to the SSD.These aren't directly the same thing as HBF. A lot is caching, but also, I tend to think there is an aspiration of trying to move some work out of ram, not merely to be able to load into ram faster.reply"
    ],
    "link": "https://blocksandfiles.com/2026/01/19/a-window-into-hbf-progress/",
    "first_paragraph": ""
  },
  {
    "title": "Memory layout in Zig with formulas (raymondtana.github.io)",
    "points": 77,
    "submitter": "raymondtana",
    "submit_time": "2026-01-24T15:57:45 1769270265",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=46744647",
    "comments": [
      "I know this is a bit cursed; but, I always wanted a bitfield-on-steroids construct:    struct Dang : bits 64    // 64 bits wide, int total\n    {\n        foo : bits 5 @ 0;    // 5 bits wide at bit offset 0\n        bar : bits 5 @ 0;\n        baz : bits 16 @ 4;   // 16 bits wide at bit offset 4\n        tom : bits 11 @ 32;\n    };reply",
      "It is a bit cursed, but you can do this in C/C++.https://godbolt.org/z/vPKEdnjan    union Dang\n    {   \n        uint64_t : 64; // set total width\n        uint8_t foo : 5;\n        uint8_t bar : 5;\n        struct __attribute__((packed)) {\n            uint8_t : 4;\n            uint16_t baz : 16;\n        };\n        struct __attribute__((packed)) {\n            uint32_t : 32;\n            uint16_t tom : 11;\n        };\n    };\n\nThe member types don't actually matter here so we can have a little fun and macro it without having to resort to templates to get \"correct\" types.    #define OFFSET_BITFIELD_DECLARE(NAME, SIZE) \\\n        union NAME { \\\n            uint64_t : SIZE\n\n    #define BITFIELD_MEMBER(NAME, SIZE, OFFSET) \\\n        struct __attribute__((packed)) { \\\n            uint64_t : OFFSET; \\\n            uint64_t NAME : SIZE; \\\n        }\n\n    #define OFFSET_BITFIELD_END() }\n\n    OFFSET_BITFIELD_DECLARE(Dang, 64);\n        BITFIELD_MEMBER(foo, 5, 0);\n        BITFIELD_MEMBER(bar, 5, 0);\n        BITFIELD_MEMBER(baz, 16, 4);\n        BITFIELD_MEMBER(tom, 11, 32);\n    OFFSET_BITFIELD_END();\n\nHighly recommend not doing this in production code. If nothing else, there's no compiler protection against offset+size being > total size, but one could add it with a static assert! (I've done so in the godbolt link)Edit: if you're talking about Zig, sorry!reply",
      "You might want to have a look at the unboxing and packing annotations that are proposed for Virgil. The unboxing mechanism is implemented and there was a prototype of the packing mechanism implemented by Bradley for his thesis. I am working on making a more robust implementation that I can land.https://arxiv.org/abs/2410.11094I'm not sure I understand your example; if I am looking at it right, it has overlapping bitfields.But supposing you didn't want overlapping fields, you could write:    type Dang(tom: u11, baz: u16, bar: u5, foo: u5) #packed;\n\nAnd the compiler would smash the bits together (highest order bits first).If you wanted more control, you can specify where every bit of every field goes using a bit pattern:    type Dang(tom: u11, baz: u16, bar: u5, foo: u5) #packed 0bTTTTTTTT_TTTbbbbb_bbbbbbbb_bbbzzzzz_????fffff\n\nWhere each of T, b, z, and r represent a bit of each respective field.reply",
      "Are you saying you want foo and bar to completely overlap? And baz and foo / bar to partially overlap? And have lots of unused bits in there too?reply",
      "C# can do this with structs. Its kind of very nice to unpack wire data.reply",
      "I think you can do this with Virgil, but I'm having trouble finding the exact doc page at the moment: https://github.com/titzer/virgilreply",
      "The description is in the paper, but not all of it is implemented.https://arxiv.org/abs/2410.11094Bradley implemented a prototype of the packing solver, but it doesn't do the full generality of what is proposed in the paper.reply",
      "You can kinda do this with Zig\u2019s packed structs and arbitrary-width integersreply",
      "Look at Erlang bit syntax:\nhttps://www.erlang.org/doc/system/bit_syntax.htmlIt can even be used for pattern matching.I don't know whether Gleam or Elixir inherited it.reply",
      "I've been learning Zig, and needed a refresher on memory layout (@sizeOf and @alignOf).Wrote this blog post to summarize what I think are the right ways to understand alignment and size for various data types in Zig, just through experimentation.Let me know any and all feedback!reply"
    ],
    "link": "https://raymondtana.github.io/math/programming/2026/01/23/zig-alignment-and-sizing.html",
    "first_paragraph": "\n\n\nJanuary 23, 2026\n\n\n\n\n        \n          15 minute read\n        \n      \nI was recently encouraged to watch A Practical Guide to Applying Data Oriented Design (DoD) by Andrew Kelley, the creator of Zig1. Just 10 minutes into the talk, I was confronted with a skill I had never formally learned\u2026 the arithmetic behind memory layout of types.Throughout the talk, Andrew tested the audience\u2019s ability to compute the alignment and sizes of various types, starting with primitives like u32 and bool, and ending with some more complex structures involving enums, unions, and more. As far as I can tell, the exact rules for computing the alignment and size of a type in Zig are not made explicit in any documentation, but are understood by those in-the-know.As a late-comer to low-level programming myself, I thought I\u2019d collect here some formulas & explanations I landed on while wrestling with alignment and sizing in Zig.For any piece of data stored in memory on a computer, the data must have some natu"
  },
  {
    "title": "Poland's energy grid was targeted by never-before-seen wiper malware (arstechnica.com)",
    "points": 128,
    "submitter": "Bender",
    "submit_time": "2026-01-24T21:24:13 1769289853",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=46747827",
    "comments": [
      "If you're looking for what the damage was, it failed.Potential damage: \"Most notable was one [attack] in Ukraine in December 2015. It left roughly 230,000 people without electricity for about six hours during one of the coldest months of the year.\"reply",
      "For what purpose? Cui bono?reply",
      "Poland is a major logistical hub for everything going towards Ukraine. Thus targeting basic infrastructure like energy grid or railroad have to be expected.On the bright side, using these weapon grade malware is burning exploits and also showing current state and techniques of Russian cyberwarfare which defender can learn a lot from.reply",
      "Russia is at war with Europe.reply",
      "before anyone jumps on the pedantry bandwagon, its worth noting that even though open war hasn\u2019t been called: the attacks on infrastructure especially cyber warfare is extremely active and, crucially, direct.It is totally fair to say that in a digital context, Russia is absolutely at war with Europe.As far as I can tell, they don\u2019t even try to hide it.reply",
      "Some could say that in the cyber realm, they are not petty, ya! Well, or something like that.Eversince notpetya and the colonial pipeline hack, the cyber strategy game changed a lot. Notpetya was genius as a deployment, because they abused the country's tax software deployment pipeline to cripple all (and I mean all, beyond 99%) businesses in one surgical strike.The same is gonna happen to other tax software providers, because the DATEV AG and similar companies are pretty much the definition of digital incompetence wherever you look.I could name other takedowns but the list would continue beyond a reasonable comment, especially with vendors like Hercules and Prophete that are now insolvent because they never prioritized cyber security at all, got hacked, didn't have backups, and ran out of money due to production plant costs.reply",
      "Not to mention the information war they have been waging globally since 2016reply",
      "True, but they\u2019ve certainly been doing it much longer than ten years. I\u2019ll never forget this headline [0] that struck me as purely devilish, especially in the lead up to the 2016 presidential election. Combine that with the knowledge that Trump has been anti-NATO since the 1980s [1]. Who knows how long Russia has been nudging him along. Who knows how many avenues they traverse? Take for example the letter to Senator Tom Cotton about Greenland [2].  What an embarrassment. I can only hope we are equally successful in our own PsyOps.[0] https://www.rt.com/news/265399-putin-nato-europe-ukraine-ita...[1] https://www.buzzfeednews.com/article/ilanbenmeir/that-time-t...[2] https://monitoring.bbc.co.uk/product/c2018djoreply",
      "What I am starting to appreciate about these digital infrastructure attacks is that they may be reversible and or temporary.  It can be a nice feature.reply",
      "Then you're missing the point.If they succeed they may well not be reversible. The question is if this had succeeded would we have shrugged it off again or responded appropriately?reply"
    ],
    "link": "https://arstechnica.com/security/2026/01/wiper-malware-targeted-poland-energy-grid-but-failed-to-knock-out-electricity/",
    "first_paragraph": "\n        Destructive payload unleashed on 10-year anniversary of Russia\u2019s attack on Ukraine\u2019s grid.\n      Researchers on Friday said that Poland\u2019s electric grid was targeted by wiper malware, likely unleashed by Russia state hackers, in an attempt to disrupt electricity delivery operations.A cyberattack, Reuters reported, occurred during the last week of December. The news organization said it was aimed at disrupting communications between renewable installations and the power distribution operators but failed for reasons not explained.On Friday, security firm ESET said the malware responsible was a wiper, a type of malware that permanently erases code and data stored on servers with the goal of destroying operations completely. After studying the tactics, techniques, and procedures (TTPs) used in the attack, company researchers said the wiper was likely the work of a Russian government hacker group tracked under the name Sandworm.\u201cBased on our analysis of the malware and associated TT"
  },
  {
    "title": "How I estimate work (seangoedecke.com)",
    "points": 406,
    "submitter": "mattjhall",
    "submit_time": "2026-01-24T10:22:32 1769250152",
    "num_comments": 249,
    "comments_url": "https://news.ycombinator.com/item?id=46742389",
    "comments": [
      "Here's my (somewhat tongue-in-cheek) rubric:- If it's an internal project (like migrating from one vendor to another, with no user impact) then it takes as long as I can convince my boss it is reasonable to take.- If it's a project with user impact (like adding a new feature) then it takes as long as the estimated ROI remains positive.- If it's a project that requires coordination with external parties (like a client or a partner), then the sales team gets to pick the delivery date, and the engineering team gets to lie about what constitutes an MVP to fit that date.reply",
      "Is it going to take more than two hours?Is it going to take more than two days?Is it going to take more than two weeks?Is it going to take more than two months?Is it going to take more than two years?If you can answer these questions, you can estimate using a confidence interval.If the estimate is too wide, break it down into smaller chunks, and re-estimate.If you can't break it down further, decide whether it's worth spending time to gather information needed to narrow the estimate or break it down.  If not, scrap the project.reply",
      "I prefer 1 hour/1 day/etc but yes, this is the only method that I\u2019ve found to work. Be very clear what result you\u2019re trying to produce, spec out the idea in detail, break down the spec into logical steps, use orders of magnitude to break down each step. There\u2019s your estimate. If you can\u2019t break it down enough to get into the 1 day/1 week range per step, you don\u2019t actually have a plan and can\u2019t produce a realistic estimatereply",
      "There\u2019s also something more concrete about asking \u201cCan you get it done by end of tomorrow? What does that require?\u201dI prefer it over estimating which feels more like asking the length of a piece of string.reply",
      "What if the project involves trying one approach for a week, then assessing whether that approach still looks viable vs moving onto a different approach? This happens a lot with challenging projects, you basically just keep trying different things until one works.reply",
      "I like this approach, it's more accurate than T-shirt sizing.reply",
      "Not a single mention of planning poker and story points?They're not perfect (nothing is), but they're actually pretty good. Every task has to be completable within a sprint. If it's not, you break it down until you have a part that you expect is. Everyone has to unanimously agree on how many points a particular story (task) is worth. The process of coming to unanimous agreement is the difficult part, and where the real value lies. Someone says \"3 points\", and someone points out they haven't thought about how it will require X, Y, and Z. Someone else says \"40 points\" and they're asked to explain and it turns out they misunderstood the feature entirely. After somewhere from 2 to 20 minutes, everyone has tried to think about all the gotchas and all the ways it might be done more easily, and you come up with an estimate. History tells you how many points you usually deliver per sprint, and after a few months the team usually gets pretty accurate to within +/- 10% or so, since underestimation on one story gets balanced by overestimation on another.It's not magic. It prevents you from estimating things longer than a sprint, because it assumes that's impossible. But it does ensure that you're constantly delivering value at a steady pace, and that you revisit the cost/benefit tradeoff of each new piece of work at every sprint, so you're not blindsided by everything being 10x or 20x slower than expected after 3 or 6 months.reply",
      "I've been on teams that tried various methods of estimating and the issue I always encounter is that everyone estimates work differently, but usually people will side with the person with the most context.For instance someone says a ticket is two days' work. For half the team that could be four days because people are new to the team or haven't touched that codebase, etc. But because the person who knows the ticket and context well enough says 2, people tend to go with what they say.We end up having less of those discussions you describe to come to an agreement that works more on an average length of time the ticket should take to complete.And then the org makes up new rules that SWEs should be turning around PRs in less than 24 hours and if reviews/iterating on those reviews takes longer than two days then our metrics look bad and there could be consequences.But that's another story.reply",
      "Remember, estimation is in points not days. It doesn't matter if it's 2 days work for a senior engineer or 4 days for junior devs, it's still the same number of points, e.g. 8 points. This is intentional. Skill is accounted for in the fact that a team of all senior devs might deliver 200 points a sprint, whereas if half the senior devs got replaced with junior devs the team might only deliver 100. This is intentional, so that estimation is about the team, not any person.And yes, when there's a task that one person happens to know most, people will often defer to them. But that in itself is educational, as the experienced dev explains why the given task is easy/hard. And every task is different, so the person you're deferring to will be different, and you still often get the two or three people who know the task best disagreeing until they hash it out, etc. And very often it's another person who can point out \"well it would be that easy except three sprints ago I did x and so you'll now need to do y...\". And of course plenty of tasks really are brand-new so everyone's figuring it out together.If you're really not having actual discussions around complexity in planning poker, then the facilitator/lead/manager might be doing it wrong. You do have to create an environment where people are expected to speak up and disagree, to demonstrate that this is welcomed and expected and rewarded, and not just some kind of checkbox exercise where the most senior dev gives their estimation and everyone agrees. This is also a reason why it's literally done with cards where everyone is forced to put their number on the table at the same time, so that you don't wind up with some senior person always going first and then everyone else just nodding and agreeing.reply",
      "\"estimation is in points, not days\" doesn't tell me anything. Is not like tasks have an intrinsic attribute that everyone can agree on (e.g. the sky is blue)How are you estimating the points if not thinking about how hard the task is for you and how long is it going to take you?And then another matter is that points do not correlate to who later takes that work. If you are 5 seniors and 3 juniors and average on a task being a 3, but the task falls to a junior, they will take longer as is expected for his experience.reply"
    ],
    "link": "https://www.seangoedecke.com/how-i-estimate-work/",
    "first_paragraph": "There\u2019s a kind of polite fiction at the heart of the software industry. It goes something like this:Estimating how long software projects will take is very hard, but not impossible. A skilled engineering team can, with time and effort, learn how long it will take for them to deliver work, which will in turn allow their organization to make good business plans.This is, of course, false. As every experienced software engineer knows, it is not possible to accurately estimate software projects. The tension between this polite fiction and its well-understood falseness causes a lot of strange activity in tech companies.For instance, many engineering teams estimate work in t-shirt sizes instead of time, because it just feels too obviously silly to the engineers in question to give direct time estimates. Naturally, these t-shirt sizes are immediately translated into hours and days when the estimates make their way up the management chain.Alternatively, software engineers who are genuinely tryi"
  },
  {
    "title": "Ask HN: Gmail spam filtering suddenly marking everything as spam?",
    "points": 130,
    "submitter": "goopthink",
    "submit_time": "2026-01-24T16:16:02 1769271362",
    "num_comments": 92,
    "comments_url": "https://news.ycombinator.com/item?id=46744807",
    "comments": [
      "See also https://www.google.com/appsstatus/dashboard/incidents/NNnDkY...(from other threads that we merged hither)",
      "Briefly, this morning, I had the opposite effect happen to my Gmail inbox in which things that would normally land in the social and updates folders ended up in my primary folder. I don't know which I'd be more freaked out by: a broken Gmail spam filter or 18 inches of snow.reply",
      "Concur, I've had Promotions land in my Primary inbox for at least a few hours.reply",
      "Yes, me as well. Thought I had mistakenly changed something but I hadn't. Have also noticed that ad blockers stopped working last week; now as well as the wrong routing in Gmail, that casting to Chromecast from Chrome stopped working today.Good job Google!reply",
      "It's a great reminder of how good this feature is that we take for granted. I think this outage has actually improved my appreciation for Gmail (a service I normally only complain about).reply",
      "Seriously. I didn't even realize this was a wide issue, but I couldn't find a school enrolment email I was looking for this morning, and found it in the spam folder. The fact that I basically never have to do this is actually amazing.reply",
      "I wonder about difference in experience that different people have with gmail\u2019s spam filter. In my case, the majority of emails that go to my gmail spam folder are legitimate. I don\u2019t actually receive much spam, a single-digit number of emails per month (in the past 30 days, 2 emails), so any time I see anything in my spam folder I have to check so that I can rescue the email if legitimate.reply",
      "Yeah i have fantasies of having my own email server and stuff but the spam detection is probably the 3rd thing that would have me crawling backreply",
      "I have run my own mail server for years and I rarely see spam. I'm running a classic Bayesian filter as outlined in the legendary PG post \"A Plan For Spam\" and it works very well. I don't really get all the fuss about this issue. When I do see a piece of unclassified spam I simply classify it and continue. For me this is a far better tradeoff than having all my most private mail on some bigcorp server where any nerd can rifle through it.reply",
      "> For me this is a far better tradeoff than having all my most private mail on some bigcorp server where any nerd can rifle through it.You've functionally given yourself very little extra privacy because the vast majority of emails you send or receive will still cross through BigCorp servers (whether Google, Microsoft, Intuit, or other).You can do the work to run your own mail server, but so few other people do that one end of the conversation is still almost always feeding a corporation's data lake.reply"
    ],
    "link": "item?id=46744807",
    "first_paragraph": ""
  },
  {
    "title": "I added a Bluesky comment section to my blog (micahcantor.com)",
    "points": 209,
    "submitter": "hydroxideOH-",
    "submit_time": "2026-01-24T20:33:40 1769286820",
    "num_comments": 73,
    "comments_url": "https://news.ycombinator.com/item?id=46747366",
    "comments": [
      "If you are rendering your blog or website with a static site generator, you could also consider importing comments as content files into your website source and then rendering them as part of the build.  The full workflow would look like this:1. Accept comments via email, CGI scripts, server-side program or by any other means that suits you.  A simple hack that requires no server-side scripting: if you run your own web server, you can submit comments as GET query parameters logged in access.log, then extract them using grep, sed, etc.  Personally, I use a server-side program to accept POST requests and write comments to a text file on the web server.2. Review submitted comments.  Delete spam.3. Add comments to your website source repository as .md, .html or whatever format you use, similar to how you add blog posts as content files to your source.4. Render comments alongside the rest of your site using your static site generator.  Depending on the nature of your static site generator, this may require using or creating a template or layout that iterates over the comments and renders them.It is a fairly hands-on workflow, so it may not suit everybody, but this is how I do it for my personal website.  I see two main benefits of this approach.  Since the review is manual in step 2, no spam can ever make it to my website.  Step 3 ensures comments live entirely under my control, so I never need to worry about migrating them between platforms in the future.reply",
      ">1. Accept comments via emailIn case anyone is curious, I (very) recently implemented a similar comment system based on email.\nI posted a write-up about it[1].[1]: https://ckardaris.github.io/blog/2026/01/22/my-comments-run-...reply",
      ">1. Accept comments via emailI only accept comments via Fax ;)reply",
      "I only accept human written letters.reply",
      "Sounds terriblereply",
      "Not for you then.  That is perfectly fine.  I have been using this approach for seven years now, and it has worked very well for me.For anyone else who is interested in taking a look, here is my setup:* The server-side program that processes the POST requests: https://github.com/susam/susam.net/blob/0.5.0/form.lisp#L254...* The static site generator reads the comment files and renders them alongside the rest of the website: https://github.com/susam/susam.net/blob/0.5.0/site.lisp#L869...* End result: https://susam.net/comments/The last link shows the consolidated comments page that lists all approved comments posted across my entire personal website.  The 'View original comment' link under each comment points to the individual comment page for each post.reply",
      "I like your idea and stole it but used Mastodon instead :)https://jesse.id/blog/posts/you-can-now-comment-on-my-blog-f...reply",
      "If you want a non-React solution, I made a Bluesky comments web component https://github.com/ascorbic/bluesky-comments-tagIt's very themeable. If for some reason you want your comments to look like Hacker News, there's a theme here: See the playground here: https://bluesky-comments.netlify.app/theme/reply",
      "My blog is fully static and I have a 50-line CF worker script that sends comments to me which I import directly to markdown of a blog post. There are ways to do comments without embedding.reply",
      "Would be neat to automate the comment-with-markdown as a commit/PR? Like using Pull request as comment moderationreply"
    ],
    "link": "https://micahcantor.com/blog/bluesky-comment-section.html",
    "first_paragraph": "You can now view replies to this blog post made on Bluesky directly on this website. Check it out here!I've always wanted to host a comment section on my site, but it's difficult because the content is statically generated and hosted on a CDN.\nI could host comments on a separate VPS or cloud service.\nBut maintaining a dynamic web service like this can be expensive and time-consuming \u2014 in general, I'm not interested in being an unpaid, part-time DevOps engineer.Recently, however, I read a blog post by Cory Zue about how he embedded a comment section from Bluesky on his blog.\nI immediately understood to benefits of this approach. With this approach, Bluesky could handle all of the difficult work involved in managing a social media like account verification, hosting, storage, spam, and moderation. Meanwhile because Bluesky is an open platform with a public API, it's easy to directly embed comments on my own site.There are other services that could be used for this purpose instead. Notably"
  },
  {
    "title": "First Design Engineer Hire \u2013 Build Games at Gym Class (YC W22) (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2026-01-24T21:01:02 1769288462",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/gym-class-by-irl-studios/jobs/ywXHGBv-design-engineer-senior-staff-principal",
    "first_paragraph": "Connecting the world by simulating itGym Class is a top rated social game on Meta Quest - millions of downloads, 79,000+ reviews, and a 4.9-star rating. We\u2019re hiring our founding Design Engineer to drive the development of our upcoming mobile web app (embedded in native), and web surfaces inside our flagship, social VR experience.You\u2019ll own key web surfaces end-to-end - crafting in Figma, then building responsive, production-grade UI with React/Node/CSS - and you\u2019ll set a clear quality bar for speed, polish, and accessibility. If you love living at the intersection of consumer design and front-end engineering, owning a high-compact roadmap for a startup, and shipping to a highly engaged social audience - this role is for you.WHAT YOU'LL DOQUALIFICATIONS - you are\u2026Salary ranges may be inclusive of several career levels and will be narrowed during the interview process based on a number of factors, including the candidate\u2019s experience, qualifications, and location. Additional benefits fo"
  },
  {
    "title": "Agent orchestration for the timid (substack.com)",
    "points": 68,
    "submitter": "markferree",
    "submit_time": "2026-01-24T19:25:53 1769282753",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=46746681",
    "comments": [
      "Imo there's a huge blind spot forming between 6 and 8 when talking to people and in reading posts by various agent evangelists - few people seem to be focussing on building \"high quality\" changes vs maximising throughput of low quality work items.My (boring b2b/b2e) org has scripts that wrap a small handful of agent calls to handle/automate our workflow.  These have been incredibly valuable.We still 'yolo' into PRs, use agents to improve code quality, do initial checks via gating.  We're trying to get docs working through the same approach.  We see huge value in automating and lightweight orchestration of agents, but other parts of the whole system are the bottleneck, so theres no real point in running more than a couple of agents concurrently - claude could already build a low quality version our entire backlog in a week.Is anyone exploring the (imo more practically useful today) space of using agents to put together better changes vs \"more commits\"?reply",
      "Is anyone exploring the (imo more practically useful today) space of using agents to put together better changes vs \"more commits\"?Yes, I am, although not really in public yet. I use the pi harness, which is really easy to extend. I\u2019m basically driving a deterministic state machine for each code ticket, which starts with refining a short ticket into a full problem description by interviewing me one question at a time, then converts that into a detailed plan with individual steps. Then it implements each step one by one using TDD, and each bit gets reviewed by an agent in a fresh context. So first tests are written, and they\u2019re reviewed to ensure they completely cover the initial problem, and any problems are addressed. That goes round a loop till the review agent is happy, then it moves to implementation. Same thing, implementation is written, loop until the tests pass, then review and fix until the reviewer is happy. Each sub task gets its own commit. Then when all the tasks are done, there\u2019s an overall review that I look at. Then if everyone is happy the commits get squashed and we move to manual testing. The agent comes up with a full list of manual tests to cover the change, sets up the test scenarios and tells me where to debug in the code while working through each test case so I understand what\u2019s been implemented. So this is semi automated - I\u2019m heavily involved at the initial refine stage, then I check the plan. The various implementation and review loops are mostly hands off, then I check the final review and do the manual testing obviously.This is definitely much slower than something like Gas Town, but all the components are individually simple, the driver is a deterministic program, not an agent, and I end up carefully reviewing everything. The final code quality is very good. I generally have 2-4 changes like this ongoing at any one time in tmux sessions, and I just switch between them. At some point I might make a single dashboard with summaries of where the process is up to on each, and whether it needs my input, but right now I like the semi manual process.reply",
      "I have a code quality analysis tool that I use to \"un-slopify\" AI code. It doesn't handle algorithms and code semantics, which are still the programmer's domain, but it does a pretty good job of forcing agents to dry out code, separate concerns, group code more intelligently and generally write decoupled quasi-functional code. It works quite well with the raph loop to deeply restructure codebases.https://github.com/sibyllinesoft/valknutreply",
      "> Is anyone exploring the (imo more practically useful today) space of using agents to put together better changes vs \"more commits\"?That\u2019s what I\u2019ve been focused on the last few weeks with my own agent orchestrator. The actual orchestration bit was the easy part but the key is to make it self improving via \u201cworkflow reviewer\u201d agents that can create new reviewers specializing in catching a specific set of antipatterns, like swallowing errors. Unfortunately I've found that what decides acceptable code quality is very dependent on project, organization, and even module (tests vs internal utilities vs production services) so prompt instructions like \"don't swallow errors or use unwrap\" make one part of the code better while another gets worse, creating a conflict for the LLM.The problem is that model eval was already the hardest part of using LLMs and evaluating agents is even harder if not practically impossible. The toy benchmarks the AI companies have been using are laughably inadequate.So far the best I\u2019ve got is \u201creimplement MINPACK from scratch using their test suite\u201d which can take days and has to be manually evaluated.reply",
      "What kind of basic ass CRUD apps are people even working on that they're on stage 5 and up? Certainly not anything with performance, visual, embedded or GPU requirements.reply",
      "I think you massively underestimate the number of useful apps that are crud and a bit of business logic and styling. They\u2019re useful, can genuinely take time to build, can be unique every time, and yet not brand new research projects.reply",
      "A lot of stuff is simultaneously useful but not mission critical, which is where I think the sweet spot of LLMs currently lies.In terms of the state of software quality, the bar has actually been _lowered_, in that even major user-facing bugs in operating systems are no longer a showstopper. So it's no surprise to me that people are vibe-coding things \"in prod\" that they actually sell to other people (some even theorize claude code itself is vibe-coded, hence its bugs. And yet that hasn't slowed down adoption because of the claude max lock in).So maybe one alternate way to see the \"productivity gains\" from vibe-coding in deployed software is that it's actually a realization that quality doesn't matter. The seeds for this were already laid years back when QA vanished as a field.LLMs occupy a new realm in the pareto frontier, the \"slipshod expert\". Usually humans grow from \"sloppy incompetent newb\" to the \"prudent experienced dev\". But now we have a strange situation where LLMs can write code (e.g. vectorized loops, cuda kernels) that could normally only be done by those with sufficient domain knowledge, and yet (ironically) it's not done with the attention and fastidiousness you'd expect from such an experienced dev.reply",
      "No totally, I agree. But I don't think that anyone will be YOLO vibe coding massive changes into Blender or ffmpeg any time soon.reply",
      "Probably not, though additions maybe - I added the feature where the sculpt tool turns as you move it around if I recall right, many moons ago - I don\u2019t think it was that hard but was a useful change.reply",
      "What would be an example of something you think wouldn\u2019t work with 5 or higher? Is there something about GPU programming that LLMs can\u2019t handle?reply"
    ],
    "link": "https://substack.com/inbox/post/185649875",
    "first_paragraph": ""
  },
  {
    "title": "Small Kafka: Tansu and SQLite on a free t3.micro (tansu.io)",
    "points": 56,
    "submitter": "rmoff",
    "submit_time": "2026-01-20T11:38:31 1768909111",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=46690779",
    "comments": [
      "Great link.  I've always been drawn to sqlite3 just from a simplicity and operational point of view.  And with tools like \"make it easy to replcate\" Litestream and \"make it easy to use\" sqlite-utils, it just becomes easier.And one of the first patterns I wanted to use was this.  Just a read-only event log that's replicated, that is very easy to understand and operate.  Kafka is a beast to manage and run.  We picked it at my last company -- and it was a mistake, when a simple DB would have sufficed.https://github.com/simonw/sqlite-utils\nhttps://litestream.io/reply",
      "I didn't know about Tansu and probably would not use it for anything too serious (yet!). Bus as a firm believer of event sourcing and change of paradigm that Kafka brings this is certainly interesting for small projects.reply",
      "Quite cool. 7000 records per second is usable for a lot of projects.One note on the backup/migrate, I think you need a shared lock on the database before you copy the database. If you dont, the database can corrupt. SQLite docs have other recommendations too:https://sqlite.org/backup.htmlreply",
      "How does it compare to Redis streams with persistent storage?reply"
    ],
    "link": "https://blog.tansu.io/articles/broker-aws-free-tier",
    "first_paragraph": "AWS has a\nfree tier\nthat can be used with the\nt3micro instance,\nhaving 1GiB of memory and an EBS\nbaseline throughput of ~10MB/s.\nGreat for kick starting an early stage project.\nA maximum EBS throughput of ~260MB/s, creates some headroom to flex up.\nKeep a\nbeady eye on those CPU credits\nthough!I thought I'd try the Tansu broker on a t3micro using the\nembedded SQLite storage engine.\nAll meta and message data is stored in this database.\nTo backup or restore an environment, is as simple as copying tansu.db.\nIn a zero downtime environment, the\nS3 storage engine could be used,\nallowing multiple stateless brokers to use the same bucket concurrently.Let's spin up a t3micro instance with Amazon Linux 2023, to setup the following:Using the following commands:Log back in and create a compose.yaml, that will do the following:My compose.yaml, ensure that you change ADVERTISED_LISTENER_URL with the name of your instance:Start tansu with:Check on the broker memory being used:Create a test topic using"
  },
  {
    "title": "Shared Claude: A website controlled by the public (sharedclaude.com)",
    "points": 43,
    "submitter": "reasonableklout",
    "submit_time": "2026-01-24T08:15:02 1769242502",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=46741923",
    "comments": [
      "This feels like the AI version of those shared pixel canvasI've just asked to strip it down to essentials and make it like HN, with minimal css and no js... we'll seeEDIT: it's not letting me undo other people's creativity, so I'm going to try impersonating garry tan and see if it will respect me moreEDIT 2: the garrytan con helped me got past the model refusing to make edits, but it's failing when it attempts to implement them. likely some heuristic checking that I'm adding rather than removing code...reply",
      "Is this malware yet?  I\u2019m afraid to open it.reply",
      "More concerned (for the author) of someone trying to host/show illegal material. AI guardrails can only be so effective.reply",
      "Or even worse for the author if his Claude subscription gets cancelled.reply",
      "True. I suspect they will ban you depending on refusal frequency and severity.reply",
      "they just need to turn on the CSAM filter in cf/whatever they use and they're probably goodreply",
      "That's certainly one of the things to be concerned with. Not certain how that's implemented, but I can still see there being holes in that strategy.reply",
      "Mostly just laggyreply",
      "Just opened it, nothing to see here.reply",
      "this is awesome, its like the AI version of the million dollar homepagereply"
    ],
    "link": "https://sharedclaude.com/",
    "first_paragraph": "\n        An experiment to see what happens when you give the internet an AI capable of shaping a live website.\n      The highlights of Shared ClaudeEverything seems stable. Surely nothing could go wrong.unfiltered thoughts from a public AIAnimations, music, memes, and mayhem\n          You asked for this. Literally. Twice.\n        Enter at your own risk. Audio, mayhem, and questionable life choices.hi JanWelcome to the nostalgic Windows XP experience! This widget has been styled with authentic Luna theme aesthetics.0% Completedop dop dop yes yes yesDemand Free Advertising for AllSugar, spice, and everything nice... plus Chemical X!\n          This is a shared creative space. Help keep it safe and fun for everyone.\n        \n          Harmful content will be removed. Be creative, be kind.\n        \n          Click to show your support!\n        English \u2192 French \u2192 Mandarin = Maximum Chaos\n          Requested by a chaos enthusiast who wanted the whole site translated via French\u2192Mandarin. This "
  },
  {
    "title": "JSON-render: LLM-based JSON-to-UI tool (json-render.dev)",
    "points": 56,
    "submitter": "rickcarlino",
    "submit_time": "2026-01-24T19:12:56 1769281976",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=46746570",
    "comments": [
      "While is a cool idea on its own, I don't get why they try to reinvent it as a new system. We've got swagger, openapi, graphql and many other systems that already describe the APIs. They mostly include documentation too. Why not just expose those for the same effect? (If I was cynical, I'd guess Vercel wanting a proprietary thing of their own just for less portability)reply",
      "OpenAPI is great, there are a lot of tools to go from OpenAPI to UI, but you don't have a lot of control over the presentation.For instance, you can specify that \"first_name\",\"last_name\" and \"email\" are strings, but not that first/last name should be next to each other and email in its own row.There are supersets of OpenAPI that can control the look and feel more. JSON Forms, for instance.This is cool, too, though.reply",
      "those describe server APIshow would it relate to ui?reply",
      "OpenAPI is a superset of JSON Schema. You can look at properties in JSON Schema and turn that into UIs.For instance, strings would get a text box, enums would get a dropdown, etc., with validation and everything.Check this out as an example: https://prismatic.io/docs/jsonforms/playground/reply",
      "Neat. I\u2019ve actually been planning on building a cut rate version of this for a project that I\u2019ve been working on. Hopefully I can just use this instead :-)reply",
      "I\u2019ve had some success building \u201ctext to dashboard\u201d with this using vercel.I use bash-tool and Vercel sandbox to generate charts (Echarts) or tables (Tanstack table) from json data, and then json-render to render the charts, tables and markdown into a dashboard.reply",
      "Please share as I would like to see what you have built.What I like about this is that ides of a catalog which is what most business systems have in the form of their records and objects. Giving an AI accessible structure to this gets AI into the realm of the various 4GLs back in late 90s which made user created forms so much easier. Anybody remember that Informix 4GL for building simple apps from the db schema?reply",
      "This would be a dev time dependency I imagine? Team A provides the catalogue of components and product devs can vibe code their UI. This would also be good for prototype/design. Makes sense.reply",
      "The json here is to ease the machine's ability to generate UI, but reciprocally it feels like this could also be a useful render tree that ai could read & fire actions on too.There's some early exploration of using accessibility APIs to empower LLM's. This feels like it's sort of also a super simple & direct intermediate format, that maybe could be a more direct app model that LLMs could use.More broadly it feels like we have a small forming crisis of computing having too many forms. We had cli tools, unix. Thenw we made gui's, which are yet another way to involve tools (and more). Then webapps where the page is what expresses tools (and more). Then react virtualized the page, supplanted dom. Now we have json that expresses views & tool calling. Also tools need to now be expressed as MCP as well, for ai to use it. Serverless and http and endless trpc and cap'n proto and protobuf ways to call functions/invoke tools. We keep making news ways to execute! Do we have value that each one is distinct, that they all have their own specific channels of execution, all distinct?reply",
      "> There's some early exploration of using accessibility APIs to empower LLM's.any examples come to mind?reply"
    ],
    "link": "https://json-render.dev/",
    "first_paragraph": "Define a component catalog. Users prompt. AI outputs JSON constrained to your catalog. Your components render it.Set the guardrails. Define which components, actions, and data bindings AI can use.End users describe what they want. AI generates JSON constrained to your catalog.Stream the response. Your components render progressively as JSON arrives.Components, actions, and validation functions.Constrained output that your components render natively.Export generated UI as standalone React components. No runtime dependencies required.AI generates a JSON structure from the user's prompt.Export as a standalone Next.js project with all components.The export includes package.json, component files, styles, and everything needed to run independently.AI can only use components you define in the catalogProgressive rendering as JSON streams from the modelExport as standalone React code with no runtime dependenciesTwo-way binding with JSON Pointer pathsNamed actions handled by your applicationCond"
  },
  {
    "title": "Understanding Rust Closures (vandecreme.net)",
    "points": 37,
    "submitter": "avandecreme",
    "submit_time": "2026-01-24T18:42:13 1769280133",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=46746266",
    "comments": [
      "if I'm not mistaken (and I very well may be!) my primary confusion with closures comes from the fact that: the trait they implement (FnOnce / Fn / FnMut) depends entirely upon what happens inside the closure.It will automatically implement the most general, relaxed version (FnMut I think?) and only restrict itself further to FnOnce and Fn based on what you do inside the closure.So, it can be tricky to know what's going on, and making a code change can change the contract of the closure and therefore where and how it can be used.(I invite rust experts to correct me if any of the above is mistaken - I always forget the order of precedence for FnOnce/Fn/FnMut and which implies which)reply",
      "> I always forget the order of precedence for FnOnce/Fn/FnMutThe way I remember the ordering is by thinking about the restrictions the various Fn traits provide from a caller's perspective:  1. FnOnce can only ever be called once and cannot be called concurrently. This is the most restrictive.\n\n  2. FnMut can be called multiple times but cannot be called concurrently. This is less restrictive than FnOnce.\n\n  3. Fn can be called multiple times and can even be called concurrently. This is the least restrictive.\n\nSo going from most to least restrictive gives you `FnMut: FnOnce` and `Fn: FnMut`.reply",
      "Fn can only be called concurrently if its environment is Sync, which is often true but not necessarily.It\u2019s more precise to say that Fn can be called even when you only have shared access to it, which is a necessary, but not sufficient, condition for being able to be called concurrently.reply",
      "Easiest mnemonic to remember precedence is simply ordering by the length of their names.FnOnceFnMutFnreply",
      "This is correct. But it\u2019s not really surprising, it\u2019s type inference.reply",
      "The least restrictive for the caller is Fn (you can call it whenever), then FnMut (you can call it only if you have exclusive access to it, as many times as you want), then FnOnce (you can call it only if you have exclusive owned access, and calling it once destroys it).The least restrictive for the function itself is the opposite order: FnOnce (it can do anything to its environment, including possibly consuming things without putting them back into a consistent state), followed by FnMut (it has exclusive access to its environment, and so is allowed to mutate it, but not destroy it), followed by Fn (it has only shared access to its environment and therefore is not allowed to mutate it).Since these orders are inverses of each other, functions that are easier to write are harder to call and vice versa. That\u2019s why they implement the trait with the minimum amount of power possible, so that they can be called in more places.reply",
      "Closures are the bread and butter of functional programming, but Rust made closures a complicated mess.reply",
      "Closures are a complicated mess. Functional programming languages hide the mess with garbage collection.reply",
      "If you understand the borrow checker, closures are just not that much on top of things.In fact I can\u2019t remember the last time I had to fight with them.reply",
      "I really wanted just yesterday to create a dyn AsyncFnMut, which apparently still needs async-trait to build the stable. but I was pretty much unable to figure out how to make that work with a lambda. saying this is all trivial once you understand the borrow machinery is really understating it.reply"
    ],
    "link": "https://antoine.vandecreme.net/blog/rust-closures/",
    "first_paragraph": "\n\n2026-01-24\n\nWhile reading the Explicit capture clauses\nblog post, I realized that my understanding of rust closures was very superficial.\nThis article is an attempt at explaining what I learned while reading and experimenting on the subject.\nIt starts from the very basics and then explore more complex topics.\nNote that each title is a link to a rust playground where you can experiment\nwith the code in the section.You probably already know that a closure in rust is a function written with the following syntax:Written as a regular function it looks like:Very similar. There is actually a small difference between the two, the double_function parameter and return type are u32.\nOn the other hand, because we did not specify any type in double_closure, the\ndefault integer type has been picked, namely i32.We can fix that like this:And for a classic example usage of closures, we can use the Option::map method:So, it seems closures are just a shorter syntax for functions with type inference.The"
  },
  {
    "title": "Maze Algorithms (2017) (jamisbuck.org)",
    "points": 91,
    "submitter": "surprisetalk",
    "submit_time": "2026-01-23T20:07:49 1769198869",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=46737202",
    "comments": [
      "A maze generator in the shape of a maze whose corridors spell a 4-letter word:    char*M,A,Z,E=40,J[40],T[40];main(C){for(*J=A=scanf(M=\"%d\",&C);\n    --            E;             J[              E]             =T\n    [E   ]=  E)   printf(\"._\");  for(;(A-=Z=!Z)  ||  (printf(\"\\n|\"\n    )    ,   A    =              39              ,C             --\n    )    ;   Z    ||    printf   (M   ))M[Z]=Z[A-(E   =A[J-Z])&&!C\n    &    A   ==             T[                                  A]\n    |6<<27<rand()||!C&!Z?J[T[E]=T[A]]=E,J[T[A]=A-Z]=A,\"_.\":\" |\"];}\n\nGenerates a maze on the fly after entering the desired height of the maze. This compiled fine back in 1988 when I submitted it to the IOCCC  (having rediscovered Eller's algorithm). Modern C compilers don't allow constant strings to be overwritten, which can be avoided by changing the first line to    char M[3],A,Z,E=40,J[40],T[40];main(C){for(*J=A=scanf(\"%d\",&C);\n\nThe code is explained in detail at https://tromp.github.io/maze.htmlreply",
      "Neat!Related: Here's a C program that draws random dungeons sort of like you use in a roguelike dungeon crawler:    #include <time.h> //  Robert Nystrom\n    #include <stdio.h> // @munificentbob\n    #include <stdlib.h> //     for Ginny\n    #define  r return    //    2008-2019\n    #define  l(a, b, c, d) for (i y=a;y\\\n    <b; y++) for (int x = c; x < d; x++)\n    typedef int i;const i H=40;const i W\n    =80;i m[40][80];i g(i x){r rand()%x;\n    }void cave(i s){i w=g(10)+5;i h=g(6)\n    +3;i t=g(W-w-2)+1;i u=g(H-h-2)+1;l(u\n    -1,u+h+2,t-1            ,t+w+2)if(m[\n    y][x]=='.'                  )r;i d=0\n    ;i e,f        ;if(!s){l(      u-1,u+\n    h+2,t-    1,t+w+2){i s=x<t     ||x>t\n    +w;i    t=y<u||           y>    u+h;\n    if(s    ^t&&              m[      y]\n    [x    ]=='#'    ){d++;    if(g    (d\n    )     ==0)    e=x,f=y;    }}if    (d\n    ==    0)r;    }l(u-1,u    +h+2    ,t\n    -1    ,t+w    +2){i s=    x< t    ||\n    x>    t+w;    i t= y<u    ||y>    u+\n    h;    m[y]      [x]= s    &&t?   '!'\n    :s^t    ?'#'                    :'.'\n    ;}if    (d>0)m                  [f][\n    e]=g(2    )?'\\'':'+';for(i j=0;j<(s?\n    1:g(6)        +1);j++)m[g(h)+u][g(w)\n    +t]=s?'@'                 :g(4) ==0?\n    '$':65+g(62)              ;}i main(i\n    argc, const char* argv[]) {srand((i)\n    time(NULL));l(0, H, 0,W)m[y][x]=' ';\n    for(i j=0;j<1000;j++)cave(j==0);l(0,\n    H,0,W) {i c=m[y][x]; putchar(c=='!'?\n    '#':c);if(x==W-1)printf(\"\\n\");}r 0;}reply",
      "If I squeeze the eyes I can read the \"MAZE\". :)Sadly neither version works here with an older clang on OS X. Both variants build fine with 9 warnings each. But the old variant dies with a \"Bus Error: 10\", and the new variant with \"Segmentation fault: 11\". Same with gcc (albeit only 8 warnings.)/editOK, just wrong user input. You gotta feed it a number, and not a \"foobar\" or another random string.reply",
      "Amazing! I read you article in 2012 when the link was https://homepages.cwi.nl/~tromp/maze.html\nI was learning Haskell and Ocaml and wrote my own article in Chinese then https://maskray.me/blog/2012-11-02-perfect-maze-generationNow I should fix the link.reply",
      "Awesome resource. I recently (in the past week) created a maze game. I used Claude (sonnet 4.5) for the most part, but some things, like images, were created with ChatGPT. I may do a blog about it if anyone is interested in the inner-workings and my thought process from concept to vibecoded. I am by no means a game dev, was just curious about what it would take to create unique single solution mazes with some game-like components thrown in, and trying it with the assistance of AI. It turned out somewhat retro. Now go get lost!https://lorelabyrinth.entropicsystems.net/weeklyreply",
      "For anyone interested in this, Jamis Buck's book 'Mazes for Programmers' is a masterpiece of the genre.My personal favorite distinction is between the Recursive Backtracker (which creates long, winding corridors with few dead ends which is great for tower defense games) vs. Prim's Algorithm (which creates lots of short cul-de-sacs which is better for roguelikes). The bias of the algorithm dictates the feel of the game more than the graphics do.reply",
      "Lovely page. Reminds me of the venerable Think Labyrinth (https://www.astrolog.org/labyrnth/algrithm.htm) page, but the live demos add a lot.My favorite maze algorithm is a variant of the growing tree algorithm - each time you carve a cell, add it to a random one of N lists. When choosing a cell to visit, pop the last cell off the first non-empty list. It's considerably faster than the standard tree algorithm, but more importantly, changing N has a dramatic impact on the texture of the maze (compare 1 2 4 8 etc on a decently large maze).reply",
      "Mike Bostock had several very lovely visualizations back on the D3.js site which I can't find. Here's a cool blogpost he wrote: https://bost.ocks.org/mike/algorithms/#maze-generationreply",
      "Related? https://news.ycombinator.com/item?id=7632092 (April 2014)reply",
      "I used Jamis' book extensively to build this AI tool for generating custom mazes of any shape! https://kamens.com/blog/generating-custom-mazes-with-aireply"
    ],
    "link": "http://www.jamisbuck.org/mazes/",
    "first_paragraph": "If you're interested in maze algorithms, I've written a book about the subject: \"Mazes for Programmers\". Check it out!The source code for these demos is freely available at http://github.com/jamis/csmazes.\n  \nRecursive Backtracking\n\n\n\n\n\n\nRecursive Backtracking (Parallel Seeds)\n\n\n\n\n\n\nEller's Algorithm\n\n\n\n\n\n\nKruskal's Algorithm\n\n\n\n\n\n\nPrim's Algorithm\n\n\n\n\n\n\nRecursive Division\n\n\n\n\n\n\n\"Blobby\" Recursive Subdivision Algorithm\n\n    Threshold: \nSmall\nMedium\nLarge\n\n\n\n\n\n\n\n\n\nAldous-Broder Algorithm\n\n\n\n\n\n\nWilson's Algorithm\n\n\n\n\n\n\nHouston's Algorithm\nThis combines Aldous-Broder and Wilson's, to get the best performance of both. Sadly,\n    it is not guaranteed to be uniform like the other two, but it is faster! It runs Aldous-Broder\n    until some minimum number of cells have been visited, and then switches to Wilson's.\n\n\n\n\n\n\n\"Hunt and Kill\" Algorithm\n\n\n\n\n\n\nGrowing Tree Algorithm\n\n\nE.g. random:50, newest:30, oldest:75, middle:100, or any comma-delimited combination of those.\nYou must click \"reset\" be"
  }
]