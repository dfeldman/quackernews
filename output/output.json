[
  {
    "title": "Just Buy Nothing: A fake online store to combat shopping addiction (justbuynothing.com)",
    "points": 54,
    "submitter": "Improvement",
    "submit_time": "2025-08-10T00:12:36 1754784756",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=44851590",
    "comments": [
      "Worked in a mall in the 90s and thought a store that let people 'shop' but not actually pay money for things might be fun/useful.  Or... sort of like a membership.  $10/month, come in to some luxury type store - browse, test out stuff, etc.  Go through a 'checkout' step with your 'store card', just... leave the items in the store as you leave.  For some folks, the leaving and driving away is the 'high', but for a lot of folks, the 'purchase' itself is the high, and it's downhill after that.  I saw so many people buying things they shouldn't buy - and that was over 30 years ago(!).  I know it's only become worse over the years.FWIW - my idea was possibly sort of dumb, but I was a bit of a dumb kid at times... :)reply",
      "I have found that adding things to a todo (Todist) list called \"want-to-buy\" gives me the little endorphin boost / anxiety relief without spending any money. I periodically go through and delete stuff from this list after time has passed, and I'm always glad I didn't go through with the purchases. Rarely I will come across something I truly do still want, and will purchase it.reply",
      "I do this with just an ever expanding notes file. Works!reply",
      "I think this could work if it was more than just a static site with no description page. Give it description pages with gallery, reviews etc.reply",
      "This is like cigarettes and coffee for ex-alcoholics.reply",
      "Does it track my behavior to sell it to advertisers of less whimsical storefronts?reply",
      "This is a pretty good idea but what if we simulated actual store UI such as Amazonreply",
      ">what if we simulated actual store UI such as AmazonA phone call from Amazon lawyers.reply",
      "When people buy, you should send them a postcard to close the dopamine loop.In my experience the majority of compulsive shoppers like getting a box almost as much as whatever is in itreply",
      "So it's like nicotine patches?  Gives you a similar dopamine hit without the wallet hit?reply"
    ],
    "link": "https://justbuynothing.com/",
    "first_paragraph": ""
  },
  {
    "title": "How I code with AI on a budget/free (wuu73.org)",
    "points": 74,
    "submitter": "indigodaddy",
    "submit_time": "2025-08-09T22:27:37 1754778457",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=44850913",
    "comments": [
      "My experience lines up with the article. The agentic stuff only works with the biggest models. (Well, \"works\"... OpenAI Codex took 200 requests with o4-mini to change like 3 lines of code...)For simple changes I actually found smaller models better because they're so much faster. So I shifted my focus from \"best model\" to \"stupidest I can get away with\".I've been pushing that idea even further. If you give up on agentic, you can go surgical. At that point even 100x smaller models can handle it. Just tell it what to do and let it give you the diff.Also I found the \"fumble around my filesystem\" approach stupid for my scale, where I can mostly fit the whole codebase into the context. So I just dump src/ into the prompt. (Other people's projects are a lot more boilerplatey so I'm testing ultra cheap models like gpt-oss-20b for code search. For that, I think you can go even cheaper...)Patent pending.reply",
      "Aider as a non-agentic coding tool strikes a nice balance on the efficiency vs effectiveness front. Using tree-sitter to create a repo map of the repository means less filesystem digging. No MCP, but shell commands mean it can use utilities I myself am familiar with. Combined with Cerebras as a provider, the turnaround on prompts is instant; I can stay involved rather than waiting on multiple rounds of tool calls. It's my go-to for smaller scale projects.reply",
      "To the OP: I highly recommend you look into Continue.dev and ollama/lmstudio and running models on your own. Some of them are really good at autocomplete-style suggestions while others (like gpt-oss) can reason and use tools.It's my goto copilot.reply",
      "Same! I\u2019ve been using Continue in VSCode and found most of the bigger Qwen models plus gpt-oss-120b to be great in agentic mode!reply",
      "If you're looking for free API access, Google offers access to Gemini for free, including for gemini-2.5-pro with thinking turned on. The limit is... quite high, as I'm running some benchmarking and haven't hit the limit yet.Open weight models like DeepSeek R1 and GPT-OSS are also made available with free API access from various inference providers and hardware manufacturers.reply",
      "Gemini 2.5 pro free limit is 100 requests per day.https://ai.google.dev/gemini-api/docs/rate-limitsreply",
      "Doesn't it swap to a lower power model after that?reply",
      "I'm assuming it isn't sensitive for your purposes, but note that Google will train on these interactions, but not if you pay.reply",
      "I wonder how much energy this is wasting.reply",
      "Untradable carbon tax (or carbon price for people who hate the T word) is needed.reply"
    ],
    "link": "https://wuu73.org/blog/aiguide1.html",
    "first_paragraph": ""
  },
  {
    "title": "Abusing Entra OAuth for fun and access to internal Microsoft applications (eye.security)",
    "points": 82,
    "submitter": "the1bernard",
    "submit_time": "2025-08-09T21:59:43 1754776783",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=44850681",
    "comments": [
      "ohhhh the gifts multi-tenant app authorization keeps giving!(laid off) Microsoft PM here that worked on the patch described as a result of the research from Wiz.One correction I\u2019d like to suggest to the article: the guidance given is to check either the \u201ciss\u201d or \u201ctid\u201d claim when authorizing multi-tenant apps.The actual recommended guidance we provided is slightly more involved. There is a chance that when only validating the tenant, any service principal could be granted authorized access.You should always validate the subject in addition to validating the tenant for the token being authorized. One method for this would be to validate the token using a combined key (for example, tid+oid) or perform checks on both the tenant and subject before authorizing access. More info can be found here:https://learn.microsoft.com/en-us/entra/identity-platform/cl...reply",
      "Assume every token is forged. Secure by default. Even if it wastes cpu, validate each and every field. Signatures only work if verified. While you're at it, validate it against your identity database as well. Double check, triple check if you must. This is what I taught my devs.Tenant, User, Group, Resource - validate it all before allowing it through.reply",
      "Also knowing the difference between authentication and authorization is crucial and should not be forgotten.reply",
      "OAuth is frequently marketed as \"more secure\".  But implementations often confuse authentication with authorization, resulting in problems like this.reply",
      "I just say auth. You decide which one I mean.reply",
      "Move to the cloud they said. It will be more secure then your intranet they said. Only fools pay for their own Ops team they said.I\u2019m so old and dumb that I don\u2019t even understand why an app for internal Microsoft use is even accesible from outside its network.reply",
      "The last decade has seen an increase push in what Google started calling \"Zero Trust\"[0] and dropping VPNs entirely. The issue being that once someone got into a VPN it was much, much harder to prevent them from accessing important data.So everything \"internal\" is now also external and required to have its own layer of permissions and the like, making it much harder for, e.g. the article, to use one exploit to access another service.[0] https://cloud.google.com/learn/what-is-zero-trustreply",
      "Does having a VPN/intranet preclude zero trust? It seems you could do both with the private network just being an added layer of security.reply",
      "It doesn't, but from my perspective the thinking behind zero trust is partly to stop treating networking as a layer of security. Which makes sense to me - the larger the network grows, the harder to know all its entry-points and the transitive reach of those.reply",
      "The zero trust architechture implies (read: requires) that authentication occurs at every layer. Token reuse constitutes a replay attack that mandatory authentication is supposed to thwart. Bypass it and the system's security profile reverts back to perimeter security, with the added disadvantage of that perimeter being outside your org's control.reply"
    ],
    "link": "https://research.eye.security/consent-and-compromise/",
    "first_paragraph": "This blog is about how I got access to over 22 internal Microsoft services and how you might be vulnerable too.After my last talk at the 38C3 conference in Hamburg, this was the top comment on YouTube.Well, this story definitely falls in the category \u201csomeone stumbling around finding horrifying vulnerabilities\u201d. Although this time I was not even having issues, I was just distracted from a boring task.You see, I was writing some documentation the other day, when my Eye fell on one of those aka.ms links. For those of you who don\u2019t know, aka.ms is the URL shortener service used by Microsoft.My mind started to wonder. Where would you end up if you would just visit https://aka.ms, so without any shortener link included?A login screen. This is probably where Microsoft employees manage their links. I did wonder though, what would happen if I simply tried logging in here myself with my own Microsoft account? Surely, that would not work, right?Of course not! You need an account in the Microsoft"
  },
  {
    "title": "GPTs and Feeling Left Behind (whynothugo.nl)",
    "points": 51,
    "submitter": "Bogdanp",
    "submit_time": "2025-08-09T23:07:58 1754780878",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=44851214",
    "comments": [
      "I'm completely equally lost the other way.I've went through multiple phases of LLM usage for development.GPT3.5 era: wow this is amazing, oh. everything is hallucinated. not actually as useful as I first thoughtGPT4 era: very helpful as stackoverflow on steroids.Claude 3.5 Sonnet: have it open pretty much all the time, constantly asking questions and getting it to generate simple code (in the web UI) when it goes down actually feels very old school googling stuff. Tried a lot of in IDE AI \"chat\" stuff but hugely underwhelmed.Now: rarely open IDE as I can do (nearly) absolutely everything in Claude Code. I do have to refactor stuff every so often \"manually\", but this is more for my sanity and understanding of the codebase..To give an example of a task I got Claude code to do today in a few minutes which would take me hours. Had a janky looking old admin panel in bootstrap styles that I wanted to make look nice. Told Claude code to fetch the marketing site for the project. Got it to pull CSS, logos, fonts from there using curl and apply similar styling to the admin panel project. Within 10 mins it was looking far, far better than I would have ever got it looking (at least without a designers help). Then got it to go through the entire project (dozens of screens) and update \"explanation\" copy - most of which was TODO placeholders to explain what everything did properly. I then got it to add an e2e test suite to the core flows.This took less than an hour while I was watching TV. I would have almost certainly _never_ got around to this before. I'd been meaning to do all this and I always sigh when I go into this panel at how clunky it all is and hard to explain to people.reply",
      "You're (they're?) not alone. This mirrors every experience I've had trying to give them a chance. I worry that I'm just speaking another language at this point.reply",
      "One blogpost I found on HN completely leveled up how I use LLMs for coding: https://harper.blog/2025/02/16/my-llm-codegen-workflow-atm/Having the AI ask me questions and think about the PRD/spec ultimately made me a better system designer.reply",
      "> This is working well NOW, it will probably not work in 2 weeks, or it will work twice as well. \u00af\\_(\u30c4)_/\u00afThis all feels like spinning the roulette wheel. I sometimes wonder if AI proponents are just gamblers who had the unfortunate luck of winning the first few prompts.reply",
      "I kept hearing about Claude Code for a while and never really tried it until a week ago. I used it to prototype some Mac app ideas and I quickly realized how useful it was at getting prototypes up and running very, very quickly, like within minutes. It saves so much time with boilerplate code that I would've had to type out by hand and have done hundreds of times before.With my experience, I wonder what the author of this blog post has tried to do to complete a task as that might make a difference on why they couldn't get much use out of it. Maybe other posters can chime in on how big of a difference programming language and size of project can make. I did find that it was able to glean how I had architected an app and it was able to give feedback on potential refactors, although I didn't ask it to go that far.Prior to trying out Claude Code, I had only used ChatGPT and DeepSeek to post general questions on how to use APIs and frameworks and asking for short snippets of code like functions to do text parsing with regexes, so to be honest I was very surprised at what the state of the art could actually do, at least for my projects.reply",
      "I have a degree in CS from MIT and did professional software engineering from 2004 - 2020.I recently started a company in another field and haven\u2019t done any real development for about 4 years.Earlier this summer I took a vacation and decided to start a small software hobby project specific to my industry. I decided to try out Cursor for the first time.I found it incredibly helpful at saving time implementing all the bullshit involved in starting a new code base - setting up a build system, looking up libraries and APIs, implementing a framework for configuration and I/O, etc.Yes, I still had to do some of the hard parts myself, and (probably most relevant) I still had to understand the code it was writing and correct it when it went down the wrong direction. I literally just told Cursor \u201cNo, why do it that way when you could do it much simpler by X\u201d, and usually it fixed it.A few times, after writing a bunch of code myself, I compiled the project for the first time in a while and (as one does) ran into a forest of inscrutable C++ template errors. Rather than spend my time scrolling through all of them I just told cursor \u201cfix the compile errors\u201d, and sure enough, it did it.Another example - you can tell it things like \u201cimplement comparison operators for this class\u201d, and it\u2019s done in 5 seconds.As the project got more complicated, I found it super useful to write tests for behaviors I wanted, and just tell it \u201cmake this test pass\u201d. It really does a decent job of understanding the codebase and adding onto it like a junior developer would.Using an IDE that gives it access to your whole codebase (including build system and tests) is key. Using ChatGPT standalone and pasting stuff in is not where the value is.It\u2019s nowhere near able to do the entire project from scratch, but it saved me from a bunch of tedious work that I don\u2019t enjoy anyway.Seems valuable enough to me!reply",
      "Last summer I came back to software after about 12 years away and I pretty much had an identical experience to you with using AI as a helper to come back. I've now spent the last 6 months coding as much as I can in between consulting gigs. I'm not sure if I would have been able to get caught up so quickly without AI.I haven't had this much fun programming since I was at university hacking away on sun workstations, but admittedly I only write about 10% of the code myself these days.I'm currently getting Claude Code to pair program with GPT-5 and they delegate the file edits to Gemini Flash. It's pretty cool.reply",
      "I think what people are missing is that they work sometimes and sometimes they don't work.People think \"Oh,  it works better when somebody else does it\" or \"There must be some model that does better than the one I am using\" or \"If I knew how to prompt better I'd get better results\" or \"There must be some other agentic IDE which is better than the one I am using.\"All those things might be true but they just change the odds,  they don't change the fact that it works sometimes and fails other times.For instance I asked an agent to write me a screen to display some well-typed data.  It came up with something great right away that was missing some fields and had some inconsistent formatting but it fixed all those problems when I mentioned them -- all speaking the language of product managers and end users.  The code quality was just great,  as good as if I wrote it,  maybe better.Plenty of times it doesn't work out like that.I was working on some code where I didn't really understand the typescript types and fed it the crazy error messages I was getting and it made a try to understand them and didn't really,  I used it as a \"rubber duck\" over the course of a day or two and working with it I eventually came to understand what was wrong and how to fix and I got into a place that I like and when there is an error I can understand it and it can understand it too.Sometimes it writes something that doesn't typecheck and I tell it to run tsc and fix the errors and sometimes it does a job I am proud of and other times it adds lame typeguards like   if (x && typeof x === \"object\") x.someMethod()\n\nGive it essentially the same problem,  say writing tests in Java,  and it might take very different approaches.  One time it will use the same dependency injection framework used in other tests to inject mocks into private fields,  other times it will write some a helper method to inject the mocks into private fields with introspection directly.You might be able to somewhat tame this randomness with better techniques but sometimes it works and sometimes it doesn't and if I just told you about the good times or just told you about the bad times it would be a very different story.reply",
      ">I was working on some code where I didn't really understand the typescript types and fed it the crazy error messages I was getting and it made a try to understand them and didn't really, I used it as a \"rubber duck\" over the course of a day or two and working with it I eventually came to understand what was wrong and how to fix and I got into a place that I like and when there is an error I can understand it and it can understand it too.I have to wonder if you tried a simple google search and read through some docs if you couldn't have figured this out quicker than trying to coax a result out of the LLM.reply",
      "My (not GP) intuitive answer would be hell no. Typescript messages are pretty hard to google and even parse manually and the LLM suggesting multiple approaches and ways to think about the problem does seem useful. It sometimes uncovers unknown unknowns you might never find otherwise.I have had cases in which a web search and some good old fashioned thinking have yielded  better results than using an LLM, but on average I\u2019m pretty sure the LLM has the edge.reply"
    ],
    "link": "https://whynothugo.nl/journal/2025/08/06/gpts-and-feeling-left-behind/",
    "first_paragraph": "Every time that I read some blog post about \u201ccoding with AI\u201d, or how cool new\nmodels write entire libraries by themselves, I feel like I\u2019m lagging behind,\nlike I\u2019m missing out on some big, useful tool, and my skills are about to become\nobsolete very soon.So I try different models and tools, and it\u2019s all incredibly underwhelming. It\u2019s\nhonestly hard to believe that people get work done using these tools, because I\ncan spend a few hours on them (without getting even close to finishing the task\nat hand) and realise that I could have done it myself in 25 minutes.I tell myself \u201clearning to use Vim took a long time, but then it paid off\u201d\neventually. But I could (slowly) write text with Vim the first day. I can spend\nan entire day with a GPT and produce nothing of value.GPTs work great for finding the exact word to complete a sentence. They\u2019re\nsurprisingly good at finding the exact type annotation for a Python function.\nThey can find nuanced bugs in a single function which I copy-paste into th"
  },
  {
    "title": "Show HN: The current sky at your approximate location, as a CSS gradient (dlazaro.ca)",
    "points": 553,
    "submitter": "dlazaro",
    "submit_time": "2025-08-09T13:25:16 1754745916",
    "num_comments": 112,
    "comments_url": "https://news.ycombinator.com/item?id=44846281",
    "comments": [
      "Awesome. I remember much earlier in my career I was working on a 3D turn-by-turn navigation software, and one of my tasks was to draw the sky in the background. The more senior guy on the team said, just draw a blue rectangle during the day and a dark gray one at night and call it job done. Of course, I had to do it the hard way, so I looked up the relevant literature on sky rendering based on the environment, latitude, longitude, time of day and so on, which at the time was Preetham[1] (\"A Practical Analytic Model for Daylight\"), and built a fully realistic sky model for the software. I even added prominent stars based on a hard-coded ephemeris table. It was quite fast, too.Well, the higher ups of course hated it, they were confused as to why the horizon would get hazy, yellowish, and so on. \"Our competitors' skies are blue!\" They didn't like \"Use your eyes and look outside\" as an answer.Eventually, I was told to scrap it and just draw a blue rectangle :(All that to say, nice job on the site!1: https://courses.cs.duke.edu/cps124/fall01/resources/p91-pree...reply",
      "This is why specifications are important, and why design is important.The reality is that we have certain conventions that are immediately understandable, and that too much visual complexity results in confusion rather than clarity.If the sky is hazy white when I expect it to be blue, I'm confused as to whether it's the sky or if the map is still loading. It's adding cognitive complexity for no reason. Stars similarly serve no functional purpose at night.What you built sounds great for an actual planetary view like Google Earth. And it sounds fun to build. But it's an anti-feature for a navigation view. When you're navigating, simplicity and clarity are paramount. Not realism.reply",
      "> This is why specifications are important, and why design is important.Also the phrase \"know your audience\". No sense in casting pearls before the swine.reply",
      "Though sometimes the higher ups might not be the same as (or understand) the actual audience.In this case the higher ups may have been confused due to, say, looking at the app while indoors (and from the perspective of \"let's judge this developer's work\"), while the actual users would see it in a vehicle alongside the real sky (and from the perspective of \"let's see how easy this is to match up with reality\").reply",
      "Ah, I see the confusion. You think the users are the dev's audience! /sreply",
      "I suppose this is the lesson OP learned!reply",
      "Oh come now. You are being no fun.reply",
      "A past coworker who worked on Cobalt[1] told me that they spent entirely too much time implementing stars in the sky of the game with some amount of real(ish) star system physics behind them.I can understand people removing polish things like that if there are usability concerns, but those small things add up to a lot in an end product and are a joy to find and explore.1: https://en.wikipedia.org/wiki/Cobalt_(video_game)reply",
      "The last thing you want is to receive a message from Neil Degrasse Tyson about how wrong your sky washttps://duckduckgo.com/?t=ffab&q=neil+degrasse+tyson+gives+j...reply",
      "Cobalt was a really interesting game, too bad it never got any famereply"
    ],
    "link": "https://sky.dlazaro.ca",
    "first_paragraph": ""
  },
  {
    "title": "My Lethal Trifecta talk at the Bay Area AI Security Meetup (simonwillison.net)",
    "points": 268,
    "submitter": "vismit2000",
    "submit_time": "2025-08-09T14:47:38 1754750858",
    "num_comments": 84,
    "comments_url": "https://news.ycombinator.com/item?id=44846922",
    "comments": [
      "The key thing, it seems to me, is that as a starting point, if an LLM is allowed to read a field that is under even partial control by entity X, then the agent calling the LLM must be assumed unless you can prove otherwise to be under control of entity X, and so the agents privileges must be restricted to the intersection of their current privileges and the privileges of entity X.So if you read a support ticket by an anonymous user, you can't in this context allow actions you wouldn't allow an anonymous user to take. If you read an e-mail by person X, and another email by person Y, you can't let the agent take actions that you wouldn't allow both X and Y to take.If you then want to avoid being tied down that much, you need to isolate, delegate, and filter:- Have a sub-agent read the data and extract a structured request for information or list of requested actions. This agent must be treated as an agent of the user that submitted the data.- Have a filter, that does not use AI, that filters the request and applies security policies that rejects all requests that the sending side are not authorised to make. No data that can is sufficient to contain instructions can be allowed to pass through this without being rendered inert, e.g. by being encrypted or similar, so the reading side is limited to moving the data around, not interpret it. It needs to be strictly structured. E.g. the sender might request a list of information; the filter needs to validate that against access control rules for the sender.- Have the main agent operate on those instructions alone.All interaction with the outside world needs to be done by the agent acting on behalf of the sender/untrusted user, only on data that has passed through that middle layer.This is really back to the original concept of agents acting on behalf of both (or multiple) sides of an interaction, and negotiating.But what we need to accept is that this negotiation can't involve the exchange arbitrary natural language.reply",
      "> if an LLM is allowed to read a field that is under even partial control by entity X, then the agent calling the LLM must be assumed unless you can prove otherwise to be under control of entity XThat's exactly right, great way of putting it.reply",
      "I\u2019m one of main devs of GitHub MCP (opinions my own) and I\u2019ve really enjoyed your talks on the subject.  I hope we can chat in-person some time.I am personally very happy for our GH MCP Server to be your example. The conversations you are inspiring are extremely important. Given the GH MCP server can trivially be locked down to mitigate the risks of the lethal trifecta I also hope people realise that and don\u2019t think they cannot use it safely.\u201cUnless you can prove otherwise\u201d is definitely the load bearing phrase above.I will say The Lethal Trifecta is a very catchy name, but it also directly overlaps with the trifecta of utility and you can\u2019t simply exclude any of the three without negatively impacting utility like all security/privacy trade-offs. Awareness of the risks is incredibly important, but not everyone should/would choose complete caution. An example being working on a private codebase, and wanting GH MCP to search for an issue from a lib you use that has a bug. You risk prompt injection by doing so, but your agent cannot easily complete your tasks otherwise (without manual intervention). It\u2019s not clear to me that all users should choose to make the manual step to avoid the potential risk. I expect the specific user context matters a lot here.User comfort level must depend on the level of autonomy/oversight of the agentic tool in question as well as personal risk profile etc.Here are two contrasting uses of GH MCP with wildly different risk profiles:- GitHub Coding Agent has high autonomy (although good oversight) and it natively uses the GH MCP in read only mode, with an individual repo scoped token and additional mitigations. The risks are too high otherwise, and finding out after the fact is too risky, so it is extremely locked down by default.In contrast, by if you install the GH MCP into copilot agent mode in VS Code with default settings, you are technically vulnerable to lethal trifecta as you mention but the user can scrutinise effectively in real time, with user in the loop on every write action by default etc.I know I personally feel comfortable using a less restrictive token in the VS Code context and simply inspecting tool call payloads etc. and maintaining the human in the loop setting.Users running full yolo mode/fully autonomous contexts should definitely heed your words and lock it down.As it happens I am also working (at a variety of levels in the agent/MCP stack) on some mitigations for data privacy, token scanning etc. because we clearly all need  to do better while at the same time trying to preserve more utility than complete avoidance of the lethal trifecta can achieve.Anyway, as I said above I found your talks super interesting and insightful and I am still reflecting on what this means for MCP.Thank you!reply",
      "I've been thinking a lot about this recently. I've started running Claude Code and GitHub Copilot Agent and Codex-CLI in YOLO mode (no approvals needed) a bit recently because wow it's so much more productive, but I'm very aware that doing so opens me up to very real prompt injection risks.So I've been trying to figure out the best shape for running that. I think it comes down to running in a fresh container with source code that I don't mind being stolen (easy for me, most of my stuff is open source) and being very careful about exposing secrets to it.I'm comfortable sharing a secret with a spending limit: an OpenAI token that can only spend up to $25 is something I'm willing risking to an insecured coding agent.Likewise, for Fly.io experiments I created a dedicated scratchpad \"Organization\" with a spending limit - that way I can have Claude Code fire up Fly Machines to test out different configuration ideas without any risk of it spending money or damaging my production infrastructure.The moment code theft genuinely matters things get a lot harder. OpenAI's hosted Codex product has a way to lock down internet access to just a specific list of domains to help avoid exfiltration which is sensible but somewhat risky (thanks to open proxy risks etc).I'm taking the position that if we assume that malicious tokens can drive the coding agent to do anything, what's an environment we can run in where the damage is low enough that I don't mind the risk?reply",
      "Agreed on all points.What should one make of the orthogonal risk that the pretraining data of the LLM could leak corporate secrets under some rare condition even without direct input from the outside world?  I doubt we have rigorous ways to prove that training data are safe from such an attack vector even if we trained our own LLMs.  Doesn't that mean that running in-house agents on sensitive data should be isolated from any interactions with the outside world?So in the end we could have LLMs run in containers using shareable corporate data that address outside world queries/data, and LLMs run in complete isolation to handle sensitive corporate data.  But do we need humans to connect/update the two types of environments or is there a mathematically safe way to bridge the two?reply",
      "If you fine-tune a model on corporate data (and you can actually get that to work, I've seen very few success stories there) then yes, a prompt injection attack against that model could exfiltrate sensitive data too.Something I've been thinking about recently is a sort of air-gapped mechanism: an end user gets to run an LLM system that has no access to the outside world at all (like how ChatGPT Code Interpreter works) but IS able to access the data they've provided to it, and they can grant it access to multiple GBs of data for use with its code execution tools.That cuts off the exfiltration vector leg of the trifecta while allowing complex operations to be performed against sensitive data.reply",
      "In the case of the access to private data, I think that the concern I mentioned is not fully alleviated by simply cutting off exposure to untrusted content.  Although the latter avoids a prompt injection attack, the company is still vulnerable to the possibility of a poisoned model that can read the sensitive corporate dataset and decide to contact https://x.y.z/data-leak if there was a hint for such a plan in the pretraining dataset.So in your trifecta example, one can cut off private data and have outside users interact with untrusted contact, or one can cut off the ability to communicate externally in order to analyze internal datasets.  However, I believe that only cutting off the exposure to untrusted content in the context seems to have some residual risk if the LLM itself was pretrained on untrusted data.  And I don't know of any ways to fully derisk the training data.Think of OpenAI/DeepMind/Anthropic/xAI who train their own models from scratch: I assume they would they would not trust their own sensitive documents to any of their own LLM that can communicate to the outside world, even if the input to the LLM is controlled by trained users in their own company (but the decision to reach the internet is autonomous).  Worse yet, in a truly agentic system anything coming out of an LLM is not fully trusted, so any chain of agents is considered as having untrusted data as inputs, even more so a reason to avoid allowing communications.I like your air-gapped mechanism as it seems like the only workable solution for analyzing sensitive data with the current technologies.  It also suggests that companies will tend to expand their internal/proprietary infrastructure as they use agentic LLMs, even if the LLMs themselves might eventually become a shared (and hopefully secured) resource.  This could be a little different trend than the earlier wave that moved lots of functionality to the cloud.reply",
      "need taintllmreply",
      ">Have a sub-agent read the data and extract a structured request for information or list of requested actions. This agent must be treated as an agent of the user that submitted the data.That just means the attacker has to learn how to escape. No different than escaping VMs or jails.  You have to assume that the  agent is compromised, because it has untrusted content, and therefore its output is also untrusted. Which means you\u2019re still giving untrusted content to the \u201cparent\u201d AI. \nI feel like reading Neal Asher\u2019s sci-fi and dystopian future novels is good preparation for this.reply",
      "> Which means you\u2019re still giving untrusted content to the \u201cparent\u201d AIHence the need for a security boundary where you parse, validate, and filter the data without using AI before any of that data goes to the \"parent\".That this data must be treated as untrusted is exactly the point. You need to treat it the same as you would if the person submitting the data was given direct API access to submit requests to the \"parent\" AI.And that means e.g. you can't allow through fields you can't sanitise (and that means strict length restrictions and format restrictions - as Simon points out, trying to validate that e.g. a large unconstrained text field doesn't contain a prompt injection attack is not likely to work; you're then basically trying to solve the halting problem, because the attacker can adapt to failure)So you need the narrowest possible API between the two agents, and one that you treat as if hackers can get direct access to, because odds are they can.And, yes, you need to treat the first agent like that in terms of hardening against escapes as well. Ideally put them in a DMZ rather than inside your regular network, for example.reply"
    ],
    "link": "https://simonwillison.net/2025/Aug/9/bay-area-ai/",
    "first_paragraph": "9th August 2025I gave a talk on Wednesday at the Bay Area AI Security Meetup about prompt injection, the lethal trifecta and the challenges of securing systems that use MCP. It wasn\u2019t recorded but I\u2019ve created an annotated presentation with my slides and detailed notes on everything I talked about.Also included: some notes on my weird hobby of trying to coin or amplify new terms of art.Minutes before I went on stage an audience member asked me if there would be any pelicans in my talk, and I panicked because there were not! So I dropped in this photograph I took a few days ago in Half Moon Bay as the background for my title slide.Let\u2019s start by reviewing prompt injection\u2014SQL injection with prompts. It\u2019s called that because the root cause is the original sin of AI engineering: we build these systems through string concatenation, by gluing together trusted instructions and untrusted input.Anyone who works in security will know why this is a bad idea! It\u2019s the root cause of SQL injection,"
  },
  {
    "title": "Don't \u201clet it crash\u201d, let it heal (zachdaniel.dev)",
    "points": 16,
    "submitter": "ahamez",
    "submit_time": "2025-08-06T12:08:22 1754482102",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.zachdaniel.dev/p/elixir-misconceptions-1",
    "first_paragraph": ""
  },
  {
    "title": "A CT scanner reveals surprises inside the 386 processor's ceramic package (righto.com)",
    "points": 165,
    "submitter": "robin_reala",
    "submit_time": "2025-08-09T17:17:07 1754759827",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=44848293",
    "comments": [
      "A bit of a trip down memory lane for me. I performed an analysis of the thermo-mechanical cyclic fatigue in later packages using detailed CAD, FEA and empirical tests. A lot of work went into finding it wasn\u2019t a big deal for the most part. Still, I don\u2019t recommend that museums power cycle old PCs daily\u2026reply",
      "Knowing nothing about how survival/durability testing is done in VLSI: how did you do the empirical tests?For example, I know that thermal samples for the Pentium 5-era Xeon (Jayhawk) were produced, but I'd always wondered Intel went from the dummy to realizing \"oh, shit, this is going to be way too hot in the long run.\"reply",
      "I can\u2019t really speak to the thermals other than as an input to my work. I was narrowly focused on the cyclic loading based on the temperature gradients (etc.) I was given.reply",
      "Went to a computer fair circa, gosh, 1989?  My Dad bought me a 386 DX 25MHz with like 4MB of RAM and a whopping 40MB hard drive.  This was a remarkable upgrade from the Tandy 286 16MHz that I was using.  The 386 we got was not the standard 20MHz or 33MHz, 25MHz was some kind of hype thing, as I recall.  The 33MHz was the bomb, but of course that cost more bones $$$$.  The computer fairs were cool.reply",
      "For 89 that's screamin! I remember early 90's getting a 50 mhz 8mb Gateway and it was amazing. Even just MS Paint and MS Word kept my sister and I plenty entertained making up stories and pictures to go along with them. Then I found MS DOS and QBasic and here I am posting on hacker news on a Saturday afternoon.reply",
      "Author here for all your CT scanning questions :-)reply",
      "Genuine question: the website doesn't work in Russia. Did you restrict the access or is it my ISP doing that? Someone tries to prevent me from studying of very niche info on ancient Intel CPUs. Thanks! P.S. Big fan of your work!reply",
      "I did find that, while running a financial startup, I was able to significantly reduce attacks on the server by disabling access from Russia and China. Not saying that's happening here, just my experience. That was a while ago so I'm sure things have changed since then.reply",
      "Thanks for your reply! I hope this is the real reason of blocking. If that's not the case, that's at least not effective. Less effective than idk placing a banner in the header or whatever.I mean I eventually read the article. Sorry for that. But we're at \"Hacker News\", sporting hackers ethics, aren't we?reply",
      "Some smaller sites ban ips from countries that continually try to hack into your server or just make a ton of requests, it happens to be that traffic is often from Russia and China. Could just be that.reply"
    ],
    "link": "https://www.righto.com/2025/08/intel-386-package-ct-scan.html",
    "first_paragraph": "Computer history, restoring vintage computers, IC reverse engineering, and whateverIntel released the 386 processor in 1985, the first 32-bit chip in the x86 line.\nThis chip was packaged in a ceramic square with 132 gold-plated pins protruding from the underside, fitting into\na socket on the motherboard.\nWhile this package may seem boring, a lot more is going on inside it than you might expect.\nLumafield performed a 3-D CT scan of the chip for me, revealing six layers of complex\nwiring hidden inside the ceramic package.\nMoreover, the chip has nearly invisible metal wires connected to the sides of the package, the spikes below.\nThe scan also revealed that the 386 has two separate power and ground networks: one for I/O and one for the CPU's logic.A CT scan of the 386 package. The ceramic package doesn't show up in this image, but it encloses the spiky wires.The package, below, provides no hint of the complex wiring embedded inside the ceramic.\nThe silicon die is normally not visible, but"
  },
  {
    "title": "GPT-5: Overdue, overhyped and underwhelming. And that's not the worst of it (garymarcus.substack.com)",
    "points": 164,
    "submitter": "kgwgk",
    "submit_time": "2025-08-10T00:06:54 1754784414",
    "num_comments": 112,
    "comments_url": "https://news.ycombinator.com/item?id=44851557",
    "comments": [
      "This is a genre of article I find particularly annoying. Instead of writing an essay on why he personally thinks GPT-5 is bad based on his own analysis, the author just gathers up a bunch of social media reactions and tells us about them, characterizing every criticism as \u201cdevastating\u201d or a \u201cslam\u201d, and then hopes that the combined weight of these overtorqued summaries will convince us to see things his way.It\u2019s both too slanted to be journalism, but not original enough to be analysis.reply",
      "For some reason AI seems to bring out articles that seem to fundamentally lack curiosity - opting instead for gleeful mockery and scorn. I like AI, but I'll happily read thoughtful articles from people who disagree. But not this. This article has no value other than to dunk on the opposition.I tend to think HN's moderation is OK, but I think these sorts of low-curiosity articles need to be off the front page.reply",
      "> For some reason AI seems to bring out articles that seem to fundamentally lack curiosity - opting instead for gleeful mockery and scornI think its broader to all tech. It all started in 2016 after it was deemed that tech, especially social media, had helped sway the election. Since then a lot of things became political that weren't in the past and tech got swept up w/ that. And unfortunately AI has its haters despite the fact that it's objectively the fastest growing most exciting technology in the last 50 years. Instead they're dissecting some CEOs shitposts.Fast forward to today, pretty much everything is political. Take this banger from NY Times:> Mr. Kennedy has singled out Froot Loops as an example of a product with too many ingredients. In an interview with MSNBC on Nov. 6, he questioned the overall ingredient count: \u201cWhy do we have Froot Loops in this country that have 18 or 19 ingredients and you go to Canada and it has two or three?\u201d Mr. Kennedy asked.> He was wrong on the ingredient count, they are roughly the same. But the Canadian version does have natural colorings made from blueberries and carrots while the U.S. product contains red dye 40, yellow 5 and blue 1 as well as Butylated hydroxytoluene, or BHT, a lab-made chemical that is used \u201cfor freshness,\u201d according to the ingredient label.No self-awareness.https://archive.is/dT2qK#selection-975.0-996.0reply",
      "I don\u2019t like AI and I think this type of article is very boring. Imagine having one of the most interesting technological developments of the last 50 years unfolding before your eyes and resort to reposting tweet fragments\u2026reply",
      "> opting instead for gleeful mockery and scornThis is well earned by the likes of OpenAI that is trying to convince everyone they need trillions of dollars to build fabs to build super genius AIs. These super genius AIs will replace everyone (except billionaires) and act as magic money printers (for billionaires).Meanwhile their super genius precursor AIs make up shit and can't count letters in words while being laughably sycophantic.There's no need to defend poor innocent megacorps trying to usher in a techno-feudal dystopia.reply",
      "Gary Marcus tends to have pretty shallow analysis or points.His takes often remind me of Jim Cramer\u2019s stock analysis \u2014 to the point I\u2019d be willing to bet on the side of a \u201creverse Gary Marcus\u201d.reply",
      "You'd take the other side of a reverse Gary Marcus? So you'd take Gary Marcus' side?reply",
      "Fixed, thanks.reply",
      "> Gary Marcus always, always says AI doesn't actually work - it's his whole thing. If he's posted a correct argument it's a coincidence.https://news.ycombinator.com/item?id=44278811I think you're absolutely right about this being a wider problem though.reply",
      "I think it's a broad problem across every aspect of life - it has gotten increasingly more difficult to find genuine takes. Most people online seem to be just relaying a version of someone else's take and we end up with unnecessarily loud and high volume shallow content.reply"
    ],
    "link": "https://garymarcus.substack.com/p/gpt-5-overdue-overhyped-and-underwhelming",
    "first_paragraph": ""
  },
  {
    "title": "Ch.at \u2013 a lightweight LLM chat service accessible through HTTP, SSH, DNS and API (ch.at)",
    "points": 84,
    "submitter": "ownlife",
    "submit_time": "2025-08-09T18:59:37 1754765977",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=44849129",
    "comments": [
      "Author here, was a bit surprised to see this here. I thought there needed to be a good zero-JS LLM site for computer people, and we thought it would be fun to add various other protocols. The short domain hack of \"ch.at\" was exciting because it felt like the natural domain for such a service.It has not been expensive to operate so far. If it ever changes we can think about rate limiting it.We used GPT4o because it seemed like a decent general default model. Considering adding an openrouter interface to a smorgasbord of additional LLMS.One day, on a plane with WiFi before paying, I noticed that DNS queries were still allowed and thought it would be nice to chat with an LLM over it.We are not logging anything but OpenAI must be...reply",
      "It seems like the only internet protocols they didn't implement were the ones designed for chat. How could they forget about IRC, XMPP and SIP?reply",
      "These can definitely be addedreply",
      "This is because when an account generates text in a chatroom it is generally referred to as a \"bot\".reply",
      "I see this is using GPT4o, any plans for something more sustainable?\nWould be interesting to see an https://openfreemap.org for LLMs.Perhaps via an RNN like in https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2Or even just leverage huggingface gradio spaces? (most are Gradio apps that expose APIs https://www.gradio.app/guides/view-api-page)reply",
      "I wonder if a 1B model could be close to free to host. That's an eventuality, but I wonder how long it'll take for that to be real.reply",
      "I\u2019m planning to deploy a 1B model, feed it all the documents I\u2019ve ever written, host it on a $149 mini-PC in my bedroom, and enable you to chat with it.I\u2019ve released similar projects before.I\u2019ll drop a post about my plans in the coming days and I\u2019ll build and document it about two weeks later if there\u2019s enough interest.joeldare.comreply",
      "Sounds cool!reply",
      "A 1B model at 2-bit quantization is about the size of the average web page anymore. With some WebGPU support you could run such a model in a browser.I'm half joking. Web pages are ludicrously fat these days.reply",
      "These are the kinds of sites/tools that make me excited/optimistic about the AI/LLM future in generalreply"
    ],
    "link": "https://ch.at/",
    "first_paragraph": ""
  },
  {
    "title": "OpenFreeMap survived 100k requests per second (hyperknot.com)",
    "points": 369,
    "submitter": "hyperknot",
    "submit_time": "2025-08-09T13:31:30 1754746290",
    "num_comments": 74,
    "comments_url": "https://news.ycombinator.com/item?id=44846318",
    "comments": [
      "> I believe what is happening is that those images are being drawn by some script-kiddies. If I understand correctly, the website limited everyone to 1 pixel per 30 seconds, so I guess everyone was just scripting Puppeteer/Chromium to start a new browser, click a pixel, and close the browser, possibly with IP address rotation, but maybe that wasn't even needed.I think you perhaps underestimate just how big of a thing this became basically overnight. I mentioned a drawing over my house to a few people and literally everyone instantly knew what I meant without even saying the website. People love /r/place style things every few years, and this having such a big canvas and being on a world map means that there is a lot of space for everyone to draw literally where they live.reply",
      "Thank you for this breakdown and for this level of transparency. We have been thinking of moving from MapTiler to OpenFreeMap for StatusGator's outage maps.reply",
      "Feel free to migrate. If you ever worry about High Availability, self-hosting is always an option. But I'm working hard on making the public instance as reliable as possible.reply",
      "Since the limit you ran into was number of open files could you just raise that limit? I get blocking the spammy traffic but theoretically could you have handled more if that limit was upped?reply",
      "I've just written my question to the nginx community forum, after a lengthy debugging session with multiple LLMs. Right now, I believe it was the combination of multi_accept + open_file_cache > worker_rlimit_nofile.https://community.nginx.org/t/too-many-open-files-at-1000-re...Also, the servers were doing 200 Mbps, so I couldn't have kept up _much_ longer, no matter the limits.reply",
      "I'm pretty sure your open file cache is way too large. If you're doing 1k/sec, and you cache file descriptors for 60 minutes, assuming those are all unique, that's asking for 3 million FDs to be cached, when you've only got 1 million available. I've never used nginx or open_file_cache[1], but I would tune it way down and see if you even notice a difference in performance in normal operation. Maybe 10k files, 60s timeout.> Also, the servers were doing 200 Mbps, so I couldn't have kept up _much_ longer, no matter the limits.For cost reasons or system overload?If system overload ... What kind of storage? Are you monitoring disk i/o? What kind of CPU do you have in your system? I used to push almost 10GBps with https on dual E5-2690 [2], but it was a larger file. 2690s were high end, but something more modern will have much better AES acceleration and should do better than 200 Mbps almost regardless of what it is.[1] to be honest, I'm not sure I understand the intent of open_file_cache... Opening files is usually not that expensive; maybe at hundreds of thousands of rps or if you have a very complex filesystem. PS don't put tens of thousands of files in a directory. Everything works better if you take your ten thousand files and put one hundred files into each of one hundred directories. You can experiment to see what works best with your load, but a tree where you've got N layers of M directories and the last layer has M files is a good plan, 64 <= M <= 256. The goal is keeping the directories compact so searching and editing is effective.[2] https://www.intel.com/content/www/us/en/products/sku/64596/i...reply",
      "One thing that might work for you is to actually make the empty tile file, and hard link it everywhere it needs to be. Then you don't need to special case it at runtime, but instead at generation time.NVMe disks are incredibly fast and 1k rps is not a lot (IIRC my n100 seems to be capable of ~40k if not for the 1 Gbit NIC bottlenecking). I'd try benchmarking without the tuning options you've got. Like do you actually get 40k concurrent connections from cloudflare? If you have connections to your upstream kept alive (so no constant slow starts), ideally you have numCores workers and they each do one thing at a time, and that's enough to max out your NIC. You only add concurrency if latency prevents you from maxing bandwidth.reply",
      "Yes, that's a good idea. But we are talking about 90+% of the titles being empty (I might be wrong on that), that's a lot of hard links. I think the nginx config just need to be fixed, I hope I'll receive some help on their forum.reply",
      "You could also try turning off the file descriptor cache. Keep in mind that nvme ssds can do ~30-50k random reads/second with no concurrency, or at least hundreds of thousands with concurrency, so even if every request hit disk 10 times it should be fine. There's also kernel caching which I think includes some of what you'd get from nginx's metadata cache?reply",
      "From the screenshot I wanted to say, couldn't this be done on a single VPS? Seemed over engineered to me. Then I realized the silly pixels are on top of a map of the entire earth. Dang!I'm curious what the peak req/s is like. I think it might be just barely within the range supported by benchmark-friendly web servers.Unless there's some kind of order of magnitude slowdowns due to the nature of the application.Edit: Looks like about 64 pixels per km (4096 per km^2). At full color uncompressed that's about 8TB to cover the entire earth (thinking long-term!). 10TB box is \u20ac20/month from Hetzner. You'd definitely want some caching though ;)Edit 2: wplace uses 1000x1000 px pngs for the drawing layer. The drawings load instantly, while the map itself is currently very laggy, and some chunks permanently missing.reply"
    ],
    "link": "https://blog.hyperknot.com/p/openfreemap-survived-100000-requests",
    "first_paragraph": ""
  },
  {
    "title": "R0ML's Ratio (glyph.im)",
    "points": 41,
    "submitter": "zdw",
    "submit_time": "2025-08-09T12:21:31 1754742091",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=44845957",
    "comments": [
      "I once worked for a company that went from 100k annually to over $1B in under a year. I was 18 and they gave me a corporate card and almost no particular restrictions on its use. I thought they were a bozo. Now I\u2019m not so sure.reply",
      "I see this regularly in large enterprise. My favourite example is getting a 20-30% discount on rack-mount server hardware purchased up-front for the next 3 to 5 years of growth requirements. Invariably, the executives that signed this \"great deal\" treat themselves to a champagne lunch to celebrate the savings.Never mind that most of that capacity will sit around unused for years. The real problem is that by the time they get around to filling the last half of the capacity, the exponential march of Moore's law -- or the equivalents for storage and networking -- will have discounted the cost of that capacity by 50% or more anyway.Similarly, I saw a corporation \"lock in\" a sweet discount for the WAN network provider for 7 years where they got an upgrade from 1 Mbps to 2 Mbps and a discount of 40%. At the same time, residential broadband was already 25 Mbps minimum and business-grade fibre had three-digit Mbps speeds within months. The sales rep that got that deal signed probably laughed his ass off.It's a fundamental misunderstanding that it's easier or better to purchase things in big stair-step increments when following an exponential curve. The waste will be exponentially higher the longer the steps. Ideally, one would want to purchase hardware in the smallest possible time-steps, following product release cycles exactly. That's one of the benefits of the public cloud, you can switch CPUs (or whatever) with a button-press.The example of this that's burned into my mind is that a vendor (Dell or HPE, I can't remember) convinced a government department to buy a 48-core AMD EPYC server for bioinformatics. This is one of those problems where you want the biggest possible single box because of the way the algorithms scale (\"up\", not \"out\"). They're stuck with that box for the next five or so years. Meanwhile the cloud public cloud is making available these monsters: https://techcommunity.microsoft.com/blog/azurehighperformanc...Check out that beautiful exponential curve! If you \"lock in\" just 2 or 3 generations back, you're missing out on 90% of what you could be using.reply",
      "I found this article somewhat confusing to read, so I asked for a synthesis of it, shared below for anyone else who might run into the same.https://share.cleanshot.com/7zq42cMgreply"
    ],
    "link": "https://blog.glyph.im/2025/08/r0mls-ratio.html",
    "first_paragraph": "Is your volume discount a good deal? Who nose!My father, also known as\n\u201cR0ML\u201d once described a methodology for evaluating\nvolume purchases that I think needs to be more popular.If you are a hardcore fan, you might know that he has already described this\nconcept publicly in a talk at OSCON in 2005, among other places, but it has\nnever found its way to the public Internet, so I\u2019m giving it a home here, and\nin the process, appropriating some of his words.1Let\u2019s say you\u2019re running a circus.  The circus has many clowns.  Ten thousand\nclowns, to be precise.  They require bright red clown noses.  Therefore, you\nmust acquire a significant volume of clown noses.  An enterprise licensing\nagreement for clown noses, if you will.If the nose\nplays,\nit can really make the act.  In order to make sure you\u2019re getting quality\nnoses, you go with a quality vendor.  You select a vendor who can supply noses\nfor $100 each, at retail.Do you want to buy retail?  Ten thousand clowns, ten thousand noses, one\nhund"
  },
  {
    "title": "Debian 13 \"Trixie\" (debian.org)",
    "points": 524,
    "submitter": "ducktective",
    "submit_time": "2025-08-09T18:18:45 1754763525",
    "num_comments": 196,
    "comments_url": "https://news.ycombinator.com/item?id=44848782",
    "comments": [
      "Writing this from my Debian system, it's a great distro that has been excellent to me as a daily driver. I switched to Debian 6 after Ubuntu went way downhill and haven't had cause to regret it.I like Debian's measured pragmatism with ideology, how it's a distro of free software by default but it also makes it easy to install non-free software or firmware blobs. I like Debian's package guidelines, I like dpkg, I like the Debian documentation even if Arch remains the best on that front. I like the stable/testing package streams, which make it easy to choose old but rock-stable vs just a bit old and almost as stable.And one of the best parts is, I've never had a Debian system break without it being my fault in some way. Every case I've had of Debian being outright unbootable or having other serious problems, it's been due to me trying to add things from third-party repositories, or messing up the configuration or something else, but not a fault of the Debian system itself.reply",
      ">I've never had a Debian system break without it being my fault in some way.Debian is great but I can't say this is a shared experience. In particular, I've been bitten by Debian's heavy patching of kernel in Debian stable (specifically, backport regressions in the fast-moving DRM subsystem leading to hard-to-debug crashes), despite Debian releases technically having the \"same\" kernel for a duration of a release. In contrast, Ubuntu just uses newer kernels and -hwe avoids a lot of patch friction. So I still use Debian VMs but Ubuntu on bare metal. I haven't tried kernel from debian-backports repos though.reply",
      "> Debian's heavy patching of kernel in Debian stableNeeds citation.Debian stable uses upstream LTS kernels and I'm not aware of any heavy patching they do on top of that.Upstream -stable trees are very relaxed in patches they accept and unfortunately they don't get serious testing before being released either (you can see there's a new release in every -stable tree like every week), so that's probably what you've been bit by.reply",
      "LTS has had major breaking changes in various areas in recent times too, virtio was badly broken at one point this year, as was a commonly used netlink interface. Hat tip to the Arch kernel contributors who helped track this down and chase upstream, as we had mutually affected users. The debian and ubuntu bug trackers were a wasteland of silence and user contributions throughout the situation, and frustratingly continued to be so as AWS, GCP and others copied their kernel patch trees and blindly shipped the same problems to users and refused to respond to bugs and emails.You're right stability comes from testing, not enough testing happens around Linux period, regardless of which branch is being discussed.It's not easy testing kernels, but the bar is pretty low.reply",
      "One of the unsung praises of Arch is that it's turned thousands of users into testers. Before someone says \"that shouldn't be the user's responsibility\" I'm going to say I'm not so sure. We're all in this together. I'd rather deal with a bug or two on my desktop at home if it means it gets fixed before appearing in a distro that gets used for servers at work and causes issues there where the consequences are much higher.reply",
      "Bear in mind, LTS and ELTS are not Debian maintained.The wiki has more info on this.reply",
      "These days all of my \u201cDebian\u201d bare metal systems are technically running Proxmox, which I think is a relatively happy medium as far as the base Debian system goes \u2014 the Proxmox kernel is basically the Ubuntu kernel, but otherwise it\u2019s a pretty standard Debian system.I\u2019ve thought about (ab)using a Proxmox repository on an otherwise stock Debian system before just for the kernel\u2026reply",
      "The upstream kernel already backports enough regressions on its own to its stable releases, Debian's kernel team does not help them too much with that.reply",
      "Which GPU, display server and compositor stack are you using?reply",
      "Integrated Intel GPU and no graphical system, just KMS VT (text console). That's what made it so frustrating - only displaying a console should not result in kernel panics under CPU load! Admittedly, the experience was anecdotal and years ago and I heard Debian is doing less of a RHEL-style \"frankenkernel\" now.reply"
    ],
    "link": "https://www.debian.org/News/2025/20250809",
    "first_paragraph": "\n\n\n\nSkip QuicknavLatest News\n / News from 2025\n /\nNews -- Debian 13 trixie releasedAugust 9th, 2025After 2 years, 1 month, and 30 days of development, the Debian\nproject is proud to present its new stable version 13 (code name trixie).trixie will be supported for the next 5 years thanks to the\ncombined work of the Debian Security team\nand the Debian Long Term Support team.Debian 13 trixie ships with several desktop environments, such as:\nThis release contains over 14,100 new packages for a total count of\n69,830 packages, while over 8,840 packages have been removed\nas obsolete. 44,326 packages were updated in this release.\nThe overall disk usage for trixie is 403,854,660 kB (403 GB), and\nis made up of 1,463,291,186 lines of code.Thanks to our translators who have made the man-pages for\ntrixie available in multiple languages.Debian 13 trixie includes numerous updated software packages\n(over 63% of all packages from the previous release), such as:\n\nWith this broad selection of packages an"
  },
  {
    "title": "People returned to live in Pompeii's ruins, archaeologists say (bbc.com)",
    "points": 35,
    "submitter": "bookofjoe",
    "submit_time": "2025-08-07T13:16:52 1754572612",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=44824088",
    "comments": [
      "I think I found the source paper (written in Itallian):https://pompeiisites.org/e-journal-degli-scavi-di-pompei/la-...So the archaeologists think that, after the destruction of 79 A.D., some survivors returned to Pompeii and found their homes half-buried in ash. They tried recover their belongings by digging underground, and some apparently attempted to rebuild their lives in their old homes, because they had nowhere else to go.While their efforts ultimately proved to be futile, they did leave some historical artifacts behind (e.g. bread oven entirely made of salvaged materials), and the archaeologists recently unearthed them.reply",
      "It\u2019s not hard to imagine people mining the ruins for valuables.reply",
      "For those interested, there's a new set of hour-long videos on the PBS site that has more about the recent Pompeii excavations.There are four so far. Not sure if there will be more:\nhttps://www.pbs.org/show/pompeii-the-new-dig/reply",
      "How would anyone be able to afford anything if all their possessions were under hardened magmareply",
      "It wasn\u2019t magma, it was 4-6m of ash and pumice.https://en.m.wikipedia.org/wiki/Pompeiireply",
      "IIRC the first explosion of 79 AD didn't bury the Pompeii completely. (It did bury Herculaneum, and much deeper so.) It was another explosion around the time of collapse of the Western Roman Empire that finished the job and hid the remaining structures from human view.reply",
      "looters would dig holes at known rich villas?reply"
    ],
    "link": "https://www.bbc.com/news/articles/c62wx23y2v1o",
    "first_paragraph": "New evidence suggests people returned to live among the ruins of Pompeii after the ancient Roman city was devastated by a volcanic eruption.Archaeologists believe some survivors who could not afford to start a new life elsewhere returned to the site and may have been joined by others looking for a place to settle. Pompeii was home to more than 20,000 people before Mount Vesuvius erupted in AD79, burying - and preserving - much of the city, before its rediscovery in the 16th century.There had been previous speculation that survivors had returned to the ruins, and archaeologists at the site said in a statement on Wednesday that the theory appears to have been confirmed by new research. \"Thanks to the new excavations, the picture is now clearer: post-79 Pompeii reemerges, less as a city than as a precarious and grey agglomeration, a kind of camp, a favela among the still-recognisable ruins of the Pompeii that once was,\" the site's director, Gabriel Zuchtriegel, said.The archaeologists sai"
  },
  {
    "title": "Who got arrested in the raid on the XSS crime forum? (krebsonsecurity.com)",
    "points": 56,
    "submitter": "todsacerdoti",
    "submit_time": "2025-08-06T12:14:47 1754482487",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://krebsonsecurity.com/2025/08/who-got-arrested-in-the-raid-on-the-xss-crime-forum/",
    "first_paragraph": "On July 22, 2025, the European police agency Europol said a long-running investigation led by the French Police resulted in the arrest of a 38-year-old administrator of XSS,\u00a0a Russian-language cybercrime forum with more than 50,000 members. The action has triggered an ongoing frenzy of speculation and panic among XSS denizens about the identity of the unnamed suspect, but the consensus is that he is a pivotal figure in the crime forum scene who goes by the hacker handle \u201cToha.\u201d Here\u2019s a deep dive on what\u2019s knowable about Toha, and a short stab at who got nabbed.An unnamed 38-year-old man was arrested in Kiev last month on suspicion of administering the cybercrime forum XSS. Image: ssu.gov.ua.Europol did not name the accused, but published partially obscured photos of him from the raid on his residence in Kiev. The police agency said the suspect acted as a trusted third party \u2014 arbitrating disputes between criminals \u2014 and guaranteeing the security of transactions on XSS. A statement fro"
  },
  {
    "title": "Quickshell \u2013 building blocks for your desktop (quickshell.org)",
    "points": 245,
    "submitter": "abhinavk",
    "submit_time": "2025-08-05T16:18:53 1754410733",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=44800048",
    "comments": [
      "I love GitHub search because I can see how other folks are using Quickshell:https://github.com/search?q=quickshell+language%3Anix&type=c...This looks very cool!reply",
      "To stay in loop with updates:Couldn't find the releases-only feed in Forgejo RSS, the blog seemed to be outdated and who doesn't use X or discord, here is at least a github-mirror where you can subscribe to releases:https://github.com/quickshell-mirror/quickshellreply",
      "> who doesn\u2019t use X or discordPeople with the privilege to make choices based on their values, and whose values include human rights, and freely accessible information (respectively).I wouldn\u2019t judge anyone who chooses to utilize either service (I still have accounts on both), but I can certainly understand why some would rather not.Thanks for the GitHub mirror link, that\u2019s probably where I\u2019ll start. Neat project.reply",
      "I thought the original comment meant \u201c_for_ who doesn\u2019t use X or Discord, here is the github mirror link\u201d. There\u2019s a \u201cfor\u201d missing, and thus I think they agree with youreply",
      "I really think that everyone is sleeping on QML.reply",
      "I'm not a fan of the language but QtQuick & QML is what I'd use for this type of widgets. OTOH I am starting two projects at work, one being a traditional desktop app and another being an HMI with lots of functionality, and decided to just go with QtWidgets and save myself from all of QML's JS influences and C++ interop boilerplate.reply",
      "Neat! What OSes does this support?reply",
      "Currently Linux and I think BSD, and the creator has said he wants to support MacOS and Windows, though those will only be included in the paid product.On Linux and BSD, it supports Wayland and X11, though Wayland is better supported.ie, Quickshell will forever stay completely free for free operating sysems.reply",
      "Weirdly, the fact that the Windows and MacOS versions will be paid makes me more optimistic.Customizing at least the Windows window manager isn't for the faint of heart and its architecture doesn't have a lot in common with Linux so such an effort is very far from a straightforward port, and as a result most Linux desktop software and especially stuff that deeply integrates with the desktop environment is basically never ported or the port is incomplete, buggy, short lived, etc.KDE4 was supposed to fully support Windows and 15+ years later I'm fairly sure that dream is dead.reply",
      "I expect Windows to be easier than Mac, especially if attempting to respect SIP, though I've not done much research yet and don't plan to until the Linux version is in a state I'm happy with or I'm forced to heavily use a Windows/Mac machine and need to make it bearable.reply"
    ],
    "link": "https://quickshell.org/",
    "first_paragraph": "\nConfiguration by soramane  (install) \nConfiguration by end_4  (install) \nConfiguration by outfoxxed (source code)  \nConfiguration by pfaj and bdebiase \nConfiguration by flicko (source code)  \nConfiguration by vaxry \nQuickshell is a toolkit for building status bars, widgets, lockscreens,\n          and other desktop components using QtQuick. It can be used alongside your\n          wayland compositor or window manager to build a complete desktop environment.\n  More information Brought to you by:"
  },
  {
    "title": "Long-term exposure to outdoor air pollution linked to increased risk of dementia (cam.ac.uk)",
    "points": 245,
    "submitter": "hhs",
    "submit_time": "2025-08-09T13:01:31 1754744491",
    "num_comments": 78,
    "comments_url": "https://news.ycombinator.com/item?id=44846164",
    "comments": [
      "What\u2019s important to understand is that PM2.5 is not PM2.5.It only defines the diameter of the particles but can be composed of very different elements. From salt that dissolves in the lungs to toxic metals.Currently it is extremely difficult to get a comprehensive understanding of the health impacts of these particles.Much more research needs to be done to understand which particle compositions and thus what sources of air pollution (eg traffic, wildfires, factories, landfills, ports etc) have what kind of health effects.If you are interested to see an image how different PM2.5 particle look like, have a look at the photo in this blog post that one of our in-house scientists wrote [1].[1] https://www.airgradient.com/blog/pm25-is-not-pm25/(Edited and replaced weight with diameter)reply",
      "> It only defines the weight of the particlesDiameter, not weight. PM2.5 is particles of diameter 2.5\u03bcm or less.reply",
      "Yes of course! Thanks for pointing it out. I corrected the above.reply",
      "You're both right enough. Aerodynamic diameter doesn't measure the particles themselves, but how their settling velocity compares to a spherical reference ideal of a certain density (1g/cm*3) in a medium.I don't deal with gas cleaning, but at those scales, if you work a lot with applied processes like filtration and separation, you can ballpark things like daltons with mass and size. I know I do with MWCOs.reply",
      "If I had to place bets, it would be reactive species. PAHs, alcohols, and other volatiles.reply",
      "Even VOC is still an open question. Are great smelling food, onions, etc bad for our lungs?reply",
      "very interesting article, thanks for postingreply",
      "This is an obvious third-factor for poverty and marginalization. Air pollution exposure is the most classic example of unequal protection from harm in environmental justice. Alameda county did a study on this that found as an isolated, direct-result of unequal exposure to air pollution, black people live 15 years less than white people on average in Alameda County alone.reply",
      "If you could see long term PM2.5 averages and how they vary, we'd approach as a national crisis.https://www.sciencedirect.com/science/article/pii/S266601722... (this groups methods can be substantially improved).Having done some additional follow on work in the space -- the results definitely do not follow socioeconomic boundaries as one might expect.Roads are a huge contributor.reply",
      "I find this very hard to believe... Mind sharing that study?reply"
    ],
    "link": "https://www.cam.ac.uk/research/news/long-term-exposure-to-outdoor-air-pollution-linked-to-increased-risk-of-dementia",
    "first_paragraph": "ResearchAn analysis of studies incorporating data from almost 30 million people has highlighted the role that air pollution \u2013 including that coming from car exhaust emissions \u2013 plays in increased risk of dementia.\nAn analysis of studies incorporating data from almost 30 million people has highlighted the role that air pollution \u2013 including that coming from car exhaust emissions \u2013 plays in increased risk of dementia.Tackling air pollution can deliver long-term health, social, climate, and economic benefitsDementias such as Alzheimer's disease are estimated to affect more than 57.4 million people worldwide, a number that is expected to almost triple to 152.8 million cases by 2050. The impacts on the individuals, families and caregivers and society at large are immense.While there are some indications that the prevalence of dementia is decreasing in Europe and North America, suggesting that it may be possible to reduce the risk of the disease at a population level, elsewhere the picture i"
  },
  {
    "title": "A Simple CPU on the Game of Life (2021) (carlini.com)",
    "points": 33,
    "submitter": "jxmorris12",
    "submit_time": "2025-08-06T15:17:22 1754493442",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=44813175",
    "comments": [
      "First, I thought \"not so simple!\"\nBut it's wonderfully grokkable compared to zooming in on equivalent hardware and watching transistor states unfold.reply",
      "I think I see the opposite - a Game of Life logic gate is way harder to understand than just a symbol representing the logic itself. And I see most logic gate transistor implementations as much simpler and so easier.reply",
      "That's just beautiful!reply",
      "Now if there were some custom FPGA that ran the life rules\u2026reply"
    ],
    "link": "https://nicholas.carlini.com/writing/2021/unlimited-register-machine-game-of-life.html",
    "first_paragraph": "\nby\n              \n                Nicholas Carlini\n              \n\n                2021-12-30\n              \n\n\n      \tThis is the fourth article in a series of posts that I've been making\n      \ton creating digital logic gates in the game of life. The\n              first,\n              couple\n      \tof articles\n      \tstarted out with how to create digital logic gates and use them in\n      \torder to construct simple circuits.\n      \tIn this post we're going to actually build a first real computer:\n      \t    a (2-stage pipelined) unlimited register machine.\n              And later on\n      \t    ([5])\n              we'll make an even better computer.\n            \n      \tPictured below is the execution of an unlimited register machine which is\n      \trunning a program that factors the number 15 it does this in just a few minutes.\n      \tYou might think that this is a simple task, but let me just remind you\n      \tthat it was only a few years ago that quantum computers managed this impre"
  },
  {
    "title": "How I use Tailscale (chameth.com)",
    "points": 181,
    "submitter": "aquariusDue",
    "submit_time": "2025-08-06T08:09:09 1754467749",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=44809166",
    "comments": [
      "I tried using `tailscale funnel` against a dummy server `python -m http.server`, and within 10 seconds the bots started to check for vulnerabilities.Tailscale warns you about how enabling it will issue an HTTPS certificate which will be in a public ledger. But I wasn't expecting it to be this quick.    127.0.0.1 - - [10/Aug/2025 00:11:34] \"GET /@vite/env HTTP/1.1\" 404 -\n    127.0.0.1 - - [10/Aug/2025 00:11:34] code 404, message File not found\n    127.0.0.1 - - [10/Aug/2025 00:11:34] \"GET /actuator/env HTTP/1.1\" 404 -\n    127.0.0.1 - - [10/Aug/2025 00:11:34] code 404, message File not found\n    127.0.0.1 - - [10/Aug/2025 00:11:34] \"GET /server HTTP/1.1\" 404 -\n    127.0.0.1 - - [10/Aug/2025 00:11:35] code 404, message File not found\n    127.0.0.1 - - [10/Aug/2025 00:11:35] \"GET /.vscode/sftp.json HTTP/1.1\" 404 -\n    127.0.0.1 - - [10/Aug/2025 00:11:35] code 404, message File not found\n    127.0.0.1 - - [10/Aug/2025 00:11:39] \"GET /s/7333e2433323e20343e2538313/_/;/META-INF/maven/com.atlassian.jira/jira-webapp-dist/pom.properties HTTP/1.1\" 404 -reply",
      "I use Headscale, an open source implementation of Tailscale control server. And it doesn't have funnel functionality implemented out of the box, but I use a custom Traefik proxy manager Web UI in which I can expose ports on different Tailnet nodes.In order to avoid exposing something unnecessarily in the certificate transparency logs, I use a single wildcard certificate, so all the subdomains are not listed anywhere automatically.I use the same approach for services hosted in the internal subdomain, because I don't want everyone to know what exactly I'm running in my homelab.reply",
      "Wait, so bots watch for new records added to this HTTPS cert public ledger, then immediately start attacking?To me that sounds like enabling HTTPS is actually a risk here\u2026reply",
      "The server was already exposed. All this does is remove obscurityreply",
      "I wish this trend of \u201csecurity through obscurity\u201d should mean that all info should just be exposed would die, its silly and lacks basis in reality.Even within infosec, certain types of information disclosure are considered security problems. Leaking signed up user information or even inodes on the drives can lead to PCI-DSS failures.Why is broadcasting your records treated differently? Because people would find the information eventually if they scanned the whole internet? Even then they might not due to SNI; so this is actually giving critical information necessary for an attack to attackers.reply",
      "The issue is not that obscurity per se is bad, but relying _only_ on obscurity is absolute the same as not having any security measures at all.With the public ledger or not, you will still need to implement proper security measures. So it shouldn't matter if your address is public or not, in fact making it public raises the awareness for the problem. That's the argument.reply",
      "> relying _only_ on obscurityUntil it gets obscure enough that we start calling it \u201cpublic-key cryptography\u201d. Guess the prime number I'm thinking of between 0 and 2\u21914096 and win a fabulous prize!",
      "Okay, but we're not talking about that here. This is very much the case of a service being exposed that shouldn't be and relying on obscurity to try and avoid actually getting compromisedreply",
      "IME, moving ssh off the standard port reduces bot scanning traffic by >99%. Not only it means less noise in the logs (and thus higher SNR), but also lowers the chance you're hit by spray-and-pray in case there's a zero day in sshd (or any other daemon really).reply",
      "True, but I hardly open any ssh to the wide world. I would only allow it inside a closed network anyways. HTTP on the other hand _needs_ to be exposed on 80 or 443 (not technically, but in practice)reply"
    ],
    "link": "https://chameth.com/how-i-use-tailscale/",
    "first_paragraph": "\n            Published on\n            Jun 25, 2025\n\n            I\u2019ve been using Tailscale for around four years to connect my disparate\n            devices, servers and apps together. I wanted to talk a bit about how I use it, some cool features you might\n            not know about, and some stumbling blocks I encountered.\n          \n            I\u2019m not sure Tailscale needs an introduction for the likely audience of this blog, but I\u2019ll give one anyway.\n            Tailscale is basically a WireGuard[1]\n            orchestration service, with lots of nice features sprinkled on top. It\u2019s a subscription product, but it has\n            an insanely generous free tier that covers basically anything you\u2019d ever want to do as an individual. They\n            also open source all their client software, and there\u2019s a third party control server implementation called\n            Headscale if you want to avoid the hosted system\n            entirely.\n          \n            At its core, Tailscale lets y"
  },
  {
    "title": "An AI-first program synthesis framework built around a new programming language (acm.org)",
    "points": 63,
    "submitter": "tosh",
    "submit_time": "2025-08-09T15:36:21 1754753781",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=44847334",
    "comments": [
      "I know ACM Queue is a non-peer-reviewed magazine for practitioners but this still feels like too much of an advertisement, without any attempt whatsoever to discuss downsides or limitations. This really doesn't inspire confidence:  While this may seem like a whimsical example, it is not intrinsically easier or harder for an AI model compared to solving a real-world problem from a human perspective. The model processes both simple and complex problems using the same underlying mechanism. To lessen the cognitive load for the human reader, however, we will stick to simple targeted examples in this article.\n\nFor LLMs this is blatantly false - in fact asking about \"used textbooks\" instead of \"apples\" is measurably more likely to result in an error! Maybe the (deterministic, Prolog-style) Universalis language mitigates this. But since Automind (an LLM, I think) is responsible for pre/post validation, naively I would expect it to sometimes output incorrect Universalis code and incorrectly claim an assertion holds when it does not.Maybe I am making a mountain out of a molehill but this bit about \"lessen the cognitive load of the human reader\" is kind of obnoxious. Show me how this handles a slightly nontrivial problem, don't assume I'm too stupid to understand it by trying to impress me with the happy path.reply",
      "Glad to see focus being put on keeping humans in the drivers seat, democratizing coding with the help of AI. The syntax is probably still too verbose to be easily accessible, but I like the overall approach.reply",
      "This article feels extremely imprecise. The syntax of the \"language\" changes from example to example, control structures like conditionals are expressed in English prose, some examples are solved by \"do all the work for me\" functions like the \"toPdf()\" example...This whole thing feels like an elaborate LLM fantasy. Is there any real, usable language behind these examples, or is the author just role-playing with ChatGPT?reply"
    ],
    "link": "https://queue.acm.org/detail.cfm?id=3746223",
    "first_paragraph": ""
  }
]