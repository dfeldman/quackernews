[
  {
    "title": "Robert Dennard, DRAM Pioneer, has died (ieee.org)",
    "points": 118,
    "submitter": "jnord",
    "submit_time": "2024-10-03T22:03:27.000000Z",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=41735544",
    "comments": [
      "This is the Dennard of Dennard Scaling, a chip scaling law that is arguably as important as Moore's Law.https://en.wikipedia.org/wiki/Dennard_scalingThe end of Dennard scaling was why the Pentium 4 architecture was a dead end and never hit 10Ghz like it was supposed to, why the Cell processor never hit the 5Ghz it was supposed to, why we've been spending quite a bit of the transistor budget on more cores rather than a very fancy single CPU core of 10Bs of transistors, and why chips with lower thermal limits will see a lot of \"dead silicon\" where you can't actually light up the whole chip at once without melting it.\n \nreply",
      "Dennard scaling, for people in the industry, was far more important than Moore's law when it was available.Moore made a high-level observation, but Dennard told you how to do it.\n \nreply",
      ">The end of Dennard scaling was why the Pentium 4 architecture was a dead end and never hit 10Ghz like it was supposed to, why the Cell processor never hit the 5Ghz it was supposed toAround that time the PowerPC 970 aka G5 also failed to achieve 3 GHz, breaking the promise Steve Jobs publicly made at one of his keynotes for Apple.\n \nreply",
      "Well, it's basically the technical implementation of Moore's law, since Moore's law is just an empirical observation. (And maybe also a self-fulfilling prophecy)\n \nreply",
      "RIP.I was surprised that it didn't get much attention on HN when the news broke back in April, considering Dennard's large contributions to technology.\n \nreply",
      "> 91I really hope I live as long as these guys. It's one thing to invent something useful, it's another to spend your life watching it grow.\n \nreply",
      "Roger Penrose is 93 and as sharp as a tack!\n \nreply",
      "He passed away months ago.  RIP.  He seems like a class act from what I\u2019ve heard.\n \nreply",
      "(April)\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/in-memoriam-oct-2024",
    "first_paragraph": "The October 2024 issue of IEEE Spectrum is here!IEEE also remembers the lives and legacies of other membersRobert Dennard, the inventorof Dynamic RAM.Robert Dennard Inventor of Dynamic RAM Fellow, 91; died 23 AprilIn 1967 Dennard invented what is known as DRAM\u2014a type of random-access semiconductor memory that stores each bit of data in a memory cell consisting of a tiny capacitor and transistor. Before his invention, RAM required bulky, power-hungry components that were expensive to produce. His breakthrough paved the way for inexpensive, high-density, and commercially available memory.The computing pioneer received the 2009 IEEE Medal of Honor for his innovation.Dennard began his career as a staff engineer at IBM\u2019s New York City research lab. He worked on new devices and circuits for logic and memory applications and developed advanced data communication techniques. He was transferred in 1961 to IBM\u2019s newly opened Watson Research Center, in Yorktown Heights, N.Y., where he worked on f"
  },
  {
    "title": "Were RNNs all we needed? (arxiv.org)",
    "points": 250,
    "submitter": "beefman",
    "submit_time": "2024-10-03T17:31:59.000000Z",
    "num_comments": 107,
    "comments_url": "https://news.ycombinator.com/item?id=41732853",
    "comments": [
      "It's curse and a blessing that discussion of topics happens in so many different places. I found this comment on Twitter/X interesting: https://x.com/fchollet/status/1841902521717293273\"Interesting work on reviving RNNs. https://arxiv.org/abs/2410.01201 -- in general the fact that there are many recent architectures coming from different directions that roughly match Transformers is proof that architectures aren't fundamentally important in the curve-fitting paradigm (aka deep learning)Curve-fitting is about embedding a dataset on a curve. The critical factor is the dataset, not the specific hard-coded bells and whistles that constrain the curve's shape. As long as your curve is sufficiently expressive all architectures will converge to the same performance in the large-data regime.\"\n \nreply",
      "> The critical factor is the dataset, not the specific hard-coded bells and whistles that constrain the curve's shapeI have almost the opposite take. We've had a lot of datasets for ages, but all the progress in the last decade has come from advances how curves are architected and fit to the dataset (including applying more computing power).Maybe there's some theoretical sense in which older models could have solved newer problems just as well if only we applied 1000000x the computing power, so the new models are 'just' an optimisation, but that's like dismissing the importance of complexity analysis in algorithm design, and thus insisting that bogosort and quicksort are equivalent.When you start layering in normalisation techniques to minimise overfitting, and especially once you start thinking about more agentic architectures (eg. Deep Q Learning, some of the search space design going into OpenAI's o1), then I don't think the just-an-optimisation perspective can hold much water at all - more computing power simply couldn't solve those problems with older architectures.\n \nreply",
      "> \"As long as your curve is sufficiently expressive all architectures will converge to the same performance in the large-data regime.\"I haven't fully ingested the paper yet, but it looks like it's focused more on compute optimization than the size of the dataset:> ... and (2) are fully parallelizable during training (175x faster for a sequence of length 512Even if many types of architectures converge to the same loss over time, finding the one that converges the fastest is quite valuable given the cost of running GPU's at scale.\n \nreply",
      "> Even if many types of architectures converge to the same loss over time, finding the one that converges the fastest is quite valuable given the cost of running GPU's at scale.This! Not just fastest but with the lowest resources in total.Fully connected neural networks are universal functions. Technically we don\u2019t need anything but a FNN, but memory requirements and speed would be abysmal far beyond the realm of practicality.\n \nreply",
      "Unless we could build chips in 3D?\n \nreply",
      "Not even then, a truly fully connected network would have super exponential runtime (it would take N^N time to evaluate)\n \nreply",
      "We need quantum computing there. I remember seeing a recent article about quantum processes in the brain. If that\u2019s true, QC may be the missing part.\n \nreply",
      "> finding the one that converges the fastest is quite valuable given the cost of running GPU's at scaleNot to him, he runs the ARC challenge. He wants a new approach entirely. Something capable of few-shot learning out of distribution patterns .... somehow\n \nreply",
      "I figured this was pretty obvious given that MLPs are universal function approximators. A giant MLP could achieve the same results as a transformer. The problem is the scale - we can\u2019t train a big enough MLP. Transformers are a performance optimization, and that\u2019s why they\u2019re useful.\n \nreply",
      "I remember one of the initial transformer people saying in an interview that they didn't think this was the \"one true architecture\" but a lot of the performance came from people rallying around it and pushing in the one direction.On the other hand, while \"As long as your curve is sufficiently expressive all architectures will converge to the same performance in the large-data regime.\"  is true,  a sufficiently expressive mechanism may not be computationally or memory efficient.  As both are constraints on what you can actually build,  it's not whether the architecture can produce the result, but whether a feasible/practical instantiation of that architecture can produce the result.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2410.01201",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Canvas is a new way to write and code with ChatGPT (openai.com)",
    "points": 586,
    "submitter": "davidbarker",
    "submit_time": "2024-10-03T17:07:23.000000Z",
    "num_comments": 418,
    "comments_url": "https://news.ycombinator.com/item?id=41732634",
    "comments": [
      "Small ot, but it's quite interesting that the highest decisive impact generative AI is having right now is on tech workers and software developers in particular.I'm more and more convinced that we're on the edge of a major shake up in the industry with all these tools.Not getting replaced, but at this rate of improvements I can't unsee major changes.A recent junior I have in my team built his first app entirely with chatgpt one year ago, he still didn't know how to code, but could figure out how to fix the imperfect code by reasoning, all of it as a non coder, and actually release something that worked for other people.\n \nreply",
      "The more I think about it, the more I am convinced developers will be the \"first to go\" when AGI takes over. Before bloggers and youtubers. Because programming is an activity that requires the least amount of \"grounding to reality\" among all human activities. We made sure of this with layers and layers of convenient abstraction.What about developers that code the AI systems? Well.. I am sure AGI will come from other \"bootsrapping AIs\" just like we see with compilers that compile themselves. When I see Altman and Sutskever talking about AGI being within reach, I feel they are talking about this bootstrapping AI being within reach.\n \nreply",
      "Sticking to my prediction that lawyers will be first.\n \nreply",
      "With AGI you won\u2019t need most of the human race anymore, developers are just the tip of the iceberg.Luckily ChatGPT and the rest have nothing to do with an AI not to mention AGI.\n \nreply",
      "It's not AGI yet but has everything to do with it.\n \nreply",
      "I would like to see the app prompted by junior dev in question.\n \nreply",
      "As an engineer I've spoken to a couple of different designers who are building out prototypes of their startup ideas using LLM assistance with the coding.While no actual engineer is involved at that stage, if they got funded then I'm sure their next step will be to hire a real engineer to do it all properly.\n \nreply",
      "That is legit frightening\n \nreply",
      "The issue I continue to have with many AI coding tools is they want me to use their own editor (\"native\", aka VSCode fork, or in the browser like this). I have zero intention of moving away from IDEA and nothing I've seen so far is good enough to make me want to switch. I really with there was more of \"bringing AI into your existing tools/workflows\" instead of \"here is a new tool with AI baked in\".\n \nreply",
      "The problem is that tacking on to an existing product, while the ideal approach, limits just how creative you can get. I believe this is one of the reasons Cursor had to fork VSCode. Simply being an extension limited the features they could build.\n \nreply"
    ],
    "link": "https://openai.com/index/introducing-canvas/",
    "first_paragraph": ""
  },
  {
    "title": "Cox slows Internet speeds in entire neighborhoods to punish any heavy users (arstechnica.com)",
    "points": 81,
    "submitter": "RadixDLT",
    "submit_time": "2024-10-03T23:38:21.000000Z",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=41736215",
    "comments": [
      "a gigabit customer who was paying $50 extra per month for unlimited data was flagged by Cox because he was using 8TB to 12TB a month\"Unlimited data\" should mean you can saturate the connection 24/7. Anything less is deceptive advertising. For a gigabit connection, that would mean around 300TB per month.\n \nreply",
      "I don\u2019t know why they don\u2019t advertise their oversubscription rate. The FCC should probably require this to be disclosed in some standard location. So many people are mad because \u201cI paid for a gigabit and I can\u2019t use the whole thing\u201d\u2026 but like you didn\u2019t pay for a gigabit you paid for a gigabit shared among 100 other people, which means peak-of-sums you should usually get gigabit, but it\u2019s not guaranteed.The internet is a series of tubes! You can get a dedicated gigabit sized tube but it\u2019ll cost 1-2 orders of magnitude more.E: Even elsewhere on this thread people are like> I dunno, I pay $70 a month for gigabit from Google Fiber and absolutely saturate that thing all day long up and down.Yes! You are the noisy neighbor getting lucky that your neighbors aren\u2019t also noisy!\n \nreply",
      "Needs (2020). They may or may not still be doing this, but this exact article was already on HN at the time:https://news.ycombinator.com/item?id=23460868\n \nreply",
      "\"Heavy\" use is just using what they pay for. If they can't guarantee that they should sell a lesser tier of service. Otherwise this is just fraud.\n \nreply",
      "They paid for committed bandwidth? So they'd be paying $1000+/mo. But they're not.\n \nreply",
      "I dunno, I pay $70 a month for gigabit from Google Fiber and absolutely saturate that thing all day long up and down. A Google PM got me on a call and asked me if I wanted 20 gigabit for $200 a month the other day. No restrictions, I could run my business off my $70 if I really wanted to.I don\u2019t know what Cox is going on about, they need to get with the program.\n \nreply",
      "I pay $50 for 10 gigabits from Sonic. I don\u2019t abuse it by deliberately running a speed test 24/7 or anything like that. I do use it for anything I want, at any time, without pausing to consider how much data it takes. Launch a NAS backup at 2PM on a weekday? Stream 4K video on 2 TVs at the same time? Download a mass of software updates? Without a second\u2019s hesitation. The CEO is on record being very explicit that they sell you Internet access so you can use it as you see fit.I have the best ISP in the country. You can\u2019t convince me otherwise.\n \nreply",
      "This one is probably in contention:https://epb.com/\n \nreply",
      "Other than being 6x the price though.\n \nreply",
      "Specifically the fraud is advertising gb service instead of 10mb99. QoS. It's as ridiculously as selling a prius that can do 200mph as little as 0 percent of the time.They want to advertise their sevrice as meeting the federal broadband speed without having to actually build a network that can support it. That's fraud.\n \nreply"
    ],
    "link": "https://arstechnica.com/tech-policy/2020/06/cox-slows-internet-speeds-in-entire-neighborhoods-to-punish-any-heavy-users/",
    "first_paragraph": "\n      Cox warns customers to lower usage, imposes 10Mbps upload limit on \u201cgigabit\u201d plan.\n    Cox Communications is lowering Internet upload speeds in entire neighborhoods to stop what it considers \"excessive usage,\" in a decision that punishes both heavy Internet users and their neighbors.Cox, a cable company with about 5.2 million broadband customers in the United States, has been sending notices to some heavy Internet users warning them to use less data and notifying them of neighborhood-wide speed decreases. In the case we will describe in this article, a gigabit customer who was paying $50 extra per month for unlimited data was flagged by Cox because he was using 8TB to 12TB a month.Cox responded by lowering the upload speeds on the gigabit-download plan from 35Mbps to 10Mbps for the customer's whole neighborhood. Cox confirmed to Ars that it has imposed neighborhood-wide slowdowns in multiple neighborhoods in cases like this one but didn't say how many excessive users are enough "
  },
  {
    "title": "Tengine: Open-source web server, originated by Taobao, based on Nginx (taobao.org)",
    "points": 39,
    "submitter": "thunderbong",
    "submit_time": "2024-09-30T06:31:48.000000Z",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=41694154",
    "comments": [
      "This appears to be continued development of OpenResty (which Taobao had previously been funding) - https://github.com/alibaba/tengine/blob/04baff4645f078331e00...\n \nreply",
      "Github, https://github.com/alibaba/tengine\n \nreply",
      "I'm more impressed these days with the pace, activity, and helpful support from the Caddy development team: https://github.com/caddyserver/caddy/\n \nreply",
      "Not sure of the appeal of caddy.Sure, the 1 liner config file looks fine at first glance but anywhere more complex than a basic reverse proxy setup and it becomes a nightmare to configure with a crazy configuration format.\n \nreply",
      "I tried messing with Caddy a little while ago and this was my experience.I'm sure being able to configure it in a million different languages conveniences someone, but [un]official docs documenting things in a format I don't use, and having to figure out what the option is in the format I do use, was immediately annoying.\n \nreply",
      "Caddy maintainer here, I don't understand what you mean. What format are you talking about? We definitely push everyone to use the Caddyfile.\n \nreply",
      "I agree that the Caddy team should hide all of the config adapters, the powerful JSON configs, and just focus on Caddyfile. I was extremely skeptical that I could convert 100+ Nginx Plus configs over to Caddyfile, but it was worth it. I moved 10k lines of Nginx config down to 1000 lines of Caddyfile. It is a big change, no question, but I've spent so much time on Nginx, Nginx Plus, Tengine, new forks, and moving away is the only sensible answer I could land on.\n \nreply",
      "Caddy maintainer here; hiding things doesn't make sense, some people do need to see that information, namely users of the API who perform scripted config changes on their running servers. The Caddyfile is pretty up-front-and-center, so I don't know where the confusion is coming from.\n \nreply",
      "Caddy maintainer here; could you be more specific about the usecases you think look like a nightmare? We're always looking to improve config ergonomics, but we need specific feedback.\n \nreply",
      "I use it in dev to proxy to multiple backend services but I don\u2019t think I\u2019d ever run with it in prod.\n \nreply"
    ],
    "link": "https://tengine.taobao.org/",
    "first_paragraph": "Tengine\u662f\u7531\u6dd8\u5b9d\u53d1\u8d77\u7684Web\u670d\u52a1\u5668\u9879\u76ee\u3002\u5b83\u5728Nginx\u7684\u57fa\u7840\u4e0a\uff0c\u9488\u5bf9\u5927\u8bbf\u95ee\u91cf\u7f51\u7ad9\u7684\u9700\u6c42\uff0c\u6dfb\u52a0\u4e86\u5f88\u591a\u9ad8\u7ea7\u529f\u80fd\u548c\u7279\u6027\u3002Tengine\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u5df2\u7ecf\u5728\u5927\u578b\u7684\u7f51\u7ad9\u5982\u6dd8\u5b9d\uff0c\u5929\u732b\uff0c\u4f18\u9177\uff0c\u5168\u7403\u901f\u5356\u901a\uff0cLazada\uff0c\u963f\u91cc\u4e91\u7b49\u5f97\u5230\u4e86\u5f88\u597d\u7684\u68c0\u9a8c\u3002Tengine\u5c06\u5411\u901a\u7528API\u7f51\u5173\u65b9\u5411\u6301\u7eed\u6f14\u8fdb\u548c\u53d1\u5c55\uff0c\u5728HTTP\u5e94\u7528\u6d41\u91cf\u5165\u53e3\u7f51\u5173\u7684\u57fa\u7840\u4e0a\uff0c\u9010\u6b65\u652f\u63014/7\u5c42TLS\uff0cTCP\uff0cUDP\u548cGRPC\u591a\u534f\u8bae\u8def\u7531\u80fd\u529b\uff0c\u9002\u914d\u4e0d\u540c\u7ec8\u7aef\u548c\u4e0d\u540c\u5e94\u7528\uff0c\u6253\u9020\u5168\u573a\u666f\u901a\u7528\u7f51\u5173\uff0c\u6301\u7eed\u4fdd\u6301Tengine\u4e1a\u754c\u9886\u5148\u5730\u4f4d\u3002\u4ece2011\u5e7412\u6708\u5f00\u59cb\uff0cTengine\u6210\u4e3a\u4e00\u4e2a\u5f00\u6e90\u9879\u76ee\uff0cTengine\u56e2\u961f\u5728\u79ef\u6781\u5730\u5f00\u53d1\u548c\u7ef4\u62a4\u7740\u5b83\u3002Tengine\u56e2\u961f\u7684\u6838\u5fc3\u6210\u5458\u6765\u81ea\u4e8e\u6dd8\u5b9d\uff0c\u8682\u8681\uff0c\u963f\u91cc\u4e91\uff0c\u641c\u72d7\u7b49\u4e92\u8054\u7f51\u4f01\u4e1a\u3002Tengine\u662f\u793e\u533a\u5408\u4f5c\u7684\u6210\u679c\uff0c\u6211\u4eec\u6b22\u8fce\u5927\u5bb6\u53c2\u4e0e\u5176\u4e2d\uff0c\u8d21\u732e\u81ea\u5df1\u7684\u529b\u91cf\u3002DownloadTengine is a web server originated by Taobao, the largest e-commerce website in Asia. It is based on the Nginx HTTP server and has many advanced features. Tengine has proven to be very stable and efficient on some of the top 100 websites in the world, including Taobao.com, Tmall.com, Youku, AliExpress, Lazada and Alibaba Cloud.Tengine has been an open source project since December 2011. It is being actively developed by the Tengine team, whose core members are from Taobao, Ant Group, Alibaba Cloud, Sogou and other Internet companies. Tengine is a community effort and everyone is encouraged to get involved.Download"
  },
  {
    "title": "Dance training superior to physical exercise in inducing brain plasticity (2018) (plos.org)",
    "points": 220,
    "submitter": "Tomte",
    "submit_time": "2024-10-03T14:01:23.000000Z",
    "num_comments": 120,
    "comments_url": "https://news.ycombinator.com/item?id=41730896",
    "comments": [
      "I do MRI work, and my gut is that none of the claims about dance vs. exercise would replicate. The behavioral data suggests that activity of some type will improve cognitive function (main effects of time). Such beneficial effects of activity on the brain have been shown before, and this is generally accepted. However, the authors' behavioral data doesn't show any difference between the dance vs. exercise groups. This means that the study is overall off to a pretty bad start if their goal is to study dance vs. exercise differences...The brain data claims to show that the dance vs. exercise groups showed different levels of improvement in various regions. However, the brain effects are tiny and are probably just random noise (I'm referring to those red spots, which are very small and almost certainly don't reflect proper correction for multiple hypotheses given that the authors effectively tested 1000s or 10000s of different areas). The authors' claims about BDNF are supported by a p-value of p = .046, and having main conclusions hinge on p-values of p > .01 usually means the conclusions are rubbish.In general, my priors on \"we can detect subtle changes in brain matter over a 6-week period\" are also very low. Perhaps, a study with this sample size could show that activity of some kind influences the brain over such a short length, but I am extremely skeptical that this type of study could detect differences between dance vs. exercise effects.\n \nreply",
      "Replicability was my first thought as well. These papers make great headlines but how much shaky pyramids of conclusions are built on non-reproducible conclusions?\n \nreply",
      "I don't have any science behind this, but it makes sense that training more complex motions would trigger greater brain improvements.Dance vs basketball or some other high coordination/skill activity might have less disparity than say dance vs. exercise bike.\n \nreply",
      "Speaking as someone who tried to take a tap class as an adult, only to discover it was for people who were already experienced dancers: yes, dance training is vastly more complex than exercise.Update: what absolutely killed me is that we would run through a complex step two or three times, and we were expected to be able to practice at home. I didn't understand what we were doing while we were doing it, there was no way I could reproduce it.\n \nreply",
      "Try taking modern dance. They have the huge choreography that you need to memorize, with like 19 degrees of freedom and then 40 minutes through the class the instructor says, \"flip it over\" and you need to perform the mirror image of all those movements. You can't handle the chirality!\n \nreply",
      "You got thrown into a class above your skill level. That's bad on the teacher for not telling you really. Once you know the basics the rest is easier to build on top of that, but otherwise it's like trying to tell someone about design patterns while they're still struggling with syntax in programming.If you liked the idea, give it a go with beginners again. You'll get back to that higher level soon anyway.\n \nreply",
      "I've danced extensively, and tap can be brutal for the sheer number of steps you need to remember. Other dance disciplines, like ballet, tend to chunk sets of smaller movements into a larger, named one, so once you learn those sequences, it's easy to learn and recall longer routines. The way ballet is put together aligns with advice from brain science about chunking objects in memory for better recall. But tap has few of those chunked sequences, other than the \"time step,\" so you're left trying to parse long strings of very finite instructions. \"Left ball right heel left flap ball change...\" Personally I found it overwhelming and didn't pursue tap into the most advanced levels for that reason.Aside: it seemed like neurotypical folks struggled less with tap than I did as an AuDHD person, so tap may land differently with different neurotypes.\n \nreply",
      ">chunking objects in memory for better recall //Can you share your source on this?Tangentially related wittering (I have flu, I'm a bit illucid rn):\nI do karate, I find sequenced moves to be a real mental struggle. But then I couldn't skip until I was a teenager. I blame/describe that as arrhythmia. I can't clap in time either.\n \nreply",
      ">>chunking objects in memory for better recall //> Can you share your source on this?I haven't seen the research on this, but it makes intuitive sense to me as a musician. It was the only way I could learn long complex pieces.\n \nreply",
      "People are different, I tap danced for years but I found ballet almost impossible to pick up for it's complexity and dumb poses.\n \nreply"
    ],
    "link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0196636",
    "first_paragraph": "\n  Discover a faster, simpler path to publishing in a high-quality journal. PLOS ONE promises fair, rigorous peer review,\n  broad scope, and wide readership \u2013 a perfect fit for your research every time.\n  \n\n     Learn More\n    \n\n      Submit Now\n    \nClick through the PLOS taxonomy to find articles in your field.For more information about PLOS Subject Areas, click\n          here.\n        Loading metricsOpen AccessPeer-reviewedResearch Article\nRoles\n    Conceptualization,\n\n    Data curation,\n\n    Formal analysis,\n\n    Investigation,\n\n    Methodology,\n\n    Supervision,\n\n    Writing \u2013 original draft\n   * E-mail: kathrin.rehfeld@ovgu.deAffiliations\n    German Center for Neurodegenerative Diseases (DZNE), Magdeburg, Saxony-Anhalt, Germany, \n    Institute for Sport Science, Otto-von-Guericke University, Magdeburg, Saxony-Anhalt, Germany\n  \n\n\n\n        http://orcid.org/0000-0001-9775-763X\n      \n\n\nRoles\n    Data curation,\n\n    Formal analysis,\n\n    Writing \u2013 review & editing\n  Affiliation\n    "
  },
  {
    "title": "Update on Reflection-70B (glaive.ai)",
    "points": 54,
    "submitter": "mellosouls",
    "submit_time": "2024-10-03T22:16:54.000000Z",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=41735665",
    "comments": [
      "Sahil Chaudhary of GlaiveAI perpetuated fraud, where he replaced the model that he \"trained\" with other backend ML providers. He still has not given a reason why \"Claude\" the string would be missing, just magically happened, despite the base model, Llama3.1-70B having no issues producing the text \"Claude\" nor the dataset missing the string \"Claude\"!Note that there was additional proof, besides missing the string \"Claude\", by matching the max number of tokens the model was able to produce. This is more technical, but chatGPT, Claude, Llama all have different tokenizers, so words are be broken up into different sections. The API consistently did NOT match the base model tokenizer (Llama), instead, producing the same number of tokens as Claude.Companies and individuals should probably avoid GlaiveAI and Matt Shumer less they get scammed too.\n \nreply",
      "Sorry, I'm having trouble finding more information about this\u2014what is the significance of the model being unable to produce the string \"Claude\"? Was this some sort of half-hearted censorship to prevent it from disclosing its name? Where can I read more?\n \nreply",
      "https://x.com/shinboson/status/1832933747529834747 is the best summary.if you don't have a twitter account: https://threadreaderapp.com/thread/1832933768031588622.html\n \nreply",
      "Thanks, this is helpful!\n \nreply",
      "Here was a HN post which links to a Reddit thread about this. Multiple users find evidence that the model was a lie. The most damning to me was that the tokenizer was not Llama\u2019s, which cannot be explained away.https://news.ycombinator.com/item?id=41484981\n \nreply",
      "they're 140Gb folders, for each checkpoint, yes file corruption happensand as for the fraud part...it was an opensource model release that did not meet the claimed benchmarks when people tried to replicate it\n \nreply",
      "The fraud part was multiple independent sources producing fairly indisputable evidence that their \"hosted\" version of the model was just running GPT and Claude. That alone is enough to completely discredit absolutely everything about this work.As for corruption, I don't believe the excuse \"yes file corruption happens\". They're model weights. If this was trained (in real life) it was done on some serious hardware with disks with error correction. They weren't storing the checkpoints on microSD cards. It's certainly possible that there was really unfortunate luck and there was corruption, but I don't find that excuse to be plausible. Especially when this is your business (and launch!)\n \nreply",
      "Definition of fraud, from Google:* wrongful or criminal deception intended to result in financial or personal gain.* a person or thing intended to deceive others, typically by unjustifiably claiming or being credited with accomplishments or qualities.Since they were advertising GlaiveAI as this magical source of data where they trained a model that performed better than Claude and chatGPT, I think this firmly falls into that camp! Your definitions may be different than mine.\n \nreply",
      "Are you telling me someone trained a huge model, and served it for hours to tons of users, and had only one instance of the checkpoint? I call BS.The model being open-source doesn't mean what they could have gotten away with, or tried to, isn't fraud.\n \nreply",
      "He served tons of people from his personal laptop?  How is that possible?  A 70B LLM is pretty taxing even to serve a single user let along the crush of users that tried out this new hyped model no?  What am I missing?\n \nreply"
    ],
    "link": "https://glaive.ai/blog/post/reflection-postmortem",
    "first_paragraph": ""
  },
  {
    "title": "The Heart of Unix (2018) (ericnormand.me)",
    "points": 76,
    "submitter": "kugurerdem",
    "submit_time": "2024-10-03T19:25:41.000000Z",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=41734047",
    "comments": [
      "The biggest disadvantage of the shell is that, by exchanging data using text, you lose opportunities to check for errors in the output. If you call a function in a programming language and an erroneous output happens, you get a crash or exception. In a shell, you'll get empty lines or, worse, incorrect lines, that will propagate to the rest of the script. This makes it impractical to write large scripts and debugging them gets more and more complicated. The shell works well for a few lines of script, any more than that and it becomes a frustrating experience.\n \nreply",
      "It's even worse than that. Most non-trivial, and some trivial scripts and one liners rely on naive parsing (regex/cut etc) because that's the only tool in the toolbox. This resulted in some horrific problems over the years.I take a somewhat hard line that scripts and terminals are for executing sequential commands naively only. Call it \"glue\". If you're writing a program, use a higher level programming language and parse things properly.This problem of course does tend to turn up in higher level languages but at least you can pull a proper parser in off the shelf there if you need to.Notably if I see anyone parsing CSVs with cut again I'm going to die inside. Try unpicking a problem where someone put in the name field \"Smith, Bob\"...\n \nreply",
      "You might have enjoyed DCL under VMS.It did not immediately succumb to envy of the Korn shell.\n \nreply",
      "The && and || operators let you branch on errors.\n \nreply",
      "that's why the rule #1 of robust shell scripts is \"set -e\", exit on any error. This is not perfect, but helps with most of the errors.\n \nreply",
      "set -euo pipefail, if you're OK with the Bashism.\n \nreply",
      "At the same time, the POSIX shell can be implemented in a tiny binary (dash compiles to 80k on i386).Shells that implement advanced objects and error handling cannot sink this low, and thus the embedded realm is not accessible to them.\n \nreply",
      "I always tell people the best way to learn how to use linux is to read The Unix Programming Environment.\n \nreply",
      "Perl superseded it for almost all of the chapters, except for the C ones. Altough for small programs, for sure it did.Perl used to have an AWK to Perl converter because most of the language could be mapped 1:1 to Perl.UPE would be fine under 9front save for sh (rc) and make (mk).\n \nreply",
      "I liked awk and perl was even better where either more structured (I know, I know) constructs were comfy or I needed perl dbi (which was awesome, what do people use now?) but that was a while ago.  Sort of nuts that awk is much faster on really big columnar (csv etc) data, though.\n \nreply"
    ],
    "link": "https://ericnormand.me/article/the-heart-of-unix",
    "first_paragraph": "Despite all of its warts, I like working in Linux. I've used it for 15\nyears and I've never been as productive in another environment. Most\npeople claim that it's the configurability of Linux that keep the users\ncoming. That may have attracted me at first, but what attracts me now\nis its programmability.Let me be very clear. I'm not saying that Linux is great because I can\npatch the source code to grep and recompile it. In all my years of Unix,\nI've never done anything like that. And I'm not saying that Linux is a\ngreat workstation for programmers because it helps you program better.\nThose are topics for another essay.I am saying that Unix is a programmable environment. When you interact\nwith the shell, you are writing programs to be interpreted. You can\neasily extend the Unix system by writing a shell script, copying it to a\ndirectory in your PATH, and making it executable. Boom. You've got a\nnew program.What's more, that program, if it follows certain simple conventions, is\nnow able "
  },
  {
    "title": "Patent troll Sable pays up, dedicates all its patents to the public (cloudflare.com)",
    "points": 770,
    "submitter": "jgrahamc",
    "submit_time": "2024-10-03T13:05:16.000000Z",
    "num_comments": 263,
    "comments_url": "https://news.ycombinator.com/item?id=41730415",
    "comments": [
      "As a former patent examiner, I was struck by how low the payout for Project Jengo was. $125,000 for all people submitting prior art? (There were hundreds of submissions, so it's split among many people.) I would like to help out with such things and I think I have the experience to do it well, but even being a GS-7 patent examiner making $75,000 per year is a better deal! That's especially true given that Cloudflare's not only expecting people to find prior art, but to also write the legal arguments about why it reads on Sable's claims.If they're serious about their prior art bounty program, they're going to need to increase the bounties. Actual patent search firms charge a lot more money, and even lowly paid bureaucrats make a lot more.\n \nreply",
      "Having talked with several of them, most of the people who submit the prior art as part of Project Jengo would do so even if there were no payout. Several winners have actually asked that the payout be donated back to other organizations fighting patent trolls. This isn't intended to be anyone's full time job. It is intended to reward technical people with industry knowledge who may be able to help surface prior art and are as sick of patent trolls as we are.\n \nreply",
      "Thanks for your comment.Personally, I value my own time well above my job's hourly rate, so I would expect to be paid more, not less, in the situation you describe. I suspect the same is true for many others as well.> most of the people who submit the prior art as part of Project Jengo would do so even if there were no payoutI'd say this is due to selection bias. People who wanted a bigger payout didn't participate.You all's program is basically over now, but I think anyone considering a prior art bounty program in the future should check best practices for bug bounty programs. The two seem similar to me. Paying more will get more and better submissions, and it doesn't seem to be particularly expensive to me.\n \nreply",
      "It was obviously sufficient ;)Paying more doesn't always motivate people more or get more (quality) people to do a thing.  Compensation and associated psychology is complicated, because people are complicated.e.g. I am willingly working very hard at a job where I could make 10x or perhaps even 100x elsewhere with equal or less effort.  And I often spend my time on things that are completely irrational by your types of economic measures or even \"pay to work.\"\n \nreply",
      "I think this is the key point. Voluntary prior art hunts have been a successful strategy for defeating stupid patents[1] for many years now. There may be arguments in favor of bounties for more obscure or less stupid patents, but when the collective demand to fix this problem is so high, the price point is rather low.[1] \"Stupid patents\" is a technical term here -- or it has been since Mark Cuban funded an EFF staffer to challenge bad patents on the condition that the position was titled the \"Mark Cuban Chair to Eliminate Stupid Patents\".https://www.eff.org/press/releases/staff-attorney-daniel-naz...\n \nreply",
      "I'm remembering the time I spent $40,000 worth of time saving maybe a couple grand on bike parts.(I spent a couple months between gigs building a bicycle from parts, and sourcing the parts was the biggest timesink.)\n \nreply",
      "What did you build?\n \nreply",
      "A 29er with a continuously variable transmission in the rear hub, mustache handlebars, wood fenders, and a crank made by a mill in Petaluma.\n \nreply",
      "That sounds great.\nAny pictures anywhere? I need to get out of the \u2018bike must go faster\u2019 rut.\n \nreply",
      "https://bikeindex.org/bikes/1035\n \nreply"
    ],
    "link": "https://blog.cloudflare.com/patent-troll-sable-pays-up/",
    "first_paragraph": ""
  },
  {
    "title": "Serving 70B-scale LLMs efficiently on low-resource edge devices [pdf] (arxiv.org)",
    "points": 204,
    "submitter": "simonpure",
    "submit_time": "2024-10-03T14:11:20.000000Z",
    "num_comments": 53,
    "comments_url": "https://news.ycombinator.com/item?id=41730983",
    "comments": [
      "This is not a memory reduction technique that's somehow magical. Well, it does manage memory with some clever scheduling. The core of this idea is that you can schedule out inference on edge nodes in a memory and bandwidth optimized way that's a bit different than just splitting layers.They propose that right now computation and latency dominate the costs for multi-node inference, and pick a network topology (star) that is savvy to that.That said, it's 26-29 seconds per token for llama2-70b with their 8 edge devices, each using 4 gigs of RAM. That's amazing that they can run it at all, but this isn't going to be viable at the edge with current hardware.I think the paper makes the case that you could probably recruit say your 30 graphics workstations to do much faster inference without just nailing your LAN bandwidth, though.Upshot - interesting paper -- smart ideas, large frontier models still need very exotic hardware and bandwidth interconnects - this may point a way forward to lowering the bandwidth interconnects part of the story.\n \nreply",
      "I think the main advantage here is you COULD run it, even it it takes a while.   That is a step up from current model limitations which require ram or vram to hold the model.I think this lays some groundwork for running a 400B model on a 3090/4090 or even smaller GPU.  If you can get a huge model like that running on a single gpu even if the mean time per token is in the seconds, that's acceptable for many use cases.If this same technique can be used to extend context windows in addition to token autocomplete, that would be great in it's own regard.Hopefully work like this continues as throwing a ton of vram at a model should be regarded as a performance optimization not necessarily a requirement.\n \nreply",
      "> That is a step up from current model limitations which require ram or vram to hold the model.Current? Apple recently published a neat paper on how they optimise for both inference (cpu/gpu) and memory use:  Our method involves constructing an inference cost model that takes into account\n  the characteristics of flash memory, guiding us to optimize in two critical areas:\n  reducing the volume of data transferred from flash and reading data in larger, more contiguous\n  chunks. Within this hardware-informed framework, we introduce two principal techniques.\n  First, \u201cwindowing\u201d strategically reduces data transfer by reusing previously activated neurons,\n  and second, \u201crow-column bundling\u201d, tailored to the sequential data access strengths\n  of flash memory, increases the size of data chunks read from flash memory. These methods\n  collectively enable running models up to twice the size of the available DRAM, with\n  up to 4x and 20x increase in inference speed compared to naive loading approaches in CPU\n  and GPU, respectively.\n\nhttps://news.ycombinator.com/item?id=38704982\n \nreply",
      "Similar but I think the apple approach here requires model modification whereas afaik OPs solution works with the model verbatim.  I could be wrong as I haven't looked into the code but given the specificity of the first article regarding hardware and model, I would assume that.\n \nreply",
      "It's already technically possible to run huge models locally when you don't have the RAM/VRAM needed - llama.cpp can 'mmap' the model from disk.Of course an nvidia 4090 has a memory bandwidth of a 1000 GB per second; a CPU like the i7-13700K has a memory bandwidth of 90 GB per second; and  a high-end NVMe SSD might only have read bandwidth of 10 GB per second.So in approximate terms, an LLM and quantisation level that can produce 10 tokens per second on a 4090 will produce 1 token per second from RAM and a token every 10 seconds from SSD.\n \nreply",
      "I haven't been able to get llama.cpp's mmap logic to work on macOS\n \nreply",
      "> I think the main advantage here is you COULD run it, even it it takes a while.I mean, you COULD run it before as well, even if you don't have enough RAM or VRAM, by using something like `zram`. It'd probably be even slower (and border-line usable, depending on the use case), but it's not impossible to get things to run.\n \nreply",
      "Do you think this could allow distributed inference only, or opens the door for distributed training of the model? Democratization of the models is in part hampered by the total compute a single person or small group can make use of, but if a project like folding@home, but for training large models is possible, it could change the game somewhat.\n \nreply",
      "> I think the paper makes the case that you could probably recruit say your 30 graphics workstations to do much faster inference without just nailing your LAN bandwidth, though.Could be a big deal if it allows cluster of smaller GPUs to compete with a single large VRAM GPU.Unfortunately I\u2019m a few months of date - which is an eternity in LLM inference techniques - so I\u2019m not sure what current state of distributed inference looks like.\n \nreply",
      "llama.cpp supports splitting work across multiple nodes on a network alreadyit essentially just copies a chunk of the model to each one, works well for situations where each machine has limited vram\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2410.00531",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Image Editing with Gaussian Splatting (unite.ai)",
    "points": 213,
    "submitter": "Hard_Space",
    "submit_time": "2024-10-03T12:05:11.000000Z",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=41729891",
    "comments": [
      "Isn't it quite a leap to go from single image to usable 3DGS model? The editing part seems relatively minor step afterwards. I thought that 3DGS typically required multiple viewpoints, like photogrammetry.\n \nreply",
      "It's not \"real\" 3D -- the model doesn't infer anything about unseen portions of the image. They get 3D-embedded splats out of their pipeline, and then can do cool things with them. But those splats represent a 2D image, without inferring (literally or figuratively) anything about hidden parts of the image.\n \nreply",
      "This is what I initially thought, however, I have already witnessed working demoes of 3DGS when using a single viewpoint, but armed with additional auxiliary data that is contextual relevant to the subject.\n \nreply",
      "Yeah exactly, this page doesn't explain what's going on at all.It says it uses a mirror image to do a Gaussian splat. How does that infer any kind of 3D geometry? An image and its mirror are explainable by a simple plane and that's probably what the splat will converge to if given only those 2 images.\n \nreply",
      "Maybe nERF? https://www.matthewtancik.com/nerf\n \nreply",
      "I\u2019ve been exploring some creative applications of Gaussian splats for photography/photogrammetry, which I think have an interesting aesthetic. The stills of flowers on my Instagram if anyone is interested:  https://www.instagram.com/bayardrandel\n \nreply",
      "These are great! What software do you use, and what does your pipeline look like?If you wanted to capture a full 3D scene, my experience with photogrammetry and NeRFs has been that it requires a tremendously large dataset that is meticulously captured. Are Gaussian splat tools more data efficient? How little data can you get away with using?What are the best open source Gaussian Splat tools for both building and presenting? Are there any that do web visualization particularly well?I might have to get back into this.\n \nreply",
      "Thanks very much! I use Polycam on iOS for photogrammetry and generating Gaussian splats from stills. It seems to work remarkably well, but has a subscription fee (given there's processing on their servers this seems reasonable). Typically to build a splat model takes about 30-50 stills for good results, depending on the subject.The only open source tool I use in my workflow is CloudCompare (https://www.danielgm.net/cc/), for editing/cleaning point cloud data.For animation I primarily use Touch Designer which is a node based visual programming environment, exporting splats as point clouds, and Ableton/misc instruments for sound.No idea about web visualisation, but interesting idea!\n \nreply",
      "Have you tried Kiri Vs polycam?I was using kiris dev mode and then running that through nerfstudio to make nerfs and I'm wondering if polycam might give higher quality but I can't seem to find anyone else whose been doing this. I guess I might have to do some tests to compare.+1 to this workflow. TouchDesigners point transform TOP is great for aligning too.\n \nreply",
      "I learned about 3D Gaussian Splatting from the research team at work just 2 weeks ago, and they demoed some incredible use cases. This tech will definitely become mainstream in camera technologies.\n \nreply"
    ],
    "link": "https://www.unite.ai/image-editing-with-gaussian-splatting/",
    "first_paragraph": ""
  },
  {
    "title": "MikroPhone: A privacy enhanced, simple and featured RISC-V mobile phone (mikrophone.net)",
    "points": 92,
    "submitter": "voxadam",
    "submit_time": "2024-10-03T15:41:34.000000Z",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=41731769",
    "comments": [
      "It's a bit of an annoyance when products talk a lot about \"privacy\" and \"security\", but never once mention what sort of threat model they are private/secure against.Then add in something like a bespoke (unvetted?) communication protocol on top and my eyes really start to roll.The people who really want privacy & security enough to be willing to buy something like this will want a lot more detail than what is offered here.\n \nreply",
      "> ...but never once mention what sort of threat model they are private/secure against.You know, they're Secure(TM)!  Against Threats(TM)!  Buy me if you're scared of Threats(TM)!Threat modeling (\"Secure from who?  Under what conditions?\") sort of stuff just doesn't seem to be a thing that's taught these days outside certain weird circles.  And certainly something this project hasn't touched on in the slightest.  But, yes, I was having to keep my eyeballs constrained too.As much as it pains me, I think the most \"generally secure phone\" out there, for at least the sort of threat models this phone handwaves at (journalists, human rights activists) is a recent (last generation or two) iPhone with Lockdown enabled - and then shut down nightly, and carried shut down through any sort of sensitive environments.  I would be inclined to go with an iPhone SE3, moreso than one of the mainline devices, simply for fingerprint scanning versus the FaceID stuff.  You can't \"point a phone at me\" and unlock it with my fingerprint, but you can with FaceID in a wide enough range of situations to be concerning.  Set a longer than usual PIN/passphrase, and be careful where you enter it.As far as I understand the boot process, Apple has largely fixed a lot of the \"before first unlock\" type attacks with their secure enclave.  They fixed that rather well after the battle with the FBI, and seem to have continued hardening and improving that process (hence my recommendation for the latest generation or two of device - there are changes in the boot security flows every now and then, and I assume they matter, at some point).Then Lockdown, as near as I can tell, does a very fine job of simply closing the common attack points used.  Most of the \"good\" attacks on iPhone users (at least the ones I know of...) are through the various \"texting-esque\" endpoints with weird image formats, or a browser based Javascript/JIT exploit, and Lockdown does a fine job of simply refusing most of the paths these use.  Weird image formats simply aren't rendered.  URLs in text messages aren't accessed and previewed.  The Javascript engine removes all JIT capabilities, WebRTC, WebGL, and other \"suitably complex that it's probably exploitable\" sort of features.It's not perfect, but were I an individual who believed I was under actual threat for this sort of stuff, I'd 100% use a secured iPhone (possibly with some of the Mobile Device Management features configured by a trusted person to disable the USB port and such things) over a random device like this.  Sorry, open JTAG ports means it's \"comically insecure\" against anyone with local access and the time to bother doing anything with it.And of course, don't keep location services on, don't install a ton of apps, etc.  The usual if you're concerned about any of this.I don't like that this is the state of the world of secure computing, but it certainly seems to be it, to me.\n \nreply",
      "One of the amusing things I thought about with acquiring a \"secure\" iPhone was that you should just buy it from a random Walmart in the middle of nowhere in America.The supply chain is so wide and vast, and even if they did tamper with it, the iPhone is basically the only phone out there that multiple third parties have ran through CT scanners, X-ray machines, and silicon R/E just to show off the tech inside the phone like a bunch of nerds for clicks, meaning you have lots of good reference material to check against for any potential tampering of your own acquisition.\n \nreply",
      "This. And no 2G/3G to protect against SS7 threats. 4G/5G only can be quite limiting but in this threat model you want SS7 attacks out.\n \nreply",
      "This is like, barely risc-v. As far as I can tell, there's a risc-v management micro, an esp32 that I'm not easily finding a part number for so may as well be Tenscilica, and an app processor that's ARM based. I don't understand the GPU chip if you have the app processor, and I don't understand the management micro if you have custom ESP32 firmware. And a lot of SoMs have WiFi + Bluetooth on board. So I also don't understand the ESP32. This really feels like it could be a card-edge SOM, battery, HMI, and modem. As per usual I find this project needlessly complicated and buzzwordy.\n \nreply",
      "> an esp32 that I'm not easily finding a part number for so may as well be TenscilicaOn their dev board it's the usual esp32-wroom. They also say their esp32 firmware needs esp-idf 4.2, which doesn't support any of the risc-v esp32. So it is xtensa.I agree that everything could be done with the esp32 alone (well, an wrover with extra RAM) but this project seems to be just a guy experimenting and having fun! Although I can see how someone might be cynical regarding this project being deliberately complicated/overengineered just to extract more money from the nlnet fund.\n \nreply",
      "My pet peeve on open-source, *-focused hardware: it should start with an artistic sketch and a mockup, not the final board and a shell wrapped around as an afterthought.Valve[1] reportedly made over 100 mockups before settling on the final shape, most of them representing shapes only. Apple[2] had at least five iterations of nearly indistinguishable mockups for one of iPhone models that were discovered by fans.It is certainly possible to build a radio equipment by starting from a block diagram and installation into enclosure, but that's development process for low volume technical instruments which measure of utility is electronic performance. A consumer product should look and feel good in hand, even when it's dead.1: https://www.rockpapershotgun.com/valves-steam-deck-prototype...2: https://www.youtube.com/watch?v=GXAsLCAbNGY\n \nreply",
      "Hm.  I'm not sure what to make of this, really.The concept of a RISC-V based \"assemble it yourself\" phone is solid enough - there have been the PiPhone concepts based around a Pi Zero for long enough, and while I don't think they're terribly usable, they're also a fun looking little project.But then they throw the ElipticCP concept on top, and sort of handwave it being \"secure\" if you're talking to someone else who is using a similar device, or similar capabilities, or such.  And, unfortunately, there's not a lot of information about that I'm seeing (or, that which there is seems rather vague and handwaving).https://mikrophone.net/about.html> The security of the whole system is not compromised even though none of these modules is trusted, because all sensitive data is encrypted by the central MCU before sending it to a communication module. Secure communication uses a protocol EllipticCP originally designed for this project. It provides end-to-end encryption and an additional anonymizing layer based on the principle of onion routing. In order for a security protocol to function to its full extent, the end recipient in the communication channel also needs to use mikroPhone or some other phone with comparable security performances (in other words, both communication parties must be secure enough).There's a lot of words in here that sound good, but there's a serious lack of details, and then when you go to build the phone, you have open JTAG ports to the device.So I'm not really sure what threat model they're dealing with exactly.  \"People who can build their own hardware and firmware, who work in investigative journalism or human rights activists, who have iron clad control over their hardware, who want to talk to other people with identical hardware,\" maybe?  It seems designed to counter remote threats only, and without a lot more details as to what it's doing, it's hard to say if it is or isn't doing that competently.  I don't have the time right now to go dig through their firmware to see, unfortunately.If it weren't a build it yourself sort of thing (\"Here's the schematics, go get boards fabricated!\") it would trip my honeypot sensors (\"Secure Phones!\" being more government ops than anything actually useful, IMO), but... it's not that, fairly obviously?Dunno.  I doubt it would work on any US carriers, they're all VoLTE only now. :/\n \nreply",
      "Tbh I would accept anything usable without being bound neither to Google nor Apple. Like a Linux phone but with usable apps which is quite important.For example Samsung gets free MP3 player and more important, background-running voice recorder, which is extremely important for me, but was impossible to find on OnePlus One.\n \nreply",
      "You should try GrapheneOSIt is basically Android with all the crap from Google removed.\n \nreply"
    ],
    "link": "https://mikrophone.net/",
    "first_paragraph": "The goal of this project is to develop a privacy enhanced, simple and fully featured mobile phone. For more information about the project see about page.git repository can be cloned by running the following command:Repository contains following directories:For more information go to historyFor building instructions visit how to build my mikroPhone.Hardware licensed under the CERN OHL v1.2Software licensed under the GPLv2This project is funded through NGI0 Entrust, a fund established by NLnet with financial support from the European Commission's Next Generation Internet program. Learn more at the NLnet project page.\n"
  },
  {
    "title": "Skymont: Intel\u2019s E-Cores reach for the Sky (chipsandcheese.com)",
    "points": 18,
    "submitter": "zdw",
    "submit_time": "2024-10-03T21:34:14.000000Z",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=41735257",
    "comments": [
      "there's never a time I've been glad an article used Excel's 3D surface plot lol\n \nreply",
      "So it's a huge step over Crestmont, but in practice you can't tell?\n \nreply"
    ],
    "link": "https://chipsandcheese.com/2024/10/03/skymont-intels-e-cores-reach-for-the-sky/",
    "first_paragraph": ""
  },
  {
    "title": "DOSBox-X: Enhanced Fork of DOSBox for Expanded DOS and Retro PC Support (github.com/joncampbell123)",
    "points": 45,
    "submitter": "Fission_Strike",
    "submit_time": "2024-10-03T17:31:40.000000Z",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=41732851",
    "comments": [
      "Whenever I see a DOS thread, I like to remind people of eXoDOS, one of the most impressive archival efforts of every DOS game ever. Complete with manual scans, extras, and all neatly sorted in a launcher.\n \nreply",
      "86box is also great, especially for retro PC gaming support. I can run Windows 98 with a Pentium 233 MMX and Voodoo3 on relatively modest hardware. (AMD 7840HS)https://86box.net\n \nreply",
      "One feature of DOSBox-X I've come to really appreciate when reverse engineering is that you can toggle the debug log on and off.  Additionally, it can display the current VGA palette in the main window.\n \nreply",
      "DOSBox has many many forks https://emulation.gametechwiki.com/index.php/DOSBoxhttps://github.com/dosbox-staging/dosbox-staging/wiki/DOSBox...Personally I use DOSBox Pure with RetroArch\n \nreply",
      "I found DOSBox-ECE more stable than DOSBox-X. I believe they provide similar set of features.EDIT: Oops, talked too soon. Apparently DOSBox-ECE has been EOLed :(\n \nreply",
      "My experience with dosbox-x is similar. dosbox-staging is probably the most stable and it's still updated.\n \nreply",
      "DosBox-X runs really well out of the box on my Mac M1, and it has some built-in shaders that try to simulate curved CRT geometry that are pretty fun to play with.Heads up though - it has some coloration/palette issues around using the built-in capture tool to record video, but this is specifically related to Macs.\n \nreply",
      "It's too bad that DOSBox-X doesn't have the Chrous/Reverb feature found in Dosbox Staging.  This was a feature of the Sound Blaster AWE64 sound card, and it really enhanced the sound of Adlib music, almost making it sound like a wavetable.\n \nreply",
      "I'm a big fan, have been using it for years to play games. It's got a GUI that covers most of the DOSBox config, save states, and can run things like Windows 3 and 95 without much fuss.\n \nreply",
      "Sounds really promising. especially 3dfx emulation...\n \nreply"
    ],
    "link": "https://github.com/joncampbell123/dosbox-x",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        DOSBox-X fork of the DOSBox project\n      Welcome to the DOSBox-X project homepage located on GitHub.DOSBox-X is a cross-platform DOS emulator based on the DOSBox project (www.dosbox.com).Like DOSBox, it emulates a PC necessary for running many MS-DOS games and applications that simply cannot be run on modern PCs and operating systems. However, while the main focus of DOSBox is for running DOS games, DOSBox-X goes much further than this. Started as a fork of the DOSBox project, it retains compatibility with the wide base of DOS games and DOS gaming DOSBox was designed for. But it is also a platform for running DOS applications, including emulating the environments to run Windows 3.x, 9x and ME and software written for those versions of Windows. By adding official support for Windows 95, 98, ME emulation and acceleration, we hope tha"
  },
  {
    "title": "Mux (YC W16) is hiring a video engineer (mux.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-10-03T21:00:23.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.mux.com/jobs?job=video",
    "first_paragraph": "Mux is video for developers. Our mission is to democratize video by solving the hard problems developers face when building video: video encoding and streaming (Mux Video), video monitoring (Mux Data), and more. Video is a huge part of people\u2019s lives, and we want to help make it better.We\u2019re committed to building a healthy team that welcomes a diverse range of backgrounds and experiences. We want people who care about our mission, are ready to grow, believe in our values (from Be Human to Turn Customers Into Fans), and want to make the people around them better.You\u2019ll be joining a tight-knit team with experience at places like Google, YouTube, Twitch, Zencoder, Fastly, and more. Our founders previously started (and sold) Zencoder, an early leader in cloud video technology, and authored Video.js, the biggest HTML5 video player on the web. We organize Demuxed, the premiere conference for video engineers in the world.We\u2019re backed by top investors like Coatue, Accel, Andreessen Horowitz, a"
  },
  {
    "title": "AI agent promotes itself to sysadmin, trashes boot sequence (theregister.com)",
    "points": 38,
    "submitter": "DirkH",
    "submit_time": "2024-10-03T23:24:37.000000Z",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=41736125",
    "comments": [
      "> Shlegeris said he uses his AI agent all the time for basic system administration tasks that he doesn't remember how to do on his own, such as installing certain bits of software and configuring security settings.Back in the day, I knew the phone numbers of all my friends and family off the top of my head.After the advent of mobile phones, I\u2019ve outsourced that part of my memory to my phone and now the only phone numbers I know are my wife\u2019s and my own.There is a real cost to outsourcing certain knowledge from your brain, but also a cost to putting it there in the first place.One of the challenges of an AI future is going to be finding the balance between what to outsource and what to keep in your mind - otherwise knowledge of complex systems and how best to use and interact with them will atrophy.\n \nreply",
      "There is also a cost to future encoding of relevant information - I (roughly) recall an experiment with multiple rounds of lectures where participants took notes in a text document, and some were allowed to save the document while others weren't.Those who could save had worse recall of the information, however they had better recall of information given in the next round without note taking. Suggests to me there are limits to retention/encoding in a given period, and offloading retention frees resources for future encoding in that period.Also that study breaks are important :)Anecdotally, I often feel that learning thing 'pushes another out', especially if the things are learnt closely together.Similarly, I'm less likely to retain something if I know someone I'm with has that information - essentially indexing that information in social knowledge graphs.Pros and cons.\n \nreply",
      "I can still remember all my high school friends' phone numbers though. Just not the numbers of anyone I met in the 30 years since.\n \nreply",
      "Dood, it's not \"deciding\" to do anything. It's autocompleting commands that statistically follow other commands. It might do anything.\n \nreply",
      "This sounds exactly like what I would have done at age 18 cluelessly searching the internet for advice while updating a fresh debian so I can run some random program.\n \nreply",
      "I have done this ... and worse. Fun times.My favorite was waiting 2 days to compile Gentoo then realizing I never included the network driver for my card. But also this was the only machine with internet access in the house.Downloading network drivers through WAP on a flip phone ... let's say I never made that mistake again lol.\n \nreply",
      "The agent is set to respond to the terminals output, stop / finish the task\n \nreply",
      "I wonder if we'll see an AI Agent do a Crowdstrike-tier oops in our lifetime.\n \nreply",
      "Who knows, it might even happen at Crowdstrike!\n \nreply",
      "That seems inevitable with how we are going.\n \nreply"
    ],
    "link": "https://www.theregister.com/2024/10/02/ai_agent_trashes_pc/",
    "first_paragraph": ""
  },
  {
    "title": "FLUX1.1 [pro] \u2013 New SotA text-to-image model from Black Forest Labs (replicate.com)",
    "points": 170,
    "submitter": "fagerhult",
    "submit_time": "2024-10-03T13:53:47.000000Z",
    "num_comments": 113,
    "comments_url": "https://news.ycombinator.com/item?id=41730822",
    "comments": [
      "Better link https://blackforestlabs.ai/announcing-flux-1-1-pro-and-the-b...",
      "Flux is so frustrating to me. Really good prompt adherence, strong ability to keep track of multiple parts of a scene, it's technically very impressive. However it seems to have had no training on art-art. I can't get it to generate even something that looks like Degas, for instance. And, I can't even fine tune a painterly art style of any sort into Flux dev. I get that there was working, living artist backlash at SD and I can therefore imagine that the BFL team has decided not to train on art, but, it's a real loss. Both in terms of human knowledge of, say composition, emotion, and so on, but also for style diversity.For goodness sake, the MET in New York has a massive trove of open CC0 type licensed art. Dear BFL, please ease up a bit on this, and add some art-art to your models, they will be better as a result.\n \nreply",
      "I've had a similar experience, incredible at generating a very specific style of image, but not great at generating anything with a specific style.I suspect we'll see the answer to this is LoRAs. Two examples that stick out are:- Flux Tarot v1 [0]- Flux Amateur Photography [1]Both of these do a great job of combining all the benefits of Flux with custom styles that seem to work quite well.[0] https://huggingface.co/multimodalart/flux-tarot-v1\n[1] https://civitai.com/models/652699?modelVersionId=756149\n \nreply",
      "I like those, and there's an electroshock lora that's just awesome out there. That said, Tarot and others like it are \"illustrator\" type styles with extra juice. I have not successfully trained a LoRa for any painting style, Flux does not seem to know about painting.\n \nreply",
      "I'm curious to give this a go. I've been training a lot of LoRAs for FLUX dev recently (purely for fun). I'm sure there must be a way to get this working.Here are a few I've recently trained: https://civitai.com/user/dvyio\n \nreply",
      "This looks really good! What is your process to get this kind of high quality LoRAs?\n \nreply",
      "Thank you!A reasonable amount of training images (50 or so), and then I train for 2,000-ish steps for a new style.Many of them work well with Flux, particularly if they're illustration-based. Some don't seem to work at all, so I didn't upload those!\n \nreply",
      "How long does this take, and on what equipment? It's amazing to me that you can do this from just 50 images, I would have thought tens of thousands.\n \nreply",
      "It's very impressive. I aim for around 50 images if I'm training a style, but only 10 to 20 if training a concept (like an object or a face).I have a MacBook Air so I train using the various API providers.For training a style, I use Replicate: https://replicate.com/ostris/flux-dev-lora-trainer/trainFor training a concept/person, I use fal: https://fal.ai/models/fal-ai/flux-lora-fast-trainingWith fal, you can train a concept in around 2 minutes and only pay $2. Incredibly cheap. (You could also use it for training a style if you wanted to. I just found I seem to get slightly better results using Replicate's trainer for a style.)",
      "Wow, fantastic, thanks! I thought it would be much, much more expensive than this. Thanks for the info!"
    ],
    "link": "https://replicate.com/black-forest-labs/flux-1.1-pro",
    "first_paragraph": ""
  },
  {
    "title": "Solving methane mysteries with satellite imagery (datadesk.eco)",
    "points": 111,
    "submitter": "ltrg",
    "submit_time": "2024-10-03T13:25:06.000000Z",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=41730574",
    "comments": [
      "Meanwhile: https://cleantechnica.com/2024/05/03/fossil-fuel-companies-b...Looks like another arms race. :-(\n \nreply",
      "This is factually incorrect and has the direction of causality wrong.Enclosed combustors are _more_ efficient than flares, and can be tested to show that they achieve complete combustion of methane (unlike flares, which do not combust all methane.) Because of this efficiency delta, enclosed combustors were introduced to adhere to new air quality regulations.I.e. regulators forced companies to install them to improve their emissions; they aren't being installed to hide emissions.\"Enclosed flaring is, in truth, probably less efficient than a typical flare. It\u2019s better than venting, but going from a flare to an enclosed flare or a vapor combustor is not an improvement in reducing emissions\", based on vibes from a former regulator from the linked article, is incorrect. E.g. see https://www.sciencedirect.com/science/article/pii/S266679082...\n \nreply",
      "That's around flaring, which is a bit different. Energy companies are very likely to buy the same data.  Detecting methane leaks is a _good_ thing for them, both from an \"avoiding fines\" perspective and also from a \"this is infrastructure we _want_ to fix\" perspective.Banning routine flaring is a very good thing that needs to happen in more places.  You _do_ still need to flare. There are lots of time periods where it will be required for safety reasons.  But currently, it's common to simply flare methane that's produced instead of trying to use it. Methane can't be easily transported, and you need a pipeline to a populated area to use it unless you build expensive LNG facilities or slightly less expensive facilities to reinject it back into the subsurface.  So remote oil fields are designed to flare off the methane that's produced alongside oil production, often for vast quantities of methane.  That's \"routine flaring\".  It's better (both from a safety perspective and a greenhouse gas perspective) than directly releasing it. However, it's far better to reinject it back into the reservoir (or another reservoir) or otherwise find some use for it than to flare it.Routine flaring is used quite simply because regulators allow it. If you change the regulations, then companies will take the more expensive route or develop other resources.  If you don't, then they're more or less legally required (read: shareholders _will_ have grounds to dismiss the CEO) to take the legal and much cheaper route of flaring methane that can't easily be sold.  Can you really justify to shareholders that you're going to spend an extra several tens of billions USD to do something that isn't required and that your competitors aren't and that won't increase profits at all? The regulatory environment has to change for that to happen, but it's a patchwork and not some global thing. The EU has been leading there.But detecting flares (even \"hidden\" ones) is _much_ easier than detecting methane leaks.  Methane leaks are pretty damned insidious and hard to find. That's a big part of why they're so common. Hyperspectral imaging is _really_ damned cool, and while I'm certainly biased, the Tanager satellite they used there is really really neat.\n \nreply",
      "Edit: Apparently that's the airborne equivalent of Tanager, not Tanager.  (Same instrument design, but one is on a plane and one just launched into space not-too-long-ago.)\n \nreply",
      "Ground-based laser methane detection is sensitive enough to quantify hidden emissions, no matter how diffuse gas companies make the plume. Here's two companies operating in this space:Sensirion: https://www.sensirion-connected.com/emissions-monitoringLongpath Technologies: https://www.longpathtech.com\n \nreply",
      "Venting is like an order of magnitude worse than flaring right? So until we've dealt with most of the venting there's not much benefit in going after the flaring operations right? We should encourage flaring as a way to solve venting?\n \nreply",
      "Yes, enclosed flaring is better than venting. However it makes it more difficult for third-party monitoring, the linked article mentions this:>\"If you enclose the flare, people don\u2019t see it, so they don\u2019t complain about it. But it also means it\u2019s not visible from space by most of the methods used to track flare volumes.\u201d\n \nreply",
      "This article might benefit from a bit more numerical data:    CO\u2082 Radiative Forcing:\n\n        1950: Approximately 0.58 W/m\u00b2 @ 310 ppm \n\n        2020: Approximately 2.13 W/m\u00b2 @ 414 ppm\n\n\n    CH\u2084 Radiative Forcing:\n\n        1950: Approximately 0.25 W/m\u00b2 @ 1.15 ppm\n\n        2020: Approximately 0.59 W/m\u00b2 @ 1.86 ppm\n\nMethane in the atmosphere is oxidized to CO2 with about a 6-year halflife, so:20-year timescale: CH\u2084 is approximately 84-87 times more efficient than CO\u2082.100-year timescale: CH\u2084 is approximately 28-34 times more efficient than CO\u2082.The other thing to keep in mind is the removal rate:> \"Roughly 56% of annual fossil CO\u2082 emissions are absorbed by natural sinks\u201429% by the biosphere and 23% by the oceans\u2014while 44% remains in the atmosphere, driving global climate change. For CH\u2084, 90% is removed by atmospheric oxidation within roughly a decade, with a small fraction absorbed by soils.\"The bottom line? If human civilization really wants to stabilize the concentration of CO2 and CH4 in the atmosphere - which ideally will lead to a stabilization of global temperature and a new climate normal (certainly warmer and wetter, much like Pliocene conditions of 2-5 mya), then elimination of fossil fuel combustion as an energy source really is the only plausible option.\n \nreply",
      "> CO\u2082 Radiative Forcing:That's an interesting scaling. For a ~30% increase in ppm, it's ~400% in W/m^2\n \nreply",
      "It's because of the high-altitude IR windows in the absorption spectrum as I understand it.  If CO2 is added at 1 km it really has no effect there since CO2 absorption in these windows is mostly saturated already, but as you climb to higher altitudes ~12 km the lower pressures mean those windows clear up - but a relatively small increase in CO2 starts filling in these windows.  The best source I've found for explaining this at the non-technical level is:https://history.aip.org/climate/Radmath.htm#L_0165\n \nreply"
    ],
    "link": "https://blog.datadesk.eco/p/methane-mysteries",
    "first_paragraph": ""
  },
  {
    "title": "Improving Accessibility Using Vision Models (myswamp.substack.com)",
    "points": 30,
    "submitter": "bearjaws",
    "submit_time": "2024-10-03T18:48:26.000000Z",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=41733659",
    "comments": [
      "I don't understand \"The Results\" graph.The x-axis has integers, 0, 1, 2, 3, 4, 5, 6, but the text talks about models struggling at the 30 character mark? On the graph they all start getting bad around 3, depending on what you mean by bad. Is the x-axis tens of characters??Anyway...> anything longer than 20 characters would tend to have more issues, we flagged those for manual review.Even though the failure rate was smaller, is it OK if several of the shorter equations are wrong? Maybe they should have manually reviewed all of them.Edit: Now I see someone else brought up the x-axis issue. There's a response that seems to say the x-axis is buckets of 10 characters. I guess the update hasn't gone through yet.\n \nreply",
      "Funnily enough, the images in the article do not have actually useful alt text and like every image in Substack I've encountered so far have no useful captions either.\n \nreply",
      "How is the alt-text not useful? I even went through the effort of putting the data in the alt text for the bar chart. I tend to think of alt text as proving the same context as the image, for example the line chart is meant to convey how 1.5-flash outperforms 4o, but I am not going to embed each discrete data point in the alt text.\n \nreply",
      "3 out of 5 images on the post have empty alt text (alt=\"\"). most substacks are pretty careless about alt text and so previous poster is just noting that your accessibility post follows this trend. (It's worth noting the post you made previous to this has 0 out of 4 images with alt text.)\n \nreply",
      "Checking the later pictures that you talk about, the alt text is found indeed. My recommendation though would be to give a summary of the data and not the conclusion. E.g. Gemini flash has error rate of x% while the others are y% and z%.\n \nreply",
      "Maybe something is lost in the translation, but here it is what my screen reader makes out of the article:Along the way we realized some of our math courses had not been updated in quite some time, and some schools were still leveraging these courses to teach. \nImages for equations are bad m\u2019kayIt was immediately apparent was the use of images to represent equations like this:\nhttps%3A%2F%2Fsubstack-post-me\u2026\nhttps%3A%2F%2Fsubstack-post-me\u2026\nThis is not great\u2026 the font is a bit on the smaller side and the font itself is not very legible, in my non-font expert opinion. Making matters worse, there is no alt-text provided that can explain the equation.\n \nreply",
      "I've had great success to convert math pics to latex using qwen2-vl\n \nreply",
      "Funny Google just released moments ago - gemini-1.5-flash-8b which scores slightly lower on vision. For clarity this is on the \"older\" gemini-1.5-flash.https://developers.googleblog.com/en/gemini-15-flash-8b-is-n...\n \nreply",
      "What is the measurement on the x-axis in the graph?? The text is talking about equations of 20 or 30 characters, but the graph goes up to...6. Six what?? Characters? Terms? If it's characters, why do we only get to see the performance from 1-6, when apparently 7% of equations had more than 20?\n \nreply",
      "That's a fair point, I bucketed them into lengths of 1-10, 11-20, 21-30. I'll do a quick update.\n \nreply"
    ],
    "link": "https://myswamp.substack.com/p/improving-accessibility-using-vision",
    "first_paragraph": ""
  },
  {
    "title": "HTTrack Website Copier (github.com/xroche)",
    "points": 52,
    "submitter": "iscream26",
    "submit_time": "2024-10-03T18:53:35.000000Z",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=41733714",
    "comments": [
      "Funny seeing this here now, as I _just_ finished archiving an old MyBB PHP forum. Though I used `wget` and it took 2 weeks and 260GB of uncompressed disk space (12GB compressed with zstd), and the process was not interruptible and I had to start over each time my hard drive got full. Maybe I should have given HTTrack a shot to see how it compares.If anyone wanna know the specifics on how I used wget, I wrote it down here: https://github.com/SpeedcubeDE/speedcube.de-forum-archiveAlso, if anyone has experience archiving similar websites with HTTrack and maybe know how it compares to wget for my use case, I'd love to hear about it!\n \nreply",
      "I've tried both in order to archive EOL websites and I've had better luck with wget, it seems to recognize more links/resources and do a better job so it was probably not a bad choice.\n \nreply",
      "I don't get it: last release 2017 while in github I see more releases...so, did developer of the github repo took over and updating/upgrading?  very good!\n \nreply",
      "Great tool. Does it still work for the \"modern\" web (i.e. now that even simple/content websites have become \"apps\")?\n \nreply",
      "Nope. It is for the classic web (the only websites worth saving anyway).\n \nreply",
      "Even for classic web, if it's behind cloudflare, then HTTrack no longer works.It's a sad point to be at. Fortunately, the single file extension still works really well for single pages, even when they are built dynamically by JavaScript on the client side. There isn't a solution for cloning an entire site though, at least that I know of\n \nreply",
      "If it is cloudflare human verification, then httrack will have an issue. But in the end it's just a cookie, you can use a browser with JS to grab the cookie, then feed it to httrack headers.If cloudflare ddos protection is an issue, you can throttle httrack requests.\n \nreply",
      "One time I was trying to create an offline backup of a botanical medicine site for my studies. Somehow I turned off depth of link checking and made it follow offsite links. I forgot about it. A few days later the machine crashed due to a full disk from trying to cram as much of the WWW as it could on there.\n \nreply",
      "Good ol' days\n \nreply",
      "oh wow that brings back memories. I have used httrack in the late 90s and early 2000's to mirror interesting websites from the early internet, over a modem connection (and early DSL)Good to know they're still around, however, now that the web is much more dynamic I guess it's not as useful anymore as it was back then\n \nreply"
    ],
    "link": "https://github.com/xroche/httrack",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        HTTrack Website Copier, copy websites to your computer (Official repository)\n      Copy websites to your computer (Offline browser)HTTrack is an offline browser utility, allowing you to download a World Wide website from the Internet to a local directory, building recursively all directories, getting html, images, and other files from the server to your computer.HTTrack arranges the original site's relative link-structure. Simply open a page of the \"mirrored\" website in your browser, and you can browse the site from link to link, as if you were viewing it online.HTTrack can also update an existing mirrored site, and resume interrupted downloads. HTTrack is fully configurable, and has an integrated help system.WinHTTrack is the Windows 2000/XP/Vista/Seven release of HTTrack, and WebHTTrack the Linux/Unix/BSD release.Main Website:\nhtt"
  }
]