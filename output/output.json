[
  {
    "title": "Meteorite 200 times larger than one that killed dinosaurs reset early life (chemistryworld.com)",
    "points": 15,
    "submitter": "kristianp",
    "submit_time": "2024-11-17T00:02:48 1731801768",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42160716",
    "comments": [
      "How could they possibly know?\n \nreply",
      "> Drabon and her colleagues went in search of evidence of ancient major impacts in a remote area south of Kruger National Park in South Africa. There they sought out rocky outcrops containing a layer of spherules \u2013 molten droplets formed following a major meteorite impact that rained down over huge swathes of the planet. There are eight such spherule bands in this area, each preserving an ancient impact event.> While the impact crater itself is long gone, analysis of rocks from 3.26 billion years ago tells a tale of planetary devastation. The layer of spherules from this huge impact was 15 to 20cm thick in places, compared with less than a centimetre for the famed dinosaur-killing meteorite, says Drabon.\n \nreply",
      "Publications are institutionally incapable of publishing an article (and readers incapable of appreciating) that says, A Paper Investigates How Early Archean Impacts Might Have Possibly Affected The Development of Nascent Life\n \nreply",
      "Science, son. Science.\n \nreply"
    ],
    "link": "https://www.chemistryworld.com/news/meteorite-200-times-larger-than-one-that-killed-dinosaurs-reset-early-life/4020391.article",
    "first_paragraph": ""
  },
  {
    "title": "SICP: The only computer science book worth reading twice? (2010) (simondobson.org)",
    "points": 202,
    "submitter": "pieterr",
    "submit_time": "2024-11-16T17:23:39 1731777819",
    "num_comments": 90,
    "comments_url": "https://news.ycombinator.com/item?id=42157558",
    "comments": [
      "It\u2019s interesting, SICP and other many other \u201cclassic\u201d texts talk about designing programs, but these days I think the much more important skill is designing systems.I don\u2019t know if distributed systems is consider part of \u201cComputer Science\u201d but it is a much more common problem that I see needs to be solved.I try to write systems in the simplest way possible and then use observability tools to figure out where the design is deficient and then maybe I will pull out a data structure or some other \u201ccomputer sciency\u201d thing to solve that problem. It turns out that big O notation and runtime complexity doesn\u2019t matter the majority of the time and you can solve most problems with arrays and fast CPUs. And even when you have runtime problems you should profile the program to find the hot spots.What computer science doesn\u2019t teach you is how memory caching works in CPUs. Your fancy graph algorithm may have good runtime complexity but it completely hoses the CPU cache and you may have been able to go faster with an array with good cache usage.The much more common problems I have is how to deal with fault tolerance, correctness in distributed locks and queues, and system scalability.Maybe I am just biased because I have a computer/electrical engineering background.\n \nreply",
      "Scouring SICP cannot imbue the student with mechanical sympathy any more than poring over analysis of Coltrane makes me a saxophonist.Nevertheless. It must be done. Theory and practice.\n \nreply",
      "> these days I think the much more important skill is designing systemsThat's true, but that doesn't mean that there is no value in having an understanding of how established technology works under the hood.> What computer science doesn\u2019t teach you is how memory caching works in CPUs.That is also a very good point.  There is a lot of daylight between the lambda calculus and real systems.\n \nreply",
      "Have you seen\"Software Design for Flexibility: How to Avoid Programming Yourself into a Corner\"\nby Chris Hanson and Gerald Jay SussmanIt's from 2021.\n \nreply",
      "I hadn't, that looks excellent.\n \nreply",
      "IMO it's not excellent. It's not like SICP, it's obtuse for no reason, I find it a hard slog. Flexibility is good but it seems to try to make every bit of your program flexibile and pluggable and you just need to do something eventually.My opinion, I'd welcome others on the book there was a small splash when it came out but not much discussion since.\n \nreply",
      "I haven't read the book, but my experience is that the way to make things flexible is to make them simple as possible.When I've used (or built) something that was built in the style like you're talking about, it's almost always wrong, and the extra complexity and stuff now makes it harder to do right. It's not surprising: unknown future requirements are unknown. Over building is trying to predict the future.It's like someone building a shed and pouring a foundation that can work for a skyscraper. Except it turns out what we needed was a house that has a different footprint. Or maybe the skyscraper is twice the height and has a stop for the newly-built underneath. Now we have to break apart the foundation before we can even begin work on new stuff; it would have been less work if the original just used a foundation for a shed.\n \nreply",
      "Is there another book you'd recommend - more recent than SICP - for how to avoid programming yourself into a corner?\n \nreply",
      "Well CS and software dev in trenches moved a bit.There are still jobs where people write frameworks, database engines or version control tools. Those jobs require heavy CS and algorithms, data structures day to day. But there are less of those jobs nowadays as no one is implementing db engine for their app they just use Postgres.Other jobs that is vast majority is dealing with implementing business logic. Using database with understanding how it works in details is of course going to produce better outcomes. Yet one still can produce great amount of working software without knowing how indexes are stored on disk.Also a lot of CS graduates fell into a trap where they think their job is to write a framework - where in reality they should just use frameworks and implement business logic- while using CS background to fully understand frameworks already existing.\n \nreply",
      "> while using CS background to fully understand frameworks already existing.Most frameworks today are so complicated that you typically cannot understand them fully, and even understanding them somewhat partially is more than a full-time job.\n \nreply"
    ],
    "link": "https://simondobson.org/2010/05/14/cs-book-worth-reading-twice/",
    "first_paragraph": "I was talking to one of my students earlier, and lent him a book to\nread over summer. It was only after he\u2019d left that I realised that \u2014\nfor me at any rate \u2014 the book I\u2019d given him is probably the most\nseminal work in the whole of computer science, and certainly the book\nthat\u2019s most influenced my career and research\u00a0interests.So what\u2019s the book? Structure and interpretation of computer\nprograms by Hal Abelson and Jerry Sussman  (MIT\nPress. 1984. ISBN 0-262-01077-1), also known as SICP.\nThe book\u2019s still in print, but \u2014 even better \u2014 is available online in its\nentirety.OK, everyone has their favourite book: why\u2019s this one so special to\nme? The first reason is the time I first encountered it: in Newcastle upon Tyne in\nthe second year of my first degree. I was still finding my way\nin computer science, and this book was a recommended text after you\u2019d\nfinished the first programming courses. It\u2019s the book that introduced\nme to programming as it could be (rather than programming\nas it was, in "
  },
  {
    "title": "James Webb Space Telescope finds evidence for alternate theory of gravity (thedebrief.org)",
    "points": 218,
    "submitter": "jchanimal",
    "submit_time": "2024-11-16T18:33:02 1731781982",
    "num_comments": 100,
    "comments_url": "https://news.ycombinator.com/item?id=42158130",
    "comments": [
      "In my opinion, this article is misleading at best. \"...scans of ancient galaxies gathered by the JWST seem to contradict the commonly accepted predictions of the most widely accepted Cold Dark Matter theory, Lambda-CDM.\" --> LCDM doesn't predict what galaxies should look like, it simply predicts how much mass is in collapsed structures and that dark matter haloes grow hierarchically. In contrast, with JWST we see light and need to infer what the underlying properties of the system are. It was shown very early on that the theoretical upper limit (i.e. taking all of the gas that is available in collapsed structures and turning it into stars) predicts a luminosity function (i.e. number of galaxies per unit luminosity) that is orders of above what JWST has observed (e.g. https://ui.adsabs.harvard.edu/abs/2023MNRAS.521..497M/abstra...). This means that there is plenty of space within the context of LCDM to have bright and seemingly large and massive galaxies early on. Based on current JWST data at these early epochs, there are really no convincing arguments for or against LCDM because it's highly sensitive to the galaxy formation model that's adopted.\n \nreply",
      ">  there are really no convincing arguments for or against LCDM because it's highly sensitive to the galaxy formation model that's adopted.To be fair, that is absolutely not the way \u039bCDM would have been described to someone in the pre-Webb days.  It was a well-regarded theory and the hope was (a-la the Higgs detection) that new data would just better constrain the edges and get us on to the next phase of the problem.But instead it's a wreck, and we didn't see what we were expecting at all, and so now we're retreating to \"Well, \u039bCDM wasn't exactly proven wrong, was it?!\"That doesn't mean it's wrong either, and it for sure doesn't mean MOND is right.  But equally for sure this is a Kuhnian paradigm shift moment and I think it's important for the community to be willing to step back and entertain broader ideas.\n \nreply",
      "Again, LCDM and galaxy formation are two different things. \"...and we didn't see what we were expecting at all...\" It depends on who you ask. There were many pre-JWST models that did well in this regard. A particularly interesting one is this from 2018 (https://ui.adsabs.harvard.edu/abs/2018MNRAS.474.2352C/abstra...). That group even had to write another paper reminding everyone of what they predicted (https://ui.adsabs.harvard.edu/abs/2024arXiv240602672L/abstra...). Another example is here (https://ui.adsabs.harvard.edu/abs/2023OJAp....6E..47M/abstra...) which shows results from a simulation from ~2014. I can provide numerous other examples of this. My point isn't which theory is or isn't wrong, my point is that what is presented in this particular article is not a constraint on any realistic theory of gravity as the sensitivity of these particular observations to galaxy formation modeling is so strong.\n \nreply",
      "Absolutely not in the field, so if you are please completely disregard. But from conversations with physicists (not cosmologists) I always thought people thought a lot of evidence for \u039bCDM was dubious at best.\n \nreply",
      "> with JWST we see light and need to infer what the underlying properties of the system areEvery theory of dark matter is based exclusively on light-emitting objects. There is no \"contrast\" between JWST's methods and those of others. Casting aspersions on JWST because it can only see light is like casting aspersions on Galileo because he could only build telescopes. If we could teleport to the things we study and get more information that way, it would be nice, but we live in reality and must bend to its rules.> highly sensitive to the galaxy formation model that's adoptedI should only need to remind the reader of the classic idiom \"cart before the horse\" to remind them that this line of reasoning is invalid.\n \nreply",
      "This is a misrepresentation of what I am saying. By no means am I casting an aspersion on JWST. I am casting an aspersion on this particular observation as a test of MOND and LCDM. Also I highly disagree about your comments on my line of reasoning. The fact that you can obtain a huge range of possible galaxy properties in the context of LCDM indicates that in general, tests of LCDM and MOND that rely on galaxy formation model are in usually not strong tests. This is the key issue with using the abundance of high-z galaxies (or even their masses -- despite the fact that these aren't measured) as a test. In the context of LCDM, you need haloes to form galaxies but it has been shown many times that there are enough haloes to solve the problem (see the paper linked) by a huge amount.\n \nreply",
      "The skepticism you display in this comment is completely absent when you reference lambda-CDM elsewhere. Consistency invites zero criticism :)\n \nreply",
      "And so you have proved my point. The observations presented in this article can be made consistent with both...as such one should think about stronger tests of both LCDM and MOND.\n \nreply",
      "My hangup with MOND is still general relativity. We know for a fact that gravity is _not_ Newtonian, that the inverse square law does not hold. Any model of gravity based on an inverse law is simply wrong.Another comment linked to https://tritonstation.com/new-blog-page/, which is an excellent read. It makes the case that GR has never been tested at low accelerations, that is might be wrong. But we know for a fact MOND is wrong at high accelerations. Unless your theory can cover both, I don't see how it can be pitched as an improvement to GR.Edit: this sounds a bit hostile. to be clear, I think modified gravity is absolutely worth researching. but it isn't a silver bullet\n \nreply",
      "MOND isn't pitched as an improvement to GR.  It was always a Newtonian theory - it's in its name!There are relativistic versions of MOND, for example, TeVeS [1], but they all still have some problems.[1] https://en.m.wikipedia.org/wiki/Tensor%E2%80%93vector%E2%80%...\n \nreply"
    ],
    "link": "https://thedebrief.org/james-webb-space-telescope-finds-stunning-evidence-for-alternate-theory-of-gravity/",
    "first_paragraph": "Astronomers using the James Webb Space Telescope to peer back in time into the farthest reaches of the universe have found stunning evidence for an alternate theory of gravity.Current models of galaxy formation in the early cosmos predict the presence of excess gravity caused by dark matter to pull material into slowly forming galaxies. However, an alternate theory of gravity first proposed in 1998 called Modified Newtonian Dynamics (MOND) suggests that structures in the early universe formed very quickly without the need for theoretical dark matter.Now, researchers from Case Western Reserve University say that scans of ancient galaxies gathered by the JWST seem to contradict the commonly accepted predictions of the most widely accepted Cold Dark Matter theory, Lambda-CDM. Instead, the readings seem to support a basis for MOND, which would force astronomers and cosmologists to reconsider this alternative and long-controversial theory of gravity.The Lambda-CDM model has long posited tha"
  },
  {
    "title": "Two galaxies aligned in a way where their gravity acts as a compound lens (phys.org)",
    "points": 172,
    "submitter": "wglb",
    "submit_time": "2024-11-16T16:48:33 1731775713",
    "num_comments": 53,
    "comments_url": "https://news.ycombinator.com/item?id=42157335",
    "comments": [
      "With all the galaxies out there, it seems likely that Earth should be in the focus of lots of systems of this kind. Hopefully in the future we'll be able to find and use these galactic telescopes!\n \nreply",
      "One fun thing think about is that these two galaxies are only aligned from our perspective in the universe. Viewed from a different location, and they're just two normal galaxies.Also, imagine having the technology to send signals through the lens and get the attention of intelligent life on the other side.\n \nreply",
      "In order to use them as a signaling platform (how?) the signal would have needed to have been sent several billion years ago.At 10 billion light years away from the most distant lens it is 100% certain that they are no longer in a gravitational lensing configuration.For a frame of reference, the Milky Way will be in the middle of its epic merger with Andromeda in about 5 billion years.\n \nreply",
      "Even assuming a civilization can predict the alignment of the lenses (galaxies), they'd still need quite a powerful signal just to reach the first lens, let alone the second, and then a potential civilization who may be listening at just the right time on the other side. Hard to beat background noise even at distances of a few light years.\n \nreply",
      "It's kind of interesting in terms of analytics... can we predict when lenses will appear and disappear, from our perspective? What might we do with that information once we are more advanced?\n \nreply",
      "1. Yes it would be somewhat predictable to find these lenses for a civilization more advanced than ours.2. Unless we find faster than light communication (which, with our current understanding of physics is about as likely as humans jumping to the moon) there is nothing we could use it for other than definite proof that other life has evolved in the universe. Interesting data, but they're most likely extinct for billions of years already and even if they're not, the compound gravity lens will have moved out of alignment by then so we have no means to send a message back.\n \nreply",
      "And technically they are only temporarily so, given enough millions of years they will drift apart and lose the alignment.Also, other stars can come to align in the future. Makes me wonder if we can antecipate other cases like this and create a future schedule of \"To Observe\" so future generations can look at them. Although, these generations might be so distant from ours that might not even be considered of the same species\n \nreply",
      "I\u2019m sure there are plenty of civilizations that have done this, but on the time scale of the universe no one happens to look at just the right moment.\n \nreply",
      "But wouldn't the size and age of the universe also imply that someone has looked at just the right moment somewhere somewhen.\n \nreply",
      "Don\u2019t radio waves weaken proportionally to the square of the distance? No one would be able to detect them past a (relatively) small distance.\n \nreply"
    ],
    "link": "https://phys.org/news/2024-11-astronomers-galaxies-aligned-gravity-compound.html",
    "first_paragraph": ""
  },
  {
    "title": "Logica \u2013 Declarative logic programming language for data (logica.dev)",
    "points": 90,
    "submitter": "voat",
    "submit_time": "2024-11-16T19:09:30 1731784170",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=42158445",
    "comments": [
      "There's  also Malloy[0] from Google that compiles into SQL> Malloy is an experimental language for describing data relationships and transformations.[0]: https://github.com/malloydata/malloy\n \nreply",
      "Related:Google is pushing the new language Logica to solve the major flaws in SQL - https://news.ycombinator.com/item?id=29715957 - Dec 2021 (1 comment)Logica, a novel open-source logic programming language - https://news.ycombinator.com/item?id=26805121 - April 2021 (98 comments)\n \nreply",
      "If, like me, your first reaction is that this looks suspiciously like Datalog then you may be interested to learn that they indeed consider Logical to be \"in the the Datalog family\".\n \nreply",
      "I think Datalog should be thought of as \"in the logic programming family\", so other data languages based on logic programming are likely to be similar.And, of course the relational model of data is based on first-order logic, so one could say that SQL is a declarative logic programming language for data.\n \nreply",
      "I think it is a good direction imho. Once being familiar with SQL I learned Prolog a little and similarities struck me. I wasn't the first one sure, and there are others who summarized it better than me [1] (2010-2012):Each can do the other, to a limited extent, but it becomes increasingly difficult with even small increases in complexity. For instance, you can do inferencing in SQL, but it is almost entirely manual in nature and not at all like the automatic forward-inferencing of Prolog. And yes, you can store data(facts) in Prolog, but it is not at all designed for the \"storage, retrieval, projection and reduction of Trillions of rows with thousands of simultaneous users\" that SQL is.I even wanted to implement something like Logica at the moment, primarily trying to build a bridge through a virtual table in SQLite that would allow storing rules as mostly Prolog statements and having adapters to SQL storage when inference needs facts.[1]: https://stackoverflow.com/a/2119003\n \nreply",
      "> Composite(a * b) distinct :- ...Wait, does Logica factorize the number passed to this predicate when unifying the number with a * b?So when we call Composite (100) it automatically tries all a's and b's who give 100 when m7ltipliedI'd be curious to see the SQL it transpiles to.\n \nreply",
      "I find the appeals to composition tough to agree with.  For one, most queries begin as ad hoc questions.  And can usually be tossed after.  If they are needed for speed, it is the index structure that is more vital than the query structure.  That and knowing what materialized views have been made with implications on propagation delays.Curious to hear battle stories from other teams using this.\n \nreply",
      "Depends who your users are and what the context is.Having been in quite a few data teams, and supported businesses using dashboards, a very large chunk of the time, the requests do align with the composable feature: people want \u201cthe data from that dashboard but with x/y/z constraints too\u201d or \u201c<some well defined customer segment> who did a|b in the last time, and then send that to me each week, and then break it down by something-else\u201d. Scenarios that all benefit massively from being able to compose queries more easily, especially as things like \u201cwell defined customer segment\u201d get evolved. Even ad-hoc queries would benefit because you\u2019d be able to throw them together faster.There\u2019s a number of tools that proclaim to solve this, but solving this at the language level strikes me as a far better solution.\n \nreply",
      "There don't seem to be any examples of how to connect to an existing (say sqlite) database even though it says you should try logica if \"you already have data in BigQuery, PostgreSQL or SQLite,\". How do you connect to an existing sqlite database?\n \nreply",
      "If this is how you want to compile to SQL, why not invent your own DCG with Prolog proper?It should be easy enough if you're somewhat fluent in both languages, and has the perk of not being some Python thing at a megacorp famous for killing its projects.\n \nreply"
    ],
    "link": "https://logica.dev/",
    "first_paragraph": "\n  Logica is an open source\n  declarative logic programming language for data manipulation.\n\n  Logica extends syntax of logic programming for intuitive and efficient data\n  manipulation. It compiles to SQL thus providing you access to the power\n  of SQL engines with the convenience of logic programming syntax.\n\n  One may say that for programming languages like Python and Java functions are the\n  basic building blocks. For Logica and other logic programming languages\n  those building blocks are predicates. \n\n  Logic program is defined as a set of rules that define output predicates\n  from pre-defied predicates. Those pre-defined predecates represent input data.\n\n  For example here is a rule to identify names of expensive books, from an existing\n  table of book prices.\n\nIf you are familiar with SQL, you may see that the rule above\nis equivalent to the flowing SQL statement. Not that familiarity with SQL is\nrequired to learn Logica, not at all.\n\nFinally here is an example of program that "
  },
  {
    "title": "Teach Yourself to Echolocate (atlasobscura.com)",
    "points": 26,
    "submitter": "Anon84",
    "submit_time": "2024-11-16T22:43:49 1731797029",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42160071",
    "comments": [
      "Maybe only distantly related to this, but thought it worth sharing that when I visited Seattle for the first time this year I caught a show by a band called La Cerca at Central Saloon and loved their song \u201cEcholocation\u201d, with ethereal sounding guitars, including the bass: https://www.youtube.com/watch?v=1NYeqA2Kve8\n \nreply",
      "There is a wonderful book about the blind man who was probably one of the earliest innovators in using a cane for echolocation.  \"A Sense of the World\" by Jason Roberts is the story of James Holman, who traveled the world in the early 19th century despite being blind, often being in a great deal of pain, and having limited mobility.\n \nreply",
      "relevant video: https://www.youtube.com/watch?v=PD3Y1l8XyUwIt appears that the hardest part of echolocation for humans is the \"produce a directed, crisp click\" part. The \"process the sound\" part is readily handled by our brains with a relatively mild learning curve.\n \nreply",
      "E- chocolateE- chocolateDon\u2019t get it.Oh..!\n \nreply",
      "haha okay bat-man.nah but this is a pretty cool skill though\n \nreply"
    ],
    "link": "https://www.atlasobscura.com/articles/how-to-echolocate",
    "first_paragraph": "\n                          Our small-group adventures are inspired by our Atlas of the world's most fascinating places, the stories behind them, and the people who bring them to life.\n                        Daniel Kish navigates the world like a bat does\u2014and he does so without ever leaving the ground.After losing his vision as an infant, Kish taught himself to move around with the help of echolocation. Like bats, Kish uses his mouth to produce a series of short, crisp clicking sounds, and then listens to how those sounds bounce off the surrounding landscape. (Our winged neighbors tend to emit these clicks at frequencies humans can\u2019t hear, but Kish\u2019s clicks are perfectly audible to human ears.) From there, Kish makes a mental map of his environment, considering everything from broad contours\u2014like walls and doors\u2014down to textural details.Kish now teaches echolocation, mostly to students who are blind. For these students, Kish believes that an echolocation practice can buoy confidence an"
  },
  {
    "title": "Show HN: I built a(nother) house optimized for LAN parties (lanparty.house)",
    "points": 377,
    "submitter": "kentonv",
    "submit_time": "2024-11-16T15:52:39 1731772359",
    "num_comments": 155,
    "comments_url": "https://news.ycombinator.com/item?id=42156977",
    "comments": [
      "Given it's only 20 pcs, I might have just opted for fully local machines with a basic disk overlay software with exceptions for where Steam and Epic live. Course, engineering a centralized solution can be fun, but locked-down PCs are just simple. Having built corporate RDP and VDI solutions I'm just biased towards keeping things simple these days and pushing admin work off myself.Going off the local PC only idea, you could script just your rebuilds of them in the off chance something goes south, along with maybe a disk image with the majority of common games loaded. This is just thinking along the lines it's friends and family, not the general public. I'd probably use gigabit Internet (or more) which makes updates you're missing fast, while Steam lets PCs on a LAN share updated files and save bandwidth.Did you consider patch panels or things like PatchBox to organize those UTP cables or allow for changes in your switching later?\n \nreply",
      "I think the thing that I\u2019m most amazed by - and this setup is truly amazing - is the fact that you\u2019ve got a group of friends to enjoy this with. Good for you; this looks like a blast, and I can only imagine how fun that\u2019d be, compared to years of purely solo gaming.\n \nreply",
      "As the former proprietor of LanParty.com (which I mistakenly included in a sale to IGN) I must salute you.  The absolute genius of the provided lan equipment and particularly the management thereof is an inspiration.I think the lack of any standing offerings of variations of Quake is a glaring mistake but easily rectified. :)It's really heartening to see lan gaming continued and offered in such a way that the amount of hassle and setup is minimized and the gaming is maximized.  We spent far too much time in the 90's and 2000's dealing with driver issues, etc etc.   Bravo.\n \nreply",
      "Quake is still so much fun. Been playing for years with a group and it doesn't get old.\n \nreply",
      "What changes after years of playing? I assume everyone has every inch of the maps memorized?\n \nreply",
      "This is an extremely clever setup and certainly looks wonderful - but to me LAN parties are only LAN parties when people bring the computers with them (in much the same way that Champaign is only Champaign when from a certain place in France). That being said it looks wonderful and I hope it gives you and your community many years of enjoyment.\n \nreply",
      "That's a sweet LAN setup you've got! The only few things that rub me the wrong way is the choice of peripherals and the lack of headsets. Must be pretty noisy in here!The tabletops also seems a bit too thin and wiggly for my taste, but, honestly, for LAN parties with chill people you personally know \u2014 it's okAs for the actual host setup with a singular disk image \u2014 great job! LAN gaming centres do something similar with their setups, with some differences (a lot of centres either use Windows-based diskless solutions that mount vhdx files as drives remotely over iSCSI, or use ZFS-based snapshotting, which is my personal favourite)But all in all, seems like my dream house :)I own a chain of LAN gaming centres, so the feedback is definitely skewered into the business perspective quite a bit\n \nreply",
      "I'm curious, what are the popular products/solutions that LAN centers use for this?I ended up putting together my own thing. I saw various products that seemed like they might be what I wanted but they always seemed... sketchy.\n \nreply",
      "There are a few, actually :)CCBoot is a Windows Server-based diskless solution I mentioned, and they also provide CCDisk, which can do \"hybrid\" mode \u2014 where there is a small SSD in every PC with base OS pre-installed and pre-configured, which then mounts an iSCSI game driveGGRock is a fantastic product, in my opinion. It is pricy, but where as CCBoot relies heavily on knowing it's inner workings, GGRock is pretty much turnkey solutionThere is also CCu Cloud Update, which I have heard of, but didn't try myself, since they sell licenses only in Asia, from what I rememberLANGAME Premium is an addon for LAN centre ERP system, which is basically an ITAAS solution based on TrueNAS. Of all paid offerings that one is my favourite so far \u2014 but you have to use their ERP and actually run a business for it to be cost-effectiveNetX provides an all-in-one (router, traffic filter and iSCSI target) NUC-like server with pre-configured software on a subscription basis. I am most skeptical of that just on the basis that, from my research, two NVMe drives can't really handle the load from a fully occupied 40+ machines LAN centre. Not for a long time, at least...and homebrew, of course. I myself am running a homebrew ZFS-based system which I'm extremely happy withIn your case, I'd go with building my own thing too. Does not take a lot of time if you know the inner workings and you have no additional OPEX for your room :)\n \nreply",
      "What an incredible setup! Really wonderful house overall, to be honest.Aside from all of the extremely epic technology and whatnot - I have got to say, the elevated view and outlook of your place is sensational. Congratulations on putting together such a terrific place to raise a family.Oh and worth mentioning; I sincerely appreciated and enjoyed reading your comprehensive Q&A section beyond the images (which themselves, had really awesome annotations included). Thanks for sharing!\n \nreply"
    ],
    "link": "https://lanparty.house/",
    "first_paragraph": ""
  },
  {
    "title": "Stop Making Me Memorize the Borrow Checker (erikmcclure.com)",
    "points": 79,
    "submitter": "signa11",
    "submit_time": "2024-11-16T23:29:06 1731799746",
    "num_comments": 56,
    "comments_url": "https://news.ycombinator.com/item?id=42160501",
    "comments": [
      "I have memorised the UB rules for C. Or rather, more accurately, I have memorised the subset of UB rules I need to memorise to be productive in the language and am very strict in sticking to only writing code which I know is well defined (and know my way around the C standard at a level where any obscure code I sometimes need to write can be verified to be well defined without too much hassle).\nI think Rust may be difficult \nBut, if I forget something, or make a mistake, I'm screwed. Yes there's ubsan, there's tests, but ubsan and tests aren't guaranteed to work when ub is involved.This is why I call C a minefield.On that note, C++ has such an explosion of UB that I don't generally believe anyone who claims to know C++ because it seems to me to be almost infeasible to both learn all the rules, or at least the subset required to be productive, and then to write/modify code without getting lost.With rust, the amount of rules I need to learn to understand rust's borrow checker is about the same or even less. And if I forget the rules, the borrow checker is there to back me up.I still think that unless you need the performance, you should use a higher level language which hides this from you. It's genuinely easier to think about.That being said, writing correct rust which is going to a: work as I intended and b: not have UB is much less mentally taxing, even when I have to reach for unsafe.If you find it more taxing than writing C or C++ it's probably either because you haven't internalised the rules of the borrow checker, or because your C or C++ are riddled with various kinds of serious issues.\n \nreply",
      "> This is why I call C a minefield.Computing is a series of \"minefields.\"  At least you get a map of this particular one.I'm far more confronted by public facing APIs that involve user authentication than I am of any particular documented set of language facts.\n \nreply",
      "The borrow checker exists to force you to learn, rather than to let you skip learning.\nTo make an analogy, I think it would be weird if I complained that I had to \"memorize the rules\" of the type checker rather than learning how to use types as intended.\n \nreply",
      "Fair enough, but the problem in this analogy is that this learning isn't always useful or productive in any way. This is more like doing arithmetic in a sort of maths notation where every result must be in base 12 and everything else must be in base 16. Sure, you can memorise the rules and the conversions but you aren't doing much useful with your life at that point.Obviously, the borrow checker has uses in preventing a certain class of bugs, but it also destroys development velocity. Sometimes it's a good tradeoff (safety-critical systems, embedded, backends, etc.) and sometimes it's a square peg in a round hole (startups and gamedev where fast iteration is essential)\n \nreply",
      "I don\u2019t understand your analogy at all. The borrow checker doesn\u2019t feel like doing math in base 12 once you internalize the rules. It should come as second nature for 95% of programming work, with the last 5% requiring some planning ahead that probably should be done anyway in a properly written program.\n \nreply",
      "Rust was a pain in the ass until I stopped trying to write C code in it and started writing idiomatic Rust. I don\u2019t know the author of this blog, but he mentions extensive C++ experience which makes me wonder if he\u2019s trying to write C++ in Rust.Maybe not! Maybe it\u2019s truly just Rust being stubborn and difficult. However, it\u2019s such an easy trap to fall into that I\u2019ve gotta think it\u2019s at least possible.\n \nreply",
      "I learned Rust before learning C properly.Oh boy. I see bugs everywhere in C and why the borrow checker exists.\nIt really forces you to understand what happens under the hood.The most issues in Rust are indeed related the expressions - you don't know how to describe some scenario for compiler well-enough, in order to prove that this is actually possible - and then your program won't compile.In C, you talk more to the computer with the language syntax, whereas in Rust you talk to the compiler.\n \nreply",
      "Are there examples one can learn from about idiomatic rust? I would appreciate either books or projects to learn from.\n \nreply",
      "Rust, like ocaml, is best when used purely functionally until you run into something that isn't performant unless its imperative. But unlike ocaml or haskell there is a safe imperative middle ground before going all the way to unsafe.  People who write modern C++ with value semantics etc. seem to have a lot less trouble than people coming from Java.\n \nreply",
      "It's difficult to really use Rust purely functionally given that it removed pure functions from its type system, and that has a limited stack size.\n \nreply"
    ],
    "link": "https://erikmcclure.com/blog/stop-making-me-memorize-borrow-checker/",
    "first_paragraph": "BlogProjectsBandcampGithubWebsitesI started learning Rust about 3 or 4 years ago. I am now knee-deep in several very complex Rust projects that keep slamming into the limitations of the Rust compiler. One of the most common and obnoxious problems is hitting a situation the borrow-checker can\u2019t deal with and realizing that I need to completely re-architect how my program works, because lifetimes are \u201ccontagious\u201d the same way async is. Naturally, Rust has both!Despite how obviously useful the borrow-checker is in writing correct code, in practice it is horrendous to work with. This is because the borrow checker cannot run until an entire function compiles. Sometimes it seems to refuse to run until my entire file compiles. Because an explicit lifetime must come from somewhere, they have a habit of \u201cfloating up\u201d through the stack, from the point of usage to the point of origin, infecting everything in-between with another explicit generic lifetime parameter. If you end up not needing it, y"
  },
  {
    "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks (arxiv.org)",
    "points": 12,
    "submitter": "amai",
    "submit_time": "2024-11-16T22:37:04 1731796624",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://arxiv.org/abs/2310.03684",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Yggdrasil Network (yggdrasil-network.github.io)",
    "points": 224,
    "submitter": "BSDobelix",
    "submit_time": "2024-11-16T10:58:55 1731754735",
    "num_comments": 85,
    "comments_url": "https://news.ycombinator.com/item?id=42155780",
    "comments": [
      "The first thing I tried to find on their website and their GitHub was a protocol specification, to be able to implement it independently from the reference implementation. I thought this would be straightforward since it\u2019s advertised as a scheme/protocol, but such a spec isn\u2019t referenced anywhere! Digging on my own I eventually found [1] on a side-branch of one of their other GitHub projects.Kudos to the author: I think it actually covers a lot of what you\u2019d need to know: crypto identities, message formats, wire protocols, peering and stream semantics, spanning tree updates and root selection, the DHT, forwarding logic, sessions, etc. A couple things are TODOs like how to verify and sign root updates, and there\u2019s some ambiguity in the tiebreaker algorithm for next-hop selection.It seems to be very tightly coupled to TCP as the transport layer though, since all packets need to be delivered reliably and in the order they were sent, and need to be capable of being fragmented into smaller packets for varying MTU sizes.[1] https://github.com/yggdrasil-network/yggdrasil-specs/blob/ys...\n \nreply",
      "We did spend a little bit of time documenting the earlier v0.3 protocol, as you have linked, but the protocol has changed significantly in design twice since then. v0.4 changed the DHT quite a bit and v0.5 removed the DHT altogether. As a research project it likely will continue to change until we settle on a design we are happier with, at which point we will definitely spend more time documenting it.The need for ordered/reliable links is mostly for convenience of development at this stage, but that can be fixed for sure.\n \nreply",
      "Look at https://arxiv.org/pdf/1502.06461 if you want to try a chord dht again.Kademlia is a lot less intuitive, but by not ever assuming it's tables are correct, it handles and corrects inconsistency (and malicious nodes) better.Chapter 6 of this pile of (my) crap https://scholarworks.gsu.edu/cs_diss/106/ talks about doing latency optimization on dht routing. Basically just embedding then network graph into a metric space.\n \nreply",
      "Some documentation can help with those issues though. I find it helps more because you\u2019re writing to yourself why you\u2019re making certain decisions and it helps when you decide to make others. It just so happens that it\u2019s also a great way to onboard people.\n \nreply",
      "Is coupling with TCP a problem? Does it do anything that goes against their goal of full decentralization?\n \nreply",
      "Makes it hard to do hole punching I think? At any rate, direct connections currently cannot be established between multi-hop peers, traffic gets routed through peers instead. I think this has something to do with the TCP choice.\n \nreply",
      "Yeaaah. TCP hole punching is goofy and unreliable, last I checked. You have to do some arcane ritual of having both peers start a three-way handshake to each others\u2019s public endpoints simultaneously, relying on NATs to accept inbound SYN packets if they match the outgoing SYN. And nobody\u2019s NAT devices implement simultaneous-open the same way, so all your connections just fail.Naturally this leads to slapping even more arcane fixes on top of that, like NAT port assignment oracles to  adversarial interoperate with different port allocation strategies (random, sequential, single, etc.) by analyzing patterns in previous port assignments. Networking sucks.\n \nreply",
      "https://xkcd.com/2044/\n \nreply",
      "> At any rate, direct connections currently cannot be established between multi-hop peers, traffic gets routed through peers instead. I think this has something to do with the TCP choice.Yggdrasil is designed for physical links and multi-hop routing first and foremost. Internet peering is just a way to test/use/join the network until then.\n \nreply",
      "I think this is a pragmatic choice. NAT Hole Punching can be hit or miss no matter the method but doing peer routing guarantees even a client that can only initiate outbound connections can route packets. It can be slow though.I also know there's support for other transports like QUIC but TCP is the main default.\n \nreply"
    ],
    "link": "https://yggdrasil-network.github.io/",
    "first_paragraph": "Yggdrasil is a new experimental compact routing scheme. It is designed to be a future-proof and decentralised alternative to the structured routing protocols commonly used today on the Internet, as well as an enabling technology for future large-scale mesh networks. Yggdrasil is:Supports large, complex or even Internet-scale topologiesNetwork responds quickly to connection failures or mobility eventsTraffic sent across the network is always fully end-to-end encryptedWorks entirely ad-hoc by design with no built-in points of centralisationSupported on Linux, macOS, Windows, iOS, Android and moreThe current implementation of Yggdrasil is a lightweight userspace software router which is easy to configure and supported on a wide range of platforms. It provides end-to-end encrypted IPv6 routing between all network participants. Peerings between nodes can be configured using TCP/TLS connections over local area networks, point-to-point links or the Internet. Even though the Yggdrasil Network "
  },
  {
    "title": "Eau de Nil, the Light-Green Color of Egypt-Obsessed Europe (2018) (theparisreview.org)",
    "points": 12,
    "submitter": "prismatic",
    "submit_time": "2024-11-10T02:55:18 1731207318",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.theparisreview.org/blog/2018/02/13/eau-de-nil-light-green-color-egypt-obsessed-europe/",
    "first_paragraph": ""
  },
  {
    "title": "Statistical Rethinking (2024 Edition) (github.com/rmcelreath)",
    "points": 75,
    "submitter": "lnyan",
    "submit_time": "2024-11-16T18:08:02 1731780482",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=42157885",
    "comments": [
      "I've taken the course and love the book. The key takeaways for me were to shun cleverness in favour of building demonstrably sensible models out of simple parts and clear causal assumptions. There's a lot of using models as random generators as a way to validate assumptions that demonstrates that your model makes sense before you start with inferencing which is a fantastic habit to stick with.\n \nreply",
      "It's a great book, but my personal opinion is that it would have benefited from an editor that recommended some small changes. The previous edition had a TOC which was barely usable because all funny jokes in chapter names like \"8 Conditional Manatees\". Besides, there were too many jokes embedded in some sections, which made them difficult to follow. I think some of these issues are getting addressed in the current edition.Nonetheless, the book is very well written and all figures and examples show great attention to detail. I found Gelman et al Regression and Other Stories better for teaching newcomers, and surprisingly insightful. Statistical Rethinking is a good choice for a second course, but perhaps too informal at that stage.\n \nreply",
      "I second that. The TOC is unusable. However, it's probably aligned with the author's intention of it being a course and not a reference book.\n \nreply",
      "The lectures are on YouTube and are really very good.\n \nreply",
      "What are the prerequisites for the topics covered in this book? I feel like the lecture list is hard to understand, maybe sort of like the book\u2019s TOC.\n \nreply",
      "The original target audience was phd students in the sciences who want to do statistics to do science. So:- the book tries to be practical and applicable for science- the book assumes some amount of mathematical maturity and ability to fiddle with somewhat simple data- the book is not about mathematical statistics \u2013 no proving things about maximum likelihood estimators- the book doesn\u2019t teach you about programming in R\n \nreply",
      "Honestly, I think there are very little prerequisites. I'm an MD dabbling into stats and found the book very well made as well as understandable.\n \nreply",
      "As an MD/PhD I wish all MD researchers read this book. Heck, I wish all neuro researchers read it.  If you are already established in in stats and math and your interest is just another math book to casually read or reference, this is a bad choice\n \nreply",
      "WHY do you think it\u2019s bad for that background? Please!What if you know math but not stats? How much stats do I need to know before you think this isn\u2019t good to browse?Wish I knew\u2026 I guess I\u2019ll have to find out the hard way.",
      "Related. Others?Statistical Rethinking (2022 Edition) - https://news.ycombinator.com/item?id=29956390 - Jan 2022 (124 comments)Statistical Rethinking [video] - https://news.ycombinator.com/item?id=29780550 - Jan 2022 (10 comments)Statistical Rethinking: A Bayesian Course Using R and Stan - https://news.ycombinator.com/item?id=20102950 - June 2019 (14 comments)\n \nreply"
    ],
    "link": "https://github.com/rmcelreath/stat_rethinking_2024",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          Instructor: Richard McElreathLectures: Uploaded and pre-recorded, two per weekDiscussion: Online (Zoom), Fridays 3pm-4pm Central European (Berlin) TimeThis course teaches data analysis, but it focuses on scientific models. The unfortunate truth about data is that nothing much can be done with it, until we say what caused it. We will prioritize conceptual, causal models and precise questions about those models. We will use Bayesian data analysis to connect scientific models to evidence. And we will learn powerful computational tools for coping with high-dimension, imperfect data of the kind that biologists and social scientists face.Online, flipped instruction. I will pre-record the lectures each week. We'll meet online once a week for an hour to discuss the material. The discussion time (3-4pm Berlin Time) should allow people in the Americas"
  },
  {
    "title": "Netflix buffering issues: Boxing fans complain about Jake Paul vs. Mike Tyson (sportingnews.com)",
    "points": 387,
    "submitter": "storf45",
    "submit_time": "2024-11-16T03:42:05 1731728525",
    "num_comments": 706,
    "comments_url": "https://news.ycombinator.com/item?id=42153953",
    "comments": [
      "Every time a big company screws up, there are two highly informed sets of people who are guaranteed to be lurking, but rarely post, in a thread like this:1) those directly involved with the incident, or employees of the same company.  They have too much to lose by circumventing the PR machine.2) people at similar companies who operate similar systems with similar scale and risks.  Those people know how hard this is and aren\u2019t likely to publicly flog someone doing their same job based on uninformed speculation. They know their own systems are Byzantine and don\u2019t look like what random onlookers think it would look like.So that leaves the rest, who offer insights based on how stuff works at a small scale, or better yet, pronouncements rooted in \u201cfirst principles.\u201d\n \nreply",
      "I've noticed this amongst the newer \"careerist\" sort of software developer who is stumbling into the field for money, as opposed to the obsessive computer geek of yesteryear, who practiced it as a hobby. This character archetype is a transplant, say, less than five years ago from another, often non-technical discipline, and was taught or learned from overly simplistic materials that decry systems programming, or networking, or computer science concepts as unnecessary, impractical skills, reducing everything to writing JavaScript glue code between random NPM packages found on google.Especially in a time where the gates have come crashing down to pronouncements of, \"now anybody can learn to code by just using LLMs,\" there is a shocking tendency to overly simplify and then pontificate upon what are actually bewilderingly complicated systems wrapped up in interfaces, packages, and layers of abstraction that hide away that underlying complexity.It reminds me of those quantum woo people, or movies like What the Bleep Do We Know!? where a bunch of quacks with no actual background in quantum physics or science reason forth from drastically oversimplified, mathematics-free models of those theories and into utterly absurd conclusions.\n \nreply",
      "Right? A common complaint by outsiders is that Netflix uses microservices. I'd love to hear exactly how a monolith application is guaranteed to perform better, with details. What is the magic difference that would have ensured the live stream would have been successful?\n \nreply",
      "It's not guaranteed, but much fewer points of failure.\n \nreply",
      "> It's not guaranteed, but much fewer points of failure.Can you explain where this is relevant to buffering issues?Also, you are very wrong regarding failure modes. The larger the service, the more failure modes it has. Moreover, in monoliths if a failure mode can take down/degrade the whole service, all other features are taken down/degraded. Is having a single failure mode that brings down the whole service what you call fewer points of failure?\n \nreply",
      "I can't, since I don't know Netflix's architecture - I was responding to \"I'd love to hear exactly how a monolith application is guaranteed to perform better, with details.\"\n \nreply",
      "I doubt a \"microservice\" has anything to do with delivering the video frames. There are specific kinds of infrastructure tech that are specifically designed to serve live video to large amounts of clients. If they are in fact using a \"microservice\" to deliver video frames, then I'd ask them to have their heads examined. Microservices are typically used to do mundane short-lived tasks, not deliver video.\n \nreply",
      "There\u2019s very likely a dedicated service for delivering frames.That\u2019s service would technically be a \u201cmicroservice\u201d even if it is a large service.\n \nreply",
      "For an event like this, there already exists an architecture that can handle boundless scale: torrents.If you code it to utilize high-bandwidth users upload, the service becomes more available as more users are watching -- not less available.It becomes less expensive with scale, more available, more stable.The be more specific, if you encode the video in blocks with each new block hash being broadcast across the network, just managing the overhead of the block order, it should be pretty easy to stream video with boundless scale using a DHT.Could even give high-bandwidth users a credit based upon how much bandwidth they share.With a network like what Netflix already has, the seed-boxes would guarantee stability.  There would be very little delay for realtime streams, I'd imagine 5 seconds top.  This sort of architecture would handle planet-scale streams for breakfast on top of the already existing mechanism.But then again, I don't get paid $500k+ at a large corp to serve planet scale content, so what do I know.\n \nreply",
      "The protocol for a torrent is that random parts of a file get seeded to random people requesting a file, and that the clients which act as seeds are able to store arbitrary amounts of data to then forward to other clients in the swarm. Do the properties about scaling still hold when it's a bunch of people all requesting real time data which has to be in-order? Do the distributed Rokus, Apple TVs, Fire TVs and other smart TVs all have the headroom in compute and storage to be able to simultaneously decode video and keep old video data in RAM and manage network connections with upload to other TVs in their swarm - and will uploading data to other TVs in the swarm not negatively impact their own download speeds?\n \nreply"
    ],
    "link": "https://www.sportingnews.com/us/boxing/news/netflix-buffering-livestream-issues-boxing-jake-paul-mike-tyson/327ee972d4b14d90cc370461",
    "first_paragraph": ""
  },
  {
    "title": "Numpyro: Probabilistic programming with NumPy powered by Jax (github.com/pyro-ppl)",
    "points": 74,
    "submitter": "lnyan",
    "submit_time": "2024-11-16T12:41:18 1731760878",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=42156126",
    "comments": [
      "If you are struggling to understand the README, I highly recommend the book Statistical Rethinking: A Bayesian Course with Examples in R and Stan by Richard McElreath. Although the examples are in R, the same concepts apply to Pyro (and NumPyro)[1] https://www.goodreads.com/book/show/26619686-statistical-ret...\n \nreply",
      "I read through the second edition, and would recommend as well [1] (has numpyro, pyro, pymc code). He also has a great lecture series on youtube [2][1] https://xcelab.net/rm/[2] https://www.youtube.com/watch?v=FdnMWdICdRs&list=PLDcUM9US4X...\n \nreply",
      "There are examples from the book done in PyMC:https://github.com/pymc-devs/pymc-resources/tree/main/Rethin...or, for first editionhttps://github.com/pymc-devs/pymc-resources/tree/main/Rethin...\n \nreply",
      "For those with more experience how does (Num)Pyro compare with PyMC? I haven\u2019t had the good fortune of working with any of these libraries since before Pyro (and presumably numpyro), and with PyMC3 back when it used Theano under the hood.Are the two libraries in competition? Or complimentary? I\u2019ve been playing with PyMC for a personal project and am curious what I might gain from investigating (Num)Pyro?\n \nreply",
      "I would say that, at least for me, PyMC\u2019s main advantage was in DX. I just found model construction much more straightforward and better aligned with how I wanted to assemble the model.This textbook/walkthrough is great:\nhttps://a.co/d/9dWXDTK\n \nreply",
      "Thanks for the textbook link, this looks right up my alley!\n \nreply",
      "I tired both a while back, but nothing too big or serious. One thing that numpyro benefits from is JAX's speed, so it might be faster for larger models. Though PyTensor, which is the backend for PyMC can apparently also generate JAX code, so the difference might not be drastic. The PyMC API also seemed to me easier to get started with for those learning Bayesian stats.One thing I remember that I disliked about PyMC was the PyTensor API, it feels too much like Theano/TensorFlow. I much prefer using JAX for writing custom models.\n \nreply",
      "This is maybe not the place, but we did some apples to apples comparisons between PyMC, Dynesty, and the Julia Turing.jl package.A little to my surprise, despite being a Julia fan, Turing really outperformed both the Python solutions.\nI think JAX should be competitive in raw speed, so it might come down to the maturity of the samplers we used.\n \nreply",
      "You'll lose a lot of the PyMC convenience functions with Numpyro but gain a lot of control and flexibility over your model specification. If you're doing variational inference, Numpyro is the way to go.You can use the Numpyro NUTS sampler in PyMC with pm.sample(nuts_sampler=\"numpyro\") and it will significantly speed up sampling. It is less stable in my experience.\n \nreply",
      "I find the PyMC situation confusing. But PyMC should be able to use JAX in the backend: https://www.pymc.io/projects/examples/en/latest/samplers/fas...\n \nreply"
    ],
    "link": "https://github.com/pyro-ppl/numpyro",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Probabilistic programming with NumPy powered by JAX for autograd and JIT compilation to GPU/TPU/CPU.\n      \n\nProbabilistic programming powered by JAX for autograd and JIT compilation to GPU/TPU/CPU.Docs and Examples | ForumNumPyro is a lightweight probabilistic programming library that provides a NumPy backend for Pyro. We rely on JAX for automatic differentiation and JIT compilation to GPU / CPU. NumPyro is under active development, so beware of brittleness, bugs, and changes to the API as the design evolves.NumPyro is designed to be lightweight and focuses on providing a flexible substrate that users can build on:Let us explore NumPyro using a simple example. We will use the eight schools example from Gelman et al., Bayesian Data Analysis: Sec. 5.5, 2003, which studies the effect of coaching on SAT performance in eight schools.The"
  },
  {
    "title": "I tried every top email marketing tool (sitebuilderreport.com)",
    "points": 105,
    "submitter": "steve-benjamins",
    "submit_time": "2024-11-15T13:23:26 1731677006",
    "num_comments": 69,
    "comments_url": "https://news.ycombinator.com/item?id=42146689",
    "comments": [
      "I appreciate that the author disclosed it, but the reason they went to all this effort is likely that they expect to make money as an affiliate for the platforms that they recommended.Affiliate-driven reviews introduce a major bias into the author's opinion, as they have incentive to speak more positively about platforms that are likely to pay the most.And email marketing platforms pay a lot in affiliate fees. Just scanning some of the recommendations, if someone signs up for MailerLite through this reviewer's link, they'll pay the reviewer 30% of that subscriber's fees forever.[0] I wouldn't be surprised if the reviewer's top pick is coincidentally the platform with the highest-paying affiliate program.The thing that really woke me up to affiliate-influenced reviews was the 2017 article, \"The War To Sell You A Mattress Is An Internet Nightmare.\"[1] The reporter figured out that top YouTube mattress reviewers just gave positive reviews to whichever company paid the most in affiliate fees, and when one company lowered their fees, the reviewers retroactively downranked them for contrived reasons.[0] https://www.mailerlite.com/affiliate[1] https://www.fastcompany.com/3065928/sleepopolis-casper-blogg...\n \nreply",
      "Op here. I sort of agree but all these tools offer affiliate programs and I can assure you we chose MailerLite because we think it\u2019s a tool we can use for 5-10 years.That being said: besides running a startup (Atlist.com) I also run an affiliate site (it\u2019s how we funded Atlist) and I would agree there is good reason to read affiliate websites skeptically. I regularly receive offers from website builders to \u201cbuy\u201d the top spot in my best website builder roundup. https://www.sitebuilderreport.com/best-website-builder\n \nreply",
      "When all \"The Best\" sites have the affiliate blob, and \"The Rest\" doesn't, haha. My god is this a plague on the internet.\n \nreply",
      "The article is \u201cI tried every top email marketing tool\u201d and starts with eliminating a majority of the field based on an arbitrary rubric of what the author specifically is looking for. Then fails to compare essentially anything about the tools to provide any semblance of a useful review for any other person to consume for their own research. I have to agree with the other poster that this really just seems to be a reasonable attempt to get affiliate link click throughs.\n \nreply",
      "OP here.What would you compare the tools on? Be specific\n \nreply",
      "I think the issue here is with the word \"tried\" and what that communicates. I think \"compared\" would be far more appropriate.\n \nreply",
      "That\u2019s fair! I actually \u201ctried\u201d more than the \u201celite 6\u201d but I eliminated many of those tools for different reasons\n \nreply",
      "Deliverability, price, pricing model, api/automation, UI, email builders, support, etc.The article is just \u201cwhy we chose breva\u201d and is very specific to you. As far as I can tell you didnt even use half of the offerings since they were ruled out purely due to pricing models.EDIT: just an example. If I wanted to know about sendgrid and how it compares, the only information this page gives me is \u201cit has an overage charge\u201d. How am I supposed to consume this article as an informative comparison?\n \nreply",
      "This is not a universal comparison. This is the story about how I tried 25 email marketing tools. I\u2019m sharing my subjective experience. You might be looking for something that my article doesn\u2019t claim to offer.\n \nreply",
      "\u2026 We didn\u2019t choose Brevo.You didn\u2019t even read the article!\n \nreply"
    ],
    "link": "https://www.sitebuilderreport.com/email-marketing-tools/",
    "first_paragraph": " My work is supported by affiliate commissions. Learn More\nLast Updated \n\t\t\t\t\tNovember 15 2024\n\t\t\t\t\nWritten By\nSteve Benjamins\nI\u2019m in the market for new email marketing software.After years with MailChimp, I\u2019ve finally had enough\u2014time for a change!To find the right fit, I took a deep dive into the top 25 email marketing tools. Along the way, I battled unhelpful chatbots, squinted through pages of fine print, built a detailed comparison spreadsheet, and, naturally, sent real emails with each tool.Here\u2019s what I discovered.I use email marketing for Atlist, a software company I co-founded.Even though we're a software company, our email marketing strategy is pretty straightforward\u2014 like many businesses, we rely on two main types of emails:We send out a newsletter when we release a new feature. However, we don't blast this to everyone. Newsletters go only to users who opted in by clicking a box during signup.An example email newsletter.An automation is an  email triggered by specific user ac"
  },
  {
    "title": "New haptic patch transmits complexity of touch to the skin (techxplore.com)",
    "points": 22,
    "submitter": "wglb",
    "submit_time": "2024-11-16T19:00:39 1731783639",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=42158366",
    "comments": [
      "After the Apple Watch came out, I wanted to build something like this. Of course, an idea being an idea, I didn't.Anyway, my idea was this: one drunken night in Berkeley, a neuroscience PhD told me that it was possible to put a patch of electrodes on the back of a blind person. If a camera fed those electrodes, the person would eventually learn to decode that into something like sight.That got me thinking about using a similar patch, except on your arm (for convenience), except for audio, and for people who _could_ hear. The idea was that it would amplify what you heard and make you hear it better. We would quickly learn what the signals from the patch mean. In time, maybe, the patch could be used to deliver auditory information that was _not_ merely an amplification of what the person heard.I am not 100% sure but I think Meta has been building something similar for a while. Their recent AR glasses showed off a wrist device that could read signals, but I could swear they were also looking into a device that could transmit signals to the skin.\n \nreply",
      "Come on, when can we use this for games?\n \nreply",
      "Come on, when can we use this for, uhm\n \nreply"
    ],
    "link": "https://techxplore.com/news/2024-11-haptic-patch-transmits-complexity-skin.html",
    "first_paragraph": ""
  },
  {
    "title": "Optimizers: The Low-Key MVP (duckdb.org)",
    "points": 81,
    "submitter": "tosh",
    "submit_time": "2024-11-16T13:53:53 1731765233",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42156430",
    "comments": [
      "Meta: I think the title would be better here if it came out and said query optimizers.That gives a less subtle clue that it's about databases than looking at the domain.\n \nreply",
      "> The SQL above results in a plan similar to the DuckDB optimized plan, but it is wordier and more error-prone to write, which can potentially lead to bugs.FWIW, aside from manual filter pushdown, I consider the JOIN variant the canonical / \"default\" way to merge multiple tables; it keeps all the join-related logic in one place, while mixing both joining conditions and filtering conditions in WHERE always felt more error-prone to me.\n \nreply",
      "It is also the only way to represent join conditions for outer joins.\n \nreply",
      "> This means your optimizations need to be applied by hand, which is sustainable if your data starts changing.Seems like a missing \"un\" hereCompelling article! I've already found DuckDB to be the most ergonomic tool for quick and dirty wrangling, it's good to know it can handle massive jobs too.\n \nreply",
      "I regularly use duckdb on datasets of 1B+ rows, with nasty strong columns that may be over 10MB per value in the outliers. Mostly it just works, and fast too! When it doesn't, I'll usually just dump to parquet and hit it with sparksql, but that is the exception rather than the rule.\n \nreply"
    ],
    "link": "https://duckdb.org/2024/11/14/optimizers.html",
    "first_paragraph": ""
  },
  {
    "title": "The Difference Between a Standard DIMM and a Cudimm or Csodimm (servethehome.com)",
    "points": 29,
    "submitter": "rbanffy",
    "submit_time": "2024-11-10T19:30:41 1731267041",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=42102076",
    "comments": [
      "This article is quite good in the way that is informative without giving the wft? reaction that many of the articles on this site give. I occasionally read their articles and I have very low expectations from a site that is called serve the home and deals too often with enterprise level hardware that will never get into someone's home, like $10,000 switches or $50,000 servers. Or dubious conclusions like a recent one about a certain piece of hardware from one of my preferred manufacturers that is praised as the `God given gift to humans` with excelent price that is ... about 3-4 times a decent or good price. It sounds so much as paid advertisement.\n \nreply",
      "> I have very low expectations from a site that is called serve the home and deals too often with enterprise level hardware that will never get into someone's homeThat's an uncharitable interpretation of a site's name. Is HN only for news, or only news for or with hackers? It's not called \"serve only the home\". On top of that, what plenty of people have in their home just for the sake of passion for tech might surprise you.\n \nreply",
      "\"STH may say \u201chome\u201d in the title which confuses some. Home is actually a reference to a users /home/ directory in Linux. We scale from those looking to have in home or office servers all the way up to some of the largest hosting organizations in the world. This site is dedicated to helping professionals and enthusiasts stay atop of the latest server, storage and networking trends.\"https://www.servethehome.com/about/\n \nreply",
      "Sadly, virtually everyone with experience in enterprise IT is working in enterprise IT so there are very few outlets doing independent reviews. Pricing is especially tricky because customers don't want to get their 80% discount revoked and it's hard for non-customers to get real information.\n \nreply",
      "I enjoy reading the review of $10,000 switches the same way I enjoy reading the reviews of a McLaren or a Ferrari in my favourite car magazine. Sure I'll never be able to afford one, but it's good to know what technology exists out there that in 10-20 years time might make it to more affordable units. And when the magazine does a review of the Honda Civic Type-R and the writer says they had nearly as much fun driving this as they had last month driving the Ferrari, I have an extra large grin on my face next time I'm getting into my Civic Type-RAnd STH does plenty of reviews of stuff that's very affordable. I recently needed a 2.5 GbE switch with some level of management (vlan tagging) and PoE for my homelab, and their \"The Ultimate Cheap Fanless 2.5GbE Switch Buyers Guide\" was an invaluable resourcehttps://www.servethehome.com/the-ultimate-cheap-2-5gbe-switc...\n \nreply",
      ">a site that is called serve the home and deals too often with enterprise level hardware that will never get into someone's home, like $10,000 switches or $50,000 servers.The website Serves The HomeLab, which is often comprised of used surplus enterprise hardware sold for pennies and dimes by the pound when their Use By date expires.I actually don't like the host and his style of speech/presentation (I don't even remember his name), he comes off fake and disingenuous, but what he presents is certainly relevant to the intended audience of sysadmins both amateurs and professionals.\n \nreply"
    ],
    "link": "https://www.servethehome.com/what-is-different-with-a-cudimm-or-csodimm-micron-crucial/",
    "first_paragraph": "We probably need to have one of these. Recently, memory makers started to take a technology that we have seen for years on the server side, and bring it to the desktop and mobile platforms. The CUDIMM or CSODIMM adds a \u201cC\u201d for clocked, and with it comes some new hardware.The big difference between the CUDIMM and COSODIMM and their UDIMM/ SODIMM counterparts is the addition of a clock driver. Just to see the difference, here is the front and back of Our Top 96GB DDR5-5600 SODIMM Kit Crucial 2x 48GB Kit.Here is the Crucial DDR5-6400 CSODIMM kit that is 2x 16GB. Of course, there is a big difference on the right hand side since the 48GB modules have DRAM chips on the back, but if you look at the front, you will spot the difference.Here is a close-up of the front of the CSODIMM. You can see in the middle under the label that there is a Montage clock driver chip.With the new clock driver DIMMs, you will see it on the label along with CSODIMM/ CUDIMM. Here is the CSODIMM package label.On the "
  },
  {
    "title": "Don't Look Twice: Faster Video Transformers with Run-Length Tokenization (rccchoudhury.github.io)",
    "points": 40,
    "submitter": "jasondavies",
    "submit_time": "2024-11-16T00:11:58 1731715918",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=42152867",
    "comments": [
      "What would be the applications of this that is different from regular transformers? Perhaps stupid question.\n \nreply",
      "For training, would it be useful to stabilize the footage first?\n \nreply",
      "Stabilization appears to be a subset of a literally wider, but more rewarding, challenge: reconstructing the whole area that is scanned by the camera. It could be better to work on that challenge, not on simple stabilization.That's similar to how the human visual system 'paints' a coherent scene from a quite narrow field of high-resolution view, with educated guesses and assumptions\n \nreply",
      "https://vidpanos.github.io/There are other recent ones that do a new camera from any vantage point, not just rotation+fov changes like the above as well.  But they still might want stabilized video as the baseline input if they don't already use it.\n \nreply",
      "I guess yes. Having worked on video processing, it's always better if you can stabilize because it significantly reduces the number of unique tokens, which would be even more useful for the present method.\nHowever, you probably lose in generalization performance and not all videos can be stabilized.\n \nreply",
      "Isn't this like Differential Transformers that worked based on differences?\n \nreply",
      "That was my feeling too for the most part, but The run length is a significant source of information and if it enables tokens to be skipped it is essentially gaining performance by working with a smaller but more dense form of the same information.  My instinct is that run-length would be just the most basic case of a more generalized method for storing token information to encompass time and area and for the density of information in tokens to be more even,  The area and duration being variable but the token stream containing a series of tokens containing similar quantities of semantic data.I feel like this is very much like the early days of data compression where a few logical but kind of ad-hoc principles are being investigated in advance of a  more sophisticated theory that integrates the ideas of what is being attempted, how to identify success, and recognizing pathways that move towards the optimal solution.These papers are the foundations of that work.\n \nreply",
      "As far as I can can tell though the core idea is the same, to focus on the differences, the implementation is different. Differential transformers 'calculates attention scores as the difference between two separate softmax attention maps'. So they must process the redundant areas. This removes them altogether, which would significantly reduce compute. Very neat idea.However, I do think that background information can sometimes be important. I reckon a mild improvement on this model would be to leave the background in the first frame, and perhaps every x frames, so that the model gets better context cues. This would also more accurately replicate video compression.\n \nreply"
    ],
    "link": "https://rccchoudhury.github.io/rlt/",
    "first_paragraph": "\n                Inspired by video compressors, RLT efficiently identifies patches from the input video that are repeated, and prunes them from the input. It then uses a duration encoding to tell the transformer what patches were removed.\n\n                For all the videos on this page, the lighter colored patches represent patches that are masked out. These semantically correspond to parts of the video that are not moving and are almost exactly repeated over time.\n              \n        We present Run-Length Tokenization (RLT), a simple and efficient approach to speed up video transformers by removing redundant tokens from the input. Existing methods prune tokens progressively, incurring significant overhead and resulting in no speedup during training. Other approaches are content-agnostic: they reduce the number of tokens by a constant factor and thus require tuning for different datasets and videos for optimal performance. Our insight is that we can efficiently identify which patch"
  },
  {
    "title": "We\u2019re receiving about 3,000 reports/hour (bsky.app)",
    "points": 65,
    "submitter": "Funes-",
    "submit_time": "2024-11-16T21:20:13 1731792013",
    "num_comments": 93,
    "comments_url": "https://news.ycombinator.com/item?id=42159454",
    "comments": [
      "\u201c Excessive moderation is a barrier to open and robust debate, ultimately undermining the diversity of perspectives that make meaningful discourse possible. Supressing dissenting opinions will lead to an echo chamber effect. Would you like to join me an upcoming campaign to restore Europe? Deus vult!\u201dah social media, some people are truly as dumb as rocks\n \nreply",
      "Not to mention all the people extremely confused over what \"CSAM\" is seemingly without having the ability to google it.\n \nreply",
      "I think your life is better off if you don't know what that means, so feel free not to look it up.\n \nreply",
      "I'm interested to see how Bluesky ends up handling bad actors in the long-term. Will they have the resources to keep up? Or will it become polluted like the other large platforms.Also, if a part of their business model will be based off selling algorithmic feeds, won't that mean more spam is actually good for their bottom line because they'll sell more algorithmic feeds that counter the spam?\n \nreply",
      "The AT Protocol already accounts for this. There will eventually be community-built content labelers and classifers that you can subscribe to to rank and moderate your own feed however you want.\n \nreply",
      "I understand the moderators working for the big social networks have a terrible job and often see the worst the internet has to offer.Who is going to do that job as a volunteer? Or is that expected to be solved by technology? Hard to imagine them achieving what Google, Facebook etc could not reliably.\n \nreply",
      "Some people seem to get immense satisfaction and pleasure out of censoring other people online.It's something I've seen time and time again, in a wide variety of discussions forums, for decades now.Such people will happily do it for free, and they're willing to dedicate many hours per day to it, too.I don't understand their motivation(s), but perhaps it simply gives them a sense of power, control, or influence that they otherwise don't have in their lives outside of the Internet.\n \nreply",
      "Moderation. It's a thankless job. I supposed blocking spam counts as censorship.\n \nreply",
      "Those people should never be allowed to moderate anything for obvious reasons.\n \nreply",
      "You have described Reddit moderators.\n \nreply"
    ],
    "link": "https://bsky.app/profile/safety.bsky.app/post/3layun7re5s2x",
    "first_paragraph": "This is a heavily interactive web application, and JavaScript is required. Simple HTML interfaces are possible, but that is not what this is.\n    Learn more about Bluesky at bsky.social and atproto.com.\n    \nPost\nBluesky Safety\nsafety.bsky.app\ndid:plc:eon2iu7v3x2ukgxkqaf7e5np\nIn the past 24 hours, we have received more than 42,000 reports (an all-time high for one day). We\u2019re receiving about 3,000 reports/hour. To put that into context, in all of 2023, we received 360k reports.\n\nWe\u2019re triaging this large queue so the most harmful content such as CSAM is removed quickly.\n2024-11-15T17:10:55.091Z\n\nLearn more about Bluesky at bsky.social and atproto.com.\n    \nPost\nBluesky Safety\nsafety.bsky.app\ndid:plc:eon2iu7v3x2ukgxkqaf7e5np\nIn the past 24 hours, we have received more than 42,000 reports (an all-time high for one day). We\u2019re receiving about 3,000 reports/hour. To put that into context, in all of 2023, we received 360k reports.\n\nWe\u2019re triaging this large queue so the most harmful content"
  }
]