[
  {
    "title": "Repairable Flatpack Toaster (kaseyhou.com)",
    "points": 306,
    "submitter": "t-3",
    "submit_time": "2025-03-03T21:19:56 1741036796",
    "num_comments": 105,
    "comments_url": "https://news.ycombinator.com/item?id=43246892",
    "comments": [
      "I hope the comments here don't end up quibbling about the practicality or economics of this toaster specifically, because the point here is the process. This project involves reverse engineering, designing from scratch, manufacturing, developing beginner-accessible documentation, and performing real-world user studies. We should be encouraging people to do more of this!\n \nreply",
      "But I am actually interested in the economics! The author mentions sending his designs out to a factory - I would expect this is astonishingly expensive for a single prototype! Wouldn\u2019t that be thousands of dollars? Is anyone familiar how to get good factory-made parts like this at DIY budgets?Not that that takes away from the article at all. This project has many merits, and although cost may not be one of them, it\u2019s still interesting!\n \nreply",
      "It all depends on what you ask the factory to do.This project seems to take its heating elements, clockwork timer and knob from a classic Dualit 2 Slice toaster - so those parts are all available off-the-shelf.Other than that, this design needs some laser cut and bent metal, and some wooden feet. If you're able to bend the metal yourself and find some off-the-shelf feet, you could probably get the flat sheets of stainless steel laser cut and shipped for less than $100.On the other hand, if one wanted a factory to do more demanding production processes, with more worker time or more machine setup - you're right that it would cost a good deal more.\n \nreply",
      "I build things like this in similarly low quantity - you are probably looking at a grand or two toaster kit there, 95% of which is the custom parts - if it was done locally. The time for someone else to do it is what your paying for. It can be done exceptionally cheap in dollars if YOU do it, but you'll still pay with your time, and you'll still need machine access.Cheap and easy \"factory\" quality is probably PCBWAY or similar in China - they do more than PCBs these days. Call it \"prototype\" budget - several hundred dollars of parts instead of thousands.\n \nreply",
      "My experience is that PCBWay and similar usually offer better quality than doing it yourself. They get their costs down by automating everything they can. They're usually competitive at initial samples than many of the big houses too for essentially the same reason.\n \nreply",
      "No, it wouldn't cost thousands.  There are plenty of shops that specialize in prototypes and small pilot runs and there's nothing complicated about the design or material of this product.\n \nreply",
      "SendCutSend offer surprisingly inexpensive sheet metal parts in single quantities.https://sendcutsend.com/\n \nreply",
      "> sending his designs outsending her designs out\n \nreply",
      "You're both assuming Kasey's pronouns. The default pronoun is they.\n \nreply",
      "It looks like off the shelf electronics with custom sheet metal parts.Are far as low volume prototyping goes, sheet metal is as cost efficient as it gets for large metal parts. If you're sourcing from China, I'd estimate 500 bucks per prototype (with two sets in case one breaks).\n \nreply"
    ],
    "link": "https://www.kaseyhou.com/#/repairable-flatpack-toaster/",
    "first_paragraph": ""
  },
  {
    "title": "Comparing Fuchsia components and Linux containers [video] (fosdem.org)",
    "points": 87,
    "submitter": "bestorworse",
    "submit_time": "2025-03-03T21:06:37 1741035997",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=43246703",
    "comments": [
      "I haven\u2019t watched this talk, but I worked on Fuchsia from the start (I\u2019m no longer at Google) and want to clear up some common questions and misconceptions:1. Fuchsia is a general-purpose operating system built to support Google\u2019s consumer hardware.2. It\u2019s not designed to compete with or replace Android. Its goal is to replace Linux, which Android is built on. One big challenge it addresses is Linux\u2019s driver problem. If Fuchsia succeeds, Android apps could run on it.Fuchsia isn\u2019t trying to replace Android. Its survival for over a decade\u2014through layoffs and with hundreds still working on it\u2014says a lot.I can\u2019t predict Fuchsia\u2019s future, but it\u2019s already running on millions of devices. The git logs show big strides in running Linux programs on Fuchsia without recompilation, as well as progress on the Android runtime. The best way to predict Fuchsia\u2019s future is to look at its hardware architecture support and which runtime is getting attention.Fuchsia\u2019s success will likely depend more on market forces than on technical innovation. Linux is \u201cgood enough\u201d for most needs, and its issues may not justify switching. The choice between sticking with Linux or moving to Fuchsia often favors Linux.Still, I hope Fuchsia succeeds.\n \nreply",
      "Thanks for the info. For those of us not familiar with it, what were the main motivations for building Fuchsia instead of just using Linux?\n \nreply",
      "They did say:> One big challenge it addresses is Linux\u2019s driver problemAndroid devices have been plagued with vendors having out-of-tree device drivers that compile for linux 3.x, but not 4.x or 5.x, and so the phone is unable to update to a new major android version wit ha new linux kernel.A micro-kernel with a clearly defined device driver API would mean that Google could update the kernel and android version, while continuing to let old device drivers work without update.That's consistently been one of the motivating factors cited, and linux's monolithic design, where the internal driver API has never been anything close to stable, will not solve that problem.\n \nreply",
      "I think the reasons have probably changed over time, but my recollection is mostly to have a stable Windows-style driver API so that kernel and drivers can be maintained separately. Making such an API on top of Linux was prototyped, but was unsuccessful.(Historically, that's one big reason that there's lots of Android phones that get a fork of whatever release was current some months before they shipped, and never get substantial updates.)\n \nreply",
      "I'm sure there's technical reasons, but from Google's perspective, one benefit has got to be the non-copyleft license.\n \nreply",
      "The best way to predict Fuchsia\u2019s future is to look at its hardware architecture support and which runtime is getting attention.Having tea leaves instead of a public strategy and roadmap is what's causing the FUD in the first place. Google probably has good reasons for not making any promises but that hedging comes with a cost.\n \nreply",
      "I'm surprised this is still being worked on I was under the impression that Google abandoned this.Also, I would be interested to see a comparison to the wasm component model as it also seems to want to do the same things docker containers do.\n \nreply",
      "It never really got released so nobody is depending on it, so why would google abandon it yet?\n \nreply",
      "They are running on Google's Nest Hub devices[0], so I guess this counts as a release?[0] https://www.theverge.com/2021/8/18/22630245/google-fuchsia-o...\n \nreply",
      "Fuchsia has been on life support for a few years now, but not completely dead yet\n \nreply"
    ],
    "link": "https://fosdem.org/2025/schedule/event/fosdem-2025-5381-comparing-fuchsia-components-and-linux-containers/",
    "first_paragraph": "Fuchsia is a new (non-Linux) operating system from Google, and one of the key pieces of Fuchsia's design is the component framework. Components on Fuchsia have many similarities with some of the container solutions on Linux (such as Docker): they both fetch content addressed blobs from the network, assemble those blobs into an isolated filesystem structure that holds all the dependencies necessary to run some piece of software, and launch namespaced processes with that created directory as its root.The most interesting details are where these two projects diverge. Both have different use cases and requirements, which leads to different strengths between the systems. This talk will largely be focusing on where and why these two similar technologies diverge.Relevant links:\n- Fuchsia's source code\n- Fuchsia's code review\n- Getting started pageBrussels / 1 & 2 February 2025"
  },
  {
    "title": "Lawrence of Arabia, Paul Atreides, and the Roots of Frank Herbert's Dune (2021) (reactormag.com)",
    "points": 25,
    "submitter": "softwaredoug",
    "submit_time": "2025-03-04T00:04:03 1741046643",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=43248429",
    "comments": [
      "When I finally got around to reading Seven Pillars, I wasn't too far in before I was convinced Herbert had the book on his desk the whole time he was writing Dune. So many minor similarities, little scenes that don't quite match up but if you squint they do. Even the arc toward eventually committing war crimes, while seeking some great end for the people he's leading, feels like a connection. But also little stuff like the early travel scenes in Pillars reminded me of early scenes among the Caladanians(?) in Dune.I think the part where I went \"OK yeah this was the reference when he was coming up with the core plot and character of Paul\" was when I came across the part where Lawrence comes up with his novel guerrilla war strategy: he's sick, feverish, possibly dying, in a tent in the desert, tended by a few companions. When he comes out of it, he's got his Path. It's too perfect.[edit] Incidentally, it's not clear to me this author has a good picture of Lawrence's background. Lines like this:> In terms of clothing, Lawrence comes to accept the Arab dress as \u201cconvenient in such a climate\u201d and blends in with his Arab companions by wearing it instead of the British officer uniform.make me think the author isn't aware that Lawrence had already spent a lot of time in the Middle East (especially, IIRC, modern Syria\u2014so, near Damascus) very shortly before the war broke out, and that was a big part of why he was recruited by British intelligence for the mission(s) in Seven Pillars. It was on an earlier, pre-war trip that he'd adopted Arab dress\u2014unlike what's suggested (if not quite stated) in the film Lawrence of Arabia, he was already quite familiar with and comfortable in it.\n \nreply",
      "What's ironic is how people often point to plot points in many franchises (Star Wars, GoT, etc) as being derivative of Dune... Yet Dune itself is fairly derivative of other works\n \nreply",
      "The first book I concur but the series goes in directions I would never have predicted in multiple places in later books, off the top of my head - books four and five stand out in memory, though he laid the groundwork in the first book.\n \nreply",
      "https://en.m.wikipedia.org/wiki/The_Hero_with_a_Thousand_Fac...A tale as old as time\n \nreply",
      "If you really wanted to you could say even George RR Martin was influenced by it.(Game of Thrones spoilers below)Look at the character arc of Daenerys Targaryen in Game of Thrones. You will find some similarities there too both with Dune and with Lawrence of Arabia. But with a female character rather than a male.\n \nreply",
      "Are you saying he knew their ways as if he was born to them?\n \nreply",
      "There\u2019s even a part at the beginning of Lawrence of Arabia film where he puts out a match with his fingers, and throughout, Lawrence proves his ability to overcome pain. Very reminiscent of the Gom Jabar.\n \nreply",
      "I was surprised how much more subtle in many ways Lawrence of Arabia was compared to dune\n \nreply",
      "Superficially, there is a sequence in the film where Peter O'Toole's very blue eyes appear to glow: https://youtu.be/nBiZu5C6lCo?t=12219\n \nreply",
      "A comment mentions it, but Sabres of Paradise is another key component to understanding Dune as a referential text as it pertains to imperialism and religious fervor as an insurrectionist response.\n \nreply"
    ],
    "link": "https://reactormag.com/lawrence-of-arabia-paul-atreides-and-the-roots-of-frank-herberts-dune/",
    "first_paragraph": ""
  },
  {
    "title": "Hacking the Xbox 360 Hypervisor Part 2: The Bad Update Exploit (icode4.coffee)",
    "points": 173,
    "submitter": "kevincox",
    "submit_time": "2025-03-03T18:06:17 1741025177",
    "num_comments": 64,
    "comments_url": "https://news.ycombinator.com/item?id=43244739",
    "comments": [
      "I'm gonna be honest, I thought the story was over when they started talking about \"oh hey here's this hypervisor code that loads extensions\", because obviously extensions are going to be a massive increase in attack surface. But even then, the system wasn't actually broken by the extension being badly designed; the extension was just the most useful target to use the actual attack on.How the hell has this the Xbox 360 hypervisor remained basically impenetrable? You'd think at some point, someone would write and sign a hypervisor extension with a cripplingly bad memory safety bug. Hell, Apple's PPL[0] has better hardware isolation than Xenon's hypervisor mode[1] and it still gets 0wned more often.[0] Page Protection Layer. On Apple processors, every ARM exception level has a corresponding guarded exception level that has privileges over the regular one; chiefly corresponding to memory management.[1] On Xenon, the hypervisor runs in \"real mode\" plus HRMOR; Apple PPL's GL1/2 still have virtual memory and page table permissions.\n \nreply",
      "It sounds like the hypervisor extensions are more like one-shot payloads, which probably have much less attack surface than normal kernel modules that are exposing new functionality to userspace.\n \nreply",
      "> You'd think at some point, someone would write and sign a hypervisor extension with a cripplingly bad memory safety bug.I'd hazard a guess that the Apple hardware is easier to work on than a video game console. Your already sitting in front of a general purpose computer running programming tools. A video game console is the antitheses of that.\n \nreply",
      "and here I am having trouble even removing the case! haha.\n \nreply",
      "Gotta stab it hard in those holes.\n \nreply",
      "Its not that hard when you get the hang of it and have the right tools.\n \nreply",
      "That was not their point.\n \nreply",
      "tour de force - I'm very impressed.\n \nreply",
      "I wish there was somewhere I could toss cash into a softmod bounty.\n \nreply",
      "Assigning dollar values to this kind of work gets messy, fast.Imagine if someone iterated on the exploit presented in the article so that it became a persistent \"softmod\" - who gets the funds?Bounties also discourage open collaboration. For example, if person A has the first half an exploit chain and person B has the second, they're each incentivised to keep the information to themselves and try to get a full chain on their own to claim the bounty. Of course, this assumes they're financially motivated - but if they're not there's no point in the bounty in the first place.\n \nreply"
    ],
    "link": "https://icode4.coffee/?p=1081",
    "first_paragraph": ""
  },
  {
    "title": "Another Conflict Between Privacy Laws and Age Authentication\u2013Murphy v Confirm ID (ericgoldman.org)",
    "points": 41,
    "submitter": "hn_acker",
    "submit_time": "2025-02-28T15:24:48 1740756288",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://blog.ericgoldman.org/archives/2025/02/another-conflict-between-privacy-laws-and-age-authentication-murphy-v-confirm-id.htm",
    "first_paragraph": "This opinion is a routine ruling over TOS formation and whether disputes must go to arbitration. However, before I dig into that question, I need to note the case\u2019s chilling implications.* * *This case involves the Adult Friend Finder service (AFF), which age-authenticates its users. To do so, AFF required users to upload government ID and a selfie as part of the account formation process. AFF uses an affiliated vendor, Confirm ID, to face-scan the selfie for the age authentication. The plaintiffs allege that Confirm ID violates the Illinois Biometric Information Privacy Act (\u201cBIPA\u201d).In other words\u2026.plaintiffs are once again arguing that deploying biometric-based age authentication violates privacy law. (I previously blogged on this issue in 2023. See Kuklinski v.\u00a0Binance). This argument implicitly clashes with how governments around the globe are pressuring, or compelling, Internet services to age-authenticate their users, with the expectation that face scans often will be the cheapes"
  },
  {
    "title": "Public health data disappeared. RestoredCDC.org is bringing it back (restoredcdc.org)",
    "points": 10,
    "submitter": "lparlett",
    "submit_time": "2025-03-04T00:28:46 1741048126",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.RestoredCDC.org",
    "first_paragraph": ""
  },
  {
    "title": "Apple's Software Quality Crisis (eliseomartelli.it)",
    "points": 358,
    "submitter": "ajdude",
    "submit_time": "2025-03-03T16:04:13 1741017853",
    "num_comments": 410,
    "comments_url": "https://news.ycombinator.com/item?id=43243075",
    "comments": [
      "While I worked at Apple in 2021-22 their issues seemed about the same as nearly every other company producing consumer apps and devices; bloated slow garbage with very mediocre quality. Their engineering culture is terrible, especially as it relates to transfer of the \u201cApple ethos\u201d to the next generation of devs. Apple is going to be indistinguishable from the rest of the pack within the next decade.But most of all it seems like it was designed by people who don\u2019t even know what it is for. That combined with the superficial \u201cimplement my Figma masterpiece in code\u201d development approach that includes little to no user testing. Tog weeps. Don Norman weeps. Observe how much breaks when you do something as trivial as bump the default font size by one notch. I am sure it is pixel perfect at default size though.Enter a birth date in a contact entry without a year. Watch as it jumps to the next day when you save because you are editing the date after 0000 of the next day in utc time. That bug has now been in MacOS/iOS for at least 17 years.Sorry, got in to rant mode. I really want \u201cless but better\u201d from things in my life. We as consumers aren\u2019t rewarding companies that take this approach apparently.\n \nreply",
      "As someone forced to use a Windows laptop for work with the new job, I've stopped complaining about my Mac.  It's so much worse on the other side of the fence...Working on Windows makes me appreciate the Mac ecosystem so, so, so very much.\n \nreply",
      "I know the feeling. I'm forced to use a Mac laptop for work, and it's really made me appreciate Linux.\n \nreply",
      "Which distro do you use? I've run a mac and Linux laptop and the Linux setup keeps sending me back to Mac.\n \nreply",
      "Ha. I have to use Linux (Arch) sometimes, and it's made me appreciate OpenBSD :)\n \nreply",
      "Corp IT recently banned my Indy with IRIX 6.5.32 daily driver from the network for some bureaucratic reason. I feel your pain.\n \nreply",
      "? How is this possible? I have zero issues on my Macbook, while Linux is consistently a PITA for me.\n \nreply",
      "For me the biggest gripe is that I cannot configure it as I want and that it assumes I'm computer-illiterate. On top of that a lot of the approaches chosen by Apple regarding e.g. the UI are simply counterintuitive to me.I still prefer it to Windows but (at least for me) it is inferior to a properly setup Linux box with stuff like a titling WM. But if I would to recommend someone a computer just for browsing, email, etc. then a Mac would be my top choice.\n \nreply",
      "can't agree more, to the point i ask if the new job mandates mac,if yes i will end the process right away.even windows is better due to wsl.\n \nreply",
      "I agree, but my M1 MacBook work laptop is by far the fastest dev machine I've ever laid hands on. It struggles a bit from the UX standpoint, two things:1. I have the same desktop layout every time, from left to right: slack desktop app, a two column wide emacs window, a 90col wide terminal. I also have two chrome windows--1 which is the same width as the slack window and overlays it, and another the same width as the emacs window which overlays that one. The problem is every single time I wake my laptop from sleep the terminal window has shrunk to fewer columns and I have to drag it back to full width.2. Sometimes the external monitor support bugs out. I don't know if that's my hub (\"pluggable\" something or other) or the OS or both.Then of course there's all the warts of homebrew, and the fact that it's not easy to build some software..However, the performance of the Apple silicon is nothing short of astonishing. I'm curious about the AMD chips that ship in the new Framework as I look towards an upgrade to my personal laptop, but it's basically between that and a new M4 Max Macbook. Never thought I'd see the day.. will probably wait a year or so before deciding but it's interesting that Apple is even a contender.\n \nreply"
    ],
    "link": "https://www.eliseomartelli.it/blog/2025-03-02-apple-quality",
    "first_paragraph": "Mar 2, 2025 - \u23f1\ufe0f 4 minutes to readThe response to this post has been overwhelming, with sharing on\nHacker News and\nReddit.\nIt's clear that many others are experiencing similar frustrations, indicating\nbroader software quality issue within Apple's ecosystem.To provide more context, here's the workflow that consistently triggers these\nissues: I create a new note at the start of each lecture, add a title and tags\nfor organization, and begin writing with my Apple Pencil Pro.\nThe issues manifest predictably: after filling roughly one page (or \"screen\")\nwith handwritten notes, the iPad starts to overheat, and lag spikes become\nincreasingly frequent.As a long-time Apple user, I've always appreciated the seamless integration of\nhardware and software that is the signature of the Apple ecosystem. However,\nrecent experiences with my iPad Air 11\" M2 has left me questioning whether\nApple has lost sight of what once made their products exceptional.In November, I visited the Apple Store in Turin to a"
  },
  {
    "title": "Ask HN: Who is hiring? (March 2025)",
    "points": 256,
    "submitter": "whoishiring",
    "submit_time": "2025-03-03T16:01:41 1741017701",
    "num_comments": 228,
    "comments_url": "https://news.ycombinator.com/item?id=43243024",
    "comments": [
      "Parts Order | Sr Software Engineer (Backend/Audio) | Remote (US/CA/EU) Hybrid (NYC)  | Full-time | $140k-$170k (US/CA range; adjusted if international)We are a seed-stage startup operating at the intersection of AI and audio engineering, with a focus on Spatial Audio. We deliver best-in-class spatial audio mastering while significantly lowering the barrier to entry for artists, audio engineers, and labels alike.We recently raised our seed round and are expanding our small team to build out the next phase of our platform. We are looking for a Senior Software Engineer who thrives in both creative prototyping and production-grade development. This role is ideal for a generalist with strong Python skills who is comfortable working with audio data. If you have a superpower\u2014whether it\u2019s building MVPs with React, tinkering with digital signal processing, or having absolute pitch\u2014we\u2019d love to hear about it!If you\u2019re interested in learning more, learn more at https://partsorder.notion.site/Sr-Software-Engineer-Backend-...\n \nreply",
      "Neptune (Floodgate-backed) | Founding Engineer | New York 3x/week OnsiteHolla, we want prenup, we want prenup.Neptune helps couples build their financial future together. We're starting with prenups. We combine conversational AI with vetted experts to make complex financial decisions simple and accessible. We're backed by Floodgate (X, Lyft, Twitch) and other world-class investors.You're a fit if you want to: \n- Own features end-to-end, from architecture to deployment. \n- Drive technical decisions that will shape our platform's future \n- Interested in using LLMs to build the next-great consumer financial platform\nStack: Typescript, NextJS, React, Python, PostgresEmail: recruiting [at] meetneptune.com. Include HN in the subject.Job description here: https://meetneptune.notion.site/Neptune-Founding-Engineer-14...\n \nreply",
      "Cathmere | Sr Engineer | Hybrid NYC | Full Time | https://cathmere.com\nWe are are a seed-stage startup revolutionizing how capital flows to emerging markets through our innovative fintech platform. We're seeking an exceptional Senior Engineer to help build our end-to-end solution that connects US investors with high-impact equipment financing opportunities in emerging markets. Our challenge is to build a scalable platform that captures real-world asset performance via multiple streams of operational and financial data in a decentralized network of business counterparties.Responsibilities- Build and maintain a decentralized processing system that can handle a high throughput and complex counterparty relationships- Creating data models that support complex financial analytics- Implementing data quality monitoring and validation systems- Developing APIs for real-time data access and analytics from decentralized ledgeMust Have- 5+ years of experience building complex distributed or decentralized systems and strong communication skillsIf you\u2019re interested in learning more, email matt@cathmere.com\n \nreply",
      "MarketOnce | Openings in Jacksonville, Denver, Boston | Full-timeMarketOnce is a leading provider of marketing and technology services. The MarketOnce portfolio takes an omnichannel approach, spanning across digital, direct, and full-service market research with over 40 years of experience. Headquartered in Denver, the global holding company has offices in Boston, Jacksonville, Portland, Tampa, Austin, Knoxville, Scottsdale, Daytona Beach, and Nantes (FR). For more information, please visit www.marketonce.com.Apply here: https://marketonce.com/careers/\n \nreply",
      "Sequence | London (Hybrid), EMEA (Remote), NYC/SF (Onsite) | Full-time | sequencehq.comSequence is reinventing accounts receivable for modern businesses. We're building a flexible toolkit that helps B2B finance teams scale their revenue collection infrastructure with a world-class billing engine.Backed by top investors including a16z and Salesforce Ventures, we're a team with decades of experience building category-defining marketplace, fintech, and enterprise software.Our tech stack includes Kotlin, TypeScript, React, Postgres, and GCP. We're looking for engineers who enjoy being hands-on, shipping code, and working closely with customers to solve complex financial problems.Open roles:* Senior Product Engineer (Backend) | London (Hybrid) / EMEA (Remote) | \u00a375k-110k + equity* Senior Product Engineer (Frontend) | London (Hybrid) / EMEA (Remote) | \u00a375k-110k + equity* Product Manager | London / NYC / SF (Hybrid)* Founding US BDR | NYC / SF (Onsite) | $100k OTE + equityApply: https://sequencehq.com/careers\n \nreply",
      "EnergyCAP | Boalsburg, PA/Greenwood Village CO (Remote-first Opportunities) | Full-timeEnergyCAP is the leading energy and sustainability platform, helping organizations take control of their utility data, reduce costs, and achieve sustainability goals. With powerful analytics, automated bill management, and seamless integrations, EnergyCAP transforms complex data into actionable insights. From tracking energy usage and optimizing efficiency to simplifying carbon accounting and financial reporting, our platform provides the tools needed to make smarter decisions. Trusted by governments, universities, healthcare systems, and businesses, EnergyCAP helps organizations streamline operations, improve accountability, and drive meaningful sustainability progress.Apply to our openings here: https://www.energycap.com/careers/\n \nreply",
      "AngelList | Senior Software Engineers, Engineering Lead, Product Designer, Senior Data Engineer, etc | Full-time | Hybrid in San FranciscoAngelList's mission is to accelerate innovation by providing startups and investors with the tools they need to succeed. We have the world\u2019s largest early-stage venture portfolio with $17B+ assets under management and $124B+ on our platform, delivering capital to over 12,000 startups. We work with some of the best emerging fund managers to ensure every deserving startup has access to capital.I was hired into the role from one of these HN threads last summer. One of the things that attracted me most was the density of former founders and aspiring entrepreneurs on the team.We're hiring full-stack engineers with experience building web applications at scale. Fintech knowledge is a plus, but not required.I'm the hiring manager for our Funds Engineering team[0]. We\u2019re building the data layer that powers our general ledger. We work on some of the most interesting datasets out there: We model capital flows in the venture ecosystem to understand where value accrues and, ultimately, which investors and founders win or lose.I'd be happy to talk about these roles, about any open role we have at AngelList[1], or any questions you might about working here.[0]: https://www.angellist.com/careers/9870a63b-ac75-436b-b1a6-a2...\n[1]: https://www.angellist.com/careers#open-positions\n \nreply",
      "ON1 | Software Engineers | Portland, OR | Full-timeFor over a decade, we've provided award-winning software to millions of photographers worldwide. We accomplish this by being different than other photo software companies. We put you the photographer first, and this means our customers have complete control over every aspect of their photography.Open roles: \n* C++ Engineer\n* AI Software EngineerApply here: https://wellfound.com/company/on1-3/jobs\n \nreply",
      "Stream | Golang Staff Engineer | On-site: Amsterdam/Boulder/Skopje (sometimes remote)Stream (https://getstream.io/) provides APIs for building in-app chat, feeds and realtime video. We do this for over a billion end users, and thousands of apps such as Nextdoor, Strava, Gojek, Alfagift and othersLooking for Go engineers of all levels, and in particular staff and above. Also very happy to train people on Go if needed (https://www.reddit.com/r/golang/comments/1eiea6q/10_week_pla...)Tech stack is Go, Postgres, Redis, RocksDB, Raft. We run an edge network of servers around the world for optimal video calling latency. Fun part of the job is the high scale and focus we place on engineering.Full job description and apply here:\nhttps://jobs.ashbyhq.com/stream/69536de0-6349-4394-a1a0-ea2e...\n \nreply",
      "FYI, Your website (https://getstream.io/) is broken :-)Application error: a client-side exception has occurred (see the browser console for more information).\n \nreply"
    ],
    "link": "item?id=43243024",
    "first_paragraph": ""
  },
  {
    "title": "SQLite-on-the-Server Is Misunderstood: Better at Hyper-Scale Than Micro-Scale (rivet.gg)",
    "points": 164,
    "submitter": "PaulHoule",
    "submit_time": "2025-03-03T17:29:12 1741022952",
    "num_comments": 92,
    "comments_url": "https://news.ycombinator.com/item?id=43244307",
    "comments": [
      "Just a single data point but... I am looking at replacing a custom database with SQL.   The application will only ever run on one server, so Sqlite3 was a contender.  The database is very read mostly, which makes it a strong contender.The custom database is extraordinary fast at some things, so it's a complex decision.  I needed benchmarks.I chose something that mirrors how data is accessed in the custom database.  The existing database does have several tables, but it's hard to use so most accesses are single table, indexed by rowid, with maybe 100,000,000 rows.  So I benchmarked a single table, indexed on primary key only, 100,000,000 small rows.  Since it was easy because I could use the same SQL I benchmarked both Sqlite3 and Postgresql.I expected Sqlite3 to beat Postgresql on single row access / updates, and Postgresql3 to get the upper hand on multi row access / updates.  I was surprised to see Sqlite3 was about twice as fast as Postgresql on everything, including inserting the 100,000,000 records.  It was always single writer.  Had I tested multiple writers I expect I would have seen really flex its muscles.In case your wondering, the custom database is 100 to 1,000 times faster than Sqlite3 at accessing a single record.  Getting that performance was achieved with some big tradeoffs, which make it so hard to code for it creates reliability problems.\n \nreply",
      "I\u2019m building a local-first web app, and SQLite works well for my case since a single project can be contained in one database file, just like users are used to with existing desktop applications.What I\u2019d really like is an easy way to sync the SQLite database state to a cloud service. Most existing options expect you to query against a remotely hosted database and charge per read/write.Since the database will have around 100,000 rows and you're typically working with all the data at once, streaming parts of it doesn\u2019t make sense for my use case.The closest I\u2019ve found is Turso, which has offline writes in private beta, and SQLite Cloud, which lists local-first and offline sync as \"coming soon.\"The simplest approach might be letting users push to S3 storage with versioning. Ideally, it would also support point-in-time restores, tracking incremental updates alongside full snapshots.Even better, I\u2019d manage minimal server-side infrastructure and just pull the SQLite database from a service that handles syncing and management.\n \nreply",
      "Maybe I am misunderstanding which part you want in the cloud, but that sounds like litestream. Let\u2019s you transparently backup a live SQLite database to a remote destination.https://litestream.io/\n \nreply",
      "I depend on litestream for production backups and as the months wear on without any releases I am getting more nervous. To be clear, I don\u2019t feel entitled to anything with an open source project like this, but bug reports and fixes seem to be accumulating. I have flirted with the idea of building from main.I\u2019ve also flirted with the idea of forking litestream and stripping it down dramatically. The reason why is that I don\u2019t like the idea of the production server being in charge of rotation and deletion. It seems like the thing getting backed up shouldn\u2019t have the privilege of deleting backups in case it gets compromised. I might even go so far as to propose that the \u201ceven liter stream\u201d process merely writes to a different local volume and then some other process does the uploading but I haven\u2019t gotten beyond the daydream stage.\n \nreply",
      "What kind of bugs have you experienced or are you worried about?  Backup software shouldn\u2019t need to be frequently updated\n \nreply",
      "Yeah, I was about to suggest litestream. Isn't it local-first-with-cloud-backups?\n \nreply",
      "SQLite has a session extension that can record changes on a local database into a changeset and you can replay those changes on another SQLite instance.  Note that it replays what the changes were, not the queries that resulted in the changes.  When applying changes you provide a conflict handler.  (You can also invert changesets making a handy undo/redo feature.)You can save conflicts to another changeset.  There is also a rebaser to help deal with multiple way syncing.https://www.sqlite.org/sessionintro.html   - overviewhttps://www.sqlite.org/session/sqlite3changeset_apply.html  - conflict informationhttps://www.sqlite.org/session/rebaser.html  - rebaser\n \nreply",
      "Have your tried CR-SQLite ? https://vlcn.io/docs/cr-sqlite/introIt implements CRDT as SQLite extension.\n \nreply",
      "Offline-first databases are a hard problem because there isn't just one copy of the database on the user's side, there are N copies - every browser tab or device on which the user can open the local database and make an edit. It's basically an AP multi-master database (= the same row can be edited at different nodes at the same time), and you likely cannot achieve good results without a database that natively supports multi-master operations.\n \nreply",
      "That\u2019s not necessarily true; if you use Origin Private Filesystem along with a Web Worker that acts as a local database server and works off a single SQLite database, you at least have a single DB file per device. From there on, your problem becomes state reconciliation on the server, which CRDTs should help solving.Not an easy problem for sure, but the web platform is surprisingly capable these days.\n \nreply"
    ],
    "link": "https://rivet.gg/blog/2025-02-16-sqlite-on-the-server-is-misunderstood",
    "first_paragraph": ""
  },
  {
    "title": "The power of interning: making a time series database smaller (gendignoux.com)",
    "points": 212,
    "submitter": "todsacerdoti",
    "submit_time": "2025-03-03T17:03:03 1741021383",
    "num_comments": 53,
    "comments_url": "https://news.ycombinator.com/item?id=43243914",
    "comments": [
      "Interning strings saves a ton of space.  I wish more programmers would use it.Back in the 32-bit days I was working on a large (multi GB) distributed Oracle database system but I couldn't use transaction log shipping (don't ask).  Database A was the \"main\" system and database B was the \"replica\".  To keep them in sync I needed a program that would compare the two databases and then generate an update script that would make B look like A.Complicating things was that, for some reason, floating point data in binary format would never match exactly between the two systems, so all floats had to be exported as text.The first attempt by a junior dev was implemented in C#.  Not only was it terribly slow, but it also ran out of memory.I wrote a new version in C that interned all strings using a hash table and a custom bump-allocator. I also exported every field in the database as a string, so I didn't have to deal with native types. Using this technique meant that a database record could be represented as a plain array of pointers to the interned strings.Since each string was only recorded once, and every field was a pointer to a string, should two database records have the same values then they must by definition point to the same string.  Comparing database rows was as easy as doing a memcmp() on the two pointer arrays, one being a record from database A and the other being a the record from database B.Not only was the system incredibly fast, but it never took more than 150MB of memory to run.\n \nreply",
      "This is mostly the real reason why interning gets used, to avoid long string comparisons over saving memory as such.Interned strings tend to not have a good cleanup mechanism, in a system where a lot of them are churned through. So often they tend to actually use more memory as data patterns evolve in a system.I use the same trick when parsing json, where a large set of rows tend to have the keys repeated & the conversion to columnar is easier if the keys are interned.\n \nreply",
      "If your language supports good strong/weak references and containers thereof, cleaning up dead interned strings isn't hard. I'm not aware of any language that provides this out-of-the-box, unfortunately.Why do so many languages make weak references such second-class citizens? Why do all containers suck so much that you have to implement your own (and that's hoping the language is actually efficient enough to let you?)\n \nreply",
      "Go recently did add a new weak pointers and string interning package to its standard library which is an interesting read.[0] https://go.dev/blog/unique\n \nreply",
      "> not aware of any language that provides this out-of-the-box, unfortunately.Many lisps. Racket, for example.\n \nreply",
      "> I'm not aware of any language that provides this out-of-the-box, unfortunatelyThe currently most prominent example would be Rust. Rc<T> is a simple generic container that implements reference counting, and any instance of it can be downgraded to a Weak<T>. Or Arc<T> and std::sync::Weak<T> if it needs to be thread safe.\n \nreply",
      "I've done it in C++, so Rust is probably capable of it if you add enough layers of rc refcell and whatever else it requires to fit into its restricted worldview.Does Rust actually have container implementations that do all of the following:* When walking the container (either iterating or looking up, even through a non-mutable reference), call a user-provided predicate (not just builtin \"weak\" like many languages have via weakset/weakkeymap/weakvaluemap) to detect if a node should be considered \"dead\", and if so transparently remove the node. [In my experience this is relatively easy to add when you're implementing the container algorithms yourself, though I've never done it for bulk algorithms yet.]* When looking up a key (which may have different type or identity), the lookup returns the actual key the container had. [This may be impossible for container implementations that split the key.]\n \nreply",
      "Mutating a container through a shared reference means the container either has to be single-threaded (not marked as Sync/Send), or be thread-safe.The single-threaded ones are easy to make, but Rust will prevent you from sending them to another thread, which is probably something you want.For thread-safe things, look into the crossbeam crate, it has really good collections.One I worked with was the dashmap, which has a .retain() method [1] that works over a shared map reference, but runs a closure which gets mutable access to each key and value, and decides whether to keep the pair or not.Its .get() [2] uses equality (so you can use a different object), but returns a reference to the original key-value pair. The .get_mut() will return it as mutable, but inside a guard that keeps the item locked until it goes out of scope.[1] https://docs.rs/dashmap/latest/dashmap/struct.DashMap.html#m...[2] https://docs.rs/dashmap/latest/dashmap/struct.DashMap.html#m...\n \nreply",
      "`.retain()` unfortunately isn't what I'm talking about, since it's at least O(n) per call. It might be better if you can arrange to call it just once after a batch of operations, but that isn't always the case.\"Delete as you go\" usually\u2020 adds no space/time complexity to the operations you're already doing (though it does affect the constant factor). It does mean you're giving up on predictable destructor calls, but generally the \"heavy\" destructor was called by whoever made it expire (e.g. the death of the last remaining shared owner if that's what expiry is based on).\u2020 If many expirations happen at once, this can easily drop to merely amortized time. It's also possible for a container to offer, say, O(1) iteration via dedicated pointers but have a delete operation that's more complicated.\n \nreply",
      "To the first question, not really, and if it did it would be pretty fragile because of mutability requirements. It's fragile in C++ too because of iterator invalidation, Rust mostly turns that into a compiler error.To the second question, yes, it's super common.\n \nreply"
    ],
    "link": "https://gendignoux.com/blog/2025/03/03/rust-interning-2000x.html",
    "first_paragraph": "\n rust perf json\nMarch 3, 2025by Guillaume Endignoux\n\n @gendx |  RSS\nThis week-end project started by browsing the open-data repository of Paris\u2019 public transport network, which contains various APIs to query real-time departures, current disruptions, etc.\nThe data reuse section caught my eye, as it features external projects that use this open data.\nIn particular, the RATP status website provides a really nice interface to visualize historical disruptions on metro, RER/train and tramway lines.\nA usual day of disruptions on ratpstatus.fr.Under the hood, the ratpstatus.fr GitHub repository contains all the JSON files queried from the open-data API, every 2 minutes for almost a year now.\nA repository with 188K commits and more than 10 GB of accumulated data at the last commit alone (as measured by git clone --depth=1) is definitely an interesting database choice!\nTo be clear, this post isn\u2019t in any way a critique of that.\nRATP status is an excellent website providing useful information t"
  },
  {
    "title": "One Logo, Three Companies (estilofilos.blogspot.com)",
    "points": 79,
    "submitter": "ghc",
    "submit_time": "2025-03-03T18:54:08 1741028048",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=43245315",
    "comments": [
      "This leaves me with more questions than answers, how did these three companies come up with the idea of using that logo? Did they just independently arrive at same design (seems unlikely)? And how did the trademark registration process go for the second and third companies that registered it?\n \nreply",
      "Turns out they used to be one conglomerate, but World War II changed that [0]:> The Mitsubishi Group traces its origins to the Mitsubishi zaibatsu, a unified company that existed from 1870 to 1946. The company, along with other major zaibatsu, was disbanded during the occupation of Japan following World War II by the order of the Allies. Despite the dissolution, the former constituent companies continue to share the Mitsubishi brand and trademark.[0] https://en.wikipedia.org/wiki/Mitsubishi\n \nreply",
      "The pencil company referenced in the article does not appear to have been part of the Mitsubishi zaibatsu however.\n \nreply",
      "I used to think it's cool the same company makes a pencil and an F-16 derivative ( https://en.wikipedia.org/wiki/Mitsubishi_F-2 ), but alas, the pencilmaker is not the same company.Mitsubishi also makes the world's most expensive toaster: https://youtu.be/Lq3iwWaoU7w\n \nreply",
      "These pages[0][1] has more details. The families had three different emblems to start. The zaibatsu came up with the now famous three diamond design in in 1873[2], but there were no trademark laws until 1884, and many companies proceeded to use the logo. The pencil company first registered this in 1903. The zaibatsu finally got to it in 1914, but the earlier filing by the pencil company was honoured.As for the cider company, sounds like they've been selling it like it since 1913[2], but registered it in 1919? My guess is that since it was a regional product with the product type in the name that's already established (like [3]), they allowed it.[0] https://ja.wikipedia.org/wiki/%E3%82%B9%E3%83%AA%E3%83%BC%E3... (translated: https://ja-m-wikipedia-org.translate.goog/wiki/%E3%82%B9%E3%...)\n[1] https://www.buzzfeed.com/jp/kenjiando/mitsubishi-pencil\n[2] https://www.mitsubishi.com/ja/series/yataro/11/\n[3] https://ja.m.wikipedia.org/wiki/%E5%9C%B0%E5%9F%9F%E5%9B%A3%...\n \nreply",
      "The Kanji for Mitsubishi is \u4e09\u83f1, which literally means \u201cthree rhombus\u201d. It is possible that they were independently invented, but the hypothesis on family crest crossovers still feels more likely\n \nreply",
      "Independent invention seems unlikely to me - there are different colors, different ways to arrange the three rhombi, etc.\n \nreply",
      "The design is much older in east asia, I've seen it on 19th century textiles and pottery for sure but I suspect it goes back a lot more than that.The shape is somehow associated with the name mitsubishi, possibly through visual or phonetic punning that is common in pictogram-based writing systems and tonal languages. Mitsubishi the name is more widespread than this one family or this group of companies, and the symbol appears to have long associated with the name per se rather than this specific mitsubishi. Mitsu sounds like three, I don't know what the rhombus connection is.That shade of red has a specific proper name in japanese (think like alice blue in english) and has long been associated with japan by the japanese.I don't think any of this is a coincidence there's a connection between all this stuff. But I don't know what it is and I don't think the article author does either.\n \nreply",
      "> a specific proper name in japanese (think like alice blue in english)I hadn't heard of that one [0], the example that comes to mind is \"Canary Yellow\", but I suppose that's not so bound up to a specific cultural history.[0] https://en.wikipedia.org/wiki/Alice_blue\n \nreply",
      "> pictogram-based writing\n\ni think you might mean ideogram-based?\n \nreply"
    ],
    "link": "https://estilofilos.blogspot.com/2016/03/one-logo-three-companies-i.html",
    "first_paragraph": "\n\nMitsubishi\u2026 3 logos .\n\nI wish I could understand what you meant, Anonymous.BT\n\n\n\nPost a Comment\nYour comments are welcome and appreciated.\n\n\n\n\n\nYour comments are welcome and appreciated."
  },
  {
    "title": "An Attempt to Catch Up with JIT Compilers (arxiv.org)",
    "points": 115,
    "submitter": "mfiguiere",
    "submit_time": "2025-03-03T16:06:50 1741018010",
    "num_comments": 101,
    "comments_url": "https://news.ycombinator.com/item?id=43243109",
    "comments": [
      "I think the missing piece here is that JavaScriptCore (JSC) and other such systems don't just use inline caching to speed up dynamic accesses; they use them as profiling feedback.So, anytime you have an IC in interpreter, baseline, or lightly optimized code, then that IC is monitored to see how polymorphic it gets, and that data is fed back into the optimization pipeline.Just having an IC as a dead-end, where you don't use it for profiling is way less profitable than having an IC that feeds into profiling.\n \nreply",
      "Well on dynamic languages the ICs do give a nice order of magnitude speed-up by themselves, since the guard eliminates a whole hashtable (or linear) lookup instead of (in this case) a single memory indirection.But yeah - on spidermonkey we found that orienting our ICs towards being stable and easy to work with, as opposed to just being fast, ended up leading to a much better design.This is a nice result though.  Negative, but good that they published it.What would be a good next step is some QEMU-style transformation, pull out basic blocks, profile them for both hotness, and incoming arguments at function starts, and dynamic dispatch targets.. then use that to re-compile the whole thing using method-jit and in particular inlining across call-paths with GVN and DCE applied.I kind of expect the results to be very positive, just based on intuition.. but it'd be cool to see how it actually turned out.\n \nreply",
      "A minor nitpick: ICs don't give that much benefit in monomorphic languages like scheme.\n \nreply",
      "Apologies if this response seems aggressive - this is just a topic I'm very passionate about :)I think technically in languages like scheme, the opportunity would be to optimize other sorts of dispatches.  The classic dispatch mechanism in scheme is the \"assoc\" style list-of-pairs lookup.In this case, the \"monomorphization\" would be extracting runtime information on the common lookups that are taken.  This is doable in a language like scheme, but it requires identifying parts of data structures that are less likely to change over time - where it makes sense to lift them up into hidden types and effectively make them \"static\".Imagine if you could designate particular `(list (cons key value) ...)` value as \"optimizable\" - maybe even with a macro/function call : `(optimizable ((a 1) (b 2) ...))`This would build a hidden shape for the association's \"backbone\" and give you back a shaped assoc list, and then you would be able to optimize all uses of `(assoc ...)` on lists of that kind in the same way you optimize shaped objects.A plumbing exposed version of this would just let you do `(let my-shape (make-shape '(prop1 prop2 ...)))` and later `(my-shape '(1 2 ...))` to build the shape-optimized association list.It's kind of neat when you realize that almost everything the runtime type-inference regime in a JIT compiler does.. is enable eliding lookups across data structures where we can assume that some part of that data structure is \"more static\" than other parts.In JS that data structure is a linked-list-of-hashtables, where the hashtable keys and the linked list backbone are expected to be stable.But the general idea applies to literally any structure you'd want to do lookups across.  If you can extract a 'conserved shape', you can apply this optimization.\n \nreply",
      "Couldn't PICs and monomorphization be seen as duals? They are both solving the problem of how to make polymorphic code have fewer branches.\n \nreply",
      "PICs are the core mechanism of monomorphization in the VMs that do it\n \nreply",
      "I was thinking of static monomorphization as in Rust.\n \nreply",
      "But then there isn't a duality.If your language is static enough, then static devirt is profitable enough that you can stop there.If your language is dynamic enough, then PICs are the main driver of devirt. (Though all PIC-based systems couple that with static analysis and that static analysis is powerful enough that it can sometimes devirt without the PICs' help.)\n \nreply",
      "I meant \"dual\" in the analogous sense, not as strict mathematical duals where one could replace the other. That both are solving devirt from opposite ends. I read @bjoli's comment with the analogous connotation.Your last sentence, would be if Rust used a PIC to optimize calls to dyn Traits?\n \nreply",
      "Indeed, this was literally the conclusion of the first paper that introduced polymorphic inline caches.I'll add that the real benefit of ICs isn't just that compiled code is specialized to the seen types, but the fact that deoptimization guards are inserted, which split diamonds in the original general cases so that multiple downstream checks become redundant. So specialization is not just a local simplification but a global simplification to all dominated code in the context of the compilation unit.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2502.20547",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Launch HN: Cuckoo (YC W25) \u2013 Real-time AI translator for global teams",
    "points": 45,
    "submitter": "yonghee",
    "submit_time": "2025-03-03T18:39:32 1741027172",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=43245153",
    "comments": [
      "Congrats on the launch! Does it support Hinglish (Hindi + English)? I'm a native Hindi speaker, but I found the Hindi in the demo video hard to follow. Many Hindi speakers, especially in everyday conversations, naturally mix Hindi and English \u2014 often using English for technical or difficult terms. Would love to know if the model handles that kind of language blend\n \nreply",
      "My batchmate was saying the same thing, actually.\nHe was trying to use only Hindi for the demo sakes and it was almost difficult for him to explain his product without referring to English.We do our best to deal with langauge changes. For example, when talking bio, almost half of the sentence is in English terms and Cuckoo does pretty well in that context as well!\n \nreply",
      "Plot twist!\n \nreply",
      "This looks super useful for global teams dealing with technical discussions! I\u2019m curious about how Cuckoo handles domain-specific jargon beyond what\u2019s included in uploaded reference documents. For instance, in fields like AI/ML or DevOps, terminology evolves rapidly, and even human interpreters sometimes struggle with nuanced technical meanings.Does Cuckoo adapt dynamically to new terms within a conversation, or does it require preloading domain knowledge beforehand? Also, how do you ensure accuracy in cases where direct translation doesn\u2019t capture the intended meaning (e.g., idiomatic phrases or cultural context differences)?Excited to see how this evolves!\n \nreply",
      "Hey Ryoo, thanks for the question.Right now, we have a set of \u201cindustry presets\u201d where we have preloaded keywords and context for different industries (GPU, LLM, GPT for AI, for example).Over time, we want our users to build upon these preset terms, for example, automatically adding the terms mentioned in different meetings. There is a challenge here\u2014how do we add terms that may be mispronounced or that the LLM may have mixed up? I think having the context of their conversation and their base documents for these conversations could definitely help.\n \nreply",
      "Congrats on the launch!Also question: your writing makes you seem quite bilingual and fluent in English. Given this, would you consider yourself a user of your own product? Do you often find yourself needing to use it? It strikes me that the main users would be people who struggle with English specifically. Though I guess with recent innovations in China, potentially more English speakers will start needing to translate from Chinese.\n \nreply",
      "I love this question!Yes, I am bilingual. I was fortunate enough to study both in Korea and Canada.I use our product every day when I\u2019m meeting with customers in Japan and China. We joke that we are our very first customers. Personally, it\u2019s best when I get to meet them in person and use our in-person meeting feature since I get to see their reactions.I would say half of our users are fluent in English since they mostly work for U.S. companies. The other half would be people in Korea, Japan, China, and more who need the language support.\n \nreply",
      "This is awesome!Is there a consumer version available?Or is there a company focused on that side of the business?\n \nreply",
      "While we are focusing on business use cases, we are seeing a few invidivuals sign up for their own uses.Email me at yonghee@cuckoo.so so I can help you out with first few months!\n \nreply",
      "Love it! Friction is part of (language) learning, so hopefully some doses will remain down the line.\n \nreply"
    ],
    "link": "item?id=43245153",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Agents.json \u2013 OpenAPI Specification for LLMs (github.com/wild-card-ai)",
    "points": 83,
    "submitter": "yompal",
    "submit_time": "2025-03-03T17:01:59 1741021319",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=43243893",
    "comments": [
      "I've been following agents.json for a little while. I think it has legs, and would love to see some protocol win this space soon.Will be interesting to see where the state/less conversation goes, my gut tells me MCP and \"something\" (agent.json perhaps) will co-exist. My reasoning being purely organisational, MCP focuses on a lot more, and there ability to make a slimmed down stateless protocol might be nigh impossible.---Furthermore, if agents.json wants to win as a protocol through early adoption, the docs need to be far easier to grok. An example should be immediately viewable, and the schema close by. The pitch should be very succinct, the fields in the schema need to have the same amount of clarity at first glance. Maybe a tool, that anyone can paste their OpenAPI schema into, and it gets passed to an LLM to generate a first pass of what their agents.json could look like.---The OpenAPI <> agents.json portability is a nice touch, but might actually be overkill. OpenAPI is popular but it never actually took over the market imo. If there is added complexity to agents.json because of this, I'd really question if it is worth supporting it. They don't have to be 100% inoperable, custom converters could manage partial support.---A lot of people are using agentic IDE's now, would be nice if agent.json shared a snippet with instructions on how to use it, where to find docs and how to pull a list and/or search the registry that people can just drop straight into Windsurf/Cursor.\n \nreply",
      "1) Thanks for being a part of the journey! We also want something that works for us as agent developers. We didn't feel like anything else was addressing this problem and felt like we had to do it ourselves.We love feedback! This is our first time doing OSS.\nI agree - MCP and agents.json are not mutually exclusive at all. They solve for difference clients.2) Agreed. Something we're investing in soon is a generic SDK that can run any valid agents.json. That means the docs might getting a revamp soon too.3) While many API services may not use OpenAPI, their docs pages often do! For example, readme.com lets you export your REST API docs as OpenAPI. As we add more types of action sources, agents.json won't be 1:1 with OpenAPI. In that way, we left the future of agents.json extensible.4) Great idea! I think this would be so useful\n \nreply",
      "interoperable*\n \nreply",
      "How does this compare to llms.txt? I think that\u2019s also emerging as a sort of standard to let LLMs understand APIs. I guess agents.json does a better packaging/ structural understanding of different endpoints?\n \nreply",
      "llms.txt is a great standard for making website content more readable to LLMs, but it doesn\u2019t address the challenges of taking structured actions. While llms.txt helps LLMs retrieve and interpret information, agents.json enables them to execute multi-step workflows reliably.\n \nreply",
      "In what ways is the agents.json file different from an OpenAPI Arazzo specification? Is it more native for LLM use? Looking at the example, I'm seeing similar concepts between them.\n \nreply",
      "We've been in touch with Arazzo after we learned of the similarities. The long-term goal is to be aligned with Arazzo. However, the tooling around Arazzo isn't there today and we think it might take a while.\nagents.json is meant to be more native to LLMs, since Arazzo serves other use cases than LLMs.To be more specific, we're planning to support multiple types of sources alongside REST APIs, like internal SDKs, GraphQL, gRPC, etc.\n \nreply",
      "Thanks, that's helpful. I agree there are many other sources REST APIs where this would be helpful. Outside of that I would be interested in understanding the ways where Arazzo takes a broader approach and doesn't really fit an LLM use case.\n \nreply",
      "It's not that Arazzo can't work for LLMs, just that it's not the primary use case. We want to add LLM enabled transformations between linkages. Arazzo having to serve other use cases like API workflow testing and guided docs experiences may not be incentivized to support these types of features.\n \nreply",
      "What's the license of the Python package: https://pypi.org/project/agentsjson/AGPL? https://github.com/wild-card-ai/agents-json/blob/master/LICE...\n \nreply"
    ],
    "link": "https://github.com/wild-card-ai/agents-json",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n\n\n\n\n\nThe agents.json Specification is an open specification that formally describes contracts for API and agent interactions, built on top of the OpenAPI standard. The current version is 0.1.0.Use the Wildcard Bridge Python package to load, parse, and run agents.json.\n\nThe agents.json Specification is an open specification that formally describes contracts for API and agent interactions, built on top of the OpenAPI standard.The current version is 0.1.0.Give feedback, share your projects, and get help in our Discord.The full schema is available here.Enabling AI agents to interact with APIs is difficult. We faced the same problem as many others building agents: altering APIs to work reliably with LLMs and executing multiple API calls successfully in a row is a trial and error process.APIs are designed for developers and not LLMs. If you're bu"
  },
  {
    "title": "The Golden Age of Japanese Pencils, 1952-1967 (2022) (stlartsupply.com)",
    "points": 197,
    "submitter": "apokryptein",
    "submit_time": "2025-03-03T16:51:33 1741020693",
    "num_comments": 53,
    "comments_url": "https://news.ycombinator.com/item?id=43243716",
    "comments": [
      "In the age of digital data storage and keyboards, these writing tools do not get the attention and celebration they deserve.There are tools so unique that really changes how you think and how you write. Some of these tools are so timeless and they are irreplaceable.When you acquire one of these tools, and appreciate all the craftsmanship and engineering went into these things, and notice how all blends and becomes invisible, you realize that that simple thing is not that simple in the end.I'm very grateful that the great papers, pens and all these supplies are still made, albeit in smaller varieties and numbers since the demand is lower than before. But, nothing replaces a silent thinking session on a good paper with good set of writing utensils. No notifications, no indirections, nothing. Just directly projecting your thoughts to a medium which has no batteries, encryption, etc.Very personal, and much more productive.\n \nreply",
      "one literally can raise the same points about a running text-editor and a keyboard with 'Silent Mode/Do Not Disturb' activated, except the timeless argument as we don't have even 200 years of electronic computers... have you ever tried to be productive by sharing letters in a mailling list when brainstorming ideas? /sand don't come up with research comparing the 2 with biases like participants not even knowing how to touch-type or that variable not even being mentioned\n \nreply",
      "OK. I'll come with another research, the one I'm doing on myself for more than 30 years.See, I'm both typing and writing for more than 30 years, and I started doing both almost the same time, so there's no inherent bias there. Here's what I found.- I can type around 75 WPM in a good day, yet writing on paper always brings out clearer ideas.- I write my blog posts in iA writer, fullscreen and in DnD mode, yet drilling some ideas on paper is still necessary for reflection.- I have encrypted diary on my personal computer, yet I always prefer to carry a good notebook with a good pen, and find that I can write more sincerely and drill into myself better during reflection sessions.- I can design programs in my head, brainstorm with mind maps, or draw architecture diagrams with relatively high speeds, yet designing on paper always results in better architectures, less bugs, and better performance.- I can skirmish in real time like an old IRC person (because I am), or fire salvos of posts with different amount of flame included (because I lived through newsgroups), yet I prefer elaborating ideas on paper or in silence before writing them, because seeing what I want to say, and scribbling them with a good pen always brings different perspectives.What I found by working on myself is ironically parallel to the research you denounce. Writing is different than typing and is a deeper experience with more connection to self. Having a small notebook around boosts my productivity 5x, while reducing planning overhead and mental load incurred by it to almost zero, and this is while I have a tool which I plan my next three weeks with great detail.So, maybe you should try writing, and while you're at it, you can even find a pen friend which spends a couple of hours to write you a nice and sincere letter, and you'll do the same and understand why some of us like writing and everything related to that.Who knows?\n \nreply",
      "i remember burning a pile, like 15 cm thick, of paper notes once. i still have another one but i didn't parsed to the computer yetyou don't know how sacred to me is my scratch (temporary) file on Emacs or my org documents or Gimp files with stuff like studies of games platform level design (quite easy to screenshot views and highlight stuff i want to comment) or digital collages materialfor me there isn't anything closer than having a coffee after weeks/months without having one, Android running Emacs on Termux in my pocket and going for a walk with or without a science podcast playing... ironically the last thing i did on paper was the tinker of specific apps layers layouts on my keyboard (now i use an online keyboard editor alongside a text document)\n \nreply",
      "Taking notes with a keyboard does not work for me.\n \nreply",
      "It's not the same. Hand writing allows infinitely more possibilities for annotations and illustration.\n \nreply",
      "There is an FAA accepted test of paint film hardness that requires special calibrated pencils, available ONLY from Mitsubishi pencil company, in packages of 17 for $224.  Each pencil comes with an individual certificate of calibration shows that it meet's it's specified hardness level. The test is ASTM D 3363, \"Standard Test Method for Film Hardness by Pencil Test\"https://www.gardco.com/Products/Hardness-Testers/Scratch-Har...\n \nreply",
      ">  Masaki had the idea to register a three-diamond trademark, along with the \"Mitsubishi\" name, which means 'three diamonds.' (It may surprise you to learn that this was ten years before the much better-known Mitsubishi Group of heavy industry companies registered its name and identical mark. Mitsubishi Pencil has no connection to the numerous other Mitsubishi companies in Japan; it is and has always been a manufacturer of writing and drawing supplies.)Well, I learned something new today. I always thought the pencils were part of the group but apparently they're not.Seeing the iconic three diamond mark along with the name always made me think the pencils were related to the cars.\n \nreply",
      "In general, I find Japan to be unmatched when it comes to stationery. Pens, pencils, notebooks, etc... Everything is just better: the simple stuff, like what you can find in \"konbini\" and \"100 yen shops\", entire floors in department stores like \"Hands\", and all the way up to luxury. As you might expect, Japanese brands of stationery are popular worldwide.So it is not surprising that Japan had a golden age of pencils, and that you can still buy the products today and that they are still the best.\n \nreply",
      "They are very good at higher end paper, pencils and erasers. Their gel and roller pens are mostly unmatched.OTOH, while they're top tier in fountain pens, Germans really equal with them. Lamy, Faber Castell, Diplomat, Kaweco and of course Montblanc make great pens. Pilot & Sailor are not behind them, though. Mitsubishi Pencil bought Lamy so things will get interesting.Inks are the same. Germans and Japanese are head to head. OTOH, except Leuchttrum and Rhodia, I can't find many fountain pen first papers from Europe.Funnily, when it comes to fountain pens, there's another interesting contender. China. While they copy most of the stuff, their domestic brands make great pens and ink.Also, a company in my country started making a paper which rivals Yu-Sari and Tomoe River. I write letters with it, and it's great.\n \nreply"
    ],
    "link": "https://notes.stlartsupply.com/the-golden-age-of-japanese-pencils-1952-1967/",
    "first_paragraph": "It was the summer of 1952, and the executives of Tombow Pencil were about to revolutionize the Japanese pencil industry\u2014or, possibly, fall flat on their faces. Hachiro Ogawa, the son of founder Harunosuke Ogawa, was Tombow's managing director, and he had just finished a years-long project, at enormous cost, to make the best pencil Japan had ever seen. It was called \"HOMO,\" because in comparison with other Japanese pencils of its day, Tombow's new model had a much more homogenous core. Pencil cores are a mixture of graphite and clay (thanks to Nicolas-Jacques Cont\u00e9's invention of the modern pencil in the late eighteenth century), and the components in early cores were not always evenly mixed. This was particularly true in Japan, where pencils had only been made since the turn of the century and advanced industrial equipment was just starting to become available.Hachiro's team at Tombow was determined to do whatever it took to produce more consistent cores. They struck up a working relat"
  },
  {
    "title": "How the U.K. broke its own economy (theatlantic.com)",
    "points": 98,
    "submitter": "speckx",
    "submit_time": "2025-03-03T18:45:42 1741027542",
    "num_comments": 153,
    "comments_url": "https://news.ycombinator.com/item?id=43245235",
    "comments": [
      "https://archive.ph/2025.03.03-161011/https://www.theatlantic...",
      "The last government intentionally banned the cheapest source of energy, onshore wind,  from being built in England, by making it so that a single complaint could stop a project.I cannot believe this article talks about planning constraints and energy prices and doesn't mention that.\n \nreply",
      "\"The cheapest source of energy\" trope again. Intermittent energy (electricity) != on demand electricity.\n \nreply",
      "I hate the large scale turbines. I'm sure small scale or other designs are OK. But screw the large turbines. Where I grew up they outsourced the jobs and then we were left with ugly giants ruining the once beautiful mountains. It's on par with strip mining (aesthetically).\n \nreply",
      "I like em.\n \nreply",
      "They're one of the only pieces of technology that I think often (not always) improve the appearance of a landscape.Maybe also lighthouses. Sometimes.\n \nreply",
      "Lighthouses have only become aesthetic because of their rarity rendering them as quaint or nostalgic. Modern versions and their impacts would be largely protested.\n \nreply",
      "I bet you like when they clear cut a mountain top for a solar project too.\n \nreply",
      "Where do you think coal and oil comes from? It's ok as long as other people's environment is destroyed for non-renewable energy but not your environment for renewable energy?\n \nreply",
      "Lol, they used to mine coal under that very mountain (until it was outsourced). It's not like we have some insular life. The region is economically depressed. People would rather have jobs while destroying the environment than not have jobs while destroying the environment  and recreational value simultaneously.\n \nreply"
    ],
    "link": "https://www.theatlantic.com/ideas/archive/2025/03/uk-needs-abundance/681877/",
    "first_paragraph": "With the best intentions, the United Kingdom engineered a housing and energy shortage.Produced by ElevenLabs and  News Over Audio (Noa) using AI narration. Listen to more stories on the Noa app.What\u2019s the matter with the United Kingdom? Great Britain is the birthplace of the Industrial Revolution, which ushered in an era of energy super-production and launched an epoch of productivity advancements that made many life essentials, such as clothes and food, more affordable. Today, the country suffers from the converse of these achievements: a profound energy shortage and a deep affordability crisis. In February, the Bank of England reported an ongoing productivity slump so mysterious that its own economists \u201ccannot account fully\u201d for it. Real wages have barely grown for 16 years. British politics seems stuck in a cycle of disappointment followed by dramatic promises of growth, followed by yet more disappointment.A new report, titled \u201cFoundations,\u201d captures the country\u2019s economic malaise i"
  },
  {
    "title": "Show HN: Knowledge graph of restaurants and chefs, built using LLMs (theophilecantelob.re)",
    "points": 143,
    "submitter": "theophilec",
    "submit_time": "2025-03-03T15:43:20 1741016600",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=43242818",
    "comments": [
      "The embedding is kind of weird. Like, there's no reason a \"degree: 1\" node should be so far away from its sibling.Example: https://imgur.com/a/7CktyzpThis makes the graph look more random/noisy/disorganized than it actually is.\n \nreply",
      "Since you did the hard work of parsing rich metadata already, it would be even cooler if your network visualization oriented nodes by some of this information. Here the 'hiveplot' idea (https://hiveplot.com/ ) is often even more useful than e.g. springloaded or UMAP based layouts; clustering into semantically-meaningful categories into axes (say, city or arrondissement? years open? cuisine? an explicit phylogeny from oldest culinary grandparents to youngest?) then choosing a coordinate to localize nodes on the axes (total node degree? prix? \"les plus\" tags?...) automatically compels us think about salient features of the data.\n \nreply",
      "I agree the spatialization could be better. I used one of the algorithms in Gephi-lite directly. Do you have a favorite spatialization algorithm to recommend?\n \nreply",
      "Yeah, they should have used UMAP or tSNE to cluster the data a bit\n \nreply",
      "This is a super cool idea! I've sort of mused about an idea for general web search that's very similar to this concept, where you start with a set of trusted entities and then branch out from there, but choosing how you establish trust is really important. But this is a really clever application, well done!\n \nreply",
      "so you mean how academics search for relevant publications on a topic, which is the direct inspiration for the original Google ranking algorithm, back in the happy days when the web was young and had not optimized itself around short-term money making.\n \nreply",
      "Very cool work.It's worth mentionion that the Graph browser using \"Retina\" is a project from Ouestware (https://www.ouestware.com/en/) which is also contributor to the GraphCommons and GephiLite projects.\n \nreply",
      "I was clicking around with the embed, and eventually hit the \"home/house\" icon. That takes me to a Retnia credit/loading screen with no way to back out of it that I could see. Was forced to hard refresh the page. If there is a close button, it could be made more obvious\n \nreply",
      "Thank you for the kind words!Yes, Retina and Gephi are great. In fact I noticed a bug which they fixed immediatly while making the project.\n \nreply",
      "Given the structured nature of the data, how does this compare to running a specialized classification model that looks for specific words in a review and uses those to assign Chefs to Restaurants? With some fine tuning, you might get more consistent results than feeding the reviews into a generative model.\n \nreply"
    ],
    "link": "https://theophilecantelob.re/blog/2025/foudinge/",
    "first_paragraph": " February 27, 2025    2025  \u00a0 \u00b7 \u00a0   llm \u00a0   food \u00a0 This post has been shared by .txt on Linkedin and Bluesky.For French restaurant intel, lefooding.com is the definitive data source. Their anonymous critics conduct systematic reviews of establishments across France (and now Belgium), documenting their findings in a witty - if peculiar - style. Beyond choosing the place for a delicious night out, they can be used to map and understand France\u2019s culinary network.A network is composed of nodes and edges. Nodes represent entities, such as people or restaurants. Edges represent the relationships between people and restaurants, or absence thereof. For the French restaurant scene, nodes represent both people and restaurants. A person node is connected to a restaurant if that person is known to have worked at the restaurant.Restaurants with very many neighbor nodes (nodes connected to it) are restaurants whose alumni go on to create and work in other prestigious restaurants. This is the case of"
  },
  {
    "title": "Ask HN: What less-popular systems programming language are you using?",
    "points": 56,
    "submitter": "fuzztester",
    "submit_time": "2025-03-01T20:11:15 1740859875",
    "num_comments": 70,
    "comments_url": "https://news.ycombinator.com/item?id=43223162",
    "comments": [
      "Loving Ada without using exceptions or inheritance on embedded and desktop. Some love Ada full OOP tagged types. I love Ada procedural style with privacy and abstract data types. I wish Flutter was written in Ada but atleast Dart is better than JavaScript atleast for procedural code without it's oop boiler plate. You don't actually need OOP for widgets.\n \nreply",
      "C#. While a popular language, it is criminally overlooked for high-performance programming. Obviously, you can't use it for embedded or kernel development. For other use cases though, it can almost reach the performance of C/C++/Rust when written with proper care.\n \nreply",
      "I sometimes write C# in my day job. But I think I don't know much about how to write really fast C#. Do you have any recommendations for learning resources on that topic?\n \nreply",
      "Sure. Here are some resources:* Span<T>: https://learn.microsoft.com/en-us/archive/msdn-magazine/2018...* C# now has a limited borrow checker-like mechanism to safely handle local references: https://em-tg.github.io/csborrow/* Here is a series of articles on the topic: https://www.stevejgordon.co.uk/writing-high-performance-csha...* In general, avoid enterprise style C# (ie., lots of class and design patterns) and features like LINQ which allocate a lot of temporaries.\n \nreply",
      "Thank you. I once read a bit about Span<T>, but some of this reference stuff is very new to me. Interesting, definitely. C# really is a big language nowadays...\n \nreply",
      "Spans are just a slice type, but those which any type based on contiguous memory can be coerced to (usually). I\u2019m sure you\u2019re already using them somewhere without realizing that. Their main use case in regular code is zero-cost slicing e.g. text.AsSpan(2..8).\n \nreply",
      "C# is specifically designed for enterprise-style OOP, so if you want to avoid that, why use C# at all?\n \nreply",
      "> C# is specifically designed for enterprise-style OOPThen why would they add Span<T>, SIMD types and overhaul ref types in the first place?\n \nreply",
      "Because some people wanted to use C# for low-level programming, so they added these things as an afterthought.\n \nreply",
      "You\u2019ve clearly never used it and have no idea what you are talking about.\n \nreply"
    ],
    "link": "item?id=43223162",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: FlakeUI (github.com/tearflake)",
    "points": 108,
    "submitter": "tearflake",
    "submit_time": "2025-03-03T05:29:02 1740979742",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=43238570",
    "comments": [
      "It reminds me of the glory days when \u201chypertext\u201d was a term uttered with a straight face to great stroking of beards\u2014HyperCard, exercises in nonlinear narrative, VRML-based \u201cnavigation,\u201d Apple eWorld [0] and the like.> Would you like to bring a touch of adventurous spirit to your contents?I personally would not, but I\u2019m really glad people more adventurous than I are still exploring the periphery of UI design![0] https://www.macworld.com/article/223467/remembering-eworld-a...\n \nreply",
      "Or that Apple space based file system back in the 80's.Try this a bit, it would be nice to be able to go directly to the grand-child, instead having to bring up the parent before going the child. Other wise can be a much better file naviation system then what we have. Especially on touch screen I would image.\n \nreply",
      "This reminds me of the time a few years ago when mind mapping sites and apps exploded into popularity among the... \"technorati\" and sort of slightly seep into the wider online awareness but then seemingly, just as quickly, disappear into the background noise of the internet (I'm terminally online to a degree, especially when it comes to tech news\u2014and have a pretty decent general awareness of pop culture trends\u2014and can't recall having seen the topic referenced since the trend faded. But perhaps I'm just not in the right circles?)\n \nreply",
      "People mistake a helpful \"view\" for a useful UI.None of these mind map, zoom first interfaces actually help with creating a global understanding.People take an occasionally helpful \"view\" for navigating items and then mistakenly believe it should be turned into an active interface for creation and editing.Graph/Mindmap views should only ever be a view and maybe a linking layer for nested text lists, actively operating in these interfaces is worse for global understanding and systems thinking.I suspect this is because mind maps don't actually map to how our brain stores information.Visual programming and even tools like KNIME work for stepwise workflow creation but they are not a good UI for new thinking, it's too much UI for novel idea generation and brainstorming, these interfaces are also useful for quickly understanding a DB structure.That's why they never take off and remain a niche tool for the small number of people who have brain structures that find them useful or are willing to bend themselves to an arbitrary interface.\n \nreply",
      "I'd like to suggest adding support for clicking and tapping for navigation. Having to drag feels unintuitive.\n \nreply",
      "Thank you for the comment. I would not have understood \"can be navigated using mouse\" to mean \"dragging\".Also I hate that I can't select text on this. Probably because \"dragging\".\n \nreply",
      "Exactly, clicking should be the default so the drag handler doesn't prevent users from highlighting text - I literally cannot read anymore without frantically double-clicking/dragging on the words in the text\n \nreply",
      "Reminds me of prezi[0]. It would be great if there is an open source version of prezi similar to reveal js.[0] https://prezi.com/p/p6evz0gdy5dr/ux-design-tips-for-product-...\n \nreply",
      "There's a significant performance issue. There's no good reason for a few ovals and texts to stutter on my system. May be worth investigating.\n \nreply",
      "May I ask, what machine you are running it on? On my Celeron (4GB RAM), things are OK-ish.\n \nreply"
    ],
    "link": "https://github.com/tearflake/flake-ui",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        fractal-structure inspired, parent-children orbiting, zooming-elements based graph visualization user interface\n      tags: user-interface, graph-visualization, zooming-elements-based, parent-children-orbiting, fractal-structure-inspiredWould you like to bring a touch of adventurous spirit to your contents? Presenting your contents, FlakeUI does things a bit differently. As an original graph content navigating environment, it provides unusual experience in discovering information among your content selection. Possible applications are surely endless, and they are waiting to be discovered by that awesome child of creativity in you.FlakeUI is a fractal-structure inspired, parent-child orbiting, and zooming-elements based graph user interface. Javascript based FlakeUI is best used in HTML pages where visitors often return, like in a cu"
  },
  {
    "title": "Chrome Returns 206 when the Server Returns 403 (aoli.al)",
    "points": 91,
    "submitter": "aoli-al",
    "submit_time": "2025-03-03T18:00:57 1741024857",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=43244680",
    "comments": [
      "> The Netlog looks scary because it not only contains the traffic while I reproduced the bug but also 1) all traffic from the Chrome plugins and 2) many websites that I have browsed before but haven\u2019t visited during the recordingIsn't that a fantastic use for a 2nd Chrome Profile or even just downloading a Chromium build[1] and using that, showing the behavior in a bleeding edge build?1: https://download-chromium.appspot.com/\n \nreply",
      "I was also pretty surprised when the OP said \"the Chromium team refused to use my server to reproduce the bug\", when the actual comments of the ticket were \"clone this repo and run my giant node app\" and the tester's response was \"It seems a bit difficult to set up an build environment to run the static server, could you provide a more minimal repro case?\". OP's description of the tester's reasonable concerns seems very unfair.Even just having a web-accessible endpoint that reproduced the issue would have made the process a lot smoother I think. Apparently in response to OP's request for an easier test case, OP asked for GCP cloud credits(?) to host their server with?. You probably used more bandwidth & CPU loading the new Chromium issue tracker page then you would have just setting up a simple vps to reproduce the issue\n \nreply",
      "No,(1) I'm not setting up your server to repo the issue. I have no idea what all that code is going to do. Is it going to try to pown my machine?(2) No, I'm not going to use your server as a repo. I have no idea you aren't updating it every 5 minutes with a new version.There's a reason developers ask for an MCVE (Minimal complete verifiable example)https://www.google.com/search?q=MCVEIt's not unreasonable to ask for one. Sorry if that sucks for you because it's difficult for you pair down your code but that's where we are. Try it on the other side and you'll start to get it.\n \nreply",
      "Yeah that's a bad repro. Took me a couple minutes to write a concise one.server.go:  package main\n  \n  import (\n      \"net/http\"\n  )\n  \n  func main() {\n      http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n          w.Header().Set(\"Content-Type\", \"text/html; charset=utf-8\")\n          w.Write([]byte(`\n          <script>\n          (async () => {\n              await fetch(\"/test\", { headers: { range: \"bytes=0-4\" } }).then(resp => console.log('bytes=0-4', resp.status));\n              await fetch(\"/test\", { headers: { range: \"bytes=0-10\" } }).then(resp => console.log('bytes=0-10', resp.status));\n          })();\n          </script>\n          `))\n      })\n      http.HandleFunc(\"/test\", func(w http.ResponseWriter, r *http.Request) {\n          w.Header().Set(\"Access-Control-Allow-Origin\", \"*\")\n          w.Header().Set(\"Content-Type\", \"text/plain; charset=utf-8\")\n          switch r.Header.Get(\"Range\") {\n          case \"bytes=0-4\":\n              w.Header().Set(\"Content-Type\", \"text/plain; charset=utf-8\")\n              w.Header().Set(\"Content-Range\", \"bytes 0-4/1000000\")\n              w.Header().Set(\"Last-Modified\", \"Mon, 03 Mar 2025 00:00:00 GMT\")\n              w.Header().Set(\"Etag\", \"1234567890\")\n              w.WriteHeader(http.StatusPartialContent)\n              w.Write([]byte(\"01234\"))\n          case \"bytes=0-10\":\n              w.WriteHeader(http.StatusForbidden)\n              w.Write([]byte(\"Forbidden\"))\n          default:\n              w.WriteHeader(http.StatusBadRequest)\n              w.Write([]byte(\"Bad Request\"))\n          }\n      })\n      http.ListenAndServe(\":8080\", nil)\n  }\n\n`go run server.go` and open up http://localhost:8080 in the browser. For Chrome, in the console one should see  bytes=0-4 206\n  bytes=0-10 206\n\nbut if we use \"disable cache\" this becomes  bytes=0-4 206\n  GET http://localhost:8080/test 403 (Forbidden)\n  bytes=0-10 403\n\nIn both Safari and Firefox the second request is 403, cache or not.Now, is this surprising? Yes. Does this violate any spec? I'll take Chromium dev's word [1] and say likely not. Should it be \"fixed\"? Hard to say, but I agree that \"fixing\" it could break existing things.[1] https://issues.chromium.org/issues/390229583#comment16\n \nreply",
      "So if you get less HTTP bytes than expected, then it\u2019s a HTTP response error and you throw the whole thing away. For example, this sort of situation happens when streaming HTTP. The server first has to send the response headers, which would be a simple 200/206, then the data, which could have a much more complicated code path. If there is an error in that data code path, all you can do is close the connection and trigger an HTTP error since less bytes were delivered than advertised. Client needs to detect this and retry. While this may seem uncommon, this is well understood behavior for HTTP systems.\n \nreply",
      "Chrome's cache is indeed acting correctly. Effectively, it is acting as an intermediary here - your application made a partial content request, and it can satisfy it (partially), so it sends you a 206.HTTP partial content responses need to be evaluated (like any other response) according to their metadata: servers are not required to send you exactly the ranges you request, so you need to pay attention to Content-Range and process accordingly (potentially issuing more requests).See:\n  https://httpwg.org/specs/rfc9110.html#status.206\n \nreply",
      "But the Content-Range header and the Content-Length header both indicated the \"expected\" number of bytes e.g. the number of bytes that would have been returned if the server had given a 206 or a 200, not the truncated number of bytes that the response actually contained. Is that expected?The latest response from the Chromium team (https://issues.chromium.org/issues/390229583#comment20) seems to take a different approach from your comment, and says that you should think of it as a streaming response where the connection failed partway through, which feels reasonable to me, except for the fact that `await`ing the response doesn't seem to trigger any errors: https://issues.chromium.org/issues/390229583#comment21\n \nreply",
      "Shouldn't the response header returned by Chrome say \"4-138724\" then though, and not \"4-1943507\"? The synthesized response body doesn't include bytes \"138725-1943507\".\n \nreply",
      "Ah - I need to remember to coffee before posting in the AM.Yes, the mismatch between the response headers and the content is a problem. Unfortunately, IME browsers often do \"fix ups\" of headers that make them less than reliable, this might be one of them -- it's effectively rewriting the response but failing to update all of the metadata.The bug summary says \"Chrome returns wrong status code while using range header with caches.\" That's indeed not a bug. I think the most concerning thing here is that the Content-Range header is obviously incorrect, so Chrome should either be updating it or producing a clear error to alert you -- which it looks like the Chrome dev acknowledges when they say \"it is probably a bug that there is no AbortError exception on the read\".I might try to add some tests for this to https://cache-tests.fyi/#partial\n \nreply",
      "This is super weird and needs a bit of editing but it seems like an actual bug. Shouldn\u2019t a 403 invalidate whatever was cached?As in it should bubble the error up to the user.\n \nreply"
    ],
    "link": "https://aoli.al/blogs/chrome-bug/",
    "first_paragraph": ""
  }
]