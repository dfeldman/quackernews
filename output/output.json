[
  {
    "title": "Show HN: Jmail \u2013 Google Suite for Epstein files (jmail.world)",
    "points": 249,
    "submitter": "lukeigel",
    "submit_time": "2025-12-20T21:00:05 1766264405",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=46339600",
    "comments": [
      "I'm impressed. You guys cloned a whole suite of products in a short period of time that cost millions of dollars. Even the little bits of humor look costly.On the other hand, it's way more information than I expected. I can see why someone would hesitate to release them - there's a lot to sift through and it's likely even the government couldn't sift through all of them to make sure their friends weren't mentioned somewhere.reply",
      "> You guys cloned a whole suite of products in a short period of time that cost millions of dollars.At the risk of stating the obvious, the functionality isn't actually cloned, only the UI. The actual code powering Gmail probably dates back to the late 80s or early 90s and has had several hundred thousands of hours of work put into it. This is just a webpage that looks kind of similar.I point this out only because I've seen people saying that software businesses don't have moats anymore because of this, which is taking away a completely false lesson.reply",
      "Out of curiosity, would you explain what you mean by that? Google was founded in 1998 and writing a mail client isn't terribly complicated. Did they buy some code for Gmail from an older company? Is Gmail older than Google?reply",
      "A full featured mailed client is insanely complicated. If you think mail client is just smtp, you probably think word is just text with some styling and excel is just some cells and functions.reply",
      "I\u2019m sure, buried somewhere deep in Google systems, are vestiges of mail server code originally written in the 80s. But when people use the name Gmail, they are generally referring to the client facing web app, which does not have any such code.reply",
      "it is not. gmail is 100% from paul bucheit.reply",
      "Gmail is not just a mail client.reply",
      "In 10 days now, with Claude Code Web, I now have feature parity+++ with several (over)paid bussiness administration suites in a niche in my country. I am planning to make it free and not decided yet whether to open source it.I think we're going to see a lot of free software coming out, especially the more CRUD like software. Single developers can compete with software with a large codebase and legacy. Of course in case of bussiness administration you want knowledgeable support and be sure the data is safe. That's worth a lot. But I have another business model in mind where I can also offer state of the art support, maintenance and data security.My software is basically SaaS  with a companion Flutter app. Security and privacy are my number one priority and by using the fabulous SKILLS.MD my privacy compliance skill helps a lot and my security skill seems to be on par with other tools I tried to verify it with and it continues to learn.Advantage of Claude Code,it will fix it immediately when P1/CRITICAL and for the rest I only have to type: yes please ;-)In one day, I created an Android app in Kotlin (never wrote a single line in Kotlin) that is completely tailored to my son,he's a toddler of almost 3.It has a start screen with games that relate to events in his life we experienced together like feeding the dog (throw bones to a dog that looks enough like ours so he understands), a whack-a-mole game where gnomes appear on red with white-dot mushrooms (he discovered them in a nearby forest) and of course things like firefighting, police,excavator. Because it's so easy I've now left him with 50 mini games (yes, feature creep like behavior). Some are educational (learning colors, counting but toddler difficulty) or fun to do, like magic paint that shows colored particles as a trail following his finger or adding color to random shapes.TTS tell him what to do, encourages him, saying his name, telling him he did well when he completes a game.He gets random videos of family members cheering when he reaches a set score.Cheering videos are also AI made. Did that with the photo to video feature of Grok. I had 15 mini-games on day one, now 50 mini-games after a week. But entire time spent with supervising Claude is about 4 hours.And, I should not forget to mention, all from the browser of my mobile phone and very often in bed with Claude  working while I sleep (previously also providing new prompts at night, when Claude would reset its limits).But I enjoy it so much and possibly I can make it a bussiness (although I worry someone else can relatively easy get feature parity with me)that I decided to get the Max 20x plan, and prompting 4 projects with each 2 to 3 running 'conversations' , never hit the limit anymore.reply",
      ">I decided to get the Max 20x plan, and prompting 4 projects with each 2 to 3 running 'conversations' , never hit the limit anymore.Can you expand on this please? Really cool btw.reply",
      "I don't know if I'm just misremembering but it feels like over the last three years or so the technical knowledge on HN has gone down the toilet.reply"
    ],
    "link": "https://www.jmail.world",
    "first_paragraph": ""
  },
  {
    "title": "Backing Up Spotify (annas-archive.li)",
    "points": 798,
    "submitter": "vitplister",
    "submit_time": "2025-12-20T18:28:41 1766255321",
    "num_comments": 283,
    "comments_url": "https://news.ycombinator.com/item?id=46338339",
    "comments": [
      "This is insane.I definitely was not aware Spotify DRM had been cracked to enable downloading at scale like this.The thing is, this doesn't even seem particularly useful for average consumers/listeners, since Spotify itself is so convenient, and trying to locate individual tracks in massive torrent files of presumably 10,000's of tracks each sounds horrible.But this does seem like it will be a godsend for researchers working on things like music classification and generation. The only thing is, you can't really publicly admit exactly what dataset you trained/tested on...?Definitely wondering if this was in response to desire from AI researchers/companies who wanted this stuff. Or if the major record labels already license their entire catalogs for training purposes cheaply enough, so this really is just solely intended as a preservation effort?reply",
      "> The thing is, this doesn't even seem particularly useful for average consumers/listeners, since Spotify itself is so convenient, and trying to locate individual tracks in massive torrent files of presumably 10,000's of tracks each sounds horrible.I wouldn\u2019t be so sure. There are already tools to automatically locate and stream pirated TV and movie content automatic and on demand. They\u2019re so common that I had non-technical family members bragging at Thanksgiving about how they bought at box at their local Best Buy that has an app which plays any movie or TV show they want on demand without paying anything. They didn\u2019t understand what was happening, but they said it worked great.> Definitely wondering if this was in response to desire from AI researchers/companies who wanted this stuff.The Anna\u2019s archive group is ideologically motivated. They\u2019re definitely not doing this for AI companies.reply",
      "> The Anna\u2019s archive group is ideologically motivated. They\u2019re definitely not doing this for AI companies.They have a page directly addressed to AI companies, offering them \"enterprise-level\" access to their complete archives in exchange for tens of thousands of dollars. AI may not be their original/primary motivation but they are evidently on board with facilitating AI labs piracy-maxxing.reply",
      "You go where the money is. Infra isn\u2019t free. Churches pass the plate every Sunday. Perhaps one day we\u2019ll exist in a more optimal socioeconomic system; until then, you do what you have to do to accomplish your goals (in this context, archivists and digital preservation).reply",
      "> Infra isn\u2019t free.There is a certain irony in people providing copyrighted works for free justifying profiting from these copyrights on the basis that providing the works to others isn\u2019t free.reply",
      "I'd have a lot more sympathy if the music industry didn't try all of the worst available options to handle piracy for years and years.They had many opportunities to get out ahead of it, and they squandered it trying to cling to album sales where 11/13 tracks were trash. They are in a bed of their own making.reply",
      "You have been able to buy DRM free digital music from all of the record labels since 2009 from Apple and other stores.reply",
      "> DRM free digital music from all of the record labelsIs this true?  Can you show me where I can get DRM-free releases from Mountain Fever?Better yet, can you add that information here?  https://pickipedia.xyz/wiki/DRM-freereply",
      "The \"iTunes going DRM free\" was a big deal around 2008.https://web.archive.org/web/20070207234839/http://www.apple....https://www.theguardian.com/technology/2008/may/15/drm.apple",
      "Your link doesn\u2019t work.  But I assume you are talking about this label? I looked at the first artist and I found the artist\u2019s music on iTunes.  Everything that Apple sells on the iTunes Music Store has been DRM free AAC or ALAC (Apple lossless) since 2009.https://mountainfever.com/colin-kathleen-ray/While ALAC is an Apple proprietary format, it is DRM free and can be converted to FLAC using ffmeg. AAC is not an Apple formatreply"
    ],
    "link": "https://annas-archive.li/blog/backing-up-spotify.html",
    "first_paragraph": "\n    annas-archive.li/blog, 2025-12-20, Discuss on Hacker News\nWe backed up Spotify (metadata and music files). It\u2019s distributed in bulk torrents (~300TB), grouped by popularity.This release includes the largest publicly available music metadata database with 256 million tracks and 186 million unique ISRCs.It\u2019s the world\u2019s first \u201cpreservation archive\u201d for music which is fully open (meaning it can easily be mirrored by anyone with enough disk space), with 86 million music files, representing around 99.6% of listens.Anna\u2019s Archive normally focuses on text (e.g. books and papers). We explained in \u201cThe critical window of shadow libraries\u201d that we do this because text has the highest information density. But our mission (preserving humanity\u2019s knowledge and culture) doesn\u2019t distinguish among media types. Sometimes an opportunity comes along outside of text. This is such a case.A while ago, we discovered a way to scrape Spotify at scale. We saw a role for us here to build a music archive prim"
  },
  {
    "title": "Ireland\u2019s Diarmuid Early wins world Microsoft Excel title (bbc.com)",
    "points": 177,
    "submitter": "1659447091",
    "submit_time": "2025-12-20T19:56:47 1766260607",
    "num_comments": 61,
    "comments_url": "https://news.ycombinator.com/item?id=46339031",
    "comments": [
      "Watch this, where he goes through the questions and explains how he did it: https://www.youtube.com/watch?v=1Mx_xhiIRpwI'm pretty good with Excel, my main tool at the job for over 20 years. I understand how he did it, but it's just really humbling...I still think quality of what you do with Excel (idea) is more important than how you do it (skill).reply",
      "My best Excel trick, which reveals how little I know, and yet Early [0] doesn't use it (or maybe doesn't need it, but that's hard to believe):1. You can drag down the bottom of the formula bar/field and make it multi-line2. You can insert arbitrary[*] newlines in an Excel formulaCombining those, you can turn the absurd default format of single-line-of-code functions into something readable and manageable. Here's a simple one from a spreadsheet I have open:  =INDEX(\n  $C$17:$S$24,\n  MATCH(A6,$A$17:$A$24,0),\n  MATCH(C6,$C$15:$S$15,0)\n  )\n\nAnd just think of highly nested functions. Once you know it, writing single-line functions of any complexity is absurd, as absurd as writing 'real' code that way.[0] Early shows how it was done: https://news.ycombinator.com/item?id=46340638[*] I think you can do it anywhere but I haven't tested anything crazy; mostly I just use them between expressions.reply",
      "I could do half-screen nested array formulas when Excel was before the ribbon (and screen resolutions were smaller), out of necessity and because I could. It was in quite demanding uni home calculations and then mostly when working as intern in IB. But then having a life is also important...The only thing I still enjoy is that any data smaller than 1M rows is sliced and diced almost without thinking. I am sometimes really grateful that MS did not break the shortcuts, while almost breaking the product overall. The muscle memory works perfectly.reply",
      "It's interesting that the challenges are not business or accounting centred, as is the expectation when using Excel. If this is now general problem solving, are we watching language-specific competitive programming through the lens of a more broadly accessible platform like MS Excel?I enjoy the idea, and love watching it grow.reply",
      "It used to be financial modeling but they realized they\u2019d get more attention with the esports audience this way.It\u2019s gone quite far now - one of the many challenges was a mock terrain map where you\u2019d calculate distances to hike while considering the weight of your pack. Even the way they walk through the tunnel is done for show.reply",
      "Excel is a general purpose computing environment and has been for quite some time.When I was in the air force we had a complete aircraft maintenance planning and performance management system entirely in Excel.  It can connect to remote workbooks on a shared drive/SharePoint too, so the higher headquarters would tie into our dashboard for their own operational readiness tracking.It was a total shit show of undocumented pseudo APIs with zero change management or version control but it worked somehow.reply",
      "There is also the mocumentary flick of the Excel eTournament scene with \"Makro\"https://www.youtube.com/watch?v=xubbVvKbUfYreply",
      "I\u2019ll always think of my fellow excel wizards as sheet heads thanks to this videoreply",
      "Don\u2019t forget part twohttps://youtu.be/ICp2-EUKQAIreply",
      "Life imitates art.reply"
    ],
    "link": "https://www.bbc.com/news/articles/cj4qzgvxxgvo",
    "first_paragraph": "Irishman Diarmuid Early emerged onto the HyperX arena in Las Vegas under the bright lights, dramatic music, and fanfare worthy of any major sporting final.Only this time, instead of a ball there was a keyboard and mouse, and the playing field a humble desktop setup.Dubbed the \"LeBron James of Excel spreadsheets\", Galway born and Waterford raised Diarmuid is now the world's best worksheet whizz.He won the 2025 Microsoft Excel World Championships, where a $60,000 (\u00a345,726) prize pot has propelled the computer program from the office into a high stakes spectacle.The esport showpiece in December attracted competitors worldwide as 256 spreadsheet heads battled it out across knockout rounds to join the final 24 in Vegas.It may sound bizarre, but Diarmuid described how the intense atmosphere of the finals lives up to the buzz.\"Most of the time you're playing at home by yourself and it's pretty calm and low-key,\" he said.\"But when you get to Vegas, it's just outrageous. You just hear everybody"
  },
  {
    "title": "Claude in Chrome (claude.com)",
    "points": 87,
    "submitter": "ianrahman",
    "submit_time": "2025-12-20T21:26:14 1766265974",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=46339777",
    "comments": [
      "Let's spend years plugging holes in V8, splitting browser components to separate processes and improving sandboxing and then just plug in LLM with debugging enabled into Chrome. Great idea. Last time we had such a great idea it was lead in gasoline.reply",
      "It's clear the endgame is to cook AI into Chrome itself. Get ready for some big antitrust lawsuit that settles in 20 years when Gemini is bundled too conveniently and all the other players complain.https://developer.chrome.com/docs/ai/built-in-apisreply",
      "The cycle must not be broken https://xkcd.com/2044/reply",
      "Innovation in the short term might trump longer term security concerns.All of these have big warning labels like it's alpha software (ie, this isn't for your mom to use). The security model will come later... or maybe it will never  be fully solved.reply",
      "> this isn't for your mom to usemany don\u2019t realize they are the momreply",
      "You can be the papa, I can be the mom (oh oooh)reply",
      "My theory that you'll need a dedicated machine to access the internet is more true by the day.reply",
      "After Claude Code couldn't find the relevant operation neither in CLI nor the public API, it went through its Chrome integration to open up the app in Chrome.It grabbed my access tokens from cookies and curl into the app's private API for their UI. What an amazing time to be alive, can't wait for the future!reply",
      "That's fantasticreply",
      "Good to see. Google only has this feature in experimental mode for $125/month subscribers: https://labs.google.com/mariner/landingGoogle allows AI browser automation through Gemini CLI as well, but it's not interactive and doesn't have ready access to the main browser profile.reply"
    ],
    "link": "https://claude.com/chrome",
    "first_paragraph": ""
  },
  {
    "title": "Pure Silicon Demo Coding: No CPU, No Memory, Just 4k Gates (a1k0n.net)",
    "points": 278,
    "submitter": "a1k0n",
    "submit_time": "2025-12-20T16:45:48 1766249148",
    "num_comments": 43,
    "comments_url": "https://news.ycombinator.com/item?id=46337438",
    "comments": [
      "I was curious about the long-term stability of the cited HAKMEM sin/cos generator. I found an overview here: https://news.ycombinator.com/item?id=3111501 (EDIT: I'm still not sure about stability, apparently it is stable in exact arithmetic under certain conditions.) Coincidentally it is related to the Verlet integration video I posted last week: https://news.ycombinator.com/item?id=46253592reply",
      "Yeah, it is exact in this specific circumstance. But yes, it's exactly the same trick; I also enjoyed that video in my Youtube recommender feed last week!reply",
      "As a computer science guy who interlops  in computer engineering i really want to find time to build something cool like this and tapeout. The retro architectures for rendering are simple but fun! I love the projectreply",
      "I recommend getting started like the author did: simulation first, then FPGA. Honestly FPGA will take you very far. I always get a kick out of being able to design my own SoC. \"Hmmm I need 9 separate I2C ports... Ok, copy block, paste paste paste...\" Or if you have an operation in software that's taking forever you can write an accelerator for itreply",
      "What are the best modern tools to get started with in simulation for those who have never dabbled before?reply",
      "The other commentator mentioned Verilator (which is indispensable in larger designs) but you may also want to grab Icarus Verilog too. It's a FOSS simulator and, unlike Verilator, is 2-bit and so it handles X (\"don't care\") and Z (\"high impedance\") signals. It's ridiculously slow compared to Verilator but the greater fidelity can be valuable depending on what you're trying to do.reply",
      "Verilator is very good. It's faster than anything else, and it is free. The downsides are it won't stimulate encrypted IP blocks. And it doesn't do mixed language sim, so vhdl is no bueno.reply",
      "It\u2019s amazing and wonderful to see the Internet support these tiny cliques of interest. Having everybody connected leads to homogenization of culture in some ways, but it also supports these couple dozen (?) people around the world finding each other for this amazing little competition.reply",
      "Having everybody connected leads to homogenization of culture in some ways\n\nThe internet may hypothetically homogenize culture relative to a society that does not have any kind of mass communication at all, but relative to the world it was actually introduced into, the internet has completely balkanised the culture. Prior to the internet, we had television, cinema, literature, radio, and newspapers, which were all centralised and controlled enough that they created a shared monoculture in nations. A signifant portion of a country's population would watch, read, and listen to the same media. The internet bucked that trend, allowing all kinds of new subcultures to pop up and to more easily cross national boundaries.reply",
      "Yeah, back in the day you would go to school the next day after a show that everyone watches released its new episode, it aired on the prime-time slot on the primary TV channel, and you'd discuss what happened in that episode, or have some references or new jokes. Created a common culture.reply"
    ],
    "link": "https://www.a1k0n.net/2025/12/19/tiny-tapeout-demo.html",
    "first_paragraph": "Dec 19, 2025In addition to the VGA\ndonut, I submitted\ntwo other entries in the Tiny Tapeout 8 demo\ncompetition,\nwhere you submit a \"tiny\" ASIC design (room for about 4000 logic gates)\nthat outputs 2-bit RGB to a VGA port and 1-bit audio to a speaker. One was a\nan old school C64/Amiga intro type of thing, and the other was a\nnyan cat. video unavailable\nClick to play -- video loops imperfectly but the hardware loop is seamlessClick to play -- video loops imperfectly but the hardware loop is seamlessThe intro features a background starfield, a panning 3D checkerboard with\ncolorful tiles, and wavy scrolling text which casts a shadow onto the\ncheckerboard.  I was inspired by lft's first AVR demo\nCraft to add a little cyan\noscilloscope to the side of the screen as well.The size limit was two Tiny Tapeout \"tiles\" which is not a lot of space for\nmusic, effects, and data; this is a very constrained platform for a demo: you\nget no ROM, no RAM, and every bit of state is a flipflop which takes up "
  },
  {
    "title": "Anatomy of US inequality (nber.org)",
    "points": 36,
    "submitter": "hhs",
    "submit_time": "2025-12-20T23:43:04 1766274184",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=46340794",
    "comments": [
      "I'm struggling to interpret this study. They seem to attribute the majority (~96%) of inequality to non-ethnic, non-racial factors.So the inequality of income of poor white people and rich white people has the same causes as for poor vs. rich black people.Am I understanding it correctly?reply",
      "aint no war but class war, correct. https://en.wikipedia.org/wiki/Pyramid_of_Capitalist_Systemjust realize that poster is more than a century old and it will probably still be being explained a hundred years from now.reply",
      "That's as may be, but still, remember what Lyndon B Johnson said about controlling the lower classes.Something about convincing the lowest white person that they're better than the best black guy.I don't see that ploy failing to work in the US for a long, long time. So maybe the elites will lose control and thus, the \"war\"? But I doubt it.reply",
      "Privilege breeds privilege. Once you have at least a little bit of it. You can generate a little bit more with it.The bar that you get to grow up in a relatively safe place, with non toxic friends or parents is quite high already. I haven\u2019t been lucky to have grown up in a safe place or in a good family, but I did manage to make good friends, and if I had not been lucky enough to have positive experiences with some real smart resourceful people, I probably wouldn\u2019t had been alive today. Not to sound too dramatic.Every time I visited the US it was mostly to visit people who were in a very privileged position. They either rented an expensive apartment in new york city in an ok neighborhood, or they owned a 3 story house in the suburbs. When we drove to walmart I could already see in the parking lots the stark differences between people who called this country home.Next to us parked a family, 3 kids, a car that looked like it was falling apart next to my friend\u2019s huge modern SUV. I wonder if any of them will ever br able to afford an SUV, I wonder how many of them will be able to go to college. The father and mother were yelling a lot at those poor kids. Everyone looked overweight(no offense), the father literally obese. And we went on with our day. I bought a bunch of random stuff I didn\u2019t really need and ate a lobster sandwich. And I kind of forgot for a long time that family ever existed, except for the couple of times I randomly remember them. Hope they\u2019re alright.reply",
      "I realize this isn't the meat of your post, but you ate a lobster sandwich at Walmart?I cannot help but think of https://youtu.be/Pj-D0jc17D0?si=BiEGWr9aacGdAkGWreply",
      "I spent a couple of years in the States before returning to Europe. I really had no idea what happened to people - what they became - if they weren't fed and watered properly. I had always assumed that the way people were was just how people are. Scary how much we take for granted.reply",
      "This is true when you look at raw numbers but you'll still get some pushback on this fact since the pace of USA's drop in median wealth has been so sudden. As in my home country of Australia now has ~2.5x the median wealth per capita compared to the USA. This never used to be the case at all. Australia still has a lower average wealth but this is just telling of the extreme inequality. The median and averages aren't meant to be this far apart.https://en.wikipedia.org/wiki/List_of_countries_by_wealth_pe...Sort by median and compare to average wealth to see the issue the USA has right now. The USA's median wealth in 2007 was $173,151 in inflation adjusted teams for some reference, it dropped in 2008 and never really recovered. It's now well below many other Western nations.reply",
      "Do you have data showing a drop in per capita median wealth?I found this nice visualisation: https://www.federalreserve.gov/releases/z1/dataviz/dfa/distr...But it\u2019s totals, and not inflation adjusted.reply",
      "https://dqydj.com/net-worth-by-year/ See \"Median Net Worth by Year\".It's inflation adjusted to 2022 though, not 2025 (hopefully at least that's still in ballpark though). There was a slight spike upwards for median wealth during the pandemic fwiw. This seems to have entirely corrected though.reply",
      "I think your historic trend data is out of date. The inflation-adjusted median wealth crossed the 2007 bar sometime between 2019 and 2022.See the \"CPI-Adjusted to 2022\" column\nhttps://dqydj.com/net-worth-by-year/reply"
    ],
    "link": "https://www.nber.org/papers/w34558",
    "first_paragraph": "\nIs income inequality in the United States primarily driven by disparities between ethnic groups or within them? The evidence reveals a striking pattern: 96% of U.S. income inequality arises from variation within groups sharing common ancestral origins, far overshadowing the comparatively small share attributable to differences between these groups. This pattern remains remarkably stable across time and regions.\n\nThe authors wish to thank Hans-Joachim Voth and David Weil for helpful discussions. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\nMARC\n\nRIS\n\nBibTe\u03a7\nDownload Citation DataIn addition to working papers, the NBER disseminates affiliates\u2019 latest findings through a range of free periodicals \u2014 the NBER\u00a0Reporter, the NBER Digest, the Bulletin on Health, and the Bulletin on Entrepreneurship\u00a0\u2014 as well as online conference reports, video lectures, and interviews.Contact Us1050 Massachusetts Ave"
  },
  {
    "title": "Log level 'error' should mean that something needs to be fixed (utcc.utoronto.ca)",
    "points": 303,
    "submitter": "todsacerdoti",
    "submit_time": "2025-12-17T12:08:21 1765973301",
    "num_comments": 198,
    "comments_url": "https://news.ycombinator.com/item?id=46301059",
    "comments": [
      "> When implementing logging, it's important to distinguish between an error from the perspective of an individual operation and an error from the perspective of the overall program or system. Individual operations may well experience errors that are not error level log events for the overall program. You could say that an operation error is anything that prevents an operation from completing successfully, while a program level error is something that prevents the program as a whole from working right.This is a nontrivial problem when using properly modularized code and libraries that perform logging. They can\u2019t tell whether their operational error is also a program-level error, which can depend on usage context, but they still want to log the operational error themselves, in order to provide the details that aren\u2019t accessible to higher-level code. This lower-level logging has to choose some status.Should only \u201ctop-level\u201d code ever log an error?  That can make it difficult to identify the low-level root causes of a top-level failure. It also can hamper modularization, because it means you can\u2019t repackage one program\u2019s high-level code as a library for use by other programs, without somehow factoring out the logging code again.reply",
      "This is why it\u2019s almost always wrong for library functions to log anything, even on \u201derrors\u201d. Pass the status up through return values or exceptions. As a library author you have no clue as how an application might use it. Multi threading, retry loops and expected failures will turn what\u2019s a significant event in one context into what\u2019s not even worthy of a debug log in another. No rule without exceptions of course, one valid case could be for example truly slow operations where progress reports are expected. Modern tracing telemetry with sampling can be another solution for the paranoid.reply",
      "Depending on the language and logging framework, debug/trace logging can be acceptable in a library.  But you have to be extra careful to make sure that it's ultimately a no-op.A common problem in Java is someone will drop a log that looks something like this `log.trace(\"Doing \" + foo + \" to \" + bar);`The problem is, especially in a hot loop, that throw away string concatenation can ultimately be a performance problem.  Especially if `foo` or `bar` have particularly expensive `toString` functions.The proper way to do something like this in java is either    log.trace(\"Doing $1 to $2\", foo, bar);\n\nor    if (log.traceEnabled()) {\n      log.trace(\"Doing \" + foo + \" to \" + bar);\n    }reply",
      "Ideally a logging library should at least not make it easy to make that kind of mistake.reply",
      "How about wrapping the log.trace param in a lambda and monkeypatching log.trace to take a function that returns a string, and of course pushing the conditional to the monkeypatched func.reply",
      "Then you still have the overhead of the log.trace function call and the lambda construction (which is not cheap because it has closure over the params being logged and is passed as a param to a function call, so probably gets allocated on the heap)reply",
      "That is why the popular `tracing` crate in Rust uses macros for logging instead of functions. If the log level is too low, it doesn't evaluate the body of the macroreply",
      "Does that mean the log level is a compilation parameter? Ideally, log levels shouldn't even be startup parameters, they should be changeable on the fly, at least for any server side code. Having to restart if bad enough, having to recompile to get debug logs would be an extraordinary nightmare (not only do you need to get your customers to reproduce the issue with debug logs, you actually have to ship them new binaries, which likely implies export controls and security validations etc).reply",
      "I don't know how rust does it, but my internal C++ framework has a global static array so that we can lookup the current log level quickly, and change it at runtime as needed.  It is very valuable to turn on specific debug logs at times, when someone has a problem and we want to know what some code is doingreply",
      "What you are proposing sounds like a nightmare to debug. The high level perspective of the operation is of course valuable for determining if an investigation is necessary, but the low level perspective in the library code is almost always where the relevant details are hiding. Not logging these details means you are in the dark about anything your abstractions are hiding from higher level code (which is usually a lot)reply"
    ],
    "link": "https://utcc.utoronto.ca/~cks/space/blog/programming/ErrorsShouldRequireFixing",
    "first_paragraph": " You're probably reading this page because you've attempted to\naccess some part of my blog (Wandering\nThoughts) or CSpace, the wiki thing it's\npart of. Unfortunately whatever you're using to do so has a HTTP\nUser-Agent header value that is too generic or otherwise excessively\nsuspicious. Unfortunately, as of early 2025 there's a plague of\nhigh volume crawlers (apparently in part to gather data for LLM\ntraining) that behave like this. To reduce the load on Wandering Thoughts I'm experimenting with\n(attempting to) block all of them, and you've run into this.  All HTTP User-Agent headers should clearly identify what they\nare, and for non-browser user agents, they should identify not just\nthe software involved but also who specifically is using that software.\nAn extremely generic value such as \"Go-http-client/1.1\"\nis not something that I consider acceptable any more. "
  },
  {
    "title": "Big GPUs don't need big PCs (jeffgeerling.com)",
    "points": 137,
    "submitter": "mikece",
    "submit_time": "2025-12-20T17:49:18 1766252958",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=46338016",
    "comments": [
      "I've been kicking this around in my head for a while. If I want to run LLMs locally, a decent GPU is really the only important thing. At that point, the question becomes, roughly, what is the cheapest computer to tack on the side of the GPU? Of course, that assumes that everything does in fact work; unlike OP I am barely in a position to understand eg. BAR problems, let alone try to fix them, so what I actually did was build a cheap-ish x86 box with a half-decent GPU and called it a day:) But it still is stuck in my brain: there must be a more efficient way to do this, especially if all you need is just enough computer to shuffle data to and from the GPU and serve that over a network connection.reply",
      "I run a crowd sourced website to collect data on the best and cheapest hardware setup for local LLM here: https://inferbench.com/Source code: https://github.com/BinSquare/inferbenchreply",
      "Cool site, I noticed the 3090 is on there twice.https://inferbench.com/gpu/NVIDIA%20GeForce%20RTX%203090https://inferbench.com/gpu/NVIDIA%20RTX%203090reply",
      "Oh nice catch, I'll fix that---Edit: Fixedreply",
      "Nice! Though for older hardware it would be nice if the price reflected the current second hand market (harder to get data for, I know). Eg. Nvidia RTX 3070 ranks as second best GPU in tok/s/$ even at the MSRP of $499. But you can get one for half that now.reply",
      "Great idea - I've added it by manually browsing ebay for that initial data.So it's just a static value in this hardware list: https://github.com/BinSquare/inferbench/blob/main/src/lib/ha...Let me know if you know of a better way, or contribute :Dreply",
      "We're not yet to the point where a single PCIe device will get you anything meaningful; IMO 128 GB of ram available to the GPU is essential.So while you don't need a ton of compute on the CPU you do need the ability address multiple PCIe lanes. A relatively low-spec AMD EPYC processor is fine if the motherboard exposes enough lanes.reply",
      "I'm holding out for someone to ship a gpu with dimm slots on it.reply",
      "Would that be worth anything, though? What about the overhead of clock cycles needed for loading from and storing to RAM? Might not amount to a net benefit for performance, and it could also potentially complicate heat management I bet.reply",
      "DDR5 is a couple of orders of magnitude slower than really good vram. That\u2019s one big reason.reply"
    ],
    "link": "https://www.jeffgeerling.com/blog/2025/big-gpus-dont-need-big-pcs",
    "first_paragraph": "Ever since I got AMD, Intel, and Nvidia graphics cards to run on a Raspberry Pi, I had a nagging question:What's the point?The Raspberry Pi only has 1 lane of PCIe Gen 3 bandwidth available for a connection to an eGPU. That's not much. Especially considering a modern desktop has at least one slot with 16 lanes of PCIe Gen 5 bandwidth. That's 8 GT/s versus 512 GT/s. Not a fair fight.But I wondered if bandwidth isn't everything, all the time.I wanted to put the question of utility to rest, by testing four things on a variety of GPUs, comparing performance on a Raspberry Pi 5 to a modern desktop PC:Yes, that's right, we're going beyond just one graphics card today. Thanks to Dolphin ICS, who I met at Supercomputing 25, I have a PCIe Gen 4 external switch and 3-slot backplane, so I can easily run two cards at the same time:The tl;dr: The Pi can hold its own in many cases\u2014it even wins on efficiency (often by a large margin) if you're okay with sacrificing just 2-5% of peak performance!The c"
  },
  {
    "title": "Go ahead, self-host Postgres (pierce.dev)",
    "points": 422,
    "submitter": "pavel_lishin",
    "submit_time": "2025-12-20T15:43:15 1766245395",
    "num_comments": 269,
    "comments_url": "https://news.ycombinator.com/item?id=46336947",
    "comments": [
      "Self-hosting is more a question of responsibility I'd say. I am running a couple of SaaS products and self-host at much better performance at a fraction of the cost of running this on AWS. It's amazing and it works perfectly fine.For client projects, however, I always try and sell them on paying the AWS fees, simply because it shifts the responsibility of the hardware being \"up\" to someone else. It does not inherently solve the downtime problem, but it allows me to say, \"we'll have to wait until they've sorted this out, Ikea and Disney are down, too.\"Doesn't always work like that and isn't always a tried-and-true excuse, but generally lets me sleep much better at night.With limited budgets, however, it's hard to accept the cost of RDS (and we're talking with at least one staging environment) when comparing it to a very tight 3-node Galera cluster running on Hetzner at barely a couple of bucks a month.Or Cloudflare, titan at the front, being down again today and the past two days (intermittently) after also being down a few weeks ago and earlier this year as well. Also had SQS queues time out several times this week, they picked up again shortly, but it's not like those things ...never happen on managed environments. They happen quite a bit.reply",
      "Me: \u201cWhy are we switching from NoNameCMS to Salesforce?\u201dSavvy Manager: \u201cNoNameCMS often won\u2019t take our support calls, but if Salesforce goes down it\u2019s in the WSJ the next day.\u201dreply",
      "This ignores the case when BigVendor is down for your account and your account only and support is mia, which is not that uncommon imereply",
      "> but it allows me to say, \"we'll have to wait until they've sorted this out, Ikea and Disney are down, too.\"From my experience your client\u2019s clients don\u2019t care about this when they\u2019re still otherwise up.reply",
      "Yes but the fact that it's \"not their fault\" keeps the person from getting fired.Don't underestimate the power of CYAreply",
      "This is a major reason the cloud commands such a premium. It\u2019s a way to make down time someone else\u2019s problem.The other factor is eliminating the \u201cone guy who knows X\u201d problem in IT. What happens if that person leaves or you have to let them go? But with managed infrastructure there\u2019s a pool of people who know how to write terraform or click buttons and manage it and those are more interchangeable than someone\u2019s DIY deployment. Worst case the cloud provider might sell you premium support and help. Might be expensive but you\u2019re not down.Lastly, there\u2019s been an exodus of talent from IT. The problem is that anyone really good can become a coder and make more. So finding IT people at a reasonable cost who know how to really troubleshoot and root cause stuff and engineer good systems is very hard. The good ones command more of a programmer salary which makes the gap with cloud costs much smaller. Might as well just go managed cloud.reply",
      "That is called \"bus factor\" or \"lottery factor\". If the one IT guy gets hit by a bus or wins the lottery and quits, what happens? You want a bus factor of two or more - Two people would have to get hit by a bus for the company to have a big problemreply",
      "That's real microeconomics.reply",
      "Over 20 year I've had lots of clients on self-hosted, even self-hosting SQL on the same VM as the webserver as you used to in the long distant past for low-usage web apps.I have never, ever, ever had a SQL box go down. I've had a web server go down once. I had someone who probably shouldn't have had access to a server accidentally turn one off once.The only major outage I've had (2/3 hours) was when the box was also self-hosting an email server and I accidentally caused it to flood itself with failed delivery notices with a deploy.I may have cried a little in frustration and panic but it got fixed in the end.I actually find using cloud hosted SQL in some ways harder and more complicated because it's such a confusing mess of cost and what you're actually getting. The only big complication is setting up backups, and that's a one-off task.reply",
      "Disks go bad. RAID is nontrivial to set up. Hetzner had a big DC outage that lead to data loss.Off site backups or replication would help, though not always trivial to fail over.reply"
    ],
    "link": "https://pierce.dev/notes/go-ahead-self-host-postgres#user-content-fn-1",
    "first_paragraph": "Self-hosting a database sounds terrifying. That narrative has certainly been pushed over the last 10 years by the big cloud providers:The rumors obscure the truth.I've had data corruption when using a 3rd party vendor just the same as I've had when self-hosting. And with a serious markup, what's the point?I've been running my own self-hosted postgres for the better part of two years now, serving thousands of users and tens of millions of queries daily2. I expected it would give me much more trouble than it has. It's caused me exactly 30mins of stress during a manual migration and that's all. Aside from that it's been fast, stable, and much cheaper.I sleep just fine at night thank you.Let me rewind for a second. The \"database as a service\" narrative wasn't always the dominant one. From the 80s to the early 2000s, everyone ran their own databases because there wasn't really an alternative. You had your application server and your database server, often on the same physical machine. It wa"
  },
  {
    "title": "Italian bears living near villages have evolved to be smaller and less agressive (phys.org)",
    "points": 54,
    "submitter": "wjSgoWPm5bWAhXB",
    "submit_time": "2025-12-15T13:30:45 1765805445",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=46274272",
    "comments": [
      "The selective pressure of a .338 Winchester Magnum, is not to be underestimated.Funny thing is something similar occurs in lab mice. Where a technician is selecting a mouse for cull the more aggressive mice are more likely to be the ones selected. Problem mice who kill their littermates can ruin experiments.reply",
      "same with russian fox fur breeders. i don't remember the numbers, but after a surprisingly small number of generations the foxes turned into cat-like pets.reply",
      "Yes, that's a quite famous experiment, and still ongoing.  Similar effects of \"domestication syndrome\" have recently been reported in wild urban foxes and raccoons.reply",
      "Remember reading something about humans themselves show the signs of domestication syndrome.reply",
      "Not in the literal sense (which would semantically impossible), but we have domesticated ourselves with the advent of farming and the domestication of crop plants. We fundamentally changed our own lifestyle into an agricultural one, the same we changed lifestyle of several large mammal species to co-exist with us in that agricultural lifestyle. So perhaps in some sense, maybe we actually did literally domesticated ourselves.reply",
      "When will humans evolve to be less aggressive before we devolve into catastrophic collapse?reply",
      "For what it\u2019s worth, I think even the worst outcomes wouldn\u2019t necessarily force us to extinction. Would be a bit of a reset though.reply",
      "Looking forward to bears being domesticated.reply",
      "The coon's too: https://www.scientificamerican.com/article/raccoons-are-show...reply",
      "that'd be a nice monthly food bill, a black bear can eat 20x as much as a dogreply"
    ],
    "link": "https://phys.org/news/2025-12-italian-villages-evolved-smaller-aggressive.html",
    "first_paragraph": ""
  },
  {
    "title": "I spent a week without IPv4 (2023) (apalrd.net)",
    "points": 108,
    "submitter": "mahirsaid",
    "submit_time": "2025-12-20T18:31:23 1766255483",
    "num_comments": 180,
    "comments_url": "https://news.ycombinator.com/item?id=46338365",
    "comments": [
      "I\u2019m surprised home many technically knowledgeable people on Internet forums still think IPv6 is some niche, unreliable thing.In my direct experience, in the USA, at least Spectrum, AT&T, and Xfinity (Comcast) still run IPv4, of course, but they also have IPv6 working and on by default on their home internet offerings.All mainstream computer and mobile OSes support it by default and will prefer to connect with it over IPv4.\u2018Everyone\u2019 in many areas is using it. For many of us, our parents are using Facebook and watching Netflix over it. Over 50% of Google\u2019s American traffic is over it. It just works.reply",
      "T-Mobile, a major phone provider, runs an ISP which is IPv6 only. That is, your phone never gets an IPv4, unless connected to WiFi. They offer home access points with a 5G modem and a router; the external address is also IPv6 only.It works plenty well. I access everything accessible via IPv6, and the rest through their 464XLAT, transparently.My LAN still has IPv4, because some ancient network printers don't know IPv6. OpenWRT on my router supports IPv6 just fine. Of course I do not expose any of my home devices to the public internet, except via Wireguard.reply",
      "My problem with IPv6 is that my ISP (Xfinity) won't give me a static prefix, so every now and again it changes.Unlike IPv4, my LAN addresses include the prefix, so every time they change it, all my LAN addresses change.Combined with the lack of DHCP6 support in many devices, this means reverse DNS lookups from IP to hostname can't be done, making identifying devices by their IP essentially impossible.reply",
      "I think you\u2019re conflating multiple things there. There\u2019s nothing magical about IPv4 that gives your LAN addresses stability when your ISP changes your IP prefix. That\u2019s provided by your router doing network address translation. You send a packet from your address which is 192.168.0.42 (a local address), and your router changes the bytes in the packet so that it comes from X.Y.Z.W (your router\u2019s public address). If you really wanted it to your router could do the same thing for IPv6.IPv6 also has local addresses, but a lot more of them. Anything starting with fd00::/8 is a local address with 40 bits available as the network number. So you can set up your local network with the prefix fdXX:XXXX:XXXX::/48 (where the Xs are chosen randomly) as the prefix and still have 16 bits left over for different subnets if you want. These addresses do not change when your ISP changes your public prefix.And if you want to add reverse dns for SLAAC addresses then just have your router listen for ICMPv6 Neighbor Announcement addresses and use them to update your DNS server as appropriate. Or configure your servers to use stable addresses based on their MAC address rather than random addresses (which are better for privacy), and then just configure the DNS as you add and remove servers.reply",
      "what servers?reply",
      "The things on your LAN that you're connecting to via DNS and IP, which cause the desire to have stable LAN IPs in the first place.reply",
      "That's what DNS is for... to not need to remember or know numerical addresses.reply",
      "And DNS is easier to set up if the IP doesn't change constantly.This conversation is going in circles.reply",
      "you should advertise a local prefix (anything in fd00::/8) in your network and it should just work. no need to use the isp-provided prefix for lan.reply",
      "There are some address source selection problems if you're still using any ipv4 for the local services https://blog.ipspace.net/2022/05/ipv6-ula-made-useless/reply"
    ],
    "link": "https://www.apalrd.net/posts/2023/network_ipv6/",
    "first_paragraph": "The time has come to talk about something uncomfortable to a lot of you. You\u2019ve been using legacy methods for far too long. It\u2019s time to move to IPv6.But, of course, there\u2019s a lot more to IPv6 than \u2018just\u2019 switching everything over. A lot of systems in the world still haven\u2019t adopted it after nearly 25 years, and although software support is virtually a requirement these days, that doesn\u2019t mean it\u2019s widely enabled. There are also still a lot of misconceptions from network administrators who are scared of or don\u2019t properly understand IPv6, and I want to address all of that.But, for me to describe to you the best setup for your networks going forward, I need to understand for myself how all of the IPv6 transition mechanisms and behaviors work. To understand where transition mechanisms fail, I\u2019m spending a fully week with only IPv6 and reporting on what works and doesn\u2019t.At the start of this, I\u2019d like to absolutely stress that the NAT you know and hate was not imagined when the Internet Pr"
  },
  {
    "title": "Gemini 3 Pro vs. 2.5 Pro in Pokemon Crystal (jcz.dev)",
    "points": 250,
    "submitter": "alphabetting",
    "submit_time": "2025-12-16T12:48:12 1765889292",
    "num_comments": 71,
    "comments_url": "https://news.ycombinator.com/item?id=46287848",
    "comments": [
      "The baked-in assumptions observation is basically the opposite of the impression I get after watching Gemini 3's CoT. With the maximum reasoning effort it's able to break out of the wrong route by rethinking the strategy. For example I gave it an onion address without the .onion part, and told it to figure out what this string means. All reasoning models including Gemini 2.5 and 3 assume it's a puzzle or a cipher (because they're trained on those) and start endlessly applying different algorithms to no avail. Gemini 3 Pro is the only model that can break the initial assumption after running out of ideas (\"Wait, the user said it's just a string, what if it's NOT obfuscated\"), and correctly identify the string as an onion address. My guess is they trained it on simulations to enforce the anti-jailbreaking commands injected by the Model Armor, as its CoT is incredibly paranoid at times. I could be wrong, of course.reply",
      "I've had some weird \"thinking outside the box\" behavior like this. I once asked 3 Pro what Ozzy Osbourne is up to. The CoT was a journey, I can tell you! It's not in its training data that he actually passed away. It did know he was planning a tour though. It had a real struggle trying to consolidate \"suspicious search results\" and even questioned whether it was fake news, or running against a simulation!, determining it wasn't going to fall for my \"test\".It did ultimately decide Ozzy was alive. I pushed back on that, and it instantly corrected itself and partially blamed my query \"what is he up to\" for being formulated as if he was alive.reply",
      "Odd, mine didn't do anything interesting.reply",
      "1.88 billion tokens * $12 / 1M tokens (output) suggests a total cost of $22,560 to solve the game with Gemini 3 Pro?reply",
      "\u201cGemini 3 Pro was often overloaded, which produced long spans of downtime that 2.5 Pro experienced much less often\u201dI was unclear if this meant that the API was overloaded or if he was on a subscription plan and had hit his limit for the moment. Although I think that the Gemini plans just use weekly limits, so I guess it must be API.reply",
      "I can't believe how massively underpaid I was when I was 11reply",
      "Do you hallucinate as a kid?reply",
      "At that age, it's called \"imagination\"reply",
      "Kids definitely do this. They fill in blanks/context with assumptions, resulting in all sorts of silly responses, for topics of sparse knowledge/certainty. They're not lying, because they think it's true. Sometimes the gap filling is wrong, but usually downright brilliant, within the context of their knowledge.reply",
      "Are you sure there is an age limit for that kind of behavior in humans?reply"
    ],
    "link": "https://blog.jcz.dev/gemini-3-pro-vs-25-pro-in-pokemon-crystal",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: HN Wrapped 2025 - an LLM reviews your year on HN (kadoa.com)",
    "points": 119,
    "submitter": "hubraumhugo",
    "submit_time": "2025-12-20T13:39:47 1766237987",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=46336104",
    "comments": [
      "> You've mentioned Futurama references so many times that I'm starting to think you're actually just Zap Brannigan with a Python Foundation fellowship.Kif, I'm feeling the \"Captain's itch\".reply",
      "I did like it, but for me it was fixated on 3-5 comments from the last 1-2 months that got a few more upvotes. It didn\u2019t really work as an overview for the year. Still, a pretty cool thingy :)reply",
      "I agree, it feels like it only read the most recent few months of comments. The \"vibe check\" was on point though!reply",
      "Yeah, same here. For the comments it took into account it made pretty great roasts, but would have been better if it was actually comprehensive over the course of the year.reply",
      "I had a similar experience but overall the idea is super charming. I do like the personalized HN for 2035. Thank you for building it!reply",
      "Got some laughs from this, thanks!https://hn-wrapped.kadoa.com/neomantraThe MCP Obsessed Vibe-Coder  A high-frequency haptics evangelist who is currently attempting to connect every physical object in his house to a DuckDB instance via an AI-controlled nipple mount.reply",
      "Nice, mine was very cool and accurate :  Your intense hatred for the concept of GDP is only matched by your strangely specific crusade against the calorie theory, making you the only person on HN who thinks the economy and thermodynamics are both just vibes.\n\nhttps://hn-wrapped.kadoa.com/ttoinoureply",
      "> You've mentioned Gemini 2.0 Flash pricing and model comparisons so many times that I'm starting to think you're actually a Google Cloud Billing alert that gained sentience.I wouldn\u2019t mention it so much if Google stopped bumping up the price.reply",
      "In contrast to many others, I did not find this particularly interesting.- The comic on is oddly cropped and contains speech attribution errors.- It calls me an \"extremist\" regarding the wrong thing (I am many kinds of extremist, but certainly not Haskell).- It claims I believe \"any software failure is merely a design error\" which is a complete misunderstanding of the ideas I presented.- It says things like \"the geometric mean of the snack bowl\" which doesn't have meaning in English.I feel like it has picked up on certain keywords and then just rolled with its own stereotypes of what those keywords represent, rather than actually taking a good look at what I think. A roast works because the roaster has clearly spent time and effort and care understanding the person roasted. This is way too shallow for that.The 2026 and 2035 predictions (with a few exceptions) don't make sense at all, and the jokes in them fall completely flat. They're not good anti-jokes either. If someone said something like it in a social situation it would be followed by an awkward silence.The vibe check and the time spent were really cool though. Super interesting. I would have loved to see those expanded.I don't mean to be negative. The project is cool. I just wish it would put its focus on the valuable parts, rather than the things it is weak at. I guess this is my 45 % pedantic, 25 % contrarian, 20 % analytical self speaking.reply",
      "Mine on the other hand could not be more accurate: https://hn-wrapped.kadoa.com/MarcelOlszreply"
    ],
    "link": "https://hn-wrapped.kadoa.com?year=2025",
    "first_paragraph": "Roasts, trends, and predictions of your year on Hacker News.by \u2022 Not affiliated with Y Combinator or Hacker News. All data is deleted within 30 days."
  },
  {
    "title": "OpenSCAD is kinda neat (nuxx.net)",
    "points": 200,
    "submitter": "c0nsumer",
    "submit_time": "2025-12-20T17:45:48 1766252748",
    "num_comments": 148,
    "comments_url": "https://news.ycombinator.com/item?id=46337984",
    "comments": [
      "It is kinda neat, but OpenSCAD's limitations are the main thing that motivated me to write this Python library to generate 3D meshes used signed distance functions:https://github.com/fogleman/sdfOne big plus to doing it this way is that it's \"just\" Python and you can use arbitrary logic to help construct your model.You can even load an existing 3D mesh and operate on it as an SDF. Great for hollowing, chopping, eroding/dilating, etc. existing models.I should probably do more with this project. I think there's a lot of interest in this space.reply",
      "Could you please elaborate on how this is different than the other python based modeling tools - build123d[0] and CadQuery[1].I recently also got annoyed with OpenSCAD and its limitations and therefore started experimenting with Build123d. I'm very much a beginner in the CAD space and would love to understand what inspired you to build sdf.My basic understanding is that STL files are essentially like Bitmap images and store a list of triangles and their positions, whereas STEP files are more like Vector art where there is a list of instructions on how to build the model. Most CAD GUI programs also operate on a similar model to vector art where they record a list of operations one on top of another. It's why STEP files are a standardized format and can be imported / exported from most GUI based CAD builders. I think.Given that SDF also seems like it builds only STL files (I could be wrong), wouldn't learning build123d or CadQuery work better if one cares about compatibility with existing GUI based CAD modeling software?Additionally, atleast build123d offers a similar conceptual model to using Fusion360 and FreeCad - I have limited experience here - but essentially you sketch something in 2D on a particular plane, and then apply some operations to convert it to 3d in a particular manner - the simplest being extruding. This means the mental modeling of how to construct something is very similar across both GUI based CAD programs and Build123d, and that makes it easier for me to jump between GUI based and code based CAD modelling.I'd love to understand your point of view, and learn more.[0] - https://github.com/gumyr/build123d[1] - https://github.com/CadQuery/cadqueryreply",
      "It seems like you already understand the differences. I wasn't aware of those other projects. build123d looks pretty neat.Like most of my projects, this was just for fun and I mainly made it for myself. I'm a DIY kind of guy when it comes to software. I just throw things up on GitHub in case anyone else can get some use or inspiration out of it.reply",
      "> You can even load an existing 3D mesh and operate on it as an SDF. Great for hollowing, chopping, eroding/dilating, etc. existing models.This has my instant interest.  Multiple times I have wanted to take an existing .STL file and cut a hole on it or add another object to it and have never had success.I've tried things like Meshlab, but while the interface has what appears to be a hundred different functions, attempting to use anything returns some error code that requires a PhD to understand and none of the \"repair\" functions seem to help.I mean seriously:  Mesh inputs must induce a piecewise constant winding number field.How the hell am I supposed to accomplish that on a STL file?reply",
      "You can do that effortlessly right in openscad itself or in freecad for a more visual way, or in every single cad app in existence I think.In freecad you first just open the stl file, then Part -> Create part from mesh, then you have a solid you can modify.reply",
      "That feature requires getting pyopenvdb installed, which can be a headache, and I never really updated the README with examples, but it does work. There is one example script:https://github.com/fogleman/sdf/blob/main/examples/mesh.pyYou basically just say:f = Mesh.from_file(path).sdf(voxel_size=0.25, half_width=1)Then you can operate on `f`.reply",
      "Blender also has a high learning curve but you typically don't need a PhD to understand the errors (instead you just watch youtube videos and copy what they do).Removing faces from an STL and adding other objects is quite straightforward.  Previously, Autodesk had Meshmixer and 123D, I guess Meshmixer is still available: https://meshmixer.org/ and I found it to be great for quick editing of the type you're describing.reply",
      "OpenSCAD can load STLs and cut holes in them.reply",
      "Tinkercad is a very low-barrier-to-entry option herereply",
      "Bless you for your service, sir!  I have used `sdf` to create a whole bunch of stuff (buttons for my mother, tealight holders, etc.) and `gg` gets used in a bunch of places (including a couple of bots).> I should probably do more with this project.I, for one, would be glad.reply"
    ],
    "link": "https://nuxx.net/blog/2025/12/20/openscad-is-kinda-neat/",
    "first_paragraph": "Making, baking, and (un-)breaking things in Southeast Michigan.Earlier this year I designed a very basic box/organizer for AA and AAA batteries in Autodesk Fusion, making it parameterized so that by changing a few variables one could adjust the battery type/size, rows/columns, etc. This worked well, and after uploading it to Printables earlier today I realized that reimplementing it would probably be a good way to learn the basics of OpenSCAD.OpenSCAD is a rather different type of CAD tool, one in which you write code to generate objects. Because my battery holder is very simple (just a box with a pattern of cutouts) and uses input parameters, I figured it\u2019d be a good intro to a new language / tool. And in the future might even be better than firing up Fusion for such simple designs.After going through part of the tutorial and an hour or so of poking, here\u2019s the result: battery_holder_generator.scadBy changing just a few variables \u2014 numRows and numColumns and batteryType \u2014 one can rend"
  },
  {
    "title": "MIRA \u2013 An open-source persistent AI entity with memory (github.com/taylorsatula)",
    "points": 59,
    "submitter": "taylorsatula",
    "submit_time": "2025-12-20T20:50:43 1766263843",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=46339537",
    "comments": [
      "I tried making something similar a while ago, and the main problem was that long-term memory makes it easy to move the AI into a bad state where it overfixates on something (context poisoning), or decides to refuse talking to me completely. So in the end, I added a command that wipes out all memory, and ended up using it all the time.Maybe I was doing it wrong. The question is: how do you prevent the AI from falling into a corrupt state from which it cannot get out?reply",
      "I use a two-step generation process which both avoids memory explosion in the window and the one turn behind problem.When a user sends a message I:\ngenerate a vector of the user message ->\npull in semantically similar memories ->\nfilter and rank them ->\nthen send an API call with the memories from the last turn that were 'pinned' plus the top 10 memories just surfaced. the first API call's job is to intelligently pick the actual worthwhile memories and 'pin' them till the next turn -> do the main LLM call with an up-to-date and thinned list of memories.Reading the prompt itself that the analysis model carries is probably easier than listening to my abstract description: https://github.com/taylorsatula/mira-OSS/blob/main/config/pr...I can't say with confidence that this is ~why~ I don't run into the model getting super flustered and crashing out though I'm familiar with what you're talking about.reply",
      "I would love to see how this shakes out using purely free/open models (for example via OpenRouter).reply",
      "Title says \"open source\", but the Business Source License (BSL) is not an Open Source Initiative (OSI) approved open-source license.reply",
      "Fixed! BSL (to my understanding) is a copy of the license and its a 'hashicorp document' so it had their title on it.However, someone earlier today put me onto the concept of AGPL licenses so I changed MIRA over to AGPL because it still aligns with my overall intent of protecting my significant effort from someone coming in and Flappy Bird-ing it while still making it freely available to anyone who wants to access, modify, anything it.reply",
      "The correct term for things like BSL is \u201csource available.\u201dreply",
      "The \"correct term\" for things like BSL is whatever they want to call it, as long as no trademarks are being infringed.reply",
      "I see this more and more used. It seems companies want to fake stuff now, aka claiming to be open source when they are not.DHH also claims he is super open source when in reality he already soul-sent to the big tech bros:https://world.hey.com/dhh/the-o-saasy-license-336c5c8fWe also had this recently with arduino. I don't understand why companies try to get that way. To me it is not an open source licence - it is a closed source business licence. Just with different names.reply",
      "(As I said above I changed to an AGPL earlier today but I'll speak to my BSL logic)I liked BSL because the code ~was~ proprietary for a time so someone couldn't duplicate my software I've worked so hard on, paywall it, and put me out of business. I'm a one-man development operation and a strong gust of wind could blow me over. I liked BSL because it naturally decayed into a permissive open source license automatically after a timeout. I'd get a head start but users could still use it and modify it from day one as long as they didn't charge money for it.reply",
      "Who asked them?reply"
    ],
    "link": "https://github.com/taylorsatula/mira-OSS",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        This is the public release of MIRA OS. Discrete memories decay through momentum loss, tools auto-configure when dropped into tools/ folder, and the system prompt composes from modular trinkets. I would like to think I've made an elegant brain-in-box. You load it and send cURL requests - it talks back, learns, and uses tools. Contributions welcome.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.10 months or so ago I had the idea to build a recipe generator that could incorporate my cuisine preferences. 10,000 scope creeps later MIRA is a comprehensive best-effort approximation of a continuous digital entity. This is my TempleOS.Mira accomplishes the end goal of continuity and recall through a blend of asynchronous conversation processing akin to REM sleep an"
  },
  {
    "title": "More databases should be single-threaded (konsti.xyz)",
    "points": 26,
    "submitter": "lawrencechen",
    "submit_time": "2025-12-20T22:12:22 1766268742",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=46340117",
    "comments": [
      "Disclaimer: ex-AWS here.This article ends up making a compelling case for DynamoDB. It has the properties he describes wanting. Many, many systems inside of Amazon are built with DDB as the primary datastore. I don't know of any OSS commensurate to DDB, but it would be quite interesting for one to appear.> \"Every transaction can only be in one shard\" only works for simple business logics.You'd be quite surprised at what you can get out of this model. Check out the later chapters of the DynamoDB Book [1] for some neat examples.[1] https://dynamodbbook.com/reply",
      "From what I understand, the complexity stays there, it's just moved from the DB layer to the app layer (now I have to decide how to shard data, how to reshard, how to synchronize data across shards, how to run queries across shards without wildly inconsistent results), so as I developer I have more headaches now than before, when most of that was taken care of by the DB. I don't see why it's an improvement.The author also mentions B2B and I'm not sure how it's going to work. I understand B2C where you can just say \"1 user=1 single-threaded shard\" because most user data is isolated/independent from other users. But with B2B, we have accounts ranging from 100 users per organization to 200k users per organization. Something tells me making a 200k account single-threaded isn't a good idea. On the other hand, artificially sharding inside an organization will lead to much more complex queries overall too, because usually a lot of business rules require joining different users' data within 1 org.reply",
      "Yeah, mgmt (and more than anything, query tools) is gonna be a PITA.But looking at it in a different way, say building something like Google Sheets.One could place user-mgmt in one single-threaded database (Even at 200k users you probably don't have too many concurrently modifying administrators) whilst \"documents\" gets their own database. I'm prototyping one such \"document\" centric tool and the per-document DB thinking has come up, debugging users problems could be as \"simple\" as cloning a SQLite file.Now on the other hand if it's some ERP/CRM/etc system with tons of linked data that naturally won't fly.Tool for the job.reply",
      "It's a different kind of complexity. Essentially, your app layer needs shift from:    - transaction serializability\n    - atomicity\n    - deadlocks (generally locks)\n    - occ (unless you do VERY long tx, like a user checkout flow)\n    - retries\n    - scale, infrastructure, parameter tuning\n\ntowards thinking about    - separating data into shards\n    - sharding keys\n    - cross-shard transactions\n\nwhich can be sometimes easier, sometimes harder. I think there are a surprising amount of problems where it's much easier to think about sharding than about race conditions!> But with B2B, we have accounts ranging from 100 users per organization to 200k users per organization.You'd be surprised at how much traffic a single core (or machine) can handle \u2014 200k users is absolutely within reach. At some point you'll need even more granular sharding (eg. per user within organization), but at that point, you would need sharding anyways (no matter your DB).reply",
      "In real-world business requirements it often need to read some data then touch other data based on previous read result.It violates the \"every transaction can only be in one shard\" constraint.For a specific business requirement it's possible to design clever sharding to make transaction fit into one shard. However new business requirements can emerge and invalidate it.\"Every transaction can only be in one shard\" only works for simple business logics.reply",
      "I talk about these problems in the \"How hard can sharding be?\" section of the article \u2014 long story short, not all business requirements can be dealt with easily, but surprisingly many can if you choose a smart sharding key.You can also still do optimistic concurrency across shards! That covers most of the remaining ones. Anything that requires anything more complex \u2014 sagas, 2PC, etc. \u2014 is relatively rare, and at scale, a traditional SQL OLTP will also struggle with those.reply",
      "Thanks for reply.So in my understanding:- The transactions that only touch one shard is simple- The transactions that read multiple shards but only write shard can use simple optimistic concurrency control- The transactions that writes (and reads) multiple shards stay complex. Can be avoided by designing a smart sharding key. (hard to do if business requirement is complex)reply",
      "The optimistic concurrency control that reads multiple shards cannot use simple CAS. It probably needs to do something like two-phase committingreply",
      "That's right!If you anticipate you will encounter the third type a lot, and you don't anticipate that you will need to shard either way, what I'm talking about here makes no sense for you.reply",
      "Give each business or customer its own schema and you almost never need sharding.reply"
    ],
    "link": "https://blog.konsti.xyz/p/8c8a399f-8cfe-47dd-9278-9527105d07dc/",
    "first_paragraph": "Last night, I caused something on X by saying that (most) transactional databases should be single-threaded and aggressively sharded:hot take: databases should be single-threadedsomeone a long time ago decided that database shards should be multi-threaded. ever since then, we've had to worry about transactions, serializability, race conditions, and locking.instead, we should have single-threaded shards,\u2026I have a lot more to say than would fit in a tweet, and the tweet also got more attention than I thought, so here's a lengthy blog post.(There's no solution that solves every problem. But I hope I can convince you that aggressively sharded, single-threaded databases would suit you best more often than you'd think, even if you're \"just\" building a generic B2B SaaS app.)In essence, a traditional SQL database can write and read to rows in a table. It can also lock those rows, meaning that other writers may have to wait for the lock to be freed.More concretely, Postgres has three different "
  },
  {
    "title": "Biscuit is a specialized PostgreSQL index for fast pattern matching LIKE queries (github.com/crystallinecore)",
    "points": 68,
    "submitter": "eatonphil",
    "submit_time": "2025-12-16T15:39:23 1765899563",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=46289884",
    "comments": [
      "Postgres\u2019s extensible index AM story doesn\u2019t get enough love, so it\u2019s nice to see someone really lean into it for LIKE. Biscuit is basically saying: \u201cwhat if we precompute an aggressive amount of bitmap structure (forward/backward char positions, case-insensitive variants, length buckets) so most wildcard patterns become a handful of bitmap ops instead of a heap scan or bitmap heap recheck?\u201d That\u2019s a very different design point from pg_trgm, which optimizes more for fuzzy-ish matching and general text search than for \u201cI run a ton of LIKE '%foo%bar%' on the same columns\u201d.The interesting question in prod is always the other side of that trade: write amplification and index bloat. The docs are pretty up-front that write performance and concurrency haven\u2019t been deeply characterized yet, and they even have a section on when you should stick with pg_trgm or plain B-trees instead. If they can show that Biscuit stays sane under a steady stream of updates on moderately long text fields, it\u2019ll be a really compelling option for the common \u201cpoor man\u2019s search\u201d use case where you don\u2019t want to drag in an external search engine but ILIKE '%foo%' is killing your box.reply",
      "Looks very interesting. I really like trigram indexes for certain use cases, but those are essentially running an ILIKE %something% on various text content in the DB. So that would fit the described limitations of this index type very well.Usually you're quickly steered towards fulltext search (tsvector) in Postgres if you want to do something like that. But depending on what kind of search you actually need, trigram indexes can be a better option. If you don't search so much for natural language, but more for specific keywords the stemming in fulltext search can get in the way.One information that would be nice here is a comparison of the index size on disk for both index types.reply",
      "This is a fairly simple idea of indexing characters for each column/offset and compressing the bitmaps. Simple is good, as the overhead of more sophisticated ideas (eg suffix sorting) is often prohibitive.One suggestion is to index the end-of-string as a character as well; then you don't need negative offsets. But that turns the suffix search into a wildcard type of thing where you have to try all offsets, which is what the '%pat%' searches do already, so maybe it's OK.reply",
      "How is the postgres ecosystem at stating when these kinds of things are ready for adoption? I can think of a usecase at work where this might be useful, but hesitant to just start throwing random opensource extensions at our monolith DB.reply",
      "The GitHub repo is about two weeks old and there's a single author - if I were you, I'd let it cook for a while longer.reply",
      "Any data on index size for big tables? Comparison (with ms/megabytes) vs trigram regarding size/speed?UPD> Biscuit is 15.0\u00d7 faster than B-tree (median) and 5.6\u00d7 faster than Trigram (median)> Trade-off: 3.2\u00d7 larger index than Trigram, but 5.6\u00d7 faster queries (median)reply",
      "I found some more info here: https://biscuit.readthedocs.io/en/latest/benchmark_roaring.h...reply",
      "Noticed Daniel Lemire talking about it and how they use Roaring Bitmaps.https://x.com/lemire/status/2000944944832504025reply",
      "Would this be a good fit to replace FTS for hybrid search? Biscuit + Vector?reply"
    ],
    "link": "https://github.com/CrystallineCore/Biscuit",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Biscuit is a specialized PostgreSQL index access method (IAM) designed for blazing-fast pattern matching on LIKE queries, with native support for multi-column searches. It eliminates the recheck overhead of trigram indexes while delivering significant performance improvements on wildcard-heavy queries.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\nBiscuit is a specialized PostgreSQL index access method (IAM) designed for blazing-fast pattern matching on LIKE and ILIKE queries, with native support for multi-column searches. It eliminates the recheck overhead of trigram indexes while delivering significant performance improvements on wildcard-heavy queries. It stands for Bitmap Indexed Searching with Comprehensive Union and Intersection Techniques.Added SQ"
  },
  {
    "title": "NTP at NIST Boulder Has Lost Power (nanog.org)",
    "points": 428,
    "submitter": "lpage",
    "submit_time": "2025-12-20T07:39:26 1766216366",
    "num_comments": 190,
    "comments_url": "https://news.ycombinator.com/item?id=46334299",
    "comments": [
      "Wind gusts were reaching 125 MPH in Boulder county, if anyone\u2019s curious. A lot of power was shut off preemptively to prevent downed power lines from starting wildfires. Energy providers gave warning to locals in advance. Shame that NIST\u2019s backup generator failed, though.reply",
      "Somewhat interesting that they themselves don't have access to the site. You'd think there would have been some disaster plans put in place?reply",
      "The disater plan is to have a few dozens stratum 1 servers spread around the world, each connected to a distinct primary atomic clock, so that a catastrophic disaster needs to take down the global internet itself for all servers to become unreachable.The failure of a single such server is far from a disaster.reply",
      "For those of us near Boulder, it's urgent.But the stratum 1 time servers can shrug and route around the damage.reply",
      "And the disaster plan for the disaster plan is to realize that it isn't that important at the human-level to have a clock meticulously set to correspond to other meticulously-set clocks, and that every attempt to force rigid timekeeping on humans is to try to make humans work more like machines rather than to make machines work more like humans.reply",
      "I really, really can't get behind this sentiment. Having a reliable, accurate time keeping mechanism doesn't seem like an outlandish issue to want to maintain. Timekeeping has been an important mechanism for humans for as long as recorded history. I don't understand the wisdom of shooting down establishing systems to make that better, even if the direct applicability to a single human's life is remote. We are all part of a huge, interconnected system whether we like it or not, and accurate, synchronized timekeeping across the world does not sound nefarious to me.reply",
      "> Timekeeping has been an important mechanism for humans for as long as recorded history.And for 99% of that history, Noon was when the sun was half-way through its daily arc at whatever point on Earth one happened to inhabit. The ownership class are the ones who invented things like time zones to stop their trains from running in to each other, and NTP is just the latest and most-pervasive-and-invasive evolution of that same inhuman mindset.From a privacy point of view, constant NTP requests are right up there alongside weather apps and software telemetry for \u201cthings which announce everyone's computers to the global spy apparatus\u201d, feeding the Palantirs of the world to be able to directly locate you as an individual if need be.reply",
      "> The ownership class are the ones who invented things like time zones to stop their trains from running in to each otherIn a world where this didn't happen, your comment would most likely read:> The ownership class are the ones who had such indifference toward the lives of the lower class passengers that they didn't bother stopping their trains from running into each other.reply",
      "Tell me how you feel about DST.reply",
      "Far more things rely on reliable and accurate time-keeping than just being on time to work. Timekeeping is vitally important (even if it's not readily visible) to lots of critical infrastructure worldwide.reply"
    ],
    "link": "https://lists.nanog.org/archives/list/nanog@lists.nanog.org/message/ACADD3NKOG2QRWZ56OSNNG7UIEKKTZXL/",
    "first_paragraph": "reposting from the Internet time service list\n\n\njeff.s...@nist.gov <jeff.sherman@nist.gov>: Dec 19 05:18PM -0800\n\nDear colleagues,\n\nIn short, the atomic ensemble time scale at our Boulder campus has failed\ndue to a prolonged utility power outage. One impact is that the Boulder\nInternet Time Services no longer have an accurate time reference. At time\nof writing the Boulder servers are still available due a standby power\ngenerator, but I will attempt to disable them to avoid disseminating\nincorrect time.\n\nThe affected servers are:\ntime-a-b.nist.gov\ntime-b-b.nist.gov\ntime-c-b.nist.gov\ntime-d-b.nist.gov\ntime-e-b.nist.gov\nntp-b.nist.gov (authenticated NTP)\n\nNo time to repair estimate is available until we regain staff access and\npower. Efforts are currently focused on obtaining an alternate source of\npower so the hydrogen maser clocks survive beyond their battery backups.\n\nMore details follow.\n\nDue to prolonged high wind gusts there have been a combination of utility\npower line damage and p"
  },
  {
    "title": "How to Write a 21st Century Proof (2011) [pdf] (lamport.azurewebsites.net)",
    "points": 12,
    "submitter": "User23",
    "submit_time": "2025-12-16T05:46:20 1765863980",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://lamport.azurewebsites.net/pubs/proof.pdf",
    "first_paragraph": ""
  },
  {
    "title": "Perfecting Steve Baer's Triple Dome (vorth.github.io)",
    "points": 3,
    "submitter": "robinhouston",
    "submit_time": "2025-12-17T17:50:45 1765993845",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://vorth.github.io/vzome-sharing/2024/02/18/baer-dome-from-H4-1001-09-13-04.html",
    "first_paragraph": "\n          In the mid-1960s, Steve Baer was involved with Drop City, an artist's community outside of Trinidad, Colorado.\n          Baer was very interested in dome structures,\n          but was frustrated with some of the features of geodesic domes popularized by Buckminster Fuller.  \n          Baer wanted something more adaptable, extensible, and modular, \n          so he explored the geometry of zonohedra, polyhedra with rings of parallel edges. \n        \n          Through his studies, Baer became intimately familiar with the Platonic and Archimedean solids.  \n          At Drop City, Baer and others built a variety of dome buildings \"on the cheap\", \n          salvaging car tops for the panels in the domes.  \n          The most iconic of these was \n            Baer's triple dome, \n          constructed with parts of\n            three rhombicosidodecahedra fused together.\n          For brevity, I'll use \"RID\" instead of \n            \"rhombicosidodecahedron\"\n          (or \"RIDs\" plural"
  }
]