[
  {
    "title": "Exposed DeepSeek database leaking sensitive information, including chat history (wiz.io)",
    "points": 293,
    "submitter": "talhof8",
    "submit_time": "2025-01-29T21:25:36 1738185936",
    "num_comments": 167,
    "comments_url": "https://news.ycombinator.com/item?id=42871371",
    "comments": [
      "This kinda does support the   'DeepSeek is the side project of a bunch of quants' angle.Seems like the kind of mistake you would make if you are not used to deploying external client facing applications.\n \nreply",
      "That's pretty much the same mistake as in VW recent \"We know where you parked\" hack. [0] So while I don't really want to say anything nice about VW, the mistake is no something that only happens to side projects.[0] https://www.spiegel.de/international/business/we-know-where-...\n \nreply",
      "Software is unfortunately a side-project for most auto makers :)\n \nreply",
      "With the amount of complexity found in modern car's pre-packaged software I'd not be so sure.\n \nreply",
      "No he is right, hardware manufacturers treat software as a line item and just part of the BOM. Typically just contracted out (although some are trying to change that) Thats why its typically mediocre from companies outside of SV.You need a software first agile mentality from the leadership of the company on downwards and these legacy companies just dont have it.",
      "There are many examples of experienced teams doing stupid things like exposing databases that I don't really think this is a valid conclusion to draw.\n \nreply",
      "Clearly it could never be enough to draw that conclusion but it might be very weak evidence in one direction\n \nreply",
      "It doesn't say much. Data breaches from unsecured or accidentally-public servers/databases are not unusual.That exact vuln probably makes up for 1/3rd of the entries of any \"top data breaches\" list.\n \nreply",
      "> This kinda does support the 'DeepSeek is the side project of a bunch of quants' angleCan we stop with this nonsense ?The list of author of the paper is public, you can just go look it up. There are ~130 people on the ML team, they have regular ML background just like you would find at any other large ML labs.Their infra cost multiple millions of dollar per month to run, and the salary of such a big team is somewhere in the $20-50M per year (not very au fait of the market rate in china hence the spread).This is not a sideproject.Edit: Apparently my comment is confusing some people. Am not arguing that ML people are good at security. Just that DS is not the side project of a bunch of quant bros.\n \nreply",
      "A bunch of ML researchers who were initially hired to do quant work published their first ever user facing project.So maybe not a side project, but if you have ever worked with ML researchers before, lack of engineering/security chops shouldn't be that surprising to you.\n \nreply"
    ],
    "link": "https://www.wiz.io/blog/wiz-research-uncovers-exposed-deepseek-database-leak",
    "first_paragraph": "A publicly accessible database belonging to DeepSeek allowed full control over database operations, including the ability to access internal data. The exposure includes over a million lines of log streams with highly sensitive information.Wiz Research has identified a publicly accessible ClickHouse database belonging to DeepSeek, which allows full control over database operations, including the ability to access internal data. The exposure includes over a million lines of log streams containing chat history, secret keys, backend details, and other highly sensitive information. The Wiz Research team immediately and responsibly disclosed the issue to DeepSeek, which promptly secured the exposure.\u00a0In this blog post, we will detail our discovery and also consider the broader implications for the industry at large.\u00a0\u00a0\u00a0DeepSeek, a Chinese AI startup, has recently garnered significant media attention due to its groundbreaking AI models, particularly the DeepSeek-R1 reasoning model. This model "
  },
  {
    "title": "An analysis of DeepSeek's R1-Zero and R1 (arcprize.org)",
    "points": 364,
    "submitter": "meetpateltech",
    "submit_time": "2025-01-29T17:44:45 1738172685",
    "num_comments": 151,
    "comments_url": "https://news.ycombinator.com/item?id=42868390",
    "comments": [
      "> But now with reasoning systems and verifiers, we can create brand new legitimate data to train on. This can either be done offline where the developer pays to create the data or at inference time where the end user pays!> This is a fascinating shift in economics and suggests there could be a runaway power concentrating moment for AI system developers who have the largest number of paying customers. Those customers are footing the bill to create new high quality data \u2026 which improves the model \u2026 which becomes better and more preferred by users \u2026 you get the idea.While I think this is an interesting hypothesis, I'm skeptical. You might be lowering the cost of your training corpus by a few million dollars, but I highly doubt you are getting novel, high quality data.We are currently in a world where SOTA base model seems to be capped at around GPT4o levels. I have no doubt that in 2-3 years our base models will compete with o1 or even o3... just it remains to be seen what innovations/optimizations get us there.The most promising idea is to use reasoning models to generate data, and then train our non-reasoning models with the reasoning-embedded data. But... it remains to be seen how much of the chain of thought reasoning you can really capture into model weights. I'm guessing some, but I wonder if there is a cap to multi-head attention architecture. If reasoning can be transferred from reasoning models to base models, OpenAI should have already trained a new model with o3 training data, right?Another thought is maybe we don't need to improve our base models much. It's sufficient to have them be generalists, and to improve reasoning models (lowering price, improving quality) going forward.\n \nreply",
      "> The most promising idea is to use reasoning models to generate data, and then train our non-reasoning models with the reasoning-embedded data.DeepSeek did precisely this with their LLama fine-tunes. You can try the 70B one here (might have to sign up): \nhttps://groq.com/groqcloud-makes-deepseek-r1-distill-llama-7...\n \nreply",
      "every time you respond to an AI model \"no, you got that wrong, do it this way\" you provide a very valuable piece of data to train on. With reasoning tokens there is just a lot more of that data to train on now\n \nreply",
      "This assumes that you give honest feedback.Efforts to feed deployed AI models various epistemic poisons abound in the wild.\n \nreply",
      "This assumes that the companies gathering the data don\u2019t have silent ways of detecting bad actors and discarding their responses. If you\u2019re trying to poison an AI, are you making all of your queries from the same IP? Via a VPN whose IP block is known? Are you using a tool to generate this bad data, which might have detectable word frequency patterns that can be detected with something cheap like tf-idf?There\u2019s a lot of incentive to figure this out. And they have so much data coming in that they can likely afford to toss out some good data to ensure that they\u2019re tossing out all of the bad.\n \nreply",
      "> If you\u2019re trying to poison an AI, are you making all of your queries from the same IP? Via a VPN whose IP block is known?We can use the same tactics they are using to crawl the web and scrape pages and bypass anti-scraping mechanisms.\n \nreply",
      "Not necessarily, not all tactics can be used symmetrically like that. Many of the sites they scrape feel the need to support search engine crawlers and RSS crawlers, but OpenAI feels no such need to grant automated anonymous access to ChatGPT users.And at the end of the daty, they can always look at the responses coming in and make decisions like \u201c95% of users said these responses were wrong, 5% said these responses were right, let\u2019s go with the 95%\u201d. As long as the vast majority of their data is good (and it will be) they have a lot of statistical tools they can use to weed out the poison.\n \nreply",
      "> As long as the vast majority of their data is good (and it will be)So expert answers are out of scope? Nice, looking forward to those quality data!\n \nreply",
      "If you want to pick apart my hastily concocted examples, well, have fun I guess. My overall point is that ensuring data quality is something OpenAI is probably very good at. They likely have many clever techniques, some of which we could guess at, some of which would surprise us, all of which they\u2019ve validated through extensive testing including with adversarial data.If people want to keep playing pretend that their data poisoning efforts are causing real pain to OpenAI, they\u2019re free to do so. I suppose it makes people feel good, and no one\u2019s getting hurt here.\n \nreply",
      "Probably it's something like \"give feedback that's on average slightly more correct than incorrect,\" though you'd get more signal from perfect feedback.That said, I suspect the signal is  very weak even today and probably not too useful except for learning about human stylistic preferences.\n \nreply"
    ],
    "link": "https://arcprize.org/blog/r1-zero-r1-results-analysis",
    "first_paragraph": "Special thanks to Tuhin and Abu from Baseten and Yuchen from Hyperbolic Labs for hosting r1-zero for us. Hardly any providers are hosting this model variant, and its availability is important for research purposes.ARC Prize Foundation\u2019s goal is to define, measure, and inspire new ideas towards AGI. To this end, we strive to create the strongest global innovation environment possible.We do not have AGI yet and are still innovation constrained \u2013 scaling up pure LLM pretraining is not the path, despite this being the dominant AI industry narrative and mainstream public view as of last summer.The reason narratives are important is they end up driving economic activity, like investment, research focus, funding, geopolitics, trade, etc. For example, in 2023-24 there was ~$20B invested into new LLM startups compared to only ~$200M into new AGI startups.We launched ARC Prize 2024 last June to grow awareness of limits of scaling LLMs and promote a useful benchmark, ARC-AGI-1, towards a new dire"
  },
  {
    "title": "Blueskyfeedbot: Post RSS Feeds to Bluesky via GitHub Actions (github.com/marketplace)",
    "points": 18,
    "submitter": "hrpnk",
    "submit_time": "2025-01-30T00:12:49 1738195969",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=42873153",
    "comments": [
      "RSS and AT Proto have been on my mind all week.Even though RSS is my main source of news, it\u2019s impossible to get around the fact that the (incredibly, unfathomably stupid) shutdown of Google Reader kneecapped it.AT Protocol is more than \u201clet\u2019s make Twitter more open\u201d \u2014 it\u2019s an open protocol for anything that\u2019s real-time and shareable. I was never a big Google Reader guy, I liked traditional clients like NetNewsWire, but the sociability of Google Reader cannot be dismissed.I\u2019m not sure that just bridging RSS to Bluesky is the future\u2026 a distinct AT Proto lexicon seems like the actual way forward IMHOExciting times!\n \nreply",
      "Google Reader shut down nearly 12 years ago. Its legacy now is that there are still people who are pissed off about that, and I get the impression that a surprising amount of them are now in positions where they get to make purchasing decisions over whether or not to buy Google Cloud.(Being a Mac+iPhone person I've settled in quite happily with NetNewsWire now, but it took me a few years to get there.)\n \nreply",
      "OK but not writing your own Cron expressions is just sad.\n \nreply",
      "I've been using Cron for over 20 years. I still use https://crontab.guru rather than trying to remember the syntax for those fields.\n \nreply",
      "Cron expressions aren't bad. Now regex.... nope.\n \nreply",
      "Not everyone knows how to use Cron, the comment is a good tip for those that don't. The target audience for a solution like this might be less technical than the average Github user.\n \nreply"
    ],
    "link": "https://github.com/marketplace/actions/feed-to-bluesky",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          Blueskyfeedbot is a bot that posts RSS feeds to Bluesky via GitHub Actions.Go to https://bsky.app/settings/app-passwords and add a new app password.Create a new GitHub repository.Go to your repository settings at https://github.com/${YOUR_REPO}/settings/secrets/actions/new, and add a new\nsecret with the value of the access token.Add a file named .github/workflows/blueskyfeedbot.yml with the following content:Commit and publish your changes.The status template (status-template) is using Handlebars as template engine.The action is passing in an instance of FeedData (field feedData) and the current FeedEntry (field item) into the template:Feed to Bluesky is not certified by GitHub. It is provided by a third-party and is governed by separate terms of service, privacy policy, and support documentation.Feed to Bluesky is not certified by GitHub. I"
  },
  {
    "title": "SmolGPT: A minimal PyTorch implementation for training a small LLM from scratch (github.com/om-alve)",
    "points": 191,
    "submitter": "amrrs",
    "submit_time": "2025-01-29T18:09:19 1738174159",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=42868770",
    "comments": [
      "Can someone help me understand what I\u2019m looking at here? This repository allows me to train a specific model on a specific data set, and finally test the result? Is that correct?I am interested in how large and small language models are trained, but as someone who has little knowledge in this world I find it hard to cut through the noise to find useful information.Really I\u2019m looking for an open source project that helps a person gain this knowledge. Something like a docker container that encapsulates all the dependencies. When training it will use any available gpu or tell me why my gpu can\u2019t be used and then fall back to cpu. Then had a simple interface to test the training results. Finally you can easily pull back the curtain to understand the process in better detail and maybe even adapt it to different model to experiment.Does something like that exist?\n \nreply",
      "Neat, I love projects like these.The next level down is to do it directly in numpy.And then from there, write a minimal numpy work-a-like to support the model above.You start with a working system using the most powerful abstractions. Then you iteratively remove abstractions, lowering your solution, then when you get low enough but still riding on an external abstraction, you rewrite that, but ONLY to support the layers above you.Following the above pattern, you can bootstrap yourself to have full system understanding. This is not unlike RL+distillation that human persons do learn complex topics.\n \nreply",
      "Numpy can use the chipmaker\u2019s BLAS (Intel MKL or AMD\u2019s Blis fork). Trying to replace it could be a good academic exercise but I think most people wisely leave that to the vendors.\n \nreply",
      "It is a purely pedagogical device, like building a go kart.\n \nreply",
      "> but still riding on an external abstraction, you rewrite that, but ONLY to support the layers above you.i don't get it. Why do i stop before stripping all abstractions?\n \nreply",
      "Where do you get that? He is postulating the external abstraction you are using has more features than you use. He is saying implement only the parts you use.\n \nreply",
      "> Where do you get that?From \"when you get low enough but still riding on an external abstraction\".> He is saying implement only the parts you use.Thanks.\n \nreply",
      "Correct, I should proof read my posts.\n \nreply",
      "Likewise. And your comment reminded me of real programmers** https://xkcd.com/378/\n \nreply",
      "github has a bunch of them for years, the most known from Andrej Karpathy:https://github.com/karpathy/nanoGPTsome other have MoE implemented.\n \nreply"
    ],
    "link": "https://github.com/Om-Alve/smolGPT",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          A minimal PyTorch implementation for training your own small LLM from scratch. Designed for educational purposes and simplicity, featuring efficient training, flash attention, and modern sampling techniques.Requirements:The provided checkpoint was trained on the TinyStories dataset.Architecture:Validation Loss - 1.0491Key parameters (modify in config.py):Model Architecture:Training:Contributions welcome! Please open an issue or PR for:Note: This implementation is inspired by modern LLM training practices and adapted for educational purposes. For production use, consider scaling up model size and dataset."
  },
  {
    "title": "Building a T1D smartwatch for my son from scratch (andrewchilds.com)",
    "points": 176,
    "submitter": "andrewchilds",
    "submit_time": "2025-01-28T16:33:32 1738082012",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=42854291",
    "comments": [
      "First of all -- awesome!Second, if you aren't aware or in case others are interested, the main dev from the xdrip project released a custom watch as well: https://bluejay.website/\nThere are some posts on reddit from the dev. (for others -- the watch is available for purchase, and allows integration w/ xdrip).Third --> we've come a long way in a short time. When I was diagnosed I had to collect urine samples and drop chemical tablets in them to see how much glucose was present. And now you are building an custom solution tailored to your and your son's needs...wow. I really applaud you.Fourth --> I'm hoping the new discoveries in the synthetic signaling w/ insulin will finally give us a biological solution and the tech will no longer be necessary. Here's to hoping!\n \nreply",
      "Yes! The BlueJay watch project is amazing, very encouraging to see that he was able to connect directly to the sensor from the watch with the latest version. Totally agree with you - one of the biggest sources of hope for us in those first few weeks after he was diagnosed was knowing how far the tech had come, and how many huge advancements were in the pipeline.\n \nreply",
      "This is really really awesome. I applaud you.I had my own project trying to achieve a similar outcome to you, I wrote about it here: https://www.bytesizego.com/blog/keeping-alive-with-go. Your approach is much more hardcore. I hope you find a path to make them available to more folks.If there is anything I can do to assist you please let me know!\n \nreply",
      "Thank you! I remember reading your article! Even though you have to maintain it, it\u2019s actually way better than what\u2019s commercially available since you actually have control over _how_ you\u2019re alerted, not to mention when to escalate to friends or family. I\u2019ve heard Type 1 kids going away to college is a huge source of stress for parents/caregivers for this reason - Sugarmate and the SugerPixel help with this but it still feels like a huge problem (a Type 1 sleeping through a low).\n \nreply",
      "Andrew : Amazing i always love to see the creative T1D projects. i home one day we have a biological cure . Back when the pebble watch was still a relatively new product. someone had an app for it to be a remote display for the dexcom. it required an external nodemcu based microcontroller to be a middleman, but what it did well was run for days off the battery. my wife used one for a long while; the take aways were black and white e-ink displays saved tons of power . pebbles os was good a power mgmt and it's limited features made devs stick yo core functionality not look for new cool extraneous add ons .\n \nreply",
      "This is great. I do watch my Dexcom on my Apple Watch, but there's times I just want the number, ASAP. I was hoping it'd talk about eliminating the need for the iOS app (\"...where he shouldn\u2019t have to run around with a phone strapped to him...\") but it sounds like it's just not possible to avoid at least periodic polling.Reading through the product development, I'm curious why you didn't use a dev kit like the Pine Time? (https://pine64.org/devices/pinetime/) I bought one a few years ago and it's just been sitting in the box, waiting for a project to build on it. I may take a stab at something similar.\n \nreply",
      "The G7 pairs directly with an Apple Watch and acts as the primary sync device. Unlike the G6, you don\u2019t get null readings displaying on the watch until you open the app. I have the dexcom widget on my Apple Watch series 10 with a G7 paired with direct to watch and it always displays the current reading\n \nreply",
      "That's great to hear that direct-to-watch pairing leads to a reliable reading. That's a big improvement over where things were a year ago. I am surprised that the watch becomes the primary sync device instead of the phone, but maybe that's not actually a problem in most situations.\n \nreply",
      "Thank you! It should be possible now with the G7 to have the watch pair directly with the sensor. We\u2019re still on the G6. Agreed that would be much better than having to go through the iOS app and up to the cloud.Honestly I don\u2019t remember why I ruled out the PineTime. I purchased a Bangle.js and while it\u2019s very small, it seemed very limited in what I could do with it. Of course with the Pebble announcement yesterday, that might change things.\n \nreply",
      "Maybe look at the new openpebble that has opensourced the Pebble Watch tech.\n \nreply"
    ],
    "link": "https://andrewchilds.com/posts/building-a-t1d-smartwatch-from-scratch",
    "first_paragraph": "My 9 y.o. son has Type 1 diabetes, which basically means his pancreas is on manual (hard) mode 24x7. A healthy pancreas not only produces insulin, which helps convert glucose in the bloodstream into energy - it also produces glucagon, which tells the liver to release glucose into the bloodstream when blood sugar levels are too low. A person with T1D has to manage without either of these guardrails, and a low blood sugar can become a medical emergency if left untreated.We\u2019re lucky to have access to life-changing technology like CGMs and Closed-loop Insulin Pumps, however, they come with their own problems, such as the alarms that go off during a high or low blood sugar. My son\u2019s pump and phone both alarm when he falls below 55 mg/dL. This can happen very quickly in a number of situations - such as if he\u2019s just had lunch, just had a large insulin dose to cover it, and recess afterwards. There\u2019s also playing at the playground, or playing soccer, where he shouldn\u2019t have to run around with "
  },
  {
    "title": "Etleap (YC W13) Is Hiring a Senior Software Engineer (SF)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-01-30T01:02:03 1738198923",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "item?id=42873532",
    "first_paragraph": ""
  },
  {
    "title": "OpenAI says it has evidence DeepSeek used its model to train competitor (ft.com)",
    "points": 399,
    "submitter": "timsuchanek",
    "submit_time": "2025-01-29T04:21:20 1738124480",
    "num_comments": 1176,
    "comments_url": "https://news.ycombinator.com/item?id=42861475",
    "comments": [
      "https://archive.is/KiSYM",
      "I think there's two different things going on here:\"DeepSeek trained on our outputs and that's not fair because those outputs are ours, and you shouldn't take other peoples' data!\" This is obviously extremely silly, because that's exactly how OpenAI got all of its training data in the first place - by scraping other peoples' data off the internet.\"DeepSeek trained on our outputs, and so their claims of replicating o1-level performance from scratch are not really true\" This is at least plausibly a valid claim. The DeepSeek R1 paper shows that distillation is really powerful (e.g. they show Llama models get a huge boost by finetuning on R1 outputs), and if it were the case that DeepSeek were using a bunch of o1 outputs to train their model, that would legitimately cast doubt on the narrative of training efficiency. But that's a separate question from whether it's somehow unethical to use OpenAI's data the same way OpenAI uses everyone else's data.\n \nreply",
      "OpenAI is taking the position similar to that if you sell a cook book, people are not allowed to teach the recipes to their kids, or make better versions of them.That is absurd.Copyright law is designed to strike a balance between two issues. One the one hand, the creator\u2019s personality that\u2019s baked into the specific form of expression. And on the other hand, society\u2019s interest in ideas being circulated, improved and combined for the common good.OpenAI built on the shoulders of almost every person that wrote text on a website, authored a book, or shared a video online. Now others build on the shoulders of OpenAI. How should the former be legal but not the latter?Can\u2019t have it both ways, Sam.(IAAL, for what it\u2019s worth.)\n \nreply",
      "As another attorney, I would impart some more wisdom:\"Karma's a bitch, ain't it.\"\n \nreply",
      "I quite agree. The NY Times must be feeling a lot of schadenfreude right now.\n \nreply",
      "You can't copyright AI generated works. OpenAI are barking up the wrong tree.\n \nreply",
      "They're not making a legal claim, they're trying to establish provenance over Deepseek in the public eye.\n \nreply",
      "Yes; and trying to justify their own valuation by pointing out that Deepseek cost more than advertised to create if you count in the cost of creating OpenAI's model.\n \nreply",
      "It still doesn\u2019t justify their valuation because it shows that their product is unprotectable.In fact, I\u2019d argue this is even worse, because no matter how much OpenAI improves their product, and Altman is prancing around claiming to need $7Trillion to improve their product, someone else can replicate it for a few million.",
      "Though I also think it\u2019s extremely bad for OpenAI\u2019s valuation.If you give me $500B to train the best model in the world, and then a couple people at a hedge fund in China can use my API to train a model that\u2019s almost equal for a tiny fraction of what I paid, then it appears to be outrageously foolish to build new frontier models.The only financial move that makes sense is to wait for someone else to burn hundreds of billions building a better model, and then clone it. OpenAI primarily exists to do one of the most foolish things you can possibly do with money. Seems like a really bad deal for investors.\n \nreply"
    ],
    "link": "https://www.ft.com/content/a0dfedd1-5255-4fa9-8ccc-1fe01de87ea6",
    "first_paragraph": "Get 2 months free with an annual subscription at was $59.88 now $49.\nAccess to eight fresh articles a day, hand-picked by senior editors. Selected to feed your curiosity.Then $75 per month. Complete digital access to quality FT journalism. Cancel anytime during your trial.Save now on essential digital access to quality FT journalism on any device.Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.Terms & Conditions applySee why over a million readers pay to read the Financial Times."
  },
  {
    "title": "Advice for a friend who wants to start a blog (henrikkarlsson.xyz)",
    "points": 47,
    "submitter": "jger15",
    "submit_time": "2025-01-29T22:45:14 1738190714",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=42872276",
    "comments": [
      "The best writing advice I ever got was from a college professor\u2014\"everybody's got 100 bad essays in them. You just have to get them out.\"I feel like this is what blogging is especially good for. You can clear out the awful stuff and then try to incrementally improve.I would diametrically disagree with point #7\u2014if you want to write well, you need to revise the hell out of it. I guess pick whose writing you like better, between me and the author, and take the corresponding advice.\n \nreply",
      "yes; quantity leads to quality\n \nreply",
      "It\u2019s better to get those bad essays out in the form of comments, you get instant feedback and you\u2019ll know what works and what doesn\u2019t work when you go to write a real blog.\n \nreply",
      "> I should add, for context, that my friend and I are talking about writing beautiful essays here. If you want to write the most precise thing possible, you need to edit mercilessly and accept that the writing ends up flat and disjointed.I don't think that merciless editing and beauty are opposed. The author perhaps underestimates the extent to which artists mercilessly edit their own work. The author employs the metaphor of musical improvisation, but the best musical pieces are usually composed, not improvised. I like jazz, but... not for the songwriting. For stream of consciousness writing, you'd almost be better off posting on social media.I've been blogging for over 15 years, and my advice, admittedly ironic, is to not take advice from anyone. Actually, that's my advice on many subjects. Everyone wants to impose their own little idiosyncrasies on you, but what works for them won't necessarily work for you. One's idiosyncrasies are not universal truths.\n \nreply",
      "I am getting started writing a blog, and this article is some of the most helpful I've read.Makes me feel OK to be quirky and unpolished when the rest of the world is making reaction faces and pointing at text to feed an algo.\n \nreply",
      "Thank you for not doing that.\n \nreply",
      "Do they have something to say, or is this like \"I want to be in a band\" of a previous generation?\n \nreply",
      "This is a question more people need to ask. Be it a blog, YouTube channel, podcast, or whatever else, they all need to be rooted in having something worth to sharing to some small piece of the world.Many people get caught up in thinking the blog, YouTube, or podcast is the thing, when it\u2019s just the thing that lets you share the real thing. It took me longer than I\u2019d like to admit to really integrate this lesson.\n \nreply",
      "I mostly write for future me.\n \nreply",
      ">  is this like \"I want to be in a band\" of a previous generation?People started blogs for that? I thought that was more the realm of supermarket bulletin boards next to the firewood flyer.Maybe I'm just from a previous previous generation.How can \"a friend\" post something on a supermarket bulletin board now that they don't seem to exist?\n \nreply"
    ],
    "link": "https://www.henrikkarlsson.xyz/p/start-a-blog",
    "first_paragraph": ""
  },
  {
    "title": "From C++ to Clojure: Jank language promises best of both (thenewstack.io)",
    "points": 55,
    "submitter": "Jeaye",
    "submit_time": "2025-01-29T21:56:11 1738187771",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=42871743",
    "comments": [
      "Related. Others?I quit my job to work on my programming language - https://news.ycombinator.com/item?id=42658898 - Jan 2025 (32 comments)Jank: Programming Language - https://news.ycombinator.com/item?id=42477992 - Dec 2024 (3 comments)Jank is now running on LLVM IR - https://news.ycombinator.com/item?id=42276672 - Nov 2024 (16 comments)Jank development update \u2013 Moving to LLVM IR - https://news.ycombinator.com/item?id=41845669 - Oct 2024 (49 comments)The Jank Language: LLVM Hosted Clojure - https://news.ycombinator.com/item?id=32493217 - Aug 2022 (79 comments)\n \nreply",
      "I'm very excited for Jank, I wish the creator well.If I could make one tiny plea, it would be to focus on tooling too, and ensuring the experience for someone trying Jank out is as smooth as possible. Don't assume everyone is already set up with paredit and can fire off emacs chords without a thought. I suspect that Jank will be of particular interest to C++ programmers, many of whom are used to a very different dev paradigm.The Clojure community has done a great job at trying to smooth out the rough edges of Lisp tooling, and ensure there are on-ramps for newcomers (e.g. things like Calva for VS Code). I hope Jank keeps this up, because those first impressions really do matter. I'd hate to watch people bounce off Jank because they get stuck on trying to figure emacs out, or because they get frustrated trying to keep parentheses matched in Notepad.\n \nreply",
      "You're absolutely right. To start with, I'm focusing on capturing and converting existing Clojure devs. However, there's a very large pool of C++ devs who could benefit from jank. As you said, jank not only needs to be ready for them, they need to be educated on lisps, functional programming, data-oriented programming, REPL-based interactive development, etc. It's not an easy learning curve right now, into Clojure.I aim to focus a good deal of time, post jank's launch, to create materials which enable this. Materials specifically targeted at OOP devs coming from the native space.\n \nreply",
      "This seems like an opportunity to plug the good work https://www.clojuriststogether.org/ do. They do good work. Plug concludes.Clojure - like many languages - seems to be benefiting a lot from the work the IDE people have been putting in to good infrastructure. A lot of the magic is being moved out of Emacs towards tools like Treesitter and the LSP server which are more platform independent. Not a huge amount to do with Clojure per say, but the Clojure ecosystem is benefiting a lot from it. The Emacs specific stuff is becoming very Emacs-specific (like paraedit, bless its socks).\n \nreply",
      "I'm not super familiar with this project, so forgive a bit of ignorance on this; what does this buy you over vanilla Clojure and native-image compiling with GraalVM.This is a genuine question, not meant to dismiss the project!ETA:Sorry, further down the article answered my question:> \u201cjank is also a good fit for any Clojure devs who want a lighter runtime without sacrificing JIT compilation, as they would if they used a Graal native image, or if they want easy access to native libraries for whatever reason.\u201d\n \nreply",
      "You can compile a native program. Not one that requires the JVM. And it also interops with a number of more native graphical tools (as opposed to having other use things that require/work-from java). To those coming to this thread who might not be familiar with Clojure: Clojure already can sit on top of javascript (as Clojurescript) and there was work making it work on top of, if I recall right, some of the C languages. But those latter ones don't seem to be anywhere near the capabilities of this project.\n \nreply",
      "As a long-time fan of Clojure (I've been using it to varying degrees since 2008 and it is my favorite programming language) I'm really excited about the interpretability it sounds like this will open up!!\n \nreply",
      "Looks similar to Clasp but implements Clojure instead of Common Lisp.https://clasp-developers.github.io/\n \nreply",
      "Very excited for jank. I've been hacking together my weekend projects in Common Lisp for roughly a year. I've been wanting to look at Clojure as a modern lisp, but I want no truck with the JVM.\n \nreply",
      "Curious what the story is for slotting Jank into gaming. Seems like a fit for Unreal, but I really love the Component-Based-Architecture of Unity. Would you get it into your Unity project the same was as getting normal C++ into your project?\n \nreply"
    ],
    "link": "https://thenewstack.io/from-c-to-clojure-new-language-promises-best-of-both/",
    "first_paragraph": ""
  },
  {
    "title": "An update on Dart macros and data serialization (medium.com/dartlang)",
    "points": 105,
    "submitter": "oltmang",
    "submit_time": "2025-01-29T22:08:30 1738188510",
    "num_comments": 81,
    "comments_url": "https://news.ycombinator.com/item?id=42871867",
    "comments": [
      "I always feel better about the stewardship of a project when you see a thoughtfully written reason for saying no to a feature, especially when there\u2019s already sunk cost. Props to the team.\n \nreply",
      "This is good news. The dart language has been getting more complicated without corresponding quality of life improvements. A first class record object without messing around with macros would be a great start.\n \nreply",
      "I might be missing something here, don't records already exist? https://dart.dev/language/records\n \nreply",
      "Why is it that many languages, at the start, don't have support for records/plain structs?\n \nreply",
      "Because implementing them is tedious, and you can always simulate them with simpler aggregation methods, or possibly lexical closures.When the language implementors start making larger programs, it will soon become apparent how the program organization is hampered without named, defined data structures.I didn't add structs to TXR Lisp until August 2015, a full six years from the start of the project. I don't remember it being all that much fun, except when I changed my mind about single inheritance and went multiple. The first commit for that was in December 2019.Another fun thing was inventing a macro system for defstruct, allowing new kinds of clauses to be written that can be used inside defstruct. Then using them to write a delegation mechanism in the form of :delegate and :mass-delegate clauses, whereby you can declare individual methods, or a swath of them, to delegate through another object.\n \nreply",
      "Because it's arguably syntactic sugar and, IMHO, it's worked out better for developers for Dart to model it as a 3rd party library problem. i.e. have a JSONSerializable protocol, and enable easy code generation by libraries.i.e. I annotate my models with @freezed, which also affords you config of the N different things people differ on with json (are fields snake case? camel case? pascal case?) and if a new N+1 became critical, I could hack it in myself in 2-4 hours.I'm interested to see how this'd integrate with the language while affording the same functionality. Or maybe that's the point: it won't, but you can always continue using the 3rd party libraries. But now it's built into the language, so it is easier to get from 0 to 1.\n \nreply",
      "> Runtime introspection (e.g., reflection) makes it difficult to perform the tree-shaking optimizations that allow us to generate smaller binaries.Does anyone have any more information on How Dart actually does Tree Shaking? And what is \"Tree Shakeable\"? This issue is still open on Github https://github.com/Dart-lang/sdk/issues/33920.I think this quote accurately sums things up> In fact the only references I can find anywhere to this feature is on the Dart2JS page:> Don\u2019t worry about the size of your app\u2019s included libraries. The dart2js tool performs tree shaking to omit unused classes, functions, methods, and so on. Just import the libraries you need, and let dart2js get rid of what you don\u2019t need.> This has led customers to wild assumptions around what is and what is not tree-shakeable, and without any clear guidance to patterns that allow or disallow tree-shaking. For example internally, many large applications chose to store configurable metadata in a hash-map:\n \nreply",
      "I don't have a full answer for you, but I know a little.  I've hacked on the Dart compiler some, but my relationship with Dart has mostly been as a creator of Flutter and briefly Eng Dir for the Dart project.Dart has multiple layers where it does tree shaking.The first one is when building the \"dill\" (dart intermediate language) file, which is essentially the \"front-end\" processing step of the compiler which takes .dart files and does amount of processing.  At that step things like entire unused libraries and classes are removed I believe.When compiling to an ahead of time compiled binary (e.g. for releasing to iOS or Android) Dart does additional steps where it collects a set of roots and walks from those roots to related objects in the graph and discards all the rest.  Not unlike a garbage collection.  There are several passes of this for different parts of the compile, including as Dart is even writing the binary it will drop things like class names for unused classes (but keep their id in the snapshot so as not to re-number all the other classes).I have no experience with tree shaking in the dart2js compiler, but there are experts on Discord who might be able to answer:\nhttps://github.com/flutter/flutter/blob/master/docs/contribu...What exactly all this means as a dev using Dart, I don't know.  In general I just assume the tree shaking works and ignore it. :)The Dart tech lead has done some writings, but none seem to cover the exact details of treeshaking:\nhttps://mrale.ph/dartvm/\nhttps://github.com/dart-lang/sdk/blob/main/runtime/docs/READ...\n \nreply",
      "Macros give their own kind of power, and it's a tough call to give that up for runtime hot-reloading.  Languages like Haxe have macros, but also have hot reloading capabilities that typically are supported in certain game frameworks. You probably don't want to mix them together, but it's also a good development process to have simpler compilation targets that enable more rapid R&D, and then save macros for larger/more comprehensive builds.https://haxe.org/manual/macro.htmlhttps://github.com/RblSb/KhaHotReload\n \nreply",
      "> Semantic introspection, unfortunately, turned out to introduce large compile-time costs which made it difficult to keep stateful hot reload hot.They must have done something wrong. Macros are expanded when you ahead-of-time compile your code, which doesn't take place in the run-time environment where you hot load, but in the build environment. It doesn't matter whether the macro are simple, or whether they can inspect lexical environments and look up type info and whatnot.Compile-time costs should never factor into hot reload, because the stuff being loaded should already be compiled.Maybe they aren't explaining it; there could be certain semantic problems preventing existing state from being re-used on what should be a hot reload.Macros create certain issues in reloading. If you change a macro such that the expansion requires different run-time support which is incompatible with existing expansions, you have problems. One option may be to reload all the code which depends on those macros, so that everything cuts over to the new run-time support. If you need to support a mixture: hot-reloaded modules using the new versions of the macros, side by side with code made using the old versions, then the old version of the run-time support has to coexist with the old.If the run-time support for the macros is something which manages state that needs to be preserved on reloads, then that can cause difficulties. The old and new macro expansions want to appear to be sharing the same state, not different silos.\n \nreply"
    ],
    "link": "https://medium.com/dartlang/an-update-on-dart-macros-data-serialization-06d3037d4f12",
    "first_paragraph": ""
  },
  {
    "title": "Parsing PDFs (and more) in Elixir using Rust (chriis.dev)",
    "points": 48,
    "submitter": "bustylasercanon",
    "submit_time": "2025-01-29T21:05:28 1738184728",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=42871143",
    "comments": [
      "I've been thinking a lot about how to accomplish various RAG things in Elixir (for LLM applications). PDF is one of the missing pieces, so glad to see work here. The really tricky part is not just parsing out the text (you can just call the pdftotext unix command line utility for that), but accurately pulling out things like complex tables, etc in a way that could be chunked/post processed in a useful way. I'd love to see something like Unstructured or Marker but in Rust (i.e., fast) that Elixir could NIF out to it. And maybe some kind of hybrid system that uses open llm models with vision capabilities. Ref:- https://github.com/Unstructured-IO/unstructured- https://github.com/VikParuchuri/marker\n \nreply",
      "Well derp, I should have read the linked extractous repo. This looks like the extract solution I've been after (see what I did there).https://github.com/yobix-ai/extractous\n \nreply",
      "Yeah I could maybe highlight how good that library is in here\n \nreply",
      "Hey, I'm the author of marker - thanks for sharing.  Most of the processing time is model inference right now. I've been retraining some models lately onto new architectures to improve speed (layout, tables, LaTeX OCR).We recently integrated gemini flash (via the --use_llm flag), which maybe moves us towards the \"hybrid system\" you mentioned.  Hoping to add support for other APIs soon, but focusing on improving quality/speed now.Happy to chat if anyone wants to talk about the difficulties of parsing PDFs, or has feedback - email in profile.\n \nreply",
      "Very cool, any plans for a dockerized API of marker similar to what Unstructured released? I know you have a very attractively priced serverless offering (https://www.datalab.to) but having something to develop against locally would be great (for those of us not in the Python world).\n \nreply",
      "It's on the list to build - been focusing on quality pretty heavily lately.\n \nreply",
      "FYI: your preview image from the html header meta tag is broken.\n \nreply",
      "Thanks! I need to fix that\n \nreply"
    ],
    "link": "https://www.chriis.dev/opinion/parsing-pdfs-in-elixir-using-rust",
    "first_paragraph": "Issue 014Jan 29, 2025 \u00b7 13 minute readHere's the thing about PDFs - they're complex beasts that require quite a bit of thinking to properly parse - they come in all shapes and sizes, and they can contain a lot of different types of data and formatting. 90% of the time, we just want to extract the text from the file, but that's not always easy - for the remaining 10%, well we won't be covering that in this blog post.If you've been in the Elixir world for long enough, you'll probably have tried to parse a PDF file and realised that it's not as easy as it seems. A quick look on the Elixir Forum will quickly show you that there is no simple way to do it.Most people will tell you to upload the file to S3 and use a Lambda to handle the contents. Offloading to AWS Lambda might seem elegant at first (\"Look, Ma, no dependencies!\"), but it comes with its own baggage:These aren't ideal solutions - and software engineering is already made more complicated than it needs to be at times - we don't ne"
  },
  {
    "title": "Soviet Shoe Factory Principle (c2.com)",
    "points": 76,
    "submitter": "dccoolgai",
    "submit_time": "2025-01-29T20:26:01 1738182361",
    "num_comments": 57,
    "comments_url": "https://news.ycombinator.com/item?id=42870690",
    "comments": [
      "> Not to me confused with the Soviet shoe factories that produced only one type and orientation of shoe - with the other half of the pair being shipped in from 500 miles away.This reminded me of something our anti-counterfeiting startup learned when integrating into manufacturing for a high-end outdoor apparel brand.There are efficiencies that the factory gets by, say, parallelizing work on the sleeves of a jacket, while other parts are also worked on.  But each batch of dyed outer material may have slight variation in color, no matter how hard they calibrate.So, their processes are designed to ensure that, when the jacket pieces come together, they all came from the same dyed batch of outer material.(There were many interesting tidbits like that, and those were just the ones relevant to us, in terms we could understand.  I'm sure the factory also knew a million other things.)Probably the Soviet shoes of TFA had worse problems than slightly mismatched color.\n \nreply",
      "> All features as found on the specification are included, but the user interface is awkward. The vendor can claim, \"we have all the features you need; thus, pick us.\"Describes every Enterprise software I've ever used.\n \nreply",
      "This is one of the reasons ML benchmarks are most useful soon after their creation. Once they become a target, they cease to meaningfully measure quality.The corollary for the Soviet shoe factory is you need incentives that propagate to guide production. You need the market to guide you in all the little details and you have to discover little by little what it is actually demanding.For ML, we need something similar if not the same.\n \nreply",
      "What gets measured gets done.But, Goodhart's Law \"When a measure becomes a target it ceases to be a good measure.\"\n \nreply",
      "A similar thing I've noticed in the last ~6 years is in regards to smartphones, and in particular their cameras.Android is pretty standardized these days, and even a fairly cheap Android phone will probably do most smartphone things (make calls, send texts, watch YouTube, etc.) reasonably well, but a corner that seems to be cut in cheaper phones is almost always the camera, and that kind of makes sense.If you're buying your phone online, you can look at the more-or-less objective specs of the phone like how much RAM it has, or how fast the CPU is, but you can't really tell how the photos will look [1], and as such it's easy to put a cheaper sensor or lens in there, especially in the cheaper phones which (I think) have lower margins.[1] Even if the listing has sample pictures, you don't know how representative they are of the actual experience; they could just have taken the sample photos with ideal lighting, with a tripod, in a controlled studio, for example.\n \nreply",
      "I realized exactly this after some frustration with a few phones. I found GSMArena that has extensive reviews including sample standardized photos.\n \nreply",
      "GSMArena is such a valuable resource.\n \nreply",
      "And with a history at that. They have reviews dating back to 2004. It's fun to check out what were the biggest concerns throughout the years.\n \nreply",
      "Yes, and those crappy sensors will still be 200 megapixels (which will be used as an advertising point), as most people don\u2019t realize that a fantastic 24 MP camera is better than a crummy 200 MP one.\n \nreply",
      "Those 200 MP sensors are 200 MP in name only. They're so small the individual pixels are smaller than the wavelength of light they're trying to capture. That's why when you use them, the max image size is usually around 50MP because binning is required physically\n \nreply"
    ],
    "link": "https://wiki.c2.com/?SovietShoeFactoryPrinciple",
    "first_paragraph": "javascript required to view this sitemeasured improvement in server performanceawesome incremental searchThis site uses features not available in older browsers."
  },
  {
    "title": "A major Postgres upgrade with zero downtime (instantdb.com)",
    "points": 90,
    "submitter": "stopachka",
    "submit_time": "2025-01-29T16:57:59 1738169879",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=42867657",
    "comments": [
      "This is impressive!  I know others are questioning the \"no downtime\" bit, but that is why service level objectives exist -- because it really depends on the customer experience.If you managed to have a cutover with no noticeable dip in business metrics (aka the users didn't notice) then I'd call that a no-downtime upgrade!Very clever on the improvement over Lyft's methods.  Thanks for the writeup.  Now maybe someone can get it down from 3 seconds of pausing. :)\n \nreply",
      "> then I'd call that a no-downtime upgrade!It'd be really convenient for me, well not me but others, if we could tell our customers this. However, those of us running DBaaS do have to offer an actual no-downtime upgrade.\n \nreply",
      "I have to wonder \u2013 are they using a connection pooler? I'm leaning towards no, since what they did in code can be natively done with PgBouncer, PgCat, et al. That would also explain the last footnote:> The big bottleneck is all the active connectionsFor anyone who is unaware, Postgres (and Aurora-compatible Postgres, which sucks but has a great marketing team) uses a process per connection, unlike MySQL (and others, I think) which use a thread per connection. This is inevitably the bottleneck at scale, long before anything else.I did feel for them here:> We couldn\u2019t create a blue-green deployment when the master DB had active replication slots. The AWS docs did not mention this. [emphasis mine]The docs also used to explicitly say that you could run limited DDL, like creating or dropping indices, on the Green DB. I found this to be untrue in practice, notified them, and I see they've since updated their docs. A painful problem to discover though, especially when it's a huge DB that took a long time to create the B/G in the first place.\n \nreply",
      "> are they using a connection poolerWe use Hikari [1] an in-process connection pooler. We didn't opt for pgbouncer at al, because we didn't want to add the extra infra yet.> since what they did in code can be natively done with PgBouncer, PgCat, et al.Can you point me to a reference I could look at, about doing a major version upgrade with PgBouncer et al? My understanding is that we would still need to write a script to switch masters, similar to what we wrote.> The big bottleneck is all the active connectionsThe active connections we were referring too were websocket connections; we haven't had problems with PG connections.Right now the algorithm we use to find affected queries and notify websockets starts to falter when the number of active websocket connections on one machine get too high. We're working on improving it in the coming weeks.I updated the footnote to clarify that it was about websocket connections.> I did feel for them here:Thank you! That part was definitely the most frustrating.[1] https://github.com/brettwooldridge/HikariCP\n \nreply",
      "I\u2019m not sure about a reference, other than their docs [0]. Basically, you\u2019d modify the config to point to the new servers, issue PAUSE to PgBouncer to gracefully drain connections, then RELOAD to pick up the new config, then RESUME to accept new traffic.This would result in client errors while paused, though, so perhaps not quite the same. To me, a few seconds of downtime is fine, but everyone has their own opinions. EDIT: you could of course also modify your client code (if it doesn\u2019t already) to gracefully retry connections, which would effectively make this zero downtime.ProxySQL (which I think now supports Postgres) has a global delay option where you can effectively make clients think that the query is just taking a long time; meanwhile, you can do the same sequence as outlined.If you had HA Bouncers (which hopefully you would), you could cheat a little as you eluded to in the post, and have one still allow read queries to hit the old DB while cutting over writes on the other one, so the impact wouldn\u2019t be as large.[0]: https://www.pgbouncer.org/usage.html\n \nreply",
      "Zero-downtime Postgres upgrades have been kind of normalized, at least in the environments I have been exposed to, with pgcat  https://github.com/postgresml/pgcat\n \nreply",
      "This is really cool/useful to know about - thanks for dropping the link!\n \nreply",
      "Is there some resource that explains how to do a major version upgrade with pgcat? Would love to take a look\n \nreply",
      "As far as I know, there is not. I could probably write something up.\n \nreply",
      "Pause all writes > let 16 to catch up > resume writes on 16Isn\u2019t that\u2026.. downtime? Unless you mean downtime to be only when reads are also not available.\n \nreply"
    ],
    "link": "https://www.instantdb.com/essays/pg_upgrade",
    "first_paragraph": "Right before Christmas we discovered that our Aurora Postgres instance needed a major version upgrade. We found a great essay by the Lyft team, showing how they ran their upgrade with about 7 minutes of downtime.We started with Lyft\u2019s checklist but made some changes, particularly with how we switched masters. In our process we got to 0 seconds of downtime.Doing a major version upgrade is stressful, and reading other\u2019s reports definitely helped us along the way. So we wanted to write an experience report of our own, in the hopes that it\u2019s as useful to you as reading others were for us.In this write-up we\u2019ll share the path we took \u2014 from false starts, to gotchas, to the steps that ultimately worked. Fair warning, our system runs at a modest scale. We have less than a terabyte of data, we read about 1.8 million tuples per second, and write about 500 tuples per second as of this writing. If you run at a much higher scale, this may be less relevant to you.With all that said, let\u2019s get into "
  },
  {
    "title": "Meta Reports Fourth Quarter and Full Year 2024 Results (atmeta.com)",
    "points": 35,
    "submitter": "mfiguiere",
    "submit_time": "2025-01-29T21:42:10 1738186930",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=42871574",
    "comments": [
      "It's really hard for me to understand how Meta makes so much money, and why the stock is growing so fast. I wonder how analysts come up with such high valuation (or maybe they don't and it's just an AI bubble?). Meta isn't diversified and their products are clunky and buggy. How long will they be able to maintain their profit? are they capable to come up with something revolutionary enough to replace their profitable products?\n \nreply",
      "Material news for observers of AI infrastructure spend given the DeepSeek market torpedo this week:CapEx Full-Year 2024: $39.23BExpected to increase to $60B-$65B in 2025, largely to support AI efforts.NVDA bumped after hours on this news about 1.6% at the moment after a down day\n \nreply",
      "Until I can run frontier models locally on modest hardware, the hardware race will continue.It\u2019s kind of like when 3d graphics came out. We all kind of knew \u2018the graphics are good, but not there yet\u2019.Eventually it does get to Crysis level graphics, and we all take a breather. But then it starts up again with Unreal 5 engine. We will get to photorealistic even if takes a 1000 years.Same goes for AGI.\n \nreply",
      "I think that\u2019s already been achieved \nhttps://youtu.be/o1sN1lB76EA?si=4nMZryeOeoVrZewW\n \nreply",
      "AGI is a very different level of problem to solve than ray tracing.Photorealism is basically a straight shot, the algorithms for light are well understood, we just haven't had the hardware to do it in realtime, but the hardware was relatively easy to iterate on.AGI is not the same. No, LLMs are not going to turn into an \"Artificial General Intelligence\" just because we keep improving them - LLMs inherently don't have the capability of thought, interest, reason, motivation, or anything we ascribe to human intelligence. And throwing faster hardware at the problem isn't going to solve that.Sure, people are going to keep trying but I doubt anyone alive today will be seeing a true artificial general intelligence. In 1000 years? That's quite the goalpost.\n \nreply",
      "NVDA looks down to me, in after hours, must have been a transitory bump.\n \nreply",
      "Transitory to mid-late twenties then by 7pm/early evening it was low one-twenties. Someone said something on a call or the major holders got out for some other reason.\n \nreply",
      "Seems like a nice optimization problem when you are at these CapEx scales.\n \nreply",
      "62 billion dollars a year in profit with revenue still growing....just nuts.\n \nreply",
      "Reality Labs lost another ~$15 billion over FY 2024, between that and the Vision Pro apparently falling short of Apples expectations it doesn't seem like VR is going anywhere fast.\n \nreply"
    ],
    "link": "https://investor.atmeta.com/investor-news/press-release-details/2025/Meta-Reports-Fourth-Quarter-and-Full-Year-2024-Results/default.aspx",
    "first_paragraph": ""
  },
  {
    "title": "Bacteria (and their metabolites) and depression (science.org)",
    "points": 382,
    "submitter": "Luc",
    "submit_time": "2025-01-29T09:54:33 1738144473",
    "num_comments": 170,
    "comments_url": "https://news.ycombinator.com/item?id=42863262",
    "comments": [
      "https://archive.is/QA7qZ",
      "I can\u2019t go into too much detail here, but I interrupted a severe depression by fasting for two straight weeks. It\u2019s hard to say why it worked \u2014 my friends assume that the microbiome is implicated because of the dietary change. Plausible, but hardly a proof. This gives me at least one direction to look in.\n \nreply",
      "As an alternate theory, I\u2019ve always thought that deliberate, controlled suffering gives you a mood boost. Or rather, the relief after the suffering has ended. The theory is that it\u2019s not good for us emotionally to be comfortable continuously. Maybe fasting is an example of that?\n \nreply",
      "Anecdotally, I've observed I feel better with a certain amount of difficulty in my life and felt particularly bad when there was nothing -wrong- but I still felt depressed. If you don't have any problems you can attribute the bad feelings to, then you naturally start to consider the possibility that the problem is you and you're broken in a way that means you're going to feel bad regardless of how much your life circumstances improve. Which is a particularly despair-inducing thought to have.Last year I cracked my hip joint and ended up in the hospital for a couple of weeks, doing physio to regain my ability to walk. I certainly don't want to repeat that experience but I was surprised that I felt less depressed during it, because there was clarity in what I immediately needed to do and I was focused on just getting through it, not existential angst.\n \nreply",
      "That\u2019s personality-specific. My Dad is like that and doesn\u2019t understand why nobody else in the family gets off on self-imposed austerity. (He\u2019s a great guy by the way \u2014 we all have our quirks.) He loves to say it\u2019s a lack of discipline, but it\u2019s not\u2014 I quit smoking after a decade-and-a-half of heavy smoking, lost 20% of my body weight on an 800 calorie per day ketogenic diet, worked many very difficult jobs, and did a number of other things that have required sustained discipline, as have others in the family. He doesn\u2019t understand that the difference is that he enjoys self-imposed misery and the feelings of superiority he gets from it, which not only leads to a lack of balance in his life, sometimes strains his closest relationships.\n \nreply",
      "> My Dad is like that and doesn\u2019t understand why nobody else in the family gets off on self-imposed austerity.My gut feeling: men of a certain generation seem to have confused the skill of seeming aloof-- like when meeting a stranger or thrust in a new situation-- for being emotionally unaffected in general. One sign I associate with this would be talking only in the past tense about having felt certain emotions, but you never really witness the person feeling or expressing those emotions (outside of anger/frustration). Alternatively, the person may never really engage in discussions about certain negative feelings, unless it's to offer low-effort problem-solving advice to others.This is difficult because there are obviously times when a small, seemingly insignificant problem can trigger an outsized emotion. It's natural and helpful to be able to sit with that negative emotion-- to feel it, express it and talk about it as an emotion you are experiencing-- to be able to eventually come to terms and get to know better whatever it is that's driving it. It's scary to do, but most people have some techniques for doing it.If you have few or no tools to do that, it must generate an immense amount of stress. Hence wacky alternative stress-relieving techniques that are more physical and less emotional in nature. (Plus the projection of \"lack of discipline\" makes me wonder if he'd feel shame from sitting with a negative emotion.)\n \nreply",
      "I\u2019ve seen that in people though I don\u2019t think it really describes him, specifically. He just really enjoys a lot of things in life the way some people really love getting the most punishing workouts possible, and would probably do it even if there were no health benefits.\n \nreply",
      "I don't find it difficult to talk about negative feelings, as long as no one is listening. Talking to other people is mostly useless these days, they're all busy hiding from themselves.\n \nreply",
      "+1 insightfulA good technique to help w this is \"affective labeling\" (sorry, citation needed, I think I got it from one of Anne Laure Le Cunff's typically awesome newsletters/posts). Set a 5m timer, start writing words that describe your emotions and don't stop writing for even 2s till the 5m are up.\n \nreply",
      "This reads like a description of someone with an avoidant attachment style.\n \nreply"
    ],
    "link": "https://www.science.org/content/blog-post/bacteria-and-their-metabolites-and-depression",
    "first_paragraph": ""
  },
  {
    "title": "Keymapper: A cross-platform context-aware key remapper (github.com/houmain)",
    "points": 36,
    "submitter": "todsacerdoti",
    "submit_time": "2025-01-29T20:55:49 1738184149",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=42871040",
    "comments": [
      "I wonder if this will finally give a solid, cross-platform way to make the Caps Lock key work as it should: press for escape, hold for control.\n \nreply",
      "the easiest cross platform way is to use qmk and flash some firmware on the keyboard itself to map that. tons of software tools for this now\n \nreply",
      "That's what i did. It's worked super well. Finding a keyboard i liked that worked with that was the hardest part. As it is my keyboard's brand (Keychron) had support in QMK, but the model did not. I had to hack a bit to get there, but in the end it worked.Sad thing is that was two years ago and i've forgotten how to flash it lol.I can't wait until something like Rust's QMK (i think RMK?) starts to work on more boards. My board having complex modes but the ease of it being written in Rust (because i love Rust) would be a dream for me.If i could i'd write those type of Rust-QMK for both my Keyboard and Mouse. It really feels great\n \nreply",
      "May I suggest the absolutely awesome Extend Layer [0] instead?Arrow keys, advanced cursor movement and modifiers chording, and more, without leaving the home row![0] https://dreymar.colemak.org/layers-extend.html\n \nreply",
      "Does anyone know why the interception (closed source) driver is the only way to differentiate between keyboards.I understand that Windows normally hides that information. My question is: What does interception do that someone else can't reinvent?\n \nreply",
      "Interception is LGPL[1], the issue is that Windows requires drivers to be signed and code signing certs aren't cheap. That's also why interception has not updated in years to fix the bug where it only supports 10 devices to be connected each boot (each reconnects count as a separate device)[2].[1]: https://github.com/oblitum/Interception/blob/master/licenses...[2]: https://github.com/oblitum/Interception/issues/193\n \nreply",
      "> Interception is LGPLThe library is LGPL for non commercial purposes. A contradiction. Drivers and installers are called binary assets. Access to all source code is provided with a commercial license.[1][1] https://github.com/oblitum/Interception/blob/master/README.m...\n \nreply",
      "Oh right, I keep forgetting that. The LGPL part is just the library that lets apps talk to the driver.I still think the main thing Interception has going for it is code signing. I once wrote a keyboard filter driver for personal use, but due to signing requirements I never felt it was worth the trouble & cost trying to use it past the computer I developed it on (and then later moved to firmware based solutions like QMK).\n \nreply",
      "The LGPL license applies to the code that they do provide. It's kinda misleading but you have the option to pay and get the code, or not pay and only get what you see. Seems fair enough.\n \nreply",
      "Cool. I run Windows, MacOS, and Linux on a regular basis. Been looking for ways to use F-keys 13 through 24 to enable useful functionality. Maybe keymapper can help.\n \nreply"
    ],
    "link": "https://github.com/houmain/keymapper",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A cross-platform context-aware key remapper.\n      \n\n\n\n\nConfiguration |\nExample |\nFunctional principle |\nkeymapperctl |\nInstallation |\nBuilding |\nChangelogA cross-platform context-aware key remapper. It allows to:Configuration files are easily written by hand and mostly consist of lines with input expressions and corresponding output expressions separated by >>:Unless overridden using the command line argument -c, the configuration is read from keymapper.conf, which is looked for in the common places:each with an optional keymapper subdirectory and finally in the working directory.The command line argument -u causes the configuration to be automatically reloaded whenever the configuration file changes.\u26a0\ufe0f In case of emergency: You can always press the special key combination Shift+Escape+K to terminate keymapperd.The keys are named a"
  },
  {
    "title": "Cognitive Reasoning Agents and the Extended Information Filter (jdsemrau.substack.com)",
    "points": 13,
    "submitter": "Brajeshwar",
    "submit_time": "2025-01-26T15:47:19 1737906439",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://jdsemrau.substack.com/p/cognitive-reasoning-agents-and-the",
    "first_paragraph": ""
  },
  {
    "title": "Case Study: ByteDance Uses eBPF to Enhance Networking Performance (ebpf.foundation)",
    "points": 101,
    "submitter": "ChrisArchitect",
    "submit_time": "2025-01-29T15:58:20 1738166300",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=42866572",
    "comments": [
      "Netkit, which is what this is built on, is pretty neat. For transmitting packets from one container/VM to another, the conventional solution is to give each its own veth device. When you do that, the kernel network stack, at like the broad logic level, is sort of oblivious to the fact that the devices aren't real ethernet devices and don't have to go through the ethernet motions to transact.Netkit replaces that logic with a simple pairing of sending and receiving eBPF programs; it's an eBPF cut-through for packet-level networking between networks that share a host kernel. It's faster, and it's simpler to reason about; the netkit.c code is pretty easy to read straight through.\n \nreply",
      ">When you do that, the kernel network stack, at like the broad logic level, is sort of oblivious to the fact that the devices aren't real ethernet devices and don't have to go through the ethernet motions to transact.Is that true even for virtio-net? I guess I just assumed all these virtual devices worked like virtiofs and had low overhead fast paths for host and guest communication.\n \nreply",
      "Yeah this is a surprise to me too - my impression was things like loopback and virtio devices were used explicitly because they don't pretend to ever be real devices, and thus bypass all the real device handling.What additional overhead is cut out by the netkit approach?\n \nreply",
      "Are you using virtual machines? They're not.The big win here as I understand it is that it gives you roughly the same efficient inter-device forwarding path that XDP gives you: you can bounce from one interface to another in eBPF without converting buffers back into skbuffs and snaking them through the stack again.\n \nreply",
      "It would be nice to see an implementation of TCP fusion (on Solaris) or SIO_LOOPBACK_FASTPATH (on Windows) for Linux.\n \nreply",
      "Came here to say the same. I'm glad linux is finally catching up to Solaris.\n \nreply",
      "Thanks for the clear explanation!\n \nreply",
      "I'd love to see a more complete picture of ByteDance's TikTok infra. They released \"KubeAdmiral\" (1) so I'm assuming they're using eBPF via a Kubernetes CNI, and I see ByteDance listed on Cilium's github (2). They're also using KubeRay (3) to orchestrate huge inference tasks. It's annoying that a company I definitely do not want to work for has such an incredibly interesting infrastructure!1. https://github.com/kubewharf/kubeadmiral2. https://github.com/cilium/cilium/blob/main/USERS.md3. https://www.anyscale.com/blog/how-bytedance-scales-offline-i...\n \nreply",
      "I also heard they replace k8s etcd with a shim [0] similar to kine because their clusters are so large.[0] - https://github.com/kubewharf/kubebrain\n \nreply",
      "They also made monoio, an io-uring based async runtime for Rust: https://github.com/bytedance/monoio\n \nreply"
    ],
    "link": "https://ebpf.foundation/case-study-bytedance-uses-ebpf-to-enhance-networking-performance/",
    "first_paragraph": "Download this case study in PDF formatBytedance, a global technology company operating a wide range of content platforms around the world at massive scale, faced significant challenges in ensuring performance and stability across its data centers. With over a million servers running containerized applications, the company required a networking solution that could handle high throughput while maintaining stability. By leveraging eBPF technology, Bytedance successfully implemented a decentralized networking solution that improved efficiency, scalability, and performance.As Bytedance scaled its operations, its existing container networking solution, which relied on virtual Ethernet devices, began showing limitations. Key challenges included:To address these challenges, Bytedance required a robust, scalable, and high-performance solution that could be deployed incrementally across its data centers.Bytedance turned to eBPF to address these challenges. eBPF, a powerful technology for dynamic"
  },
  {
    "title": "Airflow \u2013 Stream media files directly from macOS to AirPlay devices (airflow.app)",
    "points": 57,
    "submitter": "tiagod",
    "submit_time": "2025-01-29T19:47:51 1738180071",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=42870171",
    "comments": [
      "Airflow is by far the fastest and most reliable AirPlay implementation I've seen. It's lightning fast. Truly excellent. The app is also super fast for things like video scrubbing.I wish the app was a more native feeling UI, and I feel they could be doing deeper integration to enable more video playing. I'm a little worried that the app is out of development, but hoping they keep it going for new versions of macOS and AirPlay.\n \nreply",
      ">I'm a little worried that the app is out of developmentWhy? The last app release was Jan 3rd 2025, so about 3 weeks ago.But even if it was older, once an app fulfills is purpose and is free of bugs and security holes, does it still need to be updated constantly?\n \nreply",
      "More often I see two or maybe even more software products with the same name. In this case Apache Airflow. The two products serve a totally different purpose. I guess for marketing it would be better to have a unique name.\n \nreply",
      "I think it's beneficial for the product's name to be similar to AirDrop. How popular is Apache Airflow? Looking at their website it seems to be a dead product.\n \nreply",
      "It's in use in _a lot_ of data pipelines.\n \nreply",
      "It's not even remotely close to a dead product. I work in platform-layer consulting where the services are quite often for customers who run ML and other data pipelines and Airflow is overwhelmingly the most popular DAG orchestration tool they're using.I'm very curious what even gives you that impression. If you follow the link to their Github, it's got 38.5k stars, 14.6k forks, 3190 contributors, 92 releases, the most recent commit is from an hour ago. The most recent release is from last month.\n \nreply",
      "Plus everybody on Cloud Composer.\n \nreply",
      "Still, it was only one Google search away.\n \nreply",
      "it\u2019s extremely popular\n \nreply",
      "Is this a joke? I'd put Airflow almost on the level of postgres in terms of popularity (it's the most popular workflow manager).\n \nreply"
    ],
    "link": "https://airflow.app/",
    "first_paragraph": "Airflow is different We're not cutting any corners. This is not yet another FFmpeg wrapper like you might have seen elsewhere. Don't get us wrong, we love FFmpeg and use many of its parts under the hood, but our custom built video processing pipeline goes way beyond wrapping FFmpeg and calling it a day. We've been working on it for years it and it lets us do things that other similar software simply can't.It's a bold claim for sure, so here are just a few examples:Airflow is a razor sharp focused software. It supports specific set of devices and it will pull every trick in the book to get the best possible results on these devices. It may not stream video to your smart fridge, but it will gladly push your Chromecast, Apple TV and AirPlay 2 TVs to their limits.And yes, Airflow can handle pretty much any video format and codec you throw at it.Airflow can stream full 4K HDR HEVC files to Chromecast Ultra, Built-in, Apple TV 4K and AirPlay 2 enabled TVs. It will go out of its way not to to"
  },
  {
    "title": "Asteroid Impact on Earth 2032 with Probability 1% and 8Mt Energy (nasa.gov)",
    "points": 190,
    "submitter": "2-3-7-43-1807",
    "submit_time": "2025-01-29T12:47:17 1738154837",
    "num_comments": 173,
    "comments_url": "https://news.ycombinator.com/item?id=42864272",
    "comments": [
      "I work on the Near Earth Object Surveyor space telescope (NEO Surveyor) writing simulation code which predicts which objects we will see. \nThis one has drummed up a bit of interest due to its (relatively) high chance of impact. I actually spent quite a bit of time yesterday digging through archive images trying trying to see if it was spotted on some previous times it came by the Earth (no luck unfortunately). Since we saw it so briefly, our knowledge of its orbit is not that great, and running the clock back to 2016 for example ended up with a large chunk of sky where it could have been, and it is quite small. We will almost certainly see it again with NEO Surveyor years before its 2032 close encounter. I have not run a simulation for it, but I would not be surprised if LSST (a large ground telescope survey which is currently coming online) to catch it around the same time NEO Surveyor does.Our knowledge of the diameter of this object is a bit fuzzy, because of surface reflectivity, small shiny things can appear as bright as dark large things. This is one of the motivations of making the NEO Surveyor an IR telescope, since IR we see black body emission off of the objects, which is mostly size dependent, and only weakly albedo dependent.There is an even tinier chance that if it misses the Earth in 2032, it could hit the moon. I haven't run the numbers precisely for that one, but it impacted a few times in some monte-carlo simulations.If anyone is interested in orbit dynamics, I have open sourced some of the engine we are using for observation predictions: https://github.com/Caltech-IPAC/keteIt is relatively high precision, though JPL Horizons has more accurate gravitational models and is far better suited for impact studies. My code is primarily for predicting when objects will be seen in telescopes.\n \nreply",
      "Great post, thank you!Where does the uncertainty (1%) come from? For example, is it more from our ability to precisely determine the orbit based on limited observations, or is it because orbits for objects like this just aren't predictable years out, or something else?\n \nreply",
      "It's a bit of both, observing has uncertainty in a lot of places, if you are on the ground you get atmospheric effects, imprecision of timing, imprecision of optics, etc etc. You are also observing an object where you dont know how far away it is. That distance has to be solved by basically doing a sort of triangulation, which requires either the observer or the object to move enough.\nSo if you observe over a short time (hours for example), you can see it is moving, but it is hard to tell distance.Once you have an estimated orbit, if it has any interactions with planets (IE: flyby of Earth), small differences in positions during the close encounter make LARGE differences decades later. Add to this the effects of photons from the sun pushing on the smaller asteroids or dust, or out-gassing /dust from comets cause these objects to slightly drift from just the basic gravitational forces. Generally inner solar system asteroids (inside mars) are very chaotic over hundreds of years, though typically predictable less than a century.Note that I am not an expert on impact calculations, I just know a bit about and and can do back of the envelope ones.\nThere are a number of ways to get to the ~1%, the orbit fits have uncertainties on them and those can be propagated forward in time. However there are all sorts of complexities with doing that, and often the easiest method is to sample the uncertainty region a few hundred thousand times (Monte-carlo), and propagate those and see what hits.\n \nreply",
      "My guess is that small objects like this suffer greatly from the 3-body problem, and multiple trajectories are generated from various starting points inside our measured error bars for the current states of these objects.  Small inaccuracies compound over the years.\n \nreply",
      "Parent mentioned Monte Carlo simulations, which allow you to simulate across a range possible scenario parameters and see what % result in some outcome (like a collision with Earth or the moon).\n \nreply",
      "I'm sure there's a reason, but it seems like an unusual use of Monte Carlo - it's all deterministic and there is no opposing player making choices. Must have something to do with uncertainties in projected orbits or imperfect simulations maybe?\n \nreply",
      "The observations are not 100% certain. There are a variety of body states and configurations that might result in the same (few) pixels being lit up in the few measurements collected so far. As additional measurements are collected, some possibilities may be eliminated and the uncertainty of the trajectory can be reduced. This usually results in the impact probability converging toward 0%.\n \nreply",
      "...or 100%. But yeah, the MC comes in this way. You have a current most probable value for the position and some distribution around it, depending on the precision of the measurement device etc. That can be a high-dimensional space. You draw some (many) random points from this space and propagate them all deterministically. Taking into account how likely a certain random point was in the first place, you can then estimate the hit probability.MC is numerically approximating an integral. Here it replaces the high-dim integral over the start parameters.\n \nreply",
      ">it's all deterministic and there is no opposing player making choicesIt's not deterministic, it's chaotic. That is the nature of the N-body problem. We can only approximate trajectories in such a system using numerical methods, within a certain margin of error. In principle, the object is gravitationally interacting with everything else in the solar system. But for the most part, most interactions are negligible and could be ignored (eg, other small objects far away), except of the large bodies. But there are many unknowns (as stated before), where initial conditions will affect the outcome of the trajectory simulation, and errors will certainly amplify over time. I'm guessing Monte Carlo is used to \"fuzz\" the simulations with randomised initial conditions to account for the range of unknowns, and see what the outcome is under these different scenarios.\n \nreply",
      "The guesswork is uncertainity about the object's exact paraneters. Because of this they have to use informed estimates (scientiffic guesswork).\n \nreply"
    ],
    "link": "https://cneos.jpl.nasa.gov/sentry/details.html#?des=2024%20YR4",
    "first_paragraph": "The following summary tables includes basic information about the hazard for this object.\nThe maximum Torino and Palermo Scale values are listed, as well as the number of tabulated\npotential impacts and their corresponding cumulative Palermo Scale value and cumulative\nimpact probability (shown in the the first table).\nCertain parameter values depend upon the specific impact event in question,\nbut they change little among the various table entries.\nFor this reason we tabulate only mean values for these parameters (shown in the second table).\nThe observation set used for the analysis is also listed.\nUse the \"Print\" button above to print data contained in this table.\nUse the \"CSV\" or \"Excel\" buttons to download the data for use in your spreadsheet program.\nAllow a few seconds for downloads of large datasets.\n\nNO OBJECT SELECTED\n        Please go to the Data Table to select an object.\n      "
  }
]