[
  {
    "title": "Beej's Guide to Git (beej.us)",
    "points": 97,
    "submitter": "mixto",
    "submit_time": "2025-02-05T00:07:24 1738714044",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=42941283",
    "comments": [
      "I remember reading the excellent Beej's Guide to Network Programming[0] and Beej's Guide to Unix IPC[1] as a teenager, which were incredibly approachable while still having depth\u2014fantastic reads both and very influential on the programmer I ended up being.[0] https://beej.us/guide/bgnet/\n[1] https://beej.us/guide/bggit/\n \nreply",
      "Well, what's terrifying is that the guide is so long.I am aware that beej's guides are typically quite comprehensive, but the vast nuances of git truly eluded me until this.I guess Jujitsu would wind up being a much slimmer guide, or at least one that would be discoverable largely by humans?\n \nreply",
      "I have nothing but fond memories of reading Beej's guides.It's also this sort of work that's becoming less necessary with AI, for better or worse. This appears to be a crazy good guide, but I bet asking e.g. Claude to teach you about git (specific concepts or generate the whole guide outline and go wide on it) would be at least as good.\n \nreply",
      "Seems more efficient to have one reference book rather than generating entire new 20 chapter books for every person.I also think if you are at the \u201cdon\u2019t know what you don\u2019t know\u201d point of learning a topic it\u2019s very hard to direct an AI to generate comprehensive learning material.\n \nreply",
      "True although the don't know aspect is where LLMs will be magic. I envy today's youth for having them (and I'm not that old at all)I remember fumbling around for ages when I first started coding trying to work out how to save data from my programs. Obviously I wanted a file but 13 year old me took a surprisingly long time to work that out.Almost impossible to imagine with AI on hand but we will see more slop-merchants.\n \nreply",
      "Definitely more efficient in terms of power consumed, not so in terms of human effort to build such guides across nearly every topic one could think of. But you're right, we shouldn't ignore the power consumption.I have found that asking AI \"You are an expert teacher in X. I'd like to learn about X, where should I start?\" is actually wildly effective.\n \nreply",
      "I don't disagree, but since the quality of AI is largely a function of the quality of human content, there's always going to be value in well-written human content. If humans stop producing content, I think the value of AI/LLMs drop significantly as well.\n \nreply",
      "The Beej books are awesome.  I remember reading these and watching the 3DBuzz video tutorials when I was first learning how to program.\n \nreply",
      "Initial impressions: Looks great.As a cloud security analyst that is thinking of going back to coding or DevSecOps, if I'm honest with myself, there is nothing new here that I have not seen before... (This is not a criticism or anything. If anything the problem is myself: if I can allocate time to learn this or use Anki to retain this).\n \nreply",
      "I'm a fan of Beej's writing style.\n \nreply"
    ],
    "link": "https://beej.us/guide/bggit/",
    "first_paragraph": "\n\n\nBeej's Guide to Git\nPlease keep in mind that I'm only human and there is a\n            very, very high probability that there are errors in this\n            guide. Additionally, I might simply not know what I'm\n            talking about when it comes to something! So email corrections are highly\n            appreciated!\n\n\t\t\t(Click here for other guides!)\n\n\t\t\tWhat's Here for Readers:\n\n\nHTML:\n\nHTML\nHTML, widescreen\n\nHTML, single page\nHTML, single page, widescreen\n\nHTML ZIP\nHTML ZIP, widescreen\n\n\nPDF:\n\nUS Letter, one sided, syntax highlighting\nUS Letter, two sided, syntax highlighting\nA4, one sided, syntax highlighting\nA4, two sided, syntax highlighting\nUS Letter, one sided, black and white\nUS Letter, two sided, black and white\nA4, one sided, black and white\nA4, two sided, black and white\n\n\n\n\nWhat's Here for Translators and Writers:\nClone the whole thing from GitHub and follow the\n\t\t\tREADME.\n\n\t\t\t\n\t\t\tContact Beej: beej@beej.us\n\nPlease keep in mind that I'm only human and there is a\n    "
  },
  {
    "title": "WikiTok (wikitok.vercel.app)",
    "points": 818,
    "submitter": "Group_B",
    "submit_time": "2025-02-04T18:40:55 1738694455",
    "num_comments": 145,
    "comments_url": "https://news.ycombinator.com/item?id=42936723",
    "comments": [
      "Hi! I'm the dev here! I built this on a whim at after seeing someone ask for it on twitter. It was 12:30 at night but I couldn't pass down the opportunity to build it.The code is very simple, there's no backend at all actually, I believe because wikipedia's api is very permissive and you can just make the requests in the frontend. So you just simply request random articles, get some snippets, and the image attached!I used Claude and cursor do 90% of the heavy lifting, so I am positive there's plenty of room for optimizations. But right now as it stands, it's quite fun to play with, even without anything very sophisticated.Here is the source code.\nhttps://github.com/IsaacGemal/wikitok\n \nreply",
      "Shoutout to APIs that do not enforce CORS preventing requests be made from FE without a need for a BE. There's so many toy apps I started building that would have just worked if this was more common, but they have CORS restrictions requiring me to spin up a BE which for many one-off tools and personal tools just isn't worth doing and maintaining. Same with OAuth.\n \nreply",
      "nit: same-origin policy is the restriction. CORS isn't the restriction, it's the thing that helps you. CORS is the solution, not the problem.\n \nreply",
      "I kind of miss the era of JSON-P supported APIs. Feels like such a weird little moment in time.\n \nreply",
      "There's many services to solve this pain point. I've used https://allorigins.win/ in the past.\n \nreply",
      "Oh this looks neat!\n \nreply",
      "The only caveat I feel is that the speed of the API is definitely not comparable to something more purpose built for this kind of scale, but overall I'm happy as it works well enough that I don't have to think about it too hard.\n \nreply",
      "I think Github Actions could be used for scheduled builds, so that the initial load would have random articles right in. Further requests could then be made in advance so users would not notice any delay from the API.\n \nreply",
      "Do you have any examples of that I can look at as a reference? I'm used to github actions just being my CI/CD build step checking tool.\n \nreply",
      "Many platforms can enable proxying through their service to avoid CORS issues: https://pico.sh/pgs#proxy-to-another-service\n \nreply"
    ],
    "link": "https://wikitok.vercel.app/",
    "first_paragraph": ""
  },
  {
    "title": "Open Deep Research (github.com/huggingface)",
    "points": 192,
    "submitter": "transpute",
    "submit_time": "2025-02-04T19:55:27 1738698927",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=42937701",
    "comments": [
      "https://techcrunch.com/2025/02/04/hugging-face-researchers-a...> On GAIA, a benchmark for general AI assistants, Open Deep Research achieves a score of 54%. That\u2019s compared with OpenAI deep research\u2019s score of 67.36%..Worth noting is that there are a number of OpenAI deep research \u201creproductions\u201d on the web, some of which rely on open models and tooling. The crucial component they \u2014 and Open Deep Research \u2014 lack is o3, the model underpinning deep research.Blog post, https://huggingface.co/blog/open-deep-research\n \nreply",
      "theres always a lot of openTHING clones of THING after THING is announced. they all usually (not always[1]!) disappoint/dont get traction. i think the causes are1. running things in production/self hosting is more annoying than just paying like 20-200/month2. openTHING makers often overhype their superficial repros (\"I cloned Perplexity in a weekend! haha! these VCs are clowns!\") and trivializing the last mile, most particularly in this case...3. long horizon planning trained with RL in a tight loop that is not available in the open (yes, even with deepseek). the thing that makes OAI work as a product+research company is that products are never launched without first establishing a \"prompted baseline\" and then finetuning the model from there (we covered this process in https://latent.space/p/karina recently) - which becomes an evals/dataset suite that eventually gets merged in once performance impacts stabilize4. that said, smolagents and HF are awesome and I like that they are always this on the ball. how does this make money for HF?---[1]: i think opendevin/allhands is a pretty decent competitor to devin now\n \nreply",
      "> RL in a tight loop that is not available in the openCompletely agree that a real RL pipeline is needed here, not just some clever prompting in a loop.That being said, it wouldn\u2019t be impossible to create a \u201cgym\u201d for this task. You are essentially creating a simulated internet. And hiding a needle is a lot easier than finding it.\n \nreply",
      "I think models will have to some kind of internal training to teach them they are agents that can come back and work on things.Working on complex problems tends to explode in to a web of things that needs done. You need to be able to separate these in to subtasks and work on them semi-independently. In addition when a subtask gets stuck in a loop, you need to work on another task or line of thought, and then come back and 're-run' your thinking to see if anything changed.\n \nreply",
      "Maybe these open projects start to get more attention when we have a distribution system/App Store for AI projects. I know YC is looking to fund this https://www.ycombinator.com/rfs\n \nreply",
      "Is \"AI appstore\" envisioned on Linux edge inference hardware, e.g. PC+NPU, PC+GPU, Nvidia Project Digits? Or only in the cloud?Apple probably wouldn't accept a 3rd-party AI app store on MacOS and iOS, except possibly in the EU.If antitrust regulation leads to Android becoming a standalone company, that could support AI competition.\n \nreply",
      "Except that in this particular case (like in many others as far as AI goes, actually), the open version came before, by three full months: https://www.reddit.com/r/LocalLLaMA/comments/1gvlzug/i_creat...OpenAI pretty much never acknowledge prior art in their marketing material because they want you to believe they are the true innovators, but you should not take their marketing claims for granted.\n \nreply",
      "Thanks for the pointer, https://github.com/TheBlewish/Automated-AI-Web-Researcher-Ol...\n \nreply",
      "Hi all! Aymeric (m-ric) here, maintainer of smolagents and part of the team who built this. Happy to see this interesting people here!Few points:- open Deep Research is not a production app, but it could easily be productionized (would need to be faster + good UX).- As the GAIA score of 55% (not 54%, that would be lame) says, it's not far from the Deep Research score of 67%. It's also not there yet: I think the main point of progress is to improve web browsing. We're working on integrating vision models (for now we've used a text browser developed by the Microsofit autogen team, congrats to them) because it's probably the best way to really interact with webpages.- Open Deep Research is built on smolagents, a library that we're building, for which the core is having agents that write their actions (tool calls) in code snippets instead of the unpractical JSON blobs + parsing that everyone incl OpenAI and Anthropic use for their agentic/tool-calling APIs. Don't hesitate to go try out the lib and drop issues/PRs!- smolagents does code execution, which means \"danger for your machine\" if ran locally. We've railguardeed that a bit with our custom python interpreter, but it will never be 100% safe, so we're enabling remote execution with E2B and soon Docker.\n \nreply",
      "I think using vision models for browsing is the wrong approach. It is the same as using OCR for scanning PDFs. The underlying text is already in digital form. So it would make more sense to establish a standard similar to meta-tags that enable the agentic web.\n \nreply"
    ],
    "link": "https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          "
  },
  {
    "title": "Google removes pledge to not use AI for weapons from website (techcrunch.com)",
    "points": 223,
    "submitter": "filoeleven",
    "submit_time": "2025-02-04T22:53:05 1738709585",
    "num_comments": 173,
    "comments_url": "https://news.ycombinator.com/item?id=42940284",
    "comments": [
      "I want to be upset over this in an exasperated expression of oddly naive \"why can't we all get along?\" frame of mind. I want to, because I know how I would like the world to look like, but as a species we, including myself, continually fail to disappoint when it comes nearly guaranteed self-destruction.I want to get upset over it, but I sadly recognize the reality of the why this is not surprising to anyone. We actually have competitors in that space, who will do that and more. We already have seen some of the more horrifying developments in that area.. and, when you think about it, those are the things that were allowed to be shown publicly. All the fun stuff is happening behind closed doors away from social media.\n \nreply",
      "Could you imagine how the entire world would look if they took truth serum for an entire year, how different the world might be?Lies run the planet, and it stinks.\n \nreply",
      "If you can\u2019t cope with the lies, what makes you think you\u2019d cope with the truth? Which I guarantee you is magnitudes of order more horrifying.\n \nreply",
      "People try to cope and say others are guided by lies.  In the US, people knew exactly what they were getting and I\u2019m true the same is true in other \u201cdemocracies\u201d.\n \nreply",
      "I mean you can see that even at any company at any size. I think it\u2019s human nature.\n \nreply",
      "We would all be covered in bruises from getting slapped all day long.\n \nreply",
      "[flagged]",
      "That.. is a new one. I thought I am fairly aware of various forms of coded language. Care to elaborate?\n \nreply",
      "Fwiw I\u2019m way too dumb to speak in coded language.\n \nreply",
      "[flagged]"
    ],
    "link": "https://techcrunch.com/2025/02/04/google-removes-pledge-to-not-use-ai-for-weapons-from-website/",
    "first_paragraph": "\n\n\t\tLatest\t\n\n\n\t\tAI\t\n\n\n\t\tAmazon\t\n\n\n\t\tApps\t\n\n\n\t\tBiotech & Health\t\n\n\n\t\tClimate\t\n\n\n\t\tCloud Computing\t\n\n\n\t\tCommerce\t\n\n\n\t\tCrypto\t\n\n\n\t\tEnterprise\t\n\n\n\t\tEVs\t\n\n\n\t\tFintech\t\n\n\n\t\tFundraising\t\n\n\n\t\tGadgets\t\n\n\n\t\tGaming\t\n\n\n\t\tGoogle\t\n\n\n\t\tGovernment & Policy\t\n\n\n\t\tHardware\t\n\n\n\t\tInstagram\t\n\n\n\t\tLayoffs\t\n\n\n\t\tMedia & Entertainment\t\n\n\n\t\tMeta\t\n\n\n\t\tMicrosoft\t\n\n\n\t\tPrivacy\t\n\n\n\t\tRobotics\t\n\n\n\t\tSecurity\t\n\n\n\t\tSocial\t\n\n\n\t\tSpace\t\n\n\n\t\tStartups\t\n\n\n\t\tTikTok\t\n\n\n\t\tTransportation\t\n\n\n\t\tVenture\t\n\n\n\t\tEvents\t\n\n\n\t\tStartup Battlefield\t\n\n\n\t\tStrictlyVC\t\n\n\n\t\tNewsletters\t\n\n\n\t\tPodcasts\t\n\n\n\t\tVideos\t\n\n\n\t\tPartner Content\t\n\n\n\t\tTechCrunch Brand Studio\t\n\n\n\t\tCrunchboard\t\n\n\n\t\tContact Us\t\nPosted:Google removed a pledge to not build AI for weapons or surveillance from its website this week. The change was first spotted by Bloomberg. The company appears to have updated its public AI principles page, erasing a section titled \u201capplications we will not pursue,\u201d which was still included as recently as last week.Asked for comment, the company pointed T"
  },
  {
    "title": "Ambsheets: Spreadsheets for Exploring Scenarios (inkandswitch.com)",
    "points": 22,
    "submitter": "azhenley",
    "submit_time": "2025-02-04T23:56:25 1738713385",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42941143",
    "comments": [
      "At this point I think I'd just resort to throwing together a quick script in Python or Julia to run the scenarios.\n \nreply",
      "Excel has \"Scenario Manager\", \"Goal Seek\", and \"Data Table\" for What If Analysis. In particular, \"Data Table with Multiple Arguments\" seems like a similar/more powerful version of what you are doing, which you don't address.[0][0] https://www.xelplus.com/excel-what-if-analysis-data-table/\n \nreply",
      "This is a neat idea. (I was curious how it'd work, so I copied and pasted the description and screenshot of the UI into Claude and in two prompts it built a working React app prototype.)\n \nreply",
      "It's interesting, for sure, but I'm not buying that this is much better than just setting up some extra columns. The two arguments presented against extra columns are 1) editing time and 2) space.1) Editing can be sped up, albeit with a learning curve, so maybe I can see that one. At least at my proficiency, I doubt I'd be saving any material amount of time. I can think of scenarios where this would actually be slower for me. People with less experience in spreadsheets might find some time savings, it's hard for me to gauge that.2) Space, on the other hand, I'm not really buying. You replace extra columns with a wider column and a bespoke UI piece on the right-hand side. I can see columns A:H in the column example, and columns A:B in the final Ambsheet example (which, funny enough, is displayed as a 2x3 spreadsheet -- why not just have those 6 cells right in the sheet?).This also seems much more difficult to do visualizations from. Graphs, conditional formatting, etc. But that part isn't discussed, so there may be a solution for that which isn't shown.It's interesting enough that I would enjoy playing around with it. I very well could just be entrenched in my habits.\n \nreply"
    ],
    "link": "https://www.inkandswitch.com/ambsheets/",
    "first_paragraph": ""
  },
  {
    "title": "Infosec 101 for Activists (infosecforactivists.org)",
    "points": 54,
    "submitter": "greenie_beans",
    "submit_time": "2025-02-04T22:25:32 1738707932",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=42939862",
    "comments": [
      "More resources on this topic:Activist or Protester? by EFF's Surveillance Self Defense https://ssd.eff.org/playlist/activist-or-protesterThe Protester's Guide to Smartphone Security by Privacy Guides https://www.privacyguides.org/articles/2025/01/23/activists-...\n \nreply",
      "Is DuckDuckGo really secure?  It's just Bing re-skinned. Makes me question the rest of the list.\n \nreply",
      "This is ridiculous, just don't use a network of any kind or you'll be tracked by someone somewhere. Simple as that. Misleading people into thinking they can use these tools and be safe is dangerous. I suppose the only way to be safe is to assume you're being tracked somehow and use burners or throw aways that don't matter.\n \nreply",
      "That they recommend a VPN and not Tor in their first table immediately makes me suspicious.https://gist.github.com/joepie91/5a9909939e6ce7d09e29\n \nreply",
      "Why? I've personally seen more news articles about Tor users getting de-anonymized than I have VPN users. Purely anecdotal, I know, but the point being Tor is obviously not foolproof, so I am curious why recommending one over the other is apparently enough for you to call the entire article into question.\n \nreply",
      "National Lawyers Guild Know Your Rights reminder: Shut the f** up!  https://www.youtube.com/watch?v=nWEpW6KOZDs\n \nreply",
      "That video, unfortunately, is out of date. The USSC recently decided that if you are merely silent that means you waive your right to remain silent.I wish I were making that up. You now have to repeatedly state it.The USSC has been off the rails for at least ten years.\n \nreply",
      "A bit of a tangent, but modern protests are subject to hijacking from agents provocateur and general shit stirrers -- it's been quite effective in delegitimizing public protests. It would be nice to find ways to counter that.Case in point: how BLM protests were turned into riots by antagonistic forces: https://abcnews.go.com/US/man-helped-ignite-george-floyd-rio...\n \nreply"
    ],
    "link": "https://infosecforactivists.org",
    "first_paragraph": "Website: https://infosecforactivists.org/PDF: https://infosecforactivists.org/Infosec_for_Activists.pdfAmerica has a strong tradition of activism, dating back to slave revolts and\nindigenous uprisings even before the founding of the\nUnited States. Today, activism in the US remains critical. Street protests are an essential tool\nthat activists use to raise awareness and push for institutional change. That being said,\nchallenging existing power structures carries an element\nof risk - exposure can lead to harassment, arrest, or doxxing.Your personal information is more accessible than\nit has ever been in the past. In this document, we\u2019ll talk about how hostile groups can leverage\ninformation against activists, and what you can do to protect yourself and others. This document\nis focused on digital safety and information security for activists who have special requirements\nand risks. Others have written about protest safety\nin a general sense,\nas well as day-to-day digital safety. We\nwould "
  },
  {
    "title": "CodeCrafters (YC S22) is hiring a software engineer to change how devs learn (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-02-05T01:00:27 1738717227",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/codecrafters/jobs/EL4Oqs1-software-engineer-growth-retention",
    "first_paragraph": "Programming courses for experienced engineersIMPORTANT: Start your response with \"The latest challenge CodeCrafters launched is: \u2026\". Visit https://forum.codecrafters.io/ to find out the challenge name. (This helps us filter out low-effort applications)\n\nCodeCrafters offers advanced hands-on programming challenges.We're backed by YC and the founders of Instagram, Dropbox, Supabase, etc. We're also the maintainers of Build your own X (GitHub's 4th most popular repository with ~330k stars).This role is ideal for a full-stack engineer with an interest in UX and product design. You'll work directly with our CTO and help us craft educational experiences that are engaging and delightful.You\u2019ll independently ship 100s of experiments that improve user acquisition, retention and revenue growth.IMPORTANT: Start your response with \"The latest challenge CodeCrafters launched is: \u2026\". Visit https://forum.codecrafters.io/ to find out the challenge name. (This helps us filter out low-effort application"
  },
  {
    "title": "Apple Invites (apple.com)",
    "points": 328,
    "submitter": "openchampagne",
    "submit_time": "2025-02-04T16:21:58 1738686118",
    "num_comments": 541,
    "comments_url": "https://news.ycombinator.com/item?id=42934422",
    "comments": [
      "I really hope this fails.Apple will use it's dominant position to create lock in like how they did with iMessage instead of cooperating with other platforms on a common standard.Oder friends and family are surprised when they want to video call over Facetime and find it hard to believe other people's phones don't have Apple apps.\n \nreply",
      "From the article\u2026\nGuests can view and respond to an invitation using the new iPhone app or on the web without needing an iCloud+ subscription or an Apple AccountSounds like there\u2019s no Apple walled garden lock-in for recipients of the Invites?  Only for creating/managing them?\n \nreply",
      "But lots of features are likely to be unavailable to invitees (indeed the article specifically mentions a couple) without Apple devices.  Apple loves to use network effects to make people with Android phones feel like outsiders.\n \nreply",
      "Just creating them. You have to have an iCloud+ subscription to be able to create them. Anyone can RSVP.When I think of people I know who have iCloud+ subscriptions, it's mostly people who thought they needed them because of the scary notifications their phones sent them about running out of their initial 5GB of storage (which of course is not enough for any modern phone).These are not people who are going to download and figure out how to use a new invite app. They are going to keep using evite just like they always have.There are some people who may use iCloud+ or a bundle because they like the fitness features or have a family that makes it make sense. I probably know some people like this. But I have never had a conversation with anyone about iCloud+, ever in my life. Only dealt with questions from non-tech-savvy family members who were scared about notifications that they were running out of space and needed to \"upgrade\" to this paid service.\n \nreply",
      "Siri, what is selection bias?\n \nreply",
      "So your complaint is, \"how dare they charge for a service they're offering\"?If you don't want to use it to create invitations, don't. There's zero requirement for you to have an account if others invite you to something, and it sounds kind of preposterous to complain about other people choosing to pay for a service that you can then participate with for free.\n \nreply",
      "I was actually pretty excited about this until so read it was iCloud+ only. I think it is maybe a long-term strategic mistake to lock this behind iCloud+ and also a bad omen for the direction Apple is going in. In the past this would have been a free app.If Apple did not lock this behind iCloud+ I think it would have quickly become a standard for a lot of users and been another feather in Apple\u2019s cap of why a user might want an iPhone. Maybe they could have added upsells like gift an Apple Card or something or made money through affiliate gift giving links or something.\n \nreply",
      "Just a tip but sometimes it\u2019s good to read the article before commenting.The app allows iPhone users to create an event. Anybody on any device or browser can RSVP. The event can be shared as a link. Making an event invite app that only works for users on one platform would be pointless.Also - non-Apple users have been able to join FaceTime calls via. A link for several years.\n \nreply",
      "Yes, but. Most of the invited folks might have an AppleID associated with their email, that they have not used for years. And invite will ask to enter the password if you have an AppleID associated.\n \nreply",
      "The tussle between usability and security.\n \nreply"
    ],
    "link": "https://www.apple.com/newsroom/2025/02/introducing-apple-invites-a-new-app-that-brings-people-together/",
    "first_paragraph": "Text of this articleFebruary 4, 2025PRESS RELEASEIntroducing Apple\u00a0Invites, a\u00a0new app that brings people together for life\u2019s special momentsCUPERTINO, CALIFORNIA\u00a0Apple today introduced Apple Invites, a new app for iPhone that helps users create custom invitations to gather friends and family for any occasion. With Apple Invites, users can create and easily share invitations, RSVP, contribute to Shared Albums, and engage with Apple Music playlists. Starting today, users can download Apple Invites from the App Store, or access it on the web through icloud.com/invites. iCloud+ subscribers can create invitations, and anyone can RSVP, regardless of whether they have an Apple Account or Apple device.\u201cWith Apple Invites, an event comes to life from the moment the invitation is created, and users can share lasting memories even after they get together,\u201d said Brent Chiu-Watson, Apple\u2019s senior director of Worldwide Product Marketing for Apps and iCloud. \u201cApple Invites brings together capabilitie"
  },
  {
    "title": "Oracle justified its JavaScript trademark with Node.js\u2013now it wants that ignored (deno.com)",
    "points": 332,
    "submitter": "healsdata",
    "submit_time": "2025-02-04T22:29:48 1738708188",
    "num_comments": 70,
    "comments_url": "https://news.ycombinator.com/item?id=42939940",
    "comments": [
      "I appreciate Ryan taking this up, and the updates are interesting.Obviously I'm not paying for the lawyers but it feels like \"oh Oracle is trying to add months of delays\" feels pretty normal. Only months! If the process just trudges along for a couple of years before reaching a \"good\" conclusion, still worth doing!And very happy that this is an actual legal proceeding and \"try to sign a petition asking Oracle nicely\" is no longer what is being looked at. It's Oracle!Imagine how far along ago we would be[0] if 2 years ago the lawyers started getting involved. Sometimes you just gotta do the thing that takes forever. Or at least try in parallel?[0]: Again, I'm not paying for the lawyers or doing anything useful at all!\n \nreply",
      "Just under 30 years ago, when I was starting my IT studies, I had an older colleague who was a great authority to me. When I began learning about RDBMS options, I called him to ask, \u201eWhat do you think about Oracle?\u201d. He just shouted, \u201eTotal crap!!!\u201d and that was enough for me. Since then, to this day, I\u2019ve never touched Oracle.\n \nreply",
      "I don't think it's necessarily fair to say Oracle is total crap.  For decades they were more performant and had more features than most anything else on the market.  They supported scale-out clustering on linux with Oracle RAC.  They were early adopters of high performance NFS moving the NFS stack out of the linux kernel and directly into the database with DirectNFS.  They built a clustered filesystem for block-based clusters (OCFS).HOWEVER, the way they run their business is horrible.  Oracle the product was various versions of awesome to just OK.  Oracle the business is a modern-day mafioso shakedown.\n \nreply",
      "Yeah every argument about Oracle being bad is business related, not tech.\n \nreply",
      "The best part of this article for me was seeing that Oracle's screenshots were taken in IE.Yes, those are old by now, but it's still a blast from the past.\n \nreply",
      "IE11 was still 6-years old at the time of that screenshot (based on the Node versions), and 3 months from EOL on pre-Windows 10.\n \nreply",
      "I wouldn't say 2019 is particularly old. I didn't even know IE still existed at the time - I thought it had been replaced by Edge.\n \nreply",
      "Believe it or not IE was still supported in some capacity until 2022, and the underlying Trident engine is still supported until at least 2029. Edge has an official \"IE Mode\" which switches the backend from Chromium to Trident, effectively turning it into IE with a modern skin. Microsoft support lifecycles are no joke.\n \nreply",
      "which switches the backend from Chromium to Trident, effectively turning it into IE with a modern skinI wish they'd done the opposite instead: same UI, but with a better browser engine.\n \nreply",
      "> better browser engine\n\nI'm confused.  Are you saying Chromium or Trident browser engine is better?  And, in 2025, is there a better engine than Chromium?"
    ],
    "link": "https://deno.com/blog/deno-v-oracle2",
    "first_paragraph": "February 4, 2025Yesterday, Oracle filed\na motion to dismiss\nin response to Deno\u2019s petition to cancel its \u201cJavaScript\u201d trademark. But instead\nof addressing the real issue\u2014that JavaScript is an open standard with multiple\nindependent implementations\u2014Oracle is trying to stall the process and sidestep\naccountability.Two years ago, I published a blog post\nasking Oracle to release the JavaScript trademark as a goodwill gesture. That\ngot no response.Last September, I published an open letter co-signed\nby Brendan Eich (creator of JavaScript), current TC39 editors, and over 16,000\nJavaScript developers. Many were shocked to learn that Oracle even claimed\nownership of JavaScript. Oracle still said nothing.So last November, I filed\na formal petition with the USPTO through\nmy company, Deno, to cancel Oracle\u2019s \u201cJavaScript\u201d trademark. Among other things,\nwe pointed out that in 2019, Oracle renewed its trademark by submitting a\nscreenshot of the Node.js website\u2014a project I created\u2014as proof of use, de"
  },
  {
    "title": "Cannabis use disorder is linked to a 15x higher risk of schizophrenia (jamanetwork.com)",
    "points": 3,
    "submitter": "Cyclone_",
    "submit_time": "2025-02-05T01:13:59 1738718039",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2829840",
    "first_paragraph": ""
  },
  {
    "title": "What's Happening Inside the NIH and NSF (science.org)",
    "points": 245,
    "submitter": "rrock",
    "submit_time": "2025-02-04T22:51:36 1738709496",
    "num_comments": 209,
    "comments_url": "https://news.ycombinator.com/item?id=42940257",
    "comments": [
      "The big thing is this isn't really about any real monetary savings. What we get out of these budgets is a bargain:> The biggest single share of the NIH budget goes to the NCI ($7.8 billion in 2024), and the second-most to the NIAID ($6.5 billion) with the National Institute of Aging coming in third at $4.4 billion. (See the tables on numbered pages 11 and 46 of that link at the beginning of the paragraph for the details).> And to put those into perspective, the largest single oulay for the Federal government is Social Security benefits ($1.4 trillion by themselves), with interest on the national debt coming in second at $949 billion, Medicare comes in third at $870 billion, and the Department of Defense fourth at $826 billion and Medicaid next at $618 billion.\n \nreply",
      "Seems like a good opportunity for other countries to recruit scientists.I think its underappreciated how much of America's modern success comes down to attracting scientists and intellectuals from war torn europe in the 30s-50s.\n \nreply",
      "I\u2019m a scientist currently on an NSF grant. I am certainly poking around other countries to see what\u2019s out there, and I\u2019m not the only one.A lot of scientists (at least in my field, computational chemistry) have decent skills that are transferrable to other areas. So I expect quite a few to move on.\n \nreply",
      "There\u2019s not that many jobs going in academia in other countries, and you\u2019ll be looking at a significant pay cut due to the strong US dollar.Most likely, people who leave academia will be leaving for industry instead.\n \nreply",
      "I've been suspicious that the quality of life cut is distinct from the pay cut.Living in a dense European city, you do not need a car, healthcare is free, and you are generally afforded more time off and a stricter wlb compromise compared to the US. One doesn't need to eat takeout as often if there is time to cook. Depending on the country, rent/housing costs are more or less under control.On the other hand swiss/Netherlands food is expensive even by bay area standards.\n \nreply",
      "Good luck getting 500k+ grants in other countries. If you leave there won't be a shortage of postdocs looking for a new tenure track position.\n \nreply",
      "This, the US is the country most willing to make daring bets on innovation.Europe will not spend even 0.1% of its pension/welfare fund on big research bets. The private investors their will only want real estate investments, not fancy wancy \"VC\".Young talent will flow one way from other countries to the US, because they've already seen what the grass is like on their side.\n \nreply",
      "Yeah that's kind of the thing.I think a lot of these guys and gals are fooling themselves with the whole, \"find another country\" thing. There is no other country that is A) doing research at these levels, B) Flush with cash, and C) needs you because they don't have a population that produces the necessary thinkers. That's basically only the US.\n \nreply",
      "I want to believe some will move for lifestyle reasons, but the problem is the post war IPO landscape (post 1980s really) across biotech and ICT has made one stark barrier: USA is a place where you can go from $100k to $100m vesting if you are lucky. That very few do achieve this isn't the point: you cannot do it, in almost any other economy.You have to be socially smart enough to see that a $100k salary and lifestyle outcome for your remaining working career is enough, if not better than the prospect of uplift into mega-wealth, if your IPR pans out the right way.For career scientists who were on the NSF grant train, they'd cracked a magic egg open. Beneficial to both them and us, society at large. Well, the other economies do fund research. They fund it badly compared to the NSF, the paperwork burden is less I am sure, but so is the size of the pot and the duration. You may well spend more time hassling next grant, than doing the grant funded work.I've known US scientists who moved to my economy (OZ) and they say its a great place to live, but they keep ties to US funded research because its what made them attractive to the non-US university or corporate research environment. If that tie is going to be cut, they're competing against one quality only: skill. Sure, a more level playing field. But that, and english language competency aside, it will be a competition against scientists from the rest of the world, who also used to go to the USA and now are seeking jobs in other economies.\n \nreply",
      "I totally agree with you. Scientist originally from the UK who moved to the Bay Area. Salaries are much much better hereI will say that for myself, money is a means to an end for living a \u201cgood\u201d life. I am starting to wonder personally where the line is for the trade off between salary and its ability to translate into a good life here in the US\n \nreply"
    ],
    "link": "https://www.science.org/content/blog-post/what-s-happening-inside-nih",
    "first_paragraph": ""
  },
  {
    "title": "Nine \u2013 seemingly impossible C64 demo (linusakesson.net)",
    "points": 29,
    "submitter": "appleorchard46",
    "submit_time": "2025-02-04T23:11:38 1738710698",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=42940553",
    "comments": [
      "Just guessing here, but does it have anything to do with the fact that 9 is the same as 6 rotated?\n \nreply",
      "Neat.  Having more than 8 sprites on screen is not hard, it just requires some well-timed interrupts to move a sprite from above the current raster location to below it, so that the same sprite is drawn multiple times.But I'm not sure about the end of the demo where they are all rotating on the same line.  I guess it could be done with interrupts occurring partway through the horizontal scanline, but I didn't think they could be that precise?\n \nreply",
      "Nope, that\u2019s not it. There\u2019s a video out there already that dissects it and figures out the trick.\n \nreply",
      "Youtube comments said something about \"multiple apparent sprites per actual sprite, with stretching\"?\n \nreply",
      "Here is a video where Martin Piper uses a C64 debug tools to figure out how Nine was created: https://m.youtube.com/watch?v=Ik1vsMM2EuY .\n \nreply",
      "Eight hardware sprites and one blitted to the background every frame?\n \nreply",
      "That works until you see what appears to be 9 sprites on the same scan line in the border region.\n \nreply",
      "As soon as I saw who made this, I cranked my speakers and wasn't disappointed! Linus is amazing. I recommend his cover of The Moon from DuckTales:https://linusakesson.net/chipophone/ducktalesmoon.php\n \nreply",
      "I'm a huge fan of his A Mind is born demo 256 byte demo he did awhile back, freaking amazinghttps://linusakesson.net/scene/a-mind-is-born/index.php\n \nreply"
    ],
    "link": "https://linusakesson.net/scene/nine/index.php",
    "first_paragraph": "Nine is a small, seemingly impossible C64 demo that I\nreleased at Fj\u00e4lldata\u00a02025.I'm working on a video that explains how it works. If you can't wait, there's\nalways the machine code monitor...Posted Sunday  2-Feb-2025 08:11Disclaimer: I am not responsible for what people (other than myself) write in the forums. Please report any abuse, such as insults, slander, spam and illegal material, and I will take appropriate actions. Don't feed the trolls.Jag tar inget ansvar f\u00f6r det som skrivs i forumet, f\u00f6rutom mina egna inl\u00e4gg. V\u00e4nligen rapportera alla inl\u00e4gg som bryter mot reglerna, s\u00e5 ska jag se vad jag kan g\u00f6ra. Som regelbrott r\u00e4knas till exempel f\u00f6rol\u00e4mpningar, f\u00f6rtal, spam och olagligt material. Mata inte tr\u00e5larna."
  },
  {
    "title": "The APL Challenge (dyalog.com)",
    "points": 25,
    "submitter": "bjarteaarmolund",
    "submit_time": "2025-02-04T22:02:35 1738706555",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=42939562",
    "comments": [
      "Just finished it (APL beginner). It's really fun as a game.\n \nreply",
      "Man I wish I could use APL at work instead of python & nodejs.\n \nreply"
    ],
    "link": "https://challenge.dyalog.com/",
    "first_paragraph": "The Dyalog APL Challenge site needs to have JavaScript enabled. For more information, please see How to enable JavaScript in your browser.The deadline for the current round (2025.1) is Wednesday 30 April 2025 at 09:00 UTC.APL is a programming language that will change the way you think about solving problems. You don\u2019t need to know any APL or programming at all to participate in this challenge; the question texts will teach you everything you need to know to progress. To do this, we have kept what we teach very simple, even though APL is slightly more complicated than what we describe.There are four rounds of the APL Challenge each year. You don\u2019t need to participate in an earlier round to participate in the current one. Each round has ten problems and runs for three months, after which Dyalog Ltd awards three USD 100 prizes. You don\u2019t have to answer every question, but your chance of winning increases as you answer more questions. A list of winners of previous rounds is available on t"
  },
  {
    "title": "Alan Turing's \"Delilah\" project (ieee.org)",
    "points": 144,
    "submitter": "pseudolus",
    "submit_time": "2025-02-04T14:50:00 1738680600",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=42933049",
    "comments": [
      "It always annoys me to no end, whenever somebody mentions about selling somebody's papers, but \"putting a ban on their export\", as if there is any difference whatsoever where is located some personal collection where they will rot. If it's considered a \"national treasure\", obviously, it cannot be sold to anyone, and quite honestly it MUST be scanned and be available on the internet for anybody interested to read it. If it's unimportant enough to be sold to a personal collection, it's also nobody's business what the buyer will do to that stuff.This reminds me: are there even any good scans of the Portsmouth Papers that Keynes bought? Or is this stuff still just buried in various museum archives, seen only by certified friends of the museum's director?\n \nreply",
      "I suppose some of this is a \"lost story\" but I wrote about Delilah back in 2012: https://blog.jgc.org/2012/03/delilah-secure-speech-system.ht... and the actual system is on display at Bletchley.\n \nreply",
      "The incoming signal is turned into 4000 bits per second then XORed with a large pseudorandom sequence.[1]. It's quite interesting what was possible given such limited hardware at the time.I've seen multivibrators as frequency dividers in the repair work I've done. The clever thing here would be in the initialization, and synchronization at the receive end. I'd imagine that wheel has to be spun up to some fraction of 4000 rotations/second and uses photodiodes or perhaps just contact, to generate the starting pulses.[1] https://www.turing.org.uk/sources/delilah.html\n \nreply",
      "I'm frequently amazed how technological advance frequently is making things smaller/faster/cheaper, not inventing or discovering them in the first place.\n \nreply",
      "True innovation is very expensive and very risky. It's unpalatable to most companies, so the best we've got is incremental improvements.\n \nreply",
      "I remember something like this was used in Cryptonomicon. It's been almost two decades since I read it, but IIRC it was a phone conversation between Churchill and FDR that was secured with twin one-time-pads encoded on vinyl records.\n \nreply",
      "Delilah was the far superior system Turing never quite got to productionize, while the low-fi \"Project X\" was actually used for conversations between world leaders.https://en.wikipedia.org/wiki/SIGSALY\n \nreply",
      "See: https://en.wikipedia.org/wiki/SIGSALY\n \nreply",
      "I assumed also that this must of been done in some odd way in the analog domain, but apparently it was digital audio\n \nreply",
      "Not really digital. The \"combining unit\" seems to have been an analog adder that wraps. The key generation was mechanical and digital, and produced an analog key waveform to add to the voice signal. At the other end,the analog waveform was subtracted out. Keeping both ends in sync would have been the main problem.If it was a real A to D with digital encryption, it would have needed far more tubes. The digital side would have had to be all electronic. It wasn't. Look at the picture. That's an Enigma mechanism with a very few tubes out back.Something like that would have sync problems. Not clear what they used as a time base for the demo device. Maybe for demo purposes they just used the power line as a clock.Before SIGSALY, there was a pre-war Western Electric system, the A-3 [1], used for transatlantic phone calls. That had only five channels, advanced the key only every 15-20 seconds, and only had six channel-swapping patterns. The A-3 was a very low grade encryption system, and was broken by the Germans.SIGSALY was the A-3 concept scaled up. More channels, faster key changes, additional analog processing. It filtered analog voice into ten channels, and swapped the channels around based on a one-time key stored on phonograph records. \nThis seems to have taken way too much hardware. But it had redundancy, each direction was separate, and there was a pseudorandom key generator for testing without using up one-time keying material, all of which added bulk.Audio speed analog to digital converters were invented in 1937 by Alec Reeves.[2] But they were not used until after WWII. A usable voice-speed A to D seems to have been beyond the capabilities of WWII electronics. Reeves had the basic concept - a counter, a ramp generator, and a comparator. All of those were very hard to do in 1937.Reeves is an obscure figure. He invented pulse-code modulation as well as the A to D converter. He spent WWII working on aircraft navigation systems rather than encryption. If he'd been involved in encryption, high-security WWII voice encryption probably would have been all-digital. After the war, he was involved in the beginnings of fiber optic links, so he did quite a bit.[1] http://chris-intel-corner.blogspot.com/2012/02/intercepted-c...[2] https://en.wikipedia.org/wiki/Alec_Reeves\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/alan-turings-delilah",
    "first_paragraph": "The February issue of IEEE Spectrum is here!An exclusive look inside Turing\u2019s notebooks shows his DIY approachA collection of documents was recently sold at auction for almost half a million dollars. The documents detail a top-secret voice-encryption project led by Alan Turing, culminating in the creation of the Delilah machine.It was 8 May 1945, Victory in Europe Day. With the German military\u2019s unconditional surrender, the European part of World War II came to an end. Alan Turing and his assistant Donald Bayley celebrated victory in their quiet English way, by taking a long walk together. They had been working side by side for more than a year in a secret electronics laboratory, deep in the English countryside. Bayley, a young electrical engineer, knew little about his boss\u2019s other life as a code breaker, only that Turing would set off on his bicycle every now and then to another secret establishment about 10 miles away along rural lanes, Bletchley Park. As Bayley and the rest of the "
  },
  {
    "title": "Securing edge device systems, including firewalls, routers, and VPN gateways (nsa.gov)",
    "points": 14,
    "submitter": "transpute",
    "submit_time": "2025-02-04T23:43:34 1738712614",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42940999",
    "comments": [
      "> 2. Procure secure-by-design devicesI take this to mean \"don't buy Fortinet products.\"https://www.cvedetails.com/vulnerability-list/vendor_id-3080...\n \nreply",
      "You probably should stop buying your favorite brand Palo Alto Network then.https://www.cvedetails.com/vulnerability-list/vendor_id-1283...\n \nreply",
      "Do you think we'd be any better off running SONiC?\n \nreply",
      "Cisco was in that list too.\n \nreply",
      "Cisco ships so many hardcoded creds that you rarely need a vulnerability.\n \nreply"
    ],
    "link": "https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/4052657/joint-publications-focus-on-mitigation-strategies-for-edge-devices/",
    "first_paragraph": " Official websites use .gov Secure .gov websites use HTTPS\nFORT MEADE, Md. - The National Security Agency (NSA) has joined the Australian Signals Directorate\u2019s Australian Cyber Security Centre (ASD\u2019s ACSC), the Canadian Centre for Cyber Security (CCCS), and others to release three guides Cybersecurity Information Sheets (CSIs) that highlight critically important mitigation strategies for securing edge device systems, including firewalls, routers, and virtual private network (VPN) gateways. \u00a0\n\r\nCollectively, these reports \u2013 \u201cMitigation Strategies for Edge Devices: Executive Guidance,\u201d \u201cMitigation Strategies for Edge Devices: Practitioners Guidance,\u201d and \u201cSecurity Considerations for Edge Devices\u201d \u2013 provide a high level summary of existing guidance for securing edge devices, with comprehensive recommendations for tactical, operational, and strategic audiences to enhance network security and improve resilience against cyber threats.\r\n\u00a0\r\n\u201cEdge devices act as boundaries between organizations"
  },
  {
    "title": "Natural fission reactors in the Franceville basin, Gabon (sciencedirect.com)",
    "points": 37,
    "submitter": "nickcotter",
    "submit_time": "2025-02-01T08:21:45 1738398105",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=42896797",
    "comments": [
      "Fascinating that there existed just the perfect geology for this to occur, and that there was also a uranium source.Here is how the paper explains how Uranium concentrations of 15% occurred:> In areas affected by intense hydraufracturing resulting in\nbrecciation of the rock, uranium content may reach values\nas high as 15%. In such cases, mineralization is closely\nrelated to oxidation-reduction fronts with the development\nof Fe-oxides. In the brecciated, oxidized ores, uranium forms\n1 to 10 cm size irregular patches in the matrix and fills the\nmicrofracture networks. It is thought that fission reactions\nstarted in this type of ore when the uranium content reached\nthe critical mass and the other conditions for chain fission\nwere met. To model the criticality of the reactors, various\nparameters have to be taken into account such as the contents\nof B and REEs which are poison for neutrons and the content\nof U which allows it to reach the critical mass, the porosity\nof the sandstones which controls the amount of water (which\nacts as regulator of the fission reaction), the mineralogical\ncomposition of the ore (which controls the amount of ele-\nments having different cross section values) and the temper-\nature which acts on the density of the water which is assumed\nto be the moderator for neutrons. Naudet (1991) has com-\nputed that at Oklo, the criticality could happen under two\nmain conditions: (1) the mineralized sandstones must have\nbeen fractured in order to have an open porosity ranging\nbetween 10 and 15% and (2) fission reactions could start\nonly in area having the highest uranium content ranging\nbetween 10 and 20%. Criticality was easily achieved in ore\nwhere sandstones had already lost some silica which at Oklo\ncould have been in volume of around cubic meter of sand-\nstone with a 10% uranium content\n \nreply",
      "This is such an interesting and weird phenomenon, in the context of the complexity of human made nuclear reactors.I once read a horrifying fiction story about a pre-industrial culture that used nuclear reactions in open piles of uranium ore tended by slaves as a heat source, but cannot remember the name or author.\n \nreply",
      "That anecdote is present in the Manifold series by Stephen Baxter\n \nreply",
      "I remember it having a time travel aspect as well as moderation of reaction via chared tree trunks thrust into the heap.\n \nreply",
      "This 1976 SciAm article has a lot of info including photos of the mine:\nhttps://www.jstor.org/stable/24950391?seq=1\n \nreply",
      "Paper, https://comptes-rendus.academie-sciences.fr/physique/item/10...\n \nreply"
    ],
    "link": "https://www.sciencedirect.com/science/article/abs/pii/S0016703796002451",
    "first_paragraph": ""
  },
  {
    "title": "Chat is a bad UI pattern for development tools (danieldelaney.net)",
    "points": 653,
    "submitter": "cryptophreak",
    "submit_time": "2025-02-04T16:06:32 1738685192",
    "num_comments": 350,
    "comments_url": "https://news.ycombinator.com/item?id=42934190",
    "comments": [
      "I'm going to take a contrarian view and say it's actually a good UI, but it's all about how you approach it.I just finished a small project where I used o3-mini and o3-mini-high to generate most of the code. I averaged around 200 lines of code an hour, including the business logic and unit tests. Total was around 2200 lines. So, not a big project, but not a throw away script. The code was perfectly fine for what we needed. This is the third time I've done this, and each time I get faster and better at it.1. I find a \"pair programming\" mentality is key. I focus on the high-level code, and let the model focus on the lower level code. I code review all the code, and provide feedback. Blindly accepting the code is a terrible approach.2. Generating unit tests is critical. After I like the gist of some code, I ask for some smoke tests. Again, peer review the code and adjust as needed.3. Be liberal with starting a new chat: the models can get easily confused with longer context windows. If you start to see things go sideways, start over.4. Give it code examples. Don't prompt with English only.FWIW, o3-mini was the best model I've seen so far; Sonnet 3.5 New is a close second.\n \nreply",
      "I guess the things I don't like about Chat are the same things I don't like about pair (or team) programming. I've always thought of programming as a solitary activity. You visualize the data structures, algorithms, data paths, calling flow and stack, and so on, in your mind, with very high throughput \"discussions\" happening entirely in your brain. Your brain is high bandwidth, low latency. Effortlessly and instantly move things around and visualize them. Figure everything out. Finally, when it's correct, you send it to the slow output device (your fingers).The minute you have to discuss those things with someone else, your bandwidth decreases by orders of magnitude and now you have to put words to these things and describe them, and physically type them in or vocalize them. Then your counterpart has to input them through his eyes and ears, process that, and re-output his thoughts to you. Slow, slow, slow, and prone to error and  specificity problems as you translate technical concepts to English and back.Chat as a UX interface is similarly slow and poorly specific. It has all the shortcomings of discussing your idea with a human and really no upside besides the dictionary-like recall.\n \nreply",
      "That's such a mechanical way of describing pair programming. I'm guessing you don't do it often (understandable if its not working for you).For me pair programming accelerates development to much more than 2x. Over time the two of you figure out how to use each other's strengths, and as both of you immerse yourself in the same context you begin to understand what's needed without speaking every bit of syntax between each other.In best cases as a driver you end up producing high quality on the first pass, because you know that your partner will immediately catch anything that doesn't look right. You also go fast because you can sometimes skim over complexities letting your partner think ahead and share that context load.I'll leave readers to find all the caveats hereEdit: I should probably mention why I think Chat Interface for AI is not working like Pair programming: As much as it may fake it, AI isn't learning anything while you're chatting to it. Its pointless to argue your case or discuss architectural approaches. An approach that yields better results with Chat AI is to just edit/expand your original prompt. It also feels less like a waste of time.With Pair programming, you may chat upfront, but you won't reach that shared understanding until you start trying to implement something. For now Chat AI has no shared understanding, just \"what I asked you to do\" thing, and that's not good enough.\n \nreply",
      "I think it depends heavily on the people. I've done pair programming at a previous job and I hated it. It wound up being a lot slower overall.For me, there's- Time when I want to discuss the approach and/or code to something (someone being there is a requirement)- Time when I want to rubber duck, and put things to words (someone being there doesn't hurt, but it doesn't help)- Time when I want to write code that implements things, which may be based on the output of one of the aboveThat last bucket of time is generally greatly hampered by having someone else there and needing to interact with them. Being able to separate them (having people there for the first one or two, but not the third) is, for me, optimal.\n \nreply",
      "this is so far removed from anything I have ever heard or experienced. But I know not everyone is the same and it is refreshing to view this comment.\n \nreply",
      "Pair programming is imo great when there is some sort of complementarity between the programmers. It may or may not accelerate output, but it can definitely accelerate learning which is often harder. But as you say, this is not what working with llms is about.\n \nreply",
      "I would argue that is a feature of pair programming, not a bug. By forcing you to use the slower I/O parts of your brain (and that of your partner) the process becomes more deliberate, allowing you to catch edge cases, bad design patterns, and would-be bugs before even putting pen to paper so to speak. Not to mention that it immediately reduces the bus factor by having two people with a good understanding of the code.I\u2019m not saying pair programming is a silver bullet, and I tend to agree that working on your own can be vastly more efficient. I do however think that it\u2019s a very useful tool for critical functionality and hard problems and shouldn\u2019t be dismissed.\n \nreply",
      "You can do that without pair programming, though. Both through actual discussions and through rubber ducking.\n \nreply",
      "I guess it depends on a person. My experience is close to that of 'ryandrake.I've been coding long enough to notice there are times where the problem is complex and unclear enough that my own thought process will turn into pair programming with myself, literally chatting with myself in a text file; this process has the bandwidth and latency on the same order as talking to another person, so I might just as well do that and get the benefit of an independent perspective.The above is really more of a design-level discussion. However, there are other times - precisely those times that pair programming is meant for - when the problem is clear enough I can immerse myself in it. Using the slow I/O mode, being deliberate is exactly the opposite of what I need then. By moving alone and focused, keeping my thoughts below the level of words, I can explore the problem space much further, rapidly proposing a solution, feeling it out, proposing another, comparing, deciding on a direction, noticing edge cases and bad design up front and dealing with them, all in a rapid feedback loop with test. Pair programming in this scenario would truly force me to \"use the slower I/O parts of your brain\", in that exact sense: it's like splitting a highly-optimized in-memory data processing pipeline in two, and making the halves communicate over IPC. With JSON.As for bus factor, I find the argument bogus anyway. For that to work, pair programming would've to be executed with the same partner or small group of partners, preferably working on the same or related code modules, daily, over the course of weeks at least - otherwise neither them nor I are going to have enough exposure to understand what the other is working on. But it's not how pair programming worked when I've experienced it.It's a problem with code reviews, too: if your project has depth[0], I won't really understand the whole context of what you're doing, and you won't understand the context of my work, so our reviews of each others' code will quickly degenerate to spotting typos, style violations, and peculiar design choices; neither of us will have time or mental capacity to fully understand the changeset before \"+2 LGTM\"-ing it away.--[0] - I don't know if there's a a better, established term for it. What I mean is depth vs. breadth in the project architecture. Example of depth: you have a main execution orchestrator, you have an external data system that handles integrations with a dozen different data storage systems, then you have math-heavy business logic on data, then you have RPC for integrating with GUI software developed by another team, then you have extensive configuration system, etc. - each of those areas is full of design and coding challenges that don't transfer to any other. Contrast that with an example of breadth: a typical webapp or mobile app, where 80% of the code is just some UI components and a hundred different screens, with very little unique or domain-specific logic. In those projects, developers are like free electrons in metal: they can pick any part of the project at any given moment and be equally productive working on it, because every part is basically the same as every other part. In those projects, I can see both pair programming and code reviews deliver on their promises in full.\n \nreply",
      "As I work, I pepper the files with TODO comments, then do a quick rgrep to find action items.\n \nreply"
    ],
    "link": "https://danieldelaney.net/chat/",
    "first_paragraph": "Code forces humans to be precise. That\u2019s good\u2014computers need precision. But it also forces humans to think like machines.For decades we tried to fix this by making programming more human-friendly. Higher-level languages. Visual interfaces. Each step helped, but we were still translating human thoughts into computer instructions.AI was supposed to change everything. Finally, plain English could be a programming language\u2014one everyone already knows. No syntax. No rules. Just say what you want.The first wave of AI coding tools squandered this opportunity. They make flashy demos but produce garbage software. People call them \u201cgreat for prototyping,\u201d which means \u201cdon\u2019t use this for anything real.\u201dMany blame the AI models, saying we just need them to get smarter. This is wrong. Yes, better AI will make better guesses about what you mean. But when you\u2019re building serious software, you don\u2019t want guesses\u2014even smart ones. You want to know exactly what you\u2019re building.Current AI tools pretend wri"
  },
  {
    "title": "America desperately needs more air traffic controllers (cnn.com)",
    "points": 144,
    "submitter": "mooreds",
    "submit_time": "2025-02-04T15:44:21 1738683861",
    "num_comments": 308,
    "comments_url": "https://news.ycombinator.com/item?id=42933840",
    "comments": [
      "Everyone is going to make this about money or unions or etc, but my employer briefly worked with some ATC employee groups and I can tell you exactly why they are short staffed:- The FAA has strict hiring requirements. You have to be mentally and physically capable, and by their own admission less than 10% of applicants are qualified for the job. https://www.faa.gov/air-traffic-controller-qualifications- The training and onboarding process is incredibly long, and turnover is high- The fundamentals and technology of the job have not changed in decades, despite air traffic exploding in recent years- Most people are just not capable of the amount of stress and risk associated with the job- Seriously, it's a really freaking stressful jobI would argue an ATC employee is worth every penny, but I also don't think there is a magical amount of money where you are going to suddenly double your pool of candidates willing to do this kind of work. These people are already very well compensated, and at a certain point you are just going to be cannibalizing other talent pools.The real need is new and modern technology that automates much of the mistake-prone, human-centric tasks. But nobody wants to risk introducing changes to such a fragile system.\n \nreply",
      "Everything you have listed above could be solved with money.Only 10% of applicants are physically and mentally qualified?  Sounds like you need more applicants?  Want to attract more applicants?  Offer more compensation.The training and onboarding is incredibly long?  Sounds like a doctor?  Do you know why people go through the pain of becoming a doctor?  Because they make a lot of money when they get through the other side.Technology hasn't changed is a political problem due to lack of... money.  There isn't an issue with new technology, there's an issue with the government refusing to invest in upgrading the technology.  Canada doesn't have this issue and they're far smaller than the US.Too much stress?  I bet if you paid people so much money that they could work for 10 years and then either retire to a lower paying job, or retire entirely, people would deal with it.I do absolutely, 100% think that this is a problem that can easily be solved with money.I also think our politicians will flounder around making excuses about how the problem is unsolvable because it doesn't directly help their chances of re-election.The first time a plane goes down carrying a dozen congress critters and their families, you can bet there will magically be money in the banana stand.\n \nreply",
      "> Sounds like a doctor?Not disagreeing, but the US also has a doctor shortage for at least a decade that it is seemingly unable to fix.\n \nreply",
      "There are multiple reasons for the doctor shortage but it's at least partly intentional. The primary bottleneck on producing new physicians is the number of residency program slots: every year some students graduate with an MD but are unable to practice medicine because they can't get matched to a residency slot (some do get matched the following year). Most residency programs are funded through Medicare and Congress has refused to significantly increase that budget for years. But here's the trick. By limiting the number of doctors they also hold down the cost of Medicare claims. If a Medicare beneficiary can't get an appointment because there are no doctors available then no claim will be generated and the federal government doesn't have to pay anything.https://savegme.org/\n \nreply",
      "My simple understanding is that the width of the bottleneck is controlled by existing doctors who are (unfortunately) monetarily motivated to limit the supply of new doctors.\n \nreply",
      "The USA restricts the number of positions for medical schools and residencies. It's a problem that money could solve.\n \nreply",
      "There is no restriction on medical school admissions. It's really just residencies.\n \nreply",
      "My understanding is that this is a distinction that doesn't make a difference?  Without a residency, can you become a doctor?\n \nreply",
      "That's mostly correct, but there are a few students who enroll in medical school with no intention of becoming practicing physicians. They want to go into some other career like research or technology or hospital administration.\n \nreply",
      "The supply of doctors is limited by the AMA and state MA, to avoid excess doctors = price competition\n \nreply"
    ],
    "link": "https://www.cnn.com/2025/02/04/business/air-traffic-controller-shortage/index.html",
    "first_paragraph": "Markets \n\n\nHot Stocks \n\n\nFear & Greed Index \n\n\n\n            Latest Market News \n\n\n\n            Hot Stocks \n\n\nFollow:\n            The US air traffic control system has been stretched nearly to its breaking point by a decades-long staffing shortage. It\u2019s causing problems not just for the air traffic controllers that remain but the flying public at large.\n    \n            And it won\u2019t get better any time soon.\n    \n            The Federal Aviation Administration, which runs the air traffic system, stepped up the pace of hiring in 2024 under President Joe Biden. But even though 2,000 qualified applicants were hired last year, they might only just barely replace the 1,100 who left the job either through retirement or due to the heavy toll the stressful job takes on those who enter the field.\n    \n            That\u2019s because nearly half of those hired in any given year will wash out of the program before they get to actually control aircraft after about three years from their initial start da"
  },
  {
    "title": "How to scale your model: A systems view of LLMs on TPUs (jax-ml.github.io)",
    "points": 144,
    "submitter": "mattjjatgoogle",
    "submit_time": "2025-02-04T18:56:08 1738695368",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=42936910",
    "comments": [
      "I am really looking forward for JAX to take over pytorch/cuda over the next years. The whole PTX kerfuffle with Deepseek team shows the value of investing in more low levels approaches to squeeze out the most out of your hardware.\n \nreply",
      "Most Pytorch users don\u2019t bother even with the simplest performance optimizations, and you are talking about PTX.\n \nreply",
      "I like JAX but I'm not sure how an ML framework debate like \"JAX vs PyTorch\" is relevant to DeepSeek/PTX. The JAX API is at a similar level of abstraction to PyTorch [0]. Both are Python libraries and sit a few layers of abstraction above PTX/CUDA and their TPU equivalents.[0] Although PyTorch arguably encompasses 2 levels, with both a pure functional library like the JAX API, as well as a \"neural network\" framework on top of it. Whereas JAX doesn't have the latter and leaves that to separate libraries like Flax.\n \nreply",
      "You do understand that PTX is part of CUDA right?\n \nreply",
      "This has been my bible for performance work internally at Google. Kind of surprised they released it publicly, but I guess they removed all the Gemini-specific details.\n \nreply",
      "An author's tweet thread: https://x.com/jacobaustin132/status/1886844716446007300\n \nreply",
      "Here in the thread he says: https://x.com/jacobaustin132/status/1886844724339675340 : `5 years ago, there were many ML architectures, but today, there is (mostly) only one [transformers].`To what degree is this actually true, and what else is on the horizon that might become as popular as transformers?\n \nreply",
      "Not strictly related, but does anyone know why JAX uses tracing and not AST via reflection?\n \nreply",
      "The short answer is that tracing is way, way easier to implement in a predictable and reliably performant way. This especially matters for distributed computation and automatic differentiation, two areas where JAX shines.AST parsing via reflection means your ML compiler needs to re-implement all of Python, which is not a small language. This is a lot of work and hard to do well with abstractions that are not designed for those use-cases. (I believe Julia's whole language auto-diff systems struggle for essential the same reason.)\n \nreply",
      "This is awesome! Can't wait to read it. I've been very curious about why we don't hear more about LLMs on TPUs.\n \nreply"
    ],
    "link": "https://jax-ml.github.io/scaling-book/",
    "first_paragraph": "A Systems View of LLMs on TPUs (Part 0: Intro | Part 1: Rooflines)Training LLMs often feels like alchemy, but understanding and optimizing the performance of your models doesn't have to. This book aims to demystify the science of scaling language models on TPUs: how TPUs work and how they communicate with each other, how LLMs run on real hardware, and how to parallelize your models during training and inference so they run efficiently at massive scale. If you've ever wondered \u201chow expensive should this LLM be to train or \u201chow much memory do I need to serve this model myself\u201d or \u201cwhat's an AllGather\u201d, we hope this will be useful to you.Much of deep learning still boils down to alchemy, but understanding and optimizing the performance of your models doesn\u2019t have to \u2014 even at huge scale! Relatively simple principles apply everywhere \u2014 from dealing with a single accelerator to tens of thousands \u2014 and understanding them lets you do many useful things:Expected background: We\u2019re going to assu"
  },
  {
    "title": "Roc rewrites the compiler in Zig (gist.github.com)",
    "points": 269,
    "submitter": "todsacerdoti",
    "submit_time": "2025-02-04T17:21:02 1738689662",
    "num_comments": 156,
    "comments_url": "https://news.ycombinator.com/item?id=42935516",
    "comments": [
      "This is the first justification of not using Rust that I actually agree with. Well written.I recommend reading Roc's FAQ too - it's got some really great points. E.g. I'm internally screaming YESSS! to this: https://www.roc-lang.org/faq.html#curried-functionsBut then it has other weird features too, like they seem to be really emphasising \"friendliness\" (great!) but then it has weird syntax like `\\` for anonymous functions (I dunno where that dumb syntax came from by Nix also uses it and it's pretty awful).  Omitting brackets and commas for function calls is also a bad decision if you care about friendliness. I have yet to find a language where that doesn't make the code harder to read and understand.\n \nreply",
      "The syntax came from Elm, which got it\u2019s syntax from Haskell (where Nix also got it from) which got its syntax from ML.It\u2019s a syntax that\u2019s several decades old at this point.It\u2019s different, but not harder. If you learned ML first, you\u2019d found Algol/C-like syntax equally strange.\n \nreply",
      "(ETA: speaking strictly about anonymous functions; on rereading you might be talking about the absence of parens and commas for function application.)That's not ML syntax. Haskell got it from Miranda, I guess?In SML you use the `fn` keyword to create an anonymous function; in Ocaml, it's `fun` instead.\n \nreply",
      "Well, ML (or at least the first versions of it) used a \u03bbx \u2022 x syntax [1] for \u03bb-abstractions, the same (excluding the use of \u2022 over .) notation as used with the Lambda Calculus, and I've always assumed \\ was an ASCII stand in.[1]: https://homepages.inf.ed.ac.uk/wadler/papers/papers-we-love/... (can be spotted on page 353)\n \nreply",
      "I believe the `\\` character for functions is original to Haskell. Miranda does not have anonymous functions as a part of the language.\n \nreply",
      "They recently changed the syntax to add parens, commas and use `|arg|` for closures :)https://github.com/roc-lang/roc/releases/tag/0.0.0-alpha2-ro...\n \nreply",
      "Pretty much all of those changes look bad to me.\n \nreply",
      "Boooo\n \nreply",
      "Jesus, why? This is a bummer.\n \nreply",
      "It's the closest you get to '\u03bb' on a US keyboard.\n \nreply"
    ],
    "link": "https://gist.github.com/rtfeldman/77fb430ee57b42f5f2ca973a3992532f",
    "first_paragraph": "\n        Instantly share code, notes, and snippets.\n      This may become a blog post or FAQ entry or something. For now, I'm just writing up something to answer the questions I've gotten about it!Scratch-rewriting a code base in a different language is usually considered a risky move that often leads to failure. In the case of compilers, it's the reverse; most successful compilers (e.g. Java, C++, C, C#, TypeScript, Scala, Go, Rust, Zig, OCaml, Haskell, and many others) have undergone a complete scratch-rewrite...in themselves. This process is called self-hosting.We can break down the observation that self-hosting has a long track record of success into two pieces:Rewrites do more than just change the code base to a more appealing language. They're an opportunity to start fresh, with all the wisdom gained from the first version\u2014to discard accumulated cruft and to avoid mistakes from the last time around (while naturally making some new mistakes). They're also notorious for causing reg"
  }
]