[
  {
    "title": "East Germany balloon escape (wikipedia.org)",
    "points": 296,
    "submitter": "robertvc",
    "submit_time": "2026-01-16T17:16:33 1768583793",
    "num_comments": 98,
    "comments_url": "https://news.ycombinator.com/item?id=46648916",
    "comments": [
      "The photo of the balloon here really helps put the story into perspective.https://web.archive.org/web/20190408181736/https://www.museu...reply",
      "> \u201cAre we here in the West??\u201d Only this one question is asked by Peter Strelzyk and G\u00fcnter Wetzel when they were in the early morning of the 16th.The \"Handmaid's Tale\" TV series has a great variation on that moment, which chokes me up every single time.(spoilers in video title) https://www.youtube.com/watch?v=0oKZgXvpm0creply",
      "I remember when 4chan made fun of Handmaids Tale as some insane liberal fever dream. Turns out there are influential Americans who really want to turn back the clock a thousand years...reply",
      "The text in the link is nice to read. I did a Google Translate on it which you can read here: https://pastebin.com/SdkKQkC6reply",
      ">Ballonfahrt in die FreiheitGotta love the way German sounds to English ears. Always good for a chuckle.This guy is a hacker hero - do the engineering needed, get the proof of concept built, move fast, break things, start over and go big, then scores a victory over the commies and saves his family.reply",
      "I enjoyed learning about the town of Bad Kissingenreply",
      "The private mechanics and electrical hacking culture that is the base for German engineering . T\u00fbftler in jedem Schuppen..reply",
      "I keep on thinking it was a lot smaller! Wow!reply",
      "Disney made a movie about this called Night Crossing in the early 1980s. More recently, there's a 2018 German movie about it called Balloon.[0] https://www.imdb.com/title/tt0082810/[1] https://www.imdb.com/title/tt7125774reply",
      "The 2018 film is a really good movie, I would highly recommend checking it out!reply"
    ],
    "link": "https://en.wikipedia.org/wiki/East_Germany_balloon_escape",
    "first_paragraph": ""
  },
  {
    "title": "Cloudflare acquires Astro (astro.build)",
    "points": 704,
    "submitter": "todotask2",
    "submit_time": "2026-01-16T14:25:54 1768573554",
    "num_comments": 327,
    "comments_url": "https://news.ycombinator.com/item?id=46646645",
    "comments": [
      "Very nice to see these dev tools get an exit. e.g. I love `uv` and friends but did consider that perhaps dev tools are just a bad business and then no one will go into making that kind of stuff. Good exits means more of these tools.I have only used Astro for toy stuff but it seemed neat. Congrats to the team.EDIT: To put paid to the sidebar discussion below, yes I meant \"for instance, consider `uv`; they might do these nice things and go nowhere but now that companies like Bun and Astro have gotten acquired, it demonstrates a future for others; therefore we will get more things like Astral's `uv` and so on\". Hope that clarifies.reply",
      "This is Astro, not Astral. uv is Astral :-)Edit: OP clarified what they meant, I'm sorry for the misunderstanding on my part!reply",
      "They know, hence why they used e.g., i.e. exempli gratiareply",
      "For the perplex:e.g. is latin for \"exempli gratia\" = for example\ni.e. is latin for \"id est\" = that isreply",
      "As someone who was perplexed, I've only heard perplex used in past tense (I was perplexed) so seeing \"For the perplex\" just made me confused as to what \"perplex\" meant and I had to do a further search to decipher this tree of comments hahareply",
      "A good way to remember it is to use a backronym:e.g. - example giveni.e. - in effectreply",
      "I'm not sure. I wouldn't generally call Astro a \"dev tool\". It's more of a framework.It's possible you are right, but it isn't clear from the content of the comment.reply",
      "Frameworks are a category of development tool. Things that developers utilitise to be productive.reply",
      "IMO saying a framework is a dev tool is like saying a cake mix is a cooking tool, because it allows you to be more productive when making a cake. Sure, if you look at it a certain way, it is correct. But that isn't the way the term is usually used.reply",
      "Like coffee?reply"
    ],
    "link": "https://astro.build/blog/joining-cloudflare/",
    "first_paragraph": "The Astro Technology Company \u2014 the company behind the Astro web framework \u2014 is joining Cloudflare! Adoption of the Astro web framework continues to double every year, and Astro 6 is right around the corner. With Cloudflare\u2019s support, we\u2019ll have more resources and fewer distractions to continue our mission to build the best framework for content-driven websites.What this means for Astro:In 2021, Astro was born out of frustration. The trend at the time was that every website should be architected as an application, and then shipped to the user\u2019s browser to render. This was not very performant, and we\u2019ve spent the last decade coming up with more and more complex solutions to solve for that performance problem. SSR, ISR, RSC, PPR, TTI optimizations via code-splitting, tree-shaking, lazy-loading, all to generate a blocking double-data hydration payload from a pre-warmed server running halfway around the world.Our mission to design a web framework specifically for building websites \u2014 what we"
  },
  {
    "title": "Releasing rainbow tables to accelerate Net-NTLMv1 protocol deprecation (cloud.google.com)",
    "points": 71,
    "submitter": "linolevan",
    "submit_time": "2026-01-16T21:42:24 1768599744",
    "num_comments": 39,
    "comments_url": "https://news.ycombinator.com/item?id=46652617",
    "comments": [
      "This empowers script kiddies, but not significantly moreso than they already were. Of all the places this is still in use, they've been exposed for years, so this isn't likely to result in a a bunch of new exploitations.However, it's most likely to be used by governments, with legacy servers that are finicky, with filesharing set up that's impacted other computers configured for compatibility, or legacy ancient network gear or printers.I wonder who they're pushing around, and what the motivation is?reply",
      "Mandiant is Google's incident response consulting business. Having worked for many years in that field myself (though not for Mandiant), they're probably sick of going to the same old engagements where companies have been getting owned the same way over and over again for the last 15 years.What releases like this do is give IT ops people the ammunition they need to convince their leadership to actually spend some money on fixing systemic security problems.reply",
      "It also empowers IT depts and cybersecurity people to be able to easily build a PoC to show why moving on from the deprecated protocol is important.  In many white-hat jobs you can't just grab rainbow tables from a torrent, so a resource like this is helpful.  For the grays and black hats, they've had access to rainbow tables like this for a very long time, so no change there.reply",
      "Out of curiosity, why can't white hats grab rainbow tables from torrents? Is it about seeding?reply",
      "You've been able to find these for years. In fact it's entirely possible they just grabbed some or all of them out of an existing torrent originally.It would completely not surprise me if there are automagic attacks on net-ntlmv1 at this point against some cloud hosted storage. This has been doable by anyone since like 2016 if you had the space and weren't prevented from using that protocol version.reply",
      "I suspect Mandiant hears a lot of \"this is impractical to exploit so we don't care\" from their clients. Now they have a compelling rebuttal to that.reply",
      "And terrorism is just an abstract way of securing underprepared government facilities.reply",
      "I recall using ntlm rainbow tables to crack windows hashes in high school in like 2008?Amazing that this is still around and causing someone enough of a headache to justify spending money on.Also amazing what a teenager with lots of free time and a bootable Linux usb can get up to.reply",
      "There used to be a joint online project to compute these tables in a SETI like distributed system. Everyone who contributed their CPU cycles, could use the tables. And yeah, around 2005-2008.reply",
      "LM, nthash aka NTLM, net-ntlmv1 aka ntlmv1, net-ntlmv2 aka NTLMv2.  Challenge response stuff is different. Naming here is painful.reply"
    ],
    "link": "https://cloud.google.com/blog/topics/threat-intelligence/net-ntlmv1-deprecation-rainbow-tables",
    "first_paragraph": "Stop attacks, reduce risk, and advance your security.Written by: Nic LosbyMandiant is publicly releasing a comprehensive dataset of Net-NTLMv1 rainbow tables to underscore the urgency of migrating away from this outdated protocol. Despite Net-NTLMv1 being deprecated and known to be insecure for over two decades\u2014with cryptanalysis dating back to 1999\u2014Mandiant consultants continue to identify its use in active environments. This legacy protocol leaves organizations vulnerable to trivial credential theft, yet it remains prevalent due to inertia and a lack of demonstrated immediate risk.By releasing these tables, Mandiant aims to lower the barrier for security professionals to demonstrate the insecurity of Net-NTLMv1. While tools to exploit this protocol have existed for years, they often required uploading sensitive data to third-party services or expensive hardware to brute-force keys. The release of this dataset allows defenders and researchers to recover keys in under 12 hours using co"
  },
  {
    "title": "LLM Structured Outputs Handbook (nanonets.com)",
    "points": 101,
    "submitter": "vitaelabitur",
    "submit_time": "2026-01-15T16:46:50 1768495610",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=46635309",
    "comments": [
      "This is a seriously beautiful guide. I really appreciate you putting this together! I especially love the tab-through animations on the various pages, and this is one of the best explanations that I've seen. I generally feel I understand grammar-constrained generation pretty well (I've merged a handful of contributions to the llama.cpp grammar implementation), and yet I still learned some insights from your illustrations -- thank you!I'm also really glad that you're helping more people understand this feature, how it works, and how to use it effectively. I strongly believe that structured outputs are one of the most underrated features in LLM engines, and people should be using this feature more.Constrained non-determinism means that we can reliably use LLMs as part of a larger pipeline or process (such as an agent with tool-calling) and we won't have failures due to syntax errors or erroneous \"Sure! Here's your output formatted as JSON with no other text or preamble\" messages thrown in.Your LLM output might not be correct. But grammars ensure that your LLM output is at least _syntactically_ correct. It's not everything, but it's not nothing.And especially if we want to get away from cloud deployments and run effective local models, grammars are an incredibly valuable piece of this. For practical examples, I often think of Jart's example in her simple LLM-based spam-filter running on a Raspberry Pi [0]:> llamafile -m TinyLlama-1.1B-Chat-v1.0.f16.gguf \\\n>           --grammar 'root ::= \"yes\" | \"no\"' --temp 0 -c 0 \\\n>           --no-display-prompt --log-disable -p \"<|user|>\n> Can you say for certain that the following email is spam? ...Even though it's a super-tiny piece of hardware, by including a grammar that constrains the output to only ever be \"yes\" or \"no\" (it's impossible for the system to produce a different result), then she can use a super-small model on super-limited hardware, and it is still useful. It might not correctly identify spam, but it's never going to break for syntactic reasons, which gives a great boost to the usefulness of small, local models.* [0]: https://justine.lol/matmul/reply",
      "What does it do when the model wants to return something else, and what's better/worse about doing it in llamafile vs whatever wrapper that's calling it? How do I set retries? What if I want JSON and a range instead?reply",
      "This is a fantastic guide! I did a lot of work on structured generation for my PhD. Here are a few other pointers for people who might be interested:Some libraries:- Outlines, a nice library for structured generation  - https://github.com/dottxt-ai/outlines\n\n- Guidance (already covered by FlyingLawnmower in this thread), another nice library  - https://github.com/guidance-ai/guidance\n\n- XGrammar, a less-featureful but really well optimized constrained generation library  - https://github.com/mlc-ai/xgrammar\n\n  - This one has a lot of cool technical aspects that make it an interesting project\n\nSome papers:- Efficient Guided Generation for Large Language Models  - By the outlines authors, probably the first real LLM constrained generation paper\n\n  - https://arxiv.org/abs/2307.09702\n\n- Automata-based constraints for language model decoding  - A much more technical paper about constrained generation and implementation\n\n  - https://arxiv.org/abs/2407.08103\n\n- Pitfalls, Subtleties, and Techniques in Automata-Based Subword-Level Constrained Generation  - A bit of self-promotion. We show where constrained generation can go wrong and discuss some techniques for the practitioner\n\n  - https://openreview.net/pdf?id=DFybOGeGDS\n\nSome blog posts:- Fast, High-Fidelity LLM Decoding with Regex Constraints  - Discusses adhering to the canonical tokenization (i.e., not just the constraint, but also what would be produced by the tokenizer)\n\n  - https://vivien000.github.io/blog/journal/llm-decoding-with-regex-constraints.html\n\n- Coalescence: making LLM inference 5x faster  - Also from the outlines team\n\n  - This is about skipping inference during constrained generation if you know there is only one valid token (common in the canonical tokenization setting)\n\n  - https://blog.dottxt.ai/coalescence.htmlreply",
      "What a gold mine!Automata-based constraints is fun.reply",
      "This information is really presented well. I subscribed to your newsletter. Thanks!reply",
      "Very nicely written guide!If the authors or readers are interested in some of the more technical details of how we optimized guidance & llguidance, we wrote up a little paper about it here: https://guidance-ai.github.io/llguidance/llg-go-brrrreply",
      "I agree that building agents is basically impossible if you cannot trust the model to output valid json every time. This seems like a decent collection of the current techniques we have to force deterministic structure for production systems.reply",
      "Huge fan of BAML , nice coveragereply",
      "This is a nice guide. I especially like the masked decoding diagrams on this page https://nanonets.com/cookbooks/structured-llm-outputs/basic-....edit: Somehow that link doesn't work... It's the diagram on the \"constrained method\" pagereply",
      "One of the authors here, will checkout the diagram link.Every commercial model provider is adding structured outputs so will keep updating the guide.reply"
    ],
    "link": "https://nanonets.com/cookbooks/structured-llm-outputs",
    "first_paragraph": "LLMs mostly produce syntactically valid outputs when we try generating JSON, XML, code, etc., but they can occasionally fail due to their probabilistic nature. This is a problem for developers as we use LLMs programmatically, for tasks like data extraction, code generation, tool calling, etc.There are many deterministic ways to ensure structured LLM outputs. If you are a developer, this handbook covers everything you need.Structured generation is moving too fast. Most resources you find today are already outdated. You have to dig through multiple academic papers, blogs, GitHub repos, and other resources.This handbook brings it all together in a living document that updates regularly.You can read it start-to-finish, or treat it like a lookup table.We're the maintainers of Nanonets-OCR models (VLMs to convert documents into clean, structured Markdown) and docstrange (open-source document processing library).Updates from the LLM developer community in your inbox. Twice a month."
  },
  {
    "title": "6-Day and IP Address Certificates Are Generally Available (letsencrypt.org)",
    "points": 335,
    "submitter": "jaas",
    "submit_time": "2026-01-16T15:37:19 1768577839",
    "num_comments": 202,
    "comments_url": "https://news.ycombinator.com/item?id=46647491",
    "comments": [
      "As already noted on this thread, you can't use certbot today to get an IP address certificate. You can use lego [1], but figuring out the exact command line took me some effort yesterday. Here's what worked for me:    lego --domains 206.189.27.68 --accept-tos --http --disable-cn run --profile shortlived\n\n[1] https://go-acme.github.io/lego/reply",
      "I wonder if the support made it to Caddy yet(seems to be WIP https://github.com/caddyserver/caddy/issues/7399)reply",
      "It works, but as another comment mentioned there may be quirks with IP certs, specifically IPv6, that I hope will be fixed by v2.11.reply",
      "IPv4 certs are already working fine for me in Caddy, but I think there's some kinks to work out with IPv6.reply",
      "IP address certificates are particularly interesting for iOS users who want to run their own DoH servers.A properly configured DoH server (perhaps running unbound) with a properly constructed configuration profile which included a DoH FQDN with a proper certificate would not work in iOS.The reason, it turns out, is that iOS insisted that both the FQDN and the IP have proper certificates.This is why the configuration profiles from big organizations like dns4eu and nextdns would work properly when, for instance, installed on an iphone ... but your own personal DoH server (and profile) would not.reply",
      "OpenSSL is quite particular about the IP address being included in the SAN field of the cert when making a TLS connection, fwiw. iOS engineers may not have explicitly added this requirement and it might just be a side effect of using a crypto library.reply",
      "I use DoH behind a reverse proxy with my own domain daily without any kind of issuereply",
      "Why 6 day and not 8?- 8 is a lucky number and a power of 2- 8 lets me refresh weekly and have a fixed day of the week to check whether there was some API 429 timeout- 6 is the value of every digit in the number of the beast- I just don't like 6!reply",
      "> 8 lets me refresh weekly and have a fixed day of the week to check whether there was some API 429 timeoutThere\u2019s your answer.6 days means on a long enough enough timeframe the load will end up evenly distributed across a week.8 days would result in things getting hammered on specific days of the week.reply",
      "> 6 days means on a long enough enough timeframe the load will end up evenly distributed across a week.people will put */5 in cron and result will be same, because that's obvious, easy and nice number.reply"
    ],
    "link": "https://letsencrypt.org/2026/01/15/6day-and-ip-general-availability",
    "first_paragraph": "Short-lived and IP address certificates are now generally available from Let\u2019s Encrypt. These certificates are valid for 160 hours, just over six days. In order to get a short-lived certificate subscribers simply need to select the \u2018shortlived\u2019 certificate profile in their ACME client.Short-lived certificates improve security by requiring more frequent validation and reducing reliance on unreliable revocation mechanisms. If a certificate\u2019s private key is exposed or compromised, revocation has historically been the way to mitigate damage prior to the certificate\u2019s expiration. Unfortunately, revocation is an unreliable system so many relying parties continue to be vulnerable until the certificate expires, a period as long as 90 days. With short-lived certificates that vulnerability window is greatly reduced.Short-lived certificates are opt-in and we have no plan to make them the default at this time. Subscribers that have fully automated their renewal process should be able to switch to "
  },
  {
    "title": "Cursor's latest \u201cbrowser experiment\u201d implied success without evidence (embedding-shapes.github.io)",
    "points": 394,
    "submitter": "embedding-shape",
    "submit_time": "2026-01-16T14:37:49 1768574269",
    "num_comments": 169,
    "comments_url": "https://news.ycombinator.com/item?id=46646777",
    "comments": [
      "The comment that points out that this week-long experiment produced nothing more than a non-functional wrapper for Servo (an existing Rust browser) should be at the top:https://news.ycombinator.com/item?id=46649046reply",
      "Has anyone tried to rewrite some popular open source project with IA? I imagine modern LLMs can be very effective at license-washing/plagiarizing dependencies, it could be an interesting new benchmark tooreply",
      "I think it's fair enough to consider porting a subset of rewriting, in which case there are several successful experiments out there:- JustHTML [1], which in practice [2] is a port of html5ever [3] to Python.- justjshtml, which is a port of JustHTML to JavaScript :D [4].- MiniJinja [5] was recently ported to Go [6].All three projects have one thing in common: comprehensive test suites which were used to guardrail and guide AI.References:1. https://github.com/EmilStenstrom/justhtml2. https://friendlybit.com/python/writing-justhtml-with-coding-...3. https://github.com/servo/html5ever4. https://simonwillison.net/2025/Dec/15/porting-justhtml/5. https://github.com/mitsuhiko/minijinja6.  https://lucumr.pocoo.org/2026/1/14/minijinja-go-port/reply",
      "As the author, it's a stretch to say that JustHTML is a port of html5ever. While you're right that this was part of the initial prompt, the code is very different, which is typically not what counts as \"port\". Your mileage may wary.reply",
      "Interesting, IIUC the transformer architecture / attention mechanism were initially designed for use in the language translation domain. Maybe after peeling back a few layers, that's still all they're really doing.reply",
      "This has long been how I have explained LLMs to non-technical people: text transformation engines. To some extent, many common, tedious, activities basically constitute a transformation of text into one well known form from another (even some kinds of reasoning are this) and so LLMs are very useful. But they just transform text between well known forms.reply",
      "Not me personally, but a GitHub user wrote a replacement for Go's regexp library that was \"up to 3-3000x+ faster than stdlib\": https://github.com/coregx/coregex ... at first I was impressed, so started testing it and reporting bugs, but as soon as I ran my own benchmarks, it all fell apart (https://github.com/coregx/coregex/issues/29). After some mostly-bot updates, that issue was closed. But someone else opened a very similar one recently (https://github.com/coregx/coregex/issues/79) -- same deal, \"actually, it's slower than the stdlib in my tests\". Basically AI slop with poor tests, poor benchmarks, and way oversold. How he's positioning these projects is the problematic bit, I reckon, not the use of AI.Same user did a similar thing by creating an AWK interpreter written in Go using LLMs: https://github.com/kolkov/uawk -- as the creator of (I think?) the only AWK interpreter written in Go (https://github.com/benhoyt/goawk), I was curious. It turns out that if there's only one item in the training data (GoAWK), AI likes to copy and paste freely from the original. But again, it's poorly tested and poorly benchmarked.I just don't see how one can get quality like this, without being realistic about code review, testing, and benchmarking.reply",
      "Apparebtly this person actually got it to compile: https://xcancel.com/CanadaHonk/status/2011612084719796272#mreply",
      "https://x.com/CanadaHonk/status/2011612084719796272 as well.I went through the motions. There are various points in the repo history where compilation is possible, but it's obscure. They got it to compile and operate prior to the article, but several of the PRs since that point broke everything, and this guy went through the effort of fixing it. I'm pretty sure you can just identify the last working commit and pull the version from there, but working out when  looks like a big pain in the butt for a proof of concept.reply",
      "> but several of the PRs since that point broke everything, and this guy went through the effort of fixing it. I'm pretty sure you can just identify the last working commit and pull the version from there, but working out when looks like a big pain in the butt for a proof of concept.I went through the last 100 commits (https://news.ycombinator.com/item?id=46647037) and nothing there was working (yet/since). Seems now after a developer corrected something it managed to pass `cargo check` without errors, since commit 526e0846151b47cc9f4fcedcc1aeee3cca5792c1 (Jan 16 02:15:02 2026 -0800)reply"
    ],
    "link": "https://embedding-shapes.github.io/cursor-implied-success-without-evidence/",
    "first_paragraph": "\n        2026-01-16\n      On January 14th 2026, Cursor published a blog post titled \"Scaling\nlong-running autonomous coding\" (https://cursor.com/blog/scaling-agents)In the blog post, they talk about their experiments with running\n\"coding agents autonomously for weeks\" with the explicit goal ofunderstand[ing] how far we can push the frontier of agentic coding\nfor projects that typically take human teams months to completeThey talk about some approaches they tried, why they think those\nfailed, and how to address the difficulties.Finally they arrived at a point where something \"solved most of our\ncoordination problems and let us scale to very large projects without\nany single agent\", which then led to this:To test this system, we pointed it at an ambitious goal: building a\nweb browser from scratch. The agents ran for close to a week, writing\nover 1 million lines of code across 1,000 files. You can explore the\nsource code on GitHub (https://github.com/wilsonzlin/fastrender)This is where th"
  },
  {
    "title": "Michelangelo's first painting, created when he was 12 or 13 (openculture.com)",
    "points": 302,
    "submitter": "bookofjoe",
    "submit_time": "2026-01-16T13:44:25 1768571065",
    "num_comments": 156,
    "comments_url": "https://news.ycombinator.com/item?id=46646263",
    "comments": [
      "I don't trust this for one bit.  For the owners there is quite the incentive to label this as the work of a genius.  But in reality, this is just pretty complex for a 12 year-old to produce by yourself.Edit: as others have pointed out, and if I were to actually read the article carefully before commenting, the composition is not attributed to Michelangelo.  So it is just a copy.  Quite the achievement, but possible for a twelve-year old.I once confronted a gallery owner who was proudly presenting a newly discovered work by Mondriaan [1].  An original black and white photo in an old newspaper [2] was shown as proof of authenticity.  But many details such as the creases in fabric differ in the original and the new painting.  No OpenCV required to see that.  Mind you, the picture is already framed with Mondriaan standing next to it.  Unlikely that he's still working on it.Instead of responding, the gallery owner simply turned away.[1] https://upload.wikimedia.org/wikipedia/commons/b/bb/Cavalini...[2] https://www.vrt.be/vrtnws/nl/2022/03/02/nieuwe-werken-mondri...reply",
      "Just to engage with your \u201c12 year old to produce by yourself\u201d , here are some examples of art made by Picasso in his early teens to mid teens.It\u2019s absolutely possible to be that good. Especially in the middle ages / early renaissance with the work you did for guilds and working for masters as an apprentice.At eleven years old:\nhttps://www.pablo-ruiz-picasso.net/work-3939.phpAt fourteen at his sisters wedding:\nhttps://www.pablo-ruiz-picasso.net/work-9.phpAt fifteen\nhttps://www.pablo-ruiz-picasso.net/work-11.phpreply",
      "Wow. A great service to us as these are almost a \"photo\" into that world.reply",
      "Klimt did the first three when he was 17. There are even earlier works which are not much less sophisticated.http://art-klimt.com/early_works.htmlSome people are just prodigies - very, very few, but it's a real phenomenon. Even with early craft training, which people don't get today, exceptional talent still cuts through.This is why the common \"There's no such thing as talent, it's just hard work\" line can't possibly be true. It's soothing to believe that you too could be a genius if only you put the hours in, but it just doesn't work like that.Ability is set by a talent ceiling, which is on a bell curve. \"Most people don't reach their ceiling\" and \"There are extreme outliers of native ability\" can both be true at the same time.reply",
      "Along with the self-deluding work=genius idea:- Some use extreme outliers to justify their own failure to get close to their ceiling.  \"I can't be Einstein, why should I try?\"- Some (parents, coaches, motivational speakers) also use extreme outliers to claim there are no limits/ceilings for others.  \"If you can dream it you can do it!\" (but somehow it doesn't seem to apply to them)reply",
      "> This is why the common \"There's no such thing as talent, it's just hard work\" line can't possibly be true.please stop killing my delusions.reply",
      "The best essay I read last year described how there are two types of artists: those born with great talent, that usually create their masterpieces in their early 20s and coast for the rest of their life, and those that take most of their adulthood before finding their voice, peaking late in their 40s and 50s. The author used Picasso as an example of the former, and Kurt Vonnegut for the latter.Gave me the greatest impulse to explore my creative drive like nothing else before, after spending my 20s lost in a daze. I know you\u2019re joking, but if you aren\u2019t, do not lose hope yet.reply",
      "But he had not been an apprentice before making this, he started the apprenticeship that year, and this is supposed to be the first thing he ever painted.> Michelangelo's biographers\u2014Giorgio Vasari (1511\u20131574) and Ascanio Condivi (1525\u20131574)\u2014tell us that, aside from some drawings, his first work was a painted copy after a well-known engraving by Martin Schongauer (1448\u20131491) showing Saint Anthony tormented by demons. Made about 1487\u201388 under the guidance of his friend and fellow pupil Francesco Granacci, Michelangelo's painting was much admired; it was even said to have incited Ghirlandaio's envy.\n[https://www.metmuseum.org/exhibitions/listings/2009/michelan...]reply",
      "I can\u2019t respond much too this being Michelangelo\u2019 painting, but if he was an apprentice under his fellow pupil it\u2019s possible that he just did minor things or filled in. It was how you learned. You did final retouching and such.reply",
      "True, no phones, no distractions, I can see someone who finds their passion early on to get this good.reply"
    ],
    "link": "https://www.openculture.com/2026/01/discover-michelangelos-first-painting.html",
    "first_paragraph": "in Art, History |   \tJanuary 15th, 2026  3 CommentsThink back, if you will, to the works of art you cre\u00adat\u00aded at age twelve or thir\u00adteen. For many, per\u00adhaps most of us, our out\u00adput at that stage of ado\u00adles\u00adcence amount\u00aded to direc\u00adtion\u00adless doo\u00addles, chaot\u00adic comics, and a few unsteady-at-best school projects. But then, most of us did\u00adn\u2019t grow up to be Michelan\u00adge\u00adlo. In the late four\u00adteen-eight\u00adies, when that tow\u00ader\u00ading Renais\u00adsance artist was still what we would now call a \u201ctween,\u201d he paint\u00aded The Tor\u00adment of Saint Antho\u00adny, a depic\u00adtion of the tit\u00adu\u00adlar reli\u00adgious fig\u00adure beset by demons in the desert. Though based on\u00a0a wide\u00adly known engrav\u00ading, it nev\u00ader\u00adthe\u00adless shows evi\u00addence of rapid\u00adly advanc\u00ading tech\u00adnique, inspi\u00adra\u00adtion, and even\u00a0cre\u00adativ\u00adi\u00adty \u2014 espe\u00adcial\u00adly when placed under the infrared scan\u00adner.For about half a mil\u00adlen\u00adni\u00adum, The Tor\u00adment of Saint Antho\u00adny\u00a0was\u00adn\u2019t thought to have been paint\u00aded by Michelan\u00adge\u00adlo. As explained in the video from Inspi\u00adrag\u00adgio just below, whe"
  },
  {
    "title": "Just the Browser (justthebrowser.com)",
    "points": 487,
    "submitter": "cl3misch",
    "submit_time": "2026-01-16T12:03:52 1768565032",
    "num_comments": 236,
    "comments_url": "https://news.ycombinator.com/item?id=46645615",
    "comments": [
      "I had a look at what it actually does in the Firefox settings and all it seems to do is to disable one AI feature flag, change the default search engine, and then set a few other flags that are changes that you may or may not want to make, unrelated to AI. Not sure you want to run a 3rd party shell script just to do that\u2026reply",
      "For anyone interestedThis is the shell script it runs on Mac/Linux: https://github.com/corbindavenport/just-the-browser/blob/mai...For FireFox it downloads this: https://github.com/corbindavenport/just-the-browser/blob/mai...  {\n    \"policies\": {\n      \"DisableFirefoxStudies\": true,\n      \"DisableTelemetry\": true,\n      \"DontCheckDefaultBrowser\": true,\n      \"FirefoxHome\": {\n        \"SponsoredStories\": false,\n        \"SponsoredTopSites\": false,\n        \"Stories\": false\n      },\n      \"GenerativeAI\": {\n        \"Enabled\": false\n      },\n      \"SearchEngines\": {\n        \"Remove\": [\n          \"Perplexity\"\n        ]\n      }\n    }\n  }reply",
      "The second sentence in the Getting Started section invites you to follow the manual guides instead of running the 3rd party shell scripts. I think this is a good way to do it -- have both options and tell people about them right at the start of the process. Is there some other way you wish they'd share this info?reply",
      "Seems like this is for the people that need to execute random powershell scripts they don't understand in order to turn of telemetry and copilot on Windows because reading about the registry and group policy is too much for them.reply",
      "The Getting Started section invites you to follow the manual guides instead of running the scripts. That's what I did, and I really appreciate the site/guides.reply",
      "I mean, yes ...?Stupid to run random scripts you find online, but browser makers push users into it.My son wants to eat \"Chinese\"  food with chopsticks, but he can only really use a fork, so we adapt the chopsticks. He'll be able to use them eventually, but not everyone has a) the desire, nor b) the dexterity.Making it easier to do what users want with a computer without telling them 'just learn to program' (or script in this case) is actually a good thing imo.reply",
      "> users want with a computer without telling them 'just learn to program'A computer is meant to be programmed by the user. That is its raison d'\u00eatre from the very beginning and why it is called like that.reply",
      "Some people want computers, some people just want to use them like appliances. The bigger problem is those companies who want to control other people's computers no matter which type of person they are.reply",
      "Seems like you're using your computer wrong by posting here then. In fact, 99% of people are using their computing devices wrong all the time. The computer must be the most misused tool in the world.reply",
      "It's not hard to search for a few keys in the about:config menu or to set a group policy.  If you can't be bother to do this you have zero business running random scripts that update your system configuration that you have no idea how it works.Normie users would be better off reading some detailed step-by-step instructions on how to do it by hand using built-in methods than to run random code from the internet that can be malicious.My mom is 75 years old and barely knows how to use a web browser to begin with.  There is zero chance I encourage her to run random pwsh scripts from the internet.God forbid we're going to start giving them AI agents to do this kind of stuff for them.  God help us.reply"
    ],
    "link": "https://justthebrowser.com/",
    "first_paragraph": "Just the Browser helps you remove AI features, telemetry data reporting, sponsored content, product integrations, and other annoyances from desktop web browsers. The goal is to give you \"just the browser\" and nothing else, using hidden settings in web browsers intended for companies and other organizations.This project includes configuration files for popular web browsers, documentation for installing and modifying them, and easy installation scripts. Everything is open-source on GitHub.The setup script can install the configuration files in a few clicks. You can also follow the manual guides for Google Chrome, Microsoft Edge, and Firefox.Windows: Open a PowerShell prompt as Administrator. You can do this by right-clicking the Windows button in the taskbar, then selecting the \"Terminal (Admin)\" or \"PowerShell (Admin)\" menu option. Next, copy the below command, paste it into the window (Ctrl+V), and press the Enter/Return key:Mac and Linux: Search for the Terminal in your applications l"
  },
  {
    "title": "FLUX.2 [Klein]: Towards Interactive Visual Intelligence (bfl.ai)",
    "points": 13,
    "submitter": "GaggiX",
    "submit_time": "2026-01-16T23:46:17 1768607177",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence",
    "first_paragraph": "Today, we release the FLUX.2 [klein] model family, our fastest image models to date. FLUX.2 [klein] unifies generation and editing in a single compact architecture, delivering state-of-the-art quality with end-to-end inference as low as under a second. Built for applications that require real-time image generation without sacrificing quality, and runs on consumer hardware with as little as 13GB VRAM.Try it now for free hereDemo showing editing with FLUX.2 [klein]Visual Intelligence is entering a new era. As AI agents become more capable, they need visual generation that can keep up; models that respond in real-time, iterate quickly, and run efficiently on accessible hardware.The klein name comes from the German word for \"small\", reflecting both the compact model size and the minimal latency. But FLUX.2 [klein] is anything but limited. These models deliver exceptional performance in text-to-image generation, image editing and multi-reference generation, typically reserved for much large"
  },
  {
    "title": "Dell UltraSharp 52 Thunderbolt Hub Monitor (dell.com)",
    "points": 125,
    "submitter": "cebert",
    "submit_time": "2026-01-16T17:14:15 1768583655",
    "num_comments": 169,
    "comments_url": "https://news.ycombinator.com/item?id=46648885",
    "comments": [
      "I have 3 27\" 5k monitors in portrait and a 32\" 4k horizontal above those. It is all mounted with vesa cheeseplates to manfrotto magic arms on t slot aluminum attached to a C stand with manfrotto super clamps. I also have two genelec studio monitors which sound amazing.All of that cost less than this one monitor.reply",
      "What Genelecs are you using??reply",
      "what's the make/model for the monitors?  my setup is getting long in the tooth.reply",
      "1. I don't think I'll ever buy Dell again. My current monitor is a Dell S3221QS 32\" screen and it has vertical lines and starts flickering on both the Macbook M1 and the Mac Studio with the M4 Max chip after some time, which is a known issue[0][1]. It also defaults to YPbPr colors rather than RGB/SRGB, so the colors look off. I'm using HDMI to HDMI connectivity currently.Part of it is also my fault as I thought a monitor would work with any computer.2. That aside, what are you all using for window management on these large screens? I'm currently using Rectangle on Mac, but I was wondering if there's a better way.[0] https://www.reddit.com/r/Dell/comments/1221mz2/dell_s3221qs_...\n[1] https://www.reddit.com/r/Dell/comments/n8ei34/dell_s3221qs_f...reply",
      "Counter-anecdata: I have 2 Dell U2720Q (Ultrasharp 27\") bought in 2021 and they've been great.That said, I've always stuck for Dell's upper-range Ultrasharp (U prefix in models) monitors, being slightly wary of their cheaper series which the S in your S3221QS implies.reply",
      "Spectacle for Mac and power toys for windows.I\u2019ve been using a single large monitor for a while and it\u2019s been great with window managers. The biggest downside is when playing games full-screen.reply",
      "One of my Dell's would randomly decide that the mini DP connection has no signal, and rebooting the MacBook Pro was the only way to restore it. HDMI would work just fine.reply",
      "This has pixels the size of my hand, and it fully covers my field of view. Not my cup of tea.What I do recommend (having bought one) is the Kuycon G32p, 32 inches @ 6K. Incredible quality and unbelievable value for money (https://clickclack.io/products/in-stock-kuycon-g32p-6k-32-in...).reply",
      "> This has pixels the size of my handThis is 128 ppi, which would be considered \"retina\" at a viewing distance of 70cm (27in).Are you really sitting 2 feet from a 52\" monitor? I'd have to cutout a curve in the front of my desk to sit that closereply",
      "Curves in the front of desks is a thing.https://www.upliftdesk.com/curved-corner-standing-desk/reply"
    ],
    "link": "https://www.dell.com/en-us/shop/dell-ultrasharp-52-thunderbolt-hub-monitor-u5226kw/apd/210-bthw/monitors-monitor-accessories",
    "first_paragraph": "\n                    Selecting  will change the following options:\n                \n\nFrom\n\nTo\n51.5\"6144 x 2560 at 120HzIn-plane Switching (IPS) Black TechnologyHeightTilt99% DCI-P3 (CIE 1976)100% sRGB (CIE 1931)2 HDMI port/s (HDCP 2.2) (Supports up to  6144 x 2560, 120 Hz, VRR, ,  as specified in HDMI 2.1 (FRL))2 DisplayPort 1.4 (HDCP 2.2) port/sReviewsBest of CES 2026: Dell UltraSharp 52 Thunderbolt Hub Monitor - U5226KW\"The world's first 52-inch curved 6K monitor.\" \u2014 HotHardware.comBest of CES 2026: Dell UltraSharp 52 Thunderbolt Hub Monitor - U5226KW\"The ultimate command center.\" \u2014 PCWorldBest of CES 2026: Dell UltraSharp 52 Thunderbolt Hub Monitor - U5226KW\"This isn\u2019t just a bigger monitor; it\u2019s a calmer, smarter way to work.\" \u2014 Lifewire\u00aeFrom Lifewire.com \u00a92026 About Inc., a People Inc. Company. All rights reserved. Used under license.Best of CES 2026: Dell UltraSharp 52 Thunderbolt Hub Monitor - U5226KW\"Aimed at people who keep a lot on screen at once\" \u2014 Reviewed.comA trademark of"
  },
  {
    "title": "Patching the Wii News Channel to serve local news (2025) (raulnegron.me)",
    "points": 50,
    "submitter": "todsacerdoti",
    "submit_time": "2026-01-16T12:58:24 1768568304",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=46645941",
    "comments": [
      "After playing until late into the night, I would browse the weather channel, searching for places on Earth experiencing thunderstorms. Clicking on the icon would play a short audio sample of thunder. The whole thing was overflowing with personality and charm. Wii remains my favourite video game system of all time, and I've owned them all\u2014from nuon to gamepark and back.reply",
      "Author here, thanks for sharing! Happy to answer any questions or discuss it with folks.reply",
      "Dude, this is awesome. El Nuevo Dia on the Wii is peak bori brain. :)reply",
      "Thanks, Sergio! Appreciate it. Was definitely fun getting it all working and seeing that familiar logo pop up on the Wii of all places!reply",
      "I used to check this occasionally back when the console was new. It is interesting to see that people are still keeping it running. I guess there is a niche for everything.reply",
      "I really liked the Wii interface as a TV interface. It felt very much like a modern way to navigate a TV. Modern TVs have some of those features, but none with the whimsy and fun of the Wii.reply",
      "For me, it's just such a nostalgic and pleasant to use interface. I still keep the Wii up and running to play some retro games every now and then (it's a great emulation system as well). Being able to learn more about how these \"old-school\" Nintendo web apps work was something I had been curious about for awhile!reply",
      "So, would it be possible to patch Wii and NDS games as to use local servers instead of now-dead servers?I'm thinking something like Bnetd, but, say NinteNetd.reply",
      "There is Wiimmfii - not local but community run.https://wiimmfi.de/reply",
      "check out Pretendo!reply"
    ],
    "link": "https://raulnegron.me/2025/wii-news-pr/",
    "first_paragraph": "Site written in Markdown, generated by Hugo, hosted on Github\n        Pages and registered using Route 53.\ntheme: modified hugo-lanyon\n\u00a9 2025. All rights reserved.\ud83c\udfa7 Now Playing: Menu (News Channel) via Nintendo Music AppIn keeping with my passion (?) for displaying local news articles in unexpected places, I figured it would be a fun project to try and see what it would take to display current local news on the Nintendo Wii console\u2019s News Channel.Here\u2019s a sneak peek at the result:In this post, I\u2019d like to share my research and process for getting this all to work.Patched the News Channel\u2019s hardcoded Nintendo URL to point to an S3 storage bucket using Go and wadlib to extract the necessary binary file and edit it in-memoryModified WiiLink\u2019s open-source news file generator to add \u201cEl Nuevo D\u00eda\u201d as a news sourceSet up AWS Lambda + EventBridge to regenerate the necessary news binary files hourlySource code: WiiNewsPR and\nWiiNewsPR-PatcherThe News Channel debuted in North America on January"
  },
  {
    "title": "Lock-Picking Robot (github.com/etinaude)",
    "points": 259,
    "submitter": "p44v9n",
    "submit_time": "2026-01-12T13:42:42 1768225362",
    "num_comments": 120,
    "comments_url": "https://news.ycombinator.com/item?id=46588379",
    "comments": [
      "The number of comments in here slandering the developer\u2019s morality for picking locks is actually pretty surprising for a site literally called Hacker News. Every day there\u2019s a story on the front page of some grey/white-hat showing off an exploit they found to infiltrate a site we all use. It\u2019s an odd double standard.reply",
      "> the number of commentsTwo of them in total, if I counted right.reply",
      "Still astounding in a place where \"hackers\" congregate.reply",
      "Not really. Two is not a large number of comments. Turn on showdead in your profile and see the dreck that most users never see.reply",
      "This place lost its hacker lustre at least 10 years ago.I registered my first account in 2011 or so and even then it had plenty of \"pro-big corporate\" energy here.reply",
      "It's a site that was founded and run by venture capitalists. It always had \"pro-big corporate\" energy. If for nothing else, because that's one of the potential exit strategies.reply",
      "I know but there was a \"hacker spirit\" undercurrent that's been diminished of late, I feelreply",
      "Meh, when I registered in 2012 people were writing the same thing.reply",
      "Picking locks is a tradition in hacking. So morality is already off the table. Except if this is to save a cat - the offical reason for lockpicking in the 90s.Now, the robot is hardly something you put together between dessert and coffee. Someone building this must have a live for hardware and lockpicking is just a pretext.And think about the cats!reply",
      "Does anyone even know about the classic MIT Guide to Lockpicking? Back in the day, it was so entertaining to come across this while in grad school and enjoying reading it instead of working on actual work.We are made to be technical tinkerers, playing with tech, seeing if sending that input to that program will crash it. I see it not as a moral issue but as a technical skill, to understand how to work with systems, explore what doesn't work. That way you gain skills in how to make things work better.reply"
    ],
    "link": "https://github.com/etinaude/Lock-Picking-Robot",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Lock picking robot\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.An open source Lock picking robot. Basically, there are many skeleton keys in use (the most common example is the TSA 007, which is bad). Having one key which can open MANY different locks, especially without the lock owners people knowing it exists or can be bought on Aliexpres for a pound. It is a massive security issue. However, the TSA, locksmiths etc need to open locks in certain situations, and if you make locks easy to open for them, they are easy to open for everyone.So, lock-picking robot!!! The idea behind this robot is that it will be able to open difficult-to-lick locks. However, it takes a bit of time. That means you can get a lock which this robot would take 5 min to pick, but w"
  },
  {
    "title": "STFU (github.com/pankajtanwarbanna)",
    "points": 631,
    "submitter": "tanelpoder",
    "submit_time": "2026-01-16T17:32:40 1768584760",
    "num_comments": 432,
    "comments_url": "https://news.ycombinator.com/item?id=46649142",
    "comments": [
      "This is a fun app.One way I deal with people talking on speakerphone, is inviting myself into their conversation and making comments as if I were an active participant. That usually earns me a weird look, and then they go off speaker so I can't hear what's been said. Success.Similar with folks watching reels on speaker, I fake a laugh or make comments about the content. It's awkward enough that they usually stop because they want a moment alone, not an interactive session with a stranger. Which ironically is the same thing I want too.reply",
      "A friend of mine works AV at shows that have rotating DJs and one of the things she has on her mixer board is \"The Suck Button.\"It causes a mic at the other end of the room to get cut into the DJ's live feed monitor with a semitone shift down and some reverb. This causes all sorts of inner-ear chaos and usually clears a DJ off the stage when they're over time within a few minutes at most -- usually under 30 seconds. One time they were trying to figure out why it wasn't working and discovered that the DJ had muted their monitor feed, which explained why they were not only peaking the meters but over time: They hadn't heard the FOUR warnings from the back of house that it was time to wrap up.reply",
      "There was a coffee shop ages ago in SF that would every few hours play a cacophony (e.g. multiple songs at once). I assume it was to drive away people camping on their laptops to rotate tables. Understand but super annoying to people like me who had a timer to but food or drink no less than hourly to be a good citizenreply",
      "We had a friend who would play Metal when the ice cream store he worked at was closed but the customers were lingering too long.  It generally worked, as he was immune.reply",
      "I introduced my local restaurant owner to Mongolian Techno and the late night bar flies and some of the kitchen staff have never forgiven me. He won't admit if he plays it for himself, or because of them :)reply",
      "I've recently become a convert to this kind of thinking. The person invited the public to join in when they decided to have a public speakerphone call. If they don't want my responses or laughter, they get annoyed and stop the behavior I was finding annoying in the first place.I don't even have to act like I'm bothered by it, or that I find their behavior offensive. They change their behavior because they are bothered by mine.reply",
      "How is that different than two people talking in person? Do you interrupt them as well?reply",
      "Yup. Online too! I have no qualms about adding my two cents to any loud public conversations.reply",
      "A half conversation is a lot more disruptive because your brain try to fill in the gap of information.reply",
      "This comment chain is talking about people using speakerphone, though, meaning they hear both sides of the conversationreply"
    ],
    "link": "https://github.com/Pankajtanwarbanna/stfu",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        stfu\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.i was at bombay airport. some dude was watching reels on full volume and laughing loudly. asking nicely doesn't work anymore. me being me, didn't have the courage to speak up.so i built a tiny app that plays back the same audio it hears, delayed by ~2 seconds. asked claude, it spat out a working version in one prompt. surprisingly WORKS.discussion - https://x.com/the2ndfloorguy/status/2011734249871954188something something auditory feedback loop something something cognitive dissonance. idk i'm not a neuroscientist. all i know is it makes people shut up and that's good enough for me.straight up honest - originally called this \"make-it-stop\" but then saw @TimDarcet also built similar and named it STFU. wayy"
  },
  {
    "title": "Reading across books with Claude Code (pieterma.es)",
    "points": 59,
    "submitter": "gmays",
    "submit_time": "2026-01-16T18:49:29 1768589369",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=46650347",
    "comments": [
      "The mental model I had of this was actually on the paragraph or page level, rather than words like the post demos. I think it'd be really interesting if you're reading a take on a concept in one book and you can immediately fan-out and either read different ways of presenting the same information/argument, or counters to it.reply",
      "Discussed earlier this week: https://news.ycombinator.com/item?id=46567400reply",
      "This is all interesting, however I find myself most interested in how the topic tree is created. It seems super useful for lots of things. Anyone can point me to something similar with details?EDIT: Whoops, I found more details at the very end of the article.reply",
      "I did a similar thing with productivity books early last year, but never released it because it wasn't high enough quality. I keep meaning to get back to that project but it had a much more rigid hypothesis in mind - trying to get the kind of classification from this is pretty difficult and even more so to get high value from it.reply",
      "This sounds like a huge waste of time.People should be reading books with their eyes, not LLM scam tech.reply",
      "I agree that we should be reading books with our eyes and that feeding a book into an LLM doesn't constitute reading it and confers few of the same benefits.But this thing isn't (so far as I can tell) even slightly proposing that we feed books into an LLM instead of reading them. It looks to me more like a discovery mechanism: you run this thing, it shows you some possible links between books, and maybe you think \"hmm, that little snippet seems well written\" or \"well, I enjoyed book X, let's give book Y a try\" or whatever.I don't think it would work particularly well for me; I'd want longer excerpts to get a sense of whether a book is interesting, and \"contains a fragment that has some semantic connection with a fragment of a book I liked\" doesn't feel like enough recommendation. Maybe it is indeed a huge waste of time. But if it is, it isn't because it's encouraging people to substitute LLM use for reading.reply",
      "commenter above probably didn't read the post, ironicallyreply",
      "Guess we need \u201creading across hacker news articles with Claude code.\u201dreply",
      "I need a name for people who dismiss an entirely new and revolutionary class of technology without even trying it, so much so that they'll not even read about any new ideas that involve it.reply",
      "we call them ludditesreply"
    ],
    "link": "https://pieterma.es/syntopic-reading-claude/",
    "first_paragraph": "Jan 4, 2026LLMs are overused to summarise and underused to help us read deeper.To explore how they can enrich rather than reduce, I set Claude Code up with tools to mine a library of 100 non-fiction books.\nIt found sequences of excerpts connected by an interesting idea, or trails.Browse all trailsHere\u2019s a part of one such trail, linking deception in the startup world to the social psychology of mass movements (I\u2019m especially pleased by the jump from Jobs to Theranos): Steve Jobs  Bad Blood  Zero to One  The True Believer The books were selected from Hacker News\u2019 favourites, which I previously scraped and visualized.Claude browses the books a chunk at a time. A chunk is a segment of roughly 500 words that aligns with paragraphs when possible.\nThis length is a good balance between saving tokens and providing enough context for ideas to breathe.Chunks are indexed by topic, and topics are themselves indexed for search. This makes it easy to look up all passages in the corpus that relate to"
  },
  {
    "title": "Elasticsearch was never a database (paradedb.com)",
    "points": 100,
    "submitter": "jamesgresql",
    "submit_time": "2026-01-11T20:54:47 1768164887",
    "num_comments": 78,
    "comments_url": "https://news.ycombinator.com/item?id=46579954",
    "comments": [
      "> Elastic has been working on this gap. The more recent ES|QL introduces a similar feature called lookup joins, and Elastic SQL provides a more familiar syntax (with no joins). But these are still bound by Lucene\u2019s underlying index model. On top of that, developers now face a confusing sprawl of overlapping query syntaxes (currently: Query DSL, ES|QL, SQL, EQL, KQL), each suited to different use cases, and with different strengths and weaknesses.I suppose we need a new rule, \"Any sufficiently successful data store eventually sprouts at least one ad hoc, informally-specified, inconsistency-ridden, slow implementation of half of a relational database\"reply",
      "Just as any \"plain blob storage\" eventually evolves a hierarchical filesystem (but with silly quirks!) on top of it.reply",
      "Funny argument on the query languages in hindsight, since the latest release (https://www.paradedb.com/blog/paradedb-0-20-0 but that was after this blog) just completely changed the API. To be seen how many different API versions you get if you make it to 15 years ;)PS: I've worked at Elastic for a long time, so it is fun to see the arguments for a young product.reply",
      "... and then becomes an email client (https://en.wikipedia.org/wiki/Jamie_Zawinski#Zawinski%27s_La...). A two-fer. lol.reply",
      "It seems like everything converges on either LISP or emacs.reply",
      "ICYMI https://en.wikipedia.org/wiki/Greenspun's_tenth_rulereply",
      "ICYMI expands to \"in case you missed it\", ICYMI.reply",
      "ICYMI expands to ... wait, shitreply",
      "Accenture managed to build a data platform for my company with Elasticsearch as the primary database. I raised concerns early during the process but their software architect told me they never had any issues. I assume he didn\u2019t lie. I was only an user so I didn\u2019t fight and decided to not make my work rely on their work.reply",
      "I worked in a company that used elastic search as main db.\nIt worked, company made alot of money from that project. It was a wrong decision but helped us complete the project very fast. We needed search capability and a db. ES did it both.Problems that we faced by using elastic search:\nHigh load, high Ram usage : db goes down, more ram needed. Luckily we had ES experts in infra team, helped us alot.(ecommerce company)To Write and read after, you need to refresh the index or wait a refresh.\nMore inserts, more index refreshes. Which ES is not designed for, inserts become slow. You need to find a way to insert in bulk.Api starts, cannot find es alias because of connection issue, creates a new alias(our code did that when it cant find alias, bad idea). Oops whole data on alias is gone.Most important thing to use ES as main db is to use \"keyword\" type for every field that you don't text search.No transaction: if second insert fails you need to delete first insert by hand. Makes code look ugly.Advantages: you can search, every field is indexed, super fast reads. Fast development. Easy to learn. We never faced data loss, even if db crashed.reply"
    ],
    "link": "https://www.paradedb.com/blog/elasticsearch-was-never-a-database",
    "first_paragraph": "Elasticsearch was never a database. It was built as a search engine API over Apache Lucene (an incredibly powerful full-text search library), but not as a system of record. Even Elastic\u2019s own guidance has long suggested that your source of truth should live somewhere else, with Elasticsearch serving as a secondary index. Yet, over the last decade, many teams have tried to stretch the search engine into being their primary database, usually with unexpected results.Just to be clear up front, when we say database in this context we mean a system you can use as your primary datastore for OLTP transactional workloads: the place where your application\u2019s truth lives. Think Postgres (voted most loved database three years running), MySQL, or even Oracle.The story often begins with a simple need: search. A team is already using Postgres or MySQL to store their application data, but the built-in text search features don\u2019t scale. Elasticsearch looks like the perfect solution; it\u2019s fast, flexible, "
  },
  {
    "title": "HTTP RateLimit Headers (dotat.at)",
    "points": 28,
    "submitter": "zdw",
    "submit_time": "2026-01-14T16:29:50 1768408190",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=46618105",
    "comments": [
      "Looking at the rfc, I'm not sure I understand the motivation, as it suggests multiple times that a client or intermediary will have to read external documentation:> Servers MAY choose to return partition keys that distinguish between quota allocated to different consumers or different resources.  There are a wide range of strategies for partitioning server capacity, including per user, per application, per HTTP method, per resource, or some combination of those values.  The server SHOULD document how the partition key is generated so that clients can predict the key value for a future request and determine if there is sufficient quota remaining to execute the request.If external documentation is required, why send the header? It seems as though having it in the documentation is generally preferable, rather than something to avoid.reply",
      "The server would be telling the client the rate-limiting values active/effective for to it. As such, the client doesn't actually need to know what \"its partition\" is. As far as the client is concerned, \"its partition\" is the whole of the rate-limiting domain.The partitioning strategy, and partition chosen using it, would never \u2014 should never \u2014 be relevant to any automated logic inside the client. (The only way in which it could be would be if you were trying to make a client that aims to defeat the server's rate-limiting logic by using multiple accounts or IP addresses to jump between partitions, and that's... not okay.)The point of sending the partitioning info to the client, is that it enables a human developing a client, or operating a tool that embeds a client, to debug why rate-limiting is happening when by their understanding it shouldn't be \u2014 especially when they have multiple clients across multiple threads / machines each making multiple concurrent requests to the API. These HTTP-429-response heisenbugs get much easier to reason about when the server is sending the client enough information for the developer to be able to see which of the requests they sent got rate-limiting-bucketed together, and which didn't.reply",
      "The relevant word here is MAY[1]It's true that if an API requires the devs of its consumers to have consulted documentation in order to respect the RateLimit header, they can just as easily include custom API logic for traffic control, but this does provide a nice standardized way to do so nevertheless.And since the word is \"MAY\", APIs may also use standard responses that don't require an custom handling. As an example a CLI-builder library which parses OpenAPI spec can adopt changes to handle the RateLimit header automatically, in the situations where consulting docs is not required.[1] https://datatracker.ietf.org/doc/html/rfc2119reply",
      "Maintainer on the Ky library team here, a popular HTTP client for JavaScript.We support these headers, but unfortunately there\u2019s a mess of different implementations out there. The names aren\u2019t consistent. The number/date formats aren\u2019t consistent. We occasionally discover new edge cases. The standard is very late to the party. Of course, better late than never. I just hope it can actually gain traction given the inertia of some incompatible implementations.If you are designing an API, I strongly recommend using `Retry-After` for as long as you can get away with it and only implementing the rate limit headers when it really becomes necessary. Good clients will add jitter and exponential backoff to prevent the thundering herd problem.reply",
      "Yup, seems both overengineered and undercooked both at the same time, as is unfortunately common for newer headers.As you said, 429 + Retry-After is plenty good already.reply",
      "It is nice to see some actual progress on this because handling rate limits has always been kind of a mess. I really hope the major gateways pick this up quickly so we do not have to write custom logic for every integration.reply",
      "It really irks me that the de facto rate limiting headers mix camel case with the more standard dashes, i.e. RateLimit-Remaining instead of Rate-Limit-Remaining.reply",
      "At least it's not misspelled.https://en.wikipedia.org/wiki/HTTP_refererreply",
      "it's all lowercase anyway at parse time.reply",
      "rate-limit-remaining would be nicer than ratelimit-remainingreply"
    ],
    "link": "https://dotat.at/@/2026-01-13-http-ratelimit.html",
    "first_paragraph": "There is an IETF draft that aims to standardize RateLimit header\nfields for HTTP. A RateLimit header in a successful response\ncan inform a client when it might expect to be throttled, so it can\navoid 429 Too Many Requests errors. Servers can also\ninclude RateLimit headers in a 429 response to make the error more\ninformative.The draft is in reasonably good shape. However as written it seems to\nrequire (or at least it assumes) that the server uses bad quota-reset\nrate limit algorithms. Quota-reset algorithms encourage clients into\ncyclic burst-pause behaviour; the draft has several paragraphs\ndiscussing this problem.However, if we consider that RateLimit headers are supposed to tell\nthe client what acceptable behaviour looks like, they can be used with\nany rate limit algorithm. (And it isn\u2019t too hard to rephrase the draft\nso that it is written in terms of client behaviour instead of server\nbehaviour.)When a client has more work to do than will fit in a single window\u2019s\nquota, linear rate "
  },
  {
    "title": "Why DuckDB is my first choice for data processing (robinlinacre.com)",
    "points": 215,
    "submitter": "tosh",
    "submit_time": "2026-01-16T10:57:38 1768561058",
    "num_comments": 79,
    "comments_url": "https://news.ycombinator.com/item?id=46645176",
    "comments": [
      "What I love about duckdb:-- Support for .parquet, .json, .csv (note: Spotify listening history comes in a multiple .json files, something fun to play with).-- Support for glob reading, like: select * from 'tsa20*.csv' - so you can read hundreds of files (any type of file!) as if they were one file.-- if the files don't have the same schema, union_by_name is amazing.-- The .csv parser is amazing. Auto assigns types well.-- It's small! The Web Assembly version is 2mb! The CLI is 16mb.-- Because it is small you can add duckdb directly to your product, like Malloy has done: https://www.malloydata.dev/ - I think of Malloy as a technical persons alternative to PowerBI and Tableau, but it uses a semantic model that helps AI write amazing queries on your data. Edit: Malloy makes SQL 10x easier to write because of its semantic nature. Malloy transpiles to SQL, like Typescript transpiles to Javascript.reply",
      "Been playing around with Clickhouse a lot recently and have had a great experience particularly because it hits many of these same points. In my case the \"local files\" hasn't been a huge fixture but the Parquet and JSON ingestion have been very convenient and I think CH intends for `clickhouse-local` to be some sort of analog to the \"add duckdb\" point.One of my favorite features is `SELECT ... FROM s3Cluster('<ch cluster>', 'https://...<s3 url>.../data//.json', ..., 'JSON')`[0] which lets you wildcard ingest from an S3 bucket and distributes the processing across nodes in your configured cluster. Also, I think it works with `schema_inference_mode` (mentioned below) though I haven't tried it. Very cool time for databases / DB tooling.(I actually wasn't familiar with `union_by_name` but it looks to be like Clickhouse has implemented that as well [1,2] Neat feature in either case!)[0] https://clickhouse.com/docs/sql-reference/table-functions/s3...\n[1] https://clickhouse.com/docs/interfaces/schema-inference\n[2] https://github.com/ClickHouse/ClickHouse/pull/55892reply",
      ">> The .csv parser is amazingTheir csv support coupled with lots of functions and fast & easy iterative data discovery has totally changed how I approach investigation problems. I used to focus a significant amount of time on understanding the underlying schema of the problem space first, and often there really wasn't one - but you didn't find out easily. Now I start with pulling in data, writing exploratory queries to validate my assumptions, then cleaning & transforming data and creating new tables from that state; rinse and repeat. Aside from getting much deeper much quicker, you also hit dead ends sooner, saving a lot of otherwise wasted time.There's an interesting paper out there on how the CSV parser works, and some ideas for future enhancements. I couldn't seem to find it but maybe someone else can?reply",
      "not a paper but I found this: https://duckdb.org/2025/04/16/duckdb-csv-pollock-benchmarkreply",
      "This is a great sell. I have this annoyingly manual approach with a SQLite import and so on. This is great. Thank you!reply",
      "Thanks for the excellent comment! Now excuse me while I go export my spotify history to play around with duckdb <3reply",
      "Spotify says it will take 30 days for the export... it really only takes about 48 hours if I remember correctly.\nWhile you wait for the download here is an example listening history exploration in malloy - I converted the listening history to .parquet: https://github.com/mrtimo/spotify-listening-historyreply",
      "Malloy and PRQL (https://prql-lang.org/book/) are quite coolreply",
      "-- hive partitioningreply",
      "its 32 mb uncompressed and around 6MB compressed, its not that small https://cdn.jsdelivr.net/npm/@duckdb/duckdb-wasm/dist/it is also difficult to customize as compared to sqlite so for example if you want to use your own parser for csv than it becomes hard.But yes it provides lot of convenience out of the box as you have already listed.reply"
    ],
    "link": "https://www.robinlinacre.com/recommend_duckdb/",
    "first_paragraph": "Originally posted: 2025-03-16.  View source code for this page here.Over the past few years, I've found myself using DuckDB more and more for data processing, to the point where I now use it almost exclusively, usually from within Python.We're moving towards a simpler world where most tabular data can be processed on a single large machine1 and the era of clusters is coming to an end for all but the largest datasets.2This post sets out some of my favourite features of DuckDB that set it apart from other SQL-based tools.    In a nutshell, it's simple to install, ergonomic, fast, and more fully featured.An earlier post explains why I favour SQL over other APIs such as Polars, pandas or dplyr.DuckDB is an open source in-process SQL engine that is optimised for analytics queries.The performance difference of analytics-optimised engines (OLAP) vs. transactions-optimised engines (OLTP) should not be underestimated. A query running in DuckDB can be 100 or even 1,000 times faster than exactly "
  },
  {
    "title": "Slop is everywhere for those with eyes to see (fromjason.xyz)",
    "points": 191,
    "submitter": "speckx",
    "submit_time": "2026-01-16T20:03:10 1768593790",
    "num_comments": 99,
    "comments_url": "https://news.ycombinator.com/item?id=46651443",
    "comments": [
      "Ironically given the topic, the very first sentence on the page (\"The size of your plate can influence how much food you eat.\") is based on observational research that has not replicated in controlled studies.\n[0] https://pmc.ncbi.nlm.nih.gov/articles/PMC2129126/\n[1] http://link.springer.com/article/10.1186/s12966-019-0826-1?u...reply",
      "That\u2019s not irony. Interesting, perhaps, but not ironic.https://thereader.mitpress.mit.edu/what-irony-is-not/reply",
      "It's certainly ironic if an article about slop leads with a tired old glob of pseudoscience slop and the author doesn't realize.reply",
      "https://pmc.ncbi.nlm.nih.gov/articles/PMC5598018/Duelling articles at 50 paces. Same publication channel.reply",
      "I have definitely noticed that I will eat more or less depending on the size of the plate. Maybe it only applies to people who were taught to clean their plate, dunno.reply",
      "I don't agree with the article that the top couple content creators can walk away and kill a platform. Vine committed suicide for no real reason, it's a pretty poor example to point to. Nowadays on any top social media there's 1-3% of creators making the vast majority of popular content, but more importantly, there's another 15% of people out there who are vying to try and take their spots and will gladly fill the void should the top creators leave. They're mostly just not doing well because the top is being crowded out (and the algorithm keeps it that way), not for lack of trying.reply",
      "Vine died because Twitter made sure it died. The same with Periscope, nothing more.reply",
      "Side note: I love the imperfect fonts and old school design of the website. For years I've been looking for ways to re-create old book style text and graphs in the digital era. This gets so close to that vision.reply",
      "Quickly checked the CSS and the font appears to be \"Volume Tc\" and \"Volume Tc Sans\" by Tom Chalky:  https://tomchalky.com/product/volume-handcrafted-trio-font-f...reply",
      "You might be interested in the resources [1] on the H.P.Lovecraft Historical Society website.[1] https://www.hplhs.org/resources.php#fontsreply"
    ],
    "link": "https://www.fromjason.xyz/p/notebook/slop-is-everywhere-for-those-with-eyes-to-see/",
    "first_paragraph": "The size of your plate can influence how much food you eat. The absence of a clock on a casino wall can keep you gambling through the early morning. On social media, our For You Pages give us the illusion of infinite content. How our environments are designed influences how we consume. And wouldn't you know it, everything around us is designed for maximum consumption.Open TikTok, and you can easily burn through a hundred videos or more before you glance at the time. It doesn't help that the For You Page hides the time on our phones.We are over consuming content on the FYP. The sudden surge of low-quality, AI-generated content, i.e. \u201cAI slop,\u201d is a byproduct of that overconsumption. We don't see it because, well, we're conditioned not to, but slop always arrives on time. Slop is inevitable. Slop is quintessential. Slop is everywhere for those with eyes to see.Olive oil, wasabi, saffron, vanilla, Wagyu, honey, champagne, and truffle,...reality TV, all hold examples of what happens when d"
  },
  {
    "title": "High-Level Is the Goal (bvisness.me)",
    "points": 3,
    "submitter": "tobr",
    "submit_time": "2026-01-15T11:01:34 1768474894",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://bvisness.me/high-level/",
    "first_paragraph": "\r\n      If you have heard of the Handmade community, you likely think we are about \u201clow-level programming\u201d in some way. After all, we are a community inspired by Handmade Hero, a series where you learn to make a game and engine from scratch.\r\n    \r\n      We in the Handmade community often bemoan the state of the software industry. Modern software is slow and bloated beyond belief\u2014our computers are literally ten times more powerful than a decade ago, yet they run worse than they used to, purely because the software is so bad. The actual user experience has steadily declined over the years despite the insane power at our fingertips. Worst of all, people\u2019s expectations have hit rock bottom, and everyone thinks this is normal.\r\n    \r\n      The Handmade crowd seems to think that low-level programming is the key to building better software. But this doesn\u2019t really make sense on the surface. How is this practical for the average programmer? Do we really expect everyone to make their own UI fr"
  },
  {
    "title": "Install.md: A standard for LLM-executable installation (mintlify.com)",
    "points": 31,
    "submitter": "npmipg",
    "submit_time": "2026-01-16T22:15:20 1768601720",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=46652944",
    "comments": [
      "All the insecurity of running a random bash script, with all the terrifying stochasticity of an LLM in one \"makes you want to tear your eyes out\" package!reply",
      "Fascinating. My thinking was that this is an upgrade over a bash script because you can prompt the AI to check it, clear installs with you, or otherwise investigate safety before installing in a way that isn't natural with *.sh. Does that make any amount of sense or am I just crazy?reply",
      "Time and time again, be it \"hallucination\", prompt injection, or just plain randomness, LLMs have proven themselves woefully insufficient at best when presented with and asked to work with untrusted documents. This simply changes the attack vector rather than solving a real problemreply",
      "In a computing system, LLMs aren't substituting for code, they're substituting for humans. Treat them accordingly.reply",
      "Bash scripts give you visibility into what they are going to do by virtue of being machine instructions in a determimistic language. MD files you pipe to matrix multiplication has a much lower chance of being explainable.reply",
      "Yeah, someone else was pointing that the bash scripts are guaranteed to do the same thing on every system which I think is in the same vein as your feedback. It's for sure a downside of the markdown that I need to explain the docs behind the proposal.reply",
      "Appropriately, I think this was probably drafted by AI too:> How does install.md work with my existing CLI or scripts?> install.md doesn't replace your existing tools\u2014it works with them. Your install.md can instruct the LLM to run your CLI, execute your scripts, or follow your existing setup process. Think of it as a layer that guides the LLM to use whatever tools you've already built.(It doesn't X &mdash; it Ys. Think of it as a Z that Ws. this is LLM speak! I don't know why they lean on these constructions to the exclusion of all else, but they demonstrably do. The repo README was also committed by Claude Code. As much as I like some of the code that Claude produces, its Readmes suck)reply",
      "Yeah, removing that line right now. Went too fast and some this copy is definitely low quality :(. Incredibly ironic for me to say that AI needs more supervision while working at the company proposing this haha.Any other feedback you have about the general idea?reply",
      "I think my preferred version of this would be a hybrid. Keep the regular installer, add a file filled with information that an LLM can use to assist a human if the install script fails for some reason.If the installer was going to succeed in a particular environment anyway, you definitely  want to use that instead of an LLM that might sporadically fail for no good reason in that same environment.If the installer fails then you have a \"knowledge base\" to help debug it, usable by humans or LLMs, and if it fails, well, the regular installer failed too, so hopefully you're not worse off. If the user runs the helper LLM in yolo mode then the consequences are on them.reply",
      "Acknowledged. The standard includes a link to the llms.txt for a site at the bottom which is intended to give it that \"knowledge base\" to query.I think I agree with you on it needing to assist in event of failure instead of jumping straight to install though. Will think more about that.reply"
    ],
    "link": "https://www.mintlify.com/blog/install-md-standard-for-llm-executable-installation",
    "first_paragraph": "ResourcesExploreStartupsBuilt for fast-moving teamsEnterpriseScalable for large organizationsSwitchSeamless migration toolsCompanyCareersJoin our growing teamWall of LoveCustomer testimonialsGuidesGuide to technical writingDocumentationGuidesGetting StartedDeploy in minutesComponentsCustomizable components libraryDevelopersAPI ReferenceBuild integrations and custom workflowsChangelogLearn what's newJanuary 15, 2026Michael RyaboyContent StrategistInstalling software is a task which should be left to AI. Today we are proposing install.md to standardize how developers should write installation instructions for agents. It's currently live on all Mintlify sites including Cerebras, Firecrawl, and Langchain.Proposal for a standard /install.md file that provides LLM-executable installation instructions.Installing software is a task which should be left to AI. There's no reason for us humans to waste our time reading instructions and running commands.Agents are growing in capability faster than"
  }
]