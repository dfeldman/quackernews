[
  {
    "title": "Nearly half Dell's US workforce has rejected RTO. Rather WFH than get promoted (2024) (msn.com)",
    "points": 121,
    "submitter": "this_weekend",
    "submit_time": "2025-01-05T00:32:11 1736037131",
    "num_comments": 39,
    "comments_url": "https://news.ycombinator.com/item?id=42598722",
    "comments": [
      "For a lot of software engineers - especially those of us who are actually senior engineers, not folks who've been working for 2 years since graduation - I don't think a promotion makes sense, anyway. From here, it's Staff or Principal (or management, which is fine for some folks, and anathema to others), and the raise does not justify the amount of effort the next level would require.A lot of us are happy where we are, level-wise, and if we do want a significant raise, others have pointed out - we're more likely to get it by interviewing, not by getting promoted.Now, granted, if you're gung-ho and really want the responsibility and the more interesting work that Staff/Principal will give you, fuckin' go nuts - I'm happy there are ambitious folks out there. I just want to do my job and then go play with my kid.\n \nreply",
      "Being financially secure and not caring about a promotion is the biggest superpower you can have at a tech company. Work on whatever projects you want. Skip whichever meetings you want. Don't want to waste multiple days writing a pointless report? Fine. Don't want to participate in org politics? All good. As long as you are doing decent work and have a good reputation among your peers the threat of firing will always be an empty one. And soon you'll start to realize how much of the work people do at any company, especially at higher levels, is just performative ass kissing.Every single terrible job experience is the result of a shitty power tripping manager. Take away the manager's power over you and suddenly your job will be fun and fulfilling again.\n \nreply",
      "As a Principal engineer, I would refuse to RTO and would look for another position or do the same and continue WFH if it's rewarding enough.Most of the Staff/Principal engineers around you are not there because they don't have other (sometimes much better on the face of it) opportunities - we're here because of inertia and realm expertise.\n \nreply",
      "Speaking as someone who tracked up the management chain\u2026.Senior Software Engineer was the best job I ever had. The perfect balance of autonomy and responsibility.\n \nreply",
      "The next level is also a pyramid level - aka - competition and stack rank and often political. A lot of people cannot be bothered with petty politics.\n \nreply",
      "The concept that you would rather promote a mediocre employee who works in the office over a star who decides to work at home is just absurd, and horrible for business. I cannot imagine it actually being implemented.\n \nreply",
      "It's consistent with both theories about rto -- one that remote work harms productivity (and someone working remotely cant be more productive than their office peers) and the other postulating its just a power move\n \nreply",
      "The truly essential people who want to WFH will just ignore the directive and their managers will cover for them. I assume they don't show up in this 50%.The rest are literally calling Dell's bluff that any potential promotion would be less valuable (monetary or well-being). Good on them!\n \nreply",
      "I mean that's the thing. At the end of the day not WFH is at least another 2 hours of your day dedicated to work things.That's an absolutely massive chunk of time.\n \nreply",
      "Hardly \"at least\" two hours a day. The average one-way commute in the United States is under a half hour; the state with the longest average commute (New York) is at 33 minutes.https://www.census.gov/topics/employment/commuting/guidance/...\n \nreply"
    ],
    "link": "https://www.msn.com/en-us/money/companies/nearly-half-of-dell-s-full-time-workforce-in-the-u-s-has-rejected-returning-to-the-office-they-d-rather-work-from-home-than-get-promoted/ar-BB1oBygb",
    "first_paragraph": ""
  },
  {
    "title": "Combining 15s interval whole-sky-camera photos to form a 4y spanning keogram (astrodon.social)",
    "points": 348,
    "submitter": "nebalee",
    "submit_time": "2025-01-04T15:18:13 1736003893",
    "num_comments": 53,
    "comments_url": "https://news.ycombinator.com/item?id=42595190",
    "comments": [
      "This is fantastic, and like another commenter mentions, a beautiful continuation of an age old practice: measuring the sky.Very cool that their hardware keept chugging along for years without hiccup, too.If you want to do something kinda similar but far less involved: a very lo-fi, no computer involved thing to do is an ultralong photographic exposure (months, a year, longer) with a pinhole camera.The results are quite artistic IMO [1], the camera is fire-and-forget and you don't need any chemicals to develop the image. Just photograph/scan the photographic paper and invert the colors.I'm not affiliated with them, but Solarcan sells ready made single-use pinhole cameras. An almost zero-regret purchase I'd say.[1] You see the sun move through one year of skies, as seen from my balcony: https://files.rombouts.email/IMG_6500.jpegPeople have made wonderful, mildly spooky pictures with these: https://solarcan.co.uk/wp-content/uploads/2018/05/solarcan-p...\n \nreply",
      "https://archive.ph/usfd5To avoid HN hug of death on this 1832 users Mastodon instance.Very cool arrangement of those pictures, i was wondering what he has done about the daytime pictures when i read the title.\n \nreply",
      "Incredibly cool. I love this so much, both artistically as well as how it demonstrate the equation of time [1] -- the fact that the changes in sunrise and sunset are not symmetrical.On the other hand, it's driving me absolutely crazy that he centers the image at 4:00 rather than midnight. Or maybe that's to show the shimmer of sunlight a little after noon on the right hand side?I can't figure out why it's \"bluest\" closest to dawn and dusk. I'm guessing the exposure makes a huge difference, and obviously the night part is way more exposed than the daylight part, or else it would be much darker. Wondering if the camera used automatic exposure, and how much of the brightness of colors in the image are artifacts because of that? Also if he locked the white point hopefully?[1] https://en.wikipedia.org/wiki/Equation_of_time\n \nreply",
      "One of the commenters noticed this as well, the author says it is something automatic in the exposure.https://fediscience.org/@Birk_lab/113770845539931892\n \nreply",
      "Oh that's a real shame then. The resulting composite images are certainly artistically interesting to look at, and you can see the big-picture effects like sunrise/sunset and moon, but that explains why you can't see the gradual brightening at dawn, or degrees of darkness at night.It seems like if you wanted to do this accurately, you'd need to lock exposure to handle a bright blue sky without blowing out -- both aperture and shutter speed. And lock white balance. The question is whether that would allow for sufficient sensitivity at night. But if you're just averaging color values across a section of sky and mainly looking for moon and moonlit clouds, I think it would, since pixel noise will get averaged out and the moon is bright.\n \nreply",
      "Additionally to locking the exposrue time and aperture, one could also take multiple exposures, figure out the camera's light response function and fuse multiple exposures together into a single higher dynamic range (HDR) image (see OpenCV tutorial on that or Debevec et al. 1997) Assuming you can find the camera response for the very long exposure times at night _and_ the very short during the day, one could relate them to each other and display both for accurate visual comparison.\n \nreply",
      "Yeah, I agree this should be using a fixed exposure (possibly on a schedule) with locked white balance.  They are  using an astro camera so the sensor is very sensitive, they can get away with extremely low exposure times.\n \nreply",
      "> get away with extremely low exposure timesFor night sky photography with an astro cam you\u2019re still looking at exposure times of 20-60s at night (possibly also increasing the gain at night) and milliseconds during the day. The dynamic range is immense.As someone who has struggled with this for my own allskycam, it\u2019s extremly difficult to have white balance settings that perform well at all times of the cycle, especially with a camera designed to be more sensitive in the IR part of the spectrum (which will always look unrealistic). Settings that give you lovely white clouds and blue skies during the day tend to give you purple skies and green clouds at night. The quality of light is different so the white balance is different.You can use autobalance or different white balance profiles for day and night but they each have issues.\n \nreply",
      "#1 rule of timelapse: manual white balance, manual shutter speed, manual aperture, manual agc/iso, manual focus.\n \nreply",
      "I think the blueness is because clouds at dawn and dusk reflect atmospheric colors more, whereas midday the clouds light up as more of a white from diffusing sunlight. 100% made up theory on my part, but I think it makes sense?I asked o1 to estimate colors by hour and its reasoning and estimates seem fairly convincing[1], and also show more saturated blues dawn and dusk, though it did not model clouds.1. https://chatgpt.com/share/67795fe3-9ac8-8009-9922-153f40c509...\n \nreply"
    ],
    "link": "https://astrodon.social/@cgbassa/113770318993975063",
    "first_paragraph": ""
  },
  {
    "title": "ELKS: Linux for 16-bit Intel Processors (github.com/ghaerr)",
    "points": 101,
    "submitter": "emersonrsantos",
    "submit_time": "2025-01-04T19:23:43 1736018623",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=42596983",
    "comments": [
      "Cool. A quarter of a century ago a friend of mine worked on this and you can still see his name in some copyright notices (Alistair Riddoch)Most recently Al was learning Rust because he needs that for his current role, it might be fun to see whether you can write an ELKS target for Rust.\n \nreply",
      "I do wonder in today\u2019s landscape of single board computers what the right bit width is. A 64-bit system with like 1-2GB or RAM doesn\u2019t make a ton of sense since your program size and data structure size grows by quite a bit but you don\u2019t need it to since you don\u2019t get to have more than 4GB of RAM. On the other hand there you do have SBCs with 8-16GBs of RAM but that\u2019s a far cry from needing full 64 bits. Would an optimal bit width be 48 bits? 40?\n \nreply",
      "I think it's either 8, 32 or 64 bit. Maybe 16 bit.The size of memory space is not necessarily aligned with the bit width of the CPU. There are a lot of 32 bit systems that can use much more than 4 GB of RAM. And there are no 64 bit systems I know of, that are even theoretically able to use 16 exabytes (16 million TB) of RAM.AFAIK ARM32 is still the norm for embedded systems. And there is still a place for 8-bit microcontrollers.\n \nreply",
      "32-bit ARM-based systems supported up to 1 TB of RAM. 32-bit x86 only up to 64 GB. Unless you want to map more than 4 GB in a single process, you could very well stay 32-bit.But AArch64 (or ARM64) and AMD64 did bring a lot more on the table than just larger address space. More registers, and a performance boost by just being better suited for the modern CPU core design.\n \nreply",
      "32-bit x86 linux will typically support 3GB per process, with 1GB kernel address area, I think? (Windows did 2GB / 2GB split by default, custom boot options can change it to 3GB / 1GB, but only some 32-bit apps fully supported it, like photoshop).Also, FWIW, security people can get real bothered that ASLR doesn't do much in 32-bit systems.So, I think starting around 2GB DRAM, it's probably a \"big enough\" system to justify a 64-bit OS.\n \nreply",
      ">Unless you want to map more than 4 GB in a single process, you could very well stay 32-bit.Provided you are not bothered by highmem. https://www.realworldtech.com/forum/?threadid=76912&curposti...\n \nreply",
      "Many \"64-bit\" systems only have ~48 bits of virtual address space, and while canonical pointers have all the high bits equal, you can put the otherwise-wasted bits to work by storing metadata.If you're writing really space-optimised code, you can pack pointers closer together.\n \nreply",
      "Yes, this already happens.  Address bus width is usually different from register width due to cost.  I found this, old CPU's address bus only getting to 44 bits wide on Itanium.https://www.tech-faq.com/address-bus.htmlMy newish AMD laptop: `grep 'address sizes' /proc/cpuinfo`    address sizes : 48 bits physical, 48 bits virtual\n\nLooks like 36 bits wide is already plenty for anything typical, up to 68 GB:    >>> f'{2**32:>30_}'\n    '                 4_294_967_296'\n    >>> f'{2**36:>30_}'\n    '                68_719_476_736'\n    >>> f'{2**40:>30_}'\n    '             1_099_511_627_776'\n    >>> f'{2**48:>30_}'\n    '           281_474_976_710_656'\n    >>> f'{2**64:>30_}'\n    '    18_446_744_073_709_551_616'\n \nreply",
      "I guess no point wasting hardware supporting things that will never be possible for the lifetime of the processor.\n \nreply",
      "Does anybody have the archive of the original ELKS site with the quote from Linus that Linux is not suitable for anything but 486 or higher or something like that?\n \nreply"
    ],
    "link": "https://github.com/ghaerr/elks",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Embeddable Linux Kernel Subset - Linux for 8086\n      \nELKS is a project providing a Linux-like OS for systems based on the Intel\nIA16 architecture (16-bit processors: 8086, 8088, 80188, 80186, 80286,\nNEC V20, V30 and compatibles). Such systems are ancient computers (IBM-PC\nXT / AT and clones) as well as more recent SBCs, SoCs, and FPGAs. ELKS supports networking and installation to HDD using both MINIX and FAT file systems.You can play with ELKS online thanks to the v86 emulator. Login with \"root\" and no password. Go to the bin folder and try the different commands available. Try nxtetris. Start the game by pressing \"n\".ELKS running on QEMU\nOlivetti M24 8086 CPU\nELKS Networking showing netstat and process list\nRunning ELKS Basic on PC-9801UV21 (NEC V30 CPU)\nRunning Matrix and vi on multiple consoles\nOf course Doom\nTelnet to an old "
  },
  {
    "title": "University of Alabama Engineer Pioneers New Process for Recycling Plastics (ua.edu)",
    "points": 15,
    "submitter": "thunderbong",
    "submit_time": "2025-01-04T23:30:37 1736033437",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://news.ua.edu/2024/10/ua-chemical-engineer-plastic-recycling/",
    "first_paragraph": "\n\nUA News Center\n\t\t\nHome \u00bb UA Engineer Pioneers New Process for Recycling PlasticsPlastic recycling is commonplace but imperfect. Part of the problem, says Dr. Jason Bara, is that current processes yield lower-quality plastics with reduced value and fewer end uses. In a circular plastic economy, any plastic could be broken down to its component parts and then reconstituted into new products with little or no waste.The science is not there yet, but it may be one step closer.Bara, a professor in the College of Engineering, leads a team at The University of Alabama working to improve methods to recycle the ubiquitous plastics we interact with daily.Polyethylene terephthalate (PET) is a common plastic that responds well to chemolysis, a chemical process that depolymerizes plastic for recycling. Much of the previous work on chemolysis and PET has focused on water, alcohols, and amines.\u00a0 Amines are a group of compounds derived from ammonia and are especially effective for PET depolymerizatio"
  },
  {
    "title": "Object that fell from the skies identified as separation ring from a rocket (nation.africa)",
    "points": 16,
    "submitter": "dltj",
    "submit_time": "2025-01-01T18:05:58 1735754758",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42567795",
    "comments": [
      "If planes, with active propellers or jet engines, are only audible for a diameter of 20-50km around the vehicle, how could a falling unpowered ring of metal be audible from 200km away as per TFA?\n \nreply",
      "It would have been supersonic at a very high altitude and likely came in at a steep angle, so sonic boom + large exposure area + long path.\n \nreply",
      "Sonic boom(s).  Unsure what the speed is, but could have been significantly higher than terminal velocity at sea level.\n \nreply",
      "While it was supersonic it would have produced a sonic boom all along it's path of travel.\n \nreply"
    ],
    "link": "https://nation.africa/kenya/news/object-that-fell-from-the-skies-identified-as-separation-ring-from-a-rocket-4875322",
    "first_paragraph": ""
  },
  {
    "title": "How to draw an outline in a video game (ameye.dev)",
    "points": 390,
    "submitter": "alexanderameye",
    "submit_time": "2025-01-04T09:14:28 1735982068",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=42593614",
    "comments": [
      "I loved the article that this links to about 'Jump Flood Algorithm'!:https://bgolus.medium.com/the-quest-for-very-wide-outlines-b...So fascinating! Thanks for indirectly leading me to this! I love thinking about all the various approaches available at the pixel/texel/etc level!It's also another case where it's a very clever way of generating a type of SDF (Signed Distance Field) that is doing a lot of the heavy-lifting. Such a killer result here as well! Any-width-outline-you-like in linear time?!! Amazing when compared to the cost of the brute-force ones at huge widths!I wholeheartedly endorse SDFs, whether they are 'vector' ones, function-based, like Inigo Quilez's amazing work, Or 'raster' ones like in the article, texel-or-voxel-based. Houdini supports raster-SDFs very well I think, has a solid, mature set of SDF-tools worth checking out (there's a free version if you don't have a lic)!And of course there's all the many other places SDFs are used!! So useful! Definitely worth raising-awareness of I reckon!\n \nreply",
      "Note that the 'Jump Flood Algorithm' is O(N log N) where N is the number of pixels. There is a better O(N) algorithm which can be parallelized over the number of rows/columns of an image:https://news.ycombinator.com/item?id=36809404Unfortunately, it requires random access writes (compute shaders) if you want to run it on the GPU. But if CPU is fine, here are a few implementations:JavaScript: https://parmanoir.com/distance/C: https://github.com/983/dfC++: https://github.com/opencv/opencv/blob/4.x/modules/imgproc/sr...Python: https://github.com/pymatting/pymatting/blob/afd2dec073cb08b8...\n \nreply",
      "In my own projects I use JFA/SDF-based outlines the most because of their quality as well as the possibility to render distance-based effects like pulsating outlines.This (https://x.com/alexanderameye/status/1663523972485357569) 3D line painting tool also uses SDFs that I then write to a tiny texture and sample at runtime.SDFs are very powerful!\n \nreply",
      "I NEED to get into shader programming and 3D rendering.Articles like this are awesome, I wish I could actually write a shader.\n \nreply",
      "I learned using Shadron 8 or so years ago.\n \nreply",
      "One day, I'd love to dive into stylised 3D graphics as an R&D project. There's been decent progress recently, but I think there's a lot of low-hanging fruit left to pick.Some open questions:- How do you reduce the detail of a toon-rendered 3D model as the camera zooms out? How do you seamlessly transition between its more-stylised and less-stylised appearance?- Hand-drawn 2D animations often have watercolour backgrounds. Can we convincingly render 3D scenery as a watercolour painting? How can we smoothly animate things like brush-strokes and paper texture in screen space?- How should a stylised 3D game portray smoke, flames, trees, grass, mud, rainfall, fur, water...?- Hand-drawn 2D animations (and some recent 3D animations) can be physically incorrect: the artist may subtly reshape the \"model\" to make it look better from the current camera angle. In a game with a freely-moving camera, could we automate that?- When dealing with a stylised 3D renderer, what would the ideal \"mesh editor\" and \"scenery editor\" programs look like? Do those assets need to have a physically-correct 3D surface and 3D armature, or could they be defined in a more vague, abstract way?- Would it be possible to render retro pixel art from a simple 3D model? If so, could we use this to make a procedurally-generated 2D game?- Could we use stylisation to make a 3D game world feel more physically correct? For example, when two meshes accidentally intersect, could we make that intersection less obvious to the viewer?There are probably enough questions there to fill ten careers, but I suppose that's a good thing!\n \nreply",
      "> Hand-drawn 2D animations often have watercolour backgrounds. Can we convincingly render 3D scenery as a watercolour painting? How can we smoothly animate things like brush-strokes and paper texture in screen space?There are various techniques to do this. The most prominent one IMO is from the folks at Blender [0] using geometry nodes. A Kuwahara filter is also \"good enough\" for most people.> When dealing with a stylised 3D renderer, what would the ideal \"mesh editor\" and \"scenery editor\" programs look like? Do those assets need to have a physically-correct 3D surface and 3D armature, or could they be defined in a more vague, abstract way?Haven't used anything else but Blender + Rigify + shape keys + some driver magic is more than sufficient for my needs. Texturing in Blender is annoying but tolerable as a hobbyist. For more NPR control, maybe DillonGoo Studio's fork would be better [1]> Would it be possible to render retro pixel art from a simple 3D model? If so, could we use this to make a procedurally-generated 2D game?I've done it before by rending my animations/models at a low resolution and calling it a day. Results are decent but takes some trial and error. IIRC, some folks have put in more legwork with fancy post-processing to eliminate things like pixel flickering but can't find any links right now.[0]: https://www.youtube.com/watch?v=ljjUoup2uTw[1]: https://www.dillongoostudios.com/gooengine\n \nreply",
      "The best results I\u2019ve seen at procedurally generated \u201cold school\u201d style pixel art sprites have come from highly LORA-ed diffusion models. You can find some on Civit AI.[1]So the future here may be a 3D mesh based game engine on a system fast enough to do realtime stable-diffusion style conversion of the frame buffer to strictly adhering (for pose and consistency) \u201cAI\u201d pixel art generation.[1] https://civitai.com/search/models?sortBy=models_v9&query=Pix...\n \nreply",
      "> Would it be possible to render retro pixel art from a simple 3D model?Not exactly retro pixel art, or maybe it is since it's been 25 years (omfg) but in Commandos 2+ we had 3d models for the characters, vehicles, etc which we rendered at runtime to a 2d sprite which we then mixed with the rest of the pre-rendered 2d sprites and backgrounds.\n \nreply",
      "A more modern example would be Dead Cells\n \nreply"
    ],
    "link": "https://ameye.dev/notes/rendering-outlines/",
    "first_paragraph": "Different techniques for rendering outlines in Unity.21 minute readRendering outlines is a technique that is often used in games either for aesthetic reasons or for supporting gameplay by using it for highlights and selections around an object. For example in the game Sable, outlines are used to create a comic-book-like style. In The Last of Us, outlines are used to highlight enemies when the player goes into stealth mode.In this post, I will discuss 5 techniques for rendering an outline around an object.\ud83d\udd8d\ufe0f Interested in an outline rendering toolkit for Unity? 3 years after making this post I made Linework!One of the most basic outline effects can be achieved by using a so called fresnel effect which can be used to render an outline on the rim/edge of an object. The fresnel effect describes the reflection/transmission of light when falling onto a transparent surface. However, when using it for rendering outlines, this physical meaning of the effect is not important. The following formu"
  },
  {
    "title": "One Dog vs. the Windows 3.1 Graphics Stack (wuffs.org)",
    "points": 157,
    "submitter": "todsacerdoti",
    "submit_time": "2025-01-04T11:10:20 1735989020",
    "num_comments": 51,
    "comments_url": "https://news.ycombinator.com/item?id=42594024",
    "comments": [
      "That old Windows 3.1 GUI looks so much more intuitive, efficient, and usable compared to what we've got today.https://wuffs.org/user/pages/02.blog/windows-3x-graphics/640...What would Win11 even look like on a lower-resolution display such as the one in TFA?The Win11 Start Menu is borderline unusable beyond typing in a keyword and praying to the circuits.  What happened?Naive hypothesis: Windows NT and 2k hit a sweet spot, then product managers have been working their magic ever since.KDE and Gnome are looking more and more appealing over time, despite not changing much :)\n \nreply",
      "SVGA support aside, it will never cease to astound me that you can load Windows 3.x on a modern standards-supporting PC and get basic VGA working out-of-the-box but you cannot do the same on modern Linux/BSD to get a basic software-accelerated VGA framebuffer supported by Xorg/Wayland if you don't have the right drivers installed (and the correct configuration files manually set up).(the dead xfree86 project was probably the closest attempt to making this \"just work\" though it had a long way to go; this approach was not preserved in the Xorg fork)\n \nreply",
      "Xfree86 never did anything different to Xorg here. If you boot via CSM on a modern PC (which is really not something you should do, but, well) you should get Xorg running with the VBE backend using x86emu to execute the video BIOS, and if you boot via EFI you should get modesetting running on top of efifb using whatever mode your firmware and bootloader left you in.But note that this is actually easier for 16 or 32 bit operating systems! Setting VESA modes involves making a real mode 16 bit call (there's nominally a 32 bit entry point for VESA but it was specced late in the standard's life and basically nobody implements a working version), and once you're running in 64-bit mode you've lost the ability to do vm86 so calling 16 bit code from userland becomes impossible. This is why x86emu is required (basically we read the video BIOS code and then run it under an x86 emulator), and it's not always perfect.\n \nreply",
      "I just wanna say that \"we read the video BIOS code and then run it under an x86 emulator\" sounds like some truly heroic effort by a bunch of engineers and I'm glad they did the work. I hope the people involved are proud of it.\n \nreply",
      "Linux supports vgafb/vesafb so this is possible if the distribution is configured appropriately.I think some/most distributions might not enable it out of the box because it would generally result in a low performance/quality experience and the user not realizing what the problem is, and nowadays almost all GPUs are supported natively, so nobody has invested in writing code to show a \"Using unaccelerated VGA/VESA, you may want to fix this\" popup.\n \nreply",
      "It's been a very long time, but I recall X having a generic VGA driver that \"just worked\". Are you saying that's not there anymore?\n \nreply",
      "Not only that, I remember X.org source tree had a bult-in x86 emulator so VGA bios of some PCI video cards could be run on Solaris.\n \nreply",
      "Technically not related to Solaris, since Solaris x86 existed and you wouldn't need it there, but yes, this was used on Sparc and any other CPU unable to run the card BIOS (including 64-bit x86 Linux, since virtual 8086 mode goes away when you're on long mode)\n \nreply",
      "Generic VGA doesn't generally exist on EFI platforms, the firmware doesn't program the card into a state where the VGA registers are going to do anything useful. You should get a working unaccelerated framebuffer from the firmware, though.\n \nreply",
      "I remember the \"generic\" VGA driver having horrible default settings that resulted in 300x200 at 16 colors.\n \nreply"
    ],
    "link": "https://wuffs.org/blog/windows-3x-graphics",
    "first_paragraph": "\n\n\t\tPublished 2nd Jan 2025\n\t\nWherein I learn too much about VGA hardware and generate some really cool glitch art while I try to fix somebody else's fix for a video driver that's older than I am.I'm a bit of a retro tech enjoyer, but I'm also pretty bad at it -- I don't have the space or the motivation to try and acquire actual old computers. Playing with 86Box/PCem is pretty fun, but it's not quite the same.So, instead, I make do with what I have. And the most ridiculous x86 machine I own is the Asus Eee PC 1000H, a netbook that I got in 2008 when that category was still new and exciting. It's borderline useless nowadays (it can't even run most up-to-date Linux distros due to its lack of x86_64 support), so sticking weird and anachronistic OSes on it is one way to keep it relevant!I'd like to write a full-fledged blog post about these adventures at some point, but for now I'm going to focus on one particular side quest: getting acceptable video output out of the 1000H when it's runnin"
  },
  {
    "title": "It Matters Who Owns Your Copylefted Copyrights (2021) (sfconservancy.org)",
    "points": 49,
    "submitter": "pabs3",
    "submit_time": "2025-01-02T23:50:29 1735861829",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=42580397",
    "comments": [
      "My new rule is to never contribute and do my best to avoid using any free software that requires a CLA. Shared copyright ownership is very important to maintaining software freedoms. It makes it impossible for a single party to change the license in ways counter to the communities desires. There have been many recent examples of this sort of bad behavior that have driven this point home for me.\n \nreply",
      "Agreed: don't sign a CLA!But then following this philosophy, shouldn't you favour copyleft licenses, too? Because if it's permissive, they can suddenly go proprietary without caring \"much\" about copyrights, right?I have come to these rules:- Never sign a CLA.- In my projects, the \"most permissive\" licence I use is MPLv2 (which is weak copyleft). When I release OSS software, there is absolutely no point in using a permissive license: MPLv2 should be fine for everybody. Of course sometimes I like the GPL family, and recently I've come to like the EUPL.\n \nreply",
      "Would you sign a CLA if it specified the License your code would be able to the project under? (I.e. the project can only use your code if it says MIT or BSD or GPL licensed).CLAs can have a legitimate purpose in clarifying copyright ownerships.\n \nreply",
      "> Would you sign a CLA if it specified the License your code would be able to the project under?I don't understand.> CLAs can have a legitimate purpose in clarifying copyright ownerships.Isn't that the whole point of a CLA? The CLA is usually a way for the contributor to renounce their copyright. In other words, the project asks me to make a contribution for free, and on top of that they want to own the copyright for it.If they want to own the copyright for my work, how about they pay me?\n \nreply",
      "This is why I don't want to put any free testing into a copylefted copyright like Element, let alone code or docs contributions. I realize I'm testing Discord for free, but it's different with Element, because for years I perceived it as being a vendor-neutral open source project. The copyleft is so it isn't vendor neutral.OTOH there are projects like Forgejo which are copyleft but are still vendor neutral. Even though it's vendor neutral, I wouldn't be too thrilled if it were AGPL, but it's just GPL. So I am still a happy Codeberg user. (Element is AGPL)Zulip is my favorite open source chat now. It's used by some stuff that's relevant to me right now including Bytecode Alliance and Julia.\n \nreply",
      "What's the deal with Element? Is it AGPL, but exclusively developed by one company? Or does it take contributions but with a CLA giving the copyright to the company? I'm not aware of the situation there.Not sure I follow why AGPL is a problem, though.\n \nreply",
      "It's bait and switch, they built the community of Matrix and Element around a promise of openness but specifically chose a license to make it another Discord or Slack.You can read between the lines here: https://element.io/blog/element-to-adopt-agplv3/HN thread to help with the context: https://news.ycombinator.com/item?id=38162275\n \nreply",
      "To be precise, I'm guessing the problem is more of the CLA, and not AGPL itself.\n \nreply",
      "It's more that vendor neutral is a sweet spot for me. AGPL, as well as GPL for a library rather than something that works well as a standalone application, brings it closer into what feels to me like no software vendor territory - one where you find something else to sell (vend) other than software, like Stallman musing about choosing to be a waiter rather than have any non-free software:> Well, the most simple alternative was to leave the software field, do something else. Now a lot of programmers say to me, 'the employers hiring programmers demand that I do this -- if I don't do this I will starve.' Now, that's silly. Anybody can leave the field of programming. Even in the US, there are millions of people who make a living not by writing software. I have no other special skills, nothing else that I'm particularly good at. But I'm sure I could have become a waiter.  (Now, maybe I couldn't be a waiter at one of the fanciest restaurants.)  There is nothing unethical about being a waiter. And there is one thing -- you are not going to starve.http://mikro-berlin.org/Events/OS/ref-texte/stallman.html\n \nreply",
      "I am still not sure what you are saying. Are you saying that Forgejo being GPL allows e.g. Codeberg to modify it without releasing their changes, and with AGPL they couldn't build a valid business model?\n \nreply"
    ],
    "link": "https://sfconservancy.org/blog/2021/jun/30/who-should-own-foss-copyrights/",
    "first_paragraph": "Home / News / Blogby Bradley M. Kuhn\n  on June 30, 2021\nThroughout the history of Free and Open Source software (FOSS), copyright\r\n  assignment has simultaneously been controversial and accepted as the norm\r\n  in our FOSS communities.  This paradox, I believe, stems entirely from some\r\n  key misunderstandings that perpetuate.  This issue requires urgent\r\n  discussion, as two of the most important FOSS projects in history\r\n  (GCC\r\n  and glibc) are right now\r\n  considering substantial and swift changes to long-standing copyright\r\n  policies that date back to the 1980s.  This event, and other recent events\r\n  over the last few years in the area of GPL compliance and corporate FOSS\r\n  adoption, point to long-term problems for projects.  This essay works\r\n  through these nuances, and will hopefully assist FOSS contributors as they\r\n  make difficult decisions about copyright ownership for their projects.  At\r\n  the end, I provide a summary list of issues to consider when creating\r\n  copyrigh"
  },
  {
    "title": "Self driving 1993 Volvo with open pilot (practicapp.com)",
    "points": 503,
    "submitter": "trainsarebetter",
    "submit_time": "2025-01-04T06:30:24 1735972224",
    "num_comments": 116,
    "comments_url": "https://news.ycombinator.com/item?id=42592910",
    "comments": [
      "Very cool! I love the Volvo 940- it stands out as one of the best quality, and best designed cars ever made. It's an incredibly mature design evolved slowly from the Volvo 140 in the 1960s through the 240 in the 70s and the 740 in the 80s, and by the 940 they worked out almost any possible issue. I only wish one could have gotten them with AWD and a fuel efficient diesel (they had the latter but it was not sold here in the USA).The author seems to have a lot of electrical hacking knowledge, but didn't know some car stuff that could have made getting these controls installed much easier:1) They could have just swapped in a newer BOSCH ABS pump, which can activate the brakes electrically without involving the brake booster. European cars started getting these when they got traction control in the late 90s, but I believe some would be virtually (or maybe even exactly) a direct swap into this vehicle. I was able to do this in a VW with about 10 minutes of work, which uses the same basic ABS systems as Volvo. This is assuming the car already had factory ABS which I think most (but possibly not all) 940s did.2) They could fix the steering problem by swapping in an entire electric steering rack- they're fairly standard dimension wise, installing a fully manual rack from a Volvo 240, or adding an A/C compressor clutch to the hydraulic power steering pump to disable it above parking speeds (the only time torque would be high anyways). Moreover, these racks are strong enough to simply work with the hydraulic assist removed, because people in the Volvo racing/performance community do it all the time.3) This car does not have a carburetor- it is electrically fuel injected. You can see the fuel injectors and rail above the throttle body. This could be just a mistranslation if the author is not a native english speaker. However, more importantly there was a factory system on this car to electrically control the throttle for the cruise control. They are missing those parts, but they are cheap and common, and would have just dropped in to a bracket and cam already on his engine. They consist of a vacuum servo on the throttle itself connected to a box that can actuate this with an electrical signal.\n \nreply",
      "Author here! Super surprised this made hackernewsIt does indeed not have a carburator (I wrote this while half asleep on a plane, I meant to say that it has a mechanical throttle/air valve which is cable operated).It also doesn't have ABS (although I believe this was an option on the car starting this model year), so swapping out that pump wasn't an option. The iBooster is a great and easy retrofit, definitely recommend!Haven't done it yet, but I'm just going to lower the pressure of the hydraulic steering by shimming the regulator, should be enough. It already steers somewhat ok without this mod too\n \nreply",
      "Great to hear from you, and awesome project. I had an early 90s 240 that did have ABS so assumed incorrectly that yours would. Dropping the hydraulic pressure sounds like a really good idea, hope that works!\n \nreply",
      "I have a Xc70 now, from the year they stopped making these brutalistic work horses, and having owned a 740, v50 and a V70 before I believe this is peak \"herrg\u00e5rdsvagn\". Nowadays the large Volvos like the v90 are stupid Chinese luxury cars that I will never buy.\n \nreply",
      ">This is the only major actuator that I couldn\u2019t easily find a suitable modern automotive solution for. This is mainly because newer cars don\u2019t have carburators anymore, but rather use direct injection and advanced engine control units.Actually injection system is largely independent from throttle control. Required amount of fuel is calculated based on manifold air pressure sensor, so there were cars with fuel injection and cable operated throttle plate. I suppose the problem was with plumbing in modern throttle body as carburetor needs to be upstream of throttle plate.\n \nreply",
      "80's and 90's Euro's were mostly using MAF sensors, a hot wire that detects how much air is passing, not MAP+Temp sensor. That came a little later but all the LH systems at the time had MAF sensors\n \nreply",
      "would the distinction not be on wether the vehicle is turbocharged? or did they use map/maf regardless?\n \nreply",
      "the turbos also used the MAF, its on the intake side so theres no pressure. They're not very reliable. It wasnt until I think saab came out with the trionic system that a single computer controlled boost, fuel, and timing all together. Prior to that you had a few independent brains. ECU setting fuel, timing sometimes with an EZK brain, etc.\n \nreply",
      "Yup. T5 used a combination of MAP and temp sensors to do its fuel mappings. The system was able account for fuel quality by adjusting the spark timing and boost amount via controlling a solenoid valve down to whatever the actually physical waste gate spring was set to.The later T7 system involved a MAF sensor behind the intake, while still retaining the temp sensors and MAP but changed the fuel mapping in the system to be a speed - density system for greater control and better boost handling with large temp and altitude changes. (T5 shot for a pressure value t7 shot for a target air mass).There is software called t5suite / t7suite that allowed you to remap everything in the ECU, basically letting you modify the system in every possible way but not making you do all the setup that a complete stand alone unit would make you setup. It was and is, amazing. You could flash an ECU via BDM interface out of car, and in later T7 models actually make real time changes on a laptop that would immediately take effect. Later you could write them to the ECU flash (it requires more than 12v to flash, although some people rigged things up so they could also flash the memory in car).Good times. I still have a t7 Saab on my lawn. My custom tune isn't the best, but it went damn good.Interesting to note, the MAF in the t7 could be placed on the pressure side of the world and it would work perfectly fine minus the fact that they would die a lot quicker due to oils and such on the pressure side of the turbo. People sometimes did this as it was the only way to run a true blow off valve for cool effect. If the MAF is in the stick location and you do not recirculate that air, the car would be fueling for XYZ air density but you'd have dumped some out of the closed system and it ran rich as hell and like crap .\n \nreply",
      "hello fellow saab.T5 is awesome. I have a T5 setup in my 88. Works great!\n \nreply"
    ],
    "link": "https://practicapp.com/carbagepilot-part1/",
    "first_paragraph": ""
  },
  {
    "title": "Labwc: Wlroots-based window-stacking compositor for Wayland, inspired by openbox (labwc.github.io)",
    "points": 22,
    "submitter": "NeutralForest",
    "submit_time": "2025-01-04T20:02:29 1736020949",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42597283",
    "comments": [
      "> it cannot be controlled with dbus, sway/i3-IPC or other technology. The reason for this is that we believe that custom IPCs and protocols create a fragmentation that hinders general Wayland adoption.If anything, this leads to more fragmentation since now you have to hack your own way to control it, rather than using something standard (at least on Linux) like dbus.\n \nreply",
      "You can only really do that by forking though, so it's probably not going to happen. That said, the unspoken part of that is that it supports Wayland protocols for the features that would be provided by dbus/swaysock/etc. so you can just use \"stock\" tools like wlr-randr and sfwbar to manage outputs or provide a task switcher. Some of these protocols are still wlroots extensions (though that doesn't mean you won't find them supported elsewhere) but increasingly those are being replaced with standard protocols (with the caveat that GNOME Mutter will probably never implement any of them, but I'm not sure if anyone cares; GNOME is probably going to wind up just being it's own thing that is technically Wayland but not how anyone else does it.)(P.S.: I suspect Swaysock might've never existed if it were not for the fact that Sway is more or less a direct recreation of i3wm as a Wayland compositor; it is the equivalent of i3 IPC. You can and should just use Wayland protocols even in Sway where possible.)\n \nreply",
      "There is also wayboxhttps://github.com/wizbright/wayboxGlad these 2 projects exist since I would want fvwm (prefered) or a fluxbox clone under wayland when I am forced to move\n \nreply",
      "The configuration being written in xml makes me feel like I am not the target audience for this program.\n \nreply",
      "Well, that's openbox for you. Openbox though used the XML quite nicely to enable GUI configuration (never looked into the code, I assume they did not implement it just with regexps...), so there is that advantage at least.\n \nreply"
    ],
    "link": "https://labwc.github.io/",
    "first_paragraph": "Labwc is a wlroots-based window-stacking compositor for wayland, inspired by openbox.It is light-weight and independent with a focus on simply stacking windows well and rendering some window decorations. It relies on clients for panels, screenshots, wallpapers and so on to create a full desktop environment.Labwc tries to stay in keeping with wlroots and sway in terms of general approach and coding style.Labwc only understands wayland-protocols & wlr-protocols, and it cannot be controlled with dbus, sway/i3-IPC or other technology. The reason for this is that we believe that custom IPCs and protocols create a fragmentation that hinders general Wayland adoption.  "
  },
  {
    "title": "Hacking yourself a satellite \u2013 recovering BEESAT-1 [video] (ccc.de)",
    "points": 106,
    "submitter": "Duke_Pixie",
    "submit_time": "2025-01-02T15:45:34 1735832734",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42575390",
    "comments": [
      "I remember when PistonMiner finished the fix and executed it on the \"flight model\". They were quite literally jumping up and down in happiness and the whole Chair of Space Technology at TU Berlin was gossipping about it in the hallways :) Amazing hack, absolute hats off to them. By the way, this is their bachelor's thesis.We have used the vtable trick many times in satellite software operations, whenever we had to \"quickly patch\" this or that function, without having to uplink a whole software update - which as they explain in the talk, is quite an involved process that takes significant time and resources to complete.Happy to answer or forward any questions about nanosatellites, software in space, etc!P.S.: it looks there is a bit of a mix-up with the audio of the recorded talks. The default audio track contains a Polish dubbing. Select the \"eng-deu-pol 1080p (mp4)\" which seems to be the correct one. (And thanks to the A/V team at CCC for your efforts to make this content accessible to everyone <3)\n \nreply",
      "This inspires me to make a nano satellite! Haha\n \nreply",
      "You should do it! It's never been more accessible and affordable (relatively speaking, of course). And it's a lot of fun!\n \nreply",
      "I have to say this is perhaps one of my favourite CCC talks.Interesting subject matter, novel technical problems and solutions, story narrated at speed and with just enough information to fill in typical knowledge gaps, and on top of all of that there's slides and answers ready to go for random questions from the audience.Wonderful job, PistonMiner!\n \nreply"
    ],
    "link": "https://media.ccc.de/v/38c3-hacking-yourself-a-satellite-recovering-beesat-1",
    "first_paragraph": "\n\nPistonMiner\n\nIn 2013, the satellite BEESAT-1 started returning invalid telemetry, rendering it effectively unusable. Because it is projected to remain in orbit for at least another 20 years, recovering the satellite and updating the flight software would enable new experiments on the existing hardware. However, in addition to no access to telemetry, the satellite also has no functional software update feature. This talk will tell the story of how by combining space and computer security mindsets, the fault was correctly diagnosed without telemetry, software update features were implemented without having them to begin with, and the satellite was recovered in September of 2024.\nIn 2009, BEESAT-1 was launched into low earth orbit as the first 1U CubeSat of Technische Universit\u00e4t Berlin. In 2011, the satellite started returning invalid telemetry data. After a short amount of time spent diagnosing the issue, operators switched to the redundant on-board computer, which initially resolved "
  },
  {
    "title": "A Return to Polymathy (2015) [pdf] (paulrcohen.github.io)",
    "points": 91,
    "submitter": "mirawelner",
    "submit_time": "2025-01-04T18:01:39 1736013699",
    "num_comments": 41,
    "comments_url": "https://news.ycombinator.com/item?id=42596450",
    "comments": [
      "> Those colleges and universities that figure out how to organize research and teach new foundations and polymathy and prepare their students to understand a world in which every system, at every scale, acts causally on others, will see their stock rise. The rest will struggle to remain relevant.It is safe to say that this prediction did not come to pass. The system (pun) keeps churning idiot-savants, well-trained cogs in the giant machine that society had been reduced to.\n \nreply",
      "> If one is looking for a single label to encompass many of the new foundations \u2013 perhaps to form a new school or department \u2013 I would suggest Systems.What about good old philosophy?Philosophy already has a long tradition of birthing new sciences, a long tradition of encouraging polymath thinking and a long tradition of estabilishing foundational ways of thinking.\n \nreply",
      "Physics is like this too. An odd thing about physics is that it doesn't have very many of its own techniques or tools, so we have to go outside of our field if we want to solve any nontrivial problem -- theoretical or experimental. For instance I didn't take any electronics or programming classes beyond \"101\" level, but have ended up using both extensively in my later work.At my present workplace, I've noticed that the people who have polymath tendencies tend to have physical science degrees.\n \nreply",
      "Once you know the foundation of things, it\u2019s much easier to build on that than if you start with a higher abstraction\n \nreply",
      "I don't disagree with this characterization of studying physics, but there is a very noticeable tendency for physicists to expect their expertise to transfer to fields they are very definitely not experts in. (Not exclusive to physicists, obviously, it's just a lot of public science communicators are physicists.)EDIT: Most noticeably from the absolute garbage code I had to make work that these 'genius coders' produced. They were good at physics, but they should never have written any code if it could have been prevented.EDIT#2: And an adjacent poster \"FredPret\" demonstrated it perfectly. Let's derive Evolution from particle physics... and go! (Sean Carroll is a good antidote to this attitude... he knows there are levels of description that are useful.)\n \nreply",
      "It's certainly not exclusive to physicists. The syndrome is called \"engineer disease,\" maybe just because engineers outnumber physicists.And garbage code isn't exclusive to physicists. The coders often complain when they have to deal with code from other coders. The dilemma is that the last nontrivial problem to be solved without computation was probably solved sometime in the 1930s. The most powerful \"physics software\" is a coding stack.In my own case I've made a career long effort to learn good coding practices, but I don't expect my code to go straight into production.As for deriving evolution from particle physics, I don't think I've known a physicist who would recommend that.",
      "> there is a very noticeable tendency for physicists to expect their expertise to transfer to fields they are very definitely not experts inIt's interesting the converse is true: techbros often think they would've excelled in physics PhD programs\n \nreply",
      "Oh, it absolutely is. AFAICT it's universal for \"experts\" (whatever that means) in any given field.(Self-referentially: Even for me... So take what I said above with a grain of salt, but I'm pretty sure this has solid research behind it.)\n \nreply",
      "Philosophy degrees can be designed in this way. Various \u201cphilosophy of X\u201d courses to explore. Logic and theory of computation are typically found here.\n \nreply",
      "Great piece. One of the best places cultivating this type of polymathy today is Stanford's Symbolic Systems Program. https://symsys.stanford.edu/\n \nreply"
    ],
    "link": "https://paulrcohen.github.io/papers/Polymathy.pdf",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: WebGPU + TypeScript Slime Mold (github.com/suboptimaleng)",
    "points": 11,
    "submitter": "SuboptimalEng",
    "submit_time": "2025-01-02T16:37:38 1735835858",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://github.com/SuboptimalEng/slime-sim-webgpu",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        \ud83e\udda0 Slime mold simulation with WebGPU and TypeScript.\n      I made this slime mold simulation to learn more about WebGPU and compute shaders.\nIt's essentially a recreation of Sebastian Lague's coding adventure (albiet with\nfewer features).Here's a 45-second demo on\nTwitter,\nThreads, and\nr/graphicsprogramming.\nYou can also try the playable demo on my website (linked in the about section of\nthis repo). Just know that WebGPU doesn't work on all devices so you might get an\nerror screen.If you prefer a more in-depth video, you can checkout this 5-minute dev log on\nYouTube where I showcase an extended demo\nand answer the following questions:Shield: This work is licensed under a\nCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n        \ud83e\udda0 Slime mold simulation with WebGPU and TypeScript.\n      "
  },
  {
    "title": "Ask HN: How are you using LLMs for traversing decompiler output?",
    "points": 64,
    "submitter": "mjbale116",
    "submit_time": "2025-01-02T10:16:26 1735812986",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=42573232",
    "comments": [
      "These guys are building foundational models for this purpose: https://reveng.ai/. The results are quite compelling, and they have plugins for your favourite reverse engineering tools.\n \nreply",
      "The domain makes it look like \u201cRevenge AI\u201d. Terrible name. Not as risqu\u00e9 as some others\u00b9 but not as fun or memorable either.\u00b9 https://www.snopes.com/fact-check/domain-thing/\n \nreply",
      "i don't think so (that it is a terrible name); it's a pretty common term https://www.urbandictionary.com/define.php?term=reveng\n \nreply",
      "Binary Ninja has an AI integration called side kick, it has a free trial but I'm not sure it can be used in the free web version. [1]In my experience, the off the shelf LLMs (e.g. ChatGPT) do a pretty poor job with assembly, they can not reason about the stack or stack frames well.I think your job will be the same with or without AI. Figuring out the data structures and data types a function is operating on and naming variables.What are you reverse engineering for? For example, getting a full compilable decompilation has different goals than finding vulnerabilities or patching a bug.1. https://sidekick.binary.ninja/\n \nreply",
      "Out of curiosity, what would you say the current state of the art is for full compilable decompilation? This is something I have a vague interest in but I'm not involved enough in the space to be on top of the latest and greatest tooling.\n \nreply",
      "Echoing IDA but its pricing is a huge PITA if you\u2019re using it in a hobbyist capacity i.e. you don\u2019t have an employer willing to pay for it. Could opt for the home version but that\u2019s a yearly cost and you have to use their cloud decompiler. Ghidra\u2019s your best bet if you want something FOSS and community-driven although not as great at decompilation.\n \nreply",
      "Looking at an individual function, IDA hex-rays output is often recompilable as-is (or with minor modifications), but it won't necessarily be idiomatic, especially if you don't have symbol information.\n \nreply",
      "Most decompilers do not strive for recompilability. [1] I believe there are (or were) some academic projects that aimed for recompilation as a core feature, but it is a hard problem.On the commercial side, IDA / HexRays [2] is very strong for C-like decompilation. If you're looking at Go, Rust, or even C++ it is going to be a little bit more messy. As other commenters have said, you'll work function-by-function and it is expensive, though the free version does have decompilation (F5) for x86 and x64 (IIRC).Binary Ninja [3] (no affiliation) is the coolest IMO, they have multiple intermediate representations they lift the assembly through. So you get like assembly -> low level IL -> medium level IL -> high level IL. There are also SSA forms (static single assignment) that can aid in programmatic analyses. The high level IL is very readable but makes no effort to be compilable as a programming language. That being said, Binary Ninja has implemented different \"views\" on the HLIL so you can show it as pseudo-C, Rust, etc. There is a free online version and the commercial version is cheaper than IDA but still expensive. Good Python API, good UI.Ghidra [4] is the RE framework released by NSA. It is free and open source. It supports a ton of niche architectures. This is what most people use. I think the UI is awful, personally. It has a decompiler, the results are OK. They have an intermediate representation (P-Code) and plugins are in Java (since it is written in Java). I haven't worked much with it.Most online decompilations you see for old games are likely using Ghidra, some might be using IDA. This is largely a manual process of doing a function at a time and building up the mental map of the program and how things interact.Also worth mentioning are lifters. There were a few projects that aimed to lift assembly to LLVM IR (compiler framework's intermediate representation), with the idea being that then all your analyses could be written over LLVM IR as a lingua franca. Since it is in LLVM IR, it would be also recompilable and retargetable. [5][6]1. https://reverseengineering.stackexchange.com/questions/2603/...2. https://hex-rays.com/ida-free3. https://binary.ninja/free/4. https://ghidra-sre.org/5. https://github.com/avast/retdec6. https://github.com/lifting-bits/mcsema\n \nreply",
      "Meta has a foundation model trained on LLVM IR: https://ai.meta.com/research/publications/meta-large-languag...\n \nreply",
      "lol ok, now we\u2019re getting into pure-nonsense territory\n \nreply"
    ],
    "link": "item?id=42573232",
    "first_paragraph": ""
  },
  {
    "title": "Don't Clobber the Frame Pointer (nsrip.com)",
    "points": 50,
    "submitter": "felixge",
    "submit_time": "2025-01-03T06:57:26 1735887446",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=42583246",
    "comments": [
      "GoLang assembly boggles my mind - I understand why it's there, but having looked at it few times makes me wonder if it could've been prevented somehow (I guess not, cryptographic primitives would be way too slow, redirecting them through some kind of ffi would require a shared lib, yada yada yada)...\n \nreply",
      "They could have added crypto primitives via intrinsic, or had some other way of including the edge case functionality it solves.But it's good enough and I guess it compiles quick which was a major goal for golang.\n \nreply",
      "Major Rust cryptography libraries (see for instance Ring) use assembly, too. It's a pretty normal thing to do.\n \nreply",
      "It's kinda weird that languages (or at least languages with pretensions to cryptography) are still forcing people to resort to asm directly rather than offering some sort of first-class support for constant time operations and not leaving secrets lying around in memory. It doesn't need to be super high level, it just needs to clear the infinitely low bar of assembly language. Does any language offer such a dedicated facility?\n \nreply",
      "Sure but golang has its own special assembly flavor rather than using standard gcc flavor inline assembly. Probably because it's a soup to nuts compiler but still.\n \nreply",
      "The point of this article is that Go-specific assembler generators (Avo in particular) are better than standard assembly for this purpose.\n \nreply",
      ">soup to nuts compilerAny chance you can explain that to rubes like me?\n \nreply",
      "Go isn't built on an existing compiler framework like LLVM. It does its own code generation, has its own assembler.\n \nreply"
    ],
    "link": "https://nsrip.com/posts/clobberfp.html",
    "first_paragraph": "\nHome\n-\nRSS\nPosted 2024-12-24, updated 2025-01-03\nRecently I diagnosed and fixed two frame pointer unwinding crashes in Go.\nThe root causes were two flavors of the same problem:\nbuggy assembly code clobbered a frame pointer.\nBy \"clobbered\" I mean wrote over the value without saving & restoring it.\nOne bug clobbered the frame pointer register.\nThe other bug clobbered a frame pointer saved on the stack.\nThis post explains the bugs,\ntalks a bit about ABIs and calling conventions,\nand makes some recommendations for how to avoid the bugs.\n\nHere's the short version of what you should do when writing assembly for Go to avoid the problems discussed in this post:\n\n\nFirst, read the Go assembly guide.\n\n\nPrefer an assembly generator like Avo.\nTools like Avo are aware of the underlying rules for using registers and manipulating the stack,\nand generally make writing non-trivial amounts of assembly easier.\n\n\nIf your assembly function calls Go functions,\nprefer not to use the frame pointer registers a"
  },
  {
    "title": "Show HN: Open Rewind \u2013 POC for audio and screen and video streaming to S3 (github.com/janwilmake)",
    "points": 55,
    "submitter": "wwoessi",
    "submit_time": "2025-01-04T18:25:39 1736015139",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=42596607",
    "comments": [
      "capturing and uploading a whole new PNG for each screencap is not what I would call 'efficient', and to meet the use case of Rewind.ai in the first place it should have some OCR mechanism to pull up the relevant screencaps.The thing that enabled rewind.ai and MS Recall is storing the series of screenshots more like a HEIF, allowing for massive compression ratio and on-device storage + OCR provided by the OS (Live Text since Monterey 2021 [0], Microsoft introduced it last year for Snapdragon based AI-PCs [1])I guess this is a good starting point if the goal is to fill S3 buckets with screencaps of multiple users, but then we're just back to corporate spyware, not tools for helping individuals use their machine more effectively.That said, if I was using my own minio backend, it would be neat to archive my screen captures but I would change it so it captures after, say, every keystroke, and every time my mouse stops moving, and after every click. That way I have high density capture of taking actions, and low density otherwise. In any case collecting the data is not the issue, making an interface where that data becomes useful to help me remember something is.[0] https://support.apple.com/guide/preview/interact-with-text-i...[1] https://learn.microsoft.com/en-us/windows/ai/apis/text-recog...\n \nreply",
      "PNG per frame? Ouch.If I was to build one of these, which I'm not, I would try for a RTSP to bucket uploader. That way you could do the actual capture and compression with OBS like any other streamer - or use it with IP security cameras etc. You'd probably end up with a pile of video-ts pieces which could be replayed later using HLS.\n \nreply",
      "I know right :') Not very efficient yet.I've also explored Swift AVFoundation to drop frames and colours at the moment of recording, but won't be implementing it at this time.This was just a POC, and couldn't hack it in a day.\n \nreply",
      ">  we're just back to corporate spywareMost feel that Recall is also this.\n \nreply",
      "ffmpeg works well, especially on apple silicon using video toolbox. That's how I approached it.Also, automatically doesn't cost storage for identical screenshots (no activity) and very cheap for just moving your mouse around or typing a few characters.\n \nreply",
      "rem looks very intriguing, I'll give it a try, cross platform would be even better ofc. You're doing it without funding?I wish the LLMs were tuned into the open source landscape more, so when someone has an idea for a POC and asks the bot to write it for them, it would go clippy mode and say \"it looks like you're trying to build an open source rewrite of Rewind, would you like to clone the rem repo and contribute to that instead?\" lol\n \nreply",
      "No funding- I built mostly over 2023-2024 holidays and then a little here and there.And yeah- lots of similar projects and/or possible startups have popped up as well\n \nreply",
      "Interesting ideas! Seems worth exploring for sure.\n \nreply",
      "Nice! This is needed as it seems rewind.ai still stores locally but limitless the product they seem to put more energy into goes to the cloud.I really like the rewind.ai retrieval mechanism. I believe their recording mechanism is highly broken. It often fails to sync to the os calendar and will ask you to record meetings you deleted months ago.I don\u2019t understand the webcam recording need. I\u2019m not sure what signal you get from that since if you are in a web meeting you already have that on screen. Or if you are coding you might get a few WTF frown faces if working on a hard bug. But you made it optional, so that\u2019s good.\n \nreply",
      "Thank you all so much for chming in about rewind. I\u2019ve been ruminating about what to do about my subscription. To see that I\u2019m not alone in paying for this app that the founder ditched\u2026 I finally feel heard. Thank you!While we\u2019re here, has anyone been able to export audio from Rewind.ai\u2019s local storage?\n \nreply"
    ],
    "link": "https://github.com/janwilmake/efficient-recorder",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Attempt to create an Open Source Privacy Focused Rewind.ai Alternative for data capture\n      Today I was inspired by this tweet after which I fell into this rabbit hole to create an efficient recorder for your Screen, System audio, and Mic.After trying to implement this using Claude in Swift I completely failed (see folder swift-version) so I decided to create a simpler version (also using Claude) in Node.jsGoal: Create the most battery-life friendly recorder to stream video/screen/mic/system-audio to a cloud storage service of choice, open source.This package requires:SoX (Sound eXchange) for audio recordingWebcam capture toolsYou'll need:Run the recorder using npx:Audio MonitoringScreenshot CaptureWebcam CaptureEfficient UploadAudioScreenshotsWebcam\"Command not found: rec\"S3 Upload IssuesNo Audio/Video InputMIT LicenseContributio"
  },
  {
    "title": "Show HN: I created a PoC for live descriptions of the surroundings for the blind (github.com/o40)",
    "points": 14,
    "submitter": "o40",
    "submit_time": "2025-01-04T10:41:49 1735987309",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=42593919",
    "comments": [
      "au revoirhttps://www.youtube.com/watch?v=Wuntz3KDIAk\n \nreply",
      "PoC means \"point of care\" in this context?\n \nreply"
    ],
    "link": "https://github.com/o40/seesay",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Live image description solution using ESP32-CAM + Phone + Server\n      I wanted to see if I could create a low-cost tool for the blind to get live description of the scene in front of a camera.The idea is to have images taken at a set interval, which are then described using an AI model, and read back to the user using voice synthesis.Since I was going for low cost (<30$), and wanted to learn more about software development on arduino, I bought a ESP32-CAM with built-in WiFi to capture the images.To describe the image I selected the gpt-4o-mini model. I didn't think much about which model to use, but this seemed like a good start.The proof-of-concept solution works, but has some short-comings.One driving force for creating this tool is how expensive the alternatives are. However alternatives seem to be emerging at the moment so ther"
  },
  {
    "title": "The DIY Multideck (mauri.app)",
    "points": 102,
    "submitter": "vlugorilla",
    "submit_time": "2025-01-03T09:13:29 1735895609",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=42583986",
    "comments": [
      "Remotely related to something I was wondering yesterday: outside of 'house rules'-type variations, are there major changes to common games that are viable, popular and catalogued somewhere? e.g., add a few pieces and turn Monopoly into a zombie horde game, or change Scrabble into a dungeon crawler or Uno into a space RPG.\n \nreply",
      "Can you imagine how exciting 3D Bridge might be?  The post game analyses would last the square of the normal time - yay!\n \nreply",
      "I love it I really do.  Great concept and nice site.However.  Its hilariously unrealistic.  Just memorize these 500 simple glyph's and become a master of conceptual game design and you can play anything with this deck!The hard part is not the deck.  Its finding another person that will actually learn all this so you can use it,lol.\n \nreply",
      "This reminds me of https://en.wikipedia.org/wiki/Piecepack .\n \nreply",
      "This is a fantastic essay and project, but the chances of getting anyone to play an actual game with it are basically zero.\n \nreply",
      "I have read novels written by engineers. More often than not, they are unreadable. They fail to understand why people read (or write). They are an engineer's idea of what a novel should look like.In the most loving way possible and as a software engineer myself, this is an engineer's idea of what a deck of cards should look like.\n \nreply",
      "What, like There Is No Antimemetics Division?I think you read novels by bad writers, who happened to be engineers.\n \nreply",
      "I have never heard of that book, but a quick Google search reveals that the author is in the process of \"releasing version 2\". Refer to the previous comment for my opinion on that matter.\n \nreply",
      "It's being rewritten for content licensing reasons.\n \nreply",
      "Well, it's a fantastic book.\n \nreply"
    ],
    "link": "https://diymultideck.mauri.app/manual/",
    "first_paragraph": "The DIY multideck is a deck of 162 playing cards (3 standard decks) that allows you to play hundreds of existing games, not only classic card games but also modern games that use components like boards or coins. The DIY multideck is ideal for traveling, prototyping new games, and trying games before buying. The cards are waterproof and very resistant, ready to join you on your next trip!This is how it works: when you play a game, you pick a subset of cards and focus on a specific corner. Sometimes, you may draw on the cards to play games that use words, icons, or boards. On the official website, you'll find instructions to play many games. Here are some example games for every game system supported:The normal cards have the following features:The following diagram shows where each feature is located with a graphic explanation:Besides the normal cards, there are 2 extra cards with different designs.These are some example cards: one of each basic suit, the 2 extra cards, and 1 card back."
  },
  {
    "title": "Phase behavior of Cacio and Pepe sauce (arxiv.org)",
    "points": 314,
    "submitter": "rev13013",
    "submit_time": "2025-01-03T13:59:49 1735912789",
    "num_comments": 153,
    "comments_url": "https://news.ycombinator.com/item?id=42585707",
    "comments": [
      "The secret is that restaurants which make traditional cacio e pepe are using pasta water to emulsify the sauce.But it's not the same pasta water you're using at home!Only a tiny amount of starch is coming off of the 500g of pasta you just cooked in the proper ratio in 5000g of water (with 50g of salt).  They've been cooking with their pasta water all day or all week;  It's completely full of starch that came off the other pasta.Dump a bunch of cornstarch or flour in there to get above 1% concentration (or more efficiently, into a tiny portion in a bowl) to replicate the emulsifying effect, or just use a different emulsifier.\n \nreply",
      "Discussed about half way through this post: https://www.seriouseats.com/how-to-cook-pasta-salt-water-boi...\n \nreply",
      "did this recently and it totally works\n \nreply",
      "Or just use less water to cook the pasta? What\u2019s the downside?\n \nreply",
      "You certainly can, but:- It's still not going to be enough starch- You can't rely on box cooking times even as a starting point. Your pasta will take significantly longer to cook, since it will bring down the temperature of the water when you put it in, since there's so little water\n \nreply",
      "1. The starch comes from the pasta, not the water. Decreasing the water increases the concentration of the starch in said water. That\u2019s why every good recipe for cacio e Pepe I\u2019ve seen recommends using as little water as possible2. This has been thoroughly debunked. Kenji did a full write up of this but suffice to say that starches absorb water starting at 180 degrees. As long as you have the water above that temp it will cook in the same amount of time.https://www.seriouseats.com/how-to-cook-pasta-salt-water-boi...\n \nreply",
      "In my experience box cooking times are never quite right, and irrelevant if you're going to be finishing your pasta in the sauce anyway.Unless you're extremely familiar with the exact brand of pasta, temperature of your stovetop, etc., you should be tasting your pasta toward the end of cooking to decide when to stop cooking it.> - It's still not going to be enough starchI'm inclined to disagree, but only have anecdata on this, so I can't really get into an extended debate over it. So I guess now I get to look forward to experimenting with starch additions the next few times I cook pasta.\n \nreply",
      "Here is a great video on cooking pasta with less water:https://www.youtube.com/watch?v=259MXuK62gU&t=219s\n \nreply",
      "Clumping\n \nreply",
      "I don't get clumping. I use an adequate quality pasta (De Cecco mostly), stir it when I put it in the water, and a few times after that, cooking to al dente. If I'm making a Caccio e Pepe or Carbonara I cook the spaghetti or (my preference) Buccatini I'm aiming for the minimum amount of liquid left, ideally just enough to put in the sauce. I use a frying pan so I can lay the noods out flat to minimise the water.As I said I don't get clumping, it is absolutely possible to cook noods in minimal water without clumping because I do it so try switching some thing up if it's happening to you.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2501.00536",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Show HN: Pretty-print your chess games using Prettier (github.com/gmasclet)",
    "points": 74,
    "submitter": "gmasclet",
    "submit_time": "2025-01-04T14:37:52 1736001472",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=42594896",
    "comments": [
      "I've always wanted to look into writing my own Prettier plugins, how'd you feel about getting started working with their little mini DSL (fill, join, hardline, line, etc.)?\n \nreply",
      "I was actually thinking of developing something similar to make it easier to copy my chess game review comments into my Obsidian notes.Thank you!\n \nreply",
      "You're welcome!By the way, something I've considered but not implemented yet are options to tweak the formatting (for instance, whether to indent or inline comments / variations). The Prettier approach is to be opinionated, but some language-specific options may be useful.\n \nreply",
      "What about using a fenced code block?\n \nreply",
      "You mean inside markdown? Prettier supported that since 2017 [1], it will call the individual plugins to handle content inside the code blocks.[1]: https://prettier.io/blog/2017/11/07/1.8.0.html\n \nreply",
      "PGN is just a format to communicate the games. So pgn pretty-printing is at the same level as pretty printing json. Nothing wrong with that, but at some point you might want to publish a game with diagrams variations, ....For that purpose, you can use Latex (yes, I know, not everybody's cup of tea)https://www.overleaf.com/learn/latex/Chess_notation\n \nreply",
      "This looks a bit cumbersome to write to me.I really like the JS preview/rendering plugins for fenced code blocks in Markdown. For example Mermaid, which is supported by many Markdown renderers or IDEs. [1]It would be fun to have something like this for chess notation, that interactively renders the last position with forward and back buttons.Static PDFs are mostly a thing of the past.[1]: https://github.blog/developer-skills/github/include-diagrams...\n \nreply",
      "Thanks for the idea, I'd not considered it. LaTex may have some use, I'll take a look. Indeed, probably the best solution to publish a game with diagrams.For personal notes, I would also consider embedding PGN into Markdown. Sadly, PGN rendering in Markdown isn't supported yet, as far as I know (something like Mermaid would be awesome).\n \nreply"
    ],
    "link": "https://github.com/gmasclet/prettier-plugin-pgn",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A Prettier plugin for formatting PGN files\n      Prettier is an opinionated code formatter. It enforces a consistent style by parsing your code and\nre-printing it with its own rules that take the maximum line length into account, wrapping code\nwhen necessary.This plugin adds support for the Portable Game Notation format to Prettier.Portable Game Notation (PGN) is a standard plain text format for recording chess games (both the\nmoves and related data), which can be read by humans and is also supported by most chess software.This sample game:will be transformed to this:To run prettier with the PGN plugin, you're going to need node.Install prettier and the plugin using the npm CLI:The plugin can be activated in your Prettier configuration file:Alternatively, it may be declared directly in the CLI, using the --plugin option:This plugin "
  }
]