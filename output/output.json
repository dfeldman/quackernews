[
  {
    "title": "macOS menu bar app that shows how full the ISS urine tank is in real time (github.com/jaennaet)",
    "points": 280,
    "submitter": "ajdude",
    "submit_time": "2024-12-24T22:38:40 1735079920",
    "num_comments": 86,
    "comments_url": "https://news.ycombinator.com/item?id=42505454",
    "comments": [
      "> I found out about the data stream from https://iss-mimic.github.io/Mimic/, which has considerably more and more interesting stats than just how full the piss tank is.> I will not be adding any of them.This, right here, is how you communicate non-goals of a project. Just perfect open-source communication best practices. We all stand to learn from this project.(Though, predictably, some of us sit to interact with it.)\n \nreply",
      "> https://iss-mimic.github.io/Mimic/Be sure to also read the project page:https://github.com/ISS-Mimic/Mimic\n \nreply",
      "Heh, I follow a Bluesky bot that posts HN stories that have gone over 50 points and unexpectedly saw a very familiar Github link. I'd made a Show HN story about this ~5 days ago (https://news.ycombinator.com/item?id=42464454) and I was like \"huh, how'd that suddenly get more traction\" but turns out it wasn't even my post!I'm so delighted that this is easily my most popular OSS project over the past 15 or so years (I have my \"serious\" stuff elsewhere), and I'm not being sarcastic here.I'll happily answer any questions folks have (expect some reply lag because holiday season). I figure the most popular question is probably going to be \"\u2026 but why?\" though, and the honest-to-the-gods answer is \"because I thought it was funny\"; I was trying to come up with a nice and simple 1st project to do with Swift (holy crap that language's concurrency story is confusing), and once I ran into iss-mimic I knew what I had to do.\n \nreply",
      "Have you considered making this a library? I think every Swift application needs this important metric on the about panel.\n \nreply",
      "Are you planning to add AI and monetize this?\n \nreply",
      "Absolutely! Realtime data will require a subscription, which will also include an LLM analysis of the past week's data. I think one of the VCs funding my upcoming disruptive space station piss tank telemetry platform requested that.I'm pretty sure I can also shove a blockchain in there somewhere too even though they're a bit pass\u00e9.\n \nreply",
      "More agile\n \nreply",
      "You don't really want an agile toilet interface. This is more a waterfall project.\n \nreply",
      "Make sure there's plenty of space to output those logs.\n \nreply",
      "Here's a web port of this: https://gistpreview.github.io/?76f03f49be58344bfa64c9d5d9f0e... (source code here: https://gist.github.com/simonw/76f03f49be58344bfa64c9d5d9f0e... )Created by pasting the entire Swift GitHub repo into Gemini 2.0 and asking it to port it to a web page: https://gist.github.com/simonw/b4aec4e879e50ac74f6f9cc6e1cdc...\n \nreply"
    ],
    "link": "https://github.com/Jaennaet/pISSStream",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        macOS menu bar app that shows how full the International Space Station's urine tank is in real time\n      pISSStream is a macOS menu bar app that shows how full the International Space Station's urine tank is in real time:Download yours while supplies last!Not the epitome of good coding practices since this was my first Swift & macOS app ever, may break in exciting ways at the slightest excuse.I found out about the data stream from https://iss-mimic.github.io/Mimic/, which has considerably more and more interesting stats than just how full the piss tank is.I will not be adding any of them.\n        macOS menu bar app that shows how full the International Space Station's urine tank is in real time\n      "
  },
  {
    "title": "Trying out QvQ \u2013 Qwen's new visual reasoning model (simonwillison.net)",
    "points": 54,
    "submitter": "simonw",
    "submit_time": "2024-12-24T21:25:32 1735075532",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=42505038",
    "comments": [
      "My default problem for testing these systems has been this word search puzzle where you get a list of words and a grid of letters, and you need to find each word in the grid. [0] I just tried it with QvQ and it failed; none of the answers were correct.Interestingly enough, I recently tried the new Gemini release in AI Studio and it also failed at the first pass. With a bit of coaxing I was ultimately able to get it to find one word successfully, and then a second word once it understood the correct strategy. After that attempt I asked it for a program to search the grid for each word, and although the initial attempt failed, it only took 4 iterations of bug fixes to get a fully working program. The two main bugs were: the grid had uppercase letters while the words were lowercase, and one of the words (soup mix) has a space which needs to be stripped when searching the grid.Asking QvQ to generate a program to solve this puzzle... The first try gave me a program that would only check if the word was in the grid or not, but not the actual position of the word. This was partially my fault for using a bad prompt. I updated the prompt to include printing the position of each word, but it just got caught up thinking about the problem. It somehow made a mistake in converting the grid and became convinced that some of the words aren't actually present in the grid. After thinking about the problem for a very long time it ended up giving me an invalid program, and I don't feel particularly motivated to try and debug where it went wrong.What I find most interesting is that asking for a direct solution tends to fail, but asking for a program which solves the problem gets us much closer to a correct answer. Ideally the LLM should be able to just figure out that writing a program is the optimal solution, and then it can run that and extract the answer.[0] https://imgur.com/F8dm8Zo\n \nreply",
      "This seems analogous to the beat to death \"strawberry\" test where you're essentially testing if the model is trained on tokens or not. When you switch to having it write a Python program (in both scenarios) instead of trying to process it directly you stop trying to work directly with individual letter inputs+outputs so it does better.\n \nreply",
      "I also tried feeding it the famous \"tank man\" photo and asking for a description and the response came back blank!https://en.m.wikipedia.org/wiki/Tank_Man\n \nreply",
      "This model is fun. Uploading images and asking it a user research style question like \"Please think aloud while viewing this image\" creates gems like this:\"Let me take another close look at the sandwich. The way the light reflects off the melted cheese highlights its creamy texture. The pasta pieces are well-integrated into the cheese, and the herbs are evenly distributed, adding color and likely flavor.I can almost taste it now. The combination of textures and flavors must be divine. The crispy bread, the melty cheese, the soft pasta, the crunch from the fried bits, and the freshness from the herbs\u2014all working together in harmony.I think I might have to go out and find this sandwich place soon. Or perhaps I'll try recreating it at home this weekend. Either way, this image has certainly whetted my appetite for a delicious grilled cheese sandwich.\"[0] https://imgur.com/a/0OtGxeB\n \nreply",
      "Are those herbs actually evenly distributed though?\n \nreply",
      "Are the Q* models open source with open data? Asking because other than the known tianamen limitation, prompting these models for any advice about acts against authority in a corporate setting yields strong recommendations of compliance. Haven\u2019t explored this deeply, but it was enough of a red flag to limit use to  coding/logic tasks.\n \nreply",
      "Model architecture and weights are open source.Training data is not (but I don't think anyone in this game has fully open training data)\n \nreply",
      "Nobody wants to be sued to oblivion.\n \nreply",
      "There are a couple, i.e., OLMo 2\n \nreply",
      "> So I\u2019ve got this picture of some pelicans, and I need to count themIt seems to start all responses in this style, but still hilarious. Seems very anti-GPT4 in how casual it sounds.\n \nreply"
    ],
    "link": "https://simonwillison.net/2024/Dec/24/qvq/",
    "first_paragraph": "24th December 2024I thought we were done for major model releases in 2024, but apparently not: Alibaba\u2019s Qwen team just dropped the Apache 2.0 licensed QvQ-72B-Preview, \u201can experimental research model focusing on enhancing visual reasoning capabilities\u201d.Their blog post is titled QvQ: To See the World with Wisdom\u2014similar flowery language to their QwQ announcement QwQ: Reflect Deeply on the Boundaries of the Unknown a few weeks ago in November.It\u2019s a vision-focused follow-up to QwQ, which I wrote about previousy. QwQ is an impressive openly licensed inference-scaling model: give it a prompt and it will think out loud over many tokens while trying to derive a good answer, similar to OpenAI\u2019s o1 and o3 models.The new QvQ adds vision to the mix. You can try it out on Hugging Face Spaces\u2014it accepts an image and a single prompt and then streams out a very long response where it thinks through the problem you have posed it. There\u2019s no option to send a follow-up prompt.I\u2019ve tried it out with a "
  },
  {
    "title": "Masks, Smoke, and Mirrors: The story of EgyptAir flight 804 (admiralcloudberg.medium.com)",
    "points": 95,
    "submitter": "gdmt",
    "submit_time": "2024-12-24T19:39:40 1735069180",
    "num_comments": 39,
    "comments_url": "https://news.ycombinator.com/item?id=42504343",
    "comments": [
      "Does anyone know the author's background? All I see is 'analyzer of plane crashes'.\n \nreply",
      "She is I believe a pilot, and has been doing this for years and years on (originally?) reddit, in great depth and detail. She also has a really fun podcast with two other people called controlled pod into terrain.\n \nreply",
      "A little information: https://www.patreon.com/admiral_cloudberg/about\n \nreply",
      "She's one of the best, extremely thorough, and works as a researcher for another very good air crash investigator, Mentour Pilot (YouTube channel).\n \nreply",
      "A rather well-written piece. My takeaway is that the French investigators are pros and the Egyptians are hacks. And that safety culture matters. One must not bend the facts to draw a desired conclusion. One must review the data without bias, or else recuse oneself.\n \nreply",
      "> My takeaway is that the French investigators are pros and the Egyptians are hacksDescribing them as \"hacks\" is weird. In most dictatorships, the concern is usually \"What does the country's leadership want the official story to be\" rather than \"What actually happened\". Take this quote from the article for example> \"In my opinion, the problem with the report is that it appears to treat the findings of the Triple Committee \u2014 the group appointed by the public prosecutor\u2019s office \u2014 as the unquestioned truth, and fails to push back on any of its assertions, even the ones that they disagreed with. Instead, because the Triple Committee concluded that a bomb in the galley was the cause of the crash, the EAAID bent itself into a pretzel trying to make the evidence fit that theory. Unfortunately, we don\u2019t know why the Triple Committee and the EAAID chose to die on this hill\"EgyptAir is a government owned enterprise. It's managed by the \"Ministry of Civil Aviation\" who's head is always some general or commander from the Air Force. If the EAAID investigators were allowed to say that there was a \"faulty equipment\" then a lot of questions would have had to be answered. A lot of questions that have the possibility of embarrassing people all the way up the chain (especially that as mentioned, that particular oxygen mask was reported faulty from another aircraft and removed for maintenance before, and the crew frequently reported that the pilot oxygen supply always decreases on every flight).Saying \"it was terrorists\" is something that no one has to feel embarrassed about. In fact in 2016 the Egyptian government were in the midst of a lot of arrests and suspension of most freedoms to \"curb terrorist activities\". And such thing plays well into that narrative.Are you an EAAID investigator who wants to say \"it was a faulty oxygen mask\"? Ok. How do you fancy you, your brother, cousin, and neighbor spending the next 15-30 years in jail pending investigation on conspiracy against the country?\n \nreply",
      "You make a valid point. I stand corrected. \"Hacks\" is not an accurate term, and fails to account for the full circumstance. I was merely appalled at how willing EAAID were to jump to conclusions and twist facts towards a convenient narrative. France is a democracy, and that makes for an unfair comparison between the two agencies. I am sure that even the most intellectually honest individual will choose their own safety if faced with the reality of imprisonment.\n \nreply",
      "It's not the first time Egyptian investigators disregarded reality to keep face:https://admiralcloudberg.medium.com/the-crash-of-egyptair-fl...\n \nreply",
      "In all tests, the fire, propelled by the oxygen leak, produced a terrifying \u201cblowtorch\u201d effect, and the flames were literally white-hot.Sufficient concentrations of oxygen can cause even steel to burn: https://en.wikipedia.org/wiki/Thermal_lanceThere must be a reason they use pure oxygen, as regular compressed air, also breathable, would not have the same intense reactivity.\n \nreply",
      "It's used due to the low partial pressure of oxygen at high altitudes\n \nreply"
    ],
    "link": "https://admiralcloudberg.medium.com/masks-smoke-and-mirrors-the-untold-story-of-egyptair-flight-804-42c788fcac2d",
    "first_paragraph": ""
  },
  {
    "title": "Shockley Semiconductor Laboratory (abortretry.fail)",
    "points": 40,
    "submitter": "BirAdam",
    "submit_time": "2024-12-24T20:29:09 1735072149",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=42504677",
    "comments": [
      "I still think that the junction transistor was an evolutionary step from the junction diode. Once you have a working diode, adding a third terminal to control the depletion zone seems like such a natural thing to do.\n \nreply",
      "I find it a sad thing that often great achievements and ruthlessness go hand in hand. \u201cHard times create strong men and strong men create good times\u201d is a famous quote. For example, it applies to Arnold Schwarzenegger since his father apparently mentally and physically abused Arnold[1], as well as for Elon Musk\u2019s father for whom it is well documented he is a piece of work. If you read a lot about founders then this is a recurring theme. Why else would you work like a maniac your whole life? You don\u2019t need a successful company to have a happy life.There have also been studies that people tend to leash out to people below them if they feel that someone above them leashes out on them. This \u201cdisplacement behavior\u201d is a form of stress relieve. Monkeys do it too, and also there it is a form of stress relieve. It sort of makes sense right? You feel shit so then the easy solution is just to tell someone else that he/she is shit. You feel a bit better about yourself and all is good. Apart from the poor person (or monkey) who receives the abuse.So on the one hand, we want great men who produce good times, who solve difficult problems (Elon) or who inspire people (Arnold). But on the other hand I wish no child to have a tough childhood like that. That\u2019s life I guess.One thing about Shockley. The blog mentions Shockley\u2019s funeral. What the blog doesn\u2019t mention is why there was no funeral. His wife thought nobody would come anyway so decided to not hold the ceremony. A sad ending. I wonder how his parents were and what would have become of him if things would have been a bit different. Maybe he would have been smart and emphatic?[1]: Although many people don\u2019t know this, Arnold can be very ruthless. Read about his early years and his relationships for more info.\n \nreply",
      "In case people are not aware, Beckman had invented the pH meter a few years before Hewlett & Packard came up with their audio test equipment.https://americanhistory.si.edu/collections/object/nmah_1503Just like the one jn my collection :)\n \nreply",
      "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_pr...Look at the picture, you can notice who's shockley by just observing their facial expression. The cold-and-smart-than-you shockley in the middle. Shockley's personality becomes his mortal enemy, but that personality is also ncessary for his achievement.If strength cannot become your weakness, than it's just mediocrity.\n \nreply",
      "I think it\u2019s a balance. You do need to be an asshole sometimes to get things done. But you don\u2019t need to be an asshole all the time. It\u2019s an easy excuse to say you need to be a prick because otherwise you cannot do your great achievements. It\u2019s a false dichotomy. Even Einstein had friends.\n \nreply",
      "I don't think OP was saying that, they were saying that anybody who's truly different enough to be \"great\" is also going to have downsides to those strengths.It certainly does sound like shockley was way more of an asshole than necessary.\n \nreply",
      "While the article is reasonably complete, it is unbalanced, because it insists much more on the unpleasant personality of William Shockley than on his decisive contributions to the development of the semiconductor industry.The conclusion of the article is correct that besides his own contributions to the theory of semiconductor devices his second great, even if unintended, achievement was hiring truly the best people, but then annoying them enough so that they have left and founded Fairchild Semiconductor, from where some of them have gone later to create many other of the most important companies of Silicon Valley.However Shockley has contributed with much more than the invention of the bipolar junction transistor. His invention of the Shockley diode was not very important, because the PNPN switch had already been invented at Bell Labs 3 years earlier, in 1952, by Jewell J. Ebers. The innovation of Shockley was only to notice that a PNPN switch does not need control electrodes to be switched on, because it can be switched on by either the output voltage or by light, therefore it can be made as a diode instead of a tetrode, like previously.More important has been that Shockley has invented not only the bipolar junction transistor (BJT) in 1948, but also the junction field-effect transistor (JFET) in 1951. For many decades these 2 have been the most important semiconductor devices, until the technology has progressed sufficiently to allow the control of the oxide-silicon interface well enough to allow the fabrication of MOS transistors (which had been invented well before WWII, but nobody knew how to make them).Even more important is that he has published detailed theories about how the BJTs and the JFETs function, which allowed anyone to design such semiconductor devices.These theories of the semiconductor transistors conceived by Shockley have been what has really started the semiconductor industry. The previous discovery of the point-contact transistor by Bardeen and Brattain has been more or less accidental and they did not understand how it works. Without the theory of Shockley none of the quick progresses from the years following 1948 would have existed.\n \nreply",
      "It does note his achievements, but had the man had any charisma at all, Silicon Valley might not have been. That the man was repellent is pivotal to the founding of Fairchild and thus all of the fairchildren.\n \nreply",
      "No, the man's personality is the engine of his achievement.It's just that ordinary human do not really rewards genius.For the man being different, he probably can be Gorden Moore, who just noticed a pattern, and then become monumental figure.Populous rewards popularity, not genius.\nAnd being ignored by them is not a failure, it's a necessity of greatness.\n \nreply",
      "I wonder if he held the same utility function for the entirety of his life, because the beginning and end of his life is quite dismal. Not one bit of reflection in his life seemed to change his tone.\n \nreply"
    ],
    "link": "https://www.abortretry.fail/p/shockley-semiconductor-laboratory",
    "first_paragraph": ""
  },
  {
    "title": "Adversarial policies beat superhuman Go AIs (2023) (arxiv.org)",
    "points": 237,
    "submitter": "amichail",
    "submit_time": "2024-12-23T13:10:15 1734959415",
    "num_comments": 97,
    "comments_url": "https://news.ycombinator.com/item?id=42494127",
    "comments": [
      "NOTE: this is a july 2023 paper, the defense paper in september 2024 is https://arxiv.org/abs/2406.12843\n \nreply",
      "> We find that though some of these defenses protect against previously discovered attacks, none withstand freshly trained adversaries.\n \nreply",
      "This seems amazing at first sight. It's probably just me, but I find the paper to be very hard to understand even though I know a little bit about Go and Go AI and a lot about chess and chess AI. They seem to expend the absolute minimum amount of effort on describing what they did and how it can possibly work, unnecessarily using unexplained jargon to more or less mask the underlying message. I can almost see through the veil they've surrounded their (remarkable and quite simple?) ideas with, but not quite.\n \nreply",
      "https://slideslive.com/39006680/adversarial-policies-beat-su...Seems to be a good intro.Go uniquely has long periods of dead-man walking, as I like to call it. Your group might be dead on turn 30, but your opponent won't formally kill the group until turn 150 or later.If your opponent knows the truth all the way back in turn30, while you are led down the wrong path for those hundreds of turns, you will almost certainly lose.This adversarial AI tricks AlphaGo/KataGo into such situations. And instead of capitalizing on it, they focus on the trickery knowing that KataGo reliably fails to understand the situation (aka it's better to make a suboptimal play to keep KataGo tricked / glitched, rather than play an optimal move that may reveal to KataGo the failure of understanding).Even with adversarial training (IE: KataGo training on this flaw), the flaw remains and it's not clear why.------It appears that this glitch (the cyclical group) is easy enough for an amateur player to understand (I'm ranked around 10kyu, which is estimated to be the same level of effort as 1500Elo chess. Reasonably practiced but nothing special).So it seems like I as a human (even at 10kyu) could defeat AlphaGo/KataGo with a bit of practice.\n \nreply",
      "Thank you. So the attack somehow sets up a situation where AlphaGo/KataGo is the dead man walking? It doesn't realise at move 30 it has a group that is dead, and continues not to realise that until (close to the time that?) the group is formally surrounded at move 150?I still don't really understand, because this makes it sound as if AlphaGo/KataGo is just not very good at Go!\n \nreply",
      "To be clear, this is an adversarial neural network that automatically looks for these positions.So we aren't talking about 'one' Deadman walking position, but multiple ones that this research group searches for, categorizes and studies to see if AlphaGo / KataGo can learn / defend against them with more training.I'd argue that Go is specifically a game where the absurdly long turn counts and long-term thinking allows for these situations to ever come up in the first place. It's why the game is and always fascinated players.-------Or in other words: if you know that a superhuman AI has a flaw in its endgame calculation, then play in a deeply 'dead man walking' manner, tricking the AI into thinking it's winning when in truth its losing for hundreds of moves.MCTS is strong because it plays out reasonable games and foresees and estimates endgame positions. If the neural nets oracle is just plain wrong in some positions, it leads to incredible vulnerabilities.\n \nreply",
      "I think I'm starting to see after reading these replies and some of the linked material. Basically the things that confused me most about the rules of go when I first looked at it are playing a role in creating the attack surface: How do we decide to stop the game? How do we judge whether this (not completely surrounded) stone is dead? Why don't we play it out? Etc.\n \nreply",
      "Most rulesets allow you to \"play it out\" without losing points. Humans don't do it because it's boring and potentially insulting or obnoxious.Judging whether something \"is dead\" emerges from a combination of basic principles and skill at the game. Formally, we can distinguish concepts of unconditionally alive or \"pass-alive\" (cannot be captured by any legal sequence of moves) and unconditionally dead (cannot be made unconditionally alive by any sequence of moves), in the sense of Benson's algorithm (https://en.wikipedia.org/wiki/Benson%27s_algorithm_(Go) , not the only one with that name apparently). But players are more generally concerned with \"cannot be captured in alternating play\" (i.e., if the opponent starts first, it's always possible to reach a pass-alive state; ideally the player has read out how to do so) and \"cannot be defended in alternating play\" (i.e., not in the previous state, and cannot be made so with any single move).Most commonly, an \"alive\" string of stones either already has two separate \"eyes\" or can be shown to reach such a configuration inevitably. (Eyes are surrounded points such that neither is a legal move for the opponent; supposing that playing on either fails to capture the string or any other string - then it is impossible to capture the string, because stones are played one at a time, and capturing the string would require covering both spaces at once.)In rarer cases, a \"seki\" (English transliteration of Japanese - also see https://senseis.xmp.net/?Seki) arises, where both player's strings are kept alive by each others' weakness: any attempt by either player to capture results in losing a capturing race (because the empty spaces next to the strings are shared, such that covering the opponent's \"liberty\" also takes one from your own string). I say \"arises\", but typically the seki position is forced (as the least bad option for the opponent) by one player, in a part of the board where the opponent has an advantage and living by forming two eyes would be impossible.Even rarer forms of life may be possible depending on the ruleset, as well as global situations that prevent one from reducing the position to a sum of scores of groups. For example, if there is no superko restriction, a \"triple ko\" (https://senseis.xmp.net/?TripleKo) can emerge - three separate ko (https://senseis.xmp.net/?Ko) positions, such that every move must capture in the \"next\" ko in a cycle or else lose the game immediately.It gets much more complex than that (https://senseis.xmp.net/?GoRulesBestiary), although also much rarer. Many positions that challenge rulesets are completely implausible in real play and basically require cooperation between the players to achieve.\n \nreply",
      "Aji is the concept of essentially making lemonaid from lemons by using the existence of the dead stones to put pressure on the surrounding pieces and claw back some of your losses.Because they haven\u2019t been captured yet they reduce the safety (liberties) of nearby stones. And until those are fully settled an incorrect move could rescue them, and the effort put into preventing that may cost points in the defense.\n \nreply",
      "This is not a reasonable summary. The adversarial AI is not finding some weird position that relies on KataGo not understanding the status. It's relying, supposedly, on KataGo not understanding the ruleset which uses area scoring and doesn't include removing dead stones (because in area scoring you can always play it out without losing points, so this is a simple way to avoid disputes between computers, which don't get bored of it).I assume that KataGo still has this \"flaw\" after adversarial training simply because it doesn't overcome the training it has in environments where taking dead stones off the board (or denying them space to make two eyes if you passed every move) isn't expected.See https://boardgames.stackexchange.com/questions/58127 which includes an image of a position the adversarial AI supposedly \"won\" which even at your level should appear utterly laughable. (Sorry, I don't mean to condescend - I am only somewhere around 1dan myself.)(ELO is sometimes used in Go ranking, but I don't think it can fairly be compared to chess ranking nor used as a metric for \"level of effort\".)\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2211.00241",
    "first_paragraph": "The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy.Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "The number pi has an evil twin (mathstodon.xyz)",
    "points": 433,
    "submitter": "pkaeding",
    "submit_time": "2024-12-24T03:41:22 1735011682",
    "num_comments": 179,
    "comments_url": "https://news.ycombinator.com/item?id=42499567",
    "comments": [
      "> \u201d This \u221e-shaped curve is called a 'leminscate', and \u03d6 is called the 'lemniscate constant'.   I'll show you the leminiscate in my next post.\u201dThis got me confused, so I went to check. Apparently \u201dlemniscate\u201d is the correct spelling.\n \nreply",
      "Fixed - thanks.\n \nreply",
      "Hey, John \u2014 Matt Parker mentioned in one of his ellipse videos the fact that every elliptical ratio has its own pi-like constant. He just quickly rattles the fact off, but never delves into it. Do you know of any research into trying to characterize the family of pi? I mean, beyond its evil cousins.\n \nreply",
      "This discussion helped me discover my new favorite map. https://en.wikipedia.org/wiki/File:Peirce_Quincuncial_Projec...\n \nreply",
      "oh wow that's a lot like a maximally extended penrose diagram\n \nreply",
      "And to protect you from it, you can use the following lucky clover charm (polar plot r=cos(2theta) ): \nhttps://www.wolframalpha.com/input?i=+plot+r%3Dcos%282theta%... whose perimeter can also define a constant 4*E(-3) ~ 4 * 2.4221https://www.wolframalpha.com/input?i=plot+r%3Dcos%282theta%2...\n \nreply",
      "\u03c0 is derived from the circle, which is defined by distance from a single point.\u03d6 is derived from the lemniscate of Bernoulli, which is defined by distances from two points.Is there an analogous constant that is derived from a shape defined by distances from three points?\n \nreply",
      "Yes, definitely. Pi is just the perimeter of the circle, and varpi is the perimeter of the lemniscate. If you use three points, you get three tear-drops, and you can compute the perimeter of that.Let\u2019s call it a trilemniscate. ;)Here\u2019s a 3d plot of it. If you rotate to view it from +Z downward, then you\u2019ll see the trilemniscate, which is where the volume intersects with the XY plane. Note I subtracted 1 from the product in order to visualize the plane intersection. (And you can turn off the 3 points version and turn on the 2 points version to compare.)https://www.desmos.com/3d/dl9v2vqbqbOne interesting note about 2 points vs 3 points. The area inside the lemniscate and trilemniscate is the same! (True for more points, as long as they\u2019re evenly space on a circle). The perimeter, of course, goes to infinity as you add more points.\n \nreply",
      "I mean the concept of distance from 3 points introduces a mess of metrics or even measure theory.2 points always have a shortest path between each other, so the constant is about this fact. For 3 points you have the whole universe of possible triangle shapes to contend with.\n \nreply",
      "Shortest path between two points still depends on your metric.For instance, if you're constrained to travel along the surface of Earth, your shortest path is going to travel along a great circle, rather than pass through the interior of the sphere.That said, you could, for instance, pick the three vertices of an equilateral triangle (using the Euclidean distance as your metric of choice, as we do in order to derive the lemniscate and the circle), and again deal with the product of the distances from each vertex.You again start with small circles around each vertex, which eventually expand to a single looping curve, and then into ovals encircling the entire triangle.https://en.wikipedia.org/wiki/Cassini_oval#Generalizationshttps://en.wikipedia.org/wiki/Polynomial_lemniscate#Erd%C5%9...\n \nreply"
    ],
    "link": "https://mathstodon.xyz/@johncarlosbaez/113703444230936435",
    "first_paragraph": ""
  },
  {
    "title": "Cerebrum: Simulate and infer synaptic connectivity in large-scale brain networks (svbrain.xyz)",
    "points": 69,
    "submitter": "notallm",
    "submit_time": "2024-12-24T18:14:19 1735064059",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=42503696",
    "comments": [
      "Biologically inspired neuron models like Hodgkin\u2013Huxley are about as far from an emulation of real neuron behavior as a paper airplane is from the space shuttle. We can learn things from using them and they're an important stepping stone, but they aren't really that useful.That being said, I hope the founder keeps it up \u2014 it's great to have more bright, driven people in this field.\n \nreply",
      "Based on my very limited knowledge of how current \"AI\" systems work, this is the much better approach to achieving true AI. We've only modeled one small aspect of the human (the neuron) and brute forced it to work. It takes an LLM millions of examples to learn what a human can in a couple of minutes--then how are we even \"close\" to achieving AGI?Should we not mimic our biology as closely as possible rather than trying to model how we __think__ it works (i.e. chain of thought, etc.). This is how neural networks got started, right? Recreate something nature has taken millions of years developing and see what happens. This stuff is so interesting.\n \nreply",
      "> Should we not mimic our biology as closely as possible rather than trying to model how we __think__ it works (i.e. chain of thought, etc.).Should we not mimic migrating birds\u2019 biology as closely as possible instead of trying to engineer airplanes for transatlantic flight that are only very loosely inspired in the animals that actually fly?\n \nreply",
      "There\u2019s currently an enormous gulf in between modeling biology and AGI, to the point where it\u2019s not even clear exactly where one should start. Lots of things should indeed be tried, but it\u2019s not obvious what could lead to impact right now.\n \nreply",
      "Because it works. The Vikings embodied a mindset of skaldic pragmatism: doing things because they worked, without needing to understand or optimize them.Our bodies are Vikings. Our minds still want to know why.\n \nreply",
      "I'm pretty sure the Vikings understood their craft very well. You don't become a maritime power that pillages all of Europe and reaches the New World long before Columbus without understanding how things work.\n \nreply",
      "> It takes an LLM millions of examples to learn what a human can in a couple of minutesLLMs learn more than humans learn in a lifetime in under 2 years. I don't know why people keep repeating this \"couple of minutes\". Humans win on neither the data volume to learn something nor the time.How much time do you need to learn lyrics of a song? How much time do you think a LLaMA 3.1 8B on a 2x3090 need? What if you need to remember it tomorrow?\n \nreply",
      "They mean learning concepts, not rote factual information. I also hate this misanthropic \u201cLLMs know more than average humans\u201d falsehood. What it actually means \u201cLLMs know more general purpose trivial than average humans\u201d because average humans are busy learning things like what their boss is like, how their kids are doing in school, how precisely their car handles, etc.\n \nreply",
      "Do you think the information in \"what your boss is like\" and \"how your kids do in school\" larger than amount of data you'd need to learn in order to give descent law advice on a spot?Car handling is a bit harder to measure, precisely because LLMs aren't running cars quite yet, but also I am not aware of any experimental data saying they can't. So as far as I'm concerned nobody just tried that with  LLMs of >70GB.\n \nreply",
      "> How much time do you need to learn lyrics of a song? How much time do you think a LLaMA 3.1 8B on a 2x3090 need?Probably not the best example. How long does it take to input song lyrics into a file to have an operating system \"learn\" it?\n \nreply"
    ],
    "link": "https://svbrain.xyz/2024/12/20/cerebrum",
    "first_paragraph": ""
  },
  {
    "title": "Saving Nanocap Speculators from Themselves (nyuu.page)",
    "points": 32,
    "submitter": "apsec112",
    "submit_time": "2024-12-21T01:10:34 1734743434",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=42476700",
    "comments": [
      "This is my post!The case studies used here are the smallest of small fries and the implementations are very simple. but similarly simple implementations have been used in more exciting and destructive cases. like the \"squid game\" coin. the squid contract actually had almost no functionality in it. Squid code said, roughly: \"I am a coin, you can trade me. For all other functionality, I do whatever [some other address] tells me to do\". Then, when you look at that other address, its code there is unverified, & when decompiled reveals fee-setter shenanigans and stuff of that nature.\n \nreply",
      "Did any of the smart contract stuff turn out to be legit?  Any of it at all?\n \nreply",
      "I have yet to see a smart contract worth the paper it\u2019s printed on. ;)\n \nreply",
      "As far as I'm aware, the only quasi-legit services in the space are DEXs - smart contracts whose purpose is to allow users to trade tokens for other tokens. They aren't outright scams in their own right (at least, most of the big ones aren't), but they also facilitate a lot of the scams described in the article.\n \nreply",
      "stablecoins, prediction markets, decentralized exchanges and decentralized lending markets have worked pretty well\n \nreply",
      "For whom?\n \nreply",
      "The rug pullers, of course.\n \nreply"
    ],
    "link": "https://nyuu.page/essays/solidity/",
    "first_paragraph": "I spent the summer of the 2021 cryptocurrency bull market fascinated by a hive of foolishness and deception: /r/cryptomoonshots (and, to a lesser extent, its analogues on discord and telegram). People\u2019d post a token address (typically Binance Smart Chain (\u201cBSC\u201d), occasionally Ethereum (\u201cEth\u201d)) along with a pitch or maybe a website and an inducement: \u201cbuy right now!\u201d. The (implicit) conceit was \u201cyou, the reader, may have \u2018missed bitcoin\u2019, and \u2018missed dogecoin\u2019, but you can still \u2018100x\u2019\u2026 by buying my token!\u201d.All of the posts were scams, in the sense that there was no serious intention of delivering on what was promised. Within this there was variation in how scammy: timeframe, level of sophistication, and mechanics used to stack the deck against buyers.The ecosystem of that era deserves a full ethnography and I\u2019ll give it one eventually. For now I\u2019ll leave it at this: what made these scams effective was the cultural context of 2021. Many of the people buying this stuff were interacting w"
  },
  {
    "title": "What would it take to add refinement types to Rust? (yoric.github.io)",
    "points": 78,
    "submitter": "Yoric",
    "submit_time": "2024-12-23T09:15:27 1734945327",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=42493027",
    "comments": [
      "Units might seem simple but they have a ton of edge cases. Do you want to be able to add inches and feet? Be careful about potential precision/rounding issues. What is the unit for a temperature delta? You can\u2019t simply keep the original unit (eg C or F) because conversion from F to C is a different rule than \u0394F to \u0394C. Etc.Units do prevent bugs in programs, so they have an important role to play. But they also need to be designed very carefully.Java adopted units via JSR 385 (https://belief-driven-design.com/java-measurement-jsr-385-21...)\n \nreply",
      "> Do you want to be able to add inches and feet?This probably doesn't have to be too complicated; the usual answer for Rust is \"no\". Rust doesn't even let you \"just\" add two unsigned integers of different sizes. Following that design, I would imagine units would require an explicit \"turn feet into inches\" or the other way around.\n \nreply",
      "That's where Scala shines. I wrote about this here a bit: https://valentin.willscher.de/posts/contextual-syntax/Rust is heavily inspired by Scala, but I guess achieving something like the examples in my post is difficult. I really hope Rust finds one way or another to make it work. Because simply forbidding everything all the time isn't even the safest way - it drives many people to just avoid it altogether and use unsafe code.\n \nreply",
      "Might be annoying if you\u2019re working with floating point but given that each conversion introduces error, it\u2019s probably good to be explicit and recommend internally to be consistent\n \nreply",
      "Thanks to generics you could theoretically work with whatever underlying type makes the most sense for your use-case.    let v1 : Inches<Ratio> = Ratio::new(5, 8).into();\n    let v2 : Inches<Ratio> = Ratio::new(3, 8).into();\n\n    let v3 : Feet<_> = (v1 + v2).into();\n\n    assert_equal!(v3, Ratio::new(1, 12));\n \nreply",
      "I find the idea of being able to specify numerical types with arbitrary ranges very appealing.In Pascal these are called range types, e.g.month: 1..12would define an integer where the type system would ensure that it is always between 1 and 12.Apart from Ada this seems to be an alien concept to all other languages. The concept of a \"range type\" also seems to have other meanings.What is the Pascal \"range type\" properly called in type theory and what is its relationship to refinement types?\n \nreply",
      "Nim calls these subranges https://nim-lang.org/docs/manual.html#types-subrange-types  Subrange = range[0..5]\n  PositiveFloat = range[0.0..Inf]\n \nreply",
      "Anybody correct me if I'm wrong -- in Pascal these subranges can specify the size of the variable in memory and check compile-type assignment to a literal value, but don't (can't) do much else.There's the concept of dependent types in e.g. Idris which lets us correctly do this sort of range check throughout the program (not just on literal assignment) but it comes with strict requirements on what the compiler can allow, such as no unbounded recursion, because checking dependent types is roughly equivalent to running the program.\n \nreply",
      "I frequently wish I had a natural number type. So many programs would benefit from the type system guaranteeing that numbers are never negative.\n \nreply",
      "Rust has this one - though the ergonomics are a little awkward. Its called NonZero:https://doc.rust-lang.org/std/num/type.NonZeroU32.html\n \nreply"
    ],
    "link": "https://yoric.github.io/post/rust-refinement-types/",
    "first_paragraph": "A few years ago, on a whim, I wrote YAIOUOM. YAOIOUM was a static analyzer for Rust that checked that the code\nwas using units of measures correctly, e.g. a distance in meters is not a distance in centimeters, dividing meters\nby seconds gave you a value in m / s (aka m * s^-1).YAIOUOM was an example of a refinement type system, i.e. a type system that does its work after another type\nsystem has already done its work. It was purely static, users could add new units in about\none line of code, and it was actually surprisingly easy to write. It also couldn\u2019t be written within the Rust\ntype system, in part because I wanted legible error messages, and in part because Rust doesn\u2019t offer a very\ngood way to specify that (m / s) * s is actually the same type as m.Sadly, it also worked only on a specific version of Rust Nightly, and the code broke down with every new version\nof Rust. It\u2019s a shame, because I believe that there\u2019s lots we could do with refinement types. Simple things such\nas units o"
  },
  {
    "title": "What happened to the world's largest tube TV? [video] (youtube.com)",
    "points": 610,
    "submitter": "ecliptik",
    "submit_time": "2024-12-23T19:49:34 1734983374",
    "num_comments": 269,
    "comments_url": "https://news.ycombinator.com/item?id=42497093",
    "comments": [
      "It's a well done storytelling, but two odd thoughts/questions about it...As I was watching it, there was the drama of whether it would be saved from imminent destruction, and it actually seemed unlikely that they could, but their approach was to be... secretive about it.It turned out that they wanted it for themselves, and didn't that create a conflict of interest?  By keeping it quiet, they increased the chance that they would obtain it themselves (and the YouTube story to tell about it), but increased the likelihood that the TV would be lost entirely (because other efforts wouldn't be brought)?Fortunately the gamble worked out, and the TV wasn't destroyed.There's also a possibly related matter, in how Sony stopped talking with them.  Is it possible that Sony and/or Japanese government aren't very happy to learn that a possibly unique museum piece, of one of the heights of Sony achievement, was quietly removed from the country, to the US, by a YouTube influencer?I applaud preserving this rare artifact, and compliment the storytelling, but did have these couple odd thoughts.\n \nreply",
      "From the interview with the TV's original owner, this seemed like his ideal outcome.The owner had seen discussions of the TV online and knew it was a big deal. But he still couldn't get rid of it until this guy came along.The owner even said he wanted the TV to go to someone who would use, appreciate, and take care of it. The video clearly demonstrates all of the above. If the TV ended up in some museum, forever powered off, that would be even more tragic in some ways.I didn't get the impression that anyone was bamboozled or cheated.\n \nreply",
      "> But he still couldn't get rid of it until this guy came alongYep. There are always droves of \"it belongs in a museum\" crowds, but when you ask if they want it there is only silence.\n \nreply",
      "The sad reality is that there are countless more things in the world that belong in museums than there is museum space/staff to properly take care of it.\n \nreply",
      "Or money. Note the Living Computer Museum basically collapsing after Paul Allen's death.\n \nreply",
      "This was, sadly, a conscious choice made by Allen long before his death. Same as with his airplane and tank collection. He had plenty of time and legal advice to set it up with an endowment that could allow for its continued yearly operational budget and chose not to do so. His heirs don't care about his personal toy collection so it's been sold off.\n \nreply",
      "The same thing basically happened with Malcolm Forbes' collections. It's perfectly normal for heirs to just not value things you've collected in the same way you did.\n \nreply",
      "Which is why if you actually care you create an independent and well funded organization before you die so your heirs can't sell it all off.\n \nreply",
      "Would that really be better than letting your family sell it to the highest bidder? The only real concern I see if its value falls below the metal it contains or the mover breaks it. If a family cannot sell and does not value it then whats the point of keeping it?\n \nreply",
      "If one believes that collection should be for the benefit of the public, proper organization would remove it from the estate that goes to the heirs.It wouldn't matter if the heirs value it or not, because it wouldn't have been theirs. Because he let it remain in his estate, he left it to his heirs to decide what to do with it, and clearly, they did not care to keep it as a public collection, nor to endeavor to keep it together as a collection. I guess I should have visited it when I had the chance.\n \nreply"
    ],
    "link": "https://www.youtube.com/watch?v=JfZxOuc9Qwk",
    "first_paragraph": ""
  },
  {
    "title": "Gordon Mah Ung has died (pcworld.com)",
    "points": 25,
    "submitter": "EA-3167",
    "submit_time": "2024-12-24T22:55:14 1735080914",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=42505542",
    "comments": [
      "Learned about this last night and needless to say it was a gut punch. There is a whole generation of us that have Gordon to thank for our monthly pilgrimage to the magazine section looking for the latest edition of Maximum PC. Once a year we were treated to a buffet of hardware with amazingly high specs and prices to match with these Dream Machines later serving as mental checkpoints of the eras fastest personal compute. The. There was the effort that Gordon put into consumer advocacy in his columns, bad parts, badder corporate practices, and he was there fighting for us on the printed page.Rest in peace Gordon.\n \nreply",
      "This is unfortunate.Was he the one who took over from the hardware guy who died during the Boot magazine era and then went on to Maximum PC?\n \nreply",
      "That's right.> Gordon studied journalism at San Francisco State University and then worked as a police reporter for the Contra Costa Times in the late 1990s. In 1997, he joined Computerworld (a PCWorld sister publication) before I recruited him to join boot magazine (later re-launched as Maximum PC), where he would ultimately lead hardware coverage for 16 years.\n \nreply"
    ],
    "link": "https://www.pcworld.com/article/2564783/gordon-mah-ung-remembered.html",
    "first_paragraph": "When you purchase through links in our articles, we may earn a small commission. This doesn't affect our editorial independence.PCWorld executive editor Gordon Mah Ung, a tireless journalist we once described as a founding father of hardcore tech journalism, passed away over the weekend after a hard-fought battle with pancreatic cancer. Gordon was 58, and leaves behind a loving wife, two children, older sister, and mother.With more than 25 years\u2019 experience covering computer tech broadly and computer chips specifically, Gordon\u2019s dogged reporting, one-of-a-kind personality, and commitment to journalistic standards touched many, many lives. He will be profoundly missed by co-workers, industry sources, and the PC enthusiasts who read his words and followed him as a video creator.Gordon studied journalism at San Francisco State University and then worked as a police reporter for the Contra Costa Times in the late 1990s. In 1997, he joined Computerworld (a PCWorld sister publication) before"
  },
  {
    "title": "A Practitioner's Guide to Wide Events (jeremymorrell.dev)",
    "points": 24,
    "submitter": "dmazin",
    "submit_time": "2024-12-23T20:29:29 1734985769",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=42497338",
    "comments": [
      "I\u2019m quite looking forward to a future where we\u2019ve finally accepted that all this stuff is just part of the domain and shouldn\u2019t be treated like an ugly stepchild, and we\u2019ve merged OLTP and OLAP with great performance for both, and the wolf also shall dwell with the lamb, and we\u2019ll all get lots of work done.\n \nreply",
      "Practitioner of what? What is a \"wide event\"? In what context is this concept relevant? It took several sentences before I was even confident that this is something to do with programming.\n \nreply",
      "I felt like I got the gist after the first two:> Adopting Wide Event-style instrumentation has been one of the highest-leverage changes I\u2019ve made in my engineering career. The feedback loop on all my changes tightened and debugging systems became so much easier.\n \nreply",
      "That doesn\u2019t really give an objective definition of what wide events are, just an opinion and example in this one persons life.I had to lookup wide events in the middle of the article, and I can\u2019t say I can viscerally see and feel the benefits the OP was espousing. Just felt like an adderall-fueled dump of information being thrown at me.\n \nreply",
      ">I felt like I got the gist after the first two:What I get is: here's a thing that made a big improvement to how I debug systems.Except, it turns out that the systems in question are very specific ones.> The tl;dr is that for each unit-of-work in your system (usually, but not always an HTTP request / response) you emit one \u201cevent\u201d with all of the information you can collect about that work.Okay, but... as opposed to what? And why is it better this way?>\u201cEvent\u201d is an over-loaded term in telemetry so replace that with \u201clog line\u201d or \u201cspan\u201d if you like. They are all effectively the same thing.In the programming I do, \"event\" doesn't mean anything to do with logging or telemetry.\n \nreply",
      "They link to three separate articles right at the start that cover all of this. Not every article needs to start from first principles. You wouldn't expect an article about a new Postgres version to start with what databases are and why someone would need them.\n \nreply",
      ">Not every article needs to start from first principles.Sure, but it would be nice if title submissions made it feasible to predict the topic category of the article for people who are not already in the relevant niche.\n \nreply",
      "Wide events are a very well known approach, especially if you do any work with observability, and articles about it have been on the HN front page too. You not knowing about something does not automatically make it a narrow niche.\n \nreply",
      "From my point of view, \"it has something to do with web dev\" already makes it a niche. And as a rule of thumb, if you're using letter-number-letter abbreviations like \"o11y\" and assuming everyone knows what you're talking about, you're in a niche. (E.g.: I could parse \"i18n\" and \"l10n\" already, but I wouldn't expect random HN readers to. When I first saw \"k8s\" and looked it up I thought \"man, really?\".)\n \nreply",
      "It\u2019s about observability and strongly related to Honeycombs o11y 2.0 vision.\n \nreply"
    ],
    "link": "https://jeremymorrell.dev/blog/a-practitioners-guide-to-wide-events/",
    "first_paragraph": ""
  },
  {
    "title": "Ask HN: Why isn't Alex Krizhevsky as famous as Ilya Sutskever or G.Hinton?",
    "points": 26,
    "submitter": "alexcos",
    "submit_time": "2024-12-24T22:51:23 1735080683",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=42505519",
    "comments": [
      "He is working on CNN (Convolution Neural Network) while everyone else uses transformers. Furthermore, he wasn't in the OpenAI scandal with Ilya. Third, they don't publish papers at the same rate.\nIlya has 585182 citations, while Alex has 286082 (source google scholar).\n \nreply",
      "He likes to be away from the lime light. Here is the last interview I found: https://youtu.be/gwzwkv2hO5k\n \nreply",
      "Doesn't make enough drama.\n \nreply",
      "The magic of AlexNet has little do with Nvidia. It\u2019s a more fundamental breakthrough than just the compute hardware used. Further, it\u2019s not related to Nvidia either. Instead, it showed the promise of GPGPU. It just so happens that no one at the time and arguably still has a particularly competitive GPGPU offering other than Nvidia.\n \nreply",
      "Well I haven't heard of the other two either if that counts\n \nreply",
      "One of the other two won a Nobel prize this year, so has some broader publicity.\n \nreply",
      "Because those two actively self promote.\n \nreply",
      "Seems like he might be retired.\n \nreply"
    ],
    "link": "item?id=42505519",
    "first_paragraph": ""
  },
  {
    "title": "Brazilian velvet ant is ultrablack (nytimes.com)",
    "points": 7,
    "submitter": "bookofjoe",
    "submit_time": "2024-12-25T00:14:52 1735085692",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=42505937",
    "comments": [
      "Also called fuligin\nhttps://en.m.wiktionary.org/wiki/fuligin\n \nreply",
      "The number of variations produced by evolution is truly astonishing. Dizzying to think of every species of the past 500 million years that we will never know.\n \nreply",
      "https://archive.ph/0VqgN\n \nreply"
    ],
    "link": "https://www.nytimes.com/2024/12/17/science/ultrablack-velvet-ant-brazil.html",
    "first_paragraph": ""
  },
  {
    "title": "Hoarder: Self-hostable bookmark-everything app (github.com/hoarder-app)",
    "points": 248,
    "submitter": "thunderbong",
    "submit_time": "2024-12-22T11:34:17 1734867257",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=42485746",
    "comments": [
      "I tried hoarder and I didn't like the way listed view works. I prefer the simplicity of the view provided by Linkding. I find hoarder new auto tagging with ollama something I want to use because I am lazy.For references there are many options in selfhosted bookmarking apps market. These beside Hoarder are the most known software.Linkwarden (https://github.com/linkwarden/linkwarden)Shaarli (https://github.com/shaarli/Shaarli)LinkAce (https://www.linkace.org/)Linkding (https://github.com/sissbruecker/linkding)Wallabag (https://wallabag.org/)Shiori (https://github.com/go-shiori/shiori)\n \nreply",
      "One thing that I've been looking for in these, and I seem to recall few have this:Public mode? I'd like people to NOT have to log in.\n \nreply",
      "When I use a tool like this one of the most important things is that it works offline so I can read something in a plane or on the go.I've looked into most of these (and instapaper, pocket, etc) and ultimately found Wallabag to be the best. However, their app is quite buggy and site is fairly clunky for my taste. Luckily there's a pretty recent 3rd party client that works offline super well and is on Mac/Linux/android/iOS for free (yay flutter) https://github.com/casimir/frigoligoAlso, I'll note that it's basically a must to use the browser extension with the option to download via what the browser sees if you get content from a lot of sites. That being said the devs are super responsive to reports that sites aren't being scraped appropriately.My biggest wish is that they supported YouTube (at least titles) and they had a way to indicate when a article needs to be scraped client side.\n \nreply",
      "Why is Wallabag better than pocket?\n \nreply",
      "Now that I think about it, maybe I can write a little side script to fetch the YouTube titles and update it.\n \nreply",
      "Seems like the tag system is flat, which is a big limitation on the organization capability.For example, I noticed that in the demo access app, there's a note about cooking, and it has 4 tags:\n- `baking`\n- `cupcakes`\n- `oven cooking`\n- `recipe`This would get out of hand quickly.There should be a hierarchy of tags (categories):\n`cupcakes` in `baking` in `oven cooking` in `recipe`The only tag needed in this case for the note would be `cupcakes`\n \nreply",
      "Hi there! I am extremely glad to read someone else write about this necessity!I own and operate a \"list-taking\" app[0] in which every list/kanban-item can itself be a list/kanban.I currently use it for things I'm the creator of -- tasks, story outlines, etc, but looking to introduce 3rd party content for task management (I want to see GitHub tasks from work next to my own tasks) and, as you say, knowledge management of things like recipes or music.Items could be part of one or multiple hierarchies. A list of \"cake\" recipes could be under both \"baking\" and \"party essentials\", and music playlists could include other playlists.As you can tell, this can become convoluted in my mind, and so if that's something that's interesting to you (or anyone reading this), please reach out and let's discuss! hn at nestful.app[0] https://nestful.app\n \nreply",
      "The tagging system is indeed flat, but the lists can be nested. The idea being that tags are usually AI generated, and there's a lot of them (which is useful for search), but lists are meant for manual curation and this is where you can have whatever structure you want.\n \nreply",
      "Happy to see that you have considered thisIMO it would be interesting to try to combine the two approaches (curation + auto tagging).It starts out with the user scaffolding an initial hierarchy, then (after enough usage to provide meaningful data for ML predictions) the ML model predicts on subsequent entries, and asks the user for approval (which feeds a reinforcement learning model)\n \nreply",
      "This is indeed the plan. We're currently working on generating embeddings for the all the bookmarks stored, and one of the usecases of this is going to be clustering. If a bookmark is similar to all other bookmarks in a list, the model can suggest adding those bookmarks to the list. Still a manual operation, but with ML assistance.\n \nreply"
    ],
    "link": "https://github.com/hoarder-app/hoarder",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A self-hostable bookmark-everything app (links, notes and images) with AI-based automatic tagging and full text search\n      A self-hostable bookmark-everything app with a touch of AI for the data hoarders out there.\u26a0\ufe0f This app is under heavy development and it's far from stable.You can access the demo at https://try.hoarder.app. Login with the following creds:The demo is seeded with some content, but it's in read-only mode to prevent abuse.I browse reddit, twitter and hackernews a lot from my phone. I frequently find interesting stuff (articles, tools, etc) that I'd like to bookmark and read later when I'm in front of a laptop. Typical read-it-later apps usecase. Initially, I was using Pocket for that. Then I got into self-hosting and I wanted to self-host this usecase. I used memos for those quick notes and I loved it but it was l"
  },
  {
    "title": "On the nature of computing science (1984) (utexas.edu)",
    "points": 207,
    "submitter": "ColinWright",
    "submit_time": "2024-12-23T12:03:18 1734955398",
    "num_comments": 83,
    "comments_url": "https://news.ycombinator.com/item?id=42493808",
    "comments": [
      "For the academia reward system, maybe Dijkstra was right. But if you work in tech, you quickly discover that most complexity you find around is not introduced because it sells well, but because most people are incapable of good design. Then, sure: since it sells better there is little pressure in the management to replace them with people that can design...\n \nreply",
      "I don't think one should underestimate the incentives at play here though. Complexity sells not just in literal money, but in career prospects too and so on. It's really bad incentives all around in favor of complexity.\n \nreply",
      "I hate moat building. I understand why it exists, but I don\u2019t have to be happy about it.\n \nreply",
      "Moat building is why we need disruptive innovators to come along every now and then and shake things up. Moat busters.\n \nreply",
      "Absolutely! Curriculum driven development is a thing!\n \nreply",
      "That\u2019s a really funny term, is there some blog or something?\n \nreply",
      "https://www.reddit.com/r/ProgrammerHumor/comments/t1r8mn/res...\n \nreply",
      "Complexity sells in terms of dopamine.  \"Look at this incredibly complicated thing I made, that I understand and you don't!  Aren't I brilliant!\"  \"You must be - I can't understand it at all.\"People get emotional rewards from themselves from making something work that is at the limit of the complexity that they can handle.  They often also get emotional rewards from others for making things that others can't understand.  (They shouldn't, but they often do.)\n \nreply",
      "People are very capable of good design but are not given time to do good design. Temporary band-aids become permanent fixtures and eventually the company is drowning in tech debt. It is a tale as old as time.\n \nreply",
      "Time is one of the dimensions, but I often see (bad) designers to stick with the first idea they have (and if they are not very good, the first idea is likely very bad), then as the idea shows the weaknesses, instead of understanding what is going on, they invent more broken \"fixes\" complicating even more the original idea. So there is a multiplicative effect of this going on and on. This is why the 10x programmer is not a myth: it would be if we were talking about \"I can do 10 times what you do\" and that would be impossible if you compare a top programmer with an average one. What happens is instead that a 10x programmer just avoids design mistakes one after the other, so they find themselves doing less and less useless work that in turn complicates things more and so forth. Being a 10x coder is about design, not coding.\n \nreply"
    ],
    "link": "https://www.cs.utexas.edu/~EWD/transcriptions/EWD08xx/EWD896.html",
    "first_paragraph": "On the nature of Computing ScienceNow this summer school draws to a close, it seems appropriate to try to put its topic into some perspectiveIts official theme: Control Flow and Data flow: Concepts of Distributed Programming only determined the flavour, for regularly we were led to pretty general questions that seem rather central to computing in general. So, what is the nature of computing science, or, perhaps more precisely, what should its nature be?There is much confusion about that, and that confusion should not amaze us, for computers nowadays have so many different aspects:The above enumeration is not exhaustive; I leave it to you to design and justify a dominant r\u00f4le in computing for the management scientists, the linguists, the experimental physicists and the educationists. This source of confusion about the nature of computing has now been sufficiently explained, so let us turn to the other component: science.To begin with I would like to recall that Science as a whole has di"
  },
  {
    "title": "Minutes of a Meeting Held at Gear Ratio on Tuesday 13 June 78 [pdf] (forgottenweapons.com)",
    "points": 6,
    "submitter": "akehrer",
    "submit_time": "2024-12-22T01:45:23 1734831923",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.forgottenweapons.com/wp-content/uploads/2024/12/Mamba-production-meeting.pdf",
    "first_paragraph": ""
  },
  {
    "title": "i sensed anxiety and frustration at NeurIPS 24 (kyunghyuncho.me)",
    "points": 131,
    "submitter": "wavelander",
    "submit_time": "2024-12-22T05:48:55 1734846535",
    "num_comments": 83,
    "comments_url": "https://news.ycombinator.com/item?id=42484628",
    "comments": [
      "Trained as a physicist I became acutely aware of what mismatches in the academic job market look like and,  particularly,  how smoking hot fields can become burned out fields in just about the time it takes to complete a PhD.(Some physicists told me about how quickly Ken Wilson's application of RG to phase transitions went from the the next big thing to old hat,  for instance.)When I can bear to read editorials in CACM I see the CS profession has long been bothered by whipsawing demand for undergraduate CS degrees.  I've never heard about serious employment problems for CS PhDs and maybe I never will because they have a path to industry that saves face better than the paths for physics.Maybe we will hear about a bust this time.  As a cog in the social sciences department,  I used to have a view of a baseball diamond out my office window but now there is a construction site for a new building to house the computer science, information science and \"statistics and data science\" departments which are bulging in undergraduate enrollment.Will there finally be a bust?\n \nreply",
      "AI has always had the concept of a \u201cwinter\u201d or bust. [1] As in computer science in general we have had people shift concentration on what can make the most and that can occur during a PhD. The thing about the AI winter this time is that ML and Deep learning and LLMs advanced so quickly that a large bust in AI didn\u2019t really occur. Also, I have heard about people getting the AI jobs and then complaining it\u2019s all about cleaning data and nothing about applying new AI techniques to solve a big challenging problem. For CS in general and IT I can sort of see this as a business cycle and LLMs doing some replacement of jobs or it could just be a business cycle.[1] https://en.wikipedia.org/wiki/AI_winter?wprov=sfti1#\n \nreply",
      "I think you two are talking about different things. The AI winter as I understand it refers to slowing technological advancement in \u201cexpert systems\u201d for example. But technological development is moving faster than ever.This is different. It\u2019s a weird combination of huge amounts of capital chasing a very specific idea of next token prediction, and a slowdown in SWE hiring that may be related. It\u2019s the first time \u201cAI\u201d as an industry has eaten itself.\n \nreply",
      "The AI winter refers to the collapse in government and industry funding. It came as a result of a lot of hype like expert systems not panning out but it was about the money that dried up. This time \u201cit\u2019s different\u201d but VCs are just as fickle as grant reviewers and executives - that funding could dry up at any moment if one of the big AI companies has a down round or goes bankrupt altogether. I don\u2019t think a second AI winter is likely because of how useful LLMs have proven this time around, but we\u2019re probably going to have a correction sooner or later.I don\u2019t think the hiring slowdown and AI are related. Some companies are using rhetoric about AI to save face but the collapse in the job market was due to an end to ZIRP.\n \nreply",
      "From what I read, AI \"winters\" have not just had less progress but also have had less research activity in general: during them, AI was seen as a dead end that didn't provide value, which translates to less funding, which translates to less people doing research on it (or introducing AI to the research they wanted to do already).\n \nreply",
      "Universities are definitely trying to use data science as a draw for students in the face of declining enrollment. I know one R1 employee who is taking it upon themselves to enable the training of tens of thousands of students over the years. People are more excited about AI than coding.\n \nreply",
      "Physics degrees are great for a successful career outside of physics.\n \nreply",
      "If you can do an undergrad in physics and also hold a conversation, your path is open to getting to the top 0.1% in terms of problem solving ability\n \nreply",
      "If someone believed they will earn 2-5x better than in academia, with full freedom to work on whatever interests them, and no need to deliver value to the employer... Well, let's say \"ok\", we have all been young and naive, but if their advisors have not adjusted their expectations, they are at fault, maybe even fraudulent.Even being in elite research groups at the most prestigious companies you are evaluated on product and company Impact, which has nothing to do with how groundbreaking your research is, how many awards it gets, or how many cite it. I had colleagues at Google Research bitter that I was getting promoted (doing research addressing product needs - and later publishing it, \"systems\" papers that are frowned upon by \"true\" researchers), while with their highly cited theoretical papers they would get a \"meet expectations\" type of perf eval and never a promotion.\n \nreply",
      "Yet your Google Research colleagues still earned way more than in academia, even without the promo.Plus, there were quite a few places where a good publication stream did earn a promotion, without any company/business impact. FAIR, Google Brain, DM. Just not Google Research.DeepMind didn't have any product impact for God knows how many years, but I bet they did have promos happening:)\n \nreply"
    ],
    "link": "https://kyunghyuncho.me/i-sensed-anxiety-and-frustration-at-neurips24/",
    "first_paragraph": "Kyunghyun Cholast week at NeurIPS\u201924, one extremely salient thing was the anxiety and frustration felt and expressed by late-year PhD students and postdocs who were confused by the job market that looks and feels so much different from what they expected perhaps when they were applying for PhD programs five or so years ago. and, some of these PhD students and postdocs are my own under my supervision. this makes me reflect upon what is going on or what has been going on in artificial intelligence research and development. this post will be more of less a stream of thoughts rather than a well-structured piece (though, as if i ever wrote a well-structured, well-thought-out, well-prepared blog post.)the past decade or so has been an interesting time for machine learning, or more broadly artificial intelligence. starting with speech recognition in 2010 or so, deep learning has shown dramatic improvements over then-existing states of the art in a variety of challenging and also practical pro"
  },
  {
    "title": "Making AMD GPUs competitive for LLM inference (2023) (mlc.ai)",
    "points": 262,
    "submitter": "plasticchris",
    "submit_time": "2024-12-24T00:17:18 1734999438",
    "num_comments": 165,
    "comments_url": "https://news.ycombinator.com/item?id=42498634",
    "comments": [
      "The problem is that performance achievements on AMD consumer-grade GPUs (RX7900XTX) are not representative/transferrable to the Datacenter grade GPUs (MI300X). Consumer GPUs are based on RDNA architecture, while datacenter GPUs are based on the CDNA architecture, and only sometime in ~2026 AMD is expected to release unifying UDNA architecture [1]. At CentML we are currently working on integrating AMD CDNA and HIP support into our Hidet deep learning compiler [2], which will also power inference workloads for all Nvidia GPUs, AMD GPUs, Google TPU and AWS Inf2 chips on our platform [3][1] https://www.jonpeddie.com/news/amd-to-integrate-cdna-and-rdn....\n[2] https://centml.ai/hidet/\n[3] https://centml.ai/platform/\n \nreply",
      "The problem is that the specs of AMD consumer-grade GPUs do not translate to computer performance when you try and chain more than one together.I have 7 NVidia 4090s under my desk happily chugging along on week long training runs. I once managed to get a Radeon VII to run for six hours without shitting itself.\n \nreply",
      "> I have 7 NVidia 4090s under my deskI have 6 Radeon Pro VII under my desk (in a single system BTW), and they run hard for weeks until I choose to reboot e.g. for Linux kernel updates.I bought them \"new old stock\" for $300 apiece. So that's $1800 for all six.\n \nreply",
      "How does the compute performance compare to 4090\u2019s for these workloads?(I release it will be significantly lower, just try to get as much of a comparison as is possible).\n \nreply",
      "The Radeon VII is special compared to most older (and current) affordable GPUs in that it used HBM giving it memory bandwidth comparable to modern cards ~1TB/s and has reasonable FP64 (1:4) throughput instead of (1:64). So this card can still be pretty interesting for running memory bandwidth intensive FP64 workloads. Anything affordable afterward by either AMD or Nvidia crippled realistic FP64 throughput to below what a AVX-512 many-core CPU can do.\n \nreply",
      "If we speak about FP64, are your loads more like fluid dynamics than ML training?\n \nreply",
      "The 4090 offers 82.58 teraflops of single-precision performance compared to the Radeon Pro VII's 13.06 teraflops.\n \nreply",
      "On the other hand, for double precision a Radeon Pro VII is many times faster than a RTX 4090 (due to 1:2 vs. 1:64 FP64:FP32 ratio).Moreover, for workloads limited by the memory bandwidth, a Radeon Pro VII and a RTX 4090 will have about the same speed, regardless what kind of computations are performed. It is said that speed limitation by memory bandwidth happens frequently for ML/AI inferencing.\n \nreply",
      "For inference sure, for training: no.\n \nreply",
      "Are you running ml workloads or solving differential equations?The two are rather different and one market is worth trillions, the other isn't.\n \nreply"
    ],
    "link": "https://blog.mlc.ai/2023/08/09/Making-AMD-GPUs-competitive-for-LLM-inference",
    "first_paragraph": "\n\n          Aug 9, 2023\n        \n        \n        \u2022 \nMLC Community\n\n\nMLC-LLM makes it possible to compile LLMs and deploy them on AMD GPUs using ROCm with competitive performance. More specifically, AMD Radeon\u2122 RX 7900 XTX gives 80% of the speed of NVIDIA\u00ae GeForce RTX\u2122 4090 and 94% of the speed of NVIDIA\u00ae GeForce RTX\u2122 3090Ti for Llama2-7B/13B. Besides ROCm, our Vulkan support allows us to generalize LLM deployment to other AMD devices, for example, a SteamDeck with an AMD APU.\n\nThere have been many LLM inference solutions since the bloom of open-source LLMs.\nMost of the performant inference solutions are based on CUDA and optimized for NVIDIA GPUs.\nIn the meantime, with the high demand for compute availability, it is useful to bring\nsupport to a broader class of hardware accelerators. AMD is one potential candidate.\n\nFrom the spec comparison, we can see that AMD\u2019s RX 7900 XTX is a good match for NVIDIA\u2019s RTX 4090 and RTX 3090 Ti.It is harder to compare the price of 3090Ti as that was a"
  },
  {
    "title": "Build a Low-Cost Drone Using ESP32 (digikey.com)",
    "points": 346,
    "submitter": "m3at",
    "submit_time": "2024-12-24T00:20:39 1734999639",
    "num_comments": 116,
    "comments_url": "https://news.ycombinator.com/item?id=42498648",
    "comments": [
      "Note to anyone unafamiliar: There is a thriving \"FPV\" ecosystem of drones that can be DIYed. Example common setup, you can mix+match:  - Small square PCB with the main flight control MCU (STM32), and some sensors\n  - Smalls square PCB with motor drivers\n  - Carbon fiber frame\n  - Small PCB with a LoRa radio\n  - Camera and video transmission system. (90s-security-cam style analog, or digital.\n  - Brushless DC motors, props etc\n\nUses Betaflight, ArduPilot, iNav, or PX4 firmware. Or, you could write your own.The PCB-frame in the article is neat and has obvious convenience advantages, but I speculate that it would not be stiff enough for desirable controllable characteristics under high accel situations.\n \nreply",
      "And a lot of it is all open source!ESC software:- https://github.com/am32-firmware- https://github.com/mathiasvr/bluejayFlight controller (you mentioned these):- https://github.com/betaflight- https://github.com/ArduPilot- https://github.com/iNavFlightControl link:- https://github.com/ExpressLRS (also uses ESP32/ESP82 chips)Radio Controllers:- https://github.com/EdgeTX5+ years ago the vast majority of this stuff was proprietary-only and getting into the hobby cost thousands of dollars.  Now you could start at ~$500 (big price factor for FPV is the goggles, but cheap analog ones can be had for ~$100).\n \nreply",
      "With luck the OpenIPC cameras that are starting to come on the market will find traction and the entire hardware stack will run on open source firmware.https://www.youtube.com/watch?v=EdjI3ZQsmCA\n \nreply",
      "This is so cool; LFTI!\n \nreply",
      "This is all true, but just to set expectations: the open source ecosystem seems to be lagging the proprietary world pretty significantly, unless there's some corner where development is really chugging along that's not making it out to the rest of the hobbyist market.Though there have been incremental improvements in flight control software, and video subsystems have moved (mostly) from analog to 2.4/5.8 GHz and digital, the overall architecture is pretty similar to what it was 5+ years ago.  You have a hobby R/C transmitter and receiver driving PWM outputs (through the flight controller, typically an STM32) to hobby-type ESCs which control the motors.  The ESCs are microcontroller-driven and can be reflashed, but painstakingly and annoyingly.  Telemetry is typically separate from control, which is separate from video.  Everything is very short-range and non-IP.In comparison, a COTS quadcopter from DJI has a single backhaul from the airframe to the controller which does control, video, and and telemetry.  And the video is impressively low-latency.  (I'm pretty sure they use a WiFi-type chipset and just spew raw vendor frames, and the receiver picks up what it can, best effort.  You could do this with an ESP32 in ESP-NOW mode, I suspect?)  I've seen some efforts to reverse-engineer the DJI protocol but I'm not aware of a fully compatible implementation or equivalent in the OSS world.And at the upper end of the commercial/proprietary space you have systems with out-of-the-box autonomy, multiple backhauls over IP -- so they can use LOS/BLOS radio, LTE, SATCOM, whatever you want -- integration with navigation beacon systems to reduce GPS dependence, hybrid motor/generators, redundant power systems, the whole shebang.There's no real reason aside from developer interest that this situation exists, as far as I can see. The components are mostly all available.  A Raspberry Pi running a decent RTOS would have orders of magnitude more processing capacity than an STM32 and could easily do the sort of multi-sensor fusion that the commercial systems do.  LTE modems are cheap.  A bigger hexacopter or fixed-wing could easily loft one of the small Starlink dishes, if someone wanted to.  Stuff like \"perching\" (landing and recharging from solar panels) is entirely possible.But from what I can tell, the cutting edge of open source drones is happening behind closed doors in Ukraine and Iran.Happy to be corrected if there's new stuff that I'm not tracking, but the gap between the \"art of the possible\" and current practice seems large.Lots of opportunity though, is the other way to view it.\n \nreply",
      "Everything here is possible, the gap in implementation is that it\u2019s a) expensive and b) non trivial engineering work. There is vanishingly small overlap between the people whom have the capital for parts, the understanding of the engineering needed, free time to do it and desire to do it for free.The people whom have the true multidisciplinary understanding to do robotics well can usually also consult (with little difficulty finding work) for $$$s per hour and get the same \u201cproblem solving satisfaction\u201d.Open source software shortcuts a couple of these limitations because you can work on it with little investment over than time.\n \nreply",
      "Depends on what you want. A lot of the interest in FPV is acro flight, not autonomous, so there isn't a lot of interest in improving that (at least for beta flight)You act as if separate backhauls is a bad thong - I'm not sure that's self evident. Short range just seems wrong to me - RC transmitters outrage all of the current video tx systems, and in open air video tx is long enough that you run into problems with battery before anything else. The best video tx systems currently is proprietary (DJI).Open source is good enough for what most people care about. An open source high quality video tx would be awesome, but obviously a significant engineering effort relative to everything else.\n \nreply",
      "That is consistent with my experience. My highlights:- Current betaflight devevelopers and leaders are incompetent (They inherited the code base from others), and are slow, and varying degrees of willing, to add higher-level flight-control mechanics (semi-autonomous modes etc).- The Ardupilot and PX4 configuration systems, and general experience, are user-hostile.Note that the converses of these hold: Betaflight has a reasonably-good user experience, and Ardupilot and PX4 have extensive higher-level flight control mechanics. If only you could get the pros of both together!Regarding hardware, frames generally accommodate flight controllers and ESCs elegantly, but other hardware like cameras, radios, and especially batteries, feel clumsy to assemble in a safe and consistent way.\n \nreply",
      "I think to be fair BetaFlight is not aimed at autonomous flight, when I got into this stuff a few years ago I was told \"BetaFlight is for FPV (acro mode?)\" and \"iNAV is for Autonomous\" and after giving both a try I can say that my experience on iNAV was much better for what I wanted (autonomous). BetaFlight worked wonders for flying around (very low input delay, etc..) but it was pretty clear it barely had anything related to auto control. Hells, it didn't even have a reasonable return-to-home!!! It would just drift diagonally in the \"home\" direction and try to land when it felt it was close enough.iNAV on the other hand REQUIRES a full gps suite for starters (which for me was REALLY expensive) but you can do full waypoint navigation with just that and a barometer.Of course it's been at least 4 years but hey at least it was pretty good back then already on the iNAV side!\n \nreply",
      "Interesting, I wanted to look into building a drone myself next year. Mostly to learn about tinkering with engineering and electronics. What are the issues with the codebases and their development?\n \nreply"
    ],
    "link": "https://www.digikey.com/en/maker/projects/a-step-by-step-guide-to-build-a-low-cost-drone-using-esp32/8afccd0690574bcebfa0d2ad6fd0a391",
    "first_paragraph": ""
  }
]