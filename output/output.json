[
  {
    "title": "The Emperor's New LLM (dayafter.substack.com)",
    "points": 64,
    "submitter": "shmval",
    "submit_time": "2025-06-13T22:12:59 1749852779",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=44272773",
    "comments": [
      "If you want an LLMs \"opinion\" on something, you need to phrase the question in such a way that it can't tell what answer you would prefer.Don't say \"Is our China expansion a slam dunk?\u201d Say: \"Bob supports our China Expansion, but Tim disagrees. Who do you think is right?\" And then, experiment with a few different phrasings to see if the answer changes\u2014if it does, you probably shouldn't trust it. In both cases, ask the LLM to explain its reasoning, and make sure you agree with its argument.I predict someone is going to reply \"an LLM can't have opinions, its recommendations are never meaningful.\" I think I might agree with you\u2014I'm just not sure! If an LLM can write a decent-ish business plan, why shouldn't it be decent-ish at evaluating which of two business plans is better? I wouldn't expect an LLM to ever be better than a human, but if I just need a second opinion and don't have access to another real human... I don't know.\n \nreply",
      "Related read: https://futurism.com/chatgpt-mental-health-crises\n \nreply",
      "I played around with one of the less-sketchy \"chat\" apps a while ago and I've been ringing this bell ever since. Interacting with these things as if they were humans is dangerous.\n \nreply",
      "That's grim. But the Eliza effect[1] makes @sama richer, so it's all good. Be naughty[2]![1] https://en.wikipedia.org/wiki/ELIZA_effect\n[2] https://www.paulgraham.com/conformism.html\n \nreply",
      ">The same kind of bias keeps resurfacing in every major system: Claude, Gemini, Llama, clearly this isn\u2019t just an OpenAI problem, it\u2019s an LLM problem.It's not an LLM problem, it's a problem of how people use it. It feels natural to have a sequential conversation, so people do that, and get frustrated. A much more powerful way is parallel: ask LLM to solve a problem. In a parallel window, repeat your question and the previous answer and ask to outline 10 potential problems. Pick which ones appear valid, ask to elaborate. Pick your shortlist, ask yet another LLM thread to \"patch\" the original reply with these criticisms, then continue the original conversation with a \"patched\" reply.LLMs can can't tell legitimate concerns from nonsensical ones. But if you, the user, do, they will pick it up and do all the legwork.\n \nreply",
      "Feels like a high-level back propagation step. Not surprising, really!\n \nreply",
      "This feels like a pretty big ergonomics gap in presenting things as a chat window at all?\n \nreply",
      "I worked on a very early iteration of LMs (they weren't \"large\" yet) in grad school 20 years ago and we drove it with a Makefile. The \"prompt\" was an input file and it would produce a response as an artifact. It never even occurred to us to structure it as a sequential \"chat\" because at that point it was still too slow. But it does make me wonder how much the UX changes the way people think about it.\n \nreply",
      "It's more compelling to fundraising and hype-pushing stories to make it look as \"person-like\" as possible.\n \nreply",
      "Or people like the familiar chat interface and they don\u2019t want to dick around with a complicated workflow like the person above provided.What are examples of 3rd party UIs that make these alternative, superior workflows easier?\n \nreply"
    ],
    "link": "https://dayafter.substack.com/p/the-emperors-new-llm",
    "first_paragraph": ""
  },
  {
    "title": "Implementing Logic Programming (btmc.substack.com)",
    "points": 66,
    "submitter": "sirwhinesalot",
    "submit_time": "2025-06-13T21:32:21 1749850341",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=44272467",
    "comments": [
      "I second the recommendation in Sir Whinesalot's post (which I haven't fully read yet) to look at miniKanren and microKanren. I found it extremely educational to port microKanren to OCaml a few years ago, and I think the result is somewhat more comprehensible than the original Scheme, though you'll still probably have to read the paper to understand it: http://canonical.org/~kragen/sw/dev3/mukanren.mlThe most astonishing result of miniKanren is a Scheme interpreter that you can run forwards, backwards, or both.  http://webyrd.net/quines/quines.pdf demonstrates using it to generate a Scheme quine, that is, a Scheme program whose output when run is itself (\"miniKanren, Live and Untagged:\nQuine Generation via Relational Interpreters\n(Programming Pearl)\", Byrd, Holk, and Friedman).\u00a74.4 of SICP http://sarabander.github.io/sicp/html/4_002e4.xhtml also has an implementation of logic programming in Scheme which is extremely approachable.Unlike the post, I don't think Datalog is the place to look for deep insights about logic programming.  Instead, it's the place to look for deep insights about databases.\n \nreply",
      "https://github.com/Seeker04/plwmThis window manager implemented in Prolog popped up here recently. It's really cool!I jumped to it as a new daily driver in the hope that I'd learn some Prolog, and it's been quite the success, actually. The developer is really nice, and he's generously helped me with some basic questions and small PRs.Definitely recommended. I have a Guix package for it if anyone's interested.Any reading recommendations for high quality logic programming codebases?\n \nreply",
      "I did a detailed write-up of implementing miniKanren here:https://codeberg.org/ashton314/microKanrenBy the end of it, I implement a small type checker that, when you run it backwards (by giving the checker a type), it proceeds to enumerate programs that inhabit that type!\n \nreply",
      "Isn't that amazing\u203d  I wonder if you could guide its search with an LLM...\n \nreply",
      "Nice!I'll note there is a really shallow version of naive datalog I rather like if you're willing to compromise on syntax and nonlinear variable use.   edge = {(1,2), (2,3)}\n   path = set()\n   for i in range(10):\n       # path(x,y) :- edge(x,y).\n       path |= edge\n       # path(x,z) :- edge(x,y), path(y,z).\n       path |= {(x,z) for x,y in edge for (y1,z) in path if y == y1}\n\n\nSimilarly it's pretty easy to hand write SQL in a style that looks similar and gain a lot of functionality and performance from stock database engines. https://www.philipzucker.com/tiny-sqlite-datalog/I wrote a small datalog from the Z3 AST to sqlite recently along these lines https://github.com/philzook58/knuckledragger/blob/main/kdrag...\n \nreply",
      "That's exciting!\n \nreply",
      "Lately I\u2019ve been dabbling with different Prolog implementations and Constraint Handling Rules which led me to CLIPS [0] (in Public Domain, but developed at NASA - sounds neat doesn\u2019t it?)It\u2019s not very easy to get into, but it\u2019s very fast on rule resolution and being pure C is easy to integrate. I\u2019m trying to get smart code parsing using logic language and this seems promising. I\u2019m also a Lisp nerd so that works for me :)[0]: https://www.clipsrules.net/\n \nreply",
      "I think it would be really impactful to start with a problem and describe how logic programming solves that problem better than the other paradigms.\n \nreply",
      "I've been reading a bit about it, and it seems easier to make goal-driven backwards chaining AI from it, like the block world example. You could in theory use that for something like a video game AI (like GOAP, Goal-Oriented Action Planning, which is based on STRIPS). Whenever I read about GOAP though, they seem to have used a graphical editor to declaratively input rules rather than a logic programming language.Note that I'm not an expert in any of this, I've just been reading about this kind of AI recently. I haven't actually done this myself.\n \nreply",
      "The only production experience I have with logic programming is OPA Rego for writing security policies (not sure it's a \"pure\" logic language but feels like the primary paradigm).I found it pretty interesting for that use case, although the learning curve isn't trivial for traditional devs.https://www.openpolicyagent.org/\n \nreply"
    ],
    "link": "https://btmc.substack.com/p/implementing-logic-programming",
    "first_paragraph": ""
  },
  {
    "title": "Self-Adapting Language Models (arxiv.org)",
    "points": 91,
    "submitter": "archon1410",
    "submit_time": "2025-06-13T19:03:42 1749841422",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=44271284",
    "comments": [
      "The self-edit approach is clever - using RL to optimize how models restructure information for their own learning. The key insight is that different representations work better for different types of knowledge, just like how humans take notes differently for math vs history.Two things that stand out:- The knowledge incorporation results (47% vs 46.3% with GPT-4.1 data, both much higher than the small-model baseline) show the model does discover better training formats, not just more data. Though the catastrophic forgetting problem remains unsolved, and it's not completely clear whether data diversity is improved.- The computational overhead is brutal - 30-45 seconds per reward evaluation makes this impractical for most use cases. But for high-value document processing where you really need optimal retention, it could be worth it.The restriction to tasks with explicit evaluation metrics is the main limitation. You need ground truth Q&A pairs or test cases to compute rewards. Still, for domains like technical documentation or educational content where you can generate evaluations, this could significantly improve how we process new information.Feels like an important step toward models that can adapt their own learning strategies, even if we're not quite at the \"continuously self-improving agent\" stage yet.\n \nreply",
      "From Anthropic a couple days ago too, self finetuning:https://arxiv.org/html/2506.10139v1\n \nreply",
      "I wonder if anyone who\u2019s really in the know could summarize where the research is at with getting LLMs to learn \u201con the job\u201d (through continuous fine tuning or whatever) and what the blockers are to this being a useful deployable thing, e.g. having a model+coding agent that can actually learn a codebase over time (cost? model collapse? something else?).I\u2019m sure this is something the big labs are trying but from the outside as a user of LLMs it feels like people don\u2019t talk about this very much and instead the focus right now is on better training (eg reinforcement learning) with the assumption that anything else not learned during training will be stuffed into the context somehow as needed. But from a naive perspective the lack of learning from experience after training seems like the biggest thing standing between us and AGI.\n \nreply",
      "We have no idea how to do continual learning.Many people here are right, compute, collapse, forgetting whatever.The only \"real\" way to do this would be:\n1. Train a model\n2. New data\n3. Retrain the model in full + new data\n4. Repeat\n5. You still have no garuntee on the \"time\" aspect though.But CL as a field basically has zero answers on how to do this in a true sense. It's crazy hard because the \"solutions\" are hypocritical in many ways.We need to expand the model's representation space while keeping the previous representation space nearly the same?Basically, you need to modify it without changing it.Most annoying is that even the smallest of natural brains do this easily. I have a long winded theory but basically it boils down to AI likely needs to \"sleep\" or rest somehow.\n \nreply",
      "but natural brains sleep too, which I guess is your point. But actually is it even clear in human brains whether most of neural compute is evaluation vs training? maybe the brain is like for e.g. capable of running 20T model of compute and deploying like 2B model at given time and most of compute is training in background new models--I mean like you say we have no idea except for training from scratch, but if we are working much below capacity of compute we could actually actively train from scratch repeatedly (like the xAI cluster could probably train gpt4o size in a matter of hours)\n \nreply",
      "AGI likely a combination of these two papers + something new likely along the lines of distillation.1. Preventing collapse -> model gets \"full\"\nhttps://arxiv.org/pdf/1612.007962. Forgetting causes better generalization\nhttps://arxiv.org/abs/2307.011633. Unknow paper that connects this\n- allow a \"forgetting\" model that improves generalization over time.\n- I tried for a long time to make this but it's a bit difficultFun implication is that if true this implies AGI will need \"breaks\" and likely need to consume non task content of high variety much like a person does.\n \nreply",
      "The cool thing about AI that I'm seeing as an outsider/non-academic, is that it's relatively cheap to clone. Sleeping/resting could be done by a \"clone\" and benefits could be distributed on a rolling schedule, right?\n \nreply",
      "One clone takes a nap while the other works is pretty cool.But the clone couldn't run without sleeping? So that's more of a teammate than a clone.1 works while the other sleeps and then swap.If this method ever worked our current alignment methods get chucked out the window those would be two completely different AI.\n \nreply",
      "I'm no expert, but I'd imagine privacy plays (or should play) a big role in this. I'd expect that compute costs mean any learning would have to be in aggregate rather than specific to the user which would then risk leaking information across sessions very likely.I completely agree that figuring out a safe way to continually train feels like the biggest blocker to AGI\n \nreply",
      "The real answer is that nobody trusts their automated evals enough to be confident that any given automatically-trained release actually improves performance, even if eval scores go up. So for now everyone batches up updates and vibe-checks them before rolling them out.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2506.10943",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "If the moon were only 1 pixel: A tediously accurate solar system model (2014) (joshworth.com)",
    "points": 670,
    "submitter": "sdoering",
    "submit_time": "2025-06-13T08:40:05 1749804005",
    "num_comments": 212,
    "comments_url": "https://news.ycombinator.com/item?id=44266828",
    "comments": [
      "Given the great distances and how small the planets seem at that scale, I'm surprised that we can see any of the planets with the naked eye. Thinking about Jupiter, it's 140K km in diameter and about 629M km from Earth. That's a ratio of 1:4500. So imagine a U.S. dime that is 1.8cm in diameter placed 1.8 x 4500 = 8100 cm away. Would you be able to see a dime that it 81m or 266ft away at nighttime, assuming it slightly illuminated? We can see Jupiter, so I guess we should be able to see the illuminated dime too.\n \nreply",
      "Make sure you press the \"c\" button in the bottom right.Light is incredibly slow, and everything seems out of reach.I think we'll have a holodeck before we reach another star. And maybe that'll be enough.\n \nreply",
      "Is light slow? Or is the human perception of time just scaled down as a result of our rapid metabolism and infinitesimality? People historically mistake plants for being inanimate things with no reactivity, that they are far more simple and stupid than they truly are. Outside of a few exotic examples, plants simply operate on a wider timescale that's basically imperceptible without careful and particular observation. It becomes much more apparent how alive plants are when we observe them in a time-lapse. Now realize that plants are still relatively short-lived. The absolute oldest ones only go back to the early neolithic, that's only 14000 years or so. 1000 years is a long time for humans, but probably not for the trees where a single one can live 10x that.From the hypothetical perspective of a star, with a lifespan measured in billions upon billions of years, the entire ecoscape of the world changes in a blink. From the sun's perspective, MENA was green just a very short while ago. Hell, Pangea wasn't that long ago. At this timescale, continental drift would be as apparent as the movement of boats are to humans. Anything that's working at the cosmic scale where the seemingly low speed of light sounds exhausting is most definitely working at this stellar perspective at the minimum. 14000 years of travel might as well be the equivalent of a 10 minute commute to the store.Philosophically speaking, of course.\n \nreply",
      "Thanks for this.In addition to the insight, it reminded me to water a plant at a desk I no longer use. The plant's been with me through quite a bit and I have been neglecting it recently as I no longer see it regularly.\n \nreply",
      "Move your plant friend to your new desk?\n \nreply",
      "For very philosophical writings about this, read \"Last and First Men\" and \"Star Maker\" by  Olaf Stapledon. Written in the 1930's, these describe on a very expansive scale the history of, respectively, humanity and the universe. Very mind bending.\n \nreply",
      "I always think of those motor proteins moving along slowly inside our bodies, and wonder if maybe we are just the motor proteins of the cosmic scale.\n \nreply",
      "We have a long way to go before we learn to move a star (or a rosette).https://en.wikipedia.org/wiki/Stellar_enginehttps://en.wikipedia.org/wiki/Klemperer_rosette\n \nreply",
      "Dude, pass the duchy.\n \nreply",
      "Comments like this are part of the reasons I come here.\n \nreply"
    ],
    "link": "https://joshworth.com/dev/pixelspace/pixelspace_solarsystem.html",
    "first_paragraph": ""
  },
  {
    "title": "OxCaml - a set of extensions to the OCaml programming language. (oxcaml.org)",
    "points": 255,
    "submitter": "lairv",
    "submit_time": "2025-06-13T14:20:38 1749824438",
    "num_comments": 74,
    "comments_url": "https://news.ycombinator.com/item?id=44268782",
    "comments": [
      "The Janet Street folks, who created this, also did an interesting episode[0] of their podcast where they discuss performance considerations when working with OCaml. What I was curious about was applying a GC language to a use case that must have extremely low latency. It seems like an important consideration, as a GC pause in the middle of high-frequency trading could be problematic.[0] https://signalsandthreads.com/performance-engineering-on-har...\n \nreply",
      "GC compactions were indeed a problem for a number of systems.  The trading systems in general had a policy of not allocating after startup.  JS has a library, called \"Zero\" that provides a host of non-allocating ways of doing things.\n \nreply",
      "Couldn\u2019t find this after 6 seconds of googling, link?\n \nreply",
      "The linked podcast episode mentions it.\n \nreply",
      "There's no mention of a library called zero, or even JavaScript.\n \nreply",
      "Im assuming the JS refers to Janes street\n \nreply",
      "That makes sense, I guess I've got web tunnel vision.\n \nreply",
      "> This is what I like to call a dialect of OCaml. We speak in sometimes and sometimes we gently say it\u2019s zero alloc OCaml. And the most notable thing about it, it tries to avoid touching the garbage collector ...\n \nreply",
      "*Jane Street\n \nreply",
      "It's a great name for a competitor :)\n \nreply"
    ],
    "link": "https://oxcaml.org/",
    "first_paragraph": "It is both Jane Street\u2019s production compiler, as well as a laboratory for experiments focused towards making\n            OCaml better for performance-oriented programming. Our hope is that these extensions can over time be\n            contributed to upstream OCaml.OxCaml\u2019s extensions are meant to make OCaml a great language for performance engineering. Performance\n            engineering\n            requires control, and we want that control to be:By \"only where you need it\", we mean that OxCaml\u2019s extensions should be pay-as-you-go. While OxCaml aims to\n            provide\n            more power to optimize, you shouldn\u2019t need to swallow extra complexity when you\u2019re not using that power.By \"in OCaml\", we mean that all valid OCaml programs are also valid OxCaml programs. But our more profound\n            goal\n            is\n            for OxCaml to feel like OCaml evolving into a better version of itself, rather than a new language. For\n            that,\n            OxCaml\n            "
  },
  {
    "title": "Student discovers fungus predicted by Albert Hoffman (wvu.edu)",
    "points": 59,
    "submitter": "zafka",
    "submit_time": "2025-06-11T00:36:00 1749602160",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=44243059",
    "comments": [
      "I really appreciate the mature tone this article took towards the discussion of a psychedelic compound\n \nreply",
      "It is know since a long time that morning glory seeds contain LSA.Also LSA is different from LSD. While you can legally get the former it is (from my experience) way more dangerous than LSD.\n \nreply",
      "LSA distorted my perception of object sizes and made me very nauseous.I took it once in high school. HBWR seeds. Scraped the nasty stuff off the outside. Fell asleep while waiting for it to kick in. Woke up intoxicated. Puked. Went back to sleep\n \nreply",
      ">It is know since a long time that morning glory seeds contain LSAThis is one of the givens they were working off of.  The finding of the research is the fungus that produces the LSA in the seeds.\n \nreply",
      "> The researchers prepared a DNA sample and sent it away for genome sequencing, funded by a WVU Davis College Student Enhancement Grant obtained by Hazel. The sequencing confirmed the discovery of a new species and the sequence is now deposited in a gene bank with her name on it.> \u201cSequencing a genome is a significant thing,\u201d Panaccione said. \u201cIt\u2019s amazing for a student.\u201dQuestion - how is it significant, considering they sent it off to another company to do the sequencing?\n \nreply",
      "A full human sequence is only 2-5k USD so it's about money.I doubt she wrote the grant, professor looks like he would give certain students undue credit.\n \nreply",
      "I'm expecting the significant thing is knowing which DNA to sequence. Also, if I'm reading the article correctly she isolated the DNA being sequenced first, so it's not like she just sent in the fungus and offloaded all of the work.\n \nreply",
      "She did want to say: \"It is amazing for a student to get this much success by a half accidental discovery\"Then she thought about things incomprehensible for programmers and said the other sentence.\n \nreply",
      "> incomprehensible for programmersYou should stop projecting. I understand something may be incomprehensible to you and you happen to call yourself a programmer. That doesn't mean you're correct about either.\n \nreply",
      "I think what they mean is:sequencing a genome [of a new species] is a significant thing\n \nreply"
    ],
    "link": "https://wvutoday.wvu.edu/stories/2025/06/02/wvu-student-makes-long-awaited-discovery-of-mystery-fungus-sought-by-lsd-s-inventor",
    "first_paragraph": "\nMonday, June 02, 2025\nCorinne Hazel, a WVU environmental microbiology major from Delaware, Ohio, has discovered a new species of fungus that may treat a variety of medical conditions.\n           (WVU Photo/Brian Persinger)\n\n\nMaking a discovery with the\npotential for innovative applications in pharmaceutical development, a West Virginia University microbiology\nstudent has found a long sought-after fungus that produces effects similar to the\nsemisynthetic drug LSD, which is used to treat conditions like depression,\npost-traumatic stress disorder and addiction.Corinne Hazel, of Delaware,\nOhio, an environmental microbiology major and Goldwater\nScholar, discovered the new species of fungus growing in morning\nglory plants and named it Periglandula clandestina.\u00a0Hazel made the discovery\nwhile working in the lab with Daniel\nPanaccione, Davis-Michael Professor\nof Plant\nand Soil Sciences at the WVU Davis College of Agriculture and\nNatural Resources. She\nwas studying how morning glories disperse "
  },
  {
    "title": "The International Standard for Identifying Postal Items (akpain.net)",
    "points": 14,
    "submitter": "surprisetalk",
    "submit_time": "2025-06-12T15:02:23 1749740543",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.akpain.net/blog/s10-upu/",
    "first_paragraph": "The Universal Postal Union's S10 standard, in all its gloryHave you ever received a parcel from overseas? I did recently, from Switzerland!Looking at the envelope, I realised the format of the tracking number (UT038926726CH) was in a very similar (if not identical) format to ones I'd used frequently here in the UK.I'd been receiving emails from Royal Mail about the parcel, so I just presumed that there was some data-sharing agreement in place with Swiss Post and Royal Mail had just given the parcel their own internal reference number.But seeing the reference number printed on the envelope from the get-go made me want to see what Swiss Post had to say about it...The same tracking number works across multiple carriers, and it's the same format as all the UK ones! What gives??The Universal Postal Union (UPU) is an agency of the UN that coordinates postal policies to facilitate post flowing across the world. As part of this, it publishes the S10 standard for the identification of postal it"
  },
  {
    "title": "Whatever Happened to Sandboxfs? (blogsystem5.substack.com)",
    "points": 18,
    "submitter": "zdw",
    "submit_time": "2025-06-11T16:37:09 1749659829",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=44249338",
    "comments": [
      "Recently, macOS added a built-in FUSE-like API:https://developer.apple.com/documentation/fskitNo idea what the performance is like.However, over the last few years, Apple\u2019s compilers have gotten even more enthusiastic than before about caching and \u201ccontent addressable storage\u201d:https://llvm.org/devmtg/2024-10/slides/techtalk/Rastogi-Fine...Which is normally a good thing, but may exacerbate the issue described in the post where, by enforcing isolation, you end up cutting off the compiler from its caches and making the build slower.I think the ideal solution would be for Bazel to somehow know about and integrate with LLVM\u2019s CAS.edit: although just figuring out how to do \u201cexplicit module builds\u201d with Swift and Clang would probably get you most of the way.\n \nreply",
      "Seems like the new ASIF sparse image format will solve a lot of this, combined with their new containerization frameworkhttps://eclecticlight.co/2025/06/12/macos-tahoe-brings-a-new...\n \nreply",
      "Copying data into and out of a disk image is probably going to be much slower and involve just as many syscalls if not more compared to setting up a \u201csymlink forest\u201d.As far as I can tell, the containerization framework seems like it\u2019s for running Linux microvms, and doesn\u2019t seem applicable to people trying to do macOS builds. I mean, if you just want to run Bazel in a Linux VM sure it will do fine, but you can already run Bazel in a Linux vm on your Mac with Docker.app. Maybe I missed something with the containerization docs but all I saw was EXT4, OCI Linux images, etc. no Mac thingies.\n \nreply",
      "If you semi-trust your tool binaries enough not to do something silly like making syscalls directly, what about using the `LD_PRELOAD` equivalent to intercept `open` etc.? (Yes there's a long tail in that \"etc.\")Or does using Go ruin everything again?(Also it seems like it should in principle be possible to keep the symlink forest around and only delete other files)\n \nreply"
    ],
    "link": "https://blogsystem5.substack.com/p/whatever-happened-to-sandboxfs",
    "first_paragraph": ""
  },
  {
    "title": "I convinced HP's board to buy Palm and watched them kill it (philmckinney.substack.com)",
    "points": 443,
    "submitter": "AndrewDucker",
    "submit_time": "2025-06-13T18:03:30 1749837810",
    "num_comments": 385,
    "comments_url": "https://news.ycombinator.com/item?id=44270709",
    "comments": [
      "> \"Then, in late June 2011 [\u2026] I faced a medical emergency requiring immediate surgery and a eight-week recovery period confined to bed. [\u2026] On July 1, 2011, HP launched the TouchPad tablet running WebOS 3.0 [\u2026] The launch was botched from the start. HP priced the TouchPad at $499 to compete directly with the iPad, but without the app ecosystem or marketing muscle to justify that premium. The device felt rushed to market, lacking the polish that could have helped it compete.\"He claims to have been working with Palm closely for a year, yet he somehow must have missed how bad things were. The product was a week or two away from launch when he had to step away. To me it sounds like the bad decisions had already been made.\n \nreply",
      "The price was likely too high, though that is debatable.  However the real take away is if you want something like this to work out you need to invest in to for years.  There is nothing wrong with getting the size of the market wrong by that much - it happens too often for anyone to call it wrong.  It isn't clear what was predicted, but marketing should have predicted a range of units sold (and various price points having different predicted ranges!).They didn't have the app ecosystem - no surprise.  However the only way to get that ecosystem is years of investment.  The Windows phone failed a couple years latter for similar reasons - nice device (or so I'm told), but it wasn't out long enough to get a lot of apps before Microsoft gave up on it.\n \nreply",
      "> There is nothing wrong with getting the size of the market wrong by that much - it happens too often for anyone to call it wrong. It isn't clear what was predicted, but marketing should have predicted a range of units sold (and various price points having different predicted ranges!).Shout out to the Itanium sales forecast: https://upload.wikimedia.org/wikipedia/commons/8/88/Itanium_...\n \nreply",
      "And its inverse, the IEA solar energy forecast: https://en.wikipedia.org/wiki/File:Reality_versus_IEA_predic...(This version of the graph is pretty old, but it's enough to get the flavor. The rate of new installations is still increasing exponentially, and the IEA continues to predict that it'll level off any day now...)\n \nreply",
      "If they keep predicting that, eventually they\u2019ll be right!(It\u2019s hard to harvest more power from a star than a Dyson sphere is capable of)\n \nreply",
      "Those 2 charts are amazing! At least the Itanium people adjusted their curves downward over time, looks like the IEA just carried on regardless!\n \nreply",
      "It wasn't the Itanium people so much as the industry analysts who follow such things. And, yes, they (including myself) were spectacularly wrong early on but, hey, it was Intel after all and an AMD alternative wasn't even a blip on the radar and 64-bit chips were clearly needed. I'm not sure there was any industry analyst--and I probably bailed earlier than most--who was going this is going to be a flop from the earliest days.\n \nreply",
      "an AMD alternative wasn't even a blip on the radar\n\nAside from it not being 64bit initially uh.. did we live through the same time period? The Athlons completely blew the Intel competition out of the water. If Intel hadn't heavily engaged in market manipulation, AMD would have taken a huge bite out of their marketshare.\n \nreply",
      "In the 64-bit server space, which is really what's relevant to this discussion, AMD was pretty much not part of the discussion until Dell and Sun picked them up as a supplier in the fairly late 2000s. Yes, Intel apparently played a bunch of dirty pool but that was mostly about the desktop at the time which the big suppliers didn't really care about.\n \nreply",
      "Holy cow was that forecast bad!It reminds me of a meeting long ago where the marketing team reported that oil was going to hit $400/bbl and that this would be great for business.  I literally laughed out loud.  At that price, gasoline would be about $18/gal and no one could afford to move anything except by ox cart.\n \nreply"
    ],
    "link": "https://philmckinney.substack.com/p/i-convinced-hps-board-to-buy-palm",
    "first_paragraph": ""
  },
  {
    "title": "Meta invests $14.3B in Scale AI to kick-start superintelligence lab (nytimes.com)",
    "points": 391,
    "submitter": "RyanShook",
    "submit_time": "2025-06-13T13:09:56 1749820196",
    "num_comments": 407,
    "comments_url": "https://news.ycombinator.com/item?id=44268197",
    "comments": [
      "https://archive.md/vuq7u",
      "The only way to understand this is by knowing: Meta already has two (!!) AI labs who are already at existential odds with one-another and both are in the process of failing spectacularly.One (FAIR) is lead by Rob Fergus (who? exactly!) because the previous lead quit. Relatively little gossip on that one other than top AI labs have their pick of outgoing talent.The other (GenAI) is lead by Ahmad Al-Dahle (who? exactly!) and mostly comprises of director-level rats who jumped off the RL/metaverse ship when it was clear it was gonna sink and by moving the centre of genAI gravity from Paris where a lot of llama 1 was developed to MPK where they could secure political and actual capital. They've since been caught with their pants down cheating on objective and subjective public evals and have cancelled the rest of Llama 4 and the org lead is in the process of being demoted.Meta are paying absolute top dollar (exceeding OAI) trying to recruit superstars into GenAI and they just can't. Basically no-one is going to re-board the Titanic and report to Captain Alexandr Wang of all people. Its somewhat telling that they tried to get Koray from GDM and Mira from OAI and this was their 3rd pick. Rumoured comp for the top positions is well into the 10's of millions. The big names who are joining are likely to stay just long enough for stocks to vest and boomerang L+1 to an actual frontier lab.\n \nreply",
      "This is exactly why Zuck feels he needs a Sam Altman type in charge. They have the labs, the researchers, the GPUs, and unlimited cash to burn. Yet it takes more than all that to drive outcomes. Llama 4 is fine but still a distant 6th or 7th in the AI race. Everyone is too busy playing corporate politics. They need an outsider to come shake things up.\n \nreply",
      "The corporate politics at Meta is the result of Zuck's own decisions. Even in big tech, Meta is (along with Amazon) rather famous for its highly political and backstabby culture.This is because these two companies have extremely performance-review oriented cultures where results need to be proven every quarter or you're grounds for laying off.Labs known for being innovative all share the same trait of allowing researchers to go YEARS without high impact results. But both Meta and Scale are known for being grind shops.\n \nreply",
      "These people should better make a lot of money while they can, because for most of them their careers may be pretty short. The half life of AI technologies is measured in months.\n \nreply",
      "Meta is struggling here for the same reason Microsoft couldn\u2019t stop the talent bleed to Google back in the day.Even if you\u2019re giving massive cash and stock comp, OpenAI has a lot more upside potential than Meta.\n \nreply",
      "Microsoft back in the day and today still doesn\u2019t pay top dollar. So you can\u2019t get top talent with 65th percentile pay.\n \nreply",
      "This is wrong. OpenAI has almost no upside now at these valuations and there is a >2 year effective cliff on any possibility of liquidity whereas Meta is paying 7-8 figures liquid.Metas problem is that everyone knows that it\u2019s a dumpster fire so you will only attract people who only care about comp which is typically not the main motivation for the best people.\n \nreply",
      "It's not a 2 year cliff: it's 6 months before vesting, then 2 years before you can sell.\n \nreply",
      "Effective cliff. What use is vested \u201cequity\u201d (ppus aren\u2019t even equity) that you cannot sell?\n \nreply"
    ],
    "link": "https://www.nytimes.com/2025/06/12/technology/meta-scale-ai.html",
    "first_paragraph": ""
  },
  {
    "title": "RISC-V in AI and HPC Part 1: Per Aspera Ad Astra? (eetimes.com)",
    "points": 9,
    "submitter": "fork-bomber",
    "submit_time": "2025-06-10T10:27:50 1749551270",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.eetimes.com/risc-v-in-ai-and-hpc-part-1-per-aspera-ad-astra/",
    "first_paragraph": ""
  },
  {
    "title": "Using computers more freely and safely (2023) (akkartik.name)",
    "points": 62,
    "submitter": "surprisetalk",
    "submit_time": "2025-06-13T17:27:20 1749835640",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=44270434",
    "comments": [
      "Original discussion (67 comments) https://news.ycombinator.com/item?id=36113115\n \nreply",
      "I'm sure this article resonates with many people; it doesn't resonate with me.I get value out of (and even enjoy) lots of software, commercial and otherwise (except for Microsoft Teams--that's an abomination).Ultimately, everything (not just software) is a trade-off. It has benefits and hazards. As long as the benefits outweigh the hazards, I use it. [The one frustration is, of course, when an employer forces a negative-value trade-off on you--that sucks.]I'm suspicious of articles that talk about drawbacks in isolation, without weighing the benefits: \"vaccines have side-effects\", \"police arrest the wrong people\", \"electric cars harm the environment\".Ironically, the best answer to many of the article's suggestions (thousands rather than millions, easy to modify, etc.) is to write your own software with LLMs. The future everyone wants is, I think, one where users can ask the computer to do anything, and the computer immediately complies. Will that bring about a software paradise free from the buggy, one-size-fits-none, extractive software of today? I don't know. I guess we'll see.We live in interesting times.\n \nreply",
      "This approach has been my central philosophy for years, and is why I dislike central app stores so much: they put significant downward pressure on hobby coders releasing their work, which leaves the store with a bunch of primarily commercial software, which aligns incentives around extracting data or money from users. F-Droid is a great counter-example that highlights this.I think my decisions have panned out pretty well, in a /r/stallmanwasright sort of way.  I caught a lot of side glances for using Linux back in the late 90s and early 00s (I wanted to load it onto some computers on LHD-4 during my tour aboard, but the command said it was a \"hacker\" operating system), but these days, looking at what Apple and Microsoft are doing, I'm thrilled to be using System76 machines for me and my kids.Emacs has been a consistent friend over the years, and I still go back to it for anything text-centric. It's made the transition to the LLM-era quite gracefully. Tiddlywiki has also been a reliable source of value over the years.I tend to not install apps for sites on my phones. They offer less control than a browser I can add uBlock to and just visit the site. Not always (I use the Amazon app, for example), but mostly.In general, I've cultivated an attitude of reverse-entitlement: sometimes I really want things, but I have to stay real with myself that I don't need them.  Some examples that folks will probably argue with, but are good illustrations of the idea:I'm a huge fan of VR, and have had amazing times in Beat Saber and a few other games. I bought Quest and Quest 2, but when Meta locked me out due to a SNAFU with the Oculus/FB account mess up, and I was unable to file a ticket to get the account unlocked (because I couldn't log in), I lost $1000 in hardware and a couple thousand in VR software, but I just walked away.  I realize the relationship was abusive, and that I didn't need Meta in my life. That was 2 years ago, and I still miss Beat Saber, but it was a good decision.I had a LinkedIn account, and gave my name and email when I signed up. When my phone fried and I didn't have backup MFA, they demanded my state-issued ID to let me back in (rather than, say, verifying by email).  I don't trust MS with my ID - they said they would delete it, but I didn't believe them (prior data breaches at ID vendors motivated me). But more importantly, it was an escalation: they didn't verify my identity when I signed up. So they should be trying to confirm that I'm the person who signed up. But they instead wanted me to verify I'm rpdillon, which is moving the goalposts. They're doing it as a transparent data grab. So I walked away. That was a few years ago; turns out I don't need LinkedIn!There are probably dozens of examples like this, but I'll stop here, since this is already too long.My core point here is: it turns out I don't need most of the stuff these companies offer, and they do seem to be getting increasingly abusive. I read about the WebRTC backdoor in Meta's apps last night, but I quit Facebook in 2009, because the writing was on the wall. I think the article offers a good perspective. This is quite at adds with opinions I read here all the time (\"Libreoffice is a useless replacement for Excel\", \"It's literally impossible to program unless I have my liquid retina display\", \"Unless I'm rendering at 144Hz, it's like a slideshow\", etc.), so it might be a _highly_ individual thing, but I thought it was worth mentioning, since it might be a fun discussion about how folks think of these tradeoffs.\n \nreply",
      "Upvote for Love and Lua!\n \nreply"
    ],
    "link": "https://akkartik.name/freewheeling/",
    "first_paragraph": "\n\n\n\n\n\n\n\n\n      How can we use computers more freely and safely?\n\n    \n\n\n\n\n        the punchline\n      \n      Prefer software:\n      \n with thousands rather than millions of users\n         that seldom requires updates\n         that spawns lots of forks\n         that is easy to modify\n         that you can modify\n      \n\n\n      These are my suggestions.\n\n      \n Prefer software with thousands rather than millions of users,\n         that doesn't change often,\n         that seems to get forked a lot,\n         that can be modified without specialized tools, and, ideally\n         that you can make small changes to. Yourself. In a single\n        afternoon.\n      \n\n      You don't have to do all this. Their benefits are additive, but acting\n      on even one of these suggestions is better than nothing. The suggestions\n      are arranged roughly in order of increasing effort and skill required.\n      Trying to follow one suggestion will pave the way for the others, should\n      you choose later"
  },
  {
    "title": "The Hat, the Spectre and SAT Solvers (2024) (nhatcher.com)",
    "points": 77,
    "submitter": "todsacerdoti",
    "submit_time": "2025-06-13T15:14:20 1749827660",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=44269289",
    "comments": [
      "Terrific article by Nicolas Hatcher!  Aperiodic tilings are fun to make from paper, wood, and ceramics.  I've cut tiles from ceramic field tiles.No surprise that concave cuts in ceramics are a high stress, so Kite and Dart tiles don't work very well (the dart is likely to crack).  Same is true for the Turtle, Hat, and Spectre.Rhombus tiles are everywhere convex, and the P3 Rhombus tiles are easy to cut in a diamond saw (or even a snap-cutter).  With a diamond band-saw, it's possible to make Penrose rhombs with curved (parabolic) edges.But cutting tiles from stock field tiles produces sharp surface edges -- you don't want these as bathroom floor tiles.  Also, you waste a lot of the field tile as scrap. To get \"friendly\" tile shoulders, I'm experimenting with making Penrose tiles directly from high-fired porcelain clay.\n \nreply",
      "Interestingly, when firing your own, you could also make Supertiles from combinatorial collections of Penrose or other aperiodic tiles.\n \nreply",
      "Do you have some photos? It may be a nice post.\n \nreply",
      "Hey, thanks!\nGood luck with your Penrose tiles. I wish I had the time myself :)\n \nreply"
    ],
    "link": "https://www.nhatcher.com/post/on-hats-and-sats/",
    "first_paragraph": "Software developer by trade, physicist by soulIn this blog post you are going to read about two things:Hopefully by the end of the post you will know a fair amount about the hat, the turtle and the spectres and have another powerful tool under your belt, SAT solvers.Thus, you can see this post either as an exercise in recreational mathematics or as an invitation to SAT solvers.The post is organized as follows. We first introduce the Hat and state the main result. Then we define what SAT solvers are. In section 3 we describe how we use the solver in wasm and apply it to the problem of solving a Sudoku problem as a warm up exercise in the next section. In section 5 we explain Craig Kaplan\u2019s idea of how to use SAT solvers to tile finite regions of the plane and apply it to the Hat. Section 6 introduces the Turtle, a second monotile capable of tiling the plane aperiodically. With all those constructions we build the Spectre, from deformations of Hats and Turtles, a true chiral aperiodic mo"
  },
  {
    "title": "Show HN: Tattoy \u00e2\u20ac\u201c a text-based terminal compositor (tattoy.sh)",
    "points": 148,
    "submitter": "tombh",
    "submit_time": "2025-06-13T14:04:35 1749823475",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=44268644",
    "comments": [
      "This looks really cool, I'd like to give it a go. The idea of taking a screenshot of the terminal and then parsing that to determine the true colour support is definitely novel, though perhaps so, because for me I can't get it to work. Are there any debug flags I can enable?So far it was able to take the screenshot correctly (https://ibin.co/8kaRr8TIanv2.png), however the parsing of that fails with the non-descript \"Palette parsing failed.\" error.Edit: enabled tracing at got this: https://paste.ee/p/ZyNxG9FK\n \nreply",
      "> The idea of taking a screenshot of the terminal and then parsing that\nto determine the true colour support is definitely novel,A better way to do this is to send `OSC 1 0 ; ? ST` (query foreground\ncolor), `OSC 1 1 ; ? ST` (background color), then `OSC 4 ; {n} ; ? ST`\nwhere {n} is the nth XTerm color.See: https://invisible-island.net/xterm/ctlseqs/ctlseqs.html#h4-O...\n \nreply",
      "OMG really!? That link is blocked for me for some reason. If that OSC code is widely supported it's going to make things sooooo much easier.\n \nreply",
      "It\u2019s supported by any xterm compatible terminal emulator. But like with most things in this domain, expect plenty of edge cases where it should work but doesn\u2019t.\n \nreply",
      "It\u2019s very widely supported from my experience. This is how asciinema captures terminal palette.\n \nreply",
      "Thanks for trying it out. It looks like either your terminal or screenshotter isn't faithfully rendering the pure red marker column (it's needed for calibration in the parser). The red should be #ff0000, but the screenshot is using #ea3323. I've made a Github issue to keep track https://github.com/tattoy-org/tattoy/issues/98 If you can add more details it'd really useful, I'm sure there'll be more people like you.\n \nreply",
      "Id probably have an easier time finding out about this project if it didn't full screen auto play videos as I scroll?\n \nreply",
      "I didn't intend for the videos to be fullscreen. They need to be small in order to save bandwidth. They're certainly autoplaying (to replicate GIF behaviour), but maybe there's a bug with them going full screen. What browser are you using?\n \nreply",
      "Firefox on ios. They are full the screen and auto play as soon as I get to whatever part of the page they are on.Tbh I think giving the user voice as to whether to play them would be a better experience anyway, but it's really unusable as is.\n \nreply",
      "I get the same experience on iOS using the OS web view - my guess is because iOS (and maybe android?) don\u2019t typically play videos in \u201cwindowed mode\u201d (for lack of a better term) outside of eg Google video snippets which seem to do some hacky stuff to keep you \u201cin Google\u201d while watching.Regardless of the fullscreen aspect, and understanding you wanted something jiff-like, I also don\u2019t care much for auto playing video. It doesn\u2019t matter too much if it\u2019s small (as this is intended), silent (as terminals typically are), and doesn\u2019t hoist control of my browser.Edit: forgot to say that this looks really cool, great work!Editedit: also forgot to mention that the thumbnails are super blurry on my phone, and after the one video took control of the screen, all the other thumbnails went black.\n \nreply"
    ],
    "link": "https://tattoy.sh",
    "first_paragraph": "Tattoy can generally be thought of as a framework for adding eye-candy to your terminal. It is purely text-based so works in any terminal emulator that supports true colour. \"Graphics\" is rendered with UTF8 half-blocks (\u2580,\u2584). Whilst most of its effects are for getting you street credibility it also has more powerful features based around its awareness of terminal contents. For example it can detect and auto adjust text contrast whilst remaining faithful to the terminal's palette.Tattoy works with your existing shell, theme and prompt, etc. It can always and immediately toggle between its effects and your normal terminal state, allowing for easy copy-pasting for example.Tattoy works with your existing shell, theme and prompt, etc. It can always and immediately toggle between its effects and your normal terminal state, allowing for easy copy-pasting for example.Perhaps the fanciest feature of Tattoy is its ability to render GPU shaders. It is designed to be able to run most shaders from "
  },
  {
    "title": "100 years of Zermelo's axiom of choice: What was the problem with it? (2006) (mietek.io)",
    "points": 93,
    "submitter": "Bogdanp",
    "submit_time": "2025-06-13T14:46:15 1749825975",
    "num_comments": 90,
    "comments_url": "https://news.ycombinator.com/item?id=44269002",
    "comments": [
      "In topology, if you have a continuous surjective map X --> Y, then it might have a continuous splitting (a map the other way which is a \"partial\" inverse in the sense that Y ---> X ---> Y is the identity) e.g. there are lots of splittings of the projection R^2 ---> R, you could include the line back as the x-axis but also the graph of any continuous function is a splitting.On the other hand, there's no continuous splitting of the map from the unit interval to the circle that glues together the two endpoints.So the category of topological spaces does not have the property \"every epimorphism splits.\"As the article mentions, the axiom of choice says that the category of sets has this property.So we can think of the various independence results of the 20th century as saying, hey, (assuming ZFC is consistent) there's this category, Set, with this rule, and there's this other category called idk Snet, that satisfies the ZF axioms but where there are some surjections that don't split, and that's ok too.Then whatever, if you want to study something like rings but you don't like the axiom of choice, define a rning to be a snet with two binary operations such that blah blah blah, and you've got a nice category Rning and your various theorems about rnings and maybe they don't all have maximal ideals, even though rings do. You're not arguing about ontology or the nature of truth, you're just picking which category to work in.\n \nreply",
      "Yeah, it's important to think of these axioms as choosing the rules of the game, rather than what intuitively makes sense. The real question is if playing the game  produces useful results.\n \nreply",
      "Axioms are also introduced in practical terms just to make proofs and results \"better\". Usually we talk in terms of what propositions are provable, saying that indicates the strength/power of these assumptions, but beyond this there are issues of proof length and complexity.For example in arithmetic without induction, roughly, theorems remain the same (those which can still be expressed) but may have exponentially longer proofs because of the loss of those `\u2200n P(n)`-type propositions.In this sense it does sometimes come back to intuition. If for all n we can prove P(n), then `\u2200n P(n)` should be an acceptable proposition and doesn't really change \"the game\" we are trying to play. It just makes it more intuitive and playable.\n \nreply",
      "> If for all n we can prove P(n), then `\u2200n P(n)` should be an acceptable propositionBut how can you prove that P(n) for all n without induction? Maybe I misinterpret what you're saying, or I'm naive about something in formal languages, but if we can prove P(n) for all n. then `\u2200n P(n)` just looks like a trivial transcription of the conclusion into different symbols.I think the crux of the matter is that we accept inductive arguments as valid, and so we formalize it via the inductive axiom (of Peano arithmetic). i.e., we accept induction as a principle of mathematical reasoning, but we can't derive it from something else so we postualte it when we come around to doing formalizations. Maybe that's what you mean by it coming down to intuition, now that I reread it...Poincar\u00e9 has a nice discussion of induction in \"On the nature of mathematical reasoning\", reprinted in Benacerraf & Putnam Philosophy of Mathematics, where he explicates it as an infinite sequence of modus ponens steps, but irreducible to any more basic logical rule like the principle of (non-)contradiction\n \nreply",
      "Rejecting induction could be quite useful if you want to be very precise about the implications of your constructions wrt. computational complexity.  This is of course only a mildly strengthened variant of the usual arguments for constructivism.\n \nreply",
      "> But how can you prove that P(n) for all n without induction? Maybe I misinterpret what you're saying, or I'm naive about something in formal languages, but if we can prove P(n) for all n. then `\u2200n P(n)` just looks like a trivial transcription of the conclusion into different symbols.Of course it is likely that an interesting result about all positive integers, that is \"really\" about positive integers, is proved by induction, but you certainly don't need induction to prove P(n): n = 1.n, or, more boringly, P(n): 0 = 0.  (These statements are obviously un-interesting, both in the human sense of the word and in the sense that they are just statements about semi-rings, of which the non-negative integers are an example.)My understanding is that the difference between \"For every positive integer n, I can prove P(n)\" and \"I can prove \u2200n.P(n)\" is that the former only guarantees that we can come up with some terrible ad hoc proof for P(1), some different terrible ad hoc proof for P(2), and so on.  How could I be sure I have all these infinitely many different terrible ad hoc proofs without induction?  I dunno, but that's all that the first statement guarantees.  Whereas the second statement, in the context of computability, guarantees that there is some computable function that takes a positive integer n and produces a proof of P(n); that is, there is some sort of guaranteed uniformity to the proofs.I think it may be easier to picture if connected with math_comment_21's analogy in  https://news.ycombinator.com/item?id=44269153: the analogous statements in the category of topological spaces (I think one actually has to work about topos, but I don't know enough about topos theory to speak in that language) about a map f : X \\to Y are \"every element of Y has a pre-image under f in X\" versus \"I can continuously select, for each element of Y, a pre-image of it under f in X\", i.e., \"there is a continuous pre-inverse g : Y \\to X of f.\"\n \nreply",
      "I\u2019m not sure what you mean by \u201ctheorems remain the same\u201d. If you take away induction from Peano arithmetic, you get Robinson arithmetic, which has many more models, including (from https://math.stackexchange.com/a/4076545):- \u2115 \u222a {\u221e}- Cardinal arithmetic- \u2124[x]\u207aObviously, not all theorems that are true for the natural numbers are true for cardinals, so it seems misleading to say that theorems remain the same. I also believe that the addition of induction increases the consistency strength of the theory, so it\u2019s not \u201cjust\u201d a matter of expressing the theorems in a different way.I would agree more for axioms that don\u2019t affect consistency strength, like foundation or choice (over the rest of the ZF axioms).\n \nreply",
      "If I had to write again I might say \"same theorems about natural numbers\" and capitalize ROUGHLY. It is a conversation, what exactly I am weaseling around (not just nonstandard model theoretic issues), and I take your caveat about consistency strength - with that said would you still call it misleading? Why is it that eg x+y=y+x for x y given takes exponential length proof in Robinson compared to PA? For the reason stated, which is true in a very broad sense.\n \nreply",
      "Good point. I would argue, however, that having nicer proofs is a \"useful\" result of the game.\n \nreply",
      "Spoken like a true formalist.It doesn't really have to mean anything when we say that the reals are a larger set than the natural numbers - that's just the conclusion of the game that we are playing.What fraction of people who \"know\" that there are more reals than natural numbers, do you think really understand that this is not an eternal verity of mathematics, but only a conclusion that follows from a particular set of rules that we're playing the mathematics game with?\n \nreply"
    ],
    "link": "https://research.mietek.io/mi.MartinLof2006.html",
    "first_paragraph": "Machine IntuitionistPer Martin-L\u00f6f2006WORK IN PROGRESSCantor conceived set theory in a sequence of six papers published in the Mathematische Annalen during the five year period 1879\u20131884. In the fifth of these papers, published in 1883,1 he stated as a law of thought (Denkgesetz) that every set can be well-ordered or, more precisely, that it is always possible to bring any well-defined set into the form of a well-ordered set. Now to call it a law of thought was implicitly to claim self-evidence for it, but he must have given up that claim at some point, because in the 1890s he made an unsuccessful attempt at demonstrating the well-ordering principle.2The first to succeed in doing so was Zermelo,3 although, as a prerequisite of the demonstration, he had to introduce a new principle, which came to be called the principle of choice (Prinzip der Auswahl) respectively the axiom of choice (Axiom der Auswahl) in his two4 papers5 from 1908. His first paper on the subject, published in 1904, co"
  },
  {
    "title": "Apple's Liquid Glass is prep work for AR interfaces, not just a design refresh (omc345.substack.com)",
    "points": 155,
    "submitter": "lightningcable",
    "submit_time": "2025-06-13T19:44:58 1749843898",
    "num_comments": 174,
    "comments_url": "https://news.ycombinator.com/item?id=44271630",
    "comments": [
      "> The move from skeuomorphic design in iOS 6 to the stark minimalism of iOS 7 sparked similar debates about usability and aesthetic merit. [...] Yet within two years, the entire industry had adopted flat design principles, from Google's Material Design to Microsoft's Metro language.That's quite a rewrite of history considering Windows Phone and Microsoft's Metro interface launched a full three years before Apple's move to a flat design in iOS 7.\n \nreply",
      "FWIW, I always liked the Windows Phone OS design. Its text first minimalism was refreshingly useful. It was a big leap ahead from Windows Mobile. I think it had something worthwhile to offer.\n \nreply",
      "For sure, so many of its features were far ahead of the competition. Sleek minimilist UX, live tiles, Qi wireless charging, kids mode, Cortana, search within settings (so simple yet no one did it at the time). Continuum let you plug your phone into a monitor and use it like a full Windows desktop (many years before Samsung Dex and other similar efforts on Android). \"Universal apps\" that could run on desktop/mobile/web. Sucks that Microsoft fumbled it so bad.\n \nreply",
      "Beyond just the design, it was also an amazingly efficient OS. I had a cheap Lumia that had much lower specs than contemporary Samsung and iPhone flagship smartphones (500MB vs 1GB+ RAM IIRC) yet it was amazingly smooth and responsive, much smoother than the other two. Android especially, and to a lesser extent iOS, would get laggy and stutter while scrolling after a few major version updates, but Windows Phone stayed snappy even after the phone was 3+ years old.This also made the battery life much better. (Although whenever I mentioned this, the usual retort I got was, of course the battery life would be better if there were no apps to consume it...)\n \nreply",
      "It was \"efficient\" by leaving almost no memory for user applications. I used two phones with 512 MBs of RAM each, one Nokia-something (620 or 625), and the other Asus-something (completely forgot the model, but it was on Android 4 and then 5).WP would offload applications from RAM as soon as you switched into another application. It was impossible to multitask \u2014 you're writing a comment on a message board, switch into a dictionary to quickly look up a word, switch back... and the state is gone. If you're lucky and the application was written correctly, you would only have to wait for 5-10 seconds before you get your half written comment back. If not (which was the norm for the stuff I used), well...The second Android phone had none of these problems, not remotely to the same degree.It was such a widespread problem that it quickly became a meme on forums.\n \nreply",
      "Luckily it is immortalized in GTA5, when playing as Trevor at least. I found it the easiest phone to use in that game.\n \nreply",
      "Microsoft gave up on a phone operating system far too early.\n \nreply",
      "Nah they just joined the race too late. Remember that Steve Ballmer was laughing at and dismissing the iPhone when it launched (\"it's too expensive, no one will use it, it doesn't even have a keyboard\"). Microsoft continued pushing Windows Mobile at that time and even spent $1B+ acquiring Danger and releasing Kin (remember that disaster?). Then Windows Phone 7 finally launched in 2010 and was rebooted again in 2012 with Windows Phone 8. By that time the mobile OS market was a duopoly, and neither users nor developers nor manufacturers cared for a third platform.\n \nreply",
      "When discussing disasters, it\u2019s impossible to ignore BlackBerry. They crafted solid devices, and their downfall from a hardware company is a tragic one. They grew too big and failed to adapt in times of \u201cwar\u201d with a diminishing market share. However, I firmly believe they could have maintained a loyal  user base over the years, at least large enough to allow them to fight another day.Their user interface was a true gem - beautiful yet functional. The devices were incredibly fast, and the optical cursor was a revelation. I genuinely believe the way the trackpad cursor functions on the iPad is inspired by BlackBerry\u2019s design.\n \nreply",
      "My only experience with BB was awful, though it was at the perfectly wrong time. I was responsible for developing an app for the Storm and it was really the worst of both worlds.\n \nreply"
    ],
    "link": "https://omc345.substack.com/p/from-skeuomorphic-to-liquid-glass",
    "first_paragraph": ""
  },
  {
    "title": "A Study of the Winston Red: The Smithsonian's New Fancy Red Diamond (gia.edu)",
    "points": 9,
    "submitter": "bookofjoe",
    "submit_time": "2025-06-13T22:13:14 1749852794",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.gia.edu/gems-gemology/spring-2025-winston-red-diamond",
    "first_paragraph": "\r\n                  Gabriela A. Farfan, Ulrika F.S. D\u2019Haenens-Johansson, Stephanie Persaud, Elo\u00efse Gaillou, Russell C. Feather II, W. Henry Towbin, and Daniel C. Jones\r\n                Red diamonds are among the rarest gems on Earth, especially Fancy red diamonds that are pure red and unmodified by brown, orange, or purple. At 2.33 ct, the Winston Red diamond is the fifth-largest Fancy red diamond known to exist and the only Fancy red diamond on public exhibit. On April 1, 2025, it was unveiled in a new exhibit at the Smithsonian National Museum of Natural History in Washington, DC. This is the first scientific and historical study conducted on this noteworthy stone. Optical observation along with spectroscopic, cathodoluminescence, and photoluminescence analyses confirmed the presence of plastic deformation bands and dislocation network patterns that classify the Winston Red as a type IaAB (A) Group 1 \u201cpink\u201d diamond and indicate that it underwent significant pressure and temperature c"
  },
  {
    "title": "When random people give money to random other people (2017) (quomodocumque.wordpress.com)",
    "points": 68,
    "submitter": "munificent",
    "submit_time": "2025-06-13T16:49:46 1749833386",
    "num_comments": 48,
    "comments_url": "https://news.ycombinator.com/item?id=44270144",
    "comments": [
      "There's a nice interactive simulation here: https://joshworth.com/jpw/does-this-simulation-explain-why-l...And a blog post here: https://www.decisionsciencenews.com/?p=6221\n \nreply",
      "\"inequality of wealth rapidly appears and then persists (though each individual person bobs up and down from rich to poor.\"Does still sound fair to me.\n \nreply",
      "This is only when no other effects/processes are accounted for. If every participant keeps their current funds in an account with the same interest rate, then the distribution permanently skews so that there is no longer bobbing. The rich stay rich. See this more detailed simulation linked in another comment: https://joshworth.com/jpw/does-this-simulation-explain-why-l...\n \nreply",
      "The paper this is based on [1] does a good job of spelling out:The instantaneous distribution of money among the agents of a system should not be confused with the distribution of wealth. The latter also includes material wealth, which is not conserved, and thus may have a different (e.g. power-law) distribution.The simulation might be a good model of how money distribution works, but it doesn't reflect how wealth distribution works. The simulation only works because money can neither be created nor distroyed (in the simulation, at least). Wealth can be so we shouldn't expect the simulation to predict its behavior.[1]: https://physics.umd.edu/~yakovenk/papers/EPJB-17-723-2000.pd...\n \nreply",
      "Discussion (555 points, 2017, 241 comments) https://news.ycombinator.com/item?id=14729400\n \nreply",
      "I remember that discussion because of the comment [1] pointing out that the rich people don't stay rich forever - the distribution may be skewed but that doesn't mean that the richer get richer.[1] https://news.ycombinator.com/item?id=14729930\n \nreply",
      "Only if no interest is accrued on assets. When interest is introduced, there is no longer shuffling.\n \nreply",
      "Nonsense. There is a big difference between the implications of the following claims:1. The specific people that are currently rich will become richer.2. The people that are rich at any given instant in the future will be richer than the people that are rich at the current instant.\"The rich are getting richer\" almost always means #2, but it depends on the context. #1 may be more interesting to individuals competing for wealth, or for an assessment of individual mobility, but #2 is much more relevant when describing the aggregate conditions of a society. For example, the Gini Coefficient depends on #2, not #1.I don't think it's a stretch to say it's dishonest to presume the first meaning.\n \nreply",
      "> Nonsense. There is a big difference between the implications of the following claims:> 1. The specific people that are currently rich will become richer.> 2. The people that are rich at any given instant in the future will be richer than the people that are rich at the current instant.> \"The rich are getting richer\" almost always means #2, but it depends on the context. #1 may be more interesting to individuals competing for wealth, or for an assessment of individual mobility, but #2 is much more relevant when describing the aggregate conditions of a society. For example, the Gini Coefficient depends on #2, not #1.> I don't think it's a stretch to say it's dishonest to presume the first meaning.When I see people say \"the rich are getting richer\" - say, about tax cuts for high-income earners or businesses - it's always been definition 1. In the case of tax cuts, the action is expected to make the specific current rich people get richer. Or, when used as a more general idiom about the way of the world, that having money gives you a huge advantage in terms of making more money that means the gap is likely to widen over time.Some other places interpreting the idiom that way:https://www.phrases.org.uk/meanings/the-rich-get-richer-and-... \"the inevitability of the literal truth that, the richer you are the more relatively rich you become and, for the poor, vice-versa\"https://en.wikipedia.org/wiki/The_rich_get_richer_and_the_po... \"Shelley remarked that the promoters of utility had exemplified the saying, \"To him that hath, more shall be given; and from him that hath not, the little that he hath shall be taken away. The rich have become richer, and the poor have become poorer; and the vessel of the State is driven between the Scylla and Charybdis of anarchy and despotism.\"[1] It describes a positive feedback loop (a corresponding negative feedback loop would be e.g. progressive tax).\"So it's hardly dishonest to assume the first meaning. Where are examples in common discourse of it frequently meaning the latter?\n \nreply",
      "Essentially similar to \u201cafter N throws of a coin, what will the difference be between heads and tails?\u201d. It will most likely be away from 0, and probably large.\n \nreply"
    ],
    "link": "https://quomodocumque.wordpress.com/2017/06/27/when-random-people-give-money-to-random-other-people/",
    "first_paragraph": "A post on Decision Science about a problem of Uri Wilensky\u2018s has been making the rounds:Imagine a room full of 100 people with 100 dollars each. With every tick of the clock, every person with money gives a dollar to one randomly chosen other person. After some time progresses, how will the money be distributed?People often expect the distribution to be close to uniform. \u00a0But this isn\u2019t right; the simulations in the post show clearly that inequality of wealth rapidly appears and then persists (though each individual person bobs up and down from rich to poor.) \u00a0What\u2019s going on? \u00a0Why would this utterly fair and random process generate winners and losers?Here\u2019s one way to think about it. \u00a0The possible states of the system are the sets of nonnegative integers (m_1, .. m_100) summing to 10,000; if you like, the lattice points inside a simplex. \u00a0(From now on, let\u2019s write N for 100 because who cares if it\u2019s 100?)The process is a random walk on a graph G, whose vertices are these states and wh"
  },
  {
    "title": "How the Alzheimer's Research Scandal Set Back Treatment 16 Years (2022) (discovermagazine.com)",
    "points": 58,
    "submitter": "walterbell",
    "submit_time": "2025-06-13T21:53:13 1749851593",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=44272637",
    "comments": [
      "We work in neurostimulation and sleep, and collaborate with Alzheimer's researchers.The Amyloid hypothesis is not disproven, it is still ONE of the primary candidates for AD.The problem with any Alzheimer's research is that the disease is still not well understood. It is likely that multiple diseases are being bundled in as a single disease. The tests for AD, are somewhat rudimentary. Beginning with psychological and neurological tests, the blood work to rule out other conditions, followed by a PET scan to look for brain atrophy, and CSF measures for amyloid and tau levels.It seems almost like they're basically ruling out any disease we can actually measure for and then if it isn't one of those, it's AD.Does this mean the Amyloid hypothesis is wrong? Unlikely. Is it incomplete? Absolutely!But articles shouting that all the research should be thrown out are not helpful .The AD community know that they don't understand the disease, and though therapeutics have been mostly focused on amyloid and tau, research into how the disease works continues.\n \nreply",
      "Could the Amyloid/tau hypothesis be a cart before the horse situation?  It is my understanding that the current hypothesis is that the buildup of these proteins causes Alzheimer's.  Could it be that Alzheimer's causes these proteins to build up?\n \nreply",
      "It could be. That's why we need more research. Well funded and not vilified.\n \nreply",
      "Its not the only disease this has happened to either. ME/CFS has been railroaded by European governments that funded only psychological research despite numerous pathological findings and better theories of the disease, this prejudiced treatment started in the 1970s and persists to this day including the corruption of the PACE trial results which researchers tried to hide the data of.Research fraud in medicine is alarmingly common.\n \nreply",
      "There\u2019s money to be made; don\u2019t let facts stand in the way of profits!\n \nreply",
      "This argument gets invoked a lot when it comes to medical dishonesty, but I really don't think it applies in the case of ME/CFS. If we could find the pathology behind the condition, there is huge money to be made in pharmaceutical treatments. Just look at the enormous amount of money being made treating auto-immune illnesses with Humira/Skyrizi/Xeljanz/etc, treating diabetes with GLP-1 agonists and CGMs, and treating obesity with GLP-1 agonists (and depression before all that!). Sometimes treating the chronically ill is the most profitable option.\n \nreply",
      "ME/CFS shows up as immune system dysfunction and for most people is likely downstream from dysautonomia. Given the huge overlap of people with long covid and hEDS, and that long covid presents as dysautonomia, the math likely works out that most people with ME/CFS actually have hEDS. It\u2019s massively underdiagnosed. It\u2019s likely one of many TNXB SNPs, these are overlooked for a few main reasons - it\u2019s considered too common for a rare condition and it\u2019s hard to sequence and only detectable with modern high quality deep WGS.The other thing is that GLP-1As actually do appear to help with autoimmune conditions and has been in my case and many others I know of to be one of the most effective medications for treating ME/CFS.I wish it was a lack of a profit motive that caused medical researchers to be so off base but it appears what is more likely that they\u2019re not very good with stats, as being good at stats would get in the way of all the \u2018accidental\u2019 p-hacking they\u2019re so addicted to.\n \nreply",
      "I agree, but will the money be made by the same people? Like, if a psychologists group stands to benefit from treating it they wouldn't reap the benefit of the pharma research. Kind of like how the alcohol industry lobbies against weed or other possible substitutes. Or the car companies buying out the rail roads and then shutting them down to prevent competition. There is just so much inefficiency from competing industries sabotaging each other. You are also making a leap that understanding the cause is physiological means that there is a drug you could sell and not cost billions of dollars to get to market without risk. I always wonder about the PReP drugs for treating AIDS was really the best outcome instead of aiming for the cure. Definitely tradeoffs with a lot of money one way or another.\n \nreply",
      "The psychologist lobby is not as powerful as you seem to think. Pharmaceutical companies have brought numerous drugs to market that have proven to be more effective than talk therapy for certain serious chronic mental health conditions.\n \nreply",
      "I think that's reasonable. I think there's definitely an economy-wide bias by vested interests against disruption, which gets us stuck in local maxima, sometimes for centuries.\n \nreply"
    ],
    "link": "https://www.discovermagazine.com/the-sciences/false-alzheimers-study-could-set-research-back-16-years",
    "first_paragraph": "In 2006, a landmark study in\u00a0Nature\u00a0identified a possible cause of Alzheimer\u2019s disease. For almost 16 years, this study influenced how scientists approached Alzheimer\u2019s and how major research grants were given.But in the summer of 2022, the editors of\u00a0Nature\u00a0issued a chilling disclaimer. There was concern regarding the images that accompanied the article. An investigation was underway, and readers were urged to \u201cuse caution\u201d when relying on the results.A whistleblower had come forward and said the images appeared to have been manipulated, meaning the results that guided scientists were possibly wrong.In the U.S., Alzheimer\u2019s now afflicts\u00a0more than six million\u00a0people aged 65 and older. If the accusations are true, it\u2019s possible that inaccurate research was influencing significant research time and money to help these patients.The study\u00a0at the center of the controversy was published in 2006 in\u00a0Nature\u00a0and co-authored by eight researchers from the University of Minnesota, John Hopkins Univ"
  },
  {
    "title": "High-speed fluorescence light field tomography of whole freely moving organisms (optica.org)",
    "points": 31,
    "submitter": "PaulHoule",
    "submit_time": "2025-06-10T21:43:33 1749591813",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://opg.optica.org/optica/fulltext.cfm?uri=optica-12-5-674&id=570897",
    "first_paragraph": "...but your activity and behavior on this site made us think that you are a bot.Note: A number of things could be going on here. Please solve this CAPTCHA to request unblock to the website \n\t\tYou reached this page when trying to access\n\t\thttps://opg.optica.org from\n\t\t52.225.28.237 on\n\t\tJune 14 2025, 01:41:18 UTC\n"
  }
]