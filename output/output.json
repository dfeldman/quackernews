[
  {
    "title": "Non-computability of solutions of certain equations on digital computers (2022) (arxiv.org)",
    "points": 39,
    "submitter": "lisper",
    "submit_time": "2024-08-03T23:02:33",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=41149931",
    "comments": [
      "Happy to see this made it to the front page.  I submitted it because I'm hoping someone here will be able to understand it well enough to explain its significance.  Is this Big News or a parlor trick?  If it really is an example of an uncomputable physical process, that would seem to me to be earth-shattering, and yet this result does not appear to have made a splash.  It is based on a construction of J. Myhill of a continuous recursive (and hence computable) function whose derivative is not computable.  The proof is non-constructive, and it assumes the existence of a partially decidable set called A which somehow (this is the part I don't quite understand) results in a computable function despite the fact that its derivative can't be computable because that would imply A is decidable.  Any clues would be much appreciated.[UPDATE] Let me put this more succinctly: how is it possible that there can be a continuous computable function whose derivative is uncomputable?  What exactly is it about this function that makes numerical differentiation fail?[UPDATE2] Here is a link to the Myhill paper for easy reference.https://projecteuclid.org/download/pdf_1/euclid.mmj/10290006...It's remarkably short, only two pages.\n \nreply",
      "> how is it possible that there can be a continuous computable function whose derivative is uncomputable?For others: The original paper is short and very readable - https://projecteuclid.org/journalArticle/Download?urlId=10.1...The intuition seems to be that computability of a function is actually approximability, and derivatives can be large even when the function is very small, by having the large slope confined to a very small area. So by constructing a function f that contains an uncomputable (but recursively enumerable!) set encoded in the derivative by strategically placing little bumps, but having the bumps grow exponentially smaller, Myhill was able to construct a function that was easily approximable but whose derivative wasn't.The key detail here is that the bumps grow exponentially smaller in the order the  uncomputable set appears in its enumeration. This allows Myhill to construct a sequence of computable functions that (uniformly) approximate f to within 2^(-n). If the bumps weren't ordered in this way, we wouldn't know how far down the sequence to go to approximate f to within 2^(-n). The ordering lets us just look at the first n elements of the uncomputable set.Why doesn't this allow us to approximate the derivative? Because at any x, we don't know how well we have to approximate f in order to approximate f'(x), so we need knowledge of the entire uncomputable set, which we cannot have.\n \nreply",
      "Is it specifically the fact that it's a physical process that makes it earth shattering?Physical models of computers ( https://en.wikipedia.org/wiki/Billiard-ball_computer ) have been around for a long time, which means that noncomputability has been tangible for a long time.I might be numb, though, to the fact that much of what want to understand can't be. If the universe isn't discrete, then it seems like noncomputability is the default.\n \nreply",
      "> Big NewsThe paper is from May 2022. Don't you have the answer to whether it's big news? Whether the news is big is (usually) apparent/assessed when it is new.\n \nreply",
      "Two years it not that old, and the topic is obscure enough that it could plausibly have gone unnoticed.  Bell's inequality went virtually unnoticed when it was first published.My money is definitely on parlor trick, but I would still like to understand why.\n \nreply",
      "This reminds me of this reddit question https://www.reddit.com/r/askmath/comments/1eakt5c/if_you_pic...\n\"If you pick a real number from 0 to 1, what is the probability that it is rational?\"and the first response,\n\"\nWith a uniform random process over an interval (a, b), you can find the probability of picking an element from a set X by integrating 1/(b - a) over the set X.Since the (lebesgue) integral of the rational numbers is 0, the probability of picking a rational number is 0. \n\"\nAnd that kinda broke my brain for an afternoon.So, when a computer represents a number, it's always rational. You can have a lot of memory, but always finite.Now, you can go further, and represent things symbolically, like sage math or a bunch of other systems. But (I believe) at the end of the day Godel always wins. There are always true statements that can't be proved. If I remember right, computable stuff is a subset (maybe improper?) of provable stuff.Looks interesting. Thank you for posting it. I'll try to read it later. Although, I'm a math tourist. I probably won't be able to give you any grand insight.\n \nreply",
      "I haven\u2019t read the whole article yet, but I\u2019m guessing it boils down to a Halting Problem?And I assume if a Turing machine can\u2019t, neither can a human. If that is not the case, something smells fishy in Denmark.\n \nreply",
      "why is it impossible to decide whether x<0, x=0, or x>0 as in Example 1?\n \nreply",
      "This is better known as the Table Maker's Dilemma.Say you have some computable number p, that means you can compute a (rational) approximation p' to p within any given tolerance eps > 0 (i.e. you know |p - p'| < eps). To determine whether p > 0, p = 0, or p < 0, you compute an approximation p' to a certain tolerance eps. If p' > eps then you know p > 0, if p < -eps then you know p < 0, otherwise you need a better approximation... Without further knowledge about p, there is no point where you can assert p = 0.\n \nreply",
      "Only x=0 is undecidable.  It's because you have to check an infinite number of digits past the decimal point to see if all of them are zero, and that will not halt.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2205.12626",
    "first_paragraph": "Want to see access to research regardless of disability? Sign up for the arXiv Accessibility Forum in September and Learn more.Grab your spot at the free arXiv Accessibility ForumHelp | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "\"We ran out of columns\" (jimmyhmiller.github.io)",
    "points": 865,
    "submitter": "poidos",
    "submit_time": "2024-08-03T12:26:03",
    "num_comments": 323,
    "comments_url": "https://news.ycombinator.com/item?id=41146239",
    "comments": [
      "When I started at my first company, they had a very complex VB application running on dozens of customers around the country, each having some particular needs of course.\nThere was a LOT of global variables (seemingly random 4 uppercase letters) controlling everything.At some point, the application had some bugs which were not appearing when the application was run in debug mode in Visual Studio. The solution was obvious: installing Visual Studio for each customer on site and teaching the users to run the app in debug mode from Visual Studio. I don't know how they convinced the users to do this and how they managed with the license but it was done.What happened next was even worse.There was no version control of course, the code being available on a shared disk on the local network of the company with the code copied over in multiple folders each having its own version, with no particular logic to it either, V1, V2, V2.3, V2a, V2_customer_name, V2_customer_name_fix, ...After that, when there was a problem for a customer, the programmer went there to debug and modified the code on site. If the bug/problem was impacting other customers, we had to dispatch some guys for each customer to go copy/edit the code for all of them. But if the problem was minor, it was just modified there, and probably saved on the shared folder in some new folder.What happened next was to be expected: there was no consensus on what was the final version, each customer having slightly different versions, with some still having bugs fixed years before for others.\n \nreply",
      "This is amazing. I can so well imagine a bright young hire joining that team, helpfully offering to \"setup this thing called git\" only to be laughed out of the meeting by all the \"senior\" staff.\n \nreply",
      "Astonishingly, It took a long time for revision control to become widespread.Around 1991 when Cygnus had 6-7 employees and was based in the apartment complex where I lived, none of the GNU codebase was hosted in any sort of revision control.  Everything was FTPed around as obscurely named tarballs.  We had gathered something like 27 different forks of gdb floating around the net, for example.  This was back when forking was generally considered a tragedy, something I managed to change five or six years later).Rich Pixley came and said \u201call of our code should be in revision control, and I want to use this newfangled thing called CVS.\u201d  Michael was OK with it but John and I were steadfastly opposed.  We agreed to an experiment, grudgingly, subject to a whole whiteboard of absurd conditions (\u201cmust be transparently integrated into emacs so we don\u2019t have to know it\u2019s there).Pixley agreed to all of that and then ignored all of it completely.  It was immediately such a win that everybody adopted it without complaint, including us two obstreperous a-holes.A few years later a preventable crisis was how revision control first became client-server.\n \nreply",
      "Why is it astonishingly?Git is not a natural development at all. Obviously, it is a standard right now.But I as a hobby coder at my teens I started out with FileZilla and copying over index2.php, index3.php, index_final.php, index_final_2.php and all of it worked well enough when at that point.I took a little break from that hobby, and it still took me a lot of time to build intuition around Git when I started out professionally.Obviously now that I have over a decade of professional experience I think it's a no brainer, but I don't think it's natural to understand it as it's suggested to you. I at least kind of had to go along with it and trust that it's good. It felt frustrating at first as many other things are.\nGit as a thing was FAR more difficult for me to understand in my 20s than PHP and how to do anything with it was for me at teens. Coding is raw logic, Git is about imagining teamwork at scale, which is a whole other thing.The funny thing is that I feel like I was overall still more productive when I was at my teens building stuff solo with FileZilla and PHP, compared to a corporate environment now with all the processes and 1000s of engineers.\n \nreply",
      "> Why is it astonishingly?Git is still a baby as these things go.  Software version control systems go back to the early 1960s and descended from manual processes used to manage documents in large organizations, blueprints, and such from the early 20th century.  Don\u2019t imagine the Manhattan Project functioned by just passing around bundles of paper!  There were staffs of people whose job was to manage document control!  And I don\u2019t mean \u201ccontrol\u201d in the sense of secrecy, but making sure everybody was on the the same revision at the same time.And for a field so steeped in building its own tools and automating human effort in development it is indeed astonishing revision control took so long to be accepted.\n \nreply",
      "> Git is not a natural development at all. Obviously, it is a standard right now.Most standard tools we use today are not obvious to beginners nor would they be the first thing that beginners reach for.But senior developers can understand the problems that they claim to address, and why they are important and common problems.\n \nreply",
      "> Git is not a natural development at all. Obviously, it is a standard right now.Git is actually an unnatural development.  Its UI is atrocious.  And it worked like crap on Windows for forever.Over time, I taught non-computer people who used Windows all of CVS, Subversion, and Mercurial.  They got each one and why things were better.  The first time they had to recover something, they really got it.  Source control got out of their way and was generally fine.Then Github won due to VC cash dumping and foisted Git on all of us.  Source control was no longer fine.Thankfully, there is now Jujutsu(jj) which works on top of the Git storage layer so you can forget about the craptastic UI of Git.  Source control is now fine again.\n \nreply",
      "> Git[\u2018s] UI is atrocious.Well, that\u2019s in the eye of the beholder.  Yes, I hate the \u2018program command\u2019 syntax (\u201cgit add\u201d, \u201cgit commit\u201d etc) but I just call git-add etc and for me those commands are pretty clear.But I understand how git works.  I imagine most people treat it as a black box and then its behavior probably is rather obscure.  I don\u2019t think it was intended for that use case.\n \nreply",
      "GitHub won because they built a great product.  It was better than all the alternatives and people flocked to it for that reason.Git itself is mediocre, and some other DVCS like Mercurial could have won out, although HgHub really doesn't have the same ring to it.  The halo effect from the Linux kernel was also a factor.  But the VC cash dump and Microsoft buyout came later, people used GitHub because it was better than the rest.\n \nreply",
      "git didn't win because of GitHub. Sure, GitHub helped a lot. But actually, it's because its a DVCS, and it does branching and merging way better than anything I've seen so far.You can thank BitKeeper for all of this, and Andrew Tridgwell for forcing Linus Torvalds into creating git.\n \nreply"
    ],
    "link": "https://jimmyhmiller.github.io/ugliest-beautiful-codebase",
    "first_paragraph": "Oh the merchants2 table? Yeah, we ran out of columns on merchants, so we made merchants2When I started programming as a kid, I didn't know people were paid to program. Even as I graduated high school, I assumed that the world of \"professional development\" looked quite different from the code I wrote in my spare time. When I lucked my way into my first software job, I quickly learned just how wrong and how right I had been. My first job was a trial by fire, to this day, that codebase remains the worst and the best codebase I ever had the pleasure of working in. While the codebase will forever remain locked by proprietary walls of that particular company, I hope I can share with you some of its most fun and scary stories.In a large legacy system, the database is more than a place to store data, it is the culture maker. The database sets the constraints for how the system as a whole operates. It is the point where all code meets. The database is the watering hole. In our case, that wateri"
  },
  {
    "title": "Reverse engineering XC2064, the first FPGA  (2020) (righto.com)",
    "points": 16,
    "submitter": "vinnyvichy",
    "submit_time": "2024-08-03T22:51:31",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=41149882",
    "comments": [
      "I don't know why this popped up on HN after four years, but author here to answer your questions...\n \nreply",
      "What was the XC2064 used in, as far as like commercially available products or similar, if you know off hand?  I know some network cards and things like that have FPGAs.\n \nreply",
      "For example 1995 Snappy Video Snapshot LPT dongle by Paul Montgomery (Newtek co-founder https://en.wikipedia.org/wiki/Paul_Montgomery#Play_Inc.). @TubeTimeUS on twitter did a Snappy deep dive, teardown and reverse engineering down to schematic and figuring out remarked \"PLAY HD-1500\" main chip is just a XC2064 FPGA. PLL + 30msps ADC + special 2Mbit video ram capable of holding whole field = this thing grabs whole one field of video all at once after perfectly synchronizing to 14.318MHz video clock.https://threadreaderapp.com/thread/1301990455182155776wiki: \"Snappy was the first mainstream video input device for Windows personal computers, with Play selling over US$25 million of Snappy in its first year.[9] Byte Magazine awarded Snappy its Technical Excellence award in December 1995, stating \"Every once in a while, we see a product so impressive, it makes us rethink an entire category. That was certainly the case with Play Inc.'s Snappy.\"[10]\"\n \nreply",
      "It's because someone submitted it and more people voted on it. Things often take time to get randomly noticed by someone who also happens to habitually submit things to HN.\n \nreply"
    ],
    "link": "https://www.righto.com/2020/09/reverse-engineering-first-fpga-chip.html",
    "first_paragraph": ""
  },
  {
    "title": "Reverse engineering the 59-pound printer onboard the Space Shuttle (righto.com)",
    "points": 179,
    "submitter": "chmaynard",
    "submit_time": "2024-08-03T16:43:09",
    "num_comments": 56,
    "comments_url": "https://news.ycombinator.com/item?id=41147643",
    "comments": [
      "The drum for the Shuttle teleprinter replaces 10 ASCII special characters with symbols that are more useful to the Shuttle, such as Greek letters for angles.Specifically, the characters ;@[\\]^!\"#$ are replaced by \u03b8\u0081\u0081\u0081\u0081~\u03b1\u03b2\u0394\u0081.Sigh... This ancient printer could render them, but apparently the Chrome browser on my modern Android cannot.\n \nreply",
      "Somewhere between when this was originally written and when it was delivered to your browser the text here was destroyed.I don't see how that's the fault of Chrome or Android.\n \nreply",
      "Author here if anyone has questions...\n \nreply",
      ">often printing thousands of lines per flight>with a Shuttle flight costing $27,000 per pound, putting the 59-pound teleprinter in space cost over $1.5 million per flight.That budget calculation is just the weight of the printer itself. I am curious how much blank paper was flown to feed it, and how this was decided? From the font size shown in photo we can assume it was at least several more pounds of paper for \"thousands of lines\" to be true.\n \nreply",
      "The printer took a big roll of paper, maybe 5 inches in diameter (although I don't have it here to check). I was a bit surprised that they didn't trim the roll down to the minimum size necessary, but just used a full roll. I guess they didn't want to waste paper :-) Also, I'm surprised that they used cheap-looking yellow paper rather than white paper, but maybe there's a reason teletypes used yellow paper. (The paper hasn't yellowed due to age; it's yellow in contemporary photos.)\n \nreply",
      "I'm curious what the actual marginal cost of an additional pound on the shuttle is - surely you can't just divide?\n \nreply",
      "> and a ROM that holds its program codeDid you manage to dump the ROMs? (I understand from your article that article there are 3, a 4KB ROM on the CPU card, and two 8KB ROMs on the word processor card)\n \nreply",
      "I'd like to dump the ROMs, but they are soldered in and covered with conformal coating, so it would be moderately destructive to remove them and dump the contents.\n \nreply",
      "Is the drum rotating smoothly at constant RPM, or is it stopping briefly (using gears [1] similar to those found in analog movie projector) when hammers hit the paper to prevent them from tearing it?[1] https://en.wikipedia.org/wiki/Geneva_drive\n \nreply",
      "The platen and paper feed stops line for line.the drum or similarly the chains used on the hammer printers were in constant motion.I am just old enough that I had to repair both types at the beginning of my career.  Although typically rebranded Data Products and other OEMs, which I am probably wrong but vaguely remember being a supplier for the DEC L20?(maybe). But different than this printer.To these hammer action printer, the ribbon and the paper weren't even a consideration.If you have a Newton's cradle, put a piece of paper between the inner balls and it will still mostly work if you release a single ball.The high speed drum printers they typically had to rotate twice for each line (at minimum) so a 600lpm printer would have the drum rotating at about 1200rpm.If you look at the video posted in another comment, you can see the ragged vertical alignment of the chars.  IIRC that is why IBM preferred chains in their hammer printers, because the human eye was more forgiving of vertical misalignment compared to the vertical misalignment that was a natural result of the mechanical differences.Edited to add link to video from page:https://www.youtube.com/watch?v=EDeL15amsus\n \nreply"
    ],
    "link": "http://www.righto.com/2024/08/space-shuttle-interim-teleprinter.html",
    "first_paragraph": ""
  },
  {
    "title": "Make your electronics tamper-evident (anarsec.guide)",
    "points": 115,
    "submitter": "walterbell",
    "submit_time": "2024-08-03T19:25:58",
    "num_comments": 61,
    "comments_url": "https://news.ycombinator.com/item?id=41148650",
    "comments": [
      "Ctrl F tamper tapeamazon dot com tamper tapeYou can't open the case in a way that doesn't make it obvious it was opened.\n \nreply",
      "Here's some work I did a couple years ago using some of these principles to fight counterfeit medicines:  https://www.nature.com/articles/s41598-022-11234-4A side note:  I think there's an unmet need for algorithms that can convert photos of these random patterns into text (or something similar) that can be stored in a database and searched quickly for matching patterns.  I've tried image similarity algorithms like the ones used by e.g. Google Reverse Image Search, but they seem poorly suited for this task.  I ended up writing my own crude algorithm in the paper above that converts a pattern into a set of strings, and it works OK, but surely there are better ways to do this.\n \nreply",
      "I should think you could do this with fingerprinting the photo similar to how music is fingerprinted for things like Shazam or MusicBrainz. I used to work for MusicIP, which I believe developed the fingerprinting system MusicBrainz is using.\n \nreply",
      "Are there \"fuzzy\" fingerprint algorithms that tolerate some variation in pixel color / hues, edges, imaging quality, etc?\n \nreply",
      "Very cool! This seems almost like physical cryptography. Maybe there is a better term for it, but I\u2019d be very interested in other work along these lines.\n \nreply",
      "Thanks!  There are related structures in electronic circuits called physical unclonable functions (PUFs) that find uses in cryptography - you might find them interesting:  https://en.wikipedia.org/wiki/Physical_unclonable_function\n \nreply",
      "A university spinoff using the interaction between RF and nearby devices, https://www.physec.de/enhttps://www.sciencedirect.com/journal/computer-networks/vol/...> We describe the first MITM-resistant device pairing protocol purely based on a single wireless interface with an extensive adversarial model and protocol analysis. We show that existing wireless devices can be retro-fitted with the VP protocol via software updates, i.e. without changes to the hardware.\n \nreply",
      "Very cool. I actually learned something by reading just the abstract, which does not happen often.\n \nreply",
      "There are DoD standards for this. Mostly for SECRET level. Containers for SECRET level material are supposed to be tamper-evident, but not extremely resistant to attack. Filing cabinets must have welded and painted joints, and good locks. It's possible to pry open a secure filing cabinet, but the damage will show. See page 5.3.1 of [1].The U.S. Navy does authorize label-type seals but rates their security as \"minimal\". See page 6.3 of [2]Defense Counterintelligence Agency has some security seal guidelines.[3] Probably outdated.There are \"tamper-evident seals with residue.\" If you remove them, it makes a visible mess. [4] They also have bar-coded serial numbers. A well-resourced attacker with a lot of access time and a preliminary run to get a look at the seals and have duplicates made could probably remove and replace those. If you're facing that level of threat you probably shouldn't have anything of interest in an unattended laptop.[1] https://www.nispom.org/NISPOMwithISLsMay2014.pdf[2] https://exwc.navfac.navy.mil/Portals/88/Documents/EXWC/DoD_L...[3] https://www.dcsa.mil/Portals/91/Documents/CTP/NAO/security_s...[4] https://seals.com/security-tape-labels/?_bc_fsnf=1&Classific...\n \nreply",
      "When the Americans secretly captured and dissembled a Soviet satellite, one of the night\u2019s many challenges was replacing a plastic seal covering some part.  The engine had been removed, \u201cbut its mounting brackets, as well as the fuel and oxidizer tanks, were still in place,\u201d recalled Finer. That was when they hit a problem. The only way to see inside the machinery was to remove a four-way electrical outlet, but it was encased behind a plastic seal bearing a Soviet stamp. The team needed to leave the spacecraft exactly as they found it. But if the Soviets noticed a missing seal, the game would be up. Could they make a replacement in the middle of the night?\n   \u2026 \u201cMy technicians were working all that night,\u201d Zambernardi recalled. \u201cThat night we developed 280 photographs. We also had 60 samples of valves. We had samples of the fluid, rocketry fluid, or what have you.\u201d\n   As they put the assembly back together, the CIA car returned: inside was a perfect counterfeit Soviet seal. They could now reseal the panel and conceal their theft.\n\nhttps://www.technologyreview.com/2021/01/28/1016867/lunik-ci...\n \nreply"
    ],
    "link": "https://www.anarsec.guide/posts/tamper/",
    "first_paragraph": "If the police ever have physical access to an electronic device like a laptop, even for five minutes, they can install hardware keyloggers, create images of the storage media, or otherwise trivially compromise it at the hardware, firmware, or software level. One way to minimize this risk is to make devices tamper-evident. As the Threat Library notes, \"Tamper-evident preparation is the process of taking precautionary measures to make it possible to detect when something has been physically accessed by an adversary.\"'Evil maid' attacks work like this: an attacker gains temporary access to your encrypted laptop or phone. Although they can\u2019t decrypt your data, they can tamper with your laptop for a few minutes and then leave it exactly where they found it. When you return and enter your credentials, you have been hacked. The attacker may have modified data on your hard drive, replaced the firmware, or installed a hardware component such as a keylogger.\"Defense in depth\" means that there ar"
  },
  {
    "title": "Monetagium \u2013 monetary extortion in feudal Europe (jpkoning.blogspot.com)",
    "points": 56,
    "submitter": "jpkoning",
    "submit_time": "2024-08-03T19:23:09",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=41148633",
    "comments": [
      "\"democracies have not resorted to modern version of debasement as a revenue source due to the unpopularity of rising prices.\"The USA got rid of silver in its coins in 1964 and I believe copper in its pennies recently.The modern version of debasement is in the feds balance sheet, they've gotten so efficient there's no need to affect the physical substance.\n \nreply",
      "Some democracies in the 20th century have tried a form of cash debasement to fight inflation.The example I\u2019m familiar with is from Finland in 1946, when the most popular and largest circulating bills were required to be physically cut in half and lost 50% of their upfront value.The Wikipedia article seems to be only in Finnish:\nhttps://fi.wikipedia.org/wiki/SetelinvaihtoThe idea was that the left half of the bill remained valid cash (although not for long \u2014 you needed to exchange it for a new type of bill within a few months). The right half of the bill became effectively a treasury note with three-year maturation: in 1949 you could present it to a bank and get your money back from the government, but no sooner.The operation was expected to reduce inflation, but apparently it didn\u2019t work out that way. It did provide the Finnish government with about half of the funds loaned that year, but it was very unpopular among voters and never repeated.\n \nreply",
      "The US penny switched to copper-plated zinc in 1982, so not that recently (and it still contains a small percentage of copper).\n \nreply",
      "Nowadays debasement and the resulting inflation comes from loans creating cash flow out of thin air. The 20% debasement has turned into the functionally zero percent interest rates. The ritual of bringing coins to the mint for exchange has turned into investments. The feudal lord's turbocharged revenue has turned into the profits of billion dollar corporations.\n \nreply",
      "And lo, Pikkety said unto the masses \u201cThis shit is why we keep returns to labour higher than returns to capital. Introduceth thou est a wealth tax right now or suffer ye into the third generation\u201d\n \nreply",
      "Although there are other things we can do, like simplify regulations for truly small businesses so people have a chance to start something without needing legal expertise, lots of paperwork, etc.\n \nreply",
      "Bitcoin fixes this for the first time in human history.  I\u2019m excited about a future without monetary debasement.  How many wars have been started by the \u201cprint more money until it is worthless, oops I have to start a war to fix it\u201d spiral?https://en.wikipedia.org/wiki/Debasement\n \nreply",
      "Almost. Because Bitcoin is highly energetic compared to currency, it isn't all the way there. And novel attacks exist for digital currency that are not present in physical currency. Currency has to be tradeable with marginal cost nearly zero (will never be exactly zero), and in my view digital currency has this as an issue. The analogue for digital currency, when appropriately crafted and not a sh*tcoin, is gold. Hard to get, limited in volume, costly to secure and appropriately manage. Gold can be used for currency, but it is awfully insecure to hoarding and banditry.Fiat currency solves a _lot_ of problems while also having limitations broadly railed against by digital currency advocates.\n \nreply",
      "It doesn't in any way fix things. The underlying point of debasement and other schemes outlined in the article was to generate revenue for the government. Now governments use taxes to generate revenue and inflation to encourage circulation.While taxes and inflation are generally unpopular they are also vital to the functioning of society and the economy. BTC and many other token schemes implicitly or explicitly stand in the way of that.Breaking a system you don't like doesn't automatically get you a system that's better.\n \nreply",
      "Bitcoin would make an awful legal tender, because by its design supply decreases while demand for it increases. An ideal, 0% inflation currency (assuming that is ideal) would expand and contract automatically with economic conditions. Satoshi considered this problem but decided it was too complicated to implement, which is why he went with the fixed supply route.Still, it's worth remembering that a fixed supply currency will inevitably fail just as the metallic standards (which were semi fixed supply) did, and for the same reasons: politics. Metal backed currencies fell because the political will required to coordinate the system fell apart. Newly-enfranchised workers didn't find the message \"suck it up, the gold standard requires it\" very appealing politically, so they voted for other things that entailed fiscal and monetary policy. Central Banks, seeing the writing on the wall, abandoned gold en masse, with the US retaining it only for other central banks.Bitcoin would have the same problem. It would be inevitably deflationary - halvenings, for example, are tied to computational power. Presumably, more demand for Bitcoin means more computational power, so its supply decreases just as demand increases.Deflation is even worse than inflation for working people. Investment dries up, because why would somebody take a gamble on a business if they can keep their cash in their closet and make money, risk free? Employment therefore drops, while people decrease spending - why spend $100 on something today, if you can buy it for $85 in a year? Additionally, loans get more expensive in real terms, wages decline, and a whole host of other bad things happen.That's why central banks target +2% inflation, in part. Inflation is preferable because it encourages investment, discourages nominal (and only nominal) wage decreases, decreases the real value of loans to the benefit of the debtor class, and other benefits. Central Banks also have a pretty successful record dealing with runaway inflation - hike rates, cause a recession, wait - whereas they lack tools to deal with deflation. At its simplest, the solution to deflation is for everyone to get a check from the government, but this whole field is considered weirdo experimental land and has only barely been tested.All that is to say: Bitcoin is fundamentally ignorant of history and is incapable of becoming anything other than digital gold. It has a floor value which it cannot sink beneath: online gambling, illegal things, and privacy advocates (in that order) guarantee it will never truly hit $0.00. But it would be an absolute catastrophe for any country to adopt as its actual currency.Now that I think of it, Bitcoin is perhaps one of the earliest examples of technophiles assuming society should work according to computer code, thereby \"cleaning\" these imperfect human systems by replacing them with the inevitable future: A philosophically-driven (rather than pragmatically or empirically, for example) system, with clear and inviolable rules, limited to no exceptions, and a happy ignorance of why existing systems came to be. After all, why study the past when we're creating the inevitable future?\n \nreply"
    ],
    "link": "http://jpkoning.blogspot.com/2024/05/monetagium.html",
    "first_paragraph": "The way that a modern mafia protection racket works is the mafia starts doing very bad things to regular folks, say you and your business. To stop the damage, you pay them a regular fee. Both sides come out ahead. The mafia earns a tidy stream of income. Your suffering comes to an end.In feudal Europe, a monetary practice called monetagium worked along the same principles as a mafia protection racket. It began with the feudal lord threatening to do very bad things to the coinage. To prevent these very bad things from happening, the public would pay a fee\u00a0\u2013 monetagium \u2013 to the lord. Both sides came out ahead. The lord earned revenue. His vassals avoided a worsening of the coinage.To better understand the intricacies of monetagium, or monetary blackmail, we need to start off by exploring how the monetary system worked back in the 11th and 12th centuries, in particular the idea of debasement.\u00a0A feudal lord had a number of ways to earn revenue. These included gabelle, a tax on salt; heriot"
  },
  {
    "title": "StarBook 7 14\" Linux Laptop with Intel Core Ultra 7, 4K, Up to 96GB 5.6GHz RAM (starlabs.systems)",
    "points": 3,
    "submitter": "mrusme",
    "submit_time": "2024-08-04T00:17:05",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://us.starlabs.systems/pages/starbook",
    "first_paragraph": "Select your regionEuropeAustria\u20ac (EUR)Belarus$ (USD)Belgium\u20ac (EUR)Bulgaria\u043b\u0432. (BGN)Croatia\u20ac (EUR)Cyprus\u20ac (EUR)CzechiaK\u010d (CZK)Denmarkkr. (DKK)Estonia\u20ac (EUR)Faroe Islandskr. (DKK)Finland\u20ac (EUR)France\u20ac (EUR)Germany\u20ac (EUR)Greece\u20ac (EUR)Guernsey\u00a3 (GBP)HungaryFt (HUF)Icelandkr (ISK)Ireland\u20ac (EUR)Italy\u20ac (EUR)Jersey$ (USD)Latvia\u20ac (EUR)Lithuania\u20ac (EUR)Luxembourg\u20ac (EUR)Malta\u20ac (EUR)Netherlands\u20ac (EUR)Norwaykr (NOK)Polandz\u0142 (PLN)Portugal\u20ac (EUR)RomaniaLei (RON)Serbia\u0420\u0421\u0414 (RSD)Slovakia\u20ac (EUR)Slovenia\u20ac (EUR)Spain\u20ac (EUR)Swedenkr (SEK)SwitzerlandCHF (CHF)Ukraine\u20b4 (UAH)United Kingdom\u00a3 (GBP)AmericasArgentina$ (ARS)Aruba\u0192 (AWG)BoliviaBs. (BOB)Brazil$ (USD)Canada$ (CAD)Colombia$ (USD)Ecuador$ (USD)Falkland Islands\u00a3 (FKP)Georgia\u10da (GEL)Jamaica$ (JMD)Mexico$ (MXN)Panama"
  },
  {
    "title": "Can Reading Make You Happier? (2015) (newyorker.com)",
    "points": 9,
    "submitter": "kawera",
    "submit_time": "2024-08-03T23:10:17",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=41149974",
    "comments": [
      "Knowledge increases suffering\n \nreply",
      "Reading the Mistborn trilogy has made me happy in ways that are hard to quantify.The first book drags in the middle, and the trilogy as a whole drags in the second book, but they're not impossible to read\n \nreply",
      "What about those books made you happy, specifically? Could you distill it down to a set of points or aspects or ingredients? (Which, if another book series possessed, would make you happy as well.)\n \nreply",
      "Why it made you happier?\n \nreply"
    ],
    "link": "https://www.newyorker.com/culture/cultural-comment/can-reading-make-you-happier",
    "first_paragraph": "Find anything you save across the site in your account Several years ago, I was given as a gift a remote session with a bibliotherapist at the London headquarters of the School of Life, which offers innovative courses to help people deal with the daily emotional challenges of existence. I have to admit that at first I didn\u2019t really like the idea of being given a reading \u201cprescription.\u201d I\u2019ve generally preferred to mimic Virginia Woolf\u2019s passionate commitment to serendipity in my personal reading discoveries, delighting not only in the books themselves but in the randomly meaningful nature of how I came upon them (on the bus after a breakup, in a backpackers\u2019 hostel in Damascus, or in the dark library stacks at graduate school, while browsing instead of studying). I\u2019ve long been wary of the peculiar evangelism of certain readers: You must read this, they say, thrusting a book into your hands with a beatific gleam in their eyes, with no allowance for the fact that books mean different thi"
  },
  {
    "title": "Clang vs. Clang (cr.yp.to)",
    "points": 125,
    "submitter": "dchest",
    "submit_time": "2024-08-03T14:45:17",
    "num_comments": 153,
    "comments_url": "https://news.ycombinator.com/item?id=41146860",
    "comments": [
      "> compiler writers refuse to take responsibility for the bugs they introduced, even though the compiled code worked fine before the \"optimizations\". The excuse for not taking responsibility is that there are \"language standards\" saying that these bugs should be blamed on millions of programmers writing code that bumps into \"undefined behavior\"But that's not an excuse for having a bug; it's the exact evidence that it's not a bug at all. Calling the compiler buggy for not doing what you want when you commit Undefined Behavior is like calling dd buggy for destroying your data when you call it with the wrong arguments.\n \nreply",
      "I think this is actually a mistake by the author since the rant is mostly focused on implementation defined behavior, not undefined.The examples they give are all perfectly valid code. The specific bugs they're talking about seem to be compiler optimizations that replace bit twiddling arithmetic into branches, which isn't a safe optimization if the bit twiddling happens in a cryptographic context because it opens the door for timing attacks.I don't think it's correct to call either the source code or compiler buggy, it's the C standard that is under specified to the author's liking and it creates security bugs on some targets.Ultimately though I can agree with the C standard authors that they cannot define the behavior of hardware, they can only define the semantics for the language itself. Crypto guys will have to suffer because the blame is on the hardware for these bugs, not the software.\n \nreply",
      "The blog post does, at the very end, mention the thing you should actually do.You need a language where you can express what you actually meant, which in this case is \"Perform this constant time operation\". Having expressed what you meant, now everybody between you and the hardware can co-operate to potentially deliver that.So long as you write C (or C++) you're ruined, you cannot express what you meant, you are second guessing the compiler authors instead.I think a language related to WUFFS would be good for this in the crypto world. Crypto people know maths already, so the scariest bits of such a language (e.g. writing out why you believe it's obvious that 0 <= offset + k + n < array_length so that the machine can see that you're correct or explain why you're wrong) wouldn't be intimidating for them. WUFFS doesn't care about constant time, but a similar language could focus on that.\n \nreply",
      "Where suffer means \"not be lazy, implement the assembly for your primitives in a lib, optimize it as best as you can without compromising security, do not let the compiler 'improve' it\"\n \nreply",
      "But then you're not writing C, except maybe as some wrappers.  Wanting to use C isn't laziness.  Making it nearly unfeasible to use C is the most suffering a C compiler can inflict.\n \nreply",
      "There's no reason that C should be suitable for every purpose under the sun.\n \nreply",
      "Fiddling some bits cross-platform is supposed to be one of them.\n \nreply",
      "As was pointed out elsewhere, fiddling bits with constant time guarantees isn't part of the C specification. You need a dedicated implementation that offers those guarantees, which isn't clang (or C, to be pedantic).",
      "There are only two models of UB that are useful to compiler users:1) This is a bad idea and refuse to compile.2) Do something sensible and stable.Silently fail and generate impossible to predict code is a third model that is only of use to compiler writers. Hiding behind the spec benefits no actual user.\n \nreply",
      "I think this is a point of view that seems sensible, but probably hasn't really thought through how this works. For example  some_array[i]\n\nWhat should the compiler emit here? Should it emit a bounds check? In the event the bounds check fails, what should it do? It is only through the practice of undefined behavior that the compiler can consistently generate code that avoids the bounds check. (We don't need it, because if `i` is out-of-bounds then it's undefined behavior and illegal).If you think this is bad, then you're arguing against memory unsafe languages in general. A sane position is the one the Rust takes, which is by default, yes indeed you should always generate the bounds check (unless you can prove it always succeeds). But there will likely always be hot inner loops where we need to discharge the bounds checks statically. Ideally that would be done with some kind of formal reasoning support, but the industry is far that atm.For a more in depth read: https://blog.regehr.org/archives/213\n \nreply"
    ],
    "link": "https://blog.cr.yp.to/20240803-clang.html",
    "first_paragraph": "This is a blog post about an experiment with Clang.\nI need to explain some context first.Compiler \"optimizations\".\nTry skimming through recent changes to\nLLVM\nand\nGCC.\nYou'll find \"optimizations\",\nand tests for \"optimizations\",\nand fixes to tests for \"optimizations\",\nand fixes to bugs in \"optimizations\".The bugs admitted in the compiler changelogs\nare just the tip of the iceberg.\nWhenever possible,\ncompiler writers refuse to take responsibility for\nthe\nbugs\nthey\nintroduced,\neven though the compiled code worked fine before the \"optimizations\".\n[2024.08.03 edit: Added more links here.]\nThe excuse for not taking responsibility is that\nthere are \"language standards\"\nsaying that these bugs should be blamed on millions of programmers\nwriting code that bumps into \"undefined behavior\",\nrather than being blamed on the much smaller group of compiler writers\nsubsequently changing how this code behaves.\nThese \"language standards\" are written by the compiler writers.Evidently the compiler writers f"
  },
  {
    "title": "Parsing Protobuf Definitions with Tree-sitter (relistan.com)",
    "points": 42,
    "submitter": "PaulHoule",
    "submit_time": "2024-08-03T19:13:59",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=41148575",
    "comments": [
      "Huh. tree-sitter seems neat, but I don\u2019t really get why the author thinks processing the descriptor set is so hard. Seems equally difficult to learn a bunch of new abstractions in the form of tree-sitter vs just learning protobuf\u2019s own ones.Also, if you\u2019re parsing .proto files directly, you have to deal with a bunch of annoying issues like include paths, how you package sets of them to move around, etc. descriptor sets seem like a better solution to me.\n \nreply",
      "OTOH, if you can get treesitter to work, all the best editors will gain Protobuf support.\n \nreply",
      "From the docs \"The protocol compiler can output a FileDescriptorSet containing the .proto files it parses.\" (https://github.com/protocolbuffers/protobuf/blob/main/src/go...)I don't understand the point of using tree-sitter to repeat that work (almost certainly having bugs doing so).  Am I missing something?\n \nreply",
      "I need to get around to playing with tree-sitter. The approach in this article is neat.Here's another approach. The AST of a .proto file is itself a protobuf. That's how the codegen plugins work. Protobuf also has a canonical mapping to JSON, so...What you can do is use protoc to parse the .proto file, spit it out as JSON, and then process that data using your favorite pattern matching language. I wrote a [tool][1] that helps with that. For example, here's some [js code][2] that translates protobuf message definitions into \"types\" for use in an ORM.[1]: https://github.com/dgoffredo/protojson[2]: https://github.com/dgoffredo/okra/blob/master/lib/proto2type...\n \nreply",
      "Writing a protoc plugin would have been 5x easier, but its harder to get a blog article out of it.Also, this reads like they might not have seen the newer proto3 optional keyword, or know about the well-known wrapper types.\n \nreply",
      "I highly recommend it. The docs describe it as a zen-like experience, and I fully agree. Once you get the hang of it, it makes it so easy to tweak the syntax of whatever language you\u2019re building. I love it.\n \nreply",
      "Oh my god\u2026 this might\u2019ve made some tools I\u2019m developing a lot easier\n \nreply",
      "I don't get it. Why not just use a better Protobuf model? Go's serialization format for protobufs is not the most brilliant one, but it's reasonable.E.g. just use `string` instead of `StringValue`.\n \nreply",
      "StringValue let's you differentiate empty string vs no value.\n \nreply",
      "But do you actually need that? Like, really?If you absolutely do need it, then in addition to \"name\" you can also add a field \"hasName\". If you feel fancy, you can even do that in a code generator.\n \nreply"
    ],
    "link": "https://relistan.com/parsing-protobuf-files-with-treesitter",
    "first_paragraph": "Member of Technical Staff at Mozi. Co-Author of \u201cDocker: Up and Running\u201d from O\u2019Reilly MediaIf you work with Protocol Buffers (protobuf), you can really save time,\nboredom, and headache by parsing your definitions to build tools and generate\ncode.The usual tool for doing that is protoc. It supports plugins to generate\noutput of various kinds: language bindings, documentation etc. But, if you want\nto do anything custom, you are faced with either using something limited like\nprotoc-gen-gotemplate or\nwriting your own plugin. protoc-gen-gotemplate works well, but you can\u2019t\nbuild complex logic into the workflow. You are limited to what is possible in a\nsimple Go template.It\u2019s also possible to use protoreflect from Go to process the compiled results\nat runtime. This is painful. Really painful.So, at work, we had made limited use of the protobuf definitions other than\nfor their main purpose and for documentation and package configuration via\ncustom options (these are supported in protobuf). W"
  },
  {
    "title": "How to build quickly (learnhowtolearn.org)",
    "points": 243,
    "submitter": "fagnerbrack",
    "submit_time": "2024-08-03T19:05:02",
    "num_comments": 62,
    "comments_url": "https://news.ycombinator.com/item?id=41148517",
    "comments": [
      "This is incredibly simple yet incredibly powerful, and something that everyone who becomes proficient at delivering things of value learns eventually, but is rarely taught so succinctly.By the way, for the programming case, this is a big part of the reason functional programming is so powerful.  Avoiding shared state allows you to write your outline of smaller and smaller pieces, then write each piece as a stateless function, then pipe your data through a graph of these functions.With other programming paradigms, you can't just knock out all the little pieces without thinking about the other pieces because the state is all tangled up.  Which slows you down.It's surprising how very simple the individual components can be, even for very complex systems, when following this approach.\n \nreply",
      "I did something like that the first time I had to write a device driver, but I did it kind of stupidly.It was in college and I had a part time job doing doing system administration and programming for the high energy physics department. They had an RX02 8\" floppy drive that they wanted to use on their VAX 11/780 which was running Unix/32V and I was assigned to write the driver.I basically started with a C file that just had empty functions for all the functions that I knew Unix expected a driver to have, and then started filling those functions with comments recording what I had figured out that they had to do.Each started with just a few high level comments. Then I'd add more comments breaking those down, and so on, until I finally had all the information I'd need in there and could start coding.That's when I then did something stupid. As I started implementing the things the comments described I replaced the comments with the code.I got about half way through before I realized that I should adding the code below the comments rather than replacing the comments.\n \nreply",
      "This is how I work on my projects as an indie dev. When I start working on something significant (a new feature, for instance), I'll create a markdown file that has a summary of what I'm trying to achieve and then a TODOs section which turns into this massive outline of all the tasks that I'll need to do to complete the work.At first, the outline just has a few tasks that are fairly high-level, but as I dive into each one, I add more nested sub-tasks. The nesting keeps going until I end up with sort of leaf nodes that can be done without depending on other tasks. This gives me a nice visual of how complex some tasks are versus others.I generally prototype and design the implementation at the same time, and having this outline gives me a place to \"dump\" tasks and other work I'll need to do later, and you do often encounter more work than you expect, so an outline makes it easier to find a good \"parent\" for the task. Having a big outline also lets me jump around from high-level design to low-level implementation easily as well, which you need if you're prototyping and trying to find the right shape for your solution.It's great as a motivator too since I can see when I complete something big when I check off a parent task that has a lot of nested children.I find a simple text file outline like this is so much more convenient than say an app or a web UI since I can just jump around the file and cut and paste outlined sections and indent or un-indent them to re-parent them. (Having to use something like JIRA to do this would be way too slow, especially when you're in a flow state.)\n \nreply",
      "What do you do when an idea changes and now there are dozens of nested entries that need to be changed to match?\n \nreply",
      "Generally, with this type of work (where I'm trying to go fast), I have to be flexible, so I will often just let nested tasks \"die off\" after I've found alternative ways of solving the problem or I've changed the idea a bit.Sometimes I'll delete the nested entries outright, but usually I'll just keep them around until I get to a point where I'm close to completing the feature and then I'll re-visit them to see if they still apply or if I need to reincorporate them into the new design.\n \nreply",
      "Same here. I wrote a little multitree-based TUI with vim-adjacent key bindings for exactly this purpose, since I find it generalises to all complex projects, software-related or not (and who can resist any excuse to write a TUI?), but a simple markdown file is just as good, really, and for software means you can keep it in the repo directly adjacent to other project docs.\n \nreply",
      "Similar here - i use asana or linear for highlevel planning with a calendar and then as I write code I drop in TODOs and FIXMEs and such then just grep them out or use a VS Code extension called \"TODO Tree\" to track them.\n \nreply",
      "The author provides an example of the bad \"Loading bar writing\" but unfortunately not a good example of what they call \"Outline speedrunning writing\"pg, who's good at writing essays, does provide a good example of the latter, with https://byronm.com/13sentences.html.This is the writing process that lead to https://paulgraham.com/13sentences.html. (the https://code.stypi.com/hacks/13sentences?doomed=true URL on pg's blog is a now a dead link. Previous discussion: https://news.ycombinator.com/item?id=6993060).\n \nreply",
      "What I do for my blog is I write everything at once. Then I figure out where to put images, then I publish it!It makes me go back and read it carefully since I have already published it, and then I polish, rewrite sections and add stuff that I missed.\n \nreply",
      "I have done something similar on the past and I was very happy with the results. At the time I was starting up a consulting business and got the first few gigs directly from engagement with my blog.I also time boxed myself when writing. I wouldn't write unless I had a really clear topic in mind, then I'd give myself an hour to get it published. A few times I ran out of time and ended it with a \"follow-up post coming soon to dive into ___\" type message and that worked just fine.\n \nreply"
    ],
    "link": "https://learnhowtolearn.org/how-to-build-extremely-quickly/",
    "first_paragraph": "Do \u201coutline speedrunning\u201d: Recursively outline an MVP, speedrun filling it in, and only then go back and perfect.This is a ~10x speed up over the \u2018loading-bar\u2019 style (more on that below)Don\u2019t just read this article and move on. Go out and do this for the very next thing you make so you can get in the habit of doing it.(btw, here, speedrunning just means doing something as fast as possible)1. Make an outline of the project2. For each item in the outline, make an outline. Do this recursively until the items are small3. Fill in each item as fast as possible4. Finally, once completely done, go back and perfectOutline speedrunning may seem basic. That\u2019s because it is. Planning is a fundamental of doing, and outlining is a fundamental of planning.Much of becoming really efficient is about getting extremely cracked at the fundamentals (many of which you probably mistakenly dismiss).This is recursive btw, because fundamentals typically have fundamentals.I knew about outlining since I was littl"
  },
  {
    "title": "Sega Jet Rocket: The '70s arcade game with no computer or screen (newatlas.com)",
    "points": 26,
    "submitter": "geox",
    "submit_time": "2024-08-03T23:32:04",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=41150069",
    "comments": [
      "2 things.1. That is an amazing design and amazing engineering. I legitimately am astounded by the creativity and thought that went into that.2. The writing style where the author feels the need to apologize for anything even remotely technical (like: Once again, just bear with us for a bit) comes across as either childish or condescending. What purpose does that serve? Anyone clicking a link about an old arcade game with no computer or screen obviously wants to know how it works. Why write like that?\n \nreply",
      "I thought there was another screen-free arcade game discussed on HN in the past year, but I've failed to pull it up.  At first I thought this article must be about the same thing.Can you help me remember?\n \nreply"
    ],
    "link": "https://newatlas.com/games/sega-jet-rocket-arcade-game/",
    "first_paragraph": ""
  },
  {
    "title": "An FPGA built with 7400 series logic [video] (ccc.de)",
    "points": 104,
    "submitter": "chrsw",
    "submit_time": "2024-08-03T13:40:07",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=41146563",
    "comments": [
      "Back in the late 70s and 80s, many computers and other digital systems were actually designed using basic blocks similar to this.  Since chip layout is by far the biggest expense (especially back then) it made sense to design just one customizable chip, and then produce it with different metal layers to configure it into various arrangements, so it is relatively easily \"programmed\" just before manufacturing.  DEC's ill-fated VAX 9000 was done in this way, probably the last great pre-CMOS processor design, with hundreds of ECL gate array chips all based on a couple die patterns.  Unfortunately the benefits of sheer density from CMOS were undeniable.  When they crammed the VAX into a $1000 CMOS microprocessor, it outperformed the $1 million VAX 9000 four years later.  These days it's relatively uncommon as a technique except for rapid prototyping.  With standard cell ASIC designs, a gate array cell able to perform any function is still an option, but modern design tools make it much easier to use more optimized logic.\n \nreply",
      "https://github.com/mnemocron/my-discrete-fpga\n \nreply",
      "Very nice educational project, even if the main reason to use an FPGA is to no longer have to deal with the limitations of the 7400 series logic.\n \nreply",
      "Full writeup (first in series, each post links to the next): https://mnemocron.github.io/2023-12-08-DIY-FPGA-diary-0/\n \nreply",
      "If you took the right half of his CLB from his second entry[1], and connected each input bit to one of it's neighbors, that's enough to have a computing fabric.Latching them in 2 phase adds delay, but removes the need for worry about timing, as it becomes known.If you put 4 of them in parallel, so each neighbor gets it's own output, it's a BitGrid[2,3], which I've been tinkering with since the 1980s.He referenced an earlier effort 2012 in his video[4,5] that goes down the same detour towards computing fabrics. The HDL for that fabric, might prove useful for programming a bitgrid.[6]I see the appeal of routing fabric and saving silicon, and especially wanting to keep the latency to bare minimum, but the trade-offs aren't worth it in my opinion. Routing becomes a nightmare, as does getting the timing of things right, and watching out for race conditions.If you give up and embrace latency, and the wasting of gates to route signal, you end up with a homogenous, isolinear core. (The same in any linear direction) You can rather easily shift a part in any direction, rotate it, and flip it. You can route around defective cells (assuming the shift register chain doesn't break, of course).  Latency goes to hell, but because every single operation is pipelined, your throughput can go through the roof, none of the data lines ever has to make it all the way across the chip, except the clock signals. The shift registers, and only have to load things before the computing starts, so they don't have to be fast at all.[1] https://mnemocron.github.io/2023-12-08-DIY-FPGA-diary-1/[2] https://esolangs.org/wiki/Bitgrid[3] https://github.com/mikewarot/Bitgrid[4] https://hackaday.com/2012/11/01/discrete-fpga-will-probably-...[5] https://web.archive.org/web/20150621104100/http://blog.notdo...[6] https://github.com/Arachnid/dfpga/tree/master\n \nreply",
      "seems like you could get pretty far bootstrapping a higher level language. how far have people gotten?\n \nreply",
      "Not sure if this is quite the same idea, but https://www.greenarraychips.com/ ?\n \nreply",
      "No, but you could certainly use a GreenArray chip to emulate a BitGrid faster than a normal embedded processor could.\n \nreply",
      "Completely tangential, but I recently realized how annoying 7400 series is.First annoyance is that manufacturers generally put the logic tech family in the middle, so you got 74HCxxx or 74LVCxxx. This makes searching information very annoying because you can't just do a simple prefix/suffix match. And of course those logic family names have no rhyme or reason, nor are they consistent across vendors.Some parts are also available as 5400 series logic, which makes things even more confusing.Some manufacturers like to throw some letters like SN in front of the 74... part number, making even more difficult to search, or visually scan them. Others don't put any letters in the frontAt some point some people thought to integrate all of 4000 series logic into 7400 series, so you got some parts that are seemingly logically equivalent but have different part numbers because they come originally from 4000 series instead of 7400 series.The logic family names can also contain digits, so figuring out where the family name ends is not that easy.Like almost all parts, manufacturers love throwing some extra characters in the end to indicate stuff like packaging.The part specs can vary quite a lot between manufacturers, even on nominally same logic family like the popular \"74HC\" seriesbasically we have gone from 7404 to something like SN74AUP1T04DCKRall this adds up to finding parts being very annoying when building stuff\n \nreply"
    ],
    "link": "https://media.ccc.de/v/cosin2024-56234-fpga-selber-bauen-mit-74",
    "first_paragraph": "\n\nSimon Burkhardt\n\nIch habe selber eine FPGA architektur in 7400 Logik realisiert und erz\u00e4hle, was denn son ein FPGA \u00fcberhaupt ist und wie dieser funktioniert\nFPGAs sind eine einzigartige Form von high-performance chip f\u00fcr Nichenanwendugen. Nur wenige kennen FPGAs \u00fcberhaupt und noch weniger wissen, wie deren interne Architektur aussieht.\nInspiriert durch Ben Eaters 8-bit CPU on breadboard habe ich selber einen simplen FPGA in hardware aufgebaut. Anhand dieses Models erz\u00e4hle ich detailiert und low-level, was ein FPGA ist und wie ich meine Version entwickelt habe.\nIch habe selber eine FPGA architektur in 7400 Logik realisiert und erz\u00e4hle, was denn son ein FPGA \u00fcberhaupt ist und wie dieser funktioniertFPGAs sind eine einzigartige Form von high-performance chip f\u00fcr Nichenanwendugen. Nur wenige kennen FPGAs \u00fcberhaupt und noch weniger wissen, wie deren interne Architektur aussieht.\nInspiriert durch Ben Eaters 8-bit CPU on breadboard habe ich selber einen simplen FPGA in hardware aufgebaut. A"
  },
  {
    "title": "Primitive Recursive Functions for a Working Programmer (matklad.github.io)",
    "points": 107,
    "submitter": "ingve",
    "submit_time": "2024-08-03T12:32:01",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=41146278",
    "comments": [
      "The conclusion to the article has some pretty good points regarding configuration languages; I wonder if any present language satisfies all or most of those points.\n \nreply",
      "While imperative, and not 'pure' even C was created to set upper bound of the number of iterations of every loop being known before entering, thus PRDennis Ritchie's research at MIT was focused on what he called loop programming.The complexity of loop programs - ALBERT R. MEYER and DENNIS M. RITCHIEhttps://people.csail.mit.edu/meyer/meyer-ritchie.pdfStructured programming, the paradigm that almost every modern programmer follows by default is really pushing you to primitive recursive functions.That almost universal acceptance of structured programming compared to the other two types, pop and functional, is why people are confused about Dykstra's goto is harmful paper.While primitive recursive functions don't contain the entire set of computable functions, they do contain almost all intuitive ones that are guaranteed to HALT (total).Unfortunately there are some real needs for languages to support loops that have an indeterminate number of iterations when you enter the loop, but it is a foot gun that is avoidable by only using them when required.Even COBOL was modernized with unrestricted goto being moved to the ALTER command.I can't think of a modern, useful language that doesn't allow for PR functions.But even in C, if you avoid 'while', explicitly avoid fall through, etc... you will produce code that almost always is a total functions that will always HALT.There are cases like even type inference in ML, which is pathological in that it is way cheaper than the complexity class, thus worth the risk, despite not being total functions that make it hard for a language to restrict those use cases.So I would say that with a pragmatic approach, all languages support defaults that support most of the points, but imposed constraints that enforce them would seriously constrain the utility of the language.If you review even the hated SOLID and Clean frameworks, they're pushing you towards this model too IMHO.I think the universal acceptance of structured programming, makes this easy to forget or even fail to teach.  But as an old neck beard, we were taught the risks of WHILE etc...\n \nreply",
      "Let's pick Dhall and see if it fails any of the points. It seems closest.\n \nreply",
      "Skylark also. It's essentially Python with anything more than primitive recursive functions cut out.\n \nreply",
      "One point that the article is trying to make is that even something in PRF can give rise to a very long-running computation.\n \nreply",
      "I'm struggling with the mini rant / motivation of the article:> Typically, not being Turing-complete is extolled as a virtue or even a requirement in specific domains. I claim that most such discussions are misinformed \u2014 that not being Turing complete doesn\u2019t actually mean what folks want it to meanWhy are those discussion misinformed?  Most formal analysis tools (Coq, Isabelle, Agda(?)) usually require a proof that a function terminates.  This is I think is equivalent to proving that it is total implying it is primitive recursive?\n \nreply",
      "As you are talking about formal proofs, and not the scientific counterexamples modern programming uses:Proving a function is total in the general case is a NP total search problem.IIRC this is equivalent to NP with a co-NP oracle, or the second level on the PH hierarchy, aka expensive if even possible even in many small problems.Most of those tools work best if you structure your programs to be total, of which structural programing with only FOR or iteration count limited WHILE/recursion are some the most common methods.While just related to SAT, look at the tractable forms of Schaefer's dichotomy theorem is the most accessible lens I can think of.\n \nreply",
      "> Proving a function is total in the general case is a NP total search problem.My intuition suggests this should be undecidable\u2014could you elaborate on the difference between this and the halting problem?\n \nreply",
      "Halt is a decision problem, TFNP is a combinatorial problem constructed of decision problems.The HALT decider is a total Turing machine that represents a total function.I am struggling to explain this in a rigorous fashion, and there are many open problems.NP is where the answer \"yes\" is verifiable in polynomial time.co-NP is where the answer \"no\" is verifiable in polynomial time.NP is equivalent to second order predicts logic where the second term is existential 'there exists..'co-NP is equivalent to second order predicts logic where the second term is universal 'for any'We know P=co-P, or that the yes and no answers are both truthy.We think that NP!=co-NPMany useful problems are semi-decidable or recursively enumerable.Determining if a computable function is total is not semi-decidable.It is the subtle difference between proving a TM halts for any input vs  halts for each input.Termination analysis is the field of trying to figure out if it halts for each input. It is actually harder than HALT, being on the second level of PH on the co-NP side.If that granularity isn't important to you, I personally don't think there is much of a risk in using the decidable metric as a lens.Just remember that something is decidable if and only if both it and its complement are semi-decidable.Semi-decidable problems often have practical applications even without resorting to approximations etc ...\n \nreply",
      "Possibly there are more ways to be non-Turing-complete than being a nice total terminating function. For instance,  an infinite loop is neither capable of universal computation nor is terminating.\n \nreply"
    ],
    "link": "https://matklad.github.io/2024/08/01/primitive-recursive-functions.html",
    "first_paragraph": "Programmers on the internet often use \u201cTuring-completeness\u201d terminology. Typically, not being\nTuring-complete is extolled as a virtue or even a requirement in specific domains. I claim that most\nsuch discussions are misinformed \u2014 that not being Turing complete doesn\u2019t actually mean what folks\nwant it to mean, and is instead a stand-in for a bunch of different practically useful properties,\nwhich are mostly orthogonal to actual Turing completeness.While I am generally descriptivist in nature and am ok with words loosing their original meaning\nas long as the new meaning is sufficiently commonly understood, Turing completeness is a hill I will\ndie on. It is a term from math, it has a very specific meaning, and you are not allowed to\nre-purpose it for anything else, sorry!I understand why this happens: to really understand what Turing completeness is and is not you need\nto know one (simple!) theoretical result about so-called primitive recursive functions. And,\nalthough this result is simp"
  },
  {
    "title": "Discovering algorithms by enumerating terms in Haskell (twitter.com/victortaelin)",
    "points": 90,
    "submitter": "agomez314",
    "submit_time": "2024-08-02T18:57:05",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=41141553",
    "comments": [
      "Maciej Bendkowski has some related work [1] on generating random lambda terms, but was unable to overcome what he calls the asymptotic sparsity problem:  Sampling simply-typed terms seems notoriously more challenging than sampling closed ones. Even rejection sampling, whenever applicable, admits serious limitations due to the imminent asymptotic sparsity problem \u2014 asymptotically almost no term, be it either plain or closed, is at the same time (simply) typeable. [...] Asymptotic sparsity of simply-typed \u03bb-terms is an impenetrable barrier to rejection sampling techniques. As the term size tends to infinity, so does the induced rejection overhead. In order to postpone this inevitable obstacle, it is possible to use dedicated mechanisms interrupting the sampler as soon as it is clear that the partially generated term cannot be extended to a typeable one. The current state-of-the-art samplers take this approach, combining Boltzmann models with modern logic programming execution engines backed by highly-optimised unification algorithms. Nonetheless, even with these sophisticated optimisations, such samplers are not likely to generate terms of sizes larger than one hundred.\n\nI would be curious to see a more rigorous analysis of the sample complexity of generating well-typed expressions in, e.g., the STLC. Maybe there is a way to avoid or reduce the rejection rate before evaluation.[1]: https://arxiv.org/pdf/2005.08856\n \nreply",
      "I did some work on this about a decade ago, using RL on STLC, and that was the same problem I faced. It\u2019s too bad so few well typed expression trees are very useful programs.\n \nreply",
      "There are quite a few publications that explore the concept of generating programs, either using typed or untyped functional languages.For example, wake-sleep learning and NN on a Lisp-like language [1] or synthesis of Haskell guided by refinement types [2].[1] https://royalsocietypublishing.org/doi/full/10.1098/rsta.202....[2] https://dl.acm.org/doi/abs/10.1145/2980983.2908093\n \nreply",
      "The trick is not just synthesizing valid functions, but doing so in a parallel communication-free manner, without compromising soundness or completeness. You want to massively scale up a discrete sampler without replacement. One very efficient way of doing this is by constructing an explicit bijection from the sample space to the integers, sampling integers, then decoding them into programs.While this technique enjoys certain advantages, i.e., it is embarrassingly parallelizable and guaranteed to enumerate distinct solutions with a bounded delay,  it also somewhat unnatural. By flattening the distribution onto the integers a la G\u00f6del numbering, it destroys locality, does not play well with incremental decoding methods (left-to-right is currently en vogue in generative language modeling), and will fail if the sample space is uncountable.Another key step is reducing symmetries in your sample space by quotienting it somehow (e.g., by \u03b1-equivalence). The author seems to be invoking some kind of equivalence relation by \u201csuperposition\u201d, but the technical details here are a little fuzzy.This problem is also closely related to model counting in the CSP literature, so a practical speedup could lead to improvements on a lot of interesting downstream benchmarks.In general, the problem of program induction from input-output examples is not well-posed, so specialized solvers that can make stronger assumptions will usually have an advantage on domain-specific benchmarks. Most existing program synthesizers do not satisfy all of these desiderata (e.g., soundness, completeness, naturalness, incrementality).\n \nreply",
      ">> The trick is not just synthesizing valid functions, but doing so in a parallel communication-free manner, without compromising soundness or completeness.Right! A great way to do this is to learn a program by using it to prove the training examples while it is being learned. A very cool ability that some systems of the new wave of Inductive Logic Programming can pull off, but probably nothing else can far as I can tell.\n \nreply",
      ">> There are quite a few publications that explore the concept of generating programs, either using typed or untyped functional languages.That's Inductive Functional Programming (IFP), a kind of Inductive Programming that also includes Inductive Logic Programming (ILP). The canonical example of IFP is Magic Haskeller:https://nautilus.cs.miyazaki-u.ac.jp/~skata/MagicHaskeller.h...As an example of a modern ILP I suggest Popper:https://github.com/logic-and-learning-lab/Popper/Or Louise (mine):https://github.com/stassa/louiseOne of the DreamCoder papers describes Inductive Programming as a form of weakly supervised learning, in the sense that such systems learn to generate programs not from examples of programs, but from examples of the target programs' beuav908rs, i.e. their inputs and outputs. By contrast LLMs or slightly older neural program synthesis systems are trained on examples that consist of pairs of (programming-task, program-solving-the-task).Another way to see the difference between Inductive Programming systems and conventional machine learning systems used for program synthesis is that Inductive Programming systems learn by solving problems rather than from observing solutions.The advantage is that, in this way, we can learn programs that we don't know how to write (because we don't have to generate examples of such programs) whereas with conventional machine learning we can only generate programs like the ones the system's been trained on before.Another advantage is that it's much easier to generate examples. For instance, if I want to learn a program that reverses a list, I give some examples of lists and their reverse, e.g. reverse([a,b,c],[c,b,a]) and reverse([1,2,3],[3,2,1]) whereas e.g. an LLM must be trained on explicit examples of list-reversing programs; like, their source code.IFP and ILP systems are also very sample efficient, so they only need a handful of examples, often just one, whereas neural net-based systems may need millions (no exaggeration- can give a ref if needed).The disadvantage is that learning a program usually (but not always - see Louise, above) implies searching a very large combinatorial space and that can get very expensive, very, very fast. But, there are ways around that.\n \nreply",
      "Another relevant publication in this line of research: https://dl.acm.org/doi/10.1145/3632919\n \nreply",
      "What the author is getting at is a pretty cool research area called program synthesis, where the goal is to create a program that satisfies a specification.Most techniques are essentially brute force enumeration with tricks to improve performance, so they tend to struggle to find larger programs. A lot of active research is in improving performance.Compared to asking a LLM to write a program, program synthesis approaches will guarantee that a solution will satisfy the specification which can be very powerful.In particular, as the author has discovered, one area where program synthesis excels is finding small intricate bitwise operator heavy programs that can be hard to reason about as a human.The most famous example of program synthesis is Microsoft's FlashFill, which is used in Excel. You give it a few input output examples and FlashFill will try to create a small program to generalize them, and you can apply the program to more inputs, which saves you a bunch of time. An example from the paper is:  Input -> Output\n\n  International Business Machines -> IBM\n  Principles Of Programming Languages -> POPL\n  International Conference on Software Engineering -> ICSE\n\n  String Program: Loop(\\w : Concatenate(SubStr2(v_1, UpperTok, w)))\n\n\nHere's a few papers:EUSOLVER: https://www.cis.upenn.edu/~alur/Tacas17.pdfFlashFill: https://www.microsoft.com/en-us/research/wp-content/uploads/...BlinkFill: https://www.vldb.org/pvldb/vol9/p816-singh.pdfSynquid: https://cseweb.ucsd.edu/~npolikarpova/publications/pldi16.pd...\n \nreply",
      "Note that FlashFill is an example of inductive program synthesis, i.e. program synthesis from an incomplete specification, e.g. one in the form of input-output examples, program traces, or natural language descriptions.Program synthesis from a complete specification is known as deductive program synthesis and the simplest example is compilation of a program in a high-level language to a machine code. A \"complete\" specification is what it says on the tin: it fully specifies the program to be synthesised.\n \nreply",
      "Program synthesis is also pretty much equivalent to generating proofs of propositions by the Curry-Howard Isomorphism. There was a post from a few days ago about using ML to generate proofs in Lean. I'm sure there's ongoing research to do the same thing with synthesis (which imo is probably going to be more effective at pruning the search space than brute force).\n \nreply"
    ],
    "link": "https://twitter.com/VictorTaelin/status/1819208143638831404",
    "first_paragraph": ""
  },
  {
    "title": "p5.js (p5js.org)",
    "points": 382,
    "submitter": "alabhyajindal",
    "submit_time": "2024-08-03T04:53:10",
    "num_comments": 56,
    "comments_url": "https://news.ycombinator.com/item?id=41144755",
    "comments": [
      "I think the key reason why this link has been posted to HN now is because p5js has a new website? Back in 2023 the Sovereign Tech Fund gave the p5js folks a \u20ac450k grant[1] \"to enhance p5.js Documentation organization & accessibility\" and this seems to be some of the results of that investment.In my view, it seems to be paying off - the site feels a lot easier to navigate and search, with more useful information. For example, compare the old[2] and new[3][4] versions of how to use p5js editor/canvases with a screen reader.[1] - STF announcement/progress - https://p5js.org/events/stf-2024/[2] - Using p5 with a screen reader (old version) - https://archive.p5js.org/learn/p5-screen-reader.html[3] - How to Use the p5.js Web Editor with a Screen Reader (new) - https://p5js.org/tutorials/p5js-with-screen-reader/[4] - Writing Accessible Canvas Descriptions (new) - https://p5js.org/tutorials/writing-accessible-canvas-descrip...\n \nreply",
      "Recently did a digital ad entirely in processing and then ported it to p5js. In my experience with it\u2026* processing is  still much easier to work, even though p5js seems to catch-up. I did work with it like 10 years ago and it was already very easy* it was very easy to spot bugs or missing implementation in p5js* While an almost identical port was possible, it\u2019s not 1:1 and takes learning some APIs* p5js is not even close to \n d3 or three.js in terms of performance* tbh it seems to me lot easier to animate in web with modern css3 and some helper libs, than use p5js. In fact I would see a very straight pipeline from free vector apps into css with very little shading without p5js. Which makes me wonder how is p5js relevant in 2024, apart from educational tool.* For things which involve pixel level compute there is no easy way to use the GPU efficientlyFor me this whole 450k funding is quite bizarre, and in particular the fact it got directed to a not so popular framework.\n \nreply",
      "These are some great insights! Thanks for summarizing them. It will save me a lot of time in the future. I would love to see this digital ad! Is it public?\n \nreply",
      "p5.js ability to spark beginners while being reasonably capable is what stands out to me.There\u2019s lots of options now, but it hits some outcomes in empowering folks who learned actionscript when it was the only game.More options today for sure.\n \nreply",
      "Regarding performance, I once had an experience that I never bothered to look into. I made a cool little interactive simulation in the p5js sandbox; ran flawlessly. I then copy pasted it into a codesandbox, and it was choppy and abysmal. I checked the version numbers and how it was being included and couldn\u2019t see anything obvious. Maybe codesandbox has some isolation that is taxing? Like I said, never looked into it.\n \nreply",
      "It's nice that they keep the old site available as https://archive.p5js.org/I'm not a fan of the layout on large screens though. The linked tutorial [1] is fixed width left-aligned paragraphs, which makes right half of my display empty. On the other hand, other elements like on the reference overview [2] are stretched in four columns across the whole display which looks ok when the browser window is half my displays width but is hard to read when the window is maximized.[1]: https://p5js.org/tutorials/p5js-with-screen-reader/\n[2]: https://p5js.org/reference/\n \nreply",
      "We also featured it quite prominently in yesterday's JavaScript Weekly. With ~175k subscribers, things we include often make it to HN front page hours later. It's next to impossible to actually prove the connection.. but it's just part of the fun of how links spread around the ecosystem for me :-)\n \nreply",
      "Impossible yet it often happens the same old link gets featured everywhere at the same time :)\n \nreply",
      "The old site was decent and already mobile responsive.The new site is more mobile optimized and makes it possible to read more on the higher density screens.Discovery of p5.js is well deserved, as are the related coding train videos.\n \nreply",
      "ChatGPT knows P5.js pretty well so you can ask \"Write a P5 program that plots a sine wave and let's me control phase with a slider.\" and it does it.I used P5.js with ChatGPT to make a design tool that balances 5 masses on a circumference for creating a rotary magnetic bow with reduced harmonic locking.Drag vertices of the pentagon...\nhttps://editor.p5js.org/spDuchamp/full/zgtkE2xikHere 3D printed result https://www.instagram.com/p/Cr4ZXGztY27/\n \nreply"
    ],
    "link": "https://p5js.org/",
    "first_paragraph": ""
  },
  {
    "title": "To preserve their work journalists take archiving into their own hands (niemanlab.org)",
    "points": 55,
    "submitter": "bcta1",
    "submit_time": "2024-08-03T13:57:49",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=41146642",
    "comments": [
      "> \u201cThank goodness she did that because [otherwise] we would have no records of the early years of the first Women\u2019s Hockey League in Canada,\u201d Azzi said.A few years ago, Canada digitized many older television shows, https://news.ycombinator.com/item?id=35716982  With the help of many industry partners, the [Canada Media Fund] CMF team unearthed Canadian gems buried in analog catalogues. Once discovered, we worked to secure permissions and required rights and collaborate with third parties to digitize the works, including an invaluable partnership with Deluxe Canada that covered 40 per cent of the digitization costs. The new, high-quality digital masters were made available to the rights holders and released to the public on the Encore+ YouTube channel in English and French.\n\nIn late 2022, the channel deleted the entire Youtube Encore archive of Canadian television, with two weeks notice. A few months later, half of the archive resurfaced on https://archive.org/search?query=creator%3A%22Encore%20%2B%2.... If anyone independently archived the missing Encore videos from Youtube, please mirror them to Archive.org.\n \nreply",
      "Good thing I\u2019m a hoarder. If I like something, I archive it and back it up locally. For example, a couple of days ago, I needed some digital assets for an Adobe program that I had downloaded a few months ago because I liked them and thought I might need them in the future. When I went back to the company page a couple days ago, everything had vanished! I'm glad I had downloaded them before and checked my backup to retrieve them.\n \nreply",
      "Do you engage in any kind of physical archiving? I'm not going so far as to say it's superior to digital archiving but it does simplify maintenance of the assets in the event that you're incapable of it.\n \nreply",
      "Only few pictures I have them printed, the rest is all digital for the most part, unless you mean tapes/DVD/etc as a physical archive, then yeah I have that too.\n \nreply",
      "Too bad there's not something like DOI or ARK [0] available for anyone to use to give documents a searchable, permanent ID so that a location can be maintained. IME, the half-life of many URLs (5-10 years?) makes them unreliable. I recently was unable to find (by URL) an entire historical collection at a major southern US university until I discovered that it had been moved to a new server.[0] https://arks.org/about/\n \nreply",
      "My understanding is that some photographers are archiving their digital pictures by basically printing them using the 4 color process, which gives them the 4 \"negatives\" (\"postives?\" or whatever they're called) for each color (CMYK I guess).Those sheets are archival quality and should last for quite sometime, given a proper storage environment.They can always use those later to have them scanned back in should they lose their master digital files.\n \nreply",
      "I fully support the efforts.  \nbut are there not legal problems with this?\n(No I dont thik legal issues should prevent this)If I worked for CorporateMediaNews as a columnist and reporter \nfor 10 years and they decide ot remove all of it.\nDoes not CMN own the work and can (unfortunately) dispose of it if\nthey so wish?  I would not have any rights for the work?Thinking about my own career. \nI have written a hell of a lot of code and 80% at least are \nclosed source system for various companies. \nI dont retain any copies of that code.It would be interesting if I heard that System X I wrote 15\nyears ago is being shut down, and I would try to obtain the \nsource code in order to preserve it. \nI have never heard of anyone doing it, but probably in games\nand such it happens more often.\n \nreply",
      "Random old source code is probably pretty hard to do something useful with. But, honestly, random old work product is pretty hard for just about anyone to do something useful with. And even something self-contained (like writing), most of the the stuff I wrote 10 or 20 years ago is pretty uninteresting even if a few things have some historical interest.\n \nreply",
      "It's often probably at least a bit complicated. I cross-posted material between a couple of organizations (one of which is long gone) over a number of  years via pretty much informal agreement. I also reused a fair bit of that material for other purposes. Everyone was OK with the state of affairs but who actually held the copyright? Who knows and I was certainly never going to bring it to a head.As a practical matter if CorporateMediaNews or the like don't care about something any longer, they mostly don't care if someone else makes use of it so long as it isn't embarrassing or misrepresenting the organization.In the end, I have created work that I've reused for a variety of organizations as well as independently and someone other than myself probably would claim copyright to but it's often been pretty loose.\n \nreply",
      "This has been true for a long time. Had I not archived a fair bit of my own work, some of it in the CMSs of dead organizations, some of it inaccessible behind paywalls, much would no longer exist. Journalists are probably in better shape than many because they're more likely to have work they've created on a relatively open web.\n \nreply"
    ],
    "link": "https://www.niemanlab.org/2024/07/to-preserve-their-work-and-drafts-of-history-journalists-take-archiving-into-their-own-hands/",
    "first_paragraph": "When news sites shut down, those sites\u2019 owners often don\u2019t prioritize the preservation of the content.MTV pulled down MTV News in June. After Deadspin was sold, many of its archives temporarily disappeared. This week, Flaming Hydra reported that The Awl\u2019s archives are gone. And those examples are just from the past couple of months; in 2021, the authors of a Reynolds Journalism Institute report found that just 7 out of 24 newsrooms they interviewed were fully preserving their news content.RELATED ARTICLEWe\u2019ll find better ways to archive our workSam FordDecember 17, 2020\u201cIt\u2019s really kind of a web of responsibility in terms of creating an accurate record,\u201d Talya Cooper, a research curation librarian at NYU and The Intercept\u2019s former archivist, told me. \u201cWhen you hear about something being shut down, it\u2019s not just \u2018Wow, all of this content is being lost.\u2019 It\u2019s also all of the content that is derived from this content \u2014\u00a0a key bedrock of evidence that could be used to verify a claim, or bol"
  },
  {
    "title": "Show HN: Hanon Pro \u2013 piano technique and exercises for the digital age (furnacecreek.org)",
    "points": 221,
    "submitter": "albertru90",
    "submit_time": "2024-08-03T05:21:09",
    "num_comments": 88,
    "comments_url": "https://news.ycombinator.com/item?id=41144826",
    "comments": [
      "xxx"
    ],
    "link": "https://furnacecreek.org/hanon/",
    "first_paragraph": "Hanon Pro is an app for iPhone, iPad, and Mac that offers a fresh, modern take on\n\t\t\t\t\t\tkeyboard and piano technique and exercises for the digital age. Just like health and fitness\n\t\t\t\t\t\tapps track your physical health over time, Hanon Pro does the same for piano practice. It\n\t\t\t\t\t\tenables you to get feedback on your playing, visualize trends, and track practice habits over\n\t\t\t\t\t\ttime.The app features a catalog of content specifically designed to look great on iPhone, iPad, and\n\t\t\t\t\t\tMac. We design each book and score with support for responsive layouts, Dark Mode, rich\n\t\t\t\t\t\tmetadata, embedded finger numbers, and more. You can listen to each piece, practice with the\n\t\t\t\t\t\tbuilt-in metronome, and adjust the tempo.The real magic of Hanon Pro is unlocked when you connect your iPhone, iPad, or Mac with a MIDI\n\t\t\t\t\t\tkeyboard or piano over a Bluetooth or USB connection. Hanon Pro's intelligence engine can\n\t\t\t\t\t\tanalyze your playing, including accuracy, tempo, and dynamics, and even turn pag"
  },
  {
    "title": "Western Digital: We Are Sampling 32TB SMR Hard Drives (anandtech.com)",
    "points": 36,
    "submitter": "mfiguiere",
    "submit_time": "2024-08-03T21:27:47",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=41149377",
    "comments": [
      "The 8Tb WD SMR drives I have use a 40gb chunk of the surface for non-SMR reads and writes as a buffer.  Altering anything in an SMR track requires copying the data from that track and tracks near it into the buffer, then laying them back down into the SMR region sequentially.  Like erasing a flash block before writing.  Unfortunately, this means that the drive's performance takes a dump from 200mb/s down to 300kb/s if you do enough transfers to fill the 40gb buffer, or attempt multiple transfers at once.\n \nreply",
      "They still have valid use cases like backups or unedited video footage. It's just kinda lame that the manufacturers don't market them as \"slow backup devices\" clearly listing all limitations, and that you have to find it out first time you use it.\n \nreply",
      "When I have used them for backups, the 40gb buffer quickly fills and then I'm stuck with speeds slower than my internet connection until the buffer empties, which it will only do if I kill the existing transfer.  SSDs can dump 40gb of data very quickly.  Annoyingly, since the r/w head has to do it's back-and-forth dance between the buffer area and the SMR areas, this condition even impacts read speeds.  Consequently I wouldn't even use them for backups.  Nor would I buy them again.  I've set them up as Storj drives, which they seem to handle reasonably, and I am thankful to be done with them otherwise.If you're considering one, I would pay special attention to the buffer size, and ensure that all the transfers you want to do to or from the drive are significantly smaller than the buffer to ensure reasonable performance.  That excludes most video too.  Storj files are typically just a few megabytes, and typically arrive at a frequency of just one or two per second, which the drive can handle.\n \nreply",
      "I had that problem as well (8TB Seagate). It would write some data, then get completely stuck to the point where Windows would report an I/O error. So I wrote a small tool that writes data in smaller chunks, monitors the write speed and allows throttling it if needed.Weirdly enough, just using the tool instead of copying files with Explorer somehow stopped the weird hanging from happening, even without having to enable the actual throttling. Probably some bug somewhere along the driver/firmware stack triggered by the write block sizes.Overall, I wish the drive vendors would expose some API to directly manage the SMR/CMR areas via software, just like the FLASH memory chips do. That would make the job of appending new backups + overwriting the old ones actually manageable with predictable and consistent timing.\n \nreply",
      "I sure hope it honors fsync!\n \nreply",
      "Not sure, but interestingly SMR drives support TRIM commands: https://superuser.com/questions/1407990/what-does-trim-on-an...\n \nreply",
      "In case anybody got curious about the triple stage actuators, here is a nice article:https://www.mdpi.com/2076-0825/8/3/65\n \nreply",
      "I wonder why drives still use mechanical actuators to fling a single read write head back and forth across the surface, rather than a bar fixed in place that spans entire radius of the writable surface, with an addressable strip of r/w heads that runs the length of it.I assume it's because the actual read-write head itself must among the most expensive parts to manufacture, but is there no photo mask approach to this that can manufacture such a head array (or staggered series of them if there is some interference problem that prevents putting them side by side in a single strip) the same way but processor dies are made?\n \nreply",
      "I suspect the head flying just above the surface has something to do with it.A bar would need to be held at a fixed height above the surface. The flying head dynamically adjusts to both distortions across the surface, and temporal variations as the platter changes temperature. This height adjustment is entirely passive, dependant on aerodynamics rather than any electronics.\n \nreply",
      "I remember someone explaining this to me years ago when IOmega released those removable Bernoulli drive cartridges. The (for the time) high densities were possible because even though the recording medium was a flexible bit of plastic, the head was designed to draw surface of the media up to itself using the namesake effect, rather than relying entirely om precision machine tolerances. And this effect was dynamically self limiting, such that there could not be a head crash as long is the unit wasnt disturbed during operation.What I did not know is that this is still and effect that is relied upon in modern, ultra high precision, fixed medium drives.\n \nreply"
    ],
    "link": "https://www.anandtech.com/show/21498/western-digital-we-are-sampling-32-tb-smr-hard-drives",
    "first_paragraph": "In an unexpected announcement during their quarterly earnings call this week, Western Digital revealed that it has begun sampling an upcoming 32TB hard drive. The nearline HDD is aimed at hyperscalers, and relies on a combination of Westen Digital's EAMR technology, as well as shingled magnetic recording (SMR) technology to hit their highest capacity figures to date.Western Digital's 32TB HDD uses all of the company's most advanced technologies. Besides energy-assisted magnetic recording (EAMR/ePMR 2\u00a0to be more precise) technology, WD is also leveraging triple-stage actuators for better positioning of heads and two-dimensional (TDMR) read heads,\u00a0OptiNAND\u00a0for extra performance and reliability, distributed sector (DSEC) technology and a proprietary error correcting code (ECC) technology. And, most importantly, UltraSMR technology to provide additional capacity.\"We are shipping\u00a0samples of our 32TB UltraSMR/ePMR nearline hard drives to select customers,\" said David Goeckeler, chief executi"
  },
  {
    "title": "Additive manufacturing of an ultrastrong, deformable aluminum alloy (nature.com)",
    "points": 68,
    "submitter": "PaulHoule",
    "submit_time": "2024-08-02T17:09:19",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=41140623",
    "comments": [
      "Commonplace aluminum alloy 7068 is also \"ultrastrong\" (~710-750MPa, more than the 700MPa in the paper,) and ductile.WWII-vintage Japanese duralumin (Al-7075) is nearly as strong (~600MPa), and very ductile.  It's also nearly 100 years old.I guess the innovation here is that they're making this alloy with additive manufacturing techniques?  It's not that noteworthy, IMO.  It would be jaw-dropping if it were a 1000MPa alloy -- that's like the Holy Grail for aluminum -- but they're still far from that mark.\n \nreply",
      ">Additive manufacturing was performed by using a laser powder bed fusion (LPBF) instrument, SLM 125 HL metal 3D printer in Argon atmosphere with the oxygen level below 1000 PPM. Printing was conducted by utilizing a 400 W IPG fiber laser (\u03bb = 1070 nm) with a laser power of 200\u2013300 WSounds expensive.\n \nreply",
      "In materials science, are alloys as seemingly complex as \"Al92Ti2Fe2Co2Ni2\" commonplace?How do we arrive at that kind of alloy? (If it was explained in the paper, I didn't understand it)\n \nreply",
      "As mentioned in the paper, this is a typical \"medium-entropy\" alloy.Pure metals are soft, because all their atoms have the same size, so the atom layers can slide over the others.Mixing metals with different sizes increases the strength, because now the atom layers are no longer smooth, but they have bumps, which prevent sliding.It can be shown that mixing many different kinds of atoms, taking from each about the same quantity, can provide very good mechanical properties, because the bumps in the atom layer will be frequent and they will have varied sizes and a random distribution, which will prevent any alignment between bumps, which could facilitate the sliding of the layers. Think about how to design an anti-sliding shoe sole. Random bumps of random size would give the best result.The so-called \"high entropy\" alloys contain at least 5 different metals, with about the same quantity from each.However the alloys that contain almost equal quantities of each component are very expensive. In order to make a cheap alloy, one must have one or at most two components in a much larger quantity than the others, so that the abundant components can be chosen from the few cheap metals, e.g. iron, manganese or aluminum, while the other components, which are added in small quantities, can be chosen from expensive metals, like nickel and cobalt.For this reason, the better \"high-entropy\" alloys are normally replaced by the cheaper \"medium-entropy\" alloys, which use 5 metals, like the \"high-entropy\", but which are used in quite different quantities, with larger quantities from the cheaper metals, if possible.The use of \"high-entropy\" alloys and \"medium-entropy\" alloys has begun only relatively recently. They are used to replace the cheaper classic alloys only when they offer a decisive advantage that can justify their higher cost.This case is one such example. From the classic aluminum alloys, some of the weaker alloys, like AlSi or AlMgSi can be easily 3D printed, but they have low strength. The classic high-strength aluminum alloys cannot be 3D printed. Therefore this was a clear case when a newer kind of alloy must be tried, if high strength is desired. They have experimented with certain kinds of medium-entropy aluminum alloys, to keep the cost acceptable (and also in this case the high content of aluminum keeps the density low and the conductivity high, which are frequently the reasons for choosing an aluminum alloy), and the results were good.Nevertheless, this alloy is likely to be several times more expensive than AlSi or AlMgSi, so it will be used only when its high strength is necessary.\n \nreply",
      "This is not a medium entropy alloy, it's a standard alloy in terms of the ratio of components, which forms medium entropy intermetallic precipitates which gives the alloy it's properties. Intermetallic MEA is an odd term I'm not really familiar with and would want to look into more, but is a little suspicious. Furthermore, while MEAs (3-4 equal primary components) and HEAs (5+ equal components) do have good mechanical properties, I'd be wary of the atomic size argument, last time I've been involved in it, that argument has increasingly been questioned, as the atomic size of the elements in question are generally pretty similar.\n \nreply",
      "I get the sense that stronger alloys are more \"brittle\" and harder to do things like welding, as they'll crack instead of yielding from all the thermal stresses. This is probably the same sort of thing with laser melting and 3D printing: solidification under high thermal gradients. It seems this material is not only high-strength but also ductile enough to gracefully handle the thermal stresses.\n \nreply",
      "It's more complex than that. A lot of the material properties depend on both the cooling and the tempering in aluminum alloys.The phase diagrams for these types of alloys look wild (you often want to achieve \na certain material phase during cooling to \"lock\" in to get certain characteristics), and it can be difficult to ensure that the  smaller metals participate during cooling. Also difficult to dissipate these slightly during tempering, typically to increase ductility.This is probably why 3d printing hasn't been done in earnest, you can't design something within tight tolerances with unknown material properties.\n \nreply",
      "3D printing of metals is being done in earnest, although the industry prefers the term Additive Manufacturing. Metal powder bed fusion is a stable, reliable process that is being successfully used commercially. It's generally confined to high-value applications that require extreme geometric complexity, but it can be invaluable in industries like aerospace, motorsport and medical. The range of viable materials is still somewhat limited, but covers a good range from titanium and aluminium alloys through to tool steels and heat-resistant super-alloys.https://www.renishaw.com/en/metal-3d-printing--32084https://www.trumpf.com/en_US/products/machines-systems/addit...https://www.carpenteradditive.com/powderrange-metal-powders\n \nreply",
      "So you need to control the solidification process to plot a course through the phase diagram, spending the right amount of time in each region, and ending up in a good place. And this alloy has a phase diagram that is compatible with a method of 3d printing.\n \nreply",
      "It is not uncommon to have an alloy with 10 elements. Usually it is 1 to 3 main elements and some minor additives, and that\u2019s the case here.Steels can be very complex as well.\n \nreply"
    ],
    "link": "https://www.nature.com/articles/s41467-024-48693-4",
    "first_paragraph": "Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\n            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\n            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\n            and JavaScript.Advertisement\nNature Communications\nvolume\u00a015, Article\u00a0number:\u00a05122 (2024)\n            Cite this article\n4534 Accesses94 AltmetricMetrics detailsLight-weight, high-strength, aluminum (Al) alloys have widespread industrial applications. However, most commercially available high-strength Al alloys, like AA 7075, are not suitable for additive manufacturing due to their high susceptibility to solidification cracking. In this work, a custom Al alloy Al92Ti2Fe2Co2Ni2 is fabricated by selective laser melting. Heterogeneous nanoscale medium-entropy intermetallic lamella form in the as-printed Al alloy. Macroscale compression tests "
  }
]