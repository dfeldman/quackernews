[
  {
    "title": "WeatherStar 4000+: Weather Channel Simulator (netbymatt.com)",
    "points": 487,
    "submitter": "adam_gyroscope",
    "submit_time": "2025-05-29T15:38:45 1748533125",
    "num_comments": 89,
    "comments_url": "https://news.ycombinator.com/item?id=44127109",
    "comments": [
      "If you're interested in this, someone on YouTube got a WeatherStar 4000 (device that sat at cable headends and generated the local weather report graphics) and wrote all new firmware to make 90's style weather reports on the real hardware. This was necessary because the original firmware was downloaded over satellite so it's now lost. It looks basically identical to the real Weather Channel from the 90s, except it doesn't have their logo in the corner (I guess for trademark reasons). Here's a stream of his WeatherStar: https://www.youtube.com/watch?v=66mSjXpfD2c\n \nreply",
      "If it's the same one that brings the equipment to Retro Computing events. He sadly has declined to publish any kind of archive of the software for other hobbyists :(I understand that he's under no obligation to do so. But a lot of us worry that if the hard disks die or if he loses interest in the hobby, that software will be irrevocably lost for all of us\n \nreply",
      "The youtube stream has a link in the description for EEPROM dumps https://hackaday.io/project/178144-reverse-engineering-the-w...\n \nreply",
      "Importantly, it also plays Weather Channel-esque background music.\n \nreply",
      "Thanks for mentioning that, had me digging around for where I can find the music. The youtube link lead me to the project's hackaday log, which is extremely detailed but lacks any mention of music [0]The submission link's github page [1] links to a website listing all the tracks that ever played [2], explaining they dropped the music from the project so as not to deal with copyright claims. too bad fair use isn't clear enough to apply here, I think its a relatively transformative use and doesn't compete with the original.[0] https://hackaday.io/project/178144-reverse-engineering-the-w...[1] https://github.com/netbymatt/ws4kp[2] https://twcclassics.com/audio/artists.html\n \nreply",
      "why would fair use be at play here? TWC would have paid a license fee through ASCAP or whomever for the rights to broadcast that music. They didn't just download a bunch of mp3 files from Napster and try to disrupt broadcasting.\n \nreply",
      "You're right, of course. I guess I just put a lot of weight on the charm of keeping old things running, and think there's value to the public in allowing free use of music, particularly for non-commercial/educational purposes.\n \nreply",
      "Not sure if you are aware but the weather Channel sold music CDs I have one.\n \nreply",
      "Can also stream it at Internet archive:https://archive.org/details/WeatherChannelMusicYouTube has a few TWC playlists:https://www.youtube.com/playlist?list=PLMUZjd023YBtp0iB962Uz...https://www.youtube.com/watch?v=hYyTo0ex76Q&list=PL2UoJXK3rr...https://www.youtube.com/watch?v=wmGd-cx-dXcIt\u2019s on Twitch:https://m.twitch.tv/retroweatherchannel?desktop-redirect=tru...There\u2019s a nice Spotify playlist:https://open.spotify.com/playlist/0WEViIh23PGbLqGbjYNAZzAnd the site dedicated to it mentioned earlier in the thread has some recordings:https://twcclassics.com/\n \nreply",
      "If you read his devlog, he started this project not knowing assembly or C... teaching himself as he went. Incredible.\n \nreply"
    ],
    "link": "https://weatherstar.netbymatt.com/",
    "first_paragraph": ""
  },
  {
    "title": "FLUX.1 Kontext (bfl.ai)",
    "points": 256,
    "submitter": "minimaxir",
    "submit_time": "2025-05-29T17:40:35 1748540435",
    "num_comments": 80,
    "comments_url": "https://news.ycombinator.com/item?id=44128322",
    "comments": [
      "Currently am testing this out (using the Replicate endpoint: https://replicate.com/black-forest-labs/flux-kontext-pro). Replicate also hosts \"apps\" with examples using FLUX Kontext for some common use cases of image editing: https://replicate.com/flux-kontext-appsIt's pretty good: quality of the generated images is similar to that of GPT-4o image generation if you were using it for simple image-to-image generations. Generation is speedy at about ~4 seconds per generation.Prompt engineering outside of the examples used on this page is a little fussy and I suspect will evolve over time. Changing styles or specific aspects does indeed work, but the more specific you get, the more it tends to ignore the specifics.\n \nreply",
      "It seems more accurate than 4o image generation in terms of preserving original details. If I give it my 3D animal character and ask it for a minor change like changing the lighting, 4o will completely mangle the face of my character, it will change the body and other details slightly. This Flux model keeps the visible geometry almost perfectly the same even when asked to significantly change the pose or lighting\n \nreply",
      "gpt-image-1 (aka \"4o\") is still the most useful general purpose image model, but damn does this come close.I'm deep in this space and feel really good about FLUX.1 Kontext. It fills a much-needed gap, and it makes sure that OpenAI / Google aren't the runaway victors of images and video.Prior to gpt-image-1, the biggest problems in images were:  - prompt adherence\n  - generation quality\n  - instructiveness (eg. \"put the sign above the second door\")\n  - consistency of styles, characters, settings, etc. \n  - deliberate and exact intentional posing of characters and set pieces\n  - compositing different images or layers together\n  - relighting\n\nFine tunes, LoRAs, and IPAdapters fixed a lot of this, but they were a real pain in the ass. ControlNets solved for pose, but it was still awkward and ugly. ComfyUI was an orchestrator of this layer of hacks that kind of got the job done, but it was hacky and unmaintainable glue. It always felt like a fly-by-night solution.OpenAI's gpt-image-1 solved all of these things with a single multimodal model. You could throw out ComfyUI and all the other pre-AI garbage and work directly with the model itself. It was magic.Unfortunately, gpt-image-1 is ridiculously slow, insanely expensive, highly censored (you can't use a lot of copyrighted characters or celebrities, and a lot of totally SFW prompts are blocked). It can't be fine tuned, so you're suck with the \"ChatGPT style\" and (as is called by the community) the \"piss filter\" (perpetually yellowish images).And the biggest problem with gpt-image-1 is because it puts image and text tokens in the same space to manipulate, it can't retain the exact precise pixel-precise structure of reference images. Because of that, it cannot function as an inpainting/outpainting model whatsoever. You can't use it to edit existing images if the original image mattered.Even with those flaws, gpt-image-1 was a million times better than Flux, ComfyUI, and all the other ball of wax hacks we've built up. Given the expense of training gpt-image-1, I was worried that nobody else would be able to afford to train the competition and that OpenAI would win the space forever. We'd be left with only hyperscalers of AI building these models. And it would suck if Google and OpenAI were the only providers of tools for artists.Black Forest Labs just proved that wrong in a big way! While this model doesn't do everything as well as gpt-image-1, it's within the same order of magnitude. And it's ridiculously fast (10x faster) and cheap (10x cheaper).Kontext isn't as instructive as gpt-image-1. You can't give it multiple pictures and ask it to copy characters from one image into the pose of another image. You can't have it follow complex compositing requests. But it's close, and that makes it immediately useful. It fills a much-needed gap in the space.Black Forest Labs did the right thing by developing this instead of a video model. We need much more innovation in the image model space, and we need more gaps to be filled:  - Fast\n  - Truly multimodal like gpt-image-1\n  - Instructive \n  - Posing built into the model. No ControlNet hacks. \n  - References built into the model. No IPAdapter, no required character/style LoRAs, etc. \n  - Ability to address objects, characters, mannequins, etc. for deletion / insertion. \n  - Ability to pull sources from across multiple images with or without \"innovation\" / change to their pixels.\n  - Fine-tunable (so we can get higher quality and precision) \n \nSomething like this that works in real time would literally change the game forever.Please build it, Black Forest Labs.All of those feature requests stated, Kontext is a great model. I'm going to be learning it over the next weeks.Keep at it, BFL. Don't let OpenAI win. This model rocks.Now let's hope Kling or Runway (or, better, someone who does open weights -- BFL!) develops a Veo 3 competitor.I need my AI actors to \"Meisner\", and so far only Veo 3 comes close.\n \nreply",
      "Honestly love Replicate for always being up to date. It\u2019s amazing that not only do we live in a time of rapid AI advancement, but that every new research grade model is immediately available via API and can be used in prod, at scale, no questions asked.Something to be said about distributors like Replicate etc that are adding an exponent to the impact of these model releases\n \nreply",
      "That's less on the downstream distributors, more on the model developers themselves realizing that ease-of-accessibility of the models themselves on Day 1 is important for getting community traction. Locking the model exclusively behind their own API won't work anymore.Llama 4 was another recent case where they explicitly worked with downstream distributors to get it working Day 1.\n \nreply",
      "I have no affiliation with either company but from using both a bunch as a customer: Replicate has a competitor at https://fal.ai/models and FAL's generation speed is consistently faster across every model I've tried. They have some sub-100 ms image gen models, too.Replicate has a much bigger model selection. But for every model that's on both, FAL is pretty much \"Replicate but faster\". I believe pricing is pretty similar.\n \nreply",
      "Founder of Replicate here. We should be on par or faster for all the top models. e.g. we have the fastest FLUX[dev]: https://artificialanalysis.ai/text-to-image/model-family/flu...If something's not as fast let me know and we can fix it. ben@replicate.com\n \nreply",
      "Hey Ben, thanks for participating in this thread. And certainly also for all you and your team have built.Totally frank and possibly awkward question, you don't have to answer: how do you feel about a16z investing in everyone in this space?They invested in you.They're investing in your direct competitors (Fal, et al.)They're picking your downmarket and upmarket (Krea, et al.)They're picking consumer (Viggle, et al.), which could lift away the value.They're picking the foundation models you consume. (Black Forest Labs, Hedra, et al.)They're even picking the actual consumers themselves. (Promise, et al.)They're doing this at Series A and beyond.Do you think they'll try to encourage dog-fooding or consolidation?The reason I ask is because I'm building adjacent or at a tangent to some of this, and I wonder if a16z is \"all full up\" or competitive within the portfolio. (If you can answer in private, my email is [my username] at gmail, and I'd be incredibly grateful to hear your thoughts.)Beyond that, how are you feeling? This is a whirlwind of a sector to be in. There's a new model every week it seems.Kudos on keeping up the pace! Keep at it!\n \nreply",
      "A16Z invested in both. It's wild. They've been absolutely flooding the GenAI market for images and videos with investments.They'll have one of the victors, whoever it is. Maybe multiple.\n \nreply",
      "> Generation is speedy at about ~4 seconds per generationMay I ask on which GPU & VRAM?edit: oh unless you just meant through huggingface's UI\n \nreply"
    ],
    "link": "https://bfl.ai/models/flux-kontext",
    "first_paragraph": "New model suite for text-and-image-driven generation and editing.Contextual understandingTransform elements while preserving their relationship to the surrounding sceneFLUX.1 Kontext is a suite of generative flow matching models that allow you to generate and edit images. Unlike existing text-to-image models, the FLUX.1 Kontext family performs in-context image generation, allowing you to prompt with both text and images, and seamlessly extract and modify visual concepts to produce new, coherent renderings.FLUX.1 Kontext models go beyond text-to-image. Unlike previous flow models that only allow for pure text based generation, FLUX.1 Kontext models also understand and can create from existing images. With FLUX.1 Kontext you can modify an input image via simple text instructions, enabling flexible and instant image editing - no need for finetuning or complex editing workflows. The core capabilities of the the FLUX.1 Kontext suite are:Preserve unique elements of an image, such as a refere"
  },
  {
    "title": "Show HN: I wrote a modern Command Line Handbook (stribny.name)",
    "points": 251,
    "submitter": "petr25102018",
    "submit_time": "2025-05-29T14:44:33 1748529873",
    "num_comments": 64,
    "comments_url": "https://news.ycombinator.com/item?id=44126612",
    "comments": [
      "Cool book!Humble suggestion: Give more specific examples of what the reader will learn on the landing page. From the landing page, I don't know if this is for total command line beginners or has helpful tips for people familiar with bash already. I had to hunt around to find sample pages, and they provided much better sense of what's in the book.Also, spotted some small issues in the copy:\"Fresh out of press\" - The more common expression is \"hot off the press.\" \"Out of press\" sounds similar to \"out of print\" (i.e., no longer sold)\"Grok the Linux command line on only 120 pages\" - should be \"in only 120 pages\"\n \nreply",
      "True, the landing page could be more detailed. I didn't want to repeat much the info on Gumroad page, but maybe I should rethink it.And thanks for the copy suggestions. I am not a native speaker :)\n \nreply",
      "I definitely think you should add more info to the landing page - I had no idea there was further information on the Gumroad page.\n \nreply",
      "Cool, couple of comments. The website it a bit broken on mobile (at least for me) since text goes off the screen. Second, it would be good to have some sample pages or at least a page of contents so people can see the level of detail the book is aimed at. I know I could just get the book for free and then pay later but that kind of a faff and I feel bad choosing $0 for these kinds of things.\n \nreply",
      "Thanks for the feedback. I was trying to make it work on mobile but maybe I didn't test it well in the end.As for a sample, I will go and try to make something now. Thanks.Edit: Example pages here: https://drive.google.com/file/d/1PkUcLv83Ib6nKYF88n3OBqeeVff...\n \nreply",
      "One nit: ^D only exits if the cursor is at the beginning of a line. Enter <space>^D and the session remains.Otherwise, looks good! Use of \"env\" and proper quoting are strong signals!\n \nreply",
      "Thanks for the clarification. I will see if I can sneak in a note about this in the next update :)\n \nreply",
      "I agree with all this. I'm on Firefox android, pixel, lose part of the screen too.I'd love to read a table of contents. I don't want to rip you off with zero dollars!Anyway, sounds like a great book. Congratulations on completing and \"shipping\"!\n \nreply",
      "> I agree with all this. I'm on Firefox android, pixel, lose part of the screen too.Ah okay, I really wasn't aware. Thanks for reporting.\n \nreply",
      "I am also seeing text off the screen using Brave on Android.\n \nreply"
    ],
    "link": "https://commandline.stribny.name/",
    "first_paragraph": "\n              Grok the Linux command line on only 120 pages.\n            \nPay what you want. Buy on\n\n            Software developers, sysadmins, tech workers, and regular Linux/macOS\n            users all use the modern Unix/Linux command-line shells to get things\n            done. What about you?\n          \n            You don't have to read the entire shell manual or study bulky Linux\n            bibles to get going. Understand the most common concepts and commands\n            quickly instead with this short handbook.\n          \n            Terminals, shells, command-line applications, and shell scripting.\n            Don't learn things separately when you can learn them together. Tips and tricks included.\n          More than one hundred annotated shell sessions and code examples to explore and follow along. If this\n            doesn't change your approach to the command line, nothing will.\n          This handbook is a culmination of 4 years of learning, writing, and rewriting. Upda"
  },
  {
    "title": "Making C and Python Talk to Each Other (leetarxiv.substack.com)",
    "points": 74,
    "submitter": "muragekibicho",
    "submit_time": "2025-05-27T11:06:10 1748343970",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=44105746",
    "comments": [
      "I realize I'm talking about C++ not C, but coincidentally just today I ported our 7 year old library's Swig/Python interface to nanobind. What a fragile c9k Swig has been all these years (don't touch it!) and the nanobind transformation is so refreshing and clean, and lots of type information suddenly available to Python programs. One day of effort and our tests all pass, and now nanobind seems able to allow us to improve the ergonomics (from the Python pov) of our lib.\n \nreply",
      "My visualization library [1] is written in C and exposes a visualization API in C. It is packaged as a Python wheel using auto-generated ctypes bindings, which includes the shared library (so, dylib, or dll) and a few dependencies. This setup works very well, with no need to compile against each Python version. I only need to build it for the supported platforms, which is handled automatically by GitHub Actions. The library is designed to minimize the number of C calls, making the ctypes overhead negligible in practice.[1] https://datoviz.org/\n \nreply",
      "Lots of articles focus on Cython and optimizing Python using C code.This article is about embedding Python scripts inside a C codebase\n \nreply",
      "Once I needed to implement a simple python plugin engine in a C/C++ software, I've been successfully using the official guide [1].[1] https://docs.python.org/3/extending/embedding.html\n \nreply",
      "This is one of the \"killer apps\" for Nim. Nim makes makes it easy to wrap C and easy to talk to Python (via Nimpy).\n \nreply",
      "The title of the article is misleading. Making C and python talk to each other implies, calling python from C and calling C from python. The article only covers the former.\n \nreply",
      "Thanks a lot for the article. Here's a QQ: did you measure the time of some basic operations python vs C? (e.g. if I do a loop of 10 billion iterations, just dividing numbers in C and do the same in python, and then  import these operations into one another as libraries, does anything change?)I'm a beginner engineer so please don't judge me if my question is not making perfect sense.\n \nreply",
      "C is many magnitudes faster than Python and you can measure this using nested conditionals. Python is built for a higher level of abstraction and this comes at the cost of speed. It is what makes it very natural and human-like to write in.\n \nreply",
      "Syntax has nothing to do with the speed of the language: python could be \"natural\" and \"human-like\" while being much faster and also \"unnatural\" and \"inhuman\" while being slower.\n \nreply",
      "It does, actually, as the syntax is a result of the language's design and a simpler and more human-like syntax requires a higher level of abstraction that reduces efficiency.The design of a language, including its syntax, has a great bearing on its speed and efficiency.Compare C with Assembly, for example, and you will see that higher level languages take complex actions and simplify them into a more terse syntax.You will also observe that languages such as Python are not nearly as suitable for lower level tasks like writing operating systems where C is much more suitable due to speed.Languages like Python and Ruby include a higher level of built-in logic to make writing in them more natural and easy at the cost of efficiency.\n \nreply"
    ],
    "link": "https://leetarxiv.substack.com/p/making-c-and-python-talk-to-each",
    "first_paragraph": ""
  },
  {
    "title": "Learning C3 (alloc.dev)",
    "points": 201,
    "submitter": "lerno",
    "submit_time": "2025-05-29T13:33:31 1748525611",
    "num_comments": 125,
    "comments_url": "https://news.ycombinator.com/item?id=44125966",
    "comments": [
      "Some other links links on C3 that might be interesting:Interviews:- https://www.youtube.com/watch?v=UC8VDRJqXfc- https://www.youtube.com/watch?v=9rS8MVZH-vAHere is a series doing various tasks in C3:- https://ebn.codeberg.page/programming/c3/c3-file-io/Some projects:- Gameboy emulator https://github.com/OdnetninI/Gameboy-Emulator/- RISCV Bare metal Hello World: https://www.youtube.com/watch?v=0iAJxx6Ok4E- \"Depths of Daemonheim\" roguelike https://github.com/TechnicalFowl/7DRL-2025Tsoding's \"first impression\" of C3 stream:- https://www.youtube.com/watch?v=Qzw1m7PweXs\n \nreply",
      "C3 looks promising, but any language that supports nulls needs null-restricted types, not whatever those contract comments are. If I wanted to have to null-check everything, or YOLO it, I would just write Java... and even Java is seeking to fix this: https://openjdk.org/jeps/8303099\n \nreply",
      "It's an interesting problem. Originally I experimented with both having `` and `&` syntax, so `int&` being a ref (non null) and `int` being a pointer. The thing you notice then are two things:1. You want almost all pointer parameters non null.2. Non-null variables is very hard to fit in a language without constructors.Approaches to avoid constructors/destructors such as ZII play very poorly with ref values as well. What you end up with is some period of time where a value is quasi valid - since non-null types need to be assigned and it's in a broken state before it's initially assigned.It's certainly possible to create generic \"type safe\" non-null types in C3, but they are not baked into the language.\n \nreply",
      "I'm unable to edit this now... that should teach me not to comment and then go to kendo practice... It should say '*' and '&' and 'int&' and 'int*'\n \nreply",
      "the hacker news markdown parser seems to have swallowed your asterisks, which are essential to understanding your comment.\n \nreply",
      "Yeah, if you want to use asterisks without italicizing your text, you need to escape them with backslashes, and then you can write things like 5 * 2 * 1 = 10. That is, you'd write it like this:  5 \\* 2 \\* 1 = 10\n \nreply",
      "Is there a place this is all written down? It's not in the FAQ...\n \nreply",
      "Yes, and I was too slow to getting back and trying to edit it. Sorry about that.\n \nreply",
      "> Approaches to avoid constructors/destructors such as ZII play very poorly with ref values as well. What you end up with is some period of time where a value is quasi valid - since non-null types need to be assigned and it's in a broken state before it's initially assigned.I don't see that as a problem; don't separate declaration from assignment and it will never be unassigned.  Then a ZII non-null pointer is always a compile-time error.\n \nreply",
      "> don't separate declaration from assignment and it will never be unassignedThat's tricky when you want to write algorithms where you can start with an uninitialized object and are guaranteed to have initialized the object by the time the algorithm completes. (Simplest example - create an array B which contains the elements of array A in reverse order.)You can either allow declaring B uninitialized (which can be a safety hazard) or force B to be given initial values for every element (which can be a big waste of time for large arrays).\n \nreply"
    ],
    "link": "https://alloc.dev/2025/05/29/learning_c3",
    "first_paragraph": ""
  },
  {
    "title": "My website is ugly because I made it (goodinternetmagazine.com)",
    "points": 425,
    "submitter": "surprisetalk",
    "submit_time": "2025-05-28T11:53:47 1748433227",
    "num_comments": 119,
    "comments_url": "https://news.ycombinator.com/item?id=44114982",
    "comments": [
      "Hah, yes! Whereas most of my developer friends have long ago moved to off-the-shelf Hugo or Jekyll templates for their personal sites, I stubbornly maintain my blog with entirely bespoke css and a backend only a parent could love.For me, the joy is not in having a website, the joy is in building the website. Why would I want to hand off the joyful part?It's like maintaining a classic car. You can buy a reliable decent looking car, but that's not fun. If your goal is just to get somewhere, sure, but my goal is to have fun.I work on websites all day where I get less and less say in the design and functionality. Why would I not want total control over my own?\n \nreply",
      "Exactly this. My entire website is handcrafted, and not once but over the last decade almost ~10 times.It's fun and I almost end up revamping something every year.Everything handcrafted:- the matrix js code on home page. https://oxal.org click on the matrix for a surprise!- it's built using my own Static Site Generator: https://github.com/oxalorg/genox- my website uses a css theme, again handcrafted: https://github.com/oxalorg/sakura/- if you go to https://oxal.org/blog/ you will see a small cyborg following you (started with a base image generated by chatgpt and then edited and added animations manually in Piskel sprite editor)- it's deployed on a VPS manually, just run `make` (I've experimented with serving it via a handwritten C http server, but I haven't finished this toy project yet)- i have several shell scripts which uploads things to my websites in private locations (think gists, quick share videos, screenshots etc.).- the favicon is also pixel art, made when I was still in college! https://oxal.org/favicon-32x32.png- I even tried designing my own funky font but gave up and used a Naruto inspired font- and as a bonus, try to `view-page-source` on the home pageI see my website and feel extremely proud of my journey as a software engineer, and I cherish this simple thing oh so dearly!\n \nreply",
      "It\u2019s good to see you here! For a long time I was just using your project Sakura CSS file to mane everything look pretty.Even though I have moved on to using a mix of LaTeX.css and a two column theme, I still love using Sakura whenever I\u2019m crafting a hand rolled HTML page for something.\n \nreply",
      "I really like your website, it's both very clear / easy to navigate and yet unusual.Great work!\n \nreply",
      "That\u2019s hilarious, I was just using Sakura not long ago for a small mvp I made where I couldn\u2019t be arsed to write any css myself. Good stuff\n \nreply",
      "I quite like the matrix w/ the surprise!\n \nreply",
      "The floating robot makes me smile.  Reminds me of 90s silliness.  I love it!\n \nreply",
      "> For me, the joy is not in having a website, the joy is in building the website. Why would I want to hand off the joyful part?This is exactly it!My personal website https://pablo.rauzy.name/ is also entirely handcrafted, I use a few custom Bash scripts and a Makefile to build it (it is entirely static, no server side rendering, and not a single line of JS), and I have a lot of fun playing with CSS for example to make it responsive, have a mobile menu, etc. I probably (re)invented a few techniques in doing so but that's what's fun!\n \nreply",
      "I'll add one thing: since April 2009 my website files are tracked using Git, which means I can go back to what it looked like at any point in time whenever I want (`git rev-list --count HEAD` gives me 2184 commits). It's been fun to show my students what my own website looked like when I was their age!\n \nreply",
      "I love the idea of the colored links for navigation in your summary. Thanks for the inspiration!\n \nreply"
    ],
    "link": "https://goodinternetmagazine.com/my-website-is-ugly-because-i-made-it/",
    "first_paragraph": "If my mom wanted good art on her fridge, she could\u2019ve purchased reprints of works by Vermeer, Lichtenstein, Wyeth, etc. But she didn\u2019t want good art \u2013 she wanted my art.Somebody with good taste could\u2019ve made my website, but then it wouldn\u2019t be mine.To bake bread, many feel compelled to grow wheat, mine salt, culture yeast, etc. Not me. My puerile palate yearns for buckets of Olive Garden breadsticks.That\u2019s okay. Your \u201cmine\u201d is not my \"mine.\"Some folks run self-hosted websites on solar power. Others share wisdom on Substack. This guy posts manufacturing deep-dives to YouTube. Gwern doeswhatever this is.It\u2019s an itch \u2013 a feeling that something is really important, and you need to do something about it, and nobody else can possibly do it except you.I am not compelled to bake bread, nor provision servers, nor build chips. Yet that itch pervades, and it pulls me toward humor and systems and life and software and structure. And when emotion becomes unbearable, it erupts out of me: fiction, HT"
  },
  {
    "title": "A visual exploration of vector embeddings (pamelafox.org)",
    "points": 113,
    "submitter": "pamelafox",
    "submit_time": "2025-05-28T20:21:47 1748463707",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=44120306",
    "comments": [
      "I took a completely different path to visualizing embedding vectors\u2019 physical layout [1] - mainly to explain how the data structures, data volumes and comparison would radically differ, compared to your regular btree index searches. I made sure to mention that you can\u2019t make any conclusions based on just human eyeballing of these vector heatmaps, but the database people I\u2019ve demoed this to, seem to have reached some a-ha moments about understanding how radically different a vector search is compared to the usual database index lookup work:[1]  https://tanelpoder.com/posts/comparing-vectors-of-the-same-r...\n \nreply",
      "The more I've studied this stuff the less useful I actually think the vitalizations are. Pamela uses the classic approach and I'm not trying to call her wrong but I think our intuitions really fail us outside 2D and 3D.Once you move up in dimensionality things get really messy really fast. There's a contraction in variance and the meaning of distance becomes much more fuzzy. You can't differentiate your nearest neighbor from your furthest. Angles get much harder too. Everything is orthogonal, in most directions too! I'm not all that surprised \"god\" and \"dog\". I EXPECT them to be. After all, they are the reverse of one another. The question rather is about \"similar in which direction?\"There's no need to believe you've measured along a direction that is human meaningful. So doesn't have to be semantics. Doesn't have to be permutations either. Just like you can rotate your xy axis and travel in both directions.So these things can really trick us. At best, be very careful to not become overly reliant upon them\n \nreply",
      "I actually explicitly left the PCA graphs out of the blog post version, as I think they lose so much information as to be deceiving. That's what I told folks in person at the poster session as well.I think the other graphs I included aren't deceiving, they're just not quite as fun as an attempt to visualize the similarity space.\n \nreply",
      "I think the heatmap + dendogram approach can be useful for high dimensional comparisons (to a degree). Check out ClustViz for an interactive demo[0].[0]: https://biit.cs.ut.ee/clustvis/\n \nreply",
      "Yeah PCA gets tough. It isn't great for non-linear relationships and I mean that's the whole reason we use activation functions haha. And don't get me started on how people refer to t-SNE as dimensionality reduction instead of visualization...I don't think the other graphs are necessarily deceiving but I think they don't capture as much information as we often imply and I think this ends up leading people to make wrong assumptions about what is happening in the data.Embeddings and immersions get really fucking weird at high dimensions. I mean it gets weird at like 4D and insane by 10D. The spaces we're talking about are incomprehensible. Every piece of geometric intuition you have should be thrown out the window. It won't help you, it harms you. If you start digging into the high dimensional statistics and metric theory for high dimensions you'll quickly see what I'm talking about. Like the craziness of Lp distances and contraction of variance. Like you have to really dig into why we prefer L1 over L2 and why even fractional ps are of interest. We run into all kinds of problems with i.i.d. assumptions and all that. It is wild how many assumptions are being made that we generally don't even think about. They seem obvious and natural to use, but they don't work very well when D>3. I do think the visualizations become useful again once you start getting used to this again but that's more like in the way that you are interpreting it with far less generalization in meaning.I'm not trying to dunk on your post. I think it is fine. But I think our ML community needs to be having more conversations about these limits. We're really running into issues with them.\n \nreply",
      "I think visually so very helpful thanks. I also agree once you get into higher dimensionality it becomes difficult to represent visually.\nNevertheless helpful for an 'old' (50) computer scientist wrapping my head around AI concepts so I can keep up with my team.\n \nreply",
      "Geometry in higher dimensions is not only hard to imagine, it's straight up weird.Take a cube on N dimensions and pack N dimensional spheres inside that cube. Then fit a sphere inside the cube so that it touches but doesn't overlap with any of the other spheres.In 2D and 3D is easy to visualize and you can see that sphere in the center is smaller than the other spheres and of course it's smaller than the cube itself; after all, it's surrounded by the other spheres that are by construction inside the cube.Above 10 dimensions the size of the inner hypersphere is actually bigger than the size of the hypercube despite being surrounded by hyperspheres that are contained inside the hyper-cube!The math behind it is straightforward but the implication is as counterintuitive as it gets\n \nreply",
      "Or how the volume of an n-ball goes to 0[0,1]Or how gaussian balls are like soap bubbles[2]The latter of which being highly relevant to vector embeddings. Because if you aren't a uniform distribution, the density of your mass isn't uniform. MEANING if you linearly interpolate between two points in the space you are likely to get things that are not representative of your distribution. It happens because it is easy to confuse a linear line with a geodesic[3]. Like trying to draw a straight line between Los Angeles and Paris. You're going to be going through the dirt most of the time. Looks nothing like cities or even habitable land.I think the basic math is straight forward but there's a lot of depth that is straight up ignored in most of our discussions about this stuff. There's a lot of deep math here and we really need to talk a lot about the algebraic structures, topologies, get deep into metric theory and set theory to push forward in answering these questions. I think this belief that \"the math is easy\" is holding us back. I like to say \"you don't need to know math to train good models, but you do need math to know why your models are wrong.\" (Obvious reference to \"all models are wrong, but some are useful\") Especially in CS we have this tendency to oversimplify things and it really is just arrogance that doesn't help us.[0] https://davidegerosa.com/nsphere/[1] https://en.wikipedia.org/wiki/Volume_of_an_n-ball[2] https://www.inference.vc/high-dimensional-gaussian-distribut...[3] https://en.wikipedia.org/wiki/Geodesic\n \nreply",
      "I\u2019m having trouble understanding you. I would be curious to see a representation of this in 2d or 3d. Do you know of any good resources?\n \nreply",
      "> I EXPECT them to be. After all, they are the reverse of one another.That isn't how tokenized inputs work. It's partially the same reason why \"how many r's are in strawberry\" is a hard problem for LLMs.All these models are trained for semantic similarity by how they are actually used in relation to other words, so a data point where that doesn't follow intuitively is indeed weird.\n \nreply"
    ],
    "link": "http://blog.pamelafox.org/2025/05/a-visual-exploration-of-vector.html",
    "first_paragraph": "\nFor Pycon 2025, I created a poster exploring vector embedding models, which you can download at full-size. \n  In this post, I'll translate that poster into words.A vector embedding is a mapping from an input (like a word, list of words, or image) into a list of floating point numbers.\nThat list of numbers represents that input in the multidimensional embedding space of the model. We refer to the length of the list as its dimensions, so a list with 1024 numbers would have 1024 dimensions.\n\n\nEach embedding model has its own dimension length, allowed input types, similarity space, and other characteristics.For a long time, word2vec was the most well-known embedding model. It could only accept single words, but it was easily trainable on any machine, it is very good at representing the semantic meaning of words. A typical word2vec model outputs vectors of 300 dimensions, though you can customize that during training. This chart shows the 300 dimensions for the word \"queen\" from a word2vec"
  },
  {
    "title": "Human coders are still better than LLMs (antirez.com)",
    "points": 357,
    "submitter": "longwave",
    "submit_time": "2025-05-29T16:41:04 1748536864",
    "num_comments": 420,
    "comments_url": "https://news.ycombinator.com/item?id=44127739",
    "comments": [
      "Of course they are. The interesting thing isn't how good LLMs are today, it's their astonishing rate of improvement. LLMs are a lot better than they were a year ago, and light years ahead of where they were two years ago. Where will they be in five years?\n \nreply",
      "This matches my experience. I actually think a fair amount of value from LLM assistants to me is having a reasonably intelligent rubber duck to talk to. Now the duck can occasionally disagree and sometimes even refine.https://en.m.wikipedia.org/wiki/Rubber_duck_debuggingI think the big question everyone wants to skip right to and past this conversation is, will this continue to be true 2 years from now? I don\u2019t know how to answer that question.\n \nreply",
      "LLMs aren't my rubber duck, they're my wrong answer.You know that saying that the best way to get an answer online is to post a wrong answer? That's what LLMs do for me.I ask the LLM to do something simple but tedious, and then it does it spectacularly wrong, then I get pissed off enough that I have the rage-induced energy to do it myself.\n \nreply",
      "I'm probably suffering undiagnosed ADHD, and will get stuck and spend minutes picking a function name and then writing a docstring. LLMs do help with this even if they get the code wrong, because I usually won't bother to fix their variables names or docstring unless needed. LLMs can reliably solve the problem of a blank-page.\n \nreply",
      "This. I have ADHD and starting is the hardest part for me. With an LLM it gets me from 0 to 20% (or more) and I can nail it for the rest. It\u2019s way less stressful for me to start now.\n \nreply",
      "Yep.I like maths, I hate graphing. Tedious work even with state of the art libraries and wrappers.LLMs do it for me. Praise be.\n \nreply",
      "LLMs follow instructions. Garbage in = garbage out generally. When attention is managed and a problem is well defined and necessary materials are available to it, they can perform rather well. On the other hand, I find a lot of the loosely-goosey vibe coding approach to be useless and gives a lot of false impressions about how useful LLMs can be, both too positive and too negative.\n \nreply",
      "So what you\u2019re saying is you need to be very specific and detailed when writing your specifications for the LLM to spit out the code you want. Sounds like I can just skip the middle man and code it myself.\n \nreply",
      "This has been my experience as well. The biggest problem is that the answers look plausible, and only after implementation and experimentation do you find them to be wrong. If this happened every once in a while then it wouldn't be a big deal, but I'd guess that more than half of the answers and tutorials I've received through ChatGPT have ended up being plain wrong.God help us if companies start relying on LLMs for life-or-death stuff like insurance claim decisions.\n \nreply",
      "I'm not sure if you're being sarcastic, but in case you're not...  From https://arstechnica.com/health/2023/11/ai-with-90-error-rate...\"UnitedHealth uses AI model with 90% error rate to deny care, lawsuit alleges\"  Also \"The use of faulty AI is not new for the health care industry.\"\n \nreply"
    ],
    "link": "https://antirez.com/news/153",
    "first_paragraph": ""
  },
  {
    "title": "Open-sourcing circuit tracing tools (anthropic.com)",
    "points": 93,
    "submitter": "jlaneve",
    "submit_time": "2025-05-29T17:16:54 1748539014",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=44128101",
    "comments": [
      "Curious if we say \"thank you\", the model will be more activated and result in better answer. ^^\n \nreply",
      "Anthropic employees Sholto Douglas & Trenton Bricken did an interview recently with Dwarkesh Patel, pieces here and there was about the circuit tracing insights.https://www.dwarkesh.com/p/sholto-trenton-2 -- search the transcript for \"circuit\" for the quick bits.Eg, \"If you look at the circuit, you can see that it's not actually doing any of the math, it's paying attention to that you think the answer's four and then it's reasoning backwards about how it can manipulate the intermediate computation to give you an answer of four.\"https://transformer-circuits.pub/\n \nreply",
      "Imported the graph json into Neo4jHave funhttps://gist.github.com/jexp/8d991d1e543c5a576a3f1ee70132ce7...\n \nreply",
      "The conversation about this on Dwarkesh was interesting and I'm glad we're getting access to the tool.https://open.spotify.com/episode/3H46XEWBlUeTY1c1mHolqh?si=L...\n \nreply",
      "Is this Garcon [1], or a new tool?[1]: https://transformer-circuits.pub/2021/garcon/index.html\n \nreply",
      "Hi, paper author here.This is a new tool which relies on existing introspection libraries like TransformerLens (which is similar in spirit to Garcon) to build an attribution graph. This graph displays intermediate computational steps the model took to sample a token.For more details on the method, see this paper: https://transformer-circuits.pub/2025/attribution-graphs/met....For examples of using it to study Gemma 2, check out the linked notebooks: https://github.com/safety-research/circuit-tracer/blob/main/...)We also document some findings on Claude 3.5 Haiku here: https://transformer-circuits.pub/2025/attribution-graphs/bio...)\n \nreply",
      "This type of stuff is really important in my opinion. Getting this type of stuff open sourced allows academics and other researchers to try and do this type of interpretability research on a more level playing field.I think the more people looking at this the better. I have a feeling there will be some breakthroughs in identifying important circuits and being able to make more efficient model architectures that are bootstrapped from some identified primitives.\n \nreply",
      "thought this was about PCB tracing and was disappointed.\n \nreply",
      "If you only want to trace veroboards (stripboards) and not full blown PCBs I made a browser tool for that: https://github.com/dvhx/stripboard2schematic\n \nreply",
      "By total coincidence I have a project at the prototype stage that I will be building (hopefully starting tonight) on a strip board. Thanks!\n \nreply"
    ],
    "link": "https://www.anthropic.com/research/open-source-circuit-tracing",
    "first_paragraph": ""
  },
  {
    "title": "Gurus of 90s Web Design: Zeldman, Siegel, Nielsen (cybercultural.com)",
    "points": 357,
    "submitter": "panic",
    "submit_time": "2025-05-29T07:33:02 1748503982",
    "num_comments": 164,
    "comments_url": "https://news.ycombinator.com/item?id=44123852",
    "comments": [
      "This article put Nielsen in the corner of \"technically correct\", but the influence he had on me at least was a strong focus on \"empirically correct\". i.e. doing actual tests (with humans) on what kind of things work to convey information. He did this to the detriment of \"looking good\", which is why his stuff ended up looking \"hopelessly outdated\", but I think he was on the right side of the fight.\n \nreply",
      "I always had more respect for Nielsen\u2019s lineage of human-computer interaction than I did for Nielsen himself.  At the time I remember thinking how neither designers nor classic HCI people (or programmers) really got the web.  Nielsen was at least focused on the web, but the problem is that he was fixated on user expectations for a brand new medium without recognizing that it was early days and would inevitably evolve.  He would say stuff like \u201chyperlinks should always be blue and underlined\u201d because that\u2019s what users expect, without realizing that at that point in time we were still so early in the adoption of the web that it made no sense to apply such rigid rules.\n \nreply",
      "> He would say stuff like \u201chyperlinks should always be blue and underlined\u201d because that\u2019s what users expect, without realizing that at that point in time we were still so early in the adoption of the web that it made no sense to apply such rigid rules.I always remember recommendations from Nielsen as (a) backed by some testing with real users, (b) temporal, i.e. \u201cat this time users expect\u2026\u201d and ( c) only focused on usability, that is, in practice there are other things to consider like design, performance, etc.I will say that most of this nuance gets rounded to a Boolean like most advice.\n \nreply",
      "In creating documents with hyperlinks for training students, I have found blue underlined still catches the most fish, for example some do not realize that accordion-style content can be clicked to reveal more content if it is not blue underlined. Have tested icons, highlighting, different colors of underlining.I think part of the issue is that early users of the internet were more tech-savvy, and now internet users are simply \"anyone with a phone\"\u2014in a sense we're going backwards because a higher percentage of users are not learning/adapting to attempts at new approaches/standards.\n \nreply",
      "Honestly, I believe that the Web would have been better had we stuck to those expectations more diligently and evolved more slowly and thoughtfully.  That one can does not imply that one should.Blue links and purple visited links were fine.  And now on most sites there is no differentiation, and it\u2019s sometimes difficult to tell what is a link, and a lot of sites don\u2019t even bother linking.  This is not an improvement!\n \nreply",
      "Blue and purple links wouldn't be visible on any website that chose to use those as background colors (or any range of background colors where the contrast would have been too low to be visible).The web at the time was an \"anything goes\" multimedia format, not a dry digital paperback or textbook where all the content had to fit within the publisher's specifications to limit printing, weight and distribution costs.Nowadays, most browsers have a \"reading mode\" that can flatten the content into something that satisfies those Nielsen conditions though.\n \nreply",
      "I don't disagree with the opinion, but what individual experts think does not factor in much when you have a groundswell of adoption like the web did.  At that point people are going to hack whatever they can on top of it, and there are too many varied interests to have any central control, and so things just evolve well beyond the intent or control of any individual mind or architect.For me, usability mattered a lot and I saw how a lot of the web design experimentation was falling short, but Nielsen was just too backwards looking.  We needed forward thinking UX rooted specifically in web culture, and that's what we got through the Zeldmans, Veens, and 37signals of the era.\n \nreply",
      "Why didn't he say the same thing about links:> he was saying that each browser should define how headers would be displayed to their users.And let the user define the color and underline style?\n \nreply",
      "Ben Shneiderman's the \"hyperlinks should always be blue\" guy. ;)https://blog.mozilla.org/en/internet-culture/why-are-hyperli...https://news.ycombinator.com/item?id=29897811Seriously, while he was the first to use blue for links in HyperTIES, there was a historical context (like the IBM PC's color palette), and he never meant it in a \"640k ought to be enough for anybody\" way. His reasons for recommending blue are based on empirical studies, measuring visibility, comprehension, retention, etc.Blue is good not just because users recognize it (they didn't in 1983), but for how it stands out, because of how the human visual system works. He was originally a fan of cyan aka \"light blue\".Ben Shneiderman wrote:>\"Red highlighting made the links more visible, but reduced the user\u2019s capacity to read and retain the content of the text\u2026 blue was visible, on both white and black backgrounds and didn\u2019t interfere with retention,\">\"We conducted approximately 20 empirical studies of many design variables which were reported at the Hypertext 1987 conference and in array of journals and books. Issues such as the use of light blue highlighting as the default color for links, the inclusion of a history stack, easy access to a BACK button, article length, and global string search were all studied empirically.\u201d>\"My students conducted more than a dozen experiments (unpublished) on different ways of highlighting and selection using current screens, e.g. green screens only permitted, bold, underscore, blinking, and I think italic(???). When we had a color screen we tried different color highlighted links. While red made the links easier to spot, user comprehension and recollection of the content declined. We chose the light blue, which Tim adopted.\"HyperTIES Discussions from Hacker News:https://donhopkins.medium.com/hyperties-discussions-from-hac...\n \nreply",
      "\u201chyperlinks should always be blue and underlined\u201dthis honestly make life so much easier...\n \nreply"
    ],
    "link": "https://cybercultural.com/p/web-design-1997/",
    "first_paragraph": ""
  },
  {
    "title": "Editing repeats in Huntington's:fewer somatic repeat expansions in patient cells (nature.com)",
    "points": 17,
    "submitter": "bookofjoe",
    "submit_time": "2025-05-27T15:21:08 1748359268",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.nature.com/articles/s41588-025-02172-8",
    "first_paragraph": "Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\n            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\n            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\n            and JavaScript.Advertisement\nNature Genetics\n\n                         (2025)Cite this article\n4440 Accesses1 Citations49 AltmetricMetrics detailsTrinucleotide repeat (TNR) diseases are neurological disorders caused by expanded genomic TNRs that become unstable in a length-dependent manner. The CAG\u2022CTG sequence is found in approximately one-third of pathogenic TNR loci, including the HTT gene that causes Huntington\u2019s disease. Friedreich\u2019s ataxia, the most prevalent hereditary ataxia, results from GAA repeat expansion at the FXN gene. Here we used cytosine and adenine base editing to reduce the repetitiveness of TNRs in patient cells and i"
  },
  {
    "title": "Putting Rigid Bodies to Rest (twitter.com/keenanisalive)",
    "points": 102,
    "submitter": "pvg",
    "submit_time": "2025-05-29T15:43:33 1748533413",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=44127173",
    "comments": [
      "I don't have enough time to read the paper in full right now. But I'm curious if using this they could possibly find the solution to the 3 sided coin problem. I haven't heard anything about it since I watched the matt parker video about it.https://youtu.be/-qqPKKOU-yYOr I guess if anyone else knows the answer, that would also satisfy my curiosity.\n \nreply",
      "They should be able to simulate it! Here's another answer: https://news.ycombinator.com/item?id=33776796\n \nreply",
      "Looks like that post author forgot to loop back to the original question once they found a model that fit their own simulations.Just visually going off the chart, the answer is a \"coin\" has a 1/3 chance of landing on its edge when its height is 1.7x its radius, or 0.85x its diameter. (the blog author used half-height and the paper he found uses full height)\n \nreply",
      "Does this mean 2d physics simulators are about to get N times faster? Because that'd be cool if N is big enough.\n \nreply",
      "> our key observation is that we can identify dynamically stable configurations of a rigid body, and calculate their associated probabilities> this model is purely geometric, and does not directly account for momentumanswer: no\n \nreply",
      "I love this !!Only kinda related but I love having the opportunity to share this website, cataloging every possible fair die: http://www.aleakybos.ch/Shapes.htm(ie: not the sort of die in the post, they must have identical faces. this thread gave me a new appreciation for the non equal faced dice tho)\n \nreply",
      "I don\u2019t think it gets much better than this. How exceedingly clever.\n \nreply",
      "Paper https://hbaktash.github.io/files/rolling_dragons_paper.pdf and more related stuff on the page of one of the authors https://hbaktash.github.io/\n \nreply",
      "[flagged]",
      "There's still way more signal there then really any other social network (even youtube is getting slop-y)\n \nreply"
    ],
    "link": "https://twitter.com/keenanisalive/status/1925225500659658999",
    "first_paragraph": ""
  },
  {
    "title": "Flash Back: An \"oral\" history of Flash (goodinternetmagazine.com)",
    "points": 15,
    "submitter": "surprisetalk",
    "submit_time": "2025-05-28T12:21:44 1748434904",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=44115244",
    "comments": [
      "Before it was Flash, it was a vector drawing tool called FutureSplash.. in the early 90s i worked at fractal design corp (creators of what's now corel painter) and the Futuresplash folks were interested in selling it to us.. that was before it had an animation timeline or scripting features. I was kind of an in-house artist and the founders really wanted my opinion on whether or not it would be appealing to our customers, like how i as an artist felt about it.My feedback was that was kind of cumbersome compared to our other tools, and didn't see the potential appeal to our user base and recommended against it, and we declined the offer.Ultimately I'm glad Macromedia did acquire it because adding scripting and animation appealed to the already diminishing Director/Shockwave platform.I do wonder how things might have been different had we decided otherwise and acquired Futuresplash-- there'd have been no Flash as we know it!It's amazing how little decisions we make in the past can project out over time and have larger repercussions.\n \nreply",
      "https://flashpointproject.github.io/flashpoint-database/^Search 177,508 games and 32,156 animations, all playable online without a plugin\n \nreply",
      "I loved tinkering with Flash as a kid in the early 2000s. I taught myself programming through ActionScript and wrote a little physics engine that modeled 2D physics using loops, conditionals, basic trig, and increments.It's definitely outdated at this point by HTML5 and WebGL, but I will always fondly remember all those little flash games and experimenting with ActionScript, learning programming fundamentals.I would highly recommend tinkering with the HTML5 Canvas element and WebGL if you were a fan of Flash. The web browser has evolved into an OS of sorts as personal computers have evolved along with the introduction of mobile devices.Web browsers now handle email clients, word processors, photo editors, even video and code editors. Check out this neat fluid simulator experiment in WebGL, you can build even more advanced applications of this nature with this technology.https://paveldogreat.github.io/WebGL-Fluid-Simulation/\n \nreply",
      "I enjoyed this article. But Godot, SDL, and TIC-80 still have a much higher learning curve vs. tweening animations and throwing in some Actionscript to make a stick figure fighter, which gives you a self-contained artifact .swf you can pass around.It's amazing how much creativity that used to go to Newgrounds and Flash games is now funneled into TikTok shorts.Kids these days! But Super Mario Bros 63 is still fun.\n \nreply",
      "It\u2019s a shame nothing has really come along with the same functionality that the Flash editor had. The vector based graphics, animation and scripting all in one. And easily exportable to the web. Yeah you have Godot / unity etc etc but nothing as easy as flash was to pick up\n \nreply",
      "Adobe Edge was supposed to be the successor to Flash, to target the web and other platforms, but ultimately never caught on and was discontinued in a few years.https://en.wikipedia.org/wiki/Adobe_Edge_Animate\n \nreply",
      "It's not impossible that what's now called HTML5 would have been Flash, had Adobe not acquired Macromedia. Steve Jobs had long-running personal bad blood with Adobe.\n \nreply",
      "Just because it's less popular it doesn't mean it's dead. You can still create flash animations, you can still share them, and there are still flash game websites that exist.\n \nreply",
      "Yep, I sometimes go on 4chan's /f/ board to see what people are creating. They even have a little on-site emulator so you don't need to have Flash installed on your system.\n \nreply"
    ],
    "link": "https://goodinternetmagazine.com/oral-history-of-flash/",
    "first_paragraph": "I don\u2019t know what the first video game I ever played was. I would have been too young to remember. It very likely could have been a Flash game running in a web browser, patiently downloaded over a dialup connection.I got interested in computers at an early age, but it would take me a lot longer to really appreciate what a miracle and nightmare Flash was. Flash was a huge force that enabled many great applications but also loaded huge liabilities on millions of personal computers. Flash set our expectations of what the web could do, defined online culture, and enabled creative expressions online that HTML (et al.) couldn\u2019t deliver. Yet it also had constant problems with security, performance, and accessibility, on top of installing a single corporate gatekeeper to the \u201cfull\u201d web. It took hold on the web in only a few years after its creation, but would take about a decade to completely replace.I was a toddler when Flash was born. Now I\u2019m a grownup and Flash has been dead for a few years"
  },
  {
    "title": "Car Physics for Games (2003) (asawicki.info)",
    "points": 7,
    "submitter": "ibobev",
    "submit_time": "2025-05-27T16:44:36 1748364276",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.asawicki.info/Mirror/Car%20Physics%20for%20Games/Car%20Physics%20for%20Games.html",
    "first_paragraph": "Level: Advanced Abstract: An introduction to car physics modelling for\r\ngames.  version 1.9 \r\nNovember, 2003\u00a0 \r\n\u00a0\u00a0 \r\n\u00a0 This tutorial is about simulating cars in games, in other words\r\nvehicle physics.\u00a0 \u00a0One of the key points in simplifying vehicle physics is to handle\r\nthe longtitudinal and lateral forces separately.\u00a0 Longtitudinal\r\nforces operate in the direction of the car body (or in the exact\r\nopposite direction).\u00a0 These are wheel force, braking force, rolling\r\nresistance and drag (= airresistance).\u00a0 Together these forces\r\ncontrol the acceleration or deceleration of the car and therefore the\r\nspeed of the car.\u00a0 Lateral forces allow the car to turn.\u00a0\r\nThese forces are caused by sideways friction on the wheels.\u00a0 We'll\r\nalso have a look at the angular moment of the car and the torque caused\r\nby lateral forces. Throughout this tutorial I'll be assuming that the rear wheels\r\nprovide all the drive (for four wheel drives apply the neccesary\r\nadaptations). I'll be mainly using S.I. units ("
  },
  {
    "title": "The flip phone web: browsing with the original Opera Mini (spacebar.news)",
    "points": 92,
    "submitter": "protonbob",
    "submit_time": "2025-05-29T15:30:17 1748532617",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=44127027",
    "comments": [
      "A similar but \"data leaky\". one was UC browser. These two were constantly compared back in the day, their entire selling point was faster downloads over opera mini. I remember doing speed tests with both on my old nokia and Samsung feature phones on 2g networks. I'm extremely grateful these browsers exist otherwise I wouldn't have been browsing zdnet, old techradar, w3schools, phpeasystep, betanews etc. while I was 12. They got me into the world of tech and now I am working in it.Opera mini did support video playback on some phones especially on YouTube. It opens the phone's inbuilt video player with an RTSP link to the 3gp version of the video. I guess it stopped working when YouTube removed RTSP? Oh that brings a lot of memories about real player which was also available on j2me as well as pc.\n \nreply",
      "Opera Mini appeared at a strange time. This is back around when Apple had done Webkit just for the Mac (derived from KHTML) and Nokia thought it would be cool on S60, and it was.WAP/WML was a near total flop, although oddly had a second life on interactive TV in the UK where Sky funded creation of a WML browser in OpenTV to enable faster turnaround of new apps. (OpenTV was neither open nor particularly consistent between devices, and the normal mode of delivering content via satellite was difficult logistically).As someone else mentioned imode was way more popular, at least in Japan. (I happen to have been slightly involved in the relative failure of a rollout in France). NTT went to massive lengths to get Japanese companies up and running on imode, such that by about 2002 huge amounts of ecommerce were being done on it, including B2B. I know of major electronics components vendors doing >70% of their sales (by value) via mobile Internet in Japan by 2005. imode didn't really do well in the 3G transition though, and the increased complexity of the phones led to a lot of bad software on them, which was the opportunity Apple ultimately exploited.The result of all that is in 2005 J2ME was surprisingly capable, but virtually no one was using it, and so Opera Mini was kind of by itself for a while. The company that does a lot of transport ticketing in the UK had at one point a phenomenal mobile banking prototype (that worked) but was told by the banks they simply weren't ready for it, and the banks didn't think the customers were either, which was actually the truth, and so why they pivoted.\n \nreply",
      "> WAP/WML was a near total flopNot quite, WAP evolved into WAP 2.0 and WML was replaced with xHTML.  For a long period of time WAP 2.0 was the only standard while mobile development evolved.  Even as HTML support grew in the mobile space, there was a very solid duration of time where WAP 2.0 was the most supported across the majority of devices and browsers (in the mobile space).  And FWIW, I wouldn\u2019t consider WML a flop, because that was the only mobile standard supported on most phones before the iPhone.\n \nreply",
      "Another example of a really well-done J2ME app was Google Maps. It had GPS and street view support at the same time as iPhone only did tower location and Google wouldn't license them Street View.With a high-end Sony Ericsson feature phone, I could multitask(!) between Opera Mini, a J2ME twitter client, IRC client, train timetable app, Google Maps, etc better than a Symbian S60 smartphone could (and iPhone of course couldn't multitask at all) https://kalleboo.com/linked/se_j2me_multitask.jpg\n \nreply",
      "> As someone else mentioned imode was way more popular, at least in Japan. (I happen to have been slightly involved in the relative failure of a rollout in France).Do you have any info on what the iMode technical stack was like? The information available on the internet (at least in English) is scarce.\n \nreply",
      "Someone somewhere will have a French copy of the tech specs, because Bouygues paid to have the whole lot translated before any work could commence.One oddity of the Japanese setup was NTT essentially ran a cloud, of a huge pile of HP-UX servers with Oracle/JVM etc. somewhere in Yokohama which was running all the backend. When I said NTT went to big efforts to get companies on board this is what I mean: they were incredibly proactive about getting integrations into this happening, to the point of entirely hosting it on site if necessary and essentially doing the work themselves for anyone they thought valuable enough. I don't believe any version outside Japan ever replicated this, and it was the magic ingredient along with persistent data connections.The closest thing the west saw to that in practice was the Sidekick, but that never got the level of third party support.It's fairly widely understood that i-mode used CHTML but I'm unfamiliar with the lower levels of the networking stack. App wise there was Java but not normal J2ME. All I can remember that was unique was their scratchpad memory area (which was just way too big), and billing APIs!https://en.wikipedia.org/wiki/DoJa\n \nreply",
      "> I happen to have been slightly involved in the relative failure of a rollout in FranceWas it with Bouygues? IIRC they were basically the only provider to advertise i-mode.Anyway, i-mode might have been a flop, but for a brief moment in time I enjoyed being on the bleeding edge with things like Opera Mini on my Sony Ericsson. Those were the days.\n \nreply",
      "> Was it with Bouygues?It was. For reasons that now escape me I was doing billing integration work for it.\n \nreply",
      "> The result of all that is in 2005 J2ME was surprisingly capable, but virtually no one was using it, and so Opera Mini was kind of by itself for a while.Compatibility with J2ME was really fragmented and this was the time when there were literally thousands of phone models in the market at the same time, so it took a sizeable effort to ship something that worked well.With Opera Mini we had about 5-6 people (developers+QA) working on J2ME device compatibility continuously. And a really impressive physical device library. I think it had about 2.5k devices in the end.Once you got really popular though, most device manufacturers started including compatibility with your app in their shipping criterias, so then it suddenly got really easy.\n \nreply",
      "By 2005 J2ME fragmentation was nothing like as bad as what Android would become, and remains greatly exaggerated. By that point the 3650 and T610 were getting to be history, and Siemens were on the edge of dropping out.It was one of those things that was very surmountable if the market pull would have been there to overcome it, but there wasn't demand because the phone form factors of the time sucked for apps and games.> With Opera Mini we had about 5-6 people (developers+QA) working on J2ME device compatibility continuously.You realize this is tiny? To cover things like Blackberry and Brew took significantly more effort than that, and this is pre GPU or camera variation being a thing.\n \nreply"
    ],
    "link": "https://www.spacebar.news/the-flip-phone-web-browsing-with-the-original-opera-mini/",
    "first_paragraph": "Opera Mini was first released in 2005 as a web browser for mobile phones, with the ability to load full websites by sending most of the work to an external server. It was a massive hit, but it started to fade out of relevance once smartphones entered mainstream use.Opera Mini still exists today as a web browser for iPhone and Android\u2014it's now just a tweaked version of the regular Opera mobile browser, and you shouldn't use Opera browsers. However, the original Java ME-based version is still functional, and you can even use it on modern computers.In the 1990s and 2000s, most mobile phones didn't have the processing power to load full desktop websites. Instead, an alternative Wireless Application Protocol (WAP, but not that WAP) was created for simple websites that could work over a 1G/2G connection on a typical mobile phone. For example, the BBC had a regular website for users on desktop computers, and a simpler WAP site for users on mobile phones and PDAs. The WAP versions of websites "
  },
  {
    "title": "Nova: A JavaScript and WebAssembly engine written in Rust (trynova.dev)",
    "points": 140,
    "submitter": "AbuAssar",
    "submit_time": "2025-05-29T14:05:27 1748527527",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=44126264",
    "comments": [
      "How does Nova compare to Boa? Regardless it\u2019s great to see new js engines popping up\n \nreply",
      "Hi, main developer of Nova here if you want to ask any questions! I'm at a choir event for the rest of the week though, so my answers may tarry a bit.\n \nreply",
      "Have you been following Meta's work on Static Hermes? It's one of two efforts I'm aware of* to define a subset of JavaScript with different runtime semantics, by putting limitations on dynamic behavior. Their primary goal is performance, but correctness benefits also seem likely, and they share your idea that being intended for embedding in a particular application lets you get away with breaking compatibility in ways that you couldn't in a browser. And if their thing ships, and you want to reduce fragmentation, then maybe you want your \"sane subset\" to match theirs.* The other being Google's Closure Compiler, which probably isn't relevant to you as it assumes that its output has to run on existing engines in browsers.\n \nreply",
      "Given the fact that you were so precise in your time estimate on interleaved garbage collection, how long do you think it would take to get to 99% of the tests?\n \nreply",
      "Haha, I think that was a one time fluke! :DI'm aiming for something like 75-85% this year; basically get iterators properly done (they're in the engine but not very complete yet), implement ECMAScript modules, and then mostly focus on correctness, builtins, and performance improvements after that. 99% would perhaps be possible by the end of next year, barring unforeseeable surprises.\n \nreply",
      "have you considered using js polyfills to help you get closer to 100% coverage and then replacing with native implementations prioritized by performance impact?\n \nreply",
      "Not really, no. Its an interesting proposition, but for the most part I believe I'll be sticking it out the \"hard way\". The ECMAScript spec is fairly easy to read as well, after all. (Nevermind that I spent the single free hour I had today cursing at my incapability of understanding what is going wrong with my iterator code and what it even should do vis-\u00e0-vis the spec :D )\n \nreply",
      "Gz on your grant!\nI must have missed the announcement, but working on OSS for a living (even for just a bit) would be super awesome.\n \nreply",
      "Thank you! I've been a bit bad with announcing it; blog post was a month late and all that. But indeed, it's really cool to be able to do this for half a year!\n \nreply",
      "This may be too early to ask, but are you targeting a near-v8 level of performance? Or more like quickjs or duktape?\n \nreply"
    ],
    "link": "https://trynova.dev/",
    "first_paragraph": "Nova is a JavaScript (ECMAScript) and WebAssembly engine written in Rust and following data-oriented design principles. It is currently nothing more than a fun experiment to learn and to prove the viability of such an engine, but may very well become something much more in the future.The engine is still very limited in it's capabilities only passing about 70% of the test262 test suite. However development is ongoing and we are quickly improving the engine. If you are interested in the project, please check out the GitHub repository and or join our Discord server where the core team resides and where we discuss development."
  },
  {
    "title": "Notes on Tunisia (mattlakeman.org)",
    "points": 11,
    "submitter": "returningfory2",
    "submit_time": "2025-05-29T20:53:39 1748552019",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://mattlakeman.org/2025/05/29/notes-on-tunisia/",
    "first_paragraph": "Matt LakemanI spent almost three weeks in Tunisia, visiting the cities of Tunis, Bizerte, El Kef, Tozeur, Tataouine, El Jem, Sfax, Sousse, Monastir, Kairouan, and a few smaller towns in between.At first, I thought this post would be heavy on travel and light on history, but I got carried away on the background reading and surprisingly interesting politics of the country. Modern Tunisia is another one of those one-time democracies that recently fell into an authoritarian quasi-dictatorship, like Turkey\u2019s Erdogan, El Salvador\u2019s Bukele, Hungary\u2019s Orban, etc. But compared to those examples, Tunisia\u2019s President Kais Saied feels like more of an accident, a product of a flailing, random political trajectory that no one could have predicted. The fact that a guy like this ended up running a country like this in this way is more baffling than scary or exciting.Before travelling, I read \u201cTunisia, a Country Study\u201d in the Handbook series commissioned by the U.S. Army and published in 1987 (I\u2019ll ref"
  },
  {
    "title": "Grid-Free Approach to Partial Differential Equations on Volumetric Domains [pdf] (rohansawhney.io)",
    "points": 22,
    "submitter": "luu",
    "submit_time": "2025-05-27T07:16:46 1748330206",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44104653",
    "comments": [
      "What I'm interested in seeing if this could be used for multidisciplinary design optimization. Currently it is possible to do coupled aero structural optimization of wings and the like but of course you have to deal with all the mesh stuff and make it deformable as wing changes shape. And then get the gradients out.Very curious to see if it might be possible to greatly simplify the problem with this method\n \nreply",
      "I just saw this and I'm surprised that it hasn't gotten more feedback- this is sensational. I'm just working my way through the paper as a Humanities major and I find it advanced enough that you're clearly treading new water but it is accessible enough that I don't feel like a pleb. Great work! Enjoy your PhD doctor!\n \nreply"
    ],
    "link": "http://rohansawhney.io/RohanSawhneyPhDThesis.pdf",
    "first_paragraph": ""
  },
  {
    "title": "Why Is Everybody Knitting Chickens? (ironicsans.ghost.io)",
    "points": 92,
    "submitter": "mooreds",
    "submit_time": "2025-05-27T15:52:49 1748361169",
    "num_comments": 72,
    "comments_url": "https://news.ycombinator.com/item?id=44108139",
    "comments": [
      "Feels the the homogenization of culture driven by social media and online communities. Somebody makes a chicken, and it gets a good reaction, so everybody starts making chickens. At first it's organic but it turns into clout chasing. Pretty soon the chickens will start to disappear and something else will take their place.\n \nreply",
      "Leave it to hacker news to figure out why knitting chickens is actually a sign of cultural collapse.\n \nreply",
      "> the homogenization of culture driven by social media and online communitiesIt's absolutely crazy to me how quickly people have forgotten the monoculture that, by my estimate, ended only 10 years ago.\n \nreply",
      "This predates online communities.My grandmother bought a bunch of knitted chickens in the 60\u2019s-80\u2019s, as did a family dinner I used to go to etc. It\u2019s a relatively simple shape to get right, there\u2019s many options, and they end up looking fairly cute.\n \nreply",
      "Trends have been happening since long before social media. I don't see the problem with \"everybody\" getting involved in knitting and making chickens anyway, what's the harm here?\n \nreply",
      "I think the \"homogenization\" is the keyword here. It's not that trends are bad, it's just that, in the 'old days' a trend might start as a community-wide phenomena that over time might spread into neighboring communities, finally becoming part of the local / regional zeitgeist.These trends would spread slowly enough that other trends in other communities would have time and room to grow and develop. The result is you get a bunch of localized cultures, all unique in some way.The best analogy I can think of is a plant mono-crop. Instead of different species of plant gradually finding their niche, we plant 50,000 acres with corn or soy.I have to say, even over the last 20+ years or so, it really does feel like you can go anywhere in the world and get a very similar experience. You can go to the local 7-11, buy a coca-cola, hit up your local costco, listen to people arguing about American politics. It just feels like different countries have gradually been losing their unique culture, and we just have this global homogenized version with slight regional differences.\n \nreply",
      "People have been saying the exact opposite- that we used to all have the same 20 TV shows but now with internet microgenres we don't have enough shared culture anymore.If you think of the ravelry community as valid as an in person community this will be nicer I think.\n \nreply",
      "Hmm, interesting counterpoint.I think both things can be simultaneously true. There are a million sub-cultures that can now exist, that are no longer tied to a geographic location. \nThis is both good and bad. Good, insofar as if you're in the middle of Ohio in a 2000 person town, and really-really into model trains or whatever, you can find an online community that shares this. But I also think it's bad insofar as we've lost some sense of culture or commonality with our (geographic) neighbors.But to the homogenization point; I still think within a specific sub-culture (sewing circles), you can have global homogenization. The sewing circle might new be global, on facebook and tiktok, instead of 10,000 insular hamlets. Is this bad/good? I'm not sure. There's nothing from stopping you creating a local facebook group. And in theory, good ideas can spread rather than be confined to a specific geographic group. But I can't help feeling that some independent thought and ways of thinking are lost through this globalization.\n \nreply",
      "Independent thought still exists and is expressed but the network effects of influencers and copycats outranks independent thought on a platform like tiktok that group ideas and people together.  Independent thought only has a place under an existing topic or brand.\n \nreply",
      "The harm is homogenization of culture stymies concurrent evolution of new ideas. Whether that\u2019s more important than the sheer speed of good ideas traveling the world is an open question.But there\u2019s definitely less creative work produced without the direct or indirect influence of outside forces. As an artist you simply can\u2019t unsee things. So we may end up at some local maxima of creativity.\n \nreply"
    ],
    "link": "https://ironicsans.ghost.io/why-is-everybody-knitting-chickens/",
    "first_paragraph": "A couple years ago, my wife began knitting. And when she takes an interest in something, she goes all in. She\u2019s a perfectionist about learning a new craft, so in a short amount of time, she\u2019s gotten pretty damn good. I can\u2019t even pretend to know anything more than the most basic idea behind knitting, but somehow she makes incredibly complex garments with skill and precision.She\u2019s also become active in the online knitting community, which is interesting for me to watch because she\u2019s otherwise not really active online. But now she follows r/knitting and shares on Ravelry and watches knitting videos on YouTube.The other day she told me about something that\u2019s spread like wildfire through the knitting community: chickens. Knitted chickens. They\u2019re roughly the size of a throw pillow and are stuffed with material to be soft and huggable. Everybody\u2019s making them.Since the knitting pattern for this chicken was posted on Ravelry in 2023, nearly 11,000 people have posted photos of their own knitt"
  },
  {
    "title": "Airlines are charging solo passengers higher fares than groups (thriftytraveler.com)",
    "points": 222,
    "submitter": "_tqr3",
    "submit_time": "2025-05-29T18:39:22 1748543962",
    "num_comments": 352,
    "comments_url": "https://news.ycombinator.com/item?id=44128901",
    "comments": [
      "I feel like people are suspending their reasoning in order to maximally shit on airlines in this thread (because yes, they do have a history of predatory pricing practices).The problem with this isn't the difference in prices - charging less for buying in bulk is a normal thing that's probably been done by merchants since the invention of money.The problem with this is the lack of communication. There's no advertisement of a bulk/family discount at any point during the pricing process, you just see a different price. That's the problem here, not the price difference itself.\n \nreply",
      "I once tried to book a flight with the same-day return; the price for the return flight was mad expensive. The same return flight was a few hundred euros cheaper if you booked the initial flight a day earlier.My theory is that most same-day travel is for business, and businesses are far less price-sensitive than consumers and will just pay whatever.I suspect this is what's going on here. Most solo travellers are for business, not consumers for holidays. The price difference here is huge \u2013 almost half \u2013 which is far beyond a bulk discount, we're talking about 1 person vs 2 people.That's also why none of this is advertised: it's not a discount, but a \"we think you can pay more, so we'll charge you more\" type of thing.Is that a good/bad/ethical/predatory thing? I don't know. Leaves kind of a bad taste for me though.\n \nreply",
      "At times it has also been common for airlines to charge lower prices for round trip tickets that extend over a Saturday. The thinking being that business travelers usually return home on Thursday or Friday and they're less price sensitive so airlines could use that as a way to discriminate. Leisure travelers typically stayed through a weekend and received lower prices.\n \nreply",
      "Some companies more or less do the same thing with their \"enterprise\" software.\n \nreply",
      "As I understand it, prices are generated by algorithms on the spot, and may change any number of times during a day.You can't advertise prices that constantly change.\n \nreply",
      "It's intentional and working as designed.The same way they have been observed to offer higher prices to iPhone users at timesThey come up with schemes to rake in money based on market segmentation they run numbers on and have their booking systems setup in a way to make price comparison \"difficult\" for a normal user.\n \nreply",
      "All airline pricing is unadvertised and not communicated. They also sell through lots of independent agencies and channels.It would be weird to specifically advertise this. Unlike say, buying a second pair of shoes - not many people will buy an extra plane ticket to save 30% off both.\n \nreply",
      "The thing I find interesting about a lot of things like this is that they feel like a holdover of half the era where negotiating prices was normal: today, for most people in the US, most shopping is just a matter of going somewhere and paying a set price and you don\u2019t argue with the seller to get a better one. B2B transactions still usually involve a negotiation, I think, but it\u2019s basically gone for consumers.With something like airfares, the business is still doing its half of negotiations: collecting bits of data about the buyer to determine a price; but, crucially, there\u2019s no real way for the buyer to \u201ctalk back\u201d and so the process seems arbitrary.\n \nreply",
      "> not many people will buy an extra plane ticket to save 30% off both.That was kind of the premise of a movie I watched last night where a couples retreat offered a group discount for 4 couples or something.So I could see it being \"Bring your friends for 30% off!\" being a cool summer promotion to beach destinations or something.\n \nreply",
      "Are these discounts compared to prices before the change or did they raise the price for individual travelers?\n \nreply"
    ],
    "link": "https://thriftytraveler.com/news/airlines/airlines-charging-solo-travelers-higher-fares/",
    "first_paragraph": ""
  }
]