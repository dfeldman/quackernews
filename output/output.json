[
  {
    "title": "How to Code Claude Code in 200 Lines of Code (mihaileric.com)",
    "points": 313,
    "submitter": "nutellalover",
    "submit_time": "2026-01-08T19:54:26 1767902066",
    "num_comments": 143,
    "comments_url": "https://news.ycombinator.com/item?id=46545620",
    "comments": [
      "Something I would add is planning. A big \"aha\" for effective use of these tools is realizing they run on dynamic TODO lists. Ex: Plan mode is basically bootstrapping how that TODO list gets seeded and how todos ground themselves when they get reached, and user interactions are how you realign the todo lists. The todolist is subtle but was a big shift in coding tools, and many seem to be surprised when we discuss it -- most seem to focus on whether to use plan mode or not, but todo lists will still be active. I ran a fun experiment last month on how well claude code solves CTFs, and disabling the TodoList tool and planning is 1-2 grade jumps: https://media.ccc.de/v/39c3-breaking-bots-cheating-at-blue-t... .Fwiw, I found it funny how the article stuffs \"smarter context management\" into a breeze-y TODO bullet point at the end for going production-grade. I've been noticing a lot of NIH/DIY types believing they can do a good job of this and then, when forced to have results/evals that don't suck in production, losing the rest of the year on that step. (And even worse when they decide to fine-tune too.)reply",
      "I'm unsure of its accuracy/provenance/outdatedness, but this purportedly extracted system prompt for Claude Code provides a lot more detail about TODO iteration and how powerful it can be:https://gist.github.com/wong2/e0f34aac66caf890a332f7b6f9e2ba...https://gist.github.com/wong2/e0f34aac66caf890a332f7b6f9e2ba...I find it fascinating that while in theory one could just append these as reasoning tokens to the context, and trust the attention algorithm to find the most recent TODO list and attend actively to it... in practice, creating explicit tools that essentially do a single-key storage are far more effective and predictable. It makes me wonder how much other low-hanging fruit there is with tool creation for storing language that requires emphasis and structure.reply",
      "I find in coding + investigating there's a lot of mileage to being fancier on the todo list. Eg, we make sure timestamps, branches, outcomes, etc are represented.  It's impressive how far they get with so little!For coding, I actually fully take over the todo list in codex + claude: https://github.com/graphistry/pygraphistry/blob/master/ai/pr...In Louie.ai, for investigations, we're experimenting with enabling more control of it, so you can go with the grain, vs that kind of wholecloth replacementreply",
      "Ooh, am I reading correctly that you're using the filesystem as the storage for a \"living system prompt\" that also includes a living TODO list? That's pretty cool!And on a separate note - it looks like you're making a system for dealing with graph data at scale? Are you using LLMs primarily to generate code for new visualizations, or also to reason directly about each graph in question? To tie it all together, I've long been curious whether tools can adequately translate things from \"graph space\" to \"language space\" in the context of agentic loops. There seems to be tremendous opportunity in representing e.g. physical spaces as graphs, and if LLMs can \"imagine\" what would happen if they interacted with them in structured ways, that might go a long way towards autonomous systems that can handle truly novel environments.reply",
      "yep! So all repos get a (.gitignore'd) folder of `plans/<task>/plan.md` work histories . That ends up being quite helpful in practice: calculating billable hours of work, forking/auditing/retrying, easier replanning, etc. At the same time, I rather be with-the-grain of the agentic coder's native systems for plans + todos, eg, alignment with the models & prompts. We've been doing this way b/c we find the native to be weaker than what these achieve, and to hard to add these kind of things to them.RE:Other note, yes, we have 2 basic goals:1. Louie to make graphs / graphistry easier. Especially when connected to operational databases (splunk, kusto, elastic, big query, ...). V1 was generating graphistry viz & GFQL queries. We're now working on louie inside of graphistry, for more dynamic control of the visual analysis environment (\"filter to X and color Y as Z\"), and as you say, to go straight to the answer too (\"what's going on with account/topic X\"). We spent years trying to bring jupyter notebooks etc to operational teams as a way to get graph insights to their various data, and while good for a few \"data 1%'ers\", too hard for most, and Louie has been a chance to rethink that.2. Louie has been seeing wider market interest beyond graph, basically \"AI that investigates\" across those operational DBs (& live systems). You can think of it as vibe coding is code-oriented, while louie is vibe investigating that is more data-oriented. Ex: Native plans don't think in unit tests but cross-validation, and instead of grepping 1,000 files, we get back a dataframe of 1M query results and pass that between the agents for localized agentic retrieval on that vs rehammering db. The CCC talk gives a feel for this in the interactive setting.reply",
      "aren't the system prompt of Claude public in the doc at https://platform.claude.com/docs/en/release-notes/system-pro... ?reply",
      "This is for Claude Code, not just Claude.reply",
      "I've had a LOT of success keeping a \"working memory\" file for CLI agents. Currently testing out Codex now, and what I'll do is spend ~10mins hashing out the spec and splitting it into a list of changes, then telling the agent to save those changes to a file and keep that file updated as it works through them. The crucial part here is to tell it to review the plan and modify it if needed after every change. This keeps the LLM doing what it does best (short term goals with limited context) while removing the need to constantly prompt it. Essentially I feel like it's an alternative to having subagents for the same or a similar resultreply",
      "Planning mode actually creates whole markdown files, then wipes the context that was required to create that plan before starting work. Then it holds the plan at the system prompt level to ensure it remains top of mind (and survives unaltered during context compaction).reply",
      "The TODO lists are also frequently re-inserted into the context HEAD to keep the LLM aware of past and next steps.And in the event of context compression, the TODO serves as a compact representation of the session.reply"
    ],
    "link": "https://www.mihaileric.com/The-Emperor-Has-No-Clothes/",
    "first_paragraph": "\n\n\n\n\n\n\n\nToday AI coding assistants feel like magic. You describe what you want in sometimes barely coherent English, and they read files, edit your project, and write functional code. But here\u2019s the thing: the core of these tools isn\u2019t magic. It\u2019s about 200 lines of straightforward Python. Let\u2019s build a functional coding agent from scratch.Before we write any code, let\u2019s understand what\u2019s actually happening when you use a coding agent. It\u2019s essentially just a conversation with a powerful LLM that has a toolbox. That\u2019s the whole loop. The LLM never actually touches your filesystem. It just asks for things to happen, and your code makes them happen.Our coding agent fundamentally needs three capabilities:That\u2019s it. Production agents like Claude Code have a few more capabilities including grep, bash, websearch, etc but for our purposes we\u2019ll see that three tools is sufficient to do incredible things.We start with basic imports and an API client. I\u2019m using OpenAI here, but this works with a"
  },
  {
    "title": "Embassy: Modern embedded framework, using Rust and async (github.com/embassy-rs)",
    "points": 83,
    "submitter": "birdculture",
    "submit_time": "2026-01-08T23:00:45 1767913245",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=46547740",
    "comments": [
      "Probably off topic, but what's the best way to get started with embedded development? I've been a web developer for over a decade, but I'd really love to try something much lower level, and I'm currently making my way through the Rust book. I've got a Raspberry Pi on the way, but I assume that's not truly embedded development.reply",
      "I am a big fan of the embassy project and it\u2019s a great example of why async Rust is so great: Because this is possible. It works without a heap, is a really low cost abstraction and you can do stuff concurrently on a single core chip (where you can\u2019t just spawn a new \u201cthread\u201d) and you don\u2019t have the complexity of an RTOS. I believe there is a great future for embassy ahead and it\u2019s so great how far the team has come.I also want to give a shoutout to reqwless (https://github.com/drogue-iot/reqwless) which is a HTTP client for embassy-net that even supports HTTPS!Rust embedded was really never actually better then C or C++ but embassy for me is a big reason why I now make my buying decision based on how well I can use Rust on the MCU.reply",
      "This channel contains videos of journey from setting up environment and busy wait embedded LED blinking, to basically re-inventing and then using Embassy. 4 oldest videos.https://www.youtube.com/@therustybits/videosreply",
      "I had been using this to try to build a Spark modeling guitar amp pedal controller, controlling the amp via BLE. It seemed pretty promising, and they have their own fully OSS Rust BLE stack. It seemed a little early days with that though, it seemed like the APIs were changing quite a bit and it required pinning git revisions in Cargo. I'm excited to see where the project goes!reply",
      "This is at the center of a friction point in embedded rust: most of the OSS ecosystem has shifted to this framework, and as a result, is incompatible with, or is high friction if you don't want to make your firmware and control flow Async. This is notable because Rust embedded is nascent and small, so I think splitting the ecosystem along with Async is not ideal. It's also confused some people new to embedded: I regularly hear this dichotomy: \"Async vs blocking\"; the assumption being if you are not using Embassy, your code blocks the CPU when waiting for I/O, etc.If you enjoy Async PC rust programming, I think this will be a good starting point. I like how it has unified hardware access to different MCUs, and its hardware support for STM32, for example, is a step up from the initial generation of Trait-based HALs. I seem to be the odd one out as an embedded rust programmer (Personally and professionally) for whom Async is not my cup of tea.reply",
      "In my experience, most of embassy's HALs support blocking variants as well.I don't quite understand the opposition to async in this context though. Embassy's executor is quite nice. You get to write much more straightforward linear code, and it's more battery efficient because the CPU core goes to sleep at await points. The various hardware interrupts then wake up the core and notify the executor to continue making progress.The compiler transformation from async/await to a state machine is a godsend for this. Doing the equivalent by hand would be a major pain to get the same power efficiency and code ergonomics.reply",
      "I think it's interesting because they seem to have built some vaguely pretty decent interfaces and drivers. Before that there were some attempts to make a rust embedded HAL but I think they were a bit too basic and didn't seem to get much traction. Also async interfaces are probably the most generic, because you can hook them up to superloops, single-threaded applications, and threaded code relatively easily (at least, more easily than the other way around), and IMO one of the big reasons Arduino stayed firmly hobbyist tier is because it was almost entirely stuck in a single-threaded blocking mindset and everything kind of fell apart as soon as you had to do two things at once.reply",
      "> superloopsI\u2019ve been doing async non-blocking code for decades, but this is the first time I e seen that word used? I\u2019m assume you\u2019re meaning something like one big ass select!() or is this something else?> IMO one of the big reasons Arduino stayed firmly hobbyist tier is because it was almost entirely stuck in a single-threaded blocking mindset and everything kind of fell apart as soon as you had to do two things at once.This. Having to do something like this recently, in C, was not fun and end up writing your own event management layer (and if you\u2019re me, poorly).reply",
      "Superloop is common terminology in the firmware space. They are cruder than a giant-state-machine-like case statements(but may use still them for control flow). They usually involve many non-nested if statements for handling events, and you usually check for every event one by one on every iteration of the loop. They are an abstraction and organizational nightmare once an application gets complex enough and is ideally only used in places where an RTOS won\u2019t fit. I would not consider asynchronous frameworks like Embassy to be superloops.reply",
      "Maybe stuff has changed a lot in the last year but I didn\u2019t experience that problem so far. For me it was the other way around mostly. Where did you encounter that?reply"
    ],
    "link": "https://github.com/embassy-rs/embassy",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Modern embedded framework, using Rust and async.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Embassy is the next-generation framework for embedded applications. Write safe, correct, and energy-efficient embedded code faster, using the Rust programming language, its async facilities, and the Embassy libraries.The Rust programming language is blazingly fast and memory-efficient, with no runtime, garbage collector, or OS. It catches a wide variety of bugs at compile time, thanks to its full memory- and thread-safety, and expressive type system.Rust's async/await allows for unprecedentedly easy and efficient multitasking in embedded systems. Tasks get transformed at compile time into state machines that get run cooperatively. It requires no dynamic memory al"
  },
  {
    "title": "Sopro TTS: A 169M model with zero-shot voice cloning that runs on the CPU (github.com/samuel-vitorino)",
    "points": 133,
    "submitter": "sammyyyyyyy",
    "submit_time": "2026-01-08T20:37:07 1767904627",
    "num_comments": 61,
    "comments_url": "https://news.ycombinator.com/item?id=46546113",
    "comments": [
      "I don't understand the comments here at all. I played the audio and it sounds absolutely horrible, far worse than computer voices sounded fifteen years ago. Not even the most feeble minded person would mistake that as a human. Am I not hearing the same thing everyone else is hearing? It sounds straight up corrupted to me. Tested in different browsers, no difference.reply",
      "I thought it was RFKreply",
      "spasmodic dysphonia as a service.reply",
      "As I said, some reference voices can lead to bad voice quality. But if it sounds that bad, it\u2019s probably not it. Would love to dig into it if you wantreply",
      "I mean I'm talking about the mp4. How could people possibly be worried about scammers after listening to that?reply",
      "I didn\u2019t specially cherry pick those examples. You can try it anyway for yourself. But thanks for the feedback anywayreply",
      "No shade on you. It's definitely impressive. I just didn't understand people's reactions.reply",
      "That's cool and useful.IMO, the best alternative is Chatterbox-TTS-Server [0] (slower, but quite high quality).[0] https://github.com/devnen/Chatterbox-TTS-Serverreply",
      "Is there yet any model like this, but which works as a \"speech plus speech to speech\" voice modulator \u2014 i.e. taking a fixed audio sample (the prompt), plus a continuous audio stream (the input), and transforming any speech component of the input to have the tone and timbre of the voice in the prompt, resulting in a continuous audio output stream? (Ideally, while passing through non-speech parts of the input audio stream; but those could also be handled other ways, with traditional source separation techniques, microphone arrays, etc.)Though I suppose, for the use-case I'm thinking of (v-tubers), you don't really need the ability to dynamically change the prompt; so you could also simplify this to a continuous single-stream \"speech to speech\" model, which gets its target vocal timbre burned into it during an expensive (but one-time) fine-tuning step.reply",
      "Chatterbox TTS does this in \u201cvoice cloning\u201d mode but you have to implement the streaming part yourself.There are two inputs: audio A (\u201cstyle\u201d) and B (\u201ccontent\u201d). The timbre is taken from A, and the content, pronunciation, prosody, accent, etc is taken from B.Strictly soeaking, voice cloning models like this and chatterbox are not \u201cTTS\u201d - they\u2019re better thought of as \u201cS+STS\u201d, that is, speech+style to speechreply"
    ],
    "link": "https://github.com/samuel-vitorino/sopro",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A lightweight text-to-speech model with zero-shot voice cloning\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Sopro (from the Portuguese word for \u201cbreath/blow\u201d) is a lightweight English text-to-speech model I trained as a side project. Sopro is composed of dilated convs (\u00e0 la WaveNet) and lightweight cross-attention layers, instead of the common Transformer architecture. Even though Sopro is not SOTA across most voices and situations, I still think it\u2019s a cool project made with a very low budget (trained on a single L40S GPU), and it can be improved with better data.Some of the main features are:I only pinned the minimum dependency versions so you can install the package without having to create a separate env. However, some versions of Torch work best. Fo"
  },
  {
    "title": "Bose has released API docs and opened the API for its EoL SoundTouch speakers (arstechnica.com)",
    "points": 2086,
    "submitter": "rayrey",
    "submit_time": "2026-01-08T15:07:57 1767884877",
    "num_comments": 315,
    "comments_url": "https://news.ycombinator.com/item?id=46541892",
    "comments": [
      "Direct link to the announcement (from the article):https://www.bose.com/soundtouch-end-of-lifeSoundTouch API Documentation (pdf) linked from the announcement:https://assets.bosecreative.com/m/496577402d128874/original/...",
      "This is how \"end of support\" should be handled. Instead of turning devices into e-waste, open-source them and let the community extend their life. Kudos to Bose for setting a good example.More companies should follow this approach - especially as right-to-repair becomes a bigger issue.reply",
      "Bose should not receive praise for this move. Bose only took this action after community backlash. In an older version of their end-of-life announcement, most functionality of the speaker systems would have removed and transformed the devices into dumb-speakers/amps.Good that they changed their statement and took the right action. Even better for the community for stepping up and 'forcing' Bose to do so.Sources:\nhttps://web.archive.org/web/20251201051242/https://www.bose....\nhttps://arstechnica.com/gadgets/2025/10/bose-soundtouch-home...reply",
      "> Bose should not receive praise for this move. Bose only took this action after community backlash.They received the backlash, they responded to it by properly addressing the criticism and doing the right thing. It should be praised. Especially since it wasn't some PR-centric damage control, but an actual direct address of the specific points their original approach was criticized for.Compare Bose's response to that of Sonos (another large techy audio brand). Sonos had an absolutely massive backlash recently (within the past few years iirc) in regards to deprecating software support for their older speakers that I'd read about everywhere (including HN) for months and months.Afaik, it didn't lead to Sonos doing the right thing in the end (unlike the scenario at hand here), despite the online outrage being way more widespread than in the Bose's case.reply",
      "Sonos gets backlash every few years and they don\u2019t change. It\u2019s almost as if consumers are shit at boycotting companies.Which does make Boses move even more impressive when you think about how it wouldn\u2019t have affected their business to do nothing.reply",
      "And some people have been advocating for Apple to do something similar with old iPhones and tablets for a decade, and there\u2019s no sign. Their privilege but not great for the world.reply",
      "Would you elaborate? Because my understanding is that Apple has offered outstanding support for older devices in terms of iOS support for quite old devices.reply",
      "Whilst the support might be \"outstanding\" - the discussion is what happens once devices are no longer supported.reply",
      "You can't release all the documentation just because the entire phone isn't supported. Many of the components come from other suppliers and aren't obsolete, and you can't just reveal all your suppliers' IP.reply",
      "Yeah, it\u2019s good to see a sensible response to community pressure here. While I take the point that they only conceded after pressure, at least they did concede. I\u2019ve upgraded their brand in my mind from \u201cplanned obsolescence e-waste villain\u201d to \u201ccares about PR and will do the right thing while being watched\u201d. I think the only truly trustworthy companies regarding end of support handling in consumer tech are those whose brand is explicitly tied to openness / repairablity ala home assistant, framework laptops, etc\u2026Sadly those tend to be niche companies already focused on power users, but any other firms should be  considered guilty until proven innocent of enshittification (forced bricking, closed source, subscription creep, privacy violations and data brokering).reply"
    ],
    "link": "https://arstechnica.com/gadgets/2026/01/bose-open-sources-its-soundtouch-home-theater-smart-speakers-ahead-of-eol/",
    "first_paragraph": "\n        If companies insist on bricking gadgets, this is a better way to do it.\n      Bose released the Application Programming Interface (API) documentation for its SoundTouch speakers today, putting a silver lining around the impending end-of-life (EoL) of the expensive home theater devices.In October, Bose announced that its SoundTouch Wi-Fi speakers and soundbars would become dumb speakers on February 18. At the time, Bose said that the speakers would only work if a device was connected via AUX, HDMI, or Bluetooth (which has higher latency than Wi-Fi).After that date, the speakers would stop receiving security and software updates and lose cloud connectivity and their companion app, the Framingham, Massachusetts-based company said. Without the app, users would no longer be able to integrate the device with music services, such as Spotify, have multiple SoundTouch devices play the same audio simultaneously, or use or edit saved presets.The announcement frustrated some of Bose\u2019s lon"
  },
  {
    "title": "The Unreasonable Effectiveness of the Fourier Transform (joshuawise.com)",
    "points": 138,
    "submitter": "voxadam",
    "submit_time": "2026-01-08T19:00:28 1767898828",
    "num_comments": 62,
    "comments_url": "https://news.ycombinator.com/item?id=46544981",
    "comments": [
      "My favorite story about the Fourier Transform is that Carl Friedrich Gauss stumbled upon the algorithm for the Fast Fourier Algorthim over a century before Cooley and Tukey\u2019s publication in 1965 (which itself revolutionized digital signal processing).[1] He was apparently studying the motion of the asteroids Pallas and Juno and wrote the algorithm down in his notes but it never made it into public knowledge.[1] https://www.cis.rit.edu/class/simg716/Gauss_History_FFT.pdfreply",
      "There is a saying about Gauss: when another mathematician came to show him a new result, Gauss would remark that he had already worked on it, open a drawer in his desk, and pull out a pile of papers on the same topic.reply",
      "One of the things I admire about many top mathematicians today like Terence Tao is that they are clearly excellent mentors to a long list of smart graduate students and are able to advance mathematics through their students as well as on their own. You can imagine a half-formed thought Terence Tao has while driving to work becoming a whole dissertation or series of papers if he throws it to the right person to work on.In contrast, Gauss disliked teaching and also tended to hoard those good ideas until he could go through all the details and publish them in the way he wanted. Which is a little silly, as after a while he was already widely considered the best mathematician in the world and had no need to prove anything to anyone - why not share those half-finished good ideas like Fast Fourier Transforms and let others work on them! One of the best mathematicians who ever lived, but definitely not my favorite role model for how to work.reply",
      "Well, in that time it was more or less how mathematics worked. It was a way of showing off, and often it would be a case of \"Hey I've solved this problem, bet no-one else can\". It was only later it became a lot more collaborative (and a bit more focused on publishing proofs).reply",
      "You're correct that the culture of mathematics has changed a lot, and has become much more collaborative. The rise of the modern doctoral training system in Germany later in the 19th century is also relevant. So really Gauss's example points primarily to how much mathematics has changed. But at the same time, I think you could reasonably take Gauss to task even applying the standards of his own era - compare him with Euler, for example, who was much more open with publication and generous with his time and insights, frequently responding to letters from random people asking him mathematical questions, rather like Tao responding to random comments on his blog (which he does). I admire Euler more, and he was born 70 years before Gauss.Of course, irascible brilliance and eccentricity has an honorable place in mathematics too - I don't want to exclude anyone. (Think Grigori Perelman and any number of other examples!)reply",
      "Gauss is gonna Gauss.reply",
      "Gauss's notes and margins is riddled with proofs he didn't bother to publish - he was wild.Not sure if true, but allegedy he insisted his son not go into maths, as he would simply end up in his father's shadow as he deemed it utterly Impossible to surpass his brilliance in maths :'Dreply",
      "> as he would simply end up in his father's shadow as he deemed it utterly Impossible to surpass his brilliance in mathsDefinitely true but also bad parenting.  Gauss was somewhat of a freak of nature when it came to math.  Him and Euler are two of the most unreasonably productive mathematicians of all time.reply",
      "How was Gauss so productive with 6 children?reply",
      "Probably sexual division of labor.reply"
    ],
    "link": "https://joshuawise.com/resources/ofdm/",
    "first_paragraph": "Notes from Joshua Wise's talk at Teardown 2025.New: You can now watch a recording of my talk on my YouTube channel!  Or you can\njust click \"play\" below, I suppose.Here are a few resources from my talk.Thanks so much for coming!  Please let me know if you have feedback on\nthis.  I'd love to hear what you thought."
  },
  {
    "title": "Richard D. James aka Aphex Twin speaks to Tatsuya Takahashi (2017) (archive.org)",
    "points": 72,
    "submitter": "lelandfe",
    "submit_time": "2026-01-08T21:17:26 1767907046",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=46546614",
    "comments": [
      "This is also a great deep wide-ranging interview/conversation c SYRO releasehttps://web.archive.org/web/20141111015756/http://noyzelab.b...https://web.archive.org/web/20141112205122/https://noyzelab....reply",
      "The depth of knowledge of Richard James surprises me every time I come across articles about him.No wonder Aphex Twin's music stands out the way it does. He's always a fair way ahead of the curve.reply",
      "He is indeed a very technical and nerdy person. For example, he was an early adopter of the SuperCollider audio programming language.Fun trivia: he was trolling the SuperCollider mailing list under the alias \"eric hard jams\" which is an anagram of Richard [D.] James. Some of his messages were truely horrendous and he got kicked out eventually. He is quite a character...reply",
      "Was it confirmed that eric hard jams was actually him?How strange it is that we so easily forgive bad behavior from people we love.reply",
      "It hasn't been confirmed 100% but I remember reading a post by James McCartney (author of SuperCollider) himself, going something like \"Shut the fuck up, Richard!\". Since they both knew each other personally, I assume that JMC thought that \"eric hard jams\" was indeed Richard James.reply",
      "It\u2019s strange but common. I love the music of Miles Davis and consider him a genius. I also give him a pretty poor review in terms of his behavior as a human being.People are complex.reply",
      "Has anyone used SuperCollider or computer music framework to make anything resembling a pop song?Look at how easily a producer can make a pop song in Ableton\nhttps://youtube.com/watch?v=F5CPQ8LU36wreply",
      "SuperCollider would be the wrong tool for the job.reply",
      "Yes, not to long ago I was looking into the Easter egg in Windowlicker, amazing.\nhttps://eeggs.com/items/34824.htmlreply",
      "Wow, eeggs.com. Haven't thought about that site in years (maybe even decades).reply"
    ],
    "link": "https://web.archive.org/web/20180719052026/http://item.warp.net/interview/aphex-twin-speaks-to-tatsuya-takahashi/",
    "first_paragraph": "Richard D. James: I really enjoyed working on this with you. I know I only joined the project near the end, but I found it really exciting. Like a proper job, ha.Tatsuya Takahashi: Richard, it was amazing working with you on the monologue. And now to be interviewed by you?!? That's crazy. But also a lot of fun. The monologue was also the last Korg synth that I was involved with directly, so I guess it's a nice conclusion to things.RDJ: It is now the only synth on the market currently being made to have full microtuning editing, congratulations!TT: Thanks! But it was completely because of you that we included microtuning. If you hadn't insisted on it, I definitely wouldn't have discovered how powerful it was. Did you ever have a moment of realisation, or some kind of trigger that made you discover microtuning?RDJ: The first thoughts that I had about tuning in general happened with my early noodlings on a Yamaha DX100, one of the first synths I saved up for. I remember looking at the mas"
  },
  {
    "title": "The Jeff Dean Facts (github.com/lritzdorf)",
    "points": 394,
    "submitter": "ravenical",
    "submit_time": "2026-01-08T13:02:05 1767877325",
    "num_comments": 141,
    "comments_url": "https://news.ycombinator.com/item?id=46540498",
    "comments": [
      "Hey! I created Jeff Dean Facts! Not the jokes themselves, but the site that collected them.It was in 2008 I think (give or take a year, can't remember). I worked at Google at the time. Chunk Norris Facts was a popular Internet meme (which I think later faded when he came out as MAGA, but I digress...). A colleague (who wishes to remain anonymous) thought the idea of Jeff Dean Facts would be funny, and April 1st was coming up.At the time, there was a team working on an experimental web app hosting platform code named Prometheus -- it was later released as App Engine. Using an early, internal build I put together a web site where people could submit \"facts\" about Jeff Dean, rate each other's facts on a five-star scale, and see the top-rated facts. Everything was anonymous. I had a few coworkers who are funnier than me populate some initial facts.I found a few bugs in Prometheus in the process, which the team rapidly fixed to meet my \"launch date\" of April 1st. :)On the day, which I think was a Sunday, early in the morning, I sent an email to the company-wide \"misc\" mailing list (or maybe it was eng-misc?) from a fake email address (a google group alias with private membership), and got the mailing list moderator to approve it.It only took Jeff an hour or two to hack his way through the back-end servers (using various internal-facing status pages, Borg logs, etc.) to figure out my identity.But everyone enjoyed it!My only regret is that I targeted the site specifically at Jeff and not Sanjay Ghemawat. Back then, Jeff & Sanjay did everything together, and were responsible for inventing a huge number of core technologies at Google (I have no idea to what extent they still work together today). The site was a joke, but I think it had the side effect of elevating Jeff above Sanjay, which is not what I intended. Really the only reason I targeted Jeff is because he's a bit easier to make fun of personality-wise, and because \"Jeff Dean Facts\" sort of rolls off the tongue easier that \"Sanjay Ghemawat Facts\" -- but in retrospect this feels a little racist. :(My personal favorite joke is: Jeff Dean puts his pants on one leg at a time, but if he had more than two legs, you'd see his approach is actually O(log n).reply",
      "Hi Kenton! I was the recent grad you handed this web app off to after you built it, so I expanded Jeff Dean Facts so that anyone could create and rate facts about anyone at Google :). There were a ton of team in-jokes added before I stopped working on it - O(5k) IIRC! :)This web app was also how I learned the pain of maintaining a live web service with a lot of ever-changing dependencies. How I sighed when the AppEngine version changed and I had to fix things again...I handed it off again before I left Google but I have no memory of who that was to unfortunately :(.reply",
      "Hi Ari,Thanks so much for falling for my trick and taking it over, I was getting pretty sick of dealing with the same issues you describe. :)One of the reasons Cloudflare Workers has a policy that we absolutely never break deployed code, ever. (Well... at least not intentionally...)reply",
      "I just searched Moma, and your note about it going down is the most recent update on this front. Interestingly though, it looks like Moma itself has a custom SERP renderer for Jeff Dean facts that came up when I searched. The example fact that came up was hilarious, but I guess I shouldn't share it on public HN.reply",
      "I\u2019m no expert, but I certainly wouldn\u2019t call that racism. Bias, absolutely. And it\u2019s important that we acknowledge our biases.But in a more literal sense, the chance of your joke landing was likely higher due to the things that you stated and due to your audience and their biases.I don\u2019t see your joke as being in any way harmful towards Sanjay aside from potential knock on effects of Jeff Dean being more popular. But if you try to calculate every second and third order consequence of everything that you do, let alone any moments of humor you might have.. Well, you might as well lock yourself in a cell now.reply",
      "It's not remotely racist. OP is being self critical for no good reason.reply",
      "> I don\u2019t see your joke as being in any way harmful towards Sanjay aside from potential knock on effects of Jeff Dean being more popularI mean\u2026 yeah. When two people are peers and comparably well regarded, and one is elevated above the other and enjoys increased popularity, familiarity, and respect, and the elevation is because that person's name comes from a culture that is more aligned with the dominant culture and easier for them to engage with\u2026 that is a pretty textbook example of systemic racism.I'm not at all saying this to demonize Kenton. We can make mistakes and reflect on them later, and that's laudable. But it is important to recognize these systems for what they are, so that we can notice them when they happen all around us every day.reply",
      "> that is a pretty textbook example of systemic racism.It\u2019s not \u201cracism.\u201d There\u2019s plenty of Indians with names that are easy for English speakers. Conversely, the same situations would\u2019ve presented itself if the other person was any sort of white Eastern European.In fact, calling this \u201cracist\u201d is itself racist. I have close friends with family names from Poland or Croatia where we don\u2019t even try to pronounce their names correctly. Nobody feels bad about that. But for some reason if it\u2019s a \u201cbrown person\u201d we\u2019re suddenly super sensitive about it. That is differential treatment based on race.People get awkward about how to pronounce my name because I\u2019m brown. But it\u2019s hard to pronounce because it\u2019s misspelled Germanic! They wouldn\u2019t act that way if I was a white guy with the same name.reply",
      "Are we... arguing about what happened in my head?As the world's foremost expert about what happened in my head, do I get to, like, pick a winner here?If so I pick tczMUFlmoNk, I think their description is accurate. (I think you might want to re-read it as it feels like you are responding to something else.)If I don't get to pick, this is quite weird! \"People on Hacker News tell me I'm wrong about my own thoughts.\" was not on my -- actually wait, that doesn't sound unexpected at all now that I write it out! OK, carry on.reply",
      "You wrote what your thoughts were. I\u2019m just weighing in on whether your thoughts are \u201cracist.\u201d To the extent you feel sensitive about the issue because someone has darker skin, where you probably wouldn\u2019t have written that part of the post if the other guy were Polish, that\u2019s racist. It\u2019s racist to treat people differently based on skin color, even if you\u2019re well intentioned about it.reply"
    ],
    "link": "https://github.com/LRitzdorf/TheJeffDeanFacts",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A consolidated list of the Jeff Dean Facts!\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.The \"Jeff Dean facts\" are a set of jokes that revolve around the extraordinary programming prowess of their titular Google employee. Simply put, they are the coding equivalent of Chuck Norris-style jokes (for example, \"Chuck Norris can slam a revolving door\").I first encountered the Facts on a random Quora page, though as of recently many of them appear to have been removed. Thus, in order to preserve this invaluable cache of programmer humor for posterity, I decided to create this repository. It is a combined list of various versions of the Facts, beginning with a text file that I copied from the Quora post sometime in 2019, when the original answer still existed. It"
  },
  {
    "title": "Google AI Studio is now sponsoring Tailwind CSS (twitter.com/officiallogank)",
    "points": 459,
    "submitter": "qwertyforce",
    "submit_time": "2026-01-08T19:09:23 1767899363",
    "num_comments": 159,
    "comments_url": "https://news.ycombinator.com/item?id=46545077",
    "comments": [
      "https://xcancel.com/OfficialLoganK/status/200933926325156690...",
      "Ironic that you are posting a site that does exactly what Tailwind is complaining about lolreply",
      "I wouldn\u2019t use xcancel if Twitter was usable for guestsreply",
      "I don't think I'm worried about defunding a trillionaire.reply",
      "First they came for the trillionaires, and I said nothing, because good.reply",
      "This is good, but it doesn't necessarily mean that Tailwind is out of the financial difficulty that we talked about yesterday. You can sponsor Tailwind for as little as $6,000/year. 29 companies were already sponsoring Tailwind including 16 companies at the $60,000/year level. Maybe Google AI Studio has decided to shell out a lot more, but it could also be a relatively small sponsorship compared to the $1.1M in sponsorships that Tailwind is already getting. Google has deep pockets and could easily just say \"f-it, we're betting on AI coding and this tool helps us make UIs and $2M/year is nothing compared to what we're spending on AI.\" It's also possible that the AI Studio team has a small discretionary budget and is giving Tailwind $6,000/year.It's good, but it's important to read this as \"they're offering some money\" and not \"Tailwind CSS now doesn't have financial issues because they have a major sponsor.\" This could just be a 1-5% change in Tailwind's budget. We don't know.And that's not to take away from their sponsorship, but on the heels of the discussion yesterday it's important to note that Tailwind was already being sponsored by many companies and still struggling. This is a good thing, but it's hard to know if this moves the needle a bunch on Tailwind's problems. Maybe it'll be the start of more companies offering Tailwind money and that'd be great.reply",
      "No ill will towards the team, but isn\u2019t it almost absurd that a CSS library is funded to the tune of 1m+ yearly and is still in financial difficulty? It is technically complete. There is no major research work or churn like in React, no monstruous complexity like Webpack.reply",
      "Having worked on design system teams before people can burn a lot of time and money doing overly nuanced stuff. I have been in meetings discussing removing/adding a property on a React component before.That said 3 motivated developers and a designer should be more than sufficient to build a css library, but you could 100% have a team of 20 and they would find stuff to do.reply",
      "That's how bloat happens.reply",
      "I'd imagine that infrastructure costs are rather significant for Tailwind, and that there are non-neglibible organizational costs as well.reply"
    ],
    "link": "https://twitter.com/OfficialLoganK/status/2009339263251566902",
    "first_paragraph": "We\u2019ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.Help Center\nTerms of Service\nPrivacy Policy\nCookie Policy\nImprint\nAds info\n      \u00a9 2026 X Corp.\n    "
  },
  {
    "title": "Why I Left iNaturalist (kueda.net)",
    "points": 3,
    "submitter": "erutuon",
    "submit_time": "2026-01-09T01:17:51 1767921471",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://kueda.net/blog/2026/01/06/why-i-left-inat/",
    "first_paragraph": "After almost 18 years, I left iNaturalist, the product and organization I helped create. I left because I don\u2019t believe the current Leadership team is pointing the product in the right direction, and I don\u2019t think they are managing their talented staff in an empathetic or effective way. If you\u2019d like me to continue working on natural history software, support me on Patreon.This post is an announcement for those who were unaware, an explanation for those who are confused, and a record so I don\u2019t forget.I wanted to build something like iNat shortly after I moved to the San Francisco Bay Area in 2003. In 2007 I attended the UC Berkeley School of Information and built it along with my fellow students and co-founders Nate Agrin and Jess Kline. Nate and I worked on it a bit in our spare time after graduating, and I started collaborating with Scott in 2009, still in our spare time. Scott was instrumental in recruitment, funding, and collaboration, and we formed an LLC together as a way to hav"
  },
  {
    "title": "AI coding assistants are getting worse? (ieee.org)",
    "points": 222,
    "submitter": "voxadam",
    "submit_time": "2026-01-08T15:20:15 1767885615",
    "num_comments": 348,
    "comments_url": "https://news.ycombinator.com/item?id=46542036",
    "comments": [
      "One thing I find really funny is when AI enthusiasts make claims about agents and their own productivity its always entirely anecdotally based on their own subjective experience, but when others make claims to the contrary suddenly there is some overwhelming burden of proof that has to be reached in order to make any sort of claims regarding the capabilities of AI workflows. So which is it?reply",
      "It's an impossible thing to disprove. Anything you say can be countered by their \"secret workflow\" they've figured out. If you're not seeing a huge speedup well you're just using it wrong!The burden of proof is 100% on anyone claiming the productivity gainsreply",
      "This gets comical when there are people, on this site of all places, telling you that using curse words or \"screaming\" with ALL CAPS on your agents.md file makes the bot follow orders with greater precision. And these people have \"engineer\" on their resumes...reply",
      "there's actually quite a bit of research in this field, here's a couple:\"ExpertPrompting: Instructing Large Language Models to be Distinguished Experts\"https://arxiv.org/abs/2305.14688\"Persona is a Double-edged Sword: Mitigating the Negative Impact of Role-playing Prompts in Zero-shot Reasoning Tasks\"https://arxiv.org/abs/2408.08631reply",
      "Those papers are really interesting, thanks for sharing them!Do you happen to know of any research papers which explore constraint programming techniques wrt LLMs prompts?For example:  Create a chicken noodle soup recipe.\n\n  The recipe must satisfy all of the following:\n\n    - must not use more than 10 ingredients\n    - must take less than 30 minutes to prepare\n    - ...reply",
      "I've been trying to stop the coding assistants from making git commits on their own and nothing has been working.reply",
      "hah - i'm the opposite, I want everything done by the AI to be a discrete, clear commit so there is no human/AI entanglement.  If you want to squash it later that's fine but you should have a record of what the AI did. This is Aider's default mode and it's one reason I keep using it.reply",
      "Why not use something like Amp Code which doesn't do that, people seem to rage at CC or similar tools but Amp Code doesn't go making random commits or dropping databases.reply",
      "run them in a VM that doesn't have git installed. Sandboxing these things is a good idea anyways.reply",
      "> Sandboxing these things is a good idea anyways.\n\nHonestly, one thing I don't understand is why agents aren't organized with unique user or group permissions. Like if we're going to be lazy and not make a container for them then why the fuck are we not doing basic security things like permission handling.Like we want to act like these programs are identical to a person on a system but at the same time we're not treating them like we would another person on the system? Give me a fucking claude user and/or group. If I want to remove `git` or `rm` from that user, great! Also makes giving directory access a lot easier. Don't have to just trust that the program isn't going to go fuck with some other directoryreply"
    ],
    "link": "https://spectrum.ieee.org/ai-coding-degrades",
    "first_paragraph": "Newer models are more prone to silent but deadly failure modesJamie Twiss is a banker and a data scientist who works at the intersection of data science, artificial intelligence, and consumer lending. In recent months, I\u2019ve noticed a troubling trend with AI coding assistants. After two years of steady improvements, over the course of 2025, most of the core models reached a quality plateau, and more recently, seem to be in decline. A task that might have taken five hours assisted by AI, and perhaps ten hours without it, is now more commonly taking seven or eight hours, or even longer. It\u2019s reached the point where I am sometimes going back and using older versions of large language models (LLMs).I use LLM-generated code extensively in my role as CEO of Carrington Labs, a provider of predictive-analytics risk models for lenders. My team has a sandbox where we create, deploy, and run AI-generated code without a human in the loop. We use them to extract useful features for model constructio"
  },
  {
    "title": "Show HN: macOS menu bar app to track Claude usage in real time (github.com/richhickson)",
    "points": 80,
    "submitter": "RichHickson",
    "submit_time": "2026-01-08T18:24:17 1767896657",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=46544524",
    "comments": [
      "Alternatively there's also this with 1.7k stars already and supporting more services (Despite the name): https://github.com/steipete/CodexBar (https://codexbar.app)reply",
      "Good shout. CodexBar covers a lot more ground. This one is intentionally minimal and Claude focused because that is all I personally needed.reply",
      "Or just vibe code your own. Seems fitting.reply",
      "I am on Intel and that doesn't run, so f** me I guess?reply",
      "Is it safe ? It needs to reads Keychain. Like the OP appreply",
      "The app only reads ONE specific keychain entry:  Service: \"Claude Code-credentials\"\n\n  This is the entry created by the official Claude Code CLI when you log in. The app:\n\n  1. Only reads - never writes, modifies, or deletes any keychain data\n  2. Only accesses this one service name - cannot read any other passwords, keys, or credentials\n  3. Extracts only the OAuth access token - used to call api.anthropic.com/api/oauth/usage\n  4. Sends data only to Anthropic's API - no analytics, no third-party servers\n\n  The token never leaves your machine except to Anthropic's own API endpoint. You can verify this yourself - the entire source is ~400 lines of Swift: https://github.com/richhickson/claudecodeusage\n\n  macOS will also prompt you the first time the app tries to access this keychain entry, giving you control to allow or deny.reply",
      "\"Follow me on X\"Why... just why? Why do people keep insisting on using it?reply",
      "The charitable explanation is \"because that is where most people are.\"  Network effect is real.reply",
      "Was that ever true, even during the Twitter days?reply",
      "I guess that depends on the scope.  For people who like that flavor of social media, Twitter was where it was at, and X remains the main place.  But if the scope is 'of all people' then of course it is not true -- most of us (myself included) never got into Twitter or any of the similar social media sites.reply"
    ],
    "link": "https://github.com/richhickson/claudecodeusage",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\nA lightweight macOS menubar app that displays your Claude Code usage limits at a glance.\n\nBuilt by @richhicksonThen build with \u2318B and run with \u2318R.Install Claude Code if you haven't already:Log in to Claude Code:Launch Claude Usage - it will read your credentials from Keychain automaticallyClaude Usage reads your Claude Code OAuth credentials from macOS Keychain and queries the usage API endpoint at api.anthropic.com/api/oauth/usage.Note: This uses an undocumented API that could change at any time. The app will gracefully handle API changes but may stop working if Anthropic modifies the endpoint.Run claude in Terminal and complete the login flow.Check if the app is running in Activity Monitor. Try quitting and reopening.Clic"
  },
  {
    "title": "Show HN: A geofence-based social network app 6 years in development (localvideoapp.com)",
    "points": 34,
    "submitter": "Adrian-ChatLocl",
    "submit_time": "2026-01-08T20:56:32 1767905792",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=46546349",
    "comments": [
      "There's so many ways to fake your location data.  There's one way that you can't really fake:  Send them a secret code on a piece of paper in the US mail to their physical address.    NextDoor used to do this at one point.reply",
      "At the start, getting users is the first problem. No one is going to bother scamming your app if there is no one there.Then once people exploit the app, that doesn't mean they wont add value (e.g. contribute positive content). Maybe they are just a high school kid that wants to talk to his friends in his last town?Once you have users, then there will be other easy signals to detect: Is the person teleporting? Do they hit rate limits freq? Is their GPS location the exact 'center' of the city? Is there GPS a nice pretty number? Does their GPS location never move?reply",
      "I worked on a location-based app a few years ago and this was the exact validation method we used (after having learned about it from using NextDoor). It\u2019s incredibly slow and tedious though. We abandoned the app for other reasons but I always wondered how one could continued to manage this approach once network effects kick in and the app really starts to grow.reply",
      "I think it's a neat service layer: and infra layer that attests to people's addresses -- to break it you need to intercept mail, which is a federal offence in most states. So it piggybacks on assurances of states, even while being nonstate.In an experimental identity system I prototyped as a civic tech project[1], I paired this with scraping a government \"voter registration check\" form and comparing against, and it was a two-fold guarantee: someone had to either submit false info to the voter registry or intercept mail. In theory it was very cheap to get very high assurances, for only the cost of a postcard API per user[1]: https://github.com/patcon/id.c4nada.ca?tab=readme-ov-file#ab...reply",
      "Obviously not unhackable and often outdated as people move around, but I always thought phone number area codes were a quick and dirty way to establish or roughly segment people by location.reply",
      "How does this work if you don't get mail service at your physical address? (PO box service only)reply",
      "Let me start by saying that I really like this idea.Obviously social apps like this are faced with a chicken-and-egg dilemma of how to acquire users.  I'm no marketer, so I don't have any suggestions on how to solve this one.For myself, I avoid non-free/open-source programs in general, but especially chat apps.  I think that especially the programs we rely on to communicate should at least be transparent on the client-side.  That being said, I would absolutely try this out if the app were released as FOSS (which it doesn't look like it is?).reply",
      "Reminds me of Jodel (https://jodel.com/), an app originally focused on students.It lost quite some activity in the last decade though, gaining fewer users than it loses.reply",
      "I can no longer install Jodel, says my device is not compatible (GrapheneOS on Pixel 9).reply",
      "No web version? No sale.WTF is wrong with these social apps!?!? Who wants to chat on a tiny screen when they have a computer available. Especially for local apps that function only when you're home.reply"
    ],
    "link": "https://www.localvideoapp.com",
    "first_paragraph": "ChatLocalHomeFeaturesThe AppMoreChatLocalTerms of ServicePrivacy Policy|Contact Us\u00a9 2025 by ChatLocal."
  },
  {
    "title": "Pole of Inaccessibility (wikipedia.org)",
    "points": 27,
    "submitter": "benbreen",
    "submit_time": "2026-01-03T23:21:26 1767482486",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=46482844",
    "comments": [
      "Pole of Inaccessibility is also a useful technique for placing the label for a polygon at a visually pleasing location on a map. @mourner came up with a more efficient algorithm for computing the point https://blog.mapbox.com/a-new-algorithm-for-finding-a-visual... (https://github.com/mapbox/polylabel) which JTS's MaximumInscribedCircle utility is based on, which I use for \"innermost point\" label placement in planetiler.reply",
      "Fun! If you want to compute these yourself and/or if you like hiking into the mathematically middlest-of-nowhere location, here's a good blog post: https://notes.secretsauce.net/notes/2015/05/06_poles-of-inac...reply",
      "I guess Furiosa is set near Papunya, Northern Territory, Australia then...(The first chapter of the movie is titled The Pole of Inaccessibility)reply",
      "Most of the filming was in New South Wales, but it was intended to do some in the NT originally. Until they realised what the red dust does to all equipment. Cameras, cars, anything.Might be a reference to that.reply",
      "What is more accessible, the middle point of a small island or the interior of a concrete bunker near the shoreline of which the key was lost?reply",
      "Yes.reply",
      "Category error.reply",
      "Point Nemo is a nice reminder that some \u201cgeography facts\u201d are really optimization problems in disguise: it\u2019s defined by solving the \u201clongest swim\u201d from any coastline, not by some ancient explorer planting a flag.The fun twist is that the most remote point in the ocean is also our spacecraft cemetery, and sometimes the closest humans to it are orbiting overhead on the ISS.reply"
    ],
    "link": "https://en.wikipedia.org/wiki/Pole_of_inaccessibility",
    "first_paragraph": ""
  },
  {
    "title": "Ushikuvirus: Newly discovered virus may offer clues to the origin of eukaryotes (tus.ac.jp)",
    "points": 68,
    "submitter": "rustoo",
    "submit_time": "2026-01-08T04:39:55 1767847195",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=46537253",
    "comments": [
      "Incidentally, anyone know what is going on with this image - \u201cCryo-EM map of a center slice of the ushikuvirus particle\u201d: https://journals.asm.org/cms/10.1128/jvi.01206-25/asset/1357...It\u2019s one quarter of an image flipped horizontally and then vertically, you can see the patterns.It\u2019s a bit odd to do that? Shouldn\u2019t it just be the original EM image?reply",
      "https://www.sciencedirect.com/science/article/pii/S104784772... - there are similar results in this paper, too.After a bit of digging - it looks like it's done to sharpen features as one of the standard steps in producing these images. Where there are rotational symmetries in the things they're looking at, they focus on the smallest unit, and then rotate accordingly. If you had a trilateral symmetry, or hexagonal structure, they'd rotate 3 or 6 times around the center.You're not getting a real image of the thing, but apparently it's got data from those other segments mixed in with the rotations, so you're getting a kind of idealized structure, to make the details being studied pop out, but if you have some sort of significant deviation, damage, or non symmetric feature it'll show up as well.It's called \"imposed symmetry\"\nhttps://discuss.cryosparc.com/t/what-is-actually-occuring-wh...Neat stuff, cool thing to catch!reply",
      "So kind of like taking a picture of a human, and then taking each half, flipping along the midline, and blending to get an idealized Symmetrical Human?reply",
      "I think that might just be the original and it simply is symmetrical to that degree. I found a few more examples of \"cryo-em center slices\" and I've yet to find one that doesn't have really strong symmetry down to the small dot patterns.A different paper, this figure shows a number of cryo-em images, including a simulation, and they all show the same degree of pattern symmetry\nhttps://www.researchgate.net/figure/Central-sections-through...First figure in this third paper also shows symmetry of small patterns\nhttps://journals.asm.org/doi/10.1128/jvi.00990-22reply",
      "Thanks, those examples make it pretty clear.I still think it\u2019s super weird that it looks exactly like an EM image, but is generated. Anyway, good to know!reply",
      "According to this article the image is computed and not really directly captured https://www.chemistryworld.com/news/explainer-what-is-cryo-e...reply",
      "Rampant fraud in science papers has reached the point where hobbyists can point out obviously fake charts and graphics even in prestigious journals.Publish or perish needs to end.reply",
      "My brief, illustrated history of Microbial Mats (p.10) to Multicellular Eukaryotes (p.13) may be of interest:https://impacts.to/downloads/lowres/impacts.pdfreply",
      "I remember someone talking about \"last universal common ancestor\" at some point, the single \"origin of the cells\" or something. Is that the same as the \"archaeal ancestor\" they're referring to here? And is the \"archaeal ancestor\" the same as the \"Primitive archael cell\" mentioned in the last image in the article? (https://www.tus.ac.jp/en/mediarelations/20251219_9539_03.png)reply",
      "I believe \"archaeal cell\" is referring to an Archaea, one of the three branches of life.  All three branches derive from a more distant ancestor, LUCA.  LUCA was undoubtedly preceded by other ancestors, but there is (by definition) nothing else branching from them that has survived.reply"
    ],
    "link": "https://www.tus.ac.jp/en/mediarelations/archive/20251219_9539.html",
    "first_paragraph": "2026.01.07 WednesdayUshikuvirus, an amoeba-infecting giant virus, joins the family of giant viruses that may have driven the evolution of complex cellsThe origin of life on Earth becomes even more fascinating and complex as we peer into the mysterious world of viruses. Said to have existed since living cells first appeared, these microscopic entities differ greatly from other forms of life. Composed of only genetic material, they lack the ability to synthesize proteins, which are essential for carrying out cellular activity and, ultimately, for life by itself.As a result, scientists have long sought to unravel virus origins, how they evolve, and how they fit into the conventional tree of life. Professor Masaharu Takemura from the Graduate School of Science, Tokyo University of Science (TUS), Japan, has been at the forefront of this search. In 2001, he, along with Dr. Philip Bell, from the Department of Biological Sciences, Macquarie University, Sydney, independently proposed the cell n"
  },
  {
    "title": "Mux (YC W16) is hiring a platform engineer that cares about (internal) DX (mux.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2026-01-08T21:01:36 1767906096",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.mux.com/jobs",
    "first_paragraph": ""
  },
  {
    "title": "Fixing a Buffer Overflow in Unix v4 Like It's 1973 (sigma-star.at)",
    "points": 80,
    "submitter": "vzaliva",
    "submit_time": "2026-01-08T18:29:47 1767896987",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=46544610",
    "comments": [
      "The password and pwbuf arrays are declared one right after the other. Will they appear consecutive in memory, i.e. will you overwrite pwbuf when writing past password?If so, could you type the same password that\u2019s exactly 100 bytes twice and then hit enter to gain root? With only clobbering one additional byte, of ttybuf?Edit: no, silly, password is overwritten with its hash before the comparison.reply",
      "> will you overwrite pwbuf when writing past password?Right.> If so, could you type the same password that\u2019s exactly 100 bytes twice and then hit enter to gain root? With only clobbering one additional byte, of ttybuf?Almost.  You need to type crypt(password) in the part that overflows to pwbuf.reply",
      "Already patched this on my x86_64 v4 UNIX port. Hehe.reply",
      "What is up with fin? Is it really just writing an int 0 in the memory right after some variable present in libc or similar?        extern fin;\n\n        if(getpw(0, pwbuf))\n                goto badpw;\n        (&fin)[1] = 0;reply",
      "Predecessor of    extern FILE *stdin;reply",
      "I\u2019m guessing v4 C didn\u2019t have structs yet (v6 C does, but struct members are actually in the global namespace and are basically just sugar for offset and a type cast; member access even worked on literals. That\u2019s why structs from early unix APIs have prefixed member names, like st_mode.reply",
      "> I\u2019m guessing v4 C didn\u2019t have structs yetThere may have been a early C without structs (B had none,) but according to Ken Thompson, the addition of structs to C was an important change, and a reason why his third attempt rewrite UNIX from assembly to a portable language finally succeeded.  Certainly by the time the recently recovered v4 tape was made, C had structs:    ~/unix_v4$ cat usr/sys/proc.h\n    struct proc {\n            char    p_stat;\n            char    p_flag;\n            char    p_pri;\n            char    p_sig;\n            char    p_null;\n            char    p_time;\n            int     p_ttyp;\n            int     p_pid;\n            int     p_ppid;\n            int     p_addr;\n            int     p_size;\n            int     p_wchan;\n            int     *p_textp;\n    } proc[NPROC];\n\n    /* stat codes */\n    #define SSLEEP  1\n    #define SWAIT   2\n    #define SRUN    3\n    #define SIDL    4\n    #define SZOMB   5\n\n    /* flag codes */\n    #define SLOAD   01\n    #define SSYS    02\n    #define SLOCK   04\n    #define SSWAP   010reply",
      ")reply",
      "Heh. I had the same impulse but then didn't do it, upon refreshing the page your comment was there :)reply",
      "According to the chatbot, the first word of `fin` is the file descriptor, the second its state. \"Reset stdin\u2019s flags to a clean state\".reply"
    ],
    "link": "https://sigma-star.at/blog/2025/12/unix-v4-buffer-overflow/",
    "first_paragraph": "In 2025, the only known copy of UNIX v4 surfaced on a magnetic tape1.\nThis version marks a pivotal moment in computer history: the rewriting of UNIX into C.\nEnthusiasts quickly recovered the data and successfully ran the system on a PDP-11 simulator2.Fascinated by this artifact, I set up an instance to explore it.\nBecause the distribution includes the source code, I examined the implementation of several core utilities.\nWhile auditing the su(1) program, I identified a bug. Let\u2019s fix it.Although more than 50 years old, the su program functions similarly to its modern variant.\nAs a setuid-root executable, it validates the root password.\nIf the user provides the correct credentials, the program spawns a root shell, allowing an unprivileged user to escalate privileges.The source file, su.c, contains fewer than 50 lines of code.In short, the program executes the following steps:The logic is standard, except for one critical flaw: the password buffer has a fixed size of 100 bytes, yet the in"
  },
  {
    "title": "Recent Optimizations in Python's Reference Counting (rushter.com)",
    "points": 7,
    "submitter": "f311a",
    "submit_time": "2026-01-04T16:45:31 1767545131",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=46489657",
    "comments": [
      "This seems like a pretty big deal that should have gotten more attention in the 3.14 release. It's fairly buried in the \"What's New\" document, but apparently the original issue is at https://github.com/python/cpython/issues/130704 and corresponding PR https://github.com/python/cpython/pull/130708 .reply",
      "Has improving CPython performance become a huge focus in just the past five years or so, or is that just a perception issue on my end?reply",
      "Because of how much of a cornerstone python generally has been in AI circles, performance improvements have gotten a lot more attention in the past few years.reply"
    ],
    "link": "https://rushter.com/blog/python-refcount/",
    "first_paragraph": "It's been a while since I've written about CPython internals and its optimizations.\nMy last article on garbage collection was written 8 years ago.A lot of small optimizations were added since then. In this article, I will highlight a new optimization for\nreference counting that uses a static lifetime analysis.Reference counting is the primary memory management technique used in CPython.In short, every Python object (the actual value behind a variable) has a reference counter field that tracks how many references point to it.\nWhen an object's reference count drops to zero, the memory occupied by that object is immediately deallocated.For hot loops, this can lead to significant overhead due to the frequent incrementing and decrementing of reference counts.\nThe counter must be updated whenever an object is referenced or dereferenced, which hurts performance and trashes CPU caches.So, when you read a variable in Python, you actually write to the memory as well.For more details, you can rea"
  },
  {
    "title": "Making Magic Leap past Nvidia's secure bootchain and breaking Tesla Autopilots (ccc.de)",
    "points": 29,
    "submitter": "rguiscard",
    "submit_time": "2026-01-01T13:58:45 1767275925",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=46454160",
    "comments": [
      "Here\u2019s the video of the talk. Not sure why the schedule page was linked.https://media.ccc.de/v/39c3-making-the-magic-leap-past-nvidi...reply",
      "> The Tegra X2 is an SoC used in devices such as the Magic Leap One, and Tesla's Autopilot 2 & 2.5 promising a secure bootchain.I guess they didn\u2019t learn from the Tegra X1 which was famously responsible for the boot rom exploit on the original model of the Nintendo Switch.reply",
      "Huh, I thought Magic Leap went out of business.Didn't know they were still around!reply",
      "Unfortunately they are. They're a former shell of what they were. I think they're changing their focus to lenses or something. Last I heard they're partnering with Google and it's absolute ass. The company is effectively dead and being carved out for parts by Google is my take.It's a real bummer because they were the only company I was actually interested in seeing pursue Augmented Reality. Now it's literally the most evil companies Meta, Google, and Apple.The 90s optimism of future tech is dead and all that's left is whatever this is.reply",
      "Your sympathy is severely misplaced. Magic Leap was Theranos-sized fraud from the beginning: they never had the goods, put out a whole bunch of misleading hype to persuade consumers and gullible investors that they had the goods [0], and eventually it caught up to them. Good riddance.[0]: https://www.youtube.com/watch?v=E9r2Z5v_E9oreply",
      "It's a good thing they're effectively dead, but they don't have any useful parts to carve out, because they were a fraud through and through from day one, infested with toxic sexist nepotistic bro culture, and were an embarrassment and warning to the industry. No surprise at all they unethically bricked their useless overpriced electronic waste.https://news.ycombinator.com/item?id=29440213DonHopkins on Dec 4, 2021 | parent | context | favorite | on: I bought those AR cycling glasses that were on HN ...Why not simply do what Magic Leap does, and blatantly fake the camera images and games that don't really and couldn't possibly exist?https://news.ycombinator.com/item?id=28838500And then rip off other people's work without giving them any credit in your patents.https://news.ycombinator.com/item?id=28838443And then sexually harass the very women you employed to solve the endemic \"pink/blue problem\" and fix the nepotistic sexist bro culture, and rebuff and ignore her advice, and pay her off to keep their mouth shut when she sues you for sexual harassment.https://news.ycombinator.com/item?id=28838421And then give the entire AR industry a bad name by not delivering on your outrageously hyped promises, while burning through BILLIONS of dollars and fucking over your foolishly gullible investors.https://news.ycombinator.com/item?id=28838737reply",
      "Just curious, how fast can these embedded systems boot?reply",
      "Here is the talk if anyone is interested: https://media.ccc.de/v/39c3-making-the-magic-leap-past-nvidi...reply",
      "I hope Nvidia's new offerings (Orin, Thor, etc) don't have the same issue in their bootROM. That would be an incredibly expensive mistake.reply",
      "I hope they do, for those who want actual ownership of what they bought.reply"
    ],
    "link": "https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/making-the-magic-leap-past-nvidia-s-secure-bootchain-and-breaking-some-tesla-autopilots-along-the-way",
    "first_paragraph": "In mid 2024, a friend approached me about Magic Leap making their TX2 based XR headsets little more than a paperweight by disabling the mandatory activation servers. I morally dislike this, companies shouldn't turn functional devices into e-waste just because they want to sell newer devices.After obtaining one, and poking at the Fastboot implementation, I discovered it was based off NVIDIA's Fastboot implementation, which is source available. I found a vulnerability in the NVIDIA provided source code in how it unpacks SparseFS images (named sparsehax), and successfully blindly exploited the modified implementation on the Magic Leap One. I also found a vulnerability in it that allowed gaining persistence via how it loads the kernel DTB (named dtbhax).Still unsatisfied with this, I used fault injection to dump the BootROM from a Tegra X2 devkit.In the BootROM I discovered a vulnerability in the USB recovery mode. Exploiting this vulnerability proved difficult due to only having access to"
  },
  {
    "title": "Lights and Shadows (2020) (ciechanow.ski)",
    "points": 223,
    "submitter": "kg",
    "submit_time": "2026-01-02T19:24:10 1767381850",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=46468338",
    "comments": [
      "I skimmed the source code (base.js, light.js) to see what he was using.\nIt appears to be entirely custom, with no graphics libraries like Three.js.\nHe even implements his own low-level math functions from scratch.\nIt's impressive to see that kind of discipline.reply",
      "The best programmers are cautious about dependencies. Taking something is easy. But you don't learn. And you give up control.reply",
      "Honestly, if you\u2019re wanting to produce something as good as this, Three.js or other such things just aren\u2019t particularly helpful. It is easier to just ignore all the libraries and do it all from scratch. Popular libraries are good at producing finished products in a particular shape. When you\u2019re wanting to demonstrate the implementation steps and allow intricate fiddling and have everything polished like you want it, they\u2019re generally somewhere between painful and hopeless.You could still keep Three.js for bits like vector calculations, but it just doesn\u2019t feel worth it, it\u2019s easy enough to implement yourself\u2014or copy and paste from some such library and modify as needed\u2014and will be much lighter. And you build up the bits and pieces you need over time.reply",
      "What are some good sources to learn this kind of graphics programming work?reply",
      "The inverse-square law can be non-intuitive:* https://en.wikipedia.org/wiki/Inverse-square_lawI know a good number of photographers can struggle with it when they're getting into flash/strobe photography (even though may be good with f-stops generally, the moving of the flash stand appropriately takes some mental 'accounting').* https://www.youtube.com/watch?v=hySbIWzJAkM* https://www.youtube.com/watch?v=xO-J42VM448reply",
      "In your first link the narrator says he \"doesn\u2019t understand the physics of it\" but there's really no physics involved (ignoring scatter). It\u2019s just a consequence of the math. It\u2019s relatively easy to understand if you think of it in terms of the surface of a sphere. There is a fixed amount of light coming from a point source, and as the light travels outward you can think of it as being spread over the surface of a sphere. Since the surface area of a sphere is 4pir^2, if you double the radius the area quadruples, and therefore the light intensity at any point on the sphere drops by a factor of four.edit: And now after rtfm I see there's a nice demo of this!reply",
      "I love that the car animation has reversing lights on when reversing. The details are so good.reply",
      "15 years ago I thought this type of thing would be the future of education. Is the educational system anything like this nowadays or are kids still more or less still stuck with static textbooks?reply",
      "This is edutainment. Kind of like YouTube channels like 3Blue1Brown, Veritasium or MinutePhysics. All good, it helps build intuition, and to better understand the world, but it is a bit lacking for actually using that knowledge.Notice how few equations there are in this page, it is a common feature of edutainment, they won't give you equations unless they can't get away without. No linear algebra here, just a cosine (actually a dot product in disguise) and the inverse square law (1/r\u00b2), two equations he considered too fundamental to skip. Also notable is the lack of exercises.But now that you have read this page, and played with the interactive elements, you probably have a good understanding of how lights and shadows work, but can you write a 3D engine or even just calculate exposure time without your camera helping you? Without prior knowledge, probably not. For that, you actually need to do the maths, with exercises and all that. And by the way, look at the source code (it is not obfuscated), all the linear algebra that is not present in the article is there!That, I think, explain the discrepancy between edutainment and textbooks. Textbooks are for you to do actual work, do the maths, solve problems, etc... not just give you an overview. That's also why is takes way longer and requires a lot more effort on the student part.Interactive content like this one is good, and maybe it should be given a bit more consideration by the traditional educational system. But I don't think it can replace textbooks, at least not for the \"hard part\".reply",
      "Comparing this site to Veritasium seems unfair to me. I don't see any evidence that this site generates direct revenue. I also think it's an excellent exercise in pedagogy. The best professors and brightest colleagues I remember from college - or, since you said \"maths\" I guess I should say \"university\" ;) - demonstrated extreme facility with the physical concepts and regarded their mathematical representations only as a useful tool to characterize them and communicate about them, not an end to themselves.Maybe this is just personal preference. I knew capable students who were wrote and prescriptive in their approach to the courses, but I was closer to those who played in the conceptual end of the pool.I once made a resource like this site for my own educational benefit when I was grappling with MR physics. You're right - I had to do the math(s)! - and came away with a much clearer understanding of the subject. Still, I received lots of correspondence from students and professors who found the visual aid helpful on their journey.reply"
    ],
    "link": "https://ciechanow.ski/lights-and-shadows/",
    "first_paragraph": "It\u2019s hard to describe how paramount light is. Ultimately, it is the only thing we see. But just as important the presence of light is, so is its absence. To talk about light we have to start in darkness so let\u2019s jump straight into it.Light is a visible portion of electromagnetic radiation, but in this article I\u2019m not going to discuss any of the underlying details like wave-particle duality. Instead, I\u2019ll try to explain how light creates so many beautiful effects seen in everyday life. In the demonstration below you can use the sliders to control the position and size of a rectangular light source. You can also drag around the scene to see it from different angles:By the end of this article the effects of light in this demonstration should become more clear, but before we get there we have to take a few steps back and start with a much simpler setup.Let\u2019s begin by introducing a single spherical light source. While this ball of light is not very exciting, you can at least control its bri"
  },
  {
    "title": "Digital Red Queen: Adversarial Program Evolution in Core War with LLMs (sakana.ai)",
    "points": 98,
    "submitter": "hardmaru",
    "submit_time": "2026-01-08T16:16:43 1767889003",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=46542761",
    "comments": [
      "What a lovely period of time that was\u2014when \"Computer Recreations\" ran monthly in Scientific American. I read the column every month and was fascinated to learn about Eliza, Core Wars, Conway's Life, Wa-Tor, etc. It was a time when you coded simply for the fun of it\u2014to explore, learn.I know you can still do that today, but\u2026 something has changed. I don't know what it is. (Maybe I changed.)Anyway, I was unable to track down PDF versions of the original articles, but, for the curious and newcomers to Core Wars, they're transcribed here:https://corewar.co.uk/dewdney/reply",
      "Using evolution in the context of Core War is not a new idea by far, it is even referenced in the paper.Examples here: https://corewar.co.uk/evolving.htmThe difference here is that instead of using a typical genetic algorithm written in a programming language, it uses LLM prompts to do the same thing.I wonder if the authors tried some of the existing \"evolvers\" to compare to what the LLM gave out.reply",
      "Oh man, that's funny to see one of my grad school class projects in that list. Takes me back. :-)From that experience: The LLM is likely to do drastically better. Most of the prior work, mine included, took a genetic algorithm approach, but an LLM is more likely to make coherent multi-instruction modifications.It's a shame they didn't compare against some of the standard core wars benchmarks as a way to facilitate comparisons to prior work, though. Makes it hard to say that they're better for sure.  https://corewar.co.uk/bench.htmreply",
      "I'm not sure if that will hold up. The LLM is not going to do anything random and that is actually a powerful component that makes original output possible.reply",
      "That in turn makes me wonder:Given fixed opposition, finding a warrior that performs the best is an optimization problem. Maybe, for very small core sizes like a nano core, it would be possible to find the optimum directly by SAT or SMT instead of using evolution? Or would it be impractical even for those core sizes?reply",
      "See also:https://en.wikipedia.org/wiki/Tierra_(computer_simulation)https://avida-ed.msu.eduhttps://github.com/adamierymenko/nanopondLots of evolving bug corewar-style systems around.I think the interesting thing with this one is they're having LLMs create evolving agents instead of blind evolution or some similar ML system.reply",
      "How does the output fare on competitive hills like https://sal.discontinuity.info/hill.php?key=94t ?AFAIK, the best results so far for fully computer-generated warriors have been on the nano and tiny format (https://sal.discontinuity.info/hill.php?key=nano, https://sal.discontinuity.info/hill.php?key=tiny), with much shorter warriors (at most 5 or 20 instructions).reply",
      "Hi HN,I am one of the authors from Sakana AI and MIT. We just released this paper where we hooked up LLMs to the classic 1984 programming game Core War. For those who haven't played it, Core War involves writing assembly programs in a language called Redcode that battle for control of a virtual computer's memory. You win by crashing the opponent's process while keeping yours running. It is a Turing-complete environment where code and data share the same address space, which leads to some very chaotic self-modifying code dynamics.We did not just ask the model to write winning code from scratch. Instead, we treated the LLM as a mutation operator within a quality-diversity algorithm called MAP-Elites. The system runs an adversarial evolutionary loop where new warriors are continually evolved to defeat the champions of all previous rounds. We call this Digital Red Queen because it mimics the biological hypothesis that species must continually adapt just to survive against changing competitors.The most interesting result for us was observing convergent evolution. We ran independent experiments starting from completely different random seeds, yet the populations consistently gravitated toward similar behavioral phenotypes, specifically regarding memory coverage and thread spawning. It mirrors how biological species independently evolve similar traits like eyes to solve similar problems. We also found that this training loop produced generalist warriors that were robust even against human-written strategies they had never encountered during training.We think Core War is an under-utilized sandbox for studying these kinds of adversarial dynamics. It lets us simulate how automated systems might eventually compete for computational resources in the real world, but in a totally isolated environment. The simulation code and the prompts we used are open source on GitHub.Other info other than the blog link:Paper (website): https://pub.sakana.ai/drq/Arxiv: https://arxiv.org/abs/2601.03335Code: https://github.com/SakanaAI/drqreply",
      "Very interesting paper, thank you. It makes me wonder what other game substrates could form the basis for adversarial/evolutionary strategy optimization for LLMs, and whether these observations replicate across games.Since LLMs are text based, a text-based game might be interesting. Something like Nomic?Or a \"meme warfare\" game where each agent tries to prompt-inject its adversaries into saying a forbidden codeword, and can modify its own system prompt to attempt to prevent that from happening to itself.reply",
      "> adversarial evolutionary loop where new warriors are continually evolved to defeat the champions of all previous rounds.Interesting. So you're including past generation champions in the \"fights\"? That would intuitively model a different kind of evolution than just \"current factors\"-driven evolution.> We also found that this training loop produced generalist warriors that were robust even against human-written strategies they had never encountered during training.Nice. Curious, did you do any ablations for the \"all previous champions\" vs. \"current gen champions\"?reply"
    ],
    "link": "https://sakana.ai/drq/",
    "first_paragraph": "Survival of the Fittest Code. In the game Core War, assembly-like programs called \u201cwarriors\u201d fight for control of a virtual computer. Warriors may employ sophisticated strategies including targeted self-replication, data bombing, and massive multithreading, in order to crash other programs, and dominate the machine. Top: We visualize battles between assembly programs (\u201cwarriors\u201d) discovered by our Digital Red Queen (DRQ) algorithm. Each DRQ round introduces one additional warrior into the multi-agent simulation. Bottom: With more rounds, the LLM-driven evolution discovers increasingly robust strategies. By simulating these adversarial dynamics, we observe emergent behaviors that mirror biological evolution, where agents must constantly adapt simply to survive against ever-changing threats. Furthermore, as Core War is a Turing-complete environment where code and data share the same address space, this process leads to some very chaotic self-modifying code dynamics.\n\nCore War is a compet"
  }
]