[
  {
    "title": "Judge dismisses majority of GitHub Copilot copyright claims (lwn.net)",
    "points": 94,
    "submitter": "todsacerdoti",
    "submit_time": "2024-08-28T23:44:38",
    "num_comments": 65,
    "comments_url": "https://news.ycombinator.com/item?id=41385637",
    "comments": [
      "The original reporting has more details: https://www.developer-tech.com/news/judge-dismisses-majority...In particular this:An amended version of the complaint had taken issue with GitHub\u2019s duplication detection filter, which allows users to \u201cdetect and suppress\u201d Copilot suggestions matching public code on GitHub.The developers argued that turning off this filter would \u201creceive identical code\u201d and cited a study showing how AI models can \u201cmemorise\u201d and reproduce parts of their training data, potentially including copyrighted code.However, Judge Tigar found these arguments unconvincing. He determined that the code allegedly copied by GitHub was not sufficiently similar to the developers\u2019 original work. The judge also noted that the cited study itself mentions that GitHub Copilot \u201crarely emits memorised code in benign situations.\u201dI think this is the key point: reproduction is the issue, not training. And as noted in the study[1] reproduction doesn't usually happen unless you go to  extra lengths to make it.[1] Not sure but maybe https://dl.acm.org/doi/abs/10.1145/3597503.3639133? Can anyone find the filing?\n \nreply",
      ">  reproduction doesn't usually happen unless you go to extra lengths to make it.And who is to say that people who want to copy your code without adhering to your license terms or pay won't go to extra lengths? or am I missing something here?\n \nreply",
      "At that point, they might as well copy your original code without going through an LLM to do it\n \nreply",
      "Wouldn\u2019t the person themselves be in violation at that point and the owner of the code could go after them? (I know this wouldn\u2019t be super practical but it seems to match what would have without an LLM in between).\n \nreply",
      "Well then you end up with a work that a judge will allow to proceed to trial.Only expressive software is protected by copyright and sometimes that interpretation should be handled by a jury.\n \nreply",
      "It\u2019s trained on publicly available code, so what would be the point of that? If you\u2019re looking to specifically infringe the copyright of code available on the open web, using an LLM code completion engine is just about the most roundabout and unreliable way to achieve that.\n \nreply",
      "IANAL but I think for the specific dismissed claims in this specific case, reproduction is the issue, and it doesn't indicate anything about training.I think it would be extremely hard to make claims against GitHub for training AI with code on GitHub, assuming GH has the typical \"right to use data to improve service\" clause that usually shows up on free-service EULAs.\n \nreply",
      "> I think this is the key point: reproduction is the issue, not training. And as noted in the study[1] reproduction doesn't usually happen unless you go to extra lengths to make it.But Microsoft is selling a service capable of such reproduction. They're selling access to an archive containing copyrighted code.To me it's the equivalent of selling someone a DVD set of pirated movies. The DVD set doesn't \"reproduce\" the copyrighted material unless you \"prompt\" it to (by looking through the set to find the movie, and then putting it in your DVD player), but it was already there to begin with.\n \nreply",
      "Strongly disagree with your analogy here. Lots of services are capable of doing things that are against the law but in general it's the actual breaking of the law that is prosecuted.The closest thing to what you are suggesting is the Napster ruling, where a critical part was that the whole service was only about copyright infringement. In the Github case most people are using it to write original code which is not a copyright violation so there is substantial non-infringing use.But what I think doesn't matter. The judge disagreed with that interpretation too.\n \nreply",
      "There's no reason to link to LWN's one-sentence summary instead of the direct source: https://www.developer-tech.com/news/judge-dismisses-majority...\n \nreply"
    ],
    "link": "https://lwn.net/Articles/987524/",
    "first_paragraph": "\n\n\n\tJudge Jon Tigar's ruling, unsealed last week, leaves only two\n\tclaims standing: one accusing the companies of an open-source\n\tlicense violation and another alleging breach of contract. This\n\tdecision marks a substantial setback for the developers who argued\n\tthat GitHub Copilot, which uses OpenAI's technology and is owned by\n\tMicrosoft, unlawfully trained on their work.\n\n\n\n to post comments\n            \n\n\n\n\n            Copyright \u00a9 2024, Eklektix, Inc.\n            \n            Comments and public postings are copyrighted by their creators.\n            Linux  is a registered trademark of Linus Torvalds\n\n"
  },
  {
    "title": "Twenty Years of Valgrind (2022) (nnethercote.github.io)",
    "points": 154,
    "submitter": "fanf2",
    "submit_time": "2024-08-28T20:42:03",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=41384118",
    "comments": [
      "I like Valgrind, but day-to-day, I find myself typically reaching for Sanitizers[1] instead (ASan, et. al.), especially since they're built-in to many compilers these days, and are a bit faster IME.Are there any use cases that people here have experienced where Valgrind is their first choice?[1] https://github.com/google/sanitizers\n \nreply",
      "same here, also for embedded systems valgrind is too heavy to run natively\n \nreply",
      "In a complex C codebase, valgrind is absolutely indispensable for finding the last few bits of memory that may have leaked out because you got lazy and didn't free() or maybe you wrote somewhere in memory you shouldn't have, it really is like magic sometimes. I know that more recent languages (and some not so recent, looking at you, Ada!) have a lot of this built in by default, but when you need to do these things, you know you need to and it's nice to have a handy all-in-one tool to help out. It's saved me many times from myself when I was writing network code in C and I couldn't figure out where the leaks were coming from, or a performance regression by inspecting the code alone. A quick run of valgrind had that fixed in minutes.\n \nreply",
      "A great tool, I hope these fortune 500 Companies that are involve with Linux are supporting the Valgrind Developers.In reality I really doubt they are.\n \nreply",
      "Fortune 500\u2019s rarely contribute but use a lot. I work for one. It\u2019s usually a tussle between which vendor has managed to convince c-suite that software engineering is a dying discipline and their new genai tool is the utopia.Most Fortune 500 c suite are bean counters with abysmal engineering or product know how. They can\u2019t see past the next quarter earnings report. I doubt long term contribution to meaningful open source is on their list.\n \nreply",
      "One thing I've never understood regarding valgrind - other than intentional leaks, are suppression files ever actually used due to an actual false positive, or is it just due to the bug being in someone else's code that's annoying to fix?\"We implement our own memory allocator\" is no excuse; the primitives you use should be hooked by valgrind so at most there should be false negatives due to your allocations being larger than the user-facing ones ...\n \nreply",
      "> are suppression files ever actually used due to an actual false positive,There used to be one in LuaJIT because it had an optimized string comparison that compared outside of the allocation (which is allowed by the OS as long as you don't cross a page boundary, which LuaJIT's allocation algorithm made sure it never did)The suppression was removed in https://github.com/LuaJIT/LuaJIT/commit/ff34b48ddd6f2b3bdd26... when the string hashing got a new implementation\n \nreply",
      "First and foremost, suppressions can be used for more than leaks. For example, for \"Conditional jump or move depends on uninitialised value(s)\", which yes, there are very much false positives for because of e.g. tricky optimizations LLVM performs and that valgrind can't handle.But even for leaks, you can also have intentional leaks that valgrind will flag but that you can't really do anything about. One example is how using `putenv` can lead to you having to leak memory on purpose. There are many other cases.\n \nreply",
      "Maybe it's just me, but I think people tend to drive by memory leaks nowadays because the footprint of binaries has grown, but the size of ram has grown more-erly. Proper big.I routinely run on 1TB memory 128 core racks, and I don't worry about free() much.I'm not humblebragging, I actually think this is lazy (!) and I would benefit from more explicitly thinking about the memory consequences of what I do but there are some things which I used to freak out about growing to GB and now, I regard it as an investment on the future me, running the same thing: It's very likely I've got it in a hash structure of some kind already. I just add columns to the dict() elements.Down the other end, I recall some friends getting code which I expected to have to run on a major rack host to build onto a small memory model rPi and they said rust did that: allowed them to get rid of the overhang of other languages expectations to runtime size and be explicit about use and free in the heap.\n \nreply",
      "I get it. And I do wish some systems were quicker to let the OS clean up after them on exit. For instance, the other day loaded a CSV with about 20 columns and 10M rows into a list of Python tuples so I could poke at them a bit. That took a short while and I was ok with that. I was more surprised when I closed the REPL and it hung for way longer than expected as it freed a couple hundred million objects.Now it could\u2019ve been the case that some of those objects had a __del__ method that needed to be called or something. Absent that case, I\u2019d have preferred the process just exited and been done with it.If that were a program that ran synchronously from a shell script, the shutdown GC time would\u2019ve been nearly as long as the data loading time. Maybe Python could benefit from a fast_shutdown_GC function that only calls free() if an object is something with a non-trivial delete method. Otherwise, skip it and let the OS do its voodoo it does so well.I\u2019m picking on Python here because that\u2019s where I last saw this. The basic idea applies in lots of other cases though.\n \nreply"
    ],
    "link": "https://nnethercote.github.io/2022/07/27/twenty-years-of-valgrind.html",
    "first_paragraph": "\nJul 27, 2022\n      It has been twenty years since Valgrind 1.0 was released.The Valgrind website says:Valgrind is an instrumentation framework for building dynamic analysis tools.\nThere are Valgrind tools that can automatically detect many memory management\nand threading bugs, and profile your programs in detail. You can also use\nValgrind to build new tools.\u2013I first met Julian Seward in late 2001. I had moved from Australia to Cambridge\nin the UK to pursue a PhD on the topic of \u201ccache optimizations for functional\nlanguages\u201d. The Cambridge Computer Laboratory is literally next door to a\nMicrosoft Research office, and I was soon interacting with the people there\nworking on the Glasgow Haskell Compiler. Julian was one of them.Shortly after that, Julian\u2019s stint working on GHC came to a close. On his last\nday he dropped by my office in the Computer Laboratory to say goodbye. I asked\nwhat he would be doing now, and he said he was going to spend some time on a\nproject of his called Valgrind."
  },
  {
    "title": "Maker Skill Trees (github.com/sjpiper145)",
    "points": 202,
    "submitter": "saulpw",
    "submit_time": "2024-08-28T17:01:15",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=41381598",
    "comments": [
      "I thought the \"House Building\" one would include things like wiring, plumbing, drywall, mudding & taping, texturing, painting, finish carpentry, framing/formbuilding(rough carpentry), concrete, flooring, and so on. Instead it's more about doing feasability and stuff. The Renovation & Repair one lumps a lot of discrete skill sets together and maybe leaves a lot of stuff up to \"experts.\"A lot of these \"skill sets\" have fractal complexity, where if you dig in on \"Concrete work\" you'll find yourself going down a rabbit hole of form building, hydrostatic pressure, foundation squaring, and so on. Even pouring an unreinforced slab for a patio requires some distinct skills. Plumbing is the same way where being able to replace a water heater could devolve into sweating new fittings.Is the intent here to document what you could do without knowing what the residential code is and how to pull permits in your area?\n \nreply",
      "I would assume that you are not a carpenter: As a carpenter, I would include: Knowledge of Universal Building Codes, knowledge of local building codes, Local code checklists, Blue print reading, Site survey reading, and first hand knowledge of your local building department. (AHJ)Concrete forms, soil management, concrete testing, that is more than a rabbit hole, its a whole immense specialty. ( Ask Richard Sloan, who literally taught me how to finish concrete like glass... over very large areas. )Tiling is deep also. ( Thanks Alejandro ).Even after more than 10 years work, texturing as a skill is still beyond me, and like far beyond me.Your point is well taken, but...This is an amazingly LARGE amount of work.Environmental skill?I cannot wait to see this one:I should make a\nBuilding a Desktop PC,\nCoaching, Transform your life. \nTruck Driving and Taking no sh*.\n \nreply",
      ">Tiling is deep [...] texturing is a skill beyond me.Interesting.  Sounds like you have way more experience than me, but I feel pretty comfortable doing tiling and do a decent job, after doing a kitchen floor, backsplash, bathroom floor, and shower.  The biggest risk with doing tiling is once you learn how to do it, you'll start noticing bad tiling jobs everywhere.  Particularly lippage.I've also done decent jobs of texturing the couple attempts I've done.  I don't love them, if I stare at them I think they look terrible but if I'm walking past I don't notice them.  What in car racing they'd call a 50/50 paint job: looks good from 50 feet when it's going 50MPH.  :-)  But, what I'm starting to do, and what I did on my bathroom remodel, is doing a level 5 finish.  It took me a while to get to where I was happy, but not having to texture and instead just painting the untextured wall came out pretty nice.  I guess it's more common in \"the west\" to texture and level 5 in \"the east\" (USA).\n \nreply",
      ">what you could do without knowing what the residential code is and how to pull permits in your area?Don't let permits stop you.  I'm sure it varies from location to location, but our city permit office is crazy helpful to DIYers, and it makes sense: making it easy for DIYers to get permits and inspections is a huge win for safety in the long run.I love having the \"backstop\" of an inspection to find any problems I have, and the inspectors have all been extremely nice.  And for my tub drain, it was in a confined area that was really hard to work the necessary pieces in, the general inspector called in a plumbing inspector who called in a master plumber before we could get it worked out so that I didn't end up with an S-trap.\n \nreply",
      "Literally three people to do a tub waste, wow, btw just about every tub waste is in a confined area where it's hard to work the necessary pieces in. That is the nature of tub wastes.\n \nreply",
      "The Housbuilding one is like1. half \u201cyou build it yourself\u201d, and2. half \u201chave someone else build it but you make the plan\u201dI don\u2019t get who the audience for it is.You\u2019re supposed to start at the bottom too but the order is kind of all over the place.Link to the house building one for anyone interested:\nhttps://github.com/sjpiper145/MakerSkillTree/blob/main/House...\n \nreply",
      "This is a brilliant idea but execution leaves a lot to be desired. I get that there's a lot of judgement calls going on here but I really wish this would become a popular project with lots of subject experts to weigh in.Looking at the PCB design skill tree, it just doesn't look very realistic. \"Use an Autorouting tool,\" for example, is second after \"Learn PCB Software\" when it should be in the top half of the tree (if not in the top few rows).\"Design an SMD PCB\" is on the same horizontal line as \"Hand solder SMD Parts\", as is \"Learn to Read a Schematic\" and \"Learn PCB Software\"(?!) Learning the PCB design software is a process that must run in parallel with most of the skill tree.\"Use a reflow oven to solder a PCB\" is two who levels above \"Use a pick & place machine\" and so on. I get that a lot of this is path dependent on experience but \"Use SMD tweezers\" should probably go alongside \"Solder SMD parts\"...\n \nreply",
      "If you contribute your expertise to improve the skill tree you can even get a cool sticker.https://github.com/sjpiper145/MakerSkillTree?tab=readme-ov-f...\n \nreply",
      "So much hate here in these comments and threads. I know for myself, sometimes it's good to even have an idea of small/medium/large sized projects / ideas to run with to consider as an \"experience\" in doing a new skill I've never worked in before.It looks like these are an attempt to create units of work that are approachable and individually researchable to complete.I think it's pretty great!\n \nreply",
      "The great thing about open source is that it enables all the people with the time to post negative comments here to address the shortcomings that their keen eyes have spotted...right?\n \nreply"
    ],
    "link": "https://github.com/sjpiper145/MakerSkillTree",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A repository of Maker Skill Trees and templates to make your own. \n      Maker Skill Trees are printable templates that can guide and track hands-on skill progression.  Colour in the boxes as you go and get inspired to try new things.  This repository is always growing and aims to include a large variety of skills, see full list below.   Everyone's journey is different and you can interpret the goals flexibly.  Not everything needs to be completed.  Print your own in A4 or A3 size.Each skill tree has 73 skill or experience hexagonal tiles, ordered in a spectrum of basic skills at the bottom to more advanced skills at the top.  This includes 5 set your own goal tiles for you to tailor to your own interests. Under the templates section, there's also a smaller Mini Skill Tree template with only 40 tiles.  Maker Skill Trees are released"
  },
  {
    "title": "Window Maker: X11 window manager with the look and feel of the NeXTSTEP UI (windowmaker.org)",
    "points": 147,
    "submitter": "lnyan",
    "submit_time": "2024-08-28T18:05:29",
    "num_comments": 80,
    "comments_url": "https://news.ycombinator.com/item?id=41382335",
    "comments": [
      "Unlike many here, I never grokked or really liked WindowMaker.The biggest thing was how the dock icons were occupying too much precious screen space, and were distracting. Especially in the Olden Days when monitor ratios were 4:3. In contrast with a 16:9 or 16:10 modern monitor, where you can put things on the side and they'll be more out of focus.Animated/informational dock icons, to show system stats, were more of a distraction than were worth it, and if you reduced the size of the dock icon then they became worthless. Couldn't have it both ways.I did not like how big a dock icon was when you minimized a window. This was long after Windows 95 showed a thinner taskbar was perfectly sufficient.I did not like the context menus that stuck around. Muscle memory, perhaps, but again: a distraction.I did love its columnar file/tree navigator, which macOS continues to provide in Finder, although it's not a default!All in all, it felt like an interesting exploration of the computer desktop UX space, but a dead end.\n \nreply",
      "Personally, i'd wish Window Maker wouldn't fall for pointless feature creep (in-built screenshot feature!?) and would instead replace WINGs with the GNUstep framework.GNUstep has silently matured over the years but still lacks a real native window manager. Window Maker once aimed to be that but unfortunately didn't ever manage to fully integrate with GNUstep.Fully porting Window Maker to GNUstep would be a Win-Win situation for all involved parties: GNUstep already features Wayland support and also offers a theming capabality for which WINGs' hardcoded and thus unchangeable NeXTSTEP aestetics are no match. So replacing WINGs with the GNUstep framework would instantly provide Wayland and more advanced theming support, for free.People interested in an integrated Window Maker centric system based on Debian/Bookworm should have a look at https://wmlive.sourceforge.net  and https://sourceforge.net/projects/wmlive/files/ for downloads.\n \nreply",
      "Concur.On the other hand, I had high hopes for Etoile, which had the goal of being a modern GNUstep-based user environment: http://etoileos.com/etoile/\n \nreply",
      "Interesting! I used to use WM as a desktop, and while it wasn't bloated feeling, it did feel like there were rough edges. GNUStep looked interesting, but I guess I never gave it much thought, or hit a roadblock with it.If both are really two sides to the same coin, it would be nice if they merged. I obviously don't have the full details, though.\n \nreply",
      "What a blast from the past. WindowMaker served me well for many years, mainly on small screens. But once I started using multi-monitor setups, I switched to i3.One thing: WindowMaker is easy to use yet full of options to customize the appearance and behaviour of windows, per-application and per-window.Yet I think its killer feature for many years was the huge (64x64 pixel) \"dock apps\". There, you could put widgets with a ton of nice functionality, such as WiFi status, mailbox, disk monitoring \u2014 or just a clock.\nI don't remember if NeXTStep/WM were the first to offer those widgets, but I remember being a fond user of them.\n \nreply",
      "I might be wrong, but i do not think NeXTStep had dockapps, i think the idea comes from older WMs which allowed X11 windows to be \"swallowed\" inside panels and Window Maker provided the same functionality in a NeXTStep-ish way (dockapps are just small windows nested inside a frame box around them).Actually at some point in recent years i used NeXTStep and having being using Window Maker for many years by that point, i felt a bit of an uncanny valley effect: what i was looking at the screen felt very familiar but still somewhat \"wrong\" and a ton of what i was used to didn't work (or worked in a different way) :-P. It made me realize what the people around the mid-2000s who tried various Aqua-like themes on GNOME 2, etc and said that it looks off/wrong were feeling like :-P.EDIT: also i think Window Maker is not the only window manager that supports/uses dockapps. I think FVWM and Afterstep can also use the same dockapps.\n \nreply",
      "> I might be wrong, but i do not think NeXTStep had dockapps, i think the idea comes from older WMs which allowed X11 windows to be \"swallowed\" inside panels and Window Maker provided the same functionality in a NeXTStep-ish way (dockapps are just small windows nested inside a frame box around them).According to my computer history knowledge, NeXTSTEP version 0.8 introduced the dock in 1988.At the launch, Steve Jobs said: \u201cThe dock \u2014 It turns out that, when you\u2019re running applications, things can get lost. These icons can get hidden and you might want to read your mail at a moment\u2019s notice. So we allow you to take any icon and take it over to any one of these dock positions and it\u2019ll snap in and dock. And the minute it docks, nothing can go in front of it. And so it\u2019s a place to always have the applications that you use handy. You can customize it any way you want to, and nothing will ever keep these things from being a glance away. That\u2019s what the dock\u2019s all about. And if you decide that you need to use that right part of the screen for an awfully big window, and you don\u2019t want to undock things, you can just slide it down and everything, but the little logo will go off the screen.\u201dhttps://youtu.be/92NNyd3m79I?si=PjcfOEhf2-S-QzB1\n \nreply",
      "Fluxbox also has first class support for Dockapps. They're a great pairing.An old screengrab to prove my point: https://www.thran.uk/img/desktop-aug-21.jpg. I'm especially fond of the VU-meter style ethernet traffic monitor, 'wmnd'.Find dockapps here: https://www.dockapps.net/\n \nreply",
      "Fluxbox is awesome.I used to use it on hardware-limited machines at first, then started to prefer it for its minimalism.\n \nreply",
      "That screenshot makes me want to use Fluxbox, it looks great!\n \nreply"
    ],
    "link": "https://www.windowmaker.org/",
    "first_paragraph": ""
  },
  {
    "title": "The Future of TLA+ [pdf] (lamport.azurewebsites.net)",
    "points": 128,
    "submitter": "tkhattra",
    "submit_time": "2024-08-28T18:46:50",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=41382828",
    "comments": [
      "I was looking at TLA a few months ago to consider what it would take to prove multiregion fail over worked correctly. Considering I'd never looked at it before.I did not find it straight forwardly grokkable, which makes me sad. Maybe it needs a library of axioms? I feel there's probably a very nice way to work through it without ingesting effectively a graduate school course in proving software.It really is just math and proofs, it shouldn't be so hard... to start.Well, that's my take. Could be wrong. Might just need to hit the books.\n \nreply",
      "I may have been spending too much time with Lean recently, but the number one thing I\u2019d like to see for the future of TLA+ is an equivalent of Mathlib (https://github.com/leanprover-community/mathlib4). What\u2019s so great about the experience of using Lean is that I can pull theorems off the shelf from Mathlib, use them if I want to, or learn from the way their proofs work if I want to do something similar.> The reason for using TLA+ is that it isn\u2019t a programming language; it\u2019s mathematics.I love TLA+, I\u2019ve used it for a decade and reach for it often. I have a huge amount of respect for Leslie Lamport and Chris Newcombe. But I think they\u2019re missing something major here. The sematics of TLA+ are, in my mind, a great set of choices for a whole wide range of systems work. The syntax, on the other hand, is fairly obscure and complex, and makes it harder to learn the language (and, in particular, translate other ways of expressing mathematics into TLA+).I would love to see somebody who thinks deeply about PL syntax to make another language with the same semantics as TLA+, the same goals of looking like mathematics, but more familiar syntax. I don\u2019t know what that would look like, but I\u2019d love to see it.It seems like with the right library (see my mathlib point) and syntax, writing a TLA+ program should be no harder than writing a P program for the same behavior, but that\u2019s not where we are right now.> The errors [types] catch are almost always quickly found by model checking.This hasn\u2019t been my experience, and in fact a lot of the TLA+ programs I see contain partial implementations of arbitrary type checkers. I don\u2019t think TLA+ needs a type system like Coq\u2019s or Lean\u2019s or Haskell\u2019s, but I do think that some level of type enforcement would help avoid whole classes of common specification bugs (or even auto-generation of a type checking specification, which may be the way to go).> [A Coq-like type system] would put TLA+ beyond the ability of so many potential users that no proposal to add them should be taken seriously.I do think this is right, though.> This may turn out to be unnecessary if provers become smarter, which should be possible with the use of AI.Almost definitely will. This just seems like a no-brainer to bet on at this stage. See AlphaProof, moogle.ai, and many other similar examples.> A Unicode representation that can be automatically converted to the ascii version is the best alternative for now.Yes, please! Lean has a unicode representation, along with a nice UI for adding the Unicode operators in VSCode, and it\u2019s awesome. The ASCII encoding is still something I trip over in TLA+, even after a decade of using it.\n \nreply",
      "> I would love to see somebody who thinks deeply about PL syntax to make another language with the same semantics as TLA+Perhaps you would find Quint interesting? https://news.ycombinator.com/item?id=41111790There's a comment that says Quint uses TLA+ as its base language: https://news.ycombinator.com/item?id=41118162Disclaimer: I don't know anything about TLA+ or Quint, I just remembered seeing Quint here\n \nreply",
      "> The [\\EE] operator is needed to explain the theory underlying how\nTLA+ is used.There's another reason to potentially support \\EE: it's needed to refine specs with auxiliary variables. Currently, if an abstract spec has `aux_hist` to prove a property or something, you need the refinement to have an `aux_hist` equivalent, even if it doesn't affect the spec behavior at all. But if checkers could handle `\\EE` you could instead leave it out of the refinement and check `\\EE aux_hist: Abstract(aux_hist)!Spec`.I think /u/pron once told me that actually checking a property of that form is 2-EXPTIME complete, though. Which is why it's not supported in practice.\n \nreply",
      "I'm really not a fan of TLA+'s tooling, but I do really love the temporal logic. I've always kinda wanted that stuff in other proving languages, but I don't know how possible it is.Would it be actually possible to write something like an \"a la carte temporal logic library\" for other proving languages that could get you some of the confidence you can get from TLA+'s modeling?(Aside: I have a TLA+ book, but it's notably missing really much in terms of exercises or anything. If anyone has any recommendations for a large set of exercises to play around in the space I'd love to hear about it!)EDIT: turns out just searching for \"temporal logic in X language\" gets you papers, found this one paper for axiomatizing temporal logic that seems to be a good starting point for anyone looking at this [0][0]: https://lim.univ-reunion.fr/staff/fred/Enseignement/Verif-M2...\n \nreply",
      "> Would it be actually possible to write something like an \"a la carte temporal logic library\" for other proving languages that could get you some of the confidence you can get from TLA+'s modeling?Temporal logic is just a specific instance of a modal logic, which can be modeled with reasonable ease using a \"possible worlds\"-based encoding.  Note that TLA+ combines temporal logic with non-determinism, which is a different modality.\n \nreply",
      "What LaTeX package does one use to get the \"back\" link at the end of footnotes like the linked PDF exhibits?\n \nreply",
      "Yes, that's an interesting implementation. I'm using Firefox, and the jumps to the notes and back to the paragraph are recorded in history, and has the expected effect when clicking the back and forward history arrows/buttons.\n \nreply",
      "Whoa, I use TLA ironically to joke about Three Letter Acronyms, I had no idea that the Three Letter Acronym (TLA) was in any way related to Temporal Logic Actually. Fascinating!\n \nreply",
      "A TLA+ alternative people might find curious.https://quint-lang.org/\n \nreply"
    ],
    "link": "https://lamport.azurewebsites.net/tla/future.pdf",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Skip \u2013 Build native iOS and Android apps from a single Swift codebase (skip.tools)",
    "points": 109,
    "submitter": "marcprux",
    "submit_time": "2024-08-28T20:44:29",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=41384144",
    "comments": [
      "Pricing link: https://skip.tools/pricing/\n \nreply",
      "Loving this so far, I've been working with it for a week now. I have a personal app, DownPay for iOS, that is built with SwiftUI that I want to bring to Android. I tried going the React Native route to build an android-only version but the context switching between SwiftUI, React Native, and then my day job made it challenging. I also tried Ionic and Ignite and wasn't successful with those either.After testing the feasibility of other cross-platform frameworks, I landed on Skip. I LOVE that I don't have to break out of the \"Swift\" mental context, I just have to focus on writing an app in 1 language.So before I commit fully I've been testing it out (building a demo app this week) and so far I am very impressed. The syntax to write platform-specific code (#if !SKIP #endif) is very easy to use once you get the hang of it.It's amazing I don't have to learn Android to get something up and running at this speed with Skip. Hitting run in xcode and watching both emulators open feels like magic. I want to put this to the test so I plan to build a complete App with it from start to finish, ship it to both App Stores, and if all that goes smoothly I will proceed to migrate my main app using Skip.\n \nreply",
      "Good to see Skip on the home page! We were evaluating Skip just a couple weeks ago for a side project.The issue we ran into is that we've already built a native iOS app with SwiftUI + a bit of UIKit. Integrating Skip with an existing app seemed like a significant taskDoes that hold true in your experiences? Do you have any examples of small- or medium-sized existing apps that have migrated to Skip?\n \nreply",
      "Yes, the docs even say that integrating into an existing app is way harder than starting with a new one!https://skip.tools/docs/gettingstarted/#existing_development\n \nreply",
      "Have not read the docs, but what\u2019s the reason for that?Is it not just a transpiler, since then it should work at any stage. No?\n \nreply",
      "It\u2019s missing things that are harder to work around if you already have code relying on those missing bits or done in an architecture it can\u2019t transpile\n \nreply",
      "Can vouch- been using this tool the last couple months and it's been magic. It's new so there is definitely a learning curve, but once you get things working it solves the cross-platform problem completely.\n \nreply",
      "Damn, this sounds too good to be true. Really nothing to add here except than keep pushing!This fixes the big painpoint that nowadays' cross-platform frameworks come with performance tradeoffs as they have a unified presentation layer!\n \nreply",
      "Looks very cool!Looking at the docs gives a good overview of how it works.Regarding transpilation and the tradeoffs (https://skip.tools/topic/transpilation-tradeoffs/), does the limitation of certain Swift features cause any significant friction with using parts of SwiftUI or other core libraries?Wondering how much those (understandable) limitations on the transpilation limit what a random iOS dev might be able to do compared to what they can do in iOS land.Also, using SwiftUI cross-platform makes me think that many android libraries would be a no-go.One of the reasons that Xamarin development was painful (other than the numerous bugs in trying to target 2 foreign platforms) was that you couldn't _really_ utilize the large native ecosystems of either platform, and you would end up spending a lot of time \"rewriting\" libraries in dotnet.\n \nreply",
      "Thanks for the positive words! Any limitations in the transpiler (such as some advanced generics) will only be limitations on the Android side \u2013 the iOS side can still do anything that is support in Swift. We discuss this a bit at https://skip.tools/docs/platformcustomization/> Also, using SwiftUI cross-platform makes me think that many android libraries would be a no-go.A unique feature of Skip is that the Kotlin/Android side is free to integrate with whatever gradle libraries it wants (see https://skip.tools/docs/dependencies/). Similarly, the Swift side can have any SwiftPM dependencies it wants.Only your own transpiled modules, and the core Skip modules, will need to support transpilation. You can then include any native dependencies via your app's transpiled code that branches based on which platform/language you are targeting. So, for example, the Swift/SwiftUI side of the project can depend on the SwiftPM \"https://github.com/firebase/firebase-ios-sdk.git\" dependency, and the Kotlin/Compose side can depend on the Gradle \"com.google.firebase:firebase-bom\" dependency. This is what we ourselves do in the various integration modules we have (such as SkipFirebase, for this particular example).\n \nreply"
    ],
    "link": "https://skip.tools/",
    "first_paragraph": "\n  Skip brings Swift app development to Android. It is a tool that enables developers to use a single modern programming language\u00a0(Swift) and first-class development environment\u00a0(Xcode) to build genuinely native apps for both iOS\u00a0and\u00a0Android.\n\nWatch Video\nGet Started\nAs you build your Swift and SwiftUI app in Xcode, the Skip Xcode plugin continuously transpiles it into the equivalent Kotlin and Jetpack Compose for Android. Develop dual-platform libraries or entire apps, with native performance and native user interfaces on both platforms.Want to learn more? Take our video tour or browse the documentation.Eager to try Skip out? Install Skip today!Skip apps don\u2019t just \u201clook native\u201d, they are native: Swift and SwiftUI on iOS, Kotlin and Compose on Android. You know the difference, and so do your users.When you use Skip, no part of your app is hidden from your view\u2026 or from your debugger. Skip transpiles your Swift directly into Kotlin source code that you can inspect and customize.With Sk"
  },
  {
    "title": "Google's New Pipe Syntax in SQL (simonwillison.net)",
    "points": 94,
    "submitter": "heydenberk",
    "submit_time": "2024-08-25T13:33:23",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=41347188",
    "comments": [
      "Is it just me, or does this seem anachronistic? Like, this is a conversation I expected to blow up 20 years ago. Better late than never.\n \nreply",
      "Richard Hipp, creator of SQLite, has implemented this in an experimental branch: https://sqlite.org/forum/forumpost/5f218012b6e1a9dbWorth reading the thread, there are some good insights. It looks like he will be waiting on Postgres to take the initiative on implementing this before it makes it into a release.\n \nreply",
      "That comment where he explains why he's not rushing to add new unproven SQL syntax to SQLite is fascinating:> My goal is to keep SQLite relevant and viable through the year 2050. That's a long time from now. If I knew that standard SQL was not going to change any between now and then, I'd go ahead and make non-standard extensions that allowed for FROM-clause-first queries, as that seems like a useful extension. The problem is that standard SQL will not remain static. Probably some future version of \"standard SQL\" will support some kind of FROM-clause-first query format. I need to ensure that whatever SQLite supports will be compatible with the standard, whenever it drops. And the only way to do that is to support nothing until after the standard appears.\n \nreply",
      "It's so ambitious in an almost boring way, exactly the right steward for a project like this\n \nreply",
      "FROM first would be nothing short of incredible. I can only hope that Postgres and others can find it within themselves to get together and standardize on such an extension!\n \nreply",
      "The next thing I would like is to define a function / macro that has a bunch of |> terms.I pointed out that you can do this with shell:Pipelines Support Vectorized, Point-Free, and Imperative Style https://www.oilshell.org/blog/2017/01/15.htmle.g.    hist() {\n      sort | uniq -c | sort -n -r\n    }\n\n    $ { echo a; echo bb; echo a; } | hist\n      1 bb\n      2 a\n\n    $ foo | hist\n    ...\n   \n\nSomething like that should be possible in SQL!\n \nreply",
      "Previous submissions on the paper itself:https://news.ycombinator.com/item?id=41321876 (first)\nhttps://news.ycombinator.com/item?id=41338877 (plenty of discussions)I tried this new syntax and this seems a reasonable proposal for complex analytical queries. This new syntax probably does not change most simple transactional queries though. The syntax matches the execution semantic more closely, which means you less likely need to formulate query in a weird form to make query planner work as expected; usually users only need to move some pipe operators to more appropriate places.\n \nreply",
      "There was a second submission of the paper, which attracted more comments: https://news.ycombinator.com/item?id=41338877\n \nreply",
      "Thank you, added it to my comment. I missed all the discussions!\n \nreply",
      "Kinda looks like a half-assed version of what PRQL does. Like, if we\u2019re going to have nonstandard sql, let\u2019s just fix a whole bunch of things, not just one or two?\n \nreply"
    ],
    "link": "https://simonwillison.net/2024/Aug/24/pipe-syntax-in-sql/",
    "first_paragraph": "SQL Has Problems. We Can Fix Them: Pipe Syntax In SQL (via) A new paper from Google Research describing custom syntax for analytical SQL queries that has been rolling out inside Google since February, reaching 1,600 \"seven-day-active users\" by August 2024.A key idea is here is to fix one of the biggest usability problems with standard SQL: the order of the clauses in a query. Starting with SELECT instead of FROM has always been confusing, see SQL queries don't start with SELECT by Julia Evans.Here's an example of the new alternative syntax, taken from the Pipe query syntax documentation that was added to Google's open source ZetaSQL project last week.For this SQL query:The Pipe query alternative would look like this:The Google Research paper is released as a two-column PDF. I snarked about this on Hacker News: Google: you are a web company. Please learn to publish your research papers as web pages.This remains a long-standing pet peeve of mine. PDFs like this are horrible to read on mo"
  },
  {
    "title": "Kotlin for Data Analysis (kotlinlang.org)",
    "points": 92,
    "submitter": "saikatsg",
    "submit_time": "2024-08-28T18:24:39",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=41382578",
    "comments": [
      "I do both Kotlin and Python. More Kotlin than Python to be honest. But I'm pragmatic. Python is where all the action is when it comes to data science, llms, and all the rest. So it's the path of the least resistance. And there's a great argument to not challenge that and just do what everybody else does and put your head down and not criticize any of that. Which is why I use it on a few projects. The library ecosystem is great. Etc. Bla bla bla. But the bottom line is that I don't love python. It's dreary to me. Mediocre. I can't get very excited about any of it. It just seems so backwards.And it's not necessarily the most efficient path either. The interpreter is not that fast, the language is not that expressive, what passes for package management is a bad joke, etc. I can work with it but I'm not necessarily loving it.For data engineering, Kotlin has a lot to offer. I wouldn't necessarily recommend it because it's all kind of niche. But it kind of works as well. If you aren't afraid of tinkering with it, there are a lot of other niche solutions out there as well that aren't python.Kotlin is what I reach for when I want to get stuff done in a hurry. Part of that is just my limitation. It's what I know and I kind of grew up on JVM languages. I'm well aware that's not necessarily optimal and that that's just a bias I have. But objectively, it has a lot of nice things over modern python as well.Kotlin is a modern language, it's a lot more expressive than python. It has a a great library ecosystem. Including some stuff that does not depend on the JVM. And even though it's kind of niche for a lot of things I use it for (e.g. developing reactive web frontends), it holds up well and rarely disappoints me.People use python because everybody else uses python. That's it. It's not particularly good at anything it does. But it will get the job done and I can do it. But that just isn't good enough for me. It's the visual basic for data science. And that's not a compliment. Being idiot proof is it's main feature. But that doesn't make it the smart choice.\n \nreply",
      "Kotlin just needs some stubborn people not to use python just because everyone else does, and build up the Kotlin ecosystem. That's how progress is made.\n \nreply",
      "If someone wants to pay me to do this, I have some free time Mondays and Wednesdays.\n \nreply",
      "I'm not sure how someone could see Kotlin as more expressive than Python, unless I am misinterpreting what expressive means. Python has a good language features and helpful abstractions like list comprehensions.What makes Kotlin more expressive? I understand it has some functional features but I've never seen anything dramatically flexible.\n \nreply",
      "Pythons list comprehensions are sort of fun, but occupy that space because the language designers throttled the alternatives pretty hard.I used to write a lot of Python, I now write a lot of Rust, and the Rust iterator chains feel inordinately more powerful, and list comprehensions feel semantically backwards to me now: what you\u2019re doing, what you\u2019re doing it to, and whether to do it conditionally are all out of order.\n \nreply",
      "Kotlin's standard library has ruined me for other languages, especially its collections library. The consistency and comprehensiveness of its approach to collections is unmatched in any language I've tried, including all the big name functional languages. It's hard to get across what's so great about the library in writing because it's not just one standard library function, it's how they all interact with each other and how they interact with the language design\u2014you really just have to try it to understand. The net result is that transforming data from one shape to another flows effortlessly, with the dot operator seamlessly connecting a stream of transformations. The fact that it's the dot operator also means that you get really great autocomplete to help you on your way.Python, meanwhile, has always felt pretty awkward to me when it comes to data transformations. Comprehensions are okay, but they feel like they are special casing what should be a bunch of standardized helper functions operating on lambdas, as a sort of ugly workaround to the fact that Python refuses to implement proper lambdas. And when you can't use a comprehension, you're stuck with a pretty awkward collection of helper methods that are hard to find and use correctly and which are severely handicapped in expressivity by the lack of a proper lambda.\n \nreply",
      "That's interesting. I've heard complaints about Kotlins standard library in comments like this[1]. I understand they may be nitpicks but they seem annoying in practice.[1] https://www.reddit.com/r/Kotlin/comments/mh2z5u/comment/gt2n...\n \nreply",
      "As someone who uses Kotlin for work and Python for side projects (and loved Python years ago in college), Python's list comprehension feature is one of the things I hate the most about the language now.As a simple example using only two collection functions I find it much easier to read  val hundredOrLessEvenSeconds = (1..1000)\n      .toList()\n      .filter { it <= 100 }\n      .filter { it % 2 == 0 }\n      .map { it.seconds }\n\nthan  hundred_or_less_even_seconds = [timedelta(seconds=it) for it in range(1, 1001) if it <= 100 and it % 2 == 0]\n\nBut there are tons of helper functions in the collections library to express that in a variety of different ways. But not in a gross code golf way, with clearly named functionsTheres just so much built in\nhttps://kotlinlang.org/docs/collections-overview.htmlHaving lambdas built into the language from the start leads to a ton of expressibility I miss when using python\n \nreply",
      "Not that I especially want to defend Python, but can you elaborate a bit on why you find that chain easier to read? The Python version is straightforward enough - if it's just the absence of newlines you can write  hundred_or_less_even_seconds = [\n    timedelta(seconds=it)\n    for it in range(1, 1001)\n    if it <= 100 and it % 2 == 0\n  ]\n\nAlso, I don't know Kotlin well enough, but is what you wrote going to be efficient? The Python version iterates once and creates one list (and you can actually turn it into a generator and make zero lists just by swapping the square brackets for parens); to my untrained eye, it looks like the Kotlin version is going to do more iteration and make four separate lists, three of which are just garbage to be thrown away immediately. Here that probably doesn't matter, but in other cases it might be a big problem; is there an easy/idiomatic way to avoid that?\n \nreply",
      "Its a small example so the efficiency doesn't matter, but you could use sequences when it does https://kotlinlang.org/docs/sequences.html .Also the map function lets you perform any operations in it. It was a simple example but you could need to perform something slightly more complex than using another standard library function."
    ],
    "link": "https://kotlinlang.org/docs/data-analysis-overview.html",
    "first_paragraph": "Exploring and analyzing data is something you may not do every day, but it's a crucial skill you need as a software developer.Let's think about software development duties where data analysis is key: analyzing what's actually inside collections when debugging, digging into memory dumps or databases, or receiving JSON files with large amounts of data when working with REST APIs, to mention some.With Kotlin's Exploratory Data Analysis (EDA) tools, such as Kotlin notebooks, Kotlin DataFrame, and Kandy, you have at your disposal a rich set of capabilities to enhance your analytics skills and support you across different scenarios:Load, transform, and visualize data in various formats: with our Kotlin EDA tools, you can perform tasks like filtering, sorting, and aggregating data. Our tools can seamlessly read data right in the IDE from different file formats, including CSV, JSON, and TXT.Kandy, our plotting tool, allows you to create a wide range of charts to visualize and gain insights fro"
  },
  {
    "title": "Show HN: Repo2vec \u2013 an open-source library for chatting with any codebase (github.com/storia-ai)",
    "points": 48,
    "submitter": "nutellalover",
    "submit_time": "2024-08-28T19:59:10",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=41383592",
    "comments": [
      "Very cool project, I'm definitely going to try this out. One question \u2014 why use the OpenAI embeddings API instead of BGE (BERT) or other embeddings model that can be efficiently run client-side? Was there a quality difference or did you just default to using OpenAI embeddings?\n \nreply",
      "OP's cofounder here. For us, OpenAI embeddings worked best. When building a system that has many points of failure, I like to start with the highest quality ones (even if they're expensive / lack privacy) just to get an upper threshold of how good the system can be. Then start replacing pieces one by one and measure how much I'm losing in quality.P.S. I worked on BERT at Google and have PTSD from how much we tried to make it work for retrieval, and it never really did well. Don't have much experience with BGE though.\n \nreply",
      "We ran some qualitative tests and there was a quality difference. In fact, benchmarks show that trend to generally hold: https://archersama.github.io/coir/That being said, our goal was to make the library modular so you can easily add support for whatever embeddings you want. Definitely encourage experimenting for your use-case because even in our tests, we found that trends which hold true in research benchmarks don't always translate to custom use-cases.\n \nreply",
      "> we found that trends which hold true in research benchmarks don't always translate to custom use-cases.Exactly why I asked! If you don't mind a followup question, how were you evaluating embeddings models \u2014 was it mostly just vibes on your own repos, or something more rigorous? Asking because I'm working on something similar and based on what you've shipped, I think I could learn a lot from you!\n \nreply",
      "Any plans on allowing the use of a local LLM like Ollama or LM Studio?\n \nreply",
      "OP's cofounder here. Yes, we started with what we perceived as highest quality (OpenAI embeddings + Claude autocompletions), but will definitely make our way to local/OSS. The code is super modular so hopefully the community will help as well.\n \nreply",
      "Very useful! I was just thinking this kind of thing should exist!I would also like to be able to have the LLM know all of the documentation for any dependencies in the same way.\n \nreply",
      "OP's cofounder here. The nice thing is that a lot of repos include the documentation as well, so it comes for free by simply indexing the repo (like huggingface/transformers for instance).\n \nreply",
      "Thanks!This is a great idea. Definitely something we plan to support.\n \nreply",
      "Sorry for the dumb question but can I use this on private repositories or is it sending my code to OpenAI?\n \nreply"
    ],
    "link": "https://github.com/Storia-AI/repo2vec",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Chat with your codebase with 2 commands\n      TL;DR: repo2vec is a simple-to-use, modular library enabling you to chat with any public or private codebase.Ok, but why chat with a codebase?Sometimes you just want to learn how a codebase works and how to integrate it, without spending hours sifting through\nthe code itself.repo2vec is like GitHub Copilot but with the most up-to-date information about your repo.Features:Here are the two scripts you need to run:This will index your entire codebase in a vector DB, then bring up a gradio app where you can ask questions about it.The assistant responses always include GitHub links to the documents retrieved for each query.If you want to publicly host your chat experience, set --share=true:That's it.Here is, for example, a conversation about the repo Storia-AI/image-eval:\nThe src/index.py scr"
  },
  {
    "title": "Scaling Rails and Postgres to Users at Microsoft: Lessons and Takeaways (stepchange.work)",
    "points": 79,
    "submitter": "htormey",
    "submit_time": "2024-08-28T18:34:01",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=41382687",
    "comments": [
      "Postgres can be scaled vertically like Stackoverflow did. With cache on edge for popular reads if you absolutely must (but you most likely dont).No need to microservice or sync read replicas even (unless you are making a game). No load balancers. Just up the RAM and CPU up to TB levels for heavy real world apps (99% of you wont ever run into this issue)Seriously its so create scalable backend services with postgrest, rpc, triggers, v8, even queues now  all in Postgres. You dont even need cloud. Even a mildly RAM'd VPS will do for most apps.got rid of  redis, kubernetes, rabbitmq, bunch of SaaS tools. I just do everything on Postgres and scale vertically.One server. No serverless. No microservice or load handlers. It's sooo easy.\n \nreply",
      "I ran into some scaling challenges with Postgres a few years ago and had to dive into the docs.While I was mostly living out of the \"High Availability, Load Balancing, and Replication\" chapter, I couldn't help but poke around and found the docs to be excellent in general. Highly recommend checking them out.https://www.postgresql.org/docs/16/index.html\n \nreply",
      "They are excellent! Another great example is the Django project, which I always point to for how to write and structure great technical documentation. Working with Django/Postgres is such a nice combo and the standards of documentation and community are a huge part of that.\n \nreply",
      "Like many of the BSDs\n \nreply",
      "Did Postgres used to be a BSD? Are they known for good documentation?\n \nreply",
      "BSD? No, that's operating system(s)Good documentation? Yes\n \nreply"
    ],
    "link": "https://stepchange.work/blog/scaling-rails-postgres-to-millions-of-users-at-microsoft-lessons-takeaways",
    "first_paragraph": "Do you have a Rails app built on PostgreSQL and need to scale it to millions of users? In this interview, I speak with Andrew Atkinson, one of StepChange's expert consultants, who brings deep expertise in optimizing Rails applications for performance and scalability.Andrew\u2019s expertise is backed by real-world experience\u2014during his tenure at Microsoft, he played a key role in scaling the infrastructure for Flip (formerly Flipgrid), a video discussion and sharing app built for classrooms and beyond that Microsoft acquired in 2018, scaling it to support tens of millions of users.Andrew is the author of the bestselling book High Performance PostgreSQL for Rails, published by Pragmatic Programmers in 2024. He is a frequent speaker at industry conferences, including PGDay Chicago and RailsConf, and has appeared on several technology podcasts.\u00a0In this interview, Andrew shares valuable tips and lessons on scaling your application to handle millions of users. We cover key topics like fine-tuning"
  },
  {
    "title": "The journey of an internet packet: Exploring networks with traceroute (sebastianmarines.com)",
    "points": 302,
    "submitter": "marinesebastian",
    "submit_time": "2024-08-23T09:10:39",
    "num_comments": 103,
    "comments_url": "https://news.ycombinator.com/item?id=41327394",
    "comments": [
      "Traceroute doesn't see 90% of the machines your packet passes through.When your packet leaves a router at some-pop-some-port-wherever, that fiber isn't usually the same piece of glass that plugs into the next hop. There's a whole chain of amplifiers and possibly multiplexers that handle it between here and there.Some of those provide reliable transport service, giving you the illusion of a fiber that never breaks, despite backhoes doing what backhoes do. Some of those shift the wavelength of your signal, letting you use cheap optics without troubling in the nuances of DWDM that packs your signal alongside dozens of others onto the same long-haul fiber. Some of those just boost the signal, along with all those others on the same fiber.But what all those machines have in common, is that none of them speak IP. None of them touch the payload. None of them are capable of decrementing a hop count. They're \"part of the wire\" as far as the packet is concerned.In my experience, this leads to two types of network engineers, separated by their understanding of these underlying realities.\n \nreply",
      "And on the active networking component side of things it doesn't touch on MPLS which also doesn't modify the IP headers. You can enter a network in New York and get MPLS switched across the country via active network devices all the way to California and have it show up as a single hop on traceroute.The explanation is great for a toy network bu in today's Internet the vast majority of routes are going to be asymmetrical and that requires running traceroutes from both ends and interpreting the results to find the faulty hop.The author also doesn't cover equal cost multipath (ECMP) which is everywhere. With ECMP you have multiple ports that lead to the same. Next hop and packets are hashed based on some part of the fourtuple, sometimes five tuple including the input Port. In order to track down the faulty link, you need to pro each and every one of the ports which requires that you use a higher level protocol like UDP. Using icmp in this case will not show you an issue some percent of time, providing false negatives which makes it less useful.\n \nreply",
      "The OSI model exists for a reason.You don't think about the life of the electrons going through your processors when you code.Traceroute is a view at a certain level of abstraction. It also doesn't tell you if your packet was delivered using ethernet, wifi or a token ring. It just doesn't matter.\n \nreply",
      "The OSI model hasn't been accurate representation of ip networking since pretty much day 1. It was made specifically for a different protocol, but in the stack we use today some layers are better split up in 2, some protocols exists in multiple layers. It's a nice metaphor but I think it's time to drop it!https://computer.rip/2021-03-27-the-actual-osi-model.html\n \nreply",
      "Something that I've noticed that somehow ends up lost when people learn \"the model\" is the encapsulation aspects.I don't know if it's missing in people's course work or what, but I've had to use http://www.tcpipguide.com/free/diagrams/ipencap.png many a times to explain how stuff like VPNs work, correct statements like \"firewalls don't have a routing table, firewalling is layer 4\", explain things like MTU and payload size, or why certain traffic doesn't go beyond a broadcast segment normally.Personally I think this is one of the better visualizations.\n \nreply",
      "The more you know about how something works the better equipped you are to handle things breaking. It's a safe bet that semiconductor physics and the gate-level construction of CPUs isn't necessary to be a good programmer, but not much further up that stack are things like understanding superscalar processor architecture, how caches work, how CPU protection levels work, etc. Knowing about those things, for sufficiently performance or security-intensive applications, can make a ton of difference.There's an analogy to networking there, too. You don't necessarily need to know how wave-division multiplexing, BGP, or DNS work to communicate over the Internet. For some categories of problems, though, a little bit of knowledge allows you to punch just a bit above your level.\n \nreply",
      "It just doesn't matter until it does. It's fine to work at a higher level of abstraction. But people who understand a lower level of abstraction can do things people will call \"impossible\" with fault injection exploits, rowhammer etc.\n \nreply",
      "To clarify my previous post, asymmetric routing is strictly an L3 behavior, and ECMP routing can also be an L3 behavior where a router chooses one of many equal-cost next hops based purely on data in the IP headers. The exact behavior of course depends on the ECMP load-balancing algorithm in use, whether it's per packet, per destination, or using a hash. And furthermore whether it's strictly IP or if it looks deeper into the packet and uses L3+L4 headers in its decision making.Both asymmetric routing and ECMP routing are visible from L3. In the latter case, the routing decision can utilize some L4 data, so some L4 frobbing to get useful data points in practice is necessary for useful real-world diagnosis.I agree with others that the OSI model is a good metaphor and a framework for reasoning about networking, but it is far from perfect, and the reality for those designing and operating network protocols and devices is messy.MPLS is admittedly invisible and there isn't a thing you can do about it in the same way that you can't expect traceroute to give you a view of the switch ports it went through on a LAN. Of course it is useful to understand and keep in mind the fact that there may be, sometimes huge, gaps in your traceroutes. A sudden huge jump in RTT from one hop to the next can be confusing when trying to understand and troubleshoot a network issue.\n \nreply",
      "OSI was supposed to be a competitor to IP and Ethernet. That's the reason it exists.\n \nreply",
      "CCNA baby\n \nreply"
    ],
    "link": "https://sebastianmarines.com/post/journey-of-a-packet-exploring-networks-with-traceroute/",
    "first_paragraph": "We often take for granted that when we try to connect to a server, the connection will work. But that is not always the case. Sometimes there is something broken with the network that prevents us from reaching the destination. But how can we know where the problem is?I often found myself using ping to test connectivity between two machines, and while ping is a great tool to test if a machine is reachable, it doesn\u2019t give us much information about what can be wrong.The internet is a complex network of routers, switches, and computers, and when we try to connect to a server, our packets go through many routers before reaching the destination. If one of these routers is misconfigured or down, the packet can be dropped, and we can\u2019t reach the destination.In this post, we will see how traceroute works, and how it can help us diagnose network problems.When you connect to a server, either a website or any other service, your computer checks if the destination IP address is in the same network"
  },
  {
    "title": "The 4-chan Go programmer (dolthub.com)",
    "points": 204,
    "submitter": "ingve",
    "submit_time": "2024-08-28T18:35:13",
    "num_comments": 103,
    "comments_url": "https://news.ycombinator.com/item?id=41382699",
    "comments": [
      "As a scientist that ends up working closely with actual professional software engineers... lots of the stuff they do looks like this do me, and I can't for the life of me make sense of why you'd do it.I have seen a single line of code passed through 4 \"interface functions\" before it is called that call each other sequentially, and are of course in separate files in separate folders.It makes reading the code to figure out what it does exhausting, and a few levels in you start to wonder if you're even looking at the right area, and if it will ever get to the part where it actually computes something.\n \nreply",
      "This is actually really bad practice and a very \u201cover eager junior engineer\u201d way of writing software. You\u2019re not off base at all that it seems excessive and confusing. It\u2019s the kind of thing that seems technically complex and maybe even \u201celegant\u201d (in isolation, when you first write the \u201cinteresting\u201d code) at first but becomes a technical nightmare when used in real software that has to grow around and with it. You\u2019re actually more on point in worrying about the understandability and debuggability this introduces.I spent the better part of two years unfucking some Go software that (among other things) misused channels. The problem with channels is that you rarely actually need them, but can use them for a lot of different things without too much initial difficulty.I think a good litmus test for proper use of channels is if you answer no to \u201ccould this be done with a direct function call instead?\u201d and \u201ccan I use a wait group or mutex instead\u201d, and yes to (zooming out a bit to think about what kind of decisions you previously made that led you to think about using channels) \u201cam I really benefitting from concurrency/parallelism enough to justify the technical complexity of debugging concurrent code\u201d.\n \nreply",
      "> This is actually really bad practice and a very \u201cover eager junior engineer\u201d way of writing software.To recycle a brief analysis [0] of my own youthful mistakes:> I used to think I could make a wonderful work of art which everyone will appreciate for the ages, crafted so that every contingency is planned for, every need met... But nobody predicts future needs that well. Someday whatever I make is going to be That Stupid Thing to somebody, and they're going to be justified demolishing the whole mess, no matter how proud I may feel about it now.> So instead, put effort into making it easy to remove. This often ends up reducing coupling, but--crucially--it's not the same as some enthusiastic young developer trying to decouple all the things through a meta-configurable framework. Sometimes a tight coupling is better when it's easier to reason about. [...][0] https://news.ycombinator.com/item?id=41219130\n \nreply",
      "I saw some code in a job I was just starting where they had added several abstractions that I found...confusing.After taking an extra long time to understand what the code actually did, I realized that some junior engineer had been using some design pattern they didn't really understand, and that added zero actual value to the routine.After deleting all of that code and refactoring it to use completely different abstractions, everything was suddenly much easier to read and to extend.Design is a hard skill to learn, and junior developers profoundly haven't learned that skill yet. But that's what we need to teach them as senior engineers, right?Not that I could teach the author of the code I changed, since I think it was written by an intern that no longer worked for the company. But you do what you can.\n \nreply",
      "As someone in leadership, my \u2018strong opinion held loosely\u2019 on this, is that there\u2019s absolutely no way to meaningfully build this skill in people, in a theoretical setting.You can, at best, make them aware that there is such thing as \u201ctoo much\u201d, and \u201cthe right tool for the job\u201d, and keep reminding them.But nothing, nothing, comes remotely close to the real-world experience of needing to work with over-engineered spaghetti, and getting frustrated by it. Especially if it\u2019s code that you wrote 6 months prior.Juniors will always do this. It\u2019ll always be the senior\u2019s job to\u2026let it happen, so the junior learns, but to still reduce the blast radius to a manageable amount, and, at the right moment, nudge the junior toward seeing the errors in their ways.\n \nreply",
      "There is also the fact it\u2019s much easier to write something when you know where you are going. When you start you often just make lots of items general in nature to improve later on.\n \nreply",
      "I mean... some do, some don't. With experience comes appreciation for simplicity and flexibility.\n \nreply",
      "I get your point, but a wait group or a mutex can be removed in favor of a clean usage of channels if the proper concerns are isolated at first. And I would personally much rather reason about channels than mutexes and wait groups. Wait groups and mutexes are just begging for deadlocks and race conditions, where a proper channel, used correctly, eliminates both of those by design.\n \nreply",
      "when I was learning Go, I read a guide that told you to fire off a goroutine to walk a tree and send the values back to the main goroutine via a channel. I think about that \"just an example\" guide a lot when I see bad channel code.For me the biggest red flag is somebody using a channel as part of an exported library function signature, either as a param or a return value. Almost never the right call.\n \nreply",
      "I've used that pattern to write tools to e.g. re-encrypt all whatever millions of objects in an S3 bucket, and examine 400m files for jars that are or contain the log4j vulnerable code.  I had a large machine near the bucket/NFS filer in question, and wanted to use all the CPUs. It worked well for that purpose.  The API is you provide callbacks for each depth of the tree, and that callback was given an array of channels and some current object to examine; your CB would figure out if that object (could be S3 path, object, version, directory, file, jar inside a jar, whatever) met the criteria for whatever action at hand, or if it generated more objects for the tree. I was able to do stuff in like 8 hours when AWS support was promising 10 days.  And deleted the bad log4j jar few times a day while we tracked down the repos/code still putting it back on the NFS filer.The library is called \"go-treewalk\" :) The data of course never ends back in main, it's for doing things or maybe printing out data, not doing more calcualation across the tree.\n \nreply"
    ],
    "link": "https://www.dolthub.com/blog/2024-08-23-the-4-chan-go-programmer/",
    "first_paragraph": "We're using Go to write Dolt, the world's first version-controlled SQL\ndatabase. Like most Go codebases, we use channels and\ngoroutines to implement concurrent execution. Usually we use\nthese constructs in the most boring and straightforward way possible, because concurrent programming\nis hard enough without trying to be clever. But at one point we inherited some code from another\nopen source project that used channels in a very clever way: it used them to send additional\nchannels.This is a channel that sends another channel, which then sends a struct. It's basically a way to\npass channels between different goroutines, to implement some fan-out pattern among worker\ngoroutines. Think of the \"middle\" channel as the middleman in the workflow: its job is to pass new\nchannels as they are produced down to workers who actually do useful work. It did work, but it's the\nkind of overly clever idea that was hard to reason about and work with, especially once you consider\ngoroutine leaks. We rewro"
  },
  {
    "title": "What Is Post-Quantum Cryptography? \u2013 NIST (nist.gov)",
    "points": 22,
    "submitter": "rbanffy",
    "submit_time": "2024-08-28T20:20:04",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=41383843",
    "comments": [
      "Dan Bernstein (look him up, you already use a lot of his cryptography) recommends Streamlined NTRU Prime.https://libntruprime.cr.yp.to/\n \nreply",
      "Which other cryptographers agree with him about this?\n \nreply",
      "He has repeatedly collaborated with Tanja Lange who is also credited as working on the original NTRU Prime design.  The other collaborators (all respected cryptographers) presumably think the scheme is sound as well.The developers of OpenSSH agree enough to use this scheme in OpenSSH.DJB is high profile enough that all of his stuff gets a lot of cryptanalysis from experts.  This isn't a rando proposing a scheme that no one can follow, and no one can be bothered to review.  He consistently designs cryptographic systems which perform better and are less error prone to implement than systems designed by committees of people.\n \nreply",
      "CRYSTALS-Kyber had the same team structure; neither was more \"designed by a committee\" than the other. Can you articulate what would make Bernstein's team more credible than the Kyber team?I promise you, CRYSTALS-Kyber does not lack for expert analysis.I ask all this because there's a pretty big \"Schneier Facts\" vibe to anything that involves Bernstein, and because I don't think you'll find many cryptographers --- probably even including on the NTRU Prime team --- that would sign off on his critique of Kyber and the NIST process. I could be wrong, but if I am, could you tell me how?\n \nreply",
      "In case anyone wants to check out NTRU.\"NTRU encryption algorithm, is an NTRU lattice-based alternative to RSA and elliptic curve cryptography (ECC) and is based on the shortest vector problem in a lattice (which is not known to be breakable using quantum computers).\"[0][0]: https://en.wikipedia.org/wiki/NTRUEncrypt\n \nreply",
      "NIST's first PQC standard, CRYSTALS-Kyber, is also a lattice-based system (with a more complex structure and based on LWE rather than the NTRU SVP. There are now solid implementations of it all over the place, so it's much easier to check out.\n \nreply",
      "I was watching a WW2 documentary yesterday and learned Nazi Germany\u2019s encryption had already been (mostly) cracked by the time they entered the war, which (don\u2019t quote me) significantly influenced the outcome of the war. They thought their communications were encrypted, but that was a false belief.It got me thinking. Once someone truly does break current gen encryption via quantum or otherwise, how much time would go by before it is made known to the public that the encryption is broken?It seems the safest path forward is to assume encryption is broken and move to post-quantum crypto before we \u201cneed\u201d to.\n \nreply",
      "> the safest path forward is to assume encryption is broken and move to post-quantum crypto before we \u201cneed\u201d to.An even safer path would be to use both \"classical\" and post-quantum methods in combination (\"hybrid\"). You're protected against someone building quantum computers (and inventing new math to break classical cryptography, which is less likely) and also against yet-undiscovered weaknesses of post-quantum primitives.\n \nreply",
      "Pretty sure all hell would break loose financially so there would definitely be signs.  Think how much value is locked up behind cryptography.\n \nreply",
      "I addition to the incentive to hide it and not stir up any obvious evidence, anyone with enough money to throw at the problem probably doesn't want to wreck the current financial system\n \nreply"
    ],
    "link": "https://www.nist.gov/cybersecurity/what-post-quantum-cryptography",
    "first_paragraph": "An official website of the United States governmentHere\u2019s how you know\nOfficial websites use .gov\n\n              A .gov website belongs to an official government organization in the United States.\n            \nSecure .gov websites use HTTPS\n\n              A lock (  \n\nLock\nA locked padlock\n\n) or https:// means you\u2019ve safely connected to the .gov website. Share sensitive information only on official, secure websites.\n            https://www.nist.gov/cybersecurity/what-post-quantum-cryptographyResearchers worldwide are racing to develop new devices called quantum computers, which could do many things conventional computers cannot \u2014 including breaking the defenses that secure confidential electronic information. NIST is leading a global effort to create electronic defenses against such attacks through its Post-Quantum Cryptography (PQC) project. Read on for some answers to common questions about this developing technology and NIST\u2019s efforts.Encryption algorithms protect confidential electr"
  },
  {
    "title": "Charge Robotics (YC S21) is hiring MechEs to build robots that build solar farms (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-08-28T21:00:17",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/charge-robotics/jobs/ml4f9l4-senior-mechanical-engineer",
    "first_paragraph": "Robots that build solar farmsCharge Robotics is a Series A startup building robots that build solar farms.Demand for new solar projects is booming (1/5th of all the solar that exists in the US was installed last year!), but today\u2019s construction companies can\u2019t keep up due to limited labor resources.We thought this was insane, so we started working on robots to directly address this bottleneck and speed up the world\u2019s transition to renewables.Charge is a fast-moving company which means constant opportunities for learning and growth. You\u2019ll have a large impact on the direction of our company and our product, which will be reflected in significant equity compensation. And you get to work with \ud83e\udd16 giant robots \ud83e\udd16.If you are excited to work on interesting technical problems with direct climate impact, you\u2019re going to fit right in at Charge Robotics.Read more about Charge in recent press:Charge\u2019s funding:We\u2019re MIT-founded and backed by some of Silicon Valley\u2019s top investors, including Lux, YC ("
  },
  {
    "title": "What's New in SQLAlchemy 2.1? (sqlalchemy.org)",
    "points": 6,
    "submitter": "hackandthink",
    "submit_time": "2024-08-26T11:37:41",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://docs.sqlalchemy.org/en/21/changelog/migration_21.html",
    "first_paragraph": "in development\nHome\n                | Download this Documentation\n\nHome\n        | Download this Documentation\nAbout this DocumentThis document describes changes between SQLAlchemy version 2.0 and\nversion 2.1.SQLAlchemy 2.0 implemented a broad array of PEP 484 typing throughout\nall components, including a new ability for row-returning statements such\nas select() to maintain track of individual column types, which\nwere then passed through the execution phase onto the Result\nobject and then to the individual Row objects.   Described\nat SQL Expression / Statement / Result Set Typing, this approach solved several issues\nwith statement / row typing, but some remained unsolvable.  In 2.1, one\nof those issues, that the individual column types needed to be packaged\ninto a typing.Tuple, is now resolved using new PEP 646 integration,\nwhich allows for tuple-like types that are not actually typed as Tuple.In SQLAlchemy 2.0, a statement such as:Would be typed as:In 2.1, it\u2019s now typed as:When execut"
  },
  {
    "title": "Show HN: IPA, a GUI for exploring inner details of PDFs (github.com/seekbytes)",
    "points": 208,
    "submitter": "nicolodev",
    "submit_time": "2024-08-28T10:22:52",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=41377960",
    "comments": [
      "This is cool!Here are some other similar(?) tools, for seeing the inner contents of a PDF file (the raw objects etc), but I haven't compared them to this tool here:- https://pdf.hyzyla.dev/- https://github.com/itext/i7j-rups (java -jar ~/Downloads/itext-rups-7.2.5.jar)- https://github.com/desgeeko/pdfsyntax (python3 -m pdfsyntax inspect foo.pdf  > output.html)- https://github.com/trailofbits/polyfile (polyfile --html output.html foo.pdf)- https://www.reportmill.com/snaptea/PDFViewer/ = https://www.reportmill.com/snaptea/PDFViewer/pviewer.html (drag PDF onto it)- \nhttps://sourceforge.net/projects/pdfinspector/ (an \"example\" of https://superficial.sourceforge.net/)- https://www.o2sol.com/pdfxplorer/overview.htmMore?\n \nreply",
      "I am the author of PDFSyntax, thanks for mentioning it!The HTML output is like a pretty print where you can read view objects and follow links to other objects.Since I have added a new command (disasm) that is CLI oriented and displays a greppable summary of the structure.\nHere is an explanation:\nhttps://github.com/desgeeko/pdfsyntax/blob/main/docs/disasse...\n \nreply",
      "Mutool is the one I suggest to people. The easiest way to understand a PDF is to decompress it and then just read the contents.    mutool clean -d in.pdf out.pdf\n\nAt that point you\u2019ll realise that a PDF is mostly just a list of objects and  that those objects can reference each other. After that you\u2019ll journey through the spec understanding what each type of object does and what the fields in it control. The graphics stream itself is just a stack based co-ordinates drawing system that\u2019s easy to follow too.By way of an example. Here's an object that represents a Page. You can see the dimensions in the MediaBox. The contents themselves are contained at object \"9 0 obj\" (\"9 0 R\" is the pointer to it):    2 0 obj\n    <<\n      /Type /Page\n      /MediaBox [ 0 0 612 792 ]\n      /Contents 9 0 R\n    >>\n    endobj\n\nMeanwhile \"9 0 obj\" has the drawing instructions. They seem a little weird at first glance but you see the values \".23999999 0 0 -.23999999 0 792\" each get pushed on the stack and then \"cm\" pops them to interpret them as the transformation matrix.    9 0 obj\n    <<\n      /Length 18266\n    >>\n    stream\n    .23999999 0 0 -.23999999 0 792 cm\n    q\n    0 0 2551 3301 re\n    ...\n\nThe depth and detail of all of the different possible things that can be represented in a PDF is insane. But understanding the structure above is all you need to begin your journey!EDIT The rest of your journey is contained in this epic document: https://opensource.adobe.com/dc-acrobat-sdk-docs/pdfstandard...\n \nreply",
      ">     mutool clean -d in.pdf out. pdfMy tool can do exactly the same (viewing internal structure, exporting objects, and see the uncompressed raw content for stream) with a graphical interface and without all this kind of flags (which one of the reasons I started to design this project with egui), but thanks for posting yours too.\n \nreply",
      "The venerable PDFedit[1] more or less forces you to confront the internal structure of the PDF file as well.[1] http://pdfedit.cz/en/index.html\n \nreply",
      "Thanks for the list, the idea behind my tool was to try to code something that might fit an analyst that would take a fast look at the PDF. I'm also trying to figure out some fast heuristics to mark/highlight some peculiar stuff on the file itself.Now regarding the tools you mentioned, I haven't checked out all of them, but part of them are interesting (and more mature, speaking of testing and compatibility). However some (at least the ones I was trying) are very basic, and they don't allow the \"Save object as..\" or uncompress it. I like the feature of displaying the PDF for preview :)\n \nreply",
      "Recommend just letting people have their one day in the sun. We\u2019ve become less the site of builders as the red team for testing your launch.\n \nreply",
      "yeah I agree, and while everyone is suggesting tools which are really good but I designed mine to get rid of the flags and CLI interface. Good for tech people that keeps remembering flags, I'm not :(\n \nreply",
      "Sweet, currently working on PDF signature stuff so I'm sure I'll find some stuff handy :)\n \nreply",
      "For exploring the inners of a PDF you also have RUPS[1] which is open source and easily installed in Linux through flathub[2].[1] https://itextpdf.com/products/rups[2] https://flathub.org/apps/com.itextpdf.RUPS\n \nreply"
    ],
    "link": "https://github.com/seekbytes/IPA",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        GUI analyzer for deep-diving into PDF files. Detect malicious payloads, understand object relationships, and extract key information for threat analysis.\n      Interactive PDF Analysis (also called IPA) allows any researcher to explore the inner details of any PDF file. PDF files may be used to carry malicious payloads that exploit vulnerabilities, and issues of PDF viewer, or may be used in phishing campaigns as social engineering artefacts.\nThe goal of this software is to let any analyst go deep on its own the PDF file. Via IPA, you may extract important payload from PDF files, understand the relationship across objects, and infer elements that may be helpful for triage of malicious or untrusted payloads.The main inspiration goes to the fantastic people behind Zynamics, and their excellent product, called PDF dissector.\n\nWhen I st"
  },
  {
    "title": "Panasonic Toughbook 40 (panasonic.com)",
    "points": 103,
    "submitter": "fidotron",
    "submit_time": "2024-08-28T16:58:16",
    "num_comments": 105,
    "comments_url": "https://news.ycombinator.com/item?id=41381569",
    "comments": [
      "Oooh, I finally managed to coax my manager into getting me a ToughBook at a previous gig, after I'd churned through a Dell Mobile Workstation or two a year doing field work.The ToughBook lasted years. True, the keyboard left a bit to be desired, the screen ghosted like you wouldn't believe and the colours were a bit, uh, off, not to mention ridiculous resolution - but killing the thing was essentially impossible.It is the Land Cruiser of laptops. Expensive, sluggish, heavy, indestructible.It even had a serial port (and, still does as an optional extra!)Nowadays, I spend much less time in the field and use a T14. Has served me well thus far. Oh, and I can actually carry it around without thinking twice of it.\n \nreply",
      "From the full specs PDF: https://ftp.panasonic.com/connect/mobility/TOUGHBOOK%2040%20...I see: Optional Serial (True) D-sub 9-pin 6Where \"6* says:    > VGA+Serial+LAN or USB-A+HDMI+Serial or USB-A(x2)+HDMI or Fischer LAN or USB-C+USB-A+HDMI in Rear Expansion Area are mutually exclusive.\n\nWow, I had to lookup Fischer LAN.  Never heard of that before.  It looks like a IP68-rated Ethernet connection!  Ref: https://fischerconnectors.com/en/news-blog/high-speed-data-m...\n \nreply",
      "Dell's rugged line has a serial port standard AND gives you the option of having an additional serial port!  Now if they would only make a 4 serial port version then we could hook up the jet engine software we have without an adapter....\n \nreply",
      "Certain APAC markets get Toughbooks that are derivatives of their JDM Let's Note models: https://ap.connect.panasonic.com/sg/en/products/toughbook/to...Something that's a bit closer to a T14 in portability (albeit this is a X13 competitor - there's a JDM only 14\" Let's Note).Sadly, it's a few generations behind the latest 12\" Let's Note model that have welcome improvements such as 3:2 displays (rather than 16:9).\n \nreply",
      "There are some Dells that are quite indestructible as well. OTOH, it\u2019s usually cheaper to let someone kill a couple laptops than to get a single rugged one. Plus, not all parts of the dead laptop are destroyed, so things like memory and solid-state storage can be easily transplanted to the next victim.\n \nreply",
      "This is true, as long as a day in the field without a working laptop does not cost comparably to a new laptop. Imagine your laptop cracking and ceasing to work when the helicopter that brought you to the interesting location has just disappeared in the distance.\n \nreply",
      "I used to use Thoughbooks every day at work while I was working as an EMT. I kinda have a love/hate relationship with them. I loved their ruggedness. They can fall from the stretcher to the ground, you can sit on them and they even work when it's raining on them. But everything else is not that great. The keyboard feels awful, the touchscreen sometimes didn't work (which was awful because the application was designed for touchscreens) and the trackpad is tiny compared to other laptops. Still they're better than the alternatives that other cities used. Documenting with pen and paper is exhausting, iPad aren't as rugged and their on screen keyboard is even worse and smartphones just don't appear that professional.\n \nreply",
      "Do you think the keyboard is awful because it needs to survive incredible abuse and operating conditions (liquid spills, etc)?\n \nreply",
      "100% better than the GETACs my ambulance service used, those were hot garbage, with even worse touchscreens (or it may also have been that my service was so cheap they'd buy them on eBay, and even buy partially broken ones and the Director of Ops would Frankenstein multiple broken ones together to get one working)...\n \nreply",
      "I used both, but the thoughbook with Windows and the GETAC with Android. The touchscreen experience was much better in the GETACs I used. The thoughbook is good in the laptop format as linked but not the thoughbooks that are meant to be used as tablets - those die easier.\n \nreply"
    ],
    "link": "https://connect.na.panasonic.com/toughbook/rugged-computers/toughbook-40",
    "first_paragraph": "Model:Thunderbolt\u2122 4 USB-C (optional 2nd USB-C), USB-A x 2 (plus 2 more optional), MicroSDXC, HDMI (optional 2nd), Optional VGA, Ethernet (optional 2nd), Optional Fischer LAN, Optional Serial, Audio In/Out, Optional DVD Drive, Optional Blu-ray Drive, Dual SIM (4FF, eSIM)Secured-core PCs are the most secure Windows PC ever, with powerhouse protection out-of-the-box that proactively helps prevent infections, protects information, and reduces security complexity\nPanasonic Connect collaborates with a strong network of partners to deliver top-quality products that you can rely on, all at a great value. Learn more about each of our trusted reseller partners.\nThe TOUGHBOOK 40 has up to 16 cores total including two dedicated NPU cores (in addition to dedicated CPU and GPU cores). The NPU cores accelerates artificial intelligence (AI)-driven tasks for customers across law enforcement departments, federal agencies and utility companies.The TOUGHBOOK 40 features eight user-replaceable locations i"
  },
  {
    "title": "A dishwasher can make or break a restaurant (2017) (washingtonpost.com)",
    "points": 210,
    "submitter": "mhb",
    "submit_time": "2024-08-26T12:17:21",
    "num_comments": 390,
    "comments_url": "https://news.ycombinator.com/item?id=41356415",
    "comments": [
      "I have a PhD in Physics from Berkeley. Still, in the strictest real estate sense, my best and highest use is as a dishwasher.I was managing a small optics factory in Livermore. We made laser mirrors to order there \u2014 any wavelength, any reflectivity, any angle, any polarization, you name it. I was working my ass off and was unmarried at the time. Thanksgiving came around and I had nowhere to go. But I hooked up with a church in San Jose that had a dinner for poor people and went as a volunteer. After serving the dinner, I wandered back into the dish room. I immediately went over to the sink and kicked out the lady who was pretending to work. I then washed all the dishes and left.Comes one year later.  I\u2019m in the exact same situation.  I call up that church.  The lady says, \u201cOh, that\u2019s very nice of you.  But we don\u2019t need any more volunteers, we have enough.\u201d  Oh shit.  What to do.  I found my old replica army parka I had bought in a Cambridge surplus store 20 years earlier. I went as a poor person. I didn\u2019t want to be alone.The first thing I found out is that poor people are herded, controlled, treated like children. We had to wait outside the church in the mild cold until permitted to enter, in a kind of line. So I sit down.  I will never forget the beatific smiles of the volunteers that served us.  This was performance art, and they were the stars.  Everyone on the supplicant end noticed this, I\u2019m sure. So the meal ends. I walk back into the dish room, survey the situation, once again kick out whoever was pretending to do the dishes, do the dishes, and leave.\n \nreply",
      "Loved reading this. I admire your courage and ingenuity to dress as a poor person. If you haven\u2019t already read it you might enjoy Orwell\u2019s autobiographical Down and Out in Paris and London, your story tangentially reminded me of it.\n \nreply",
      "Great book!\n \nreply",
      "Hungry people tend to not care at all who smiles for show and who actually cares, as long as the food is served. Just my 2 cents, but I get where you're coming from.\n \nreply",
      "When going to foodbanks as a kid, I could 100% tell.\n \nreply",
      "As a person whose been on both sides, it makes a difference if you truly interact with someone.\n \nreply",
      "I worked as a dishwasher in the '90s at a fancy hotel for a couple years as a teen. I was in high school, was from a single parent household, my mom needed financial help, I had no skills, and had to learn the language (English).This particular kitchen though was enormous since it serviced 3 restaurants and a lounge. So the days were long and I had to work pretty much every major holiday and/or especial day. The worst was Mother's Day. It was brutal. Always a Sunday morning and unrelenting.It was a sweaty, tough, smelly, and undignified, at times, kind of job. The chemicals in the soap used by the machine were abrasive as hell. Burned your freaking fingertips over time.Only good thing about the job was lunch time. Having access to a lot of good food.  But you never wanted to eat your meal around other people since everyone else thought you smelled and were put off by your sweaty appearance. Again, the job was not dignified, not in appearance anyway.And I don't care what label or euphemistic title employers come up with for the job. (e.g. dishie, dishy, etc) It still doesn't change the job. It does not make it dignified until they start paying more money and more and more people want to do it cause it pays more. Until then, job freaking sucks.A dishwasher by any other name would still have smelly, peeled and cracked hands.\n \nreply",
      "Why did you not wear gloves? When I did dishwashing at Mc Donald's I wore gloves they worked fine. Maybe my stint was shorter at only a year, but I'm pretty sure McDonalds is using plenty hard chemicals too.\n \nreply",
      "I'm a little late to conversation.As a chef for 17 years, here is the advice I was given when I asked someone at age 16 what I should do to become a chef. He said I should get a job as a dishwasher. His reasoning is I needed to learn to appreciate and respect the restaurant's dishwasher because, in his words, 'when shit hits the fan and everything goes wrong, it is the dishwasher who will save your ass.'At 17, I walked into one of the best restaurants in the California and told them I was looking for a job as a dishwasher and why. I was very lucky because it lead to working in the best restaurants in the world. It took another 5 years, age 22, before I learned the power of telling an employee out loud I can't succeed without them. They didn't even want a raise, they only wanted me to say those words.\n \nreply",
      "This as good a place as any to mention one of my favorite authors, Dishwasher Pete[0], who wrote a zine about dishwashing and dishwashers[1], and a memoir about his quest to wash dishes in all 50 states[2].0:https://en.wikipedia.org/wiki/Dishwasher_Pete1:https://archive.org/details/dishwasher72: https://www.goodreads.com/book/show/827458.Dishwasher\n \nreply"
    ],
    "link": "https://www.washingtonpost.com/sf/style/2017/08/07/chefs-say-a-dishwasher-can-make-or-break-a-restaurant-so-i-signed-up-for-a-shift/",
    "first_paragraph": "Chefs say a dishwasher can make or break a restaurant. So I signed up for a shift.ShareOur food critic works a shift to understand why top chefs are starting to give dishwashers their due.Our food critic works a shift to understand why top chefs are starting to give dishwashers their due.My dish hose has a mind of its own.Every time I use it to spray a geyser of water onto a dirty plate, it splashes clean whatever it touches \u2014 and shoots much of the detritus back into my face. By the end of my shift, I\u2019ve ingested specks of just about every dish at this restaurant: rice, seafood, salsa, black beans, you name it. And each time I set the wriggling rubber\u00a0snake down between tasks, it repositions itself, obliging me to apologize to colleagues for soaking more than just myself. Above: ABOVE: Dishwashers Esteban Soc, right, and Joselino Aguilar work in the kitchen of Mexican restaurant Caracol in Houston on July 12. (Scott Dalton for The Washington Post)Until recently, the most dishes I\u2019ve e"
  },
  {
    "title": "Formal CHERI: design-time proof of full-scale architecture security properties (2022) (lightbluetouchpaper.org)",
    "points": 95,
    "submitter": "fanf2",
    "submit_time": "2024-08-28T11:42:02",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=41378428",
    "comments": [
      "I'd take this over incremental clock speed improvements any day.\n \nreply",
      "I've been a huge fan of CHERI for years, but unfortunately the hardware is super closed (huge research institutions only). Hopefully there will be a hardware option for hobbyists one of these years (the virtual machines don't interest me).\n \nreply",
      "FWIW...Morello boards are hard to come by, but there have been efforts to offer cloud-computing style use of them, especially now that bhyve support exists; if you're interested I can try to find out more (I'd offer you time on my cloud-computing Morello cluster from MSR, but it's offline for silly reasons).  The \"Big CHERI\" RISC-V FPGA boards are indeed quite expensive, but CHERIoT-Ibex runs on the Arty A7 or the purpose-built Sonata board, and those are much more reasonable.  (I'd still love to see it brought up on cheaper boards, too...)\n \nreply",
      "I wonder why emulators aren't interesting for a hobbyist.  However, there are FPGA implementations [1], and micro-controller-type systems on FPGA available commercially [2].[3] has a list of publications for the rigorous engineering agenda.1. https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/cheri...2. https://cheriot.org/3. https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/cheri...\n \nreply",
      "> I wonder why emulators aren't interesting for a hobbyist.I have the Rust programming language to fill the software part of this niche. The hardware part of CHERI is what makes it interesting to me.(e.g. I've tinkered with Rust bootloaders before, and it doesn't matter too much whether the emulator is CHERI or not since Rust itself lets me express memory safety in the type system.)\n \nreply",
      "> it doesn't matter too much whether the emulator is CHERI or not since Rust itself lets me express memory safety in the type systemYou might be interested in a very timely blog post: https://cheriot.org/cheri/myths/2024/08/28/cheri-myths-safe-...\n \nreply",
      "CHERIoT chips will be available early next year from SCI Semiconductor iirc. I have heard of some samples floating around\n \nreply",
      "This project shares so many foundational computing ideals this represents the best aspects of \u201cthe many eyes surfacing not just bugs but assumptions\u201d.I\u2019m actually more interested in developing from the riscV branch.Thinking about computers in the 100 year time frame I decided that processing \u201clocal\u201d symbols is better than more sophisticated processes. The risc-V direct access to symbol development is where I want to start defining my \u201clocal\u201d symbols.The hope is that this line of thought will reduce the number of abstraction layers between the user and their environment.CHERI having these concepts defined for risc-v creates a foundation for local processing\nOf symbols with a \u201cgood computing seal of integrity. I also see it as leading to less re-invention which should help progress.\n \nreply",
      "Hi, sorry, this is all quite a bit above my head but I am interested in alternate architectures, would you mind expanding on what kinds of symbols you're talking about? My mind jumps to the members of an instruction set, but I assume you would have called them instructions in that case, what's the alternative to a local symbol?\n \nreply",
      "Sorry I'm always trying to keep it simple.The symbols I'm discussing were first documented by Claude Shannon. When I'm discussing symbols or large circuits I consider them interchangeable views of the same thing. They represent each other.https://en.wikipedia.org/wiki/Claude_ShannonI'm actually a designer so if I wanted to describe them as instructions I easily could. I'm a stickler for language so I believe that the use of the term instructions limits the conversation because it is too specific to communicate what I'm thinking about.I would say an early example local symbol development might be libC. Our current computing environment evolved from the Personal Computing revolution and the internet. This came about through commercial interests and public adoption. I see this development as reaffirming the ideals first proposed by the \"mother of all demos\".https://en.wikipedia.org/wiki/The_Mother_of_All_DemosWhat I consider a \"local\" symbol is being demonstrated by Apple with their on device ML. The highest ideal to me is that everyone develops their own personal symbol table of digital services. I see CHERI as offering the fast track to that type of computing. I see this a integrating rather than programming.\n \nreply"
    ],
    "link": "https://www.lightbluetouchpaper.org/2022/07/22/formal-cheri/",
    "first_paragraph": "Memory safety bugs continue to be a major source of security vulnerabilities, with their root causes ingrained in the industry:Over the last twelve years, the CHERI project has been working on addressing the first two of these problems by extending conventional hardware Instruction-Set Architectures (ISAs) with new architectural features to enable fine-grained memory protection and highly scalable software compartmentalisation, prototyped first as CHERI-MIPS and CHERI-RISC-V architecture designs and FPGA implementations, with an extensive software stack ported to run above them.The academic experimental results are very promising, but achieving widespread adoption of CHERI needs an industry-scale evaluation of a high-performance silicon processor implementation and software stack. To that end, Arm have developed Morello, a CHERI-enabled prototype architecture (extending Armv8.2-A), processor (adapting the high-performance Neoverse N1 design), system-on-chip (SoC), and development board"
  },
  {
    "title": "Typing lists and tuples in Elixir (elixir-lang.org)",
    "points": 181,
    "submitter": "idmitrievsky",
    "submit_time": "2024-08-28T11:49:32",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=41378478",
    "comments": [
      "I really respect Elixir core team\u2019s approach to adding gradual typing to the language. They don\u2019t rush it. They didn\u2019t put too much focus on syntax so far (I\u2019d argue the syntax in many cases is less important than foundations) and instead they focused on soundness of the system. With each new Elixir version the compiler is getting smarter, catching more bugs. Not hugely smarter, but smarter enough that I feel safer. Looking forward to Elixir 1.18!\n \nreply",
      "I'm not sure... I'm a huge Elixir fan and I trust Jos\u00e9 to build a great solution, but I've found the rollout to be a bit confusing.  There was the announcement that \"Elixir is now a gradually typed language\" prior to 1.17 - but it seems that most of the changes were behind the scenes, and 1.17 largely didn't expose user-facing type errors or warnings.Again, I definitely trust them to get it right in the long term, but in the meantime, the progress has been a bit confusing to me.\n \nreply",
      "Thanks for vote of confidence!We need to type every data type and every function, so the type system will be rolled out over a long period of time.The 1.17 release meant that we now have a gradual type system, which runs in every code being compiled, but it only supports a handful of types (including the dynamic one). The full list of supported types and examples of typing violations it can now detect is on the announcement: https://elixir-lang.org/blog/2024/06/12/elixir-v1-17-0-relea...There is no support for type annotations, that comes in a later stage. The overall stages have been described in an earlier article (and I believe also in the paper): https://elixir-lang.org/blog/2023/06/22/type-system-updates-...\n \nreply",
      "> but it seems that most of the changes were behind the scenes, and 1.17 largely didn't expose user-facing type errors or warnings.That's how it normally goes with gradual type systems for existing languages, I think. The first step seems to be almost always adding a type checker that doesn't do anything in particular other than handling untyped code. Since being able to handle untyped code makes a type system gradual, announcing Elixir as \"gradually typed\" when this milestone is reached seems justified. After that, you're free to improve the type system and type checker(s), improve type inference, add specialized syntax, improve typed/untyped interactions, cover more language patterns, and so on. MyPy for Python also started without support for many things that were added later (and it's still being actively developed ten years later).\n \nreply",
      "The wording was a little odd, but there are certainly user-facing errors in 1.17, namely:- Map keys (called with '.') are checked at compile time.- Using comparison operators with different types causes a warning.I may be forgetting something.\n \nreply",
      "F# has both a `head` and `tryHead` function to handle lists that may or may not be empty. In general, `tryFoo` is a good pattern for naming functions that might fail.Having a separate NonEmptyList type might seem like a good idea in theory, but in my experience, it leads to code that is significantly more complicated.\n \nreply",
      "Just to clarify for the crowd though. In F#, `List.head` throws an exception when it fails whereas `List.tryHead` returns an `option`, which returns `None` when it fails instead of an exception.A general confusion of mine in Elixir is generally how libraries and functions treat errors. There's the common idiot of returning either `{:ok, ____}` or `{:error, ____}`, but what can be inside the error tuple is not always clear. The other thing is that sometimes a function can both throw an exception and also return a success tuple. Such cases are confusing to handle, and there's a large gap between handling cases like that and the philosophy of \"let it crash\", which I think is preached a little looser than it should actually be practiced.I do like F#'s way of disambiguating the two situations. The only issue I have in F#, which actually exists in every language that I know of that has exceptions, is that there is no way to know, up front and clearly, what exceptions can be thrown by a given function. This is particularly frustrating in F#, which has fantastic pattern matching for exceptions. I wish there was exhaustive pattern matching in F# for exception handling, such that it would warn you that you have an unhandled exception in a try/with expression (https://learn.microsoft.com/en-us/dotnet/fsharp/language-ref...) but of course would allow for wildcard patterns.\n \nreply",
      "Java has checked exceptions and they are used, e.g. in Android. But it seems to be the exception (heh) rather than the rule.IMO the problem is proper exception handling with checked exceptions and wrapping each function (or at least small blocks) in try catch is just so insanely verbose that even though it is possible to get error handling as good as something like Rust, nobody actually does it in practice.\n \nreply",
      "How does it make it more complicated?In my view, you\u2019re moving the potential for failure to a different place (the constructor), rather than changing some fundamental property or introducing new complexity.Is it handling the construction of these types you find complicated? And is it simply not worth the guarantees?\n \nreply",
      "The intuitive definition of NonEmptyList is:    type NonEmptyList<'t> = 't * 't list\n\nBut this cannot be passed to any function that expects a List<'t>. This is odd though, since intuitively, all non-empty lists are lists.See Rich Hickey's \"Maybe Not\" talkOOP solution is to use inheritance. Typical ML solution is to use type-classes.F# sits in an awkward middle-ground where neither is a perfect fit.I believe that dependently-typed languages solve this more elegantly.There's also the syntactic inconvenience of wrapping at construction, where in theory the compiler could figure it out for you.For example:    let xs = 1 :: 2 :: 3 :: []\n\nHere xs is non-empty, but we must tell the compiler:    let xs = 1, 2 :: 3 :: []\n\nTypeScript does a better job here (although a great cost!)What you end up with is massive code duplication or lots of extra function calls:   xs\n   |> NonEmptyList.toList\n   |> List.map (fun x -> x + 1)\n   |> NonEmptyList.unsafeFromList\n\n(I say this all as someone who really likes F#)\n \nreply"
    ],
    "link": "https://elixir-lang.org/blog/2024/08/28/typing-lists-and-tuples/",
    "first_paragraph": "We have been working on a type system for the Elixir programming language. The type system provides sound gradual typing: it can safely interface static and dynamic code, and if the program type checks, it will not produce type errors at runtime.It is important to emphasize type errors. The type systems used at scale today do not guarantee the absense of any runtime errors, but only typing ones. Many programming languages error when accessing the \u201chead\u201d of an empty list, most languages raise on division by zero or when computing the logarithm of negative numbers on a real domain, and others may fail to allocate memory or when a number overflows/underflows.Language designers and maintainers must outline the boundaries of what can be represented as typing errors and how that impacts the design of libraries. The goal of this article is to highlight some of these decisions in the context of lists and tuples in Elixir\u2019s on-going type system work.In this article, the words \u201craise\u201d and \u201cexcep"
  }
]