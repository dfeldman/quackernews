[
  {
    "title": "Anyone can access deleted and private repository data on GitHub (trufflesecurity.com)",
    "points": 930,
    "submitter": "__0x1__",
    "submit_time": "2024-07-24T18:24:02",
    "num_comments": 185,
    "comments_url": "https://news.ycombinator.com/item?id=41060102",
    "comments": [
      "I reported this on their HackerOne many years ago (2018 it seems) and they said it was working as intended. Conclusion: don't use private forks. Copy the repository instead.Here is their full response from back then:> Thanks for the submission! We have reviewed your report and validated your findings. After internally assessing the finding we have determined it is a known low risk issue. We may make this functionality more strict in the future, but don't have anything to announce now. As a result, this is not eligible for reward under the Bug Bounty program.> GitHub stores the parent repository along with forks in a \"repository network\". It is a known behavior that objects from one network member are readable via other network members. Blobs and commits are stored together, while refs are stored separately for each fork. This shared storage model is what allows for pull requests between members of the same network. When a repository's visibility changes (Eg. public->private) we remove it from the network to prevent private commits/blobs from being readable via another network member.\n \nreply",
      "There seems to be no such thing as a \"private fork\" on GitHub in 2024 [1]:> A fork is a new repository that shares code and visibility settings with the upstream repository. All forks of public repositories are public. You cannot change the visibility of a fork.[1] https://docs.github.com/en/pull-requests/collaborating-with-...\n \nreply",
      "A fork of a private repo is private. When you make the original repo public, the fork is still a private repo, but the commits can now be accessed by hash.\n \nreply",
      "According to the screenshot in the documentation, though, new commits made to the fork will not be accessible by hash. So private feature branches in forks may be accessible via the upstream that was changed to public, if those branches existed at the time the upstream's visibility changed, but new feature branches made after that time won't be accessible.\n \nreply",
      "Not through the GitHub interface, no. But you can copy all files in a repository and create a new repository. IIRC there's a way to retain the history via this process as well.\n \nreply",
      "That\u2019s beside the point. The article is specifically about \u00ab GitHub forks \u00bb and their shortcomings. It\u2019s unrelated to pushing to distinct repositories not magically \u00b4linked\u2019 by the GH \u00ab fork feature \u00bb.\n \nreply",
      "You can create a private repository on GitHub, clone it locally, add the repo being \"forked\" from as a separate git remote (I usually call this one \"upstream\" and my \"fork\", well, \"fork\"), fetch and pull from upstream, then push to fork.\n \nreply",
      "That's not the GitHub concept / almost trademark of \"fork\" anymore though, which is what your parent was talking about\n \nreply",
      "All you should have to do is just clone the repo locally and then create a blank GitHub repository, set it as the/a remote and push to it.\n \nreply",
      "I mean it's git, just git init, git remote add for origin and upstream, origin pointing to your private, git fetch upstream, git push to origin.\n \nreply"
    ],
    "link": "https://trufflesecurity.com/blog/anyone-can-access-deleted-and-private-repo-data-github",
    "first_paragraph": ""
  },
  {
    "title": "Dungeons and Dragons taught me how to write alt text (ericwbailey.website)",
    "points": 93,
    "submitter": "ohjeez",
    "submit_time": "2024-07-24T20:35:49",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=41061755",
    "comments": [
      "Great advice: describing things in order of importance.Most people intuitively describe images from foreground to background or left to right, a bit like they are mentally completing a checklist of all the things to describe. As correctly noted by the author, describing by importance first has the added benefit of allowing screen reader users to skip irrelevant/uninteresting images early.Compare:Torn-up painting in a gallery, observers standing in front of the work.vs.Gallery interior, people standing in front of a painting with visible damage.\n \nreply",
      "I think of this in cinematic terms. The first sentence is going to start with a shot of the painting, then it will either cut, zoom or dolly out to reveal the crowd. Whereas the second shot starts with the wide angle of the gallery and then does the opposite. Each has a slightly different effect on the scene and the audience.\n \nreply",
      "The first sentence leads me to imagine a torn up painting and a group of people clustered around it.The second sentences leads me to imagine a large gallery space with high ceilings with a smattering of people in front of one of the paintings.Both ways have their pros and cons. Describing the space first lets the reader paint a setting for the eventual object of interest.\n \nreply",
      "That's true, but consider the context; this isn't a novel, it's functional text for someone who is probably trying to accomplish a task.  A user in a hurry might skip part way through the second description and be misled to thinking the photo was just a normal picture of a gallery.\n \nreply",
      "Absolutely true in the context of the article ie alt text. I was speaking more universally.\n \nreply",
      "Related, but for me it was Dwarf Fortress. The item descriptions aren't exceptionally good per se, they're auto-generated after all. But they employ particularly poignant verbs[1] over more adjective-heavy descriptions. Taking a cue from that style dramatically improved my alt text.1. like encrusted, encircled, adorned\n \nreply",
      "The author says in the first paragraph that he used to play a lot of D&D (dndbeyond.com) and now prefers Dungeon World (dungeon-world.com; PDF is $6). Does anyone know why he might prefer the latter? As context, I play D&D weekly, love it, and am always interested in learning more. Dungeon World is designed to focus on creativity and shared storytelling with simpler mechanics to make the game more fluid. However, there's nothing simpler than having a clear D&D rule for something like fall damage, instead of having the party debate if a player survived the fall. Dungeon World doesn't have fall damage calculator and instead relies in the narrative, presumably from the pre-written story or DM.\n \nreply",
      "Most people who prefer DW would say that D&D sometimes has clear rules for something, but often has no rules, boring rules, or rules that aren't necessarily \"fun\". Combat, while tactical, tends to be slow and can frequently consume a lot of time in a session, plus the majority of rules and character powers are focused on combat.If you're playing sessions with a lot of RP, DW will have a much better balance of rules:session-time, it's much easier to prep for, and given how rules-lite D&D really is outside combat, will probably have about the same amount of narrative input. Note that it's not necessarily the \"group debating if the player survived\", but typically the GM giving the player a choice when they fail to climb the wall, like \"you fall and take a little damage, or you slip a little, cursing loudly and alerting the enemies at the top to you\".Done well, it gives the players a lot more agency, and much better buy-in for the story as they're now shaping it, instead of just being along for the ride. I would also say that pre-written narratives aren't really a thing for DW (at least, as far as I know!), so it's really down to what the DM sees as an appropriate penalty or choice, often phrased as \"you succeed, but <thing>\".It's not really better or worse than D&D overall, I'd just say that it's much better suited for certain play-styles. If you enjoy tactical gameplay and using miniatures, then D&D (or maybe Pathfinder) are much better options. If the thought of yet another fight makes you want to gouge your eyes out, I'd recommend giving DW a try.\n \nreply",
      "The player choices and handling of partial success in PBtA games (like dungeon world) really makes them sing. A partial success leads to adding complications, which creates really interesting situations.The original Apocalypse World book has some really great ideas on how to run a campaign, as well - very worth reading for anyone who runs ttrpgs.\n \nreply",
      "Dungeon World is a Powered by the Apocalypse game. It's both ligher in rules and gives the players increased control over the narrative of the game. It's a narrative TTRPG. If you've played FATE or Blades in the Dark you've played a narrative RPG.Dungeon World is an open game and there is an SRD for it: https://www.dwsrd.org/In the case of falling, the GM would assign damage based on how dangerous it is: https://www.dwsrd.org/playing/playing-the-game.html#damageBear in mind that HP essentially doesn't scale with level. PCs are likely to have an HP maximum between 15 and 25 for the entire campaign.If you're conscious, the GM might let you Defy Danger to mitigate some of that, but you have to describe what your character is doing to achieve that: https://www.dwsrd.org/playing/basic-moves.html#defy-dangerIf it was a fall from a great height, you'd just skip to Last Breath: https://www.dwsrd.org/playing/special-moves.html#last-breathThere's no specific rules for it because the general rules are good enough, especially considering how often falling damage actually comes up\n \nreply"
    ],
    "link": "https://ericwbailey.website/published/dungeons-and-dragons-taught-me-how-to-write-alt-text/",
    "first_paragraph": "\n    Things might look a little janky for a bit, but that\u2019s okay! I am in the process of refreshing the design.\n  \nHome \u279e Writing\nI played a lot of the pen-and-paper roleplaying game in high school and college. I\u2019m now conceptually more into Dungeon World\u2019s approach, but I digress.Unlike Tom Hanks, I avoided turning into a delusional murderer. Instead, I deepened some friendships, had a lot of big laughs, learned some cool vocabulary, and had an indirect introduction to systems design. Importantly, I also annoyed the hell out of my high school principal.If you are not familiar with Dungeons & Dragons, there are two general flavors for how to play:We elected for theater of the mind more often than not. This was mostly because the rule books by themselves were expensive enough, and my friends and I were lower middle class.Theater of the mind play means that the entire game is conducted verbally. The sole exception is your character sheet, which is a text and number-based armature you bui"
  },
  {
    "title": "A Multimodal Dataset with One Trillion Tokens (github.com/mlfoundations)",
    "points": 102,
    "submitter": "kulikalov",
    "submit_time": "2024-07-24T20:04:24",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=41061390",
    "comments": [
      "Does it make sense to measure a dataset in tokens? Shouldn't it be tokenizer-agnostic? I.e. the OpenAI tokenizer encodes about ~4 characters per token, but I could also have a tokenizer that does 1 character per token leading to a ~4x increase in token count (relative to the OpenAI tokenizer.)\n \nreply",
      "More info on the Salesforce blog:\nhttps://blog.salesforceairesearch.com/mint-1t/\n \nreply",
      "Wow did not expect sales force to be behind this.It\u2019s basically free advertising for technical people to join sales force.\n \nreply",
      "Salesforce has long been involved in publishing quality NLP papers, especially during Stephen Merity's tenure.Smerity's papers are some of my favourite. Check outhttps://ar5iv.labs.arxiv.org/html/1708.02182And my all-time favouritehttps://ar5iv.labs.arxiv.org/html/1911.11423\n \nreply",
      "Salesforce produced one of the best Llama-3(8B) finetunes, IMO: SFR-Iterative-DPO-LLaMA-3-8B-RHopefully they do something with Llama-3.1\n \nreply",
      "What's the license though?\n \nreply",
      "From https://huggingface.co/datasets/mlfoundations/MINT-1T-HTML#l...:We release  MINT-1T under a CC-BY-4.0 license, designating it primarily as a research artifact. While the dataset is freely available, users are responsible for ensuring its legal use in commercial settings. Users must independently verify compliance with applicable laws before employing MINT-1T for commercial purposes.Same page includes this caveat:Potential Legal and Ethical Concerns: While efforts were made to respect robots.txt files and remove sensitive information, there may still be content that individuals did not explicitly consent to include.\n \nreply",
      "How effective would modeling raw byte sequences be, with the individual bytes as the \"tokens\", and a vocabulary of 256 elements?You could then train on any kind of digital data.\n \nreply",
      "Due to the way tokenization usually works with LLMs (using BPE \u2014 Byte Pair Encoding), there's actually usually already a 256-element embedding within the token-space that represents \"raw bytes.\" You could say that this 256-element set is \"pre-seeded\" into any BPE encoding \u2014 and will remain as part of the encoding as long as at least one document in the dataset used to determine the tokenization, uses each byte at least once in a non-high-frequency-suffix-predictable way.These tokens are also already very much in use by the tokenizer \u2014 they get emitted in sequences, to encode single Unicode codepoints that weren't common enough in the dataset to get their own tokens, and so instead require multiple tokens to represent them. I believe most tokenizers (e.g. tiktoken) just take the UTF-8 byte-sequences underlying these codepoints and encode them literally as sequences of the above 256-element set.If you're curious, here's the definition of the encoding used by most modern LLMs, in newline-delimited \"[base64 of raw input byte sequence] [tokenID to encode as]\" format: https://openaipublic.blob.core.windows.net/encodings/cl100k_... . If you decode it, you can observe that the rest of the 256-element single-byte embedding space gets mapped to tokenIDs immediately following those of the ASCII printables.\n \nreply",
      "Somewhat inefficient for text, very inefficient for images, specially if you work in pixel space. The max context a model today has been trained is 1M tokens, which takes up a lot of memory. Even if context was not an issue, to generate a 1000x1000 image would take ~3 hours on 100token/s inference.Google has trained an encoder/decoder LLM on bytes called ByT5[1][1] https://huggingface.co/google/byt5-xxl\n \nreply"
    ],
    "link": "https://github.com/mlfoundations/MINT-1T",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        MINT-1T: A one trillion token multimodal interleaved dataset.\n      Paper | Dataset | Blog Post\ud83c\udf43 MINT-1T is an open-source Multimodal INTerleaved dataset with one trillion text tokens and 3.4 billion images, a ~10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers.We release all subsets of MINT-1T, including:If you found our work useful, please consider citing:\n        MINT-1T: A one trillion token multimodal interleaved dataset.\n      "
  },
  {
    "title": "Phish-friendly domain registry \".top\" put on notice (krebsonsecurity.com)",
    "points": 135,
    "submitter": "LinuxBender",
    "submit_time": "2024-07-24T16:03:30",
    "num_comments": 93,
    "comments_url": "https://news.ycombinator.com/item?id=41058424",
    "comments": [
      "I have a story on using weird/fishy/phishy TLDs: Recently my colleagues and I started collecting information on all the available compression methods for 3D Gaussian Splatting (3DGS, a popular method for 3d scene representation). There were quite a few works in the area with naming conflicts already, so I thought to give it a unique short name to refer to - and came up with \u201c3dgs.zip\u201d.A few days later we started putting together a web page, and I noticed that .zip actually is available as a TLD. Impulsively I bought the domain, https://3dgs.zip/, launched it and printed it on a few shirts before heading off to a conference. Felt a bit weird that there is a .zip TLD, but I was in a rush and I didn\u2019t ponder its existence any further.But strange things started happening: setting up the domain for a GitHub page worked, but in the process downloaded a 0 Byte file called \u201c3dgs.zip\u201d, when submitting content one of the GitHub.com forms. And a few days later colleagues told me they had trouble accessing the site. After some DNS sleuthing and then some back-and-forth with our IT dept, it turned out that our organization has blocked the whole TLD - for every Windows user, out of phishing concerns of people being confused.I\u2019m no security person, so the reasoning felt a bit weird to me, as I guess the .zip TLD can\u2019t hurt anybody; downloading a .zip might, which you can attach to any link name? But in any case I wasn\u2019t able to find any .zip URL with a purpose, but lots of Reddit posts of angry sysadmins who bemoaned the influx of terrible TLDs with mostly phishing use and vowed to block them all. So they probably have a point in downright blocking the whole TLD.Now I\u2019m sitting here with my .zip url. Had to revert the page to use github.io, so people in my organization (and similarly thinking ones) would be able to access it. Guess I\u2019m cured for a while, won\u2019t be using any novelty TLDs anytime soon\u2026\n \nreply",
      "> I\u2019m no security person, so the reasoning felt a bit weird to me, as I guess the .zip TLD can\u2019t hurt anybody; downloading a .zip might, which you can attach to any link name?Turn of all of your developer knowledge for a minute.You click on a link \"very-trustworthy-ceo-information.zip\" in a mail, since you want to download this very important information from your CEO. Sure, your browser pops up, but it does that all the time so who cares, and then there is a file \"very-trustworthy-ceo-information.zip\" in your downloads folder. Native Outlook might usually open it in a different way usually, but who cares? OWA - you won't notice a difference in the UI at all. But anyway, important CEO information. Open the zip, open the PDF, oops your workstation is compromised.If we turn our technical knowledge back on, it's rather simple. A user was phished to open a link to \"https://very-trustworthy-ceo-information.zip\". This returned with a file download, obviously called \"very-trustworthy-ceo-information.zip\", containing whatever I want to contain based off of IPs and whatever I can stuff into the link in a hidden fashion the average user won't note.A lot of people would not be able to distinguish between https://foo.zip answering with a binary content type and naming the file foo.zip through content disposition headers and foo.zip coming from a trusted source.And honestly, I would personally have to double-check what's going on there if it happened to me.\n \nreply",
      "My point was that the person fooled by https://foo.zip/ would have been also fooled by https://foo.com/bar.zip, so the existence .zip wouldn\u2019t change much.But now I\u2019ve understood that the auto-linkification of a simple non-link mention like update.zip can be indeed dangerous.\n \nreply",
      "> I\u2019m no security person, so the reasoning felt a bit weird to me, as I guess the .zip TLD can\u2019t hurt anybody; downloading a .zip might, which you can attach to any link name? But in any case I wasn\u2019t able to find any .zip URL with a purpose, but lots of Reddit posts of angry sysadmins who bemoaned the influx of terrible TLDs with mostly phishing use and vowed to block them all. So they probably have a point in downright blocking the whole TLD.The problem is auto-linkification. It is extremely common in forum posts or emails to refer to attached filenames. Most forum softwares or email clients are helpful it automatically turning obvious URLs (doesn't start with a protocol:// but ends in a .tld) into clickable links. Anybody's reference to a zip filename is now a clickable link, only waiting to be registered for phishing attempts.\n \nreply",
      "That makes sense. Funnily enough I had kind of an inverse problem building the https://3dgs.zip/ landing page, or linking to the project from elsewhere - I\u2019d point a link to the compression survey with the link text \u201csurvey.3dgs.zip\u201d.And had to have people point out to me that they don\u2019t want to click on that, because they don\u2019t want to download a big file.\n \nreply",
      "That's not a new problem tho. .TS and .CS are TLDs and Heck even .COM is also a file extension, should we block that too? What changed suddendly?\n \nreply",
      "The thing is '.com' is relatively unknown.  When was the last time you had a genuine .com file you needed to use?And you're someone who's tech-savvy.Most people are going to see \".com\" and think \"Website\" not \"Program\". So if suddenly a .com file is downloaded there's at least a chance they might stop and wonder what's going on.I run into this issue all the time - We get Bug bounty reports constantly because $SecurityResearcher put \"example.com\" into the \"Company Name\" field, and we sent them an email saying \"Thanks for signing up. We hope you find $OurProduct useful at $CompanyName. \"When the email turns up in their Gmail inbox, Gmail \"helpfully\" turns the plain text \"example.com\" into a link to https://example.com - so $SecurityResearcher reports it to us as a vulnerability in our platform because \"we\" are linking to example.com - except we never did, and we have no way to tell Gmail, or Outlook, or any other platform to stop doing that.Services we pay for, directly, also do this - Notion and Slack are two I can think of immediately.  I have to fight them to stop turning my mention of some file into a link to a random domain.   (e: Maybe Slack has stopped doing this, perhaps - in testing it didn't do automatic linkifying for messages to myself)This was bad enough with .cs, .ts, .js., .json and so forth - but having a .zip link appear in the middle of documentation on how to do something with a zip file is a recipe for disaster.I've had a document saying \"please download package.zip from the build artifacts site\" - which was then auto linked to https://package.zip - and anyone not paying attention might expect that it was a link to the build artifacts site.\n \nreply",
      "I think less than 1% of Internet users are ever going to see a file with those names. .ts and .cs are typically shared via source control, not email.\n \nreply",
      "The type of user who has email conversations about .COM files is the same type of user who will realize that the link was automatically created by someone's email client and have a laugh about it.I don't know if you can say the same about zip files, an average user they might encounter someone mentioning a zip filename a handful of times in a year and they might click on the link expecting to get that zip file.\n \nreply",
      "COM files are a good point I hadn't considered.\n \nreply"
    ],
    "link": "https://krebsonsecurity.com/2024/07/phish-friendly-domain-registry-top-put-on-notice/",
    "first_paragraph": "The Chinese company in charge of handing out domain names ending in \u201c.top\u201d has been given until mid-August 2024 to show that it has put in place systems for managing phishing reports and suspending abusive domains, or else forfeit its license to sell domains. The warning comes amid the release of new findings that .top was the most common suffix in phishing websites over the past year, second only to domains ending in \u201c.com.\u201dImage: Shutterstock.On July 16, the Internet Corporation for Assigned Names and Numbers (ICANN) sent a letter to the owners of the .top domain registry. ICANN has filed hundreds of enforcement actions against domain registrars over the years, but in this case ICANN singled out a domain registry responsible for maintaining an entire top-level domain (TLD).Among other reasons, the missive chided the registry for failing to respond to reports about phishing attacks involving .top domains.\u201cBased on the information and records gathered through several weeks, it was dete"
  },
  {
    "title": "Alexa is in millions of households and Amazon is losing billions (wsj.com)",
    "points": 208,
    "submitter": "marban",
    "submit_time": "2024-07-23T05:54:39",
    "num_comments": 435,
    "comments_url": "https://news.ycombinator.com/item?id=41042929",
    "comments": [
      "https://archive.ph/5VPB5",
      "The core issue is that Amazon envisioned Alexa as a product that would help it increase sales. Smart home features were always an afterthought. How convenient would it be if people could shout \"Alexa order me Tide Pods\" from wherever they were in their home and the order got magically processed? That demo definitely got applause from a boardroom full of execs.The problem is that consumers don't behave like that. This is also why Amazon's Dash buttons failed. I always want to see a page with the product details and price before I click \"buy\". Reducing the number of clicks is not going to make me change my decision and suddenly order more things.If they want to salvage Alexa, they need to forget shopping and start doubling down on the smart home and assistant experience. The tech is still pretty much where it was in 2014. Alexa can set timers and tell me the weather, and...that's basically it. Make it a value add in my life and I wouldn't mind paying a subscription fee for it.\n \nreply",
      "> I always want to see a page with the product details and price before I click \"buy\". Reducing the number of clicks is not going to make me change my decision.This is compounded by the multi-headed monster that large orgs like theirs have no choice but to become. If customers could trust that every day essentials had a relatively stable price and availability pattern like they trust from their local grocery store (rightly or wrongly), blind ordering might be more tenable.But some other head on the beast wants to keep Amazon shaped like an unmonitored digital marketplace where orders are fulfilled dynamically by bidders and algorithms, so your Tide Pods could be anywhere from $6.99 to $64.99, and you might get anywhere between 10 and 100, and they might arrive tomorrow or next week, and they might come in retail packaging or as a bag of tide-pod-resembling-mystery-objects, etc.Of course blind ordering won't work when you can't give your customers any  assurances (let alone guarantees) about price, quality, volume, etc\n \nreply",
      "This a million times.I swear I will never understand why Amazon's supply, organization, and pricing for household goods is such a disaster.Because their experience for mainstream books is mostly perfectly fine -- there's a single listing for each book, and the price doesn't change much, just some discount from list. It works.But for things like paper towels or Tide or whatever, it's utter chaos. Multiple listings for the same item, sizes and quantities that mysteriously move from one listing to another, prices that vary 10x or more...It's utterly baffling to me why Amazon created this consumer-hostile nightmare. I buy a lot of stuff from Amazon, but everything home and toiletries I buy from Target online, simply because the listings and prices are totally consistent. Even though I have Prime! I don't understand why Amazon doesn't figure out that Prime consumers like me buy from Target instead because Amazon's household supplies listings are such utter unpredictable garbage, while Target just works like a normal store.\n \nreply",
      "Don\u2019t be baffled\u2026 \u201cworlds largest selection\u201d pivoted to worlds largest fence. Financially, the third party product is better for Amazon (no inventory - the third party stuff is a revenue generator!)How exactly does Spooky23 have the buying power to price competitively with a major retailer? The answer is pretty obvious. It\u2019s cheaper to pay crackheads to loot CVS than to troll for clearance blowouts.\n \nreply",
      "It's ridiculous that I have to have a spreadsheet with pricing information and links for household goods just to spot check and make sure I'm getting the actual price per quantity that I want due to the multiple dynamic listings that change every day nightmare.Don't ask how many times I receive more or less than I thought I would or something came in 10 packages of 3 instead of 1 30x package.\n \nreply",
      "It'd not even that I care much about small deviations--I just don't trust that something really crazy won't happen if I put it on autopilot. If I walk into Walmart and grab a big armful of Bounty I'm fairly confident things will be fine.\n \nreply",
      "I'm in the process of moving.  I thought maybe I'd give Prime Day a try and send some soaps ahead to my new place.In my last apartment, I used Method's pump-dispenser laundry detergent and their basil-scented kitchen hand soap.Amazon is selling the laundry detergent for $75 and the hand soap for $15.  I'm guessing Method discontinued the SKUs I was used to, and there's some leftover stock on Amazon with crazy prices.\n \nreply",
      "I switched to Walmart a few years ago and their online experience has been more consistent.Although I detect it\u2019s starting to drift as they switch to a \u201cmarketplace\u201d approach similar to Amazon.\n \nreply",
      "Prescription drugs have something called \"NDC number\" ( National Drug Codes ).  What we need is NDC numbers but for consumer goods.\n \nreply"
    ],
    "link": "https://www.wsj.com/tech/amazon-alexa-devices-echo-losses-strategy-25f2581a",
    "first_paragraph": ""
  },
  {
    "title": "When eyesight fades and climbing provides comfort (lacrux.com)",
    "points": 19,
    "submitter": "bryanrasmussen",
    "submit_time": "2024-07-21T14:17:01",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.lacrux.com/en/videos/when-eyesight-fades-and-climbing-provides-comfort/",
    "first_paragraph": ""
  },
  {
    "title": "Hiding Linux Processes with Bind Mounts (righteousit.com)",
    "points": 78,
    "submitter": "indigodaddy",
    "submit_time": "2024-07-24T15:52:06",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=41058292",
    "comments": [
      "If you have the privilege to pull off bind-mounting on top of a particular process directory in /proc you should also be able to mount something on top of /proc itself that will present a false /proc/mounts and conceal your trickery. I experimented a bit and wasn't able to bind a single file on top of /proc/mounts and I seem to be having trouble getting procfs to participate in an overlayfs, but if you were really dedicated you could construct a whole fake /proc by hand, or create something using FUSE to create a /proc that redacts anything you want, including mountpoints.\n \nreply",
      "If you have root of course, you can do anything, BUT -- could a eBPF module be loaded into the kernel to just filter the syscalls to hide some nefarious process?  This simplifies the 'evil VM' attack.  Does eBPF have anything that could plausably help with that?\n \nreply",
      "https://sysdig.com/blog/ebpf-offensive-capabilities/\n \nreply",
      "Can you do similar trickery to ensure `mount` doesn't list your bind mount?\n \nreply",
      "I don't see why not - that's just reading from /proc/mounts.\n \nreply",
      "Nitpicking:  mount reads from /proc/self/mounts, that is, the mounts as seen from that process.   This is how tricks such as chroot jails work, as they present a different mount set and filesystem tree from the base system.\n \nreply",
      "Same thing, really - /proc/mounts is a symlink to /proc/self/mounts.\n \nreply",
      "Fair enough, though casual readers may not realise this.\n \nreply",
      "creating a whole fake proc is simple.  I mean the code is already in the kernel as a module.  Just copy it, put a few if/else's in the code to control what its readdir ( this case just pid's there's proc_readdir() that calls proc_pid_readdir(), just modify that very simply).mount is a bit harder (as its calls into code that is more in the core kernel fs code, but one can just reimplement it oneself, to filter out the mounts you don't want to show.\n \nreply",
      "Interesting.  Could you not perhaps tank the system attempting something like that?\n \nreply"
    ],
    "link": "https://righteousit.com/2024/07/24/hiding-linux-processes-with-bind-mounts/",
    "first_paragraph": "Righteous ITJoin the crusade!Lately I\u2019ve been thinking about Stephan Berger\u2019s recent blog post on hiding Linux processes with bind mounts. Bottom line here is that if you have an evil process you want to hide, use a bind mount to mount a different directory on top of the /proc/PID directory for the evil process.In the original article, Stephan uses a nearly empty directory to overlay the original /proc/PID directory for the process he is hiding. I started thinking about how I could write a tool that would populate a more realistic looking spoofed directory. But after doing some prototypes and running into annoying complexities I realized there is a much easier approach.Why try and make my own spoofed directory when I can simply use an existing /proc/PID directory from some other process? If you look at typical Linux ps output, there are lots of process entries that would hide our evil process quite well:These process entries with low PIDs and process names in square brackets (\u201c[somenam"
  },
  {
    "title": "Biological Circuit Design (biocircuits.github.io)",
    "points": 70,
    "submitter": "drones",
    "submit_time": "2024-07-24T16:22:58",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=41058671",
    "comments": [
      "Can anything be made out of this?\n \nreply",
      "The individual techniques described in the class are in use all over the pharmaceutical and biotech industries (especially biosensors) but we're still in the early stages of biological circuits, akin to where electronics was right before the vacuum tube was invented. We've got stuff that operates sort of like basic logic elements like relay switches did in electronics, but we're far from designing any complex integrated circuits.I think this point from the introduction says it best:> - From a more practical point of view, we have a very limited ability to construct, test, and compare designs. Even with recent developments such as CRISPR, our ability to rapidly and precisely produce cells with well-defined genomes remains limited compared to what is possible in more advanced disciplines. (This situation is rapidly improving!)\n \nreply",
      "I sure hope this is a graduate-level course.\n \nreply",
      "BE150 is sort of inbetween: https://www.catalog.caltech.edu/current/2023-24/department/B...\"Advanced undergraduate\" or \"beginning graduate.\" It's Caltech though so the distinction is a bit meaningless.\n \nreply",
      "It looks like it is. I took a similar one at MIT.\n \nreply",
      "The difference between graduate and undergraduate level is that graduate is often easier (ie less work load) than undergrad\u2026This is because graduate students main focus is their research, and courses are a distant second.\n \nreply",
      "Hmm, sort of?  I can't really generalize from my own (first year, before joining a lab full-time) PhD courses, but I found them challenging in a way that differed from undergrad: more expectation that you'd be able to figure out the solutions to the hard problems by learning entire new fields on the fly.\n \nreply",
      "Completely depends on the program of study and school. In astronomy and physics programs, graduate courses are far more advanced, but grades are based almost entirely on homework sets. Undergraduate course are less comprehensive, but have a mix of homework and tests.  Working on problem sets is chronically hard, while tests are acutely hard.\n \nreply",
      "What do u mean \"graduate level\"?\n \nreply",
      "I mean that PhD candidates would be the primary target, rather than undergraduates, although talented, advanced undergraduates would also be welcome (this being Caltech, I'd expected there to be plenty of juniors and seniors in the course).I say this because it drops into diffeqs and fairly subtle/complex behavior quickly.\n \nreply"
    ],
    "link": "https://biocircuits.github.io/index.html",
    "first_paragraph": "ChaptersTechnical AppendicesAppendicesPackage docsChaptersTechnical AppendicesAppendicesPackage docs\nLast updated on Jul 10, 2023.\n      \u00a9 2022 Michael Elowitz, Justin Bois, and John Marken. With the exception of pasted graphics, where the source is noted, this work is licensed under a Creative Commons Attribution License CC BY-NC-SA 4.0. All code contained herein is licensed under an MIT license.This document was prepared at Caltech with financial support from the Donna and Benjamin M. Rosen Bioengineering Center.Built with Sphinx using a theme provided by Read the Docs.\n\n \n\n"
  },
  {
    "title": "Space-filling curves, constructively (andrej.com)",
    "points": 46,
    "submitter": "luu",
    "submit_time": "2024-07-24T17:20:44",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=41059412",
    "comments": [
      "An interesting variant of space-filling curves + dimensionality reduction is Geohash (https://en.wikipedia.org/wiki/Geohash, http://geohash.org/) which takes a lon/lat and uses a Z-curve approach to produce a hash such as `u4pruydqqvj` representing the location. This hash value is basically \"how far along the space-filling curve is the lon/lat located\". You're reducing two dimensions (lon/lat) into one dimension (how far along the space-filling curve).There's a unique side-effect to Geohashes in that the value (`u4pruydqqvj`) can have it's end \"lopped off\" (i.e. cut down to `u4pru`) and it still represents a less precise, but generally accurate representation of the original lon/lat in most cases (when the curve isn't near the edge of the 2d map!). This allows you to index locations (lat/lon) using a string ('u4pru') which opens you up to doing b-tree, range queries, etc. in traditional database, with one field.Just a rad math quirk! I'm not an expert, and it's a very dense book, but if someone really wants to get into this kind of stuff the \"Bible\" is \"Foundations of Multidimensional and Metric Data Structures\" by Hanan Samet.\n \nreply",
      "They're also useful for representing IPv4 space. Tom7 had a nice video where he does this after pinging the whole (IPv4) internet [0][0] https://www.youtube.com/embed/JcJSW7Rprio at about 6 minutes, though I recommend the whole thing.\n \nreply",
      "The thing about SFC addressing is that two places near each other always share a prefix, and the closer they are, the longer the common prefix. It's sort of like Gray coding. Quadtree addressing, for example, also has the property that you mentioned (as do, of course, normal lat/long coordinates!) but two adjacent locations may not have similar addresses at all if they happen to straddle a subdivision boundary. (Again, compare to \"normal\" numbers where, say, |2000-1999| = 1 but there's no common prefix at all!)\n \nreply",
      "The boustrophedonic version of Rosenberg-Strong function is my favorite because it doesn't have any such jumps and has better locality preserving qualities than most alternatives.See https://hbfs.wordpress.com/2018/08/07/moeud-deux/.\n \nreply",
      "The same happens, in fact, for space-filling curves. Sure, in _most_ of the area covered by the curve, closeness of points means closeness on the curve. But in this Hilbert curve:https://upload.wikimedia.org/wikipedia/commons/7/7c/Hilbert-...Consider the two points on the curve straddling the middle of the top edge of the square. They are very close together, but their 1D addresses on the curve are very far apart (one close-ish to the beginning, and one close-ish to the end)!There ain't no free lunch.\n \nreply",
      "Space filling curves like Hilbert sound esoteric but they\u2019re actually useful in for high performance database indexing.DuckDB has a Hilbert index.https://github.com/rustyconover/duckdb-lindel-extensionIt\u2019s not just DuckDB. SQL server and Postgres support Hilbert too.The big idea is that you can project tuples e.g (lat, long) or just about any other tuple structure (string or floats) in a single integer, ordered in a way that is locality preserving.Because most queries tend to lookup data close to each other, this can lead to performance gains. Parquet statistics can be built to be locality sensitive (locality defined broadly, not just numbers).If you index on Hilbert, you can specific compute a max min of the Hilbert range of your predicate and eliminate looking outside that range, speeding up query performance.\n \nreply",
      "\"Liquid Clustering\" from Databricks also uses Hilbert curves.\n \nreply",
      "I read an entire book on applications of space filling curves, and none of the applications seemed to actually work as advertised (though perhaps they did at the time the book was written, I am skeptical).All the stuff about database indexing and cache locality seems to have become obsolete with improvements in compilers; the overhead of computing the Hilbert curve indices is just too high. The indexing method that people seem to actually use is the Z-order/Morton order, which just interleaves bits and is mathematically less interesting [1].[1]: https://en.wikipedia.org/wiki/Z-order_curve\n \nreply",
      "One problem I\u2019ve encountered is the inverse mapping is not necessarily locality preserving.Hilbert maps (x,y) -> h such that nearby h correspond to nearby x,y.But the inverse isn\u2019t true: similar x,y don\u2019t always have similar h.This is a problem for some applications where we are trying to find dense hot spots cheaply by only looking at h\u2019s,  but turns out this doesn\u2019t work out so well.\n \nreply",
      "There's an even stronger result. Every space filling curve's inverse is necessarily not locality preserving.Should an inverse exist that is continuous then 2D space would be homeomorphic to 1D, but they're obviously different (if you want a proof, consider that removing a point from a line makes it disconnected while in 2D the space remains connected, now note that 'connectedness' is preserved by homeomorphisms).\n \nreply"
    ],
    "link": "https://math.andrej.com/2024/01/30/space-filling-curves-constructively/",
    "first_paragraph": "In 1890 Giuseppe Peano discovered a square-filling curve, and a year later David Hilbert published his variation. In those days people did not waste readers' attention with dribble \u2013 Peano explained it all on 3 pages, and Hilbert on just 2 pages, with a picture!But are these constructive square-filling curves?There's no doubt that the curves themselves are defined constructively, for instance as limits of uniformly continuous maps. A while ago I even made a video showing the limiting process for Hilbert curve:Is Hilbert's curve constructively surjective? Almost:Theorem 1: For any point in the square, its distance to Hilbert curve is zero.Proof. Recall that the Hilbert curve $\\gamma : [0,1] \\to [0,1]^2$ is the limit of a sequence $\\gamma_n : [0,1] \\to [0,1]^n$ of uniformly continuous maps, with respect to the supremum norm on the space of continuous maps $\\mathcal{C}([0,1], [0,1]^2)$. The finite stages $\\gamma_n$ get progressively closer to every point in the square. Thus, for any $\\eps"
  },
  {
    "title": "The Puzzle of How Large-Scale Order Emerges in Complex Systems (wired.com)",
    "points": 24,
    "submitter": "achristmascarl",
    "submit_time": "2024-07-22T16:47:10",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=41036506",
    "comments": [
      "xxx"
    ],
    "link": "https://www.wired.com/story/the-puzzle-of-how-large-scale-order-emerges-in-complex-systems/",
    "first_paragraph": "To revisit this article, visit My Profile, then View saved stories.The original version of this story appeared in Quanta Magazine.A few centuries ago, the swirling polychromatic chaos of Jupiter\u2019s atmosphere spawned the immense vortex that we call the Great Red Spot.From the frantic firing of billions of neurons in your brain comes your unique and coherent experience of reading these words.As pedestrians each try to weave their path on a crowded sidewalk, they begin to follow one another, forming streams that no one ordained or consciously chose.The world is full of such emergent phenomena: large-scale patterns and organization arising from innumerable interactions between component parts. And yet there is no agreed scientific theory to explain emergence. Loosely, the behavior of a complex system might be considered emergent if it can\u2019t be predicted from the properties of the parts alone. But when will such large-scale structures and patterns arise, and what\u2019s the criterion for when a "
  },
  {
    "title": "Physicists may now have a way to make element 120 (newscientist.com)",
    "points": 104,
    "submitter": "_Microft",
    "submit_time": "2024-07-24T13:06:41",
    "num_comments": 120,
    "comments_url": "https://news.ycombinator.com/item?id=41056694",
    "comments": [
      "https://archive.ph/MqCpl",
      "It's all about stability. If you try and jam too many protons and neutrons together, they won't stay together for long. If the nucleus disintegrates before there's time for electrons to form and act like a chemical element, it's not really in the realm of chemistry any more.\"IUPAC defines an element to exist if its lifetime is longer than 10^\u221214 second, which is the time it takes for the atom to form an electron cloud.[7]\"https://en.m.wikipedia.org/wiki/Island_of_stabilityhttps://en.m.wikipedia.org/wiki/Superheavy_element(Edit: this was intended in response to ssijak's question about the theoretical limits)\n \nreply",
      "If you jam a lot of protons and neutrons together they become stable thanks to gravity. I wonder where the crossover point is.\n \nreply",
      "Apparently the smallest a neutron star could theoretically be is 0.1 - 0.2 Solar masses. [0]And then 1.4 Solar masses is the upper limit. [1][0] https://physics.stackexchange.com/a/143174/43351\n[1] https://en.wikipedia.org/wiki/Chandrasekhar_limit\n \nreply",
      "The Chandrasekhar limit is the maximum size of a white-dwarf, not a neutron star.  It's usually defined as the minimum size of a neutron star (since it has to overcome electron-degeneracy pressure).  The TOV limit[0] is the maximum size of a neutron star.[0] https://en.wikipedia.org/wiki/Tolman%E2%80%93Oppenheimer%E2%...\n \nreply",
      "Ah yes - you\u2019re right.I was thinking of neutron stars all along - which, correct me if I\u2019m wrong, look like they have similar density to an atomic nucleus and for which the upper limit is apparently 3 Sols. [0][0] https://en.m.wikipedia.org/wiki/Neutron_star#:~:text=Neutron....\n \nreply",
      "Don\u2019t we all. This is the reason general relativity and the standard model don\u2019t fit under one framework (string theory has currently failed to combine them in a testable way), we don\u2019t understand why or when gravity starts to become important.\n \nreply",
      "I don't think that's accurate. We can pretty carefully calculate the forces involved and answer the question as posed. Related: We can calculate how strong a magnet has to be to hold your artwork on the fridge in opposition to the force of Earth's gravity.The GR vs Standard Model breakdown occurs at the extreme limits of GR, for example Planck scale regions of space and black hole singularities. A heavy nucleus is way too big to probe these limits and is well within the domain of physics where we don't see a conflict between the two theories.\n \nreply",
      "My understanding is that the repulsion between the positive charges (protons) far exceeds the gravitational force.\n \nreply",
      "For normal nuclei it does, but when you get up into a solar mass sized blob of them, gravity exceeds it.  That's because the repulsion is going in all directions and nets out against itself but all the gravity points inwards cumulatively.(Edit because we got a little confused in the replies: If it were all protons, they would repel and overcome gravity.  But real matter isn't that, it's always protons and neutrons and electrons with close to no net repulsion so gravity wins.)Similarly, radioactive nuclei (alpha emitters and spontaneous fission) happen because the electromagnetic repulsion exceeds the strong force, which has a very short range and doesn't reach across those large nuclei.\n \nreply"
    ],
    "link": "https://www.newscientist.com/article/2440445-physicists-may-now-have-a-way-to-make-element-120-the-heaviest-ever/",
    "first_paragraph": "AdvertisementA method that helped create two atoms of the rare, super-heavy element livermorium may pave the way towards making the hypothetical element 120By Karmela Padavic-Callaghan\n                                    23 July 2024\n                                                                    Jacklyn Gates at Lawrence Berkeley National Laboratory separating atoms of livermoriumMarilyn Sargent/Berkeley Lab 2024 The Regents of the University of CaliforniaJacklyn Gates at Lawrence Berkeley National Laboratory separating atoms of livermoriumMarilyn Sargent/Berkeley Lab 2024 The Regents of the University of CaliforniaThe third-heaviest element in the universe has been made in a way that offers a route for synthesising the elusive element 120, which would be the heaviest element in the periodic table.\u201cWe were very shocked, very surprised, very relieved that we didn\u2019t make any bad choices in setting up the instrumentation,\u201d says Jacklyn Gates at\u00a0Lawrence Berkeley National Laboratory ("
  },
  {
    "title": "Solving the out-of-context chunk problem for RAG (d-star.ai)",
    "points": 196,
    "submitter": "zmccormick7",
    "submit_time": "2024-07-22T13:44:19",
    "num_comments": 68,
    "comments_url": "https://news.ycombinator.com/item?id=41034297",
    "comments": [
      "xxx"
    ],
    "link": "https://d-star.ai/solving-the-out-of-context-chunk-problem-for-rag",
    "first_paragraph": ""
  },
  {
    "title": "InteractiVenn \u2013 Interactive Venn Diagrams (interactivenn.net)",
    "points": 72,
    "submitter": "histories",
    "submit_time": "2024-07-24T14:57:14",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=41057766",
    "comments": [
      "Kamala Harris approves this service [0]![0]: https://www.youtube.com/watch?v=edDnGiJStvs\n \nreply",
      "Dammit, beat me to it!\n \nreply",
      "Wow, that 6-set visualization feels more asymmetric than 2,3,4,5. Is there an upper bound to the number of sets you can visualize in 2D, and how about 3D?\n \nreply",
      "I've had an inkling at various times to make a widget or app or something for players of Diplomacy [0] that uses this visualization to make it easier to see activity in all of your conversations at a glance / select who to talk to.  It always ends up being more of a 'fun' idea than something that would actually be useful, though, because the 6-set Venn diagram is so much more difficult to visually parse than just a list of countries in a \"To\" line.[0] when played with \"white press\", which is where players can send messages to another player / to multiple other players, and the source of the message is authenticated.  This is in contrast to \"gray press\" where the source is / can be anonymous instead of authenticated and \"black press\" where the source can be spoofed.\n \nreply",
      "The 5-set Venn diagram looks beautiful (and is apparently by Branko Gr\u00fcnbaum), but is a similar symmetric 4-set Venn diagram also possible? Can't find any such example on the internetEDIT: apparently not, from Wikipedia: \"David Wilson Henderson showed, in 1963, that the existence of an n-Venn diagram with n-fold rotational symmetry implied that n was a prime number.\"\n \nreply",
      "Relevant xkcd: https://xkcd.com/2962/\n \nreply",
      "This would almost certainly be more difficult, but I'd love if the diagram's relative sizes (optionally) accounted for the size of the sets' data \u2013 e.g. if set A and B have 2 items each with zero overlap, it would show two circles of equal size, completely apart from one another\n \nreply",
      "I've just found this for the exact same reason, for 2-10 sets. The gradient descent is done client-side (Tensorflow.js):https://www.deepvenn.com/I've stress-tested it with ~5 sets around ~50k items, comes back in seconds, well under a minute.\n \nreply",
      "In case you need a quick Euler diagram (Venn and Euler are often mixed up), I recently found: https://eulerr.co/\n \nreply",
      "Interesting that this was published in a bioinformatics journal.\n \nreply"
    ],
    "link": "https://interactivenn.net/index.html",
    "first_paragraph": "Export current diagram:\r\n                                        Try opening the .svg diagram using Inkscape to move shapes, resize, change font, colors and more.Save sets:Load Sets:Sample data set: \nSimple Model\n\nPlease cite: Heberle, H.; Meirelles, G. V.; da Silva, F. R.; Telles, G. P.; Minghim, R. InteractiVenn: a web-based tool for the analysis of sets through Venn diagrams. BMC Bioinformatics 16:169 (2015). 10.1186/s12859-015-0611-3"
  },
  {
    "title": "The Assyrian Renaissance (archaeology.org)",
    "points": 13,
    "submitter": "Caiero",
    "submit_time": "2024-07-19T05:15:13",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://archaeology.org/issues/july-august-2024/features/the-assyrian-renaissance/",
    "first_paragraph": ""
  },
  {
    "title": "Rediscovering Transaction Processing from History and First Principles (tigerbeetle.com)",
    "points": 52,
    "submitter": "todsacerdoti",
    "submit_time": "2024-07-23T10:52:57",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=41044657",
    "comments": [
      "Joran from TigerBeetle here! Really stoked to see a bit of Jim Gray history on the front page and happy to dive into how TB's consensus and storage engine implements these ideas (starting from main! https://github.com/tigerbeetle/tigerbeetle/blob/main/src/tig...).\n \nreply",
      "I\u2019m curious if you ever came across any of the writings of Toon Koppelaars and the idea of ThickDB/SmartDB, it seems very in alignment.\n \nreply",
      "In this X thead Joran comments how it all started with a HN comment back in 2019. https://x.com/jorandirkgreef/status/1815702485190774957\n \nreply",
      "It was pretty surreal to sit next to someone at a dinner in NYC two months ago, be introduced, and realize that they're someone you had an HN exchange with 5 years ago.Here's the HN thread: https://news.ycombinator.com/item?id=20352439And the backstory (on how this led to TigerBeetle): https://x.com/jorandirkgreef/status/1788930243077853234\n \nreply",
      "I honestly don't understand why this isn't a Postgres extension. In what case is it better to have two databases?Realistically, I can't see a scenario where you need something like this but at the same time are sure that you don't need both atomic database and financial ledger operations.\n \nreply",
      "> We saw that there were greater gains to be had than settling for a Postgres extension or stored procedures.> Today, as we announce our Series A of $24 millionCan't raise a 24 million series A for a Postgres extension!\n \nreply",
      "That's true, but who are the executives that buy this and then force developers to create a monstrous architecture that has all sorts of race conditions outside of the ledger?\n \nreply",
      "To be clear, TB moves the code to the data, rather than the data to the code, and precisely so that you don't have \"race conditions outside the ledger\".Instead, all kinds of complicated debit/credit contracts (up to 8k financial transactions at a time, linked together atomically) can be expressed in a single request to the database, composed in terms of a rich set of debit/credit primitives (e.g. two-phase debit/credit with rollback after a timeout), to enforce financial consistency directly in the database.On the other hand, moving the data to the code, to make decisions outside the OLTP database was exactly the anti-pattern we were wanting to fix in the central bank switch, as it tried to implement debit/credit primitives but over general-purpose DBMS. It's really hard to get these things right on top of Postgres.And even if you get the primitives right, the performance is fundamentally limited by row locks interacting with RTTs and contention. Again, these row locks are not only external, but also internal (i.e. how I/O interacts with CPU inside the DBMS), and why stored procedures or extensions aren't enough to fix the performance.\n \nreply",
      "Can you expand on why sproc isn't a good solution (e.g. send set of requests, process those that are still in valid state, error those that aren't, return responses)?Maybe knowing the volumes you are dealing would help also.\n \nreply",
      "Hey mihaic! Thanks for the question.> I honestly don't understand why this isn't a Postgres extension.We considered a Postgres extension at the time (as well as stored procedures or even an embedded in-process DBMS).However, this wouldn\u2019t have moved the needle to where we needed it to be. Our internal design requirements (TB started as an internal project at Coil, contracting on a central bank switch) were literally a three order of magnitude increase in performance\u2014to keep up with where transaction workloads were going.While an extension or stored procedures would reduce external locking, the general-purpose DBMS design implementing them still tends to do far too much internal locking, interleaving disk I/O with CPU and coupling resources. In contrast, TigerBeetle explicitly decouples disk I/O and CPU to amortize internal locking and so \u201cpipeline in bulk\u201d for mechanical sympathy. Think SIMD vectorization but applied to state machine execution.For example, before TB\u2019s state machine executes 1 request of 8k transactions, all data dependencies are prefetched in advance (typically from L1/2/3 cache) so that the CPU becomes like a sprinter running the 100 meters. This suits extreme OLTP workloads where a few million debit/credit transactions need to be pushed through less than 10 accounts/rows (e.g. for a small central bank switch with 10 banks around the table). This is pathological for a general-purpose DBMS design, but easy for TB because hot accounts are hot in cache, and all locking (whether external or internal) is amortized across 8k transactions.I spoke at QCon SF on this (https://www.youtube.com/watch?v=32LMicc0gRA) and matklad did two IronBeetle episodes walking through the code (https://www.youtube.com/watch?v=v5ThOoK3OFw&list=PL9eL-xg48O...).But the big problem with extensions or stored procedures is that they still tend to have a \u201cone transaction at a time\u201d mindset at the network layer. In other words, they don\u2019t typically amortize network requests beyond a 1:1 ratio of logical transaction to physical SQL transaction; they\u2019re not ergonomic if you want to pack a few thousand logical transactions in one physical query.On the other hand, TB\u2019s design is like \u201cstored procedures meets group commit on steroids\u201d, packing up to 8k logical transactions in 1 physical query, and amortizing the costs not only of state machine execution (as described above) but also syscalls, networking and fsync (it\u2019s something roughly like 4 syscalls, 4 memcopies and 4 network messages to execute 8k transactions\u2014really hard for Postgres to match that).Postgres is also nearly 30 years old. It's an awesome database but hardware, software and research into how you would design a transaction processing database today has advanced significantly since then. For example, we wanted more safety around things like Fsyncgate by having an explicit storage fault model. We also wanted deterministic simulation testing and static memory allocation, and to follow NASA's Power of Ten Rules for Safety-Critical code.A Postgres extension would have been a showstopper for these things, but these were the technical contributions that needed to be made.I also think that some of the most interesting performance innovations (static memory allocation, zero-deserialization, zero-context switches, zero-syscalls etc.) are coming out of HFT these days. For example, Martin Thompson\u2019s Evolution of Financial Exchange Architectures: https://www.youtube.com/watch?v=qDhTjE0XmkEHFT is a great precursor to see where OLTP is going, because the major contention problem of OLTP is mostly solved by HFT architectures, and because the arbitrage and volume of HFT is now moving into other sectors\u2014as the world becomes more transactional.> In what case is it better to have two databases?Finally, regarding two databases, this was something we wanted explicit in the architecture. Not to \"mix cash and customer records\" in one general-purpose mutable filing cabinet, but rather to have \"separation of concerns\", the variable-length customer records in the general-purpose DBMS (or filing cabinet) in the control plane, and the cash in the immutable financial transactions database (or bank vault) in the data plane.See also: https://docs.tigerbeetle.com/coding/system-architectureIt's the same reason you would want Postgres + S3, or Postgres + Redpanda. Postgres is perfect as a general-purpose or OLGP database, but it's not specialized for OLAP like DuckDB, or specialized for OLTP like TigerBeetle.Again, appreciate the question and happy to answer more!\n \nreply"
    ],
    "link": "https://tigerbeetle.com/blog/2024-07-23-rediscovering-transaction-processing-from-history-and-first-principles",
    "first_paragraph": "DocsBlogSlack9kSubscribeDocsBlogSlack9kSubscribeJul 23, 2024Joran Dirk GreefIn July 2020, I stumbled into a fundamental limitation in the general-purpose database design for transaction processing.This insight led to the creation of TigerBeetle, and attracted a special team who would engineer a financial transactions database from first principles and bring it to production.Today, as we announce our Series A of $24 million, I want to tell you the story of how TigerBeetle came to solve this fundamental limitation, but first I want to take you back to the future of transaction processing.40 years ago, Turing Award-winner Jim Gray, who is regrettably no longer with us, wrote a paper that would be published as A Measure of Transaction Processing Power.Jim Gray's paper was striking to me for three reasons.First, he proposed a metric to evaluate the performance of Online Transaction Processing (OLTP) databases in terms of \"Transactions per Second\" (TPS).We take the term \"TPS\" for granted tod"
  },
  {
    "title": "Why Discover is no American Express (popularfintech.com)",
    "points": 87,
    "submitter": "kazanins",
    "submit_time": "2024-07-21T14:28:35",
    "num_comments": 164,
    "comments_url": "https://news.ycombinator.com/item?id=41025341",
    "comments": [
      "When I was a kid, I remember my parents used to try to pay for everything with their Discover card and would only use their Visa as a fallback. I was too young to understand the differences between credit cards at the time, but I remembered my parents always asking \"do you take Discover?\" everywhere we went. I never realized this wasn't just something only my parents did until I saw the Futurama joke about this years later: https://www.youtube.com/watch?v=2-nSwKOVZykFry: Do you take Visa?\nCashier: Visa hasn't existed for 500 years\nFry: American Express? \nCashier: 600 years\nFry: Discover? \nCashier: Sorry, we don't take Discover\n \nreply",
      "Great gag, and I'm still in my depths of that practice. Luckily, about 80%+ of the places I ask take Discover.\n \nreply",
      "I clicked on the comment section ready to make this same reference. Such a good joke.\n \nreply",
      "One thing not discussed is the relationship between AmEx's big purchases and its fees. American Express puts a lot of effort into keeping its paying consumers happy because they are both American Express customers (because they pay fees) and because customers really want to use American Express cards at merchants (and so American Express can charge those merchants higher fees).I'll give you an example. Years ago, I bought a fancy sofa online. The merchant sent periodic \"don't worry, we're just behind schedule\" every week or so for months, and then sent one last email saying they had declared bankruptcy. I was way out of my chargeback window, but when I called American Express, they instantly refunded me my purchase anyway. That experience meant that, if I ever make another big purchase online, I will go out of my way to use an American Express card, including switching merchants if necessary.Keeping customers who make big purchases happy costs money, both in losses and in administrative overhead, but it pays out on both the customer and merchant side in the long run.\n \nreply",
      "AmEx was my first card, and I've stuck with them. Despite some decline, their customer service is still top-notch.  I always put major purchases on there.Their Platinum travel insurance has been a lifesaver. When a volcano stranded us, AmEx covered our stay at the TWA hotel in JFK. Lost luggage? They reimbursed $400 in clothes, no questions asked. Disputes? I haven't had many but they were always resolved in my favor.I experimented with having a Chase card when they launched the Reserve. Europcar defrauded me, and Chase wouldn't even let me file a dispute. I eventually won by sending a written complaint (FCBA), but I ditched the card right after. Plenty of reports online suggest Chase discourages disputes heavily.\n \nreply",
      "> Moreover, Discover cards come with no annual fees, while American Express keeps increasing its card fees with every card \u201crefresh\u201d.Not all Amex cards have fees.Also it would have been nice if the author went a little into the history like how Discover was once part of Sears. Not sure if this is still the case or not, but the classic Green, Gold, and Platinum cards are charge cards and have to be paid in full each month.  Some of these things may explain why Amex is more affluent.\n \nreply",
      "Discover also doesn't come with all the \"perks\" that Amex does.Airport lounges, concierge, exclusive books on Michelin star restaurants (also via Tock acquisition), etc.\n \nreply",
      "My Discover card has actually benefitted me more than my (entry -level) AMEX. Discover has 5% cashback categories every quarter, and most of them are for common charges. I've had the bonus on \"gas\" two or three times already. Also Amazon, grocery stores, paypal.\n \nreply",
      "I find the mental overhead of tracking categories to be extremely irritating and I have just stopped using the discover card.\n \nreply",
      "Is this because the discover card is otherwise less ideal to use?\n \nreply"
    ],
    "link": "https://www.popularfintech.com/p/why-discover-is-no-american-express",
    "first_paragraph": ""
  },
  {
    "title": "A Multimodal Automated Interpretability Agent (arxiv.org)",
    "points": 55,
    "submitter": "el_duderino",
    "submit_time": "2024-07-24T12:42:53",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=41056463",
    "comments": [
      "> We think MAIA augments, but does not replace, human over- sight of AI systems. MAIA still requires human supervision to catch mistakes such as confirmation bias and image generation/editing failures. Absence of evidence (from MAIA) is not evidence of absence: though MAIA\u2019s toolkit enables causal interventions on inputs in order to evaluate system behavior, MAIA\u2019s explanations do not provide formal verification of system performance.For folks who are more familiar with this branch of literature, given the above, why is this a fruitful line of inquiry? Isn't this akin to stacking turtles on top of each other?\n \nreply",
      "https://arxiv.org/pdf/2404.14394Actual paper to save you from having to read the PR release.\n \nreply",
      "Ok, we'll change the URL to that from https://news.mit.edu/2024/mit-researchers-advance-automated-.... Users may still want to read the latter for a quick intro.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2404.14394",
    "first_paragraph": "The arXiv Accessibility Forum takes place this September. Free, fully remote and open to all. Learn more and spread the word.Grab your spot at the free arXiv Accessibility ForumHelp | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Show HN: Hooper \u2013 AI-driven stats and highlights for basketball play (hooper.gg)",
    "points": 9,
    "submitter": "grub007",
    "submit_time": "2024-07-24T21:39:55",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.hooper.gg",
    "first_paragraph": "Be first to use AI to track your stats and generate highlights clips with just your phone.1v1, 2v2, 3v3, 4v4, 5v5, subs? It just works.Not just for shootarounds. Built for everyday pickup games to tournament playoffs.Until now, only NBA pros have had the luxury of the latest basketball tech. Lets change that."
  },
  {
    "title": "The Origin of Emacs in 1976 (onlisp.co.uk)",
    "points": 211,
    "submitter": "rhabarba",
    "submit_time": "2024-07-24T00:49:26",
    "num_comments": 93,
    "comments_url": "https://news.ycombinator.com/item?id=41052593",
    "comments": [
      "The TECO version of EMACS is always a kick to look at, here is the dired code:http://pdp-10.trailing-edge.com/mit_emacs_170_teco_1220/01/e...From this we can conclude that humans do not need any nice things like programming languages, assemblers or compilers to program effectively.\n \nreply",
      "> From this we can conclude that humans do not need any nice things like programming languages, assemblers or compilers to program effectively.But maybe not efficiently. I realize that was tongue and cheek, but since this sometimes comes up in earnest (especially when it comes to dynamic vs. static type systems), it's worth noting that we knew from the very beginning that we can effectively program in what we'd now call adverse conditions: Not only were some earlier computers programmed directly in binary code (and Woz famously still programmed a 6502 by entering hex bytes into a monitor), before that we programmed by plugging in cords into panels, and in some sense still do in logic circuit design.However, we sure became much more efficient in programming not only because higher level abstractions allow us to do things quicker, but because in addition of static type systems, we either catch really hard to debug issues much earlier, or won't make a lot of mistakes in the first place (you won't accidentally save a temporary into some register that you already used for something else even if it's \"just C\").\n \nreply",
      "What's that weird APL derivative called that quants love so much? Julia? Reminds me a little of that, in the syntax at least. A little like MUMPS, too, maybe, and probably closer to that level in terms of its abstractions.\n \nreply",
      "You probably mean K.\nJ is also up there, but more educational and less \"let's make some money\".\n \nreply",
      "Julia != J. Or K. Or anything like either of those. It's not an array language.It's a very readable language that anybody who knows Python etc could understand and has nothing to do with any of this.EDIT: LOL, downvotes why?\n \nreply",
      "Downvotes are because the person you are replying to was talking about J, not Julia.\n \nreply",
      "Beh. The first person said Julia (wrong). Then the reply assumed that parent meant J, but did not correct parent about Julia, leaving the misconception in place. Which I clarified.\n \nreply",
      "No, I think they tried to correct them about that by saying \"J is also up there\" (implying \"in contrast to Julia\"), but I agree that that wasn't very clear, and that your clarification, even if it was under a wrong assumption, did help a lot. I stumbled across this as well.\n \nreply",
      "You replied to the wrong person, then.\n \nreply",
      "About this:    To: CBF at MIT-AI, EAK at MIT-AI, ED at MIT-AI, MOON ay MIT-AI\n\nDoes anyone know if this is literally how they wrote email addresses in 1976? Instead of using the @ symbol they typed the word \"at\"?I realize this was before DNS was invented, so I am not surprised by the lack of TLD.\n \nreply"
    ],
    "link": "https://onlisp.co.uk/On-the-Origin-of-Emacs-in-1976.html",
    "first_paragraph": "\nNext: Index, Previous: preview-auto in LaTeX buffers, Up: Emacs, Lisp and related technical subjects \u00a0 [Contents][Index](Date: 23 Jul 2024)\nSummary:\nEMACS was developed at the MIT AI Lab in 1976.  The specifics of the\norigin have been documented by different people in various places.\nThere is an interesting thread which was discussed on the blog of the\nlate Dan Weinreb, and preserved by archive.org.  Ultimately, Guy\nSteele pulled up his records (in the form of printed emails).  The\nbelow is an extract which is of historical interest and includes\nemails from the first couple months of Emacs in 1976.  I quote some\nsections and include the verbatim text at the bottom, which starts\nwith an ITS email from RMS to GLS.\nDan Weinreb introduces\nAnd now, here\u2019s the mail from Guy Steele. I think this is the\nbest information we are ever going to get, and that this is\nthe last word on the topic.\nThe summary from Guy Steele is:-\nMy conclusions: (1) Clearly, by the end of 1976 and thereafter, RMS was"
  },
  {
    "title": "Large Enough \u2013 Mistral AI (mistral.ai)",
    "points": 553,
    "submitter": "davidbarker",
    "submit_time": "2024-07-24T15:32:21",
    "num_comments": 430,
    "comments_url": "https://news.ycombinator.com/item?id=41058107",
    "comments": [
      "Links to chat with models that released this week:Large 2 - https://chat.mistral.ai/chatLlama 3.1 405b - https://www.llama2.ai/I just tested Mistral Large 2 and Llama 3.1 405b on 5 prompts from my Claude history.I'd rank as:1. Sonnet 3.52. Large 2 and Llama 405b (similar, no clear winner between the two)If you're using Claude, stick with it.My Claude wishlist:1. Smarter (yes, it's the most intelligent, and yes, I wish it was far smarter still)2. Longer context window (1M+)3. Native audio input including tone understanding4. Fewer refusals and less moralizing when refusing5. Faster6. More tokens in output\n \nreply",
      "It seems to be the way with these releases, sticking with Claude, at least for the 'hard' tasks. In my agent platform I have LLMs assigned for easy/medium/hard categorised tasks, which was somewhat inspired from the Claude 3 release with Haiku/Sonnet/Opus. GPT4-mini has bumped Haiku for the easy category for now. Sonnet 3.5 bumped Opus for the hard category, so I could possibly downgrade the medium tasks from Sonnet 3.5 to Mistral Large 2 if the price is right on the platforms with only 123b params compared to 405b. I was surprised how much Llama3 405b was on together.ai $5/mil for input/output! I'll stick to Sonnet 3.5. Then I was also surprised how much cheaper Fireworks was at $3/milGemini has two aces up its sleeve now with the long context and now the context caching for 75% reduced input token cost. I was looking at the \"Improved Facuality and Reasoning in Language Models through Multi-agent debate\" paper the other days, and thought Gemini would have a big cost advantage implementing this technique with the context caching. If only Google could get their model up to the level of Anthropic.\n \nreply",
      "Claude needs to fix their text input box. It tries to be so advanced that code in backticks gets reformatted, and when you copy it, the formatting is lost (even the backticks).\n \nreply",
      "They are using Tiptap for their input and just a couple of days ago we called them out on some perf improvements that could be had in their editor: https://news.ycombinator.com/item?id=41036078I am curious what you mean by the formatting is lost though?\n \nreply",
      "Claude is truly incredible but I'm so tired of the JavaScript bloat everywhere. Just why. Both theirs and ChatGPTs UIs are hot garbage when it comes to performance (I constantly have to clear my cache and have even relegated them to a different browser entirely). Not everyone has an M4, and if we did - we'd probably just run our own models.\n \nreply",
      "Hardly a day passes without ML progress...405B dethroned in ONE DAY\n \nreply",
      "All 3 models you ranked cannot get \"how many r's are in strawberry?\" correct.  They all claim 2 r's unless you press them.  With all the training data I'm surprised none of them fixed this yet.\n \nreply",
      "Tokenization make it hard for it to count the letters, that's also why if you ask it to do maths, writing the number in letters will yield better results.for strawberry, it see it as [496, 675, 15717], which is str aw berry.If you insert characters to breaks the tokens down, it find the correct result: \nhow many r's are in \"s\"t\"r\"a\"w\"b\"e\"r\"r\"y\" ?> There are 3 'r's in \"s\"t\"r\"a\"w\"b\"e\"r\"r\"y\".\n \nreply",
      "Where did you get this idea from?Tokens aren\u2019t the source of facts within a model. it\u2019s an implementation detail and doesn\u2019t inherently constrain how things could be counted.\n \nreply",
      ">If you insert characters to breaks the tokens down, it find the correct result: how many r's are in \"s\"t\"r\"a\"w\"b\"e\"r\"r\"y\" ?The issue is that humans don't talk like this. I don't ask someone how many r's there are in strawberry by spelling out strawberry, I just say the word.\n \nreply"
    ],
    "link": "https://mistral.ai/news/mistral-large-2407/",
    "first_paragraph": ""
  }
]