[
  {
    "title": "Intel sells 51% stake in Altera to private equity firm on a $8.75B valuation (intel.com)",
    "points": 183,
    "submitter": "voxadam",
    "submit_time": "2025-04-14T21:59:13 1744667953",
    "num_comments": 123,
    "comments_url": "https://news.ycombinator.com/item?id=43686773",
    "comments": [
      "Without arguing the merits of the Altera investment or divestment, a common pattern for Intel seems to be a wild see-sawing between an aggressive and a defensive market posture - it\u2019s a regular occurrence for Intel to announce a bold new venture to try to claim some new territory, and just as regular that they announce they\u2019re halting that venture in the name of \u201cconsolidating\u201d and \u201cfocusing on their core.\u201d The consequence is that they never give new ventures time to actually succeed, so they just bleed money creating things they murder in the cradle, and nobody born before last Tuesday is investing in bothering to learn the new Intel thing because its expected lifespan is shorter than the average Google product.Intel either needs to focus or they need to be bold (and I\u2019d actually prefer they be bold - they\u2019ve started down some cool paths over time), but what they really need is to make up their goddamn minds and stop panicking every other quarter that their \u201cten-year bets\u201d from last quarter haven\u2019t paid off yet.\n \nreply",
      "Speaking from personal experience, many director-level and above positions at Intel, especially in growth related areas are filled through nepotism and professional connections. I've never seen a headline about Intel\u2019s decline and thought, 'Wow, how could that happen?'\n \nreply",
      "I had a business partner that I agreed on a lot of things with but not about Intel.  My assumption was that any small software package from Intel,  such as a graph processing toolkit,  was trash.  He thought they could do no wrong.Intel really is good at certain kinds of software like compilers or MKL but my belief is that organizations like that have a belief in their \"number oneness\" that gets in their way of doing anything that it outside what they're good at.  Maybe it is the people, processes, organization, values, etc. that gets in the way.  Or maybe not having the flexibility to know that what is good at task A is not good at task B.\n \nreply",
      "See the funny thing is, even with all of this stuff about Intel that I hear about (and agree with as reported), I also just committed a cardinal sin just recently.I'm old, i.e. \"never buy ATI\" is something that I've stuck to since the very early Nvidia days. I.e. switched from Matrox and Voodoo to Nvidia while commiserating and witnessing friend's and colleagues ATI woes for years.The high end gaming days are long gone, even had a time of laptops where 3D graphics was of no concern whatsoever. I happened to have Intel chips and integrated graphics. Could even start up some gaming I missed out on during the years or replay old favourites just fine as even a business laptop Intel integrated graphics chip was fine for it.And then I bought an AMD based laptop with integrated Radeon graphics because of all that negative stuff you hear about Intel and AMD itself is fine, sometimes even better, so I thought it was fair to give it a try.Oh my was that a mistake. AMD Radeon graphics is still the old ATI in full blown problem glory. I guess it's going to be another 25 years until I might make that mistake again.\n \nreply",
      "It's a bummer you've had poor experiences with ATI and later AMD, especially on a new system. I have an AMD laptop with Ryzen 7 7840U which includes a Radeon 780M for integrated graphics and it's been rock solid. I tested many old and new titles on it, albeit at medium-ish settings.What kind of problems did you see on your laptop?\n \nreply",
      "Well back when, literally 25 years ago, when it was all ATI, there were constant driver issues with ATI. I think it's a pretty well known thing. At least was back when.I did think that given ATI was bought out by AMD and AMD itself is fine it should be OK. AMD always was. I've had systems with AMD CPUs and Nvidia GPUs back when it was an actual desktop tower gaming system I was building/upgrading myself. Heck my basement server is still an AMD CPU system with zero issues whatsoever. Of course it's got zero graphics duties.On the laptop side, for a time I'd buy something with discrete Nvidia cards when I was still gaming more actively. But then life happened, so graphics was no longer important and I do keep my systems for a long time / buy non-latest gen. So by chance I've been with Intel for a long time and gaming came up again, casually. The Intel HD graphics were of course totally inadequate for any \"real\" current gaming. But I found that replaying some old favs and even \"newer\" games I had missed out on (new as in, playing a 2013 game for the very first time in 2023 type thing) was totally fine on an Intel iGPU.So when I was getting to newer titles, the Intel HD graphics no longer cut it but I'm still not a \"gamer\" again, I looked at a more recent system and thought I'd be totally fine trying an AMD system. Exactly like another poster said, \"post 2015 should be fine, right?! And then there's all this recent bad news about Intel, this is the time to switch!\".Still iGPU. I'm not going to shell out thousands of dollars here.And then I get the system and I get into Windows and ... everything just looks way too bright, washed out, hard to look at. I doctored around, installed the latest AMD Adrenalin driver, played around with brightness, contract, HDR, color balance, tried to disable the Vari-Brightness I read was supposed to be the culprit etc. It does get worse once you get into a game. Like you're in Windows and it's bearable. Then you start a game and you might Alt-Tab back to do something and everything is just awfully weirdly bright and it doesn't go away when you shut down the game either.I stuck with it and kept doctoring for over 6 months now.I've had enough. I bought a new laptop, two generations behind with an Intel Iris Xe for the same amount of money as the ATI system. I open Windows and ... everything is entirely totally 150% fine, no need to adjust anything. It's comfortable, colors are fine, brightness and contrast are fine. And the performance is entirely adequately the same as with the AMD system. Again, still iGPU and that's fine and expected. It's the quality I'm concerned with, not the performance I'm paying for. I expect to be able to get proper quality software and hardware even if I pay for less performance than gamer kid me back when was willing to.",
      "Not tharkun__:AMD basically stopped supporting (including updating drivers) for GPUs before RDNA (in particular GCN), while such GPUs were still part of AMD's Zen 3 APU offerings.\n \nreply",
      "Did you time travel from 2015 or something?  Haven't heard of anyone having AMD issues in a very long time...\n \nreply",
      "I\u2019ve been consistently impressed with AMD for a while now. They\u2019re constantly undervalued for no reason other than CUDA from what I can tell.\n \nreply",
      "AMD is appropriately valued IMO, Intel is undervalued and Nvidia is wildly overvalued.  We're hitting a wall with LLMs, Nvidia was at one point valued higher than Apple which is insane.Also CUDA doesn't matter that much, Nvidia was powered by intense AGI FOMO but I think that frenzy is more or less done.\n \nreply"
    ],
    "link": "https://newsroom.intel.com/corporate/intel-partner-deal-news-april2025",
    "first_paragraph": ""
  },
  {
    "title": "GPT-4.1 in the API (openai.com)",
    "points": 448,
    "submitter": "maheshrijal",
    "submit_time": "2025-04-14T17:01:45 1744650105",
    "num_comments": 362,
    "comments_url": "https://news.ycombinator.com/item?id=43683410",
    "comments": [
      "As a ChatGPT user, I'm weirdly happy that it's not available there yet. I already have to make a conscious choice between- 4o (can search the web, use Canvas, evaluate Python server-side, generate images, but has no chain of thought)- o3-mini (web search, CoT, canvas, but no image generation)- o1 (CoT, maybe better than o3, but no canvas or web search and also no images)- Deep Research (very powerful, but I have only 10 attempts per month, so I end up using roughly zero)- 4.5 (better in creative writing, and probably warmer sound thanks to being vinyl based and using analog tube amplifiers, but slower and request limited, and I don't even know which of the other features it supports)- 4o \"with scheduled tasks\" (why on earth is that a model and not a tool that the other models can use!?)Why do I have to figure all of this out myself?\n \nreply",
      "> - Deep Research (very powerful, but I have only 10 attempts per month, so I end up using roughly zero)Same here, which is a real shame. I've switched to DeepResearch with Gemini 2.5 Pro over the last few days where paid users have a 20/day limit instead of 10/month and it's been great, especially since now Gemini seems to browse 10x more pages than OpenAI Deep Research (on the order of 200-400 pages versus 20-40).The reports are too verbose but having it research random development ideas, or how to do something particularly complex with a specific library, or different approaches or architectures to a problem has been very productive without sliding into vibe coding territory.\n \nreply",
      "I also like Perplexity\u2019s 3/day limit! If I use them up (which I almost never do) I can just refresh the next day\n \nreply",
      "I use them as follows:o1-pro: anything important involving accuracy or reasoning. Does the best at accomplishing things correctly in one go even with lots of context.deepseek R1: anything where I want high quality non-academic prose or poetry. Hands down the best model for these. Also very solid for fast and interesting analytical takes. I love bouncing ideas around with R1 and Grok-3 bc of their fast responses and reasoning. I think R1 is the most creative yet also the best at mimicking prose styles and tone. I've speculated that Grok-3 is R1 with mods and think it's reasonably likely.4o: image generation, occasionally something else but never for code or analysis. Can't wait till it can generate accurate technical diagrams from text.o3-mini-high and grok-3: code or analysis that I don't want to wait for o1-pro to complete.claude 3.7: occasionally for code if the other models are making lots of errors. Sometimes models will anchor to outdated information in spite of being informed of newer information.gemini models: occasionally I test to see if they are competitive, so far not really, though I sense they are good at certain things. Excited to try 2.5 Deep Research more, as it seems promising.Perplexity: discontinued subscription once the search functionality in other models improved.I'm really looking forward to o3-pro. Let's hope it's available soon as there are some things I'm working on that are on hold waiting for it.\n \nreply",
      "You probably know this and are looking for consistency but, a little trick I use\nis to feed the original data of what I need as a diagram  and to re-imagine, it as an image \u201cready for print\u201d - not native, but still a time saver and just studying with unstructured data or handles this surprisingly well. Again not native\u2026naive, yes. Native, not yet. Be sure to double check triple check as always. give it the ol\u2019 OCD treatment.\n \nreply",
      "Phind was fine-tuned specifically to produce inline Mermaid diagrams for technical questions (I'm the founder).\n \nreply",
      "Gemini 2.5 Pro is quite good at code.Has become my go to for use in Cursor. Claude 3.7 needs to be restrained too much.\n \nreply",
      "You probably know this but it can already generate accurate diagrams. Just ask for the output in a diagram language like mermaid or graphviz\n \nreply",
      "My experience is it often produces terrible diagrams. Things clearly overlap, lines make no sense. I'm not surprised as if you told me to layout a diagram in XML/YAML there would be obvious mistakes and layout issues.I'm not really certain a text output model can ever do well here.\n \nreply",
      "FWIW I think a multimodal model could be trained to do extremely well with it given sufficient training data.  A combination of textual description of the system and/or diagram, source code (mermaid, SVG, etc.) for the diagram, and the resulting image, with training to translate between all three.\n \nreply"
    ],
    "link": "https://openai.com/index/gpt-4-1/",
    "first_paragraph": ""
  },
  {
    "title": "Meta antitrust trial kicks off in federal court (axios.com)",
    "points": 235,
    "submitter": "c420",
    "submit_time": "2025-04-14T13:18:19 1744636699",
    "num_comments": 174,
    "comments_url": "https://news.ycombinator.com/item?id=43680957",
    "comments": [
      "> \"The FTC's lawsuit against Meta defies reality. The evidence at trial will show what every 17-year-old in the world knows: Instagram, Facebook and WhatsApp compete with Chinese-owned TikTok, YouTube, X, iMessage and many others,\" Meta spokesperson Chris Sgro said in a statement.Everyone knew at the time that Facebook bought Instagram because it threatened Facebook's dominance, and hindsight shows that exactly that happened. There's a huge swath of people that dropped off FB and now use Insta, but Meta owns both. It was a great move but it was absolutely anti-competitive at the time.\n \nreply",
      "Stop, this all is much easier to see in hindsight. At the time, the deal was rerided as an overpay and ridiculous.This isnt a personal shot at OP, but even reviewing the majority of the comments here it all feels like pro-tiktok astroturfing.\n \nreply",
      "> Everyone knew at the time that Facebook bought Instagram because it threatened Facebook's dominance, and hindsight shows that exactly that happened.This is something that people can claim to know from hindsight. When Facebook acquired it, Instagram was a photo sharing app that had 13 employees.\n \nreply",
      "The conversations at that time were definitely about how Instagram had the heat that Facebook was losing. I felt there was no question that they were neutralizing a competitor.(WhatsApp only had, like, 50 employees when FB bought it for $19B, as a bit of evidence that headcount isn\u2019t necessarily a measure of value.)\n \nreply",
      "If they bought both Instagram and Snapchat it would have been neutralizing competition. Instagram acquisition was just a business staying relevant after a youth market grew up and soured on them while the next younger ones wanted something else cooler designed for 100% around mobile.I believe intention and behaviour matters much more to antitrust than simply continuing to be a dominant market leader by smartly staying on top of what the public wants. Google search doing horizontal integration into Android and Chrome to cut off competition's market entry points at lower levels is far more plausible antitrust narrative IMO.\n \nreply",
      "If everyone indeed \"knew at the time\" then why did the FTC allow the acquisition to go through in a 5-0 vote?\n \nreply",
      "The government just kinda forgot that competition law existed for a few decades.They were busy doing things like bringing freedom and democracy to Afghanistan, having a financial crisis, stuff like that. Very important stuff. Social media? Oh yes I think my grandson told me about that.\n \nreply",
      "I didn't know the FTC got involved in Afghanistan\n \nreply",
      "FTC does what the public opinion expects them to do.\n \nreply",
      "This is what I don't get, the FTC is suing because the FTC allowed something to happen, when the platforms had even more dominance than they do now?Kind of stinks of less than valid motivations based on the timing of bringing this up over a decade after the fact.\n \nreply"
    ],
    "link": "https://www.axios.com/pro/tech-policy/2025/04/14/ftc-meta-antitrust-trial-kicks-off-in-federal-court",
    "first_paragraph": ""
  },
  {
    "title": "Tomb Engine (tombengine.com)",
    "points": 64,
    "submitter": "ibobev",
    "submit_time": "2025-04-14T22:22:52 1744669372",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=43686936",
    "comments": [
      "The homepage says \"The open-source engine\" but I can't find a link to the source code anywhere on the site. The link that says \"Download\" is a link to some Windows binary.Edit: https://github.com/TombEngine/TombEngine that seems to be the code.\n \nreply",
      "MIT license. Open Source indeed.\n \nreply",
      "This is more interesting: https://github.com/MontyTRC89/Tomb-Editor (the level editor that goes with the engine)What made the eternal life of Quake has been its open source level editor which means designers can have a field day making new levels.The editor is as important as the engine, if not more. Exciting stuff!\n \nreply",
      "This is so cool! I hope it starts some kind of new genre or creator community. Maybe a souls-like take on TR as well? Or some unexpected mashup with another game? So many ideas come to mind!\n \nreply",
      "This looks awesome.On a side note, this looks like it could be used to make a great Armored Core like haha\n \nreply",
      "From the footer:> \"TombEngine is not be sold.\"Ironically, perhaps, this makes me trust it more.\n \nreply",
      "Seems at odds with their repo's MIT license, which explicitly allows it to be sold.https://github.com/TombEngine/TombEngine/blob/master/LICENSE\n \nreply",
      "I guess it's ok to build it and sell it but not to sell the binaries provided? It's odd indeed. Most likely split brain decision.\n \nreply",
      "Most likely it's from inexperience with licensing. Many people choose MIT because it's the most popular open source license, without really thinking about how permissive it is and whether they want that for their project.Maybe someone from this thread could open an issue and suggest they clarify this.I don't personally know enough about licensing to say whether a sentence in the README.md (saying it can't be sold) is enough to override the LICENSE.md (which says it can be sold).Personally I'd always choose a copyleft license for something like this.\n \nreply",
      "Been following their Discord for a while. It's pretty impressive looking. I was waiting until TR2 had a decent fan port. Now with the official remasters I don't have time to play them.Still great to see fans taking the games in directions publishers won't, like with MP\n \nreply"
    ],
    "link": "https://tombengine.com/",
    "first_paragraph": "The open-source engine for custom Tomb Raider adventuresThis is a community project which is not affiliated with Core Design, Eidos Interactive, or Embracer Group AB. Tomb Raider is a registered trademark of Embracer Group AB. TombEngine is not be sold. The code is open-source to encourage contributions and to be used for study purposes. We are not responsible for illegal uses of this source code. This source code is released as-is and continues to be maintained by non-paid contributors in their free time."
  },
  {
    "title": "The Cost of Being Crawled: LLM Bots and Vercel Image API Pricing (metacast.app)",
    "points": 55,
    "submitter": "navs",
    "submit_time": "2025-04-14T23:33:38 1744673618",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=43687431",
    "comments": [
      "$5 to resize 1,000 images is ridiculously expensive.At my last job we resized a very large amount of images every day, and did so for significantly cheaper (a fraction of a cent for a thousand images).Am I missing something here?\n \nreply",
      "It's the usual PaaS convenience tax, you end up paying an order of magnitude or so premium for the underlying bandwidth and compute. AIUI Vercel runs on AWS so in their case it's a compound platform tax, AWS is expensive even before Vercel adds their own margin on top.\n \nreply",
      "I would call it ignorance tax, paas can be fine if you know what you are doing.\n \nreply",
      "(I work at Vercel) We moved to a transformation-based price: https://x.com/TheBuildLog/status/1892308957865111918\n \nreply",
      "Sweet! That's much more reasonable!\n \nreply",
      "Absolutely insane pricing, maybe for small blogs, but didn\u2019t they calculate this trough?Millions of episode, of course they will be visited and the optimization is run.\n \nreply",
      "Death by stupid micro services. Even at 1.5 mil pages, and the traffic they are talking about this could easily be hosted on a a fixed $80/month linode.\n \nreply",
      "The issue is Vercel Image API is ridiculously expensive and also not efficient.I would recommend using Thumbor instead: https://thumbor.readthedocs.io/en/latest/. You could have ChatGPT write up a React image wrapper pretty quickly for this.\n \nreply",
      "(I work at Vercel) While it's good our spend limits worked, it clearly was not obvious how to block or challenge AI crawlers\u00b9 from our firewall (which it seems you manually found). We'll surface this better in the UI, and also have more bot protection features coming soon. Also glad our improved image optimization pricing\u00b2 would have helped. Open to other feedback as well, thanks for sharing.\u00b9: https://vercel.com/templates/vercel-firewall/block-ai-bots-f...\u00b2: https://vercel.com/changelog/faster-transformations-and-redu...\n \nreply",
      "I'm sure what you can share is limited, as I'm guessing this is cat and mouse. That being said, is there anything you can share about your implementation?\n \nreply"
    ],
    "link": "https://metacast.app/blog/engineering/postmortem-llm-bots-image-optimization",
    "first_paragraph": "A misconfiguration that might have cost us $7,000Table of ContentsOn Friday, Feb 7, 2025 we had an incident with our Next.js web app hosted on Vercel that could've cost us $7,000 if we didn't notice it in time.We had a spike in LLM bot traffic coming from Amazonbot, Claudebot, Meta and an unknown bot. Together they sent 66.5k requests to our site within a single day. Bots scraped thousands of images that used Vercel's Image Optimization API, which cost us $5 per 1k images.The misconfiguration on our side combined with the aggressive bot traffic created an economically risky situation for our tiny bootstrapped startup.Metacast is a podcast tech startup. Our main product is a podcast app for iOS and Android.For every podcast episode on the platform, our web app has a web page. Our platform has ~1.4M episodes, which means we have 1.4M web pages that are discoverable by crawlers. These pages are generated server-side at request time, then cached.First, we received a cost alert from Vercel "
  },
  {
    "title": "The path to open-sourcing the DeepSeek inference engine (github.com/deepseek-ai)",
    "points": 367,
    "submitter": "Palmik",
    "submit_time": "2025-04-14T15:03:10 1744642990",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=43682088",
    "comments": [
      "In March, vLLM picked up some of the improvements in the DeepSeek paper. Through these, vLLM v0.7.3's DeepSeek performance jumped to about 3x+ of what it was before [1].What's exciting is that there's still so much room for improvement. We benchmark around 5K total tokens/s with the sharegpt dataset and 12K total token/s with random 2000/100, using vLLM and under high concurrency.DeepSeek-V3/R1 Inference System Overview [2] quotes \"Each H800 node delivers an average throughput of 73.7k tokens/s input (including cache hits) during prefilling or 14.8k tokens/s output during decoding.\"Yes, DeepSeek deploys a different inference architecture. But this goes onto show just how much room there is for improvement. Looking forward to more open source![1] https://developers.redhat.com/articles/2025/03/19/how-we-opt...[2] https://github.com/deepseek-ai/open-infra-index/blob/main/20...\n \nreply",
      "I really empathised with this part:> Codebase Divergence: Our engine is based on an early fork of vLLM from over a year ago. Although structurally similar, we\u2019ve heavily customized it for DeepSeek models, making it difficult to extend for broader use cases.I've been there. Probably a few of us have.Their approach of working on splitting out maintainable sublibraries and sharing info directly even if not integrated seems a really nice way of working with the community -- ie, they have obstacles, but they're not letting the obstacles cause them to take the easy route of not contributing at all. And while it might seem better to someone wanting to use their techniques to share only working code, not info on the techniques, at least it's still knowledge sharing. And again I think it'd be easier for them not to do it. So kudos to them.\n \nreply",
      "Non-runnable code can be really useful. I often wish it was available for some papers even if I never run it just to check what they actually did, because text and equations are often not specific enough.\n \nreply",
      "They customized and optimized vLLM for their use case, so much that it became a different product (e.g. Debian vs Ubuntu).The fact they share back some of their improvements is great.\n \nreply",
      "What motivates the commercial AI companies to share their research results and know-how?Why did Google published the Transformer architecture instead of keeping it to themselves?I understand that people may want to do good things for humanity, facilitate progress, etc. But if an action goes against commercial interest, how can the company management take it and not get objections from shareholders?Or there is a commercial logic that motivates sharing of information and intellectual property? What logic is that?\n \nreply",
      "My understanding is that frontier researchers will work for companies that will let them publish papers and discuss them with their peers.When you're an engineer at the tier of these AI researchers, winning an extra 100k/year on top of you current 500k (numbers out of my ass) is not worth it vs getting name recognition. Being known as one of the authors that made the transformer for example will enable you work with other bright minded individuals and create even better things.So essentially these commercial companies have \"we'll let you publish papers when you work for us\" as a perk.\n \nreply",
      "> When you're an engineer at the tier of these AI researchers, winning an extra 100k/year on top of you current 500k (numbers out of my ass) is not worth it vs getting name recognition. Being known as one of the authors that made the transformer for example will enable you work with other bright minded individuals and create even better things.Also, instead of an extra 100k a year, you get to raise a billion dollars in VC funds for your next company\n \nreply",
      "Indeed, is there a chance Google did not evaluate properly what the transformer will eventually be used for/become. It was created for translation as an improvement on seq2seq, right? Which was for translation, not for thinking, and to a certain extent... still is about translation, and are not other emergent capabilities actually a side-effect, only observed later when parameter size grew?\n \nreply",
      "This may be related to Google's business model. Google's main businesses - search engine and advertising - both rely on an open web ecosystem. Therefore, Google has long maintained a friendly attitude toward open source and the open web, such as with Chromium, Noto fonts, Go, Flutter, and others. By providing infrastructure tools that benefit the open web, Google extends the reach of its searchable content and advertising. When the entire Web ecosystem benefits, Google ultimately benefits as well. This model also aligns with the philosophy of the open source community, where everyone is a beneficiary and naturally becomes a contributor.\n \nreply",
      "I would guess it comes down to that the best researchers in the world want their work out in the open\n \nreply"
    ],
    "link": "https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          A few weeks ago,\nduring Open Source Week,\nwe open-sourced several libraries.\nThe response from the community has been incredibly positive - sparking inspiring collaborations, productive\ndiscussions, and valuable bug fixes.\nEncouraged by this, we\u2019ve decided to take another step forward: contributing our internal inference engine back to the\nopen-source community.We are deeply grateful for the open-source ecosystem, without which our progress toward AGI would not be possible.\nOur training framework relies on PyTorch, and our inference engine is built\nupon vLLM,\nboth of which have been instrumental in accelerating the training and deployment of DeepSeek models.Given the growing demand for deploying models like DeepSeek-V3\nand DeepSeek-R1, we want to give back to the community as much as we can.\nWhile we initially considered open-sourcing our fu"
  },
  {
    "title": "A hackable AI assistant using a single SQLite table and a handful of cron jobs (geoffreylitt.com)",
    "points": 470,
    "submitter": "stevekrouse",
    "submit_time": "2025-04-14T13:52:58 1744638778",
    "num_comments": 120,
    "comments_url": "https://news.ycombinator.com/item?id=43681287",
    "comments": [
      "I don't know if I love this more for the sheer usefulness, or for the delightful over-the-top \"Proper English Butler\" diction.But what really has my attention is: Why is this something I'm reading about on this smart engineer's blog rather than an Apple or Google product release? The fact that even this small set of features is beyond the abilities of either of those two companies to ship -- even with caveats like \"Must also use our walled garden ecosystem for email, calendars, phones, etc\" -- is an embarrassment, only obscured by the two companies' shared lack of ambition to apply \"AI\" technology to the 'solved problem' areas that amount to various kinds of summarization and question-answering.If ever there was a chance to threaten either half of this lumbering, anticompetitive duopoly, certainly it's related to AI.\n \nreply",
      "There\u2019s actually a good answer to this, namely that narrowly targeting the needs of exactly one family allows you to develop software about 1000x faster. This is an argument in favor of personal software.\n \nreply",
      "The Apple walled garden argues against you here. There are at least 20 million families in America where this holds true:\u2022 Everyone in household uses an iPhone\u2022 Main adult family members use iCloud Mail or at least use Apple Mail to read other mail\u2022 Family members use iCloud contacts and calendars\u2022 USPS Informed Delivery could be used (available to most/all US addresses)\u2022 It can be ascertained what ZIP code you're in, for weather.I think that's the full list of 'requirements' this thing would require. Just what's standing in their way?\n \nreply",
      "This made me think: what if my little utility assistant program that I have, similar to your Stevens, had access to a mailbox?I've got a little utility program that I can tell to get the weather or run common commands unique to my system.  It's handy, and I can even cron it to run things regularly, if I'd like.If it had its own email box, I can send it information, it could use AI to parse that info, and possibly send email back, or a new message.  Now, I've got something really useful.  It would parse the email, add it to whatever internal store it has, and delete the message, without screwing up my own email box.Thanks for the insight.\n \nreply",
      "I\u2019ve been thinking lately that email is a good interface for certain modes of AI assistant interaction, namely \u201cresearch\u201d tasks that are asynchronous and take a relatively long time. Email is universal, asynchronous, uses open standards, supports structured metadata, etc.\n \nreply",
      "This is how I initially pitched an AI assistant in my last shop.It is a lot cheaper to leverage existing user interfaces & tools (i.e., Outlook) than it is to build new UIs and then train users on them.\n \nreply",
      "Also an email that comes back a minute later feels fast. A chat that types at the same speed feels slow.\n \nreply",
      "I've build adaptive agent swarms using email, mailing lists and ftp servers.If you don't need to have the lowest possible latency for your work and you're happy to have threads die then it's better than any bespoke solution you can build without an army of engineers to keep it chugging along.What's even better is that you can see all the context, and use the same command plane as the agents to tell them what they are doing wrong.\n \nreply",
      "yep went down a rabbit hole trying to build a company around this. it\u2019s the perfect UItext + attachments into the system, text + attachments out\n \nreply",
      "> trying to build a company around thisI am still very open to this one. An email-based, artificial coworker is so obviously the right way to penetrate virtually every B2B market in existence.I don't even really want to touch the technology aspects. Writing code that integrates with an LLM provider and a mailbox in E365 or Gmail is boring. The schema is a grand total of ten tables if we're being pedantic about things.Working with prospects and turning them into customers is a way more interesting problem. I hunger for tangible use cases that are actually compatible with this shiny new LLM tooling. We all know they're out there, and email is probably the lowest friction way to get them applied to most businesses.\n \nreply"
    ],
    "link": "https://www.geoffreylitt.com/2025/04/12/how-i-made-a-useful-ai-assistant-with-one-sqlite-table-and-a-handful-of-cron-jobs",
    "first_paragraph": "There\u2019s a lot of hype these days around patterns for building with AI. Agents, memory, RAG, assistants\u2014so many buzzwords! But the reality is, you don\u2019t need fancy techniques or libraries to build useful personal tools with LLMs.In this short post, I\u2019ll show you how I built a useful AI assistant for my family using a dead simple architecture: a single SQLite table of memories, and a handful of cron jobs for ingesting memories and sending updates, all hosted on Val.town. The whole thing is so simple that you can easily copy and extend it yourself.The assistant is called Stevens, named after the butler in the great Ishiguro novel Remains of the Day. Every morning it sends a brief to me and my wife via Telegram, including our calendar schedules for the day, a preview of the weather forecast, any postal mail or packages we\u2019re expected to receive, and any reminders we\u2019ve asked it to keep track of. All written up nice and formally, just like you\u2019d expect from a proper butler.Here\u2019s an example"
  },
  {
    "title": "What Is Entropy? (jasonfantl.com)",
    "points": 134,
    "submitter": "jfantl",
    "submit_time": "2025-04-14T18:32:08 1744655528",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=43684560",
    "comments": [
      "I don\u2019t see Sean Carroll\u2019s musings mentioned yet, so repeating my previous comment:Entropy got a lot more exciting to me after hearing Sean Carroll talk about it. He has a foundational/philosophical bent and likes to point out that there are competing definitions of entropy set on different philosophical foundations, one of them seemingly observer dependent:\n- https://youtu.be/x9COqqqsFtc?si=cQkfV5IpLC039Cl5\n- https://youtu.be/XJ14ZO-e9NY?si=xi8idD5JmQbT5zxNLeonard Susskind has lots of great talks and books about quantum information and calculating the entropy of black holes which led to a lot of wild new hypotheses.Stephen Wolfram gave a long talk about the history of the concept of entropy which was pretty good: https://www.youtube.com/live/ocOHxPs1LQ0?si=zvQNsj_FEGbTX2R3\n \nreply",
      "I'm not in any way qualified to have a take here, but I have one anyway:My understanding is that entropy is a way of quantifying how many different ways a thing could 'actually be' and yet still 'appear to be' how it is. So it is largely a result of an observer's limited ability to perceive / interrogate the 'true' nature of the system in question.So for example you could observe that a single coin flip is heads, and entropy will help you quantify how many different ways that could have come to pass. e.g. is it a fair coin, a weighted coin, a coin with two head faces, etc. All these possibilities increase the entropy of the system. An arrangement _not_ counted towards the system's entropy is the arrangement where the coin has no heads face, only ever comes up tails, etc.Related, my intuition about the observation that entropy tends to increase is that it's purely a result of more likely things happening more often on average.Would be delighted if anyone wanted to correct either of these intuitions.\n \nreply",
      "> My understanding is that entropy is a way of quantifying how many different ways a thing could 'actually be' and yet still 'appear to be' how it is. So it is largely a result of an observer's limited ability to perceive / interrogate the 'true' nature of the system in question.When ice cubes in a glass of water slowly melt, and the temperature of the liquid water decreases, where does the limited ability of an observer come into play?It seems to me that two things in this scenario are true:1) The fundamental physical interactions (i.e. particle collisions) are all time-reversible, and no observer of any one such interaction would be able to tell which directly time is flowing.2) The states of the overall system are not time-reversible.\n \nreply",
      "It's tricky when you think of a continuous system because the \"differential entropy\" is different (and more subtle) than the \"entropy\". Even if a system is time-reversible, the \"measure\" of a set of states can change.For example: Say I'm at some distance from you, between 0 and 1 km (all equiprobable). Now I switch to being 10x as far away. This is time-reversible, but because the volume of the set of states changed, the differential entropy changes. This is the kind of thing that happens in time-reversible continuous systems that can't happen in time-reversible discrete systems.\n \nreply",
      ">purely a result of more likely things happening more often on averageaccording to your wording, no. if you have a perfect six sided die (or perfect two sided coin), none/neither of the outcomes are more likely at any point in time... yet something approximating entropy occurs after many repeated trials. what's expected to happen is the average thing even though it's never the most likely thing to happen.you want to look at how repeated re-convolution of a function with itself always converges on the same gaussian function, no matter the shape of the starting function is (as long as it's not some pathological case, such as an impulse function... but even then, consider the convolution of the impulse function with the gaussian)\n \nreply",
      "This is based on entropy being closely tied to your knowledge of the system. It's one of many useful definitions of entropy.\n \nreply",
      "> 'actually be' and yet still 'appear to be'esse quam videri\n \nreply",
      "I'm not sure I understand the distinction between \"high-entropy macrostate\" and \"order\". Aren't macrostates just as subjective as order? Let's say my friend's password is 6dVcOgm8. If we have a system whose microstate consists of an arbitrary string of alphanumeric characters, and the system arranges itself in the configuration 6dVcOgm8, then I would describe the macrostate as \"random\" and \"disordered\". However, if my friend sees that configuration, they would describe the macrostate as \"my password\" and \"ordered\".If we see another configuration M2JlH8qc, I would say that the macrostate is the same, it's still \"random\" and \"unordered\", and my friend would agree. I say that both macrostates are the same: \"random and unordered\", and there are many microstates that could be called that, so therefore both are microstates representing the same high-entropy macrostate. However, my friend sees the macrostates as different: one is \"my password and ordered\", and the other is \"random and unordered\". There is only one microstate that she would describe as \"my password\", so from her perspective that's a low-entropy macrostate, while they would agree with me that M2JlH8qc represents a high-entropy macrostate.So while I agree that \"order\" is subjective, isn't \"how many microstates could result in this macrostate\" equally subjective? And then wouldn't it be reasonable to use the words \"order\" and \"disorder\" to count (in relative terms) how many microstates could result in the macrostate we subjectively observe?\n \nreply",
      "One thing that helped me was the realization that, at least as used in the context of information theory, entropy is a property of an individual (typically the person receiving a message) and NOT purely of the system or message itself.> entropy quantifies uncertaintyThis sums it up. Uncertainty is the property of a person and not a system/message. That uncertainty is a function of both a person's model of a system/message and their prior observations.You and I may have different entropies about the content of the same message. If we're calculating the entropy of dice rolls (where the outcome is the 'message'), and I know the dice are loaded but you don't, my entropy will be lower than yours.\n \nreply",
      "Not true. The uncertainty of the dice rolls is not controlled by you. It is the property of the loaded dice itself.Here's a better way to put it. If I roll the dice infinite times. The uncertainty of the outcome of the dice will become evident in the distribution of the outcomes of the dice. Whether you or another person is certain or uncertain of this does not indicate anything.Now when you realize this you'll start to think about this thing in probability called frequentists vs. bayesian and you'll realize that all entropy is, is a consequence of probability and that the philosophical debate in probability applies to entropy as well because they are one and the same.I think the word \"entropy\" confuses people into thinking it's some other thing when really it's just probability at work.\n \nreply"
    ],
    "link": "https://jasonfantl.com/posts/What-is-Entropy/",
    "first_paragraph": "People say many things about entropy: entropy increases with time, entropy is disorder, entropy increases with energy, entropy determines the arrow of time, etc.. But I have no idea what entropy is, and from what I find, neither do most other people. This is the introduction I wish I had when first told about entropy, so hopefully you find it helpful. My goal is that by the end of this long post we will have a rigorous and intuitive understanding of those statements, and in particular, why the universe looks different when moving forward through time versus when traveling backward through time.This journey begins with defining and understanding entropy. There are multiple formal definitions of entropy across disciplines\u2014thermodynamics, statistical mechanics, information theory\u2014but they all share a central idea: entropy quantifies uncertainty. The easiest introduction to entropy is through Information Theory, which will lead to entropy in physical systems, and then finally to the relati"
  },
  {
    "title": "How I Don't Use LLMs (gleech.org)",
    "points": 3,
    "submitter": "jxmorris12",
    "submit_time": "2025-04-15T01:06:20 1744679180",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.gleech.org/llms",
    "first_paragraph": "I enjoy shocking people by telling them I don\u2019t use LLMs.This isn\u2019t true, but it\u2019s morally true for the reference class I\u2019m in (people who wrote a book about em, 2024 AI PhD, ML twitter member in good standing, trying to do intellectual work on a deadline).Attack ships on fire off the shoulder of Orion bright as magnesiumI was there when GBMs still beat neural networks on tabular data.I was there for Keras 1.0, and trained 100-class image recognisers on a Sandy Bridge Xeon CPU (and ok later a single Titan GPU, once Procurement got their shit together).I was there when GANs started working. I was there when GANs \"stopped\" working.I was there when OpenAI was a pure RL lab, and when OpenAI was a lab and not a company.I was there when BERT hit and sounded the death knell for entire subfields of NLP.I was there when GPT-2 prompted with \"tl;dr\" destroyed hundreds of corporate data science projects on summarisation.I've spent a hundred hours with various of them, but almost entirely in roboan"
  },
  {
    "title": "AudioX: Diffusion Transformer for Anything-to-Audio Generation (zeyuet.github.io)",
    "points": 108,
    "submitter": "gnabgib",
    "submit_time": "2025-04-14T17:35:11 1744652111",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=43683907",
    "comments": [
      "The toilet flushing one is full of weird, unrelated noises.The tennis video, as other commented, is good but there is a noticeable delay between the action and the sound.\nAnd the \"loving couple holding IA hands and then dancing\", well, the input is already cringe enough.For all these diffusion models, look like we are 90% here, now we just need the final 90%.\n \nreply",
      "really the next big leap is something that gives me more meaningful artistic control over these systems.It's usually \"generate a few, one of them is not terrible, none are exactly what I wanted\" then modify the prompt, wait an hour or so ...The workflow reminds me of programming 30 years ago - you did something, then waited for the compile, see if it worked, tried something else...All you've got are a few crude tools and a bit of grit and patience.On the i2v tools I've found that if I modify the input to make the contrast sharper, the shapes more discrete, the object easier to segment, then I get better results. I wonder if there's hacks like that here.\n \nreply",
      "> The workflow reminds me of programming 30 years ago - you did something, then waited for the compile, see if it worked, tried something else...Well sure... if your compiler was the equivalent of the Infinite Improbability Drive.I assume you're referring to the classic positive/negative prompts that you had to attach to older SD 1.5 workflows. From the examples in the repo as well as the paper, it seems like AudioX was trained to accept relatively natural english using Qwen2.\n \nreply",
      "no, I'm talking pretty recent stuff. I was dealing with https://huggingface.co/bytedance-research/UNO and https://huggingface.co/HiDream-ai/HiDream-I1-Full earlier todayThese are both released this month.What I'd like to see is some kind of i2i with multiple i input and guidanceSo I can roughly sketch, and I don't mean controlnet or anything where I'm dealing with complex 3d characters, but give some kind of destination - and I don't mean the crude stuff that inpainting gives ... none of these things are what I'm talking about.i'm familiar with the comfyui workflows and stay pretty on top of things. I've used the krita and photoshop plugin and even have built a civitai mcp server for bringing in models. AFAIK nobody else has done this yet.None of these are hands on in the right way.\n \nreply",
      "The video to audio examples are really impressive! The video featuring the band showcases some of the obvious shortcomings of this method (humans will have very precise expectations about the kinds of sounds 5 trombones will make)\u2014but the tennis example shows its strengths (decent timing of hit sounds, eerily accurate acoustics for the large internal space). I'm very excited to see how this improves a few more papers down the line!\n \nreply",
      "There were a lot of shortcomings.- The woman playing what I think was an Erhu[1] seemed to be imitating traditional music played by that instrument, but really badly (it sounded much more like a human voice than the actual instrument does). Also, I'm not even sure if it was able to tell which instrument it was, or if it was picking up on other cues from the video (which could be problematic, e.g. if it profiles people based on their race and attire)- Most of the sound was pretty delayed from the visual cues. Not sure why- The nature sounds were pretty muddy- (I realize this is from video to music, but) the video with pumping upbeat music set to the text \"Maddox White witnessed his father getting butchered by the Capo of the Italian mob\" was almost comically out of touch with the sourceNevertheless, it's an interesting demo and highlights more applications for AI which I'm expecting we'll see massive improvements in over the next few years! So despite the shortcomings I agree it's still quite impressive.[1] https://en.wikipedia.org/wiki/Erhu\n \nreply",
      "That \"pseudo-human laughter\" gave me some real chills; didn't realize uncanny valley for audio is a real thing but damn...\n \nreply",
      "Sometimes when I lie awake at night I wonder what it is about things that are \"almost human\" that terrifies so many of us so deeply.It's like the markings on the back of tiger's heads that simulate eyes to prevent predators from attacking it. I'm sure there used to be something that tigers benefited from having this defense for enough for it to survive encoding into their DNA, right?So, what was it that encoded this fear response into us?\n \nreply",
      "Dead things, and behaviors that don't align with our predictive models shift the context to one of threat - if something shaped like something you understand starts behaving in a way that you no longer understand, you'll become progressively more concerned. If a pencil started rolling around aggressively chasing you, it'd evoke fear, even though you'd probably defend yourself fairly capably.If enough predictive models are broken, people feel like they've gone crazy - various drugs and experiments demonstrate a lot of these factors.The interesting thing about uncanny valley is that the stimuli are on a threshold, and humans are really good at picking up tiny violations of those expectations, which translates to unease or fear.\n \nreply",
      "Other hominids as well as visibly diseased humans.\n \nreply"
    ],
    "link": "https://zeyuet.github.io/AudioX/",
    "first_paragraph": "\n                Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audi"
  },
  {
    "title": "The Wisconsin cartographer who mapped Tolkien's fantasy world (wpr.org)",
    "points": 126,
    "submitter": "bookofjoe",
    "submit_time": "2025-04-11T17:26:49 1744392409",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=43656267",
    "comments": [
      "Atlas of Middle-earth is a truly monumental feat.I think the article writer misses how much of it is really about The Silmarillion, rather than about Lord of the Rings. Tolkien put a lot of work into First Age geography, an entire (interminable, excruciating) chapter of The Silmarillion. Very little of it would be familiar to viewers of the films, and a lot of it opaque even to readers just of LotR.\n \nreply",
      "> an entire (interminable, excruciating) chapter of The SilmarillionI\u2019ve read The Silmarillion easily more than 20 times and I swear Of Beleriand and its Realms gets longer every time I read it.\n \nreply",
      "I just wish it had been relegated to an appendix. A lot of people drop Silmarillion there, but you can just skip it and get on to much better material.It could be replaced on first read with a decent map. Or even a mediocre map. Or nothing; you just don't need it.\n \nreply",
      "Most of the Silmarillion can be read out of order, if you want. That should have been made more clear to the \"casual\" reader (as if casual readers pick it up!).\n \nreply",
      "> I just wish it had been relegated to an appendix.It was; it wasn't even published.\n \nreply",
      "I have The Silmarillion on Audible and use the chapter Of Beleriand and its Realms when I'm having trouble going to sleep.\n \nreply",
      "My favorite parts of the Silmarillion were the ones where I learned the back story of the world: the Valaquenta, the Ainulindal\u00eb, and Of the Rings of Power and the Third Age. I don't have my copy here, but if I recall correctly that last section starts with Of old there was Sauron the Maia.... That's the stuff I wanted to know.\n \nreply",
      "I was also impressed by Tolkien's creation myth. I'm not really familiar with the various religions or mythologies, whether invented by a single author or developed over time by pre-scientific societies, but his is the only one I know of where the creation was based on music.\n \nreply",
      "A wonderful atlas -- my favorite are her trail maps where she depicts a character's daily journey in the book.The article (almost) footnotes her other work which was equally impressive:* She also created atlases for the worlds of fantasy authors Anne McCaffrey, creator of the \u201cDragonriders of Pern\u201d series, and Stephen R. Donaldson, who wrote \u201cThe Chronicles of Thomas Covenant\u201d series.\n \nreply",
      "I love fantasy in general, and have read a ton of it. other than tolkien, I have never read a novel with that strong a sense of geography in a constructed world - specifically, that there is an entire rich land out there, and not just a graph of interesting places with the focus shifting from one point to another. when the hobbits have to go from the shire to rivendell, or aragorn has to take the paths of the dead to reach his destination in time, tolkien really manages to convey the experience of a difficult journey that takes a significant amount of time even when nothing plot-significant is happening along the way.\n \nreply"
    ],
    "link": "https://www.wpr.org/news/wisconsin-cartographer-karen-wynn-fonstad-mapped-tolkien-fantasy-world-oshkosh",
    "first_paragraph": "\n\t\u2018Wisconsin Today\u2019 explores the legacy of Oshkosh\u2019s Karen Wynn Fonstad and her 1981 book, \u2018The Atlas of Middle-earth\u2019If you\u2019ve ever wanted to explore the world of \u201cThe Hobbit\u201d and \u201cThe Lord of the Rings,\u201d the best place to start might be Oshkosh.That\u2019s where a Wisconsin cartographer created dozens of maps that went into \u201cThe Atlas of Middle-earth,\u201d the official geographic guide to the world of author J.R.R. Tolkien. Her work went on to influence \u201cThe Lord of the Rings\u201d movie trilogy.Sign up for WPR\u2019s email newsletter.Like many readers, Karen Wynn Fonstad fell in love with the fantasy series and went through multiple readings. Unlike most readers, she was trained as a cartographer, and came up with an ambitious plan to use the texts to create realistic maps from Tolkien\u2019s texts.Fonstad\u00a0passed away 20 years ago. Now, her husband and her son \u2014 both geographers themselves \u2014 have embarked on a new quest: to digitize her original maps and find an archive to house them.Rob Ferrett and Beatri"
  },
  {
    "title": "Stripe's payment API: The first 10 years (2020) (stripe.com)",
    "points": 9,
    "submitter": "lispybanana",
    "submit_time": "2025-04-12T07:14:47 1744442087",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://stripe.com/blog/payment-api-design",
    "first_paragraph": "\nOnline payments\n\nIn-person payments\n\nFraud prevention\n\nAcceptance optimizations\n\nPayments for platforms\n\nFinancial accounts\n\nCustomer financing\n\nPhysical and virtual cards\n\nSubscriptions and usage-based\n\nAccounting automation\n\nSales tax & VAT automation\n\nOnline invoices\n\nCustom reports\n\nData sync\n\nAccess to 100+ globally\n\nAccelerated checkout\n\nLinked financial account data\n\nOnline identity verification\n\nStartup incorporation\n\nOnline payments\n\nIn-person payments\n\nFraud prevention\n\nAcceptance optimizations\n\nPayments for platforms\n\nFinancial accounts\n\nCustomer financing\n\nPhysical and virtual cards\n\nSubscriptions and usage-based\n\nAccounting automation\n\nSales tax & VAT automation\n\nOnline invoices\n\nCustom reports\n\nData sync\n\nAccess to 100+ globally\n\nAccelerated checkout\n\nLinked financial account data\n\nOnline identity verification\n\nStartup incorporation\n\nCarbon removal\nStart integrating Stripe\u2019s products and toolsA few years ago, Bloomberg Businessweek published a feature story on Stripe. Fo"
  },
  {
    "title": "AI used for skin cancer checks at London hospital (bbc.com)",
    "points": 79,
    "submitter": "chris_overseas",
    "submit_time": "2025-04-12T07:43:47 1744443827",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=43662263",
    "comments": [
      "Anecdotally, when I had a potential skin cancer checked at a London hospital they were completely ill-prepared.When I came back to Australia, it was checked and immediately removed as an obvious melanoma.Perhaps the idea of Comparative Advantage also applies to healthcare between countries with natural variances to types of disease?\n \nreply",
      "Australia, Queensland, Brisbane has the highest incidence of skin cancer in World.   So Drs and dermatologists would have more experience detecting it here.   UV is probably much less intense in England .   This link graph shows skin cancer is over 2x in Australia\n     https://www.statista.com/statistics/1032114/countries-with-t...\n \nreply",
      "Sadly, a friend of mine died from this type of problem. He traveled from Ghana to Jordan and fell ill in Jordan, the Jordanian doctors didn't diagnose it as malaria in time to save him because Jordan doesn't have malaria. I'm sure it would have been obvious to a Ghanaian doctor.\n \nreply",
      "Friend did a lot of work in Africa. Got back to the US and starts getting symptomatic and says to himself \u201cI think I got malaria\u201d.Goes to the biggest university hospital nearby he can find.Was initially dismissed but waited it out for the infectious diseases specialist and they quickly agreed with his self-diagnosis.They kept them in hospital for a few days so a parade of clinical students/residents could come by for the specialist to say: \u201cthis man has malaria\u201d\n \nreply",
      "Weird. Pretty much every time I've gone to a doctor (in the Midwest US, no less) for an illness, the second question they ask is always \"have you been out of the country recently?\" just to rule out anything caught from abroad.Then again, my one experience with a university hospital was pretty shitty, so maybe that's it?\n \nreply",
      "I came back from Bali and shortly after came down with a really high fever when I was on vacation in Maine. When I got home week later called my primary as I was still running a bit of a fever and got blood work which was pretty bad. Went to ER and my primary care figured it was something tick related. The infectious disease guy on call immediately identified as Dengue Fever. At the end, including sending blood to the Louis Pasteur Institute in Paris I spent something south of $1K to get told I\u2019d recover on my own\u2014the Paris tests came back like weeks later\u2014and there really wasn\u2019t much they could do.\n \nreply",
      "When I first heard the clinical idiom \"When you hear hoof-beats, think horses not zebras\" I thought it was a precautionary saying about the bias toward assuming the familiar. But it's meant to be instructive!\n \nreply",
      "Being a pasty Brit, going to Australia was a real eye opener in how much more on the ball they were about skin cancer, not just in medical terms but culturally. We're getting better here (I was there more than a decade ago) but it's still seen as quite amusing when people get sunburnt here.\n \nreply",
      "Awareness started in the 1980s in Australia with Sid the Seagull:https://www.youtube.com/watch?v=tGgn5nwYtj0https://www.abc.net.au/btn/classroom/sun-safety-campaign/103...\n \nreply",
      "Speaking with some 70 year olds, their opinion on the \"best doctor in town for skincare\" was basically a doctor who'd simply cut out whatever you like and send it for a biopsy.At most you had to deal with a stitch or two but often only a bandaid. Nowadays the hydrocolloid bandages seem magic.\n \nreply"
    ],
    "link": "https://www.bbc.com/news/articles/czd3ygd7mrno",
    "first_paragraph": "An NHS hospital in west London is pioneering the use of Artificial Intelligence (AI) to help check for skin cancer.Chelsea and Westminster Hospital said its AI technology has been approved to give patients the all-clear without having to see a doctor.Once photos are uploaded to the system, the technology analyses and interprets the images, with 99% accuracy in diagnosing benign cases, the hospital said.Thousands of NHS patients have had urgent cancer checks using the AI tool, freeing up consultants to focus on the most serious cases and bringing down waiting lists.The system conducts the checks in minutes, with medical photographers taking photos of suspicious moles and lesions using an iPhone and the DERM app, developed by UK firm Skin Analytics.The images are then transferred to a desktop computer for greater analysis before the tool determines the result.Patients with benign cases are then discharged without any input from a specialist, except in a small number of cases.The hospital"
  },
  {
    "title": "DolphinGemma: How Google AI is helping decode dolphin communication (blog.google)",
    "points": 247,
    "submitter": "alphabetting",
    "submit_time": "2025-04-14T13:12:00 1744636320",
    "num_comments": 95,
    "comments_url": "https://news.ycombinator.com/item?id=43680899",
    "comments": [
      "Wow, there's a lot of cynicism in this thread, even for HN.Regardless of whether or not it works perfectly, surely we can all relate to the childhood desire to 'speak' to animals at one point or another?You can call it a waste of resources or someones desperate attempt at keeping their job if you want, but these are marine biologists. I imagine cross species communication would be a major achievement and seems like a worthwhile endeavor to me.\n \nreply",
      "I'm as or more cynical than the next guy - but it seems to me that being able to communicate with animals has high utility for humans. Partly from an emotional or companionship perspective as we've been doing with dogs for a long time, but maybe even on purely utilitarian grounds.If we want to know something about what's going on in the ocean, or high on a mountain or in the sky or whatever - what if we can just ask some animals about it? What about for things that animals can naturally perceive that humans have trouble with - certain wavelengths of light or magnetic fields for example? How about being able to recruit animals to do specific tasks that they are better suited for? Seems like a win for us, and maybe a win for them as well.Not sure what else, but history suggests that the more people have been able to communicate with each other, the better the outcomes. I assume this holds true more broadly as well.\n \nreply",
      "I was just reading how fishing industry\u2019s longlines have caught many dolphins and other bycatches. It would be great to be able to give them warnings, or even better, to ask them to keep other big animals away from the longlines.\n \nreply",
      "I know this comment is totally innocent but it does kind of bum me out to be at a point in time where instead of addressing our impact on the environment directly, we're trying to make computers that can talk to dolphins so we can tell them to stay out of the way lol\n \nreply",
      "You don't tend to hear about it and not that there isn't still progress to be made, but there has been tonnes of progress on fisheries interactions with protected bycatch species. For ex the infamous dolphin problem in the eastern tropical Pacific purse seine tuna fishery is down 99.8% from its peak to the point populations are recovering, despite the fishery intentionally setting on dolphin schools to catch > 150,000 t of yellowfin tuna per year.Pelagic gillnets are probably the gear that still have the most issues with dolphin bycatch, and acoustic pingers that play a loud ultrasonic tone when they detect an echolocation click are already used to reduce interactions in some fisheries.\n \nreply",
      "Or, like, we could stop ravaging the oceans by industrial fishing, stop pretending magical technology will save the day, and try to limit our resource consumption to sustainable levels?Humanity\u2019s relationship with animals is so schizophrenic. On the one hand, let\u2019s try to learn how to talk to cute dolphins and chat with them what it\u2019s like to swim!, and on the other, well yeah that steak on my table may have once lead a subjective experience before it was slaughtered, and mass-farming it wrecks the ecosystem I depend on to live, but gosh it\u2019s so tasty, I can\u2019t give that up!\n \nreply",
      "Humans are omnivores.  I am unapologetic about obeying biological imperatives to eat other animals.At the same time, I want to be as humane as practical; I don\u2019t want to cause needless suffering to any creature.  If I kill a bug, I don\u2019t want it to suffer.  Same with food animals.The more like me an animal is, the less I want to eat it.There are a lot of humans.  Any action to forcefully reduce the number of humans or to forcefully reduce birth rates is almost certainly way more morally abhorrent to me, than doing what is necessary to feed those humans.\n \nreply",
      "I suppose this isn\u2019t exactly what you were getting at, but now I can\u2019t help but wonder exactly how delicious a dolphin is.\n \nreply",
      "Dear Mr Dolphin, can you please tell the large sharks to not go that way?- No, f... the sharks!\n \nreply",
      "I am all for the Disney utopian fantasy of us living with animals.However if universal communication was to be made. Don't you think that animals are going to be pretty pissed to discover what we have done with their kingdom?\"Hi Mr Dolphin, how's the sea today?\" \"Wet and a plastic bottle cap got lodged in my blowhole last night...\"\n \nreply"
    ],
    "link": "https://blog.google/technology/ai/dolphingemma/",
    "first_paragraph": "Apr 14, 2025[[read-time]] min read\n          DolphinGemma, a large language model developed by Google, is helping scientists study how dolphins communicate \u2014 and hopefully find out what they're saying, too.\n        For decades, understanding the clicks, whistles and burst pulses of dolphins has been a scientific frontier. What if we could not only listen to dolphins, but also understand the patterns of their complex communication well enough to generate realistic responses?Today, on National Dolphin Day, Google, in collaboration with researchers at Georgia Tech and the field research of the Wild Dolphin Project (WDP), is announcing progress on DolphinGemma: a foundational AI model trained to learn the structure of dolphin vocalizations and generate novel dolphin-like sound sequences. This approach in the quest for interspecies communication pushes the boundaries of AI and our potential connection with the marine world.Understanding any species requires deep context, and that's one of t"
  },
  {
    "title": "Hassabis Says Google DeepMind to Support Anthropic's MCP for Gemini and SDK (techcrunch.com)",
    "points": 153,
    "submitter": "thoughtpeddler",
    "submit_time": "2025-04-10T17:34:40 1744306480",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=43646227",
    "comments": [
      "Given their anti-trust  struggles, if Google for some reason dominates AI, they'd not want people to bring up anti-competitive behavior as a reason for that. Adopting open standards, especially open standards conceived outside Google is good for everyone including Google. They're well placed - from research to hardware to software and data.They'll also want the industry to rapidly move forward and connect data to AI. MCP has momemtum.\n \nreply",
      "It doesn't really matter what it is as there are many equally good implementations, but whoever sets up the framework first and cements usage is likely to guarantee dominance for the foreseeable future. Probably into AGI and post.Model Context Protocol seems good enough to me.\n \nreply",
      "Related, discussion on A2A from the other day:https://news.ycombinator.com/item?id=43631381\"The Agent2Agent Protocol (A2A)\", 279 comments\n \nreply",
      "I hope Gemini gets a desktop app where MCP servers are more useful, but wonder if Google's security posture allows it.\n \nreply",
      "I hope they also improve their JSONSchema support for structured output and tool calling. Currently it has many limitations compared to OpenAI\u2019s, for example it doesn\u2019t support \u201cadditionalProperties\u201d which eliminates an entire class of use cases and makes it immediately incompatible with many MCP servers.Marketing the API as OpenAI-compatible and then me getting 400s when I switch to Gemini leaves a sour taste in the mouth, and doesn\u2019t make me confident about their MCP support.\n \nreply",
      "Well, Google is one of the major investors in Anthropic, so I'm not surprised.\n \nreply",
      "Are they going to release a Gemini desktop app with MCP support so normal people can use it?\n \nreply",
      "Does MCP solve authentication on user's behalf which stifled OpenAI's GPTs?Tools often need access to data sources but I don't want to hard code passwords.\n \nreply",
      "It\u2019s terribly insecure as-is [1]. But so was HTTP. The spec isn\u2019t final, so hopefully it will improve.[1] https://blog.sshh.io/p/everything-wrong-with-mcp\n \nreply",
      "> MCP initially didn\u2019t define an auth spec and now that they have people don\u2019t like it.Just wrap it in an SSH tunnel or a HTTPS websocket> MCP servers can run (malicious code) locally.Just run it in a Docker container\n \nreply"
    ],
    "link": "https://techcrunch.com/2025/04/09/google-says-itll-embrace-anthropics-standard-for-connecting-ai-models-to-data/",
    "first_paragraph": "\n\n\t\tLatest\t\n\n\n\t\tAI\t\n\n\n\t\tAmazon\t\n\n\n\t\tApps\t\n\n\n\t\tBiotech & Health\t\n\n\n\t\tClimate\t\n\n\n\t\tCloud Computing\t\n\n\n\t\tCommerce\t\n\n\n\t\tCrypto\t\n\n\n\t\tEnterprise\t\n\n\n\t\tEVs\t\n\n\n\t\tFintech\t\n\n\n\t\tFundraising\t\n\n\n\t\tGadgets\t\n\n\n\t\tGaming\t\n\n\n\t\tGoogle\t\n\n\n\t\tGovernment & Policy\t\n\n\n\t\tHardware\t\n\n\n\t\tInstagram\t\n\n\n\t\tLayoffs\t\n\n\n\t\tMedia & Entertainment\t\n\n\n\t\tMeta\t\n\n\n\t\tMicrosoft\t\n\n\n\t\tPrivacy\t\n\n\n\t\tRobotics\t\n\n\n\t\tSecurity\t\n\n\n\t\tSocial\t\n\n\n\t\tSpace\t\n\n\n\t\tStartups\t\n\n\n\t\tTikTok\t\n\n\n\t\tTransportation\t\n\n\n\t\tVenture\t\n\n\n\t\tEvents\t\n\n\n\t\tStartup Battlefield\t\n\n\n\t\tStrictlyVC\t\n\n\n\t\tNewsletters\t\n\n\n\t\tPodcasts\t\n\n\n\t\tVideos\t\n\n\n\t\tPartner Content\t\n\n\n\t\tTechCrunch Brand Studio\t\n\n\n\t\tCrunchboard\t\n\n\n\t\tContact Us\t\nJust a few weeks after OpenAI said it would adopt rival Anthropic\u2019s standard for connecting AI models to the systems where data resides, Google is following suit. In a post on X on Wednesday, Google DeepMind CEO Demis Hassabis said Google would add support for Anthropic\u2019s Model Context Protocol, or MCP, to its Gemini models and SDK. He did not specify a timelin"
  },
  {
    "title": "Podman Quadlets with Podman Desktop (podman-desktop.io)",
    "points": 97,
    "submitter": "teleforce",
    "submit_time": "2025-04-14T17:16:51 1744651011",
    "num_comments": 39,
    "comments_url": "https://news.ycombinator.com/item?id=43683641",
    "comments": [
      "I have a RPi 2 that\u2019s more than enough for my home server needs and I\u2019ve tried to migrate from docker to podman, it simply failed miserably with weird errors, on my laptop everything worked fine, so maybe that\u2019s an issue with Arch ARM or the 32-bit version of it, but really, I wish they could abstract away quadlet, service files generation and so on.\n \nreply",
      "If you install the `podman-docker` package, you get compatibility with docker commands. If you have errors during runtime, it's probably something in your container/config that's assuming a docker socket/runtime. Some of which can be remedied by pointing `DOCKER_HOST` to your podman socket, etc.\n \nreply",
      "What is the killer feature that will make me want to switch from Docker Compose to Podman Quadlets?\n \nreply",
      "Podman quadlet supports \"Socket activation of containers\" https://github.com/containers/podman/blob/main/docs/tutorial...\nThis allows you to run a network server with `Network=none` (--network=none). If the server would be compromised, the intruder would not have the privileges to use the compromised server as a spam bot. There are other advantages, such as support for preserved source IP address and better performance when running a container with rootless Podman + Pasta in a custom network.\n \nreply",
      "That's neat. Does it require 1 connection = 1 process to work? I don't see how you can have a long running server with this feature.\n \nreply",
      "No, the init process hands over the listener FD allowing the server to accept() connections.You can also do 1 connection = 1 process, though, but it's absolutely not required nor particular common these days.\n \nreply",
      "What's old is new again. That's effectively how inetd worked circa 1986. The inetd daemon had some serious security vulnerabilities so the world move away from using \"socket activated daemons\" to having always listening services (performance reasons as well).\n \nreply",
      "inetd supported \"socket activation\" using the \"wait\" directive, where inetd would listen on the socket and then hand off the listening socket when there was activity as fd 0 where the server would need to call accept, and could continue to call accept for new connections, or exit when all clients were handled, and inetd would respawn when there was new pending connection on the listening socket.\n \nreply",
      "I really like the user namespace handling `--user-ns=keep-id`. It makes it easy for me to create a new Linux user and then have that user run some container and have bind mounts, etc just work correctly. It is the least fuss way I have found of running little services that need access to the host filesystem.https://docs.podman.io/en/latest/markdown/podman-run.1.html#...\n \nreply",
      "I prefer quadlet for 2 reasons:1. Podman is simpler than Docker. There is no long-running daemon. Rootless is default.2. Quadlets can be managed as systemd services, giving me the same tools to manage and view logs for system daemons and containers.Quadlets have been especially nice for bundling up an AI app I wrote as a cloud-init file, making it easy to deploy the hardware, software and models as one artifact.\n \nreply"
    ],
    "link": "https://podman-desktop.io/blog/podman-quadlet",
    "first_paragraph": "Containers are typically deployed in Kubernetes clusters.\nHowever, for smaller-scale use cases such as on a single-node server or during development, Kubernetes can be overkill.What\u2019s a more lightweight solution for running autonomous applications with multiple interacting containers?In this blog, we'll dive into what Quadlets are, their benefits, and how to use them within Podman Desktop.Podman Quadlets allow you to manage containers declaratively using systemd1.\nSince version 4.4, Podman can create, start, and manage containers (including pulling images, creating volumes, and managing pods) through systemd.Quadlets are simplified configuration files\u2014recognized by their specific extensions,\nsuch as *.container, *.pod, or *.image that are processed during startup or when you reload the daemon using the systemctl daemon-reload command.Quadlets generate the equivalent systemd unit files, streamlining the container management process.Below is an example of an nginx.container Quadlet file,"
  },
  {
    "title": "Show HN: ClipCapsule \u2013 A Clipboard Manager for Linux (Built with Go and Wails) (github.com/victor-evogor)",
    "points": 18,
    "submitter": "victorevogor",
    "submit_time": "2025-04-14T21:52:49 1744667569",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=43686715",
    "comments": [
      "how does this compare to KDE's Klipper?https://userbase.kde.org/Klipper\n \nreply",
      "I assume this is X11 only or using one of the wayland protocols which is not supported by all major compositors?(The exception likely being Gnome, which seems to be very set against implementing protocols allowing generic applications like this to work)\n \nreply",
      "Interesting idea.  Parcellite works better for me though.  It's been around forever, captures both 'select' and 'copy' and can paste with a middle click or Ctl-v with or without formatting.\n \nreply"
    ],
    "link": "https://github.com/Victor-Evogor/clipcapsule",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        ClipCapsule is a minimalist clipboard manager for Linux, built with Go and WailsJS. It supercharges your productivity by allowing you to manage and switch clipboard entries using only keyboard shortcuts\u2014no mouse or GUI required.\n      ClipCapsule is a minimalist clipboard manager for Linux, built with Go and WailsJS. It supercharges your productivity by allowing you to manage and switch clipboard entries using only keyboard shortcuts\u2014no mouse or GUI required.\u26a0\ufe0f This is a work-in-progress project. Currently, the GUI must be open for shortcuts to work, but we're actively working on a background daemon to make the app run seamlessly without launching the interface.When you copy items, the database stores them like this:Follow instructions from the Wails documentation.You'll need to build with elevated privileges to allow the app to lis"
  },
  {
    "title": "SQLite File Format Viewer (sqlite-internal.pages.dev)",
    "points": 155,
    "submitter": "ilumanty",
    "submit_time": "2025-04-14T14:55:57 1744642557",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=43682006",
    "comments": [
      "This really would've come in handy when I was debugging my own SQLite parser a couple weeks ago.One thing that initially confused me was how exactly the pages worked w.r.t. the first page on disk... I misunderstood the SQLite documentation in different ways, but it's really rather simple: the very first page is just treated as containing the file header in it, and it pushes down the rest of the data, making the page shorter than the other pages. You can see that illustrated clearly if you click into the first page of a database using this tool: the database header comes first, then the page header.This tool will undoubtedly come in handy for anyone who has a reason to be dealing with SQLite data structures directly for whatever reason, especially since the SQLite documentation is a bit terse at times.\n \nreply",
      "I really want a data format that is effectively binary JSON. What is the subset of all of the features of SQLite that makes either a read-only or an updatable data set that is compact. But better searchability than a streaming parser.\n \nreply",
      "Have you tried MessagePack[0]?0: https://msgpack.org/index.html\n \nreply",
      "If you want to maintain the properties that SQLite has for read use cases, you'll need to replicate a couple of features. At the very least, you'll probably want the format to still be page-based with a BTree structure. You really could get away with just using the SQLite format if you didn't mind the weirdness; a functional SQLite parser that can read tables would not be a significant amount of code. I think, though, that if you want to read the schema as SQLite understands it, you'd need to interpret the CREATE TABLE syntax, which would make it a bit more complex for sure. Otherwise, you can read tables and columns themselves relatively easily, and the values are all stringified.\n \nreply",
      "Yeah if I wasn\u2019t clear I\u2019m talking about a minimal file that SQLite can still open read only without errors, not a third party implementation. Though there might be a few tweaks that would allow SQLite to be a bit more lenient. For instance missing metadata that can be assumed. Maybe b tree nodes exceeding the usual load factor.\n \nreply",
      "Parquet or some other column oriented data format is probably closest to what you want without getting into indexing your flat files or similar\n \nreply",
      "sqlite itself supports a binary encoding of JSON: https://sqlite.org/jsonb.html\n \nreply",
      "When I said binary JSON I didn\u2019t mean literal JSON. I meant \u201ccommon denominator interchange format\u201d. It\u2019s too chatty by far and has dismal performance for queries.  So you\u2019re better off asking a specific question and getting a larger document that could answer many questions that you do t yet have. For CDNs things like this matter a lot.\n \nreply",
      "MongoDB's BSON?\n \nreply",
      "I especially like that it provides a sample database. Would be nice it were downloadable as a file, so we could explore it with our own tools as well and get a better feeling for how the tool works.\n \nreply"
    ],
    "link": "https://sqlite-internal.pages.dev",
    "first_paragraph": ""
  },
  {
    "title": "W65C832 in an FPGA (mikekohn.net)",
    "points": 42,
    "submitter": "homarp",
    "submit_time": "2025-04-11T20:52:32 1744404752",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=43658417",
    "comments": [
      "I love these retro cores which extend \"what might have been\" from the past into the future. Another impressive example is the Apollo FPGA core which over the past decade has faithfully built on the Motorola 68000 series to extend beyond the last chip (68060) adding a variety of modern CPU affordances like 64-bit, instruction fusing/bonding, super scalar, out of order execution for CPU/FPU, dynamic branch prediction, etc. There's an add-in accelerator board that replaces the CPU in an Amiga and the team is working on now adding Atari ST emulation/acceleration to the core.http://www.apollo-core.com/index.htm?page=features\n \nreply",
      "The W65C816 is a 16 bit version of the 6502 CPU.The 32 bit version, called the W65C832, only exists as a datasheet \nThis is a FPGA version.\"builds with the regular opensource FPGA tools: yosys, nextpnr-ice40, icepack, and iceFUNprog. I recently added support for the Tang Nano 20k board (should work with the 9k model too)\"\n \nreply",
      "So this actually makes more sense to me than the '816. Here's why:The 816 has a 24-bit address bus. But its registers are limited to 16-bits. So there's no way to represent a \"full\" address in a register. Aka a pointer can't be held in a register.Unfortunately this core, staying true to the 816, still has a 16-bit program counter and the awkward banked address scheme of the 816. I assume it has the same restriction of limiting stack and direct-page to the 00-bank, too?The 816 really felt to me like Mensch just kind of bolted a little circuitry on the side of the 502. It doesn't have the simplistic elegance of the 6502.A \"better\" 32-bit 65xx I think would be just to widen all registers (including program counter) out to 32-bits and leave it at that, forgetting about binary compatibility. A big linear memory.\n \nreply",
      "I would love to see a IIGS MiSTer core.  Could this work be ported somehow?\n \nreply",
      "The IIGS uses a 65816, the same 16-bit CPU that's used in the SNES. Since the MiSTer already has excellent SNES support, I don't think there's anything to be done as far as modeling the CPU for the IIGS.This is a \"fantasy\" 32-bit extension of the 65816 that WDC specced out but never built. It wouldn't help with getting the IIGS on MiSTer.\n \nreply",
      "What clock rate does this FPGA version run?\n \nreply"
    ],
    "link": "https://www.mikekohn.net/micro/w65c832_fpga.php",
    "first_paragraph": "\nPosted: October 16, 2024Introduction\nBack in the 80's, the\nWestern Design Center\n(WDC) created a 16 bit version of the 6502 CPU called the W65C816.\nI believe it was created for the Apple IIgs computer, although it was\nalso used in the Super Nintendo. It appears they had a data sheet for\na 32 bit version of the chip called the W65C832. I decided to do a\nVerilog version of it in an FPGA.\n\nI updated naken_asm so it has a .65832\ndirective that allows immediate values for things like lda and such\nto have a .l modifier so it can do 32 bit values now.\nThe instruction set is able to access 16MB of RAM. The FPGA itself\nhas a small amount of block RAM that this w65c832 core uses as 4k of\nRAM at the bottom of memory (for zero / direct page), 4k as ROM, and\n4k as pages for a Winbond W25Q128J flash chip. More on that further\non this page.\nThe Verilog source code is available on GitHub and builds with the\nregular opensource FPGA tools: yosys, nextpnr-ice40, icepack,\nand iceFUNprog. I recently added"
  },
  {
    "title": "KaiPod Learning (YC S21) Is Hiring a PM (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-04-14T17:59:11 1744653551",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/kaipod-learning/jobs/TgR2OZg-senior-product-manager",
    "first_paragraph": "A national network of microschools bringing school choice to every\u2026ABOUT NEWTONNewton is the platform powering a new era of education \u2014 one where learning is personal and schools are built by educators. Designed for the unique needs of microschools, Newton helps educators launch, grow, and run their programs with ease \u2014 combining tools for operations, academics, and family engagement into one seamless experience.But Newton is more than a product. It\u2019s the operating system for a movement \u2014 a reimagining of how education works at its core.As the fastest-growing platform in the space, Newton is poised to shape the future of schooling for millions of students and families across the country.ABOUT THE ROLEWe\u2019re looking for a visionary and execution-oriented Senior Product Manager to take ownership of major product initiatives and help scale Newton into a platform that serves millions.You\u2019ll work directly with the CEO and the Product & Engineering teams to turn insights into strategy, ship d"
  }
]