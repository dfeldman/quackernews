[
  {
    "title": "Fleurs du Mal (fleursdumal.org)",
    "points": 52,
    "submitter": "Frummy",
    "submit_time": "2025-05-09T22:42:05 1746830525",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=43941598",
    "comments": [
      "\u201cHypocrite lecteur, \u2013 mon semblable, \u2013 mon fr\u00e8re!\u201cIt is always interesting when random touchstones from my life appear on Hacker News: books like the Aubrey-Maturin (master and commander) series, Ursula Le Guin\u2019s works, Dante, John Le Carre\u2019s George Smiley novels, Tolstoy... and now Charles Baudelaire, at the top of the page no less.Baudelaire was a dark misanthrope and the poetry is very bleak. His life was not happy and he died at 46. You probably need to have at least a little of the same darkness in your soul to get something out of it.It\u2019s worth remembering too, how strange and controversial this work was when it first came out, using traditional verse forms but with a relentlessly modern subject, poetry from the gutter of the 19th-century city. Modernism in literature has had 150 years to settle but this is the raw beginnings.Some good ones: The Albatross, Invitation to the Voyage, Evening Harmony, and the Epilogue (\u201cLe coeur content, je suis mont\u00e9 sur la montagne\u201d). And many others.\n \nreply",
      "I love the book.This page is super interesting to me, because it's so focused and simple. I love the idea of an almost Wiki-like \"this is some public domain thing you should know, so it has a dedicated website\".Would make a lot of sense to make it easy to create and host those.\n \nreply",
      "As a husband and cat person, I can relate to this one: https://fleursdumal.org/poem/132\n \nreply",
      "Love this book and love this website. So many favorites, but just gonna mention one (had to \"remix\" and edit the translations, none of them sounded good): https://fleursdumal.org/poem/109\u2014 \u041e grief! \u041e grief! Time eats life.And the hidden Enemy who gnaws the heartgrows on the blood we lose and thrives.\u2014 \u00d4 douleur! \u00f4 douleur! Le Temps mange la vie,Et l'obscur Ennemi qui nous ronge le coeurDu sang que nous perdons cro\u00eet et se fortifie!\n \nreply",
      "I remember in college when I took French classes the professor very highly recommended Fleurs du mal. It was a difficult read for students with just one year of French, but I remember reading some translations and liked them.\n \nreply",
      "I'm pleased: I'm nearly through Duolingo French and I can more or less read that.I've done a fair bit of outside study, including a few (young adult) books. But it's nice to think that I could perhaps pass a college French class.\n \nreply",
      "How long did it take to get through the DuoLingo French course?\n \nreply",
      "Had to study this at school a while back, it was one of the first books (after Candide by Voltaire) that I found interesting at the time, and still have in my little library.\n \nreply",
      "Stupid but semi-related: when I lived in Chicago, I had just gotten a print copy of this and remember doing a double take when I saw a flower shop with the same name (Les Fleurs du Mal).\n \nreply",
      "I have a variety of early printings of this. My favorite being a 1931 edition illustrated by Major Felten, its beautiful.\n \nreply"
    ],
    "link": "https://fleursdumal.org",
    "first_paragraph": "\nCharles Baudelaire'sFleurs du mal / Flowers of Evil\n\n\"I once read a lot of Baudelaire + my Angel kid has read every translation \u2014 apparently, if you don't know french (I do) you have to read all the translations to get a good idea.\" \u2014 Allen Ginsberg, Letter to David Cope, 25 Jan 1977.\n\nFleursdumal.org is dedicated to the French poet Charles Baudelaire (1821 - 1867) and his poems Les Fleurs du mal (Flowers of Evil). The definitive online edition of this masterpiece of French literature, Fleursdumal.org contains every poem of each edition of Les Fleurs du mal, together with multiple English translations. \n\nAbout. Fleursdumal.org launched on 1 Feb 2004. The site is a labor of love created and maintained by Supervert. At supervert.com you can find information about Supervert's books, which include Extraterrestrial Sex Fetish, Necrophilia Variations, Perversity Think Tank, Post-Depravity, Apocalypse Burlesque, and Music for Erotomaniacs. \n\nNote on translations. Most of the translations tha"
  },
  {
    "title": "WebGL Water (2010) (madebyevan.com)",
    "points": 42,
    "submitter": "gaws",
    "submit_time": "2025-05-10T00:13:38 1746836018",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=43942149",
    "comments": [
      "Saw the \u201cmade by Evan Wallace\u201d and went \u201chuh, that sounds familiar\u2026\u201dYeah, not surprising this guy went on to build Figma! Super cool\n \nreply",
      "Back in 2010, this \"require[d] a decent graphics card\"Now, my Ryzen 7's integrated graphics can run it very smoothly. Moore's law at play.\n \nreply",
      "Here I am running just fine on a 3 year old phone\n \nreply",
      "This is my most voted submission. This thing literally never gets old\n \nreply",
      "Here is a trick: pause the simulation and drag the ripples back and forth really fast, it will create a \"mega\" wave. Then unpause and it will create a massive tsunami\n \nreply",
      "Or pause it and click the water surface 100 times to raise up a lot of potential energy that makes a very profound wave front when it comes down when you start it.\n \nreply",
      "On this note, can anyone recommend basic webgl 2d effects tutorial? I have a super exciting project I'm really close to announcing, but the last step is adding some pretty Animal Well style effects via webgl2, but I know practically nothing about webgl except the very very basics that you learn from webgl2fundamentals.org. Any pointers would be appreciated.\n \nreply",
      "I like https://thebookofshaders.com/ . It\u2019s unfinished and I don\u2019t think it\u2019s been updated in years, but what\u2019s there is pretty good\n \nreply",
      "Webgl2fundamentals is pretty great :)\n \nreply",
      "Wasnt this one of the demo that Figma co-founder used make a case for web-based editor?\n \nreply"
    ],
    "link": "https://madebyevan.com/webgl-water/",
    "first_paragraph": "Made by Evan WallaceThis demo requires a decent graphics card and up-to-date drivers. If you can't run the demo, you can still see it on YouTube.* requires the OES_texture_float extension** requires the OES_standard_derivatives extensionTile texture from zooboing on Flickr"
  },
  {
    "title": "ALICE detects the conversion of lead into gold at the LHC (home.cern)",
    "points": 482,
    "submitter": "miiiiiike",
    "submit_time": "2025-05-09T14:31:20 1746801080",
    "num_comments": 254,
    "comments_url": "https://news.ycombinator.com/item?id=43937214",
    "comments": [
      "The relevant part: \"The ALICE analysis shows that, during Run 2 of the LHC (2015\u20132018), about 86 billion gold nuclei were created at the four major experiments. In terms of mass, this corresponds to just 29 picograms (2.9 \u00d710-11 g).\"Just need to scale it by trillions to make 1 ounce, but transmutation of lead to gold - the dream of many alchemists - is now just a by product of particle accelerators.\n \nreply",
      "Ran the numbers. The LHC would break even if the price of gold was $48 trillion trillion per ounce.\n \nreply",
      "Only if the LHC doesn't quire gold to operate. If you're using ICs and components  that have some gold in them and they need maintenance, you consume more than you produce.\n \nreply",
      "Can still recover the gold from old parts though.Quite fitting actually, alchemists scamming investors with needing a \"starting\" amount to get their reaction going\n \nreply",
      "You mean like gas money?\n \nreply",
      "Well, except for in particle accelerators, stars, and supernovae, atoms are never created or destroyed, so if they're creating gold, it's here for good.\n \nreply",
      "Except that everyone with a fusor can feed the gold atom a neutron which converts it to unstable Au-198 that decays to mercury. Fun times when you can (theoretically) transmute gold to mercury with stuff you can order on the internet.\n \nreply",
      "You could do that for decades already!It just doesn't make a lot of economic sense, but I wonder why nobody made fusion art yet.\n \nreply",
      "I definitely will mis-speak/mis-write, but my mathematic (also flawed) tells me that if Gold + 1 = Mercury, then Something + 1 = Gold, so we can find that \"something\" add 1 of the thingie, and booya!! gold!! (right?) (please read the above with silly humor)In a slightly more serious note, I remember listening to Elon in some podcast 1-2 years ago saying how they create new metals/alloys that nobody had created previously, because they needed specific needs covered, and no known material had the attributes they needed. So.. in a way..\n \nreply",
      "The same mechanism that lets you convert gold-197 to mercury does in fact work to convert the equivalent isotope (that is, 1u lighter than gold) of the element left of gold on the periodic table to gold.The only problem, the element left of gold is platinum, and platinum-196 is not even the most common isotope of platinum, making up ~25% of it. You're rather unlikely to be able to make money on this.(Not that you would have been able to regardless of the price of platinum. There are 3,000,000,000,000,000,000,000 atoms in a gram of gold, and a desktop fusor is going to generate ~<1m neutrons per second.)\n \nreply"
    ],
    "link": "https://www.home.cern/news/news/physics/alice-detects-conversion-lead-gold-lhc",
    "first_paragraph": "At CERN, we probe the fundamental structure of particles that make up everything around us. We do so using the world's largest and most complex scientific instruments.Know more\nWho we are\n\nOur Mission\n\nOur Governance\n\nOur Member States\n\nOur History\n\nOur People\n\nWhat we do\n\nFundamental research\n\nContribute to society\n\nEnvironmentally responsible research\n\nBring nations together\n\nInspire and educate\n\nFast facts and FAQs\n\nKey Achievements\nKey achievements submenuThe Higgs BosonThe W bosonThe Z bosonThe Large Hadron ColliderThe Birth of the webAntimatterLatest news\nNews\n\nAccelerators\n\nAt CERN\n\nComputing\n\nEngineering\n\nExperiments\n\nKnowledge sharing\n\nPhysics\n\nEvents\n\nCERN Community\n\nNews and announcements\n\nOfficial communications\n\nEvents\n\nScientists\n\nNews\n\nPress Room\nPress Room submenuMedia NewsResourcesContactThe research programme at CERN covers topics from kaons to cosmic rays, and from the Standard Model to supersymmetryKnow more\nPhysics\n\nAntimatter\n\nDark matter\n\nThe early universe\n\nThe "
  },
  {
    "title": "Business books are entertainment, not strategic tools (theorthagonist.substack.com)",
    "points": 63,
    "submitter": "ZeroTalent",
    "submit_time": "2025-05-09T20:51:45 1746823905",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=43940747",
    "comments": [
      "I am always amazed how most business book authors take a simple idea that could be described in one page, and turn it into a 200+ page book with popularizing narrative. What's more amazing is that the ideas are usually commonsense, but due to human nature are seldom practiced.\n \nreply",
      "I'm not much of a book reader. Someone told me that if they follow the style guide you only have to read the first sentence of each chapter. It was kinda mind blowing to read a bunch of example books he gave me. The funniest part was returning to a previous book to read more of one of the chapters. I apparently didn't know how [those] books work.\n \nreply",
      "I\u2019d say most books period, not just business books - could be shortened to just a couple of pages.\n \nreply",
      "Naval has said most books could be essays and most essays could be tweets\n \nreply",
      "I think there are maybe 5 business books out there. I\u2019m not sure how exactly I\u2019d define the 5 different business books, but I think of you read 10-15 business books you\u2019ve pretty much read them all. After a while, they all start boiling down to the same few points with differences in narrative content. If I were to take an unconsidered stab at a few of them: hard work + luck is about the closest formula anyone has found for success if applied over long time periods; you have to be disagreeable and believe in yourself, but not so disagreeable that you can\u2019t get along with anyone; people are important, and treating them well leads to better businesses (over long time periods); sometimes you get dealt a bad hand.\n \nreply",
      "Don't forget the tried-and-true \"Don't sell to your customers; listen for their actual needs\" advice that's repeated ad nauseum in a thousand different ways by B2B experts who claim to have \"cracked the code\" to increasing sales.\n \nreply",
      "I would also add taking steps to \"increase luck surface area\".  One can't control luck and luck is, unfortunately, essential, but one can increase the opportunities for luck to play its part.\n \nreply",
      "one corollary is that \"proximity is power\". being around people who are successful increases the chances that you will be successful. easier said than done, however.\n \nreply",
      "Thanks, you just saved me half of my reading list!\n \nreply",
      "Most popular nonfiction is entertainment. When I realized this I went back to reading fiction. The quality of entertainment is so much better!\n \nreply"
    ],
    "link": "https://theorthagonist.substack.com/p/why-reading-business-books-is-a-waste",
    "first_paragraph": ""
  },
  {
    "title": "Brandon's Semiconductor Simulator (brandonli.net)",
    "points": 16,
    "submitter": "dominikh",
    "submit_time": "2025-05-10T00:37:48 1746837468",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://brandonli.net/semisim/",
    "first_paragraph": "Draw your own circuits. Turn on voltage and see what happens. Some features of my program include:You need a good computer to run this simulation on your browser. There is also a downloadable version that's much faster\n      (but requires Java).Want to know more? You can read about it here.(c) Brandon Li, 2025. Ported to Javascript with the help of Paul Falstad."
  },
  {
    "title": "Charles Bukowski, William Burroughs, and the Computer (2009) (realitystudio.org)",
    "points": 7,
    "submitter": "zdw",
    "submit_time": "2025-05-10T00:45:47 1746837947",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://realitystudio.org/bibliographic-bunker/charles-bukowski-william-burroughs-and-the-computer/",
    "first_paragraph": "16-bit Intel 8088 chipwith an Apple Macintosh  you can\u2019t run Radio Shack programs  in its disc drive.  nor can a Commodore 64  drive read a file  you have created on an  IBM Personal Computer.  both Kaypro and Osborne computers use  the CP/M operating system  but can\u2019t read each other\u2019s  handwriting  for they format (write  on) discs in different  ways.  the Tandy 2000 runs MS-DOS but  can\u2019t use most programs produced for  the IBM Personal Computer  unless certain  bits and bytes are  altered  but the wind still blows over  Savannah  and in the Spring  the turkey buzzard struts and  flounces before his  hens.\u2014 Charles BukowskiOn Christmas Day, 1990, Charles Bukowski received a Macintosh IIsi computer and a laser printer from his wife, Linda. The computer utilized the 6.0.7 operating system and was installed with the MacWrite II word processing program. By January 18 of the next year, the computer was up and running and so, after a brief period of fumbling and stumbling, was Bukowski. H"
  },
  {
    "title": "Memory Safety Features in Zig (gencmurat.com)",
    "points": 9,
    "submitter": "hmac1282",
    "submit_time": "2025-05-09T23:48:38 1746834518",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://gencmurat.com/en/posts/memory-safety-features-in-zig/",
    "first_paragraph": "\n\uf5ec \n    2025-04-19\n\nMemory safety is a cornerstone of Zig\u2019s design philosophy. While maintaining the performance benefits of manual memory management, Zig incorporates sophisticated safety mechanisms to prevent common memory-related errors. This article provides an in-depth exploration of Zig\u2019s memory safety features with detailed examples and explanations.One of Zig\u2019s fundamental principles is eliminating hidden control flow, which makes programs more predictable and easier to reason about:The try keyword makes it immediately clear that this operation might fail. If an error occurs, execution immediately returns from the current function, propagating the error upward. This explicit approach prevents situations where errors silently propagate through code, causing unpredictable behavior.Zig uses a robust error union type system that forces developers to handle all potential error cases:This approach ensures that errors cannot be accidentally ignored. Every error must be explicitly hand"
  },
  {
    "title": "What\u2019s new in Swift 6.2 (hackingwithswift.com)",
    "points": 105,
    "submitter": "ingve",
    "submit_time": "2025-05-09T20:20:52 1746822052",
    "num_comments": 75,
    "comments_url": "https://news.ycombinator.com/item?id=43940539",
    "comments": [
      "Free-form identifiers are neat for test-case naming, but not for `HTTPStatus.`404``. I think having `HTTPStatus.Error404` was a bad idea to begin with. Just use semantic names like `HTTPStatus.NotFound` and you wouldn't have a problem in the first place. Now, a single character typo can easily make a 404, 403 and create a bug. It's less of a problem with semantic names.If you want constrained numeric types in Swift, that's another problem to tackle. But `HTTPStatus.`404`` seems to be the least ideal way to go about it. It lets you do stuff like to declare `HttpStatus.`404`` with a value of 403 too.\n \nreply",
      "The article doesn\u2019t give enough attention to the glacial but steady changes in the ownership model that will have great benefit in avoiding copies in value types, Swift\u2019s strength and Achilles heel.I have to say Paul Hudson has almost single-handedly taken over communicating the essentials of Swift to the world; he\u2019s fantastically reliable, brief but enthusiastic, guiding people around the many pitfalls.\n \nreply",
      "Agreed about Paul Hudson. He also just seems like a genuinely nice guy. I was kind of shocked to receive an email from him out of the blue last weekend (well, from GitHub, but with his name in the \"From\" field). Turns out it was about a PR [0] to one of my packages where he fixed typos in the README.[0] https://github.com/visfitness/reorderable/pull/2\n \nreply",
      "As someone who has worked with him, no he is not a nice guy. He is the opposite of a nice guy.\n \nreply",
      "I\u2019d love to believe you, but you\u2019re not giving us much to go on with\n \nreply",
      "The last sentence is exactly how I would describe Paul Hudson, as a HwS reader/user.\n \nreply",
      "Honest question. Not trying to troll. One of the pitches in the earlier days was \u201cC/Objective-C OK, but you can\u2019t write safe/next level code with it\u2014-Swift will close that gap.\u201dN years later, it doesn\u2019t feel like there has been a step change in Apple software quality; if anything Apple software feels less solid, and looks cool \u201clook what I did\u201d extension points. I mean, some of the tings you could do with runtime categories, and runtime prototypes were really cool. Now when I work on my 2 apps that originally happily port to Swift/UIKit, I\u2019m just left confused with how to make things work. I\u2019m happy when it finally works, and don\u2019t ever try to improve the thing, it\u2019s too much work.There\u2019s lots of different variables at play here; I\u2019m not trying to stretch inference too much. Heck, it could have been that with adding Swift to the mix, the forces that have contributed to reduced quality in Apples stuff would be even worse.I\u2019m just frustrated. When I work in Elixir, I\u2019m like this is cool. When I work in Kotlin, I don\u2019t feel like \u201cApples got a language like this too, but it\u2019s got that extra special zing that used to make stuff Apple touched cool.\u201d\n \nreply",
      "I feel the same. Apple software quality certainly hasn\u2019t increased. Years back I remember some apps crashing suddenly after updating MacOS. I checked the binary and saw they\u2019d started adding Swift.Half a decade later it seems like it should be better and Swift stuff should be stabilized. But nope, I\u2019ve had more little glitches in both iOS and MacOS. It\u2019s hard to say it\u2019s due to Swift, and not management priorities. Still it feels partially related to Swift.Swift\u2019s goals are great, I like the syntax, but the language implementation seems to just special case everything rather than having coherent language design.That and Swift de-emphasizes Obj-C message passing. I have a pet theory that message passing produces more robust GUI software that\u2019s easier to adapt to complex needs.\n \nreply",
      "Lot of nice improvements here. I'm actually quite liking the async API after using it in a couple small apps and learning the fundamentals.I really wish the entire Swift team would spend a quarter fixing bugs and improving the compiler speed, though. So many trivial SwiftUI cases trigger the \"this is too complex for the compiler\" errors which are so frustrating to fix.\n \nreply",
      "I've been starting to use Swift again lately after like four years, and while the language is beautiful & the package management story is now a LOT better with SwiftPM, I found that none of it plays nicely with XCode, Simulator, or anything to do with iOS/macOS development -- its primary use-case!I found myself awestruck that I *HAD* to use XCode or xcodebuild, and could not just run `swift build` and generate an iOS app.In the end, I had to:- compile the .app structure myself- run codesign manually- manage the simulator manually- bundle xcAssets manually- provide swift linker flags manually targeting the toolchain/sdk I needed- manually copy an embedded.mobileprovisionIt was a good learning experience, but what's the story here? Did Apple just give away Swift to the OSS community and then make no ongoing effort to integrate their platforms nicely into the non-xcode build tooling? Looking into https://github.com/stackotter/swift-bundler\n \nreply"
    ],
    "link": "https://www.hackingwithswift.com/articles/277/whats-new-in-swift-6-2",
    "first_paragraph": ""
  },
  {
    "title": "Sofie: open-source web based system for automating live TV news production (nrkno.github.io)",
    "points": 267,
    "submitter": "rjmunro",
    "submit_time": "2025-05-09T13:18:42 1746796722",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=43936408",
    "comments": [
      "As a live television newscast director in a major market, I would be very interested to see a feature comparison between this product and its main competitors: Ross OverDrive, Sony ELC, and Grass Valley Ignite.Due to the substantial complexity of these automation systems, they tend to have a lot of inertia. But if anything could drive a station group to make a change, the \"free\" part can be effective.I did take a look at the supported hardware (1). I think that's the pain point for many shops. Free open source production software is great, but being forced to choose form hardware products you don't prefer is a pretty tough tradeoff.Historically, I suppose that's been one of FOSS' big challenges.(1) https://nrkno.github.io/sofie-core/docs/user-guide/supported...\n \nreply",
      "It looks like great support (Blackmagicdesign) for building a small broadcast studio from scratch tho.I could see BMD embracing this.  There are lots of studios that are not commercial broadcast that could really use a system like this.Isn't one of the problems with hardware support is that hardware vendors have agreements with the competitors you listed?Computers are fast enough now that once you can get the signals into a machine, many of the special functions that previously required dedicated hardware can now be run in software? With proper timing signal distribution of course.Seems like 12G SDI to SFP+ would enable server class machines to subsume most of the special function hardware.\n \nreply",
      "Disclaimer... I am a director and not an engineer. I can only give you my relatively limited understanding....> It looks like great support (Blackmagicdesign) for building a small broadcast studio from scratch tho.Agreed!> I could see BMD embracing this. There are lots of studios that are not commercial broadcast that could really use a system like this.Also agreed. Black Magic definitely makes a lot of reasonably-priced and very capable gear. They're not a major player in the TV automation space, but perhaps with the help of Sofie, they could make inroads.> Isn't one of the problems with hardware support is that hardware vendors have agreements with the competitors you listed?That's not a topic I'm knowledgeable about. It is my understanding that most shops who have a particular vendor's automation platform will also have that vendor's hardware running at its core. In all the shops I've seen, the switcher that's controlled by the automation system is made by the same company. Or if its another vendor's product, it's sold and provisioned along with the automation system when its purchased. Other stuff like audio mixers, robo-cam products, clip players, and CG/graphics platforms can be from other vendors.> Computers are fast enough now that once you can get the signals into a machine, many of the special functions that previously required dedicated hardware can now be run in software? With proper timing signal distribution of course.> Seems like 12G SDI to SFP+ would enable server class machines to subsume most of the special function hardware.For audio, I think that would be a relatively easy lift with technologies like Dante. However, in most TV stations, you're going to need to literally plug upwards of 100 HDSDI video cables into a piece of hardware so that those sources can be switched to on TV, mixed and keyed on multiple mixed-effects banks,  and viewed on multiviewer screens in the control room. I don't know that a regular-ol' PC has what it takes to take in and simultaneously process that amount of video. But just because don't know about it doesn't mean it doesn't exist. ;-)  Just haven't seen it yet.\n \nreply",
      "Sofie drives the matrix (black magic videohub for small ones like the 120 squared, but ours are 1000+ which black magic won\u2019t do), the audio mixers, the video mixers, the graphics and by machines (Caspar), etc. your mixers don\u2019t need that many inputs - a typical one might be 24 or 32 inputs with a few ME banks.All these devices use standard protocols, or it\u2019s just a new plugin for sofie ti drive it.Of course increasingly the industry is using 2110 on spine and leaf networks rather than SDI. I don\u2019t know if there any COTS mixers aside from the vmix/obs level, I believe some 2110 controllers will provide video matrix style interfaces. Nmos seems challenged in this area from what I hear.\n \nreply",
      ">For audio, I think that would be a relatively easy lift with technologies like Dante. However, in most TV stations, you're going to need to literally plug upwards of 100 HDSDI video cables into a piece of hardwareI don't know the TV stations requirements, but you can maybe have 10 interconnected servers that manage 10 HDSDI flux each (and can send them on another if required for processing) ?\n \nreply",
      "> Computers are fast enough now that once you can get the signals into a machine, many of the special functions that previously required dedicated hardware can now be run in software?I\u2019m a big fan of the Richard Cartwright view of asynchronous signal processinghttps://creativecow.net/matrox-video-announces-nab-2023-line...But I don\u2019t think it has the traction isn\u2019t deserves. Too many people in the industry are still wedded to ptp timing their packets to arrive in the same 30us windows.\n \nreply",
      "bmd is in a decent position to help with this. making davinci be the nle that ties into this like avid / airspeed or whatever ppl use now, seems pretty cool.\n \nreply",
      "I fall pray to this often.  Internal complexity and growth over time lead to great giant feature charts and comparison matrixes.  But sometimes you just need a tool that gets a job done.It's one thing for something simple to not be a drop in replacement.  But simplicity and minimalism can also be a virtue.  Can this complete the task in an environment designed around using it?\n \nreply",
      "Do you have any recommendations of where I could read more about integrating content on those softwares? I help run a service that provides content for news sites and I would like to make it easy to make that content available on newscasts.\n \nreply",
      "how are you still a director??? i miss tv fondly but the pain they inflict on everyone and the hours / pay / etc make the best people bounce.someone on hn surely could use their talents for good elsewhere haha\n \nreply"
    ],
    "link": "https://nrkno.github.io/sofie-core/",
    "first_paragraph": "Sofie is a web-based, open\u00a0source TV\u00a0automation system for studios and live shows, used in daily live\u00a0TV\u00a0news productions by the Norwegian public\u00a0service broadcaster NRK since September\u00a02018.General documentation describing functionality, installation, and operation of a Sofie system.Specific documentation regarding development and contribution to the Sofie code base.Current, past, and upcoming releases of the Sofie system.Please join the growing Slack to meet the developers and other Sofie users."
  },
  {
    "title": "Launch HN: Nao Labs (YC X25) \u2013 Cursor for Data",
    "points": 114,
    "submitter": "ClaireGz",
    "submit_time": "2025-05-09T16:28:28 1746808108",
    "num_comments": 51,
    "comments_url": "https://news.ycombinator.com/item?id=43938607",
    "comments": [
      "Been using this for several weeks now and it's genuinely improved my workflow\u2014I'm choosing it over VSCode and extensions more than half the time.The chat for exploratory data analysis (\"what can you tell me about this column I just added?\"), the worksheets and column lineage are real game-changers for dbt development. These features feel purposefully designed for how I actually work.Claire and Christophe are super responsive to feedback, implementing features and fixes quickly. You can see the product evolving in all the right directions!\n \nreply",
      "Thanks for your kind message \u2014 and for helping us make nao better!\n \nreply",
      "Cool idea! How did you train your tab model? Fill in the middle or is it based on edit history like cursor? Someone posted this yesterday and I found it fascinating https://www.coplay.dev/blog/a-brief-history-of-cursor-s-tab-...\n \nreply",
      "Yes we use Fill in the middle models (Mistral and our own trained Qwen). And we feed it with your data context - we have our own SQL parser to feed you with the right data schema context depending on where your cursor is in the query.\n \nreply",
      "I hadn't realized you trained your own model.  That's an important differentiator. How do you get training data of schemas in the wild?\n \nreply",
      "This is really slick. I watched the YouTube video (a couple of times; I didn't grok what was happening immediately) and I really love how this accelerates feedback cycles. Very, very cool.\n \nreply",
      "Thank you! Actually this is exactly what we target, we've seen that data teams have often a longer feedback loop than software engineers. That's a goal for us to shorten it and to bring data the closest to your dev flow.\n \nreply",
      "This looks like many LLM assisted data projects which help and are flexible, but aren't repeatable and aren't fast enough to be interactive.  Nao is a good execution of the concept.I built Buckaroo as a data table UI for Jupyter and Pandas/Polars, that first lets you look at the data in a modern performant table with histograms, formatting, and summary stats.Yesterday I released autocleaning for Buckaroo. This looks at data and heuristically chooses cleaning methods with definite code. This is fast (less than 500ms). Multiple cleaning strategies can be cycled through and you can choose the best approach for your data. For the simple problems we shoudn't need to consult an LLM to do the obvious things.All of this is open source and extensible.[1] https://youtube.com/shorts/4Jz-Wgf3YDc[2] https://github.com/paddymul/buckaroo[3] https://marimo.io/p/@paddy-mullen/buckaroo-auto-cleaning Live WASM notebook that you can play with - no downloads or installs required\n \nreply",
      "Thanks for sharing. I like the view you built to visualize the profiling of your data, I think that's indeed key to understand your data.\n \nreply",
      "Congrats on the HN launch! Really excited to give this a try I think this could be a huge unlock for my team.One quick issue - unable to connect to my postgres instance that requires SSL.SSH tunneling seems to be broken as well because when the box is checked I am unable to select a private key path and the connect button is goneParsing DB URI would be a helpful feature as well!Thanks so much, excited to get this up and running when everything is fixed!\n \nreply"
    ],
    "link": "item?id=43938607",
    "first_paragraph": ""
  },
  {
    "title": "Using Git-upload-pack for a simpler CI integration (screenshotbot.io)",
    "points": 15,
    "submitter": "tdrhq",
    "submit_time": "2025-05-09T22:44:05 1746830645",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=43941608",
    "comments": [
      "> it also makes it easier for us to integrate with other Git providers such as GitLab, BitBucket or Phabricator.I worry sometimes that we've made GitHub too loadbearing so its great to see a devtools company embrace git more generally that just GitHub\n \nreply",
      "Thanks! (author here)\n \nreply"
    ],
    "link": "https://blog.screenshotbot.io/2025/05/09/using-git-upload-pack-for-a-simpler-ci-integration/",
    "first_paragraph": "Screenshotbot BlogOne of the early decisions we made in Screenshotbot was to not have read-access to your GitHub repositories.This turned out to be a huge advantage for us. It makes our customers much more confident using our product, and makes it easier to get through Security reviews at bigger enterprises.But it also makes it easier for us to integrate with other Git providers such as GitLab, BitBucket or Phabricator. If we don\u2019t depend on custom APIs, our code just works for everyone. (We still need to access their APIs to comment on Pull Requests, but on many platforms that permission is a lot more granular.) In fact, a subset of our features will also work a self-hosted Git repository.But even though we don\u2019t need read access to your repository, we do need access to the \u201cgraph\u201d to make decisions such as \u201cfind the last commit for which we have a build available\u201d, or \u201cfind the best merge-base\u201d. We\u2019ll talk about how we did this in the past, and we\u2019ll also talk about our new feature u"
  },
  {
    "title": "21 GB/s CSV Parsing Using SIMD on AMD 9950X (nietras.com)",
    "points": 244,
    "submitter": "zigzag312",
    "submit_time": "2025-05-09T13:38:06 1746797886",
    "num_comments": 121,
    "comments_url": "https://news.ycombinator.com/item?id=43936592",
    "comments": [
      "It feels crazy to me that Intel spent years dedicating die space on consumer SKUs to \"make fetch happen\" with AVX-512, and as more and more libraries are finally using it, as Intel's goal is achieved, they have removed AVX-512 from their consumer SKUs.It isn't that AMD has better AVX-512 support, which would be an impressive upset on it's own. Instead, it is only that AMD has AVX-512 on consumer CPUs, because Intel walked away from their own investment.\n \nreply",
      "That is what Intel does, they build up a market (Optane) and then do a rug pull (Depth Cameras).  They continue to do this thing where they do a huge push into a new technology, then don't see the uptake and let it die. Instead of building slowly and then at the right time, doing a big push. Optane support was just getting mature in the Linux kernel when they pulled it. And they focused on some weird cost cutting move when marketing it as a ram replacement for semi-idle VMs, ok.They keep repeating the same mistakes all the way back to https://en.wikipedia.org/wiki/Intel_iAPX_432\n \nreply",
      "The rugpull on Optane was incredibly frustrating. Intel developed a technology which made really meaningful improvements to workloads in an industry that is full of sticky late adopters (RDBMSes). They kept investing until the point where they had unequivocally made their point and the late adopters were just about getting it... and then killed it!It's hard to understand how they could have played that particular hand more badly. Even a few years on, I'm missing Optane drives because there is still no functional alternative. If they just held out a bit longer, they would have created a set of enterprise customers who would still be buying the things in 2040.\n \nreply",
      "They are a weird company. Their marketing people showed up and invested a significant amount into a buy of optane gear with our OEM a few months before they killed the product. They pulled the rug in themselves in addition to the customers.\n \nreply",
      "Optane was incredible.  It's insane that Intel dropped this.\n \nreply",
      "I made the mistake early in our startup of spending several months and quite a bit of cash building our first iot product on the Intel Edison platform, only to get zero support on the bugs in the SPI chip and the non-existent (but advertised) microcontroller.  We finally gave up and made our own boards based on another SOM (and eventually stopped building boards entirely) and they rather unceremoniously cancelled the Edison in 2017.  I guess nobody else was surprised, but I had naively thought the platform did have potential and a huge company like Intel would support the things they sold.\n \nreply",
      "> this thing where they do a huge push into a new technology, then don't see the uptake and let it die.Do we need a second \"killed by google\"?To companies like Intel or Google anything below a few hundred million users is a failure. Had these projects been in a smaller company, or been spun out, they'd still be successful and would've created a whole new market.Maybe I'm biased \u2014 a significant part of my career has been working for German Mittelstand \"Hidden Champions\" \u2014 but I believe you don't need a billion customers to change the world.\n \nreply",
      "Intel's 5G radio department was formed in 2011 by buying another firm and then it was bought by Apple in 2019. Apple announced a 5G modem this year (C1) . It took 14 years to get a viable 5g wireless modem but still doesn't have feature parity with Apple's cellular modems in the other iPhones. So this happens pretty often by Intel.\n \nreply",
      "Until this day, I miss Optane \u2014 I work for a timeseries database company focused on finance, the amount of use cases I have that screams \u201cfaster than NVMe, slower than RAM\u201d is insane. And these companies have money to throw at these problems.Which begs the question, why isn\u2019t anyone else stepping into this gap? Is the technology heavily patented?\n \nreply",
      "Indeed. Octane/3dxpoint was mind blowing futuristic stuff but it was just gone after 5 years? On the market? Talk about short sighted.\n \nreply"
    ],
    "link": "https://nietras.com/2025/05/09/sep-0-10-0/",
    "first_paragraph": "Programming, mechanical sympathy, machine learning and .NET \u2764.Sep 0.10.0 was released April 22nd,\n2025 with optimizations\nfor AVX-512 capable CPUs like the AMD\n9950X (Zen 5) and updated benchmarks\nincluding the 9950X. Sep now achieves a staggering 21 GB/s on the 9950X for\nthe low-level CSV parsing. \ud83d\ude80 Before 0.10.0, Sep achieved ~18 GB/s on 9950X.See v0.10.0 release for\nall changes for the release, and Sep README on\nGitHub for full details.In this blog post, I will dive into how .NET 9.0 machine code for AVX-512 is\nsub-optimal and what changes were made to speed up Sep for AVX-512 by\ncircumventing this, showing interesting code and assembly along the way, so get\nready for SIMD C# code, x64 SIMD assembly and tons of benchmark numbers.However, first let\u2019s take a look at the progression of Sep\u2019s performance from\nearly 0.1.0 to 0.10.0, from .NET 7.0 to .NET 9.0 and from AMD Ryzen 9 5950X (Zen\n3) to 9950X (Zen 5), as I have also recently upgraded my work PC.The benchmark numbers above are fo"
  },
  {
    "title": "Math Machine \u2013 A notebook will show your kid how far they have travelled (kidswholovemath.substack.com)",
    "points": 44,
    "submitter": "sebg",
    "submit_time": "2025-05-06T09:29:15 1746523755",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=43903162",
    "comments": [
      "Seeing math on Lined Pages is unsettling to me. I agree with author that Blank Pages tend to become a mess, and that Gridded Pages are too \"noisy\", that's why Dotted pages are perfect and prefered for Journals, especially ones with very light dots. I wonder why I don't see more of those in math.\n \nreply",
      "Yeah, I also found dotted notebooks to be the sweet spot. It's cleaner than a lined or gridded notebooks and especially helpful if they're already numbered.The tweaks they found in the article is basically a proto-version of the Bullet Journal but just with its index system.Physical notebooks are nice but as I have to come to know throughout the years, they are also kind of \"disposable\" and cannot survive long-term if you have to do any amount of moving. You wish you could keep all of your journals/notebooks in an archive but seems infeasible when you don't have your own house or your house is just too small. The rising rent and house prices just makes this all the worse.\n \nreply",
      "I LOVE physical notebooks, paper, and ink. Physical notebooks do not crash, they do not lock up, they do not have DRM on them, they are not impacted by \"updates\" to pens or inks. Sure they can be difficult to \"backup\" and can be lost, but these are small cons against the much larger pros.From math, to science, to world building, and even to learning a notebook can be your best friend. A quote that has always stuck with me about notebooks has been  from Robert M. Pirsig's \"Zen and the Art of Motorcycle Maintenance\":> \u201cFor this you keep a lab notebook. Everything gets written down, formally, so that you know at all times where you are, where you\u2019ve been, where you\u2019re going and where you want to get. In scientific work and electronics technology this is necessary because otherwise the problems get so complex you get lost in them and confused and forget what you know and what you don\u2019t know and have to give up.\u201d\n \nreply",
      "Related: https://wiki.c2.com/?LogBook and GreyPatter https://news.ycombinator.com/item?id=29661167 (My productivity app for the past 12 years has been a single .txt file (2020))\n \nreply",
      "I've had similar ideas throughout the years of my personal mathematical journey:- When I took my Discrete Mathematics course, I began to keep a separate notebook to compile proofs that I found to be particularly clever or, I thought, illustrated a particular concept/approach clearly. It was partially inspired by Erd\u0151s' concept of The Book.- I did something similar for \"Leetcode\" problems but in this notebook I would only document solutions I personally came up with for when I feel like having solved the problem gave me considerable \"XP\" if not outright leveling me up. They ended up mostly dynamic programming problems and clever applications of number theory---I never really felt like I grokked these topics even now so it was useful when I detected similarities to past problems.- Lately I've decided to give signal processing a deeper shot and a grid notebook has been my invaluable companion for the task. The last two notebooks were very neatly organized but this one is more like lab/field notes but for mathematics. It's gritty and dirty in there. There isn't really much point learning signal processing only from books so I'm always in front of my computer (with a lecture video or an interactive Python script) when I'm working on this notebook. Being able to formulate hypotheses/intuition, writing down thoughts to be considered later, annotating graphs/proofs where things don't make sense to me yet---it has been an extremely liberating learning experience. I've only been on this endeavor for a few weeks but I can definitely say it has allowed me to interact with the material at a deeper level.The only way my signal processing effort could be better was if I had a teacher whom I can ask my noted-down questions to. I know the suggestion is gonna come up and I'll be lying if I say it hasn't crossed my mind so I'm just gonna address it unprompted (pun intended): I don't bring my questions to a LLM because I'm not yet smart enough to detect bullshit in this field of study. I don't think setting-up my own agent to ingest my notes would make any sense because then I would have to structure my notes and the core reason why it's been so liberating/has enabled me to interact deeper with the material is because I gave myself the freedom to be unstructured.\n \nreply",
      "This is an insane amount of verbiage to say \"your kids should have a notebook for every field they study\". Actually it doesn't even generalize it that far, it just obsesses on insane minutae of why it's good to have a notebook for math. Is this LLM spew?You can go to any drugstore or office supply shop and find cheap spiral-bound notebooks. Your school's supply list is probably asking you to do this and even specifying a size. Is your school not doing this? Are you home-schooling?\n \nreply",
      "Given your abhorrence of verbiage, here is some LLM spew instead:In Search of Lost Time by Marcel Proust is a monumental novel about memory, time, and the subtle shifts of consciousness. Through the narrator\u2019s recollections\u2014often triggered by sensory experiences like the taste of a madeleine\u2014Proust explores love, art, jealousy, and the passage of life in early 20th-century French society.\n \nreply"
    ],
    "link": "https://kidswholovemath.substack.com/p/math-machine",
    "first_paragraph": ""
  },
  {
    "title": "Past, present, and future of Sorbet type syntax (jez.io)",
    "points": 107,
    "submitter": "PaulHoule",
    "submit_time": "2025-05-09T16:09:45 1746806985",
    "num_comments": 69,
    "comments_url": "https://news.ycombinator.com/item?id=43938400",
    "comments": [
      "One thing that I've wondered is why sorbet didn't choose to use the stabby lambda syntax to denote function signatures?  sig ->(_: MyData) { }\n  def self.example(my_data)\n    ...\n  end\n\nObviously this opens up a potential can of worms of a dynamic static type system, but it looks sufficiently close enough to just ruby. My opinion is that sorbet doesn't lean into the weirdness of ruby enough, so while it has the potential to be an amazingly productive tool, this is the same community that (mostly) embraces multiple ways of doing things for aesthetic purposes. For example you could get the default values of the lambda above to determine the types of the args by calling the lambda with dummy values and capturing via binding.Personally having written ruby/rails/c#/etc and having been on a dev productivity team myself, I say: lean into the weird shit and make a dsl for this since that's what it wants to be anyways. People will always complain, especially with ruby/rails.\n \nreply",
      "I've been a ruby dev for 15+ years; i'd really love it if ruby adopted a C# similar approach to typing (albeit, more ruby-like and more flexible). It's the most readable, simplest way I would enjoy as a rubyist. Everything else (including sorbet) feels bolted on, cumbersome and cringe. I appreciate the article and how it goes over the constraints; but genuinely sorbet is just not good enough from a DSL standpoint.Type's can be fun and useful, and i'd love to see them incorporated into ruby in a tasteful way. i don't want it to become a new thing developers are forced to do, but there is a lot of utility from making them more available.\n \nreply",
      "While I'm most familiar with C#, and haven't used Ruby professionally for almost a decade now, I think we'd be better off looking at typescript, for at least 3 reasons, probably more.1. Flowsensitivity: It's a sure thing that in a dynamic language people use coding conventions that fit naturally to the runtime-checked nature of those types.  That makes flow-sensitive typing really important.2. Duck typing: dynamic languages and certainly ruby codebases I knew often use ducktyping.  That works really well in something like typescript, including really simple features such as type-intersections and unions, but those features aren't present in C#.3. Proof by survival: typescript is empirically a huge success.  They're doing something right when it comes to retrospectively bolting on static types in a dynamic language. Almost certainly there are more things than I can think of off the top of my head.Even though I prefer C# to typescript or ruby _personally_ for most tasks, I don't think it's perfect, nor is it likely a good crib-sheet for historically dynamic languages looking to add a bit of static typing - at least, IMHO.Bit of a tangent, but there was a talk by anders hejlsberg as to why they're porting the TS compiler to Go (and implicitly not C#) - https://www.youtube.com/watch?v=10qowKUW82U - I think it's worth recognizing the kind of stuff that goes into these choices that's inevitably not obvious at first glance.  It's not about the \"best\" lanugage in a vacuum, it's a about the best tool for _your_ job and _your_ team.\n \nreply",
      "Tangent, has C# recovered from nulls being included in all reference types by default?\n \nreply",
      "\"Recovered\" sounds so binary.I think it's pretty usuable now, but there is scarring. The solution would have been much nicer had it been around from day one; especially surrounding generics and constraints.It's not _entirely_ sound, nor can it warn about most mistakes when those are in the \"here-be-dragons\" annotations in generic code.The flow sensitive bit is quite nice, but not as powerful as in e.g. typescript, and sometimes the differences hurt.It's got weird gotcha interactions with value-types, for instance but likely not limited to interaction with generics that aren't constrained to struct but _do_ allow nullable usage for ref types.Support in reflection is present, but it's not a \"real\" type, and so everything works differently, and hence you'll see that code leveraging reflection that needs to deal with this kind of stuff tends to have special considerations for ref type vs. value-type nullabilty, and it often leaks out into API consumers too - not sure if that's just a practical limitation or a fundamental one, but it's very common anyhow.There wasn't last I looked code that allowed runtime checking for incorrect nulls in non-nullable marked fields, which is particularly annoying if there's even an iota of not-yet annoted or incorrectly annotated code, including e.g. stuff like deserialization.Related features like TS Partial<> are missing, and that means that expressing concepts like POCOs that are in the process of being initialized but aren't yet is a real pain; most code that does that in the wild is not typesafe.Still, if you engage constructively and are willing to massage your patterns and habbits you can surely get like 99% type-checkable code, and that's still a really good help.\n \nreply",
      "> Related features like TS Partial<> are missing, and that means that expressing concepts like POCOs that are in the process of being initialized but aren't yet is a real pain; most code that does that in the wild is not typesafe.If it's an object, it's as simple as having a static method on a type, like FromA(A value) and then have that static method call the constructor internally after it has assembled the needed state. That's how you'd do it in Rust anyway. There will be a warning (or an error if you elevate those) if a constructor exits not having initialized all fields or properties. Without constructor, you can mark properties as 'required' to prohibit object construction without assignment to them with object initializer syntax too.\n \nreply",
      "Yeah, before required properties/fields, C#'s nullability story was quite weak, it's a pretty critical part of making the annotations cover enough of a codebase to really matter. (technically constructors could have done what required does, but that implies _tons_ of duplication and boilerplate if you have a non-trivial amount of such classes, records, structs and properties/fields within them; not really viable).Typescript's partial can however do more than that - required means you can practically express a type that cannot be instantiated partially (without absurd amounts of boilerplate anyhow), but if you do, you can't _also_ express that same type but partially initialized. There are lots of really boring everyday cases where partial initialization is very practical. Any code that collects various bits of required input but has the ability to set aside and express the intermediate state of that collection of data while it's being collected or in the event that you fail to complete wants something like partial.E.g. if you're using the most common C# web platform, asp.net core, to map inputs into a typed object, you now are forced to either expression semantically required but not type-system required via some other path.  Or, if you use C# required, you must choose between unsafe code that nevertheless allows access to objects that never had those properties initialized, or safe code but then you can't access any of the rest of the input either, which is annoying for error handling.typescript's type system could on the other hand express the notion that all or even just some of those properties are missing; it's even pretty easy to express the notion of a mapped type wherein all of the _values_ are replaces by strings - or, say, by a result type.  And flow-sensitive type analysis means that sometimes you don't even need any kind of extra type checks to \"convert\" from such a partial type into the fully initialized flavor; that's implicitly deduced simply because once all properties are statically known to be non-null, well, at that point in the code the object _is_ of the fully initialized type.So yeah, C#'s nullability story is pretty decent really, but that doesn't mean it's perfect either.  I think it's important to mention stuff like Partial because sometimes features like this are looked at without considering the context. Most of these features sound neat in isolation, but are also quite useless in isolation. The real value is in how it allows you to express and change programs whilst simultaneously avoiding programmer error. Having a bit of unsafe code here and there isn't the end of the world, nor is a bit of boilerplate.  But if your language requires tons of it all over the place, well, then you're more likely to make stupid mistakes and less likely to have the compiler catch them. So how we deal with the intentional inflexibility of non-nullable reference types matters, at least, IMHO.Also, this isn't intended to imply that typescript is \"better\".  That has even more common holes that are also unfixable given where it came from and the essential nature of so much interop with type-unsafe JS, and a bunch of other challenges.  But in order to mitigate those challenges TS implemented various features, and then we're able to talk about what those feature bring to the table and conversely how their absence affects other languages. Nor is \"MOAR FEATURE\" a free lunch; I'm sure anybody that's played with almost any language with heavy generics has experienced how complicated it can get. IIRC didn't somebody implement DOOM in the TS type system?  I mean, when your error messages are literally demonic, understanding the code may take a while ;-).\n \nreply",
      "nope, still recovering: https://github.com/dotnet/core/blob/main/release-notes/10.0/...\n \nreply",
      "This has nothing to do with null analysis. It simply lets you replace an assignment behind an if with an inline expression.\n \nreply",
      "Yes. It will complain if you assign one to something that isn't T?.For the best experience you may want to add `<WarningsAsErrors>nullable</WarningsAsErrors>` to .csproj file.\n \nreply"
    ],
    "link": "https://blog.jez.io/history-of-sorbet-syntax/",
    "first_paragraph": "A lightly edited transcript of a talk I first gave on April 23, 2025.April 25, 2025Here\u2019s the elephant in the room: Sorbet\u2019s syntax is ugly.When people start complaining about Sorbet\u2019s syntax, I have to spend a lot of time deflecting or even defending it, which is annoying: I\u2019m right there with you, the syntax is ugly! It\u2019s verbose. It\u2019s foreign. It doesn\u2019t resemble any typed language, nor does it complement Ruby\u2019s unique style.My counter is that when it comes to language design, semantics\u2014what the types mean\u2014are easily 10 times more important than syntax. This is what I was taught; it\u2019s also what I\u2019ve seen. Programming is an act of theory building, and when you sit down to write code, you\u2019re trying to codify how you\u2019re thinking about a problem as much as instructions for the machine. Types become a tool to help get the semantics from your head into the codebase. That\u2019s a lossy process, and types fill a role kind of like \u201cerror correcting codes\u201d for brain dumping.So I try very hard to "
  },
  {
    "title": "Rust\u2019s dependencies are starting to worry me (vincents.dev)",
    "points": 179,
    "submitter": "chaosprint",
    "submit_time": "2025-05-09T09:11:05 1746781865",
    "num_comments": 229,
    "comments_url": "https://news.ycombinator.com/item?id=43935067",
    "comments": [
      "IMO any system where taking a dependency is \"easy\" and there is no penalty for size or cost is going to eventually lead to a dependency problem. That's essentially where we are today both in language repositories for OSS languages and private monorepos.This is partly due to how we've distributed software over the last 40 years. In the 80s the idea of a library of functionality was something you paid for, and painstakingly included parts of into your size constrained environment (fit it on a floppy). You probably picked apart that library and pulled the bits you needed, integrating them into your builds to be as small as possible.Today we pile libraries on top of libraries on top of libraries. Its super easy to say `import foolib`, then call `foolib.do_thing()` and just start running.  Who knows or cares what all 'foolib' contains.At each level a caller might need 5% of the functionality of any given dependency. The deeper the dependency tree gets the more waste piles on. Eventually you end up in a world where your simple binary is 500 MiB of code you never actually call, but all you did was take that one dependency to format a number.In some cases the languages make this worse. Go and Rust, for example, encourage everything for a single package/mod to go in the same file.  Adding optional functionality can get ugly when it would require creating new modules, but if you only want to use a tiny part of the module, what do you do?The only real solution I can think of to deal with this long term is ultra-fine-grained symbols and dependencies.  Every function, type, and other top-level language construct needs to declare the set of things it needs to run (other functions, symbols, types, etc).  When you depend on that one symbol it can construct, on demand, the exact graph of symbols it needs and dump the rest for any given library.  You end up with the minimal set of code for the functionality you need.Its a terrible idea and I'd hate it, but how else do you address the current setup of effectively building the whole universe of code branching from your dependencies and then dragging it around like a boat anchor of dead code.\n \nreply",
      "Way back when, I used to vendor all the libraries for a project (Java/Cpp/Python) into a mono repo and integrate building everything into the projects build files so anyone could rebuild the entire app stack with whatever compiler flags they wanted.It worked great, but it took diligence, it also forces you to interact with your deps in ways that adding a line to a deps file does not.\n \nreply",
      "This is the default way of doing things in the monorepo(s) at Google.It feels like torture until you see the benefits, and the opposite ... the tangled mess of multiple versions and giant transitive dependency chains... agony.I would prefer to work in shops that manage their dependencies this way. It's hard to find.\n \nreply",
      "This idea is already implemented in Dotnet, with Trimming and now ahead of time compilation (AOT). Maybe other languages can learn from dotnet?https://learn.microsoft.com/en-us/dotnet/core/deploying/trim...https://learn.microsoft.com/en-us/dotnet/core/deploying/nati...\n \nreply",
      "dead code elimination is a very old shoewhich get reinvented all the time, like in dotnet with \"trimming\" or in JS with \"tree-shaking\".C/C++ compiler have been doing that since before dot net was a thing, same for rust which does that since it's 1.0 release (because it's done by LLVM ;) )The reason it gets reinvented all the time is because while it's often quite straight forward in statically compiled languages it isn't for dynamic languages as finding out what actually is unused is hard (for fine grained code elimination) or at lest unreliable (pruning submodules). Even worse for scripting languages.Which also brings use to one area where it's not out of the box, if you build .dll/.so in one build process and then use them in another. Here additional tooling is needed to prune the dynamic linked libraries. But luckily it's not a common problem to run into in Rust.In general most code size problems in Rust aren't caused by too huge LOC of dependencies but by an overuse of monopolization. The problem of tons of LOC in dependencies is one of supply chain trust and review ability more then anything else.\n \nreply",
      "Those are done at compile time.  Many languages (including Rust, which this story is about) also remove unused symbols at compile time.The comment you're replying to is talking about not pulling in dependencies at all, before compiling, if they would not be needed.\n \nreply",
      "As far as I'm aware, LTO completely solves this from a binary size perspective. It will optimise out anything unused. You can still get hit from a build time perspective though.\n \nreply",
      "\"completely solves\" is a bit of an overstatement. Imagine a curl-like library that allows you to make requests by URL. You may only ever use HTTP urls, but code for all the other schemas (like HTTPS, FTP, Gopher) needs to be compiled in as well.This is an extreme example, but the same thing happens very often at a smaller scale. Optional functionality can't always be removed statically.\n \nreply",
      "That only applies when dynamic dispatch is involved and the linker can't trace the calls. For direct calls and generics(which idiomatic Rust code tends to prefer over dyn traits) LTO will prune extensively.\n \nreply",
      "let uri = get_uri_from_stdin();\n    networking_library::make_request(uri);\n\nHow is the compiler supposed to prune that?\n \nreply"
    ],
    "link": "https://vincents.dev/blog/rust-dependencies-scare-me/?",
    "first_paragraph": "\n            I do not pretend to have more experience or understanding than the top level engineers I often see writing blogs here, but what I am is annoyed. I love rust. It's by far my favorite language, I enjoy the community and the language ergonomics. I've been able to incorporate it into projects in my day job where fit.\n        \n            Rust's dependencies are starting to worry me. For those who don't code in Rust, Rust provides an ecosystem of crates through crates.io, allowing users to helpfully install them with the command cargo add, or by simply adding the requested crate and version to the Cargo.toml file. This paradigm is similar to NPM for nodejs.\n        Cargo is helpful and a huge productivity booster, ensuring I don't have to hunt around and link files myself with something like CMake. This also allows me to swap between architectures and operating systems pretty frequently, mainly my M1 macbook air and my x86 Debian desktop. In general I don't have to think much a"
  },
  {
    "title": "New Tool: lsds \u2013 List All Linux Block Devices and Settings in One Place (tanelpoder.com)",
    "points": 72,
    "submitter": "mfiguiere",
    "submit_time": "2025-05-09T18:13:38 1746814418",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=43939617",
    "comments": [
      "I'll note, lsblk can return a heck of a lot more data than it does by default (and nvme drives show up there). lsblk -H will list for your system, and you can specify columns. You can also adjust output.I guess with this in mind, I'm curious how this is different?\n \nreply",
      "Hi, yep lsblk targets a wider area of functionality, like showing mountpoints, device UUIDs, while lsds focuses only on block device settings.Maybe the latest Linux versions have lsblk versions that support these columns, but in RHEL9 at least I don't see equivalents to lsds'es WBT_LAT, QDEPTH (not the same as lsblk's RQ-SIZE), WCACHE, FUA and some others. But these 4 are which I regularly need (especially when troubleshooting a yet another slow fsync() issue etc). I did and do use lsblk all the time too, but still end up catting and grepping various additional files and correlating the results, sometimes on systems with 100+ multipath block devices.The other reason was that I wanted a tool that shows me where it gets these values too (for myself and sometimes for explaining stuff to others).Edit: That being said, it shouldn't be hard at all to add the said extra fields to lsblk too.\n \nreply",
      "Rewrote most of the functionality in C as an exercisehttps://gist.github.com/grahameger/2507019334f07036f84080a87...\n \nreply",
      "Very nice, needs option for json/jsonl output.\n \nreply",
      "Thanks! Yep I was thinking of doing that next, will be very easy as under the hood the data is stored in Python dictionaries.\n \nreply",
      "I always wanted the /dev/zero character device driver, which you can map into memory to clear it, or use as an infinite source of nulls, to use the minor node number as the value that got mapped into memory or produced, so you could make an infinite source of beeps with:mknod /dev/seven c 1 7I wonder what would happen if you made a /dev/seven device in your http servers public_html directory? Would it dutifully serve it up?Better yet, support for utf-8 unicode, so you can make an infinite source of poo emojis.The \"Everything Is A File\" philosophy should be taken to its logical conclusion.\n \nreply",
      "Awesome! That actually inspired me to code this: https://codeberg.org/mco-system/pooper\n \nreply",
      "Question: what actually reads /etc/pooper to configure the character? I can\u2019t work out how that file\u2019s contents ends up as module parameters and I\u2019d love to know!\n \nreply",
      "I challenge anyone to find another place on the Internet where one person's joke is another person's kernel module.\n \nreply",
      "Easy to get an infinite stream of bell codes with:\nyes ^V^G\n \nreply"
    ],
    "link": "https://tanelpoder.com/posts/lsds-list-linux-block-devices-and-their-config/",
    "first_paragraph": "When dealing with disks and I/O things on Linux, you\u2019d regularly run commands like lsblk, lsscsi, nvme list, etc. All of them tend to report a different set of information, so I ended up running multiple commands and correlating their output based on the device name or number.And then I had to run commands like these, to get extra info about the current OS-level configuration settings for specific disks:The above commands would show the hardware-advertised device queue depth and the OS block device level (software) queue depth.I finally had enough and created a single Python program lsds for showing all interesting bits for a disk in one place. The lsds tool does not execute any other commands under the hood, it just reads the relevant info from sysfs /sys/class/blocks/... directories directly.See my 0x.tools  toolset for more Linux performance & troubleshooting tools.Here\u2019s an output from my machine with 21 SSDs in it. You may need to scroll right to see the full output:All the above "
  },
  {
    "title": "Show HN: Hydra (YC W22) \u2013 Serverless Analytics on Postgres (hydra.so)",
    "points": 41,
    "submitter": "coatue",
    "submit_time": "2025-05-09T15:24:35 1746804275",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=43937852",
    "comments": [
      "Hello Joe, thanks a lot for hydra and pg_duckdb. I wanted to confirm that for self hosting hydra i have to generate a token from your platform? what data is shared with hydra for this case. We need to double check as our data has restriction of sharing.> Visit http://platform.hydra.so/token to fetch the access token and paste it into the section above.\n \nreply",
      "Hello thawab, yes! you can self-host Hydra with a token from the platform. Sign-up and visit that URL to take you to the right spot. We call it Bare Metal deployment, here's 1 minute setup guide (https://docs.hydra.so/guides/bare_metal)\n \nreply",
      "thanks a lot, the other part of the question:1- what data is shared with hydra for this case?2- whats the pricing for the bare metal deployment?\n \nreply",
      "billing (usage) metrics so we know what to charge. We offer BYOC 'Bare Metal' deployments as part of the Business plan. You can set it up now, but we offer volume discounts so you should talk to our team directly. Feel free to DM me on X (@JoeSciarrino) or email founders@\n \nreply",
      "Hey there, congrats on publicly launching this after your work over the past months!Having followed the project for a while now, I really scratch my head when looking at your pricing.The entire innovation of the past decade in database land has gone towards decoupling storage and compute, driving query engines (like DuckDB) and file formats (like Iceberg).Yet you force-bundle storage and compute in your pricing while also selling a serverless product.What's the reason behind that?Why do it in the first place?How does your pricing work?The 40/ 500 compute hours I get are included in the spend limit per tier (i.e. max 160 additional hours in Starter etc.) or completely separate?Why are there member constraints on a database product?How does that factor into cost/ map to SDL / reasonable team setups of people operating analytics projects revolving around a database like yours?I have never seen such a limit with any other vendor and esp. when you wanna get a hold in the market/ have people start using Hydra for the specialized role it can provide, having a 2 person limit for the minimum tier if I wanna PoC this would likely be a show stopper tbh...\n \nreply",
      "[Joe, Hydra cofounder] Hey there, I appreciate you taking the time to write this up - helps a lot to hear what's confusing.One of the downsides of serverless is that it can be difficult to predict the overall monthly cost when the granularity of billing (per invocation, memory usage, or execution time) is complex. For developers this might be totally fine (even preferred), but we think that giving a single, predictable price: Hydra $100 / month is better for businesses to plan around.Usage caps per plan are purely soft limits so users don't actually encounter them. Yes, we want people to upgrade to higher plans. In the words of Maya Angelou \"Be careful when a naked person offers you a shirt\" - meaning, we believe these are the best prices we can offer today to build a sustainable project on. That said, I appreciate your point about our # of users limit. If we removed that limit would you try out Hydra?\n \nreply",
      "my team is currently looking into offloading some of our analytics data into a columnar database next year. hydra and clickhouse were the top ones on the list. would love a breakdown of how the two compare.\n \nreply",
      "[Joe, Hydra cofounder] Hey, that's really great - I love hearing that. Hydra is a columnar database with an integrated Postgres rowstore. Analytics aren't purely best on columnar: we've heard from users that their analytics workload would benefit from fast lookup on row tables too, not just scanning large tables. Our goal for Hydra is to enable realtime analytics on Postgres without requiring an external analytics database. This makes it possible to join the rowstore and columnstore data in Postgres with direct SQL. Other analytics databases typically rely on ETL pipelines to move data out of Postgres, which depending on your scale, can become expensive and introduce delay.\n \nreply",
      "from what you wrote above, it seems like a great value add for greenfield projects.we currently use aws aurora. how easy would it be to simply sql dump and load into hydra and how well would it serve as a drop in replacement?\n \nreply",
      "Close to a drop-in replacement since Aurora bills itself as Postgres. Any data you load into Hydra will automatically be converted into the columnstore! we're happy to help out and feel free to DM me directly.\n \nreply"
    ],
    "link": "https://www.hydra.so/",
    "first_paragraph": "Connect and receive 14 days free Open source, Postgres-nativePredictable, serverless executionCompute autoscalingBottomless, columnar storageSub-second, realtime analyticsAutomatic cachingHydra offers predictable, sub-second response times at every scale. Every project includes autoscaling with automatic caching.Hannes M\u00fchleisenFounder & CEO, DuckDB Foundation & DuckDB LabsPete HuntFounder & CEO, DagsterTom HacohenFounder & CEO, SvixOwain BrennanFounder & CEO, SeerBiOvais TariqFounder & CEO, Tigris DataHarold GimenezSVP Engineering, HashiCorpJames ArthurFounder & CEO, ElectricSQLJordan TiganiFounder & CEO, MotherDuckStar it \u2b50\ufe0f Follow us on X\u201cHydra's columnar Postgres just works. We've run on Hydra for a year now without tuning performance. Great service!\u201cWe are building the foundation of our analytics stack around Postgres. Hydra is a no-brainer and our data compressed by 5X.\"\u201cHydra is a very well documented open source solution, and has an incredibly engaged team. They\u2019ve always been "
  },
  {
    "title": "Itter.sh \u2013 Micro-Blogging via Terminal (itter.sh)",
    "points": 198,
    "submitter": "rrr_oh_man",
    "submit_time": "2025-05-09T14:02:02 1746799322",
    "num_comments": 60,
    "comments_url": "https://news.ycombinator.com/item?id=43936884",
    "comments": [
      "It didn't get much attention when I posted it earlier this week, but I made an SSH movie player:ssh ansi.rya.nc(currently shows Sneakers, complete with subtitles)\n \nreply",
      "Usually I get really annoyed with people who hijack a thread to post their own thing, but ok, yea, this is pretty amazing.  The quality is superb.I do also love itter.sh\n \nreply",
      "I was about to say 'please do not hijack the thread' but read your comment and ssh'ed into it. This is amazing.\n \nreply",
      "HOLY FUCK, WOW. Can we have a call?\n \nreply",
      "Just mplayer -vo caca myvideo.mp4\n \nreply",
      "It's _way_ higher quality than that.\n \nreply",
      "drop me an email\n \nreply",
      "It looks completely garbled on my end\n \nreply",
      "here's a screenshot with a terminal that works with it https://media.infosec.exchange/infosec.exchange/media_attach...\n \nreply",
      "It needs a terminal with 24 bit color support, and at least 80x24. In particular, gnu screen doesn't work.\n \nreply"
    ],
    "link": "https://www.itter.sh/",
    "first_paragraph": "Social Media for Purists.New user? Register:Already joined? Login:itter.sh is your escape from the noise. It's a micro-blogging platform accessed entirely via SSH. No web browser. No JavaScript. No endless scroll of algorithmic 'content'. Just you, your trusty terminal, and 180 characters at a time (\"eets\"). Why? Because terminals are cool. Because less is more. Because sometimes, you just need to type.You'll need an SSH key. Don't have one? Generate like this: (We only need your .pub file for registration.)Ready to join? Use the register: prefix with your desired username:Once registered, just SSH in. Type help for commands.ittereet <text>- Post an eet (max 180 chars). Supports #hashtags and @mentions.ittertimeline [mine|all|#chan|@user] [<page>]- Show eets. Default: 'all', page 1.itterwatch [mine|all|#chan|@user]- Live timeline view. Updates automatically.itterfollow @<user>- Follow a user.itterunfollow @<user>- Unfollow a user.itterprofile [@<user>]- View user profile (yours or anot"
  },
  {
    "title": "Reverse Engineering \"DNA Sequences\" in the Lost World: Jurassic Park Video Game (32bits.substack.com)",
    "points": 56,
    "submitter": "bbayles",
    "submit_time": "2025-05-07T15:51:08 1746633068",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=43917278",
    "comments": [
      "This got me itching  to mess with old game code again,  honestly. ever notice games like this kinda stick in your brain for way too long?\n \nreply",
      "I love these reverse engineering game posts. Seems like a fun hobby.\n \nreply",
      "This is an interesting one in the same vein! https://steamcommunity.com/sharedfiles/filedetails/?id=28557...\n \nreply",
      "This is such a weird coincidence! I think about those DNA sequences surprisingly often, and I'm considering building a similar UI/concept into a puzzle game I'm working on.\n \nreply"
    ],
    "link": "https://32bits.substack.com/p/under-the-microscope-the-lost-world",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Aberdeen \u2013 An elegant approach to reactive UIs (aberdeenjs.org)",
    "points": 184,
    "submitter": "vanviegen",
    "submit_time": "2025-05-09T12:42:29 1746794549",
    "num_comments": 101,
    "comments_url": "https://news.ycombinator.com/item?id=43936097",
    "comments": [
      "Conceptually, this sounds incredibly similar to what Vue was in the 0.x stage. Are you familiar with Vue perhaps? What would you say are the core conceptual differences? At a glance, both aim for the same idea, that your data is proxied, you detect what changes actually matter, and you try and wrap all of that up in a neat little package. These days of course Vue is a larger beast, but I feel like the origins were very similar to what you have here. Interested to hear what you think about this comparison.\n \nreply",
      "At first glance and on a syntax level, Vue from the start had part of its code in html syntax and the rest on JS. Aberdeen goes fully into JS.So if I get it right, in Aberdeen there would not be any pure html written at all, right? Is that the \"ideal\"? Or it would be more of a hybrid with Aberdeen accompanying plain html?\n \nreply",
      "Correct!As far as I know, Vue has always had its own HTML-based template engine, with special HTML attributes for conditions, loops, etc. Different trade-off.Since Vue 3, it does indeed rely on `Proxy` for its reactivity, like Aberdeen.The idea is the write whole applications without HTML. We've done some pretty big projects in this style, and in terms of DX and velocity it's actually really good. Recycling (tiny) components becomes really easy, as it's all just JavaScript functions.\n \nreply",
      "Aberdeen looks a bit like the venerable Mithril in terms of DX.\n \nreply",
      "Yes, I noticed that similarity too a bit -- from the use of a \"HyperScript\"-like approach (although Mithril can be used with JSX too). https://mithril.js.org/hyperscript.htmlAberdeen is innovative in that it's great to be able to avoid the VDOM in a technical sense. For fun, years ago, I messed around with a different technical approach of just rendering HTML from code directly (similar to Aberdeen to that extent), but instead of an observables-ish idea, I used Mithril's approach of hooking into the event listening system to trigger redraws by wrapping event handlers. Of course, I quickly ran into the issue of losing state like cursor insertion points in editors when re-rendering (which is what motivates the vdom approach). I can wonder if maybe the HTML standard itself could  be upgraded somehow to support that approach of replacement of HTML widgets but with retained widget state if an ID or such remains the same?While I applaud Aberdeen as an experiment, with Aberdeen, it seems problematical developer ergonomics to have everything be a reactive pipeline of some form of observables or signals. It's not especially more terrible than many other systems (e.g. Angular in practice, encouraging the use of RxJS Observables as a new way of encouraging writing hard-to-understand code). As Aberdeen has the previously mentioned innovation of avoiding the vdom, overall it might be a win for people who like that style of coding.But by comparison, I have found Mithril is so much easier to work with -- given the core idea of just assuming the UI is \"dirty\" after any listened-for event happens and re-rendering it.It's true though that Angular has a \"Zones\" approach to redraw when state changes which hides a lot of complexity but has OK developer ergonomics similar to Mithril in that sense -- until you get mired in RxJS code encouraged by the platform (at least when I last used it seven or so years ago). And then Angular code get mired in various other Angular-isms which requires bloated code in practice with lots of files to do the simplest component. Yes, you can try to work around a proliferation of files, but that is not the Angular way, or at least was not several years ago. And Angular advocates would argue the standardization is a big win for larger projects with multiple developers. YMMV.Also, dsego, thanks for submitting to HN in 2022 the essay I wrote on all that (which I just saw thinking prompted by seeing this article to check if it had already been submitted somehow): https://news.ycombinator.com/item?id=25194873For others, this the essay which goes into more depth comparing developer ergonomics of React, Angular, and Mithril from my experience:\n\"Why I prefer Mithril over Angular and React\"\nhttps://github.com/pdfernhout/choose-mithrilFor a simple Mithril example with no special Observable or signal magic, see, say: https://kevinfiol.com/blog/simple-state-management-in-mithri...    let count = 0;\n\n    const Counter = {\n      view: () =>\n        m('div',\n          m('h1', 'Counter'),\n          m('p', count),\n          m('button', { onclick: () => count += 1 }, '+'),\n          m('button', { onclick: () => count -= 1 }, '-')\n        )\n    };\n\n    m.mount(document.body, Counter);\n\nThe state is stored in just a regular JavaScript variable (\"count\" in this case), not a wrapped something or other special object (like Aberdeen or any other Observable or signal approach requires). This is made possible in Mithril in this case by the onclick method being transparently wrapped bu the HyperScript \"m\" function (or JSX equivalent) to call m.redraw() after the click event is handled. As with Mithril's HyperScript approach of leveraging JavaScript to generate HTML instead of inventing a weird non-standard templating system like JSX, Mithril's wrapping of event handlers leverages the ability to just simply define state in JavaScript closures or objects -- or whatever other more complex approach you want including even RxJS observables (as long as m.redraw() is called as needed if you make data changes outside an event handler). Lego Horie, Mithril's original inventor, just was brilliant with Mithril at creating great developer ergonomics.\n \nreply",
      "Here's what your Mithril examples looks like with Aberdeen:  let count = proxy(0);\n  \n  function Counter() {\n      $('div', () => {\n          $('h1:Counter'),\n          $('p:'+count.value),\n          $('button:+', { click: () => count.value += 1 }),\n          $('button:-', { click: () => count.value -= 1 })\n      })\n  }\n  \n  $(Counter);\n  // Or just Counter() would work in this case\n\nThe '.value' is needed because we can't proxy a number directly. This extra step goes a way if you have more data to store, like `person = {name: 'Peter', age: 42}`.Besides that, looks pretty similar yeah! Works rather differently though. :-)\n \nreply",
      "Thanks for the example. I agree they look very similar, which is what dsego saw intuitively and commented on.The difference in the two examples is essentially whether things need to be proxied or not.If someone does not mind that proxying, then Aberdeen has an advantage over Mithril by avoiding the complexity of the vdom internally. So, presumably the rendering code is simpler, which may have some benefits in performance and also debuggability in some cases. You can get surprising vdom-code-related errors from Mithril sometimes for something misconfigured in a hyperscript \"m\" call. Also the vdom regeneration can sometimes have issues if you don't specify a \"key\" for each m() widget, needed sometimes so the vdom can be sure to replace DOM nodes efficiently and correctly.If someone does mind the proxying requirement (like me), then Mithril has the advantage in that regard of seeming simpler to code for (without the data wrappers). Mithril applications also may be easier to debug in some other cases -- because you don't have an extra proxy levels as indirection in viewing your data model.So, both approaches have their pros and cons depending on preferences and priorities.\n \nreply",
      "> it seems problematical developer ergonomics to have everything be a reactive pipeline of some form of observables.Why do you think that would hurt ergonomics? You can use whatever data structures you normally would (including typed class instances), you'll only need to proxy() them to be observable.\n \nreply",
      "Thanks for the replies. Having to add \"proxy\" or similar everywhere is the ergonomics issue to me (others might disagree).\n \nreply",
      "Congrats for reaching 1.0! Nice little library, but as it's signals based, it would be nice to make it compatible with the signals proposal (https://github.com/tc39/proposal-signals)At the same time for me, while it's super nice, in my opinion it just doesn't differentiate enough from other signals based frameworks to get mass adopted / make CRUD apps that much easier to make.The problem with remote server/database is ,,what data to sync and when'' by the way, it's very different problem from what your framework is solving.I loved Svelte until I started using SvelteKit and realized how hard the data synchronization part is.\n \nreply"
    ],
    "link": "https://aberdeenjs.org/",
    "first_paragraph": "Build blazing-fast, reactive UIs in pure TypeScript/JavaScript \u2013 no virtual DOM.Aberdeen offers a refreshingly simple approach to reactive UIs. Its core idea:Use many small, anonymous functions for emitting DOM elements, and automatically rerun them when their underlying proxied data changes. This proxied data can be anything from simple values to complex, typed, and deeply nested data structures.Now, let's dive into why this matters...First, let's start with the obligatory reactive counter example. If you're reading this on the official website you should see a working demo below the code, and an 'edit' button in the top-right corner of the code, to play around.Okay, next up is a somewhat more complex app - a todo-list with the following behavior:Pfew.. now let's look at the code:Some further examples:And you may want to study the examples above, of course!"
  }
]