[
  {
    "title": "You should write an agent (fly.io)",
    "points": 261,
    "submitter": "tabletcorry",
    "submit_time": "2025-11-06T20:37:06 1762461426",
    "num_comments": 130,
    "comments_url": "https://news.ycombinator.com/item?id=45840088",
    "comments": [
      "There is a lot of stuff I should do. From making my own CPU from a breadboard of nand gates to building a CDN in Rust. But aint got time for all the things.That said I built an LLM following Karpathy's tutorial. So I think it aims good to dabble a bit.reply",
      "Two years ago I wrote an agent in 25 lines of PHP [0]. It was surprisingly effective, even back then before tool calling was a thing and you had to coax the LLM into returning structured output. I think it even worked with GPT-3.5 for trivial things.In my mind LLMs are just UNIX strong manipulation tools like `sed` or `awk`: you give them an input and command and they give you an output. This is especially true if you use something like `llm` [1].It then seems logical that you can compose calls to LLMs, loop and branch and combine them with other functions.[0] https://github.com/dave1010/hubcap[1] https://github.com/simonw/llmreply",
      "I love hubcap so much. It was a real eye-opener for me at the time, really impressive result for so little code. https://simonwillison.net/2023/Sep/6/hubcap/reply",
      "I should? what problems can I solve, that can be only done with an agent? As long as every AI provider is operating at a loss starting a sustainably monetizable project doesn't feel that realistic.reply",
      "The post is just about playing around with the tech for fun. Why does monetization come into it? It feels like saying you don't want to use Python because Astral, the company that makes uv, is operating at a loss. What?reply",
      "Agents use Apis that I will need to pay for and generally software dev is a job for me that needs to generate income.If the Apis I call are not profitable for the provider then they won't be for me either.This post is a fly.io advertisementreply",
      "Yeah we have open source models too that we can use, and it\u2019s actually more fun than using cloud providers in my opinion.reply",
      "> As long as every AI provider is operating at a lossNone of them are doing that.They need funding because the next model has always been much more expensive to train than the profits of the previous model. And many do offer a lot of free usage which is of course operated at a loss. But I don't think any are operating inference at a loss, I think their margins are actually rather large.reply",
      "Parent comment never said operating inference at a loss, though it wouldn't surprise me, they just said \"operating at a loss\" which they most definitely are [0].However, knowing a few people on teams at inference-only providers, I can promise you some of them absolutely are operating inference at a loss.0. https://www.theregister.com/2025/10/29/microsoft_earnings_q1...reply",
      "> Parent comment never said operating inference at a lossContext. Whether inference is profitable at current prices is what informs how risky it is to build a product that depends on buying inference, which is what the post was about.reply"
    ],
    "link": "https://fly.io/blog/everyone-write-an-agent/",
    "first_paragraph": "Some concepts are easy to grasp in the abstract. Boiling water: apply heat and wait. Others you really need to try. You only think you understand how a bicycle works, until you learn to ride one.There are big ideas in computing that are easy to get your head around. The AWS S3 API. It\u2019s the most important storage technology of the last 20 years, and it\u2019s like boiling water. Other technologies, you need to get your feet on the pedals first.LLM agents are like that.People have wildly varying opinions about LLMs and agents. But whether or not they\u2019re snake oil, they\u2019re a big idea. You don\u2019t have to like them, but you should want to be right about them. To be the best hater (or stan) you can be.So that\u2019s one reason you should write an agent. But there\u2019s another reason that\u2019s even more persuasive, and that\u2019sAgents are the most surprising programming experience I\u2019ve had in my career. Not because I\u2019m awed by the magnitude of their powers \u2014 I like them, but I don\u2019t like-like them. It\u2019s because"
  },
  {
    "title": "Two billion email addresses were exposed (troyhunt.com)",
    "points": 316,
    "submitter": "esnard",
    "submit_time": "2025-11-06T20:20:23 1762460423",
    "num_comments": 225,
    "comments_url": "https://news.ycombinator.com/item?id=45839901",
    "comments": [
      "There have been enough data breaches at this point that I'm sure all my info has been exposed multiple times (addresses, SSN, telephone number, email, etc).  My email is in over a dozen breaches listed on the been pwned site.  I've gotten legal letters about breaches from colleges I applied to, job boards I used, and other places that definitely have a good amount of my past personal information.  And that's not even counting the \"legal\" big data /analytics collected from past social media, Internet browsing, and whatever else.I now use strong passwords stored in bitwarden to try to at least keep on top of that one piece. I'm sure there are unfortunately random old accounts on services I don't use anymore with compromised passwords out there.Not really sure what if anything can be done at this point. I wish my info wasn't out there but it is.reply",
      "I used per-account email with alias servives and password managers.Also started migrating old accounts in free time.Now its pretty easy to tell the source of leak by email addresses as well as sources of spam.---Per-account alias might sound much, but using sieve filtering [1] is amazing, and you can get a comprehensive filtering solution going with 'envelope to' (the actual address receiving the email) + 'header to' (the recipient address you see, sometimes filtering rules don't filter for BCC or sometimes recipients are alias instead of your actual email) that are more comprehensive then normal filtering rules to sort your emails into folders.[1]: https://datatracker.ietf.org/doc/html/rfc5228---Amusingly, I've managed to recover old accounts from emails that contains my old passwords with demands for crypto payment, it just provided me enough help to recall old variations for my passwords.reply",
      "+1 for Bitwarden. It is literally the best solution out there. Been getting to increase uptake in personal circles with (very) limited success. The wife keeps trying to convince me that the ship has sailed in trying to protect info online. She's probably right.reply",
      "Now that I'm not only using a Macbook and iPhone, I've been looking for cross-platform solutions.For a week I've been using KeePassXC + Syncthing between four devices. Syncthing is also syncing my Obsidian vaults which has replaced Apple-only Notes.app.Bitwarden is definitely more polished, and Syncthing is definitely (much) more fiddly than using Bitwarden's and Obsidian's ($5/mo) native syncing tools.But I like the idea of having the same syncing solution across all apps on all devices. Curious if anybody can recommend this setup or if collisions will make it unbearable.reply",
      "I have used this setup for 6 years or so with KeePassXC and it's fine. Just being mindful of not editing stuff on other devices before the first one has had the chance to sync has been enough to avoid pretty much all sync conflicts. I have only had to resolve those a few times so far, iirc my android client was misconfigured at the time or something.I still recommend Bitwarden for password management for any \"laypeople\" since it will just work. Also worth noting that the basic functionality is free.reply",
      "This is the same setup I used for years with no issues, both KeePassXC and multiple Obsidian vaults, along with some other random files and folders. Syncthing is pretty much rock solid. Now I have the KeePassXC database stored on my NAS which is even simpler.reply",
      "The cool thing with KeePass is that each client is also a local backup. It's pretty neat.reply",
      "I use a similar setup, but with Onedrive instead of Syncthing (and, before that, Dropbox).In the almost 10 years I've been running this setup, I think I hit a conflict one single time. I don't quite remember the details, but I think I accidentally edited something in the mobile app, and before saving, edited something else in the desktop app or vice-versa. So it was pretty much my fault.Other than that, literally never had an issue. Password managers are by their nature mostly reads, and very occasional writes, so it's very hard to put yourself in a situation where conflicts happen, even if you don't pay attention to it. I've made an identical setup for my (fairly savvy but non-technical) fiancee, and she's never hit an issue either. I had to insist a bit for her to get on board, but years later she actually loves using KeePass. She's thanked me multiple times for how convenient it is not having to remember passwords anymore!reply",
      "strongbox is a reasonable app for iOS and you can set it up for sftp to your main self hosted server.reply",
      "I originally started using Bitwarden to achieve sync across Mac, Windows, and Linux machines, along with all major browser platforms. It's been great!reply"
    ],
    "link": "https://www.troyhunt.com/2-billion-email-addresses-were-exposed-and-we-indexed-them-all-in-have-i-been-pwned/",
    "first_paragraph": ""
  },
  {
    "title": "Kimi K2 Thinking, a SOTA open-source trillion-parameter reasoning model (moonshotai.github.io)",
    "points": 568,
    "submitter": "nekofneko",
    "submit_time": "2025-11-06T15:06:06 1762441566",
    "num_comments": 227,
    "comments_url": "https://news.ycombinator.com/item?id=45836070",
    "comments": [
      "uv tool install llm\n  llm install llm-moonshot\n  llm keys set moonshot # paste key\n  llm -m moonshot/kimi-k2-thinking 'Generate an SVG of a pelican riding a bicycle'\n\nhttps://tools.simonwillison.net/svg-render#%3Csvg%20width%3D...Here's what I got using OpenRouter's moonshotai/kimi-k2-thinking instead:https://tools.simonwillison.net/svg-render#%20%20%20%20%3Csv...reply",
      "I suspect that the OpenRouter result originates from a quantized hosting provider. The difference compared to the direct API call from Moonshot is striking, almost like night and day. It creates a peculiar user and developer experience since OpenRouter enforces quantization restrictions only at the API level, rather than at the account settings level.reply",
      "Love seeing this benchmark become more iconic with each new model release. Still in disbelief at the GPT-5 variants' performance in comparison but its cool to see the new open source models get more ambitious with their attempts.reply",
      "Only until they start incorporating this test into their training data.reply",
      "Dataset contamination alone won't get them good-looking SVG pelicans on bicycles though, they'll have to either cheat this particular question specifically or train it to make vector illustrations in general. At which point it can be easily swapped for another problem that wasn't in the data.reply",
      "I like this one as an alternative, also requiring using a special representation to achieve a visual result: https://voxelbench.aiWhat's more, this doesn't benchmark a singular prompt.reply",
      "Why is this a benchmark though? It doesn\u2019t correlate with intelligencereply",
      "It's simple enough that a person can easily visualize the intended result, but weird enough that generative AI struggles with itreply",
      "It started as a joke, but over time performance on this one weirdly appears to correlate to how good the models are generally. I'm not entirely sure why!reply",
      "it has to do with world model perception. these models don't have it but some can approximate it better than others.reply"
    ],
    "link": "https://moonshotai.github.io/Kimi-K2/thinking.html",
    "first_paragraph": ""
  },
  {
    "title": "Game design is simple (raphkoster.com)",
    "points": 89,
    "submitter": "vrnvu",
    "submit_time": "2025-11-06T22:24:23 1762467863",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=45841262",
    "comments": [
      "For reference Raph Koster wrote \"the book\" on game design, and was the lead designer for Ultima Online (among other things) https://en.wikipedia.org/wiki/Raph_Kosterreply",
      "Raph was the lead game designer on SWTOR a game that was way ahead of it's time and one of the most enjoyable sandbox mmorpg's I've ever played. I'm working on a new game that will take inspiration from lessons learned there.reply",
      "I remember when Raph was working on Metaplace[1], which was a kid-targeting, programmable (Lua dialect), virtual world/user generated content factory that was contemporary to the launch of Roblox ca. 2006-2007. I wonder quite often what things might be like if Metaplace had gotten to the scale and scope that Roblox wound up achieving.1: https://www.raphkoster.com/2007/09/18/metaplace/, or this demo https://www.youtube.com/watch?v=tZiB_JcRH_s, or https://en.wikipedia.org/wiki/Metaplacereply",
      "What was interesting/worked about it's design (and why did the players care?[1])Was it resilient to the, uh, many, many well-documented problems that the genre pushes players/itself into?---[1] There's a lot of ideas in this space that sound interesting on paper to nerds bikeshedding, but often fall flat in actual implementation. I'm curious as to what were the ones that worked.reply",
      "Game was SWG, not SWTOR. Launched in 2003 and was sunset in 2011 when SWTOR launched.SWG set out to be like Dwarf Fortress in terms of depth to the worlds physics; for example, gunsmiths could tinker on all parts of a gun and maybe get a lucky roll to unlock +N more damage or -N recoil. Same with land vehicles and bioengineered animals, droids. Parameters to noodle all the way down. Some under user control, others random to foster sense of a chaotic physical world.As the in game object economy was entirely propped up by crafters this fostered economic PVP.Lucasarts of 2000-2003, when the game was developed, did not understand MMO, and 3D games take much longer than 2D adventure games and shoved it out the door 2 years too early.It also suffered from 90s OOP heavy software development patterns. Devs had difficulty managing it and updating over the years.Ultimately it failed at being a Star Wars game. PVE was just \"kill a nest of bugs\" and failed to leverage storylines and characters. Players with nothing else to do ended up ruling the economy or whatever. Could have made them compete against Star Wars power brokers, IMO. Jabba sabotaged your factory, or something. Once a player was kitted out they had nothing to do.Some have spent the last 10+ years implementing a server emulator, various tools and mods. An emulator built around the original release is here: https://github.com/swgemuWhen WOW launched SWG was redesigned to play more like that. Typical MBA \"copy paste what they are doing\" project management.reply",
      "Raph is, at once, incredibly accomplished, thoughtful about design, and humble about it. I once caught him coming off an international flight, and he was excitedly showing off a game he'd coded on the plane. He genuinely loves working on the stuff and thinking about it.His writing is often SO full of ideas that I can't absorb an entire piece in one sitting. It's like a 12 course tasting menu. The neat thing with his writing is that, despite what he says here about all 12 pieces being important together, you can often just pick an isolated bit and chew on it for a while, and still learn something.(Presumably return to the other 11 courses later; they'll still be fresh.)reply",
      "This reads like the handbook for people making grind-based games. Sure enough, the author exclusively works in the mmorpg space.If you are a game designer, please take this with a grain of salt.Fun does not equal repeated challenges. And let me also reject the implicit notion that stories are entertainment but not, academically speaking, fun.reply",
      "Have you made any games?reply",
      "I ask this because Ralph is a luminary in the field and you just likened his contribution to the industry to that of somebody who designs predatory engagement loops and this is utterly ridiculous.reply",
      "Saying game design is simple doesn\u2019t make it so. It\u2019s a cute article though, reads like an explanation of learning.It feels like the author makes his living more by telling, rather than showing.reply"
    ],
    "link": "https://www.raphkoster.com/2025/11/03/game-design-is-simple-actually/",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: I scraped 3B Goodreads reviews to train a better recommendation model (book.sv)",
    "points": 222,
    "submitter": "costco",
    "submit_time": "2025-11-05T17:50:45 1762365045",
    "num_comments": 88,
    "comments_url": "https://news.ycombinator.com/item?id=45825733",
    "comments": [
      "Neat! It's a validation of the model that 75%+ of the recommendations are things I've read and also enjoyed, with a few \"read, didn't like\" and some more \"didn't read, don't really want to.\"But I think to break the content-bubble effects to find the longer tail, some way to reject or blacklist things - and have that be taken into effect in the model - might help.reply",
      "OK, I just added books until you told me I had too many. Fun idea! I have a couple of suggestions:* UI - once someone clicks \"Add\" you really should remove that item from the suggested list - it's very confusing to still see it.* Beam search / diversification -- Your system threw like 100 books at me of which I'd read 95 and heard of 2 of the other 3, so it worked for me as a predictor of what I'd read, but not so well for discovery.I'd be interested in recommendations that pushed me into a new area, or gave me a surprising read. This is easier to do if you have a fairly complete list of what someone's read, I know. But off the top of my head, I'm imagining finding my eigenfriends, then finding books that are either controversial (very wide rating differences amongst my fellow readers) or possibly ghettoized, that is, some portion of similar readers also read this X or Y subject, but not all.Anyway, thanks, this is fun! Hook up a VLM and let people take pictures of their bookshelf next.reply",
      "(From the site)\n>If you visit the \"intersect\" page, you can input multiple books and find the set of users that have read all of those books. This can be useful for finding longer tail books that weren't popular enough to meet the threshold. For instance, if you like reading about the collapse of the Soviet Union, you could put in \"Lenin's Tomb\" and \"Secondhand Time\", and see what other books the resultant users have read.This is how filmaffinity works, which is the best recommendation system I've tried. They have a group of several dozen 'soulmates', which are users with the most similar set of films seen and ratings given; recommendations are other stuff they also liked, and you get direct access to their lists.>then finding books that are either controversial  or possibly ghettoizedNaively, I\u2019d say the surprises are going to be better if you filter more different friends, rather than more controversial books among your friends. As in \u201cfind me a person that\u2019s like me only in some ways, tell me what they love\u201d. Long term this method is much better at exposing you to new ideas rather than just finding your cliques holy wars.reply",
      "Considering how much treasure has been poured into building recommendation engines for just about everything online, books have always been very difficult for me to find recommendations that work. Interested to try it!reply",
      "Echoing what everyone else has said here - awesome site, love how fast it was.I did notice that when I put in a single book in a series (in my case Going Postal, Discworld #33) that tended to dominate the rest of the selection. That does make sense, but I don't want recommendations for a series I'm already well into.Also noticed that a few books (Spycraft by Nadine Akkerman and Pete Langman, Tribalism is Dumb by Andrew Heaton) that I know are in goodreads and reviewed didn't show up in the search. I tried both author's name and the title of the book. Maybe they aren't in the dataset.It did stumble with some books more niche books (The Complete Yes Minister). Trying the \"Similar\" button gave me more books that were _technically_ similar because they were novelizations of British comedy shows, but not what I was looking for.For more common books though it lined up very well with books already on my wishlist!reply",
      "Yes I would say the handling of series is probably the biggest problem.  Once my test metrics got to a point I was happy with and my quality spot checks passed (can I follow the models recommendations from one generic history book to Steven Runciman, also making sure popular books don't always dominate the results), I was ready to release because I had been working on this project for so long.  The solution is probably using the transformer model to generate 100-200 candidates and then having a reranker on top.reply",
      "Awesome site and speed!My advice from someone who has built recommendation systems: Now comes the hard part! It seems like a lot of the feedback here is that it's operating pretty heavily like a content based system system, which is fine. But this is where you can probably start evaluating on other metrics like serendipity, novelty, etc. One of the best things I did for recommender systems in production is having different ones for different purposes, then aggregating them together into a final. Have a heavy content-based one to keep people in the rabbit hole. Have a heavy graph based to try and traverse and find new stuff. Have one that is heavily tuned on a specific metric for a specific purpose. Hell, throw in a pure TF-IDF/BM25/Splade based one.The real trick of rec systems is that people want to be recommnded things differently. Having multiple systems that you can weigh differently per user is one way to be able to achieve that, usually one algorithm can't quite do that effectively.reply",
      "You should filter out authors from the input books in the output. If liked a book by an author, surely I'd read more of their work if I wanted to \u2014 recommending them isn't helpful. Along the same lines, I think interesting recommendations tend to be the ones that (1) I like and (2) I didn't expect. The more similar the recommendations are to the input, the more likely I already know them, and the more likely to create a recommendation echo chamber.reply",
      "yep, was gonna say this. Getting recommended all of the same books I've already read isn't greatreply",
      "It works pretty well in the sense that after inputting only a few quite diverse books it gave me recommendations for a lot of books that I\u2019ve already also read and enjoyed.I would also really like a possibility to add negative signal. It did also recommend books that seemed interesting to me but I ultimately didn\u2019t like.Overall quite impressive.reply"
    ],
    "link": "https://book.sv",
    "first_paragraph": "No books selected yet. Search and select books above."
  },
  {
    "title": "Scientists find ways to boost memory in aging brains (vt.edu)",
    "points": 13,
    "submitter": "stevenjgarner",
    "submit_time": "2025-11-07T00:22:53 1762474973",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://news.vt.edu/articles/2025/10/cals-jarome-improving-memory.html",
    "first_paragraph": "In two separate studies, Virginia Tech researchers identified age-related molecular changes in the brain and adjusted them to improve memory.22 Oct 2025Memory loss may not simply be a symptom of getting older. New research from Virginia Tech shows that it\u2019s tied to specific molecular changes in the brain and that adjusting those processes can improve memory.In two complementary studies,\u00a0Timothy Jarome, associate professor in the\u00a0College of Agriculture and Life Sciences\u2019\u00a0School of Animal Sciences, and his graduate students used gene-editing tools to target those age-related changes to improve memory performance in older subjects. The work was conducted on rats, a standard model for studying how memory changes with age.\u201cMemory loss affects more than a third of people over 70, and it\u2019s a major risk factor for Alzheimer\u2019s disease,\u201d said Jarome, who also holds an appointment in the\u00a0School of Neuroscience. \u201cThis work shows that memory decline is linked to specific molecular changes that can "
  },
  {
    "title": "Universe's expansion 'is now slowing, not speeding up' (ras.ac.uk)",
    "points": 85,
    "submitter": "chrka",
    "submit_time": "2025-11-06T20:45:39 1762461939",
    "num_comments": 90,
    "comments_url": "https://news.ycombinator.com/item?id=45840200",
    "comments": [
      "\u201cSupernova (SN) cosmology is based on the key assumption that the luminosity standardization process of Type Ia SNe remains invariant with progenitor age. However, direct and extensive age measurements of SN host galaxies reveal a significant (  ) correlation between standardized SN magnitude and progenitor age, which is expected to introduce a serious systematic bias with redshift in SN cosmology. This systematic bias is largely uncorrected by the commonly used mass-step correction, as progenitor age and host galaxy mass evolve very differently with redshift. After correcting for this age bias as a function of redshift, the SN data set aligns more closely with the  cold dark matter (CDM) model\u201d [1].[1] https://academic.oup.com/mnras/article/544/1/975/8281988?log...reply",
      "> type Ia supernovae, long regarded as the universe\u2019s \"standard candles\", are in fact strongly affected by the age of their progenitor stars.A key point in the article. From what I understand, this is the main way we measure things of vast distance and, from that, determine the universe's rate of expansion. If our understanding of these supernovae is wrong, as this paper claims, that would be a massive scientific breakthrough.I'm really interested in the counterargument to this.reply",
      "Seems like the problem should be pretty easy to figure out. Just need to wait ~5 gigayears and see which model is right. I'm personally hoping for deceleration so that we have more total visitable volume.I'll set a reminder to check back at that time to see who was right.reply",
      "Anyone know how credible this is? If true, then that means the big bounce is back on the menu, and the universe could actually be an infinitely oscillating system.reply",
      "At least The Guardian has a comment from an independent expert:\"Prof Carlos Frenk, a cosmologist at the University of Durham, who was not involved in the latest work, said the findings were worthy of attention. \u201cIt\u2019s definitely interesting. It\u2019s very provocative. It may well be wrong,\u201d he said. \u201cIt\u2019s not something that you can dismiss. They\u2019ve put out a paper with tantalising results with very profound conclusions.\u201d\"https://www.theguardian.com/science/2025/nov/06/universe-exp...reply",
      "As an academic, that is exactly what the kind of noncommittal, don\u2019t burn your bridges with colleagues and funding bodies thing that I would say about even clearly flawed research if I were put on the spot by a popular-press publication. In fact, if you know you can rebut flawed research in time, you might want to assist in hyping it first so that your rebuttal will then make a bigger splash and benefit your personal brand.reply",
      "It's also something you could say if you forgot to read the assignment and the professor called on you.reply",
      "> If true, then that means the big bounce is back on the menuI don't think so. Deceleration does not imply recollapse. AFAIK none of this changes the basic fact that there isn't enough matter in the universe to cause it to recollapse. The expansion will just decelerate forever, never quite stopping.reply",
      "Wait but decelerating forever does in fact imply recollapse doesn't it?reply",
      "No. The simplest example is a matter-dominated universe at exactly the critical density. It decelerates forever but never quite stops expanding--the expansion rate asymptotes to zero.reply"
    ],
    "link": "https://ras.ac.uk/news-and-press/research-highlights/universes-expansion-now-slowing-not-speeding",
    "first_paragraph": "Contact usThe universe's expansion may actually have started to slow rather than accelerating at an ever-increasing rate as previously thought, a new study suggests.\"Remarkable\" findings published today in\u00a0Monthly Notices of the Royal Astronomical Society\u00a0cast doubt on the long-standing theory that a mysterious force known as 'dark energy' is driving distant galaxies away increasingly faster.Instead, they show no evidence of an accelerating universe.If the results are confirmed it could open an entirely new chapter in scientists' quest to uncover the true nature of dark energy, resolve the 'Hubble tension', and understand the past and future of the universe.Lead researcher Professor Young-Wook Lee, of Yonsei University in South Korea, said: \"Our study shows that the universe has already entered a phase of decelerated expansion at the present epoch and that dark energy evolves with time much more rapidly than previously thought.\"If these results are confirmed, it would mark a major para"
  },
  {
    "title": "Swift on FreeBSD Preview (swift.org)",
    "points": 178,
    "submitter": "glhaynes",
    "submit_time": "2025-11-06T17:37:49 1762450669",
    "num_comments": 102,
    "comments_url": "https://news.ycombinator.com/item?id=45837871",
    "comments": [
      "A lot of good news recently for swift. I am a bit jealous as my go to language C# / .NET is recently not announcing fancy things.I really like swift going beyond Apple. Particularly the port to android is IMHO crucial, however, now they are in the UI cross platform hell. Let us see if Apple is playing this better than Microsoft. Unfortunately, I have little hope. The only native contenders in the field right now are IMHO are react native and flutter which are both UI toolkits first and language second. Which I find gruesome.reply",
      "It'd be nice if Apple made SwiftUI cross platform and I'd be singing in the streets if UIKit got ported, but that seems unlikely at best.I believe that there's strong community interest in some kind of Swift UI framework for Android, though, and so there's a substantial chance that a third party solution will appear.reply",
      "Correct me if im wrong, but isn't the pain points for mobile devs, the need to have intimate knowledge of both pl to build & maintain a good \"backend/functionality\" of the app over time and that the UI portion of the app is quite simple to learn, build and maintain.So is it necessary for the swift team to try get swift ui onto android, versus a developer building their app \"backend/functionality\" in swift, compiling it down for both ios and android, then bridging the android bindings with a UI made in kmp etcI recently learnt that amo and protonmail use this solution but instead of swift android, they were using uniffi-rs and seemed to have great results, I think proton ditched react native for this solution, which to me sounds like a more streamlined way of getting native performance without needing the overhead of managing multiple language. I guess we will have to see how mature swift android gets and if it can replace uniffi-rs etc which would save even more timereply",
      "From what I've read, SwiftUI is using parts of UIKit under the hood, so it also doesn't seem too likely.reply",
      "skip.tools is one third-party solution for creating Swift + SwiftUI apps for Android today.reply",
      "It still translates SwiftUI to Kotlin+Compose, right? Very cool and useful, but not quite the same as a Swift-based UI framework for Android.reply",
      "Oops, TIL Jetpack Compose doesn't use Android native controls. That is unfortunate.reply",
      "Jetpack Compose is just as \"native\" as Android views at this point; it hooks into the same accessibility frameworks and renders to the same surfaces as the framework toolkit. This isn't like Flutter which renders to an opaque Skia buffer.reply",
      "Great! I appreciate the claification, thank you.reply",
      "I\u2019d absolutely love it if they made SwiftUI cross platform for both mobile and desktop. Flutter is nice but it\u2019s still sort of a mess sometimes when targeting desktop instead of mobile.reply"
    ],
    "link": "https://forums.swift.org/t/swift-on-freebsd-preview/83064",
    "first_paragraph": "We have been hard at work to bring the Swift toolchain to FreeBSD. A preview Swift bundle for FreeBSD 14.3+ is available at https://download.swift.org/tmp-ci-nightly/development/freebsd-14_ci_latest.tar.gz. The bundle contains a Swift development compiler and Swift runtimes needed for compiling Swift programs on, and for, FreeBSD 14 on x86_64 machines.The Swift compiler and runtimes have a few dependencies. Please install the following dependencies:The compiler in the bundle is still under development and isn't part of a release yet and we're not quite done porting everything to FreeBSD.Here is a list of known issues that you may run into while trying things out.We are investigating adding aarch64 support and making the bundle available for all minor versions of FreeBSD 14.As you find more bugs, please file issues at https://github.com/swiftlang/swift/issues.We look forward to hearing your feedback. If you're interested in helping add the finishing polish, please feel free to reach out"
  },
  {
    "title": "Open Source Implementation of Apple's Private Compute Cloud (github.com/openpcc)",
    "points": 350,
    "submitter": "adam_gyroscope",
    "submit_time": "2025-11-05T15:52:27 1762357947",
    "num_comments": 70,
    "comments_url": "https://news.ycombinator.com/item?id=45824243",
    "comments": [
      "Reading the whitepaper, the inference provider still has the ability to access the prompt and response plaintext. This scheme does seem to guarantee that plaintext cannot be read for all other parties (e.g. the API router), and that the client's identity is hidden and cannot be associated with their request. Perhaps the precise privacy guarantees and allowances should be summarized in the readme.With that in mind, does this scheme offer any advantage over the much simpler setup of a user sending an inference request:- directly to an inference provider (no API router middleman)- that accepts anonymous crypto payments (I believe such things exist)- using a VPN to mask their IP?reply",
      "Howdy, head of Eng at confident.security here, so excited to see this out there.I'm not sure I understand what you mean by inference provider here? The inference workload is not shipped off the compute node once it's been decrypted to e.g. OpenAI, it's running directly on the compute machine on open source models loaded there. Those machines are cryptographically attesting to the software they are running. Proving, ultimately, that there is no software that is logging sensitive info off the machine, and the machine is locked down, no SSH access.This is how Apple's PCC does it as well, clients of the system will not even send requests to compute nodes that aren't making these promises, and you can audit the code running on those compute machines to check that they aren't doing anything nefarious.The privacy guarantee we are making here is that no one, not even people operating the inference hardware, can see your prompts.reply",
      "> no one, not even people operating the inference hardwareYou need to be careful with these claims IMO. I am not involved directly in CoCo so my understanding lacks nuance but after https://tee.fail I came to understand that basically there's no HW that actually considers physical attacks in scope for their threat model?The Ars Technica coverage of that publication has some pretty yikes contrasts between quotes from people making claims like yours, and the actual reality of the hardware features.https://arstechnica.com/security/2025/10/new-physical-attack...My current understanding of the guarantees here is:- even if you completely pwn the inference operator, steal all root keys etc, you can't steal their customers' data as a remote attacker- as a small cabal of arbitrarily privileged employees of the operator, you can't steal the customers' data without a very high risk of getting caught- BUT, if the operator systematically conspires to steal the customers' data, they can. If the state wants the data and is willing to spend money on getting it, it's theirs.reply",
      "> I came to understand that basically there's no HW that actually considers physical attacks in scope for their threat model?xbox, playstation, and some smartphone activation locks.Of course, you may note those products have certain things in common...reply",
      "I'm happy to be careful, you are right we are relying on TEEs and vTPMs as roots of trust here and TEEs have been compromised by attackers with physical access.This is actually part of why we think it's so important to have the non-targetability part of the security stack as well, so that even if someone where to physically compromise some machines at a cloud provider, there would be no way for them to reliably route a target's requests to that machine.reply",
      "Nvidia has been investing in confidential compute for inference workloads in cloud - that covers physical ownership/attacks in their thread model.https://www.nvidia.com/en-us/data-center/solutions/confident...https://developer.nvidia.com/blog/protecting-sensitive-data-...reply",
      "Thanks for the reply! By \"inference provider\" I meant someone operating a ComputeNode. I initially skimmed the paper, but I've now read more closely and see that we're trying to get guarantees that even a malicious operator is unable to e.g. exfiltrate prompt plaintext.Despite recent news of vulnerabilities, I do think that hardware-root-of-trust will eventually be a great tool for verifiable security.A couple follow-up questions:1. For the ComputeNode to be verifiable by the client, does this require that the operator makes all source code running on the machine publicly available?2. After a client validates a ComputeNode's attestation bundle and sends an encrypted prompt, is the client guaranteed that only the ComputeNode running in its attested state can decrypt the prompt? Section 2.5.5 of the whitepaper mentions expiring old attestation bundles, so I wonder if this is to protect against a malicious operator presenting an attestation bundle that doesn't match what's actually running on the ComputeNode.reply",
      "Great questions!1. The mechanics of the protocol are that a client will check that the software attested to has been released on a transparency log. dm-verity is what enforces that the hashes of the booted filesystem on the compute node match what was built and so those hashes are what are put on the transparency log, with a link to the deployed image that matches them. The point of the transparency log is that anyone could then go inspect the code related to that release to confirm that it isn't maliciously logging. So if you don't publish the code for your compute nodes then the fact of it being on the log isn't really useful.So I think the answer is yes, to be compliant with OpenPCC you would need to publish the code for your compute nodes, though the client can't actually technically check that for you.2. Absolutely yes. The client encrypts its prompt to a public key specific to a single compute node (well, technically it will encrypt the prompt N times for N specific compute nodes) where the private half of that key is only resident in the vTPM, the machine itself has no access to it. If the machine were swapped or rebooted for another one, it would be impossible for that computer to decrypt the prompt. The fact that the private key is in the vTPM is part of the attestation bundle, so you can't fake itreply",
      "> The privacy guarantee we are making here is that no one, not even people operating the inference hardware, can see your prompts.that cannot be met, period. your asssumptions around physical protections are invalid or at least incorrect. It works for Apple (well enough) because of the high trust we place in their own physical controls, and market incentive to protect that at all costs.> This is how Apple's PCC does it as well [...] and you can audit the code running on those compute machines to check that they aren't doing anything nefarious.just based on my recollection, and I'm not going to have a new look at it to validate what I'm saying here, but with PCC, no you can't actually do that. With PCC you do get an attestation, but there isn't actually a \"confidential compute\" aspect where that attestation (that you can trust) proves that is what is running. You have to trust Apple at that lowest layer of the \"attestation trust chain\".I feel like with your bold misunderstandings you are really believing your own hype. Apple can do that, sure, but a new challenger cannot. And I mean your web page doesn't even have an \"about us\" section.reply",
      "Apple actually attests to signatures of every single binary they install on their machines, before soft booting into a mode where no further executables can be installed: https://security.apple.com/documentation/private-cloud-compu...We don't _quite_ have the funding to build out our own custom OS to match that level of attestation, so we settled for attesting to a hash of every file on the booted VM instead.reply"
    ],
    "link": "https://github.com/openpcc/openpcc",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        An open-source framework for provably private AI inference\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.OpenPCC is an open-source framework for provably private AI inference, inspired by Apple\u2019s Private Cloud Compute but fully open, auditable, and deployable on your own infrastructure. It allows anyone to run open or custom AI models without exposing prompts, outputs, or logs - enforcing privacy with encrypted streaming, hardware attestation, and unlinkable requests.OpenPCC is designed to become a transparent, community-governed standard for AI data privacy.Read the OpenPCC Whitepaper: https://github.com/openpcc/openpcc/blob/main/whitepaper/openpcc.pdfConfident Security is building a fully managed service, called CONFSEC, based on the OpenPCC standard. To"
  },
  {
    "title": "LLMs encode how difficult problems are (arxiv.org)",
    "points": 96,
    "submitter": "stansApprentice",
    "submit_time": "2025-11-06T18:29:03 1762453743",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=45838564",
    "comments": [
      "It's all very clear when you mentally replace \"LLM\" with \"text completion driven by compressed training data\".E.g.[Text copletion driven by compressed training data] exhibit[s] a puzzling inconsistency: [it] solves complex problems yet frequently fail[s] on seemingly simpler ones.Some problems are better represented by a locus of texts in the training data, allowing more plausible talk to be generated.  When the problem is not well represented, it does not help that the problem is simple.If you train it on nothing but Scientology documents, and then ask about the Buddhist perspective on a situation, you will probably get some nonsense about body thetans, even if the situation is simple.reply",
      "I have a hard time trying to conceptualize lossy text compression, but I've recently started to think about the \"reasoning\"/output as just a by product of lossy compression, and weights tending towards an average of the information \"around\" the main topic of prompt. What I've found easier is thinking about it like lossy image compression, generating more output tokens via \"reasoning\" is like subdividing nearby pixels and filling in the gaps with values that they've seen there before. Taking the analogy a bit too far, you can also think of the vocabulary as the pixel bit depth.I definitely agree replacing AI or LLMs with \"X driven by compressed training data\" starts to make a lot more sense, and a useful shortcut.reply",
      "You're right about \"reasoning\". It's just trying to steer the conversation in a more relevant direction in vector space, hopefully to generate more relevant output tokens. I find it easier to conceptualize this in three dimensions. 3blue1brown has a good video series which covers the overall concept of LLM vectors in machine learning: https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_...To give a concrete example, say we're generating the next token from the word \"queen\". Is this the monarch, the bee, the playing card, the drag entertainer? By adding more relevant tokens (honey, worker, hive, beeswax) we steer the token generation to the place in the \"word cloud\" where our next token is more likely to exist.I don't see LLMs as \"lossy compression\" of text. To me that implies retrieval, and Transformers are a prediction device, not a retrieval device. If one needs retrieval then use a database.reply",
      "> You're right about \"reasoning\". It's just trying to steer the conversation in a more relevant direction in vector space, hopefully to generate more relevant output tokens.I like to frame it as a theater-script cycling through the LLM. The \"reasoning\" difference is just changing the style so that each character has film noir monologues. The underlying process hasn't really changes, and the monologues text isn't fundamentally different from dialogue or stage-direction... but more data still means more guidance for each improv-cycle.> say we're generating the next token from the word \"queen\". Is this the monarch, the bee, the playing card, the drag entertainer?I'd like to point out that this scheme can result in things that look better to humans in the end... even when the \"clarifying\" choice is arbitrary or irrational.In other words, we should be alert to the difference between \"explaining what you were thinking\" versus \"picking a firm direction so future improv makes nicer rationalizations.\"reply",
      "It is not a useful shortcut because you don't know what the training data is, nothing requires it to be an \"average\" of anything, and post-training arbitrarily re-weights all of its existing distributions anyway.reply",
      "Well, that's what a LLM is. The problem is if one's mental model is built on \"AI\" instead of \"LLM.\"The fact that LLMs can abstract concepts and do any amount of out-of-sample reasoning is impressive and interesting, but the null hypothesis for a LLM being \"impressive\" in any regard is that the data required to answer the question is present in it's training set.reply",
      "This is true, but also misleading. We are learning that the models achieve compression by distilling higher level concepts and deriving generalized human like abilities, for example the recent introspection paper from Anthropic.reply",
      "> Text copletion driven by compressed training data...solves complex problemsSure it does. Obviously. All we ever needed was some text completion.Thanks for your valuable insight.reply",
      "Thank you for posting this. I'm struck with how there is a lot of studying of the behavior and isolating it from other assumptions and then these individual capabilities are then described as a new solution or discovered capability that would work with all of those other assumptions. This makes most all of the LLM research feel like whack a mole if the goal was to make accurate and reliable models by understanding these techniques. Instead, it's more like seeing faces in cars and buildings and other artifacts of patterns and pattern groupings and recognition of patterns. Building houses on sand, etc.reply",
      "Sound a lot like Kolmogorov complexityreply"
    ],
    "link": "https://arxiv.org/abs/2510.18147",
    "first_paragraph": "YOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n"
  },
  {
    "title": "The Parallel Search API (parallel.ai)",
    "points": 87,
    "submitter": "lukaslevert",
    "submit_time": "2025-11-06T17:04:41 1762448681",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=45837425",
    "comments": [
      "> Traditional search engines were built for humans. They rank URLs, assuming someone will click through and navigate to a page. The search engine's job ends at the link. The system optimizes for keywords searches, click-through rates, and page layouts designed for browsing - done in milliseconds and as cheaply as possible.\n  > ... AI search has to solve a different problem: what tokens should go in an agent's context window to help it complete the task? We\u2019re not ranking URLs for humans to click\u2014 we\u2019re optimizing context and tokens for models to reason over.\n\nI also want a search engine which ranks the results based on how it's useful to reason about, not how it can sell potential ads by invoking false rage or insecurities. And it would be better if unrelated information or fancy gimmicks are removed from the website like Reader View.reply",
      "I like Parallel and been using it for tests but I am not sure about the terms.> The materials displayed or performed or available on or through our website, including, but not limited to, text, graphics, data, articles, photos, images, illustrations and so forth (all of the foregoing, the \u201cContent\u201d) are protected by copyright and/or other intellectual property laws. You promise to abide by all copyright notices, trademark rules, information, and restrictions contained in any Content you access through our website, and you won\u2019t use, copy, reproduce, modify, translate, publish, broadcast, transmit, distribute, perform, upload, display, license, sell, commercialize or otherwise exploit for any purpose any Content not owned by you, (i) without the prior consent of the owner of that Content or (ii) in a way that violates someone else\u2019s (including Parallel's) rights.reply",
      "IANAL but think this is to remind you that fragments of text it returns to you after pulling them from various sites in response to your query are protected by whatever copyright notices might be found on those websites. Seems reasonable to me.reply",
      "Search accuracy, when used in the context of an agent, is so important because when you are delivered search results which are incorrect, the agent tends to interpret them as fact because they come from a \"credible\" source. So, this is very much an industry that still has plenty of room for improvement, and I'm excited to see how this product performs.reply",
      "Congrats Parag and team on the launch, I am impressed by the quality and latency of Parallel search APIs.reply",
      "Interesting, but I'm not totally convinced that searching for LLMs is different than for us (humans). In the end, we both want to get information that's relevant to our query (intent). Besides, I wonder whether there will be able to convince big players like OpenAI to use them, instead of Google Search with its proven record :)reply",
      "Does this help? https://x.com/paraga/status/1986480529701806324reply",
      "You're right, at the end the final end user is human.The major difference is the how the data is structured for consumption.reply",
      "Hi Parag, congrats on the launch. We'll try this out at FutureSearch.I agree there is a need for such APIs. Using Google or Bing isn't enough, and Exa and Brave haven't clearly solved this yet.reply",
      "I've been saying for quite some time now that AI is going to kill the traditional (free) search engine.  This is just another nail in the coffin.When an AI searches google.com for you, the ads never get shown to the user.  Search engines like kagi.com are the future.  You'll give the AI your Kagi API key and that'll be it.  You won't even need cloud-based AI for that kind of thing!  Tiny, local models trained for performing searches on behalf of the user will do it instead.Soon your OS will regularly pull down AI model updates just like it pulls down software updates today.  Every-day users will have dozens of models that are specialized for all sorts of tasks\u2014like searching the Internet.  They won't even know what they're for or what they do.  Just like your average Linux user doesn't know what the `polkit` or `avahi-daemon` services do.My hope: This will (eventually) put pressure on hardware manufacturers to include more VRAM in regular PCs/consumer GPUs.reply"
    ],
    "link": "https://parallel.ai/blog/introducing-parallel-search",
    "first_paragraph": "Web search, built from the ground up for AIA second user has arrived on the web: AI. And it needs fundamentally different infrastructure than humans do.The Parallel Search API, built on our proprietary web index, is now generally available. It's the only web search tool designed from the ground up for AI agents: engineered to deliver the most relevant, token-efficient web data at the lowest cost. The result is more accurate answers, fewer round-trips, and lower costs for every agent.Traditional search engines were built for humans. They rank URLs, assuming someone will click through and navigate to a page. The search engine's job ends at the link. The system optimizes for keywords searches, click-through rates, and page layouts designed for browsing - done in milliseconds and as cheaply as possible.The first wave of web search APIs used in AI-based search made this human search paradigm programmatically accessible, but failed to solve the underlying problem of how you design search for"
  },
  {
    "title": "FBI tries to unmask owner of archive.is (heise.de)",
    "points": 677,
    "submitter": "Projectiboga",
    "submit_time": "2025-11-06T16:18:18 1762445898",
    "num_comments": 357,
    "comments_url": "https://news.ycombinator.com/item?id=45836826",
    "comments": [
      "We need to preserve data. The FBI is trying to kill data.We can not allow the FBI to work for Evil here. I actually think there should be a human right to data. With that I mean, primarily, knowledge, not to data about a single human being as such (e. g. \"doxxing\" or any such crap - I mean knowledge).Knowledge itself should become a human right. I understand that the current law is very favourable to mega-corporations milking mankind dry, but the law should also be changed. (I am not anti-business per se, mind you - I just think the law should not become a tool to contain human rights, including access to knowledge and information at all times.)Wikipedia is somewhat ok, but it also misses a TON of stuff, and unfortunately it only has one primary view, whereas many things need some explanation before one can understand it. When I read up on a (to me) new topic, I try to focus on simple things and master these first. Some wikipedia articles are so complicated that even after staring at them for several minutes, and reading it, I still haven't the slightest clue what this is about. This is also a problem of wikipedia - as so many different people write things, it is sometimes super-hard to understand what wikipedia is trying to convey here.reply",
      "> We can not allow the FBI to work for Evil hereHistorically speaking I can't see this as even being in the top 100 evil things the FBI has done.reply",
      "But once FBI has the power to erase knowledge, the other 100 evil things will be rounded to zero.reply",
      "> Historically speaking I can't see this as even being in the top 100 evil things the FBI has done.Perhaps, but we can't change the past: we can only fight against what is happening in the present to try to get a better future.reply",
      "Probably a bit of a 'baby with the bathwater' situation here.  At almost no point has that institution been a net positive - at times snooping on 'political dissidents' (like MLK Jr.), and at others bungling cases so bad they become moments of national shame (Ruby Ridge).You're never going to get a system with a clandestine domestic service running ethically for long, esp. not with qualified immunity.  It's simply too attractive to dumb psychopaths with delusions of grandeur and concurrently not of interest to people with a strong sense of community or morals.reply",
      "> At almost no point has that institution been a net positiveHard to measure, isn't it. In the eyes of the millions of americans who have at some point in their life been victims or related to or friends of victims of some kind of serious crime, the FBI has often times been helpful and/or the prospect of being caught has been a deterrent for crimes.You contrast that with all the bad that has come from there, of which there is surely plenty, but how come you claim thay the bad obviously must outweigh the good?reply",
      "> At almost no point has that institution been a net positiveThe FBI's anticorruption work is good and necessary.reply",
      "I assume that\u2019s why the original argument is that it\u2019s not been a net positive. I.e. the assumption is that lots of work can be good and necessary, while even more that is evil and excessive can end up with a net negative.reply",
      "Like that's happening under this administration, see tom Homan.reply",
      "Or the Trump coin crypto rugpull and money laundering scheme. Or the open insider trading. Or the $400 million jet \"gifted\" from Qatar. This year has been one grift after another.reply"
    ],
    "link": "https://www.heise.de/en/news/Archive-today-FBI-Demands-Data-from-Provider-Tucows-11066346.html",
    "first_paragraph": "\n        The mysterious website Archive.today is coming under the FBI's crosshairs. A court order is forcing the provider Tucows to hand over user data.\n      \n      \n        Archive.today is apparently in the crosshairs of investigations by the US Federal Bureau of Investigation (FBI).\n      \n    \n      (Image:\u00a0heise medien)\n    It is one of the most mysterious and, at the same time, best-known websites on the internet. Archive.today has built up a user base over a period of more than ten years who use the service to access previous snapshots of a web page. So basically like the Wayback Machine of the Internet Archive, only largely free of rules and presumably therefore also anonymous. To the chagrin of the media industry, the service is also often used to bypass paywalls. This is also possible because the service does not adhere to common rules and laws and offers no opt-out option.And so far, the operators have gotten away with it. Although there have been minor problems in the hist"
  },
  {
    "title": "Eating stinging nettles (rachel.blog)",
    "points": 167,
    "submitter": "rzk",
    "submit_time": "2025-11-06T11:57:01 1762430221",
    "num_comments": 163,
    "comments_url": "https://news.ycombinator.com/item?id=45834254",
    "comments": [
      "I grew up in Ukraine and stinging nettle soups were a popular part of our diet in the summers. It is delicious and I definitely don\u2019t agree that it is bland. But I suspect a big part of it is what else you add to it. My suggestion is to look up \u201c\u0441\u0443\u043f \u0441 \u043a\u0440\u0430\u043f\u0438\u0432\u043e\u0439\u201d and use your favorite method of translating it to your language of choice to look at the variety of recipes.reply",
      "The key is to add lots of onions and garlic and some butter to give it base flavor. The nettles give off great colour and a more subtle flavor and of course add more nutrients.The real key though is stinging nettles just simply grow like crazy in your backyard (at least in Ireland) so it's a two birds with the one stone kind of deal, you're gardening as well as cooking. There is also the 'badass' feeling of eating something that previously was dangerous. The heat will denature any stingers in the soup.reply",
      "As a kid is somewhat rural western washington our backyard bordered on a many acred wood and just beyond our backyard fence was just a huge tangle of blackberry and nettles. As kids we'd get our dads machetes and carve a path into the woods proper every spring and every few years our family and the families on either side would spend a day trying to eradicate the encroaching blackberries to no ultimate avail.We never ate the nettles, just had 1000 remedies for stings, but we did eat a lot of blackberry jam, cobblers and pies.reply",
      "I'm in western washington and some people (not me) do eat the nettles. The blackberries are of course, delicious and well used. Always a good idea to pick above waist height of dogs. ;)reply",
      "Or have small dogs.reply",
      "You ate it in a hearty soup, likely made on pork bone broth, with a boiled egg, and sour cream added. It makes a lot of difference for culinary experience :) The other commenter probably just tried to add it to some rice, or as a \"side green\". On itself nettle is more or less like spinach, but with weaker tastereply",
      "Axe head soup strikes againreply",
      "I've always heard it as Stone Soup, but I presume it's the same thing.reply",
      "I know it as Stone Porridge. These stories probably share an origin. https://sites.pitt.edu/~dash/type1548.html is sitting in my browser bookmarks, and https://en.wikipedia.org/wiki/Stone_Soup has some variations not listed on that page.reply",
      "One of my all-time favorite stories.My dear mother told me this story when I was just a boy. I was enchanted by the idea of this magical stone, too young to consider the clever trick the tramp was playing on the woman.The sense of cooking being a magical endeavor has stayed with me ever since.reply"
    ],
    "link": "https://rachel.blog/2018/04/29/eating-stinging-nettles/",
    "first_paragraph": "rachel.blogSpring is here and the nettles are growing again so I decided it was time to make a meal out of them. Most people know that stinging nettles are pesky green plants that irritate the skin when you touch them. What you probably don\u2019t know is that they\u2019re a nutritious source of iron, calcium, potassium, and silica as well as vitamins A, B, C, and K1. Stinging nettles also have anti-inflammatory properties and can relieve arthritis and rheumatism. They can be turned into soups, curries, and risottos (some recipes here) and you can get them completely free from practically everywhere in Britain over the summer. You\u2019ve likely even got some in your garden.When you collect them you need to wear gloves because they sting. The advantage of this is it allows you to make sure you\u2019re collecting the right thing. If you\u2019re unsure, just touch one and see whether it hurts which is exactly what I did. It hurt.The even look a bit scary with their toothy-edged leaves.Once you\u2019ve got them inside"
  },
  {
    "title": "Hightouch (YC S19) Is Hiring (greenhouse.io)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-11-06T21:23:46 1762464226",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://job-boards.greenhouse.io/hightouch/jobs/5542602004",
    "first_paragraph": "Hightouch is the modern AI platform for marketing and growth teams. Our AI agents reimagine marketing workflows, allowing marketers to create content, plan campaigns, and execute strategies with transformational velocity and performance.Hightouch is a rare company built on the intersection of two fundamental technological shifts: advances in LLMs and agentic AI, and the creation and rapid adoption of cloud data warehouses like Snowflake and Databricks. Building on these tailwinds, we\u2019ve become a leader in AI marketing and partner with industry leaders like Domino\u2019s, Chime, Spotify, Ramp, Whoop, Grammarly, and over 1000 others.Our team focuses on making a meaningful impact for our customers. We approach challenges with first-principles thinking, move quickly and efficiently, and treat each other with compassion and kindness. We look for team members who are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.We\u2019re looking to add a\u00a0product"
  },
  {
    "title": "I analyzed the lineups at the most popular nightclubs (karltryggvason.com)",
    "points": 139,
    "submitter": "kalli",
    "submit_time": "2025-11-06T13:37:07 1762436227",
    "num_comments": 67,
    "comments_url": "https://news.ycombinator.com/item?id=45835083",
    "comments": [
      "I'm just gonna say that some aspect of the data collection here seems flawed: among the SF clubs listed are DNA Lounge and Public Works \u2013 which are great clubs, don't get me wrong \u2013 but they are very much on the smaller side. And, Phonobar? That is a bar/restaurant, not a nightclub at all.  Meanwhile, The Warfield and 1015 Folsom are left out \u2013 how does that make any sense?reply",
      "It's based on popularity on Resident Advisor / RA (those are the clubs that had the most \"followers\" among their userbase)reply",
      "This is a classic example of data sourcing bias.RA is primarily used by independent promoters in the States, which tend to be much smaller and have smaller or less frequent events.Large promoters who regularly throw events or have the budget for larger events would use their own promotion mechanisms and general population ad networks instead of listing on RA.reply",
      "> Large promoters who regularly throw events or have the budget for larger events would use their own promotion mechanismsNo (at least in the US) - it\u2019s because of exclusive contracts with the ticketing platforms.Whereas you can list on RA and other platforms too, the biggest clubs and venues get lucrative deals with eg AXS to only list tickets on their platform.reply",
      "That's ticketing. There's plenty of listings on RA that don't use RA's ticketing service.Large promoters that use or have exclusivity deals with AXS or Ticketmaster/LiveNation or Dice still advertise/promote on platforms like Facebook/Instagram, EDM Train, Radiate, etc alongside the ticketing platform's promotion platforms.reply",
      "dunno about the rest of your post but phonobar is legit, it's tiny but it has a dance floor and regularly hosts top-tier talentreply",
      "Which is the best SF nightclub? I got baited by online reviews to visit DNA and was disappointed.reply",
      "DNA is good - but it's a club for the actual artist and music to go - and less as a place to hang out on a random nightreply",
      "It depends on the music and scene that interests you. Most SF nightclubs are event spaces that host independent promoters of recurring events, so the music and attendance can vary. You'll generally get best results if your friends will be there and you want to see the performing artist(s).reply",
      "My favorites are Public Works, The Midway, and 1015 Folsomreply"
    ],
    "link": "https://dev.karltryggvason.com/how-i-analyzed-the-lineups-at-the-worlds-most-popular-nightclubs/",
    "first_paragraph": "A few years back I did a bit of dance music related data visualization over at Lazily Evaluated. My favourite was an analysis of clubs and their lineups using Resident Advisor / RA data, I called it Clubster Analysis. I always wanted to dig into the technical aspects of gathering the data, analyzing it and building the charts and graphs to tell a story and give people insight. With this blog I now have the right venue for that kind of tech talk, so here goes.To visualize data, first you have to get some! For this purpose I wrote a little scraper in Python. I used Beautiful Soup to parse the html and grab the bits and pieces I was interested in.My scraping of a few thousand pages didn\u2019t cause considerable load on the RA servers. But in the age of overzealous AI scrapers it\u2019s worth being polite, so I throttled according to their robots.txt. I also maintained a local cache of html files I had already downloaded, so that I wouldn\u2019t have fetch the same data repeatedly (past lineups are unli"
  },
  {
    "title": "The Geometry of Schemes [pdf] (ed.ac.uk)",
    "points": 9,
    "submitter": "measurablefunc",
    "submit_time": "2025-10-31T23:06:25 1761951985",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://webhomes.maths.ed.ac.uk/~v1ranick/papers/eisenbudharris.pdf",
    "first_paragraph": ""
  },
  {
    "title": "ICC ditches Microsoft 365 for openDesk (binnenlandsbestuur.nl)",
    "points": 527,
    "submitter": "vincvinc",
    "submit_time": "2025-11-06T16:57:55 1762448275",
    "num_comments": 163,
    "comments_url": "https://news.ycombinator.com/item?id=45837342",
    "comments": [
      "Hei hei,I'm working for the XWiki and CryptPad projects, which are integrated in openDesk. Here are a couple links / infos that can be interesting to understand the context of openDesk.The openDesk project comes initially from an initiative of the Ministry of Interior of Germany in 2021, to build the alternative to Office 365. The project was progressively transferred in 2025 to a state-owned organization, the ZenDis (https://zendis.de), which oversees the global development of openDesk.The source code is mainly available on https://gitlab.opencode.de/bmi/opendesk, where you will find mirrors of every project which is bundled into openDesk (Nextcloud, Collabora, Element, Univention, XWiki, Jitsi, OpenXchange, CryptPad, OpenProject, \u2026)There was also a couple public presentations about openDesk at FOSDEM during the past years :* In 2024 : https://archive.fosdem.org/2024/schedule/event/fosdem-2024-3...* In 2025 : https://archive.fosdem.org/2025/schedule/event/fosdem-2025-5...reply",
      "I appreciate your comment. I'm thrilled to learn that CryptPad is part of the openDesk solution.>CryptPad was selected to join the German \"Sovereign Workplace\" project, now called openDesk.https://blog.cryptpad.org/2025/01/28/CryptPad-Funding-Status...Many more details in this blog post from XWiki: https://xwiki.com/en/Blog/XWiki-CryptPad-knowledge-managemen...reply",
      "I find it fascinating to see how much power Germany's \"digital sovereignty\" initiative has gained. In the beginning, it looked like yet another government thingy that nobody will use. But by now, they must be well above 100k government employees using it daily.Also, in case you missed that: StackIt is the AWS / G Cloud competitor by LIDL: https://www.stackit.de/en/ It's the basebone for their app strategy with 100 mio+ client installs and about 500k employees.reply",
      "Every time this happens Microsoft either threatens to move out or promises to move in with a chunk of their operation. Blackmailing with jobs has been very effective for them.reply",
      "I think that strategy might be running out of steam though, before these projects seemed to have more commercial reasons or just pushing the idea of openness, but sovereignty is a much stronger ideal and much more likely to be the one that will weather the blackmail.reply",
      "Looks like openDesk uses Collabora Online, which is itself based on libreoffice online - web based libreoffice.https://www.opendesk.eu/en/product#document-management (\"Collabora Online powers openDesk with a robust office suite designed for efficient teamwork and secure document editing.\")https://en.wikipedia.org/wiki/Collabora_Online (\"Collabora Online (often abbreviated as COOL) is an open-source online office suite developed by Collabora, based on LibreOffice Online, the web-based edition of the LibreOffice office suite.\")reply",
      "More than that--Collabora is a major (maybe the biggest) contributor to LibreOffice.reply",
      "Rightly so. They should have never used it in the first place. What with the US not recognizing the court it always made very little sense to me that they would rely on the infrastructure components to be supplied by the USA. The latest sanctions are just another step in something that was already in motion from day #1.The world order at the highest level relies on the nations themselves to behave, especially the largest ones because nobody has the practical power to enforce the decisions of the court in case defendants are in places where the court is not recognized. To USA not recognizing the court has always shown that they don't care about the crimes they commit.reply",
      "> To USA not recognizing the court has always shown that they don't care about the crimes they commit.I'd nitpick the \"don't care\" part. To me, it's that they do care precisely because they know they are guilty. I think Trump is guilty for the boats being shot. Obama guilty for the drone strikes. W guilty for well, the whole shit show. Didn't really pay attention to Biden, but I'd assume drone strikes continued there too. From Clinton on back, I admit I just wasn't paying attention to those kinds of issues.reply",
      "Lawyers historically are notoriously linked to Microsoft and its formats as a somewhat unintentional industry side standard.Moves like this hearten me as for certain lawyers the formats and standards they now will be expected to follow has just shifted, towards open source no less.reply"
    ],
    "link": "https://www.binnenlandsbestuur.nl/digitaal/internationaal-strafhof-neemt-afscheid-van-microsoft-365",
    "first_paragraph": "Het ICC stapt over naar Open Desk, een Europese opensource kantooromgeving \n            Het Internationaal Strafhof (International Criminal Court, ICC) ruilt Microsoft 365 in voor Open Desk, een Europees open source alternatief. Dat schrijft de Duitse krant Handelsblatt. De krant verwacht dat het ICC met de overstap mogelijk een trend start binnen de Europese publieke sector.\n            \n            Microsoft bevestigt de breuk aan de nieuwssite\u00a0Euractiv. \u2018Wij hechten waarde aan onze relatie met het ICC als klant en zijn ervan overtuigd dat niets ons vermogen in de weg staat om in de toekomst diensten aan het ICC te blijven leveren,\u2019 zegt een woordvoerder van Microsoft.\n            Schrijf je in voor de Binnenlands Bestuur nieuwsbrief\n            Bij Europese overheden leven al langer zorgen over de digitale afhankelijkheid van Amerikaanse bedrijven. Die zorgen zijn sterk toegenomen sinds Donald Trump voor de tweede maal president van de Verenigde Staten werd.\n            \n           "
  },
  {
    "title": "Mathematical exploration and discovery at scale (terrytao.wordpress.com)",
    "points": 223,
    "submitter": "nabla9",
    "submit_time": "2025-11-06T09:24:42 1762421082",
    "num_comments": 108,
    "comments_url": "https://news.ycombinator.com/item?id=45833162",
    "comments": [
      "It's really tiring that LLM fans will claim every progress as breakthrough and go into fantasy mode on what they can do afterwards.This is a really good example of how to use the current capabilities of LLM to help research. The gist is that they turned math problems into  problems for coding agents. This uses the current capabilities of LLM very well and should find more uses in other fields. I suspect the Alpha evolve system probably also has improvements over existing agents as well. AI is making steady and impressive process every year. But it's not helpful for either the proponents or the skeptics to exaggerate their capabilities.reply",
      "One could say the same about these kinds of comments.  If you don't like the content, simply don't read it?And to add something constructive: the timeframes for enjoying a hype cycle differ from person to person.  If you are on top of things, it might be tiring, but there are still many people out there, who haven't made the connection between, in this case, LLMs and mathematics.  Inspiring some people to work on this may be beneficial in the long run.reply",
      "GP didn\u2019t say they didn\u2019t like it. They criticized it. These things are not the same.Discussions critical of anything are important to true advancement of a field. Otherwise, we get a Theranos that hangs around longer and does even more damage.reply",
      "I don't think you read the comment you replied to correctly. He praised the article and approach therein, contrasting it to the LLM hype cycle, where effusive praise is met with harsh scorn, both sides often completely forgetting the reality in the argument.reply",
      "Ah yes, the bootlicker's desire of letting the bootlickers winout so there is only walls of bootlicking for any agnostic that happens across a post.I'd rather dissent so others know dissent is a rational response.reply",
      "Did you read what she/he says?reply",
      "It's really tiring that LLM skeptics will always talk about LLM fans every time AI comes up to strawman AI and satisfy their fragile fantasy world where everything is the sign of an AI bubble.But, yes this is a good way to use LLMs. Just like many other mundane and not news-worthy ways that LLMs are used today. The existence of fans doesn't require a denouncement of said fans at every turn.reply",
      "I am criticizing how AI progress is reported and discussed -- given how important this development is, accurate communication is even more important for the discussion.I think you inferring my motivation for the rant and creating a strawman yourself.I do agree that directing my rant at the generic \"fans\" is not productive. The article Tao wrote was a good example of communicating the result. I should direct my criticism at specific instances of bad communication, but not the general \"fans\".reply",
      "> The existence of fans doesn't require a denouncement of said fans at every turn.When said 'fans' are harmful it really does.Here's a counterexample to your hypothesis.  Fans of Nazis require denouncement at every turn.reply",
      "Hopefully this will finally stop the continuing claims[1] that LLMs can only solve problems they have seen before!If you listen carefully to the people who build LLMs it is clear that post-training RL forces them to develop a world-model that goes well beyond a \"fancy Markov chain\" that some seem to believe. Next step is building similar capabilities on top of models like Genie 3[2][1] eg https://news.ycombinator.com/item?id=45769971#45771146[2] https://deepmind.google/discover/blog/genie-3-a-new-frontier...reply"
    ],
    "link": "https://terrytao.wordpress.com/2025/11/05/mathematical-exploration-and-discovery-at-scale/",
    "first_paragraph": "Updates on my research and expository papers, discussion of open problems, and other maths-related topics.  By Terence Tao5 November, 2025 in math.CA, math.CO, math.MG, paper | Tags: Adam Zsolt Wagner, AlphaEvolve, Artificial Intelligence, Bogdan Georgiev, Javier Gomez-Serrano, optimization | by Terence Tao \nBogdan Georgiev, Javier G\u00f3mez-Serrano, Adam Zsolt Wagner, and I have uploaded to the arXiv our paper \u201cMathematical exploration and discovery at scale\u201c. This is a longer report on the experiments we did in collaboration with Google Deepmind with their AlphaEvolve tool, which is in the process of being made available for broader use. Some of our experiments were already reported on in a previous white paper, but the current paper provides more details, as well as a link to a repository with various relevant data such as the prompts used and the evolution of the tool outputs.\n\nAlphaEvolve is a variant of more traditional optimization tools that are designed to extremize some given sco"
  },
  {
    "title": "Show HN: TabPFN-2.5 \u2013 SOTA foundation model for tabular data (priorlabs.ai)",
    "points": 62,
    "submitter": "onasta",
    "submit_time": "2025-11-06T18:26:53 1762453613",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=45838540",
    "comments": [
      "The current go to solution for the kinds of problems that TabPFN is solving would be something like XGBoost. In general it's a good baseline, but the challenge is always that you need to spend a lot of time feature engineering and tweaking the data representation before something like XGBoost can deliver good performance on your regression or classification problems.For me the promise of foundation models for tabular data is that there are enough generalizable patterns, so that you need less manual feature engineering and data cleaning.And kudos to the team, I think it's a really creative application of neural networks. I was always frustrated with neural networks, since they were hard to tune on \"structured\" data and always under-performed (for me), but we also never had real foundational models for structured data.reply",
      "Less feature engineering is definitely something we are aiming for. The current version is actually only based on statistics, the real world connections between features is something we're working on right now and hope to show results for soon. That's the next stepreply",
      "Looks really cool. In reading through the FAQ, it says this:\nQ: \"How are text features handled?\"\nA: \"In the local package version text features are encoded as categoricals without considering their semantic meaning. Our API automatically detects text features and includes their semantic meaning into our prediction. The local package version encodes text as numerical categories and does not include semantic meaning.\"So that means that automatic embedding/semantic meaning is reserved for API use of TabPFN, right? Otherwise, if I use it locally, it's going to assign each of my distinct text values an arbitrary int, right?reply",
      "Yes exactly, the API is the best way to handle text features. The actual semantics often matter a lot . Is the API an option for you or would you need this local?reply",
      "I think you need a custom benchmark -- have you considered making one out of the excel world championships?reply",
      "It's fascinating how this works with such a small model. Especially given that the training is a kind of meta learning of \"how to do in-context learning\". I wonder, is there a good intuition of the role of the MLP in this architecture? For LLMs the consensus seems to be that they store knowledge...what would that be for tabular data?reply",
      "Tabular data is still underrated!reply",
      "When we released TabPFNv1 over three years ago, I didn\u2019t expect at all the hundreds of comments and reposts we would see. Tabular data had been a field getting little love from AI research\u2014but we immediately felt that this was a topic that data scientists, scientists, financial analysts, and enterprise users deeply cared about. Glad its useful to people!reply",
      "how does it compare to automl tools?reply",
      "TabPFN-2.5 default (one forward pass) matches AutoGluon 1.4 tuned for four-hours. Autogluon is the strongest AutoML including stacking of XGB and cat boost and even\nincludes the previous TabPFNv2.reply"
    ],
    "link": "https://priorlabs.ai/technical-reports/tabpfn-2-5-model-report",
    "first_paragraph": "The first tabular foundation model, TabPFN, and its successor TabPFNv2 have impacted tabular AI substantially, with dozens of methods building on it and hundreds of applications across different use cases.This report introduces TabPFN-2.5, the next generation of our tabular foundation model, scaling to 20\u00d7 data cells compared to TabPFNv2. On industry standard benchmarks with up to 50,000 data points and 2,000 features, TabPFN-2.5 substantially outperforms tuned tree-based models and matches the accuracy of AutoGluon 1.4, a complex four-hour tuned ensemble that even includes the previous TabPFNv2.For production use cases, we introduce a new distillation engine that converts TabPFN-2.5 into a compact MLP or tree ensemble, preserving most of its accuracy while delivering orders-of-magnitude lower latency and plug-and-play deployment.This new release will immediately strengthen the performance of the many applications andmethods already built on the TabPFN ecosystem.This new release will s"
  },
  {
    "title": "The APM paradox: Too much data, too few answers (honeybadger.io)",
    "points": 6,
    "submitter": "todsacerdoti",
    "submit_time": "2025-11-03T07:57:48 1762156668",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.honeybadger.io/blog/apm-paradox/",
    "first_paragraph": "Application Performance Monitoring (APM) means many things to many people. At its core, it enables developers to diagnose why their applications are slow and helps them provide a better experience to their users. Traditionally, this is accomplished by collecting a lot of data and displaying it in the form of dashboards and request traces. The problems you're trying to solve are generally known up front.For example, N+1 queries are a common issue in many web applications, so many APMs offer purpose-built tools to address them. Third-party HTTP requests are another common culprit, and so they provide instrumentation to track slow external API calls, timeouts, and retry patterns that slow down your response times. This cookie-cutter approach is common across legacy APM solutions. The data they collect and the interfaces they provide are glued together to solve a specific problem: application performance.Modern applications fail in ways that developers never anticipated. Your payment proce"
  }
]