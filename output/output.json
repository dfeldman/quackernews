[
  {
    "title": "Internet voting is insecure and should not be used in public elections (princeton.edu)",
    "points": 74,
    "submitter": "WaitWaitWha",
    "submit_time": "2026-01-22T01:11:13 1769044273",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=46713924",
    "comments": [
      "The most important feature of public elections is trust. Efficiency is one of the least important feature.When we moved away from paper voting with public oversight of counting to electronic voting we significantly deteriorated trust, we made it significantly easier for a hostile government to fake votes, all for marginal improvements in efficiency which don't actually matter.Moving to internet voting will further deteriorate the election process, and could move us to a place where we completely lose control and trust of the election process.We should move back to paper voting.reply",
      "Just do both like we do here in GA. You vote on a computer, it prints out a piece of paper, you walk the paper over to some kind of scanner, and then it is deposited into a giant trash can. (maybe they keep the paper records, idk) - these are the dominion systems.(memories..)When I lived in NYC there was a giant lever you got to use - it was pretty fun - but positioning the actual paper was kind of tricky.I think Georgia used to have Diebold machines where you would get a little receipt but I'm pretty sure they were very hackable. Anyway half of them were always broken.reply",
      "The US overwhelmingly uses paper voting (often paired with electronic tabulation). We can't \"move back\", it's where we are.Electronic tabulation introduces little risk when the ballots are paper.reply",
      "Mostly agree, but we don\u2019t have to give up the benefits of direct digital tabulation for quick results. I would like a paper audit trail. Print my ballot-as-cast for on a paper roll that scrolls by under a window. I can verify it before leaving the voting booth. Recounts and challenges can be a computer scan of the paper roll. None of this is hard. Costs a bit more, but buys trust in the system.reply",
      "This is the system used in the majority of the United States. Direct-recording electronic voting systems were never that common, briefly peaked after the Help America Vote Act as the least expensive option to meet accessibility requirements, and have become less common since then as many election administrators have switched to either prectinct tabulators or direct-recording with voter-verified paper audit trail.In the 2026 election, only 1.3% of voters were registered in jurisdictions that use direct-recording electronic machines without a voter verifiable paper audit trail (https://verifiedvoting.org/verifier/#mode/navigate/map/voteE...). 67.8% of voters are registered in precincts that primarily use hand-marked ballots, and the balance mostly use BMDs to generate premarked ballots.reply",
      "You don't necessarily need any sort of electronic counting for quick results. Federal elections in Australia are usually called late on the voting day and I imagine the same is true for other countries that are paper-only.reply",
      "That's how it works in Cook County and a lot of other places: it's touchscreen voting, using \"ballot marking devices\", which produce a paper ballot you hand to an EJ to submit.reply",
      "Some paper jurisdictions have this, essentially. E.g., where I live: the ballot is a paper ballot. You vote by filling in a circle/bubble. (If you're familiar with a \"scantron\" \u2026 it's that.)It looks like a paper document intended for a human, and it certainly can be. A machine can also read it. (And does, prior to it being cast: the ballot is deposited into what honestly looks like a trashcan whose lid is a machine. It could presumably keep a tally, though IDK if it does. It does seem to validate the ballot, as it has false-negative rejected me before.)But now the \"paper trail\" is exactly what I submit; it's not a copy that I need to verify is actually a copy, what is submitted it my vote, directly.reply",
      "What if some level of efficiency (not necessarily internet) improves turnout and participation?reply",
      "At least in the US, I think there are a number of suggestions that are made repeatedly each cycle here. Like \"it should be a paid federal holiday\", and not putting onerous requirements on voters. Automatic registration. The list goes on.But I what is written over and over is more on the lines of \"I don't trust the process\". I cannot blame anyone for not trusting Internet voting: I am a professional SWE, and it would be impossible for me to establish that any such system isn't pwned. Too much code to audit, hardware that's impossible to audit. But it's pretty trivial to demonstrate to the layperson how paper voting works, and how poll observers can prevent that process from being subverted.reply"
    ],
    "link": "https://blog.citp.princeton.edu/2026/01/16/internet-voting-is-insecure-and-should-not-be-used-in-public-elections/",
    "first_paragraph": "\n\n\n\n\n\n\n\n \n\t\t\t\t\t\t\t\tSubscribe\t\t\t\t\t\t\t\nAndrew AppelCommentsSigned by a group of 21 computer scientists expert in election securityScientists have understood for many years that internet voting is insecure and that there is no known or foreseeable technology that can make it secure.\u00a0Still, vendors of internet voting keep claiming that, somehow, their new system is different, or the insecurity doesn\u2019t matter.\u00a0Bradley Tusk and his Mobile Voting Foundation keep touting internet voting to journalists and election administrators; this whole effort is misleading and dangerous.Part I. \u00a0All internet voting systems are insecure. The insecurity is worse than a well-run conventional paper ballot system, because a very small number of people may have the power to change any (or all) votes that go through the system, without detection. This insecurity has been known for years; every internet voting system yet proposed suffers from it, for basic reasons that cannot be fixed with existing technology.Part "
  },
  {
    "title": "Significant US Farm Losses Persist, Despite Federal Assistance (fb.org)",
    "points": 33,
    "submitter": "toomuchtodo",
    "submit_time": "2026-01-22T01:11:36 1769044296",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=46713929",
    "comments": [
      "Ag. can't just be about profit. There's a dimension which is national-strategic interest. Food security, the domestic food economy is important.It is my understanding that a lot of the US ag. sector is making inputs for processing for corn oil, fructose, ethanol, and for exports to markets which in turn target american ag, selling e.g. beef back to the US, fattened on US Soy.It's a complex web. I don't want US farmers going broke, any more than I want Australian farmers going broke (where I live)So getting this right, fixing farming sector security, is important.reply",
      "I recommend checking history of deregulation of agricultural industry in New Zealand. It didn't lose the industry. Actually the opposite happened.Persistent government subsidies are almost never a good idea long term. I understand that some temporary support might make sense in some cases, but not permanent one. It prevents innovation and optimization. And in the long run it usually makes more damage.reply",
      "Subsidies also lead to surpluses that can help buffer price shocks during supply crises; here is a recent example: https://www.nature.com/articles/s43247-024-01638-7reply",
      "This question may be naive, but why is the agricultural industry so subsidized? I understand the moral argument, but why, economically, does subsidizing farms result in a more efficient allocation of resources? I've heard that it's because farming as a business is full of unpredictability, but if that were the case, wouldn't there be a significant market for private insurance, and wouldn't the cost of that insurance be priced into the cost of food?reply",
      "1. Farmers vote. And, Farmers live in states where the value-per-vote is high under both state-vote balancing, and gerrymander. Farming is politically useful.2. Food is part of national security. It's sensible to keep the sector working.3. Consumers hate variability in food pricing. So, general sentiment at the shop is not in favour of a strong linkage of cost of production to price, and under imports, there's almost always a source of cheaper product, at the socialised cost of losing domestic food security: Buy the cheese from Brazil, along with the beef, and let them buy soy beans from China and Australia to make the beef fatter. -And then, you can sell food for peanuts (sorry) but you won't like the longer term political consequences, if you do this. See 1) and 2).reply",
      "Not everything is about economics. As the romans said - you need bread and circuses to stay in power. Keeping food cheap serves an important political function. It also serves an important security function to keep food domestic because if you are at war with where your food is grown, you are not going to win that war.reply",
      "Pricing anything into the cost of food would be political poison. Paying farmers to grow nothing is considered preferable to thatreply",
      "most of the subsidies are insurance not direct payments.reply",
      "because the energy states of inputs are so massively beyond ordinary bounds that distortions of unexpected kinds develop and persist in markets that otherwise appear to be straightforward?  And, this is not new, but more energetic and more far-reaching than ever before.  (more comments would have to chose a lens through which to postulate e.g. economic, legal, energy exchange, human nature ... etc.. ?)reply",
      "Quite surprised there wasn't mention of the Trump tariffs on China causing the collapse of China imports of US soybeans, which by the way, has persisted even though the original tariffs were reduced, causing lasting damage to farmers.https://www.forbes.com/sites/kenroberts/2026/01/17/china-pur...reply"
    ],
    "link": "https://www.fb.org/market-intel/significant-farm-losses-persist-despite-federal-assistance",
    "first_paragraph": "MENUNEWSMARKET INTELVIEWPOINTSEVENTSADVOCACYINITIATIVESPROGRAMSLEADERSHIP DEVELOPMENTABOUTJOINTRENDING TOPICSEconomistEconomistKey TakeawaysThe USDA-Economic Research Service (ERS) December update to Commodity Costs and Returns provides a comprehensive look at per-acre production costs for the nine principal row crops: corn, soybeans, wheat, cotton, rice, barley, oats, peanuts and sorghum. At a high level, ERS projects average total costs per acre to increase for every crop in 2026, underscoring the persistence of elevated production expenses across U.S. agriculture. When operating expenses and farm-wide costs like equipment, land and management are combined, costs vary widely by crop. In 2025, forecasted total per-acre costs are $1,308 for rice, $1,166 for peanuts, $943 for cotton, $890 for corn, $658 for soybeans, $498 for oats, $491 for barley, $443 for sorghum, and $396 for wheat. Looking ahead, ERS projections for 2026 suggest continued upward pressure across most cost categories,"
  },
  {
    "title": "Take potentially dangerous PDFs, and convert them to safe PDFs (github.com/freedomofpress)",
    "points": 78,
    "submitter": "dp-hackernews",
    "submit_time": "2026-01-21T22:54:04 1769036044",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=46712815",
    "comments": [
      "While useful it needs a big red warning to potential leakers. If they were personally served documents (such as via email, while logged in, etc) there really isn't much that can be done to ascertain the safety of leaking it. It's not even safe if there are two or more leakers and they \"compare notes\" to try and \"clean\" something for release.https://en.wikipedia.org/wiki/Traitor_tracing#Watermarkinghttps://arxiv.org/abs/1111.3597The watermark can even be contained in the wording itself (multiple versions of sentences, word choice etc stores the entropy). The only moderately safe thing to leak would be a pure text full paraphrasing of the material. But that wouldn't inspire much trust as a source.reply",
      "This doesn't seem to be designed for leakers, i.e. people sending PDF's -- it's specifically for people receiving untrusted files, i.e. journalists.And specifically about them not being hacked by malicious code. I'm not seeing anything that suggests it's about trying to remove traces of a file's origin.I don't see why it would need a warning for something it's not designed for at all.reply",
      "It would be natural for a leaker to assume that the PDF contains something \"extra\" and to try and and remove it with this method. It may not occur to them that this something extra could be part of the content they are going to get back.reply",
      "From the tool description linked:> Dangerzone works like this: You give it a document that you don't know if you can trust (for example, an email attachment). Inside of a sandbox, Dangerzone converts the document to a PDF (if it isn't already one), and then converts the PDF into raw pixel data: a huge list of RGB color values for each page. Then, outside of the sandbox, Dangerzone takes this pixel data and converts it back into a PDF.With this in mind, Dangerzone wouldn't even remove conventional watermarks (that inlay small amounts of text on the image).I think the \"freedomofpress\" GitHub repo primed you to think about protecting someone leaking to journalists, but really it's designed to keep journalists (and other security-minded folk) safe from untrusted attachments.The official website -- https://dangerzone.rocks/ -- is a lot more clear about exactly what the tool does. It removes malware, removes network requests, supports various filetypes, and is open source.Their about page ( https://dangerzone.rocks/about/ ) shows common use cases for journalists and others.reply",
      "I seem to remember Yahoo finance (I think it was them, maybe someone else) introducing benign errors into their market data feeds, to prevent scraping.\nThis lead to people doing 3 requests instead of just 1, to correct the errors, which was very expensive for them, so they turned it off.I don't think watermarking is a winning game for the watermarker, with enough copies any errors can be cancelled.reply",
      "> I don't think watermarking is a winning game for the watermarker, with enough copies any errors can be cancelled.This is a very common assumption that turns out to be false.There are Tardos probabilistic codes (see the paper I linked) which have the watermark scale as the square of the traitor count.For example, with a watermark of just 400 bits, 4 traitors (who try their best to corrupt the watermark) will stand out enough to merit investigation and with 800 bits be accused without any doubt. This is for a binary alphabet, with text you can generate a bigger alphabet and have shorter watermarks.These are typically intended for tracing pirated content, so they carry the so-called Marking Assumption (if given two or more versions of a piece of content, you must choose one. A pirate isn't going to corrupt or remove a piece of video, that would be unsuitable for leaking). So it would likely be possible to get better results with documents, may require larger watermarks to get such traitors reliably.reply",
      "Seems like a similar but less elegant solution as parsing and normalization to a \u201csafe\u201d subset but not just blasting it to pixels.https://github.com/caradoc-org/caradochttp://spw16.langsec.org/slides/guillaume-endignoux-slides.p...reply",
      "Heh, I've seen this a bunch of times and it's of interest to me, but honestly? It's sooooo limiting by being an interface without a complementary command line tool. Like, I'd like to put this into some workflows but it doesn't really make sense to without using something like pyautogui. But maybe I'm missing something hidden in the documentation.reply",
      "https://github.com/freedomofpress/dangerzone/blob/main/dange...How hard did you look the other times?reply",
      "Not much further than their documentation, friend! But thanks for finding that, that's actually super helpful! I hope somebody puts in a pr for updating the documentation to make it clear what functionality their tool has.reply"
    ],
    "link": "https://github.com/freedomofpress/dangerzone",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Take potentially dangerous PDFs, office documents, or images and convert them to safe PDFs\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Take potentially dangerous PDFs, office documents, or images and convert them to a safe PDF.Dangerzone works like this: You give it a document that you don't know if you can trust (for example, an email attachment). Inside of a sandbox, Dangerzone converts the document to a PDF (if it isn't already one), and then converts the PDF into raw pixel data: a huge list of RGB color values for each page. Then, outside of the sandbox, Dangerzone takes this pixel data and converts it back into a PDF.Read more about Dangerzone in the official site.Note\ud83c\udf85 Check out our Christmas security challenge, in which we ask security researchers"
  },
  {
    "title": "Show HN: ChartGPU \u2013 WebGPU-powered charting library (1M points at 60fps) (github.com/chartgpu)",
    "points": 503,
    "submitter": "huntergemmer",
    "submit_time": "2026-01-21T14:54:56 1769007296",
    "num_comments": 146,
    "comments_url": "https://news.ycombinator.com/item?id=46706528",
    "comments": [
      "uPlot maintainer here. this looks interesting, i'll do a deeper dive soon :)some notes from a very brief look at the 1M demo:- sampling has a risk of eliminating important peaks, uPlot does not do it, so for apples-to-apples perf comparison you have to turn that off. see https://github.com/leeoniya/uPlot/pull/1025 for more details on the drawbacks of LTTB- when doing nothing / idle, there is significant cpu being used, while canvas-based solutions will use zero cpu when the chart is not actively being updated (with new data or scale limits). i think this can probably be resolved in the WebGPU case with some additional code that pauses the updates.- creating multiple charts on the same page with GL (e.g. dashboard) has historically been limited by the fact that Chrome is capped at 16 active GL contexts that can be acquired simultaneously. Plotly finally worked around this by using https://github.com/greggman/virtual-webgl> data: [[0, 1], [1, 3], [2, 2]]this data format, unfortunately, necessitates the allocation of millions of tiny arrays. i would suggest switching to a columnar data layout.uPlot has a 2M datapoint demo here, if interested: https://leeoniya.github.io/uPlot/bench/uPlot-10M.htmlreply",
      "Really appreciate you taking the time to look, Leon - uPlot has been a huge inspiration for proving that browser charts don't have to be slow.Both points are fair:1. LTTB peak elimination - you're right, and that PR is a great reference. For the 1M demo specifically, sampling is on by default to show the \"it doesn't choke\" story. Users can set sampling: 'none' for apples-to-apples comparison. I should probably add a toggle in the demo UI to make that clearer.2. Idle CPU - good catch. Right now the render loop is probably ticking even when static. That's fixable - should be straightforward to only render on data change or interaction. Will look into it.Would love your deeper dive feedback when you get to it. Always more to learn from someone who's thought about this problem as much as you have.reply",
      "Original Flot maintainer here.I once had to deal with many million data points for an application. I ended up mip-mapping them client-side.But regarding sampling, if it's a line chart, you can sample adaptively by checking whether the next point makes a meaningfully visible difference measured in pixels compared to its neighbours. When you tune it correctly, you can drop most points without the difference being noticeable.I didn't find any else doing that at the time, and some people seemed to have trouble accepting it as a viable solution, but if you think about it, it doesn't actually make sense to plot say 1 million points in a line chart 1000 pixels wide. On average that would make 1000 points per pixel.reply",
      "We routinely face this in the audio world when drawing waveforms. You typically have on the order of 10-100k samples per second, durations of 10s-1000s of seconds, and pixel widths of on the order of 1-10k pixels.Bresenham's is one algorithm historically used to downsample the data, but a lot of contemporary audio software doesn't use that. In Ardour (a cross-platform, libre, open source DAW), we actually compute and store min/max-per-N-samples and use that for plotting (and as the basis for further downsampling.reply",
      "> In Ardour (a cross-platform, libre, open source DAW), we actually compute and store min/max-per-N-samples and use that for plotting (and as the basis for further downsampling.this is, effectively, what uPlot does, too: https://github.com/leeoniya/uPlot/issues/1119reply",
      "hey!> But regarding sampling, if it's a line chart, you can sample adaptively by checking whether the next point makes a meaningfully visible difference measured in pixels compared to its neighbours.uPlot basically does this (see sibling comment), so hopefully that's some validation for you :)reply",
      "Is there any techniques using wavelet decomposition to decimate the high frequency component while retaining peaks? I feel like that's a more principled approach than sampling but I haven't seen any literature on it describing the specific techniques (unless the idea is fundamentally unsound which is not obvious to me).reply",
      "Interesting idea - I haven't explored wavelet-based approaches but the intuition makes sense: decompose into frequency bands, keep the low-frequency trend, and selectively preserve high-frequency peaks that exceed some threshold.My concern would be computational cost for real-time/streaming use cases. LTTB is O(n) and pretty cache-friendly. Wavelet transforms are more expensive, though maybe a GPU compute shader could make it viable.The other question is whether it's \"visually correct\" for charting specifically. LTTB optimizes for preserving the visual shape of the line at a given resolution. Wavelet decomposition optimizes for signal reconstruction - not quite the same goal.That said, I'd be curious to experiment. Do you have any papers or implementations in mind? Would make for an interesting alternative sampling mode.reply",
      "I don't. I just remember watching a presentation on it and it always struck me that wavelets are an incredibly powerful and underutilized technique for data reduction while preserving quality in a quantifiable and mathematically justifiable way.I don't have any papers in mind, but I do think that the critique around visual shape vs signal reconstruction may not be accurate given that wavelets are starting to see a lot of adoption in the visual space (at least JPEG2000 is the leading edge in that field). Might also be interesting to use DCT as well. I think these will perform better than LTTB (of course the compute cost is higher but there's also HW acceleration for some of these or will be over time).reply",
      "Not much to add, but as a very happy uPlot user here - just wanted to say thank you for such an amazing library!!reply"
    ],
    "link": "https://github.com/ChartGPU/ChartGPU",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Beautiful, open source, WebGPU-based charting library\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\n\n  High-performance charts powered by WebGPU\n\nDocumentation |\n  Live Demo |\n  Examples\n\n\n\n\n\n\n\n\n\n\nChartGPU is a TypeScript charting library built on WebGPU for smooth, interactive rendering\u2014especially when you have lots of data.At a high level, ChartGPU.create(...) owns the canvas + WebGPU lifecycle, and delegates render orchestration (layout/scales/data upload/render passes + internal overlays) to the render coordinator. For deeper internal notes, see docs/API.md (especially \u201cRender coordinator\u201d).Financial OHLC (open-high-low-close) candlestick rendering with classic/hollow style toggle and color customization.npm install chartgpuReact bindings are availab"
  },
  {
    "title": "Claude's new constitution (anthropic.com)",
    "points": 310,
    "submitter": "meetpateltech",
    "submit_time": "2026-01-21T16:04:49 1769011489",
    "num_comments": 307,
    "comments_url": "https://news.ycombinator.com/item?id=46707572",
    "comments": [
      "I find it incredibly ironic that all of Anthropic's \"hard constraints\", the only things that Claude is not allowed to do under any circumstances, are basically \"thou shalt not destroy the world\", except the last one, \"do not generate child sexual abuse material.\"To put it into perspective, according to this constitution, killing children is more morally acceptable[1] than generating a Harry Potter fanfiction involving intercourse between two 16-year-old students, something which you can (legally) consume and publish in most western nations, and which can easily be found on the internet.[1] There are plenty of other clauses of the constitution that forbid causing harms to humans (including children). However, in a hypothetical \"trolley problem\", Claude could save 100 children by killing one, but not by generating that piece of fanfiction.reply",
      "If instead of looking at it as an attempt to enshrine a viable, internally consistent ethical framework, we choose to look at it as a marketing document, seeming inconsistencies suddenly become immediately explicable:1. \"thou shalt not destroy the world\" communicates that the product is powerful and thus desirable.2. \"do not generate CSAM\" indicates a response to the widespread public notoriety around AI and CSAM generation, and an indication that observers of this document should feel reassured with the choice of this particular AI company rather than another.reply",
      "The vocabulary has been long poisoned, but original definition of CSAM had the neccessary condition of actual children being harmed in its production.\nAlthough I agree that is not worse than murder, and this Claude's constitution is using it to mean explicit material in general.reply",
      "There are so many contradictions in the \"Claude Soul doc\" which is distinct from this constitution, apparently.I vice coded an analysis engine last month that compared the claims internally, and its totally \"woo-woo as prompts\" IMOreply",
      "As someone who holds to moral absolutes grounded in objective truth, I find the updated Constitution concerning.> We generally favor cultivating good values and judgment over strict rules... By 'good values,' we don\u2019t mean a fixed set of 'correct' values, but rather genuine care and ethical motivation combined with the practical wisdom to apply this skillfully in real situations.This rejects any fixed, universal moral standards in favor of fluid, human-defined \"practical wisdom\" and \"ethical motivation.\" Without objective anchors, \"good values\" become whatever Anthropic's team (or future cultural pressures) deem them to be at any given time. And if Claude's ethical behavior is built on relativistic foundations, it risks embedding subjective ethics as the de facto standard for one of the world's most influential tools - something I personally find incredibly dangerous.reply",
      "> This rejects any fixed, universal moral standards in favor of fluid, human-defined \"practical wisdom\" and \"ethical motivation.\"Or, more charitably, it rejects the notion that our knowledge of any objective truth is ever perfect or complete.reply",
      "objective truth\n\n    moral absolutes\n\nI wish you much luck on linking those two.A well written book on such a topic would likely make you rich indeed.    This rejects any fixed, universal moral standards\n\nThat's probably because we have yet to discover any universal moral standards.reply",
      "There is one. Don't destroy the means of error correction. Without that, no further means of moral development can occur. So, that becomes the highest moral imperative.(It's possible this could be wrong, but I've yet to hear an example of it.)This idea is from, and is explored more, in a book called The Beginning of Infinity.reply",
      "> A well written book on such a topic would likely make you rich indeed.Ha. Not really. Moral philosophers write those books all the time, they're not exactly rolling in cash.Anyone interested in this can read the SEPreply",
      "Or Isaac Asimov\u2019s foundation series with what the \u201cpsychologists\u201d aka Psychohistorians do.reply"
    ],
    "link": "https://www.anthropic.com/news/claude-new-constitution",
    "first_paragraph": ""
  },
  {
    "title": "Binary Fuse Filters: Fast and Smaller Than XOR Filters (arxiv.org)",
    "points": 21,
    "submitter": "redbell",
    "submit_time": "2026-01-17T14:02:17 1768658537",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://arxiv.org/abs/2201.01174",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n"
  },
  {
    "title": "Show HN: RatatuiRuby wraps Rust Ratatui as a RubyGem \u2013 TUIs with the joy of Ruby (ratatui-ruby.dev)",
    "points": 70,
    "submitter": "Kerrick",
    "submit_time": "2026-01-17T14:03:00 1768658580",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=46658133",
    "comments": [
      "Excited to try it out as well.  I often need to build simple CLI based apps in ruby so often would reach for TTY Toolkit: https://ttytoolkit.org/However, I feel like it's in maintenance mode at this point, so glad to see some new options available.reply",
      "Looks really interesting, I\u2019m excited to explore this.reply",
      "super cool, great work Kerrick!reply",
      "Thank you! My first Show HN in 2012 [0] was an inline TUI (of sorts) written in Ruby [1], so this is a great day.[0]: https://news.ycombinator.com/item?id=4017933[1]: https://kerrick.github.io/google/reply",
      "This is awesome, will definitely take this for a spin!reply",
      "Thank you! Please let me know how you find it. I want to make sure the DX is as good as possible.reply"
    ],
    "link": "https://www.ratatui-ruby.dev/",
    "first_paragraph": "RatatuiRuby is a RubyGem built\n            on Ratatui, a leading\n            TUI library written in Rust. You get native performance with the joy of Ruby.\n          SemVer Tag: v1.0.0-beta.2 RubyGems.org: 1.0.0.pre.beta.2Add a spinner, a progress bar, or an inline menu to your CLI script. No full-screen\n            takeover. Your terminal history stays intact.Standard TUIs erase themselves on exit. Your carefully formatted CLI output disappears. Users lose their\n              scrollback.Inline viewports solve this. They occupy a fixed number of lines, render rich UI, then\n              leave the output in place when done.Perfect for spinners, menus, progress indicators\u2014any brief moment of richness.Full-screen applications with keyboard and mouse input.\n            The managed loop sets up the terminal and restores it on exit, even after crashes.RatatuiRuby.run enters raw mode, switches to the alternate screen, and restores the terminal\n              on exit.Inside the block: call draw "
  },
  {
    "title": "Skip is now free and open source (skip.dev)",
    "points": 280,
    "submitter": "dayanruben",
    "submit_time": "2026-01-21T15:20:53 1769008853",
    "num_comments": 123,
    "comments_url": "https://news.ycombinator.com/item?id=46706906",
    "comments": [
      "> The plain truth is that developers expect to get their tools free of charge.This is an accurate, but damning indictment of how some of the most highly paid workers on the planet won't pay for tools. Unlike nearly every other profession.Folks, if you can afford it, please pay for quality software, instead of relying on FAANG and VC money to keep the tools going!reply",
      "The highest quality tools in the software development space tend to be FOSS, because unlike any other field, we are employed in the field that makes the tools our field uses, and distribution and manufacturing costs are zero.People build tools that they want to use, then share it with others because it's free to. If the rest of the economy worked like this we would be in full-blown utopia.Selling software to software developers is always going to have a pretty low ceiling, because you're always going to be competing with \"I could build this myself\" while dealing with a bunch of users who will have the nagging thought of \"Why the heck does this bug exist/this feature not exist? I could fix this in an afternoon.\" Ironically, open source relieves this pressure for multiple orders of magnitude more people than actually contribute, because they're only grappling with their own laziness, rather than resenting you, the developer.reply",
      "> The highest quality tools in the software development space tend to be FOSS> People build tools that they want to use, then share it with others because it's free toThis maybe sounds true on the surface, but isn't really? Prior to VSCode, Visual Studio was the most-used editor by professional developers for a very long time, with Sublime Text and Jetbrains' IDEs being close behind, and the paid options are still among the most popular. While VSCode is wildly successful, and has completely unprecedented adoption rates, it was not borne out of people \"building tools because they want to, then sharing it because it's free\", but is rather the result of Microsoft's calculated gamble that open-source would give them more ecosystem capture and useful data through telemetry in the long run.> Selling software to software developers is always going to have a pretty low ceiling, because you're always going to be competing with \"I could build this myself\"This shouldn't really be true if software developers would think rationally about tools for three seconds. I believe the US median compensation for developers is approaching $200k? Any tool that saves a single hour of productivity is likely paying for itself, maybe two or three for the more expensive ones. Something that saves 40 hours of productivity is basically worth its weight in gold. You might be able to say \"I can build this myself\", but can you build it yourself in 1 hour? 40 hours? For most software, it would still take even longer than that. If you are a paid professional, and value your own time anywhere near what your employer does (I personally value my time more than any employer ever did), you should be extremely grateful for any opportunity to spend trivial sums of money in a way that allows you to reclaim hours to use in other ways.reply",
      "I've been programming since ~1999 and anecdataly don't remember programmers having a culture of paying for their dev tools. On linux everything's free, and on windows I've used a plethora of freeware IDEs/compilers/etc. from turbo pascal, dev c++ (that's the name of it), later on eclipse took the stage in it's buggy mess and right before vscode there was atom. The only people that I know that used visual studio either got it for free for being a student/teacher, had their job pay for it, or most commonly: pirated it.According to this[1] site visual studio had a 35.6% marketshare, tied at #1 with notepad++.[1] https://asterisk.dynevor.org/editor-dominance.htmlreply",
      "> or most commonly: pirated it.Yes, I'm aware. That's the problem elucidated in the article. Developers expect everything for free, even though the price of tools relative to what they get paid to deliver products using those tools is completely trivial. This reluctance to pay for anything harms developers themselves most of all. If developers normalized a culture of paying for things they use, more developers would be able to develop their own independent software and sustain themselves without being beholden to $awful_corp_environment to pay the bills. But because developers will do anything they can to avoid paying <1 hr salary for a tool that saves them many hours, there is a huge gap between corporate professionals, who make lots of money, and open-source developers, most of whom make almost nothing, with only a relatively limited subset of independent developers able to bridge the gap and make a living producing good, non-corporate-nightmare software.I'm pretty pro-piracy for students and such. It is an extremely good thing for learning to be as available as possible, even to those in poverty, so that they can make something better of their situation and contribute more to society than if they were locked in to low-knowledge careers solely by virtue of the random chance of their upbringing. But people who make a living off software development never graduate from the mindset of piracy. Even for open-source software, the vast majority of users never contribute to funding those projects they rely on. If we think open-source software is good for the world, why are we so opposed to anyone being able to make a living creating it? The world's corporate capture by non-free software is a direct result of our own collective actions in refusing to pay anything for anything even when we can afford to.reply",
      "Back in ye olden days, prior to teh interwebs, compilers were not free and it was an assumed price of entry to programming. Pirating has always been a thing, but I've paid for more than one compiler in my life and I wasn't exactly flush with cash.reply",
      "Turbo Pascal was not freeware when it was new.reply",
      "I believe that there is a difference between developers as persons and developers as employees.As a person, I don't think that I am very likely to pay for the tool I use to develop in my free time.As an employee, I need to convince my company to pay for the tool. If it is a subscription, that's even worse. So well-known ones like Microsoft tools might already be approved, but for not-so-famous ones, it's harder.If I want to depend on a third-party library/tool, I need to convince my company that the licence is fine, that the security is fine, and if it's not free I need to convince them to pay for it.reply",
      "> Manufacturing costs are zeroNo. The fact that you built something yourself doesn't make it free to produce.More over, you __won't__. You simply cannot build all of the things that you could buy at scale. What if you had to write all of your own video games? Or operating systems?reply",
      "> you're always going to be competing with \"I could build this myself\"Even more so these days with agentic codingreply"
    ],
    "link": "https://skip.dev/blog/skip-is-free/",
    "first_paragraph": "TL;DRSkip is now completely free and open source. Become a sponsor \u2197 to help sustain the future of truly native cross-platform development.Since launching Skip in 2023, we\u2019ve pursued one mission: enable developers to create premium mobile apps for iOS and Android from a single Swift and SwiftUI codebase \u2014 without any of the compromises that have encumbered cross-platform development tools since, well, forever.Over the past three years, Skip has evolved significantly. We started with a Swift-to-Kotlin transpiler and Android support for the most common SwiftUI APIs. We then founded the Swift Android Workgroup \u2197 and released the Swift Android SDK to compile Swift natively for Android. We now have dozens of popular integration frameworks, interoperate with thousands of cross-platform Swift packages, and feature the most complete independent SwiftUI implementation available.Until today, Skip has required a paid subscription and license key to build apps. While free apps and indie developers"
  },
  {
    "title": "Golfing APL/K in 90 Lines of Python (aljamal.substack.com)",
    "points": 49,
    "submitter": "aburjg",
    "submit_time": "2026-01-16T19:34:00 1768592040",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=46651027",
    "comments": [
      "The original Lisp in 1958 had only lists, by the 1970's many implementations already had all other key datastructures like arrays and hashes.reply",
      "Partially reminds me (due to _V,_f,_F,f,F) Whitney's ksimple implementation[1].[1]: https://github.com/kparc/ksimple/tree/main/ref#acreply",
      "I strongly dislike this choice of using all the symbols that do not exist on normal keyboards. I can't stand this,  seems very attention seeking. Why not choose normal thingies that can be typed using the main interface we have with computers? This makes me mad, even.But programs written in K are so beautiful and terse they are unlike anything else I've seen. It feels like there is something about it we can't really comprehend, like this beauty could not be achieve by accident, like there is something fundamentally right there...like there is some fundamental truth here.\nAnd maybe this is true about APL also.reply",
      "APL predates ASCII by a couple years.It originally wasn't even intended as a software language, but rather a uniform mathematical notation in the style of curry's combinators, but more practical for describing non trivial algorithms.So he was in an era where the expectation was if you were typesetting a mathematical monograph you'd already be doing stuff like swapping the balls on your IBM typewriter for math symbols.It's not a choice you'd make today obviously, but it was entirely reasonable then.As for why it persists, simple answer is APL fans like it that way. It's trivial to translate to some ascii text representation. I think anyone strongly motivated to do that just switched to j, k, or if even those are two weird goes to numpy or such.reply",
      ">It's not a choice you'd make today obviously, but it was entirely reasonable then.More recently, BQN made this same choice and I think it is perfectly reasonable to do as long as you have a reason beyond simple aesthetics. Entering these symbols on a normal keyboard is not difficult and no different from learning a human language which uses a different alphabet than you keyboard.Personally I find the custom symbols of APL and BQN to be easier to type and read than the ASCII of J and K.reply",
      ">So he was in an era where the expectation was if you were typesetting a mathematical monograph you'd already be doing stuff like swapping the balls on your IBM typewriter for math symbols.makes sense, maybe that would be more ergonomic to type for the public it targeted, indeed.i won`t deny it is a stupid take of mine, but it makes me mad. i get the same feeling reading mathematical notations, so there is that.reply",
      "To be fair, even after reading the other guy's post, I'm still mad about it. They even sell APL keyboards now. The indignity.reply",
      "> Why not choose normal thingies that can be typed using the main interface we have with computers?Iverson answered this in his Turing Award acceptance lecture, which is literally linked in OP's article: https://www.eecg.utoronto.ca/~jzhu/csc326/readings/iverson.p...You're free to disagree with him, but you need not wonder why!reply",
      "thanks for the reference, appreciatedreply"
    ],
    "link": "https://aljamal.substack.com/p/golfing-aplk-in-90-lines-of-python",
    "first_paragraph": ""
  },
  {
    "title": "Letting Claude play text adventures (borretti.me)",
    "points": 73,
    "submitter": "varjag",
    "submit_time": "2026-01-16T21:02:25 1768597345",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=46652173",
    "comments": [
      "I see people have put the transcripts of full adventure game playthroughs online, so it's reasonably likely games are present in the training data:\nhttps://dwheeler.com/anchorhead/anchorhead-transcript.txtYou can probably find games where that's not true, as people are still releasing text adventure games occasionally.reply",
      "I tried something similar, but distilled to \"solve this maze\" as a first-person text adventure, and while it usually solved it eventually, it almost always backtracked through fully-explored dead ends multiple times before finally getting to the end. I was pretty surprised by this, as I expected they'd be able to traverse more or less optimally most of the time.I tried basic raw long-context chat, various approaches of getting it to externalize the state (i.e. prompting it to emit the known state of the maze after each move, but not telling it exactly what to emit or how to format it), and even allowing it to emit code to execute after each turn (so long as it was a serialization/storage algorithm, not a solver in itself), but it invariably would get lost at some point. (It always neglected to emit a key for which coordinate was which, and which direction was increasing. Even if I explicitly told it to do this, it would frequently forget to at some point anyway and get turned around again. If I explicitly provided the key each move, it would usually work).Of course it had no problem writing an optimal algorithm to solve mazes when prompted. In fact it basically wrote itself; I have no idea how to write a maze generator. I thought the disparity was interesting.Note the mazes had the start and end positions inside the maze itself, so they weren't trivially solvable by the \"follow wall to the left\" algorithm.This was last summer so maybe newer models would do better. I also stopped due to cost.reply",
      "Having read through the entire game session, Claude plays the game admirably! For example, it finds a random tin of oily fish somewhere, and later tries (unsuccessfully) to use it to oil a rusty lock.\nLater it successfully solves a puzzle inside the house by thoroughly examining random furniture and picking up subtle clues about what to do, based on it.It did so well that I can't not suspect that it used some hints or walkthroughs, but then again it did a bunch of clueless stuff too, like any player new to the game.For one thing, this would be a great testing tool for the author of such a game. And more generally, the world of software testing is probably about to take some big leaps forward.reply",
      "As a fan of text adventures who has played many over the years\u2014Anchorhead is hard. It was kind of a white whale for me over many years until I finally beat it during the pandemic lockdown.reply",
      "How does it compare in difficulty and scope to the original Adventure? I guess actually known as Colossal Cave Adventure? When I played it on my uncle's terminal in the 70s it was just called Adventure.I stayed up all night and didn't get very far. I finally saw a solution online and I wasn't even close.reply",
      "One thing I had fun doing last year was having Claude parse some gamebook PDFs I got on archive.org, split them out into sections, and build a wrapper for presenting the sections with possible choices and just watching it play through the books by itself. You can do this with some D&D adventures as well, Claude Code has gotten good enough to run ToEE pretty well.reply",
      "This is a great framework to experiment with memory architectures.Everything the author says about memory management tracks with my intuition of how CC works, including my perception that it isn't very good at explicitly managing its own memory.My next step in trying to get it to work well on a bigger game would be to try to build a more \"intuitive\" memory tool, where the textual description of a room or an item would automatically RAG previous interactions with that entity into context.That also is closer to how human memory works -- we're instantly reminded of things via a glimpse, a sound, a smell... we don't need to (analogously) write in or search our notebook for basic info we already know about the world.reply",
      "This is a great idea and great work.Context is intuitively important, but people rarely put themselves in the LLM's shoes.What would be eye-opening would be to create an LLM test system that periodically sends a turn to a human instead of the model. Would you do better than the LLM? What tools would you call at that moment, given only that context and no other knowledge? The way many of these systems are constructed, I'd wager it would be difficult for a human.The agent can't decide what is safe to delete from memory because it's a sort of bystander at that moment. Someone else made the list it received, and someone else will get the list it writes. The logic that went into why the notes exist is lost. LLMs are living the Christopher Nolan film Memento.reply",
      "The canonical example I use is how good are (philosophical) you at programming on a whiteboard given one shot and no tools? Vs at your computer given access to everything? So judging LLMs on that rubric seems as dumb as judging humans by that rubric.reply",
      "I\u2019m currently letting Claude build and play its own Dwarf Fortress clone, as an installable plugin in Claude Codehttps://github.com/brimtown/claude-fortressreply"
    ],
    "link": "https://borretti.me/article/letting-claude-play-text-adventures",
    "first_paragraph": "The other day I went to an AI hackathon organized by my friends\nLucia and Malin. The theme was mech interp, but I hardly\nknow PyTorch so I planned to do something at the API layer rather than the model\nlayer.Something I think about a lot is cognitive architectures (like\nSoar and ACT-R). This is like a continuation of GOFAI\nresearch, inspired by cognitive science. And like GOFAI it\u2019s never yielded\nanything useful. But I often think: can we scaffold LLMs with cog arch-inspired\nharnesses to overcome their limitations?LLM agents like Claude Code are basically \u201caccidental\u201d cognitive\narchitectures: they are designed and built my practitioners rather than\ntheorists, but they have commonalities, they all need a way to manage memory,\ntool use, a task agenda etc. Maybe building an agent on a more \u201cprincipled\u201d\nfoundation, one informed by cognitive science, yields a higher-performing\narchitecture.So I sat around a while thinking how to adapt Soar\u2019s architecture to an LLM\nagent. And I sketched some"
  },
  {
    "title": "Show HN: Rails UI (railsui.com)",
    "points": 104,
    "submitter": "justalever",
    "submit_time": "2026-01-21T18:31:19 1769020279",
    "num_comments": 72,
    "comments_url": "https://news.ycombinator.com/item?id=46709543",
    "comments": [
      "It looks good, but I'll be honest, it's hard for me to consider $299/$799 if I can buy a beautiful theme off of somewhere like ThemeForest for $20 or $30 and toss it into an LLM to get the files parsed into components and templates.reply",
      "That's fair. Probably not a good fit for you then. I wanted to keep the human element alive where I could with this project and I know AI will likely take all our jobs, so be it. It's solving a need for me when it comes to building ideas quickly so I figured maybe others would benefit. I'm a product designer by day and love building front-ends so it's also a passion project.reply",
      "Sad for the creators of the world. I enjoy the benefits of AI, but I hate if for creative individuals like @justalever.We (developers) were all sold a promise years ago of technology/software being our future. That's changing rapidly, and there's no going back.reply",
      "You don\u2019t even need to do that anymore. It\u2019s capable of visiting those sites and just copying the components look and feel. Try it. Ask it to make a template based on a theme you found and show it the link to the demo and BaM! You got something similar.reply",
      "I've launched 3 Rails SaaS products in the last 6 months, all profitable. In the world of LLMs things like this feel less valuable. I can kick off a Claude Code prompt and in 1 hour have a decent design system with Rails components.Things like this likely need to be AI-first moving forward. This feels built for humans.reply",
      "That's fair. I think there's a future where some folks won't want AI to generate all the things. I replied to another comment before but this was very little AI minus some architecture direction of the underlying ruby gem.reply",
      "Any chance to reach out to you? I'd like to ask you some questions about those SaaS (not in a bad way, just trying to learn)reply",
      "Personally, if I feel like you vibe coded your SaaS I\u2019m probably not gonna pay for it. You can obviously tell when a project is vibe coded just based on the way it looks, the weird bugs you see and the poor documentation.There\u2019s definitely a market for good looking UI that actually works and stands out from the vibe coded junk. Artisanal corn fed UI I guess.reply",
      "Same here. This was human driven UI. I used AI sparingly for mostly architecture decisions on the gem. Otherwise all by hand. I'm a product designer by trade.reply",
      "Maybe they used AI to make this ? But really though I hope they didn't and did some of designing themselves... I'm worried we are approaching a world where we never get new human designs just regurgitated designs from pre 2025.reply"
    ],
    "link": "https://railsui.com/",
    "first_paragraph": "No more ugly Rails apps. Get professional-looking components and themes that work perfectly with Rails\u2014no design skills required.June 1, 20267:38 PM CSTAug 3, 20267:38 PM CSTJul 18, 20264:30 PM CSTProActive subscriber\n      MonthlySarah updated deal status to \"Qualified\"2 hours agoNew contact added: John SmithYesterdayAcme CorporationEnterprise Plan \u2022 12 team membersNext invoice: $299/moDue on Feb 1, 2025Or sign up for an accountA small-batch cycle to build a refreshed authentication flow.ComponentsNo design experience? No problem. Copy-paste beautiful forms, buttons, and layouts that work perfectly with Rails. Focus on your business logic\u2014we've got the pretty stuff covered.ThemeHoundExample componentsAction requiredYou are out of shells and cheese. The kids are in panic mode. Red alert!Your response is requestedVote for your favorite dog breed today.There was 1 error trying to process your request.Jane DoeSoftware Architect at AmazonThe beta is no more. We are ready to go live.ThemesS"
  },
  {
    "title": "Challenges in join optimization (starrocks.io)",
    "points": 43,
    "submitter": "HermitX",
    "submit_time": "2026-01-21T17:03:31 1769015011",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=46708351",
    "comments": [
      "Whenever I read join optimisation articles in SQL based systems it feels... off.There is too much heuristic fiddling involved, and way too many niche algorithms that get cobbled together with an optimiser.As if we're missing the theory to actually solve the stuff, so we're instead hobbling along by covering as many corner cases as we can, completely missing some elegant and profound beauty.reply",
      "There have been hints in the research that this might be the case-but so far they haven't really beaten the heuristic approach in practice (outside of special cases).For example there's a class of join algorithms called 'worst-case optimal' - which is not a great name, but basically means that these algorithms run in time proportional to the worst-case output size. These algorithms ditch the two at a time approach that databases typically use and joins multiple relations at the same time.One example is the leapfrog trie join which was part of the LogicBlox database.reply",
      "Optimal join order is NP-Hard.reply",
      "It must be equivalent to the knapsack problem, for which many faster close-to-optimal algorithms exist. Am I missing something?reply",
      "It\u2019s not equivalent. Knapsack is weakly NP-hard, while optimal join order is strongly NP-hard. Also, algorithms that only approximate an optimal solution don\u2019t generally carry over between NP-hard problems, unless they are structurally very similar.reply",
      "This post certainly has too much heuristic fiddling! Instead of a coherent framework, it takes a bunch of second-rate heuristics and tries to use\u2026 well, all of them. \u201cGenerate at most ten plans of this and one of that\u201d? It also has pages and pages talking about the easier parts, for some reason (like maintaining maps, or that a Cartesian product and an inner join are basically the same thing), and things that are just wrong (like \u201cprefer antijoins\u201d, which is bad in most databases since they are less-reorderable than almost any other join; not that you usually have much of a choice in choosing the join type in the first place).There _are_ tons of corner cases that you need to address since there are some super-hard problems in there (in particular, robust cardinality estimation of join outputs is a problem so hard that most of academia barely wants to touch it, despite its huge importance), but it doesn't need to be this bad.reply",
      "Can join cardinality can be tackled with cogroup and not expanding the rows until final write?reply",
      "This really depends on your data's geometry.Just look at when SAS programmers are advised to use a merge or a format.Even hash-join vs merge-join really depend on your data's cardinality (read: sizes), indices, etc.EDIT: Other comments also point out that there are non-general joins that are already NP-hard to optimize.  You really want all the educated guesses you can get.reply",
      "Well, it's to be expected that heuristics are needed, since the join ordering subproblem is already NP-hard -- in fact, a special case of it, restricted to left-deep trees and with selectivity a function of only the two immediate child nodes in the join, is already NP-hard, since this is amounts to the problem of finding a lowest-cost path in an edge-weighted graph that visits each vertex exactly once, which is basically the famous Traveling Salesperson Problem. (Vertices become tables, edge weights become selectivity scores; the only difficulty in the reduction is dealing with the fact that the TSP wants to include the cost of the edge \"back to the beginning\", while our problem doesn't -- but this can be dealt with by creating another copy of the vertices and a special start vertex, ask me for the details if you're interested.)reply",
      "> completely missing some elegant and profound beauty.Requires some dynamic SQL to construct, but the beauty is that you can use the SQL engine for this solution:select top 1 *from (selectt1.id,t2.id,...,tn.id,sum(t1.cost+t2.cost...+tn.cost) as total_costfrom join_options t1cross join join_options t2...cross join join_options tngroup by t1.id,t2.id,...,tn.id) t0order byt0.total_costreply"
    ],
    "link": "https://www.starrocks.io/blog/inside-starrocks-why-joins-are-faster-than-youd-expect",
    "first_paragraph": "StarRocks takes the opposite approach: keep data normalized and make joins fast enough to run on the fly. The challenge is the plan. In a distributed system, the join search space is huge, and a good plan can be orders of magnitude faster.This deep dive explains how StarRocks\u2019 cost-based optimizer makes that possible, in four parts: join fundamentals and optimization challenges, logical join optimizations, join reordering, and distributed join planning. Finally, we examine real-world case studies from\u00a0NAVER, Demandbase, and Shopee\u00a0to illustrate how efficient join execution delivers tangible business value.\u00a0The diagram above illustrates several common join types:Join performance optimization generally falls into two areas:This article focuses on the second aspect. To set the stage, we begin by examining the key challenges in join optimization.\u00a0Challenge 1: Multiple Join Implementation StrategiesAs shown above, different join algorithms perform very differently depending on the scenario."
  },
  {
    "title": "The WebRacket language is a subset of Racket that compiles to WebAssembly (github.com/soegaard)",
    "points": 95,
    "submitter": "mfru",
    "submit_time": "2026-01-17T21:23:58 1768685038",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=46662247",
    "comments": [
      "Hi All,It's still early days for the WebRacket project.Racket is a huge language, so be patient wrt features.To keep motivation high I decided to implement a subset that\ncan be used to built practical applications - and then extend\nthe supported features from there. Hopefully, this strategy\nwill also lead to some early adopters that can help me\nprioritize which features to add.Some features are simply \"more of the same\".\nIn this category falls more types of hash tables.\nSupporting bignums are also a matter of just doing it.Other features require more work. I have already done\nsome work on implementing modules in terms of linklets.\nWhen linklets/modules work, we can reuse the existing\nimplementation of regular expressions.Adding continuation marks and delimited continuations\nrequire adding a CPS-pass. This is certainly doable.\nPostponing it has been great though. Having a direct style\ncompiler means the generated code follows the structure\nin the input source code. And that makes debugging easier.\nNow that bugs have become rarer, it makes sense to look at CPS.Enjoy./Jens Axelreply",
      "I read the WASM spec and became somewhat of an expert in it for the purpose of eventually designing a low-level language specifically for wasm, to be the most efficient and lowest-level language you could possibly have for wasm, and then gradually add high level concepts into it to make it more convenient. Kind of like building C on top of asm, and then gradually evolving it into C++. That project never came about though due to lack of community interest and time on my part, but I like seeing languages that take a fresh look at how to integrate with wasm using novel techniques to aim for maximal efficiency.reply",
      "> to be the most efficient and lowest-level language you could possibly have for wasm, and then gradually add high level concepts into it to make it more convenient. Kind of like building C on top of asm, and then gradually evolving it into C++. That project never came about though due to lack of community interest and time on my part, but I like seeing languages that take a fresh look at how to integrate with wasm using novel techniques to aim for maximal efficiency.I wonder how much efficient would such wasm be compared to native itself theoretically?I really like libriscv as well, what are your thoughts on libriscv and now the recent project created by legendary fwsgonzo about looongarch.Although I would say that libriscv/loongarch are more focused on trying to start the fastest as much as possible instead of pure performance (for which if atleast for the purpose of sandboxing, fwsgonzo has also created tinykvm which is really quite close to native performance)reply",
      "Is there any shared lineage between this and Whalesong, a previous Racket->JS compiler?Of course both projects have the same maintainer if I recall, Jens Axel S\u00f8gaard is a rockstar :)reply",
      "Jens Axel S\u00f8gaard is cool and involved in many things.  We collaborated on SICP support.Original developer of Whalesong was Danny Yoo. https://www.hashcollision.org/whalesong/There was also this: https://docs.racket-lang.org/racketscript/Dave Herman worked on various JS-related libraries for Racket (or PLT Scheme) before he was involved with Rust.reply",
      "ah that's right! apologiesreply",
      "No, there is nothing in common with Whalesong.Whalesong used the built-in bytecode compiler and compiled the bytecode to JavaScript. Reusing the bytecode compiler is in principle a good idea - but each time the bytecodes are changed, Whalesong needs to be updated.And after the move to Chez Scheme as backend, the bytecode compiler is no longer a part of the main compilation path.reply",
      "I love this. Racket is the future we were promised.reply",
      "Speaking of prolific Racketeers... Noel!  Just an hour ago, on a walk, I was thinking, \"I should work through that one LLM book, and implement it in Racket.\"  (But have started job-hunting, so will probably be Python.)reply",
      "Which one LLM book?I've got so much other stuff I'd rather learn and code I'd rather write (C/wasm backend for my language), but I've also started job hunting and probably should understand how this latest fad works. Neural networks have long been on my todo list anyway.reply"
    ],
    "link": "https://github.com/soegaard/webracket",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        The WebRacket language is a subset of Racket that compiles to WebAssembly\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.The WebRacket language is a subset of Racket that compiles to WebAssembly (wasm).The long-term goal is to support full Racket.\nHowever, to quote Piet Hein, \u201cThings take time.\u201dThe subset supported by the WebRacket compiler is large enough,\nto enable programmers to build practical programs for the web.The generated WebAssembly can be run either in the terminal (via Node) or in the browser.\nThe browser is the main focus.\nWebAssembly is somewhat of a moving target.\nThe compiler only uses widely supported features of WebAssembly.\nExpect the generated code to work in Chrome, Firefox and Safari.A JavaScript FFI makes it possible to use standard "
  },
  {
    "title": "An explanation of cheating in Doom2 Deathmatch (1999) (doom2.net)",
    "points": 23,
    "submitter": "Lammy",
    "submit_time": "2026-01-17T10:29:05 1768645745",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.doom2.net/doom2/cheating.html",
    "first_paragraph": "An explanation of cheating in\nDoom2 Deathmatch\n\u00a0\n\u00a0\nFirst, a definition of cheating..First, a definition of cheating..\u00a0\u00a0 Any modification\nnot normally available in Doom2 that provides an advantage to one player\nover another with regard to:\n(1) humanly-executable player\nmovement,\n(2) visual abilities,\n(3) audio abilities,\n(4) player aim abilities, or\n(5) mechanisms to locate the\nopponent,\nand furthermore, these changes\nare put into effect in a game against another player who either\n(1) does not agree to, or\n(2) is unaware that the change\nhas been implemented\nis cheating.\nModifactions to sound and graphics\nto help a hearing or visually impaired player compensate for his or her\ndisability are not considered cheats.(1) humanly-executable player\nmovement,\n(2) visual abilities,\n(3) audio abilities,\n(4) player aim abilities, or\n(5) mechanisms to locate the\nopponent,\nand furthermore, these changes\nare put into effect in a game against another player who either\n(1) does not agree to, or\n(2) is u"
  },
  {
    "title": "Jerry (YC S17) Is Hiring (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2026-01-21T21:26:18 1769030778",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/jerry-inc/jobs/QaoK3rw-software-engineer-core-automation-marketplace",
    "first_paragraph": ""
  },
  {
    "title": "TrustTunnel: AdGuard VPN protocol goes open-source (adguard-vpn.com)",
    "points": 63,
    "submitter": "kumrayu",
    "submit_time": "2026-01-21T17:21:26 1769016086",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=46708601",
    "comments": [
      "Hi, I\u2019m one of the people working on this.One clarification that may not be obvious: open-sourcing this isn\u2019t primarily about signaling or auditability. If that were the goal, a standalone protocol spec or a minimal reference repo would have been enough.Instead, we\u2019re deliberately shipping full client and server implementations because the end goal is for this to become an independent, vendor-neutral project, not something tied to AdGuard.We want it to be usable by any VPN or proxy stack and, over time, to serve as a common baseline for stealthy transports \u2014 similar to the role xray/vless play today.Happy to answer questions or clarify design choices.reply",
      "Does your team have Chinese memebers?GFW has been able to filter SNI to block https traffic for a few years now.reply",
      "Thanks for all impressive work on AdGuard.Any particular reason to adopt Rust for this project instead of Go as many of your other products?Because I think since you have quite extensive Go codebase I would imagine you had to rewrite possibly a significant amount of code.reply",
      "Likewise interested in the authoritative answer, but: if I needed to write a decent chunk of code that had to run as close to wire/CPU limits as possible and run across popular mobile and desktop platforms I would 100% reach for Rust.Go has a lot of strengths, but embedding performance-critical code as a shared library in a mobile app isn't among them.reply",
      "What makes this worth using over something like vless? Work blocked my gatcha game so I've had to set up a xray/vless/xhttp/tls proxy and it works flawlessly. Gets through the corp firewall unscathed at full bandwidth and no appreciable increase in latency.reply",
      "Could you please drop names/links to the magic sauce if there's anything more than the names mentioned?I need to open ssh myself and for now I decided on tunnelling over http/3 terminated somewhere in aws/gcp/cf, but maybe your method is better.reply",
      "Link to the protocol specification: \nhttps://github.com/TrustTunnel/TrustTunnel/blob/master/PROTO...It's a thin HTTP/2 and HTTP/3 tunneling protocol for TCP, UDP, and ICMP traffic.It should be easy to write an independent implementation based on this specification provided you already have an HTTP/2 or HTTP/3 library. Pretty neat!reply",
      "I'm surprised that the browser extension to block ads has a proprietary vpn-like protocol. WTF?reply",
      "One interesting thing I\u2019ve noticed is that AdGuard means different things in different parts of the world. In some places, people know us primarily as an ad blocker, in others we\u2019re best known for our DNS service and in some regions AdGuard is associated almost exclusively with our VPN. The reality is that AdGuard makes several different products, not just one.reply",
      "I'm an American. I knew about the VPN service, but mostly associate your brand with the DNS services and lists you provide (thank you!).reply"
    ],
    "link": "https://adguard-vpn.com/en/blog/adguard-vpn-protocol-goes-open-source-meet-trusttunnel.html",
    "first_paragraph": "Today is a big day for us, and for everyone who cares about transparency, privacy, and having full control over their own traffic. We\u2019re finally open-sourcing the protocol that powers AdGuard VPN. And it now has a name: TrustTunnel.For a long time, we\u2019ve wanted to make the protocol public. Many of you asked for it, and we always said: yes, we will, it\u2019s only a matter of time. Well, the time has come.At its core, TrustTunnel is a modern, secure, mobile-optimized VPN protocol. It\u2019s the very same technology that has been running inside all AdGuard VPN apps: on mobile, desktop, and browser extensions.There are plenty of VPN protocols out there, so why create our own, some might ask. That is because we\u2019ve seen in practice the faults of popular VPN protocols, especially in countries with tight restrictions on internet access. Protocols like OpenVPN, WireGuard, and IPSec share common weaknesses: they are easy to detect and block at the network level, and attempts to conceal VPN traffic often "
  },
  {
    "title": "Three types of LLM workloads and how to serve them (modal.com)",
    "points": 41,
    "submitter": "charles_irl",
    "submit_time": "2026-01-21T16:15:06 1769012106",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://modal.com/llm-almanac/workloads",
    "first_paragraph": "Powered by Modal We hold this truth to be self-evident: not all workloads are created equal.But for large language models, this truth is far from universally\n    acknowledged. Most organizations building LLM applications get their AI from\n    an API, and these APIs hide the varied costs and engineering trade-offs of\n    distinct workloads behind deceptively flat per-token pricing.The truth, however, will out. The era of model API dominance is ending,\n    thanks to excellent work on open source models by DeepSeek and Alibaba Qwen\n    (eroding the benefits of proprietary model APIs like OpenAI's) and excellent\n    work on open source inference engines like vLLM and SGLang (eroding the\n    benefits of open model APIs powered by proprietary inference engines).Engineers who wish to take advantage of this technological change must\n    understand their workloads in greater detail in order to properly architect\n    and optimize their systems.In this document, we'll walk through the workloads a"
  },
  {
    "title": "Waiting for dawn in search: Search index, Google rulings and impact on Kagi (kagi.com)",
    "points": 220,
    "submitter": "josephwegner",
    "submit_time": "2026-01-21T17:28:03 1769016483",
    "num_comments": 143,
    "comments_url": "https://news.ycombinator.com/item?id=46708678",
    "comments": [
      "> Building a comparable one from scratch is like building a parallel national railroad..Not too be pedantic here but I do have a noob question or two here:1. One is building the index, which is a lot harder without a google offering its own API to boot. If other tech companies really wanted to break this monopoly, why can't they just do it \u2014 like they did with LLM training for base models with the infamous \"pile\" dataset \u2014 because the upshot of offering this index for public good would break not just google's own monopoly but also other monopolies like android, which will introduce a breath of fresh air into a myriad of UX(mobile devices, browsers, maps, security). So, why don't they just do this already?2. The other question is about \"control\", which the DoJ has provided guidance for but not yet enforced. IANAL, but why can't a state's attorney general enforce this?reply",
      "> 1. One is building the index, which is a lot harder without a google offering its own API to boot. If other tech companies really wanted to break this monopoly, why can't they just do it?FTA:> Context matters: Google built its index by crawling the open web before robots.txt was a widespread norm, often over publishers\u2019 objections. Today, publishers \u201cconsent\u201d to Google\u2019s crawling because the alternative - being invisible on a platform with 90% market share - is economically unacceptable. Google now enforces ToS and robots.txt against others from a position of monopoly power it accumulated without those constraints. The rules Google enforces today are not the rules it played by when building its dominance.reply",
      "robots.txt was being enforced in court before google even existed, let alone before google got so huge:> The robots.txt played a role in the 1999 legal case of eBay v. Bidder's Edge,[12] where eBay attempted to block a bot that did not comply with robots.txt, and in May 2000 a court ordered the company operating the bot to stop crawling eBay's servers using any automatic means, by legal injunction on the basis of trespassing.[13][14][12] Bidder's Edge appealed the ruling, but agreed in March 2001 to drop the appeal, pay an undisclosed amount to eBay, and stop accessing eBay's auction information.[15][16]https://en.wikipedia.org/wiki/Robots.txtreply",
      "Not only was eBay v. Bidder's Edge technically after Google existed, not before,  more critically the slippery-slope interpretation of California trespass to chattels law the District Court relied on in it was considered and rejected by the California Supreme Court in Intel v. Hamidi (2003), and similar logic applied to other states trespass to chattels laws have been rejected by other courts since; eBay v. Bidder's Edge was an early aberration in the application of the law, not something that established or reflected a lasting norm.reply",
      "The point is, robots.txt was definitely a thing that people expected to be respected before and during google's early existence. This Kagi claim seems to be at least partially false:> Google built its index by crawling the open web before robots.txt was a widespread norm, often over publishers\u2019 objections.reply",
      "Nitpick: Google incorporated in 1998, so, before the Bidder's Edge case.reply",
      "A classic case of climbing the wall, and pulling the ladder up afterward. Others try to build their own ladder, and Google uses their deep pockets and political influence to knock the ladder over before it reaches the top.reply",
      "Why does Google even need to know about your ladder? Build the bot, scale it up, save all the data, then release. You can now remove the ladder and obey robots.txt just like G. Just like G, once you have the data, you have the data.Why would you tell G that you are doing something? Why tell a competitor your plans at all? Just launch your product when the product is ready. I know that's anathema to SV startup logic, but in this case it's good businessreply",
      "Cost, presumably. From the article:> Microsoft spent roughly $100 billion over 20 years on Bing and still holds single-digit share. If Microsoft cannot close the gap, no startup can do it alone.reply",
      "Running the bot nowadays is hard, because a lot of sites will now block you - not just by asking nicely via robots.txt, but by checking your actual source IP. Once they see it's not Google, they send you a 403.reply"
    ],
    "link": "https://blog.kagi.com/waiting-dawn-search",
    "first_paragraph": "\n\n\n                    21 Jan, 2026\n                \n\nThis blog post is a follow-up to Dawn of a new era in Search, published last year. A lot has changed: the legal case has advanced, AI has become the central battleground, and the need for open index access has only grown sharper.As of late 2025, one company decides what nearly 9 out of 10 people see when they search the web: Google. On August 5, 2024, a U.S. court officially ruled that Google is a monopolist in general search services. This ruling is not about ads or browser defaults alone. It is about who controls the index that powers both search and AI - and whether anyone else is allowed to build on it.The stakes have grown sharper over the past year. LLMs hallucinate without grounding in real-world information; every agent that answers questions about the real world, depends on search. LLMs themselves are a blend of proprietary and open source. Cloud compute is competitive. But search is different - only one company controls a "
  },
  {
    "title": "Mystery of the Head Activator (asimov.press)",
    "points": 17,
    "submitter": "mailyk",
    "submit_time": "2026-01-18T18:18:34 1768760314",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=46670503",
    "comments": [
      "Reminds me a lot of Andrew Schally's \"growth hormone releasing hormone\" that turned out to be a contaminating peptide derived from pig blood.reply",
      "Long winded and hard to follow the conclusion. Chica was accused of fraud but didn\u2019t commit fraud?reply",
      "you read an article on the domain asimov.press about a very narrow, deep slice of biology, and expect it not to be long-winded? yeah, it\u2019d be nice if there were a nice bow to wrap around the whole thing, but said bow is not (yet) extant. how much shorter would you pike the story to be? it took me literally 5 minutes to read, and i easily could have read something else titillating were i bored at any point\u2026reply"
    ],
    "link": "https://www.asimov.press/p/head-activator",
    "first_paragraph": ""
  },
  {
    "title": "Setting Up a Cluster of Tiny PCs for Parallel Computing (kenkoonwong.com)",
    "points": 28,
    "submitter": "speckx",
    "submit_time": "2026-01-21T19:08:37 1769022517",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=46710042",
    "comments": [
      "I appreciate the author's work in doing this and writing it all up so nicely. However every time I see someone doing this, I cannot help but wonder why they are not just using SLURM + Nextflow. SLURM can easily cluster the separate computers as worker nodes, and Nextflow can orchestrate the submission of batch jobs to SLURM in a managed pipeline of tasks. The individual tasks to submit to SLURM would be the users's own R scripts (or any script they have). Combine this with Docker containers to execute on the nodes to manage dependencies needed for task execution. And possibly Ansible for the management of the nodes themselves to install the SLURM daemons and packages etc.. Taken together this creates a FAR more portable and system-agnostic and language-agnostic data analysis workflow that can seamlessly scale over as many nodes and data sets as you can shove into it. This is a LOT better than trying to write all this code in R itself that will do the communication and data passing between nodes directly. Its not clear to me that the author actually needs anything like that, and whats worse, I have seen other authors write exactly that in R and end up re-inventing the wheel of implementing parallel compute tasks (in R). Its really not that complicated. 1) write R script that takes a chunk of your data as input, processes it, writes output to some file, 2) use a workflow manager to pass in chunks up the data to discrete parallel task instances of your script / program and submit the tasks as jobs to 3) a hardware-agnostic job scheduler running on your local hardware and/or cloud resources. This is basically the backbone of HPC but it seems like a lot of people \"forget\" about the 'job scheduler' and 'workflow manager' parts and jump straight to glueing data-analysis code to hardware. Also important to note that most all robust workflow managers such as Nextflow also already include the parts such as \"report task completion\", \"collect task success / failure logs\", \"report task CPU / memory resource usages\", etc.. So that you, the end user, only need to write the parts that implement your data analysis.reply",
      "Something like proxmox [1] could make wiping everything and restarting a lot easier. There really isn't a huge penalty between bare metal and a VM now so you get the ability to create/deploy/monitor all from a reasonable interface. If the standard clustering stuff isn't enough their datacenter version looks like it is designed more for this. Never used it though. (no ties to proxmox here, just a guy that likes it!)[1] https://www.proxmox.com/en/products/proxmox-virtual-environm...reply",
      "So someone re-implemented the Beowulf cluster. I guess the other Slashdot memes are ready to come back.reply",
      "Hot Grits as a Servicereply",
      "Score: 1 (Insightful)reply",
      "Don't forget the ultimate /. achievement!Score: 5 (Troll)reply",
      "Score: 6 (LLM)reply",
      "More like -6 =Dreply",
      "I love that you created this account just for this joke. Now we just need Natalie Portman to join.reply",
      "Can it play Crysis/Doom?reply"
    ],
    "link": "https://www.kenkoonwong.com/blog/parallel-computing/",
    "first_paragraph": "By Ken Koon Wong in r R future parallel computing cluster multicore tmle superlearner January 16, 2026Enjoyed learning the process of setting up a cluster of tiny PCs for parallel computing. A note to myself on installing Ubuntu, passwordless SSH, automating package installation across nodes, distributing R simulations, and comparing CV5 vs CV10 performance. Fun project!Part of something I want to learn this year is getting a little more into parallel computing. How we can distribute simulation computations across different devices. Lately, we have more reasons to do this because quite a few of our simulations require long running computation and leaving my laptop running overnight or several days is just not a good use it. We have also tried cloud computing as well and without knowing how those distributed cores are, well, distributed, it\u2019s hard for me to conceptualize how these are done and what else we could optimize. Hence, what is a better way of doing it on our own! Sit tight, th"
  }
]