[
  {
    "title": "I've acquired a new superpower (danielwirtz.com)",
    "points": 994,
    "submitter": "wirtzdan",
    "submit_time": "2025-01-10T14:34:34 1736519674",
    "num_comments": 396,
    "comments_url": "https://news.ycombinator.com/item?id=42655870",
    "comments": [
      "At a local bar they had a game machine, and if you got a high score on any of the games, your tab for the evening was free.One of the games was a \"spot the differences\" between two pictures with an ever decreasing timer for each round. Using this trick I was able to easily surpass the high score, and garner a crowd watching me perform this mind numbing feat.Probably my peak fame right there.\n \nreply",
      ">Probably my peak fame right there.My son and I always make jokes about everyone's 5 minutes of fame.   Some random person on the jumbotron at a sporting event \"Yup, there's his moment, it's over now.\"At least yours got you something ;)\n \nreply",
      "One of my dad's sayings when somebody in a film delivered a line and then disappeared was \"6 months rehearsal for that.\"\n \nreply",
      "I envision happy families watching the end credits for Dad's name as Third Assistant Caterer on a big budget film.\n \nreply",
      "And getting pissed off because Netflix minimizes it into the corner to already try to push some other show on you.\n \nreply",
      "Best Boy Grip, the assistant to the Key Grip\n \nreply",
      "You may or may not be aware that Andy Warhol famously quipped that, \"in the future, everyone will be famous for 15 minutes,\" back in the late 1960s. As media has gotten to be ever more ubiquitous and the cost of entry lower, he was clearly onto something decades before the internet!\n \nreply",
      "And then there\u2019s Banksy\u2019s \u201cin the future, everyone will be anonymous for 15 minutes\u201c. For pretty much the same reasons you stated above, I assume.\n \nreply",
      "To update this excellent quote to 2025, change minutes to seconds and you just described TikTok.\n \nreply",
      "Yeah, I was thinking that the while modern social media has made the \"cost of entry lower,\" and everyone can theoretically reach more people than ever, it's hard to even describe most of it as \"fame\" anymore. I mean, does content even \"go viral\" anymore, with users subdivided into the tiniest niche communities or audiences? Even if things get wider traction for a while, there's so much competition with so much other content that everything seems to get quickly drowned out and then can't even be found again later through search.\n \nreply"
    ],
    "link": "https://danielwirtz.com/blog/spot-the-difference-superpower",
    "first_paragraph": ""
  },
  {
    "title": "Portals and Quake (30fps.net)",
    "points": 39,
    "submitter": "ibobev",
    "submit_time": "2025-01-10T22:48:13 1736549293",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=42661185",
    "comments": [
      "Interesting. I recall Prey being the first game to hype portals for rendering, though I think theirs may have been for drawing not just culling? Does anyone know how it worked?Dark Forces apparently also used portals for culling, IIRC.\n \nreply",
      "I think Build used some form of portals as well... primarily for water/underwater transitions, but there was at least one level in Duke3d that used it for some non-euclidian geometry.\n \nreply"
    ],
    "link": "https://30fps.net/pages/pvs-portals-and-quake/",
    "first_paragraph": ""
  },
  {
    "title": "Cuttle \u2013 a MTG like game using a standard 52 card deck (pagat.com)",
    "points": 186,
    "submitter": "7thaccount",
    "submit_time": "2025-01-10T18:43:26 1736534606",
    "num_comments": 56,
    "comments_url": "https://news.ycombinator.com/item?id=42658614",
    "comments": [
      "I'm a huge fan of card games that can be played with a regular deck of cards, and I play Cuttle with my kids somewhat regularly. It's a fast-paced game, but you do have to get over the initial learning curve of the effects. I find just printing out a piece of paper with a quick reference on it helps.As others have alluded to in the thread, teaching people the rules is a barrier. As I looked around for a professionally printed game, I found a game that was very much like Magic the Gathering, but also just a single box of cards called Mindbug.https://mindbug.me/Turns out it was designed by some folks that brought Richard Garfield in near the end of its design and he ended up having some say in the final product. I've played it a couple of times and each game is only played with a random subset of the cards, so the combinatorics create a lot of replay value.\n \nreply",
      "+1 for mindbug and its extensions. The games are quick and nervous, players only have 3 pv.The card effects are interesting, the illustrations illarious and the interractions a lot of fun.The twist of the game, the mindbug itself, is a blast of bluffing and gambling that garanties some smiles.Limited to 2 players but a great small box to have on all occasion of you are into that kind of stuff.The rules are simple so ir's easy to get in.\n \nreply",
      "For anyone who wants a more complex game proximate to this design, there's a small production called Reinforcements (https://reinforcementscg.myshopify.com/). Non-collectible, single box.You play a hand of up to 5 cards each turn: Adding cards to (concealed) stacked ranks of defending troops, attacking an opponent's ranks, using a card's ability. The suits have different defensive properties when arranged in a rank, and combine in interesting ways; there are also \"ultimate\" powers players can grab from the center by forming their ranks with particular arrangements of suits, which act as turtle-busters.Highly recommended, quite fun, probably plays best 1v1. Definitely a lot of small rules to absorb, so it's a more complex beast. But nowhere close to the complexity of Magic.\n \nreply",
      "If co-op games are more your speed, check out Regicide.\n \nreply",
      "+1\nRegicide is a great game, difficult to master, but when playing with the same people often you learn how to play and work off each other in subtle ways.\n \nreply",
      "Thank you!! This looks awesome. Cannot wait to try.https://boardgamegeek.com/boardgame/307002/regicide\n \nreply",
      "I'd never heard of reinforcements, but it looks awesome. Definitely will purchase.\n \nreply",
      "I'd never heard of astroturfing, but it looks intruguing. Definitely suspicious.\n \nreply",
      "> Please don't post insinuations about astroturfing...If you're worried about abuse, email hn@ycombinator.com and we'll look at the data.\n \nreply",
      "They have 300 times and 1000 times the reputation you have. Maybe dail down the paranoia.\n \nreply"
    ],
    "link": "https://www.pagat.com/combat/cuttle.html",
    "first_paragraph": "This 2-player game which appeared in North America in the 1970's is slightly reminiscent of later commercially successful combat games such as Magic the Gathering though the similarity is probably a coincidence.Class: Combat GamesBrowse classification networkRegion: USA, Britain\u2b50\u2b50\u2b50\u2b50Difficulty\u2764\ufe0f\u2764\ufe0f\u2764\ufe0f\u2764\ufe0f\u2764\ufe0fPopularity\ud83d\udd25\ufe0fTrendThe exact origin of this unusual two-player game is unknown. Dating from the 1970's at the latest, it is the earliest example I have found of a combat card game. The aim is to be the first build a layout worth at least 21 points. Cards can be used for their point value, or to attack your opponent's layout by destroying or capturing cards.For some years a FAQ by Richard Sipie, first published in 2000, was the only generally available documentation of the game. I am grateful to Michael Pearson for his help in preparing the new description on this page and to Greg Pallis, an enthusiastic player and winner of the Cuttle tournament in the 2009 Mind Sports Olympiad, for answeri"
  },
  {
    "title": "Kimchi Refrigerator (wikipedia.org)",
    "points": 65,
    "submitter": "tosh",
    "submit_time": "2025-01-07T20:08:00 1736280480",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=42626809",
    "comments": [
      "In 2000 I moved into a shared house in NJ. A few weeks into living there we noticed a growing awful smell coming from the basement. We found that the previous residents had left their kimchi fridge full of kimchi \u2026 and had unplugged it before they left.\n \nreply",
      "And odor. Kimchee taste really does work itself into things in the fridge.I recall a roommate from decades ago (son of Korean immigrants) mention that his parents always kept a small side fridge / mini-fridge for the kimchee, so everything else in the standard fridge wouldn't adopt the flavor -- I think a comment of his was \"kimchee flavored butter is not pleasant.\"\n \nreply",
      ">  I think a comment of his was \"kimchee flavored butter is not pleasant.\"Hah, I saw video recipes of kimchi and gochujang-garlic flavored butter last year. Tho probably both were eye-catching online culinary fads rather than practical products.Woman whose channel I follow uses both plastic and ceramic containers for kimchi - both include a small valve in the lid to allow exchange of air. And of course cabbage lands in that special fridge; she said she uses it also for storing vegetables and rice\n \nreply",
      "My one weird kimchi fusion experiment that's worked out beyond all reason was making a classic cheese-crusted German pasta casserole with kimchi in the mix ... it's so addicting it probably needs to be regulated.\n \nreply",
      "I started using gochujang in soups to boost the flavor - tomato one gets a serious kick now\n \nreply",
      "try og gochujang jjigae\n \nreply",
      "Kimchi is so important in Korean culture they sent it with their astronauts\"If a Korean goes to space, kimchi must go there, too\"https://www.nytimes.com/2008/02/22/world/asia/22iht-kimchi.1...\n \nreply",
      "Is there a good one that can work through -20\u00b0F winters? Would love to put one in my garage. (Ideally with custom panelling compatability.)\n \nreply",
      "Haven't seen these in a while, but the H Marts in Washington state would stock a couple of the newest LG ones.\n \nreply",
      "A chest freezer with a temp controller would be quite similar.  You might need a fan and simple heat source as well.  For about $200 I was able to buy and setup one for beer fermentation.\n \nreply"
    ],
    "link": "https://en.wikipedia.org/wiki/Kimchi_refrigerator",
    "first_paragraph": "Soups & stews\nBanchan\nTteok\nA kimchi refrigerator is a refrigerator designed specifically to meet the storage requirements of kimchi and facilitate different fermentation processes. The kimchi refrigerator aims to be colder, with more consistent temperature, more humidity, and less moving air than a conventional refrigerator, providing the ideal environment for fermentation of kimchi. Some models may include features such as a UV Sterilizer.[1]\nIn a consumer survey aimed at South Korean homemakers conducted by a top-ranking Korean media agency in 2004, the kimchi refrigerator was ranked first for most wanted household appliance.[2] [better\u00a0source\u00a0needed]\nThe start of the Kimchi refrigerator dates back to 1984. At that time, LG predecessor, GoldStar (\uae08\uc131\uc0ac), first used the word 'Kimchi refrigerator'  (\uae40\uce58 \ub0c9\uc7a5\uace0). The model name of the first kimchi refrigerator was 'GR-063', and according to the advertisement], the inside was made of stainless steel and the internal temperature could be set, "
  },
  {
    "title": "Datadog acquires Quickwit (quickwit.io)",
    "points": 127,
    "submitter": "lrx",
    "submit_time": "2025-01-09T17:49:21 1736444961",
    "num_comments": 49,
    "comments_url": "https://news.ycombinator.com/item?id=42648043",
    "comments": [
      "Co-founder of Quickwit here.  Seeing our acquisition by Datadog on the HN front page feels like a truly full-circle moment.HN has been interwoven with Quickwit's journey from the very beginning. Looking back, it's striking to see how our progress is literally chronicled in our HN front-page posts:- Searching the web for under $1000/month [0]- A Rust optimization story [1]- Decentralized cluster membership in Rust [2]- Filtering a vector with SIMD instructions (AVX-2 and AVX-512) [3]- Efficient indexing with Quickwit Rust actor framework [4]- A compressed indexable bitset [5]- Show HN: Quickwit \u2013 OSS Alternative to Elasticsearch, Splunk, Datadog [6]- Quickwit 0.8: Indexing and Search at Petabyte Scale [7]- Tantivy \u2013 full-text search engine library inspired by Apache Lucene [8]- Binance built a 100PB log service with Quickwit [9]- Datadog acquires Quickwit [10]Each of these front-page appearances was a milestone for us. We put our hearts into writing those engineering articles, hoping to contribute something valuable to our community.I'm convinced HN played a key role in Quickwit's success by providing visibility, positive feedback, critical comments, and leads that contacted us directly after a front-page post. This community's authenticity and passion for technology are unparalleled. And we're incredibly grateful for this.Thank you all :)[0] https://news.ycombinator.com/item?id=27074481[1] https://news.ycombinator.com/item?id=28955461[2] https://news.ycombinator.com/item?id=31190586[3] https://news.ycombinator.com/item?id=32674040[4] https://news.ycombinator.com/item?id=35785421[5] https://news.ycombinator.com/item?id=36519467[6] https://news.ycombinator.com/item?id=38902042[7] https://news.ycombinator.com/item?id=39756367[8] https://news.ycombinator.com/item?id=40492834[9] https://news.ycombinator.com/item?id=40935701[10] https://news.ycombinator.com/item?id=42648043\n \nreply",
      "I think you forgot to add the linksAnyway tantivy is great! I love pg_search https://www.paradedb.com/blog/introducing_search (which appears to be built by another company, but on top of tantivy, which is a great feature of open source)Now, I am worried about development being stalled after this acquisition. How does further developing tantivy in the open helps Datadog's bottom line?\n \nreply",
      "I love quickwit, unfortunately datadog has a history of murdering open source (e.g. vector.io halting development and never fixing gross bugs)\n \nreply",
      "Congratulations! The fact you and your team managed to built Tantivy is a huge contribution to open source.As someone who never managed to built a fond relationship with Apache Lucene based products (Solf, Elastic). I was extremely happy to see Tantivy in open source.BM25 scoring, proper asian language support, speed, memory foot prints, etc - amazing job! Thank you so much!https://github.com/quickwit-oss/tantivyIMHO Datadog made a smart move!If Tantivy itself just stays permanently under Apache2 licence and find a sustainable path to co exist with the rest of open source community - it's all good guys. You are more than deserve a commercial success.\n \nreply",
      "Congrats!!\n \nreply",
      "I hate Datadog. We use their name as an epithet at our company for how not to sell/market. Their selling tactics circa 2015-2018 completely burned us out. Endless calls and emails. The icing on the cake was an AWS reInvent presentation on Lambda right when lambda was first announced. We were pumped to get in on lambda early. Got the whole crew to attend the talk. Turned out to be a rudimentary copy of a Barr \"lambda up and running\" blog wrapped in a stand up comedy routine hawked by a Datadog employee who made sure to tell us he was a Datadog employee. Get us all drunk and happy and think Datadog is cool.Genuine question: has the company changed enough in the interim to deserve a second look?\n \nreply",
      "What does any of that have to do with the actual product?\n \nreply",
      "The product itself is very good, but the sales process is truly awful. Random calls with non-technical reps unable to answer basic questions like, \"now that you've added this to my GCP account for 2 weeks, how much is this going to cost?\" They'd say they're not sure but they have a startup deal with $xxxx minimum commit for 12 months gets you two months of extra trial, cancel anytime no questions asked. It's not just bad, it's comically bad.\n \nreply",
      "Different person with similar stance: Those specific examples? Whatever.We did get absolutely burnt by other manifestations of the DataDog approach: The billing model was (is?) very much not good, transparent or predictable and staying on top of costs was close to a nightmare. The way surprise costs and contract changes (triggered by them) was handled did not feel honest.The product itself is great but from my perspective it's absolutely not worth having to deal with their business side of things and the risks, costs (money, time, attention) and stress associated.If I were a Quickwit customer I'd start looking for alternatives.\n \nreply",
      "It made me extremely distrustful of any and all interactions I would have with an employee. Is every email I send to my rep going to turn into an upsell? Are they being straight with me in answering my question?\n \nreply"
    ],
    "link": "https://quickwit.io/blog/quickwit-joins-datadog",
    "first_paragraph": "We are thrilled to announce that Quickwit is joining Datadog! We will be heads down building a new product with Datadog, so to ensure our open-source community can continue on, we will soon release a new version of Quickwit under the Apache License 2.0. Stay tuned!But first, I want to share our story\u2014a journey that spans four years, three continents, and countless plates of gyoza.It all began with a decade-long friendship between three engineers\u2014Paul, Adrien, and me\u2014who first met in Paris in 2010. Back then, we'd sit in our favorite gyoza restaurant\u2014the gyoza bar, it still exists. We would typically go there after attending an event called Start in Paris, where rookie founders came to pitch their startup ideas. Think Dragon\u2019s Den with a mustache. There, we would dream about building something revolutionary together. But dreams remained dreams until 2020 when Paul called us to talk about his pet project, tantivy. The pet had sharp teeth - it was already a popular alternative to Lucene.W"
  },
  {
    "title": "Phi-4 Bug Fixes (unsloth.ai)",
    "points": 56,
    "submitter": "danielhanchen",
    "submit_time": "2025-01-10T21:17:21 1736543841",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=42660335",
    "comments": [
      "Hey HN family! I found a few bugs for Phi-4 - Microsoft's latest MIT licensed LLM to be on par with GPT-4o mini1. End of sentence should be <|im_end|> not <|endoftext|>2. Chat template should not auto add an assistant prompt3. Padding token should not be EOS but <|dummy_87|>I also converted Phi-4 to Llama-arch. I uploaded GGUFs, 4bit quants, dynamic quants and all fixes to https://huggingface.co/unslothI also made a Colab notebook to finetune Phi-4 on a free GPU: https://colab.research.google.com/github/unslothai/notebooks...\n \nreply",
      "> We converted Phi-4 to Llama\u2019s architecture for better accuracy and easier use.What does this mean? When I think about \"model architecture\", I think about the number of weights in each layer, the organization of the layers, etc. And AFAIK, it's untenable to \"port\" a model from one to the other without effectively retraining it. So what does it actually mean to \"convert to Llama's architecture\"?\n \nreply",
      "Oh Phi-4's architecture is inspired from Llama itself, except they merged the attention matrices into 1 large matrix for better FLOP utilization, and the gate/up matrices in the MLP.Phi-3 use to use sliding window attention, but they got rid of that in Phi-4.So, you can \"Mistral-fy\" Phi-3 and convert it to Mistral arch (by unmerging the merges), and now you can \"Llama-fy\" Phi-4 to Llama arch.The reason why accuracy increases in finetuning is because during LoRA finetuning, you learn only 1 A matrix for merged QKV, whilst unmerging it creates 3 A matrices - this allows the model to have more freedom to learn new features.\n \nreply",
      "Would guess GGUF so you can run on llama.cpp, LM Studio, etc..., but OP can hopefully clarity further for you.\n \nreply",
      "Yep converting to Llama arch definitely makes accessibility much better - also many fast LLM serving libraries normally support Llama, so it makes it easier to port and use!",
      "Huh! That may explain why I kept on getting visible <|im_end|> output when I tried running a Phi-4 GGUF file using llama.cpp.\n \nreply",
      "Oh yes exactly! I trimmed it out now :)The better chat template should be:{% for message in messages %}{% if (message['role'] == 'system') %}{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'user') %}{{'<|im_start|>user<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'assistant') %}{{'<|im_start|>assistant<|im_sep|>' + message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant<|im_sep|>' }}{% endif %}\n \nreply",
      "Wasn't Phi-3 also bugged/is still bugged? Seems like Microsoft just doesn't care.>to be on par with GPT-4o miniPhi is known to overfit benchmarks. It's way, way worse then that.\n \nreply",
      "Phi-3 should be fixed as well - but yes there were bugs as well! https://x.com/danielhanchen/status/1782853167572832650Phi-3's sliding window should be 2048 and not 2047, and they also had chat template issues - I uploaded correct versions to https://huggingface.co/unsloth/Phi-3.5-mini-instruct\n \nreply",
      "Anecdotally, I've been experimenting with Phi-4 the past hour or so (so, yeah, not very comprehensive) and it's certainly a strong model. Definitely better than the previous Phi models.\n \nreply"
    ],
    "link": "https://unsloth.ai/blog/phi4",
    "first_paragraph": ""
  },
  {
    "title": "Meta's memo to employees rolling back DEI programs (axios.com)",
    "points": 665,
    "submitter": "bsilvereagle",
    "submit_time": "2025-01-10T17:48:04 1736531284",
    "num_comments": 873,
    "comments_url": "https://news.ycombinator.com/item?id=42657901",
    "comments": [
      "I wrote about my experience working as a software developer and being black in the industry and I was lucky to have it published on BBC [1].What immediately followed, every large company reached out to have me work as a consultant for their diversity program. I found it fascinating that they had a team of DEI experts in place already. Like what makes one an expert?In addition to my job, I spent nights developing programs trying to help these companies. Some folks right here on HN shared their successful experiences and I presented it to several companies. I was met with resistance every step of the way.Over the course of a year and hundreds of candidates I presented, I've managed to place just one developer in a company.However, most these companies were happy to change their social media profile to a solid black image or black lives matters. They sent memos, they organized lunches, even sold merch and donated. But hiring, that was too much to ask. A lot of graduates told me they never even got to do a technical interview.Those DEI programs like to produce a show. Something visible that gives the impression that important work is being done. Like Microsoft reading who owned the land where the campus was built [2] in the beginning of every program. It eerily reminds me of \"the loyalty oath crusade\" in Catch-22.[1]: https://news.ycombinator.com/item?id=23669188[2]: https://youtu.be/87JXB0t6de4?si=wtnQtBOE-fs4V7gR\n \nreply",
      "Yes. What too few people realized was that the rollout of DEI was driven by what was trending at the time, designed to win political points with the groups that were politically ascendant. These programs were never a victory for the principles or the people, they were marketing.So it should come as no shock whatsoever that now that another political group is politically ascendant the marketing that is valuable has changed, so there go the marketing programs that were designed for the old power structure.Change that occurs through fear of your power can only last as long as your power. Lasting change is only possible by actually changing hearts and minds. Progressives have forgotten in the last 10-15 years that the progress which we've won took generations not because our predecessors were weak and slow but because it inherently takes generations to effect lasting change. It's a slow, painful process, and if you think you accomplished it in a decade you're almost certainly wrong.\n \nreply",
      "I agree with most of your points. Though with respect lasting change, where is your impression coming from that the gains are in the last 10 to 15 years? Or even that is a widespread belief?According to reporting at the guardian [1], FBs DEI program increased black and brown employees from 8% to 12%. Seems abysmal.My perspective, US society is still fighting for gains that _started_ 160 years ago. Still painstakingly slow. We take for granted perhaps the first black president is _recent_, the first time having two black senators is now, school integration is about 40 years old in some places - not even one lifetime.i don't think it's an accurate characterization that huge strides were made in just the last decade, or that we were even starting at a \"good\" place.I fundamentally agree on how slow the progress has been. I don't know if it needs to be that slow. I disagree that there is a wide held belief that everything was done in the last decade. Notably because of how little has been done. It's not like we're in that good of a place, never really were.[1] https://www.theguardian.com/us-news/2025/jan/10/meta-ending-...\n \nreply",
      "> My perspective, US society is still fighting for gains that _started_ 160 years ago. Still painstakingly slow.I feel this comment won\u2019t win me many friends, but since no one has mentioned it: one of the striking features of the DEI/social justice movement was its rejection of MLK-style racial equality ideals. An entirely new language was invented to describe the new philosophy. And in some circles, if you appealed to MLK\u2019s of vision equality you were ostracized.\n \nreply",
      "Equity instead of equality. Sounded awful close to promoting equal outcomes over equal opportunity. I dont trust people who want to engineer society from the top down to be the result they think is fair and just.\n \nreply",
      "This is a common misinterpretation.  It's not about equality of outcomes.It's about recognizing that some people have potential that they wouldn't be able to realize due to longstanding historical inequalities that are highly correlated with race and working to account for historial injustices that still impact people today.It's not anyone's fault that these issues exist today, but it's our responsibility as a civilized society to at least ensure we don't actively perpetuate them.\n \nreply",
      "> This is a common misinterpretation. It's not about equality of outcomes.Could you inform Kamala Harris? She just ran a campaign which was largely predicated on the need for \"equity\", the goal of which she repeatedly described as meaning we need to take proactive measures to ensure that \"we all wind up at the same place\".https://x.com/KamalaHarris/status/1322963321994289154\n \nreply",
      "> This is a common misinterpretation. It's not about equality of outcomes.That's because no one really defined what \"equity\" means in the first place. In absence of a clear definition, people just fill in whatever they want.\n \nreply",
      "Evaluating potential is difficult.  Measure something that isn't in a thin history summary.  Measure stuff you have an opportunity to see without human bias or algorithms that are easily gamed?  Measure, what is a desirable outcome?As someone who's been looking for a job that will take a chance on how I can grow to full their needs rather than already being a perfect match; I would really love someplace that had a 'career pivot' entry track and not just a recent / about to grad track.Maybe something like a 1 week, then 1 month (3 more weeks), then 3 months (total), then every 3rd month evaluation track for working the job in a 'temp to hire' sense with a 1 year cutoff so they can't just keep hiring 'perma temps' like in the past.I understand there's risks, and I understand it's very hard for both sides.  However there's a ton of untapped potential and corporations are the ones who aren't offering a way of tapping it.\n \nreply",
      "> Evaluating potential is difficult. Measure something that isn't in a thin history summary.Ivy League schools in the US have been doing this for rather a long time now. Whether they are any good at it is subject to significant debate, but they certainly like to pretend that they can evaluate it. Their evaluations tend to show a strong belief in the hereditary properties of \"potential\", which is not well established in actual objective research.\n \nreply"
    ],
    "link": "https://www.axios.com/2025/01/10/meta-dei-memo-employees-programs",
    "first_paragraph": ""
  },
  {
    "title": "Finland's zero homeless strategy (2021) (oecdecoscope.blog)",
    "points": 107,
    "submitter": "zdw",
    "submit_time": "2025-01-10T15:53:07 1736524387",
    "num_comments": 249,
    "comments_url": "https://news.ycombinator.com/item?id=42656711",
    "comments": [
      "They also do a lot of compulsory psychiatric detention: https://www.cambridge.org/core/journals/psychiatric-bulletin...> Finnish mental health legislation takes a medical approach to compulsory measures, emphasising the need for treatment of psychiatric patients over civil liberties concerns... Finland has the highest rates of detention per 100 000 inhabitants, about 214 compared with 93 in the UK and 11 in Italy.> If at the end of the 3-month period it is considered likely that detention criteria are still fulfilled, new recommendations MII and MIII are filed and the renewed detention is then valid for 6 months. However, this second period of detention has to be immediately confirmed by a local administrative court.edit: I should mention that I've seen fairly convincing cross-sectional evidence that homelessness is more related to the housing market than mental illness: https://www.ucpress.edu/books/homelessness-is-a-housing-prob... ,  https://www.nahro.org/wp-content/uploads/2022/08/NAHRO-Summi...\n \nreply",
      "> I should mention that I've seen fairly convincing cross-sectional evidence that homelessness is more related to the housing market than mental illnessThis is absolutely the right diagnosis. For instance, SROs used to be very affordable.[1] Placing someone into housing was well within the means of local governments and non-profits.In Coppola's 1974 movie The Conversation, a large portion of the titular dialogue is about a homeless person Williams' character spots while walking around a crowded Union Square. That's how much homelessness stood out back then.[1] https://ccsroc.net/s-r-o-hotels-in-san-francisco/\n \nreply",
      "\"Finland has the highest rates of detention per 100 000 inhabitants, about 214\"If by detention you mean incarceration, that is still shy of half of the US rate https://en.wikipedia.org/wiki/United_States_incarceration_ra...\n \nreply",
      "They're referring to psychiatric civil commitment\n \nreply",
      "No, these aren't criminals. Finland doesn't think mad people have somehow committed a crime, it just won't let them leave. They're detained against their will until the doctors decide they've fixed the problem.Compare the decision not to let your five year old have pudding because she hit her brother and refused to apologise, versus the decision not to let her jump into the tiger pit because she might die. These are both restraints on this kids' freedom, but they come from very different places.\n \nreply",
      "Incarceration and detention are totally different things.  Incarceration is generally for things that have already happened.  Detention is for things that might happen in the future.  A convicted criminal is incarcerated.  A dangerous patient is detained to prevent them hurting themselves or others going forwards.\n \nreply",
      "\"Life, liberty, and the pursuit of happiness\" does not automatically mean \"good, moral, and upstanding lifestyle.\"To the extent that people have a natural right to exist and society does not I think it should be contingent on administrators to prove the standard they're applying is actually reasonable and non discriminatory.\n \nreply",
      "The standard ought to be they have or imminently are going to harm others.  Like actually harm a real victim, criminally by violence or taking property.  If they want to live in a gutter worshipping lizard king, well, not everyone has the same idea of the pursuit of happiness.\n \nreply",
      "What about babies and children?  What about enfeebled old people?  Clearly some people can't take care of themselves.  Presumably you don't think babies and alzheimers patients should be left to roam free.  Why are severely mentally ill people any different?\n \nreply",
      "Without digging too deep into the nature of the statistics they use, I'm a little skeptical of this.The transition to using the word \"homeless\" has resulted in transforming something we can't easily measure -- \"drug addicted or mentally ill people being a public menace\" -- into something that we can measure -- \"people without a good living arrangement\".Sure, the latter is important in a lot of ways too. And there housing is a tolerable solution.But the former is the actual problem that we care about. It's nearly impossible to measure. It's nearly impossible to fix. The horrors of involuntary commitment vs. the horrors of not having involuntary commitment vs. the horrors of using the criminal justice system vs. the horrors of not using the criminal justice system.The fact is that we have no real model for treatment of severely mentally ill people. We have a number of effective drugs, but they rapidly become ineffective if not taken. Our ability to treat or \"cure\" people in these conditions is essentially non-existent.The question I would ask of Finland before considering this data or analysis to be interesting is what is their state of involuntary indefinite commitment.\n \nreply"
    ],
    "link": "https://oecdecoscope.blog/2021/12/13/finlands-zero-homeless-strategy-lessons-from-a-success-story/",
    "first_paragraph": "ECOSCOPEECOSCOPEBy Laurence Boone, Boris\u00a0Courn\u00e8de, OECD Economics Department;\u00a0and Marissa\u00a0Plouin,\u00a0OECD Directorate for Employment, Labour and Social Affairs\u00a0Following a period when homelessness rose in many countries, the onset of the COVID-19 pandemic prompted governments across the OECD\u00a0area\u00a0to provide unprecedented public support \u2013 including to the homeless. In the\u00a0United Kingdom, for instance, people who had been living on the streets or in shelters were housed in individual accommodations in a matter of days. And in cities and towns across the OECD, public authorities worked closely with service providers and other partners to provide support to the homeless that had previously been considered impossible.\u00a0\u00a0How can\u00a0countries\u00a0build on this momentum and\u00a0ensure more durable outcomes? The experience of Finland over the past several decades \u2013 during which the country has nearly eradicated homelessness \u2013 provides a glimpse of what can be possible with a sustained national strategy and en"
  },
  {
    "title": "Mercury's shadowy North Pole revealed by M-CAM 1 (esa.int)",
    "points": 46,
    "submitter": "divbzero",
    "submit_time": "2025-01-10T18:34:55 1736534095",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=42658512",
    "comments": [
      "With labels describing the surface features: https://www.esa.int/ESA_Multimedia/Images/2025/01/Mercury_s_...\n \nreply",
      "Absolutely love the ESA Like button in the article. Conveys the feeling it's designed by a flight instruments engineer rather than a social media frontend person (make sure to click it twice).\n \nreply",
      "You tricked me\n \nreply",
      "If you refresh the browser the button is unpressed, but when you press it again it gives a stern message about liking twice.\n \nreply",
      ">M-CAM 1 took this long-exposure photograph of Mercury's north poleI'm curious how this works. The dynamic range between the sunlit parts and the dark portions must be huge at that distance from the Sun. Anyone have the technical details on the camera or post processing they use to achieve this? Is it really a long exposure or is it a series of photos at different exposures stitched together?Edit: details at the bottom seem to imply a single photo, but that \"long exposure\" really isn't that long>This image of Mercury's surface was taken by M-CAM 1 [...] using an integration time of 40 milliseconds.\n \nreply",
      "If you're thinking that long exposure automatically means something longer than 1 second, it might not mean long exposure to you.I'd guess this is a fixed aperture system where the main way to control the exposure is with shutter speed. But for images taken in bright sunlight, you can use shutter speeds 1/250, 1/1000, 1/2000, or even higher type numbers. In those terms, 40 milliseconds is 10 times slower/longer than 1/250.So for the M-CAM 1 system, 40 milliseconds could be an extremely long exposure\n \nreply",
      "Integration implies multiple pictures taken and stacked. This is how we do deep sky astrophotography on our back yards.\n \nreply",
      "Integration over time and integration over bit fields are both ways we do deep sky astrophotography. \nIntegration over time is used to collect more photons without saturating sensor wells and integrating bit fields (stacking) is used to increase signal to noise\n \nreply",
      "I think this is their terminology for exposure time. The sensor is integrating charge from incident photons during this period. Of course the image could be stacked also!Edit: the sensor is integrating CURRENT. Charge is the integral!\n \nreply",
      "Wondering same. If my math is right, this was a 1/25 sec. exposure. (40/1000)\n \nreply"
    ],
    "link": "https://www.esa.int/ESA_Multimedia/Images/2025/01/Mercury_s_shadowy_north_pole_revealed_by_M-CAM_1",
    "first_paragraph": "Thank you for likingYou have already liked this page, you can only like it once!This is one of a\u00a0series of images\u00a0taken by the ESA/JAXA BepiColombo mission on 8 January 2025 as the spacecraft sped by for its sixth and final gravity assist manoeuvre at the planet. Flying over Mercury's north pole gave the spacecraft's\u00a0monitoring camera 1\u00a0(M-CAM 1) a unique opportunity to peer down into the shadowy polar craters.M-CAM 1 took this long-exposure photograph of Mercury's north pole at 07:07 CET, when the spacecraft was about 787 km from the planet\u2019s surface. The spacecraft\u2019s closest approach of 295 km took place on the planet's night side at 06:59 CET.In this view, Mercury\u2019s terminator, the boundary between day and night, divides the planet in two. Along the terminator, just to the left of the solar array, the sunlit rims of craters Prokofiev, Kandinsky, Tolkien and Gordimer can be seen, including some of their central peaks.Because Mercury\u2019s spin axis is almost exactly perpendicular to the "
  },
  {
    "title": "Getting silly with C, part (void*)2 (lcamtuf.substack.com)",
    "points": 127,
    "submitter": "justmarc",
    "submit_time": "2025-01-10T17:16:12 1736529372",
    "num_comments": 64,
    "comments_url": "https://news.ycombinator.com/item?id=42657591",
    "comments": [
      "I'm going to speculate a bit on why these silly things are in C.C was developed on a PDP-11 that had 64Kb of memory. That's not much of any at all. Therefore, the compiler must be extremely tightly coded.The fundamental rules of the C language are pretty simple. But articles like these expose consequences of such simple rules. Fixing them requires adding more code. Adding more code means less room for the code being compiled.Therefore, if the intended use of the language works, the pragmatic approach would be to simply not worry about the quirky consequences.A more interesting question would be \"why do these characteristics persist in modern C compilers?\"The stock answer is \"backwards compatibility\", \"Obfuscated C Code contests\" and \"gotcha job interview questions\". My argument would be that there is no reason for the persistence of such \"junk DNA\" and it should be deprecated and removed.I've done my part. D doesn't support that stuff, even though the basic use of the language is easily confused with C.For example:    #include <stdio.h>\n    void main()\n    {\n        int i;\n        for (i = 0; i < 10; ++i);\n            printf(\"%d\\n\", i);\n    }\n\nI've died on that hill. I know others who lost an entire day staring at it wondering what's wrong with it. I saw it on X recently as \"99% of C programmers will not be able to find the bug.\"The equivalent D code:    import core.stdc.stdio;\n    void main()\n    {\n        int i;\n        for (i = 0; i < 10; ++i);\n            printf(\"%d\\n\", i);\n    }\n\ngets you:    test.d(5): Error: use `{ }` for an empty statement, not `;`\n\nC'mon, Standard C! Fix that!\n \nreply",
      "I expect many people know this one, but it's a useful teaching aid when understanding the relationship between arrays and pointers  int array[10];\n  *(array+1) = 56;\n  array[2] = 4;\n  3[array] = 27;\n\nThe first two are obvious, but the third is also legal.  It works because array indexing is just sugar for pointer arithmetic, so array[2]=4 is identical in meaning to *(array+2)=4.  Therefore 3[array]=27 is identical to *(3+array)=27 and so is legal.  But just because you can doesn't mean you should.\n \nreply",
      "> The first two are obvious, but the third is also legal.D doesn't have that bug!In 44 years of C programming, I've never encountered a legitimate use for the 3rd. (Other than Obfuscated C, that is.))\n \nreply",
      "It's not a bug. You're seeing the difference between \"this is how you're taught to access arrays\" and \"this is how array access actually works\".\n \nreply",
      "Agreed - I've only been programming C for 38 years but I've also never found a legitimate use.  However I have used it to illustrate a point when teaching C to beginners - it looks so odd they tend to remember it.\n \nreply",
      "The best, most entertaining book I've ever read on C covered that (unless I'm misremembering, but I doubt it): Expert C Programming.https://www.goodreads.com/book/show/198207.Expert_C_Programm...\n \nreply",
      "First part discussed on HN:https://news.ycombinator.com/item?id=40835274 (113 comments)\n \nreply",
      "I should keep this link handy for when people claim C is a simple language. Even without the GNU extensions, the examples here are pretty wretched.\n \nreply",
      "hehe. similar toHow to Get Fired Using Switch Statements & Statement Expressions:https://blog.robertelder.org/switch-statements-statement-exp...\n \nreply",
      "Without information about how identifiers are declared, you do not know how to parse this:  (A)(B);\n\nIt could be a cast of B to type A, or function A being called with argument B.Or this (like the puts(puts) in the article):  A(B):\n\nCould be a declaration of B as an identifier of type A, or a call to a function A with argument B.Back in 1999 I made a small C module called \"sfx\" (side effects) which parses and identifies C expressions that could plausibly contain side effects. This is one of the bits provided in a small collection called Kazlib.This can be used to make macros safer; it lets you write a #define macro that inserts an argument multiple times into the expansion. Such a macro could be unsafe if the argument has side effects. With this module, you can write the macro in such a way that it will catch the situation (albeit at run time!).  It's like a valgrind for side effects in macros, so to speak.https://git.savannah.gnu.org/cgit/kazlib.git/tree/sfx.cIn the sfx.c module, there is a rudimentary C expression parser which has to work in the absence of declaration info. In other words it has to make sense of an input like (A)(B).I made it so that when the parser encounters an ambiguity, it will try parsing it both ways, using backtracking via exception handling (provided by except.c).  When it hits a syntax error, it can backtrack to an earlier point and parse alternatively.Consider (A)(A+B). When we are looking at the left part (A), that could plausibly be a cast or declaration. In recursive descent mode, we are going left to right and looking at left derivations. If we parse it as a declaration, we will hit a syntax error on the +, because there is no such operator in the declarator grammar. So we backtrack and parse it as a cast expression, and then we are good.Hard to believe that was 26 years ago now. I think I was just on the verge of getting into Lisp.I see the sfx.c code assumes it would never deal with negative character values, so it cheerfully uses the <ctype.h> functions without a cast to unsigned char. It's a reasonable assumption there since the inputs under the intended use case would be expressions in the user's program, stringified by the preprocessor. Funny bytes would only occur in a multi-byte string literal (e.g. UTF-8).  When I review code today, this kind of potential issue immediately stands out.The same exception module is (still?) used in the Ethereal/Wireshark packet capture and analysis tool. It's used to abort \"dissecting\" packets that are corrupt or truncated.\n \nreply"
    ],
    "link": "https://lcamtuf.substack.com/p/getting-silly-with-c-part-void2",
    "first_paragraph": ""
  },
  {
    "title": "Doing Hard Things While Living Life: Why We Built Vade Studio in Clojure (vadelabs.com)",
    "points": 98,
    "submitter": "puredanger",
    "submit_time": "2025-01-07T13:52:09 1736257929",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=42622264",
    "comments": [
      "Interesting story.  I am not entirely convinced that all credit should go to the programming language here, though.My theory is that communicating abstractions is hard.  If you work on your own, or in a (very) small team, you can come up with powerful abstractions that allow you to build amazing systems, quickly.  However, sharing the underlying ideas and philosophy with new team members can be daunting.  As systems grow, and mistakes are made, it becomes more and more likely that you run into serious problems.This may also be why Java and similar object oriented programming languages are so successful for systems that have to be maintained for ages, by large teams of developers.  There are but few abstractions and patterns, and it does not allow you to shoot yourself in the foot, nor to blow your whole leg off.  Conversely, this may also be why complex frameworks, such as Spring, are not always so nice, because they introduce (too?) powerful abstractions, for example through annotations.  It may also clarify why more powerful languages such as Scala, Common Lisp, Smalltalk, Haskell, etc, consistently fail to pick up steam.Another theory is that not every developer is comfortable with abstract concepts, and that it simply takes a team of smart people to handle those.\n \nreply",
      "Another theory is that C inspired languages are very mechanistic and easier to visualize. Same goes for OOP with the Animal->{Cat,Dog} explanation. But that's just surface level and once you get to the difficult part (memory management in C and software design in Java) where the ability to grasp abstractions is required, we're back to square one.I believe once you've got to some point, dealing with abstractions is a way of life. It's either in the language, the technical requirements, or the software design.\n \nreply",
      "> It may also clarify why more powerful languages such as Scala, Common Lisp, Smalltalk, Haskell, etc, consistently fail to pick up steam.Languages need a window of opportunity, and many of those squandered it.Clojure won over Scala because at the time when people were loooking for an alternative JVM langauge, Clojure was more of a departure from Java and seemed to have better tooling (compile times and syntax support) than Scala.Smalltalk and Common Lisp wasted their moment by not being cheap/free to people using micros in the 1980s.Lisp, especially, very much wasted its moment with micros.  The fact that no Lisper had the vision to dump a Lisp onto the bank switched micros (which makes GC really easy and useful) of the mid to late 1980s is a self-inflicted bullet wound.  Lots of us hated doing assembly language programming but had no real alternative.  This was a loss born of pure arrogance of Lispers who looked down on those micros as not being \"real machines\".I weep for all the hours I wasted doing assembly language as a teenager that I could have been writing Lisp.  How much software could have been written that would have been <100 lines of Lisp if only someone had written that tool?\n \nreply",
      "...in what sense has Clojure actually won over Scala?I see way more Scala in companies last ~5y and have the impression of its ecosystem being more robust. Not uncommon for greenfields. It's longer than that I even encountered an active Clojure codebase. This is from a data-engineer perspective.Clojure may be more popular for some niche of app startups perhaps? We are in different \"bubbles\" I suppose.EDIT: Data disagrees with you also.https://www.tiobe.com/tiobe-index/https://redmonk.com/sogrady/2024/09/12/language-rankings-6-2...https://survey.stackoverflow.co/2024/technology#1-programmin...\n \nreply",
      "> Lots of us hated doing assembly language programming but had no real alternative.I kind of fail to see Lisp as an alternative to assembler on mid 80s micros.Though, there were several cheap Lisps for PCs...\n \nreply",
      "The bank switched memory architectures were basically unused in mid 80s micros (C128, CoCo3, etc.).Lots of utility software like spell checkers and the like still existed.  These would be trivial to implement in Lisp but are really annoying in assembler.Lisp would have been really good relative to BASIC interpreters at the time--especially since you could have tokenized the atoms.  It also would have freed people from line numbers.  Linked lists work well on these kinds of machines.  64K is solid for a Lisp if you own the whole machine.  You can run over a bank of 16K of memory for GC in about 50 milliseconds or so on those architectures.Had one of the Lisperati evangelized Lisp on micros, the world would look very different.  Alas, they were off charging a gazillion bucks to government contracts.However, to be fair, only Hejlsberg had the correct insights from putting Pascal on the Nascom.\n \nreply",
      "Clojure has some interesting advantages - which doesn't mean others might not.Rapid application technologies, methedologies, or frameworks are not unusual.I know some wonderfully productive polyglot developers who by their own choice end up at Clojure.  It doesn't have to be for everyone.I wouldn't rule out that Clojure doesn't deserve credit.  I wouldn't think it's a good idea to discredit Clojure from not having tried it myself.I do hope someone with extensive Clojure experience can weigh in on the advantages.How easy something is a codebase grows is something to really consider.This product regardless of how it's built is pretty impressive. I'd be open to learning advantages and comparisons without denying it.\n \nreply",
      "I'm curious if Elixir could provide a similar development environment?Seems like many similar capabilities, like a focus on immutable data structures, pure functions, being able to patch and update running systems without a restart, etc.\n \nreply",
      "@OP \"Model our domain as a graph of attributes and relationships\" and \"generate resolvers\". I'm curious what your model looks like so that you are able to \"generate resolvers\"? I had looked into using Malli as the model, but curious what route you took.\n \nreply",
      "I think these words will make more sense in the context of Pathom.https://pathom3.wsscode.com/\n \nreply"
    ],
    "link": "https://bytes.vadelabs.com/doing-hard-things-while-living-life-why-we-built-vade-studio-in-clojure/",
    "first_paragraph": ""
  },
  {
    "title": "Learning How to Think with Meta Chain-of-Thought (arxiv.org)",
    "points": 173,
    "submitter": "drcwpl",
    "submit_time": "2025-01-10T12:37:54 1736512674",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=42655098",
    "comments": [
      "I find their critique compelling, particularly their emphasis on the disconnect between CoT\u2019s algorithmic mimicry and true cognitive exploration. The authors illustrate this with examples from advanced mathematics, such as the \"windmill problem\" from the International Mathematics Olympiad, a puzzle whose solution eludes brute-force sequential thinking. These cases underscore the limits of a framework that relies on static datasets and rigid generative processes. CoT, as they demonstrate, falters not because it cannot generate solutions, but because it cannot conceive of them in ways that mirror human ingenuity.As they say - \"Superintelligence isn't about discovering new things; it's about discovering new ways to discover.\"\n \nreply",
      "> \"Superintelligence isn't about discovering new things; it's about discovering new ways to discover.\"Wow I love that quote.\n \nreply",
      "Thank you for mentioning the windmill problem. Great insights!https://www.3blue1brown.com/lessons/windmills\n \nreply",
      "I love the quote you mentioned at the end. Do you remember the original source?\n \nreply",
      "https://x.com/nathanthinks/status/1877510438621163987\n \nreply",
      "Just train it on meta reasoning, ie train it on people discovering ways to discover. It's not really a big problem, just generate the dataset and have at it.\n \nreply",
      "This doesn't give you the ability to process ideas through the derived new insights, any more than loading the contents of a VLSI program into regular RAM gives you an FPGA.The linear-algebra primitives used in LLM inference, fundamentally do not have the power to allow an LLM to \"emulate\" its own internals (i.e. to have the [static!] weights + [runtime-mutable] context, together encode [runtime-mutable] virtual weights, that the same host context can be passed through.) You need host support for that.\n \nreply",
      "> The linear-algebra primitives used in LLM inference, fundamentally do not have the power to allow an LLM to \"emulate\" its own internals [\u2026] You need host support for that.Neither do biological brains (explicitly), yet we can hypothesize just fine.\n \nreply",
      "You're conflating two steps:1. hypothesizing \u2014 coming up with a novel insight at runtime, that uncovers new parts of the state space the model doesn't currently reach2. syllogizing \u2014 using an insight you've derived at runtime, to reach the new parts of the state spaceLLMs can do 1, but not 2.(Try it for yourself: get an LLM to prove a trivial novel mathematical theorem [or just describe the theorem to it yourself]; and then ask it to use the theorem to solve a problem. It won't be able to do it. It \"understands\" the theorem as data; but it doesn't have weights shaped like an emulator that can execute the theorem-modelled-as-data against the context. And, as far as I understand them, current Transformer-ish models cannot \"learn\" such an emulator as a feature. You need a slightly different architecture for that.)And actually, humans can't really do 2 either!That is: humans can't immediately make use of entirely-novel insights that weren't \"trained in\", but only just came to them, any more than LLMs can!Instead, for humans, the process we go through is either:\u2022 come up with the insight; sleep on it (i.e. do incremental training, converting the data into new weights); use the insight\u2022 build up 99% of the weights required for the insight \"in the background\" over days/months/years without realizing it; make the final single connection to \"unlock\" the insight; immediately use the insightLLMs don't get to do either of these things. LLMs don't do \"memory consolidation\"; there is no gradual online/semi-online conversion of \"experiences\" into weights, i.e. reifying the \"code stored as data\" into becoming \"code\" that can be executed as part of the model.With (current) LLMs, there's only the entirely-offline training/fine-tuning/RLHF \u2014 at much greater expense and requiring much greater hardware resources than inference does \u2014 to produce a new iteration of the model. That's why we're (currently) stuck in a paradigm of throwing prompts at ever-larger GPT base models \u2014 rather than just having an arbitrary stateful base-model that you \"install\" onto a device like you'd install an RDBMS, and then have it \"learn on the job\" from there.\n \nreply",
      "> And actually, humans can't really do 2 either!> That is: humans can't immediately make use of entirely-novel insights that weren't \"trained in\", but only just came to them, any more than LLMs can!Agreed \u2013 but I'd argue that they both can, albeit in an extremely clunky way (i.e. very similar to \"chain-of-thought\" LLMs): Mechanically applying the new insights in a low-efficiency, \"emulated\" layer.> LLMs don't get to do either of these things. LLMs don't do \"memory consolidation\"; there is no gradual online/semi-online conversion of \"experiences\" into weights, i.e. reifying the \"code stored as data\" into becoming \"code\" that can be executed as part of the model.At the moment that's definitely their biggest weakness, but one could argue that memory consolidation happens ~once per year, globally, as past interactions with them almost certainly become future training data.In some ways, that's more powerful than a single human gradient descending overnight and in the shower; in others, it's obviously much worse.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2501.04682",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Flattening ASTs (and Other Compiler Data Structures) (cornell.edu)",
    "points": 60,
    "submitter": "aw1621107",
    "submit_time": "2025-01-10T19:23:50 1736537030",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=42659061",
    "comments": [
      "\"Instead of allocating Expr objects willy-nilly on the heap, we\u2019ll pack them into a single, contiguous array.\"\nZig compiler pipeline (AST, Zir, Air, Sema) does exactly this on all layers. Not only contiguous, but instead of array-of-structs it is struct-of-arrays, so walking the tree is even more cache friendly. For AST see: https://github.com/ziglang/zig/blob/master/lib/std/zig/Ast.z...\n \nreply",
      "Makes me wonder if people in APL/J/K community have not been influenced or influencing this kind of technique. IIRC Aaron Hsu does tree processing through arrays (but i'm not skilled enough to analyze his code)\n \nreply",
      "Rust-analyzer uses a similar technique for parsing  https://github.com/rust-lang/rust-analyzer/blob/master/crate... which then gets fed into https://github.com/rust-analyzer/rowan (lossless syntax tree)\n \nreply",
      "There is a longer overview of it here: https://github.com/rust-lang/rust-analyzer/blob/master/docs/...\n \nreply",
      "As the article mentions, this makes it quite similar to a bytecode vm. I think the traditional wisdom is that an AST walker is easy to write, but for speed you'd want a bytecode interpreter. It'd be interesting to see how close the performance gets with this flattened AST.In practice I think there are more differences. E.g. AST interpreters tend to pass environments around while bytecode interpreters often store these on a stack (though I guess there's nothing stopping you from doing this with an AST either). I wonder if there's some goldilocks zone for ease of implementation with decent performance.\n \nreply",
      "If you instead flatten the expression tree into RPN, then you can execute it like that, with a stack machine.I seem to recall that the Red Dragon Book (Compilers: Principles, Techniques and Tools, Aho, Sethi, Ullman [1988]) describes a technique whereby intermediate code is represented in RPN, and transformations are performed by pattern matches on it.\n \nreply",
      "> Instead of allocating Expr objects willy-nilly on the heap, we\u2019ll pack them into a single, contiguous array.This happens naturally if you bump-allocate them in a garbage-collected run-time, particularly under a copying collector. Free lists also tend to co-locate because they are produced during sweep phases of GC which run through heaps in order of address.Don't make me bring out the L word for the billionth time.> A flat array of Exprs can make it fun and easy to implement hash consingOK, it's not a case of L-ignorance, just willful neglect.\n \nreply",
      "FWIW I did acknowledge this in the article:> A sufficiently smart memory allocator might achieve the same thing, especially if you allocate the whole AST up front and never add to it> Again, a really fast malloc might be hard to compete with\u2014but you basically can\u2019t beat bump allocation on sheer simplicity.\n \nreply",
      "Cool! Carbon is doing exactly this. I had asked leads if there was a paper on this approach, but they didn't have anything for me. I'll send them this post!\n \nreply",
      "Zig uses a MultiArrayList which sounds similar https://mitchellh.com/zig/parser\n \nreply"
    ],
    "link": "https://www.cs.cornell.edu/~asampson/blog/flattening.html",
    "first_paragraph": "May 1, 2023Arenas, a.k.a. regions, are everywhere in modern language implementations.\nOne form of arenas is both super simple and surprisingly effective for compilers and compiler-like things.\nMaybe because of its simplicity, I haven\u2019t seen the basic technique in many compiler courses\u2014or anywhere else in a CS curriculum for that matter.\nThis post is an introduction to the idea and its many virtues.Arenas or regions mean many different things to different people, so I\u2019m going to call the specific flavor I\u2019m interested in here data structure flattening.\nFlattening uses an arena that only holds one type, so it\u2019s actually just a plain array, and you can use array indices where you would otherwise need pointers.\nWe\u2019ll focus here on flattening abstract syntax trees (ASTs), but the idea applies to any pointer-laden data structure.To learn about flattening, we\u2019ll build a basic interpreter twice:\nfirst the normal way and then the flat way.\nFollow along with the code in this repository, where yo"
  },
  {
    "title": "Formal Methods: Just Good Engineering Practice? (2024) (brooker.co.za)",
    "points": 154,
    "submitter": "aiono",
    "submit_time": "2025-01-10T15:25:42 1736522742",
    "num_comments": 89,
    "comments_url": "https://news.ycombinator.com/item?id=42656433",
    "comments": [
      "Formal verification of software, as the article acknowledges, relies heavily on the type of software and the development process.To use formal verification you need to have formal requirements of the behavior of your software. Most software projects and design philosophies are simply incompatible with this.In software development and design can often fall together, but that means that it is uniquely ill suited for formal methods. If you are developing, before you are sure what you even want, then formal methods do not apply.But I agree that there are certain areas, mostly smaller, safety critical, systems, which rely on upfront specifications, which can heavily benefit from formal verification. E.g. Aerospace software relies heavily on verification.\n \nreply",
      "It hasn\u2019t been my experience that it is as niche as this. I believe the \u201ccosts,\u201d people refer to in these discussions have come way down over the last couple of decades. I\u2019ve taught developers how to use tools like TLA+ and Alloy in week.It\u2019s not a skill that requires a PhD and years of research to acquire these days.Nor does writing a basic, high level specification.If anything you will learn something about the system you are modelling by using a model checker. And that can be useful even if it is used for documentation or teaching.The fundamental power of formal methods is that they force you to think things through.All too often I find software developers eager to believe that they can implement concurrent algorithms armed with their own wit, a type checker, and a smattering of unit tests. It can be humbling to find errors in your design and assumptions after using a model checker. And perhaps it\u2019s hubris that keeps programmers from using such tools in more \u201cmundane\u201d and \u201creal world\u201d contexts.There are a lot more \u201csmall\u201d distributed systems than you\u2019d expect and state spaces are generally larger than you\u2019d anticipate if you weren\u2019t open to formalizing your work.\n \nreply",
      "Seems like it would really slow you down though if you adopt it too early. Sometimes you don't even know if the thing you want to do is possible.My development style is:- prototype ideas in a quick and dirty dynamic typed language just to gain confidence it's going to work (a lot of ideas die here)- rewrite in a more performant and safe language with a \"type checker and a smattering of unit tests\" (usually here I'm \"done\" and have moved onto the next idea/task. If there's an issue I fix it and add another test case)I'm trying to imagine where formal verification comes in. I'm imagining something like:- prototype ideas in a quick and dirty dynamic typed language just to gain confidence it's going to work (a lot of ideas die here)- Formally model the requirements- rewrite in a language that can be formally verified and which is hopefully performant and lets me do things like simd and/or cuda if needed- Never have to fix a bug unless there was a bug in the requirements (?)To me, it just seems like it would take an order of magnitude longer to develop things this way for not much benefit (I've traded development time and potentially runtime performance in exchange for correctness)\n \nreply",
      "https://adsharma.github.io/agentic-transpilers/Long way to go. But there is a design.Use z3/SMT instead of TLA+\n \nreply",
      "I would say that the \"nicheness\" depends on how you treat software. Your development process and software architecture are engineering choices you make and these engineering choices affect how well formal specification applies.I didn't talk about \"costs\" or about \"how hard\" it is, but that common practices in software development make using formal methods infeasible. If you want to use formal verification you likely have to change how you develop software. In an environment where there is general uncertainty about architecture and behavior, as is common in agile environments, formal verification is difficult to implement. By its nature, formal verification discourages fast iteration.\n \nreply",
      "> By its nature, formal verification discourages fast iteration.Not necessarily. You can develop the specification and implementation in parallel. That way, at every point in time, you could have a much more thorough and deep understanding of what you're currently building, as well as how exactly you decide to change it.\n \nreply",
      "> I\u2019ve taught developers how to use tools like TLA+ and Alloy in week.TLA+:- https://en.wikipedia.org/wiki/TLA%2B- https://lamport.azurewebsites.net/tla/tla.htmlAlloy:- https://alloytools.org/- https://en.wikipedia.org/wiki/Alloy_(specification_language)\n \nreply",
      "For distributed systems this makes sense. Most people aren't writing distributed system components though but things where the risk usually isn't technical like business software. I worked on a project where I got into arguments with the PMs because I pushed back on optimizing performance on the main page of our pre-launch product. I argued that we don't have any real feedback that the page is what is needed and the PM thought I was insane for doubting the main page was needed. We completely redesigned that page twice, deleted it entirely and then had to bring it back because the sales team liked it for initial demos.Every process is a tradeoff and it anyways depends on your specific circumstances which choice is best for your team and project.\n \nreply",
      "It does depend a lot on circumstance and context.Is it absolutely important that your system is correct? ... begs the question, correct with respect to what? Generally: a specification.There are lots of situations where you don't know what your system is supposed to do, where testing a few examples is sufficient, or it's not terribly important that you know that it does what you say it ought to. Generating a batch report or storing some customer responses in a database? Trust the framework, write a few tests if you need to, nobody is going to find a formal specification valuable here.However, if you need to deploy configuration to a cluster and need to ensure there is at least two nodes with the a version of the configuration that matches the database in the load balancer group at all times during the migration? Need to make sure the migration always completes and never leaves the cluster in a bad state?Even smaller in scale: need to make sure that references in your allocator don't contain addresses outside of your memory pool? Need to make sure that all locks are eventually released?It's definitely much faster to iterate on a formal specification first. A model checker executes your model against the entire state space. If you're used to test-driven development or working in a statically typed language, this is useful feedback to get early on in the design process.What the scope is that is appropriate for using tools like this is quite large and not as niche as some folks imply. I don't do aerospace engineering but I've used TLA+ to model deployment scripts and find bugs in OpenStack deployments, as well as to simply learn that the design of certain async libraries are sound.Update: more examples.\n \nreply",
      "What's your recommendation on how to rapidly learn TLA+? I spent some time staring at references and the UI a few months ago and came away very defeated. But I'd like to actually level up here.\n \nreply"
    ],
    "link": "https://brooker.co.za/blog/2024/04/17/formal",
    "first_paragraph": "Yes. The answer is yes. In your face, Betteridge.Earlier this week, I did the keynote at TLA+ conf 2024 (watch the video or check out the slides). My message in the keynote was something I have believed to be true for a long time: formal methods are an important part of good software engineering practice. If you\u2019re a software engineer, especially one working on large-scale systems, distributed systems, or critical low-level system, and are not using formal methods as part of your approach, you\u2019re probably wasting time and money.Because, ultimately, engineering is an exercise in optimizing for time and money1.\u201cIt would be well if engineering were less generally thought of, and even defined, as the art of constructing. In a certain important sense it is rather the art of not constructing; or, to define it rudely but not inaptly, it is the art of doing that well with one dollar, which any bungler can do with two after a fashion.\u201d Arthur Wellington2At first, this may seem counter-intuitive"
  },
  {
    "title": "17th-century priory in France converted into a medieval-inspired topiary garden (houseandgarden.co.uk)",
    "points": 4,
    "submitter": "NoRagrets",
    "submit_time": "2025-01-08T20:10:03 1736367003",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.houseandgarden.co.uk/gallery/prieure-de-vauboin-garden",
    "first_paragraph": "The design of the hortus conclusus that wraps round the priory can be seen from the vantage point of the steep slope behind the 17th-century building. To the right, a striking labyrinth with geometric box topiary linked by cinder paths draws the eye, while, on the left, is the potager, where the owner grows edible plants and herbs.An unassuming wooden sign on a country lane an hour north of Tours belies the spectacular garden that it points towards. It is only as you walk through a simple trellis gate into a narrow passage that you get the first indication something special is about to happen. Its walls are lined with chestnut logs stacked two metres high, topped with a blanket of vine twigs. Underfoot, log sections are set on end, creating an uneven surface that obliges visitors to slow their pace for what lies beyond.As you enter the garden proper, it is the house that first draws your attention \u2013 the beautiful, imposing old priory Le Prieur\u00e9 de Vauboin, which dates back to the 17th "
  },
  {
    "title": "Show HN: TubePen \u2013 My attempt to get more out of YouTube learning (tubepen.com)",
    "points": 32,
    "submitter": "n0vella",
    "submit_time": "2025-01-07T13:14:21 1736255661",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=42621989",
    "comments": [
      "I tried signing up for the free trial subscription, and am starting to regret it.I successfully entered my credit card details (4 times), yet it still says I don't have an active subscription. So besides not being able to try this app, there is also no way to cancel my automatic payments, which starts on Jan 18.After fixing this onboarding problem, I suggest making it possible to try without giving any personal information. (I expected the video player on the landing page to be functional, but I guess just the note section is.)Also there are many typos on the site. Like \"loged in as\"\n \nreply",
      "Please write me an email to n0vella@outlook.com to see what happened. I'm going to check the logs, nothing must be charged to you.Typos are also my fault as english isn't my mother language, Will check that also.\n \nreply",
      "The OP sorted out the subscription/signup/billing issues.The main reason I signed up was for a nice way to search through YouTube transcripts. YouTube requires many clicks, and the transcript is jammed into a small div.Another reason I signed up was because I was curious how the YouTube videos were being embedded. It seems like just via the YouTube API?\n \nreply",
      "I forwarded the email I already sent to support@tubepen.com.Another typo is \"Acess to the entire platform\"\n \nreply",
      "Seems like outlook stop receiving emails from cloudflare redirection:upstream (outlook-com.olc.protection.outlook.com.) temporary error: Unknown error: transient error (451): 4.7.650 The mail server [104.30.10.49] has been temporarily rate limited due to IP reputationI don't know how to deal with this so right now you can contact me on n0vella@outlook.com\n \nreply",
      "Really neat idea actually! \nI love the landing page, both the interactive part and particularly the visual style. \nOne thing that I would note though, is that scrolling down, I expected the images to be interactive since they blend in with the rest of the page and look like interactive elements. I would either try to make it more obvious that they are example images, or actually make them interactive\n \nreply",
      "You are right. I think those images need some css to let the user know that are images.Thank you very much for the feedback!\n \nreply",
      "Looks cool, love work in this direction. I wonder how learning rates w this would compare to taking notes on paper like in a traditional lecture, which some studies show improves learning [0].[0]: https://www.scientificamerican.com/article/why-writing-by-ha...\n \nreply",
      "What I'd like from a site like this is to be quizzed on the videos I watch, right after watching and maybe a few days after. I think that would have a large effect on learning.\n \nreply",
      "There are quite a few services that use generative models to generate quizzes from YouTube videos.\n \nreply"
    ],
    "link": "https://www.tubepen.com/",
    "first_paragraph": ""
  },
  {
    "title": "Visualizing All ISBNs (annas-archive.org)",
    "points": 328,
    "submitter": "RyanShook",
    "submit_time": "2025-01-10T04:45:55 1736484355",
    "num_comments": 100,
    "comments_url": "https://news.ycombinator.com/item?id=42652577",
    "comments": [
      "I see that bounty at the bottom, so tossing away my chances here, but this visualization is just asking to be mapped onto a Hilbert Curve. [0] When you \"stripe\" the data like this, points that are sorted close together could end up pretty far apart, since a distance in the Y axis skips an entire row of data as you move down, rather than a distance in the X axis which is 1-to-1 with the source data.If you map it onto a hilbert curve, the X and Y axis mean nothing, but visually points that are close together in the sorted list, will be visually close together in the output image.Since the first part of an ISBN is the country, then the second part is the publisher, and the third part is the title, with a check sum at the end, I would remove the checksum and sort them each as a big number. (no hyphens)You should end up with \"islands\", where you see big areas covered by big publishing countries, with these \"islands\" having bright spots for the publisher codes.Bonus points for labeling these areas!I set up something a while ago [1] for an interview that does this with weather data. It makes the seasons really obvious since they're all grouped together.[0] https://en.wikipedia.org/wiki/Hilbert_curve[1] https://graypegg.com/hilbert (https://github.com/graypegg/hilbertcurveplayground code if anyone wants to go for the prize using this! Please at least mention me if you decide to reuse this code, but I can't stop ya lol)\n \nreply",
      "What property makes the Hilbert curve desirable compared to, say, a snake pattern, with which neighbouring ISBNs are also neighbours in the visualisation?The worry I have with Hilbert curves is that they make the result look like there are distinct \"squares\" of data [0] when really this is just an artifact of how Hilbert curves work. In that sense, the current visualization is more useful, because it's straightforward to identify the location of each country in it.[0] https://raw.githubusercontent.com/jakubcerveny/gilbert/maste...\n \nreply",
      "And there's a generalized Hilbert curve, the Gilbert curve, for non powers of two rectangular regions [0] (online demo [1]).[0] https://github.com/jakubcerveny/gilbert[1] https://jakubcerveny.github.io/gilbert/demo/\n \nreply",
      "The thing is, ISBNs aren't hierarchical --- they are bought in blocks (or even individually at an exorbitant markup, says the guy who bought one to reprint a single book), so this doesn't show anything really interesting/useful.A visualization using LoC or even Dewey Decimal would be far more useful, esp. if it also linked to public domain and copyright-free repositories/lists, say an interactive and visual version of John Mark Ockerbloom's:https://onlinebooks.library.upenn.edu/\n \nreply",
      "ISBN's are hierarchical, what do you mean? Like Gaul, ISBNs are divided into multiple parts, where one part is for the language, another is for the publisher, and the last is for the title. The last part is a checksum. https://en.wikipedia.org/wiki/ISBN#Overview\n \nreply",
      "Yes, but this internal hierarchy for an issued number doesn't tell anything beyond those facts about a specific edition of a specific text.One can't use ISBNs alone to create a hierarchical listing of texts which is useful for anything beyond browsing by language/publisher/order in which the ISBN was generated.A visual and interactive representation of books by LoC or some other cataloging system would actually be useful.\n \nreply",
      "I got into an argument with the manager of South End Press back in '94 about whether 'Futuresplash' (soon to be Macromedia Flash) had a future,  he thought it did and he was right.Years later I was working at the library and got a little bit steamed because South End Press was reusing ISBN's after books went out of print which was allowed but, I think, lame.One of my strategies for researching a topic is looking a few up in the OPAC,  finding them in the stacks,  and finding more books on the topic in those areas. (In the Library of Congress system,  machine vision could be under QA56 with the rest of computer science or around TA1630, thus \"areas\".)From time to time I've thought about trying to replicate the feel of this with some kind of UI given that our library moved a lot of the collection into deep archives and we have a very fast 'Borrow Direct' service with other peers)\n \nreply",
      "totally agree, but thats not in the data. however, since blocks are assigned to agencies associated with countries and publishers, you might find some utility in showing coverage by likely language and/or country of origin and date.\n \nreply",
      "It shows what they want to show, which is mostly how much of the world books they have. Hierarchical has nothing to do with it.\n \nreply",
      "It only sort of shows that. ISBNs are issued by edition, not title, so many books would have more than one. And books published before 1970 or so might not be represented at all if they have no recent edition.\n \nreply"
    ],
    "link": "https://annas-archive.org/blog/all-isbns.html",
    "first_paragraph": ""
  },
  {
    "title": "Refactoring with Codemods to Automate API Changes (martinfowler.com)",
    "points": 6,
    "submitter": "teivah",
    "submit_time": "2025-01-08T08:16:26 1736324186",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://martinfowler.com/articles/codemods-api-refactoring.html",
    "first_paragraph": "ArchitectureRefactoringAgileDeliveryMicroservicesDataTestingDSLAboutBooksFAQVideosContent IndexBoard GamesPhotographyInsightsCareersRadarRSSMastodonLinkedInX (Twitter)BGG\n    Refactoring is something developers do all the time\u2014making code easier to\n    understand, maintain, and extend. While IDEs can handle simple refactorings\n    with just a few keystrokes, things get tricky when you need to apply changes\n    across large or distributed codebases, especially those you don\u2019t fully\n    control. That\u2019s where codemods come in. By using Abstract Syntax Trees\n    (AST), codemods allow you to automate large-scale code changes with\n    precision and minimal effort, making them especially useful when dealing\n    with breaking API changes. This article looks at how codemods can help\n    manage these challenges, with practical examples like removing feature\n    toggles or refactoring complex React components. We\u2019ll also discuss\n    potential pitfalls and how to avoid them when using codemods at "
  },
  {
    "title": "Stone selection by wild chimpanzees shares patterns with Oldowan hominins (sciencedirect.com)",
    "points": 26,
    "submitter": "benbreen",
    "submit_time": "2025-01-07T06:32:05 1736231525",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=42619898",
    "comments": [
      "There is a fascinating story documented in \"How to Tame a Fox (and Build a Dog): Visionary Scientists and a Siberian Tale of Jump-Started Evolution\" by by Lee Alan Dugatkin and Lyudmila Trut (https://www.amazon.com/How-Tame-Fox-Build-Dog/dp/022644418X) about a long-running domestication of silver foxes, started in USSR in 1950ies and still running. One of the early comments in that book is what Dmitri Belyaev (idea man) really wanted was to run this on chimpanzees, but (paraphrasing his words) \"a) due to breeding cycles the minimum time to results 6 centuries\" and \"b) the ethics of doing this would not pass any committee\". He chose foxes since they breed once a year and he was able to justify it on commercial grounds (fur production). Results are absolutely spectacular, as the book lovingly documents\n \nreply",
      "Earlier: https://news.ycombinator.com/item?id=42594851\n \nreply"
    ],
    "link": "https://www.sciencedirect.com/science/article/pii/S0047248424001337",
    "first_paragraph": ""
  },
  {
    "title": "Who Can Understand the Proof? A Window on Formalized Mathematics (stephenwolfram.com)",
    "points": 170,
    "submitter": "ColinWright",
    "submit_time": "2025-01-10T12:21:06 1736511666",
    "num_comments": 92,
    "comments_url": "https://news.ycombinator.com/item?id=42654995",
    "comments": [
      "One of the perennial questions about proof automation has been the utility of proofs that cannot be understood by humans.Generally, most computer scientists using proof automation don't care about the proof itself -- they care that one exists. It can contain as many lemmas and steps as needed. They're unlikely to ever read it.It seems to me that LLMs would be decent at generating proofs this way: so long as they submit their tactics to the proof checker and the proof is found they can generate whatever is needed.However for mathematicians, of which I am not a member of that distinguished group, seem to appreciate qualities in proofs such as elegance and simplicity. Many mathematicians that I've heard respond to the initial question believe that a proof generated by some future AI system will not be useful to humans if they cannot understand and appreciate it. The existence of a proof is not enough.Now that we're getting close to having algorithms that can generate proofs it makes the question a bit more urgent, I think. What use is a proof that isn't elegant? Are proofs written for a particular audience or are they written for the result?\n \nreply",
      "Mathematician here (trained as pure, working as applied). Non-elegant proofs are useful, if the result is important. e.g. People would still be excited by an ugly proof of the Riemann hypothesis.^1  It's important too a lot of other theorems if this is true or not. However, if the result is less central you won't get a lot of interest.Part of it is, I think, that \"elegance\" is flowery language that hides what mathematicians really want: not so much new proofs as new proof techniques and frameworks. An \"elegant\" proof can, with some modification, prove a lot more than its literal statement. That way, even if you don't care much about the specific result, you may still be interested because it can be altered to solve a problem you _were_ interested in.1: It doesn't have to be as big of a deal as this.\n \nreply",
      "Then again, even an 'elegant' proof can be surprisingly inflexible. I've recently been working through Ap\u00e9ry's proof that \u03b6(3) is irrational. It's so simple that even a clueless dabbler like me can understand all the details. Yet no one has been able to make his construction work directly for anything else (that hasn't already been proven irrational). C'est la vie, I suppose.\n \nreply",
      "There was a post yesterday of a quanta article: https://news.ycombinator.com/item?id=42644896.The article explains that two mathematicians were able to place Apery's proof that \u03b6(3) is irrational into a much wider (and hence more powerful) framework. I doubt that framework is as easy to understand as the original proof. But in the end something with wider applicability did come out of the proof.\n \nreply",
      "Yeah, many of the fancy analytic methods are beyond my level of dabbling. I've been trying to learn more about them, so I can solve the myriad exercises left to the reader in all the Diophantine approximation papers.Still, the newer methods publicized in the Quanta article definitely get more involved, and at least from my perspective they don't establish things as elegantly as Ap\u00e9ry's \u03b6(2) and \u03b6(3) arguments do. Hopefully they turn out to be powerful in practice, to make up for it.\n \nreply",
      "> Part of it is, I think, that \"elegance\" is flowery language that hides what mathematicians really want: not so much new proofs as new proof techniques and frameworks. An \"elegant\" proof can, with some modification, prove a lot more than its literal statement. That way, even if you don't care much about the specific result, you may still be interested because it can be altered to solve a problem you _were_ interested in.Do you feel this could be a matter of framing? If you view the \"proof\" as being the theorem prover itself, plus the proof that it is correct, plus the assumptions, then whatever capability it gains that lets it prove your desired result probably is generalizable to other things you were interested in. It would seem like a loss if they're dismissed simply because their scratch work is inscrutible.\n \nreply",
      "And of course if you come across an inelegant proof you suddenly have the opportunity to think about it and see if you can make it more elegant!\n \nreply",
      "\"It doesn't have to be as big of a deal as this.\"Agree. The truthfulness of the four-colour theorem is good to know, although there is not yet any human-readable proof.\n \nreply",
      "I feel like the four-color theorem automated proof is much more 'human-readable' than the proofs done with automated theorem provers. Because with the four-color theorem, there is a human readable proof that says \"if this finite set of cases are all colorable, then all planar graphs are colorable\". And then there is some rather concrete code that generates all the finite cases, and finds a coloring for them. Every step in there makes sense, and is fully understandable. The fact that the exhaustive checking wasn't done by hand doesn't mean its hard to understand how the proof works, or what is 'actually going on'.For a general theorem prover, reading the code doesn't explain anything insightful about why the theorem is true. For the 4 color theorem, the code that proved it actually gives insight in how the proof works.\n \nreply",
      "One thing that many mathematicians today don\u2019t think about is how deeply intertwined the field has historically been with theology. This goes back to the Pythagoreans at least.That survives in the culture of mathematics where we continue to see a high regard for truth, beauty, and goodness. Which, incidentally, are directly related to logic, aesthetics, and ethics.The value of truth in a proof is most obvious.The value of aesthetics is harder to explain, but there's no denying that it is in fact observably valued by mathematicians.As for ethics, remember that human morality is a proper subset thereof. Ethics concerns itself with what is good. It may feel like a stretch, but it's perfectly reasonable to say that for two equally true proofs of the same thing, the one that is more beautiful is also more good. Also, obviously, given two equally beautiful proofs, if only one is true then it is also more good.\n \nreply"
    ],
    "link": "https://writings.stephenwolfram.com/2025/01/who-can-understand-the-proof-a-window-on-formalized-mathematics/",
    "first_paragraph": "Theorem (Wolfram with Mathematica, 2000): The single axiom ((a\u2022b)\u2022c)\u2022(a\u2022((a\u2022c)\u2022a))\uf7d9c is a complete axiom system for Boolean algebra (and is the simplest possible)For more than a century people had wondered how simple the axioms of logic (Boolean algebra) could be. On January 29, 2000, I found the answer\u2014and made the surprising discovery that they could be about twice as simple as anyone knew. (I also showed that what I found was the simplest possible.) It was an interesting result\u2014that gave new intuition about just how simple the foundations of things can be, and for example helped inspire my efforts to find a simple underlying theory of physics. But how did I get the result? Well, I used automated theorem proving (specifically, what\u2019s now FindEquationalProof in Wolfram Language). Automated theorem proving is something that\u2019s been around since at least the 1950s, and its core methods haven\u2019t changed in a long time. But in the rare cases it\u2019s been used in mathematics it\u2019s typically been"
  }
]