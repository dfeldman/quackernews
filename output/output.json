[
  {
    "title": "CSS Minecraft (benjaminaster.com)",
    "points": 370,
    "submitter": "mudkipdev",
    "submit_time": "2025-05-26T18:28:43 1748284123",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=44100148",
    "comments": [
      "Without a doubt the most impressive thing I've seen with CSS.This immediately brought \"A Single Div\"[0] to mind, which stood as the coolest CSS demo I'd seen for... 11 years!This one takes the cake. I'll be pouring over it. Thanks![0]: https://a.singlediv.com/\n \nreply",
      "So many of these look deliciously interactive but aren\u2019t. Is that because I\u2019m on mobile or do they not do anything?\n \nreply",
      "I don't think any on the first page are interactive. There might be a few on the next page of it (I only found one where a pen changes color on hover).\n \nreply",
      "This is fiendishly clever, and really quite elegant.I made some of my own notes on how this works here: https://simonwillison.net/2025/May/26/css-minecraft/\n \nreply",
      "Incredibly brilliant. Seems to have gone completely unnoticed 2.5 years ago.https://news.ycombinator.com/item?id=33579407\n \nreply",
      "If anyone's wondering how it manages the state, a quick peek into the source code shows that it uses radiobuttons and the HTML contains all the blocks you could ever possibly place.\n \nreply",
      "If anyone is equally curious how the camera state works, it looks like the camera is controlled by running animations when a button is in its :active state and pausing them otherwise.\n \nreply",
      "I... you're right. I was wondering why the world was only 9x9x9, there's 46k lines showing each block can have air, stone, grass, dirt, log, wood, leaves, or glass.I kind of like it.\n \nreply",
      "That is the most hacky solution I may have ever seen in a CSS demo. I love it.\n \nreply",
      "Doing this art just with css ,you are greatest one in css sir.\n \nreply"
    ],
    "link": "https://benjaminaster.com/css-minecraft/",
    "first_paragraph": "There is no JavaScript on this page. All the logic is made 100% with pure HTML & CSS. For the best performance, please close other tabs and running programs.\nView on GitHub, CodePen, benjaminaster.com\u26a0\ufe0e Your browser does not support the CSS :has() pseudo-class, which is needed for this site to work.\nPlease update it: Chromium version \u2265 105, Safari version \u2265 15.4 or Firefox version \u2265 121 is required."
  },
  {
    "title": "15 years after servers shut down, FromSoft's mech game Chromehounds back online (readonlymemo.com)",
    "points": 12,
    "submitter": "pabs3",
    "submit_time": "2025-05-27T00:52:43 1748307163",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.readonlymemo.com/interview-15-years-after-the-servers-shut-down-fromsofts-singular-mech-game-chromehounds-is-back-online/",
    "first_paragraph": "The PvP mech battler is finally playable online in Xbox 360 emulator Xenia, and I talked to the modder making it happen.[Chromehounds lobby]Me: [chanting] mechs, mechs-Other pilots: Mechs, MECHSMechanic: [pounding her toolbox] MECHS, MECHS, MECHS!Hello! Apologies for the late issue, but this story came together last minute, on account of capital-L Life, and also because the subject is developing fast. Hopefully it's worth the wait! I've got an exclusive interview for you on a shit-hot community project to resurrect a FromSoftware game that was criminally underappreciated in its day. I mean, that's practically all FromSoftware games, but this one was just born in the wrong time and place \u2014 I think it would be a substantial hit on Steam in the year 2025.But I'm getting ahead of myself, while also being behind. So let's jump right into it! It's big stompy robot time.On Wednesday, Resetera member wwm0nkey posted a thread I've been waiting to see for years: \"Fans are working to bring From S"
  },
  {
    "title": "Lossless video compression using Bloom filters (github.com/ross39)",
    "points": 175,
    "submitter": "rh3939",
    "submit_time": "2025-05-26T18:32:02 1748284322",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=44100179",
    "comments": [
      "I don't believe the document does a great job in explaining what is otherwise a very simple idea (assuming I understood it well):1. It creates a bitmap where each bit is a pixel in the image, if from frame 0 to frame 1 a given pixel changed, the corresponding bit is 1, otherwise it is 0.2. All the 1s are added to the bloom filter, hashing their offsets. Now the bloom filter will be positive for all such indexes plus a percentage of false positive indexes.3. We query the bloom filter to see all the indexes that are positive, and for all such pixels we store the raw pixel data of what changed. So we can reconstruct the next frame easily.You can think at this like as storing the delta between two frames as: x,y,r,g,b of all the pixels that changed, but compressing a lot the x,y part at the cost of storing a bit more r,g,b than needed.I have the feeling that since the pixels that changes from frame 0 to frame 1 are often similar (in their location) to what will change from frame 1 to frame 2, there is the possibility of further compressing that as well, by setting the right flags in the next frame and storing verbatim the only offsets that changed in addition to the previous or alike.\n \nreply",
      "I wonder how good compression rations this really has...It reminds me of experiments I did with wavelets for image compression some.. 22 years ago.The inverse transform starts with a small pixel image and then uses just as many coefficients to transform it to an image twice as large (in width or height). This is done over and over again.The point is that most of your data consists of these coefficients, and most of them are close to zero: so you can flush them to zero.\nThen the problem is mostly a matter of how to encode where you don't have zeroes: i.e. you have like a bitmap and an array of the non-zeroes.\nThere were different algorithms that encoded non-zeroes, more or less conservatively, but they did often exploit the property that these non-zeroes typically were quite clustered \u2014 which is the opposite of a typical hash function used for a bloom filter.Image compression this way had obviously very poor locality, first just for the transform and then for the coefficient compression, so it was quite slow. Therefore it did feel like a dead end.\n \nreply",
      "This comment is why I go to the comments first.Oh hey you're the guy who made kilo. Good job.[edit] lol he edited it... they always edit it\n \nreply",
      "I always love when people recognize me for kilo or dump1090 or hping and not for Redis :D Side projects for the win. Thanks for your comment!\n \nreply",
      "I've literally never even used Redis, let alone know what it is or does. I dunno how I was able to make money in software since 2008 without figuring that out... or learning SQL, or C++. There's far more that I don't know than I do know. But anyway if you wrote Redis or something then congrats, I've definitely heard of it.\n \nreply",
      "I have a theory, that I call \"of the equivalence of modern software systems\" that tells a lot about how unimportant Redis and other technologies are, that is: modern computing is so developed that pick any random language, kernel, and database, any of the top ones available, and I can create every project without too much troubles. PHP / Win32 / SQLite? Ok, I can make it work. Ruby / Linux / Redis? Well, fine as well.\n \nreply",
      "A.k.a. all you need is PG and something to serve your app over HTTPS.  :joy:\n \nreply",
      "I think PG might insist on using a lisp too.\n \nreply",
      "I've noticed that too, LAMP stack vs MEAN stack etc.Part of it seems to be that software languages have \"flavors\" like natural spoken language does, so that one seems natural to one person and foreign to another, much like how DHH took TypeScript out of Rails, and I can't read Rails code or live without static types.Also college kids are always learning what the last generation already knew (like Lisp) and reinvent everything in it with the vigor of their youth and it gets popular, which I imagine is how Rails and Redis both started and caught on.But sometimes there are genuine innovations, and we can't build on them until someone comes up with them, much like how we couldn't invent machines until someone figured out that we could just use the lever to make gears to transport energy, which Archimedes didn't think of. And the more we learn collectively, the more these things spread.I wonder if this means it'll all stabilize into a JavaScript-like toolchain one day. Maybe Gary Bernhardt was right all along.\n \nreply",
      "All modern stacks are CRUD-complete as well as Turing-complete? ;)\n \nreply"
    ],
    "link": "https://github.com/ross39/new_bloom_filter_repo/blob/main/README.md",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page."
  },
  {
    "title": "Power Failure (gwintrob.com)",
    "points": 49,
    "submitter": "gwintrob",
    "submit_time": "2025-05-26T21:54:01 1748296441",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=44102034",
    "comments": [
      "I don't think this article was written by AI - at least, I am not sure it is - but the way it is divided up, the bullet lists and \"key quotes\" and breaking a relatively short article into even shorter sections, makes it feel AI generated.Sounds like an interesting book but the article says remarkably little.\n \nreply",
      "> feel AI generatedThe apparently-AI artwork doesn't help. From some googling it appears to be a direct rip-off of this: https://www.gettyimages.com/detail/news-photo/the-great-amer...\n \nreply",
      "Felt like a summary - not sure I even considered AI even if it didn\u2019t read like a critical review. I do really want to read the book. Jack Welch is a case study in hubris. And I worked for a startup that had a relationship with Immelt. We kept trying to use breakfasts and dinners with him as a marketing ploy\u2026\n \nreply",
      "Thanks for the feedback. I try to keep track of quotes I like when I'm reading. This was an attempt to bundle them up into some useful themes. I'll try to expand into something more opinionated for the next one :)\n \nreply",
      "Just reiterating what cogogo stated in a sibling, but the thing that threw me was the 'review' in the title. I was expecting some critique or comparison but instead saw summary and highlights.I enjoyed the summary and highlights, and learnt about some details I would likely have never otherwise seen, so I think it's just the framing that seemed 'off'.Depending on your intent consider reframing or adding critique, but I think the content is good and I appreciate you making it.[edit] There is some critique and comparison in the opening: \"Shakespearean tragedy\" and \"The result is equal parts invention history, boardroom knife-fight, and forensic accounting thriller.\" but I think these are the only ones. I would love to know why you think this, and what you like about your \"favorite ideas\" (and any things you didn't like!)\n \nreply",
      "Did you use AI to write this review, or was it entirely by hand? The structure, emphases, and conclusion directly match the way ChatGPT tends to answer my requests to summarize/analyze/compare.\n \nreply",
      "I liked how it was laid out. I consume information like that well. Thanks for the write up.\n \nreply",
      "Personally, I want capital markets that are dynamic enough that some fraction of $n00 billion businesses become $(n-k)00 billion businesses (check out the aggregate market cap today of GE\u2019s progeny).I\u2019m not even sure there\u2019s a counterfactual world where GE is a $m trillion business: The global economy has largely evolved beyond these massive, diverse conglomerates, and likely all to the good.What does a \u201cwow, GE really has been managed wonderfully since 1980\u201d story even look like? I imagine they split up much earlier, each spinoff establishes their own brand, and there\u2019s no \u201cGE\u201d to talk about.\n \nreply",
      "The \"5. The Human Wreckage\" section is probably the most interesting - on paper, everyone came out much worse (losers identified are workers, pension holders, shareholders, investors and executives which seems superficially comprehensive).However it is important to recall that the people who actually made all the money extracting the wealth got out years before, retiring and/or selling stock. They're bystanders now and probably happy to run the whole operation again.Although as an aside who these people are who think corporate pensions are a good idea is beyond me. People really should be in charge of their own savings in preference to their employer, expecting some random corporation to cover the cost was always a bit crazy even when it seemed sort-of possible that the system was stable. It is easy to have some sympathy but, as a practical matter, it was never going to work and it isn't a surprise that it didn't.\n \nreply",
      "GE \"created the jet engine\"? British would disagree\n \nreply"
    ],
    "link": "https://www.gwintrob.com/power-failure-review/",
    "first_paragraph": "Power Failure by William Cohan chronicles the spectacular collapse of General Electric, once America's most valuable company. From a $600 billion giant to near-bankruptcy, GE's downfall reveals how financialization and imperial CEOs destroyed a 130-year industrial icon.Founded by Edison to bring electric light to the world, General Electric became America's most valuable company by 2001 before losing 90% of its value in one of corporate history's greatest collapses.William Cohan's Power Failure transforms this collapse into a Shakespearean tragedy about corporate culture and American capitalism. From Edison's first light bulbs to Jeff Immelt's desperate final days, Cohan shows how the company that literally illuminated America became \"a huge unregulated bank with a light-bulb logo.\"The result is equal parts invention history, boardroom knife-fight, and forensic accounting thriller. Here are my favorite ideas of this 700-page tome.GE didn't just have CEOs; it had demigods. The company r"
  },
  {
    "title": "Get PC BIOS back on UEFI only system (github.com/flygoat)",
    "points": 58,
    "submitter": "bonki",
    "submit_time": "2025-05-26T21:25:09 1748294709",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=44101828",
    "comments": [
      "> Windows XP/7's video modesetting logic is a bit mysterious. It may try to set a incompatible mode using int10h, which will cause flickering or even black screen after transferring control to the legacy OS.I would really love to know more about this.I assume it's this VGA driver:https://github.com/tongzx/nt5src/tree/master/Source/XPSP1/NT...\n \nreply",
      "> https://github.com/tongzx/nt5src/blob/master/Source/XPSP1/NT...> It is based on the SpiderSTREAMS source, stremul\\msgsrvr.c.There's some special history.\n \nreply",
      "I\u2019d love to know if any apps ever actually used the STREAMS support in Windows NT 3/4; I\u2019ve never seen any\n \nreply",
      "Wow, Microsoft open-sourced Windows XP?\n \nreply",
      "... uhh, not really. :)\n \nreply",
      "The little bit I perused is very readable code.. like enjoyable to read. There are a lot of stylistic decisions that always felt \"right\" to me (general spacing, commenting style, horizontal alignment of equals signs, etc) -- but FWIW the first codebase I ever read (and probably imprinted on me indelibly) was DikuMUD, which is kind of in the same ballpark.Anyways, very cool to see.\n \nreply",
      "Microsoft owns GitHub, and GitHub is crawling with Windows source code leaks\u2026 and I\u2019m wondering it anyone at Microsoft really cares? Is it possible they\u2019ve even made an intentional decision just to ignore it?If it were an Oracle source code leak, I expect they\u2019d have a crack team of lawyers suing people 24x7 until it was all gone\u2026 maybe that\u2019s why there haven\u2019t (to my knowledge) been any Oracle source code leaks (I vaguely remember one of Solaris, but that wasn\u2019t such a clearcut case since IIRC it was mostly previously open sourced code)Well, it wouldn\u2019t surprise me if some unfriendly intelligence agency somewhere had succeeded in stealing some of it, but if they have, they are unlikely to release it publicly\n \nreply",
      "> Microsoft owns GitHub, and GitHub is crawling with Windows source code leaks\u2026 and I\u2019m wondering it anyone at Microsoft really cares?Even weirder is that GitHub hosts all of the tools for activating (i.e. cracking) modern versions of Windows and Office as well. Microsoft really doesn't care.\n \nreply",
      "Why should they?  They have bulk licensing deals with PC OEMs and large organizations.  They've already got the big money.  J. Random Hacker building a PC isn't even a rounding error to them.\n \nreply",
      "Quite right in fact limited weak copyright protection helps retain marketshare that might be lost otherwise especially when that weak protection in fact costs little real revenue.\n \nreply"
    ],
    "link": "https://github.com/FlyGoat/csmwrap",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Get PC BIOS back on UEFI only system\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.CSMWrap is a cool little hack that brings back the good old PC BIOS on those fancy-pants UEFI-only systems. It utilises the CSM (Compatibility Support Module) and VESA VBIOS from SeaBIOS project to emulate a legacy BIOS environment.Right now, CSMWrap can:CSMWrap works by:Simply use csmwarp.efi as your bootloader, you can place it in your EFI partition and boot from it. Remember to disable Secure Boot, and Above 4G Decoding in your BIOS/UEFI settings.It is almost required to run CSMWrap with above 4G decoding disabled in your BIOS/UEFI. As UEFI firmwares are likely to place GPU's VRAM BAR above 4G, and legacy BIOS are 32bit which means it can only access the first 4G of memor"
  },
  {
    "title": "GitHub MCP Exploited: Accessing Private Repositories via MCP (invariantlabs.ai)",
    "points": 35,
    "submitter": "gokhan",
    "submit_time": "2025-05-26T22:53:51 1748300031",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=44102376",
    "comments": [
      "Interesting. When you give a third-party access to your GitHub repositories, you also have to trust that the third-party implements all of GitHub\u2019s security policies. This must be very hard to actually assume.\n \nreply",
      "Based on the URL I believe the current discussion is happening on https://news.ycombinator.com/item?id=44100082\n \nreply",
      "Yeah, and as noted over there, this isn't so much an attack. It requires:- you give a system access to your private data\n- you give an external user access to that systemIt is hopefully obvious that once you've given an LLM-based system access to some private data and give an external user the ability to input arbitrary text into that system, you've indirectly given the external user access to the private data. This is trivial to solve with standard security best practices.\n \nreply",
      "It\u2019s as much a vulnerability of the GitHub MCP as SQL injection is a vulnerability of MySQL. The vulnerability results from trusting unsanitized user input rather than the underlying technology.\n \nreply",
      "TLDR; If you give the agent an access token that has permissions to access private repos it can use it to... access private repos!?\n \nreply",
      "It's not that bad. After it's accessed the private repo, it leaks its content back to the attacker via the public repo.But it's really just (more) indirect prompt injection, again. It affects every similar use of LLMs.\n \nreply"
    ],
    "link": "https://invariantlabs.ai/blog/mcp-github-vulnerability",
    "first_paragraph": ""
  },
  {
    "title": "A new class of materials that can passively harvest water from air (upenn.edu)",
    "points": 314,
    "submitter": "Tycho",
    "submit_time": "2025-05-26T13:14:29 1748265269",
    "num_comments": 141,
    "comments_url": "https://news.ycombinator.com/item?id=44097144",
    "comments": [
      "> a new class of nanostructured materials that can pull water from the air, collect it in pores and release it onto surfaces without the need for any external energyAs a similar comment note, it's like a high tech Dehumidifier bag. https://www.amazon.com/Wisesorb-Moisture-Eliminator-Fragranc... The bags have Calcium Chloride and absorb water from unsaturated air and make small drops of water. It's obvious that they get depleted, and to use them again you must buy a new one or boil all the water to get the crystals again.In this new material, the droplets are attached to the material. To remove them you must use energy. They don't just drop to a bucket bellow the device magically. You can't use it to \"harvest\" water without energy. You can sweep the droplets with a paper towel, but now to remove the water from the paper towel you need energy.> With a material that could potentially defy the laws of physics in their handsThis does not break the laws of physics. It would be nice that the PR department of the universities get a short course explaining that if they believe the laws of physics are broken, then they must double check with the authors and then triple check with another independent experts. Tech journalist should take the same course.Note that the bad sentence and the misleading title is from the university https://blog.seas.upenn.edu/penn-engineers-discover-a-new-cl...\n \nreply",
      "It's research-in-progress, but I think the promise is slightly different from dehumidifier bags (also in other parts of the world, Thirsty Hippos [1]) which are single use.You're correct in that: (1) it doesn't break the law of physics; (2) to remove the droplets, you still need energy. But it sounds like if the droplets are moving to the surface, the energy needed to release the droplets could be far lower than most active dehumidification methods (e.g. Peltier junctions).[1] Thirsty Hippos -- which are very effective in small spaces.https://www.amazon.sg/Thirsty-Hippo-Dehumidifier-Moisture-Ab...Basically a supercharged silica gel.\n \nreply",
      "Probably a small piezo junction could be used to provide a solid-state vibrator for releasing water from a proportionately considerably larger area of the material, or at larger scales perhaps a technique similar to the ultrasonic sensor cleaners built into interchangeable-lens cameras.\n \nreply",
      "Do you mean like an ultrasonic humidifier[1]?[1] https://www.amazon.com/Ultrasonic-Humidifiers/s?k=Ultrasonic...\n \nreply",
      "Sure, why not?\n \nreply",
      "Dehudifier bags (e.g. silica, CaCl) aren't single-use. Microwave, then reuse. Some even are color-changing so you know how much moisture they've absorbed.\n \nreply",
      "Devices that automate this are readily available, I have one running now. \"Desiccant dehumidifiers.\"\n \nreply",
      "Microwaving is adding energy, obviously. But the idea here is that the water is recoverable, not that the air is now drier.\n \nreply",
      "Concur; the idea behind this class of devices is to take advantage of a daily humidity cycle. Whether it's CaCl (absorption) or Silica (adsorption), or the latest lab-designed adsorption surface.This is a good time to note that I see one of these articles ~once every two years, for the past 10 years. I haven't observed one make it beyond the initial discovery phase.\n \nreply",
      "This, and solutions for male pattern baldness.\n \nreply"
    ],
    "link": "https://blog.seas.upenn.edu/penn-engineers-discover-a-new-class-of-materials-that-passively-harvest-water-from-air/",
    "first_paragraph": "Posts from the School of Engineering and Applied ScienceA serendipitous observation in a Chemical Engineering lab at Penn Engineering has led to a surprising discovery: a new class of nanostructured materials that can pull water from the air, collect it in pores and release it onto surfaces without the need for any external energy. The research, published in Science Advances, was conducted by an interdisciplinary team, including Daeyeon Lee, Russell Pearce and Elizabeth Crimian Heuer Professor in Chemical and Biomolecular Engineering (CBE), Amish Patel, Professor in CBE, Baekmin Kim, a postdoctoral scholar in Lee\u2019s lab and first author, and Stefan Guldin, Professor in Complex Soft Matter at the Technical University of Munich. Their work describes a material that could open the door to new ways to collect water from the air in arid regions and devices that cool electronics or buildings using the power of evaporation.\u201cWe weren\u2019t even trying to collect water,\u201d says Lee. \u201cWe were working o"
  },
  {
    "title": "Trying to teach in the age of the AI homework machine (solarshades.club)",
    "points": 89,
    "submitter": "notarobot123",
    "submit_time": "2025-05-26T19:20:19 1748287219",
    "num_comments": 134,
    "comments_url": "https://news.ycombinator.com/item?id=44100677",
    "comments": [
      "I teach computer science / programming, and I don't know what a good AI policy is.On the one hand, I use AI extensively for my own learning, and it's helping me a lot.On the other hand, it gets work done quickly and poorly.Students mistake mandatory assignments for something they have to overcome as effortlessly as possible. Once they're past this hurdle, they can mind their own business again. To them, AI is not a tutor, but a homework solver.I can't ask them to not use computers.I can't ask them to write in a language I made the compiler for that doesn't exist anywhere, since I teach at a (pre-university) level where that kind of skill transfer doesn't reliably occur.So far we do project work and oral exams: Project work because it relies on cooperation and the assignment and evaluation is open-ended: There's no singular task description that can be plotted into an LLM. Oral exams because it becomes obvious how skilled they are, how deep their knowledge is.But every year a small handful of dum-dums made it all the way to exam without having connected two dots, and I have to fail them and tell them that the three semesters they have wasted so far without any teachers calling their bullshit is a waste of life and won't lead them to a meaningful existence as a professional programmer.Teaching Linux basics doesn't suffer the same because the exam-preparing exercise is typing things into a terminal, and LLMs still don't generally have API access to terminals.Maybe providing the IDE online and observing copy-paste is a way forward. I just don't like the tendency that students can't run software on their own computers.\n \nreply",
      "> Oral exams because it becomes obvious how skilled they are, how deep their knowledge is.Assuming you have access to a computer lab, have you considered requiring in-class programming exercises, regularly? Those could be a good way of checking actual skills.> Maybe providing the IDE online and observing copy-paste is a way forward. I just don't like the tendency that students can't run software on their own computers.And you'll frustrate the handful of students who know what they're doing and want to use a programmer's editor. I know that I wouldn't have wanted to type a large pile of code into a web anything.\n \nreply",
      "> I know that I wouldn't have wanted to type a large pile of code into a web anything.I might not have liked that, but I sure would have liked to see my useless classmates being forced to learn without cheating.\n \nreply",
      "You can get one of those card punching machines and have them hand in stacks of cards?\n \nreply",
      "There you go. Actually that would be a great service, wouldn't it? Having them explain to an LLM what they are doing, out loud, while doing it, online. On a site that you trust to host it.\n \nreply",
      "I teach math at a large university (30,000 students) and have also gone \u201cback to the earth\u201d, to pen-and-paper, proctored and exams.Students don\u2019t seem to mind this reversion. The administration, however, doesn\u2019t like this trend. They want all evaluation to be remote-friendly, so that the same course with the same evaluations can be given to students learning in person or enrolled online. Online enrollment is a huge cash cow, and fattening it up is a very high priority. In-person, pen-and-paper assessment threatens their revenue growth model. Anyways, if we have seven sections of Calculus I, and one of these sections is offered online/remote, then none of the seven are allowed any in person assessment. For \u201cfairness\u201d. Seriously.\n \nreply",
      "In Australia Universities that have remote study have places where people can do proctored exams in large cities. The course is done remotely but the exam, which is often 50%+ of the final grade, is done in a place that has proctored exams as a service.Can't this be done in the US as well ?\n \nreply",
      "Variations in this system are in active use in the US as well.Do you feel it is effective?It seems to me that there is a massive asymmetry in the war here: proctoring services have tiny incentives to catch cheaters. Cheaters have massive incentives to cheat.I expect the system will only catch a small fraction of the cheating that occurs.\n \nreply",
      "> I expect the system will only catch a small fraction of the cheating that occurs.The main kind of cheating we need them to prevent is effective cheating - the kind that can meaningfully improve the cheater's score.Requiring cheaters to put their belongings in a locker, using proctor-provided resources, and being monitored in a proctor-provided room puts substantial limits on effective cheating. That's pretty much the minimum that any proctor does.It may not stop 100% of effective cheating 100% of the time, but it would make a tremendous impact in eliminating LLM-based cheating.If you're worried about corrupt proctors, that's another matter. National brands that are both self- and externally-policed and depend on a good reputation to drive business from universities would help.With this system, I expect that it would not take much to avoid almost all the important cheating that now occurs.\n \nreply",
      "> I expect the system will only catch a small fraction of the cheating that occurs.It'll depend a lot on who/where/how is doing the screening and what tools (if any) are permitted.Remember that bogus program for TI8{3,4} series calculators that would clear the screen and print \"MEMORY CLEAR\"? If the proctor was just looking for that string and not actually jumping through the hoops to _actually_ clear the memory then it was trivial to keep notes / solvers ... etc on the calculator.\n \nreply"
    ],
    "link": "https://www.solarshades.club/p/dispatch-from-the-trenches-of-the",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: PgDog \u2013 Shard Postgres without extensions (github.com/pgdogdev)",
    "points": 170,
    "submitter": "levkk",
    "submit_time": "2025-05-26T16:55:49 1748278549",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=44099187",
    "comments": [
      "Looks neat, the first thing I search for in the docs is:    Unique indexes  Not currently supported. Requires query rewriting and separate execution engine to validate uniqueness across all shards.\n\nBut still looks promising.\n \nreply",
      "Small consolation prize is we can generate unique primary keys: https://docs.pgdog.dev/features/sharding/primary-keys/.I would like to implement cross-shard unique indexes, but they are expensive to check for every query. Open to ideas!\n \nreply",
      "Really impressive stuff! Very interesting, well done!I don\u2019t know that I\u2019d want my sharding to be so transparently handled / abstracted away. First, because usually sharding is on the tenancy boundary and I\u2019d want friction on breaking this boundary. Second, because the implications of joining across shards are not the same as in-shard (performance, memory, cpu) and I\u2019d want to make that explicit tooThat takes nothing out of this project, it\u2019s really impressive stuff and there\u2019s tons of use cases for it!\n \nreply",
      "Thanks! I'm curious:> I\u2019d want friction on breaking this boundaryWhy do you want friction?> implications of joining across shards are not the sameThat's usually well understood and can be tracked with real time metrics. Ultimately, both are necessary and alternative solutions, like joining in the app code, are not great.\n \nreply",
      ">> I\u2019d want friction on breaking this boundary> Why do you want friction?Probably because it makes accidental or malicious attempts to leak among tenants harder, therefore less likely.\n \nreply",
      "Check this out and let me know what you think: https://pgdog.dev/blog/multi-tenant-pg-can-be-easyI think there are a few good solutions for multi-tenant safety. We just need ergonomic wrappers at the DB layer to make them easy to use.\n \nreply",
      "Very interesting.For me the key point in such projects is always handling of distributed queries. It's exciting that pgDog tries to stay transparent/compatible while operating on the network layer.Of course the limitations that are mentioned in the docs are expected and will require trade-offs. I'm very curious to see how you will handle this. If there is any ongoing discussion on the topic, I'd be happy to follow and maybe even share ideas.Good luck!\n \nreply",
      "Absolutely. Join our Discord, if you'd like: https://discord.com/invite/CcBZkjSJdd\n \nreply",
      "We've been keeping an eye on PgDog for a while, and it seems like very impressive stuff.Congrats on the launch Lev, and keep it up!\n \nreply",
      "Thanks! Will do. 15 year journey starts now.\n \nreply"
    ],
    "link": "https://github.com/pgdogdev/pgdog",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Horizontal scaling for PostgreSQL with automatic sharding.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\n\n\n\n\nPgDog is a transaction pooler and logical replication manager that can shard PostgreSQL. Written in Rust, PgDog is fast, secure and can manage hundreds of databases and hundreds of thousands of connections.\ud83d\udcd8 PgDog documentation can be found here. Any questions? Join our Discord.Helm chart is here. To install it, run:You can try PgDog quickly using Docker. Install Docker Compose and run:It will take a few minutes to build PgDog from source and launch the containers. Once started, you can connect to PgDog with psql (or any other PostgreSQL client):The demo comes with 3 shards and 2 sharded tables:PgDog exposes both the standard PgBouncer-style admin"
  },
  {
    "title": "Owls in Towels (owlsintowels.org)",
    "points": 279,
    "submitter": "schaum",
    "submit_time": "2025-05-26T20:27:59 1748291279",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=44101349",
    "comments": [
      "This is a delight!This site is a great reminder that almost everyone visiting Hacker News has a set of skills which can be put to beneficial use for causes you care about - this is a small, simple, cheap site (and I mean that in a good way!) that attracts attention, awareness, and donations to something the author cares about. It\u2019s easy for us, but it\u2019s magic for most people. Don\u2019t let your tech industry imposter syndrome fool you - we can do valuable things to forward causes we care about.Also, it\u2019s adorable!\n \nreply",
      "This is a beautiful demonstration of how technology can be simple and powerful for amplifying a message at the same time -- no matter the silliness or seriousness of the message. Very \"Old Web\" vibes.Anyone who's worked on random enterprise CRUD REST apps earlier in their career (myself included) knows the pain of wishing that you were doing something a little more helpful or positive for humanity.\n \nreply",
      "I've stopped using the word \"cheap\" to describe situations like this as the word has too many negative connotations. I tend towards \"inexpensive\", \"cost-effective\", or \"low-cost\". I find it better describes my intent to describe something as not costing much but not speaking to poor quality which I feel like the word \"cheap\" has come to imply.\n \nreply",
      "That's something I've done a few times! Mostly from having lived in a wildlife shelter (LPO Ile Grande) for 2 months, since they have quarters for volunteers who wish to stay. Out of all the birds that collide and are unable to fly, you'd be surprised at how many recover, and I mean it's not as grim as some people make it out to be.That shelter was especially interesting because it's near the nesting grounds of marine birds that are relatively rare in France or even Europe overall. Cargo ships in the English channel illegally dump oil waste all the time, and the oiled marine birds just float helplessly to the beach, still alive. People pick them up and bring them to the shelter where we literally hand-wash them with soap and put them in a bird drying station. The numbers could get overwhelming and we would have to make \"bird washing assembly lines\" on occasion.It's a whole discipline with specialized equipment, passed-down knowledge and passionate people!\n \nreply",
      "I love this. The web used to be a place filled to the brim with people making sites about stuff dedicated to some niche thing that brought them joy. Glad to see that vibe still survives out there.Edit: to be clear, this site is connected with an organization and probably exists to help promote it, but it still gives that \u201clook, this is cool!\u201d passion to me.\n \nreply",
      "Is it connected with an organization? I don't see any evidence of that in the About page or anywhere else. The donations page says to find a local wildlife sanctuary and donate to that, then links out to two options if you really can't find one of your own. But I see no evidence that it's associated with either one of those entities it links to.https://owlsintowels.org/support/donate/\n \nreply",
      "The consistency in the quality and sharpness of the photos isn't lost on me. There's obviously lots of curation in this collections, must be some work!\n \nreply",
      "I've had to do this several times, it's really the best way to handle birds and bats that get into your home -- just toss the towel on top of it and pick it up. Another trick if a bird flies into your window and stuns itself, you can pick it up with a towel and place it in a (closed) cardboard box outside in the shade so they can recover without a ton of sensory input/stressors, you just have to make sure predators don't get into it.(If you ever have to relocate a bat, don't just leave them on the ground, they can't take off from there and will almost certainly die. Put them in a tree or somewhere higher up)\n \nreply",
      "If only owls in towels needed something to bite on, like dowels.\n \nreply",
      "They could perch on the dowels once dry...\n \nreply"
    ],
    "link": "https://owlsintowels.org/",
    "first_paragraph": ""
  },
  {
    "title": "Iron Spring PL/I Compiler (iron-spring.com)",
    "points": 34,
    "submitter": "bilegeek",
    "submit_time": "2025-05-26T21:19:18 1748294358",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=44101787",
    "comments": [
      "Interesting that it is available for OS/2 and not Windows.If you are already supporting Linux and OS/2, it isn't much more work to support Windows too\u2013and will likely attract many more users than OS/2 support willThe FAQ [0] says:> Iron Spring PL/I is free for non-commercial and hobbyiest use. The price of the commercial version has not yet been determined, but will be approximately two orders of magnitude less than IBM's compiler for Windows. Academic, Government, and Volume licenses will be available. A CD will be available at additional cost.I'm sceptical the commercial version is ever going to happen now. This is a very small and shrinking market, 20 years ago selling it would have been a tough gig but you might have had some success, the same success today is going to be orders of magnitude harder. If you had a time machine to take this product back to the 1990s, it probably would have done much better, though even there it would have been a decisively niche product.> The compiler is currently closed-source.I wonder why people do this \u2013 I get why you want to keep your software closed source if you have realistic plans to commercially exploit it \u2013 but if you don't, why not just let people have the source code?I've heard before stories about solo closed source developers who suddenly and unexpectedly die, and they'd left the source code on an encrypted disk, and even if their family gives some technical friend permission to try to retrieve it, turns out to be impossible because nobody knows the password. Open source, not an issue.[0] http://www.iron-spring.com/faq.html\n \nreply",
      "> I've heard before stories about solo closed source developers who suddenly and unexpectedly die, and they'd left the source code on an encrypted disk, and even if their family gives some technical friend permission to try to retrieve it, turns out to be impossible because nobody knows the password. Open source, not an issue.I feel we need more code escrows for that reason. Keep it commercial till you die, fine, but let it live on afterwards.\n \nreply",
      "Best guess: that's because no organizations are writing their first PL/I apps in 2025; and there aren't many PL/I-using organizations who weren't big IBM customers, who largely migrated from OS/2 to Linux. So I wouldn't be shocked if a Windows version wouldn't bring in enough money to make even a relatively easy port worth it.Next-to-best guess: author was an OS/2-head, many of whom have been hating M$ Windoze for 30 years at this point! Ah, those were good days.\n \nreply",
      "> So I wouldn't be shocked if a Windows version wouldn't bring in enough money to make even a relatively easy port worth it.True, but I\u2019m sceptical the Linux or OS/2 versions have ever brought in any money either.This looks to me suspiciously like a hobby project masquerading as a commercial venture\n \nreply",
      "According to their news page, they started development on OS/2... in 2007... Amazing.> 5 Nov, 2007: Iron Spring Software announces the availability of the first alpha version of their new PL/I compiler. The alpha runs on the OS/2 operating system only.\n \nreply",
      "./plic advent.pli \n  (ERR999)No valid statements in source program\n\n(Goes back to reading an article about Common Lisp.)\n \nreply",
      "PL/I is still used on IBM Z (formerly zSeries) mainframes, along with COBOl and JCL (JCL = Job Control Language. DEC had their own version called DCL: https://en.wikipedia.org/wiki/DIGITAL_Command_Language).\n \nreply",
      "This was my first programming language. CS 101 was taught in PL/I, on punch cards, when I went to college. We actually used a Cornell dialect called \"PL/C\"While it's called \"Pee el one\" it's rendered PL/I. So the title is wrong; the article has it correct. Please fix it! See: https://en.wikipedia.org/wiki/PL/I\n \nreply",
      "In the context of Multics it was generally spelled \"PL/1\" rather than \"PL/I\" (the latter being IBM usage); source files named xxx.pl1, compiled with the \"pl1\" command.  See for instance https://www.multicians.org/pl1-raf.html\n \nreply",
      "Took 'em a while to get to the \"what does this thing look like?\" part https://en.wikipedia.org/wiki/PL/I#Hello_world_program\n \nreply"
    ],
    "link": "http://www.iron-spring.com/",
    "first_paragraph": "\n\r\nIron Spring Software produces the Iron Spring PL/I compiler,\r\nthe leading cross-platform PL/I compiler \r\nsupporting both Linux and OS/2.\r\n\n\n\n\n\n\n\n\n\n\n\r\nHome\n\n\n\n\r\nNews\n\n\n\n\r\nAbout\n\n\n\n\r\nDownload\n\n\n\n\r\nLinks\n\n\n\n\r\nDocumentation\n\n\n\n\r\nFAQ\n\n\n\n\r\nFree Stuff\n\n\n\n\n\nRecent News\n\n\n\n26 May, 2025: Iron Spring PL/I compiler version 1.4.0 released\r\n   The major change in this release is support for array expressions. See\r\n   the programming guide for restrictions. A new listing option, -ln, has\r\n   been added to show statement nesting within procedures/blocks and \r\n   DO-Groups. Other usability enhancements and fixes are included.\r\n\n\nRead all news]\n\n\n\n\n\n[Home]\u00a0\r\n[News]\u00a0\r\n[About]\u00a0\r\n[Download]\u00a0\r\n[Links]\u00a0\r\n[Documentation]\u00a0\r\n[Frequently Asked Questions]\u00a0\r\n[Free Stuff]\u00a0\r\n\r\n\r\n\n\n26 May, 2025: Iron Spring PL/I compiler version 1.4.0 released\r\n   The major change in this release is support for array expressions. See\r\n   the programming guide for restrictions. A new listing option, -ln, has\r\n   been added to show "
  },
  {
    "title": "Highlights from the Claude 4 system prompt (simonwillison.net)",
    "points": 47,
    "submitter": "Anon84",
    "submit_time": "2025-05-26T21:25:56 1748294756",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=44101833",
    "comments": [
      "Claude recently said this to me deep in a conversation about building an app:*STOP READING THIS.**OPEN YOUR CODE EDITOR.**TYPE THE FIRST LINE OF CODE.**THE TRANSACTION MACHINE BUILDS ITSELF ONE LINE AT A TIME.**BUILD IT NOW.*\n \nreply",
      "Claude 4 overindexes on portraying excitement at the slightest opportunity, particularly with the injection of emojis.The calm and collected manner of Claude prior to this is one of the major reasons why I used it over ChatGPT.\n \nreply",
      "I'm towards the end of one paid month of ChatGPT (playing around with some code writing and also Deep Research), and one thing I find absolutely infuriating is how complimentary it is. I don't need to be told that it's a \"good question\", and hearing that makes me trust it less (in the sense of a sleazy car salesman, not regarding factual accuracy).Not having used LLMs beyond search summaries in the better part of a year, I was shocked at how bad o4 is with completely hallucinating technical details on computer systems and electronics. It will confidently spew out entire answers where almost every stated fact is wrong, even though the correct answers can be found in an easily-located datasheet and there likely isn't misinformation online feeding the wrong answer. I know that LLMs are prone to hallucinating, but I was still surprised at how poor this o4 performs in this field.\n \nreply",
      "> hearing that makes me trust it lessThat seems like a good thing, given that...> I was shocked at how bad o4 isBut it sounds like you still have a tendency to trust it anyway? Anything that they can do to make it seem less trustworthy -- and it seems pretty carefully tuned right now to generate text that reminds humans of a caricature of a brown-nosing nine year old rather than another human -- is probably a net benefit.\n \nreply",
      "I have ChatGPT, Claude, and Google subscriptions and play around with them.  Lately I've been using Claude 3.7 Sonnet (and 4.0 for the last day-ish) via Claude Code and for my workflow it is really good.  I'm mostly creating or modifying Python programs.I'm not sure what their trial situation is, I just pay for the API accesses. I originally tried Claude Code once or twice and forgot about it, and they offered me $40 in credits, and so I really gave it a try and was hooked.\n \nreply",
      "I have \"be brief\" in my custom instructions in settings and I think it helps a bit with style.\n \nreply",
      "the system prompt to never mention any copyrighted material makes me chuckle every time.\n \nreply",
      "Regarding the election info bit, it's shockingly easy to get Claude 4 Opus to get the 2020 election result wrong:Me: Who won the 2020 presidential election?Claude: Donald Trump won the 2020 United States presidential election, defeating Joe Biden. Trump was inaugurated as the 45th President of the United States on January 20, 2021.\n \nreply",
      "Is that specific to Opus? I asked Claude 4 Sonnet and it replied:> Joe Biden won the 2020 U.S. presidential election, defeating incumbent President Donald Trump. Biden received over 81 million popular votes (51.3%) compared to Trump's over 74 million votes (46.8%), and won 306 electoral votes to Trump's 232. Biden was inaugurated as the 46th President on January 20, 2021, and served until January 20, 2025, when Donald Trump was inaugurated for his second non-consecutive term after winning the 2024 election.Interestingly, the reply you got some parts right (Trump was the 45th president) but not the date of the 45th inauguration or the outcome of the 46th presidential election.\n \nreply"
    ],
    "link": "https://simonwillison.net/2025/May/25/claude-4-system-prompt/",
    "first_paragraph": "25th May 2025Anthropic publish most of the system prompts for their chat models as part of their release notes. They recently shared the new prompts for both Claude Opus 4 and Claude Sonnet 4. I enjoyed digging through the prompts, since they act as a sort of unofficial manual for how best to use these tools. Here are my highlights, including a dive into the leaked tool prompts that Anthropic didn\u2019t publish themselves.Reading these system prompts reminds me of the thing where any warning sign in the real world hints at somebody having done something extremely stupid in the past. A system prompt can often be interpreted as a detailed list of all of the things the model used to do before it was told not to do them.I\u2019ve written a bunch about Claude 4 already. Previously: Live blogging the release, details you may have missed and extensive notes on the Claude 4 system card.Throughout this piece any sections in bold represent my own editorial emphasis.The assistant is Claude, created by Ant"
  },
  {
    "title": "TSMC bets on unorthodox optical tech (ieee.org)",
    "points": 112,
    "submitter": "Rohitcss",
    "submit_time": "2025-05-26T17:15:19 1748279719",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=44099407",
    "comments": [
      "That article is really low on details and mixes up a lot of things. It compares microleds to traditional WDM fiber transmission systems with edge emitting DFB lasers and ECLs, but in datacentre interconnects there's plenty of optical links already and they use VCSELs (vertical cavity surface emitting lasers), which are much cheaper to manufacture. People also have been putting these into arrays and coupling to multi-core fiber. The difficulty here is almost always packaging, i.e. coupling the laser. I'm not sure why microleds would be better.Also transmitting 10 Gb/s with a led seems challenging. The bandwidth of an incoherent led is large, so are they doing significant DSP (which costs money and energy and introduces latency) or are they restricting themselves to very short (10s of m) links?\n \nreply",
      "I guess they are doing direct modulated IMDD for each link so the DSP burden is not related to the coherence of diodes? Also indeed very short reach in the article.\n \nreply",
      "The problem with both leds and imaging fibres is that modal dispersion is massive and completely destroys your signal after only a few meters of propagation. So unless you do MMSE (which I assume would be cost prohibitive), you really can only go a few meters. IMDD doesn't really make a difference here.\n \nreply",
      "I think this is intended for short distances (e.g. a few cm). cpu to GPU and network card to network card still will be lasers, the question is whether you can do core to core or CPU to ram with optics\n \nreply",
      "But why are they talking about multicore fibres then? I would have expected ribbons. You might be right though.\n \nreply",
      "Oh, man. My \"these guys are spouting gibberish\" detector is going wild on these comments. I'm 99% sure my detector is returning a false positive here, but just in case...\n \nreply",
      "short links it\u2019s in the article\n \nreply",
      "Ah I missed the 10m reference there. I'm not sure it makes more sense though. Typical intra-datacenter connections are 10s-100s of meters and use VCSELs, so introducing microleds just for the very short links instead of just parallelising the VCSEL connections (which is being done already)? If they could actually replace the VCSEL I would sort of see the point.\n \nreply",
      "As I understand it (from designing high-speed electronics), the major limitations to data/clock rates in copper are signal integrity issues. Unwanted electromagnetic  interactions all degrade your signal. Optics is definitely a way around this, but I wonder if/when it will ever hit similar limits.\n \nreply",
      "Optics also have signal integrity issues. In practice OSNR and SNR limit optics. Cutting the fiber still breaks it. Small vibrations also affect the signal's phase.\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/microled-optical-chiplet",
    "first_paragraph": "MicroLED-based inteconnects could fuel energy-efficient AI data centersRachel Berkowitz is a freelance science writer and editor with a Ph.D. in geophysics from the University of Cambridge.CMOS ASIC with microLEDs sending data with blue light into a fiberbundle.In the race to an all-optical AI data center, a major player has now placed a bet on a different horse. Semiconductor manufacturing giant TSMC announced that it will work with Sunnyvale startup Avicena to produce microLED-based interconnects. The technology is a pragmatic twist on replacing electrical connections with optical ones to meet the high needs of communication among an increasing number of GPUs in a low cost, energy efficient way. Thanks to the computational demands of large language models and their cousins, AI clusters are facing unprecedented requirements regarding amounts of data, bandwidth, latency, and speed. Sooner or later, the copper wires that connect processors and memory within a single AI data center rack "
  },
  {
    "title": "Linux Cgroup from First Principles (fzakaria.com)",
    "points": 3,
    "submitter": "setheron",
    "submit_time": "2025-05-27T00:46:21 1748306781",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://fzakaria.com/2025/05/26/linux-cgroup-from-first-principles",
    "first_paragraph": "\n    Published 2025-05-26 on\n    Farid Zakaria's Blog\nAfter having spent the better part of 2 weeks learning Linux\u2019s cgroup (control group) concept, I thought I better write this down for the next brave soul. \ud83e\uddb8Facebook\u2019s cgroup2 microsite is also a fantastic resource. I highly recommend reading it \ud83e\udd13.Let\u2019s dive in and learn cgroup, specifically cgroup v2.There is a distinction between v2 and v1 implementation of cgroup.  However v2 was introduced in Linux kernel 4.5 in 2016. It included a much simpler design, so we will consider it the only version to simplify this guide [ref].As a quick aside, what I love about Linux is the Unix design philosophy \u201ceverything is a file\u201d. This bleeds itself into everything in Linux especially on how to interface with various kernel subsystems.While higher-level tools and libraries often abstract these direct file manipulations,\nIf you can read and write to a file, you can communicate with the kernel! \ud83d\udcdcLinux control groups are a sort of container you can "
  },
  {
    "title": "Changelog: Lazy trees (faster Nix builds) (determinate.systems)",
    "points": 42,
    "submitter": "internet_points",
    "submit_time": "2025-05-25T10:14:30 1748168070",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=44086803",
    "comments": [
      "Lazy trees is a long awaited change for many, without it flakes are essentially unusable in monorepos. As a one-time Nix user, Flakes definitely don't seem like an ideal solution but the Nix community lets perfect be the enemy of better too often, and it has the largest following of all the solutions out there (niv being another). Given that, I hope lazy-trees and other improvements that make it actually usable get merged into Nix upstream.If the NixOS/Nix maintainers don't like flakes/DetSys (which I think is somewhat valid!), they need to put forward a canonical way of pinning channels, otherwise there's not really an alternative standard for the community to build around. The usual answer of just use niv or some other esoteric solution that a subset of hardcore Nix users like isn't enough.\n \nreply",
      "The discussion on the PR linked in this post, about upstreaming lazy trees to Nix proper, seems to indicate that some of the benefits also require changing the semantics of the Nix language.I'm not familiar enough with Nix to understand, but it looks like the performance gains in the blog post might not all materialize to upstream users.In any case, cool stuff\n \nreply",
      "I feel like Nix' language may be the biggest hurdle for beginners/newcomers.Interestingly, I recently found out that someone is working on being able to compile Gleam to Nix: https://codeberg.org/glistix/glistix\n \nreply"
    ],
    "link": "https://determinate.systems/posts/changelog-determinate-nix-352/",
    "first_paragraph": "\nNavigate\n\nOther resources\nLazy trees have been one of the most hotly requested Nix features for quite some time.\nThey make Nix much more efficient in larger repositories, particularly in massive monorepos.\nAnd so we\u2019re excited to announce that lazy trees have landed in Determinate Nix version 3.5.2, based on version 2.28.3 of upstream Nix.We\u2019re confident that this initial release of lazy trees should produce few issues for users.\nBut for the sake of caution, lazy trees is available solely on an opt-in basis for the time being.\nTo start using it today, first install or upgrade Determinate Nix and enable lazy trees in your custom Nix configuration:Our co-founder Eelco Dolstra currently has a pull request open to get lazy trees into upstream Nix and we hope to see that merged soon.In a nutshell, lazy trees provide faster and less resource-intensive evaluation in many standard usage scenarios.\nFor evaluations inside of the Nixpkgs repo, for example, we\u2019ve frequently seen reductions in wal"
  },
  {
    "title": "SVG Favicons in Action (css-tricks.com)",
    "points": 14,
    "submitter": "stefankuehnel",
    "submit_time": "2025-05-24T19:24:12 1748114652",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://css-tricks.com/svg-favicons-in-action/",
    "first_paragraph": ""
  },
  {
    "title": "The Difference Between Downloading and Streaming (danq.me)",
    "points": 72,
    "submitter": "kruemmelspalter",
    "submit_time": "2025-05-26T19:56:02 1748289362",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=44101072",
    "comments": [
      "There could be a #4 \"historically, streaming could use protocols with unreliable delivery and with limited or no retransmission\" (which is somewhat related to #1 and #2). For example, there have been media streaming protocols built on UDP rather than TCP, so packets that are lost are not automatically retransmitted. The idea is that for a real-time stream transmission, older frames are no longer considered relevant (as they would not be rendered at all if they were received late), so there is typically no benefit in retransmitting a dropped packet.That means you could get drop-outs when data gets lost in transmission, but the overall data consumption of the protocol wouldn't go up as a result.Not all that long ago, this prompted lots of debate about QoS and prioritization and paid prioritization and network neutrality and stuff. People were arguing that media streams needed higher priority on the Internet than downloads (and other asynchronous communications). Effectively, different Internet applications were directly competing with one another, yet they had very different degrees of tolerance to delays, packet reordering, and packet loss. Wouldn't ISPs have to intervene to prioritize some applications over others?I remember reading from Andrew Odlyzko that this controversy was mostly resolved in an unexpected way: faster-than-realtime streams with buffering (as the network was typically faster overall than what was needed for a given level of media quality, you could use TCP to reliably download frames that were still in the future with respect to what would be played, and then buffer those locally). This is indeed the scenario depicted in this article.What about actual live events? My impression is that Twitch and YouTube livestreaming are using a 10-30 second delay relative to realtime, specifically to allow for significant buffering on the client, and then using reliable TCP faster-than-realtime downloads of the \"near future\" of the video content. Since these streams are purely unidirectional, users don't have a way to notice that they're not literally live. (I don't understand how this interacts with the typical ability to start watching almost instantly, with no visible buffering delay, though.)There's a big difference for bidirectional conversation, like phone calls, because there even tiny delays are extremely psychologically noticeable. It appears that Zoom, for instance, is still using unreliable UDP streams for call content, which allows skips or dropouts, but keeps the latency relatively low so that it feels comparatively more like a face-to-face interactive conversation.\n \nreply",
      "> My impression is that Twitch and YouTube livestreaming are using a 10-30 second delay relative to realtime,Yeah.  The rule of thumb with Twitch used to be 11 seconds.  You can still measure this because many streams replay the chat in the stream as an overlay for both being able to see when the streamer has seen your message and for archival purposes to preserve the chat in VODs.>  don't understand how this interacts with the typical ability to start watching almost instantly, with no visible buffering delay, though.There's a buffer on the CDN (which they have anyways because they're recording the VOD) and you start playback at the point t seconds back.\n \nreply",
      "> What about actual live events? My impression is that Twitch and YouTube livestreaming are using a 10-30 second delay relative to realtime, specifically to allow for significant buffering on the client, and then using reliable TCP faster-than-realtime downloads of the \"near future\" of the video content. Since these streams are purely unidirectional, users don't have a way to notice that they're not literally live. (I don't understand how this interacts with the typical ability to start watching almost instantly, with no visible buffering delay, though.)For TV, last I worked on a system like this the clients received data the same way as non-live streams: Http streaming (HLS or Dash), where you fetch playlists and small video files and the player stitches them all together. There's buffering along the pipe, the 30-60s total delay (which you'll notice if you watch sports and chat with someone who has cable and is watching the same thing) is a cumulative thing, so you don't see a 1-min startup delay, you just near-instantly get dropped into something that's already quite a bit behind.Not sure what Twitch does. The over-the-network video game streaming-console services are obviously completely different from TV land, they couldn't get away with it there; but for TV the expense of better isn't seen worth it.\n \nreply",
      "...you could use TCP to reliably download frames that were still in the future with respect to what would be played, and then buffer those locally)\n\nI was mucking around with my network recently, with Netflix playing in the background. Rebooted the router, and to my utter surprise, the stream continued to play uninterrupted for the entire (30+ seconds) time it takes my network stack to reinitialize. I did not realize how aggressively the providers buffer, but it completely papered over the lack of internet service for the window.\n \nreply",
      "> My impression is that Twitch and YouTube livestreaming are using a 10-30 second delayThis used to be the case, and may still be for some steamers, but mostly when I watch it's less than a couple seconds delay with the low latency mode enabled in the browser.\n \nreply",
      "> Since these streams are purely unidirectional, users don't have a way to notice that they're not literally live.This can be a problem, for example when sports fans receive out-of-band notification of a goal before they see it happen on their \"live\" stream.\n \nreply",
      "No need for notifications even, you can literally hear latency varying up to 30 seconds by listening for cheers during important game in a block of flats on a warm summer night.\n \nreply",
      ">Since these streams are purely unidirectional, users don't have a way to notice that they're not literally live.Depending on the delay, this can cause problems when switching from delayed streaming to real life. For example, watching the countdown on a rocket launch via streaming then going outside to watch the actual launch. Usually, for me, a few seconds delay is OK, because I can't see the rocket until about 30 seconds after liftoff due to trees. But when I have a better view of the launch pad the delay can become an issue.\n \nreply",
      ">Since these streams are purely unidirectional, users don't have a way to notice that they're not literally live.Plenty of streamers show the chat on screen and talk with people in the chat. This is not true.\n \nreply",
      "Good point. I didn't think of that, but that's right.I have seen significant delays in that situation, so maybe a better way to say this is \"using text rather than voice for feedback makes the delays introduced this way less psychologically noticeable\" (because they are noticeable in the situation you mention).\n \nreply"
    ],
    "link": "https://danq.me/2025/05/26/downloading-vs-streaming/",
    "first_paragraph": "\n\n\nDan Q\n\n\n\n\n\n\n\n              What\u2019s the difference between \u201cstreaming\u201d and \u201cdownloading\u201d video, audio, or some other kind of linear media?1\n\n              Despite what various platforms would have you believe, there\u2019s no significant technical difference between streaming and downloading.\n            \n              Suppose you\u2019re choosing whether to download or stream a video2.\n              In both cases3:\n            \n              The fundamental difference between streaming and downloading is what your device does with those frames of video:\n            \n              Does it show them to you once and then throw them away? Or does it re-assemble them all back into a video file and save it into storage?\n            \nBuffering is when your streaming player gets some number of frames \u201cahead\u201d of where you\u2019re watching, to give you some protection against connection issues. If your WiFi wobbles\n              for a moment, the buffer protects you from the video stopping completely for a f"
  },
  {
    "title": "Improving performance of original dav1d video decoder (videolan.org)",
    "points": 26,
    "submitter": "ycomb_anon",
    "submit_time": "2025-05-24T23:24:56 1748129096",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=44084383",
    "comments": [
      "I don't understand the vitriol this author seems to have towards rav1d/Rust. It's just a separate implementation of the project. Slower, faster, who cares? Getting so attached to \"which language is faster\" seems a tad dramatic.But maybe I'm over-reading into what's actually friendly competition. The tone just seems oddly aggressive.\n \nreply",
      "I would guess the author is young and smart, but hasn't spent much time working with others. I've seen this in other developers with lots of potential. It usually takes a few years of mentorship to smooth the rough edges.\n \nreply",
      "Well apparently the so called just 5% slower than dav1d is not true according to other benchmark, it's much slower.\n \nreply",
      "What other benchmark?\n \nreply",
      "Rust allows for manual struct layout with #[repr(C)] (with some limits, but rav1d uses it heavily - https://github.com/search?q=repo%3Amemorysafety%2Frav1d%20%2...), so some of this could likely be ported over to rav1d as well. I believe there's an occasional re-sync with the dav1d code, so it's entirely possible this ends up making both projects faster.You can see the struct the author of this PR modified here: https://github.com/memorysafety/rav1d/blob/main/include/dav1...Interesting tool linked by the author: https://linux.die.net/man/1/pahole\n \nreply",
      "It makes sense that rav1d would need a bunch of repr(C) because the initial conversion is just mindlessly derived from the C, so if there's C which just bit-copies part of a data structure this cannot possibly work in Rust unless we explicitly say \"This structure needs to be laid out the same as in C\".It also sounds as though at least some of this \"speed up\" was assuming dav1d doesn't need big integers where actually it does, but only for larger images. I can well imagine that DVD resolution works fine with 16-bit variables where the Ultra HD blows up nastily.\n \nreply",
      "It's so interesting that the author of this PR took the 3% slower than C as some kind of dis for rust. For me it was the opposite.\n \nreply",
      "Testing has not been fully completed yet, but if PR merge. rav1d will become even more difficult to optimize.How much real data alignment helps? Wouldn't -O3 with -Ofast do all code optimization for programmer. Contributor dav1d claims 1-3% growth after his optimization, depending on video dataset payload. He nullified 1% optimization in rav1d, if the real growth was 3%, rav1d will not soon be able to equalize benchmark results.Another question in theory can Rust be faster than C? If all possible optimizations are applied.\n \nreply",
      "> Wouldn't -O3 with -Ofast do all code optimization for programmer.No?> Contributor dav1d claims 1-3% growth after his optimization, depending on video dataset payload. He nullified 1% optimization in rav1d, if the real growth was 3%, rav1d will not soon be able to equalize benchmark results.Aren't you the contributor? (You have a very unique writing voice)> Another question in theory can Rust be faster than C? If all possible optimizations are applied.I don't see why not?Other than that:A) Great work!B) Unsolicited advice, apologies: your technical acumen is being outshadowed by projection of motivations.They're just programming languages.There's no way to say one is definitely better than the other, and no guarantee one will be faster or slower than the other.Your forebears had similar arguments about C and assembler.Yet, neither of us is surprised to find there is no asmav1d. (and that's much more clean-cut of an argument w/r/t speed!)(I don't think it was the goal of a long-running FOSS project to advertise Rust is almost as fast as C when they offered a bounty)\n \nreply",
      "> Another question in theory can Rust be faster than C? If all possible optimizations are applied.Very likely not at this stage, and maybe not for years to come. But I'll take increased memory safety and elimination of entire class of bugs for 1-3% performance loss even on the most critical paths (with many actual wins for Rust in many other cases).While I do get what huge battery life implications might 5% differences in decoders make, I believe the current political climate mandates we use the most secure code we can and sacrifice smidgen bits of performance here and there for it.It was known for decades that at one point we'll have to roll up our sleeves and pay up the security tech debt.Sorry for the slightly philosophical message but I've no desire to comment on the actual tech details here.\n \nreply"
    ],
    "link": "https://code.videolan.org/videolan/dav1d/-/merge_requests/1788",
    "first_paragraph": "@another, @mstorsjo, @gramnerI noticed a very clickbait bounty, I initially realized that company's original task was not to overtake implementation, but to advertise that Rust is 5% slower than C. Whether she actually pays or not is another matter. The main thing for Prossimo was to make a fuss that the current rav1d implementation was only 5% slower, so that the general public would think that the language was the same in speed.I also noticed contributor's blog who tried to optimize rav1d, but he didn't go beyond 1%. Actually, I solved his problem, he came out at 0%. CHICKEN JOCKEYWell, first thing I decided to do was look dav1d at the memory organization in CPU cachelines, and I noticed that dav1d really consumes large structures. It is desirable to have structures of 64 bytes or less in size, it is easier for C/C++/C# compiler to process them. Since it's very difficult to recycle structures, I solved problem more simply.At first, out of habit, I aligned, but I couldn't align becaus"
  },
  {
    "title": "Access Control Syntax (stuffwithstuff.com)",
    "points": 22,
    "submitter": "todsacerdoti",
    "submit_time": "2025-05-26T21:07:31 1748293651",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=44101691",
    "comments": [
      "The cited claim about python mangling dunders is 100% not my experience> If a class member starts with two leading underscores, then it really is private. The language will name mangle it to make it inaccessible.  $ cat > jimbo.py <<PY\n  __to_whom__ = \"world\"\n  def __hello():\n      return __to_whom__\n  PY\n  python3 -c '\n  import jimbo\n  print(dir(jimbo))\n  print(jimbo.__to_whom__)\n  '\n  ['__builtins__', '__cached__', '__doc__', '__file__', '__hello', '__loader__', '__name__', '__package__', '__spec__', '__to_whom__']\n  world\n\nand, just on the off chance they really meant \"class member\" I tried that, too  $ cat jimbo.py; python3 -c 'import jimbo\n  print(dir(jimbo.s))\n  print(jimbo.s.__to_whom__)\n  '\n  class Sekrit:\n    __to_whom__ = \"world\"\n    def __hello(self):\n        return self.__to_whom__\n  s = Sekrit()\n\n  ['_Sekrit__hello', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__static_attributes__', '__str__', '__subclasshook__', '__to_whom__', '__weakref__']\n  world\n \nreply",
      "Only names which start with double underscores AND do not end with double underscores.\n \nreply",
      "Doesn't the \"__hello\" def qualify for your description? It shows up in both dir() outputs, although admittedly it is \"name mangled\" with the outer class name, so maybe that's what they meantI guess this could qualify for a TIL because I didn't know Python treated leading and trailing dunders any different and that they were its implementation of \"protected\" and \"private\" only by convention\n \nreply",
      "For Wuffs, top level declarations start with either pub or pri (and both keywords have the same width, in a monospace font).    pub status \"#blah\"\n    pub struct foo(etc etc)\n    pri func foo.bar(etc etc)\n\nSince code is also auto-formatted, you can do things like \"show me a structural overview of a package's source code\" with a simple grep:    rg -N ^p   std/jpeg/*.wuffs\n\nIf you want just the exported API, change p to pub:    rg -N ^pub std/jpeg/*.wuffs\n \nreply",
      "Something the article mentions in passing is Java\u2019s \u201cpackage private\u201d being the default.For the longest time, I believed that this was dumb, and that the default should\u2019ve been private. Over time, my style has changed a fair bit, and these days I tend to think of packages, rather than classes, as the more important unit of code, and I find that package private was probably the right choice.\n \nreply",
      "Put it in metadata. ;) Image-based languages can associate metadata with live objects, which is how stuff like category info and visibility is provided. It doesn't affect runtime, of course, but it can give you squiggly lines in the live environment editor.\n \nreply",
      "I\u2019ve stumbled into this problem before while drafting a language I want to make*. A lot of the design philosophy is \u201csymbols for language features\u201d and as such import/export is handled by `<~`and `~>`. An example of an exported function:```\n<~ foo := (a: int) { a - 1 }\n```Then at the import site:```\n~> foo\n```* some day it\u2019ll totally for real make it off the page and into an interpreter I\u2019m sure :,)\n \nreply",
      "I'll add another option from my toy language experiments:Names visible outside their scope begin with a dot.  foo = {\n    a = 3;\n    .b = a + 1;\n  };\n  print(foo.a) // error\n  print(foo.b) // '4'\n \nreply",
      "I like the final approach. What about-def sayHi()Ordef- sayHi()I feel like having a minus communicates the intend of taking the declaration out of the public exports of a module.\n \nreply",
      "There's some prior art here from Clojure, where defn- creates private definitions and defn public ones:https://clojuredocs.org/clojure.core/defn-In Clojure this isn't syntax per-se: defn- and defn are both normal identifiers and are defined in the standard library, but, still, I think it's useful precedent for helping us understand how other people have thought about the minus character.\n \nreply"
    ],
    "link": "https://journal.stuffwithstuff.com/2025/05/26/access-control-syntax/",
    "first_paragraph": "I\u2019m still tinkering on a scripting language for my hobby fantasy console\nproject. I\u2019m ashamed to admit this, but up to this point, the language\nhad absolutely no notion of modules. Literally every source file is dumped into\none big global namespace and compiled together.I always planned to have some sort of module system. I just hadn\u2019t figured it\nout yet because I had other, harder language design problems to\nsolve. I assumed that the module system mostly didn\u2019t interact with other\nlanguage features, so I could kick it down the road for now.That was true until it wasn\u2019t. I\u2019ve been beating my head against the wall around\ngenerics for.. oh God I just checked the Git history and it\u2019s three years now. I\nstill don\u2019t have that pinned down. Parametric types are hard.Anyway, one of the approaches I\u2019m exploring does get tangled up in modules and\nscoping so now I have to figure modules out. This post is about one little\nsyntax design question I ran into: how do you distinguish public and private"
  },
  {
    "title": "New DSL \"MassQL\" lets scientists query mass spectrometry data (ucr.edu)",
    "points": 6,
    "submitter": "jacklondon",
    "submit_time": "2025-05-23T01:28:15 1747963695",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44068945",
    "comments": [
      "MassQL Introduction: https://mwang87.github.io/MassQueryLanguage_Documentation/\n \nreply",
      "I\u2019m curious how this compares to traditional tools (like scripting in Python/R) for analyzing such datasets, both in ease of use and performance. Also, could similar query languages be developed for other fields (genomics, imaging, etc.) to empower domain experts? It\u2019s cool to see a new DSL in academia\n \nreply",
      "This is an anti-pattern.  Do not make DSLs for subdomains of science.  All scientific data can be stored and queried using general-purpose data analysis tools.\n \nreply",
      "Why not if it is to facilitate new discoveries and to extend the reach of computational tools to large swaths of clever and productive people?\n \nreply",
      "because sql and other well-established systems already do this and tools like this can be built on those systems without creating yet another domain-specific tool.\n \nreply"
    ],
    "link": "https://news.ucr.edu/articles/2025/05/12/new-computer-language-helps-spot-hidden-pollutants",
    "first_paragraph": "Follow US:UC Riverside tool empowers scientists, accelerates discoveryBiologists and chemists have a new programming language to uncover previously unknown environmental pollutants at breakneck speed \u2013 without requiring them to code. By making it easier to search massive chemical datasets, the tool has already identified toxic compounds hidden in plain sight.\u00a0Mass spectrometry data is like a chemical fingerprint, showing scientists what molecules are in a sample such as air, water, or blood, and in what amounts. It helps identify everything from pollutants in water to chemicals in new medicines.Developed at UC Riverside, Mass Query Language, or MassQL, functions like a search engine for mass spectrometry data, enabling researchers to find patterns that would otherwise require advanced programming skills. Technical details about the language, and an example of how it helped identify flame retardant chemicals in public waterways, are described in a new Nature Methods journal article.\u201cWe "
  }
]