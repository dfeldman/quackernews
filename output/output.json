[
  {
    "title": "Uv is the best thing to happen to the Python ecosystem in a decade (emily.space)",
    "points": 1071,
    "submitter": "todsacerdoti",
    "submit_time": "2025-10-29T18:57:29 1761764249",
    "num_comments": 628,
    "comments_url": "https://news.ycombinator.com/item?id=45751400",
    "comments": [
      "I gotta say, I feel pretty vindicated after hearing for years how Python\u2019s tooling was just fine and you should just use virtualenv with pip and how JS must be worse, that when Python devs finally get a taste of npm/cargo/bundler in their ecosystem, they freaking love it. Because yes, npm has its issues but lock files and consistent installs are amazingreply",
      "There is nothing I dread more within the general context of software development, broadly, than trying to run other people's Python projects. Nothing. It's shocking that it has been so bad for so long.reply",
      "Never underestimate cultural momentum I guess. NBA players shot long 2 pointers for decades before people realized 3 > 2. Doctors refused to wash their hands before doing procedures. There\u2019s so many things that seem obvious in retrospect but took a long time to become acceptedreply",
      ">NBA players shot long 2 pointers for decades before people realized 3 > 2And the game is worse for it :')reply",
      "This is a fundamental problem in sports.  Baseball is going the same way.  Players are incentivized to win, and the league is incentivized to entertain.  Turns out these incentives are not aligned.reply",
      "Hey and you can use both lanes in a zip merge!reply",
      "They did wash their hands.  Turns out that soap and water wasn't quite enough.  Lister used carbolic acid (for dressing and wound cleaning) and Semmelweis used chlorinated lime (for hand washing).reply",
      "People paid 100x more for their hosting when using aws cloud until they realized they never neded 99.97% uptime for their t-shirt business.  Oh wait too soon.  Save for post for the future.reply",
      "People paid only 100x more than self hosting to use AWS until they realized that they could get a better deal by paying 200x for a service that is a wrapper over AWS but they never have to think about since it turns out that for most businesses that 100x is like 30 bucks a month.",
      "People spent half their job figuring out self hosted infrastructure until they realized they rather just have some other company deploy their website when they make a commit.reply"
    ],
    "link": "https://emily.space/posts/251023-uv",
    "first_paragraph": "ASTRONOMER & SCIENCE COMMUNICATOR Programming23 October 2025 | Reading time: 6 minutesIt\u2019s 2025. Does installing Python, managing virtual environments, and synchronizing dependencies between your colleagues really have to be so difficult? Well\u2026 no! A brilliant new tool called uv came out recently that revolutionizes how easy installing and using Python can be.uv is a free, open-source tool built by Astral, a small startup that has been churning out Python tools (like the excellent linter Ruff) for the past few years. uv can:What\u2019s best is that it can do all of the above better than any other tool, in my opinion. It\u2019s shockingly fast, written in Rust, and works on almost any operating system or platform.uv is straightforward to install. There are a few ways, but the easiest (in my opinion) is this one-liner command \u2014 for Linux and Mac, it\u2019s:or on Windows in powershell:You can then access uv with the command uv. Installing uv will not mess up any of your existing Python installations \u2014 i"
  },
  {
    "title": "Meta and TikTok are obstructing researchers' access to data, EU commission rules (science.org)",
    "points": 118,
    "submitter": "anigbrowl",
    "submit_time": "2025-10-29T22:54:53 1761778493",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=45754165",
    "comments": [
      "Remember that Cambridge Analytica was \"research\" as well. Laws like these sound good on paper, but it's the company that has to deal with the fallout when the data is used improperly. Unless the government can also come up with a fool proof framework for data sharing and enforce adequate protections, it's always going to be better for the companies to just say no and eat the fines.reply",
      "Long story short, this \"research\" and data access wouldn't be allowed under the DSA, because (i) the researcher didn't provide any data protection safeguards, (ii) his university (and their data protection officer) didn't assume legal liability for his research, (iii) his research isn't focused on systemic risks to society.reply",
      "Platforms (reasonably!) do not trust random academic researchers to be safe custodians of user data.  The area of research focus and assumption of liability do not matter.  Once a researcher's copy of data is leaked, the damage is done.reply",
      "As I recall it Cambridge Analytica was a ton of OAuth apps (mostly games and quizzes) requesting all or most account permissions and then sharing this account data (the access for which had been expressly (foolishly) granted by the user) with a third-party data aggregator, namely Cambridge Analytica. Only this re-sharing of data with a third party was against Facebook Terms of Service.I would not classify Cambridge Analytica as research. They were a data broker that used the data for political polling.reply",
      "From https://en.wikipedia.org/wiki/Cambridge_Analytica> The New York Times and The Observer reported that the company had acquired and used personal data about Facebook users from an external researcher who had told Facebook he was collecting it for academic purposes.reply",
      "link from sentence that you copy pasted https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Ana...The data was collected through an app called \"This Is Your Digital Life\", developed by data scientist Aleksandr Kogan and his company Global Science Research in 2013.[2] The app consisted of a series of questions to build psychological profiles on users, and collected the personal data of the users' Facebook friends via Facebook's Open Graph platform.[2] The app harvested the data of up to 87 million Facebook profilesreply",
      "This \"research\" and data access wouldn't be allowed under the DSA, because (i) the researcher didn't provide any data protection safeguards, (ii) his university (and data protection officer) didn't assume legal liability for his research, (iii) his research isn't focused on systemic risks to society.reply",
      "Which is why the EU doesn't use a letter-of-the-law system, and also have an ethics regulation system.So it falls on those misusing the data, unless you knew it would be misused but collected it anyway.Golden rule: Don't need the data? Don't collect it.reply",
      "Republicans and Elon Musk have become very skilled at exerting political influence in the US [1] and Europe [2] through social media in ways the public isn't really aware of. Is this really that far from the goal of Cambridge Analytica of influencing elections without people's knowledge? Is it fine for large online platforms to influence election outcomes? Why wouldn't an online platform be used to this end if that's beneficial for it and there is no regulation discouraging it?[1] https://www.techpolicy.press/x-polls-skew-political-realitie...[2] https://zenodo.org/records/14880275reply",
      "I don't think you get it: the EU has a law that says these researchers need to find casus belli to wrestle the norms of online freedom of speech away from American corporations. Therefore they get to request data on every account that has ever interacted with certain political parties on those platforms, as a treat.reply"
    ],
    "link": "https://www.science.org/content/article/meta-and-tiktok-are-obstructing-researchers-access-data-european-commission-rules",
    "first_paragraph": ""
  },
  {
    "title": "China has added forest the size of Texas since 1990 (yale.edu)",
    "points": 335,
    "submitter": "Brajeshwar",
    "submit_time": "2025-10-28T13:42:44 1761658964",
    "num_comments": 199,
    "comments_url": "https://news.ycombinator.com/item?id=45732800",
    "comments": [
      "Wise to use forests to contain deserts. Problem is that China still plays a big role in importing deforestation-linked commodities and fund overseas projects that exacerbate global loses. There are low tree survival rates and falsified coverage, like the Three-North Shelterbelt program which is plagued by inefficiencies over its 40 years of operation.It's also hard to balance afforestation without causing scarcity of water and displacement of native forest habitats. For example, instances where shrubs are misclassified as forests inflate the report figures. China seems to be the global leader in biodiversity loss, with about 80% of its coral reefs and 73% of its mangroves gone since 1950. Everyone knows their abusive fishing practices, and the millions of tons of plastic pollution into the ocean every year.So, keep up the good environmental efforts, China, and hope you do even better.reply",
      "Honest question, aren't coral reefs also very sensitive to climate change? How much of that loss is because of regional activities and how much is due to global environmental changes?reply",
      "India too has been adding more green cover than ever. Higher CO2 in atmosphere leads to faster growth of forests. But more important factor is urbanization for India. As people move to cities the need to cut down trees goes down.reply",
      "India doesn't do it in an organized way though.You'll read about some 70 year old woman/man in an obscure village who's reforested thousands of acres on their own, or resuscitated a lake (e.g. the lake guy in Bengaluru).But there's little effort to harness their knowledge in a systematic way, add knowledge from others into the knowledge bank, do peer review, and then systematically dispense the knowledge in the form of a kit to environmentalists and bureaucrats across the country. China did this, and that's why they're so successful.reply",
      "Yeah another example of the saying \"India is a disappointment to both optimists and pessimists\".reply",
      "One nice thing about these developing countries is due to the power infrastructure tends to be not very good - which prompts people to take things into their hands and install solar, not to save the planet but to stave off brownouts, and be able to run the AC around the clock to stave off the heat.For residential, solar + batteries straight up beats legacy infra on cost, and with the upcoming cheap sodium batteries, things are only going to get better.reply",
      "Doesn't that put pressure on the cities itself especially the peripheral counties to pave way for housing and concrete roads?reply",
      "Cities tend to expand up. Almost all buildings in Mumbai that are under 5 stories are targeted for \"redevelopment\" i.e. a developer buying it out and building something taller in its place.reply",
      "That is too costly for cities that have cheap and abandoned agricultural land waiting to be deforested and build upon.reply",
      "What does \u201cdeforested\u201d mean?  Isn\u2019t agricultural land already deforested?reply"
    ],
    "link": "https://e360.yale.edu/digest/china-new-forest-report",
    "first_paragraph": ""
  },
  {
    "title": "Crunchyroll is destroying its subtitles for no good reason (daiz.moe)",
    "points": 101,
    "submitter": "Daiz",
    "submit_time": "2025-10-29T23:31:24 1761780684",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=45754509",
    "comments": [
      "This is the result of a ton of research into Crunchyroll's recent subtitle changes that have tanked the service's first-party presentation quality to an all-time low. The article ended up being quite long, so I highly appreciate anyone taking the time to read it in full!reply",
      "Does Netflix really only allow 2 lines of subtitling on screen at a time? That's really stupid.Also I remember when CR killed the Kodi plugin, that irked me enough to stick to DVD imports + fan subs for a while.Finally, Ruri Rocks is such a good show, it got me to resubscribe to CR after not having subbed for years. If they screw with its subs I'm gonna root for this mess to bankrupt CR for good.reply",
      "See for yourself: https://partnerhelp.netflixstudios.com/hc/en-us/articles/215...> 4. Line Treatment> 2 lines maximumreply",
      "I didn't find an answer skimming: are they actively deleting old good subtitles and replacing with low-quality ones (as the title seems to suggest), have they simply changed their process for new subtitles, not spending as much to make nice ones? If they're actively deleting old good ones, that seems malicious.reply",
      "I did find instances of them actively deleting old subtitles and replacing them with lower quality ones in the back catalog, yes. Which seems indicative of Crunchyroll wanting to eventually get rid of good subtitles altogether.reply",
      "What possible incentive could they have for doing this?reply",
      "I worked at crunchyroll.Keeping the \"hard subs\" content is a lot of videos as the subtitles were encoded into the video stream.This makes CDNs and other systems more difficult to utilize because we have a ton of video streams with just caption changes as opposed to just the Japanese audio source + caption files.It's one of those things that doesn't seem that problematic till you include all the video_qualities to support streaming bandwith.  So you also get a #hardSubLanguages * #videoQualitiesreply",
      "Obviously you probably thought about it but what about rendering the subtitles on top of the video stream? Was there a reason it was not possible (e.g DRMs?)reply",
      "If you hardsub the video, then you need to have a full copy of the video for every language. That's the opposite of what people want. They want a single textless video source that can then accommodate any internationalization.",
      "This kind of softsubbing is what Crunchyroll primarily does, but it has hardsubbed encodes for devices that cannot do softsubbed rendering of the ASS subtitles that Crunchyroll uses. I go over some ways in how they could do away with these hardsubbed variants in the article without any notable loss in primary experience quality."
    ],
    "link": "https://daiz.moe/crunchyroll-is-destroying-its-subtitles-for-no-good-reason/",
    "first_paragraph": "Since the beginning of the Fall 2025 anime season, a major change has started taking place at the anime streaming service Crunchyroll: the presentation quality for translations of on-screen text has taken a total nosedive compared to what has been on offer for many years, all the way up until the previous Summer 2025 season. Now, more and more subtitles on Crunchyroll are looking like this:Poor presentation quality like this isn\u2019t entirely new to Crunchyroll, as a portion of the subtitles on the site have always been of third-party origin \u2014 that is, provided by the licensor \u2014 and Crunchyroll just puts them up with zero oversight. This in itself has caused numerous issues over the years, but the pressing issue here is that low quality presentation like this can now be found even in first-party subtitles created by Crunchyroll\u2019s own subtitling staff. For comparison, here\u2019s the kind of presentation quality that first-party subtitles were providing just earlier this year:Given the technica"
  },
  {
    "title": "Tell HN: Azure outage",
    "points": 630,
    "submitter": "tartieret",
    "submit_time": "2025-10-29T16:01:18 1761753678",
    "num_comments": 624,
    "comments_url": "https://news.ycombinator.com/item?id=45748661",
    "comments": [
      "It still surprises me how much essential services like public transport are completely reliant on cloud providers, and don't seem to have backups in place.Here in The Netherlands, almost all trains were first delayed significantly, and then cancelled for a few hours because of this, which had real impact because today is also the day we got to vote for the next parlement (I know some who can't get home in time before the polls close, and they left for work before they opened).reply",
      "Is voting there a one day only event? If not, I feel the solution to that particular problem is quite clear. There\u2019s a million things that could go wrong causing you to miss something when you try to do it in a narrow time range (today after work before polls close)If it\u2019s a multi day event, it\u2019s probably that way for a reason. Partially the same as the solution to above.reply",
      "In europe, voting typically happens in one day, where everyone physically goes to their designated voting place and puts papers in a transparent box. You can stay there and wait for the count at the end of the day if you want to. Tom Scott has a very good video about why we don't want electronic/mail voting: https://www.youtube.com/watch?v=w3_0x6oaDmIreply",
      "Electronic voting and mail voting are very different things though.reply",
      "They both share the fact that you don't see your vote enter a ballot box.reply",
      "Well \"mail in voting\" in Washington state pretty much means you drop off your ballot in a drop box in your neighborhood. Which is pretty much the same thing as putting it in a ballot box.reply",
      "How is that the same?The description of voting in the Netherlands is that you can see your ballot physically go into a clear box and stay to see that exact box be opened and all ballots tallied.Dropping a ballot in a box in tour neighborhood helps ensure nothing with regards to the actually ballot count.",
      "In Sweden, mail/early votes get sent through the postal system to the official ballot box for those votes. In 2018, a local election had to be redone because the post delivered votes late. Mail delivery occasionally have packaged delayed or lost, and votes are note immune to this problem. In one case the post also gave the votes to an unauthorized person, through the votes did end up at the right place.It is a small but distinct difference between mail/early voting and putting the votes directly into the ballot box.reply",
      "One of these things is much easier to burn or otherwise tamper with.reply",
      "You should research what\u2019s inside the boxes in Oregon before just assuming they\u2019re easier to tamper with.reply"
    ],
    "link": "item?id=45748661",
    "first_paragraph": ""
  },
  {
    "title": "A century of reforestation helped keep the eastern US cool (agu.org)",
    "points": 59,
    "submitter": "softwaredoug",
    "submit_time": "2025-10-29T23:17:02 1761779822",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=45754390",
    "comments": [
      "Anecdata: I have a plot of land in the Santa Cruz Mountains and half of it has redwood coverage and the other half is sparsely covered by much smaller species. On hot days I can go to the redwood half and get an easy 10F temperature drop.Shade is part of the equation and so is retaining water. Once I was introduced to the idea of check dams and their role in water conservation, I started noticing how the redwoods often build their own on hilly terrain.The landscape in a forest can be quite complex and rich.reply",
      "Can feel the same effect here in CA.  I\u2019ve heard that in areas with more humidity the effect is much weaker though, presumably because the air has higher heat capacity or something and so doesn\u2019t cool as quickly in the shade.reply",
      "Apparently earthworms are a problem here. The saplings need the brush to protect them and the worms which are non native are mulching it. IIRC. If half of what I hear is happening in the Canadian forests or Amazon is true it\u2019s sickening. Of course you have the naive and confused among us who debate or defend this abhorrent and unnecessary exploitation.reply"
    ],
    "link": "https://news.agu.org/press-release/a-century-of-reforestation-helped-keep-the-eastern-us-cool/",
    "first_paragraph": ""
  },
  {
    "title": "Minecraft removing obfuscation in Java Edition (minecraft.net)",
    "points": 547,
    "submitter": "SteveHawk27",
    "submit_time": "2025-10-29T16:12:56 1761754376",
    "num_comments": 189,
    "comments_url": "https://news.ycombinator.com/item?id=45748879",
    "comments": [
      "It's extraordinary to me that Minecraft is both the game that has the most robust mod community out there and that the modders were working from obfuscated, decompiled Java binaries. With elaborate tooling to deobfuscate and then reobfuscate using the same mangled names. For over a decade! What dedication.reply",
      "More proof that you don't need the source code to modify software. Then again, Java has always been easy to decompile, and IMHO the biggest obstacle to understanding is the \"object-oriented obfuscation\" that's inherent in large  codebases even when you have the original source.reply",
      "First time I have heard of object-oriented obfuscation.I get it, but in general I don't get the OO hate.It's all about the problem domain imo. I can't imagine building something like a graphics framework without some subtyping.Unfortunately, people often use crap examples for OO. The worst is probably employee, where employee and contractor are subtypes of worker, or some other chicanery like that.Of course in the real world a person can be both employee and contractor at the same time, can flit between those roles and many others, can temporarily park a role (e.g sabbatical) and many other permutations, all while maintaining history and even allowing for corrections of said history.It would be hard to find any domain less suited to OO that HR records. I think these terrible examples are a primary reason for some people believing that OO is useless or worse than useless.reply",
      "For me, it's the fact that the mess of DAOs and Factories that constituted \"enterprise\" Java in the 00s was a special kind of hellscape that was actively encouraged by the design of the language.Most code bases don't need dynamically loaded objects designed with interfaces that can be swapped out.  In fact, that functionality is nearly never useful.  But that's how most people wrote Java code.It was terrible and taught me to avoid applying for jobs that used Java.I like OOP and often use it.  But mostly just as an encapsulation of functionality, and I never use interfaces or the like.reply",
      "I am currently being radicalised against OOP because of one specific senior in my team that uses it relentlessly, no matter the problem domain. I recognise there are problems where OOP is a good abstraction, but there are so many places where it isn't.I suspect many OOP haters have experienced what I'm currently experiencing, stateful objects for handing calculations that should be stateless, a confusing bag of methods that are sometimes hidden behind getters so you can't even easily tell where the computation is happening, etcreply",
      "You could write crappy code in any language. I don't think it's specific for Java. Overall I think java is pretty good, especially for big code bases.reply",
      "But there's a real difference how easy it is to write crappy code in a  language. In regards to java that'd be, for example, nullability, or mutability. Kotlin, in comparison, makes those explicit and eliminates some pain points. You'd have to go out of your way and make your code actively worse for it to be on the same level as the same java code.And then there's a reason they're teaching the \"functional core, imperative shell\" pattern.reply",
      "You gotta admit, though, that a language which strongarms you into writing classes with hidden state and then extending and composing them endlessly is kinda pushing you in that direction.It\u2019s certainly possible to write good code in Java but it does still lend itself to abuse by the kind of person that treated Design Patterns as a Bible.reply",
      "Tried to modify one boolean in a codebase a few weeks ago and I had to go thru like 12 levels of indirection to find \"the code that actually runs\".reply",
      "(Hi Andrew)It's the misuse of OO constructs that gives it a bad name, almost always that is inheritance being overused/misused. Encapsulation and modularity are important for larger code bases, and polymorphism is useful for making code simpler, smaller and more understandable.Maybe the extra long names in java also don't help too, along with the overuse/forced use of patterns? At least it's not Hungarian notation.reply"
    ],
    "link": "https://www.minecraft.net/en-us/article/removing-obfuscation-in-java-edition",
    "first_paragraph": ""
  },
  {
    "title": "Raspberry Pi Pico Bit-Bangs 100 Mbit/S Ethernet (elektormagazine.com)",
    "points": 45,
    "submitter": "chaosprint",
    "submit_time": "2025-10-29T23:21:51 1761780111",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=45754439",
    "comments": [
      "this is classic computing wheel of life stuff (Bell, Mudge, MacNamara wrote that up in the 70s)* first you do it in the cpu\n* then you do it in a dedicated card off the bus\n* then you find the FPGA or whatever too slow, so you make the card have it's own CPU\n* then you wind up recursing over the problem, implementing some logic in a special area of the cpu, to optimise its bus to the other bus to ...I expect to come back in 10years and find there is a chiplet which took the rpi core, and implements a shrunk version which can be reprogrammed, into the chiplet on the offload card, so we can program terabit network drivers with a general purpose CPU model.reply",
      "Fun stuff. You kids don't know how lucky you are to have really capable MCU's for just a few bucks. :-)It is kind of the ultimate \"not a TOE[1]\" example yet.[1] TOE or TCP Offload Engine was a dedicated peripheral card that implements both the layer 1 (MAC), layer 2 (Ethernet), and layer 3 (IP) functions as a co-processing element to relieve the 'main' CPU the burden of doing all that.reply",
      "Would this have been possible without PIO?reply",
      "On a Pico? No - the PIOs replace other peripherals a \u00b5C might be able to use to achieve this sort of bitrate, so you'd not really have the tools you'd need to change GPIO pin states once every 3-4 CPU clock cycles.In a sense the PIO is a bit 'cheaty' when claiming \"bit-banging\", because the PIO is the ultimate peripheral, programmable to be whatever you need. It's no mean feat to make the PIO do the sorts of things happening here, by any stretch, but \"bit-banging\" typically means using the CPU to work around the lack of a particular peripheral.From that perspective, there's precious few \u00b5Cs out there that could bit-bang 100MBit/s Ethernet - I'm no expert, but I _think_ that's a 125MHz IO clock, so if you want 4 CPU cycles per transition to load data and push it onto pins, you're looking for a 500MHz \u00b5C, and at those speeds you definitely have to worry about the bus characteristics, stalls, caching, and all those fun bits; it's not your old 8-bit CPU bit-banging a slow serial protocol over the parallel port any more.reply",
      ">\"bit-banging\" typically means using the CPUThis is significant. It's using a hardware peripheral that is designed and intended for high frequency IO manipulation without CPU intervention. This isn't bit-banging, lest we start calling it \"bit-banging\" any time an FPGA or ASIC or even a microcontroller peripheral handles any kind of signalling.reply",
      "Not at that transfer rate. SPI which is the next fastest (common) protocol you find on micros typically operates around 10 Mhz, but this isn\u2019t an apples to apples comparison.reply",
      "How does PIO compare to Cypress PSoC?reply",
      "PIO is great but the competition has working silicon and SDK for all of the common peripherals while RP gives you crappy example code.  Want to connect to an audio codec with I2S?  Almost every MCU has this built in but for RP2040/RP2350 you'll have to roll your own production quality version from a demo that only shows how to send.  Years after release.reply"
    ],
    "link": "https://www.elektormagazine.com/news/rp2350-bit-bangs-100-mbit-ethernet",
    "first_paragraph": "Three years ago, @kingyoPiyo\u2019s Pico-10BASE-T project drew wide attention right here on Elektor for implementing 10 Mbit/s Ethernet on the Raspberry Pi Pico using just a few resistors. In 2023, another milestone followed with bit-banged USB, showing how far the RP2040\u2019s (and now RP2350) programmable I/O could be pushed.Now, developer Steve Markgraf (GitHub @steve-m) has extended the concept with Pico-100BASE-TX \u2014 a 100 Mbit/s Fast Ethernet transmitter running entirely in software.\n\r\nMarkgraf\u2019s implementation uses the PIO and DMA to perform MLT-3 encoding, 4B5B line coding, and scrambling at a 125 MHz symbol rate. The result is a functioning 100 Mbit/s link capable of streaming about 11 Mbyte/s over UDP, demonstrated by real-time audio and ADC data streams.\r\n\u00a0As before, this is a transmit-only proof of concept and must not be connected to PoE-enabled hardware. A pulse transformer or intermediary Ethernet switch is recommended for isolation.Example applications in the repository include a"
  },
  {
    "title": "Responses from LLMs are not facts (stopcitingai.com)",
    "points": 123,
    "submitter": "xd1936",
    "submit_time": "2025-10-29T21:40:05 1761774005",
    "num_comments": 74,
    "comments_url": "https://news.ycombinator.com/item?id=45753422",
    "comments": [
      "> Responses from Large Language Models like ChatGPT, Claude, or Gemini are not facts.\n> They\u2019re predicting what words are most likely to come next in a sequence.I wish we'd move away from these reductive statements that sound like they mean something but are actually a non-sequitur. \"Articles on Wikipedia are not facts. They're variations in magnetic flux on a platter transferred over the network\".Yeah, that doesn't make them not facts, though. The LLM should simply cite its sources, and so should Wikipedia, a human, or a dog, otherwise I'm not believing any of them. Especially the human.reply",
      "> > not facts [but] predicting what words are most likely to come next>  [Sarcastically:] not facts [but] variations in magnetic fluxI disagree, you're conflating two different things here:1. There's a difference between an answer f(x)=N reached by a good/reliable process, versus the same answer g(x)=N reached by using a flawed/unreliable process.2. There's difference between the platonic information/concept versus how it reaches you via symbols and atoms and photons.In other words, the first is about how one reaches a result, while the second how one expresses a result._______Imagine I took a pen and wrote down \"3+4=\", then I rolled dice (2d6) which totaled to 7, causing e to complete the equation with \"3+4=7\".That's #1 issue, right? While it happens to be textually \"correct\", the process is flawed in a way that taints the result. Complaining about the use of dice is not a \"reductive non-sequitur.\"In contrast, as you read this, you're even remotely worried about #2 stuff, like how the dice had carved pips on them instead of number-symbols, or that the \"equation\" is merely electrical signals from reflected photons from ink molecules on cellulose.reply",
      "How do you suppose an LLM can cite it's sources when it doesn't have one?! It's a language model, not an encyclopedia. The LLM doesn't even get to choose what it outputs - it just gives next word probabilities and one of those is selected AT RANDOM by the sampler.So, maybe words 1-3 of the LLMs answer are some common turn of speech that was predicted by 1000s of samples, word 4 came from 4chan (a low probability random pick from the sampler), and word 5 was hallucinated. So, what's the \"source\" for this \"fact\"?reply",
      "Articles on Wikipedia are not facts. They\u2019re the product of community contributions on a topic.I don\u2019t think that\u2019s really a non-sequitur, but I guess it depends on what\u2019s meant by facts in your epistemology.reply",
      "We can call wikipedia content facts by consensus. It's hard to say the same for LLMs since the input is not curated for accuracy, even though the wikipedia content is a subset of the entire training corpus.In short, the curation is the key differentiator between the two.reply",
      "> not curated for accuracyI thought accuracy is one of metrics that the models are trained for\u2026reply",
      "Consensus of what the news media says. Wikipedia doesn't actually care if the content is true.reply",
      "https://en.wikipedia.org/wiki/Wikipedia:Verifiability> Even if you are sure something is true, it must have been previously published in a reliable source before you can add it. If reliable sources disagree with each other, then maintain a neutral point of view and present what the various sources say, giving each side its due weight.Wikipedia cares that its contents are taken from reliable sources, which can be independently verified. Not all news media are reliable sources, and in fact academic papers and journals and published books are generally more reliable than news media.reply",
      "The funny irony is that for years and years universities would as a policy not accept wikipedia as a reference. I think the thinking was that a published book was more likely to have been written by an expert in the field. Now, even that is less and less likely.reply",
      "I agree that next word prediction, while technically correct, doesn\u2019t capture the full nature of what LLMs are optimized for. And the article gets it wrong. In fact they\u2019re optimized for sycophancy and human preference, to produce plausible feel good slop that looks good and makes you read it uncritically, the high fructose corn syrup of reading.So things like brainstorming or summarization actually give horrible results optimized to make you feel smart and not to help you learn or critically appraise anything.OTOH, for most actual facts, I think LLMs are pretty good and continue to get better (as long as you\u2019re asking direct questions about a real thing).So yeah they\u2019re not just next word predictors, even if that describes how they work; they\u2019re something much more insidious that has been optimized by world experts to be more convincing than you, whether right or wrong. If your boss is citing LLMs you\u2019ve already lost, just move on.reply"
    ],
    "link": "https://stopcitingai.com/",
    "first_paragraph": "\n        They\u2019re predicting what words are most likely to come next in a sequence.\n        \n        They can produce convincing-sounding information, but that information may not be accurate or reliable.\n        \n          What kinds of things might they be good at?\n        \n          What kinds of things might they be bad at?\n        \n          Sure, you might get an answer that\u2019s right or advice that's good\u2026\n          but what \u201cbooks\u201d are it \u201cremembering\u201d when it gives that answer?\n          That answer or advice is a common combination of words, not a fact.\n        When you do that, you\u2019re basically saying \u201chere are a bunch of words that often go together in a sentence.\u201dSometimes that can be helpful or insightful. But it\u2019s not a truth, and it\u2019s certainly not the final say in a matter."
  },
  {
    "title": "Dithering \u2013 Part 1 (visualrambling.space)",
    "points": 202,
    "submitter": "Bogdanp",
    "submit_time": "2025-10-29T18:21:35 1761762095",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=45750954",
    "comments": [
      "This is halftone (i.e., an apparent palette with more colors than the actual palette, by ensuring that you aren't just rounding the same way everywhere) but it isn't dithering in my opinion. To me, dithering means fading away the banding that occurs when the palette (or the apparent palette achieved via halftone) isn't large enough to avoid banding on its own.The halftone technique demonstrated here takes a palette of 2 colors and increases it to something on the order of 20 apparent colors, but even with 20 there are extremely obvious bands.That banding can be virtually eliminated by having way more colors (say, 256 if grayscale, 256^3 if RGB) or it can be virtually eliminated via dithering. I suspect the \"error diffusion\" technique (which is teased at the end of this demo) does what I'm talking about.Noise is the key to dithering, and I don't see any noise in this demo. Everything is deterministic.But the presentation is spectacular!reply",
      "You made me curious. It looks like dithering is still an accepted name for this kind of technique: https://en.wikipedia.org/wiki/Ordered_ditheringreply",
      "Glad I snuck in that it's just my opinion! But the article you linked to sort of admits what I'm saying:> The above thresholding matrix approach describes the Bayer family of ordered dithering algorithms. A number of other algorithms are also known; they generally involve changes in the threshold matrix, equivalent to the \"noise\" in general descriptions of dithering.Basically, I'm leaning into \"general descriptions of dithering\" with my noise requirement, and the lack of noise in \"ordered dithering\" leads me to consider it not-quite-dithering.The very first sentence of the general Dithering article [0] connects with my perspective as well:> preventing large-scale patterns such as color bandingAside: I probably misspoke with the word \"halftone\" earlier; apparently that's a specific thing as opposed to an umbrella term. I'm not sure there's a great word (other than \"dither\"...) for techniques to trade resolution for color.[0] https://en.wikipedia.org/wiki/Ditherreply",
      "Very cool way to visualize it but I will be honest the threshold map doesn\u2019t make sense. This didn\u2019t seem to explain how to form the map, how to choose the threshold values, and so on. It showed grey pixels passing through white, black, and grey pixels and moved onto generalizing this pattern.Is this just me being dumb or the curse of knowledge where something is so obvious to the author that they don\u2019t even bother explaining it?reply",
      "They say that part 2 will discuss how that\u2019s formed, and 3 will discuss error diffusion dithering.reply",
      "Two videos from Daniel Shiffman's Coding Train:Turning Images into Dots: The Magic of Dithering\nhttps://www.youtube.com/watch?v=0L2n8Tg2FwICoding Challenge 181: Weighted Voronoi Stippling\nhttps://www.youtube.com/watch?v=Bxdt6T_1qgcreply",
      "Beautiful demo, but I\u2019m not sure it\u2019s accurate to call dithering an \u201cillusion\u201d of more shades than is available?If you apply a low pass filter to a dithered image, and compare it to a low passed filtered thresholded, you\u2019ll see that the \u201cillusory\u201d shades are really there in the dithered version, they\u2019re just represented differently in the full resolution image.Similarly, a class D amplifier emits purely off/on pulses before a low pass filter is applied, but no one would call the output an auditory \u201cillusion\u201d. In the case of image dithering, isn\u2019t the low pass filter your own vision + the distance to the screen?reply",
      "I would call it an illusion because if you pay attention you can clearly see that the color you perceive isn't actually present. You can see white on an RBG computer screen since your eyes simply don't have the resolution to discern the subpixel colors. However, in a dithered image with only black and white, you perceive gray, but you can also tell what the reality is without much effort. Personally, I think that fits the definition of an illusion.reply",
      "That's verisimilitude. We were doing that with representational art way before computers, and even doing stipple and line drawing to get \"tonal indications without tonal work\". Halftone, from elsewhere in the thread, is a process that does similar. When you go deeper into art theory verisimilitude comes up frequently as something that is both of practical use(measure carefully, use corrective devices and appropriate drafting and markmaking tools to make things resemble their observed appearance) and also something that usually isn't the sole communicative goal.All the computer did was add digitally-equivalent formats that decouple the information from its representation: the image can be little dots or hex values. Sampling theory lets us perform further tricks by defining correspondences between time, frequency and amplitude. When we resample pixel art using conventional methods of image resizing, it breaks down into a smeary mess because it's relying on certain artifacts of the representational scheme that differ from a photo picture that assumes a continuous light signal.Something I like doing when drawing digitally is to work at a high resolution using a non-antialiased pixel brush to make black and white linework, then shrink it down for coloring. This lets me control the resulting shape after it's resampled(which, of course, low-pass filters it and makes it a little more blurry) more precisely than if I work at target resolution and use an antialiased brush; with those, lines start to smudge up with repeated strokes.reply",
      "In the case of dithering, that\u2019s only because the monitor has insufficient resolution. Put a 1:1 Floyd steinberg dithered image on your phone, hold it at arm\u2019s length, and unless you have superhuman vision you\u2019ll already start having a hard time seeing the structure.If you look at analogue B&W film for instance (at least the ones I\u2019m familiar with), each individual crystal is either black or white. But the resolution is so high you don\u2019t perceive it unless you look under a microscope, and if you scan it, you need very high res (or high speed film) to see the grain structure.Dithering is not an illusion because the shades are actually still there. With the correct algorithms, you could upscale an image, dither it, down res it, and get back the exact same tones. The data isn\u2019t \u201cfaked\u201d, it\u2019s just represented in a different way.If you\u2019re calling it an illusion, you\u2019d have to call pretty much every way we have of representing an image, from digital to analog, an illusion. Fair, but I\u2019d rather reserve the term for when an image is actually misinterpreted.reply"
    ],
    "link": "https://visualrambling.space/dithering-part-1/",
    "first_paragraph": "Loading assets, please wait...Understanding how dithering works, visually.tap/click the right side of the screen to go forward \u2192I\u2019ve always been fascinated by the dithering effect. It has a unique charm that I find so appealing.\u2190 tap/click the left side to go backI was even more amazed when I learned how dithering works.\u2190 or use arrow keys to navigate \u2192Look closely, and you\u2019ll see this animation is made of alternating black and white pixels.But these black and white pixels are specifically arranged to create the illusion of multiple shades.That\u2019s what dithering does: it simulates more color variations than what are actually used.Here, it uses black and white to give the impression of multiple gray shades.To me, dithering is about creating the most out of what we have, and that's what amazes me the most!It inspired me to learn more about it, and now I want to share what I\u2019ve learned.Please note that this is just part one out of three, so I\u2019ll only scratch the surface here.I\u2019ll go deeper"
  },
  {
    "title": "AWS to bare metal two years later: Answering your questions about leaving AWS (oneuptime.com)",
    "points": 594,
    "submitter": "ndhandala",
    "submit_time": "2025-10-29T11:14:38 1761736478",
    "num_comments": 423,
    "comments_url": "https://news.ycombinator.com/item?id=45745281",
    "comments": [
      "I'm so surprised there is so much pushback against this.. AWS is extremely expensive. The use cases for setting up your system or service entirely in AWS are more rare than people seem to realise. Maybe I'm just the old man screaming at cloud (no pun intended) but when did people forget how to run a baremetal server ?> We have 730+ days with 99.993% measured availability and we also escaped AWS region wide downtime that happened a week ago.This is a very nice brag. Given they are using their ddos protection ingress via CloudFlare there is that dependancy, but in that case I can 100% agree than DNS and ingress can absolutely be a full time job. Running some microservices and a database absolutely is not. If your teams are constantly monitoring and adjusting them such as scaling, then the problem is the design. Not the hosting.Unless you're a small company serving up billions of heavy requests an hour, I would put money on the bet AWS is overcharging you.reply",
      "The direct cost is the easy part. The more insidious part is that you're now cultivating a growing staff of technologists whose careers depend on doing things the AWS way, getting AWS certified to ensure they build your systems the AWS Well Architected Way instead of thinking themselves, and can upsell you on AWS lock-in solutions using AWS provided soundbites and sales arguments.(\"Shall we make the app very resilient to failure? Yes running on multiple regions makes the AWS bill bigger but you'll get much fewer outages, look at all this technobabble that proves it\")And of course AWS lock-in services are priced to look cheaper compared to their overpricing of standard stuff[1] - if you just spend the engineering effort and IaC coding effort to move onto them,  this \"savings\" can be put to more AWS cloud engineering effort which again makes your cloud eng org bigger and more important.[1] (For example implementing your app off containers to Lambda, or the db off PostgreSQL to DynamoDB etc)reply",
      "> The direct cost is the easy partI don't think it is easy. I see most organizations struggle with the fact that everything is throttled in the cloud. CPU, storage, network. Tenants often discover large amounts of activity they were previously unaware of, that contributes to the usage and cost. And there may be individuals or teams creating new usages that are grossly impacting their allocation. Did you know there is a setting in MS SQL Server that impacts performance by an order of magnitude when sending/receiving data from the Cloud to your on-premises servers? It's the default in the ORM generated settings.Then you can start adding in the Cloud value, such as incomprehensible networking diagrams that are probably non-compliant in some way (guess which ones!), and security? What is it?reply",
      "Yes. Cloud sellers new this: Happy path for this flagship project, the shinny new object, and some additional services. After the point of no return what usually happens is, that cloud will be a replica of bare metal development.As an Computer Science dude and former C64/Amiga coder in Senior Management of a large international Bank, I saw first hand, how cost balloon simply due to the fact, that the bank recreates and replicates its bare metal environment in the cloud.So increasing costs while nothing changed. Imagine that: fixed resources, no test environments, because virtualisation was out of the equation in the cloud due to policies and SDLC processes. And it goes on: releases on automation? Nope, request per email and attached scan of a paper document as sign-off.Of course your can buy a Ferrari and use it as a farm tractor. I bet it is possible with a little modification here and there.Another fact is, that lock in plays a huge role. Once you are in it, no matter what you subscribe to, magically everything slows suddenly down, a bit, but since I am a guy who uses a time tracker to test and monitor apps, I could easily draw a line even without utilizing my Math background: enforced throtelling.There is a difference between 100, 300 and 500ms for SaaS websites - people without prior knowledge of peceptual psychology feel it but cannot but their finger in the wound. But since we are in the cloud, suddenly a cloud manager will offer you an speed upgrade - just catered for your needs! Here, have a trial period over 3 month for free and experience the difference for your business!I am a bit of opinionated here and really suppose, that cloud metrics analysed the banks traffic and service usage to willingly slow it down in a way, only professionals could find out. Have you promised to be lightning fast in the first place? No, that's not what the contract says. We fed you with it, but a \"normal\" speed was agreed upon. It is like getting a Porsche as a rental car for free when you take your VW Beetle to the dealer for a checkup. Hooked, of course. A car is a car after all. How to boil a frog? Slowly.Of course there will be more sales and this is achilles' heel for every business and indifferent customers - easy prey.It is a vicious cycle, almost like taxation. You cannot hide from it, no escape and it is always on the rise.reply",
      "Ferrari actually makes tractors.reply",
      "> Did you know there is a setting in MS SQL Server that impacts performance by an order of magnitude when sending/receiving data from the Cloud to your on-premises servers? It's the default in the ORM generated settings.Sounds interesting, which setting is that?reply",
      "Multiple Active Result Sets (MARS). During large query responses or bulk loads, \"full\" packets cause an additional packet to be sent over the wire with about five bytes to hold the MARS \"wrapper\". The net result is one full packet, and one empty packet on the wire, alternating. The performance impact in LAN latency is negligible. However on higher latency between AWS and your premises it has a terrible performance impact.MARS isn't strictly needed for most things. Some features that requires it are ORM (EF) proxies and lazy loading. If you need MARS, there are third party \"accelerators\" that workaround this madness.\"MARS Acceleration significantly improves the performance of connections that use the Multiple Active Result Sets (MARS) connection option.\"https://documentation.nitrosphere.com/resources/release-note...reply",
      "Yeah, honestly most lazy loading and EF proxy use I have seen is more aptly named lazy coding instead. There are times when you might be running 3-4 queries to project some combination of them and want to do that in parallel, but in general if you have lazy loading enabled in EF you are holding up a sign that says \u201cinconsistent reads happening in here\u201d.I use and love EF, but generally leave MARS off when possible because it is responsible for more trouble than performance gains nearly every time.reply",
      "Is that not a client connection flag? MARS does not require a setting change on the server?reply",
      "I think you may have misinterpreted what he said. I can see why it seems to imply a server setting but that isn't the case> Did you know there is a setting in MS SQL Server that impacts performance by an order of magnitude when sending/receiving data from the Cloud to your on-premises servers? It's the default in the ORM generated settingsreply"
    ],
    "link": "https://oneuptime.com/blog/post/2025-10-29-aws-to-bare-metal-two-years-later/view",
    "first_paragraph": "Status PagesStatus Page helps you be more transparent with your customers\n                      and showcase reliability.MonitoringAnalyse uptime and performance of all of your resources.Incident ManagementProtect revenue and improve customer experiences by resolving\n                      incidents faster.On-Call and AlertsAlert right people at the right time. Create on-call schedules\n                      and more.Logs ManagementFastest log software on the planet. Ingest logs from any\n                      source and search in seconds.APMMonitor performance of any app, any service, any stack.WorkflowsIntegrate with 5000+ different services and products without\n                      writing any code.DocsLearn more about OneUptime by reading our docs.API ReferenceConnect OneUptime with the rest of your software stack.Learning Resources and BlogLearn about observability and keep yourself updated.Help & SupportGet all of your questions answered by contacting support.GitHubCheck the code o"
  },
  {
    "title": "How the U.S. National Science Foundation Enabled Software-Defined Networking (acm.org)",
    "points": 47,
    "submitter": "zdw",
    "submit_time": "2025-10-29T21:22:50 1761772970",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=45753222",
    "comments": [
      "> 2003: The goal of the 100\u00d7100 project was to create communication architectures that could provide 100Mb/s networking for all 100 million American homes.Well you failed horribly.> The project brought together researchers from Carnegie Mellon, Stanford, Berkeley, and AT&T.I think I see why.>  This research led to the 4D architecture for logically centralized network control of a distributed data planeWhat?  How was this meant to benefit citizens?> Datacenter owners grew frustrated with the cost and complexity of the commercially available networking equipment; a typical datacenter switch cost more than $20,000 and a hyperscaler needed about 10,000 switches per site. They decided they could build their own switch box for about $2,000 using off-the-shelf switching chips from companies such as Broadcom and MarvellWhat role did the NSF play here?  It sounds like basic economics did most of the actual work.> The start-up company Nicira, which emerged from the NSF-funded Ethane project, developed the Network Virtualization Platform (NVP)26 to meet this needWhich seems to have _zero_ mentions outside of academic papers.reply",
      "The 10-year research-to-production timeline is the key lesson. Today's funding (VC or government grants) demands results in 2-3 years. We've systematically eliminated the \"patient capital\" that creates foundational infrastructure imho...reply",
      "To say nothing of systematically eliminating the foundational infrastructure for nationally funded science in general.reply",
      "> a network should have logically centralized control, where the control software has network-wide visibility and direct control across the distributed collection of network devices.Including a backdoor for wiretapping in SDN-enabled routers.reply",
      "Is it really a \u201cback door\u201d when it\u2019s controlled by the network owner? It feels like we need a different term for that since it\u2019s increasingly common on large networks.reply",
      "Unless the goal of the backdoor is to redirect traffic flows through packet inspection devices that the attacker also controls, the decoupling of the control and data plane in SDN deployments requires a more creative, intricate solution to allow for wiretapping compared to traditional routers.reply",
      "I worked with a quite few of the folks mentioned in this article when I was at the Open Networking Foundation, if anyone has questions.reply",
      "what a wonderful chronicle of how esoteric research became not-esoteric, and truly world-changing, and how the NSF enabled itpour one out for the NSF folks. RIP </3reply"
    ],
    "link": "https://cacm.acm.org/federal-funding-of-academic-research/how-the-u-s-national-science-foundation-enabled-software-defined-networking/",
    "first_paragraph": ""
  },
  {
    "title": "AOL to be sold to Bending Spoons for $1.5B (axios.com)",
    "points": 177,
    "submitter": "jmsflknr",
    "submit_time": "2025-10-29T16:28:56 1761755336",
    "num_comments": 164,
    "comments_url": "https://news.ycombinator.com/item?id=45749161",
    "comments": [
      "https://archive.is/Ouc0B",
      "People usually mention Evernote when Bending Spoons is brought up, but I also know them as purchasing Meetup (after it was already sort of struggling) and, more recently, entering an agreement to purchase Vimeo (of which I'm a paid user).AOL was already a husk, and has been arguably since they got rid of the triangle logo. It was already owned by a private equity firm, Apollo Global Management, as a subsidiary of Yahoo!. Some of the still-relevant tech news sites like TechCrunch and Engadget were apparently moved from AOL to being directly under Yahoo! a few years ago. So I'm not too worried about AOL, but it's interesting how often I've heard about Bending Spoons in relation to brands I know over the past few years.(Edit: AOL deleted all of my childhood emails back in the 2010s-- on an account that had previously been part of a paid AOL family subscription for years-- after I failed to sign into my account for more than 6 months, which also contributes to my current feeling that it's dead to me.)reply",
      "Vimeo is the really interesting case for me, because they are the white label hosting providers for a large number of niche streaming services -- Criterion Channel comes to mind, for example. Evernote failing is sad but lower on indirect effects. Vimeo going down would leave a noticeable hole in the streaming world.reply",
      "It sounds like Bending Spoons is where old tech products go to die? I guess that's private equity for you.reply",
      "Understandably people don't like Bending Spoons - they fired the whole dev team on Evernote, and the price has gone way up too.. but as a user I have to say Evernote the product has gotten better and better since the acquisition. They've improved performance and have great new features every month.reply",
      "They've kept the product alive but I don't know that it's terribly improved... I've been a paid user since 2008. Switching would be painful for me given how familiar I am with it but I came close this last year when it stopped letting me stay logged in on multiple Mac computers at the same time...reply",
      "I was with Evernote since 07, and found it a doddle to ditch. Export the lot, bring into Apple Notes or Bear. Or a combination of the two. Sorted.reply",
      "> they fired the whole dev team on Evernote, and the price has gone way up too.. but as a user I have to say Evernote the product has gotten better and better since the acquisitionI'd say it's only just slightly improved now, with a few bugs fixed and features improved.  Not at all worth the price increase.And it was horrible for a good 6 months after the acquisition...  Some days I could not login to the website for several hours.  Images in some notes wouldn't load some days.  Searches would be missing results.  Bug reports sat idle for a couple months before someone would respond asking for more info.reply",
      "Wow, that's interesting.I was a very early Evernote (paid) user.  But they lost their way sometime after they became a unicorn, so I bailed out.I had assumed, since they were bought, that it was just a way to squeeze money from existing users.  I had no idea they were actually improving things.reply",
      "I like Evernote but it just isn\u2019t worth $130 / year for me. Last year they had a sale for $50 (or was it $60) for a year and I paid for that. If I can\u2019t renew at that I\u2019ll have to figure out how to migrate to Obsidian.reply"
    ],
    "link": "https://www.axios.com/2025/10/29/aol-bending-spoons-deal",
    "first_paragraph": ""
  },
  {
    "title": "Kafka is Fast \u2013 I'll use Postgres (topicpartition.io)",
    "points": 289,
    "submitter": "enether",
    "submit_time": "2025-10-29T14:06:01 1761746761",
    "num_comments": 243,
    "comments_url": "https://news.ycombinator.com/item?id=45747018",
    "comments": [
      "My general opinion, off the cuff, from having worked at both small (hundreds of events per hour) and large (trillions of events per hour) scales for these sorts of problems:1. Do you really need a queue? (Alternative: periodic polling of a DB)2. What's your event volume and can it fit on one node for the foreseeable future, or even serverless compute (if not too expensive)? (Alternative: lightweight single-process web service, or several instances, on one node.)3. If it can't fit on one node, do you really need a distributed queue? (Alternative: good ol' load balancing and REST API's, maybe with async semantics and retry semantics)4. If you really do need a distributed queue, then you may as well use a distributed queue, such as Kafka. Even if you take on the complexity of managing a Kafka cluster, the programming and performance semantics are simpler to reason about than trying to shoehorn a distributed queue onto a SQL DB.reply",
      "I suspect the common issue with small scale projects is that it's not atypical for the engineers involved to perform a joint optimization of \"what will work well for this project\", and \"what will work well at my next project/job.\" Particularly in startups where the turnover/employer stability is poor - this is the optimal action for the engineers involved.Unless employees expect that their best rewards are from making their current project as simple and effective as possible - it is highly unlikely that the current project will be as simple as it could be.reply",
      "What I've found to be even more common than resume driven development has been people believing that they either have or will have \"huge scale\". But the problem is that their goal posts are off by a few orders of magnitude and they will never, ever have the sort of scale required for these types of tools.reply",
      "This is something to catch in hiring and performance evaluation. Hire people who don't build things to pad their own CVs, tell them to stop if you failed, fire them if that failedreply",
      "> Do you really need a queue? (Alternative: periodic polling of a DB)In my experience it\u2019s not the reads, but the writes that are hard to scale up. Reading is cheap and can be sometimes done off a replica. Writing to a PostgreSQL at high sustained rate requires careful tuning and designs. A stream of UPDATEs can be very painful, INSERTs aren\u2019t cheap, and even a batched COPY blocks can be tricky.reply",
      "I dont disagree, and I am trying to argue for it myself, and have used postgres as a \"queue\" or the backlog of events to be sent (like outbox pattern). But what if I have 4 services that needs to know X happened to customer Y? I feel like it quickly becomes cumbersome with a postgres event delivery to make sure everyone gets the events they need delivered. The posted link tries to address this at least.reply",
      "Call me dumb - I'll take it! But if we really are trying to keep it simple simple...Then you just query from event_receiver_svcX side, for events published > datetime and event_receiver_svcX = FALSE. Once read set to TRUE.To mitigate too many active connections have a polling / backoff strategy and place a proxy infront of the actual database to proactively throttle where needed.But event table:| event_id | event_msg_src | event_msg           | event_msg_published | event_receiver_svc1 | event_receiver_svc2 | event_receiver_svc3 ||----------|---------------|---------------------|---------------------|---------------------|---------------------|---------------------|| evt01    | svc1          | json_message_format | datetime            | TRUE                | TRUE                | FALSE               |reply",
      "Periodic polling of a DB gets bad pretty quick, queues are much better even on small scale.But then distributed queue is most likely not needed until you hit really humongous scale.reply",
      "Maybe in the past this was true, or if you\u2019re using an inferior DB. I know first hand that a Postgres table can work great as a queue for many millions of events per day processed by thousands of workers polling for work from it concurrently. With more than a few hundred concurrent pollers you might want a service, or at least a centralized connection pool in front of it though.reply",
      "Millions of events per day is still in the small queue category in my book. Postgres LISTEN doesn't scale, and polling on hot databases can suddenly become more difficult, as you're having to throw away tuples regularly.10 message/s is only 860k/day. But in my testing (with postgres 16) this doesn't scale that well when you are needing tens to hundreds of millions per day. Redis is much better than postgres for that (for a simple queue), and beyond that kafka is what I would choose in you're in the low few hundred million.reply"
    ],
    "link": "https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks",
    "first_paragraph": "Search \u276f  \u276f Oct 28, 202534 min readI feel like the tech world lives in two camps.This camp tends to adopt whatever\u2019s popular without thinking hard about whether it\u2019s appropriate. They tend to fall for all the purported benefits the sales pitch gives them - real-time, infinitely scale, cutting-edge, cloud-native, serverless, zero-trust, AI-powered, etc.You see this everywhere in the Kafka world: Streaming Lakehouse\u2122\ufe0f, Kappa\u2122\ufe0f Architecture, Streaming AI Agents1.This phenomenon is sometimes known as resume-driven design. Modern practices actively encourage this. Consultants push \u201cinnovative architectures\u201d stuffed with vendor tech via \u201cinsight\u201d reports2. System design interviews expect you to design Google-scale architectures that are inevitably at a scale 100x higher than the company you\u2019re interviewing for would ever need. Career progression rewards you for replatforming to the Hot New Stack\u2122\ufe0f, not for being resourceful.This camp is far more pragmatic. They strip away unnecessary complex"
  },
  {
    "title": "Tailscale Peer Relays (tailscale.com)",
    "points": 246,
    "submitter": "seemaze",
    "submit_time": "2025-10-29T16:21:36 1761754896",
    "num_comments": 71,
    "comments_url": "https://news.ycombinator.com/item?id=45749017",
    "comments": [
      "Great! This feature made a lot of sense, and it took a long time.It\u2019s like falling back to hub and spoke, except that the traffic is end to end encrypted, and the middle node is used only when direct connection is not possible, and for some clients. It\u2019s also similar to running your own derp server (which works also in TCP), but without the hassle of doing so, and perhaps without having to open ports to the internet (needed in derp) so long as the relay is reachable by peers.The derp servers have low throughput. Another option could be a pay-as-you-go derp service.They might also be on their way to remove the need for reverse proxies, with the recent announcement on Tailscale services.BTW, why could it be paid for more than two relays? You are using just your own devices and bandwidth :)It actually lower the bandwidth bill for Tailscale by reducing the usage of their own relays. Ideally, by default the software will find whatever nodes could help with direct connection. It\u2019s just routing within your own network.reply",
      "> It\u2019s also similar to running your own derp server (which works also in TCP), but without the hassle of doing so, and perhaps without having to open ports to the internet (needed in derp) so long as the relay is reachable by peers.I think most folks will need to open a port to the internet, because otherwise you wouldn't need the tailscale to begin with. e.g. connecting your cloud network to your on premise network etc.Of course exceptions apply, like both clients can reach the peer relay, but not each other directly.reply",
      "I could open a port to the internet, but it would be Tailscale\u2019s responsibility to secure the software that listens to the port (subject to an up-to-date software, that is my responsibility).It\u2019s not a standard Wireguard port. With Wireguard included in Linux, I would not be worried.reply",
      "This was better solved by tinc about 20 years ago. All tinc nodes can work as relays (but you can disallow that if you want), it does not rely on a centralized server, and works fine without access to the internet. It is a true mesh. The world would be better served by porting tinc to wireguard and some memory safe language instead of reimplementing parts of its functionality from scratch.reply",
      "Heh. I have a 30-nodes Tinc network over the internet but some hosts are behind a NAT. It keeps randomly losing routes between these nodes. It even has the infuriating behavior that often it loses the route a few seconds after I successfully established a SSH connection.Also, traffic seems to be decrypted and re-encrypted by relaying nodes. For end-to-end encryption, you need \"ExperimentalProtocol = yes\" added by Tinc 1.1, which was never formally released.I'd like to rewrite something like it in a language I'm familiar with (perhaps based on cjdns' protocol which is better documented than Tinc's) but it's not easy.reply",
      "Wireguard can do what you're describing.reply",
      "Wireguard can't punch through NATs or firewalls without third party software like Tailscale.  Also I'm pretty sure each peer to peer connection needs to be individually set up in a config file ahead of timereply",
      "Nebula[0] addresses this and is IMO an improvement over WireGuard. Came out of Slack originally, and it supports peer discovery, NAT hole punching, and some other cool features. Also still uses the Noise Protocol.In practice, the extra networking features + better first class peer config management baked in is very nice (Nebula\u2019s \u201clighthouses\u201d are configured with a tool similar to DSNet for Wireguard[1])[0] https://github.com/slackhq/nebula\n[1] https://github.com/naggie/dsnetreply",
      "I use Nebula but the relay config is somewhat clunky and the macOS port is pretty buggy.reply",
      "So now we're back \"tailscale but with different steps\"reply"
    ],
    "link": "https://tailscale.com/blog/peer-relays-beta",
    "first_paragraph": "Tailscale Peer Relays provides a customer-deployed and managed traffic relaying mechanism. By advertising itself as a peer relay, a Tailscale node can relay traffic for any peer nodes on the tailnet, even for traffic bound to itself. Tailscale Peer Relays can only relay traffic for nodes on your tailnet, and only for nodes that have access to the peer relay. Because they\u2019re managed entirely by the customer, peer relays are less throughput-constrained than Tailscale\u2019s managed DERP relays, and can provide higher throughput connections for traffic to and from locked-down cloud infrastructure, or behind strict network firewalls.In testing with early design partners, we\u2019ve seen throughputs nearing that of a direct connection; often multiple orders of magnitude higher than Tailscale\u2019s managed DERP fleet.Over the past few weeks, you\u2019ve heard us talk about improvements we\u2019ve made to Network Address Translation (NAT) traversal techniques, so that Tailscale can establish direct connections where"
  },
  {
    "title": "OpenAI\u2019s promise to stay in California helped clear the path for its IPO (wsj.com)",
    "points": 149,
    "submitter": "badprobe",
    "submit_time": "2025-10-29T17:44:34 1761759874",
    "num_comments": 192,
    "comments_url": "https://news.ycombinator.com/item?id=45750425",
    "comments": [
      "Can someone explain to me why California would believe Sam Altman plans to stay in California? This is a weak handshake agreement that could easily be flipped post IPO. The very flip from non-profit to IPO shows he will do what it takes for OpenAI to \"succeed\", so why would the geographical location be any more permanent than corporate structure. This isnt a diss to Sam either, it just shows he is motivated by whatever is best for the entity at any given time.They might stay in California, but that probably has far more to do with available researchers and employee preferences than some agreement with the Attorney General.reply",
      "In any negotiation, you need to understand what leverage either side has. In this case, CA could block the conversion, and OpenAI could leave California. Both are possible but extremely unlikely outcomes! So the whole point is to take these unlikely outcomes to the table, negotiate in good faith, and come out with an agreement.So California needs to believe that OpenAI will stay in California just as much as OpenAI needs to believe that CA won't block the conversion (or impose other onerous regulations around AI). So yes, it's possible to speculate about whether or not people are sincere in their motivations, but when you need to make a deal, there needs to be a measure of good faith and trust on both sides in order to make something happen.And in this case, both sides are incentivized to make the deal. OpenAI wants to be a PBC in order to access more capital, and California wants OpenAI to be a PBC so that it can IPO so that all employees (all of whom are likely CA residents), will sell stock, which can then be taxed as CA income.reply",
      "If I am understanding things correctly, OpenAI could leave in the future - but CA can\u2019t retroactively block the conversion in that case.reply",
      "Yup, but the IPO will still have happened in CA, and there's going to be a tax windfall from that.It's about a moment in time, not an \"in perpetuity\" agreement.reply",
      "Unless you're Universal and Marvel, leaving Disney unable to buy out Universal's contract with Marvel, and unable to use classic comic book characters because the parks too close to Universals. Still cracks me up.reply",
      "Not if the big holders aren't residents, they can move away just before like Rogan with his Spotify deal, or Jonathan Blow just before a game release after developing in California and going to public college there, etc.Since it's a non-profit still holding it any gains to the non-profit entity upon the conversion don't go to California, and principal stakeholders can move away.  Other funds raised from the IPO can be invested in other states untaxed (long term datacenter leases instead of booking the capital of building one) until they move the company away I think.There will probably be a lot of smaller stakeholders that stay with a lot of money for the state, and California at least doesn't do the $15 million QSBS so they may get a lot from that tail of employees.  A large portion of this tail of lower compensated employees may get laid off due to AI replacement before IPO and lose a lot of unvested years, if we are to believe OpenAI's own claims about timelines for job replacement in that field at lower levels.reply",
      "I'd recommend anyone expecting to profit from OpenAI stock and aiming to avoid California taxes to look into the subject in depth with paid advisors. The California FTB is not stupid or na\u00efve and has a history of successfully getting paid for stock that vested with a California nexus, even if the beneficiary moves out of state. Good luck!reply",
      "Take a look at Shohei Ohtani's contract.reply",
      "You're right, it's harder with vesting stock compensation than other things you build up over time like an audience or a developing a game over a long span.reply",
      "> Can someone explain to me why California would believe Sam Altman plans to stay in California?The simple answer is unless developing LLMs becomes commoditised, the best place in the world to do it is in San Francisco. You don't take your manufacturing business out of Shenzhen without very good reason.reply"
    ],
    "link": "https://www.wsj.com/tech/ai/openais-promise-to-stay-in-california-helped-clear-the-path-for-its-ipo-3af1c31c",
    "first_paragraph": ""
  },
  {
    "title": "How to Obsessively Tune WezTerm (rashil2000.me)",
    "points": 74,
    "submitter": "todsacerdoti",
    "submit_time": "2025-10-29T19:39:57 1761766797",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=45751995",
    "comments": [
      "I love WezTerm! In the author's spirit of obsessive tweaking, here's a completely inconsequential configuration change I made.By default, WezTerm doesn't have a scrollbar, but you can easily enable it with:  config.enable_scroll_bar = true\n\nBut now you always have a scrollbar, just a big line on the side when there's no scrollback or you're in alternate screen mode. Horrible! So, here's an event handler that will automatically hide the scrollbar when not needed, giving it the same behavior as scrollbars in modern applications:  -- Hide the scrollbar when there is no scrollback or alternate screen is active\n  wezterm.on(\"update-status\", function(window, pane)\n    local overrides = window:get_config_overrides() or {}\n    local dimensions = pane:get_dimensions()\n\n    overrides.enable_scroll_bar = dimensions.scrollback_rows > dimensions.viewport_rows and not pane:is_alt_screen_active()\n\n    window:set_config_overrides(overrides)\n  end)\n\nAnd that kinda sums up the development philosophy of WezTerm. It has basically all the building blocks you'd ever need with nice APIs. It's set up quite usably by default, but anything that's missing you can probably implement yourself.reply",
      "Thanks, I've copied that into my config!reply",
      "The thing I love most about wezterm is the `wezterm cli list --format json`, and being able to get various values like the cwd of any terminal by window id.\nWhile I don't much tweak my actual wezterm config at all. I use that a lot for various scripts to launch other terminals/file managers in the same directory as the currently active terminal...reply",
      "Wezterm is a lot of fun. I played around with notifications and copy/paste which work even through ssh back to the host.https://blog.gripdev.xyz/2025/01/08/wezterm-easily-copy-text...reply",
      "Wasn't aware of user-var-changed, cool write-up!I had used urxvt forever before and the simple solution that works (even for ssh e.g.) is to ring the terminal bell, and urxvt just sets the window urgency hint upon that. I just do that in shell prompt unconditionally because if it's triggered in a focused window, then nothing happens. But if it's from a different workspace, I get this nice visual cue in my top bar for free.But features like setting urgency isn't available in wezterm (understandable, as it's not a cross-platform thing). I could patch that in the source, but the Emacser in me chose to do something more unholy. By default Lua is started in safe mode, which means loading shared C module is forbidden. I disabled that, and now use a bunch of missing stuff written in Rust and Zig interfaced with cffi. Don't recall ever having a crash so I am surprised by some of the other comments.reply",
      "Yeah, I use wezterm for remote editing at work because it has the best support for chords over ssh, and everything basically just works out of the box.  Really convenientreply",
      "I get the error `Your store is blocked` when trying to load the imagesreply",
      "Samereply",
      "Me too- these images are very important to the blog post, I wish I could see themreply",
      "Sorry, I ran out of the free Blob Storage transfer quota, should be fixed nowreply"
    ],
    "link": "https://rashil2000.me/blogs/tune-wezterm",
    "first_paragraph": "Posted: 6 October 2025BlogsHome"
  },
  {
    "title": "OS/2 Warp, PowerPC Edition (os2museum.com)",
    "points": 11,
    "submitter": "TMWNN",
    "submit_time": "2025-10-29T23:52:09 1761781929",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=45754660",
    "comments": [
      "I\u2019m always curious how these projects come about and survive: why go to all of the effort to port for a dead-end product line? As technically sweet as it is? I imagine they would\u2019ve found a decent market if they\u2019d ported to Power Mac.(Also, was the x86 emulation implemented in-house? I wouldn\u2019t be surprised if some niche small company had a x86 emulator for PPC product that they could be paid to port.)reply",
      "Didn\u2019t know that OS/2 had a PowerPC port, but more surprisingly, Windows NT also had a PowerPC port. Never heard of those.reply",
      "One of the original design requirements for NT was that it be portable between different CPU architectures, it was one of the driving forces behind its creation.So much so in fact, Microsoft developed NT 3.1 first on non-x86 architectures (i860 and MIPS), then later ported to x86, to ensure no x86 specific code made it in.NT supported quite a few architectures:> https://en.wikipedia.org/wiki/Windows_NT#Supported_platforms\"Windows NT 3.1 was released for Intel x86 PC compatible and PC-98 platforms, and for DEC Alpha and ARC-compliant MIPS platforms. Windows NT 3.51 added support for the PowerPC processor in 1995\"...NT is a pretty interesting bit of PC history, I can highly recommend the book \"Show Stopper!\" by G. Pascal Zachary that recounts its development, and also dives a bit into why making the OS portable across CPU architectures was so important to the team at the time.reply",
      "What could have been. If the respective parties had just gotten their acts together on the PPC 615, OS/2, WordPerfect, and Lotus.reply"
    ],
    "link": "https://www.os2museum.com/wp/os2-history/os2-warp-powerpc-edition/",
    "first_paragraph": "The PowerPC adventure\u2014by far the most exotic release of OS/2In December 1995, after unexpectedly long development (but is that really unexpected?), IBM finally \u201cshipped\u201d\u00a0OS/2 Warp, PowerPC edition. For brevity, this release will be further referred to as OS/2 PPC. Following years of hype and high expectation, the release was very low key and in fact marked the end of development of OS/2 for PowerPC. The product was only available to a limited number of IBM customers and was never actively marketed. OS/2 PPC may not even had a box, although there were nice looking official CDs.OS/2 PPC only supported an extremely limited range of hardware\u2014IBM Personal Power Series machines. Those were desktop models 830 and 850, and OS/2 PPC probably also supported the Power Series ThinkPads 820 and 850, though that can be only inferred from the fact that the graphics chipset employed by these ThinkPads was on the very short list of supported devices in OS/2 PPC.The IBM Power Series computers were IBM\u2019s"
  },
  {
    "title": "Board: New game console recognizes physical pieces, with an open SDK (board.fun)",
    "points": 130,
    "submitter": "nicoles",
    "submit_time": "2025-10-29T03:58:57 1761710337",
    "num_comments": 48,
    "comments_url": "https://news.ycombinator.com/item?id=45742456",
    "comments": [
      "I was looking for more info about developing for Board on https://board.fun/pages/developers and became confused because the page mentions an SDK that can be accessed but does not explain how or link to any other information. Poking around the website some more, I found https://board.fun/pages/support?hcUrl=%2Fen-US%23article-289... which clarifies:> Can I add or create my own games?> Soon. We\u2019re building tools that will let anyone design their own Board games, starting with developers and expanding to players. The future of play is one you can help create. Learn more at board.fun/developers.So I think I understand the SDK is not available yet. Can you clarify that developer tools are not yet available but are coming soon on https://board.fun/pages/developers to avoid confusion?reply",
      "To expand on the topic of the SDK: will the SDK be open-source?  Will I need to register as a developer or pay a fee to get the SDK?  If the SDK is open-source with no registration or fees required, then you have my attention.reply",
      "The SDK is open-source, no fees required. Coming in the next week or two. We're figuring out the specific details regarding registration, and would love feedback one way or the other if this is critical for you. If you want to be notified the moment it hits, email us: developers@board.funreply",
      "I can tell this is much more than just \u201cTabletop Simulator on a tablet\u201d, although at $500 you\u2019re likely to get a lot of attention from the Twilight Imperium and Gloomhaven crowd. I know more than a few childless people in my local gaming circle who would drop a half-large on accessories that simplify game execution.But clearly this product isn\u2019t about making existing board games easier to set-up/play/clean-up. I think the marketing dept has a lot of heavy lifting to do, convincing buyers that this isn\u2019t just Juicero for existing board games.reply",
      "I play games like Gloomhaven and TI4. Not sure how this product would simplify anything. Far too small for any of the more complex board games. I guess I could scroll around but then what does physical piece detection give me? Then it\u2019s $500USD. My game group and myself got Gloomhaven from Epic for free and played through the campaign together. BGA subscription is cheap. So many games have online implementations that are free. And I can buy a lot of boardgames for $500.What\u2019s the draw here?reply",
      "Exact same thoughts here. This should, imo, be marketed at boardgame nerds, who are adults, and not 3-7 year olds which it seems to currently be. Which toddler is asking for this for christmas? I suppose a boardgame nerd might buy it to use with their toddler, but that is a niche of a niche of a niche.reply",
      "Cool product. Is the SDK open? Any time I play a complex board game like Ark Nova, Spirit Island, etc. game running consumes a lot of time. So this tool is to me better showcased with a complex game that needs game-running that computers handle better. Also I'm curious about the board pieces and how more could be made. Do they have stickers on the bottom I could just transpose onto existing pieces, etcreply",
      "As a player: What's the lag? Does it depend on the game and the gesture?As a developer: I'd like to implement a \"game\" which would be ideal for Dynamicland (tens of cards with ID stickers on the corners), but this might be a simpler platform to set up and use. Would that be possible with the board as sold?reply",
      "Also curious about latency. In the past I've worked around latency using video sensors for high-bandwidth high-latency features, then literally glued a contact mic to my interface to get low latency tap detection. How does the Board hide latency?reply",
      "Not quite the first such product, Microsoft's original \"Surface\" advertised similar boardgame potential. But if it worked well, I don't know of anyone who was rich enough to try it!Hopefully the technology has matured since then.reply"
    ],
    "link": "https://board.fun/",
    "first_paragraph": "The first ever \nface-to-face game console \ndesigned to bring everyone together.\n    \n      Arcade, strategy, action, and more \u2014 the world\u2019s best game studios crafted an exclusive library of games just for Board, designed for every player and every age.\n    Open Board Arcade when the room shouts \u201cgame night.\u201d Grab the space ships and robots, pick teams and jump between four reimagined arcade classics. Ages 6+Tie on your apron grab your knifes, spoons, spice grinder, sponge, and hit the line! It\u2019s co-op chaos, culinary strategy, and party-night energy\u2014without the messy clean-up. Ages 6+The Bloogs need a hero. Use cannons, stairs, a shapeshifter ring, and blocks to guide adorable Bloogs across tricky terrain collecting crystals and exploring new worlds. Ages 6+Strata is a multi-layered strategy game where Tetris meets Chess. Place 3D blocks to claim territory and outmaneuver opponents in a beautiful contest of spatial skill. Ages 8+From New York to Tokyo to London, use your Spy Kit and yo"
  },
  {
    "title": "GLP-1 therapeutics: Their emerging role in alcohol and substance use disorders (oup.com)",
    "points": 144,
    "submitter": "PaulHoule",
    "submit_time": "2025-10-28T02:00:58 1761616858",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=45728525",
    "comments": [
      "Much has been said about the GLP drugs and their interactions with all kinds of addictive disorders. Alcohol, drugs, even gambling... Anecdotally, I \"struggled\" at times with gaming (not joking). I would find myself skipping meetings at times or ducking away to play online sometimes. It never became a real issue but I knew I did it and it was embarrassing.Once I started on tirzepatide, and then with retatrutide, the \"urge\" to swap over to my PC between meetings and load up a game is pretty much zeroed out.Is this an \"addiction\" or a form of \"abuse\" similar to alcohol or other drugs? I would have said no some time ago, but now I'm not sure. I definitely feel like, looking back, I was more or less \"addicted\" to video games. I don't want to romanticize it as some sort of \"escape\", it just is what it was.This was an unintended side effect (benefit?) of the drug for sure, in addition to acute weight loss of course.Unlike many others, even after titrating down and coming off the GLP's, I have not felt the urge to binge food, video games, or anything else. I maintain a healthy, active lifestyle and have kept my weight exactly where I prefer it. My relationship with my body and my time has massively improved. I feel like I am at risk of sounding like a complete shill, obviously, but in my mind these drugs can be something that absolutely has the potential to turn life around for many, many people.reply",
      "What do you now do instead of gaming? Do you find you have swapped for a different activity or a more balanced allocation of time among other things? Or do you still spend your off hours in the same way, but kicked the compulsion for gaming all day?reply",
      "I think it's probably still useful to distinguish addictions with hardcore substance related barriers to quitting (think withdrawals) from addictions where the barrier is a lack of dopamine or serotonin or simple habituation.For people with normal executive function, the second category of problems should be fairly tractable to overcome, whereas the first is still quite difficult.The second only really becomes an issue when you have a bit of executive dysfunction.Maybe that distinction is important and one merits the term addiction while the other doesn't? Though both categories seem to be relatively treatable with drugs that massively improve executive function, so the parallels are pretty glaring.reply",
      "The distinction you're reaching for is addiction vs dependance.reply",
      "How do you just start on retatrutide?  Did you sign up for a Phase 3 trial?reply",
      "Hard to say, you\u2019d need a study.League of Legends is \u201cused\u201d by a lot of people as medicine. Nobody hides away to play Stanley Parable. Lots of games, lots of genres, difficult to generalize.reply",
      "side question - has retatrutide been different enough to tirzepatide for you that you would recommend going to the extra effort to source it?reply",
      "I'm going to put on my Boomer pull-yourselves-up-by-the-bootstraps hat, but are you concerned about the loss of grit resulting from changing your behaviors without the drug?reply",
      "Definitely not GP, but I think it\u2019s pretty clear that whatever grit there was to have, GP did not have it. \u201cDie an early death due to being overweight or build the grit\u201d is strictly worse than \u201close the weight without building the grit, or build the grit\u201d, and it\u2019s even more so when you realize that \u201cor build the grit\u201d was never in the cards. Because then the choice becomes \u201cdie an early death or don\u2019t\u201c. Building the grit can be done on other, hopefully less lethal, projects.reply",
      "Not the GP, but do you think Serena Williams - world number 1 tennis womens' tennis player for 319 weeks, who trained for 5 hours per day at her peak - has insufficient grit?Because she went on GLP-1 to lose weight.reply"
    ],
    "link": "https://academic.oup.com/jes/article/9/11/bvaf141/8277723?login=false",
    "first_paragraph": ""
  }
]