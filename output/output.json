[
  {
    "title": "Wikipedia loses challenge against Online Safety Act (bbc.com)",
    "points": 567,
    "submitter": "phlummox",
    "submit_time": "2025-08-11T16:33:26 1754930006",
    "num_comments": 467,
    "comments_url": "https://news.ycombinator.com/item?id=44866208",
    "comments": [
      "> The government told the BBC it welcomed the High Court's judgment, \"which will help us continue our work implementing the Online Safety Act to create a safer online world for everyone\".Demonstrably false. It creates a safer online world for some.> In particular the foundation is concerned the extra duties required - if Wikipedia was classed as Category 1 - would mean it would have to verify the identity of its contributors, undermining their privacy and safety.Some of the articles, which contain factual information, are damning for the UK government. It lists, for example, political scandals [1] [2]. Or information regarding hot topics such as immigration [3], information that the UK government want to strictly control (abstracting away from whether this is rightfully or wrongfully).I can tell you what will (and has already) happened as a result:1. People will use VPNs and any other available methods to avoid restrictions placed on them.2. The next government will take great delight in removing this law as an easy win.3. The likelihood of a British constitution is increasing, which would somewhat bind future parliaments.[1] https://en.wikipedia.org/wiki/List_of_political_scandals_in_...[2] https://en.wikipedia.org/wiki/Category:Labour_Party_(UK)_sca...[3] https://en.wikipedia.org/wiki/Modern_immigration_to_the_Unit...reply",
      "The law was passed by the previous government and everyone assumed the next government would take great delight in reversing it.I wouldn\u2019t be so sure that any next government (which, by the way, there is still a non zero chance could be Labour) will necessarily reverse this. Maybe Reform would tweak the topics, but I\u2019m not convinced any party can be totally trusted to reverse this.reply",
      "If the current government reversed it, the 'oh think of the children' angle from the Tories/Reform against them would be relentless. I cant say they have been amazing at messaging as it is.reply",
      "> I wouldn\u2019t be so sure that any next government will necessarily reverse this.Agreed. I think the supposed justifications for mass population-wide online surveillance, restrictions and de-anonymization are so strong most political parties in western democracies go along with what surveillance agencies push for once they get in power. Even in the U.S. where free speech & personal privacy rights are constitutionally and culturally stronger, both major parties are virtually identical in what they actually permit the surveillance state to do once they get in office (despite sometimes talking differently while campaigning).The reason is that the surveillance state has gotten extremely good at presenting scary scenarios and examples of supposed \"disaster averted because we could spy on everyone\", or the alternative, \"bad thing happened because we couldn't spy on everyone\" to politicians in non-public briefings. They keep these presentations secret from public and press scrutiny by claiming it's necessary to keep \"sources and methods\" secret from adversaries. Of course, this is ridiculous because adversary spy agencies are certainly already aware of the broad capabilities of our electronic surveillance - it's their job after all and they do the same things to their own populations. The intelligence community rarely briefs politicians on individual operations or the exact details of the sources and methods which adversarial intelligence agencies would care about anyway. The vast majority of these secret briefings could be public without revealing anything of real value to major adversaries. At most it would only confirm we're doing the things adversaries already assume we're doing (and already take steps to counter). The real reason they hide the politician briefings from the public is because voters would be creeped out by the pervasive surveillance and domain experts would call bullshit on the incomplete facts and fallacious reasoning used to justify it to politicians.Even if a politician sincerely intended to preserve privacy and freedom before getting in office, they aren't domain experts and when confronted with seemingly overwhelming (but secret) evidence of preventing \"big bad\" presented unanimously by intelligence community experts, the majority of elected officials go along. If that's not enough for the anti-privacy agencies (intel & law enforcement) to get what they want, there's always the \"think of the children\" arguments. It's the rare politician who's clear-thinking and principled enough to apply appropriate skepticism and measured nuance when faced with horrendous examples of child porn and abuse which the law enforcement/intelligence agency lobby has ready in ample supply and deploys behind closed doors for maximum effect. The anti-privacy lobby has figured out how to hack representative democracy to circumvent protections and because it's done away from public scrutiny, there's currently no way to stop them and it's only going to keep getting worse. IMHO, it's a disaster and even in the U.S. (where I am) it's only slightly better than the UK, Australia, EU and elsewhere.reply",
      ">2. The next government will take great delight in removing this law as an easy win.As a rule of thumb, governments don't take actions which reduce their power.reply",
      "> 3. The likelihood of a British constitution is increasing, which would somewhat bind future parliaments.It would be an extraordinary amount of work for a government that can barely keep up with the fires of its own making let alone the many the world is imposing upon them.  Along with that, watching the horse trading going on over every change they make - I don't see how they ever get a meaningful final text over the line.It's not a mainstream political priority at all to my knowledge, so I'm mostly curious why you disagree!reply",
      "> It creates a safer online world for some.The thieves no longer have to hack servers in order to obtain sensitive data, they can just set up an age-check company and lure businesses with attractive fees.In that sense it is safer (for criminals).reply",
      "A British constitution makes no sense, power is delegated from the king not from the member states like in the US or Canada. The only way the UK could end up with a constitution that's meaningful and not performative would be after a civil war.reply",
      "https://en.wikipedia.org/wiki/Constitution_of_the_United_Kin...It may not make sense to you, but they've been arguing constitutional law there for hundreds of years.Plenty of monarchies also have modern single-document constitutions, like Norway, Spain and Thailand.reply",
      "We already have a constitution. It just isn't a written constitution:> The United Kingdom constitution is composed of the laws and rules\nthat create the institutions of the state, regulate the relationships\nbetween those institutions, or regulate the relationship between the\nstate and the individual.\nThese laws and rules are not codified in a single, written document.Source for that quote is parliamentary: https://www.parliament.uk/globalassets/documents/commons-com... - a publication from 2015 which considered and proposed a written constitution. But other definitions include unwritten things like customs and conventions. For example:> It is often noted that the UK does not have a \u2018written\u2019 or \u2018codified\u2019 constitution. It is true that most countries have a document with special legal status that contains some of the key features of their constitution. This text is usually upheld by the courts and cannot be changed except through an especially demanding process. The UK, however, does not possess a single constitutional document of this nature. Nevertheless, it does have a constitution. The UK\u2019s constitution is spread across a number of places. This dispersal can make it more difficult to identify and understand. It is found in places including some specific Acts of Parliament; particular understandings of how the system should operate (known as constitutional conventions); and various decisions made by judges that help determine how the system works.https://consoc.org.uk/the-constitution-explained/the-uk-cons...reply"
    ],
    "link": "https://www.bbc.com/news/articles/cjr11qqvvwlo",
    "first_paragraph": "Wikipedia has lost a legal challenge to new Online Safety Act rules which it says could threaten the human rights and safety of its volunteer editors.The Wikimedia Foundation - the non-profit which supports the online encyclopaedia - wanted a judicial review of regulations which could mean Wikipedia has to verify the identities of its users.But it said despite the loss, the judgement \"emphasized the responsibility of Ofcom and the UK government to ensure Wikipedia is protected\".The government told the BBC it welcomed the High Court's judgment, \"which will help us continue our work implementing the Online Safety Act to create a safer online world for everyone\".Judicial reviews challenge the lawfulness of the way in which a decision has been made by a public body.In this case the Wikimedia Foundation and a Wikipedia editor tried to challenge the way in which the government decided to make regulations covering which sites should be classed \"Category 1\" under the Online Safety Act - the st"
  },
  {
    "title": "I tried every todo app and ended up with a .txt file (al3rez.com)",
    "points": 792,
    "submitter": "al3rez",
    "submit_time": "2025-08-11T13:59:02 1754920742",
    "num_comments": 505,
    "comments_url": "https://news.ycombinator.com/item?id=44864134",
    "comments": [
      "Reading through the comments under this thread, there are many users who swear by a plain text file, but who then build quite a lot of snowflake software to regain functionality offered by more structured TODO applications. That includes:- having your computer alert you to things that come up- being able to tag notes- being able to add events to a calendar- being able to set priority of tasks- expecting prioritized/currently relevant tasks to be at the top of the agenda- being able to add recurring tasks- full-text search (grepping)- formatting features (markdown)Some of the laborious (or, in my opinion, plain unholy) solutions include:- feeding TODOs to an LLM to filter for the currently relevant ones and send Telegram notifications- hand-copying currently relevant tasks to the top of the TODO list- running a script on a VPS to sync notifications- set up cron job with git commit- writing post-it notes by handI would encourage everyone to try out emacs with org-mode. It takes some time to get used to the editor and its keybindings (though provisions exist for vim users), but _every_ item on the list above is handled out of the box, or is offered through a free and maintained plugin.The author of the OP claims to have tried _every_ todo app, and has afterwards moved (regressed?) to writing notes in a plain text file, but there is a path extending from this point that the author has not walked yet. I strongly suggest that, especially for people with a computing or technical background, it is an undisputed upgrade. https://doc.norang.ca/org-mode.html being the bible, of course.reply",
      "What this shows to me, as someone who has committed some of the unholy crimes above, is that people want their system, however esoteric, to come naturally to them.I think reading docs, understanding a new system which someone else has designed, and fitting one's brain into _their_ organisational structure is the hard part. Harder than designing one's own system. It's the reason many don't stick with an off-the-shelf app. Including Org mode.reply",
      "> What this shows to me, as someone who has committed some of the unholy crimes above, is that people want their system, however esoteric, to come naturally to themI think this is a vocal minority. Outside of internet comment sections, most everyone I know doesn\u2019t care that much about their todo list software.The most productive people I ever worked with all had really minimal productivity software. For one person it was a Google doc with nested lists. I know several people who preferred physical sticky notes or 3x5 note cards.A lot of the people I\u2019ve worked with who built elaborate productivity systems and custom software weren\u2019t all that productive. They seemingly spent as much time doing productivity rituals and rearranging their productivity software stack as they did doing actual work. I count the really heavy Notion users in this category because I\u2019ve recently been pulling my hair out dealing with a couple PMs who think \u201creorganizing Notion\u201d and adding more rules for Notion is a good use of their time each week.The most extreme example I remember was the eccentric coworker who was building an AI-powered productivity tool that was supposed to optimize his todo lists and schedule them according to his daily rhythms. He spent so much time working on it that our manager had to remind him daily to stay on track with his real work. He was obsessed with \u201cproductivity tooling\u201d but the productivity was secondary.Not everyone is like this, but it happens a lot.reply",
      "I strongly agree. I think it's a form of procrastinating.I read about all these complex systems for notes and second brains and whatnot.All procrastinating imho.reply",
      "Same. I dabbled in the second brain fad for a while, until I realized I already have a first brain.It was helpful to create new mental models, but I now much prefer using my actual brain to organize my thoughts.reply",
      "That\u2019s been my personal experience. Spend plenty of time looking at all kind of options to optimize my ir my teams workflow.  Then just fallback on pen and paper or some very simple excel spreadsheet. Something thinking about being more productive makes you feel productive.reply",
      "Sounds similar to playing video games: the rules are simple, so once you understand them, you can feel mighty and powerful simply by accomplishing banal tasks. Makes for a great dopamine rush.reply",
      "I think it's actually a selection bias. Who is more likely to spend a lot of time on productivity systems -- a person who is on top of their obligations or a person who is drowning in them. A naturally organized person can do with simple txt, they are already doing okay. A chaotic person can build whatever complex process they wish, they will still fail.reply",
      "I spent several years trying to make a custom todo system and ended up back where I started using CalDAV and a basic todo app and calendar. Turns out I was always procrastinating because I didn't want to force myself to adapt to something simple.reply",
      "There\u2019s a now quite dated comment from Merlin Mann: \"Joining a Facebook group about productivity is like buying a chair about jogging.\u201dIt\u2019s fuzzy - but my recollection was Mann was a fairly renown productivity influencer (although I guess we wouldn\u2019t have called it that then), who had an apostasy about it all.reply"
    ],
    "link": "https://www.al3rez.com/todo-txt-journey",
    "first_paragraph": "\nAlireza Bashiri\n Alireza Bashiri   Home    Blog    Work    Projects    \nNewsletter\n    RSS feed    Contact    Twitter    GitHub    \nLinkedIn\n   August 11, 2025 I\u2019ve tried them all. Notion, Todoist, Things 3, OmniFocus, Asana, Trello, Any.do, TickTick. I even built my own todo app once (spoiler: I never finished it). After years of productivity app hopping, I\u2019m back to where I started: a plain text file called todo.txt.I\u2019m not alone in this. Jeff Huang wrote about his \u201cnever-ending .txt file\u201d that he\u2019s used for over 14 years. Reading his post validated everything I\u2019d discovered on my own.My productivity journey started like everyone else\u2019s. I\u2019d devour blog posts about getting things done or spot a cool app and think \u201cthis is it, this will finally organize me.\u201d I\u2019d burn hours building the perfect system, creating categories, tags, projects, labels. Setting it up felt like work.Then reality hits. The app wants $9.99/month. The sync breaks. The company sells out and dies. Or worse - I was"
  },
  {
    "title": "GitHub is no longer independent at Microsoft after CEO resignation (theverge.com)",
    "points": 924,
    "submitter": "Handy-Man",
    "submit_time": "2025-08-11T15:47:48 1754927268",
    "num_comments": 692,
    "comments_url": "https://news.ycombinator.com/item?id=44865560",
    "comments": [
      "I think that just like it happened with Apple after they made it out of bankruptcy, Microsoft being the cool guys phase is slowly over.Xamarin is no more, after the whole MAUI rewrite without backwards compatibility to Xamarin.Forms, killing VS4Mac, shortly after having rewriten the underlying Xamarin based IDE into Mac, what survives is a subset of Xamarin tech for mobile and WebAssembly workloads..NET is now cross platform, but only as long as it doesn't hurt VS sales, with GUI workloads, profilers, still being mostly Windows only, and partially supported on VSCode, which also has the same VS license.A proper cross platform IDE experience requires getting Rider.Then there is the issue they seem to be shoting into all directions, with GUI frameworks, Web, Blazor, Aspire, to see what sticks.Github even with the previous CEO was already a delivery mechanism for Azure and AI efforts, now it will be full steam ahead, as per new org chart.VC++ after betting other compilers in C++20 support, seems to have lost its resources struggling  to deliver C++23, and also probably affected by the Secure Future Initiative, and decisions for safer languages.But hey 4 trillion valuation, so from shareholders point of view, everything is going great.reply",
      "Microsoft being the cool guys? The cool guys? Mwuhahahhaa.This gave me the good belly laugh I needed.For the last 25 years, Microsoft was known for:- being the no. 1 enemy of free software- shipping the worst web browser in existence, despite 80%+ market share- making corrupt deals with governments around the world to tie them to their office software suite- creating vendor-locked proprietary extensions to kill open technologies (ActiveX plugins, Silverlight, C++/CLI, MSJVM, etc.)- making cringe hardware that basically noone purchased (Zune, Windows Phone)The last time they might have been considered the \"cool guys\" was sometime in the 90s.reply",
      "This comment comes some 15 years late. Microsoft runs the biggest org on github and has open sourced a lot of their own code under permissive licenses.IE has been dead and buried for ages. Edge doesn't have even close to the same market share and is based on Chromium.They build more and more of their own UIs on Electron.I honestly don't remember when they tried to snare someone to use proprietary extensions to something open. I probably have missed a few instances.Long story short: MS isn't a saint. They are a business. And they have behaved relatively nice for so long that some young adults don't know any other side of MS now.reply",
      "Idk i can think of a long list of awful stuff coming out of ms that is modern. They put fing ads in an os among other atrocities.I put them behind meta on the evilness meter but i think google  is less evil which speaks volumes.The only side of ms that i have any love for is xbox but that is also waning with all the studio acquisitions.reply",
      "Don\u2019t Apple and Ubuntu also advertise products in their OS also?reply",
      "Apple barely does it and only for their products. I agree with you that that\u2019s already too much and too annoying but that\u2019s an order of magnitude less than Microsoft who advertise their products pretty aggressively AND ALSO are advertising for whoever gave them money too.Ubuntu I didn\u2019t use it for years, there are tons of other distributions that I prefer now but last time I checked, there was a removable default shortcut to amazon. That\u2019s an awful symbol, if you ask me, to associate Ubuntu and its meaning to Amazon but it\u2019s nothing when compared to Apple or Microsoft (dare I say Google) behaviors.reply",
      ">Apple barely does it and only for their productsAnd U2reply",
      "Apple barely does it and only for their products.With the recent notable exception of the F1 movie advertisement that arrived as a notification from the Wallet app.\nhttps://daringfireball.net/2025/06/more_on_apples_trust-erod...I disabled Wallet notifications immediately :-(reply",
      "Obviously not defending it, but isn\u2019t the F1 movie produced by Apple?reply",
      "Apple pushes their products often on iOS and many of them can't ve turned off or can't be turned off easily.So you have notifications that you can only get rid of by engaging with the Apple ads.reply"
    ],
    "link": "https://www.theverge.com/news/757461/microsoft-github-thomas-dohmke-resignation-coreai-team-transition",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: I built an offline, open\u2011source desktop Pixel Art Editor in Python (github.com/danterolle)",
    "points": 37,
    "submitter": "danterolle",
    "submit_time": "2025-08-11T22:21:53 1754950913",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=44870137",
    "comments": [
      "I recently discovered and have been fairly happy with PixelLab - an AI pixel art generator. I feel like they have a ways to go in features and UX, but it shows promise.reply",
      "Very nice to see that this project is hand-crafted and not AI-generated like 99% of the submissions hereSo, congrats on your release.reply",
      "Thanks! Although I had to use it for some things (like the logo, for example, and I\u2019m not a \"graphic guy\"), in the end, since it\u2019s a simple project by design, I didn\u2019t mind, and the result isn\u2019t bad at all.reply",
      "I like that it really is simply built and packaged, I'm sure it was fun to hack away at. There's something about gluing together a million packages which sucks the fun out of tinkering (for me, at least).reply",
      "That\u2019s also why the project was built from scratch. The only real dependency of the project is PySide6. The icons don\u2019t come from any package. PyInstaller is used solely for bundling purposes. As outlined in the README.md, running Tilf requires nothing more than an installed version of Python (3).reply",
      "Great project!I have one very silly question... Why is the elf logo not pixel art? :)reply",
      "Congratulations!What made you decide to go with PySlide6?reply",
      "I already have some experience with Python/PySide6, and I was mainly interested in having a working prototype as soon as possible (I\u2019m experimenting with SDL3 and animating squares isn\u2019t exactly thrilling!). Plus, Qt widgets integrate very well with Python, it is so easy to create a section, especially when the documentation is well written, that helps a lot. Also, with PyInstaller, the build process for each platform is fairly straightforward (although for customized icons, there are a few extra steps to take).There are some downsides of course (like the bundle size, for example), but that's not a problem, the core idea is: double-click on Tilf and start drawing right away.reply",
      "why not just the default tk widgets, might be much less of external dependencies?reply"
    ],
    "link": "https://github.com/danterolle/tilf",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Tilf (Tiny Elf) is a simple yet powerful pixel art editor built with PySide6.\nIt\u2019s designed for creating sprites, icons, and small 2D assets with essential tools, live preview, undo/redo, and export options.There are several Pixel Art Editors that do the same things and even much more,\nbut many require an account registration or the insertion of an e-mail or have a certain business model.I'm not interested in all that, my goal is to be able to create sprites freely, with (almost) zero dependencies,\nzero setup time, zero installations:Tilf starts and I begin to draw what I want.It runs on Windows, MacOS and GNU/Linux.Download the latest release from the releases page.Windows 11 -> Download\n\nMacOS 15.6 -> Download\n\nGNU/Linux (P"
  },
  {
    "title": "Claude Code is all you need (dwyer.co.za)",
    "points": 480,
    "submitter": "sixhobbits",
    "submit_time": "2025-08-11T14:03:46 1754921026",
    "num_comments": 270,
    "comments_url": "https://news.ycombinator.com/item?id=44864185",
    "comments": [
      "I love this article just for the spirit of fun and experimentation on display. Setting up a VPS where Claude is just asked to go nuts - to the point where you're building a little script to keep Claude humming away - is a really fun idea.This sort of thing is a great demonstration of why I remain excited about AI in spite of all the hype and anti-hype. It's just fun to mess with these tools, to let them get friction out of your way. It's a revival of the feelings I had when I first started coding: \"wow, I really can do anything if I can just figure out how.\"Great article, thanks for sharing!reply",
      "> \u201cwow, I really can do _anything_ if I can just figure out howExcept this time it\u2019s \u201cif I can just figure out how and pay for the Claude API usage\u201d.This is one of the sadder things about AI usage getting more standard that I haven\u2019t seen discussed much\u2014-the barrier to entry is now monetary rather than just knowledge-based, which will make it _much_ harder for young people with no money to pick up.Yes, they can still write code the manual way, but if the norm is to use AI I suspect that beginner\u2019s guides, tutorials, etc. will become less common.reply",
      "Yep, I used to spend a lot of time learning PHP on a web server which was part of my internet subscription. Without it being free, I would never have learn how to create websites and would have never got in programming, the trigger was that free web hosting with PHP that was part of the internet connection my parents were already paying forreply",
      "Eh back in the day computers were expensive and not everyone could afford one (and I don't mean a library computer that you can work on, one you can code and hack on).  The ubiquity of computing is not something that's been around forever.There have always been costs and barriers for the cutting edge.reply",
      "For me, I can\u2019t get into using AI tools like Claude Code.  As far as I go is chat style where I\u2019m mostly in control.  I enjoy the actual process of crafting code myself.  For similar reasons, I could never be a manager.Agents are a boon for extraverts and neurotypical people.  If it gets to the point where the industry switches to agents, I\u2019ll probably just find a new careerreply",
      "I strongly disagree agents are for extroverts.I do agree it\u2019s definetly a tool category with a unique set of features and am not surprised it\u2019s offputting to some. But it\u2019s appeal is definetly clear to me as an introvert.For me LLM:s are just a computer interface  you can program using natural language.I think I\u2019m slightly ADD. I love coding _interesting_ things but boring tasks cause extreme discomfort.Now - I can offload the most boring task to LLM and spend my mental energy on the interesting stuff!It\u2019s a great time to be a software engineer!reply",
      "> For me LLM:s are just a computer interface you can program using natural language.I wish they were, but they're not that yet because LLMs aren't very good at logical reasonsing. So it's more like an attempt to program using natural language. Sometimes it does what you ask, sometimes not.I think \"programming\" implies that the machine will always do what you tell it, whatever the language, or reliably fail and say it can't be done because the \"program\" is contradictory, lacks sufficient detail, or doesn't have the necessary permissions/technical capabilities. If it only sometimes does what you ask, then it's not quite programming yet.> Now - I can offload the most boring task to LLM and spend my mental energy on the interesting stuff!I wish that, too, were true, and maybe it will be someday soon. But if I need to manually review the agent's output, then it doesn't feel like offloading much aside from the typing. All the same concentration and thought are still required, even for the boring things. If I could at least trust the agent to tell me if it did a good job or is unsure that would have been helpful, but we're not even there yet.That's not to say the tools aren't useful, but they're not yet \"programming in a natural language\" and not yet able to \"offload\" stuff to.reply",
      "> ... LLMs aren't very good at logical reasonsing.I'm curious about what experiences led you to that conclusion. IME, LLMs are very good at the type of logical reasoning required for most programming tasks. E.g. I only have to say something like \"find the entries with the lowest X and highest Y that have a common Z from these N lists / maps / tables / files / etc.\" and it spits out mostly correct code instantly. I then review it and for any involved logic, rely on tests (also AI-generated) for correctness, where I find myself reviewing and tweaking the test cases much more than the business logic.But then I do all that for all code anyway, including my own. So just starting off with a fully-fleshed out chunk of code, which typically looks like what I'd pictured in my head, is a huge load off my cognitive shoulders.reply",
      "The experience was that I once asked an LLM to write a simple function and it produced something very wrong that nothing with good reasoning abilities should ever do. Of course, a drunk or very tired human could have done the same mistake, but they would have at least told me that they were impaired and unsure of their work.I agree that most of the time it does most simple tasks mostly right, but that's not good enough to truly \"offload\" my mental effort. Again, I'm not saying it's not useful, but more than working with a junior developer it's like working with a junior developer who may or may not be drunk or tired and doesn't tell you.But mostly my point is that LLMs seem to do logical reasoning worse than other things they do better, such as generating prose or summarising a document. Of course, even then you can't trust them yet.> But then I do all that for all code anyway, including my ownI don't, at least not constantly. I review other people's code only towards the very end of a project, and in between I trust that they tell me about any pertinent challenge or insight, precisely so that I can focus on other things unless they draw my attention to something I need to think about.I still think that working with a coding assistant is interesting and even exciting, but the experience of not being able to trust anything, for me at least, is unlike working with another person or with a tool and doesn't yet allow me to focus on other things. Maybe with more practice I could learn to work with something I can't trust at all.reply",
      "What was the simple function?reply"
    ],
    "link": "https://dwyer.co.za/static/claude-code-is-all-you-need.html",
    "first_paragraph": "How I use Claude Code for work, fun, and as a text editorI installed Claude Code in June. I'd tried Cursor and Cline and Zed and a few others, but all of them felt clunky to me because I'm used to doing nearly everything in vanilla vim and my terminal. Claude Code was the first tool I tried that felt like it fit into my workflows perfectly rather than needing me to adapt to new tools.It also worked really, really well.I quickly cancelled my GPT subscription and put the $20/month towards Anthropic instead. Losing GPT advanced voice mode and dealing with the extra UI lag and lack of polish in Claude Desktop and Mobile apps was a bit of an adjustment to make but the terminal tool was fun enough that I didn't care.Within a few days I'd upgraded to the $100/month MAX plan to try out Opus and to stop hitting limits.Here's a description of some of the things I've used it for so far. Mainly fun stuff while I figure out how to use it, but I'm starting to use it more and more for 'real' work stu"
  },
  {
    "title": "Show HN: Play Pok\u00e9mon to unlock your Wayland session (github.com/adopi)",
    "points": 63,
    "submitter": "anajimi",
    "submit_time": "2025-08-10T12:15:01 1754828101",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=44854686",
    "comments": [
      "Heavy customization is important to me on the Linux desktop. This project has given me a lot more faith in Wayland than 5 years of hearing debates about it.reply",
      "Incredible that we're getting something like this before a plain good old screensaverreply",
      "Thanks for your idea! I think it is totally possible to implement a screensaver with the ext-session-lock protocol. I will try to explore this idea when I have time in the next few months :)reply",
      "Do you have a Ko-fi?\n'cause I would really enjoy that screensaver.I adore this project on its own merits too because using the memory values in an emulated game is something that has fascinated me since Twitch Plays Pok\u00e9mon integrated their Twitch display!reply",
      "For use with vintage computers that use CRTs? If not, what kind of oddball display / use-case do you have, where it would be better to play a screensaver than to follow the usual modern flow of display dim -> display black -> display sleep -> computer lock -> [maybe] computer sleep?reply",
      "One of the great strengths of Linux, and one of the things that draws new people in, is the custizability and making the system your own to whatever degree you want. That a \"modern\" display manager doesn't let you have a screensaver and people try to cover up for it with \"you're just trying to use your system wrong. Be normal and use your system like we say is normal\" is embarassing.reply",
      "What modern display manager doesn't let you? In KDE the screensaver is merged into the screen locker settings, you can pick any \"wallpaper plugin\" which includes slideshows, video, or animation if you plug in e.g. https://store.kde.org/p/2143912 or https://store.kde.org/p/2194089reply",
      "I think that's a little dramatic. Screen savers originally served a purpose, and it's not unreasonable to be unaware that some people see them are customization.If you think it's embarrassing, you're welcome to contribute a working implementation or pay someone else to do it. Otherwise, I don't see how it's embarrassing.reply",
      "If there's one thing I never tire of, it's someone telling me that I don't need something or how I'm doing it wrong. I would love a screensaver that scrubs my OLED pixels.reply",
      "OLEDs still have burn-in issues even with all the fancy mitigation systems they have.reply"
    ],
    "link": "https://github.com/AdoPi/wlgblock",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Wayland Gameboy locker\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\nThis project replaces the usual password screen with a Gameboy emulator running a patched Pok\u00e9mon game!\nTo unlock your session, you have to solve a little challenge, kind of like a mini escape room built into your OS.I've been a Linux enthusiast since I was a kid. What always captivated me was the freedom to customize my system exactly the way I wanted. With Wayland, we've reached an incredible level of performance. It's like turning your operating system into a video game! I've always been fascinated by the blend of fun and the serious, technical nature of an OS. That\u2019s what inspired me to create this project.I started by studying Wayland, its protocol and how to build a compositor. The"
  },
  {
    "title": "How to Teach Your Kids to Play Poker: Start with One Card (bloomberg.com)",
    "points": 39,
    "submitter": "ioblomov",
    "submit_time": "2025-08-08T19:11:13 1754680273",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=44840573",
    "comments": [
      "https://archive.ph/AjiWY",
      "I like this. Most people try to teach card games by listing every rule, but it's much easier to play a simpler version then add in new rules.I play the Chinese card game Zhao Peng You (Finding Friends, part of the Sheng Ji family of games https://en.wikipedia.org/wiki/Sheng_ji), which is a trick taking game with a trump suit that changes between games, a trump number that changes between games, and a team selection mechanic rather than fixed teams. It's insanely hard to learn everything at once, so we usually start new people with fixed teams and trumps just to get the feel of a team-based trick-taking game, before adding in the complications.reply",
      "Had some family come over and play Texas Hold\u2019em with us and their kids.  It was clear they were too stupid to be intimidated so there was no possibility of bluffing, instead I just folded over and over again until I had two really good cards and then would see me no matter what I bet and ai grew my bankroll that way.reply",
      "playing poker with the following truly undermines the entire experience:people that dont understand rules 100%wagers with no real value (time/money/snacks)people who dont want to play outrightreply",
      "GTO goes out the window when a drunk guy sits down with a few friends. Either you're gonna grab the pot a few times or bust because the dude went all in with dueces against your KA. he wins a flush on the river.reply",
      "One of the best life lessons I learned was while perusing a poker strategy book in a bookstore as a teen. I\u2019ve never been into poker, not even sure why I picked it up.\nOne thing it said was the most important thing to remember is that most of your hands will be crap. Don\u2019t get attached to a bad hand and don\u2019t convince yourself that an ok hand is a good hand. If you just fold the bad hands and play the good ones you\u2019re already a better player than most.I took that to heart and it has served me well in life.reply",
      "For me, it's \"decisions, not results.\" Poker will teach you patience and acceptance of that which is out of your control.reply",
      "That just makes you a tight passive  player which is not the worst kind of player to be but also not likely to win you a lot of moneyreply",
      "Being a loose aggressive player is far more likely to lead to you losing a lot of money, than winning a lot of money.Once you consider what the house earns, poker is a net negative for the players. In order for there to be some big winners, there have to be a lot of losers. And a shocking number of those losers will, thanks to our selective memories, consider themselves winning players.reply",
      "In popular poker you are just playing against other players, not the house.reply"
    ],
    "link": "https://www.bloomberg.com/news/articles/2025-08-08/how-to-teach-your-kids-poker-with-one-card-at-age-four",
    "first_paragraph": ""
  },
  {
    "title": "OpenSSH Post-Quantum Cryptography (openssh.com)",
    "points": 351,
    "submitter": "throw0101d",
    "submit_time": "2025-08-11T12:01:58 1754913718",
    "num_comments": 94,
    "comments_url": "https://news.ycombinator.com/item?id=44863242",
    "comments": [
      "The most important point is buried at the bottom of the page:> all the post-quantum algorithms implemented by OpenSSH are \"hybrids\" that combine a post-quantum algorithm with a classical algorithm. For example mlkem768x25519-sha256 combines ML-KEM, a post-quantum key agreement scheme, with ECDH/x25519, a classical key agreement algorithm that was formerly OpenSSH's preferred default. This ensures that the combined, hybrid algorithm is no worse than the previous best classical algorithm, even if the post-quantum algorithm turns out to be completely broken by future cryptanalysis.Using a hybrid scheme ensures that you're not actually losing any security compared to the pre-quantum implementation.reply",
      "Hybrid schemes give you improved security against algorithmic flaws. If either algorithm being used is broken, the other gives you resilience. But hybrid schemes also double (or more) your exposure to ordinary implementation bugs and side-channels.Since Quantum Computers at scale aren't real yet, and those kinds of issues very much are, you'd think that'd be quite a trade-off. But so much work has gone into security research and formal verification over the last 10 years that the trade-off really does make sense.reply",
      "Unless the implementation bug is severe enough to give RCE, memory dumping, or similar, I don't see how a bug in the MLKEM implementation (for example) would be able to leak the x25519 secret, even with sidechannels. A memory-safe impl would almost guarantee you don't have any bugs of the relevant classes (I know memory-safe != sidechannel-safe, but I don't see how sidechannels would be relevant). You still need to break need both to break the whole scheme.reply",
      "I've rewritten some PQ implementations that had RCEs and memory disclosure vulnerabilities in them. No shade, but those implementations were from scientists who don't typically build production systems. As an industry, we're past this phase. Side-channels more commonly reveal plaintext than key material, but that shouldn't be fatal in the case of hybrid key agreement.Based on what we've seen so far in industry research, I'd guess that enabling Denial of Service is the most common kind of issue.reply",
      "I always wondered about this claim.If I have a secret, A, and I encrypt it with classical algorithm X such that it becomes A', then the result again with nonclassical algorithm Y such that it becomes A'', doesn't any claim that applying the second algorithm could make it weaker imply that any X encrypted string could later be made easier to crack by applying Y?Or is it that by doing them sequentially you could potentially reveal some information about when the encryption took place?reply",
      "Here's we're talking about hybrid key-agreement. It's more like you agree secret A with a peer using the magic of Diffie-Helman, separately you make up secret B and encapsulate (which is basically a form of asymmetric encryption) that using a PQ algorithm and then send that on, and then derive C by mixing A and B. You're not actually encrypting something twice.Some government and military standards do call for multiple layers of encryption when handling data, but it's just that multiple layers. You can't ever really make that kind of encryption weaker by adding a new \"outer\" layer. But you can make encryption weaker if you add a new \"inner\" layer that handles the plaintext. Side-channels in that inner layer can persist even through multiple layers of encryption.reply",
      "This is true, but there is a subtle point that key K1 used for the classical algorithm must be statistically independent of key K2.If they're not, you could end up where second algorithm is correlated with the first in some way and they cancel each other out. (Toy example: suppose K1 == K2 and the algorithms are OneTimePad and InvOneTimePad, they'd just cancel out to give the null encryption algorithm. More realistically, if I cryptographically break K2 from the outer encryption and K1 came from the same seed it might be easier to find.)reply",
      "I think the answer is either very simple, or impossible to give without details.If I recall my crypto classes and definitions correctly, if you have a perfect encryption X, a C = X(K, P) has zero information about P unless you know K. Thus, once X is applied, Y is not relevant anymore.Once you have non-perfect encryptions, it depends on X and Y. Why shouldn't a structure in some post-quantum algorithm give you information about, say, the cycle length of an underlying modular logarithm like RSA? This information in turn could shave fractions of bits off of the key length of the underlying algorithm. These could be the bits that make it feasible to brute-force. Or they could be just another step.On the other hand, proving that this is impossible is ... would you think that a silly sequence about rabbits would be related to a ratio well-known in art? There are such crazy connections in math. Proving that something cannot possibly connected is the most craziest thing ever.But that's the thing about crypto: It has to last 50 - 100 years. RSA is on a trajectory out. It had a good run. Now we have new algorithms with new drawbacks.reply",
      "What kinds of side channels are you thinking of? Given the key exchanges have a straightforward sha256/sha512 combiner, it would be surprising that a flaw in one of the schemes would give a real vulnerability?I could see it being more of a problem for signing.reply",
      "Yeah, key agreement in the context of SSH is quite forgiving of timing side channels as SSH uses ephemeral keys. There's no prospect of repeatedly re-doing the key agreement to gather more statistics on the counterparty's timing.reply"
    ],
    "link": "https://www.openssh.com/pq.html",
    "first_paragraph": "\nOpenSSH supports a number of cryptographic key agreement algorithms\nconsidered to be safe against attacks from quantum computers. \nWe recommend that all SSH connections use these algorithms.\n\nOpenSSH has offered post-quantum key agreement (KexAlgorithms)\nby default since release 9.0 (2022), initially via the\nsntrup761x25519-sha512 algorithm. More recently, in OpenSSH 9.9,\nwe have added a second post-quantum key agreement mlkem768x25519-sha256\nand it was made the default scheme in OpenSSH 10.0.\n\nTo encourage migration to these stronger algorithms, OpenSSH 10.1 will warn\nthe user when a non post-quantum key agreement scheme is selected, with the\nfollowing message:\n\n** WARNING: connection is not using a post-quantum kex exchange algorithm.\n** This session may be vulnerable to \"store now, decrypt later\" attacks.\n** The server may need to be upgraded. See https://openssh.com/pq.html\n\n\nThis warning is displayed by default but may be disabled via the\nWarnWeakCrypto option in\nssh_config(5).\n\n"
  },
  {
    "title": "Why tail-recursive functions are loops (kmicinski.com)",
    "points": 65,
    "submitter": "speckx",
    "submit_time": "2025-08-08T15:10:54 1754665854",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=44837949",
    "comments": [
      "Practically the day after I learned about tail recursion in CS class, I learned that almost all recursive calls can be translated to iteration, that in many cases the iterative version is easier to scan, is as fast if not faster, and that they can usually handle much much larger inputs than recursion due to avoiding stack overflow.Tail recursion is meant to fix the latter. But what we mean to happen and what actually happens ain't ever exactly similar.Tail recursion IME is a bigger foot gun than relying on someone to add a new conditional branch at the end of a block in an iterative algorithm without fucking it up in the process. And iteration responds generally better to Extract Function. And while I can think of counter cases easily enough, in the large iteration is less work and less vigilance. And you cannot scale a project up without the vigilance requirement amortizing basically to 0 per line of code.reply",
      "In my mind, the biggest advantage to using tail recursion over vanilla loops is the ability to keep using persistent data structures without any (apparent) mutation.At least in theory, a tail recursive call will be converted into a dumb jump, so there shouldn't be a performance penalty, but since from your code's perspective you're passing in the stuff for the next iteration, you can keep using the pretty and easy-to-reason-about structures without creating any kind of mutable reference.I'm not 100% sold on tail recursion in a broader sense, but at least with Clojure's loop/recur stuff it is kind of cool to be able to keep using persistent data structures across iterations of loops.reply",
      "What makes tail recursion \"special\" is that there exists a semantically equivalent mutable/iterative implementation to something expressed logically as immutable/recursive. [0]Of course, this means that the same implementation could also be directly expressed logically in a way that is mutable/iterative.func pow(uint base, uint n): n == 0 ? return 1 : return n * pow(base, n-1)is justfunc pow(uint base, uint n): uint res = 1; for(i=0; i<n; i++){ res *= n} return resThere is no real \"advantage\" to, or reason to \"sell\" anybody on tail call recursion if you are able to easily and clearly represent both implementations, IMO. It is just a compiler/runtime optimization, which might make your code more \"elegant\" at the cost of obfuscating how it actually runs + new footguns from the possibility that code you think should use TCO actually not (because not all immutable + recursive functions can use TCO, only certain kinds, and your runtime may not even implement TCO in all cases where it theoretically should).As an aside, in C++ there is something very similar to TCO called copy-elision/return-value-optimization (RVO): [1]. As with TCO it is IMO not something \"buy into\" or sell yourself on, it is just an optimization you can leverage when structuring your code in a way similar to what the article calls \"continuation passing style\". And just like TCO, RVO is neat but IMO slightly dangerous because it relies on implicit compiler/runtime optimizations that can be accidentally disabled or made non-applicable as code changes: if someone wanders in and makes small semantic to changes to my code relying on RVO/TCO for performance they could silently break something important.[0] EXCEPT in practice all implementation differences/optimizations introduce observable side effects that can otherwise impact program correctness or semantics. For example, a program could (perhaps implicitly) rely on the fact that it errors out due to stack overflow when recursing > X times, and so enabling TCO could cause the program to enter new/undesirable states; or a program could rely on a functin F making X floating point operations taking at least Y cycles in at least Z microseconds, and not function properly when F takes less than Z microseconds after enabling vectorization. This is Hyrum's Law [2].[1] https://en.wikipedia.org/wiki/Copy_elision#Return_value_opti...[2] https://www.hyrumslaw.com/reply",
      "I would argue having the parameters that change during the loop be explicit is a very nice advantage. Agree that the things can be equivalent in terms of execution but the readability and explicitness, and the fact that all the parameters are listed in the same place is great.reply",
      "Agreed. Some people really like FP a lot, and I think it's underrated that the kinds of functions where TCO is applicable tend to be so simple that they are not really that inelegant when expressed imperatively. My true opinion is that relying on TCO is usually choosing ideological adherence to FP (or \"code that looks cooler\") over reliability/performance/communication.That said, just as I'd expect experienced C++ programmers to be able to recognize others' code using RVO and be careful not to restructure things to break it, I'd concede that experienced FP programmers might be unlikely to accidentally break others' TCO. It's just that ad absurdum you cannot expect every developer to be able to read every other developers' mind and recognize/workaround all implicit behavior they encounter.reply",
      "I think you're confusing mutation and variable reassignment?reply",
      "I'm saying that if I do a regular loop, something needs to explicitly mutate, either a reference or a value itself.`for (var i = 0; i < n, i++)`, for example, requires that `i` mutate in order to keep track of the value so that the loop can eventually terminate. You could also do something like:    var i = new Obj(); \n    while (!i.isDone()) {\n        //do some stuff\n        i = new Obj(); \n    }\n\nThere, the reference to `i` is being mutated.For tail recursion, it's a little different.  If I did something like Factorial:    function factorial(n, acc) {\n        if (n <= 1) return acc;\n        return factorial(n - 1, acc * n);\n    }\n    \nDoing this, there's no explicit mutation.  It might be happening behind the scenes, but everything there from the code perspective is creating a new value.In something like Clojure, you might do something like    (defn vrange [n]\n      (loop [i 0 v []]\n        (if (< i n)\n          (recur (inc i) (conj v i))\n          v)))\n\n(stolen from [1])In this case, we're creating \"new\" structures on every iteration of the loop, so our code is \"more pure\", in that there's no mutation.   It might be being mutated in the implementation but not from the author's perspective.I don't think I'm confusing anything.[1] https://clojure.org/reference/transientsreply",
      ">without creating any kind of mutable reference.The parameter essentially becomes a mutable reference.reply",
      "No disagreement on that but that's an implementation detail and from the coder's perspective there's no mutable references.reply",
      "From the coder's perspective, there are no mutable references only if the coder does not really rely on or care about the possibility that their code uses TCO. If they actively want TCO then they definitely care about the performance benefits they get from underlying mutability/memory reuse/frame elision.reply"
    ],
    "link": "https://kmicinski.com/functional-programming/2025/08/01/loops/",
    "first_paragraph": "Aug 1, 2025One story every computing enthusiast should hear is the lesson of\nhow loops and tail-recursion are equivalent. We like recursive\nfunctions because they\u2019re amenable to induction, and we can derive\nthem in a way that is in direct correspondence with the definition of\nthe datatype over which they recur. We like loops because they\u2019re\nfast and make intuitive sense as long as variables don\u2019t change in too\ntricky a way.In general, recursive functions are slower than loops because they\npush stack frames: the performance of most programs today is dominated\nby memory reads/writes. The data we touch the most lives in the\ncache\u2013we do not want to evict a ton of stuff from the cache, under\nany circumstance. In a direct-style implementation of a recursive\nfunction, the recursive call has to push a stack frame to remember\nwhat to do once the function returns:When we get to (+ (first l) (sum (rest l))), we first call (first\nl) (which returns the first element). While we\u2019re making that call,\n"
  },
  {
    "title": "Neki \u2013 sharded Postgres by the team behind Vitess (planetscale.com)",
    "points": 148,
    "submitter": "thdxr",
    "submit_time": "2025-08-11T18:03:57 1754935437",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=44867374",
    "comments": [
      "This is exciting. The announcement says it will be open source. I really hope that this includes a functionally complete control plane so you realistically self-host.I looked Neon recently, and it appears that it's designed as a SaaS product from the outset; while it is technically possible to self-host the individual components of the architecture, it does not look trivial, in large part because the control plane is closed source (and probably extremely specific to Neon's SaaS operations).reply",
      "Your probably better off with the original flavor (the guy that made Vitesse)https://multigres.com/He is making a open source version of porting Vitesse to Postgres.reply",
      "Looks like there is two ongoing vitess for postgres projects. Hopefully this competition leads to a better postgres ecosystem.https://supabase.com/blog/multigres-vitess-for-postgresreply",
      "There is also pgdog by the author of pgcat: https://pgdog.devreply",
      "It gets more spicy when you realize the founder of vitess, also the founder of planet scale, left planet scale to build this at supabasereply",
      "he left PlanetScale 4 years ago.reply",
      "Supabase also working on OrioleDBreply",
      "OrioleDB is not about sharding, it's about the storage layer.reply",
      "I did not claim OrioleDB is about sharding. It was just an observation that Supabase is contributing to Postgres ecosystem through multiple projects.reply",
      "they likely said that because the context is \"vitess for postgres projects\" and OrioleDB is not \"vitess for postgres\"reply"
    ],
    "link": "https://planetscale.com/blog/announcing-neki",
    "first_paragraph": "Blog|ProductWant to learn more about unlimited IOPS w/ Metal for Postgres and Vitess?Talk to SolutionsGet the RSS feedBy Andres Taylor, Dirkjan Bussink, Harshit Gangal, Nick Van Wiggeren, Noble Mittal, Rohit Nayak, Roman Sodermans, Shlomi Noach, Sam Lambert | August 11, 2025Today, we are announcing Neki \u2014 sharded Postgres by the team behind Vitess. Vitess is one of PlanetScale\u2019s greatest strengths and contemporary Vitess is the product of our experience running at extreme scale. We have made explicit sharding accessible to hundreds of thousands of people and it is time to bring this power to Postgres.Neki is not a fork of Vitess. Vitess\u2019 achievements are enabled by leveraging MySQL\u2019s strengths and engineering around its weaknesses. To achieve Vitess\u2019 power for Postgres we are architecting from first principles and building alongside design partners at scale. When we are ready we will release Neki as an open source project suitable for running the most demanding Postgres workloads.To st"
  },
  {
    "title": "Ollama and gguf (github.com/ollama)",
    "points": 86,
    "submitter": "indigodaddy",
    "submit_time": "2025-08-11T17:54:08 1754934848",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=44867238",
    "comments": [
      "I recently discovered that ollama no longer uses llama.cpp as a library, and instead they link to the low level library (ggml) which requires them to reinvent a lot of wheel for absolutely no benefit (if there's some benefit I'm missing, please let me know).Even using llama.cpp as a library seems like an overkill for most use cases. Ollama could make its life much easier by spawning llama-server as a subprocess listening on a unix socket, and forward requests to it.One thing I'm curious about: Does ollama support strict structured output or strict tool calls adhering to a json schema? Because it would be insane to rely on a server for agentic use unless your server can guarantee the model will only produce valid json. AFAIK this feature is implemented by llama.cpp, which they no longer use.reply",
      "I got to speak with some of the leads at Ollama and asked more or less this same question. The reason they abandoned llama.cpp is because it does not align with their goals.llama.cpp is designed to rapidly adopt research-level optimisations and features, but the downside is that reported speeds change all the time (sometimes faster, sometimes slower) and things break really often. You can't hope to establish contracts with simultaneous releases if there is no guarantee the model will even function.By reimplementing this layer, Ollama gets to enjoy a kind of LTS status that their partners rely on. It won't be as feature-rich, and definitely won't be as fast, but that's not their goal.reply",
      "Georgi gave a response to some of the issues ollama has in the attached thread[1]> Looking at ollama's modifications in ggml, they have too much branching in their MXFP4 kernels and the attention sinks implementation is really inefficient. Along with other inefficiencies, I expect the performance is going to be quite bad in ollama.ollama responded to that> Ollama has worked to correctly implement MXFP4, and for launch we've worked to validate correctness against the reference implementations against OpenAI's own.\n> Will share more later, but here is some testing from the public (@ivanfioravanti\n) not done by us - and not paid orleading to another response> I am sure you worked hard and did your best. \n> But, this ollama TG graph makes no sense - speed cannot increase at larger context. Do you by any chance limit the context to 8k tokens?\n> Why is 16k total processing time less than 8k?Whether or not Ollama's claim is right, I find this \"we used your thing, but we know better, we'll share details later\" behaviour a bit weird.[1] https://x.com/ggerganov/status/1953088008816619637reply",
      "Thank you. This is genuinely a valid reason even from a simple consistency perspective.(edit: I think -- after I read some of the links -- I understand why Ollama comes across as less of a hero. Still, I am giving them some benefit of the doubt since they made local models very accessible to plebs like me; and maybe I can graduate to no ollama )reply",
      "I think this is the thing: if you can use llama.cpp, you probably shouldn't use Ollama. It's designed for the beginner.reply",
      "This is a good handwave-y answer for them but truth is they've always been allergic to ever mentioning llama.cpp, even when legally required, they made a political decision instead of an engineering one, and now justify it to themselves and you by handwaving about it somehow being less stable than the core of it, which they still depend on.A lot of things happened to get to the point they're getting called out aggressively in public on their own repo by nice people, and I hope people don't misread a weak excuse made in conversation as solid rationale, based on innuendo. llama.cpp has been just fine for me, running on CI on every platform you can think of, for 2 years.EDIT: I can't reply, but, see anoncareer2012's reply.reply",
      "It's clear you have a better handle on the situation than I do, so it's a shame you weren't the one to talk to them face-to-face.> llama.cpp has been just fine for me.Of course, so you really shouldn't use Ollama then.Ollama isn't a hobby project anymore, they were the only ones at the table with OpenAI many months before the release of GPT-OSS. I honestly don't think they care one bit about the community drama at this point. We don't have to like it, but I guess now they get to shape the narrative. That's their stance, and likely the stance of their industry partners too. I'm just the messenger.reply",
      "> ...they were the only ones at the table with OpenAI many months before the release of GPT-OSSIn the spirit of TFA:This isn't true, at all. I don't know where the idea comes from.You've been repeating this claim frequently. You were corrected on this 2 hours ago. llama.cpp had early access to it just as well.It's bizarre for several reasons:1. It is a fantasy that engineering involves seats at tables and bands of brothers growing from a hobby to a ???, one I find appealing and romantic. But, fantasy nonetheless. Additionally, no one mentioned or implied anything about it being a hobby or unserious.2. Even if it wasn't a fantasy, it's definitely not what happened here. That's what TFA is about, ffs.No heroics, they got the ultimate embarrassing thing that can happen to a project piggybacking on FOSS: ollama can't work with the materials OpenAI put out to help ollama users because llama.cpp and ollama had separate day 1 landings of code, and ollama has 0 path to forking literally the entire community to use their format. They were working so loosely with OpenAI that OpenAI assumed they were being sane and weren't attempting to use it as an excuse to force a community fork of GGUF and no one realized until after it shipped.3. I've seen multiple comments from you this afternoon spiking out odd narratives about Ollama and llama.cpp, that don't make sense at their face from the perspective of someone who also deps on llama.cpp. AFAICT you understood the GGML fork as some halcyon moment of freedom / not-hobbiness for a project you root for. That's fine. Unfortunately, reality is intruding, hence TFA. Given you're aware, it makes your humbleness re: knowing whats going on here sound very fake, especially when it precedes another rush of false claims.4. I think at some point you owe it to even yourself, if not the community, to take a step back and slow down on the misleading claims. I'm seeing more of a gish-gallop than an attempt to recalibrate your technical understanding.It's been almost 2 hours since you claimed you were sure there were multiple huge breakages due to bad code quality in llama.cpp, and here, we see you reframe that claim as a much weaker one someone else made to you vaguely.Maybe a good first step to avoiding information pollution here would be to invest time spent repeating other peoples technical claims you didn't understand, and find some of those breakages you know for sure happened, as promised previously.In general, I sense a passionate but youthful spirit, not an astro-turfer, and this isn't a group of professionals being disrespected because people still think they're a hobby project. Again, that's what the article is about.reply",
      "Feels like BS I guess wrapping 2 or even more versions should not be that much of a problem.There was drama that ollama doesn\u2019t credit llama.cpp and most likely crediting it was \u201enot aligning with their goals\u201d.reply",
      "That's a dumb answer from them.What's wrong with using an older well-tested build of llama.cpp, instead of reinventing the wheel? Like every linux distro ever who's ever ran into this issue?Red Hat doesn't ship the latest build of the linux kernel to production. And Red Hat didn't reinvent the linux kernel for shits and giggles.reply"
    ],
    "link": "https://github.com/ollama/ollama/issues/11714",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.The original model coudl run without problem, but the gguf model fail to run for below errorsMay need an update of ggml dependencies like llama.cpp ggml-org/llama.cpp#15091Note: Running gguf on llama.cpp without problem.No responseNo responseNo responseNo response"
  },
  {
    "title": "The value of institutional memory (timharford.com)",
    "points": 106,
    "submitter": "leoc",
    "submit_time": "2025-08-11T16:53:43 1754931223",
    "num_comments": 64,
    "comments_url": "https://news.ycombinator.com/item?id=44866500",
    "comments": [
      "I had long term business relationship with a company, originating and developing a product for them.From 50 - 1000 employees things worked very well. There was a great deal of continuity in the relationship. Lots of trust and flexibility in both directions. Our product quickly became the best available, by a long margin, and for a couple decades.But after they passed about 1500 - 2000 employees they got more organized. A formalized organization and process system. Things quickly went downhill. As someone working from outside the company, their processes were incredibly disruptive and inefficient for me. Likewise, their turnover replaced a situation of working with long time friendly colleagues, who knew me very well, to working with people who had no idea what my positive reputation was, my track record of delivering quality without the hammer of conformance, etc.The project's ambitious upward trajectory stalled. Even then it took about ten years to fall behind other players. But it never recovered. Today it operates deep in the shadows of others.Virtually every employee I worked with was wonderful, inclined to be as supportive as restrictions allowed, etc. But the institutionalization smothered the organizations ability to operate with any flexibility, no matter how dysfunctional or value destroying the results.The company became like someone who has permanently lost the ability to form new memories.You can't build anything special with someone who keeps forgetting any context. I spent many years cycling between depression and resurrected determination trying. But finally gave up.reply",
      "Sorry to hear this. It's such a tricky thing for an org to balance, if not impossible.One thing I notice is it's very easy to add additional layers of relatively small actual value that look like lots of value. So you might say you've earned a degree of respect by working consistently for years, and people don't mind that you don't always update your status reports. But then if you don't defend vigorously in the org, someone might come in who does very little work in terms of company output, but always gets your status reports in and reports up the chain so you \"don't have to\". And that looks like value to the person above, but it wasn't really. And now you have a new boss.reply",
      "That company not only failed to \"institutionalize\" the specialized knowledge they had, once they became big enough for bureaucracy to self-assemble they ended up institutionalizing the concept of not valuing things that led to initial success.reply",
      "This is correct. Senior management operated intelligently, creatively, benevolently as far as that goes, but intentionally distant. With every management layer tasked with keeping trains running on time, and not making demands on their attention.Despite the majority of the company's early successes, which still defined the company, coming from individual creatives.Then the whole company completely missed a major industry wave they had been perfectly built and positioned to ride. My product's last dozen+ years of struggle/stagnation, despite delivering modest progress where I still could, was not a small aspect of that epic whiff.reply",
      ">You can't build anything special with someone who keeps forgetting any context. I spent many years cycling between depression and resurrected determination trying. But finally gave up.Was that an LLM reference or is it the myopia in me?There's a parallel here, either way. All the documentation in the world will not make a person, or llm session interchangable.In some sense the new way of coding feels like building a big org with people without memory. If you can document the process perfectly, there is a holy grail out there somewhere.Or maybe there isn't.reply",
      "@throwaway13337 You plucked this out of my head. \"If you can document the process perfectly, there is a holy grail out there somewhere.\"\u2514\u2500\u2500 Dey well; Be wellreply",
      "There is a (possibly apocryphal) story of cars being specified to understand a 100kmh air speed on the rear windscreen. 'Ridiculous, it can't reverse at more than 30kmh said the car designers' and ignored the spec. The first time new cars were transported on a train, all the rear windscreens blew in.A long time ago I worked on a software product to try to record design decisions in the creation of long-lived artefacts, such as nuclear reactors. The idea being that engineers looking to make a change 20+ years later (when the original engineers had retired) would understand why something had been designed the way it had.The project was not a success, despite some initial enthusiasm from some commercial sponsors. I think this was due to 2 main issues:a) The software infrastructure of the days wasn't really up to it. This was just before wikis, intranets etc, which would have made everything a lot easier.b) The engineers working on the design had no incentive to record the rationale of their decisions. It was extra work with no benefit for them (any benefit was by someone else, years down the line). In fact it could make it more likely for them to be held liable for a bad decision. And, in an age of cheap outsourcing, it could reduce their job security.The second problem was by far the more important and I don't know how you get around it.reply",
      "There's also a problem c)c) Work tends to proceed in drafts/iterations, with earlier initial ideas refined and processed over time. You can record the reasons behind the initial ideas, but eventually those ideas become implicit background and new ideas are diffs against them. It's hard to make all of that explicit and keep it up to date.You'd think something like wikis would help here because you can see the history. But eventually as you move around in the design space you can make moves that seem obvious but cut across multiple dimensions simultaneously. And it's hard to note something like that well in a wiki commit.A long form lab notebook (or doc) is often useful here.reply",
      "You have to start thinking about product as the entire provenance chain, not just the end product. That means nobody gets paid for a working reactor. They only get paid for a working AND DOCUMENTED reactor.Related, you have to make it benefit the current generation of engineers as well. Want to get your thing built? Deliver your designs in format X, which also happens to support long term reference.None of this is easy\u2026 but it is actually possible to align incentives like this. You just have to do it from very high up, and with a very firm hand.reply",
      ">They only get paid for a working AND DOCUMENTED reactor.Unfortunately that is rather easy to game. Especially in an age of LLMs.reply"
    ],
    "link": "https://timharford.com/2025/05/the-value-of-institutional-memory/",
    "first_paragraph": ""
  },
  {
    "title": "36B solar mass black hole at centre of the Cosmic Horseshoe gravitational lens (oup.com)",
    "points": 112,
    "submitter": "bookofjoe",
    "submit_time": "2025-08-11T14:42:27 1754923347",
    "num_comments": 74,
    "comments_url": "https://news.ycombinator.com/item?id=44864680",
    "comments": [
      "With good quantization, I bet we can get it down to 8B and it will easily fit on consumer grade galaxy.(Sorry, I had to, with all the AI flood, I really was about to skip this info after the first 3 characters)reply",
      "Don't be sorry, that was pretty goodreply",
      "So you're saying it might fit on the S26?reply",
      "I had a bit of a pause trying to figure out if someone named a model \u201eblack hole\u201d from that title.Hype is strong.reply",
      "Glad you wrote it, the title took me down the same path for a few seconds :-Dreply",
      "They very rare great HN joke.reply",
      "thanks for brightening the day :)reply",
      "But can you make it talk dirty to mereply",
      "Researchers discovered the black hole has been consuming AI VC money, at the rate of $50M per day, and so finally explaining why it is gotten so big.reply",
      "That sounds low...reply"
    ],
    "link": "https://academic.oup.com/mnras/article/541/4/2853/8213862?login=false",
    "first_paragraph": ""
  },
  {
    "title": "Japan's largest paper, Yomiuri Shimbun, sues Perplexity for copyright violations (niemanlab.org)",
    "points": 30,
    "submitter": "aspenmayer",
    "submit_time": "2025-08-12T00:07:09 1754957229",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=44870895",
    "comments": [
      "Japan has extremely favorable copyright laws to the holders. My understanding is that without explicit permission, there is no fair use and so any reproduction or modified work is only allowed as long as they don't request a takedown.reply",
      "I don't know why Perplexity in particular gets everyone in a nit. It's not even particularly special: a user inputs a query, an AI model does a web search and fetches some pages on the user's behalf, and then it serves the result to the user.Putting aside that other products, such as OpenAI's ChatGPT and modern Google Search have the same \"AI-powered web search\" functionality, I can't see how this is meaningfully different from a user doing a web search and pasting a bunch of webpages into an LLM chat box.> But what about ad revenue?The user could be using an ad blocker. If they're using Perplexity at all, they probably already are. There's no requirement for a user agent to render ads.> But robots.txt!!!11`robots.txt` is for recursive, fully automated requests. If a request is made on behalf of a user, through direct user interaction, then it may not be followed and IMO shouldn't be followed. If you really want to block a user agent, it's up to you to figure out how to serve a 403.> It's breaking copyright by reproducing my content!Yes, so does the user's browser. The purpose of a user agent is to fetch and display content how the user wants. The manner in which that is done is irrelevant.reply",
      "The Japan Newspaper Publishers & Editors Association is very active lobbying about this area https://www.pressnet.or.jp/english/reply",
      "If they are copying and pasting news articles on their site, that's a pretty straightforward copyright case I would think.In the US at least this should be pretty well covered by the case law on news aggregators.reply",
      "Before someone mentions Japan effectively making all data fair use for AI training, Japan specifically forbids direct recreation which is what this lawsuit is about.reply",
      "Original title edited for length:> Japan\u2019s largest newspaper, Yomiuri Shimbun, sues AI startup Perplexity for copyright violationsreply"
    ],
    "link": "https://www.niemanlab.org/2025/08/japans-largest-newspaper-yomiuri-shimbun-sues-perplexity-for-copyright-violations/",
    "first_paragraph": ""
  },
  {
    "title": "The Joy of Mixing Custom Elements, Web Components, and Markdown (deanebarker.net)",
    "points": 77,
    "submitter": "deanebarker",
    "submit_time": "2025-08-11T16:16:56 1754929016",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=44865997",
    "comments": [
      "I recently discovered a static site generator called Astro, which supports many syntaxes but the .astro is a nice mix of TypeScript and JSX-like syntax. Content can use MDX which is like Markdown but with {JSX} style markup for variables and etc. The static components are used very similar to React, with familiar import statements and <ComponentName props=etc> patterns. It is extremely easy to pick up. Best of all, it has plugins to support all sorts of other interactivity, so you can create interactive 'islands' of content using React or Preact or SolidJS or Vue etc. That way you have most of your content statically generated, and then the dynamic parts can be done from the client side.Best of all, if you use simple unchanged files for other dynamic stuff like JSON etc, you can just generate those on build and serve those files in the host directly as the 'response' to a simple REST request, which is sometimes overlooked despite being the most fundamental form of a REST API.https://astro.build/I came across this after researching various options for a website that had, mostly for my own entertainment, restrictions on wanting to be mostly statically generated but customizable easily without learning a lot of new syntax / etc, something JSX-like with Markdown support etc, and MDX was an immediate find - and astro was the easiest SSG I found for it after trying with 11ty and several others. Actually felt like a delight playing with it.reply",
      "\"...Pre-processing and the initial excitement of a domain-specific language is a siren that might lead you to your doom.(I swear, I have never tried to do something this foolish\u2026 I swear\u2026)...\"I will say, that almost didn't make me laugh, bur, I will not swear.I've recently come off from one such rabbit hole - A CLI tool for Pre-processing vue project templates. It was over-engineered.Will probably go back to re-build it when I get the free time.\u2514\u2500\u2500 Dey well; Be wellreply",
      "Custom elements are really great for editors and developers. You can provide a rich set of primitives that editors can use to display certain content. In the past, I used MDX [1] extensively so non-technical writers can create a rich UI for a documentation site.- [1] https://mdxjs.com/reply",
      "I just wanted to throw some extra :hearts: towards https://mdxjs.com/ it's really the best of both worlds.reply",
      "My tool of choice for what TFA describes is Vitepress.  Markdown plus about a dozen useful plugins and understands Vue3 components and treats each *.md as an SFC.It hits a sweet spot for me.reply",
      "At Stripe, we built Markdoc to solve many of the issues mentioned!https://stripe.com/blog/markdocreply",
      "Nice. But reminds us that most templating languages support markdown by default.reply",
      "> I didn\u2019t find the need for blank lines in the CommonMark spec,It's in #6 and #7https://spec.commonmark.org/0.31.2/#html-blocksI too ran a site on markdown with HTML. Originally I had my own hacky way of using HTML which was effectively to (1) replace all HTML with placeholder_number (2) format the markdown (3) replace placeholder_number with the html that used to be there. Not as simple as that but close.Eventually i switched to a commonmark spec formatter and then tried to fix any old pages that had spaces in the HTML. Some were basically hard to fix like a pre section with code inside so I added some other hacks to do the same as above for those sections by surrounding them with {{#html}}....stuff..with...blank...lines{{/html}}.So now my solution is handlebars.js meets markdown-it.reply",
      "I use https://github.com/nuxt-modules/mdc with a Nuxt Content project. Also looked at https://markdoc.devreply",
      "Unfortunately, also not-joy of completely unbreakable website ... https://usercontent.irccloud-cdn.com/file/TPJZ2AeN/100001246...reply"
    ],
    "link": "https://deanebarker.net/tech/blog/custom-elements-markdown/",
    "first_paragraph": "\n                \u2191 Back to Tech Blog\nI love Markdown. I write faster and more natively in it than any other format or tool.If we zoom way out, here\u2019s the most basic philosophy of Markdown: replace complicated stuff with simpler stuff.That\u2019s all it does, really. It replaces some tedious nested taggy stuff with way simpler stuff that makes more visual sense and is  faster to type. At its core, Markdown is really just a bunch of macros.This website runs on 6,000-ish Markdown files. They\u2019re processed on the server \u2013 meaning the Markdig library from .NET processes them, then sends back the resulting HTML. I\u2019m all-in on Markdown, to the point where I wrote my own online editor for it.Server-side processing of Markdown is a pretty common model \u2013 most static site generators fundamentally do the same thing: they process the Markdown, turn it into HTML, and that gets sent to the server (via being written to files).One of the philosophical points of Markdown is that you can mix it with HTML. Mean"
  },
  {
    "title": "Byte Buddy is a code generation and manipulation library for Java (bytebuddy.net)",
    "points": 70,
    "submitter": "mooreds",
    "submit_time": "2025-08-08T17:25:03 1754673903",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=44839496",
    "comments": [
      "Note that Java now has its own API for this purpose.https://openjdk.org/jeps/484reply",
      "for those who might be clicking through thinking \"since when??\", the emphasis is on \"now\" - this was released in JDK 24.bytebuddy predates it by at least a decade.reply",
      "Hence the now on my comment.  :)reply",
      "Ideally, tools like ByteBuddy will adopt that API as it's for low level concerns.reply",
      "We are already living in an (almost) ideal world: https://github.com/raphw/byte-buddy/discussions/1798reply",
      "How does that compare in terms of usability and completeness?reply",
      "It is complete, and I\u2019ve found it extremely usable when writing code to trawl over a large number of class files. Looks like it should be good for code generation as well but I haven\u2019t used that yet.reply",
      "I have not yet used it, only raising awareness.This came to be, because Oracle noticed everyone, including themselves, were depending on ASM, so the JEP was born.reply",
      "Yup, the ASM dependency is one that would constantly cause us headaches.  A load of frameworks have a path to ASM for one reason or another and it requires an update every time you move up JVM runtimes.It's usually not painful to update (just bump the version) but it's an annoyance.In fact, Byte buddy has a dep on ASM.reply",
      "It's complete but low level compared to Byte Buddy.  A better comparison is the to ASM (which is what it was meant to replace).https://asm.ow2.io/reply"
    ],
    "link": "https://bytebuddy.net/",
    "first_paragraph": ""
  },
  {
    "title": "Launch HN: Halluminate (YC S25) \u2013 Simulating the internet to train computer use",
    "points": 46,
    "submitter": "wujerry2000",
    "submit_time": "2025-08-11T15:30:49 1754926249",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=44865290",
    "comments": [
      "This is very interesting. I think a lot of people may be quick to overlook the value of such simulators when thinking about AI agents at the extremes. (Either they're not good enough to trust or they're so good they'll leapfrog over any economic value here.)My own experience makes me lean toward thinking that the truth is somewhere in the middle in this situation, and that simulators like these will be valuable. I've been experimenting a lot with computer use on my website Bingeclock, passing through different prompts along the lines of \"make a movie marathon based on X.\" The newest agents are consistently impressive, while also being consistently imperfect in surprising and interesting ways.Whether or not all the labs are already running this kind of thing internally for themselves, you would know better than I. But it's an idea that seems very useful nonetheless. Congratulations on the launch!reply",
      "Computer use agents are starting to perform well on websites/apps that are in their training distribution, but still struggle a lot when dealing with tasks outside their distribution. A big reason why is because many more niche/enterprise applications are really hard to test on in the real world, hence the need for sims!re: labs doing this internally. They definitely are! However, the scale of sims buildout is going to be massive, probably many orders of magnitude above what we have today. We think it makes sense for one central player to do this because a really good simulator can be used by multiple people at once. It doesn\u2019t make sense for every AI lab/company to build out their own environments if an industry standard catalog exists.reply",
      "Intriguing analysis. I'll be following along with interest!reply",
      "Have you looked at agents from OpenAI and perplexity? Sure, they aren't perfect, but at the same time, they aren't far from near ready.Does this simulation really required? There's another YC startup, they're processing PDFs I believe. They didn't train their systems on any simulation.Edited to reword and add more context.reply",
      "OpenAI agent is very impressive!That being said, there are still a lot of use cases its not good at, and also looking at long trajectory tasks, enterprise work tasks, etc. I imagine those are all still very nascent.I think we are still very early on computer use, being \"production ready\" requires probably close to 95%+ accuracy on most tasks and we're not there yet for most use cases.reply",
      "Very cool - is it possible to simulate this on a live production site (i.e. instead of Halluminate Flights, just test the agent live on Expedia)? Even though you don't have access to the backend json, presumably you could verify the right values were entered in the frontend/UI?reply",
      "yup, though without access to the code it's much harder to pull the state of the components - becomes more like a web scraping problem, it's a brittle and much hackier than just intentionally exposing component state like we can do in the sim.more importantly though are use cases that depend on the data. the data on real google flights/expedia is constantly changing, so it's impossible to build datasets based ground truth, e.g. the answer for a task like \"Find the cheapest round-trip flight option from Bologna (BLQ) to Dushanbe (DYU) if I leave on 2026-05-05 and come back on 2026-05-15. Return the total price and the flight numbers for all flights.\" isn't stable. on our site, we control the data, so that answer is stable (deterministically random). so controlling the whole clone rather than running on the prod site unlocks richer and more repeatable tasks/testing.lastly, our site runs the exact same locally as deployed, it has zero internet dependencies. so it can be run offline directly on the cluster with no issue for network latency/failuresreply",
      "Good luck Jerry!!! Interesting pivot for sure, playgrounds for AI seems like a good idea, I wish someone tackled them in 3D too (not just for browser/computer agent I mean) :Preply",
      "Love what you\u2019re doing. Are you currently open to interns? Would love to connect with you and chat more about using high quality data to help people better train and evaluate their ai agents!reply",
      "hey not hiring right now but connect with me on twitter and we can talk more there: https://x.com/wgm752reply"
    ],
    "link": "item?id=44865290",
    "first_paragraph": ""
  },
  {
    "title": "The History of Windows XP (abortretry.fail)",
    "points": 19,
    "submitter": "achairapart",
    "submit_time": "2025-08-10T10:22:23 1754821343",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=44854171",
    "comments": [
      "The design language of the Neptune UI and the \u201cWatercolor\u201d UXTheme are like Peak Microsoft. Amazingly good looking to this day.> Windows Whistler/2002/XP logo design concepts by Frog DesignI like how there's a vestige of \u201cWindows 2002\u201d in the little \u201cVersion 2002\u201d on the bottom right of all the XP RTM packaging, which disappeared from the later SP2-integrated boxes: https://www.usatoday.com/gcdn/-mm-/0e422e4a7e951800d133d6d73...reply",
      ">up to a year after release, many gamers still recommended Windows 98. Why? Mostly due to compatibility where things a Voodoo card and a Soundblaster running in MS-DOS were preferable for many titles, and this is something that simply wasn\u2019t on offer with XP.Actually, mostly since Wxp was slow as a dog compared to W98, because W9x still had direct control of the hardware rather than the sluggishness-inducing Hardware Abstraction Layer (HAL) that NT has always had inserted between the OS and the devices.W95 was noticeably faster than W98 was too, and both of course move like lightning-speed compared to W10 whose 64bit drags compared to W10-32bit, and W11 is more embarrassing as it continues to further slow with each update (almost every month now rather than only once per year), which makes W10 seem like it was a quite a bit less encumbered than W11.reply",
      "Also the fact that pre-SP2, Windows XP actually crashed (and permanently broke in \"interesting\" ways) more than Windows 98 in practice, theory be damned. I became so familiar with how to install Windows during this time ...Yes, SP1 wasn't horrible if you could get it (but who can download something that big on dial-up?), but it still was not great.reply",
      "95 and 98 were roughly the same speed; any differences would likely be due to drivers. The main difference between the 9x and NT lineage is the former is actually a hypervisor for DOS VMs (and the GUI itself can be considered a DPMI application, running in its own VM) while the latter is a \"full\" OS with a very limited DOS emulator.reply",
      "I know you can run microbenchmarks to show the increased pointer size of 64 bit Windows can cause a few percentage points of performance difference in certain scenarios but that doesn't jive with the statement \"W10 whose 64bit drags compared to W10-32bit\".reply",
      "Race cars are barren of safety and security features, creature comforts, and even frequently missing windows.But boy are they sure fast.But I wouldn\u2019t daily drive one.reply",
      "Considering a common use for Windows these days is Steam Launcher, performance is kind of a big deal, actually. Literally the only thing I use my desktop for is to play games, so yes, performance is pretty much the only thing I care about with it.reply"
    ],
    "link": "https://www.abortretry.fail/p/the-history-of-windows-xp",
    "first_paragraph": ""
  },
  {
    "title": "How Boom uses software to accelerate hardware development (bscholl.substack.com)",
    "points": 70,
    "submitter": "flabber",
    "submit_time": "2025-08-10T19:37:25 1754854645",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=44857643",
    "comments": [
      "AKA they're doing what every other aerospace company has been doing for decades, multidisciplinary design analysis and optimization [0] with simulation in the loop. If you were to ask them how they're leveraging Design of Experiments I bet it'd be met with \"design of what?\".[0]  https://en.wikipedia.org/wiki/Multidisciplinary_design_optim...reply",
      "> Together with a few other optimizations, these tweaks yielded over 1,000mi in increased range\u2014enough that we could now afford a remarkable passenger cabin without sacrificing fuel efficiency or range.Honestly, the way the narrative reads, they're still sacrificing 1,000mi of range in the interests of an improved cabin experience.  They've just  found an optimisation that enables them to reach a net neutral state.Given we're effectively talking about fuel efficiency here, it's hard to imagine airlines wanting an improved cabin vs less fuel consumption.  All the incentives are on them already to meet a \"barest minimum\" cabin experience that they can get away with, because every bit of luxury costs them in numbers of passengers, and fuel costs.reply",
      "To be fair, modern airliners, even budget ones, are way more comfortable than Concorde. You can visit one in a museum, it's very cramped, and noisier. Concorde had way better service tho.reply",
      "It might be a fad, but the current trend in US public aviation is increasing premium cabins and premium revenue:\nhttps://simpleflying.com/why-us-carriers-doubling-down-premi...This is the reason Delta and United and doing well right now and Southwest and the LCCs are struggling.It wasn't true just a few years ago, but if this continues as a trend, I could see an airline sacrificing fuel efficiency for a dramatically improved onboard experience.reply",
      "Premium cabins tend to be a very small proportion of overall seats and are about overcharging for a little extra legroom and service rather than trading off operational flexibility for unique luxury though. Big difference between charging 3x economy rates for 2x the space for a carefully estimated proportion of seats in a mixed configuration (no brainer) and hoping your layout is so good it justifies thirstier, less flexible aircraft to operators (tough sell)...That said, Boom's customers - if they ever exist - will be a new business class pay extra for supersonic flights category anyway.reply",
      "> Premium cabins tend to be a very small proportion of overall seatsMost of the profit on a plane is made in business class. If airlines could fly an all-business configuration, they would. The problem is the smallest planes that can do high-paying routes like LON-NYC are bigger than that customer set. So the airline throws in economy seats, often barely breaking even on those, to fill space.In a world with small airliner planes that can make those transoceanic and transcontinental journeys, I suspect we\u2019ll see more all-business class flights.reply",
      "Another factor in this mix is frequency, which matters a lot, especially to business travellers.A once-daily supersonic flight might minimize \u201ctime in the air\u201d while a once hourly mostly-economy 737 shuttle minimises \u201ctime away from home.\u201dreply",
      "But that's just it - the airlines have finally (lol) realized that a huge price \"Delta\" (lolx2) between normal cattle class and first class was a mistake.People aren't usually paying 4x for first, but they will pay $10 more for Y, $30 for Z, etc.The future of airlines is fully adjustable planes!reply",
      "Business Class trades well above 3X tourist class.reply",
      "> Business Class trades well above 3X tourist class.If you are a tourist searching business class on Google Flights, of course it\u2019s 5-6x more expensive.True business class / upper class travelers get discounts of 20-50% for J. And no, they\u2019re not using Amex/Chase Travel.reply"
    ],
    "link": "https://bscholl.substack.com/p/move-fast-and-dont-break-safety-critical",
    "first_paragraph": ""
  },
  {
    "title": "AOL to discontinue dial-up internet (nytimes.com)",
    "points": 134,
    "submitter": "situationista",
    "submit_time": "2025-08-11T07:15:38 1754896538",
    "num_comments": 159,
    "comments_url": "https://news.ycombinator.com/item?id=44861521",
    "comments": [
      "Dialup became useless long ago because of web bloat.My mom had a rural dialup connection that typically managed about 30kbps.  15+ years ago this was enough to load Facebook, Gmail (even without its fallback basic html mode which is gone now anyway) and so on.  You just had to be patient the first time while all the graphic assets got cached.Some years later she was on a cell network connection with 128kbps fallback if you go over your limit.  Hey, 4x as fast as she had before, effectively unlimited right?  Wrong.  Bloat was by now such that sites simply wouldn't load at 128kbps.  Things timed out before all the bloat was loaded and you would not get the UI regardless how patient you were.Hacker News still worked of course.reply",
      "The web bloat is definitely real. There are so many things which could be done with a simple HTML form, and often were, that got replaced with huge bloated JS-obligatory SPAs because... \"modern\".Even IM clients were possible without JS, just plain HTML forms and pure applied skill, which I'll leave as exercise for the reader to figure out. I remember using a few HTML-IRC gateways which worked that way.reply",
      "To be fair when you are put on the 128kbps penalty box with the cell provider they also de-prioritize your traffic to the very very bottom of the queue so it's almost impossible to even get the 128kbps, and if the network is busy at all you often get nothing.but you are correct that modern web frequently leaves low bandwidth high latency users out in the cold, but there are a few holdouts.  Craigslist is still pretty usable for example.  Hackernews is quite bandwidth friendly.  Email is always an option.  It's not all doom and gloom for the soda straw crowd.reply",
      "This was rural though, with the cell tower serving a small town, population 600, and folks on the highway and in the nearby backcountry.  As far as we could tell it really was 128kbps.  But definitely not enough for the modern (then - this is already 7-8 years ago) web.We ran out the (then) measly data allotment of the day (500MB) on purpose on the last day of the billing period to try this.reply",
      ">  Email is always an optionProvided you have Outlook or Thunderbird or whatever set up on your computer.  That's beyond most grandmothers, who are likely logging into Yahoo or MSN or something.reply",
      "Had a similar experience. I grew up in a rural area and broadband penetration was late. Later when I bought a house I was lied to by comcast about availability and ended up dialup again. (My fault for believing them tbh) Most of the tricks I used to make the most out of a dialup connection (disable images, disable flash player, load multiple pages so they could be browsed offline) didn't make a difference anymore. In the case of loading multiple pages, lazy loading meant this didn't really work. It was a much more brutal experience than the first go around.The worst part was how little actual content actually makes up the bloat. Sure video was right out, but I was often struggling to load pages that were mostly text.The only other option at the time was Hugesnet. After doing the math I determined the data caps were so low dialup was actually cheaper at MBs/month and had less latency issues. Realistically the next best available step up wasn't Hughesnet it was shotgunned 56K.reply",
      "A firm I used to do network work for had Hughesnet as failover Internet for a couple locations. I always knew they were on it by the 500ms+ pings. Better than nothing though for sure. And I'd also believe that's typical of any non-Starlink Internet around the end of the 2010s.reply",
      "Heh.  I know someone who uses VoIP via geostationary satellite internet connection.  He tried Starlink as an upgrade, however, the 100W continuous power draw was a dealbreaker with his off-grid solar setup.  This is off-grid enough not to have cell coverage.Anyway talking with him on the phone you pretty much have to use a \"over\" / \"over and out\" kind of protocol because of the long latency.reply",
      "And the only reason she doesn\u2019t have access to higher bandwidth is because rural America and conservatives consistently vote for politicians who cut funding for it\u2026.Whether she voted for it or not, she should blame her neighbors who voted for her representatives.reply",
      "I remember going from ATDP to ATDT\u2026 just one letter but I felt I was living in the future :-)reply"
    ],
    "link": "https://www.nytimes.com/2025/08/11/business/aol-dial-up-internet.html",
    "first_paragraph": ""
  }
]