[
  {
    "title": "Show HN: I wrote a new BitTorrent tracker in Elixir (github.com/dahrkael)",
    "points": 91,
    "submitter": "dahrkael",
    "submit_time": "2025-06-19T22:49:49 1750373389",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44323253",
    "comments": [
      "Very cool! Is this suitable for using as a private tracker?\n \nreply",
      "Really cool! You looking to write Elixir as your main job?\n \nreply",
      "Hey congrats on the launch! Can you provide any details on how it runs compared to opentracker? I'm really interested in the performance etc.\n \nreply",
      "Now that's neat. The Beam VM sounds like a natural fit for a torrent tracker\n \nreply",
      "Now this is serious business, congrats on the project! I can see how this is perfect fit for elixir...\n \nreply"
    ],
    "link": "https://github.com/Dahrkael/ExTracker",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n        Elixir-powered BitTorrent Tracker\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\nThe Bittorrent Tracker made in Elixir\n\n\ud83d\udc77\u200d\u2642\ufe0fThis project is a Work In Progress. While not ready for full industrial usage it does work.\nThere is a testing instance running at extracker.dahrkael.net:6969 with all current features enabled (Live statistics).Implementation Legend:\n\ud83d\udd32 Not Yet \ud83d\udd30 Partially \u2705 Done \u274c Won't doThere are 3 main ways of running ExTracker currentlyFor this method to work you need to have Erlang and Elixir installed on your systemCurrently there are no official releases built (soon\u2122\ufe0f). You can however make your own and deploy it where needed:Fo"
  },
  {
    "title": "Compiling LLMs into a MegaKernel: A path to low-latency inference (zhihaojia.medium.com)",
    "points": 153,
    "submitter": "matt_d",
    "submit_time": "2025-06-19T19:20:54 1750360854",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=44321672",
    "comments": [
      "Hi author(s), the on-GPU interpreter approach looks like a promising path forward, have you seen this strikingly similar concurrent work?https://news.ycombinator.com/item?id=44111673I find it curious that fundamentals of the CUDA programming model (eg kernel launches) are being subverted in favor of fine grained task based parallelism that ends up using the hardware more effectively. Makes me wonder if CUDA has been holding us back in some ways.What are the chances we see your work land in PyTorch as an experimental backend?Awesome stuff thanks for sharing.P.S. minor typo, your first two paragraphs under part 1 are nearly identical.\n \nreply",
      "This is very cool. I enjoyed going through the writeup and GitHub README.I was wondering if these same optimizations can be brought to bear on training as well, rather than only inference. I guess the challenge here is fusing backward computations with gradient communication.I also saw that this currently does not handle dynamic workloads such as MoE. I recently came across this paper that does exactly this:FlashDMoE: Fast Distributed MoE in a Single Kernel - https://arxiv.org/pdf/2506.04667\n \nreply",
      "Thanks for reading the post and github README. Supporting training is definitely feasible but the benefit may not be as significant as low-latency inference since training generally involves much larger kernels, making kernel launch overhead less significant.Thanks for sharing the FlashDMoE work. Our next step is to support MoE models. Stay tuned!\n \nreply",
      "Thanks for the inputs. It's very helpful to know.I look forward to following mirage development.\n \nreply",
      "Personally I think its a bit of a waste to invest time into gradient training optimizations. A lot of training tasks IRL have discrete values in nature, which can't be trained with gradients.\n \nreply",
      "After working pretty closely with vLLM and SGLang over the past few months, this is EXACTLY what I had envisioned what a successor project would look like - analyzing an operation dependency graph and then fusing (or, at a minimum, scheduling tasks smarter).  Congrats to the team.\n \nreply",
      "Thanks a lot for your positive feedback! We believe that MPK can enhance existing LLM serving systems, especially for low-latency LLM serving. We are very excited about the opportunity to collaborate with others on direction.\n \nreply",
      "Next step - compile straight to verilog so I can buy some LLMs on aliexpress\n \nreply",
      "https://riscv.org/blog/2021/02/hardware-description-language...\nThat was one of the promising ideas before AI & GPUs come to the scene.\nAs CPUs are stagnant, and naturally people want further optimize the middle layers software and hardware.But I suspect parallel computing in GPU style is going to dominate acclerated computing.General purpose CPUs are going to stay to become the little brain that orchestrates GPUs.Ideas of software direct to hardware transition might never be the mainstream.\n \nreply",
      "I'm thinking more like pseudointellect over serial to attach a $3 esp32 to. Since it's basically tokens in, tokens out, let's just cut the unnecessary parts out. It's like querying the cloud models, except it's your silicon you personally soldered to the esp so nobody will break your home assistant with a system prompt update or a fine tuning run.\n \nreply"
    ],
    "link": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17",
    "first_paragraph": ""
  },
  {
    "title": "Literate programming tool for any language (github.com/zyedidia)",
    "points": 44,
    "submitter": "LorenDB",
    "submit_time": "2025-06-19T22:18:45 1750371525",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=44323045",
    "comments": [
      "I actually did some small hobby projects using Literate Coffeescript a long time ago. Looking at the source code today, and I can't help but feel like the proponents of literate programming were really onto something. I'm coming back a decade later, but I can easily see what's going on and why at a glance. Compared to many other projects that I've written in the past without documentation, it's a completely different vibe. The Gulpfile in particular is such a treat to read.Yeah, it can look a bit repetitive if the code is already clear, but the context of why a thing is being done is still valuable. In the modern era with LLM tools, I'm sure it could be even more powerful.\n \nreply",
      "If you are interested with Literate programming, you should try Emacs. Some packages are org-mode, eev and even elisp are best for literate programming. Example https://www.youtube.com/watch?v=dljNabciEGg\n \nreply",
      "I've been wondering if AI coding agent world makes literate programming valuable again. I got into it with JavaScript being a mess prior to the modern changes. Needed a lot of workarounds. Then they improved the language and it felt like coding could be efficient with them. But if the programmer switched from coding to reviewing, maybe it would be good to be able to have various snippets, with an explanation preceding it and then verifying it. Haven't tried it yet. But I do wonder.\n \nreply",
      "The fact that the actual implementation is in `lit` too is really helpful - getting to see how one would actually use this on a larger program does make it much more intriguing than the simple examples (and much more approachable than TeX itself).https://github.com/zyedidia/Literate/tree/master/lit\n \nreply",
      "Been around for a long time indeed. I first learned literate programming in college at Tufts, from Norman Ramsey. He wrote noweb[1], an early implementation of Knuth's ideas.[1]: https://en.wikipedia.org/wiki/Noweb\n \nreply",
      "Oh you beat me to it!\n \nreply",
      "This doesn't seem to provide any context for literate programming, or the core literate operations?cf leo editor for literate programming in python [0]Yes, markdown has code blocks, and notebooks have embedded code in documentation since Mathematica in the 1980's.  It is possible to get IDE support in such blocks.But for literate programming, weaving/tangling sources is needed to escape the file structure, particularly when the build system imposes its own logic, and sometimes one needs to navigate into the code.  Leo shows how complicated the semantics of weaving can get.Eclipse as an IDE was great because their editor component made it easy to manage the trick of one editor for many sources, and their markers provided landmarks for cross-source navigation and summaries.[0] https://leo-editor.github.io/leo-editor\n \nreply",
      "Maybe I'm missing something but how often is the English in literate programming repeating what's already written in the code? Does it work for large projects where it's often hard to explain all the parts in a linear way in the style of an essay?I avoid code comments where I can because English is way less precise than code, it's an extra chore to keep the comments and code in sync, and when the comments and code inevitably get out of sync it's confusing which one is the source of truth. Does literate programming sidestep this somehow? Or have benefits that outweigh this?\n \nreply",
      "I think this certainly happens a fair bit.  Not at all uncommon to have a section that largely says what is going to happen next, which, fair that what is going to happen is what happens.I think where it shines, is where it helps you break the code up, without having to break it up in a way that makes sense for the computer.  Show an outline, but then drill into a section.  The overall function can then be kept as a single unit, and you can sort of punt on sub sections.  I tried this just recently in https://taeric.github.io/many_sums.html.  I don't know that I succeeded, necessarily.  Indeed, I think I probably should have broken things into more sections.  That said, I did find that this helped me write the code more than I expected it to.  (I also was very surprised at how effective the goto style of thinking was...  Much to my chagrin.)I will have to look again at some of the code I've read this way.To directly answer the question of if it helped keep the documentation in sync, as it were, that is tough.  I think it helps keep the code in a section directly related to the documentation for that section.  All too often, the majority of code around something is not related to what you were wanting to do.  Even the general use of common code constructs gets in the way of reading what you were doing.  Literate programming seems the best way I have seen to give the narrative the ability to say \"here is the outline necessary for a function\" and then \"this particular code is to do ...\"  Obviously, though, it is no panacea.\n \nreply",
      "I'm not sure who first coined this idea or put it in a book or where I've read it, but for code comments I generally like the  \"explain why, not what\" philosophy.\nThe \"what\" is answered by the code itself and should be easy enough to comprehend if your design is simple and your names meaningful.\nThe \"why\" is much more important. Why does this parser check for some magic numbers at this specific offset and change some parameters if it finds them? If you don't explain that its because of e.g. compatibility with some legacy format, its gonna be a mystery to the reader.\n \nreply"
    ],
    "link": "https://github.com/zyedidia/Literate",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A literate programming tool for any language\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Literate programming is a style of programming invented by Donald Knuth, where the main idea is that a program's source code is made primarily to be read and understood by other people, and secondarily to be executed by the computer.This frees the programmer from the structure of a program imposed by the computer and means that the programmer can develop programs in the order of the flow of their thoughts.A Literate program generally consists of explanation of the code in a natural language such as English, interspersed with snippets of code to be executed. This means that Literate programs are very easy to understand and share, as all the code is well explained.Lite"
  },
  {
    "title": "Infinite Mac OS X (persistent.info)",
    "points": 16,
    "submitter": "kristianp",
    "submit_time": "2025-06-20T00:16:57 1750378617",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44323719",
    "comments": [
      "> Infinite Mac is a collection of classic Macintosh and NeXT system releases and software, all easily accessible from the comfort of a web browser.https://infinitemac.org/\n \nreply",
      "I love things like this. Aqua was such a revelation at the time.\n \nreply"
    ],
    "link": "https://blog.persistent.info/2025/03/infinite-mac-os-x.html",
    "first_paragraph": "tl;dr: Infinite Mac can now run early Mac OS X, with 10.1 and 10.3 being the best supported versions. It\u2019s not particularly snappy, but as someone who lived through that period, I can tell you that it wasn\u2019t much better on real hardware. Infinite HD has also been rebuilt to have some notable indie software from that era.\n\nI\u2019ve been tracking DingusPPC progress since my initial port and making the occasional contribution myself, with the hope of using it to run Mac OS X in Infinite Mac. While it has continued to improve, I reached a plateau last summer; my attempts would result in either kernel panics or graphical corruption. I tried to reduce the problem a bit via a deterministic execution mode, but it wasn\u2019t really clear where to go next. I decided to take a break from this emulator and explore alternate paths of getting Mac OS X to run.PearPC was the obvious choice \u2013 it was created with the express purpose of emulating Mac OS X on x86 Windows and Linux machines in the early 2000s. By "
  },
  {
    "title": "Andrej Karpathy: Software in the era of AI [video] (youtube.com)",
    "points": 1113,
    "submitter": "sandslash",
    "submit_time": "2025-06-19T00:33:21 1750293201",
    "num_comments": 622,
    "comments_url": "https://news.ycombinator.com/item?id=44314423",
    "comments": [
      "I think it's interesting to juxtapose traditional coding, neural network weights and prompts because in many areas -- like the example of the self driving module having code being replaced by neural networks tuned to the target dataset representing the domain -- this will be quite useful.However I think it's important to make it clear that given the hardware constraints of many environments the applicability of what's being called software 2.0 and 3.0 will be severely limited.So instead of being replacements, these paradigms are more like extra tools in the tool belt. Code and prompts will live side by side, being used when convenient, but none a panacea.\n \nreply",
      "I kind of say it in words (agreeing with you) but I agree the versioning is a bit confusing analogy because it usually additionally implies some kind of improvement. When I\u2019m just trying to distinguish them as very different software categories.\n \nreply",
      "The versioning makes sense to me. Software has a cycle where a new tool is created to solve a problem, and the problem winds up being meaty enough, and the tool effective enough, that the exploration of the problem space the tool unlocks is essentially a new category/skill/whatever.computers -> assembly -> HLL -> web -> cloud -> AINothing on that list has disappeared, but the work has changed enough to warrant a few major versions imo.\n \nreply",
      "For me it's even simpler:V1.0: describing solutions to specific problems directly, precisely, for machines to execute.V2.0: giving machine examples of good and bad answers to specific problems we don't know how to describe precisely, for machine to generalize from and solve such indirectly specified problem.V3.0: telling machine what to do in plain language, for it to figure out and solve.V2 was coded in V1 style, as a solution to problem of \"build a tool that can solve problems defined as examples\". V3 was created by feeding everything and the kitchen sink into V2 at the same time, so it learns to solve the problem of being general-purpose tool.\n \nreply",
      "That's less a versioning of software and more a versioning of AI's role in software. None -> Partial -> Total. Its a valid scale with regard to AI's role specifically, but I think Karpathy was intending to make a point about software as a whole, and even the details of how that middle \"Partial\" era evolves.\n \nreply",
      "What do you think about structured outputs / JSON mode / constrained decoding / whatever you wish to call it?To me, it's a criminally underused tool. While \"raw\" LLMs are cool, they're annoying to use as anything but chatbots, as their output is unpredictable and basically impossible to parse programmatically.Structured outputs solve that problem neatly. In a way, they're \"neural networks without the training\". They can be used to solve similar problems as traditional neural networks, things like image classification or extracting information from messy text, but all they require is a Zod or Pydantic type definition and a prompt. No renting GPUs, labeling data and tuning hyperparameters necessary.They often also improve LLM performance significantly. Imagine you're trying to extract calories per 100g of product, but some product give you calories per serving and a serving size, calories per pound etc. The naive way to do this is a prompt like \"give me calories per 100g\", but that forces the LLM to do arithmetic, and LLMs are bad at arithmetic. With structured outputs, you just give it the fifteen different formats that you expect to see as alternatives, and use some simple Python to turn them all into calories per 100g on the backend side.\n \nreply",
      "Even more than that. With Structured Outputs we essentially control layout of the response, so we can force LLM to go through different parts of the completion in a predefined order.One way teams exploit that - force LLM to go through a predefined task-specific checklist before answering. This custom hard-coded chain of thought boosts the accuracy and makes reasoning more auditable.\n \nreply",
      "I also think that structured outputs are criminally underused, but it isn't perfect... and per your example, it might not even be good, because I've done something similar.I was trying to make a decent cocktail recipe database, and scraped the text of cocktails from about 1400 webpages. Note that this was just the text of the cocktail recipe, and cocktail recipes are comparatively small. I sent the text to an LLM for JSON structuring, and the LLM routinely miscategorized liquor types. It also failed to normalize measurements with explicit instructions and the temperature set to zero. I gave up.\n \nreply",
      "have you tried schema-aligned parsing yet?the idea is that instead of using JSON.parse, we create a custom Type.parse for each type you define.so if you want a:   class Job { company: string[] }\n\nAnd the LLM happens to output:   { \"company\": \"Amazon\" }\n\nWe can upcast \"Amazon\" -> [\"Amazon\"] since you indicated that in your schema.https://www.boundaryml.com/blog/schema-aligned-parsingand since its only post processing, the technique will work on every model :)for example, on BFCL benchmarks, we got SAP + GPT3.5 to beat out GPT4o ( https://www.boundaryml.com/blog/sota-function-calling )\n \nreply",
      "Interesting! I was using function calling in OpenAI and JSON mode in Ollama with zod. I may revisit the project with SAP.\n \nreply"
    ],
    "link": "https://www.youtube.com/watch?v=LCEmiRjPEtQ",
    "first_paragraph": ""
  },
  {
    "title": "Curved-Crease Sculpture (erikdemaine.org)",
    "points": 156,
    "submitter": "wonger_",
    "submit_time": "2025-06-19T14:13:15 1750342395",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=44318874",
    "comments": [
      "Eric Demaine is one of the better intersections of origami and mathematics, you should also read up on Dr Robert Lang, the OG and perhaps the most famous American JPL-physicist-turned-origamist: https://langorigami.com/On the flip side the late Eric Joisel created perhaps the most amazing curved-crease and natural folding that we\u2019ll ever see, his works were truly amazing art: https://ericjoisel.fr/en/home/\n \nreply",
      "Looking at Lang's site, yes it is a super niche area, but there is a lot of self promotion - books, events, etc.  I was first introduced to the general area of curved crease, etc was with David Huffman in the early 90s.  He started that work in the early 70s.  So, Lang proclaims to the the first, but salesmanship is important.Eric himself reconstructs some of huffman's work - https://erikdemaine.org/papers/Huffman_Origami5/paper.pdfIt's an interesting area.\n \nreply",
      "In addition to being great artists, I also learned dynamic programming from this guy via his outstanding lectures: https://www.youtube.com/watch?v=tp4_UXaVyx8&list=PLJl4xQazDg...It looks like there's a more recent series as well: https://www.youtube.com/watch?v=r4-cftqTcdI\n \nreply",
      "Curved creases aside, the fact that folding a piece of paper gives you a straight line is itself quite amazing and deep.Even if I couldn't trust a cheap ruler, a straight edge is a piece of paper away.\n \nreply",
      "One of the underappreciated causes and effects of the industrial revolution is the precision that's around us all the time. To make that piece of paper required thousands of precision surfaces, rollers, etc.\n \nreply",
      "And oh how we take it for granted. I recently spent a few minutes trying to make sense of a situation where I was using a corner of a paper for a square. It turned out the piece of paper was not at all square, at least a quarter of an inch out of square!\n \nreply",
      "One important lesson I remember from high school woodworking class ~45 years ago - when using a set square, make your markings twice with the square  flipped over in the opposite direction, so if the square isn't accurate you'll get two distinct markings - and for most wood working purposes just splitting the difference by eye will be accurate enough.\n \nreply",
      "But folding any piece of paper will give you a straight line, no?\n \nreply",
      "Sure, this would probably work with nice handmade paper. But you won't necessarily get a clean fold with thicker or uneven paper, and depending on fiber length and distribution you might get waviness or other issues\n \nreply",
      "Le Klint makes hand folded curved lamp shades that are prtty neat. They have workshops to teach people how to do it, too.https://www.leklint.com/collections/pendants/products/le-kli...\n \nreply"
    ],
    "link": "https://erikdemaine.org/curved/",
    "first_paragraph": "\nWhen folded along curved creases,\npaper shapes itself into a natural equilibrium form.\nThese equilibria are poorly understood, especially for curved creases.\nWe are exploring what shapes are possible in this genre of self-folding\norigami, with applications to deployable structures, manufacturing, and\nself-assembly.  This transformation of flat paper into swirling surfaces\ncreates sculpture that feels alive.\n\n\n\n\n \n\nFreedom Series (2024)\n  \n\n\n \n\nBrush With Words (2024)\n  \n\n\n \n\nBraille Series (2023)\n  \n\n\n \n\nIMO Series (2023)\n  \n\n\n \n\nHanging Out (2023)\n  \n\n\n \n\nBeethoven Series (2021)\n  \n\n\n \n\nPrecious Metal Series (2021)\n  \n\n\n \n\nAlbers Series (2019)\n  \n\n\n \n\nShakespeaRED Series (2019)\n  \n    Folger Shakespeare Library\n\n\n \n\n200 Circles (2018)\n  \n    Dalhousie University\n\n\n \n\nPyro Series (2017-)\n  \n\n\n \n\nCane Series (2017-)\n  \n\n\n \n\nSunflower Series (2017-2018)\n  \n\n\n \n\nCalligraphy Series (2015)\n  \n\n\n \n\nFrog Series (2015)\n  \n\n\n \n\nKnotty Object (2015)\n  \n\n\n \n\nNutrition Series (2015)\n  \n\n\n \n\nSomet"
  },
  {
    "title": "Show HN: EnrichMCP \u2013 A Python ORM for Agents (github.com/featureform)",
    "points": 88,
    "submitter": "bloppe",
    "submit_time": "2025-06-19T17:32:21 1750354341",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=44320772",
    "comments": [
      "This looks very interesting but I\u2019m not sure how to use it well.\nWould you mind sharing some prompts that use it and solve a real problem that you encountered ?\n \nreply",
      "Imagine you're building a support agent for DoorDash. A user asks, \"Why is my order an hour late?\" Most teams today would build a RAG system that surfaces a help center article saying something like, \"Here are common reasons orders might be delayed.\"That doesn't actually solve the problem. What you really need is access to internal systems. The agent should be able to look up the order, check the courier status, pull the restaurant's delay history, and decide whether to issue a refund. None of that lives in documentation. It lives in your APIs and databases.LLMs aren't limited by reasoning. They're limited by access.EnrichMCP gives agents structured access to your real systems. You define your internal data model using Python, similar to how you'd define models in an ORM. EnrichMCP turns those definitions into typed, discoverable tools the LLM can use directly. Everything is schema-aware, validated with Pydantic, and connected by a semantic layer that describes what each piece of data actually means.You can integrate with SQLAlchemy, REST APIs, or custom logic. Once defined, your agent can use tools like get_order, get_restaurant, or escalate_if_late with no additional prompt engineering.It feels less like stitching prompts together and more like giving your agent a real interface to your business.\n \nreply",
      "Why wouldn't we just give the agent read permission on a replica db? Wouldn't that be enough for the agent to know about:- what tables are there- table schemas and relationshipsBased on that, the agent could easily query the tables to extract info. Not sure why we need a \"framework\" for this.\n \nreply",
      "Disclaimer: I don't know the details of how this works.Time-to-solution and quality would be my guess. In my experience, adding high level important details about the way information is organized to the beginning of the context and then explaining the tools to further explore schema or access data produces much more consistent results rather than each inference having to query the system and build its own world view before trying to figure out how to answer your query and then doing it.It's a bit like giving you a book or giving you that book without the table of contents and no index, but you you can do basic text search over the whole thing.\n \nreply",
      "Because you also need proper access controls. In many cases database access is too low level, you need to bring it up a layer or two to know who can access what. Even more so when you want to do more than read data.\n \nreply",
      "This is the motivating example I was looking for on the readme: a client making a request and an agent handling it using the MCP.  Along with a log of the agent reasoning its way to the answer.\n \nreply",
      "Yes but the agent reasoning is going to use an LLM, I sometimes run our openai_chat_agent example just to test things out. Try giving it a shot, ask it to do something then ask it to explain its tool use.Obviously, it can (and sometimes will) hallucinate and make up why its using a tool. The thing is, we don't really have true LLM explainability so this is the best we can really do.\n \nreply",
      "are you saying that a current gen LLM can answer such queries with EnrichMCP directly? or does it need guidance via prompts (for example tell it which tables to look at, etc. ) ? I did expose  a db schema to LLM before, and it was ok-ish, however often times the devil was in the details (one join wrong, etc.), causing the whole thing to deliver junk answers.what is your experience with non trivial db schemas?\n \nreply",
      "So one big difference is that we aren't doing text2sql here, and the framework requires clear descriptions on all fields, entities, and relationships (it literally won't run otherwise).We also generate a few tools for the LLM specifically to explain the data model to it. It works quite well, even on complex schemas.The use case is more transactional than analytical, though we've seen it used for both.I recommend running the openai_chat_agent in examples/ (also supports ollama for local run) and connect it to the shop_api server and ask it a question like : \"Find and explain fraud transactions\"\n \nreply",
      "So explicit model description (kind of repeating the schema into explicit model definition) provides better results when used with LLM because it\u2019s closer to the business domain(or maybe the extra step from DDL to business model is what confuses the LLM?). I think I\u2019m failing to grasp why does this approach work better than straight schema fed to Llm.\n \nreply"
    ],
    "link": "https://github.com/featureform/enrichmcp",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        EnrichMCP is a python framework for building data driven MCP servers\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.The ORM for AI Agents - Turn your data model into a semantic MCP layer\n\n\n\nEnrichMCP is a Python framework that helps AI agents understand and navigate your data. Built on MCP (Model Context Protocol), it adds a semantic layer that turns your data model into typed, discoverable tools - like an ORM for AI.Think of it as SQLAlchemy for AI agents. EnrichMCP automatically:Transform your existing SQLAlchemy models into an AI-navigable API:AI agents can now:Wrap your existing APIs with semantic understanding:Build a complete data layer with custom logic:AI agents explore your entire data model with one call:Define relationships once, AI agents traver"
  },
  {
    "title": "Show HN: A DOS-like hobby OS written in Rust and x86 assembly (github.com/krustowski)",
    "points": 147,
    "submitter": "krustowski",
    "submit_time": "2025-06-19T13:38:57 1750340337",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=44318588",
    "comments": [
      "Memory-safe language. x86_64, with Arm on the roadmap. Networking stack. Boots from a CD and via multiboot. Your hobby project wipes the floor with DOS.\n \nreply",
      "Whoa there. Gotta run Doom and BASIC to compete with DOS. That is the officially recognized DOS-K\u00e1rm\u00e1n line.\n \nreply",
      "What a challenge! Need to implement some interrupts it seems then, to provide an API for filesystem and so... Thank you for such idea\n \nreply",
      "Doomgeneric has a very thin platform-specific layer: https://github.com/ozkl/doomgeneric?tab=readme-ov-file#porti...\n \nreply",
      "I want some TSRs and print spoolers...\n \nreply",
      "DOOM required DOS 5.0. rou2exOS is only the second take. Watch this area ;)\n \nreply",
      "Nobody cares about Lotus 1-2-3 support any longer :)\n \nreply",
      "What about Wordstar?\n \nreply",
      "also, can't be a dos with the 'dir' command.\n \nreply",
      "Afaik there is a 'DIR' command in MS-DOS. Anyway, what would be a better command to list a directory? I could think of 'ls' maybe\n \nreply"
    ],
    "link": "https://github.com/krustowski/rou2exOS",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A second iteration of the DOS-like hobby OS.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.A second iteration of the RoureXOS operating system, rewritten in Rust.To run the OS, you can use the attached ISO image from any Release, and run it in QEMU emulator. The system was also tested on x86_64 baremetal (booted from the USB flash disk).Run the kernel in QEMU to get the pty number in stdout:Listen for SLIP packets and create a sl0 interface:Catch packets using tcpdump:\n        A second iteration of the DOS-like hobby OS.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page."
  },
  {
    "title": "Octobass (atlasobscura.com)",
    "points": 9,
    "submitter": "keepamovin",
    "submit_time": "2025-06-16T11:33:45 1750073625",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.atlasobscura.com/places/octobass",
    "first_paragraph": "\n            No search results found for\n            \u201c\u201d\n          \n            Make sure words are spelled correctly.\n          \n            Try searching for a travel destination.\n          Composers and orchestras like to use their lowest instruments sparingly, for maximum impact. Few instruments go lower and are used more sparingly than the huge and strange octobass, one of the rarest classic instruments in existence.Invented in 1850 by Jean-Baptiste Vuillaume, the octobass was intended to bring an extremely deep rumble to the orchestra sound. The three-stringed instrument stands between 11 and 12 feet tall, about twice the height of a double bass. This giant bass produces sound so low, some of the notes fall outside the range of human hearing\u2014these vibrations can only be felt.The rare instrument is almost too large to play. The strings are too big to press with your fingers, so fretting them requires operating pedals and levers that control capo-like mechanisms to press down the st"
  },
  {
    "title": "How OpenElections uses LLMs (thescoop.org)",
    "points": 84,
    "submitter": "m-hodges",
    "submit_time": "2025-06-19T16:11:46 1750349506",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=44320001",
    "comments": [
      "I was thinking LLMs can be long-term regressive?As the \"proper solution\" here is of course not using PDFs that are hard-to-parse, but force elections to have machine parseable outputs. And LLMs can \"fix in place\" stupid solutions.That's not a hate on the author though. I needed to do some PDF parsing for bank statements before; but also; the proper long-term solution is force banks (by law or by public interest) to have parseable statements, not parse it!Like putting LLMs to understand bad codebase will not fix the bad codebase, but will build on top of it.oh well c'est la vie\n \nreply",
      "I think that we should encourage elections to _not_ be standardized. The problems among various polities in the USA have many different issues and should not be forced to conform to a specific way that elections should be done. This is a social problem and we should not cram it into a technical solution. Legibility of elections should be maintained at the local level, trying to make things legible at a national level is in my opinion unwanted. As much as I would like the data to be clean, people are not so clean. Even if they used slightly more structured formats than PDFs, the differences between polities must be maintained as long as they are different polities.The way that OpenElections handles this, with 'sources' and 'data' directories I think is a good way to bridge the gap.\n \nreply",
      "Not being standardized is fine and even a positive (diversity of technology vendors is a security feature and increases confidence in elections). But producing machine readable outputs of some sort, instead of physical paper and PDFs, is clearly a positive as well.\n \nreply",
      "Just breaking down the thought a little, we truly can't say elections shouldn't have standards, right?\n \nreply",
      "Elections at the local level should be governed by the locality. I do not see the need for standards at a higher level, other than for democracy to be maintained in some fashion. External data reporting certainly need not be standardized at t\u0336h\u0336e\u0336 \u0336l\u0336o\u0336c\u0336a\u0336l\u0336 [sic] a higher level.\n \nreply",
      "Totally reasonable view, and one of our volunteers actually got the law in Kansas changed to mandate electronic publishing of statewide precinct results in a structured format! But finding legislative champions for this issue isn't easy.\n \nreply",
      "In college (about 15 years ago) I worked for a professor who was compiling precint level results for old elections. My job was just to request the info and then do manual data entry. It was abysmally slow.This application seems very good - but still a bit amazing that lawmakers haven't just required that all data be uploaded via csv! Even if every csv was slightly different format, it would be way easier for everyone (LLM or not).\n \nreply",
      "I could be wildly off-base, but I wonder if some of these systems are airgapped, and the only way the data comes off of the closed system is via printing, to avoid someone inserting a flash drive full of malware in the guise of \"copying the CSV file.\" Obviously there are or should be technical ways to safely extract data in a digital format, but I can see a little value in the provable safety that airgapping gives you.\n \nreply",
      "In some cases that's true, but for many jurisdictions the results systems are third-party vendor platforms, too.\n \nreply",
      "Related: Interesting mockups to turn X/open source Bsky into direct democratic massive \"prothetic\" polls in each post.And paid polls that the author claims will replace prediction markets:https://x.com/MelonUsks/status/1929660387995115713\n \nreply"
    ],
    "link": "https://thescoop.org/archives/2025/06/09/how-openelections-uses-llms/index.html",
    "first_paragraph": "Derek Willis June 9, 2025In the 12-plus years that we\u2019ve been turning official precinct election results into data at OpenElections, the single biggest problem has been converting pictures of results into CSV files. Many of the precinct results files we get are image PDFs, and for those there are essentially two options: data entry or Optical Character Recognition. The former has some advantages, but not many. While most people are not great at manual repetitive tasks, you can improve with lots of practice, to the point where the results are very accurate. In the past we did pay for data entry services, and while we developed working relationships with two individuals in particular, the results almost always contained some mistakes and the cost could run into the hundreds of dollars pretty quickly. For a volunteer project, it just didn\u2019t make sense.We also used commercial OCR software, most often Able2Extract, which did pretty well, but had a harder time with PDFs that had markings or "
  },
  {
    "title": "Guess I'm a Rationalist Now (scottaaronson.blog)",
    "points": 221,
    "submitter": "nsoonhui",
    "submit_time": "2025-06-19T10:22:06 1750328526",
    "num_comments": 638,
    "comments_url": "https://news.ycombinator.com/item?id=44317180",
    "comments": [
      "The article made me think deeper about what rubs me the wrong way about the whole movementI think there is some inherent tension btwn being \"rational\" about things and trying to reason about things from first principle.. And the general absolutist tone of the community. The people involved all seem very... Full of themselves ? They don't really ever show a sense of \"hey, I've got a thought, maybe I haven't considered all angles to it, maybe I'm wrong - but here it is\". The type of people that would be embarrassed to not have an opinion on a topic or say \"I don't know\"In the Pre-AI days this was sort of tolerable, but since then.. The frothing at the mouth convinced of the end of the world.. Just shows a real lack of humility and lack of acknowledgment that maybe we don't have a full grasp of the implications of AI. Maybe it's actually going to be rather benign and more boring than expected\n \nreply",
      "They remind me of the \"Effective Altruism\" crowd who get completely wound up in these hypothetical logical thought exercises and end up coming to insane conclusions that they feel trapped in because they got there using pure logic.  Not realizing that their initial conditions were highly artificial so any conclusion they reach is only of academic value.There is a term for this.  \"Getting stuck up your own butt.\"  It wouldn't be so bad except that said people often take on an air of absolute superiority because they used \"only logic\" and in their head they can not be wrong.  Many people end up thinking like this as teenagers or 20 somethings, but most will have someone in their life who smacks them over the head and tells them to stop being so foolish, but if you have enough money and the Internet you can insulate yourself from that kind of oversight.\n \nreply",
      "The overlap between the Effective Altruism community and the Rationalist community is extremely high. They\u2019re largely the same people. Effective Altruism gained a lot of early attention on LessWrong, and the pessimistic focus on AI existential risk largely stems from an EA desire to avoid \u201ctemporal-discounting\u201d bias. The reasoning is something like: if you accept that future people count just as much as current people, and that the number of future people vastly outweighs everyone alive today (or who has ever lived), then even small probabilities of catastrophic events wiping out humanity yield enormous negative expected value. Therefore, nothing can produce greater positive expected value than preventing existential risks\u2014so working to reduce these risks becomes the highest priority.People in these communities are generally quite smart, and it\u2019s seductive to reason in a purely logical, deductive way. There is real value in thinking rigorously and in making sure you\u2019re not beholden to commonly held beliefs. But, like you said, reality is complex, and it\u2019s really hard to pick initial premises that capture everything relevant. The insane conclusions they get to could be avoided by re-checking & revising premises, especially when the argument is going in a direction that clashes with history, real-world experience, or basic common sense.\n \nreply",
      "Intelligence and rational thought is useful, but like any strategy it has its tradeoffs and limitations.  No amount of intelligence can overcome the chaos of long time horizons, especially when we're talking about human civilization.  IMHO it's reasonable to pick a long-term problem/risk and focus on solving it.  But it's pure hubris to think rationality will give you anything approaching high confidence of what the biggest problems and risks actually are on a 20-50 year time horizon, let alone 200-500 years or longer.The whole reason we even have time to think this way is because we are at the peak of an industrial civilization that has created a level of abundance that allows a lot of people a lot of time to think.  But the whole situation that we live in is not stable at all, \"progress\" could continue, or we could hit a peak and regress.  As much as we can see a lot of long-term trajectories (eg. peak oil, global warming), we really have no idea what will be the triggers and inflection points that change the social fabric in ways that are unforeseeable and quickly invalidate whatever prior assumptions all that deep thinking was resting upon.  I mean 50 years ago we thought overpopulation was the biggest risk, and that thinking has completely flipped even without a major trajectory change for industrial civilization in that time.\n \nreply",
      "I think one can levy a much more specific critique of rationalism: rationalism is in some sense self-defeating.  If you are rational you will necessarily conclude that the fundamental dynamic that drives the (interesting parts of) the universe is Darwinian evolution, which is not rational.  It blindly selects for reproductive fitness at the expense of all else.  If you are a gene, you can probably produce more offspring in an already-industrialized environment by making brains that lean more towards misogyny and sexual promiscuity than gender equality and intellectual achievement.The real conflict here is between Darwinism and enlightenment ideals.  But I have yet to see any self-styled Rationalists take this seriously.\n \nreply",
      "I always liken this to that we\u2019re all asteroids floating in space. There\u2019s no free will and everything is determined. We just see the whole thing unfold from one conscious perspective.Emotionally I don\u2019t subscribe to this view. Rationally I do.My critique for rational people is that they don\u2019t seem to fully take experience into account. It\u2019s assumptions + rationality + experience/data + whatever strong inclinations one has that seems to be the full picture for me.\n \nreply",
      "> no free willThat always seemed like a meaningless argument to me.  To an outside observer free will is indistinguishable from a random process over some range of possibilities.  You aren\u2019t going to randomly go to sleep with your hand in a fire, there\u2019s some hard coded biology preventing that choice but that only means human behavior isn\u2019t  completely random, hardly a groundbreaking discovery.At the other end we have no issues making an arbitrary decision where there\u2019s no way to predict what the better choice is.  So what exactly does free will bring to the table that we\u2019re missing without it?  Some sort of mystical soul, well what if that\u2019s also deterministic?  Unpredictability is useful in game theory, but computers can get that from a hardware RNG based on quantum processes like radioactive decay, so  it doesn\u2019t mean much.Finally, subjectively the answer isn\u2019t clear so what difference does it make?\n \nreply",
      "> That always seemed like a meaningless argument to me.Same as that is not the lived experience. I notice that I care about free choice.The idea that there's no free will may be a pessimistic outlook to some but to me it's a strictly neutral one. It used to be a bit negative, until I looked more closely that there's a difference between looking at a situation objectively and having a lived experience. When it comes to my inclinations and how I want to live life, lived experience takes precedence.I don't have my thoughts sharp on it, but I don't think the concept even exists philosophically, but I think that's also what you're getting at. It's a conceptual remnant from the past.\n \nreply",
      "If you get down to the quantum level there is no such thing as objective reality.  Our perception that the world is made of classical objects that actually exist at particular places at particular times and have continuity of identity is an illusion.  But it's a really compelling illusion, and you won't go far wrong treating it as if it were the truth in 99% of real-world situations.  Likewise, free will is an illusion, nothing more than a reflection of our ignorance of how our brains work.  But it is a really compelling illusion, and you won't go far wrong treating it as if it were the truth, at least some of the time.\n \nreply",
      "> If you get down to the quantum level there is no such thing as objective reality.What do you mean by that? It still exists doesn't it? Albeit in a probabilistic sense that becomes non-probabilistic at larger scales.I don't know much about quantum other than the high level conceptual stuff.\n \nreply"
    ],
    "link": "https://scottaaronson.blog/?p=8908",
    "first_paragraph": "A week ago I attended LessOnline, a rationalist blogging conference featuring many people I\u2019ve known for years\u2014Scott Alexander, Eliezer Yudkowsky, Zvi Mowshowitz, Sarah Constantin, Carl Feynman\u2014as well as people I\u2019ve known only online and was delighted to meet in person, like Joe Carlsmith and Jacob Falkovich and Daniel Reeves.  The conference was at Lighthaven, a bewildering maze of passageways, meeting-rooms, sleeping quarters, gardens, and vines off Telegraph Avenue in Berkeley, which has recently emerged as the nerd Shangri-La, or Galt\u2019s Gulch, or Shire, or whatever.  I did two events at this year\u2019s LessOnline: a conversation with Nate Soares about the Orthogonality Thesis, and an ask-me-anything session about quantum computing and theoretical computer science (no new ground there for regular consumers of my content).What I\u2019ll remember most from LessOnline is not the sessions, mine or others\u2019, but the unending conversation among hundreds of people all over the grounds, which took p"
  },
  {
    "title": "String Interpolation in C++ Using Glaze Stencil/Mustache (stephenberry.github.io)",
    "points": 18,
    "submitter": "npalli",
    "submit_time": "2025-06-16T19:05:14 1750100714",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://stephenberry.github.io/glaze/stencil-mustache/",
    "first_paragraph": "Glaze provides string interpolation for C++ structs through the stencil and mustache formats. These provide templating mechanisms for formatting structured data into strings, inspired by the Mustache templating language. This enables the generation of dynamic output by combining predefined templates with C++ structs.[!NOTE]result in these examples is a std::expected<std::string, glz::error_ctx>. Like most functions in Glaze (e.g. glz::write_json) you can also pass in your own string buffer as the last argument, in which case the return type is glz::error_ctx.Glaze provides glz::mustache with the same interface as glz::stencil, but with HTML escaping behavior:The following characters are escaped in mustache double-brace interpolation:\n- < \u2192 &lt;\n- > \u2192 &gt;\n- & \u2192 &amp;\n- \" \u2192 &quot;\n- ' \u2192 &#x27;The stencil/mustache functions return std::expected<std::string, glz::error_ctx>. Common errors include:[!TIP]Use glz::format_error like the rest of Glaze to generate a nicely formatted error with "
  },
  {
    "title": "Show HN: RM2000 Tape Recorder, an audio sampler for macOS (rm2000.app)",
    "points": 29,
    "submitter": "marcelox86",
    "submit_time": "2025-06-17T16:20:59 1750177259",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=44300854",
    "comments": [
      "Well done; this app has been needed forever, yet shockingly missing for years. I have some homebrew solutions in Max/MSP and Ableton but they're cumbersome.My one request / question: is there any way this could be triggered with a global shortcut? I've long dreamed of being able to record ambient audio from films while watching without leaving full screen, much like one would take a screenshot.\n \nreply",
      "Love the app, UI is great, congrats! Small side note, website loads a bit slow on mi computer, using latest Chrome on latest MacOS.\n \nreply",
      "From the website> Recording system audio is still very clunky: audio routing hacks, third-party tools that aren't meant for the task - too many steps.I really love the idea because I'm one of the people who previously had to use those hacks!But I wonder what are the privacy implications? Maybe the OP could clarify that this:> RM2000 Tape Recorder just listens. Whatever your Mac is playing, it can record it instantlymeans that nothing is retained unless explicitly recorded by the user, right? There isn't cache somewhere that captures last X minutes of audio or anything like that?\n \nreply",
      "why call it a tape recorder instead of a digital recorder? nothing in the skeuomorphic design even hints at a tape recorder and looks much more like a digital recorder. if it's a tape recorder, do you introduce tape hiss based on a preference of what tape formula is being used? maybe then introduce a NR option? also, what type of tape are you recording too? is it cassette tape, or a professional studio tape? that would also have quality implications to consider. what about dirty heads or pinch roller that is sticking so the tape doesn't move at a constant rate? also, is it assumed that it's always a new tape with each recording, or will it emulate sound quality degradation of multiple re-use of the same tape? could be a cool easter egg like the crayon color picker with a system clock set far into the futureor is this just a for the lulz throw back because it's kool type of name?\n \nreply",
      "\"This app is currently not available in your country or region.\"  (Ireland)Seems like a strange thing to be geolocked?\n \nreply",
      "I like the design, very \"Delicious Era\". That rosetta icon gave me hope it worked on PowerPC though (Rosetta 2 icon is different)\n \nreply",
      "Wow, I see Garamond! It's been a long time. :)\n \nreply",
      "I like Oneohtrix Point Never :)\n \nreply",
      "\"lifetime\" means \"for the lifetime of the application\" I assume? Or does it mean \"for the lifetime of macOS 15 and the cost of updating the app to macOS 26 will be covered by additional purchases\"? I'm just wary of this \"lifetime\" idea.\n \nreply"
    ],
    "link": "https://rm2000.app",
    "first_paragraph": " \nThe fastest way to record inspiration from anywhere\n  \nYour Mac plays sound all day - it just doesn't make it easy to keep any of it.\n \nRecording system audio is still very clunky: audio routing hacks, third-party tools that aren't meant for the task - too many steps.\n \nRM2000 Tape Recorder just listens. Whatever your Mac is playing, it can record it instantly - then, lets you file, tag, and organize it.\n  \nRM2000 Tape Recorder works straight out of the box and hears what you hear. No more fighting with Soundflower or audio routing.\n  \nFile-over-app philosophy - use your samples everywhere\n  \nNo subscriptions - only a Lifetime License, alongside a free trial.\n \nLaunch sale: Get a Lifetime License for $6.99. Never expires! Offer ends June 30th.\n  \n\u00a9 2025 Marcelo Mendez  \nRM2000 Tape Recorder works completely offline, stores all of your samples with longetivity in mind, and features a beautiful user interface. There are still many features yet to come out - stay tuned!\n \nI made this we"
  },
  {
    "title": "Homegrown Closures for Uxn (krzysckh.org)",
    "points": 71,
    "submitter": "todsacerdoti",
    "submit_time": "2025-06-19T17:29:35 1750354175",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=44320747",
    "comments": [
      "Built on top of Owl Lisp[1]. TIL about this dialect, and it looks interesting! Instead of native threads, it has continuation-based threads[2], and it seems the whole VM architecture is based on that.[1]: https://haltp.org/posts/owl.html[2]: https://haltp.org/posts/owl.html#heading26\n \nreply",
      "If you've never heard of Uxn I highly recommend reading this: https://100r.co/site/weathering_software_winter.html\n \nreply",
      "Always wondered why I don't go on living like the folks at 100r.co. Not necessarily in a boat across the ocean, but at least writing software for fun and learning as days pass. What a contrast if we talk about writing software so that the c-level executives can get richer and richer while you have to work for decades to pay a middle size house (if not an apartment). Not to mention the on-call rotation that eats your free time, the countless stupid tech interviews you need to pass and the amount of self control one has to have in every useless daily standup.\n \nreply",
      "Personally, when it came to buying a house it was based on something that I could pay down quick and be happy with until my last days. I call it the cottage but it is really just a very small house. But work hours are staggering reasonable because of it.If you have low needs and wants, you do not have to strive as hard to get places. I live small so that I do not have to serve the king.\n \nreply",
      "I think because they have next to no overhead living on a boat is a major reason why they can do so much creative output. When you dont have rent to pay and other major bills, you have more idle time to do fun things you enjoy doing.\n \nreply",
      "Like navigate your boat. And maintain your boat. And repair your boat...\n \nreply",
      "Yep, boat maintenance is an ongoing predicament. It would be nice if you could build it and leave it be but the sea is harsh to all things and maintenance is necessary.\n \nreply",
      "> writing software for fun and learning as days passThese kinds of projects always remind me that it's still possible for software itself to be fun and have a soul of sorts, similar in spirit to the wonderful why's (poignant) Guide to Ruby.It's a far cry from what can feel like an environment that, in a lot of cases, seems to devour complexity for complexity's sake and gorge itself on solving ethereal problems that don't really need to exist. End tiny rant.\n \nreply",
      "I'm not surprised because of their presence in the community, but I am glad to see Permacomputing mentioned. https://permacomputing.net/\n \nreply"
    ],
    "link": "https://krzysckh.org/b/Homegrown-closures-for-uxn.html",
    "first_paragraph": "For a week or so now, I've been writing ni\u00ebnor, a \"lispy\nenvironment for uxn\". I did not want it to become a full-blown lisp\nbut just another way of writing uxntal. Uxntal is a\nbit too dense for my liking, and I prefer lisp/scheme s-expression\nsyntax. Ni\u00ebnor is just a compiler and a macroexpander that takes in\nscheme-like code and spits out uxn roms.This article describes my homegrown method of creating lexically\nscoped closures in this environment.Or, anonymous functions.When ignoring lexical scope lambdas are really simple to implement if\nwe can already compile named functions.Here is some simplified code with commentary from the compiler that\ndoes exactly that (I've skipped some boring parts). We simply give the\nanonymous function a name, skip the compilation for now (add to\nepilogue), and push the name (that will get resolved later\nby the compiler) in its place.Or, lambdas with extra steps.Closures are anonymous functions that capture variables from the\nenvironment. Consider this c"
  },
  {
    "title": "Show HN: Unregistry \u2013 \u201cdocker push\u201d directly to servers without a registry (github.com/psviderski)",
    "points": 632,
    "submitter": "psviderski",
    "submit_time": "2025-06-18T23:17:10 1750288630",
    "num_comments": 139,
    "comments_url": "https://news.ycombinator.com/item?id=44314085",
    "comments": [
      "Docker creator here. I love this. In my opinion the ideal design would have been:1. No distinction between docker engine and docker registry. Just a single server that can store, transfer and run containers as needed. It would have been a much more robust building block, and would have avoided the regrettable drift between how the engine & registry store images.2. push-to-cluster deployment. Every production cluster should have a distributed image store, and pushing images to this store should be what triggers a deployment. The current status quo - push image to registry; configure cluster; individual nodes of the cluster pull from registry - is brittle and inefficient. I advocated for a better design, but the inertia was already too great, and the early Kubernetes community was hostile to any idea coming from Docker.\n \nreply",
      "I naively sent the Docker developers a PR[1] to add this functionality into mainline Docker back in 2015. I was rapidly redirected into helping out in other areas - not having to use a registry undermined their business model too much I guess.[1]: https://github.com/richardcrichardc/docker2docker\n \nreply",
      "Nice. And the `pussh` command definitely deserves the distinction of one of the most elegant puns: easy to remember, self-explanatory, and just one letter away from its sister standard command.\n \nreply",
      "It's fine, but it wouldn't hurt to have a more formal alias like `docker push-over-ssh`.EDIT: why I think it's important because on automations that are developed collaboratively, \"pussh\" could be seen as a typo by someone unfamiliar with the feature and cause unnecessary confusion, whereas \"push-over-ssh\" is clearly deliberate. Think of them maybe as short-hand/full flags.\n \nreply",
      "That's a valid concern. You can very easily give it whatever name you like. Docker looks for `docker-COMAND` executables in ~/.docker/cli-plugins directory making COMMAND a `docker` subcommand.Rename the file to whatever you like, e.g. to get `docker pushoverssh`:  mv ~/.docker/cli-plugins/docker-pussh ~/.docker/cli-plugins/docker-pushoverssh\n\nNote that Docker doesn't allow dashes in plugin commands.\n \nreply",
      "can easily see an engineer spotting pussh in a ci/cd workflow or something and thinking \"this is a mistake\" and changing it.\n \nreply",
      "> The extra 's' is for 'sssh'> What's that extra 's' for?> That's a typo\n \nreply",
      "https://www.youtube.com/watch?v=3m6Blqs0IgY\n \nreply",
      "and prone to collision!\n \nreply",
      "Indeed so! Because it's art, not engineering. The engineering approach would require a recognizably distinct command, eliminating the possibility of such a pun.\n \nreply"
    ],
    "link": "https://github.com/psviderski/unregistry",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Push docker images directly to remote servers without an external registry\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\u25b8 Push docker images directly to remote servers without an external registry \u25c2\n\n\nUnregistry is a lightweight container image registry that stores and serves images directly from your Docker daemon's\nstorage.The included docker pussh command (extra 's' for SSH) lets you push images straight to remote Docker servers over SSH.\nIt transfers only the missing layers, making it fast and efficient.You've built a Docker image locally. Now you need it on your server. Your options suck:You just want to move an image from A to B. Why is this so hard?That's it. Your image is on the remote server. No registry setup, no subscription, no intermediate st"
  },
  {
    "title": "Show HN: Claude Code Usage Monitor \u2013 real-time tracker to dodge usage cut-offs (github.com/maciek-roboblog)",
    "points": 195,
    "submitter": "Maciej-roboblog",
    "submit_time": "2025-06-19T09:46:43 1750326403",
    "num_comments": 108,
    "comments_url": "https://news.ycombinator.com/item?id=44317012",
    "comments": [
      "I really like this idea as I find Claude's transparency frustrating. Claude code's killer features revolve around better tools to manage context and limits vs the desktop app (compact and the % remaining until auto compact), but it's not enough.If I can offer any advice, it's that the high use of emojis in a project readme (at least for me) looks so unprofessional and makes me worry that a project was vibe -coded in the sense that the AI was possibly not babysat to the extent I think they should. That's just me, though\n \nreply",
      "I got into software in a time where you would get sent to a mental institution when spotted using emojis in a code base. Times have changed.. I use emojis regularly because they help me organize context more visually. Code has now many emojis to keep me happy.\n \nreply",
      "This code was written in pure vibe-coding style \u2014 mostly for fun.\nI've got about 10 years of experience in IT, and even I fully agree:\na 1000-line main file like this one probably deserves to be locked away in a secure facility.But hey \u2014 if it's stupid and it works, it ain't stupid.\n \nreply",
      "Always depends on the cost of failure I suppose.\n \nreply",
      "It strikes me as very much a current aesthetic in younger companies or smaller startups, maybe highly influenced by Notion. No one makes a list or page or calendar invite in my current company without choosing an emoji for it.\n \nreply",
      "It was cool until 2022. Then LLMs started injecting these emoji everywhere and it became the chief marker for code/doc smell.\n \nreply",
      "> looks so unprofessional and makes me worry that a project was vibe -coded in the sense that the AI was possibly not babysat to the extent I think they should. That's just me, thoughThe irony of comments like this on software designed entirely for ai coding...\n \nreply",
      "Looks so unprofessional, lol, says the guy wanting to use a free app, this isn't a microsoft made app lol it's a guy making a github app for free the audacity people have these days to shit on peoples project for 0 reason\n \nreply",
      "AI coding where the human stays in control and reads and confirms code is totally different from vibe coding where you don't read code and just prompt until it sort of works.\n \nreply",
      "I completely agree. Andrej's definition was pretty explicit and in my experience it is two separate worlds of AI use.\n \nreply"
    ],
    "link": "https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Real-time Claude Code usage monitor with predictions and warnings\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\nA beautiful real-time terminal monitoring tool for Claude AI token usage. Track your token consumption, burn rate, and get predictions about when you'll run out of tokens.For immediate testing (not recommended for regular use):Using a virtual environment is strongly recommended because:If you don't have venv module available:Alternatively, use the virtualenv package:After initial setup, you only need:Create an alias for quick access:The default timezone is Europe/Warsaw. Change it to any valid timezone:Claude Code operates on a 5-hour rolling session window system:Default reference times (in your configured timezone):\u26a0\ufe0f Important: These are ref"
  },
  {
    "title": "DNA floating in the air tracks wildlife, viruses, even drugs (sciencedaily.com)",
    "points": 72,
    "submitter": "karlperera",
    "submit_time": "2025-06-16T09:38:00 1750066680",
    "num_comments": 60,
    "comments_url": "https://news.ycombinator.com/item?id=44287926",
    "comments": [
      "I suppose you could try and see where I've been since I have my sequence publicly stored here https://my.pgp-hms.org/profile/hu81A8CCIf nothing else, I'll serve as a cautionary tale against this if something happens to me as a result of having my DNA publicly available to all.\n \nreply",
      "I keep my genome on my hard drive \u2013 it\u2019s only a few tens of GBs!Language models are pretty good at looking up and testing SNPs, but even that has low utility for me. Haven\u2019t found a good use case for it yet.\n \nreply",
      "How much did getting your DNA sequenced cost?\n \nreply",
      "I guess nothing will happen to you, but it's a bit like being naked on the internet?\n \nreply",
      "a person is not their DNA\n \nreply",
      "pictures of a person's private bits are more closely linked to their identity (or self-concept) than DNA?One is alterable, the other isn't\n..\n \nreply",
      "More closely linked to their phenotype at any rate. And though DNA is in fact alterable, that's pretty irrelevant, culturally speaking.\n \nreply",
      "Oops should have said raw, uncompiled, bitsI agree DNA isn't that culturally relevant to an identity but that just seems to be due to anti-intellectualismSeparate from the idea that the easier to alter something is, the more it should considered as a healthy part of identity..\n \nreply",
      "Supposedly, small vacuums built into bags and purses were used to surreptitiously gather DNA to identify unidentified occupants of a compound suspected to be used by Osama bin Laden and those related to him.https://en.wikipedia.org/wiki/Manhunt_for_Osama_bin_Laden\n \nreply",
      "What is somewhat amusing to me is that any one who has ever run PCRs for humans or low template DNA knows to do this with the utmost precaution for airborne DNA contamination. 35 to 45 cycles of 2x amplification for paleolithic sample.\n \nreply"
    ],
    "link": "https://www.sciencedaily.com/releases/2025/06/250603114822.htm",
    "first_paragraph": "Dublin is known as a city where you can enjoy a few pints of Guiness, get a warm welcome from the locals and hear lively traditional music drifting out of pubs and into the city air.But it's not just music floating on the breeze. The air of Dublin also contains cannabis, poppy, even magic mushrooms -- at least their DNA.That's according to a new study that reveals the power of DNA, vacuumed up from the air, which can track everything from elusive bobcats to illicit drugs.\"The level of information that's available in environmental DNA is such that we're only starting to consider what the potential applications can be, from humans, to wildlife to other species that have implications for human health,\" said David Duffy, Ph.D., a professor of wildlife disease genomics at the University of Florida and lead author of a new study showing the widespread utility of DNA vacuumed from the air.Housed at UF's Whitney Laboratory for Marine Bioscience, Duffy's lab developed new methods for decipherin"
  },
  {
    "title": "Public/protected/private is an unnecessary feature (catern.com)",
    "points": 53,
    "submitter": "PaulHoule",
    "submit_time": "2025-06-17T17:56:04 1750182964",
    "num_comments": 51,
    "comments_url": "https://news.ycombinator.com/item?id=44301875",
    "comments": [
      "I primarily work in C# and access modifiers allow me to constrain which types or their members on it are accessible to other types or assemblies.This is particularly useful when authoring libraries as I know that for anything not public, I can refactor to my hearts content (provided the externally observed behaviour is otherwise unchanged).It\u2019s a bit of a pity that it was only relatively recently that Visual Studio changed the template for new classes to use internal instead of public.There are lots of public classes in codebases I work on which are probably only public due to the old default and not used externally so could be refactored, but it\u2019s hard to be sure.Whereas with internal classes I can be sure (unless someone has used reflection, but then that\u2019s their problem).\n \nreply",
      "Same here. I recently came across a real use for the newer `private protected` (`FamANDAssembly`) modifier combo. Only derived types that are internal to my assembly can access this member. Beautiful. It re-confirmed my long standing belief that OOP is an incredibly powerful and valid pattern.\n \nreply",
      "I also wish 'sealed' was the default for classes in templates. Usually you're not thinking about what will happen if someone shows up 5 years later to derive from a random class you created, and it has performance benefits to seal a type. You can always remove the sealed qualifier later on purpose.\n \nreply",
      "I agree that it's unnecessary, and Python's success despite not having private members/fields is testament to that. But public/private fields/methods, like pretty much everything in OOP, is a 'social' rather than engineering innovation. Just as a class tells you certain operations and data are inter-related, private methods or fields signal to a class's users that you probably will break things if you interfere with certain parts.Otherwise, at least in C++, you can often bypass the private specifier without much difficulty. Perhaps the laziest and easiest way to do so is    #define private public\n    #include \"foo.hpp\"\n    #undef private\n \nreply",
      "#define duck_season wabbit_season#include \"shoot.hpp\"\n \nreply",
      "You should safely shoot yourself in the foot with `friend`, which is a core language feature for accessing private members/functions/etc.https://www.cppreference.com/w/cpp/language/friend.htmlRedefining public as private can violate the ODR rule and triggers undefined behaviour because private members are ordered in memory in an undefined way, while public members are ordered in a standard way.\n \nreply",
      "Python absolutely does have private members. If a member starts with two underscores, its class name (and an underscore) is prefixed to the member name, essentially resulting in the same behavior as a private modifier. (Although there are ways to circumvent this in Python, there are also ways to access private fields in most other languages.)But even if that were true, that wouldn't mean that accessibility modifiers are unnecessary in other languages. Python is a certain language with certain idioms, and other languages are more or less OOP and hence some constructs that might not make sense in Python definitely make sense elsewhere.\n \nreply",
      "I see what you mean what I wouldn't really qualify name mangling as private members, you're discouraged to access them but you can access them all the same.\n \nreply",
      "I'd consider it the same, personally. It's a social contract that you can absolutely break if you want, just with a bit of work.Less work in Python than it is Java, but there are ways to get to them in Java, or hell, just modify the source and change it directly. By doing that, you're doing something the developer didn't intend and you shouldn't be surprised if it doesn't work at some point in the future. Same as I'd expect in the Python case.\n \nreply",
      "Sure, I think the bar to modify is higher in Java for example but I can't really find fault in what you're saying because I'd never unknowingly modify a member with double underscores in Python.\n \nreply"
    ],
    "link": "https://catern.com/private.html",
    "first_paragraph": "\nSince Car can't restrict subclasses to only use an interface,\nany subclasses can violate Car's internal invariants.\n\nSo either Car should disallow inheritance\n(always an option by simply not exposing the Car type at all, as in point 2),\nor Car needs another way to define an interface that will take effect on subclasses.\n\nThat other way to define an interface which works for subclasses is the access modifiers:\npublic, protected, and private.\nBy marking members as private,\nCar can do something like define an interface which affects subclasses;\nthis pseudo-interface contains exactly and only those members of Car which are public and protected.\n\nBut this is absurd:\nWhy do we have two ways to define an interface?\n\nWouldn't it be better for Car to be able to force subclasses to only use Car through a specific (normal) interface?\nThere'd be no loss in functionality;\nCar could still have separate interfaces for instantiators and subclasses.\nWe'd just define those interfaces with the same mech"
  },
  {
    "title": "What would a Kubernetes 2.0 look like (matduggan.com)",
    "points": 141,
    "submitter": "Bogdanp",
    "submit_time": "2025-06-19T12:00:54 1750334454",
    "num_comments": 223,
    "comments_url": "https://news.ycombinator.com/item?id=44317825",
    "comments": [
      "The #1 problem with Kubernetes is it's not something that \"Just Works.\" There's a very small subset of engineers who can stand up services on Kubernetes without having it fall over in production \u2013 not to mention actually running & maintaining a Kubernetes cluster on your own VMs.In response, there's been a wave of \"serverless\" startups because the idea of running anything yourself has become understood as (a) a time sink, (b) incredibly error prone, and (c) very likely to fail in production.I think a Kubernetes 2.0 should consider what it would look like to have a deployment platform that engineers can easily adopt and feel confident running themselves \u2013 while still maintaining itself as a small-ish core orchestrator with strong primitives.I've been spending a lot of time building Rivet to itch my own itch of an orchestrator & deployment platform that I can self-host and scale trivially: https://github.com/rivet-gg/rivetWe currently advertise as the \"open-source serverless platform,\" but I often think of the problem as \"what does Kubernetes 2.0 look like.\" People are already adopting it to push the limits into things that Kubernetes would traditionally be good at. We've found the biggest strong point is that you're able to build roughly the equivalent of a Kubernetes controller trivially. This unlocks features more complex workload orchestration (game servers, per-tenant deploys), multitenancy (vibe coding per-tenant backends, LLM code interpreters), metered billing per-tenant, more powerful operators, etc.\n \nreply",
      "I really dislike this take and I see it all the time. Also I'm old and I'm jaded, so it is what it is...Someone decides X technology is too heavy-weight and wants to just run things simply on their laptop because \"I don't need all that cruft\". They spend time and resources inventing technology Y to suit their needs. Technology Y gets popular and people add to it so it can scale, because no one runs shit in production off their laptops. Someone else comes along and says, \"damn, technology Y is too heavyweight, I don't need all this cruft...\"\"There are neither beginnings nor endings to the Wheel of Time. But it was a beginning.\u201d\n \nreply",
      "I hope this isn't the case here with Rivet. I genuinely believe that Kubernetes does a good job for what's on the tin (i.e. container orchestration at scale), but there's an evolution that needs to happen.If you'll entertain my argument for a second:The job of someone designing systems like this is to decide what are the correct primitives and invest in building a simple + flexible platform around those.The original cloud primitives were VMs, block devices, LBs, and VPCs.Kubernetes became popular because it standardized primitives (pods, PVCs, services, RBAC) that containerized applications needed.Rivet's taking a different approach of investing in different three primitives based on how most organizations deploy their applications today:- Stateless Functions (a la Fluid Compute)- Stateful Workers (a la Cloudflare Durable Objects)- Containers (a la Fly.io)I fully expect to raise a few hackles claiming these are the \"new primitives\" for modern applications, but our experience shows it's solving real problems for real applications today.Edit: Clarified \"original cloud primitives\"\n \nreply",
      "It\u2019s also possible for things to just be too complex.Just because something\u2019s complex doesn\u2019t necessarily mean it has to be that complex.\n \nreply",
      "IMHO, the rest of that sentence is \"be too complex for some metric within some audience\"I can assure you that trying to reproduce kubernetes with a shitload of shell scripts, autoscaling groups, cloudwatch metrics, and hopes-and-prayers is too complex for my metric within the audience of people who know Kubernetes\n \nreply",
      "Or too generic. A lot of the complexity if from trying to support all use cases. For each new feature there is a clear case of \"we have X happy users, and Y people who would start using it if we just added Z\". But repeat that often enough and the whole things becomes so complex and abstract that you lose those happy users.The tools I've most enjoyed (including deployment tools) are those with a clear target group and vision, along with leadership that rejects anything that falls too far outside of it. Yes, it usually doesn't have all the features I want, but it also doesn't have a myriad of features I don't need\n \nreply",
      "Because of promo-driven, resume-driven culture, engineers are constantly creating complexity.  No one EVER got a promotion for creating LESS.\n \nreply",
      "Just write the stuff you need for the situation you\u2019re in.This stupid need we have to create general purpose platforms is going to be the end of progress in this industry.Just write what you need for the situation you\u2019re in.  Don\u2019t use kubernetes and helm, use your own small thing that was written specifically to solve the problem you have; not a future problem you might not have, and not someone else\u2019s problem.  The problem that you have right now.It takes much less code than you think it will, and after you\u2019ve done it a few times, all other solutions look like enormous Rube Goldberg machines (because that\u2019s what they are, really).It is 1/100th of the complexity to just write your own little thing and maintain it than it is to run things in Kubernetes and to maintain that monster.I\u2019m not talking about writing monoliths again.  I\u2019m talking about writing only the tiny little bits of kubernetes that you really need to do what you need done, then deploying to that.\n \nreply",
      "See also: JavaScript frameworks\n \nreply",
      "Rivet may be the best of both worlds, because it's not only another complex project built to simplify the complexity of doing complex things, but you also get to have to write all your management in TypeScript.\n \nreply"
    ],
    "link": "https://matduggan.com/what-would-a-kubernetes-2-0-look-like/",
    "first_paragraph": "Old fashioned YAML farmerAround 2012-2013 I started to hear a lot in the sysadmin community about a technology called \"Borg\". It was (apparently) some sort of Linux container system inside of Google that ran all of their stuff. The terminology was a bit baffling, with something called a \"Borglet\" inside of clusters with \"cells\" but the basics started to leak. There was a concept of \"services\" and a concept of \"jobs\", where applications could use services to respond to user requests and then jobs to complete batch jobs that ran for much longer periods of time. Then on June 7th, 2014, we got our first commit of Kubernetes. The Greek word for 'helmsman' that absolutely no one could pronounce correctly for the first three years. (Is it koo-ber-NET-ees? koo-ber-NEET-ees? Just give up and call it k8s like the rest of us.) Microsoft, RedHat, IBM, Docker join the Kubernetes community pretty quickly after this, which raised Kubernetes from an interesting Google thing to \"maybe this is a real pr"
  },
  {
    "title": "We Can Just Measure Things (pocoo.org)",
    "points": 61,
    "submitter": "tosh",
    "submit_time": "2025-06-17T11:15:25 1750158925",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=44297761",
    "comments": [
      "Another, related benefit of LLMs in this situation is that we can observe their hallucinations and use them for design. I've come up with a couple situations where I saw Copilot hallucinate a method, and I agreed that that method should've been there. It helps confirm whether the naming of things makes sense too.What's ironic about this is that the very things that TFA points out are needed for success (test coverage, debuggability, a way to run locally etc) are exactly the things that typical LLMs themselves lack.\n \nreply",
      "I've found LLM's to be extremely helpful in naming and general function/API design, where there a lot of different ways to express combinations of parameters.I know what seems natural to me but that's because I'm extremely familiar with the internal workings of the project. LLM's seem to be very good at coming with names that are just descriptive enough but not too long, and most importantly follow \"general conventions\" from similar projects that I may not be aware of. I can't count the number of times an LLM has given me a name for a function that I've thought, oh of course, that's a much clearer name that what I was using. And I thought I was already pretty good at naming things...\n \nreply",
      "More often, LLMs give me wildly complex and over-engineered solutions.\n \nreply",
      "I have had this happen too, but a \"this seems complex, do we need that complexity? can you justify it? or can we make this simpler?\" or similar has them come back with something much better.\n \nreply",
      "We can just measure things, but then there\u2019s Goodhart's law.With the proposed way of measuring code quality, it\u2019s also unclear how comparable the resulting numbers would be between different projects. If one project has more essential complexity than another project, it\u2019s bound to yield a worse score, even if the code quality is on par.\n \nreply",
      "I would argue you can't compare between projects due to the reasons you state. But you can try and improve the metrics within a single project.Cycolmatic complexity is a terrible metric to obsesses over yet in a project I was on it was undeniably true that the newer code written by more experienced Devs was both subjectively nicer and also had lower cycolmatic complexity than the older code worked on by a bunch of juniors (some of the juniors had then become some of the experienced Devs who wrote the newer code)\n \nreply",
      "> But you can try and improve the metrics within a single project.Yes. But it means that it doesn\u2019t let you assess code quality, only (at best) changes in code quality. And it\u2019s difficult as soon as you add or remove functionality, because then it isn\u2019t strictly speaking the same project anymore, as you may have increased or decreased the essential complexity. What you can assess is whether a pure refactor improves or worsens a project\u2019s amenibility to AI coding.\n \nreply",
      "I'm really skeptical of using current LLMs for judging codebases like this. Just today I got Gemini to solve a tricky bug, but it only worked after providing it more debug output after solving some of it myself.The first time I tried without the deeper output, it \"solved\" it by writing a load of code that failed in loads of other ways, and ended up not even being related to the actual issue.Like you can be certain it'll give you some nice looking metrics and measurements - but how do you know if they're accurate?\n \nreply",
      "> I'm really skeptical of using current LLMs for judging codebases like this.I'm not necessarily convinced that the current generation of LLMs are overly amazing at this, but they definitely are very good at measuring inefficiency of tooling and problematic APIs.  That's not all the issues, but it can at least be useful to evaluate some classes of problems.\n \nreply",
      "What do you mean that it \"ended up not even being related to the actual issue\"? If you give it a failing test suite to turn green and it does, then either its solution is indeed related to the issue, or your tests are incomplete; so you improve the tests and try again, right? Or am I missing something?\n \nreply"
    ],
    "link": "https://lucumr.pocoo.org/2025/6/17/measuring/",
    "first_paragraph": "written on Tuesday, June 17, 2025This week I spent time with friends to letting agents go wild\nand see what we could build in 24 hours.  I\ntook some notes for myself to reflect on that experience.  I won't bore\nyou with another vibecoding post, but you can read Peter's post\nabout how that went.As fun as it was, it also was frustrating in other ways and in entire\npredictable ways.  It became a meme about how much I hated working with\nXcode for this project.  This got me thinking quite a bit more that this\nhas been an entirely unacceptable experience for a long time, but with\nprogramming agents, the pain becomes measurable.When I first dove into programming I found the idea of RTFM quite hilarious.  \u201cWhy are you\nasking dumb questions, just read it up.\u201d  The unfortunate reality is that\nthe manual often doesn't exist \u2014 or is wrong.  In fact, we as engineers\nare quite willing to subject each others to completely inadequate tooling,\nbad or missing documentation and ridiculous API footguns al"
  }
]