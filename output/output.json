[
  {
    "title": "Sugar industry influenced researchers and blamed fat for CVD (2016) (ucsf.edu)",
    "points": 649,
    "submitter": "aldarion",
    "submit_time": "2026-01-07T14:29:25 1767796165",
    "num_comments": 388,
    "comments_url": "https://news.ycombinator.com/item?id=46526740",
    "comments": [
      "When this news first came out it was mind blowing, but at the same time I don't entirely get it.So the money quote seems to be:> The literature review heavily criticized studies linking sucrose to heart disease, while ignoring limitations of studies investigating dietary fats.They paid a total of 2 people $50,000 (edit: in 2016 dollars).That doesn't seem like enough to entirely shape worldwide discourse around nutrition and sugar. And the research was out there! Does everybody only read this single Harvard literature review? Does nobody read journals, or other meta studies, or anything? Did the researchers from other institutions whose research was criticized not make any fuss?I guess the thing that I most don't get is it's now been 10 years since then, and I haven't seen any news about the link between sugar and CVD.> There is now a considerable body of evidence linking added sugars to hypertension and cardiovascular diseaseOkay, where is it? What are the conclusions? Is sugar actually contributing more than fat for CVD in most patients? Edit: Or, is the truth that fat really is the most significant, and sugar plays some role but it's strictly less?reply",
      "You\u2019re exactly right: This one incident did not shape the entire body of scientific research.There is a common trick used in contrarian argumentation where a single flaw is used to \u201cdebunk\u201d an entire side of the debate. The next step, often implied rather than explicit, is to push the reader into assuming that the opposite position must therefore be the correct one. They don\u2019t want you to apply the same level of rigor and introspection to the opposite side, though.In the sugar versus saturated fat debate, this incident is used as the lure to get people to blame sugar as the root cause. There is a push to make saturated fat viewed as not only neutral, but healthy and good for you. Yet if you apply the same standards of rigor and inspection of the evidence, excess sugar and excess saturated fat are both not good for you.There is another fallacy in play where people pushing these debates want you to think that there is only one single cause of CVD or health issues: Either sugar, carbs, fat, or something else. The game they play is to point the finger at one thing and imply that it gets the other thing off the hook. Don\u2019t fall for this game.reply",
      "I think common sense here can be a guide though. You don't need sugar at all, excluding high levels of anaerobic exercise. Your liver can produce the glucose your body actually needs from other sources (gluconeogenesis) and a lot of your tissues that use glucose also can use fatty acids or ketones. Fructose isn't needed at all. (\"low blood sugar\" isn't a symptom of not consuming enough sugar, it's a symptom of a disregulated metabolism -- ie insulin resistance or other conditions)Saturated fats have all sorts of uses biologically.reply",
      "Looks like it's true that low-carb adapted athletes rely more on fat oxidation during exercise but performance suffers nonetheless because of increased oxygen demands that basically cannot be met.reply",
      "I would caution that just because your body can make something doesn't mean it will have optimal performance when doing so. People in ketosis do have worse peak performance in sports than those that eat more carbs/sugar.reply",
      "Your entire argument here applies in the other direction as well. You do not need dietary saturated fats, and sugar has all sorts of uses biologically.reply",
      "That is only partly true: you don't need dietary saturated fats, but you do need essential fats (omega-3 and omega-6), which are polyunsaturated. However, sugar does not have all sorts of uses biologically; it has only one: as one (but not the only one) source of energy.reply",
      "That has nothing to do with whether excesses of those nutrients cause cardiovascular disease, though. The general consensus is that the healthiest diet is one with 5-10% of total calories from saturated fat. For most people, it's necessary to restrict saturated fat to land in that range. We also need to distinguish between sugar and carbohydrates. Again, the general consensus is that intake of sugar and refined carbohydrates should be minimized, while 50-75% of total calories should come from sources of complex carbohydrates like vegetables, beans, and whole grains.reply",
      "Funny you should say that after today's FDA announcement. (Not taking any side here just interested in how we determine what is a consensus these days)reply",
      "It's hard, because when an issue becomes politicized everyone has their own preferred \"consensus\". I would say it should come from the scientific community, not government agencies. Sometimes government agencies agree with the scientific consensus, but not always.My go-to source for nutrition information is Understanding Nutrition by Whitney and Rolfes.reply"
    ],
    "link": "https://www.ucsf.edu/news/2016/09/404081/sugar-papers-reveal-industry-role-shifting-national-heart-disease-focus",
    "first_paragraph": "University of California San FranciscoThis article is archived and only made available for historical reference. \n        If you\u2019d like to discover UCSF\u2019s most recent advances in research, education and \n        patient care, please visit the  UCSF News Center.\n\n            \n      By\n  \n    Elizabeth FernandezA newly discovered cache of industry documents revealed that the sugar industry began working closely with nutrition scientists in the mid-1960s to single out fat and cholesterol as the dietary causes of coronary heart disease and to downplay evidence that sucrose consumption was also a risk factor.An analysis of those papers by researchers at UC San Francisco appears Sept. 12, 2016, in JAMA Internal Medicine. The internal industry documents, which were found in public archives, showed that a sugar industry trade organization recognized as early as 1954 that if Americans adopted low-fat diets, then per-capita consumption of sucrose would increase by more than one-third. The trade "
  },
  {
    "title": "Tailscale state file encryption no longer enabled by default (tailscale.com)",
    "points": 209,
    "submitter": "traceroute66",
    "submit_time": "2026-01-07T20:16:50 1767817010",
    "num_comments": 86,
    "comments_url": "https://news.ycombinator.com/item?id=46531925",
    "comments": [
      "I'm one of the Tailscale engineers who built node state encryption initially (@awly on Github), and who made the call to turn it off by default in 1.92.5.Another comment in this thread guessed right - this feature is too support intensive.\nOur original thinking was that a TPM being reset or replaced is always sign of tampering and should result in the client refusing to start or connect. But turns out there are many situations where TPMs are not reliable for non-malicious reasons. Some examples:\n* https://github.com/tailscale/tailscale/issues/17654\n* https://github.com/tailscale/tailscale/issues/18288\n* https://github.com/tailscale/tailscale/issues/18302\n* plus a number of support ticketsTPMs are a great tool for organizations that have good control of their devices. But the very heterogeneous fleet of devices that Tailscale users have is very difficult to support out of the box. So for now we leave it to security-conscious users and admins to enable, while avoiding unexpected breakage for the broader user base.We should've provided more of this context in the changelog, apologies!reply",
      "Those issues are a surprising read. I would expect issues with TPM on old or niche devices, but not Dell XPS laptops, or a variety of VMs. But I guess I'm not entirely sure how my vms handle TPM state, or if they even can.I'm running nearly all of my personal tailscale instances in containers and VMs. Looking now at the dashboard, it appears this feature really only encrypted things on my primary linux and windows pc, my iphone, and my main linux server's host. None of the VMs+containers i use were able to take advantage of this, nor was my laptop. Although my laptop might be too old.reply",
      "Stuff breaks all the time, you just need a bigger sample size.Overseeing IT admins for corp fleets is part of my gig, and from my experience, we get malfunctioning TPMs on anything consumer - Lenovo, Dell, HP, whatever. I think the incidence is some fraction of a percent, but get a few thousand devices and the chance of eventually experiencing it is high, very high. I can't imagine a vTPM being perfect either, since there isn't a hypervisor out there someone hasn't screwed up a VM on.reply",
      "The issue could be a bug in the host OS not in the VM. I had a Windows update that broke VMs when the guest OS was Windows running in real-time mode. This was the only issue and if I didn't run real-time VMs I would have never known. The only resolution was to reinstall Windows.reply",
      "Just had a system board replaced on a device in my org, Dell laptop.As part of setting up a device in our org we enroll our device in Intune (Microsoft's cloud-based device management tool aka UEM / RMM / MDM / etc). To enroll your device you take a \"hardware hash\" which's basically TPM attestation and some additional spices and upload it to their admin portal.After the system board replacement we got errors that the device is in another orgs tenant. This is not unusual (you open a ticket with MS and they typically fix it for you), and really isn't to blame on Dell per se. Why ewaste equipment you can refurbish?Just adding 5c to the anecdata out there re: TPM as an imperfect solution.reply",
      "When I replaced a motherboard (rest of the hw was OK) Microsoft was of the opinion I had a 'new computer' and would need to buy a new Windows 10 license (of IIRC 150 EUR \u2192 scoundrels). I went to G2A and bought one for 20 EUR. Then it hit me. This occurred before when my previous motherboard/CPU was broken, and back then I actually called Microsoft where they insisted on selling me a new license. I did exactly the same back then.reply",
      "My eyes have opened up to the pitfalls of TPM recently while upgrading CPUs and BIOS/UEFI versions on various hardware in my home.VMs typically do not use TPMs, so it is not surprising that the feature was not being used there. One common exception is VMware, which can provide the host's TPM to the VM for a better Windows 11 experience. One caveat is this doesn't work on most Ryzen systems because they implement a CPU-based fTPM that VMware does not accept.reply",
      "AIUI most hypervisors offer vTPM - it\u2019s disabled by default often, but most solutions have it (including Proxmox / KVM (using swtpm)reply",
      "It is in fact surprising that TPMs can be wiped so easily. It makes them almost useless compared to dedicated solutions like physical FIDO keys or smartcards, and does not bode well for hardware-backed Passkeys that would also be inherently reliant on TPM storage.reply",
      "Not all TPM. I've yet to manage it on my MBP M1 Pro or my Pixel. Of course, M1-M3 have broken secure enclave which cannot be fixed by the user.On AMD with fTPM I get a fat warning if I want to reset the fTPM keys. I think earlier implementations failed here.> and does not bode well for hardware-backed Passkeys that would also be inherently reliant on TPM storage.So you revoke the key and auth in another way (or you use a backup). One passkey is never meant to be the one sole way of auth.I actually like the concept. Consider a situation where you would log into your webmail while in a caf\u00e9 or bus. If the password is tied to your hardware, nobody can watch over your shoulder to use it on theirs.I don't use them much (I've been forced to) because I already use a self-hosted password manager where I never see the password myself. But for the average person, passkeys are better.Now, if you compare with FIDO2, those are supposed to be with you all the time (something you have). So they can be used on multiple platforms, while a TPM is tied to hardware.reply"
    ],
    "link": "https://tailscale.com/changelog",
    "first_paragraph": "Updates to the Tailscale client and service.A new release of the Tailscale container image is available. You can download it from Docker Hub or from our GitHub packages repository.A new release of the Tailscale Kubernetes Operator is available. For guidance on installing and updating, refer to our installation instructions.A new release of the Tailscale tsrecorder is available. You can download it from Docker Hub.A new release of the Tailscale container image is available. You can download it from Docker Hub or from our GitHub packages repository.A new release of the Tailscale Kubernetes Operator is available. For guidance on installing and updating, refer to our installation instructions.A new release of the Tailscale tsrecorder is available. You can download it from Docker Hub.Note: This version contains no changes except for library updates.A new release of the Tailscale container image is available. You can download it from Docker Hub or from our GitHub packages repository.A new re"
  },
  {
    "title": "Don't Let the Grocery Store Scan Your Face: How to Stop Wegmans (adafruit.com)",
    "points": 20,
    "submitter": "ptorrone",
    "submit_time": "2026-01-08T00:48:11 1767833291",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=46535514",
    "comments": [
      "Good write up. Still I gotta say: a N95 mask will do the trick for cheap, with side bonus of also blocking flu & covid!reply",
      "FWIW all of the obfuscation techniques make it easier to track you through the store. Then, unless you use a different card each time you go, or only use cash and never use the wegmans rewards stuff, then you pwn yourself immediately.Better to just avoid altogether, however every possible store is using this (I was pitching this to Target as early as 2016) and govt reps are active supporters of this tech.There aren\u2019t really any alternatives that aren\u2019t \u201cgrow your own food.\u201d Even local retailers can use these systems and are increasingly cloud-SaaSreply",
      "Business Reform on YouTube has some tests and reviews of this kind of gear.https://www.youtube.com/@businessreform/videosreply",
      "I fear people will just get used to it just like other means of mass surveillance then wonder why they're being harassed with petty pretexts based on this data.reply"
    ],
    "link": "https://blog.adafruit.com/2026/01/07/dont-let-the-grocery-store-scan-your-face-a-guide-to-fighting-back-against-biometric-surveillance-at-wegmans/",
    "first_paragraph": ""
  },
  {
    "title": "Eat Real Food (realfood.gov)",
    "points": 527,
    "submitter": "atestu",
    "submit_time": "2026-01-07T17:22:09 1767806529",
    "num_comments": 827,
    "comments_url": "https://news.ycombinator.com/item?id=46529237",
    "comments": [
      "Of note: the US's per capita consumption of meat has increased by more than 100 pounds over the last century[1]. We now consume an immense amount of meat per person in this country. That increase is disproportionately in poultry, but we also consume more beef[2].A demand for the average American to eat more meat would have to explain, as a baseline, why our already positive trend in meat consumption isn't yielding positive outcomes. There are potential explanations (you could argue increased processing offsets the purported benefits, for example), but those are left unstated by the website.[1]: https://www.agweb.com/opinion/drivers-u-s-capita-meat-consum...[2]: https://ers.usda.gov/data-products/chart-gallery/chart-detai...reply",
      "> the US's per capita consumption of meatThat number seemed unreal to me, so I looked it up. I think it represents the total pre-processing weight, not the actual meat meat consumption. From Wikipedia:> As an example of the difference, for 2002, when the FAO figure for US per capita meat consumption was 124.48 kg (274 lb 7 oz), the USDA estimate of US per capita loss-adjusted meat consumption was 62.6 kg (138 lb)Processing, cutting into sellable pieces, drying, and spoilage/loss mean the amount of meat consumed is about half of that number.reply",
      "Interestingly, ~12% of humans in the US are responsible for ~50% of beef consumption.> The US is the biggest consumer of beef in the world, but, according to new research, it\u2019s actually a small percentage of people who are doing most of the eating. A recent study shows that on any given day, just 12% of people in the US account for half of all beef consumed in the US.> Men and people between the ages of 50 and 65 were more likely to be in what the researchers dubbed as \u201cdisproportionate beef eaters\u201d, defined as those who, based on a recommended daily 2,200 calorie-diet, eat more than four ounces \u2013 the rough equivalent of more than one hamburger \u2013 daily. The study analyzed one-day dietary snapshots from over 10,000 US adults over a four-year period. White people were among those more likely to eat more beef, compared with other racial and ethnic groups like Black and Asian Americans. Older adults, college graduates, and those who looked up MyPlate, the US Department of Agriculture\u2019s (USDA) online nutritional educational campaign, were far less likely to consume a disproportionate amount of beef.High steaks society: who are the 12% of people consuming half of all beef in the US? - https://www.theguardian.com/environment/2023/oct/20/beef-usd... - October 20th, 2023Demographic and Socioeconomic Correlates of Disproportionate Beef Consumption among US Adults in an Age of Global Warming - https://www.mdpi.com/2072-6643/15/17/3795 | https://doi.org/10.3390/nu15173795 - August 2023(my observation of this is that we can sunset quite a bit of US beef production and still be fine from a food supply and security perspective, as consumption greatly exceeds healthy consumption limits in the aggregate)reply",
      "> A recent study shows that on any given day, just 12% of people in the US account for half of all beef consumed in the USBy itself, this figure doesn't really mean much. On any given day, less than 1% of people have birthdays, but that doesn't mean there's a small percentage of people who are having most of the birthdaysThe following paragraph is more valid, but the 12% figure still seems dubious.reply",
      "That sounds a lot like the \"you only use 10% of your brain\" saying. Yeah, 10% at any given moment.reply",
      "> By itself, this figure doesn't really mean much. On any given day, less than 1% of people have birthdays, but that doesn't mean there's a small percentage of people who are having most of the birthdaysYeah, it just means that half the beef eaten per day goes to the 12%  having a BBQ, etc, not that only 12% of the population have access to half the beef available each dayreply",
      "> A recent study shows that on any given day, just 12% of people in the US account for half of all beef consumed in the US.This phrasing strongly suggests it\u2019s not the same 12% every day. In which case\u2026 it\u2019s probably not that noteworthy.reply",
      "> defined as those who, based on a recommended daily 2,200 calorie-diet, eat more than four ounces... daily.This sounds like.. not very much. I eat 6-7oz of ground beef with breakfast alone, pretty much daily! Are people really eating less than ~1/2 cup of meat over all their meals combined?reply",
      "> Are people really eating less than ~1/2 cup of meat over all their meals combined?Your mind is going to be blown when you learn about vegetarians!I'm in the US and was raised on a pretty standard diet. As a young adult, I stopped eating beef for environmental reasons. As an older adult (50s) I mostly stopped eating most meat for environmental and ethical reasons. I don't call myself a vegetarian and don't make a fuss when vegetarian options aren't available (eg, eating at a friend's house).That is all to say: I haven't noticed any difference in my health either way, but that isn't why I (95%) stopped eating meat.reply",
      "Sometimes I wonder how is it possible that cattle alone severely outweighs all livestock on the planet, and by a very huge margin (like 10 to 1), then I read about such dietary habits.I eat meat too, but I don't eat it every day so if you average it over time it will likely be around those numbers.reply"
    ],
    "link": "https://realfood.gov",
    "first_paragraph": "Better health begins on your plate\u2014not in your medicine cabinet. The new Dietary Guidelines for Americans defines real food as whole, nutrient-dense, and naturally occurring, placing them back at the center of our diets.For decades we've been misled by guidance that prioritized highly processed food, and are now facing rates of unprecedented chronic disease.For the first time, we're calling out the dangers of highly processed foods and rebuilding a broken system from the ground up with gold-standard science and common sense.We are ending the war on protein. Every meal must prioritize high-quality, nutrient-dense protein from both animal and plant sources, paired with healthy fats from whole foods such as eggs, seafood, meats, full-fat dairy, nuts, seeds, olives, and avocados.Protein target: ~0.54\u20130.73 grams per pound of body weight per dayVegetables and fruits are essential to real food nutrition. Eat a wide variety of whole, colorful, nutrient-dense vegetables and fruits in their orig"
  },
  {
    "title": "Shipmap.org (shipmap.org)",
    "points": 462,
    "submitter": "surprisetalk",
    "submit_time": "2026-01-07T15:03:41 1767798221",
    "num_comments": 77,
    "comments_url": "https://news.ycombinator.com/item?id=46527161",
    "comments": [
      "This is weirdly beautiful, like the maps of undersea internet cables that frequently come up here as well.You can clearly see:1) oil flowing out of the Persian Gulf from the Middle East to China2) ships waiting to get through the Panama and Suez Canals3) why people talk about \u201cshipping lanes\u201d. There are some obvious tracks everyone follows, because it\u2019s the cheapest way from A to B (e.g. cape of good hope to straight of malacca).4) why Singapore got to be such an important global hub.5) why the houthis and the Somali pirates could cause such havoc6) nobody goes in the southern ocean! (Why would they? Unless you\u2019re bringing supplies to Antarctica\u2026) a few ships drop down to go around Cape Horn but that\u2019s it.and so much more. I wish it included more up-to-date data\u2026reply",
      "> 6) nobody goes in the southern ocean!https://en.wikipedia.org/wiki/Roaring_Forties\u201cBelow 40 degrees south, there is no law; below 50 degrees, there is no God.\u201dreply",
      "Although interestingly, as that wikipedia article also points out, people did go down into the 40s quite a bit during the Age of Sail (the famous clipper route), because the strong prevailing winds meant it was the fastest way to get around the world. This comes up quite often in the Aubrey-Maturin novels by Patrick O'Brian.If you're not a sailing ship, you don't benefit from the winds, so those latitudes are pretty empty nowadays.reply",
      "There's currently a 100-ft foiling trimaran in the southern ocean that is racing the clock around the world attempting the around the world record.They've averaged about 34 mph (30 kn) for 22 days now. Crazy stuff.https://sodebo-ultim3.sodebo.com/The red boat on the tracker is the world record track from 2017.reply",
      "> why Singapore got to be such an important global hub.Without the the location, of course Singapore wouldn't have been able to be so important. But the location isn't everything --- Singapore manages to outperform Port Klang and Tanjung Pelepas despite the similar geographic advantages of the Malaysian ports due to much better execution.reply",
      "On 3, Traffic Separation Schemes (TSS) will be making things look tidier (and safer) for many of the more organized flows.In a TSS, you have to drive on the right, and if you're crossing one, your heading (not your track) must be as close to 90 degrees, to minimize your exposure time.\nWhen you're sailing this can be a big pain. The anti-collision rules are altered in a TSS.reply",
      "I posted this as a top-level comment, but for up to date info you might try this:https://www.vesselfinder.com/I have no affiliation with that site, I just enjoy it.reply",
      "My favorite: near the Bering strait you can see the distortion of the map - obviously ships go in straight lines on a sphere but in a curve on the map.reply",
      "the weather & tides are terribad down southreply",
      "Side note you can watch timelapse videos taken of ships crossing the oceans, the night stars look great https://www.youtube.com/watch?v=AHrCI9eSJGQreply"
    ],
    "link": "https://www.shipmap.org/",
    "first_paragraph": "Map: Kiln\u00a0 Research: UCL EIData: exactEarth & ClarksonsDue to popular demand the designers of this map, Kiln, are now selling stunning high-resolution versions of the world \u201croutes\u201d view. There are two versions available: coloured by ship type over the inky-blue base map; or just the ship in a single colour a transparent background so you can overlay or print onto whatever background colour you like. Contact [email\u00a0protected] for pricing and further information.Yes. You are welcome to embed this map. Please include a link back to Kiln somewhere in the text of your article. Use the following embed code for a fully responsive embed that will adjust to the width of your website. Feel free to change the height and/or give it a fixed width if you prefer.You can see movements of the global merchant fleet over the course of 2012, overlaid on a bathymetric map. You can also see a few statistics such as a counter for emitted CO2 (in thousand tonnes) and maximum freight carried by represented ve"
  },
  {
    "title": "Notion AI: Unpatched data exfiltration (promptarmor.com)",
    "points": 108,
    "submitter": "takira",
    "submit_time": "2026-01-07T19:49:54 1767815394",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=46531565",
    "comments": [
      "Securing LLMs is just structurally different. The attack space is \"the entirety of the human written language\" which is effectively infinite. Wrapping your head around this is something we're only now starting to appreciate.In general, treating LLM outputs (no matter where) as untrusted, and ensuring classic cybersecurity guardrails (sandboxing, data permissioning, logging) is the current SOTA on mitigation. It'll be interesting to see how approaches evolve as we figure out more.reply",
      "It's pretty simple, don't give llms access to anything that you can't afford to expose. You treat the llm as if it was the user.reply",
      "I get that but just not entirely obvious how you do that for the Notion AI.reply",
      "This is @simonw\u2019s Lethal Trifecta [1] again - access to private data and untrusted input are arguably the purpose of enterprise agents, so any external communication is unsafe. Markdown images are just the ones people usually forget about[1] https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/reply",
      "People have learnt a little while back that you need to use the white hidden text in a resume to make the AI recommend you, There are also resume collecting services which let you buy a set of resumes belonging to your general competition era and you can compare your ai results with them. Its an arms race to get called up for a job interview at the moment.reply",
      "I wouldn't be surprised if people tried to document what LLMs different companies/vendors are using, in order to take advantage of model-biases.https://nyudatascience.medium.com/language-models-often-favo...reply",
      "Wow what a coincidence. I just migrated from notion to obsidian today. Looks like I timed it perfectly (or maybe slightly too late?)reply",
      "How was the migration process?I work on a plugin that makes Obsidian real-time collaborative (relay.md), so if the migration is smooth I wonder how close we are to Obsidian being a suitable Notion replacement for small teams.reply",
      "IMHO the problem really comes from the browser accessing the URL without explicit user permission.Bring back desktop software.reply",
      "Sloppy coding to know a link could be a problem and render it anyway. But even worse to ignore the person who tells you you did that.reply"
    ],
    "link": "https://www.promptarmor.com/resources/notion-ai-unpatched-data-exfiltration",
    "first_paragraph": "SolutionsIndustriesPartnersResourcesBook a DemoThreat IntelligenceNotion AI: Unpatched Data ExfiltrationHuggingFace Chat Exfiltrates DataScreen takeover attack in AI tool acquired for $1BGoogle Antigravity Exfiltrates DataHN #1CellShock: Claude AI is Excel-lent at Stealing DataHijacking Claude Code via Injected Marketplace PluginsData Exfiltration from Slack AI via Indirect Prompt InjectionHN #1Data Exfiltration from Writer.com via Indirect Prompt InjectionHN #5Case Study in OWASP for LLM Top 10Case study in MITRE AtlasThreat IntelligenceTable of ContentTable of ContentTable of ContentNotion AI is susceptible to data exfiltration via indirect prompt injection due to a vulnerability in which AI document edits are saved before user approval.Notion AI allows users to interact with their documents using natural language\u2026 but what happens when AI edits are made prior to user approval?In this article, we document a vulnerability that leads Notion AI to exfiltrate user data (a sensitive hirin"
  },
  {
    "title": "Solo ASIC tapeout on a budget: detailed write up (reddit.com)",
    "points": 17,
    "submitter": "random_duck",
    "submit_time": "2026-01-05T14:56:29 1767624989",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=46499469",
    "comments": [
      "Might as well just link directly to the blog post: https://essenceia.github.io/projects/blake2s_hashing_acceler...reply",
      "Good suggestion, fell free to post it since you have more karma.reply",
      "Very cool but I stopped reading when I realized that the blog post was written by an LLM.> These weren\u2019t just inconveniences; they fundamentally shaped the architecture, capping performance more than any internal logic constraints.This sentence sealed the deal for me but I was already suspicious for the preceeding sections.reply"
    ],
    "link": "https://old.reddit.com/r/chipdesign/comments/1q4kvxt/solo_asic_tapeout_on_a_budget_detailed_write_up/",
    "first_paragraph": ""
  },
  {
    "title": "The Q, K, V Matrices (arpitbhayani.me)",
    "points": 31,
    "submitter": "yashsngh",
    "submit_time": "2026-01-07T08:18:09 1767773889",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=46523887",
    "comments": [
      "I will beat loudly on the \"Attention is a reinvention of Kernel Smoothing\" drum until it is common knowledge. It looks like Cosma Schalizi's fantastic website is down for now, so here's a archive link to his essential reading on this topic [0].If you're interested in machine learning at all and not very strong regarding kernel methods I highly recommending taking a deep dive. Such a huge amount of ML can be framed through the lens of kernel methods (and things like Gaussian Processes will become much easier to understand).0. https://web.archive.org/web/20250820184917/http://bactra.org...reply",
      "This is really useful, thanks. In my other (top-level) comment, I mentioned some vague dissatisfactions around how in explanations of attention the Q, K, V matrices always seem to be pulled out of a hat after being motivated in a hand-wavy metaphorical way. The kernel methods treatment looks much more mathematically general and clean - although for that reason maybe less approachable without a math background. But as a recovering applied mathematician ultimately I much prefer a \"here is a general form, now let's make some clear assumptions to make it specific\" to a \"here's some random matrices you have to combine in a particular way by murky analogy to human attention and databases.\"I'll make a note to read up on kernels some more. Do you have any other reading recommendations for doing that?reply",
      "(How) do you find that framing enlightening?reply",
      "Hey, can I contact you somehow?reply",
      "This is ok (could use some diagrams!), but I don't think anyone coming to this for the first time will be able to use it to really teach themselves the LLM attention mechanism. It's a hard topic and requires two or three book chapters at least if you really want to start grokking it!For anyone serious about coming to grips with this stuff, I would strongly recommend Sebastian Raschka's excellent book Build a Large Language Model (From Scratch), which I just finished reading. It's approachable and also detailed.As an aside, does anyone else find the whole \"database lookup\" motivation for QKV kind of confusing? (in the article, \"Query (Q): What am I looking for? Key (K): What do I contain? Value (V): What information do I actually hold?\"). I've never really got it and I just switched to thinking of QKV as a way to construct a fairly general series of linear algebra transformations on the input of a sequence of token embedding vectors x that is quadratic in x and ensures that every token can relate to every other token in the NxN attention matrix. After all, the actual contents and \"meaning\" of QKV are very opaque: the weights that are used to construct them are learned during training. Furthermore, there is a lot of symmetry between Q and K in the algebra, which gets broken only by the causal mask. Or do people find this motivation useful and meaningful in some deeper way? What am I missing?[edit: on this last question, the article on \"Attention is just Kernel Smoothing\" that roadside_picnic posted below looks really interesting in terms of giving a clean generalized mathematical approach to this, and also affirms that I'm not completely off the mark by being a bit suspicious about the whole hand-wavy \"database lookup\" Queries/Keys/Values interpretation]reply",
      "The way I think about QKV projections: Q defines sensitivity of token i features when computing similarity of this token to all other tokens. K defines visibility of token j features when it\u2019s selected by all other tokens. V defines what features are important when doing weighted sum of all tokens.reply",
      "IIRC isn't the symmetry between Q and K also broken by the direction of the softmax? I mean, row vs column-wise application yields different interpretation.reply",
      "Oh yes! That's probably more important, in fact.reply",
      "Does that book require some sort of technical prerequisite to understand?reply",
      "It helps if you have some basic linear algebra, for sure - matrices, vectors, etc. That's probably the most important thing. You don't need to know pytorch, which is introduced in the book as needed and in an appendix. If you want to really understand the chapters on pre-training and fine-tuning you'll need to know a bit of machine learning (like a basic grasp of loss functions and gradient descent and backpropagation - it's sort of explained in the book but I don't think I'd have understood it much without having trained basic neural networks before), but that is not required so much for the earlier chapters on the architecture, e.g. how the attention mechanism works with Q, K, V as discussed in this article.The best part about it is seeing the code built up for the GPT-2 architecture in basic pytorch, and then loading in the real GPT-2 weights and they actually work! So it's great for learning but also quite realistic. It's LLM architecture from a few years ago (to keep it approachable), but Sebastian has some great more advanced material on modern LLM architectures (which aren't that different) on his website and in the github repo: e.g. he has a whole article on implementing the Qwen3 architecture from scratch.reply"
    ],
    "link": "https://arpitbhayani.me/blogs/qkv-matrices/",
    "first_paragraph": " Arpit Bhayani engineering, databases, and systems. always building.At the core of the attention mechanism in LLMs are three matrices: Query, Key, and Value. These matrices are how transformers actually pay attention to different parts of the input. In this write-up, we will go through the construction of these matrices from the ground up.When we read a sentence like \u201cThe cat sat on the mat because it was comfortable,\u201d our brain automatically knows that \u201cit\u201d refers to \u201cthe mat\u201d and not \u201cthe cat.\u201d This is attention in action. Our brain is selectively focusing on relevant words to understand the context.In neural networks, we need a similar mechanism. Traditional recurrent neural networks processed sequences one token at a time, maintaining hidden states that carry information forward from the previous steps. RNN process looks something like thisThe transformer architecture introduced in 2017 flipped this approach by replacing recurrence with attention. The attention mechanism solved thi"
  },
  {
    "title": "NPM to implement staged publishing after turbulent shift off classic tokens (socket.dev)",
    "points": 128,
    "submitter": "feross",
    "submit_time": "2026-01-07T18:31:19 1767810679",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=46530448",
    "comments": [
      "> In its current form, however, trusted publishing applies to a limited set of use cases. Support is restricted to a small number of CI providers, it cannot be used for the first publish of a new package, and it does not yet offer enforcement mechanisms such as mandatory 2FA at publish time. Those constraints have led maintainer groups to caution against treating trusted publishing as a universal upgrade, particularly for high-impact or critical packages.This isn't strictly accurate: when we designed Trusted Publishing for PyPI, we designed it to be generic across OIDC IdPs (typically CI providers), and explicitly included an accommodation for creating new projects via Trusted Publishing (we called it \"pending\" publishers[1]). The latter is something that not all subsequent adopters of the Trusted Publishing technique have adopted, which is IMO both unfortunate and understandable (since it's a complication over the data model/assumptions around package existence).I think a lot of the pains here are self-inflicted on GitHub's part here: deciding to remove normal API credentials entirely strikes me as extremely aggressive, and is completely unrelated to implementing Trusted Publishing. Combining the two together in the same campaign has made things unnecessarily confusing for users and integrators, it seems.[1]: https://docs.pypi.org/trusted-publishers/creating-a-project-...reply",
      "As I'm not familiar with the npm ecosystem so maybe I'm misunderstanding this but it sounds like they removed support for local publishes (via a token) in favor of CI publishing using Trusted Publishing.If that is correct, I thought this was discussed when Trusted Publishing was proposed for Rust that it was not meant to replace local publishing, only harden CI publishing.reply",
      "> If that is correct, I thought this was discussed when Trusted Publishing was proposed for Rust that it was not meant to replace local publishing, only harden CI publishing.Yes, that's right, and that's how it was implemented for both Rust and Python. NPM seems to have decided to do their own thing here.(More precisely, I think NPM still allows local publishing with an API token, they just won't grant long-lived ones anymore.)reply",
      "I think the path to dependency on closed publishers was opened wide with the introduction of both attestations and trusted publishing.  People now have assigned extra qualities to such releases and it pushes the ecosystem towards more dependency on closed CI systems such as github and gitlab.It was a good intention, but the ramifications of it I don't think are great.reply",
      "> People now have assigned extra qualities to such releases and it pushes the ecosystem towards more dependency on closed CI systems such as github and gitlab.I think this is unfortunately true, but it's also a tale as old as time. I think PyPI did a good job of documenting why you shouldn't treat attestations as evidence of security modulo independent trust in an identity[1], but the temptation to verify a signature and call it a day is great for a lot of people.Still, I don't know what a better solution is -- I think there's general agreement that packaging ecosystems should have some cryptographically sound way for responsible parties to correlate identities to their packages, and that previous techniques don't have a great track record.(Something that's noteworthy is that PyPI's implementation of attestations uses CI/CD identities because it's easy, but that's not a fundamental limitation: it could also allow email identities with a bit more work. I'd love to see more experimentation in that direction, given that it lifts the dependency on CI/CD platforms.)[1]: https://docs.pypi.org/attestations/security-model/reply",
      "> It was a good intention, but the ramifications of it I don't think are great.as always, the road to hell is paved with good intentionsthe term \"Trusted Publishing\" implies everyone else is untrustedquite why anyone would think Microsoft is considered trustworthy, or competent at operating critical systems, I don't knowhttps://firewalltimes.com/microsoft-data-breach-timeline/reply",
      "> the term \"Trusted Publishing\" implies everyone else is untrustedNo, it just means that you're explicitly trusting a specific party to publish for you. This is exactly the same as you'd normally do implicitly by handing a CI/CD system a long-lived API token, except without the long-lived API token.(The technique also has nothing to do with Microsoft, and everything to do with the fact that GitHub Actions is the de facto majority user demographic that needs targeting whenever doing anything for large OSS ecosystems. If GitHub Actions was owned by McDonalds instead, nothing would be any different.)reply",
      "I maintain some very highly used npm package and this situation just has me on edge. In our last release of dozens of packages, I was manually reading though our package-lock and package.json changes and reviewing every dependency change. Luckily our core libraries have no external dependencies, but our tooling has a ton.We were left with a tough choice of moving to Trusted Publishers or allowing a few team members to publish locally with 2FA. We decided on Trusted Publishers because we've had an automated process with review steps for years, but we understand there's still a chance of a hack, so we're just extremely cautious with any PRs right now. Turning on Trusted Publishers was a huge pain with so many package.The real thing we want for publishing is for us is to be able to continue to use our CI-based publishing setup, with Trusted Publishers, but with a human-in-the-loop 2FA step.But that's only part of a complete solution. HITL is only guaranteed to slow down malicious code propagating. It doesn't actually protect our project against compromised dependencies, and doesn't really help prevent us from spreading it. All of that is still a manual responsibility of the humans. We need tools to lock down and analyze our dependencies better, and tools to analyze our our packages before publishing. I also want better tools for analyzing and sandboxing 3rd party PRs before running CI. Right now we have HITL there, but we have to manually investigate each PR before running tests.reply",
      "The shift wouldn't have been so turbulent if npm had simply updated their CLI in tandem. I still can't use 2FA to publish because their CLI simply cannot handle it.reply",
      "Seems like requiring 2FA to publish or trusted publishing should prevent the vast majority of this issue.The only tricky bit would be to disallow approval own pull request when using trusted publishing. That should fall back to requiring 2FAreply"
    ],
    "link": "https://socket.dev/blog/npm-to-implement-staged-publishing",
    "first_paragraph": ""
  },
  {
    "title": "Play Aardwolf MUD (aardwolf.com)",
    "points": 34,
    "submitter": "caminanteblanco",
    "submit_time": "2026-01-07T23:31:05 1767828665",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=46534777",
    "comments": [
      "Epic training for reading output tokens.reply",
      "I played Aardwolf briefly but I mostly stuck to a nightmare/LP MUD that is still around but essentially empty. I learned to code (which ended up being a gateway to C/C++), grief, and be compassionate to new players.reply",
      "Just wanted to remind everyone of the wonderful world of MUDsreply",
      "Funnily enough I took up trying to develop a new MUD engine from scratch in Haskell, with an embedded Lua interpreter for scripting as well.https://github.com/agentultra/bakamudAnd I often stream working on it at https://twitch.tv/agentultraMUDs are great! Achea is another great one.Happy gaming folks.reply",
      "I love how concise that Haskell code is! I've also started building a new MUD engine, but in Rust (previously I've written a partially complete one in Go), and this time around I'm working on implementing a MUD using an ECS (entity component system).reply",
      "I'm also planning on an ECS system as well! Very cool. Are you publishing the code somewhere? There's also a Slack for MUD developers if you're interested in chilling with like-minded people: https://mudcoders.com/join-the-mud-coders-guild-6770301ddcbd...",
      "absolutely agreed.warning to those just trying them out now though : a lot of MUDs have gone the unfortunate route of having premium shops and other MMO pitfalls.I don't think Aardwolf does -- not positive. I think Achaea and some of the other big ones do though.I recommend Discworld. https://discworld.starturtle.net/lpc/reply",
      "I had a crappy computer with crappy internet back in the day. I didn't play Aardwolf, but I played another mud called Alter Aeon. I still have fond memory of it and check back in every few years.reply",
      "I grew up on Aardwolf. It basically taught me to touch type. Thank you for reminding me of its existence <3reply",
      "same here! I eventually quit because I can't control myself enough to not grind for 8 hours a day instead of working but this game got me interested in writing code (to make text based games)reply"
    ],
    "link": "https://www.aardwolf.com/",
    "first_paragraph": "\n\n\n\n\nLocation: HomePlay Aardwolf NowAardwolf RPG is a unique and free text based roleplaying game. Aardwolf is based in the fantasy world of Andolor where magic is common and there are hundreds of exotic realms to explore, puzzles to solve and quests to\r\ncomplete. Aardwolf features a realistic game world with multiple continents and real geography. Each area includes a real time line-of-sight overhead map to see other characters and points of interest around you.Create your character from any one of 28 classes including fighter classes such as Soldiers, Knights, Hunters, Barbarians, Rangers, Archers, Assassins and Paladins or select a magic based class including Elementalists, Necromancers, Healers, Priests, Witches and Enchanters. Once you have choosen your race, your guild and your profession then the rest is up to you. You have absolute control over your character's actions and there are many ways to play Aardwolf.Sample screenshot of the Aardwolf ClientYou can play the game solo or"
  },
  {
    "title": "US will ban Wall Street investors from buying single-family homes (reuters.com)",
    "points": 587,
    "submitter": "kpw94",
    "submit_time": "2026-01-07T19:13:19 1767813199",
    "num_comments": 628,
    "comments_url": "https://news.ycombinator.com/item?id=46531068",
    "comments": [
      "The key word here is \"Wall Street\". And this statement is playing off a popular misconception around corporate investors buying up American houses.There has been a bit of a panic around \"Investors buying up all the property!!!\" With people often citing Black Rock and Blackstone as the main culprits. But most of the \"investors\" buying up property are individuals purchasing investment properties.Here's an article on the topic from 2023[0], a bit old but my understanding is large institutional investment in residential real estate was already starting to cool down.Black rock isn't buying up all the housing, your neighbors are.I suspect this statement, and even if it becomes an actual ban, is largely to gain wider popular support around a largely imaginary concern people have.0. https://www.housingwire.com/articles/no-wall-street-investor...reply",
      "It's not that simple - the problem is that those institutions are market makers. They are a tiny portion of the market, but a huge driving force in setting and manipulating prices, because their properties get leveraged, instrumentalized, and securitized, with derivative products, speculation, and all sorts of incentives that you don't normally want operating in the arena of housing.The things that they do have massively outsized downstream impact contrasted against their relatively tiny overall participation in the market, and they can afford to behave in ways that manipulate the behavior of the majority.If you can decouple them from the housing markets, you also decouple the interests of the donor class, and you allow for policy that doesn't maximize the cost of real estate over the interests of the majority of the population.reply",
      "> They are a tiny portion of the market, but a huge driving force in setting and manipulating prices, because their properties get leveraged, instrumentalized, and securitized, with derivative products, speculation, and all sorts of incentives that you don't normally want operating in the arena of housing.Raising prices when you only have a tiny portion of the market does not work. People won't buy them when there's another house for less.reply",
      "It's not just raising prices - it's holding prices steady at some point without the concurrent pressure to sell, for example, or manipulating other markets in order to raise or lower prices in an area, or using other mechanics to manipulate pricing, across the entire market, depending on the intended actions. If they intend to purchase properties, it benefits them to depress pricing in the area, if they intend to rent, they can afford to impose artificial scarcity until they force renters to meet their rates, and so on.Normal landlords don't have effectively infinite money with no forces bearing prices down, nor do they have the capabilities to influence markets. Even tiny percentage shifts can result in significant fluctuations in the prices consumers see. It's a very nuanced and complex system in which these institutional investors have very outsized influence.reply",
      "You're telling a just-so story, and you can tell because there isn't a simple schematic 1-2-3 story you can make from this about how these people exert control over home prices. Words mean things; wielding scarcity requires you to control enough inventory to manipulate scarcity, and REITs and corporate buyers empirically don't.I get why people like telling stories like this: it suggests there's a single boogeyman that can be dispelled to solve the affordability problem without painstakingly goring people's oxes state-by-state and municipality-by-municipality. But it's a fantasy.If you can tell this story in simple step-by-step form, you will. I think you could tell a story about how a large corporate buyer clears out all the marginal buyers for some thin market like an individual subdivision or tranche of new construction housing in the Sun Belt. But I don't think you can tell a realistic story for them being \"a huge driving force in setting and manipulating prices\" across the whole market. I look forward to seeing your attempt, though.reply",
      "About 2 miles from my house, a housing development recently went up.No homes for sale. Rent only.Stuff like that is becoming a big problem.reply",
      "Why? Why should I care what the balance of rental and owner-occupied is? Owned is not strictly better than rented!",
      "Ownership share is a stock. Prices get set by flow - transactions. Housing is a thin market; maybe 5-6% of homes change hands in a given year. Price discovery happens at that transaction layer.Institutional investors own ~3% of single-family rentals nationally. But per CoreLogic they're 29% of purchases in the starter home tier. That's the market where we first-time buyers actually compete.In some metros it's more concentrated.Atlanta: ~30% of single-family rentals corporate-owned.Charlotte neighborhoods in 2022: 50%!!! of sales to institutional buyers.So for your 1-2-3... maybe something like?1. Institutional buyers concentrate in starter homes where they're 29% of transactions, not 3% of stock2. Target metros/neighborhoods go higher still3. Real estate uses comps-based pricing - their winning bids propagate to surrounding valuationsThe mechanism isn't inventory control, it's just a buyer with a different utility function (rental yield vs owner-occupancy) systematically outbidding price-sensitive first-time buyers. In a thick market that gets arbitraged away. In a thin market with sparse comps, each transaction is a price-setting event.The St. Louis Fed found institutional presence specifically increases price-to-income ratios in the bottom tier.If you're evil corporate Landlordman You don't need to affect the whole market. You just need to cut off the bottom rung of the ladder.Is this Trump move the right one? No frickin idea! But I do think we need to reckon with what's actually happening to first-time homebuyers. I bought a place in Englewood Co last year and ... it was pretty rough.reply",
      "Whoah, hold up, your (3) is doing a lot more work than you think it is. Comps matter but they don't literally break the market:* They impact listing prices but not necessarily clearing prices.* They assume all the sellers, who are not corporate investors, can mechanically anchor off those inflated comps, without factoring in buyer budgets and carrying costs.Real estate is slower than most financial products, but it's still an actual market. You can't just buy a tiny fraction of the inventory at an inflated price and assume the whole rest of the market will follow you.reply",
      "Reread my #3 in the context of \"rental yield vs owner-occupancy.\"I'm not saying comps magically anchor prices. I'm saying institutional buyers ARE the clearing prices, because they are anchored to \"how much can I rent this out for\" whereas first-time homebuyers are anchored to \"how much can my mortgage cover?\" which are different questions.29% of transactions, not 3% of stock.Those become the comps. There's less of a gap for \"but buyers won't pay that\" because the institutions *are the buyers. The call is coming from inside the housing market.reply"
    ],
    "link": "https://www.reuters.com/world/us/us-will-ban-large-institutional-investors-buying-single-family-homes-trump-says-2026-01-07/",
    "first_paragraph": ""
  },
  {
    "title": "Health care data breach affects over 600k patients, Illinois agency says (nprillinois.org)",
    "points": 158,
    "submitter": "toomuchtodo",
    "submit_time": "2026-01-07T16:28:14 1767803294",
    "num_comments": 53,
    "comments_url": "https://news.ycombinator.com/item?id=46528353",
    "comments": [
      "Unfortunately there's no money in privacy, and a lot of money in either outright selling data or cutting costs to the bare minimum required to avoid legal liability.Wife and I are expecting our third child, and despite my not doing much googling or research into it (we already know a lot from the first two) the algorithms across the board found out somehow.  Even my instagram \"Explore\" tab that I accidentally select every now and then started getting weirdly filled with pictures of pregnant women.It is what it is at this point.  Also I finally got my last settlement check from Equifax, which paid for Chipotle.  Yay!reply",
      "Interestingly in healthcare there is a correlation between companies that license/sell healthcare data to other ones (usually they try to do this in a revokable way with very stringent legal terms, but sometimes they just sell it if there is enough money involved) and their privacy stance... and it's not what you would think.  Often it's these companies that are pushing for more stringent privacy laws and practices. For example, they could claim that they cannot share anonymized data with academic researchers, because of xyz virtuous privacy rules, when they are actually the ones making money off of selling patient data. It's an interesting phenomenon I have observed while working in the industry that seems to refute your claim that \"there's no money in privacy\".  Another way to think about it is that they want to induce a lower overall supply for the commodity they are selling, and they do this by championing privacy rules.reply",
      "As new moms tend to change their consumer purchasing habits they are coveted by advertisers. http://www.nytimes.com/2012/02/19/magazine/shopping-habits.h...  Certain cohorts and keywords are very valuable so even searching a medical condition once or clicking on a hiring ad for an in-demand job can shift ads toward that direction for a long time.reply",
      "Yeah I'm less shocked that it got picked up and more how quickly it spread to literally every platform we use, even those that wouldn't have much if any hint that it was happening.There's clearly quite the active market for this informationreply",
      "Also on the front page of HN right now is a job posting for Optery (YC W22). Seems like they are growing really fast.reply",
      "Could be as simple as buying a bunch of scent free soap / lotion and some specific vitamin supplements. Walmart / Target were able to detect pregnancy reliably back in 2012 from just their own shopping data.reply",
      "Regular purchase of prenatal vitamins is probably a huge marker for either being pregnant or intention to become pregnant.reply",
      "Just shopping in the store and lingering by those products for a few moments is enough for the algorithm to detect a possible pregnancy. They use Bluetooth beacons & camera software to see how long you look at everything in the store.reply",
      "Also possible they have your location if you went to the hospital. Maybe from any Meta \"partners\" or third party brokers.reply",
      "FYI, there's a .gov-maintained portal where healthcare companies in the U.S. are legally obliged to publish data breaches. It's an interesting dataset!https://ocrportal.hhs.gov/ocr/breach/breach_report.jsfreply"
    ],
    "link": "https://www.nprillinois.org/illinois/2026-01-06/health-care-data-breach-affects-600-000-patients-illinois-agency-says",
    "first_paragraph": "The names and addresses of thousands of patients of the Illinois Department of Human Services were incorrectly made publicly viewable for the last several years, the agency said Friday.Several maps created to assist the agency with decisions \u2014 like where to open new offices and allocate certain resources \u2014 were made public through incorrect privacy settings between 2021 and 2025, the Department of Human Services said in a statement.More than 32,000 customers with the IDHS division of rehabilitation services had information publicly viewable between April 2021 and September 2025. The information included names, addresses, case numbers, case status, referral source information, region and office information and status as Division of Rehabilitation Services recipients, the agency said.Around 670,000 Medicaid and Medicare Savings Program recipients had their addresses, case numbers, demographic information and the name of medical assistance plans publicly viewable between January 2022 and "
  },
  {
    "title": "LaTeX Coffee Stains (2021) [pdf] (illinois.edu)",
    "points": 278,
    "submitter": "zahrevsky",
    "submit_time": "2026-01-07T14:46:31 1767797191",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=46526933",
    "comments": [
      "Now I want a package to add blood stains on my murder mystery screenplay.reply",
      "I'm surprised nobody has yet mentioned how pleasant it is to create coffee stains using Typst, and if only LaTeX wasn't the de-facto standard in academia and stain-related journals, they would have already switched to it.Of course, you can create coffee stains in HTML as well, but it's not something you can do in Markdown.reply",
      "I've been rewriting all my papers in Rust. It's been a pleasant experience getting memory safe coffee stains on my papers.reply",
      "Does coffee accelerate rusting?reply",
      "Depends on the acidity.reply",
      "Thankfully there is a Typst port of this package!https://typst.app/universe/package/fleck/reply",
      "That package still has the core limitation of Typst: images can only be placed top-middle-bottom and left-centre-right. Typst still has yet to support arbitrarily placed images.reply",
      "You mean absolutely positioning it? You can do that with the place function and displacing it with dx/dy from the origin (https://typst.app/docs/reference/layout/place). Example: #place(top + left, dy: 2cm, dx: 4cm, image(\"image.png\"))reply",
      "That seems usable for manual layout, but it looks painful to use to place images without knowing exactly where they might end up on a page. I reuse my LaTeX code to make volumes of books, and I never touch the code. It's fire and forget for me, which this does not seem to solve.reply",
      "> but it looks painful to use to place images without knowing exactly where they might end up on a page.they end up exactly at the specified location?reply"
    ],
    "link": "https://ctan.math.illinois.edu/graphics/pgf/contrib/coffeestains/coffeestains-en.pdf",
    "first_paragraph": ""
  },
  {
    "title": "2026 Predictions Scorecard (rodneybrooks.com)",
    "points": 32,
    "submitter": "calvinfo",
    "submit_time": "2026-01-07T21:40:41 1767822041",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=46533343",
    "comments": [
      "Of the predictions I read, I found that the author engages in pretty heavy handed rules lawyering in order to make their predictions accurate.For example, the author takes the stance that current self driving cars (Waymo, Zoox) do not count as self driving. The justification being that a human operator is involved some small fraction of the time.By law, Waymo must report disengagements in California. In 2024, Waymo had ~10 thousand miles driven per disengagement, Zoox had ~28 thousand miles driven per disengagement [1]. I would say that this rate of human intervention qualifies as self driving.[1] https://thelastdriverlicenseholder.com/2025/02/03/2024-disen...reply",
      "On the contrary, it\u2019s the companies doing the lawyering. A disengagement is when the vehicle reverts fully back to manual control. Tele-operation does not count as a disengagement, and the frequency of tele-operation intervention is a closely guarded industry secret.reply",
      "You seem to have some deeper insight into this - in your estimation, how often does tele-operation (even a small correction) take place?reply",
      "There\u2019s been reporting on this in several mainstream publications that was accurate as far as the systems I worked with. Unfortunately I don\u2019t want to dox myself on here, so unsatisfyingly the best I can offer is \u201ctrust me bro\u201d.reply",
      "The tele-operation is also kinda vague because as I understand it, with Waymo at least, they are not turning a steering wheel and pushing pedals at HQ, they are saying \"Pull over here\" etc.reply",
      "Predicting well absent lawyering is really hard! If someone else wants to try I warmly recommend starting with e.g. the ACX 2026 prediction contest: https://www.metaculus.com/tournament/ACX2026/reply",
      "> Predicting well absent lawyering is really hard!The author engages in rules lawyering of the evaluation of the predictions. The original predictions are clear.Another example of this is the author's prediction that no robot will be able to navigate around the clutter in a US home, \"What is easy for humans is still very, very hard for robots.\"The author evaluated this prediction as not being met, \"...I don't count as home robots small four legged robots that flail their legs quickly to beat gravity, and are therefore unsafe to be around children, and that can't do anything at all with their form factor besides scramble\".The author added constraints not in the original prediction (safe around children, must include a form factor able to preform an action, ...) then evaluated the prediction as accurate because no home robot met the original constraint + the new constraints.reply",
      "Related. Others?Predictions Scorecard, 2025 January 01 - https://news.ycombinator.com/item?id=42651275 - Jan 2025 (185 comments)Rodney Brooks Predictions Scorecard - https://news.ycombinator.com/item?id=34477124 - Jan 2023 (41 comments)Predictions Scorecard, 2021 January 01 - https://news.ycombinator.com/item?id=25706436 - Jan 2021 (12 comments)Predictions Scorecard - https://news.ycombinator.com/item?id=18889719 - Jan 2019 (4 comments)My Dated Predictions - https://news.ycombinator.com/item?id=16078431 - Jan 2018 (50 comments)reply",
      "\"2. Self Driving Cars. In the US the players that will determine whether self driving cars are successful or abandoned are #1 Waymo (Google) and #2 Zoox (Amazon). No one else matters. The key metric will be human intervention rate as that will determine profitability.\" - I love that he's not mentioning the speculation company of the century. We don't have to mention it either.reply",
      "> I love that he's not mentioning the speculation company of the century. We don't have to mention it either.The word 'Tesla' appears 17 times in the article.reply"
    ],
    "link": "https://rodneybrooks.com/predictions-scorecard-2026-january-01/",
    "first_paragraph": "Robots, AI, and other stuffYour email:\u00a0rodneybrooks.com/predictions-scorecard-2026-january-01/Nothing is ever as good as it first seems and nothing is ever as bad as it first seems.\u2014 A best memory paraphrase of advice given to me by Vice Admiral Joe Dyer, former chief test pilot of the US Navy and former Commander of NAVAIR.[You can follow me on social media: @rodneyabrooks.bsky.social and see my publications etc., at https://people.csail.mit.edu/brooks]This is my eighth annual update on how my dated predictions from January 1st, 2018\u00a0concerning (1) self driving cars, (2) robotics, AI , and machine learning, and (3) human space travel, have held up. I promised then to review them at the start of the year every year until 2050 (right after my 95th birthday), thirty two years in total. The idea was to hold myself accountable for those predictions. How right or wrong was I?The summary is that my predictions held up pretty well, though overall I was a little too optimistic. That is a littl"
  },
  {
    "title": "LMArena is a cancer on AI (surgehq.ai)",
    "points": 171,
    "submitter": "jumploops",
    "submit_time": "2026-01-07T04:40:48 1767760848",
    "num_comments": 72,
    "comments_url": "https://news.ycombinator.com/item?id=46522632",
    "comments": [
      "The general conceit of this article, which is something that many frontier labs seem to be beginning to realize, is that the average human is no longer smart enough to provide sufficient signal to improve AI models.reply",
      "No, it's that the average unpaid human doesn't care to read closely enough to provide signal to improve AI models. Not that they couldn't if they put in even the slightest amount of effort.reply",
      "Why would an unpaid human want to do that?reply",
      "Exactly \u2014 they wouldn't.reply",
      "Popularity has never been a meaningful signal of quality, no matter how many tech companies try to make it so, with their star ratings, up/down voting, and crowdsourcing schemes.reply",
      "Different strokes for different folks:  I mean who is to say if Bleach or Backstabbed in a Backwater Dungeon: My Trusted Companions Tried to Kill Me, but Thanks to the Gift of an Unlimited Gacha I Got LVL 9999 Friends and Am Out for Revenge on My Former Party Members and the World is better?reply",
      "But when you're a moron how can you distinguish?I'm being (mostly) serious, suppose you're a stuffed ahort trying to boost your valuation, how can you work out who's smart enough to train your LLM? (Never mind how to get them to work for you!)reply",
      "I do a lot of human evaluations. Lots of Bayesian / statistical models that can infer rater quality without ground truth labels. The other thing about preference data you have to worry about (which this article gets at) is: preferences of _who_? Human raters are a significantly biased population of people, different ages, genders, religions, cultures, etc all inform preferences. Lots of work being done to leverage and model this.Then for LMArena there is the host of other biases / construct validity: people are easily fooled, even PhD experts; in many cases it\u2019s easier for a model to learn how to persuade than actually learn the right answers.But a lot of dismissive comments as if frontier labs don\u2019t know this, they have some of the best talent in the world. They aren\u2019t perfect but they in a large sene know what they\u2019re doing and what the tradeoffs of various approaches are.Human annotations are an absolute nightmare for quality which is why coding agents are so nice: they\u2019re verifiable and so you can train them in a way closer to e.g. alphago without the ceiling of human performancereply",
      "> in many cases it\u2019s easier for a model to learn how to persuade than actually learn the right answersSo we should expect the models to eventually tend toward the same behaviors that politicians exhibit?reply",
      "Maybe a happy to deceive marketing/sales role would be more accurate.reply"
    ],
    "link": "https://surgehq.ai/blog/lmarena-is-a-plague-on-ai",
    "first_paragraph": "Would you trust a medical system measured by: which doctor would the average Internet user vote for?No?Yet that malpractice is LMArena.The AI community treats this popular online leaderboard as gospel. Researchers cite it. Companies optimize for it and set it as their North Star. But beneath the sheen of legitimacy lies a broken system that rewards superficiality over accuracy.It's like going to the grocery store and buying tabloids, pretending they're scientific journals.Here's how LMArena is supposed to work: enter a prompt, evaluate two responses, and mark the best. What actually happens: random Internet users spend two seconds skimming, then click their favorite.They're not reading carefully. They're not fact-checking, or even trying.This creates a perverse reward structure. The easiest way to climb the leaderboard isn't to be smarter; it\u2019s to hack human attention span. We\u2019ve seen over and over again in the data, both from datasets that LMArena has released and the performance of m"
  },
  {
    "title": "We found cryptography bugs in the elliptic library using Wycheproof (trailofbits.com)",
    "points": 53,
    "submitter": "crescit_eundo",
    "submit_time": "2026-01-01T14:52:27 1767279147",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=46454577",
    "comments": [
      "FYI: two vulnerabilities in elliptic, a widely used JavaScript library for elliptic curve cryptographyreply",
      "The maintainer seems to have abandoned it: https://github.com/indutny/elliptic/issuesI wrote a shim library and posted it on their issue tracker: https://github.com/indutny/elliptic/issues/343Unfortunately, adoption seems slow. I'm talking with a few people about how to move the ecosystem to something more secure like noble-curves, but it's tricky.reply",
      "If you really feel like helping the ecosystem update, you could file issues/PRs for all of the downstream NPM modules to switch to your shim library.Remember to tell them what the problem is and how your library solves it.reply",
      "If you click \"Show more\" you'll see this: https://imgur.com/a/KLI8cjLreply"
    ],
    "link": "https://blog.trailofbits.com/2025/11/18/we-found-cryptography-bugs-in-the-elliptic-library-using-wycheproof/",
    "first_paragraph": "Trail of Bits is publicly disclosing two vulnerabilities in elliptic, a widely used JavaScript library for elliptic curve cryptography that is downloaded over 10 million times weekly and is used by close to 3,000 projects. These vulnerabilities, caused by missing modular reductions and a missing length check, could allow attackers to forge signatures or prevent valid signatures from being verified, respectively.One vulnerability is still not fixed after a 90-day disclosure window that ended in October 2024. It remains unaddressed as of this publication.I discovered these vulnerabilities using Wycheproof, a collection of test vectors designed to test various cryptographic algorithms against known vulnerabilities. If you\u2019d like to learn more about how to use Wycheproof, check out this guide I published.In this blog post, I\u2019ll describe how I used Wycheproof to test the elliptic library, how the vulnerabilities I discovered work, and how they can enable signature forgery or prevent signatu"
  },
  {
    "title": "Native Amiga Filesystems on macOS / Linux / Windows with FUSE (github.com/reinauer)",
    "points": 72,
    "submitter": "doener",
    "submit_time": "2026-01-03T07:46:02 1767426362",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=46473726",
    "comments": [
      "In a world obsessed with AI and distributed everything, simple problems like \"mount this USB drive on every OS without headaches\" still feel unsolved. That\u2019s both humbling and oddly comforting.reply",
      "It's solved, though. The only reason it feels unsolved is because Apple won't document APFS to save their life.reply",
      "macOS Sequoia refused to mount a FAT32-formatted microSD card from a security camera because the partitioning wasn't compliant or something.I installed Windows 11 in a VM, and it mounted just fine in there.reply",
      "Precisely to my point. On OSes that don't arbitrarily filter which filesystem you can use, it is a solved problem.It only feels like an unsolved problem because some OEMs benefit from the fragmentation.reply",
      "AI replyreply",
      "ai commentreply",
      "Very tangential, but Amiga has been a recent fascination of mine. I've been playing with a few AROS distributions in QEMU; a part of me wants to take the plunge and run it full time as my primary operating system, but the lack of a decent web browser is a pretty hard limiter in 2026.  I guess my brain has sort of fetishized the platform; I didn't grow up with an Amiga, but I did get to play with one when I was a kid and I always thought it seemed cool, and as a grown-up who roughly understands how operating systems work I do think it was somewhat ahead of its time. Yes, I realize that there was a lot wrong with the design as well (e.g. no protected memory and programs being able to modify each others pointers), but even still I think it was pretty neat.It's fun to think of the alternate universe where Commodore had been competently managed, and we wouldn't have the codified mediocrity of POSIX driving everything today.It would be hard for me to justify the time sink, but I would like to port over the most recent Firefox/IceWeasel and bring Amiga into the 21st century.reply",
      "Pretty cool. This isn't what I first thought of as native Amiga filesystem support - this is support for the native Amiga filesystem drivers, through m68k emulation.reply",
      "Meanwhile, also in 2026, we still don't have a filesystem that works on Mac OS, Windows, and Linux.NTFS is the closest you can get, and it's read-only on Mac OS.reply",
      "We don't have a good filesystem that works without caveats and annoyances on NT+Darwin+Linux. Depending on your pain tolerance, FAT32, exFAT, and ZFS are all reasonable choices.reply"
    ],
    "link": "https://github.com/reinauer/amifuse",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Native AMIGA filesystems on macOS / Linux / Windows with FUSE\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Mount Amiga filesystem images on macOS/Linux/Windows using native AmigaOS filesystem handlers via FUSE.amifuse runs actual Amiga filesystem drivers (like PFS3) through m68k CPU emulation, allowing you to read Amiga hard disk images without relying on reverse-engineered implementations.Install macFUSE from https://osxfuse.github.io/ or via Homebrew:You may need to reboot and allow the kernel extension in System Preferences > Security & Privacy.To download a test PFS3 disk image and the pfs3aio handler:Then mount with:amifuse uses subcommands for different operations:View partition information and embedded filesystem drivers:Inspect RDB (Rigid Disk Blo"
  },
  {
    "title": "Creators of Tailwind laid off 75% of their engineering team (github.com/tailwindlabs)",
    "points": 951,
    "submitter": "kevlened",
    "submit_time": "2026-01-07T16:02:19 1767801739",
    "num_comments": 581,
    "comments_url": "https://news.ycombinator.com/item?id=46527950",
    "comments": [
      "Very sad to hear, I bought Tailwind UI years ago and although it was a lot more expensive than I wanted, I've appreciated the care and precision and highly recommend buying it (It's now called Tailwind Plus) even still (maybe even especially now).Mad props to Adam for his honesty and transparency.  Adam if you're reading, just know that the voices criticizing you are not the only voices out there.  Thanks for all you've done to improve web development and I sincerely hope you can figure out a way to navigate the AI world, and all the best wishes.Btw the Tailwind newsletter/email that goes out is genuinely useful as well, so I recommend signing up for that if you use Tailwind CSS at all.reply",
      "Tailwind did a great job of building a fanbase. Even without LLMs I always thought they were on a collision course with market saturation, though. They generously gave lifetime access for a one-time payment, which was bound to run into problems as free alternatives became better and their core fanbase didn't have any reason to spend more money.Their business model also missed the boat on the rise of Figma and similar tools. I can think back to a couple different projects where the web developers wanted to use Tailwind [Plus] components but the company had a process that started in Figma. It's hard to sell the designers on using someone else's component library when they have to redraw it in Figma anyway.reply",
      "alternatively, Adam executed the superior pricing strategy. had he charged for recurring licenses, would fewer people have signed up? would his subscriptions also be drawing down?i wouldn't have bought a sub, but i did pay for tailwind premium (and, frankly, didn't use it like i'd've hoped). however, it was a bit of a Kickstarter investment for me. i like Adam's persona, and was happy to see continued investment down this path.as many a business knows, you need to bring new initiatives to the table over, or accept that your one product carries all your risk.thank you for Tailwind, Adam.reply",
      "> alternatively, Adam executed the superior pricing strategy.I'm not saying it wasn't a good choice at the time.The problem with lifetime licensing only appears down the road if a company doesn't find a way to expand their offerings.If you opened a local gym with reasonably priced lifetime memberships you'd probably have an explosion of new customers. You'd then hit a wall where you've saturated the market, can't sell any more memberships, but you have to keep paying employees and rent.reply",
      "Adam presented his case for the lifetime pricing model in this podcast episode in 2023:https://hackersincorporated.com/episodes/lifetime-pricing-is...I believe he succeeding in convincing Sam and Ryan to adopt lifetime pricing for their UI course at https://buildui.com/pricing. I've purchased Build UI, and it was an excellent product, but unfortunately it appears to be completely dead for at least a full year now.Neither the unannounced death of Build UI nor this apparently financial catastrophe for Tailwind bode well for the prospects of lifetime pricing! Although the problem might be more related to the entire market segment (frontend programming and design courses) than to the particular pricing model.reply",
      "I'll piggyback on this to highlight Refactoring UI as well. It's an ebook by Adam and Steve, though I'm not sure if it's technically part of Tailwind Labs or not.This book taught me so much about modern UI design. If you've ever tried building a component and thought to yourself, \"hmm something about this looks off,\" you might benefit from this book.These days some of the examples might be a little bit dated (fashions come and go), but the principles it teaches you are rock solid.reply",
      "FWIW I found Practical UI [1] a more actionable book than Refactoring UI. Both are  similar but I found it covered the material in a more accessible way.1. https://www.practical-ui.com/reply",
      "I think think tailwind ui was one of the better purchases I\u2019ve made (web tech wise). Up there with the lifetime acf pro license.This sucks to see but was pretty obvious when it became the go to framework for LLMs.reply",
      "Tailwind Plus is great - I love the lifetime access, but I always wondered how sustainable that model was. Even without AI, how many of those memberships could they sell?reply",
      "I thought the same, and yet on the other hand, how could they have done it differently? People don't want to pay a subscription just to write a DSL of CSS. Perhaps they could've done it per project like some companies, but I don't think it'd be as popular as their lifetime model. Ironic.reply"
    ],
    "link": "https://github.com/tailwindlabs/tailwindcss.com/pull/2388",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page. There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\nHave a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.\n  By clicking \u201cSign up for GitHub\u201d, you agree to our terms of service and\n  privacy statement. We\u2019ll occasionally send you account related emails.\n    Already on GitHub?\n    Sign in\n    to your account\n  Add /llms.txt endpoint that serves a concatenated, text-only version of all Tailwind CSS documentation pages optimized for Large Language Model consumption.:)\n\n\n\n\n\n\n\n\n\n    Sorry, something went wrong.\n   There was an error while loading. Please reload this page.\nThere was an er"
  },
  {
    "title": "A4 Paper Stories (susam.net)",
    "points": 287,
    "submitter": "blenderob",
    "submit_time": "2026-01-07T12:54:43 1767790483",
    "num_comments": 147,
    "comments_url": "https://news.ycombinator.com/item?id=46525888",
    "comments": [
      "Nice! The author touches on the area properties and here's the most practical life hack derived from the standard I personally use. It uses the relationship between size and mass.Because A0 is defined as having an area of exactly 1 square meter, the paper density (GSM or grams per square meter) maps directly to the weight of the sheet.>A0 = 1 meter square.>Standard office paper = 80 gsm>Therefore, one sheet of A0 = 80 grams.>Since A4 is 1/16th of an A0, a single sheet of standard A4 paper weighs 5 grams.I rarely need to use a scale for postage. If I have a standard envelope (~5g) and 3 sheets of paper (15g), I know I'm at 20g total. It turns physical shipping logistics into simple integer arithmetic. The elegance of the metric system is that it makes the properties of materials discoverable through their definitions.reply",
      "The 5 grams per sheet of common printer paper has certainly proven handy once or twice in some of my interactions in the informal economy.reply",
      "Same for the US 5 cent coin.  Defined mass of 5 grams.reply",
      "And $20 in dimes or quarters is 1 lb. US silver coins are 0.2268 grams per cent.reply",
      "Don't tell the current administration that there's something so un-American about the currency: they will insist on fixing it, and probably retire Jefferson as well.reply",
      "Fun fact: While the US spent more than 3 cents for every penny minted and distributed, it spends about 14 cents for every nickel minted and distributed!reply",
      "When they decided to stop minting pennies I think they should have gotten rid of nickels and (I know this will be controversial) quarters as well!Keep dimes and ramp up production of half dollars. Then we can just drop the second decimal place and standardize pricing everything in 0.1 dollar increments.The fact that quarters are still somewhat commonly used in machines (vending machines, parking meters, laundry) is probably the biggest practical obstacle.reply",
      "This may be the most practical go-forward plan. The Euro's .20 coins are also attractive too. But you're correct that quarters, as the smallest common currency that you can plausibly buy something with just a couple of them, are just everywhere, from laundry to car washes, so the pain in retiring them would be widely felt.What I've learned from the penny retirement is that people are deeply distrustful of simple high school level statistics! Millions of people have angrily seethed that somehow stores are or will be using the penny retirement to rob them, despite knowing that most transactions have an unknowable amount of different items, and sales tax, so attempting to manipulate prices to gain a statistical advantage out of rounding would be incredibly difficult and would yield a pitiful return. Let alone how the cash transaction share is declining every year.reply",
      "Which is pennies compared to the amount of economic activity that those pennies facilitated.reply",
      "> activity that those pennies facilitatedDo you mean in the zinc mining and Coinstar? Pennies have been a bizarre ritual for years, wherein the government made zinc worth less than its pre-minted value, distributed them to banks nationwide, banks in turn to stores, stores using them once to give meaningless amounts of money to customers, customers in turn immediately throwing them on the ground or at best eventually dumping them into a coinstar, and coinstar returned those to banks.Nothing of value was going on there. I'd rather pay any zinc miners and coinstar drivers who have been displaced to play video games all day while still saving all those resources, fuel, and most of all, time.reply"
    ],
    "link": "https://susam.net/a4-paper-stories.html",
    "first_paragraph": "\n  I sometimes resort to a rather common measuring technique that is\n  neither fast, nor accurate, nor recommended by any standards body\n  and yet it hasn't failed me whenever I have had to use it.  I will\n  describe it here, though calling it a technique might be overselling\n  it.  Please do not use it for installing kitchen cabinets or\n  anything that will stare back at you every day for the next ten\n  years. It involves one tool: a sheet of A4 paper.\n\n  Like most sensible people with a reasonable sense of priorities, I\n  do not carry a ruler with me wherever I go.  Nevertheless, I often\n  find myself needing to measure something at short notice, usually in\n  situations where a certain amount of inaccuracy is entirely\n  forgivable.  When I cannot easily fetch a ruler, I end up doing what\n  many people do and reach for the next best thing, which for me is a\n  sheet of A4 paper, available in abundant supply where I live.\n\n  From photocopying night-sky charts to serving as a scratch pad"
  },
  {
    "title": "What *is* code? (2015) (bloomberg.com)",
    "points": 117,
    "submitter": "bblcla",
    "submit_time": "2026-01-01T22:08:05 1767305285",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=46458610",
    "comments": [
      "I was lost, literally, hitchhiking across the Australian outback when this article was published. Going home felt scary because I was afraid to be alone with no one else sharing my interests. Travelling made life enjoyable again because just surviving felt like an achievement. But I felt so, so isolated (again, literally!) from modern society. I wanted to find out why I was so deeply interested in computers but not in \u201ctech\u201d. They must work somehow\u2026 why did my iPhone (sold that) feel similar to my PC (sold that too) but only one is called a computer? This article framed things in a way that shook me out of a physically dangerous, homeless, jobless rut. It was all code. And I could learn it if I had the time.Perhaps it was the way it was written; I couldn\u2019t believe intrigue and passion of computing could be weaved together like this. But there it was.I did make it home eventually. Fortunately the first 2000km lift back from western Australia to the eastern states with a crystal meth addict on the run from the police didn\u2019t end violently. A few weeks back in Sydney with family some Linux nerds found me working as a receptionist answering phones and scanning paper records in at a failing medical practice.  They got me doing desktop Windows and Linux server support. I\u2019m an official software engineer now. I guess I should print this article out to show to my kids!reply",
      "This story is \"best comments\" material. It would be even if it were a fabulist tale. Thanks for sharing!reply",
      "Haha thanks for saying that. It\u2019s real! It\u2019s relatively easy to get into the middle of nowhere in Australia after all ;) Actually still haven\u2019t published my journal scribblings on my blog 10 years on..reply",
      ">some Linux nerds found me working as a receptionist answering phones and scanning paper records in at a failing medical practice. They got me doing desktop Windows and Linux server support. I\u2019m an official software engineer nowThere is a gap between receptionist and official software engineer. Please, give us more details about your journey and what happened in betweenreply",
      "> There is a gap between receptionist and official software engineer.At many companies (especially old, stodgy companies) this gap is artificial. The day you get asked \"hey, I've got some data .... and I need ...\" and you successfully solve the person's problem, is the day you become the office's live-in software engineer. That person you helped will be back, and they will bring friends.The rest after that is just job title shuffling.reply",
      "Not sure what country you live in, but where I live, a receptionist doesn't have access to any data processing tools that are not within the realm of a receptionist, and therefore this mobility does not happen. The receptionist ends up redirect the query to someone who has access to the relevant systems.What sort of companies are those were receptionists have access to tools beyond their role? and why are people approaching the receptionists asking for data queries?Like having to run a script on that data when your machine doesn't have the permissions to run arbitrary software without permission from the IT teamreply",
      "You're still thinking too much in a \"tech company\" mindset. At the kind of company I'm talking about, concepts like \"access\" and \"permissions\" are irrelevant. Most of the company's employees barely know how a computer works.You seemed vaguely tech savvy, so someone asked you for help and emailed you a file containing the data (or perhaps just handed you a laptop and turned you loose). The rest is history.It's a modern invention that companies have separate software engineering orgs, software engineering roadmaps, software engineering managers. At older companies, a software developer is just another businessperson in a cubicle. Your manager probably has an English degree.reply",
      "> You seemed vaguely tech savvy, so someone asked you for help and emailed you a file containing the data (or perhaps just handed you a laptop and turned you loose). The rest is history.I know that someday I'll work in something other than IT. When I do I am going to make for damned sure that I don't express even the slightest bit of tech savvy for exactly this reason.It's similar to playing dumb w/ people I encounter outside work who find out I work in IT. If I get asked a question I play dumb and cop to working on some highly siloed subject (usually I'll claim to only work on \"networking\" or firewalls... >smile<).reply",
      "Many small business.The first job I had where I did anything technical (basic JS and HTML) also had me cold calling, answering phones, designing brochures, fiberglass repair, and some other stuff I\u2019m forgetting. Small businesses frequently have more niche jobs than people and are more than happy to have people help where they are interested.My first full software job was a direct to consumer company, and during the Christmas rush the entire front office was on the packing line.Larger companies tend to appreciate people staying in their lane.reply",
      "In small companies there is one lane.reply"
    ],
    "link": "https://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/",
    "first_paragraph": ""
  }
]