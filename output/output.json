[
  {
    "title": "Evertop: E-ink IBM XT clone with 100+ hours of battery life (github.com/ericjenott)",
    "points": 175,
    "submitter": "harryvederci",
    "submit_time": "2025-04-21T22:07:49 1745273269",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=43757037",
    "comments": [
      "Love it! Any idea how long the display can last? I've been playing around with e-paper (nothing as impressive as this!) dashboards. I use Waveshare displays that has a max of 1 million refresh cycles. The display you've used seems more capable.My own humble e-paper projects:https://www.asciimx.com/projects/e-reader/\nhttps://www.asciimx.com/projects/etlas/\n \nreply",
      "I think there is a class of device here that is missing. Low power but forever devices that have some basic functionality. Over time I could see this taking over laptops and the like as ultra-low-power became more and more capable.\n \nreply",
      "Most people sell or give away fully functional, very powerful mobile phones, because of the end of the software support.Hardware is more than capable for a long time, and is often very durable. But it takes a special kind of audience to put up with decade-old unsupported software, let alone with IBM XT-level software (which I remember using).Security is not a consideration for such devices, because of their very limited number. Nobody is going to crack into your internet-connected Amiga except maybe some of your friends, as a prank. But a forever-device used for something substantial, something touching money in any way, would have to be much more up-to-date.\n \nreply",
      "I\u2019ve never dumped a phone over its software.  Ware, damage, swapping networks, meaningfully better hardware, or just losing the things explain basically all the replacements me or my friends / family have done.Sure, eventually people stop updating software to work on old devices but that\u2019s because the overwhelming majority of people have already stoped using that hardware for other reasons.\n \nreply",
      "I dumped my last phone, the Palm PVG100, because software updates made it too slow and ate up its battery life too quickly. It's too bad because the updates didn't improve anything and the PVG100 has the best form factor of any phone I've owned.\n \nreply",
      "There are a lot of 'forever devices' currently touching money in major financial institutions.\n \nreply",
      "I spent a good chunk of my career in banking. I had many conversations to the effect of \u201csee that RS/6000 in the corner of the network diagram? It processes $45bn in payments every day.\u201d\n \nreply",
      "did you work at Chase too\n \nreply",
      "Yea, and armies of engineers supporting them.\n \nreply",
      "More often in my experience, it\u2019s one or two greybeards who have been there for 30 years, and are the only two people still in the workforce (or still alive) who understand how it works.\n \nreply"
    ],
    "link": "https://github.com/ericjenott/Evertop",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        E-ink IBM XT clone with solar power, ultra low power consumption, and ultra long battery life. \n      Evertop is a portable PC that emulates an IBM XT with an 80186 processor and 1MB RAM. It can run DOS, Minix, and some other old 1980s operating systems.  It also runs Windows up to version 3.0.  Because it's based on a powerful yet very low power microcontroller, uses an e-ink display, packs two 10,000mAh batteries, and implements extreme power saving measures, it can run for hundreds or even thousands of hours on a single charge.  Combine that with its built in solar panel, and you should be able to use it indefinitely off grid without ever having to worry about battery life.\nIt features a built in keyboard, external PS/2 keyboard and mouse ports, full CGA, Hercules, and MCGA graphics support, partial EGA and VGA support, PC speake"
  },
  {
    "title": "Prolog Adventure Game (github.com/stefanrodrigues2)",
    "points": 22,
    "submitter": "shakna",
    "submit_time": "2025-04-22T00:25:12 1745281512",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=43757916",
    "comments": [
      "Very nice. I gave a lab assignment like this once! It's a great way to learn Prolog.https://github.com/agiacalone/cecs-342-lab-prolog\n \nreply",
      "There's even a book \"Adventure in Prolog\" by Dennis Merritt (ISBN 1520918917)It's a lot of fun to work through, other prolog resources can be a little dry\n \nreply"
    ],
    "link": "https://github.com/stefanrodrigues2/Prolog-Adventure-game",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Text Adventure game in SWI Prolog.\n      Adventure game with prolog language. The player has to find the treasure hidden inside the castle to win the game. A total of 3 lives will be available. Implemented interactive gameplay mechanics such as locked doors, hidden objects, incomplete objects, limited resources, and inventory management.\n        Text Adventure game in SWI Prolog.\n      "
  },
  {
    "title": "Blog hosted on a Nintendo Wii (infected.systems)",
    "points": 361,
    "submitter": "edent",
    "submit_time": "2025-04-21T18:29:04 1745260144",
    "num_comments": 69,
    "comments_url": "https://news.ycombinator.com/item?id=43754953",
    "comments": [
      "For those unaware, the \"SSL Added and removed here!\" image is a reference to a diagram describing unencrypted communications between Google datacenters that leaked from the NSA in 2013 [1].[1] https://arstechnica.com/tech-policy/2013/10/new-docs-show-ns...\n \nreply",
      "Once I had a professor who had a top secret clearance and did some government work. He couldn't reference this because it's never been unclassified. But in one of his courses, he had an image of these two circles aligned just like this when discussing a relevant topic, and I don't know if anyone in my section picked up on it.\n \nreply",
      "That :\u00ac) face makes for a great custom emote in security-related channels.\n \nreply",
      "Still kind of insane to think about the US government using underwater submarines or something to hack into the communications of US companies to spy on Americans and that not being, like, a bigger deal in 2013.\n \nreply",
      "It\u2019s so funny so see a top secret label below what\u2019s clearly a hastily scribbled diagram\n \nreply",
      "Looks like a design interview round\n \nreply",
      ">SSL Added and removed here!And CloudFlare!\n \nreply",
      "> Rebooting NetBSD reboots the whole console, and not just the NetBSD \u2018app\u2019, so you\u2019ll find yourself back at the Wii Menu after any kernel patch or system upgrade.This can be mitigated by installing Priiloader, and having it autoboot into either the Homebrew Channel or the NetBSD .dol file\n \nreply",
      "One of the coolest revelations to me, just looking at the status page [1] is that the Wii only had 88 MB of RAM - split between 24 MB built in to the SoC and 64 MB of GDDR3.Given that this is the case, ntpd using 15% of the system memory means it was using about 13 MB of RAM - hardly a huge chunk by today's standards but not small either. I wonder if reducing the number of time servers would improve this much? On my system I can see I'm tracking about 9 servers from the debian pool.Compare this to the XBox 360 which even at the time had 512 MB, it's really amazing how much they managed to squeeze out of such a tiny chip.[1] - https://blog.infected.systems/status/\n \nreply",
      "FYI - instead of Photo Booth you can use Quicktime Player and \"create new movie recording\". I believe that should fix the image flipping problem.\n \nreply"
    ],
    "link": "https://blog.infected.systems/posts/2025-04-21-this-blog-is-hosted-on-a-nintendo-wii/",
    "first_paragraph": "\n\nHome\n\t\t\t\n\t\t\t\n\t\t\t| Posts\n\t\t\t\n\t\t\t| Blogroll\n\t\t\t\n\t\t\t| About\n\nIf you are reading this message, the experiment below is still ongoing. This page was served to you by a real Nintendo Wii.You can check the Wii\u2019s live status page for system load info.For a long time, I\u2019ve enjoyed the idea of running general-purpose operating systems on decidedly not-general-purpose hardware.There\u2019s been a few good examples of this over the years, including a few which were officially sanctioned by the OEM. Back in the day, my PS3 ran Yellow Dog Linux, and I\u2019ve been searching for a (decently priced) copy of PS2 Linux for 10+ years at this point.There are some other good unofficial examples, such as Dreamcast Linux, or PSPLinux.But what a lot of these systems have in common is that they\u2019re now very outdated. Or they\u2019re hobbyist ports that someone got running once and where longer-term support never made it upstream. The PSP Linux kernel image was last built in 2008, and Dreamcast Linux is even more retro, usin"
  },
  {
    "title": "'Immediate red flags': questions raised over 'expert' much quoted in UK press (theguardian.com)",
    "points": 70,
    "submitter": "mellosouls",
    "submit_time": "2025-04-19T16:00:09 1745078409",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=43737272",
    "comments": [
      "> Her qualifications are described there as \u201cpsychologist and sex adviser \u2013 University of Oxford\u201d. However, the British Psychological Society (BPS) said she was not one of its members.It appears as they could not verify if she was in Oxford at all. If there is no way to check that then anyone could pretend. I would not be surprised if anyone was just relying on the choice of words Santini used when communicating, appearing as overly educated in the British system.\n \nreply",
      "I don't read The Guardian enough to know if the snark was intentional or not but this line gave me a chuckle,> She does not appear to have social media profiles, though she has two followers on the blogging site Medium.Talk about damning with faint praise!\n \nreply",
      "I feel if you know anything about British culture, the snark is always intentional.\n \nreply",
      "> damning with faint praiseHadn't encountered this phrase before. TIL.https://en.wikipedia.org/wiki/Damning_with_faint_praise\n \nreply",
      "They are usually somewhat staid, but I suspect the opportunity to dig into the Daily Mail was irresistible.\n \nreply",
      "> it has raised the issue of how journalists verify the credentials of sources in the AI agePerforming background checks is not difficult. Professional background check services are fast and commonly used in hiring processes. It seems like this article is (deliberately?) missing the actual questions raised by this case: why are these various outlets/journalists so lacking in rigor when it comes to the accuracy of their content, and how is a fraudulent expert consistently being chosen for their articles.\n \nreply",
      "Very interesting article.  This \u2018person\u2019 has a commercial sex related website and some medium posts but no presence otherwise.Apparently reporters found her through some services that connect experts with reporters and I\u2019m guessing the reporters trusted that service.\n \nreply",
      "They\u2019re called expert networks. Usually, basic research practices like \u201cfact checking\u201d and \u201csample size\u201d apply. I guess they just decided on the lottery approach.\n \nreply",
      "Well they found a sexpert network by mistake. Very innocently of course.\n \nreply",
      "As the old saw goes, on the internet nobody knows you're a dog.\n \nreply"
    ],
    "link": "https://www.theguardian.com/media/2025/apr/19/questions-raised-over-barbara-santini-expert-much-quoted-in-uk-press",
    "first_paragraph": "News outlets pull articles featuring \u2018psychologist and sex adviser\u2019 Barbara Santini amid doubts over her credentialsOver the past couple of years, the Oxford-educated psychologist Barbara Santini has been widely quoted as an expert. She has contributed thoughts on everything from the psychological impact of the Covid pandemic to the importance of vitamin D and how playing darts can improve your health.However, her pronouncements have begun to disappear from articles after concerns that Santini may not be all that she appears. Major news outlets have removed entire articles featuring Santini, or comments made by her, after a series of questions were raised over her qualifications \u2013 and even whether her entire identity could be an elaborate hoax.The case has been described as a wake-up call for newsrooms, as AI tools make it far easier for bad actors to invent supposed experts for their own purposes. Santini\u2019s output has been prolific, with comments in Vogue, Metro, Cosmopolitan, the i n"
  },
  {
    "title": "Show HN: Dia, an open-weights TTS model for generating realistic dialogue (github.com/nari-labs)",
    "points": 343,
    "submitter": "toebee",
    "submit_time": "2025-04-21T17:07:07 1745255227",
    "num_comments": 117,
    "comments_url": "https://news.ycombinator.com/item?id=43754124",
    "comments": [
      "That Sesame CSM-1B voice sounds sooo done with life, haha.\n \nreply",
      "Very cool!Insane how much low hanging fruit there is for Audio models right now. A team of two picking things up over a few months can build something that still competes with large players with tons of funding\n \nreply",
      "Thank you for the kind words <3\n \nreply",
      "Hey, do yourself a favor and listen to the fun example:> [S1] Oh fire! Oh my goodness! What's the procedure? What to we do people? The smoke could be coming through an air duct!Seriously impressive.  Wish I could direct link the audio.Kudos to the Dia team.\n \nreply",
      "For anyone who wants to listen, it's on this page: https://yummy-fir-7a4.notion.site/dia\n \nreply",
      "Wow. Thanks for posting the direct link to examples. Those sound incredibly good and would be impressive for a frontier lab. For two people over a few months, it's spectacular.\n \nreply",
      "A little overacted, it reminds me of the voice acting in those flash cartoons you'd see in the early days of YouTube. That's not to say it isn't good work, it still sounds remarkably human. Just silly humans :)\n \nreply",
      "Reminded me of the Fenslerfilm G.I. Joe sketch where the kids have something on the stove burning\n \nreply",
      "Stop all the downloading!\n \nreply",
      "This is oddly reminiscent of the office. I wonder if tv shows were part of its training data!\n \nreply"
    ],
    "link": "https://github.com/nari-labs/dia",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A TTS model capable of generating ultra-realistic dialogue in one pass.\n      \n\n\n\n\n\n\n\n\n\n\nDia is a 1.6B parameter text to speech model created by Nari Labs.Dia directly generates highly realistic dialogue from a transcript. You can condition the output on audio, enabling emotion and tone control. The model can also produce nonverbal communications like laughter, coughing, clearing throat, etc.To accelerate research, we are providing access to pretrained model checkpoints and inference code. The model weights are hosted on Hugging Face. The model only supports English generation at the moment.We also provide a demo page comparing our model to ElevenLabs Studio and Sesame CSM-1B.This will open a Gradio UI that you can work on.or if you do not have uv pre-installed:Note that the model was not fine-tuned on a specific voice. Hence, you w"
  },
  {
    "title": "Cheating the Reaper in Go (mcyoung.xyz)",
    "points": 42,
    "submitter": "ingve",
    "submit_time": "2025-04-21T21:46:32 1745271992",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=43756871",
    "comments": [
      "I've been doing some performance tuning in Go lately to really squeak performance, and ended up with a very similar arena design except using byte slices for buf and chunks instead of unsafe pointers.  I think I tried that too and it wasn't any faster and a whole lot uglier, but I'll have to double check before saying that with 100% confidence.A couple other easy wins -if you start with a small slice and find some payloads append large amounts, write your own append that preemptively is more aggressive in cap bumping before calling the builtin append.unsafe.String is rather new and great for passing strings out of byte slices without allocating.  Just read the warnings carefully and understand what you're doing.\n \nreply",
      "The append(slice,slice2...) code is all well and good but its going to hit into the expansion quite often. When you know the second append is going to be large its often faster to allocate a new slice with the right size and no elements and then append both slices to it, then there is no expansion costs the values just get copied in and it also produces less garbage to be collected.I have done a few other things in the past where I had sliceLike's which took two slices and point to one and then the other and a function mapped to the indices as if they were appended, costs a bit on access but saves on the initial allocation if you don't intend to iterate through the entire thing or only do so once.The base library in go does not do much for optimising this sort of thing, its not a dominate operation in most applications so I can see why we don't have more advanced data structures and algorithms. You have to be quite heavily into needing different performance characteristics to outperform the built ins with custom code or a library. All parts of Go's simplicity push that seems to assume people don't need anything else other than Array Lists and hash maps.\n \nreply",
      "Off topic, but I love the minimap on the side -- for pages where I might be jumping around the content (long, technical articles, to refer back to something I read earlier but forgot) -- how can I get that on my site? Way cool.\n \nreply",
      "No idea how plug-n-playable it is, but the source seems to be self contained: https://github.com/mcy/mcy.github.io/blob/master/public/js/m...\n \nreply"
    ],
    "link": "https://mcyoung.xyz/2025/04/21/go-arenas/",
    "first_paragraph": "Even though I am a C++ programmer at heart, Go fascinates me for none of the reasons you think. Go has made several interesting design decisions:It has virtually no Undefined Behavior1.It has very simple GC semantics that they\u2019re mostly stuck with due to design decisions in the surface language.These things mean that despite Go having a GC, it\u2019s possible to do manual memory management in pure Go and in cooperation with the GC (although without any help from the runtime package). To demonstrate this, we will be building an untyped, garbage-collected arena abstraction in Go which relies on several GC implementation details.I would never play this kind of game in Rust or C++, because LLVM is extremely intelligent and able to find all kinds of ways to break you over the course of frequent compiler upgrades. On the other hand, although Go does not promise any compatibility across versions for code that imports unsafe, in practice, two forces work against Go doing this:Go does not attempt to"
  },
  {
    "title": "Astronomers confirm the existence of a lone black hole (phys.org)",
    "points": 107,
    "submitter": "wglb",
    "submit_time": "2025-04-21T18:36:23 1745260583",
    "num_comments": 82,
    "comments_url": "https://news.ycombinator.com/item?id=43755017",
    "comments": [
      "> Prior to this new finding, all the black holes that have been identified have also had a companion star\u2014they are discovered due to their impact on light emitted by their companion star. Without such a companion star, it would be very difficult to see a black hole.It seems like we think there's many more of these black holes, but we just can't see them\n \nreply",
      "Lone stars are actually the exception, so not radically more as you might think. But there are also binary black holes.\n \nreply",
      "This only covers stellar black holes. (Note that this black hole is believed to be a stellar black hole.) Those statistics could change quickly if you add to it a currently unknown number of primordial black holes that arose around the Big Bang.If those primordial black holes are mostly on their own, and are both numerous and small, they make a potential candidate for dark matter. They could also be potentially small enough to be evaporating in our current era. This has been suggested as a potential source of a very high energy neutrino that was found in February. See https://www.livescience.com/space/black-holes/evidence-for-s....(Note that this is just a single observation. We are a very long way from being able to obtain strong experimental evidence for such speculative theories.)\n \nreply",
      "I thought there were too many constraints to make PBHs a significant contributor?\n \nreply",
      "A type-1a supernova peer would produce this effect, leaving only the black hole (or the oversize star that would become it). I don't know any other types where the star is completely destroyed.\n \nreply",
      "Astrophysical Journal article: https://iopscience.iop.org/article/10.3847/1538-4357/adbe6eEarlier article about first discovery: https://iopscience.iop.org/article/10.3847/1538-4357/ac739e/...\n \nreply",
      "Thanks, the original article is unreadable on mobile due to the ads.\n \nreply",
      "What I don't understand is how big bang could exist if such relatively \"small\" mass concentration creates black holes?\n \nreply",
      "Excellent question.Gravity pulls things in by causing space-time to accelerate in a particular direction. In other words we accelerate towards the Earth at 9.8 meters per second per second because that is what space-time itself does. The space-time that is in our frame of reference accelerates down, carrying us with it. The floor pushes up on us, causing us to accelerate up. Balancing things out so that we remain where we are.A dense mass will cause flat space-time to start falling in. Enough mass, densely enough, will cause it to fall in so fast that not even light can escape. This is a black hole.However the Big Bang wasn't a flat space-time. The space-time that was the structure of the universe was moving apart extremely quickly. There was more than enough mass around to create a black hole today. But what it did is cause the expansion rate to slow. Not to stop, reverse, and fall back in on itself into a giant black hole.\n \nreply",
      "I don't understand this answer. By GR there is no possible flat space-time around a dense mass no? BC the energy will curve the space-time. Saying that the space-time was expanding very quickly is also describing the shape of the space-time. Isn't it kind of circular to say that big bang doesn't end in a singularity b/c it is curved out? You can still ask why it's curved out with so much energy and whether it is compatible with GR? But I guess the answer if GR was holding near big bang must just be that there's some solution which is compatible with GR with so much energy in a small place which doesn't end in singularity.\n \nreply"
    ],
    "link": "https://phys.org/news/2025-04-astronomers-lone-black-hole.html",
    "first_paragraph": ""
  },
  {
    "title": "Launch HN: Magic Patterns (YC W23) \u2013 AI Design and Prototyping for Product Teams",
    "points": 117,
    "submitter": "alexdanilowicz",
    "submit_time": "2025-04-21T14:07:03 1745244423",
    "num_comments": 76,
    "comments_url": "https://news.ycombinator.com/item?id=43752176",
    "comments": [
      "I don't normally comment on these things, but I gave it a quick shot for a project I'm working on (fairly generic dashboard-style prompt, but that's fine).I'm actually pretty impressed. A couple things though:1. It took a _while_ to give me anything. Not sure if that's related to load, but it was ~17 files, and probably took 5+ minutes. It was not clear what was going on in that time, or what would happen if I left it. I literally left my machine to go something else before coming back.2. I really hate saying this, but your pricing is probably way too low, especially at the \"pro\" level from your pricing page. When stepping into team-based config management and pre-sets, you're leaving a ton of money on the table without enterprise-style custom value-based pricing. If you were asking me, I would recommend moving the team based features (shared presets, custom access control, etc) into an \"enterprise\" level above pro).I'm not going to comment on any sort of \"correctness\" as far as any complex UX behaviours or workflows; I'm only considering this from a mockup/design/demo-of-new-ideas perspective.\n \nreply",
      "1. We're using Sonnet 3.7 for the first prompt. I've noticed with some prompts that require lots of files it can be PAINFULLY slow. Our servers also might be getting slammed from the HN traffic. We have a \"fast\" mode that uses 3.5 that you can toggle and that's the default for editing, however, it won't be as visually rich. We need to improve the loading experience for sure. One big UX/UI difference between our product and others is that our preview is always shown versus on other tools the code is always shown. Other tools will stream in the code to mask the load time. We used to do that, and will likely bring it back.2. Re pricing - that's the most important feedback we'll hear all day! We used to have a \"contact us for pricing\" tier, but have found self-serve a lot more effective and easier to scale.We actually still only 2 people, just my co-founder and me. When you say \"custom value-based\" are you referring to a \"contact us for pricing\" tier?\n \nreply",
      "> When you say \"custom value-based\" are you referring to a \"contact us for pricing\" tier?Ya. Not saying that it's applicable to everyone (or even most people), but really once a team gets above maybe 20+ people actively using this, they're not going to blink at $1200/month (good for you now, but you'll be leaving a ton of money on the table, and it's hard to adjust expectations later).Maybe capping the size of a team on the \"pro\" plan would be an inbetween, but it's something to talk to your customers about.Happy to chat more directly; my email's in my bio.\n \nreply",
      "> Maybe capping the size of a team on the \"pro\" plan would be an inbetweenThat's interesting. We already have teams with 20+ folks on it today at very large companies, but haven't thought about that type of stuff too much - have been laser-focused on core product building. I think in the early days we spent a little too much time tweaking minor pricing plan details. You're right though we are now at a point where we are very likely leaving money on the table.For example, \"centralized billing\" on our platform only exists because it was the result of a feature request from a larger customer.P.S. I emailed you, but it bounced!\n \nreply",
      "Email you too; thanks for the heads on the bounce. Should be fixed now.\n \nreply",
      "I used magic patterns for a couple of months and it was one of the first no brainer AI services I've paid for outside of the main LLMs and IDEs. It did such an amazing job on quite an esoteric frontend that's very much not your normal web app. Impressive. Next time I need to design and build some more frontend code I'll be subscribing again.Edit: to add some meat to that comment what surprised me was just how much better it was than Anthropic and OpenAI tools at that time for coming up with great looking products with minimal prompting. I also fed it other designs for inspiration and it replicated them brilliantly while incorporating my requirements. Good stuff.\n \nreply",
      "Good to hear from you and see you on here!\n \nreply",
      "I'm interested in understanding your desire to do design and prototyping as a single shot?My expectation was that I'd iterate on a few UX designs with the LLM and then when I'm happy with what the LLM is suggesting, I'd output to figma, and then maybe move to code.It's great that you're generating code, but isn't that increasing your cost and processing time to write code for each iteration?\n \nreply",
      "What do you mean by \"single shot?\" Are you referring to one-shot prompting? I should have clarified in my post \u2014 we do have customers who one-shot designs - but that's very rare (it's usually landing pages because Sonnet 3.7 is really good at those). We heavily encourage iteration and expect it. The longest thread on the platform is 850+ messages in a single chat!We have a fine-tuned fast apply model for applying code diffs, so that helps minimize the processing time. Always trying to make it faster though.We view code as a way to 1) unlock interactivity, 2) communicate with the LLM.A question for you: if we weren't asking the LLM to generate code, what would we ask it to generate?\n \nreply",
      "This is something I would definitely use, as my company pays for v0 today for these exact purposes (product design/PM).I've tried some of the same prompts I've done on v0 but didn't notice a lot of difference -- needs a lot of back-and-forth, as with v0. So not sure what would make me switch at this point.\n \nreply"
    ],
    "link": "item?id=43752176",
    "first_paragraph": ""
  },
  {
    "title": "The Future of Compute: Nvidia's Crown Is Slipping (mohitdagarwal.substack.com)",
    "points": 84,
    "submitter": "wilson090",
    "submit_time": "2025-04-21T22:06:14 1745273174",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=43757017",
    "comments": [
      "Seems like another article based on the assumption that Nvidia just sits there doing nothing while everyone who has so far proven unable to compete suddenly figures it out and steals their lunch.At some point one of these Nvidia doomers will be right but there is a long line of them who failed miserably.\n \nreply",
      "> everyone who has so far proven unable to competeThe article explains that Nvidia's biggest customers (50% of datacenter revenue) are switching to their own hardware.\n \nreply",
      "But then they're not boosting NVidia's competitors either, so wouldn't NVidia stay in the top position in their market ?The article seems focused more on stock price and where to bet, than the market for GPUs or generic hardware vendors.\n \nreply",
      "> But then they're not boosting NVidia's competitors either, so wouldn't NVidia stay in the top position in their market ?Nope; if hypothetically 100% left NVidia, whether to their own hardware or to not use GPUs at all, it'd be easy to say NVidia would be last in the market\n \nreply",
      "By switching to their own hardware, they become NVidia's competitors.\n \nreply",
      "Seems like the opposite to me: https://www.datacenterdynamics.com/en/news/google-in-talks-t...\n \nreply",
      "You linked to an article about Google renting an insignificant amount of additional capacity.Google runs AI / HPC workloads on their own hardware and has been doing that for more than a decade. Google Gemini was trained on TPUs developed in house. It does not run on Nvidia hardware.\n \nreply",
      "And I believe Apple refuses to use NVidia as well, they\u2019re actually using Google\u2019s TPUs.https://www.tomshardware.com/tech-industry/artificial-intell...\n \nreply",
      "That was for training. For inference, they reportedly use their own silicon.",
      "Google, last month: \u201cwe\u2019re doubling down on our partnership with NVIDIA\u201dhttps://blog.google/technology/ai/google-nvidia-gtc-ai/\n \nreply"
    ],
    "link": "https://mohitdagarwal.substack.com/p/from-dominance-to-dilemma-nvidia",
    "first_paragraph": ""
  },
  {
    "title": "Ultra-precision formation flying demonstration for space-based interferometry (arxiv.org)",
    "points": 29,
    "submitter": "PaulHoule",
    "submit_time": "2025-04-21T21:31:40 1745271100",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=43756723",
    "comments": [
      "If you know how hard it is to align an interferometer on an optical table, where things are BOLTED to it, this looks absolutely insane.\n \nreply",
      "That should be useful for stellar coronagraphs for finding extrasolar planets. The idea is to have an opaque disc that occludes the star so you can see the planets. This is usually attached to the telescope, but one could be flown out a kilometer or so with this approach. JPL was looking at this but went with a more traditional design instead.\n \nreply",
      "Presumably the limiting factor for this is density variations in the tiny bit of atmosphere present up there...And the fix is to fly higher...?\n \nreply",
      "Much higher. Like, 1.5 million kilometers.LEO is just for demonstration\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2504.05001",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "A new form of verification on Bluesky (bsky.social)",
    "points": 219,
    "submitter": "ink_13",
    "submit_time": "2025-04-21T16:16:51 1745252211",
    "num_comments": 163,
    "comments_url": "https://news.ycombinator.com/item?id=43753651",
    "comments": [
      "I got verified in the initial round of verification.On a technical level, this sort of works like a Root CA: anyone can verify anyone by publishing a `app.bsky.graph.verification` record to their PDS. Bluesky then chooses to turn those from trusted accounts into the blue check, similar to browsers bundling root CAs into the browser.* https://pdsls.dev/at://did:plc:z72i7hdynmk6r22z27h6tvur/app.... <- bluesky verifying me. it's coming from at://bsky.app, and therefore, blue check* https://pdsls.dev/at://did:plc:3danwc67lo7obz2fmdg6jxcr/app.... <- me verifiying people I know. it's coming from at://steveklabnik.com, and therefore, no blue check.I am not 100% sure how I feel about this feature overall, but it is something that a lot of users are clamoring for, and I'm glad it's at least \"on-protcol\" instead of tacked on the side somehow. We'll see how it goes.\n \nreply",
      "I wish it'd work like labelers and other moderation features: with users able to choose which verifiers to use.  I trust the NYT as far as I can throw them when it comes to verification, for example, whereas I'd be interested in something flagging Bluesky employees or contributors to a given GitHub repository or whatever other bizarre things people would use this for like they already use labels.\n \nreply",
      "> I trust the NYT as far as I can throw them when it comes to verificationYou don't trust the NYT to verify its own reporters?Also, why do you say that in any circumstance? Who do you trust?\n \nreply",
      "What's good is that the technical design here allows them to pivot into that if they choose, and alternative clients can already do that if they wish.\n \nreply",
      "Initially I just thought they verified people working at Bluesky, which made enough sense, but this initial batch seeming so arbitrarily decided isn't a good look. It feels all too similar to the \"I know someone at Twitter\" verification in the SF tech community.\n \nreply",
      "Some employees aren't even verified!I hear you. I haven't investigated every account that got the badge, but it feels to me like they picked people who are both technical and engaged with the protocol, so not entirely arbitrary. That naturally will have some correlation with \"I know someone at bsky\". I know I've seen accounts that I think are cooler than I am who didn't get verified yet! I'm sure they'll be expanding soon, which will dilute this sort of association.\n \nreply",
      "I can empathize with their position; I know this is something the community, especially the newer users coming from the continued rapid degradation of Twitter, are asking for.The concept of verification and Bluesky's original mission of decentralization are two very at-odds concepts, and I think they've bridged that pretty well and left a lot of options for themselves to expand it in the future. I'm just worried about the very visible parallels to the Twitter ecosystem emerging.My opinions on this will change if I join the verified elite, in case any bsky employees are in the thread.\n \nreply",
      "> The concept of verification and Bluesky's original mission of decentralization are two very at-odds concepts,Not necessarily. Consider the PGP Web-of-Trust model. Centralization of trust is choice, nothing inherent in verification as such.\n \nreply",
      "Unfortunately that\u2019s how I\u2019m beginning to see this too, a sign of old school nepotism and struggle to regain lost status. We\u2019ve seen how this unfolded for Twitter.\n \nreply",
      "How did that unfold on Twitter? I thought they did it better than anybody before the takeover but maybe I\u2019m missing something.\n \nreply"
    ],
    "link": "https://bsky.social/about/blog/04-21-2025-verification",
    "first_paragraph": ""
  },
  {
    "title": "A M.2 HDMI capture card (interfacinglinux.com)",
    "points": 79,
    "submitter": "Venn1",
    "submit_time": "2025-04-21T19:01:55 1745262115",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=43755286",
    "comments": [
      ">Fortunately, those extra PCIe lanes tend to get repurposed as additional M.2 holes.Or unfortunately, for the unlucky people who didn't do their research, so now their extra M.2 drives are sucking up some of their GPU's PCIe bus.\n \nreply",
      "The vast majority of people run just one gpu, which motherboards have a dedicated direct to CPU x16 slot for. Stealing lanes comes into play with chipset connected slots\n \nreply",
      "I bought a Gigabyte X870E board with 3 PCIe slots (PCIe5 16x, PCIe4 4x, PCIe3 4x) and 4 M.2 slots (3x PCIe5, 1x PCIe 4). Three of the M.2 slots are connected to the CPU, and one is connected to the chipset. Using the 2nd and 3rd M.2 CPU-connected slots causes the board to bifurcate the lanes assigned to the GPU's PCIe slot, so you get 8x GPU, 4x M.2, 4x M.2.I wish you didn't have to buy Xeon or Threadripper to get considerably more PCIe lanes, but for most people I suspect this split is acceptable. The penalty for gaming going from 16x to 8x is pretty small.\n \nreply",
      "For a moment I didn't believe you, then I looked at the X870E AORUS PRO ICE (rev. 1.1) motherboard [1] and found this:> 1x PCI Express x16 slot (PCIEX16), integrated in the CPU:> AMD Ryzen\u2122 9000/7000 Series Processors support PCIe 5.0 x16 mode> * The M2B_CPU and M2C_CPU connectors share bandwidth with the PCIEX16 slot.> When theM2B_CPU orM2C_CPU connector is populated, the PCIEX16 slot operates at up to x8 mode.[1]: https://www.gigabyte.com/Motherboard/X870E-AORUS-PRO-ICE-rev...\n \nreply",
      "Even with a Threadripper you're at the mercy of the motherboard design.I use ROG board that has 4 PCIe slots.  While each can physically seat an x16 card, only one of them has 16 lanes -- the rest are x4.  I had to demote my GPU to a slower slot in order to get full throughput from my 100GbE card.  All this despite having a CPU with 64 lanes available.\n \nreply",
      "The real PITA is when adding the NVMe disables the SATA ports you planned to use.\n \nreply",
      "New chipsets have become PCIe switches since broadcom rug pulled the PCIe switch market.\n \nreply",
      ">broadcom rug pulled the PCIe switch market.What does this mean? Did they jack up prices?\n \nreply",
      "Avego wanted PLX switches for enterprise storage, not low margin PC/server sales.Same thing that Avego did with Broadcom, LSI, Brocade etc... during the 2010's, buy a market leader, dump the parts that they didn't want, leaving a huge hole in the market.When you realize that Avego was the brand produced when KKR and Silver Lake bought the chip biz from Agilent, it is just the typical private equity play, buy your market position and sell off or shut down the parts you don't care about.\n \nreply",
      "The website is wheezing a bit. Here's a link to the video https://www.youtube.com/watch?v=xNebV8KIlZQ\n \nreply"
    ],
    "link": "https://interfacinglinux.com/2025/04/18/magewell-eco-m-2-hdmi-capture-card/",
    "first_paragraph": "Have you ever looked at an NVMe drive and thought to yourself, \u2018Hey, this would be 31.7% cooler with HDMI ports?\u2019 Yeah, me neither, but I\u2019ve always wanted to see how well (or if) one of these critters works on Linux.Today is that day.PCIe slots are becoming an endangered species on modern motherboards, and you know what? I get it. Most people just pop in a GPU and call it a day. Fortunately, those extra PCIe lanes tend to get repurposed as additional M.2 holes. Since that\u2019s \u201cjust\u201d an x4 connection, why not feed it something other than a storage device?In the box, you get two SHD-to-HDMI Type-A cables, a static-free bag, and the sense that Magewell expects you to figure the rest out.To the best of my knowledge exactly one company sells this elusive sliver of metal, MODDYI. They come in full and low profile.Mounting the SHD-to-HDMI Type-A cables was relatively straightforward, and the Magewell Eco pops in like any other M.2 device.Magewell is pretty good about drivers and the Eco series "
  },
  {
    "title": "Pipelining might be my favorite programming language feature (herecomesthemoon.net)",
    "points": 257,
    "submitter": "Mond_",
    "submit_time": "2025-04-21T12:16:16 1745237776",
    "num_comments": 222,
    "comments_url": "https://news.ycombinator.com/item?id=43751076",
    "comments": [
      "The author keeps calling it \"pipelining\", but I think the right term is \"method chaining\".Compare with a simple pipeline in bash:  grep needle < haystack.txt | sed 's/foo/bar/g' | xargs wc -l\n\nEach of those components executes in parallel, with the intermediate results streaming between them.  You get a similar effect with coroutines.Compare Ruby:  data = File.readlines(\"haystack.txt\")\n    .map(&:strip)\n    .grep(/needle/)\n    .map { |i| i.gsub('foo', 'bar') }\n    .map { |i| File.readlines(i).count }\n\nIn that case, each line is processed sequentially, with a complete array being created between each step.  Nothing actually gets pipelined.Despite being clean and readable, I don't tend to do it any more, because it's harder to debug.  More often these days, I write things like this:  data = File.readlines(\"haystack.txt\")\n  data = data.map(&:strip)\n  data = data.grep(/needle/)\n  data = data.map { |i| i.gsub('foo', 'bar') }\n  data = data.map { |i| File.readlines(i).count }\n\nIt's ugly, but you know what?  I can set a breakpoint anywhere and inspect the intermediate states without having to edit the script in prod.  Sometimes ugly and boring is better.\n \nreply",
      "> The author keeps calling it \"pipelining\", but I think the right term is \"method chaining\". [...] You get a similar effect with coroutines.The inventor of the shell pipeline, Douglas McIlroy, always understood the equivalency between pipelines and coroutines; it was deliberate. See https://www.cs.dartmouth.edu/~doug/sieve/sieve.pdf It goes even deeper than it appears, too. The way pipes were originally implemented in the Unix kernel was when the pipe buffer was filled[1] by the writer the kernel continued execution directly in the blocked reader process without bouncing through the scheduler. Effectively, arguably literally, coroutines; one process call the write function and execution continues with a read call returning the data.Interestingly, Solaris Doors operate the same way by design--no bouncing through the scheduler--unlike pipes today where long ago I think most Unix kernels moved away from direct execution switching to better support multiple readers, etc.[1] Or even on the first write? I'd have to double-check the source again.\n \nreply",
      "If you add in a call to \u201c.lazy\u201c it won\u2019t create all the intermediate arrays. There since at least 2.7. https://ruby-doc.org/core-2.7.0/Enumerator/Lazy.html\n \nreply",
      "> The author keeps calling it \"pipelining\", but I think the right term is \"method chaining\".I believe the correct definition for this concept is the Thrush combinator[0].  In some ML-based languages[1], such as F#, the |> operator is defined[2] for same:  [1..10] |> List.map (fun i -> i + 1)\n\nOther functional languages have libraries which also provide this operator, such as the Scala Mouse[3] project.0 - https://leanpub.com/combinators/read#leanpub-auto-the-thrush1 - https://en.wikipedia.org/wiki/ML_(programming_language)2 - https://fsharpforfunandprofit.com/posts/defining-functions/3 - https://github.com/typelevel/mouse?tab=readme-ov-file\n \nreply",
      "Syntactic sugar can sometimes fool us into thinking the underlying process is more efficient or streamlined. As a new programmer, I probably would have assumed that \"storing\" `data` at each step would be more expensive.\n \nreply",
      "It absolutely becomes very inefficient, though the threshold data set size varies according to context. Most languages don't have lightweight coroutines as an alternative (but see Lua!), so the convenient alternatives have larger fixed cost. Plus cache locality means cache utilization might be helpful, or even better, as opposed to switching back-and-for every data element, though coroutine-based approaches can also use buffering strategies, which not coincidentally is how pipes work.But, yes, naive call chaining like that is sometimes a significant performance problem in the real world. For example, in the land of JavaScript. One of the more egregious examples I've personally seen was a Bash script that used Bash arrays rather than pipelines, though in that case it had to do with the loss of concurrency, not data churn.\n \nreply",
      "I learned the term for this as a fluent interface. Pipelining is in my mind something quite different.\n \nreply",
      "I'm personally someone who advocates for languages to keep their feature set small and shoot to achieve a finished feature set quickly.However.I would be lying if I didn't secretly wish that all languages adopted the `|>` syntax from Elixir.```params|> Map.get(\"user\")|> create_user()|> notify_admin()```\n \nreply",
      "> I would be lying if I didn't secretly wish that all languages adopted the `|>` syntax from Elixir.This is usually the Thrush combinator[0], exists in other languages as well, and can be informally defined as:  f(g(x)) = g(x) |> f\n\n0 - https://leanpub.com/combinators/read#leanpub-auto-the-thrush\n \nreply",
      "We might be able to cross one more language off your wishlist soon, Javascript is on the way to getting a pipeline operator, the proposal is currently at Stage 2https://github.com/tc39/proposal-pipeline-operatorI'm very excited for it.\n \nreply"
    ],
    "link": "https://herecomesthemoon.net/2025/04/pipelining/",
    "first_paragraph": "Free-standing function call syntax considered kind of suboptimal.\nEpistemic status: Don\u2019t take it too seriously. Or do. idk, I can\u2019t stop you.\nPipelining might be my favorite programming language feature.\n              What is pipelining? Pipelining is the feature that allows you to omit a single argument from your\n              parameter list, by instead passing the previous value.\n            When I say pipelining, I\u2019m talking about the ability to write code like this:\n              As opposed to code like this. (This is not real Rust code. Quick challenge for the curious Rustacean, can\n              you explain why we cannot rewrite the above code like this, even if we import all of the symbols?)\n            \n              I honestly feel like this should be so obvious that it shouldn\u2019t even be up for debate. The first code\n              example\u2014with its nice \u2018pipelining\u2019 or \u2018method chaining\u2019 or whatever you want to call it\u2014it\n              just works. It can be read line-by-line. I"
  },
  {
    "title": "101 Basic Computer Games (github.com/maurymarkowitz)",
    "points": 16,
    "submitter": "sohkamyung",
    "submit_time": "2025-04-21T22:47:38 1745275658",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=43757341",
    "comments": [
      "If you are craving a BASIC fix, I highly recommend getting a DOS emulator like DosBos-X and just installing a copy of Quickbasic 4.5 (which has a compiler among other niceties over the original Microsoft QBASIC). You can easily find it on the Internet Archive.There are modern variants like QB64, but personally I find that BASIC really loses a lot of its appeal/flavor when you move from an interpretative language to a compiled one.https://dosbox-x.comI made this a while ago and it ran beautifully in DosBox on my Mac:https://specularrealms.com/q-basic\n \nreply",
      "Many happy memories of checking out books like this from my public library and trying to get the programs working on the C64. The BASIC dialect never matched exactly.\n \nreply",
      "BASIC, that is.\n \nreply"
    ],
    "link": "https://github.com/maurymarkowitz/101-BASIC-Computer-Games",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Type-in programs from the original 101 BASIC Computer Games, in their original DEC and Dartmouth dialects. No, this is *not* the same as BASIC Computer Games.\n      This folder contains the programs found in the March 1975 3rd printing of David Ahl's 101 BASIC Computer Games, published by Digital Equipment Corp. You can download all of the programs in a single archive using the Releases link to the right.Contrary to popular belief, 101 is not the same as the later and more famous BASIC Computer Games (BCG) published by Ahl through Creative Computing. A number of the games in this collection did not appear in BCG, and vice versa. In comparison to BCG's used of Microsoft-like BASIC, the source in this book spans several different and sometimes incompatible dialects. For instance, CAN-AM is in its original Dartmouth version, and BATTLE"
  },
  {
    "title": "Cekura (Formerly Vocera) (YC F24) Is Hiring (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-04-21T21:00:53 1745269253",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/cekura-2/jobs/xaoCPco-founding-engineer",
    "first_paragraph": "Testing & Observability for AI voice agentsWe're looking for a Founding Engineer who loves solving complex problems. As a member of the team, you'll lead the development and optimisation of our core AI agents specifically designed for testing and monitoring of conversational agents. You will collaborate directly with the founders and the customers to shape product direction and technological innovation.ResponsibilitiesMinimum RequirementsPreferred QualificationsWhy Join Cekura?Cekura is a Y Combinator\u2013backed startup redefining AI voice agent reliability. Founded by IIT Bombay alumni with research credentials from ETH Zurich and proven success in high-stakes trading, our team built Cekura to solve the cumbersome, error-prone nature of manual voice agent testing.We automate the testing and observability of AI voice agents by simulating thousands of realistic, real-world conversational scenarios\u2014from ordering food and booking appointments to conducting interviews. Our platform leverages c"
  },
  {
    "title": "LLM-powered tools amplify developer capabilities rather than replacing them (matthewsinclair.com)",
    "points": 200,
    "submitter": "matthewsinclair",
    "submit_time": "2025-04-21T14:36:14 1745246174",
    "num_comments": 139,
    "comments_url": "https://news.ycombinator.com/item?id=43752492",
    "comments": [
      "> Traditionally, coding involves three distinct \u201ctime buckets\u201d:>    Why am I doing this? Understanding the business problem and value>    What do I need to do? Designing the solution conceptually>    How am I going to do it? Actually writing the code> For decades, that last bucket consumed enormous amounts of our time. We\u2019d spend hours, days or weeks writing, debugging, and refining. With Claude, that time cost has plummeted to nearly zero.That last part is actually the easiest, and if you're spending inordinate amount of time there, that usually means the first two were not done well or you're not familiar with the tooling (language, library, IDE, test runner,...).There's some drudgery involved in manual code editing (renaming variable, extracting functions,...) but those are already solved in many languages with IDEs and indexers that automate them. And so many editors have programmable snippets support. I can genuinely say in all of my programming projects, I spent more time understanding the problem than writing code. I even spent more time reading libraries code than writing my own.The few roadblocks I have when writing code was solved by configuring my editor.\n \nreply",
      "I have a feeling that people who got bogged down in step 3 were the kind of people who write a lot of wordy corporate boilerplate with multiple levels of abstraction for every single thing.  AKA \"best practices\" type coding.For me the most important part of a project is working out the data structures and how they are accessed.  That's where the rubber meets the road, and is something that AI struggles with.  It requires a bit too high a level of abstract thinking and whole problem conceptualization for existing LLMs.  Once the data structures are set the coding is easy.\n \nreply",
      "> Once the data structures are set the coding is easy.I don't always find this, because there's a lot of \"inside baseball\" and accidental complexity in modern frameworks and languages. AI assist has been very helpful for me.I'm fairly polyglot and do maintenance on a lot of codebases. I'm comfortable with several languages and have been programming for 20 years but drop me in say, a Java Spring codebase and I can get the job done but I'm slow. Similarly, I'm fast with TypeScript/CDK or Terraform but slow with cfndsl because I skipped learning Ruby because I already knew Python. I know Javascript and the DOM and the principles of React but mostly I'm backend. So it hurts to dive into a React project X versions behind current and try to freshen it up because in practice you need reasonably deep knowledge of not just version X of these projects but also an understanding of how they have evolved over time.So I'm often in a situation where I know exactly what I want to do, but I don't know the idiomatic way to do it in a particular language or framework. I find for Java in particular there is enormous surface area and lots of baggage that has accumulated over the years which experienced Java devs know but I don't, e.g. all the gotchas when you upgrade from Spring 2.x to 3.x, or what versions of ByteBuddy work with blah maven plugin, etc.I used to often experience something like a 2x or 3x hit vs a specialised dev but with AI I am delivering close to parity for routine work. For complex stuff I would still try to pair with an expert.\n \nreply",
      "> I have a feeling that people who got bogged down in step 3 were the kind of people who write a lot of wordy corporate boilerplate with multiple levels of abstraction for every single thing. AKA \"best practices\" type coding.Or they're the kind of people who rushed to step 3 too fast, substantially skipping steps 1 and/or 2 (more often step 2). I've worked with a lot of people like that.\n \nreply",
      "While probably not useful for everyone, the best method for myself actually leverages that.I am using a modified form of TDD's red/green refactor, specifically with an LLM interface independent of my IDE.While I error on good code over prompt engineering, I used the need to submit it to both refine the ADT and domain tests, after creating a draft of those I submit them to the local LLM, and continue on with my own code.If I finish first I will quickly review the output to see if it produced simpler code or if my domain tests ADT are problematic. For me this avoids rat holes and head of line blocking.If the LLM finishes first, I approach the output as a code base needing a full refactor, keeping myself engaged with the code.While rarely is the produced code 'production ready' it often struggles when I haven't done my job.You get some of the benefits of pair programming without the risk of demoralizing some poor Jr.But yes, tradeoff analysis and choosing the least worst option is the part that LLM/LRMs will never be able to do IMHO.Courses for horses and nuance, not \"best practices\" as anything more than reasonable defaults that adjust for real world needs.\n \nreply",
      "Enterprise code with layers of abstraction isnt best practice. It\u2019s enterprise code.\n \nreply",
      "I would imagine that's why they had \"best practices\" in quotes. Lots of enterprisey things get pushed as a \"good practice\" to improve reuse (of things that will never be reused) or extensibility (of things that will never be extended) or modularity (of things that will never be separated).\n \nreply",
      "Enterprise development has particular problems you won't find in other environments, for instance having hundreds of different developers with widely varying levels of skill and talent, all collaborating together, often under immense time and budget pressure.The result ain't going to be what you get if you've got a focused group of 10x geniuses working on everything, but I think a lot of the aspects of \"enterprise development\" that people complain about is simply the result of making the best of a bad situation.I like Java, because I've worked with people who will fuck up repeatedly without static type checking.\n \nreply",
      "> for instance having hundreds of different developers with widely varying levels of skill and talentThat's a management problem. Meaning you assess that risk and try to alleviate it. A good solution like you say is languages with good type checking support. Another is code familiarity and reuse through frameworks and libraries. A third may be enforcing writing tests to speed up code review (and checklist rules like that).It's going to be boring, but boring is good at that scale.\n \nreply",
      "I can attest to that and see it as the reason why Angular is still so popular in the enterprise world - it has such a strong convention that no matter the rate of staff rotation the team can keep delivering.Meanwhile no two React projects are the same because they typically have several dependencies, each solving a small part of the problem at hand.\n \nreply"
    ],
    "link": "https://matthewsinclair.com/blog/0178-why-llm-powered-programming-is-more-mech-suit-than-artificial-human",
    "first_paragraph": "\n\n    We can't find the internet\n  \n    Attempting to reconnect\n    \n\n\n    Something went wrong!\n  \n    Hang in there while we get back on track\n    \n\n                  How LLM-powered programming tools amplify developer capabilities rather than replace them\n                \n                  Tags: braingasm, llm, programming, ai, claude, code, mech-suit, 2025\n\n\n\nPhoto by Screen Rant\n\n[ED: There\u2019s a lot of talk of replacing human programmers. Sure, some will go, but not all. My views on how this plays out have changed considerably over the last 2 months. 3 out of 5 hats.]\nLast month, I used Claude Code to build two apps: an MVP for a non-trivial backend agent processing platform and the early workings of a reasonably complex frontend for a B2C SaaS product. Together, these projects generated approximately 30k lines of code (and about the same amount again thrown away over the course of the exercise). The experience taught me something important about AI and software development that co"
  },
  {
    "title": "A new record for California's highest tree (sciencedaily.com)",
    "points": 5,
    "submitter": "docmechanic",
    "submit_time": "2025-04-19T12:49:53 1745066993",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.sciencedaily.com/releases/2025/04/250417145019.htm",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Open Codex \u2013 OpenAI Codex CLI with open-source LLMs (github.com/codingmoh)",
    "points": 48,
    "submitter": "codingmoh",
    "submit_time": "2025-04-21T17:57:03 1745258223",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=43754620",
    "comments": [
      "Without any changes, you can already use Codex with a remote or local API by setting base URL and key environment variables.\n \nreply",
      "Does it work for local though? It's my understanding this is still missing.\n \nreply",
      "If your favorite LLM inference program can run a Chat Completions API.\n \nreply",
      "Thanks for bringing that up - it's exactly why I approached it this way from the start.Technically you can use the original Codex CLI with a local LLM - if your inference provider implements the OpenAI Chat Completions API, with function calling, etc. included.But based on what I had in mind - the idea that small models can be really useful if optimized for very specific use cases - I figured the current architecture of Codex CLI wasn't the best fit for that. So instead of forking it, I started from scratch.Here's the rough thinking behind it:   1. You still have to manually set up and run your own inference server (e.g., with ollama, lmstudio, vllm, etc.).\n   2. You need to ensure that the model you choose works well with Codex's pre-defined prompt setup and configuration.\n   3. Prompting patterns for small open-source models (like phi-4-mini) often need to be very different - they don't generalize as well.\n   4. The function calling format (or structured output) might not even be supported by your local inference provider.\n\nCodex CLI's implementation and prompts seem tailored for a specific class of hosted, large-scale models (e.g. GPT, Gemini, Grok). But if you want to get good results with small, local models, everything - prompting, reasoning chains, output structure - often needs to be different.So I built this with a few assumptions in mind:   - Write the tool specifically to run _locally_ out of the box, no inference API server required.\n   - Use model directly (currently for phi-4-mini via llama-cpp-python).\n   - Optimize the prompt and execution logic _per model_ to get the best performance.\n\nInstead of forcing small models into a system meant for large, general-purpose APIs, I wanted to explore a local-first, model-specific alternative that's easy to install and extend \u2014 and free to run.\n \nreply",
      "i think this was made before that PR was merged into codex.\n \nreply",
      "Maybe it was part of the reason that they accepted the PR. The fork would happen anyways if they don't allow any LLM.A bit like how Android came after iPhone with open source implementation.\n \nreply",
      "Good correction - while the SDK used has supported changing the API through environment variables for a long time, Codex only recently added Chat Completions support recently.\n \nreply",
      "curious why you went with Phi as the default models, that seems a bit unusual compared to current trends\n \nreply",
      "I went with Phi as the default model because, after some testing, I was honestly surprised by how high the quality was relative to its size and speed. The responses felt better in some reasoning tasks-but were running on way less hardware.What really convinced me, though, was the focus on the kinds of tasks I actually care about: multi-step reasoning, math, structured data extraction, and code understanding.There\u2019s a great Microsoft paper on this: \"Textbooks Are All You Need\" and solid follow-ups with Phi\u20112 and Phi\u20113.\n \nreply",
      "agreed - thought the qwen2.5-coder was kind of standard non-reasoning small line of coding models right now\n \nreply"
    ],
    "link": "https://github.com/codingmoh/open-codex",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Fully open-source command-line AI assistant inspired by OpenAI Codex, supporting local language models.\n      Lightweight coding agent that runs in your terminalbrew tap codingmoh/open-codex\nbrew install open-codexOpen Codex is a fully open-source command-line AI assistant inspired by OpenAI Codex, supporting local language models like phi-4-mini.No API key is required. Everything runs locally.Supports:Once installed, you can use the open-codex CLI globally.\u2705 Codex suggests a shell command\n\u2705 Asks for confirmation / add to clipboard / abort\n\u2705 Executes if approvedAll models run locally. Commands are only executed after explicit approval.PRs welcome! Ideas, issues, improvements \u2014 all appreciated.MIT\u2764\ufe0f Built with love and caffeine by codingmoh.\n        Fully open-source command-line AI assistant inspired by OpenAI Codex, supporting loca"
  },
  {
    "title": "Pydrofoil: Accelerating Sail-based instruction set simulators (arxiv.org)",
    "points": 11,
    "submitter": "luu",
    "submit_time": "2025-04-21T21:52:29 1745272349",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://arxiv.org/abs/2503.04389",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "AI assisted search-based research works now (simonwillison.net)",
    "points": 139,
    "submitter": "simonw",
    "submit_time": "2025-04-21T14:15:17 1745244917",
    "num_comments": 65,
    "comments_url": "https://news.ycombinator.com/item?id=43752262",
    "comments": [
      "The various deep research products don't work well for me.  For example I asked these tools yesterday, \"How many unique NFL players were on the roster for at least one regular season game during the 2024 season?  I'd like the specific number not a general estimate.\"I as a human know how to find this information.  The game day rosters for many NFL teams are available on many sites.  It would be tedious but possible for me to find this number.  It might take an hour of my time.But despite this being a relatively easy research task all of the deep research tools I tried (OpenAI, Google, and Perplexity) completely failed and just gave me a general estimate.Based on this article I tried that search just using o3 without deep research and it still failed miserably.\n \nreply",
      "That is an excellent prompt to tuck away in your back pocket and try again future iterations of this technology. It's going to be an interesting milestone when or if any of these systems get good enough at comprehensive research to provide a correct answer.\n \nreply",
      "Is it accurate that there are 544 rosters? If so, even at 2 minutes a roster isn't that days of work, even if you coded something? How would you go about completing this task in 1 hour as a human? (also chatgpt 4.1 gave me 2,503 and it said it used the NFL 2024 fact book)\n \nreply",
      "I bet these models could create a python program that does this\n \nreply",
      "Maybe eventually, but I bet it\u2019s not going to work with less than 30 minutes of effort on your part.If \u201cIt might take an hour of my time.\u201d to get the correct answer then there\u2019s a lower bond for trying a shortcut that might not work.\n \nreply",
      "This is just a bad match to the capabilities. What you are actually looking for is analysis, similar in nature to what a data scientist may do.The deep research capabilities are much better suited to more qualitative research / aggregation.\n \nreply",
      "First person that makes a good exact aggregation AI will make so much money...Precise aggregation is what so many juniors do in so many fields of work it's not even funny...\n \nreply",
      "> The deep research capabilities are much better suited to more qualitative research / aggregation.Unfortunately sentiment analysis like \"Tell me how you feel about how many players the NFL has\" is just way less useful than: \"Tell me how many players the NFL has.\"\n \nreply",
      "I think it's important to keep tabs on things that LLM systems fail at (or don't do well enough on) and try to notice when their performance rises above that bar.Gemini 2.5 Pro and o3/o4-mini seem to have crossed a threshold for a bunch of things (at least for me) in the last few weeks.Tasteful, effective use of the search tool for o3/o4-mini is one of those. Being able to \"reason\" effectively over long context inputs (particularly useful for understanding and debugging larger volumes of code) is another.\n \nreply",
      "One issue I can find with this workflow is tunnel vision, making ill informed decision because of the lack of surrounding information. I often skim books because even if I don't retain the content, I can have a mental map that can help me find further information when I need them. I wouldn't try to construct a complete answer to a question with just this amount of information, but I will use that map to quickly locate the source and have more information to synthesize the answer.One could use the above workflow in the same way and argues that natural language search is more intuitive than keyword based search. But I don't think that brings any meaningful productivity improvement.> Being able to \"reason\" effectively over long context inputs (particularly useful for understanding and debugging larger volumes of code) is another.Any time I saw this \"wish\" pop up, my suggestion is to try a disassembler to reverse engineer some binary to really understand the problem of coming up with a theory of a program (based on Naur's definition).  Individual statements are always clear (programming language are formal and have no ambiguity). The issue is grouping them, unambiguously define the semantic of these groups, and find the links between them, recursively.Once that's done, what you'll have is a domain. And you could have skipped the whole thing by just learning the domain from a domain expert. So the only reason to do this is because the code doesn't really implement the domain (bugs) or it's hidden purposefully. So the most productive workflow there is to learn the domain first to find discrepancy (first case) or focus yourself on the missing part (second case). In the first case, the easiest approach is writing tests, and the more complete one is to do a formal verification of the software.\n \nreply"
    ],
    "link": "https://simonwillison.net/2025/Apr/21/ai-assisted-search/",
    "first_paragraph": "21st April 2025For the past two and a half years the feature I\u2019ve most wanted from LLMs is the ability to take on search-based research tasks on my behalf. We saw the first glimpses of this back in early 2023, with Perplexity (first launched December 2022, first prompt leak in January 2023) and then the GPT-4 powered Microsoft Bing (which launched/cratered spectacularly in February 2023). Since then a whole bunch of people have taken a swing at this problem, most notably Google Gemini and ChatGPT Search.Those 2023-era versions were promising but very disappointing. They had a strong tendency to hallucinate details that weren\u2019t present in the search results, to the point that you couldn\u2019t trust anything they told you.In this first half of 2025 I think these systems have finally crossed the line into being genuinely useful.First came the Deep Research implementations\u2014Google Gemini and then OpenAI and then Perplexity launched products with that name and they were all impressive: they coul"
  }
]