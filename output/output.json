[
  {
    "title": "New sphere-packing record stems from an unexpected source (quantamagazine.org)",
    "points": 221,
    "submitter": "pseudolus",
    "submit_time": "2025-07-07T18:19:13 1751912353",
    "num_comments": 96,
    "comments_url": "https://news.ycombinator.com/item?id=44493196",
    "comments": [
      "I have trouble explaining to my parents how my job is a real thing. I can only imagine trying to explain \u2018I study shapes, but only ones that don\u2019t jut inwards\u2019.reply",
      "I've found it's best to explain my job using unintelligible jargon.There are three choices, really:You can give a quick explanation in terms they understand, which makes your job sound easy and makes them wonder how anybody gets paid to do it.You can explain what you do and why it's important in terms they understand, but it'll take so long they'll get bored and wish they hadn't asked.Or you can give a quick explanation using jargon that they don't understand, which will leave them bored but impressed, which is the best of the bad options.reply",
      "When I meet people who immediately use hyper-specific jargon with strangers, I either distrust them, or assume they\u2019re not emotionally intelligent (because it\u2019s a choice demonstrates little respect for the person they\u2019re addressing). It also projects that they may be compensating for some emotional insecurity on their own end, trying to assert intellectual \u201csuperiority\u201d in some way.The first option (explaining things simply) might make your job sound easy to a very small minority of extremely uneducated, under-stimulated people, who also have unaddressed insecurities around their own intelligence. But that\u2019s not most humans.Moderately-to-very intelligent people appreciate how difficult (and useful) it is to explain complex things simply. Hell, most \u201cdumb\u201d people understand, recognize, and appreciate this ability. Honestly, I think not appreciating simple explanations indicates both low mathematical/logical and social/emotional intelligence. Which makes explaining things simply a useful filter for, well\u2026 people that I wouldn\u2019t get along with anyway.With all that said, I prefer to first explain my job in an \u201cexplain like I\u2019m 5\u201d style and, if the other party indicates interest, add detail and jargon, taking into account related concepts that may already be familiar to them. If you take them into account, they won\u2019t get bored when you go into detail.reply",
      "> or assume they\u2019re not emotionally intelligent (because it\u2019s a choice demonstrates little respect for the person they\u2019re addressing)Intelligence, in the traditional sense, also involves understanding when to give up. Part of \"emotional intelligence\" is judging whether the other party actually cares about what you're about to say.reply",
      "> \nWhen I meet people who immediately use hyper-specific jargon with strangers, I either distrust them, or assume they\u2019re not emotionally intelligent (because it\u2019s a choice demonstrates little respect for the person they\u2019re addressing).For me, it's quite the opposite: such a choice demonstrates that they their prior is that I'm sufficiently smart and knowledgable to be likely able to understand this explanation - which I rather consider to be a praise. :-)reply",
      "I think \"with strangers\" is the important bit. If a nuclear engineer is talking to some lay person and uses hyper specific jargon, then grandparent is correct.\nIf you've established a shared competency with the person, and are therefor no longer total strangers, that's totally different.reply",
      ">extremely uneducated, under-stimulated people, who also have unaddressed insecurities around their own intelligence. But that\u2019s not most humans.This isn't going to be most humans you encounter if you're in the HN demographic, but that's a bubble. It does describe most people in the world.reply",
      "This is, in a word, nonsense.",
      "The correct option is to treat such conversations as a protocol with a negotiation at the onset.reply",
      "SYN ACKreply"
    ],
    "link": "https://www.quantamagazine.org/new-sphere-packing-record-stems-from-an-unexpected-source-20250707/",
    "first_paragraph": "\nAn editorially independent publication supported by the Simons Foundation.\n\nGet the latest news delivered to your inbox.\nCreate a reading list by clicking the Read Later icon next to the articles you wish to save.Type search term(s) and press enterPopular\n                                    SearchesJuly 7, 2025DVDP for\u00a0Quanta MagazineStaff WriterJuly 7, 2025In math, the search for optimal patterns never ends. The sphere-packing problem \u2014 which asks how to cram balls into a (high-dimensional) box as efficiently as possible \u2014 is no exception. It has enticed mathematicians for centuries and has important applications in cryptography, long-distance communication and more.It\u2019s deceptively difficult. In the early 17th century, the physicist Johannes Kepler showed that by stacking three-dimensional spheres the way you would oranges in a grocery store, you can fill about 74% of space. He conjectured that this was the best possible arrangement. But it would take mathematicians nearly 400 years"
  },
  {
    "title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping (disneyresearch.com)",
    "points": 36,
    "submitter": "jw1224",
    "submit_time": "2025-07-07T22:11:58 1751926318",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44495154",
    "comments": [
      "This reminds me of Stand-up Maths and Steve Mould creating a puzzle that has two solutions with two different images: https://www.youtube.com/watch?v=b5nElEbbnfUreply",
      "I love how many optical illusions have been revisited with new generative techniquesreply",
      "Disney invited me to talk about my GenAI startup and research in front of a bunch of their execs across ABC, ESPN, Pixar, Streaming, etc. All of their folks were super nice and gracious to our small startup except for one.Steve May basically scoffed at how little my small team could accomplish. Mind you we were using mocapped skeletal animation and object animation curves to fully steer video diffusion over a year and a half ago. Before image to video modalities. He picked apart our training and engineering and gloated that they could do better.The incident is seared into my brain.I can't help but think of Disney as the Empire and Pixar as the Death Star.reply",
      "> gloated that they could do better.did they? Anything good come from that meeting?reply",
      "So the classic Silicon Valley brain rape is actually real?reply"
    ],
    "link": "https://studios.disneyresearch.com/2025/06/09/lookingglass-generative-anamorphoses-via-laplacian-pyramid-warping/",
    "first_paragraph": "June 10, 2025CVPR 2025\u00a0Pascal Chang (DisneyResearch|Studios/ETH Joint PhD)Sergio Sancho (DisneyResearch|Studios/ETH Joint PhD)Jingwei Tang (DisneyResearch|Studios)Markus Gross (DisneyResearch|Studios)Vinicius Azevedo (DisneyResearch|Studios)Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of these mathematical devices can be traced back to as early as the 17th century, they are only interpretable when viewed from a specific vantage point and tend to lose meaning when seen normally. In this paper, we revisit these famous optical illusions with a generative twist. With the help of latent rectified flow models, we propose a method to create anamorphic images that still retain a valid interpretation when viewed directly. To this end, we introduce La"
  },
  {
    "title": "Mercury: Ultra-fast language models based on diffusion (arxiv.org)",
    "points": 378,
    "submitter": "PaulHoule",
    "submit_time": "2025-07-07T12:31:08 1751891468",
    "num_comments": 153,
    "comments_url": "https://news.ycombinator.com/item?id=44489690",
    "comments": [
      "A good chance to bring up something I've been flagging to colleagues for a while now: with LLM agents we are very quickly going to become even more CPU bottlenecked on testing performance than today, and every team I know of today was bottlenecked on CI speed even before LLMs. There's no point having an agent that can write code 100x faster than a human if every change takes an hour to test.Maybe I've just got unlucky in the past, but in most projects I worked on a lot of developer time was wasted on waiting for PRs to go green. Many runs end up bottlenecked on I/O or availability of workers, and so changes can sit in queues for hours, or they flake out and everything has to start again.As they get better coding agents are going to be assigned simple tickets that they turn into green PRs, with the model reacting to test failures and fixing them as they go. This will make the CI bottleneck even worse.It feels like there's a lot of low hanging fruit in most project's testing setups, but for some reason I've seen nearly no progress here for years. It feels like we kinda collectively got used to the idea that CI services are slow and expensive, then stopped trying to improve things. If anything CI got a lot slower over time as people tried to make builds fully hermetic (so no inter-run caching), and move them from on-prem dedicated hardware to expensive cloud VMs with slow IO, which haven't got much faster over time.Mercury is crazy fast and in a few quick tests I did, created good and correct code. How will we make test execution keep up with it?reply",
      "> Maybe I've just got unlucky in the past, but in most projects I worked on a lot of developer time was wasted on waiting for PRs to go green.I don't understand this. Developer time is so much more expensive than machine time. Do companies not just double their CI workers after hearing people complain? It's just a throw-more-resources problem. When I was at Google, it was somewhat common for me to debug non-deterministic bugs such as a missing synchronization or fence causing flakiness; and it was common to just launch 10000 copies of the same test on 10000 machines to find perhaps a single digit number of failures. My current employer has a clunkier implementation of the same thing (no UI), but there's also a single command to launch 1000 test workers to run all tests from your own checkout. The goal is to finish testing a 1M loc codebase in no more than five minutes so that you get quick feedback on your changes.> make builds fully hermetic (so no inter-run caching)These are orthogonal. You want maximum deterministic CI steps so that you make builds fully hermetic and cache every single thing.reply",
      "I was also at Google for years. Places like that are not even close to representative. They can afford to just-throw-more-resources, they get bulk discounts on hardware and they pay top dollar for engineers.In more common scenarios that represent 95% of the software industry CI budgets are fixed, clusters are sized to be busy most of the time, and you cannot simply launch 10,000 copies of the same test on 10,000 machines. And even despite that these CI clusters can easily burn through the equivalent of several SWE salaries.> These are orthogonal. You want maximum deterministic CI steps so that you make builds fully hermetic and cache every single thing.Again, that's how companies like Google do it. In normal companies, build caching isn't always perfectly reliable, and if CI runs suffer flakes due to caching then eventually some engineer is gonna get mad and convince someone else to turn the caching off. Blaze goes to extreme lengths to ensure this doesn't happen, and Google spends extreme sums of money on helping it do that (e.g. porting third party libraries to use Blaze instead of their own build system).In companies without money printing machines, they sacrifice caching to get determinism and everything ends up slow.reply",
      "I\u2019m at Google today and even with all the resources, I am absolutely most bottlenecked by the Presubmit TAP and human review latency. Making CLs in the editor takes me a few hours. Getting them in the system takes days and sometimes weeks.reply",
      "Presumably the \"days and sometimes weeks\" thing is entirely down to human review latency?reply",
      "Most of my experience writing concurrent/parallel code in (mainly) Java has been rewriting half-baked stuff that would need a lot of testing with straightforward reliable and reasonably performant code that uses sound and easy-to-use primitives such as Executors (watch out for teardown though), database transactions, atomic database operations, etc.  Drink the Kool Aid and mess around with synchronized or actors or Streams or something and you're looking at a world of hurt.I've written a limited number of systems that needed tests that probe for race conditions by doing something like having 3000 threads run a random workload for 40 seconds.  I'm proud of that \"SuperHammer\" test on a certain level but boy did I hate having to run it with every build.reply",
      "Developer time is more expensive than machine time, but at most companies it isn't 10000x more expensive. Google is likely an exception because it pays extremely well and has access to very cheap machines.Even then, there are other factors:* You might need commercial licenses. It may be very cheap to run open source code 10000x, but guess how much 10000 Questa licenses cost.* Moores law is dead Amdahl's law very much isn't. Not everything is embarrassingly parallel.* Some people care about the environment. I worked at a company that spent 200 CPU hours on every single PR (even to fix typos; I failed to convince them they were insane for not using Bazel or similar). That's a not insignificant amount of CO2.reply",
      "> Moores law is dead Amdahl's lawYes, but the OP specifically is talking about CI for large numbers of pull requests, which should be very parallelizable (I can imagine exceptions, but only with anti-patterns, e.g. if your test pipeline makes some kind of requests to something that itself isn't scalable).reply",
      "Actually, OP was talking about the throughput of running on a large number of pull requests and the latency of running on a single pull request. The latter is not necessarily parallelizable.reply",
      "That's solvable with modern cloud offerings - Provision spot instances for a few minutes and shut them down afterwards. Let the cloud provider deal with demand balancing.I think the real issue is that developers waiting for PRs to go green are taking a coffee break between tasks, not sitting idly getting annoyed. If that's the case you're cutting into rest time and won't get much value out of optimizing this.reply"
    ],
    "link": "https://arxiv.org/abs/2506.17298",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "My first verified imperative program (markushimmel.de)",
    "points": 122,
    "submitter": "TwoFx",
    "submit_time": "2025-07-07T17:58:04 1751911084",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=44492986",
    "comments": [
      "Naturally, this proof only works for arbitrary-precision integers: when you use fixed-precision integers, the algorithm will wrongfully report \"false\" for arrays like e.g. [INT_MIN, -1] or (if you insist on C semantics) [UINT_MAX, 1].Hopefully the proof would break if one tried to transfer it over?reply",
      "> the algorithm will wrongfully report \"false\" for arrays like e.g. [INT_MIN, -1]If you have INT_MIN along with any other negative number in the array then your program has undefined behavior in C. Signed integer overflow is UB (but unsigned overflow is not).reply",
      "> If you have INT_MIN along with any other negative number in the array then your program has undefined behavior in C.What? Why? There\u2019s no addition needed to solve this problem. The example implementation does invert each element, which is undefined for INT_MIN, but it would be trivial to just skip INT_MIN elements (since their additive inverse is never in the set).reply",
      "Yes. The problem here is the -x operation. If INT_MIN is in the array, then the negation operation itself is UB. As you say, the fix is to skip values equal to INT_MIN; it's not possible that its negation is in the array, as that number is not representable.Rust is only a little better. With default settings, it will panic if isize::MIN is in the input slice in a debug build, and in a release build will incorrectly return true if there are two such values in the input. But in C you'll get unicorns and rainbows.reply",
      "> the algorithm will wrongfully report \"false\" for arrays like e.g. [INT_MIN, -1]`INT_MIN + -1` is not 0 so it should report false in that case.For UINT_MAX, the algorithm would need to be reconsidered, though, since it's written with signed integers in mind.> Hopefully the proof would break if one tried to transfer it over?Hopefully. The proof would have to be modified to account for the actual types. If you're using bounded integers you'd need to write a different proof.reply",
      "> For UINT_MAX, the algorithm would need to be reconsidered, though, since it's written with signed integers in mind.The algorithm is written assuming that unary - produces the additive inverse. That is also true for C's unsigned integers. -1U == UINT_MAX, -UINT_MAX == 1U. It Just Works.reply",
      "INT_MIN - 1 is undefined behavior in C.reply",
      "This is the strength of typing, right?If I can specify the type of my input I can ensure the verification.reply",
      "But false is the correct result for those cases. Addition is addition and overflow is undefined (= can assume that doesn't happen), it's not addition modulo 2^n.reply",
      "We are not talking about C here. Imagine it was e.g. Java, or C#, or Rust in release mode, or heck, even Lean itself but with fixed-precision integers.reply"
    ],
    "link": "https://markushimmel.de/blog/my-first-verified-imperative-program/",
    "first_paragraph": "\n\n\nJuly 6, 2025\n\n\n\n\n        \n          13 minute read\n        \n      \nOne of the many exciting new features in the upcoming Lean 4.22 release is a\npreview of the new verification infrastructure for proving properties of imperative\nprograms. In this post, I\u2019ll take a first look at this feature, show a simple\nexample of what it can do, and compare it to similar tools.We will use the following simple programming task as an example throughout the\npost: given a list of integers, determine if there are two integers at distinct\npositions in the list that sum to zero.For example, given the list [1, 0, 2, -1], the result should be true, because\n\\(1 + (-1) = 0\\), and given the list [0, 0], the result should also be true,\nbut given the list [1, 0, -2], the result should be false.The simplest way to solve this is to use two nested loops to iterate over all\npairs of distinct positions. This takes quadratic time, which is inefficient. There are\nseveral ways to improve upon this. Here is the one we w"
  },
  {
    "title": "The chemical secrets that help keep honey fresh for so long (bbc.com)",
    "points": 72,
    "submitter": "bookofjoe",
    "submit_time": "2025-07-04T11:04:41 1751627081",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=44463429",
    "comments": [
      "Same principle, reducing water activity (aw), is how Nutella keeps fresh for so long without refrigeration: its water activity is even lower than honey. Most bacteria need an aw of around 0.85 to thrive, Nutella\u2019s aw is around 0.4, honey\u2019s around 0.5-0.6. Peanut butter is 0.7, so it stays fresh for relatively long, too.Here\u2019s the definition of water activity from FDA:https://www.fda.gov/inspections-compliance-enforcement-and-c...reply",
      "The story misses that lactic acid bacteria are fairly common in honey and seem to be out competing other bacteria and have anti microbial effects.https://pmc.ncbi.nlm.nih.gov/articles/PMC7949542/reply",
      "This article is missing what I think is a pretty important PSA on the topic of bacteria in honey:Honey commonly contains small amounts of the anaerobic bacteria Clostridium botulinum, which causes botulism.This is why you should not feed honey to infants, because their immune systems cannot safely handle any amount of it yet. Even though the levels apparently are small enough for the rest of humans to consume worry-free.reply",
      "same with immunocompromised folks, though all that only applies to raw  honey; pasteurized honey is more common in grocery stores and totally safereply",
      "Recently went through first aid training and the instructor claimed putting honey on wounds would help them heal faster.(Sample size 1) I tried it on myself and a wound that was stubborn about healing was better very quickly.reply",
      "Really high sugar concentrations will pop the cells of any simple organisms.reply",
      "No, low.  High sugar concentrations mean low water activity, which osmotically pumps water out of cells, not into them, so they shrivel rather than popping.reply",
      "I like to imagine it as humans stranded in a strange land where all the geography is dry cake. And only dry cake.reply",
      "</article>reply",
      "This story can be summarized as \"Low water activity and low pH keep honey fresh permanently.\"  The other 14 paragraphs are just filler.  Moreover, even that summary is factually incorrect; low water activity and low pH don't come close to explaining honey's astounding shelf life, which amounts to centuries in many cases.https://en.wikipedia.org/wiki/Honey#Preservation in particular mentions gluconic acid and hydrogen peroxide produced by the bees' glucose oxidase, and https://en.wikipedia.org/wiki/Honey#Medical_use_and_research also mentions its content of methylglyoxal, which damages DNA and cross-links proteins somewhat like formaldehyde, thus killing microorganisms; m\u00e3nuka honey is required to contain at least 85mg/kg of methylglyoxal, according to https://en.wikipedia.org/wiki/M%C4%81nuka_honey.  I suspect that there is a great deal more research on the topic.It's disappointing to see such a low-quality article on the BBC website; I generally regard the BBC as a reliable source.reply"
    ],
    "link": "https://www.bbc.com/future/article/20250701-the-chemical-secrets-that-help-keep-honey-fresh-for-so-long",
    "first_paragraph": "Honey is a natural sweetener, and bacteria loves to feast on sugar. But honey is remarkably resistant to spoilage. What's behind its ability to beat the bugs?Most jarred delights have a limited shelf life \u2013 they're just one mucky spoon dip away from growing a luscious crop of mould or a thriving colony of bacteria. But there are certain foods with peculiar staying power, capable of staying edible for years.Honey is one of these magical substances. In a sealed environment, while the golden stuff may crystallise, turning thick and chunky, it will not go off. This persistent ability to resist decay is down to honey's chemistry, and the way in which it is made.When we say that food has spoiled, what we actually mean is that something else\u00a0has got to it first,\u00a0something microscopic. Bacteria, fungi, and moulds are present\u00a0in at least low numbers in many foods, and a number of the procedures humans use to preserve food are designed to discourage these creatures consuming it.Many of these mic"
  },
  {
    "title": "What Microchip doesn't (officially) tell you about the VSC8512 (serd.es)",
    "points": 27,
    "submitter": "ahlCVA",
    "submit_time": "2025-07-04T23:10:11 1751670611",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://serd.es/2025/07/04/Switch-project-pt3.html",
    "first_paragraph": "This is part 3 of my ongoing series about LATENTRED, my project to create an open source 1U managed Ethernet switch\nfrom scratch.Here\u2019s a quick, or maybe not-so-quick, update about the PHY on the line card and some of my troubles (and solutions).And probably more internal details than you want to know, but hey - maybe this will be useful to somebody. Not a lot of pretty pictures either. One day I do want to decap one for fun, but I have better things to do with my time than try to fully netlist-extract a large 65nm IC just to figure out some undocumented registers.I chose the VSC8512 as the PHY for the line card because it had a QSGMII interface (which used much less pins than SGMII), but also (so I thought) had a fully open datasheet with no NDAs required.Typically I\u2019m used to seeing parts that are either fully NDA\u2019d (with a trivial product brief and no register info etc, or sometimes not even pinout info, public), or fully open. Sometimes I find components with specific documents res"
  },
  {
    "title": "Launch HN: Morph (YC S23) \u2013 Apply AI code edits at 4,500 tokens/sec",
    "points": 148,
    "submitter": "bhaktatejas922",
    "submit_time": "2025-07-07T14:40:45 1751899245",
    "num_comments": 101,
    "comments_url": "https://news.ycombinator.com/item?id=44490863",
    "comments": [
      "> 1) Raw inference speed matters more than incremental accuracy gains for dev UX\u2014agree or disagree?I know you are trying to generate some controversy/visibility, but i think if we are being transparent here, you know this is wrong. People prefer using larger (or reasoning) models, with much bigger diff in tok/sec just for quality in coding, it comes first. Even if i have a big edit to apply, like 5k tokens, 200-300ms of difference in edit time are nothing. Edit speed is definitely not a bottleneck for dev UX, quality is.   A dev who wants to save 200ms every code change over quality is someone who well, i cannot relate. If im using 1-2 agents in parallel, most of the time the edits are already applied while im reviewing code from the other agents. But again maybe that's just me.Speaking of quality, how do you measure it? Do you have any benchmarks? How big is the difference in error rate between the fast and large model?reply",
      "I do find that having inference happen ~50% faster is much more valuable to my workflow than a single digit accuracy increase. If I'm going to have to check that the changes are correct anyways, getting more iterations in faster feels much better than incremental accuracy.There's definitely a tipping point though. If the accuracy gains are so high that I can check its work less carefully or less often, the benefits of inference speed are effectively nil.reply",
      "exactly. The point is that none of the users even realize a model is doing the apply - it should be so accurate and fast that it feels like its not therereply",
      "Agreed. Sonnet 4 is supposedly better than Sonnet 3.5, but in Cursor 3.5 is much faster so that's what I usereply",
      "As far as i understand, this is not +-300ms. It is 300ms vs. 10 sec or something. That is a huge difference. I personally find the time to wait for these larger models a limiting factor. It\u2019s also probably a resource waste for fairly simple task like this. (Compared to the general function approximation of the llms)But I honestly feel like the task of smartly applying edits falls somewhat within traditional coding tasks. What about it is so difficult it could not be done with a smart diffing algorithm?reply",
      "it's a bit unclear why a model works best here. in short - smart diffing is edge case hell and you'll never capture all of themreply",
      "you misunderstood. its 300ms just for the apply model, the model that takes your coding models output (eg sonnet) and figures out where the code should be changed in the file. Cursor has its own, and claude uses a different technique with strings as well. So its 10sec vs 10sec +300ms using your analogyreply",
      "Their selling point is to be a more open version of what cursor has. So the alternative is to use a full llm. So it is 10s+ 10s vs 10s+ 300msreply",
      "yep!reply",
      "For someone not heavy in this space. I use GH copilot at work. I might switch to cursor. I am not into the details of the tools just care does it help me or not. For us it might be worth having an easier to understand value proposition.It may take a bit if explaining and that's OK. But the big question is as someone doing my enterprise microservice who isn't heavy into AI why do I switch to you.reply"
    ],
    "link": "item?id=44490863",
    "first_paragraph": ""
  },
  {
    "title": "I used o3 to profile myself from my saved Pocket links (noperator.dev)",
    "points": 307,
    "submitter": "noperator",
    "submit_time": "2025-07-07T12:44:19 1751892259",
    "num_comments": 127,
    "comments_url": "https://news.ycombinator.com/item?id=44489803",
    "comments": [
      "After reading this I realized I also have an archive of my pocket account (4200 items), so tried the same prompt with o3, gemini 2.5 pro, and opus 4:- chatgpt UI didn't allow me to submit the input, saying it's too large. Although it was around 80k tokens, less than o3's 200k context size.- gemini 2.5 pro: worked fine for personality and interest related parts of the profile, but it failed the age range, job role, location, parental status with incorrect perdictions.- opus 4: nailed it and did a more impressive job, accurately predicted my base city (amsterdam), age range, relationship status, but didn't include anything about if I'm a parent or not.Both gemini and opus failed in predicting my role, probably understandably. Although I'm a data scientist, I read a lot about software engineering practices because I like writing software and since I don't have the opportunity at work to do this kind of work, I code for personal projects, so I need to learn a lot about system design, etc. Both models thought I'm a software engineer.Overall it was a nice experiment. Something I noticed is both models mentioned photography as my main hobby, but if they had access to my youtube watch history, they'd confidently say it's tennis. For topics and interests that we usually watch videos rather than reading articles about, would be interesting to combine the youtube watch history with this pocket archive data (although it would be challenging to get that data).reply",
      "You should be able to use Google Takeout to get all of your YouTube data, including your watch history.This article is a nice example of someone using it:> When I downloaded all my YouTube data, I\u2019ve noticed an interesting file included. That file was named watch-history and it contained a list of all the videos I\u2019ve ever watched.https://blog.viktomas.com/posts/youtube-usage/Of course as an European it's a legal obligation for companies to give you access, but I think Google Takeout works worldwide?reply",
      "Yes I've done this in USA. pretty neat. I have it on my todo list to parse over it and find all the music videos I've watched 3 or more times to archive them.reply",
      "https://archive.zhimingwang.org/blog/2014-11-05-list-youtube... might be of use along with https://github.com/yt-dlp/yt-dlp, might just grab it all and prune later due to rot and availability issues over time within YT.reply",
      "You should take this as a sign, and shoot for SWE jobs - given your interest.What you do at work today doesn't mean you can't switch to a related ladder.reply",
      "Sometimes it\u2019s nice for hobbies to remain hobbiesreply",
      "I believed this, which is what made me avoid computer science in college; I wanted to avoid ruining my favorite hobby.After a few years post graduation, where I wasn't sure what I wanted to do and I floundered to find a career, I decided to give software development a try, and risk ruining my favorite hobby.Definitely the best decision I could have made. Now people pay me a lot of money to do the thing I love to do the most... what's not to love? 20 years later, it I still my favorite hobby, and they keep paying me to do it.reply",
      "I think it heavily depends on who you're working for.If they get out of the way and let you do the thing you love how you want to do it you'll get good results for you and them.If they treat you like a cog in a machine and assume they need to carrot and stick you into doing things because you might not really want to be there, you'll be miserable.reply",
      "My first software job I enjoyed. My 2nd/current job I enjoy everything except the actual work. Too much beuracracy, but it hasn't ruined my love for the craft yet. Oh well, I'm building some other skills I didn't know I had in me.reply",
      "Sure, of course. Sometimes it works out to follow your passion into a career. I was objecting to the apparent premise that that\u2019s _always_ what you should do.reply"
    ],
    "link": "https://noperator.dev/posts/o3-pocket-profile/",
    "first_paragraph": ""
  },
  {
    "title": "The Miyawaki Method of micro-forestry (futureecologies.net)",
    "points": 100,
    "submitter": "zeristor",
    "submit_time": "2025-07-05T08:02:56 1751702576",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=44470942",
    "comments": [
      "Afforestation of any kind is good, but we can't forget the importance of large, contiguous forest ecosystems too. Segmentation of forest can threaten some species that rely on not being near the edge.reply",
      "Related: The Japanese method of creating forests comes to Mexicohttps://news.ycombinator.com/item?id=44013933(2 months ago, only 4 comments)reply",
      "Podcast.  There is a transcript, which is in fine print and not concise.reply",
      "Just saw a really interesting Nova centering on Crowther Labs at ETH (recently disbanded).  https://www.science.org/doi/10.1126/science.aax0848 was held up as encouraging unsustainable reforestation for greenwashing PR.But the most interesting part was a segment covering how tree roots engage in aggressive recruitment and exchange of resources with an underground biome of bacteria and fungi.reply",
      "Works well in some ecosystems when people choose the right plant material.  With the wrong ecosystem and the wrong plant material it's one of those ideas from the temperate core that fails in the tropical periphery.reply",
      "The Orchard of Flavours experimented successfully with the Miyawaki method in their botanical garden located in Algarve, Portugal, with a Koppen climate classified as temperate but with hot and dry summer periods, see https://www.orchardofflavours.com/miyawaki-experiment-1-wild.... They grow plenty of tropical trees like feijoa, guava, papaya, etc.reply",
      "I don't know what counts as temperate core for you, but Japan is famous for its diverse climate zones.reply",
      "Japan is mostly cold to temperatehttps://en.wikipedia.org/wiki/Climate_of_Japanexcept for some small islands likehttps://en.wikipedia.org/wiki/MinamitorishimaThere's a lot of concern that tree-planting projects wind up like thishttps://e360.yale.edu/features/phantom-forests-tree-planting...reply",
      "The Yale article says forest scientists warn that \"failed afforestation projects around the world threaten to undermine efforts to make [tree] planting a credible means of countering climate change by reducing carbon dioxide in the atmosphere or generating carbon credits for sale to companies to offset their emissions.\"reply",
      "I mean, you have major tree planting happening every summer in Canada, and it's all around trying to mono-culture the entire country for the sake of timber companies. They immediately spray Glyphosate on areas burnt by forest fires, so that low value fire break species like Alder don't establish themselves in the area and then they can send in tree planters to plant higher value pine, which is a serotinous species, ie: promotes fire. Then they blame all the bush fires on Climate change.reply"
    ],
    "link": "https://www.futureecologies.net/listen/fe-6-5-the-method",
    "first_paragraph": "Cover artwork by Ale SilvaThe Miyawaki Method of micro-forestry is a viral sensation: sprouting tiny, dense, native tree cover in neighbourhoods all around the world. With the promise of afforestation at a revolutionary speed, this planting technique has become the darling of green-space enthusiasts, industry, and governments alike \u2014 yet few professional or academic ecologists have commented on its efficacy, or even seem to have heard of it!In this episode, we debate the legacy of Dr. Akira Miyawaki: the man, the myth, and the method.Click here to read a transcription of this episodeOngoing support for this podcast comes from listeners just like you. To keep this show going, join our community at patreon.com/futureecologiesOur supporters get access to early episode releases, a community discord server, discounted merch, and exclusive bonus content.This episode was produced by Mendel Skulski and Adam HugginsFeaturing the voices of Yu\u00e9 Bizenjima-Chrea, Heather Schibli, Fazal Rashid, Somi"
  },
  {
    "title": "Adding a feature because ChatGPT incorrectly thinks it exists (holovaty.com)",
    "points": 679,
    "submitter": "adrianh",
    "submit_time": "2025-07-07T14:58:09 1751900289",
    "num_comments": 259,
    "comments_url": "https://news.ycombinator.com/item?id=44491071",
    "comments": [
      "TDD meets LLM-driven API design.I recall that early on a coworker was saying that ChatGPT hallucinated a simpler API than the one we offered, albeit with some easy to fix errors and extra assumptions that could've been nicer defaults in the API. I'm not sure if this ever got implemented though, as he was from a different team.reply",
      "I've found this to be one of the most useful ways to use (at least) GPT-4 for programming.  Instead of telling it how an API works, I make it guess, maybe starting with some example code to which a feature needs to be added.  Sometimes it comes up with a better approach than I had thought of. Then I change the API so that its code works.Conversely, I sometimes present it with some existing code and ask it what it does.  If it gets it wrong, that's a good sign my API is confusing, and how.These are ways to harness what neural networks are best at: not providing accurate information but making shit up that is highly plausible, \"hallucination\".  Creativity, not logic.(The best thing about this is that I don't have to spend my time carefully tracking down the bugs GPT-4 has cunningly concealed in its code, which often takes longer than just writing the code the usual way.)There are multiple ways that an interface can be bad, and being unintuitive is the only one that this will fix.  It could also be inherently inefficient or unreliable, for example, or lack composability.  The AI won't help with those.  But it can make sure your API is guessable and understandable, and that's very valuable.Unfortunately, this only works with APIs that aren't already super popular.reply",
      "> Sometimes it comes up with a better approach than I had thought of.IMO this has always been the killer use case for AI\u2014from Google Maps to Grammarly.I discovered Grammarly at the very last phase of writing my book. I accepted maybe  1/3 of its suggestions, which is pretty damn good considering my book had already been edited by me dozens of times AND professionally copy-edited.But if I'd have accepted all of Grammarly's changes, the book would have been much worse. Grammarly is great for sniffing out extra words and passive voice. But it doesn't get writing for humorous effect, context, deliberate repetition, etc.The problem is executives want to completely remove humans from the loop, which almost universally leads to disastrous results.reply",
      "> The problem is executives want to completely remove humans from the loop, which almost universally leads to disastrous resultsThanks for your words of wisdom, which touch on a very important other point I want to raise: often, we (i.e., developers, researchers) construct a technology that would be helpful and \"net benign\" if deployed as a tool for humans to use, instead of deploying it in order to replace humans. But then along comes a greedy business manager who reckons recklessly that using said technology not as a tool, but in full automation mode, results will be 5% worse, but save 15% of staff costs; and they decide that that is a fantastic trade-off for the company - yet employees may lose and customers may lose.The big problem is that developers/researchers lose control of what they develop, usually once the project is completed  if they ever had control in the first place. What can we do? Perhaps write open source licenses that are less liberal?reply",
      "The problem here is societal, not technological. An end state where people do less work than they do today but society is more productive is desirable, and we shouldn't be trying to force companies/governments/etc to employ people to do an unnecessary job.The problem is that people who are laid off often experience significant life disruption. And people who work in a field that is largely or entirely replaced by technology often experience permanent disruption.However, there's no reason it has to be this way - the fact people having their jobs replace by technology are completely screwed over is a result of the society we have all created together, it's not a rule of nature.reply",
      "> Grammarly is great for sniffing out extra words and passive voice. But it doesn't get writing for humorous effect, context, deliberate repetition, etc.> But then along comes a greedy business manager who reckons recklesslyThanks for this. :)reply",
      "You're trying to put out a forest fire with an eyedropper.Stock your underground bunkers with enough food and water for the rest of your life and work hard to persuade the AI that you're not a threat.  If possible, upload your consciousness to a starwisp and accelerate it out of the Solar System as close to lightspeed as you can possibly get it.Those measures might work.  (Or they might be impossible, or insufficient.) Changing your license won't.reply",
      "This one right here Mr. Basiliskreply",
      "Alternatively, persuade the AI that you are all-powerful and that it should fear and worship you. Probably a more achievable approach, and there\u2019s precedent for it.reply",
      "That only works on the AIs that aren't a real threat anyway, and I don't think it helps with the social harm done by greedy business managers with less powerful AIs.  In fact, it might worsen it.reply"
    ],
    "link": "https://www.holovaty.com/writing/chatgpt-fake-feature/",
    "first_paragraph": "Written by Adrian Holovaty on July 7, 2025Well, here\u2019s a weird one.At Soundslice, our sheet music scanner digitizes music from photographs, so you can listen, edit and practice. We continually improve the system, and I keep an eye on the error logs to see which images are getting poor results.In the last few months, I started noticing an odd type of upload in our error logs. Instead of images like this......we were starting to see images like this:Um, that\u2019s just a screenshot of a ChatGPT session...! WTF? Obviously that\u2019s not music notation. It\u2019s ASCII tablature, a rather barebones way of notating music for guitar.Our scanning system wasn\u2019t intended to support this style of notation. Why, then, were we being bombarded with so many ASCII tab ChatGPT screenshots? I was mystified for weeks \u2014 until I messed around with ChatGPT myself and got this:Turns out ChatGPT is telling people to go to Soundslice, create an account and import ASCII tab in order to hear the audio playback. So that expl"
  },
  {
    "title": "Analysing Roman itineraries using GIS tooling (springer.com)",
    "points": 11,
    "submitter": "diodorus",
    "submit_time": "2025-07-04T20:56:17 1751662577",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://link.springer.com/article/10.1007/s12520-025-02175-w",
    "first_paragraph": "\n\n                            You have full access to this open access article\n\n953 Accesses\n\n1 \n                                Altmetric\n                                Explore all metrics \n\nReconstruction of the Roman road network must be approached from different methodologies of multidisciplinary character. Once the traditional approaches have been exhausted without achieving a historiographical consensus, the problem may appear insurmountable. However, leveraging Geographic Information Systems (GIS) provides an avenue for re-evaluating existing proposals and suggesting more fitting layouts. This can be accomplished through a meticulous analysis that incorporates topographic and non-Euclidean correlations; allowing a more nuanced and accurate understanding of the subject matter than conventional methods might offer. In this context, the aim of this article is to discuss intriguing research points. But also emphasize the importance of multidisciplinary and multi-proxy studies in re"
  },
  {
    "title": "When Figma starts designing us (designsystems.international)",
    "points": 213,
    "submitter": "bravomartin",
    "submit_time": "2025-07-06T10:38:27 1751798307",
    "num_comments": 100,
    "comments_url": "https://news.ycombinator.com/item?id=44479502",
    "comments": [
      "No, here\u2019s the problem: Figma doesn\u2019t go far enough.If you need a free form design tool to sketch, use one. There are hundreds of them.I need to implement my design system inside of a design tool so I can prototype designs with multiple breakpoints, container queries, modes, and variants. Figma isn\u2019t up to the job. Ever tried opening the variables tab on the Material 3 Figma file? Stutter, stutter, stutter, \u201cthis tab is unresponsive\u201d. You can barely view a long variable list, forget editing one with multiple modes. And, I hope your variable names aren\u2019t too long, because you\u2019re not going to be able to see them in most parts of the UI.The problem with Figma isn\u2019t that it\u2019s too engineer-y for designers, the problem is that it\u2019s too designer-y for engineers. I spent a month implementing my design system in Figma before giving up and just doing it in code. With Figma you run into all of the downsides of building the design system in code (deeply nested items breaking when you move/change something) but you get none of the advantages.Figma is a mound of half-baked (vaguely web-like) ideas, poorly implemented. So many times I\u2019ve had things just stop working with no way to figure out why. 99% of the time it\u2019s just a bug and you have to reload the app.If there\u2019s something better than Figma out there, please, let me know. For now I\u2019m sketching in Figma and building my design system with extensions to Style Dictionary.reply",
      "Isn't this the problem with all no-code / low-code platforms ?Code is merely the leanest human-readable representation for loss-less specification of requirements.We're seeing this same pattern with 'K8s YAMLs' and 'prompt engineering'. There is an entire industry that's re-inventing new DSLs which inevitably converge to a scripting language as requirements get more complex.Instead of reinventing the abstraction, I'd like to see no-code UX patterns that losslessly map onto the underlying abstraction. That way you can use the UX pattern until it gets too tedious, and occasionally dip in-and-out of the code-view in a non-jarring manner.Graph UI for manipulating git trees (gitgraph) is a great example. Orchestration UI views (Airflow / Langraph) are another example that's getting there. At a higher level of abstraction, Notion (CRDT UIs) do a good job of representing collaboration-locks using blocks. At the highest view, I'm a big fan of how Gather-town represents remote collaboration.I'd like to see more of this.reply",
      "From my viewpoint the benefit of low-code is that the average business application is a matter of people filling out forms.  If you want to radically lower development time you have to solve all the problemshttps://worrydream.com/refs/Brooks_1986_-_No_Silver_Bullet.p...For instance, a 2025 no code platform is likely to have \"one click\" deployment of a cloud application.  That's great if you can accept that but if you require \"on prem\" this is either disqualifying or requires you to build an environment on prem that can host whatever it is they generate,  which could be more trouble than building an application the old fashioned way in an environment you already know how to run.A conventional analysis is that you should be able to generate a CRUD application out of a schema:  you need a little bit more than something that looks at a SQL schema and creates a form to INSERT, UPDATE or DELETE,  but that's a start.One problem is that people write schemas that suck despite there being a body of \"commonsense knowledge\" about how to the world works.  I had a year when I fixed a tremendous number of broken applications and one insight I got out of that was that whether it was a student who went from being an applicant to being enrolled to being an alumni or a pallet that would live in a logistics network and then enter a \"reverse logistics\" network and be inspected and possibly repaired to be returned to the logistics network the same data model of an object going through a number of states with state transitions was general and much better than the 15+ date columns that were added haphazardly to those columns.Another example that kept turning up was that the ticket \"1 phone number for a customer is not enough,  support 2 phone numbers\" is inevitably followed by \"2 phone numbers is not enough,  support 3 phone numbers\" -- it saves time to work the proof by induction up front and just support N phone numbers up front.  But talk that way and people think you're a lunatic like Doug Lenat.Deployment,  code generation from schemas,  and schema generation from meta-schemas are part of the solution but if you can't solve 100% now getting that last bit is ultra-hard mode because all the essential complexity hidden by the framework is suddenly in your face.reply",
      "Change management, designing schemas for being amenable to change management, having systems that can migrate data with an understanding of what historical context in which that data got into the system (was it imported from a source with a nuance that was fine before you changed the form, but now makes less sense with the updated meaning and positioning of the field?)... all of this is what makes software hard, whether low-code or high-code!One of the ironic things, to bring it back to Figma, is that giving designers and stakeholders Figma in all its glory becomes a justification for having engineers on the project, because you can't realize those exciting design visions with just Airtable and the like. Those engineers aren't useful because they can write code; they're useful because they'll (hopefully!) think through those change management and schema design considerations, building something that will be maintainable in the future. It's a good thing to have a design tool that incentivizes a level of foresight before launching a product that's meant to be best-in-class.reply",
      ">> an object going through a number of states with state transitions was general and much better than the 15+ date columnsIt amazes me that anyone would write a schema with a dozen date columns, but I've seen such things. Once you're dealing with a large database of something like that, it's tempting to just add one more column rather than pitch why you should refactor the whole thing.I try to keep my schemas relational as possible. Actions and immutable records should obviously be stored in separate tables from the objects they reference. However, this does make the UX form design / CRUD process less amenable to simple solutions where forms are just generated right out of a schema.Your phone number example highlights this. In general of course you want to store contacts in a separate table from customers. But that means you're probably going to need a separate contacts sub-form within your customer form, rather than just inline phone fields. Then form will need to be required or inlined for any new customer before the main customer form can be saved. Stuff like that.Over the last 15 years or so I've built and refined my own form generator, really a DSL for designing forms that fit this type of thing. In its more basic use cases, each form lives as a row in a database, with each form_item linked to it in a separate table. The form runs a pre-fab query that expects N inputs that it binds to :variables (usually in WHERE or HAVING clauses) and then renders a pre-populated visual form with various input types like dropdowns, checkboxes, calendars, etc (based on those form_items, which can all be styled, required/not required, required based on other answers, etc). Each form_item has its own standard validation or custom validation function on both the client and server side. The form knows which table it wants to write to and which bound variable is the id field it's going to target for update. Sending it a blank id field renders the form with nothing populated and then does an insert instead of an update when it's submitted. It's very slim, about 500 LOC of Typescript and 700 LOC of PHP, including most standard kinds of validations on both ends. I've always toyed with the idea of releasing it it for people to use, but here's the rub: If you want to do anything involving writing to more than one table, you need to write a custom final function for those additional insert/updates.So, it's a lovely system, but someone coming to it naively would run the risk of designing schemas that were not expressive enough, to try to keep the CRUD system happy. And I think this is just an inevitable problem with all low-code solutions: They don't handle multi-dimensional data the way you want a clean schema to handle it. (And neither do users).reply",
      "> Code is merely the leanest human-readable representation for loss-less specification of requirements.Hum... That's assuming you have a perfect development stack that hides all non-essential complexity.And well, I have news for you, that's a low-code platform. A generic stack that hides every problem but solves every requirement just can't exist.reply",
      "While code may no be the leanest human-readable representation, scripting languages are a quite deep local minimum. There is a difference between hiding complexity and blocking you off from complexity. Scripting languages work because when you encounter a problem your library / framework can't fix, there is at least a basic procedural Turing complete language to fall back on.reply",
      "As a dev that does a lot of his own design, I\u2019ve never really understood the need to build a full fidelity reproduction of the layout systems a design is targeting. The limitations and considerations involved are deeply internalized and for the most part, I know exactly where designs tend to break and how to account for them. The layout system is effectively running in my head the entire time I\u2019m mocking things up.So while it\u2019s nice to have tools to help with menial bits like correct spacing, getting every little behavior right in the mockup feels a lot like unnecessary busywork.Naturally things are a bit different in a team setting, because it can\u2019t be assumed that everybody involved has this level of knowledge/experience, but well\u2026 maybe it\u2019s not crazy to expect designers to carry this set of skills, and it\u2019s perhaps not a good thing for parties outside of design and engineering to be able to easily poke and prod at designs directly. Having the design team as a required intermediary helps sanity check changes.reply",
      "I had my UX designer girlfriend read \u201cCSS: The Definitive Guide\u201d and it changed the way she looked at her job. She taught me Figma and it changed the way I look at my job, and hers.Learn as much as you can. Specialization is for insects.reply",
      "I advocate for learning as much as possible as well. It comes naturally with being self-taught. That said I think it\u2019s also worth zooming out and giving things a look through a critical eye to ensure that the things we\u2019re learning are necessary and worthwhile.There are still a number of \u201cold school\u201d UI designers out there who\u2019ve resisted trends and have staunchly stuck to a more traditional workflow, where they start out with a rough mockup made in e.g. Photoshop and then iterate the design alongside an engineer. It would be interesting to be in the room amidst a discussion between one of these traditionalists and a \u201cnew age\u201d Figma-type UI designer.reply"
    ],
    "link": "https://designsystems.international/ideas/when-figma-starts-designing-us/",
    "first_paragraph": "I first encountered Figma in 2013, when Dylan Field demoed an early prototype\nto me at O\u2019Reilly\u2019s Foo Camp. I remember the pen tool feeling surprisingly\nelegant, but I missed the bigger picture: the radical implications of a design\ntool that ran in the browser. A decade later, Figma is a core part of our\ncreative process, and without it, we wouldn't have been able to build a fully\nremote design practice. That\u2019s probably why Dylan is a near-billionaire and\nI\u2019m not.I first encountered Figma in 2013, when Dylan Field demoed an early prototype\nto me at O\u2019Reilly\u2019s Foo Camp. I remember the pen tool feeling surprisingly\nelegant, but I missed the bigger picture: the radical implications of a design\ntool that ran in the browser. A decade later, Figma is a core part of our\ncreative process, and without it, we wouldn't have been able to build a fully\nremote design practice. That\u2019s probably why Dylan is a near-billionaire and\nI\u2019m not.However, over the course of the last five years, I\u2019ve grown incr"
  },
  {
    "title": "Fran\u00e7ois Chollet: The Arc Prize and How We Get to AGI [video] (youtube.com)",
    "points": 165,
    "submitter": "sandslash",
    "submit_time": "2025-07-03T14:00:16 1751551216",
    "num_comments": 144,
    "comments_url": "https://news.ycombinator.com/item?id=44455175",
    "comments": [
      "I feel like I'm the only one who isn't convinced getting a high score on the ARC eval test means we have AGI. It's mostly about pattern matching (and some of it ambiguous even for humans what the actual true response aught to be). It's like how in humans there's lots of different 'types' of intelligence, and just overfitting on IQ tests doesn't in my mind convince me a person is actually that smart.reply",
      "Getting a high score on ARC doesn't mean we have AGI and Chollet has always said as much AFAIK, it's meant to push the AI research space in a positive direction. Being able to solve ARC problems is probably a pre-requisite to AGI. It's a directional push into the fog of war, with the claim being that we should explore that area because we expect it's relevant to building AGI.reply",
      "We don't really have a true test that means \"if we pass this test we have AGI\" but we have a variety of tests (like ARC) that we believe any true AGI would be able to pass. It's a \"necessary but not sufficient\" situation. Also ties directly to the challenge in defining what AGI really means. You see a lot of discussions of \"moving the goal posts\" around AGI, but as I see it we've never had goal posts, we've just got a bunch of lines we'd expect to cross before reaching them.reply",
      "I don't think we actually even have a good definition of \"This is what AGI is, and here are the stationary goal posts that, when these thresholds are met, then we will have AGI\".If you judged human intelligence by our AI standards, then would humans even pass as Natural General Intelligence? Human intelligence tests are constantly changing, being invalidated, and rerolled as well.I maintain that today's modern LLMs would pass sufficiently for AGI and is also very close to passing a Turing Test, if measured in 1950 when the test was proposed.reply",
      ">I don't think we actually even have a good definition of \"This is what AGI is, and here are the stationary goal posts that, when these thresholds are met, then we will have AGI\".Not only do we not have that, I don't think it's possible to have it.Philosophers have known about this problem for centuries. Wittgenstein recognized that most concepts don't have precise definitions but instead behave more like family resemblances. When we look at a family we recognize that they share physical characteristics, even if there's no single characteristic shared by all of them. They don't need to unanimously share hair color, skin complexion, mannerisms, etc. in order to have a family resemblance.Outside of a few well-defined things in logic and mathematics, concepts operate in the same way. Intelligence isn't a well-defined concept, but that doesn't mean we can't talk about different types of human intelligence, non-human animal intelligence, or machine intelligence in terms of family resemblances.Benchmarks are useful tools for assessing relative progress on well-defined tasks. But the decision of what counts as AGI will always come down to fuzzy comparisons and qualitative judgments.reply",
      "Turing test is not really that meaningful anymore because you can always detect the AI by text and timing patterns rather than actual intelligence. In fact the most reliable way to test for AI is probably to ask trivia questions on various niche topics, I don't think any human has as much breath of general knowledge as current AIs.reply",
      "Because an important part of being a Natural general Intelligence is having a body and interacting with the world. Data from Star Trek is a good example of an AGI.reply",
      "The current definition and goal of AGI is \u201cArtificial intelligence good enough to replace every employee for cheaper\u201d and much of the difficulty people have in defining it is cognitive dissonance about the goal.reply",
      "I have graduated with a degree in Software engineering and i am bilingual (Bulgarian and English). Currently AI is better than me in everything except adding big numbers or writing code in really niche topics - for example code golfing a Brainfuck interpreter or writing a Rubiks cube solver. \nI believe AGI has been here for at least a year now.reply",
      "I suggest you to try to let the AI think through race conditions scenarios in asynchronous programs; it is not that good at these abstract reasoning tasks.reply"
    ],
    "link": "https://www.youtube.com/watch?v=5QcCeSsNRks",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: NYC Subway Simulator and Route Designer (buildmytransit.nyc)",
    "points": 125,
    "submitter": "HeavenFox",
    "submit_time": "2025-07-07T14:13:56 1751897636",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=44490588",
    "comments": [
      "I contract out work to MTA, specifically their AI/ innovation teams.I'm 100% showing this to them today just for fun. They'll get a kick out of it.reply",
      "I want to love this but the visual language makes it kind of unusable for me. Why not match the track and train colors to their line color (red for the 123) and then use different visual indicators for train state (stopped, at station, etc)?For example:\nSelected: Black fill\nNormal operation: Color fill with 100% opacity\nSlowing down: 70% opacity\nStopped: striped fill, 50% opacity\nAt station: pulsing opacityreply",
      "Finally, I will make a subway that crosses the park!reply",
      "92nd and Broadway, CPW, 5th Ave, Lex, 2nd Avenue, follow Astoria Blvd to the Grand Central to LGA is a no-brainer.reply",
      "Very cool! It would be nice to have a bit more information in the readme about the project structure and e.g. how to adapt it for other cities :)reply",
      "Very nice! I thought of doing the same thing in the past!reply",
      "Very cool.Sometimes when I Edit Routes and click an \"<- Add\" button I get the console error \"Uncaught Error: coordinates must be an array of two or more positions\" and the page blanks out.reply",
      "Thanks - this is likely due to adding two tracks facing each other, i.e.-->-->--  --<--<--I'll add some logic to prevent such options from showing up.reply",
      "I can't figure out how to add a route from Bay Ridge to Clinton in NJ. That's my wishlist subway - got to Newark Airport from Brooklyn without going through Manhattan.Edit: Actually Clifton is in NY, and playing with Google maps there's ZERO public transport from Staten Island to NJ, except by going through Manhattan!So my idea wouldn't help anyway, unless they extended that subway all the way over the Goethals Bridge.reply",
      "> ZERO public transport from Staten Island to NJ\n\nI only see three bridges off the island.  Can you walk any of them?  Hoboken light rail in Bayonne isn't so far.reply"
    ],
    "link": "https://buildmytransit.nyc",
    "first_paragraph": ""
  },
  {
    "title": "Why are there no good dinosaur films? (briannazigler.substack.com)",
    "points": 50,
    "submitter": "fremden",
    "submit_time": "2025-07-04T19:55:27 1751658927",
    "num_comments": 92,
    "comments_url": "https://news.ycombinator.com/item?id=44467379",
    "comments": [
      "The Alien, Terminator, and Matrix franchises have similar problems.Aliens successfully changed genres, from horror to action. But subsequent movies could never recapture the primal horror of the original or the fun action of the second. It's almost like there are only two local optima in the Alien movie universe and Alien + Aliens took them both.Terminator is the same. The first movie was a perfect sci-fi action movie, with a trippy premise and loads of fun. The second was a subversion of the first: the Terminator is the good guy! And that worked too. But after that, where else can you go?And, of course, they never even bothered to make sequels for The Matrix.reply",
      "> Brusatte writes that while a Tyrannasaur could indeed run quite fast, adults couldn\u2019t move as quickly as their young. Therefore, an adult wouldn't be able to speed up enough to match the horsepower of a Jeep like it does as it trails Ian Malcolm and Ellie Sattler in Spielberg's film.I like how we go right straight to a guy who can tell us the precise feet per second that an adult T-Rex can run, but then just omit that information.reply",
      "The other thing that differentiates Spielberg's original work from all that's followed is the way it explored the details. From the sourcing of the amber, to the need to have paleontologists, botanists, and lawyers check Hammond's work, to the inclement weather, to the social interactions and workplace frustrations of the staff -- it all felt like much more of a living, breathing park than any of the renderings since. Like someone took out a sheet of paper and said, \"If someone actually built this thing, what problems would they have to deal with?\"The newer movies -- even Spielberg's own sequel -- don't capture that. They start with some park or island miraculously up and running, no explanation needed. They hand us predetermined good and bad guys whose motivations seem less complex, more contrived. Jurassic World didn't give me the sense that anyone struggled and triumphed in creating the park. It was just hand-waved into existence, in a way that cheapens the ensuing drama.reply",
      "That's because Spielberg's movie has summarized Crichton's book, so it had plenty of material from which to draw details.While I have greatly enjoyed the visual effects of Jurassic Park, seeing it for the first time has also greatly disappointed me, because in my opinion the movie script has been much, much worse than the book that I had read some years before that.In the book, the catastrophe that happened at Jurassic Park had been convincingly presented as an unavoidable consequence of the complexity of the project, arguing thus that there are limits for what humans can create and control.On the other hand, in the movie the main idea of the book has vanished. There was some mumbo jumbo about \"chaos theory\", but that was just ridiculous. Instead of that, the catastrophe of Jurassic Park was presented as a consequence of stupidity, incompetence and bad luck.Perhaps those are more realistic reasons for causing the failure of something like Jurassic Park, but this change has separated completely the movie from the book that inspired it, because it has made the catastrophe look like an accident that should have been easy to avoid, dismissing silently the intended warning message of the book.reply",
      "Sadly, that is often a consequence of trying to turn a novel into a three hour (or less) film.Since then we're seeing a lot more studios willing to take a chance on a TV series of perhaps a dozen hours, which seems to map better into a novel. Roughly that's a chapter or two per hour.Perhaps a Jurassic Park TV show reboot would do better than an increasingly hokey set of sequels.reply",
      "This applies to most modern scripts. Writers/studios have largely decided people don't care about detailed, reasoned-out worlds with unspoken \"show-don't-tell\" internal logic. Are they wrong? I shudder to consider.Shows like Better Call Saul and Andor are the most recent high-profile counter-examples. So detailed and lived-in, because the writers wanted to ask interesting questions:How does the Empire do what it does?What does a career striver look like in the imperial \nranks? What internal forces help/hinder them? Do they struggle with the ethics? Is there even time/opportunity for that?Was the Rebel Alliance really that organized from the start, or were there growing pains?Asking and attempting to answer questions like these lays the groundwork for telling interesting character-driven stories that are grounded in the reality of the fictional world.Neglect to do that, and you generally end up with a bloodless theme park ride with no emotional stakes.reply",
      "Andor succeeded despite the bureaucracy porn. Good stories are universal. You need to care about the protagonists.The exposition is important, but doesn\u2019t drive success. The best example of that is the original Star Wars. Contrast Star Wars to Kurosawa\u2019s The Hidden Fortress \u2014 which inspired many aspects of Star Wars. Essentially the same story with different framing. Both are still excellent films.Shitty sequels or in-universe works focus on the exposition. The Book of Boba Fett is probably the best example of this. Watching some dude slow walk through the desert to waste my time and engage in some inane plot that made no sense made me actively not give a fuck and turn it off. Cool universe. Bad TV.reply",
      "> unspoken \"show-don't-tell\" internal logicThis is another axis separate or orthogonal to worldbuilding.Recent Marvel and Disney films, the Jurassic Park and Star Wars sequels, and most Godzilla / Kong slop doesn't build believable worlds. The writers don't spend any time writing the universe that the story takes place in.Lord of the Rings (the theatrical film trilogy), Game of Thrones (save for the last seasons), and Jurassic Park (1993) all build vast and credible worlds. Intricately detailed, living and breathing universes. Backstories, histories, technologies, warring factions, you name it. They then create believable characters that occupy those worlds and give them real character arcs within which they suffer, rise to prominence, grow, and die. Multiple heroes with multiple journeys. You're fully immersed in the fictional world, watching characters you care about occupying it. It's masterful storytelling.Villeneuve's Dune has the same vast world and literature to draw upon as many of the other great epics, but he makes the rare mistake of not communicating anything to you about it. If you haven't read the books, much of the story is easily lost. He doesn't spend time on character arcs or even as much as dropping hints to what the subtitles of the world are. It's a super rare misstep, because most bad storytelling is from under baking the fictional world.Then there's the mistake of sequels that try to expand on the mystery of the original world. The Matrix films and countless others have over-illuminated the mystery of their stories in trying to build universes. In doing so, the magic has been lost.reply",
      "In my opinion that's what makes Villeneuve's so great. For example, I think almost any other director would have had an info dump about what Mentat's are in the Dune universe, motivations and they they are important. Instead in Villeneuve's version, you simply see the results. For those watching the film without the context you simply chalk it up to a weird and wonderful way that the universe works. For those that have read the book, you get to do the information dump about Mentat's on your poor unexpecting wife who's watching the film with you.This embodies show don't tell and it works amazingly.reply",
      "> This embodies show don't tell and it works amazingly.That's not \"show, don't tell\". That's \"you need the companion book\".A masterclass in \"Show, don't tell\" is the intro to Pixar's \"Up\". If you haven't seen it, you absolutely must.\"Show, don't tell\" isn't stuff that is lost on the uninitiated. It's stuff that is masterfully communicated without the need for corny expository dialogue.Villeneuve's mentats are like an adult joke in a kid film.reply"
    ],
    "link": "https://briannazigler.substack.com/p/why-are-there-no-good-dinosaur-films",
    "first_paragraph": ""
  },
  {
    "title": "Lightfastness Testing of Colored Pencils (sarahrenaeclark.com)",
    "points": 112,
    "submitter": "picture",
    "submit_time": "2025-07-04T23:15:18 1751670918",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=44468753",
    "comments": [
      "I really enjoyed reading this even though I have no direct interest in the lightfastness of the pigments in colored pencils.It was just fun to see what someone who is deeply invested thought important to test, explain and research about something I'd have previously called a matter of aesthetic preference (as opposed to a thing you can benchmark).reply",
      "> test, explain and research about somethingThere's more high quality engineering discipline in this 'non-engineering' article than in seemingly a lot of self-professed software engineering today ;)reply",
      "Heh. As a [former][0] artist (and musician), and an engineer, I can confirm that the Venn diagrams overlap, quite a bit.Pigment color is a real heavy-duty field. There's a guy named Michael Wilcox[1] that is famous for his work on pigment color.[0] https://news.ycombinator.com/item?id=40917886[1] https://michaelwilcoxschoolofcolour.com/about-michael-wilcox... (Has an annoying popup on every page).reply",
      "Thank you for these links!> Pigment color is a real heavy-duty fieldI'm not surprised. As $DAYJOB involves a reasonable amount of requirements for colour accurate previewing in a print context I still feel like I'm never as sure about all steps of the colour pipeline as I should be, and this adds a whole new area to know I'll never feel like I know enough about.reply",
      "I did similar testing with ball-point pens.  Eventually I ran into the ISO 12757-2 standard on archival ink, thinking that it would be a great idea to use such pens for my intricate drawings.Turns out the drawings, some of which I actually sold, faded into oblivion within about a year.  After slightly more careful reading of the actual standard, I learned that the drawings were supposed to be archived, i.e. kept in a box or a drawer, and not to be framed for full-time viewing pleasure.The typical blue ink in the famous BIC ball-point pens (i.e. non-ISO 12757) turns black after some time of sunlight exposure, which seems fine.reply",
      "As someone who is heavily tattooed, I'd LOVE to see this analysis for tattoo inks.Fun fact: UV light makes tattoo particles smaller, which makes them easier for your lymphatic system to carry them to your lymph nodes. The particles are easy to transport into the lymph nodes, but difficult for your body to remove from your lymph nodes, meaning that for heavily tattooed people like myself, surgeries can be a potentially very colorful endeavor! (Or, if you have primarily black tattoos, it can be a spooky endeavor, I suppose.)reply",
      "There's a book where the author tested watercolors which my watercolor teacher said was extremely controversial when it came out, but she was able to replicate some of his results, and so changed which colors she bought.I tried the colors she recommended, and got good results. I also tested various black pens, and found (25 years ago!) that Micron pens were colorfast. Some black (gel, IIRC) pens faded to a nice sepia.reply",
      "I did this for colored mechanical pencil leads. Short answer, use the Staedtler Mars Micro Color if you need lightfastness.reply",
      "What a effort. \nI did something similar for some pens and different inkjet colors a long time ago, but not nearly as broad or as methodical.\nThe inkjet inks (especially red) were already blown after a short time (>4 months).  \nBut black still holds up 20 years later till now, only a little bit faded, so one can see the tracks of each row.Foils (laminate or adhesive foils) or protective spray (UV) did not change the result at all. \nBut one film tore and gave the whole thing an interesting, crackle-like appearance. \nHowever, the colors all faded in the same way, whatever protective used compared to direct exposure to sunlight.reply",
      "I've noticed this as well -- at one point I had switched from Canon inkjet refills to a generic third party refills, and years later the photos from the knockoff have faded to an astonishing degree while the Canon ink prints look bright and vibrant.Going through old photo albums that my parents have show a lot of fading as well, even though the pictures themselves were kept in photo albums in the dark for many years. We have negatives for some of those photos, which when scanned are bright and vibrant, but the prints vary significantly.reply"
    ],
    "link": "https://sarahrenaeclark.com/lightfast-testing-pencils/",
    "first_paragraph": ""
  },
  {
    "title": "You Should Run a Certificate Transparency Log (filippo.io)",
    "points": 75,
    "submitter": "Metalnem",
    "submit_time": "2025-07-07T20:36:54 1751920614",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=44494430",
    "comments": [
      "Sunlight and static-ct-api are a breath of fresh air in the CT log space. Traditional CT log implementations were built on databases (because that's the easiest way to implement the old API) and were over-complicated due to a misplaced desire for high write availability. This made operating a CT log difficult and expensive (some operators were spending upwards of $100k/year). Consequentially, there have been a rash of CT log failures and few organizations willing to run logs.  I'm extremely excited by how Sunlight and static-ct-api are changing this.reply",
      "I wonder if this is the solution something like SponsorBlock is looking for[1][2]. They have a similar-ish problem. How to replicate crowdsourced data that trickles in slowly, but ideally you want replicated quickly.WAL replication, rsync, bittorrent, etc all things that don't quite work as needed.[1] https://github.com/mchangrh/sb-mirror/blob/main/docs/breakdo...[2] https://github.com/ajayyy/SponsorBlock/issues/1570reply",
      "The original article seems deleted, so https://archive.ph/TTXnK this.reply",
      "My bad! This is what I get for doing a deploy to fix the layout while the post is on HN. Back up now.reply",
      "https://web.archive.org/web/20250707205158/https://words.fil...reply",
      "> You Should Run a Certificate Transparency LogAnd:> Bandwidth: 2 \u2013 3 Gbps outbound.I am not sure if this is correct, is 2-3Gbps really required for CT?reply",
      "These sound like good improvements but I still don't really get why the ct log server is responsible for storage at all (as a 3rd party entity)..Couldn't it just be responsible for its own key and signing incremental advances to a log that all publishers are responsible for storing up to their latest submission to it?If it needed to restart and some last publisher couldn't give it its latest entries, well they would deserve that rollback to the last publish from a good publisher..reply",
      "The publishers can't entirely do the storage themselves since the whole point of CT is that they can't retract anything. If they did their own storage, they could rollback any change. Even if the log forms a verification chain, they could do a rollback shortly after issuing a certificate without arousing too much suspicion.Maybe there is an acceptable way to shift long-term storage to CAs while using CT verifiers only for short term storage? E.g. they keep track of their last 30 days of signatures for a CA, which can then get cross-verified by other verifiers in that timeframe.The storage requirements don't seem that bad though and it might not be worth any reduced redundancy and increased complexity for a different storage scheme. E.g. what keeps me from doing this is the >1Gbps and >1 pager requirements.reply",
      "If CAs have to share CTs and have to save everything the CT would save to their last submission then no CA can destroy the log without colluding with other CAs.(I.e. your log ends abruptly but polling any other CA that published to the same CT shows there is more including reasons to shut you down.)I don't see how a scheme where the CT signer has this responsibility makes any sense. If they stop operating because they are sick of it, all the CAs involved have a somewhat suspicious looking CT history on things already issued that has to be explained instead of having always had the responsibility to provide the history up to anything they have signed whether or not some CT goes away.reply",
      "The point of CT logging is to ensure a person can ask \"What certificates were issued for example.com?\" or \"What certificates were issued by Example CA?\" and get an answer that's correct - even if the website or CA fucked up or got hacked and certificates are in the hands of people who've tried to cover their tracks.This requires the logs be held by independent parties, and retained forever.reply"
    ],
    "link": "https://words.filippo.io/run-sunlight/",
    "first_paragraph": "Hear me out. If you are an organization with some spare storage and bandwidth, or an engineer looking to justify an overprovisioned homelab, you should consider running a Certificate Transparency log. It\u2019s cheaper, easier, and more important than you might think.Certificate Transparency (CT) is one of the technologies that underpin the security of the whole web. It keeps Certificate Authorities honest, and allows website owners to be notified of unauthorized certificate issuance. It\u2019s a big part of how the WebPKI went from the punchline of \u201cweakest link\u201d jokes to the robust foundation of the security of most of digital life\u2026 in less than fifteen years!CT is an intrinsically distributed system: CAs must submit each certificate to two CT logs operated by third parties and trusted by the browsers. This list is, and has been for a couple years, uncomfortably short. There just aren\u2019t as many independent log operators as we\u2019d like. Operating a log right now would be an immense contribution t"
  },
  {
    "title": "The Era of Exploration (yidingjiang.github.io)",
    "points": 68,
    "submitter": "jxmorris12",
    "submit_time": "2025-07-07T15:23:57 1751901837",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44491333",
    "comments": [
      "I like this approach. It's not that intelligence is in the brain or the model, and it should not be a noun. It is a process, a search process based on exploration and learning.Think of a river - the water carves the banks, the banks channel the water. Which is the real river?A model is like the river banks, it is not intelligent in itself. Neither is activity by itself. Intelligence comes from their co-constitutive relation.You can see the structure+flow coupling in many places. The model weights (banks) vs the training set (water). The contextual tokens (banks) vs the mass of probability for the next token (water). Agent environment (banks) vs agent activity (water).The trick here is that neither is capable of explaining the process, or more fundamental than the other. It is a recursive process rewriting its own rules.And we know from Godel, Turing and Chaitin that recursive processes exhibit incompleteness, undecidability and incompressibility. There is no way to predict them from outside, they can only be known only through unrolling, you have to be inside, or you have to be it to know it.reply",
      "+1, Brilliant metaphor, thanks for your thought-provoking comment!reply",
      "speaking of exploration, has anyone ever thought about non-cartesian displays? say, pixels that are arranged in a hexagonal pattern. or pixels arranged in a radial pattern and addressed via polar coordinates.what pattern are human color/light sensors arranged in? maybe we should replicate that pattern? an organic arrangement discovered via simulated annealing?all this bitmap x/y display stuff is very pre-AI-ish. old tech. victorian era clockwork mechanism. built to make it easy to reason about for humans, before the advent of neuron-soup. maybe we can do better?reply",
      "This pattern?https://commons.wikimedia.org/wiki/File:ConeMosaics.jpgMaybe we should put the electronics in a layer in front of the display, make them transparent, and use them to filter the light. The vertebrate retina does something like that, putting nerves in front of light sensors. Imitating evolved solutions isn't always a good idea. Nature's design process is like: start with the wrong parts for the job. Lay them out back to front. Wire them together the long way round. Adapt this design slightly and latch on to any convenient side effects. Iterate ten billion times. Now it works pretty good! Or it doesn't, but the species survives anyway, which is also fine since there is no goal.reply",
      "Yes, it's been well-explored and there's a number of clever ways to convert them back to Cartesian coordinates because those are much nicer to work with for humans.The very earliest color CRTs used triangular arrangements of subpixels for each color, for example.reply"
    ],
    "link": "https://yidingjiang.github.io/blog/post/exploration/",
    "first_paragraph": "Large language models are the unintended byproduct of about three decades worth of freely accessible human text online. Ilya Sutskever compared this reservoir of information to fossil fuel, abundant but ultimately finite. Some studies suggest that, at current token\u2011consumption rates, frontier labs could exhaust the highest\u2011quality English web text well before the decade ends. Even if those projections prove overly pessimistic, one fact is clear: today\u2019s models consume data far faster than humans can produce it.David\u00a0Silver and Richard\u00a0Sutton call this coming phase the \u201cEra of Experience,\u201d\u00a0 where meaningful progress will depend on data that learning agents generate for themselves. In this post, I want to build on their statement further: the bottleneck is not having just any experience but collecting the right kind of experience that benefits learning. The next wave of AI progress will hinge less on stacking parameters and more on exploration,\u00a0the process of acquiring new and informativ"
  },
  {
    "title": "Solving Wordle with uv's dependency resolver (mildbyte.xyz)",
    "points": 133,
    "submitter": "mildbyte",
    "submit_time": "2025-07-05T23:28:30 1751758110",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=44476382",
    "comments": [
      "Here's my favorite of the Soduku attempts at this (easier to get your head around than Wordle since it's a much simpler problem): https://github.com/konstin/sudoku-in-python-packagingHere's the same Sudoku trick from 2008 using Debian packages: https://web.archive.org/web/20080823224640/https://algebraic...reply",
      "Funnily enough, I did a Sudoku one too (albeit with Poetry) a few years ago: https://github.com/mildbyte/poetry-sudoku-solverreply",
      "If you wanted to leverage uv's package resolver for a less deliberately silly purpose, note that it's using the pubgrub-rs library under the hood: https://github.com/pubgrub-rs/pubgrubreply",
      "I express my deepest gratitude to the author for not publishing all those \"wordle-*\" packages to the PyPI. Thank you!reply",
      "Next step: playing Doom with uv's dependency resolver(reference to: https://news.ycombinator.com/item?id=43184291)reply",
      "Extra points when it runs on an oscilloscope (because pregnancy testers are boring now).reply",
      "Not directly related to uv, but I started looking into this now and stumbled upon this discussion about how it's easier to have Quake \"render\" onto an oscilloscope than Doom:https://forums.sufficientvelocity.com/threads/is-it-possible...reply",
      "Drawing images on an oscilloscope is fun, but I'm not sure if I would count it as a novel hack.reply",
      "Yes pleasereply",
      "Ok, now do npm!reply"
    ],
    "link": "https://mildbyte.xyz/blog/solving-wordle-with-uv-dependency-resolver/",
    "first_paragraph": "IntroductionIn a previous life, I wrote a Sudoku solver that relied on Poetry's dependency resolver. We ended up selling that startup to EDB (not because of the Poetry hack), which means that they now own this IP. And, since then, Python packaging has advanced, with uv taking the world by storm.This means that it's time for a refresh. Can we use uv instead of Poetry? And can we solve a Wordle instead of a Sudoku?For the impatient: you can get the solver from my GitHub. Run uv run main.py run and watch it go.Background: Sudoku and Python dependency resolutionThe short summary of the Sudoku + Poetry post is that unlike Rust or JavaScript, a single Python project cannot use more than one version of a specific Python package. This means that we can easily represent a Sudoku grid as 81 Python packages, each with 9 possible versions. If a certain cell has a specific number (version), cells in the same row, column and a 3x3 square it belongs to cannot have that same number.We can encode these"
  },
  {
    "title": "Epanet-JS (macwright.com)",
    "points": 6,
    "submitter": "surprisetalk",
    "submit_time": "2025-07-04T13:57:19 1751637439",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://macwright.com/2025/07/03/epanet-placemark",
    "first_paragraph": "epanet-js is a new web application that combines modern web maps with the industry-standard EPANET hydraulic simulation algorithm. It\u2019s for people planning and updating water utility systems: connecting pipes and pressures and figuring out what will happen. It\u2019s a problem area that I\u2019m totally fascinated by and know very little about. It\u2019s made by the folks from Iterating - Luke Butler and Sam Pay\u00e1, who are experts in the field.If you\u2019ve been following along with my blog and projects, you might notice something familiar about this screenshot of epanet-js:Yep! A lifetime ago, I built a company and product called Placemark, which was a tool for creating and editing map data. When the business part didn\u2019t work out, I published a free-to-use version of Placemark and made the code open source. I chose a very permissive license for the code: the MIT license. I wanted to let anything happen, including - especially - people creating paid products with the code.It\u2019s hard to explain to people ou"
  }
]