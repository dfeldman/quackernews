[
  {
    "title": "FFmpeg merges WebRTC support (ffmpeg.org)",
    "points": 534,
    "submitter": "Sean-Der",
    "submit_time": "2025-06-04T15:58:50 1749052730",
    "num_comments": 122,
    "comments_url": "https://news.ycombinator.com/item?id=44182186",
    "comments": [
      "I am so incredibly excited for WebRTC broadcasting. I wrote up some reasons in the Broadcast Box[0] README and the OBS PR [1]Now that GStreamer, OBS and FFmpeg all have WHIP support we finally have a ubiquitous protocol for video broadcasting for all platforms (Mobile, Web, Embedded, Broadcasting Software etc...)I have been working on Open Source + WebRTC Broadcasting for years now. This is a huge milestone :)[0] https://github.com/Glimesh/broadcast-box?tab=readme-ov-file#...[1] https://github.com/obsproject/obs-studio/pull/7926\n \nreply",
      "That pr is really great work both technically and interpersonally. A fun read for sure. Great work and thank you for your determination.\n \nreply",
      "Your work in this area has been phenomenal. Thank you! I use broadcast-box all the time.\n \nreply",
      "What sort of infrastructure do you need for scaling WebRTC multicast?Are we entering an era where you don't need Amazon's budget to host something like Twitch?\n \nreply",
      "Yes we are :) When OBS merges the PR [0] things are going to get very interesting.Before you needed to run expensive transcoding jobs to be able to support heterogenous clients. Once we get Simulcast the only cost will be bandwidth.With Hetzner I am paying $1 a TB. With AV1 or H265 + Simulcast I am getting 4K for hundreds of users on just a single server.We will have some growing pains, but I am not giving up until I can make this accessible to everyone.[0] https://github.com/obsproject/obs-studio/pull/10885\n \nreply",
      "I have found it's hard to get past ~18Gbps on commodity servers and ~90Gbps on high spec, carefully specced servers. I presume you find the same?\n \nreply",
      "A twitch 720p stream is only 4 Mbps. 1080p? 6-8 MbpsSo if you've got ~18 Gbps of upload bandwidth you're ready for 10,000-20,000 viewers.\n \nreply",
      "With how ubiquitous gigabit symmetric is becoming, I wonder if you could even do P2P nowadays.Assuming a single gigabit symmetric connection could dedicate at most 100mb of upload bandwidth, you'd need one such viewer per each 25 viewers with a worse connection. This feels achievable.You'd have 1 server that broadcasts to at most 10K tier-1 viewers. Tier 2 viewers get 3 tier1 ips for failover, and pre-negotiate a connection with them, e.g. through STUn, to get sub-second failovers in case their primary source quits the stream. A central control plane load balances these.With something like 15s of buffer (acceptable for a gaming stream, not so much for sports, where your neighbors might be watching on satellite and cheer), this feels achievable.\n \nreply",
      "In theory I think you're right, but you do need a really smart control plane. Just because a connection is fast doesn't mean it's not metered (let's say fast 5G, starlink, rural fiber) and so on.Are all of the issues P2P brings really worth it?I'd say this definitely opens up streaming from your desktop with 1 CPU core handling a SFU for say 100 viewers = 500Mb or from a $5/month VPS if you've not got the capacity at home. That's pretty awesome, for most people no need to use P2P.\n \nreply",
      "Not sure I follow your maths there.If we assumed an average of 6Mb per stream that's 3000 streams, practically speaking a little lower.It's all relative I guess but it's not that high.\n \nreply"
    ],
    "link": "https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00",
    "first_paragraph": ""
  },
  {
    "title": "A Spiral Structure in the Inner Oort Cloud (iop.org)",
    "points": 45,
    "submitter": "gnabgib",
    "submit_time": "2025-06-04T23:22:07 1749079327",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=44186660",
    "comments": [
      "Meta: I can't even get into the site, the hCaptcha shows an image (a table and chairs) that never shows up even after 20+ \"Skip\" clicks...\n \nreply",
      "you have to click on the fruit cups, as it asks what fits the \"theme\" of the image.here's the pdf, though: https://iopscience.iop.org/article/10.3847/1538-4357/adbf9b/...\n \nreply",
      "Also no access, even after passing human validation - screen dark.\n \nreply",
      "Here's the PDF: https://iopscience.iop.org/article/10.3847/1538-4357/adbf9b/...\n \nreply",
      "I got the same thing.  The correct answer for me was the fancy desserts, because they go on a fancy restaurant table I guess.\n \nreply",
      "I fucking hate that chaptcha\n \nreply",
      "Our own solar system has what appear to be spiral arms. Very cool finding.\n \nreply"
    ],
    "link": "https://iopscience.iop.org/article/10.3847/1538-4357/adbf9b",
    "first_paragraph": "To ensure we keep this website safe, please can you confirm you are a human by ticking the box below. If you are unable to complete the above request please contact us using the below link, providing a screenshot of your experience.\nhttps://ioppublishing.org/contacts/\n"
  },
  {
    "title": "After court order, OpenAI is now preserving all ChatGPT user logs (laurenweinstein.org)",
    "points": 414,
    "submitter": "ColinWright",
    "submit_time": "2025-06-04T21:47:33 1749073653",
    "num_comments": 254,
    "comments_url": "https://news.ycombinator.com/item?id=44185913",
    "comments": [
      "> OpenAI is NOW DIRECTED to preserve and segregate all output log data that would otherwise be deleted on a going forward basis until further order of the Court (in essence, the output log data that OpenAI has been destroying), whether such data might be deleted at a user\u2019s request or because of \u201cnumerous privacy laws and regulations\u201d that might require OpenAI to do so.Spicy. European courts and governments will love to see their laws and legal opinions being shrugged away in ironic quotes.\n \nreply",
      "Will the EU respond by blocking them from doing business in the EU, given they're not abiding by GDPR?\n \nreply",
      "Hopefully.We need many strong AI players. This would be a great way to ensure Europe can grow its own.\n \nreply",
      "Note that this also applies to GPT models on the API> That risk extended to users of ChatGPT Free, Plus, and Pro, as well as users of OpenAI\u2019s application programming interface (API), OpenAI said.This seems very bad for their business.\n \nreply",
      "> This seems very bad for their business.Well, it is gonna be all _AI Companies_ very soon so unless everyone switches to local models which don't really have the same degree of profitability as a SaaS, its probably not going to kill a company to have less user privacy because tbh people are used to not having privacy these days on the internet.It certainly will kill off the few companies/people trusting them with closed source code or security related stuff but you really should not outsource that anywhere.\n \nreply",
      "Did an American court just destroy all American AI companies in favor of open weight Chinese models?\n \nreply",
      "afaik only OpenAI is enjoined in this\n \nreply",
      "Sure. But this means the rest of the AI companies are exposed to such risk; and there aren't that many of them (grok/gemini/anthropic).\n \nreply",
      "> It certainly will kill off the few companies/people trusting them with closed source code or security related stuff but you really should not outsource that anywhere.And how many companies have proprietary code hosted on Github?\n \nreply",
      "Some established businesses will need to review their contracts, regulations, and risk tolerance.And wrapper-around-ChatGPT startups should double-check their privacy policies, that all the \"you have no privacy\" language is in place.\n \nreply"
    ],
    "link": "https://mastodon.laurenweinstein.org/@lauren/114627064774788581",
    "first_paragraph": ""
  },
  {
    "title": "A proposal to restrict sites from accessing a users\u2019 local network (github.com/explainers-by-googlers)",
    "points": 238,
    "submitter": "doener",
    "submit_time": "2025-06-04T18:15:54 1749060954",
    "num_comments": 143,
    "comments_url": "https://news.ycombinator.com/item?id=44183799",
    "comments": [
      "I wish they'd (Apple/Micrsoft/Google/...) would do similar things for USB and Bluetooth.Lately, every app I install, wants bluetooth access to scan all my bluetooth devices. I don't want that. At most, I want the app to have to declare in their manifest some specific device IDs (short list) that their app is allowed to connect to and have the OS limit their connections to only those devices. For for example the Bose App should only be able to see Bose devices, nothing else. The CVS (pharmacy app) should only be able to connect to CVS devices, whatever those are. All I know is the app asked for permission. I denied it.I might even prefer if it had to register the device ids and then the user would be prompted, the same way camera access/gps access is prompted. Via the OS, it might see a device that the CVS.app registered for in its manifest. The OS would popup \"CVS app would like to connect to device ABC? Just this once, only when the app is running, always\" (similar to the way iOS handles location)By id, I mean some prefix that a company registers for its devices.  bose.xxx, app's manifest says it wants to connect to \"bose.*\" and OS filters.Similarly for USB and maybe local network devices. Come up with an id scheme, have the OS prevent apps form connecting to anything not that id. Effectively, don't let apps browser the network, usb, bluetooth.\n \nreply",
      "I like this on the first glance. The idea of a random website probing arbitrary local IPs (or any IPs for that matter) with HTTP requests is insane. I wouldn't care if it breaks some enterprise apps or integrations - enterprises could reenable this \"feature\" via management tools, normal users could configure it themselves, just show a popup \"this website wants to control local devices - allow/deny\".\n \nreply",
      "This is a misunderstanding. Local network devices are protected from random websites by CORS, and have been for many years. It's not perfect, but it's generally quite effective.The issue is that CORS gates access only on the consent of the target server. It must return headers that opt into receiving requests from the website.This proposal aims to tighten that, so that even if the website and the network device both actively want to communicate, the user's permission is also explicitly requested. Historically we assumed server & website agreement was sufficient, but Facebook's recent tricks where websites secretly talked to apps on your phone have broken that assumption - websites might be in cahoots with local network servers to work against your interests.\n \nreply",
      "Doesn't CORS just restrict whether the webpage JS context gets to see the response of the target request? The request itself happens anyway, right?So the attack vector that I can imagine is that JS on the browser can issue a specially crafted request to a vulnerable printer or whatever that triggers arbitrary code execution on that other device. That code might be sufficient to cause the printer to carry out your evil task, including making an outbound connection to the attacker's server. Of course, the webpage would not be able to discover whether it was successful, but that may not be important.\n \nreply",
      "No, a preflight (OPTIONS) request is sent by the browser first prior to sending the request initiated by the application.  I would be surprised if it is possible for the client browser to control this OPTIONS request more than just the URL.  I am curious if anyone else has any input on this topic though.Maybe there is some side-channel timing that can be used to determine the existence of a device, but not so sure about actually crafting and delivering a malicious payload.\n \nreply",
      "The idea is, the malicious actor would use a 'simple request' that doesn't need a preflight (basically, a GET or POST request with form data or plain text), and manage to construct a payload that exploits the target device. But I have yet to see a realistic example of such a payload (the paper I read about the idea only vaguely pointed at the existence of polyglot payloads).\n \nreply",
      "There doesn't need to be any kind of \"polyglot payload\". Local network services and devices that accept only simple HTTP requests are extremely common. The request will go through and alter state, etc.; you just won't be able to read the response from the browser.\n \nreply",
      "Exactly. People who are answering must not have been aware of \u201csimple\u201d requests not requiring preflight.\n \nreply",
      "Oh, you can only send arbitrary text or form submissions. That\u2019s SO MUCH.\n \nreply",
      "Here's a formal definition of such simple requests, which may be more expansive than one might expect: https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/COR...\n \nreply"
    ],
    "link": "https://github.com/explainers-by-googlers/local-network-access",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A proposal to restrict sites from accessing a users' local network without permission\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.This proposal is an early design sketch by the Chrome Secure Web and Network team to describe the problem below and solicit feedback on the proposed solution. It has not been approved to ship in Chrome.Currently public websites can probe a user's local network, perform CSRF attacks against vulnerable local devices, and generally abuse the user's browser as a \"confused deputy\" that has access inside the user's local network or software on their local machine. For example, if you visit evil.com it can use your browser as a springboard to attack your printer (given an HTTP accessible printer exploit).Local Network Access aims to "
  },
  {
    "title": "Why I wrote the BEAM book (happihacking.com)",
    "points": 418,
    "submitter": "lawik",
    "submit_time": "2025-06-04T10:36:49 1749033409",
    "num_comments": 117,
    "comments_url": "https://news.ycombinator.com/item?id=44179257",
    "comments": [
      "Here is the link to the git repo - https://github.com/happi/theBeamBook",
      "> I kept going because I wanted to understand the BEAM properly. There\u2019s value in following the real logic, not just the surface explanations.This resonates with me. That's the kind of drive that results in great output. Buying it just for that.I've been approached by publishers several times throughout my career. Each time the process was similar: they had an idea, I had an idea, we tried to come to common ground, and then the deal fell through because we couldn't find any. E.g. I didn't want to write a Java book aimed at 14 year olds. They didn't want me to write about classloaders (or whatever niche subject I was diving into at the time).Would love to learn how people find (non-empty) intersections of their passions & what readers want.\n \nreply",
      "Having written three books, what I found was that you either self publish or you write books that publishers want. Some of that is choosing publishers that specialize in certain types of books. Some publishers want \u201cLearn AI in 21 Minutes with Python,\u201d while others want \u201cDeep Dark Secrets of Java Class Loaders.\u201d O\u2019Reilly is the best for niche technical stuff. Most of the rest of the industry wants beginner stuff because that\u2019s where the volume is. Fortunately, self-publishing is easier than ever and various sites make it easy to sell copies online, so you don\u2019t necessarily have to just give it away for free. But yea, there\u2019s no magic formula here. If you really want to write something niche, don\u2019t expect that a publisher will be interested in. Expect that you\u2019ll have to self publish and promote it yourself.\n \nreply",
      "That\u2019s my experience too. My most recent book (my fifth) was closer to one publisher\u2019s stated mission than 95% of its catalog, but they all but admitted that they had given up on it. After that rejection I didn\u2019t have the energy to approach anyone else and did it myself instead. LeanPub first, then Amazon (print and kindle), Apple, Kobo, and Google.\n \nreply",
      "Yea, good call. My books were all published back in the 1990s, back when paper was still king and you really couldn\u2019t self-publish. Today, I\u2019d probably look much more heavily at the self-publishing route, unless I wanted to write something uber-commercial. What was your experience with the order volume you could drive via self-publishing? Did you do any of your own marketing, or did you just list it in LeanPub and Amazon and let search engines lead people to it organically. If your subject was niche, there isn\u2019t much volume in any case, but you want to get all you can of that small market.\n \nreply",
      "More niche than mainstream I guess, but I do have what could be called a community, a large following on LinkedIn, a mailing list in the low thousands, paid subscribers, and a partner programme. Not enough to generate the kind of sales that anyone could live on, but not nothing either. Much of my blogging since publication has been in support of the book.Two of my five did have publishers. I\u2019m grateful for that experience. Learned a lot!\n \nreply",
      "Nice! Thanks for the follow up.\n \nreply",
      "> I kept going because I wanted to understand the BEAM properly. There\u2019s value in following the real logic, not just the surface explanations.Teaching is the best way to learn. I found that out when I started tutoring classmates for math in high school.Same thing with writing a book. Something about learning a subject and turning around to speak/write out about it that really crystallizes the in-depth understanding beneath the surface.\n \nreply",
      "It is easy to fool oneself into thinking one knows how something works, when in reality one only has a surface level understanding without really knowing about the internals. The serialization process exposes all the dangling pointers.Your average person \"knows\" how a toilet works; water is pumped in to fill the cistern, released into the bowl when you pull the plunger, and flushes out the drain. Ask them to explain in detail how that happens, and most realize that they don't actually know how the cistern doesn't just keep filling until it overflows, or how it's not constantly leaking water into the bowl, or how the bowl can be flushed while neither overflowing nor draining completely.\n \nreply",
      "If you have something interesting to say then people will figure a way to understand it. Early in my career I remember coming across \u201cEssential .Net\u201d by Don Box and I don\u2019t think Don Box had a particular audience in mind. He just unwrapped what\u2019s under the hood in Common Language Runtime (CLR). It took me multiple times to really understand \u201cessential .net\u201d.\n \nreply"
    ],
    "link": "https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/",
    "first_paragraph": "Happi Hacking ABKIVRA: 556912-2707106 31 Stockholm+46735047442info@happihacking.se\ncareers@happihacking.seLinkedInAfter ten years of keeping Klarna\u2019s core system upright I know this: a 15\nmillisecond pause in the BEAM can stall millions of peak-shopping payments, trigger a 3 a.m. Christmas-Eve post-mortem, and earn you a very awake call from the CEO. I wrote The BEAM Book so the next engineer fixes that pause before the coffee cools.I opened the project on 12 October 2012 with a lone DocBook file with four lines of text and an oversized sense of optimism.\nAfter two weeks, the commit log is mostly me adding structure, moving\nheadings, and updating metadata. Most of it is scaffolding. The actual\ncontent is still just a few hopeful lines.By November I had abandoned DocBook for AsciiDoc, written a custom build\nscript, and convinced myself the book could be wrapped up in six months.\nThose early commits glow with energy: adds, rewrites, then more\nrewrites to fix the rewrites.\nDelusion is und"
  },
  {
    "title": "The iPhone 15 Pro\u2019s Depth Maps (marksblogg.com)",
    "points": 214,
    "submitter": "marklit",
    "submit_time": "2025-06-04T17:57:15 1749059835",
    "num_comments": 57,
    "comments_url": "https://news.ycombinator.com/item?id=44183591",
    "comments": [
      "Other commenters here are correct that the LIDAR is too low-resolution to be used as the primary source for the depth maps. In fact, iPhones use four-ish methods, that I know of, to capture depth data, depending on the model and camera used. Traditionally these depth maps were only captured for Portrait photos, but apparently recent iPhones capture them for standard photos as well.1. The original method uses two cameras on the back, taking a picture from both simultaneously and using parallax to construct a depth map, similar to human vision. This was introduced on the iPhone 7 Plus, the first iPhone with two rear cameras (a 1x main camera and 2x telephoto camera.) Since the depth map depends on comparing the two images, it will naturally be limited to the field of view of the narrower lens.2. A second method was later used on iPhone XR, which has only a single rear camera, using focus pixels on the sensor to roughly gauge depth. The raw result is low-res and imprecise, so it's refined using machine learning. See: https://www.lux.camera/iphone-xr-a-deep-dive-into-depth/3. An extension of this method was used on an iPhone SE that didn't even have focus pixels, producing depth maps purely based on machine learning. As you would expect, such depth maps have the least correlation to reality, and the system could be fooled by taking a picture of a picture. See: https://www.lux.camera/iphone-se-the-one-eyed-king/4. The fourth method is used for selfies on iPhones with FaceID; it uses the TrueDepth camera's 3D scanning to produce a depth map. You can see this with the selfie in the article; it has a noticeably fuzzier and low-res look.You can also see some other auxiliary images in the article, which use white to indicate the human subject, glasses, hair, and skin. Apple calls these portrait effects mattes and they are produced using machine learning.I made an app that used the depth maps and portrait effects mattes from Portraits for some creative filters. It was pretty fun, but it's no longer available. There are a lot of novel artistic possibilities for depth maps.\n \nreply",
      "> but apparently recent iPhones capture them for standard photos as well.Yes, they will capture them from the main photo mode if there\u2019s a subject (human or pet) in the scene.> I made an app that used the depth maps and portrait effects mattes from Portraits for some creative filters. It was pretty fun, but it's no longer availableWhat was your app called? Is there any video of it available anywhere? Would be curious to see it!I also made a little tool, Matte Viewer, as part of my photo tool series - but it\u2019s just for viewing/exporting them, no effects bundled:https://apps.apple.com/us/app/matte-viewer/id6476831058\n \nreply",
      "> Yes, they will capture them from the main photo mode if there\u2019s a subject (human or pet) in the scene.One of the example pictures on TFA is a plant. Given that, are you sure iOS is still only taking depth maps for photos that get the \"portrait\" icon in the gallery? (Or have they maybe expanded the types of possible portrait subjects?)\n \nreply",
      "It will capture the depth map and generate the semantic mattes (except in some edge cases) no matter the subject if you explicitly set the camera in Portrait mode, which is how I would guess the plant photo from the article was captured.My previous comment was about the default Photo mode.If you have a recent iPhone (iPhone 15 or above iirc) try it yourself - taking a photo of a regular object in the standard Photo mode won\u2019t yield a depth map, but one of a person or pet will. Any photo taken from the Portrait mode will yield a depth map.You can find out more about this feature by googling \u201ciPhone auto portrait mode\u201d.Apple\u2019s documentation is less helpful with the terminology; they call it \u201cApply the portrait effect to photos taken in Photo mode\u201dhttps://support.apple.com/guide/iphone/edit-portrait-mode-ph...\n \nreply",
      "> [...]  if you explicitly set the camera in Portrait mode, which is how I would guess the plant photo from the article was captured.Ah, that makes sense, thank you!\n \nreply",
      "For method 3 that article is 5 years old, see: https://github.com/apple/ml-depth-pro?tab=readme-ov-file\n \nreply",
      "Cool article. I assume these depth maps are used for the depth of field background blurring / faux bokeh in \"Portrait\" mode photos. I always thought it was interesting you can change the focal point and control the depth of field via the \"aperture\" after a photo is taken, though I really don't like the look of the fake bokeh. It always looks like a bad photoshop.I think there might be a few typos of the file format?- 14 instances of \"HEIC\"- 3 instances of \"HIEC\"\n \nreply",
      "Fixed those. Cheers for pointing them out.\n \nreply",
      "I think the reason it looks fake is because they actually have the math wrong about how optics and apertures work, and they make some (really bad) approximations but from a product standpoint can please 80% of people.I could probably make a better camera app with the correct aperture math, I wonder if people would pay for it or if mobile phone users just wouldn't be able to tell the difference and don't care.\n \nreply",
      "There are a few projects now that simulate defocus properly to match what bigger (non-phone camera) lenses do - I hope to get back to working on it this summer but you can see some examples here: https://x.com/dearlensformThose methods come from the world of non-realtime CG rendering though - running truly accurate simulations with the aberrations changing across the field on phone hardware at any decent speed is pretty challenging...\n \nreply"
    ],
    "link": "https://tech.marksblogg.com/apple-iphone-15-pro-depth-map-heic.html",
    "first_paragraph": "I have 15 years of consulting & hands-on build experience with clients in the UK, USA, Sweden, Ireland & Germany. Past clients include Bank of America Merrill Lynch, Blackberry, Bloomberg, British Telecom, Ford, Google, ITV, LeoVegas, News UK, Pizza Hut, Royal Mail, T-Mobile, Williams Formula 1, Wise & UBS. I hold both a Canadian and a British passport. My CV, Twitter & LinkedIn.\n      \nHome\n        | Benchmarks\n\n        | Categories\n\n            | Atom Feed\nPosted on Wed 04 June 2025  under PythonSince 2017, Apple have supported depth maps in the images its iPhones capture either via LiDAR scanners, 3D time-of-flight scanner-less LIDAR or structured-light 3D scanning.These depth maps, along with other imagery, are stored in High Efficiency Image File Format (HEIF) container files. These files can contain multiple images and vast amounts of metadata. This format was originally designed between 2013 and 2015 and Apple adopted its HEIC variant in 2017.Since then, HEIC files are the defau"
  },
  {
    "title": "Tesla seeks to guard crash data from public disclosure (reuters.com)",
    "points": 101,
    "submitter": "kklisura",
    "submit_time": "2025-06-04T23:40:47 1749080447",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=44186780",
    "comments": [
      "Wife has a relative who was just (this weekend) in a major accident where a tesla ran into them and pushed their a ditch where it rolled a few times.  Initial report says the Tesla was in self drive mode.  Will be interesting to see who was at fault here but so far it is not looking good for Tesla.\n \nreply",
      "> ran into them and pushed their a ditch where it rolled a few timesthat sounds rough; hopefully they're OK! did the car drive into them from the side or from behind?where did it happen? googling \"Tesla ditch self-driving accident\" turns up nothing, but I would have thought it would have made the news.\n \nreply",
      "Seems like culpability should come down to whether or not the Telsa driver could have prevented the accident.Although there's a good argument to be made that Tesla's entire system has fundamental design flaws which they have negligently disregarded.\n \nreply",
      "Unless there's a very good reason, if National Highway Transportation Safety Administration has it then the taxpayers who paid for it should have access too.\n \nreply",
      "Provided they release crash data for all manufacturers and don't single out just one manufacturer.\n \nreply",
      "Crash data for all other ADAS systems is already public [1]. The only manufacturer with heavily redacted information in that data to the point of being useless is Tesla.[1] https://www.nhtsa.gov/laws-regulations/standing-general-orde...\n \nreply",
      "I agree, although that's more about the request(er) than anything else.\n \nreply",
      "Uhh, WaPo was requesting crash data from NHTSA on driver assistance systems. Tesla is the only manufacturer trying to prevent that disclosure.\n \nreply",
      "Please provide proof that they are selectively withholding data based on manufacturer.  Should be trivial if this is indeed an issue.We'll wait.\n \nreply",
      "I guess they haven\u2019t doge\u2019d enough people to bury this\n \nreply"
    ],
    "link": "https://www.reuters.com/legal/government/musks-tesla-seeks-guard-crash-data-public-disclosure-2025-06-04/",
    "first_paragraph": ""
  },
  {
    "title": "The Prompt Engineering Playbook for Programmers (addyo.substack.com)",
    "points": 173,
    "submitter": "vinhnx",
    "submit_time": "2025-06-04T15:58:57 1749052737",
    "num_comments": 63,
    "comments_url": "https://news.ycombinator.com/item?id=44182188",
    "comments": [
      "In my experience there's really only three true prompt engineering techniques:- In Context Learning (providing examples, AKA one shot or few shot vs zero shot)- Chain of Thought (telling it to think step by step)- Structured output (telling it to produce output in a specified format like JSON)Maybe you could add what this article calls Role Prompting to that.  And RAG is its own thing where you're basically just having the model summarize the context you provide.  But really everything else just boils down to tell it what you want to do in clear plain language.\n \nreply",
      "Regarding point 3, my colleagues and i studied this for a use case in science: https://doi.org/10.1038/s41467-024-45563-x\n \nreply",
      "Can you provide a \"so what?\" summary?\n \nreply",
      "Even role prompting is totally useless imo. Maybe it was a thing with GPT3, but most of the LLMs already know they're \"expert programmers\". I think a lot of people are just deluding themselves with \"prompt engineering\".Be clear with your requirements. Add examples, if necessary. Check the outputs (or reasoning trace if using a reasoning model). If they aren't what you want, adjust and iterate. If you still haven't got what you want after a few attempts, abandon AI and use the reasoning model in your head.\n \nreply",
      "As a clarification, we used fine tuning more than prompt engineering because low or few-shot prompt engineering did not work for our use case.\n \nreply",
      "Did you find it hard to create structured output while also trying to make it reason in the same prompt?\n \nreply",
      "You use a two-phase prompt for this. Have it reason through the answer and respond with a clearly-labeled 'final answer' section that contains the English description of the answer. Then run its response through again in JSON mode with a prompt to package up what the previous model said into structured form.The second phase can be with a cheap model if you need it to be.\n \nreply",
      "Great, will try this! But, in a chain-based prompt or full conversational flow?\n \nreply",
      "Sometimes I get the feeling that making super long and intricate prompts reduces the cognitive performance of the model. It might give you a feel of control and proper engineering, but I'm not sure it's a net win.My usage has converged to making very simple and minimalistic prompts and doing minor adjustments after a few iterations.\n \nreply",
      "That's exactly how I started using them as well. 1. Give it just enough context, the assumptions that hold and the goal. 2. Review answer and iterate on the initial prompt. It is also the economical way to use them. I've been burned one too many times by using agents (they just spin and spin, burn 30 dollars for one prompt and either mess the code base or converge on the previous code written ).I also feel the need to caution others that by letting the AI write lots of code in your project it makes it harder to advance it, evolve it and just move on with confidence (code you didn't think about and write it doesn't stick as well into your memory).\n \nreply"
    ],
    "link": "https://addyo.substack.com/p/the-prompt-engineering-playbook-for",
    "first_paragraph": ""
  },
  {
    "title": "Apple Notes Expected to Gain Markdown Support in iOS 26 (macrumors.com)",
    "points": 131,
    "submitter": "danso",
    "submit_time": "2025-06-04T18:29:46 1749061786",
    "num_comments": 76,
    "comments_url": "https://news.ycombinator.com/item?id=44183923",
    "comments": [
      "Markdown is the defacto language of the llms. Between this and the python library from Microsoft that converts all of their formats to markdown I think that markdown officially has won the markup language war.\n \nreply",
      "I would not recommend investing in the iOS Notes ecosystem. There is no way to export these notes with their metadata and media intact without using iCloud. If you need a note-taking app, go with Joplin instead; it supports Markdown, has both iOS and desktop clients, and supports self-hosted syncing.\n \nreply",
      "You can get at them with AppleScript. I wrote a tool for that a while ago: https://github.com/dogsheep/apple-notes-to-sqlite\n \nreply",
      "Which iOS doesn't support.\n \nreply",
      "Of course not, no developer has access to Notes on iOS for export except \uf8ffI applaud the effort simonw put into this, it works great on macOS... the platform AppleScript runs on.\n \nreply",
      "You can connect iOS Notes with any IMAP server. This supports a reduced feature set (no embedded images, tables, or audio) but does allow for self-hosting.\n \nreply",
      "This is great until you edit your notes on IMAP from more than one app. Apple Notes, Outlook, Gmail, and others with access to IMAP notes will corrupt one another's entries very easily. Style is also very difficult to control in this setup.Apple Notes is not intended for people who want to own their data and have control over export\n \nreply",
      "Interesting. Did they add this feature recently? How do you set it up?Embedded files (photos, video, audio, PDFs, etc.) and bullet hierarchies were the two things that I remember being the most trouble; you can copy bullet hierarchies from iOS Notes and paste them into another app, but they aren\u2019t detected when copying the note via Shortcuts. Embedded files had to be manually saved. I think you could Airdrop notes to a MacOS device, but it would wipe the date created metadata from what I remember.\n \nreply",
      "I wrote python code to extract the notes years ago[1], but it's bit-rotted, so it no longer supports tables, raw stroke data, or the newer features like equations. Maybe I'll update it someday, but I'm not using notes at the moment and have too many other things going one.https://github.com/dunhamsteve/notesutils\n \nreply",
      "New term to add to the lexicon... bit-rot. I like it.\n \nreply"
    ],
    "link": "https://www.macrumors.com/2025/06/04/apple-notes-rumored-markdown-support-ios-26/",
    "first_paragraph": ""
  },
  {
    "title": "Authentication with Axum (mattrighetti.com)",
    "points": 10,
    "submitter": "mattrighetti",
    "submit_time": "2025-06-04T23:33:41 1749080021",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44186736",
    "comments": [
      "This is very useful, thanks. I've implemented similar authentication with tower_cookies in a home project, and it took me a long time to figure out; Axum is powerful but not always the most intuitive. It was a great learning experience, but I'd love to offload more of this to a crate next time.\n \nreply",
      "Not sure how this is better or easier than Neon Auth mentioned a few articles further down on today's HN homepage: https://news.ycombinator.com/item?id=44184849\n \nreply"
    ],
    "link": "https://mattrighetti.com/2025/05/03/authentication-with-axum",
    "first_paragraph": ""
  },
  {
    "title": "Cockatoos have learned to operate drinking fountains in Australia (science.org)",
    "points": 307,
    "submitter": "pseudolus",
    "submit_time": "2025-06-04T09:42:29 1749030149",
    "num_comments": 139,
    "comments_url": "https://news.ycombinator.com/item?id=44178902",
    "comments": [
      "Cockies are the pranksters of the bird world. They're smart and they think it's hilarious to mess with each other and anyone else. They also tear everything to pieces. So it's no surprise really that if any bird worked out how to operate a drinking fountain it'd be these hilarious little jerks.\n \nreply",
      "I was visiting a place that takes in rescue animals, in this case they had a lot of birds.In their typical speech to people about NOT keeping birds as pets they described some of the birds as \"highly curious, the maturity of a human 5 year old, with an intense desire to be destructive\".\n \nreply",
      "My wife always joke about how parrots sound like a fun pet until you consider the phrase \"Flying eternal toddlers, that cannot be diapered or potty-trained, with can-opener mouths.\"\n \nreply",
      "On top of that, they have one tool, and it's a pair of boltcutters you can't take away. And the most clever of them have a good chance to outlive their owners.\n \nreply",
      "And the means to achieve that destruction. Cockatoos are like flying bolt cutters.\n \nreply",
      "I aspire to one day befriend a local murder of crows.  Not to keep as pets or to make dependent on me, but maybe to bribe to clean up trash or steal quarters for me... or to defend my honor should the need arise.\n \nreply",
      "If they are the pranksters, I wonder what that makes the Kea.  I think they are counted as smarter, they definitely enjoy a bit of malicious fun.\n \nreply",
      "I liked the Kea messing with traffic cones and redirecting traffic, apparently slowing cars and getting fed.https://www.youtube.com/watch?v=yZ4Y7svFgnQ\n \nreply",
      "The most accurate representation of \"Chaotic Neutral\" - the cheeky bastards love stealing ANYTHING, and when there's nothing to steal they'll start ripping the rubber off your car door seals (or windshield wipers).They are amazing birds, very deserving of the name \"Clown of the Mountains\".\n \nreply",
      "I'll never understand why we New Zealanders chose a flightless defenseless bird as our national bird when we have so many other great candidates.\n \nreply"
    ],
    "link": "https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia",
    "first_paragraph": ""
  },
  {
    "title": "PromptArmor (YC W24) Is Hiring in San Francisco (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-06-04T22:35:56 1749076556",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/promptarmor/jobs/hZ3xFlj-founding-engineer-full-stack",
    "first_paragraph": "LLM Security and ComplianceWe break LLM Applications.Or at least, that\u2019s how we started. This is core to our DNA, puts us on the cutting edge of AI security, and you can get an idea of some of our discoveries below:We\u2019ve been using this expertise to build defenses, helping the largest enterprises in America securely accelerate their AI journey by solving the most complex, net-new AI security problem set that exists today (hint: not content moderation).We work with teams from the cutting edge AI-native unicorn to iconic Fortune 50s, to the biggest law firms in the world.We are building one of the only product platforms in AI security that people actually want.Here\u2019s some other things you might care about (but hopefully less than the above):The biggest barrier to enterprise AI adoption is security. This is a hard, category-defining problem.But the rewards are immense. If we win, we will enable trillions in economic value and unlock human potential.We intend to win. And we hope you join u"
  },
  {
    "title": "Not All Tokens Are Meant to Be Forgotten (arxiv.org)",
    "points": 12,
    "submitter": "MarcoDewey",
    "submit_time": "2025-06-04T23:15:48 1749078948",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44186607",
    "comments": [
      "> However, they tend to memorize unwanted information, such as private or copyrighted content,I mean humans don't forget copyrighted information. We just typically adjust it enough (some of the time) to avoid getting a copyright strike while modifying it in some way useful.We don't forget 'private' information either. We might not tell other people that information, but it still influences our thoughts.The idea of a world where we have AI minds forget vast amounts of information that humans have to deal with every day is concerning and dystopian to me.\n \nreply",
      "I'd counter with an anecdote; I had a colleague that boasted how he memorized a classmate's SSN in college and would greet him by SSN when seeing him years later. Is the goal of AI to replicate the entirety of the human experience (including social pressures, norms, and shame) or a tool to complement human decision making?While, yes, you can argue the slippery slope, it may be advantageous to flag certain training material as exempt. We as humans often make decisions without perfect knowledge, and \"knowing more\" isn't a guarantee that it produces better outcomes, given the types of information consumed.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2506.03142",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "IRS Direct File on GitHub (chrisgiven.com)",
    "points": 435,
    "submitter": "nickthegreek",
    "submit_time": "2025-06-04T16:16:39 1749053799",
    "num_comments": 189,
    "comments_url": "https://news.ycombinator.com/item?id=44182356",
    "comments": [
      "> Direct File also incorporates the Fact Graph, a declarative, XML-based knowledge graph data structure that is designed to reason about incomplete information, such as a partially completed tax return. The Fact Graph is written in the Scala programming language; it runs on the JVM on the backend and is transpiled via Scala.js to run on the client as well. Direct File's Fact Graph is not domain-specific, and it may be useful to revenue agencies and as a reference for business rules engine implementations.\n \nreply",
      "The code that defines how the fact graph works is here: https://github.com/IRS-Public/direct-file/tree/main/direct-f...but the actual tax definitions that deal with facts and derived calculation are here:\nhttps://github.com/IRS-Public/direct-file/tree/main/direct-f...See for example the standard deduction and tax rate calculations\nhttps://github.com/IRS-Public/direct-file/blob/main/direct-f...\nhttps://github.com/IRS-Public/direct-file/blob/main/direct-f...I imagine these are based on the MeF (Modernized e-File) schemas because the system needs to transform the input data into XML MeF schemas to submit electronically to the MeF system (See https://www.irs.gov/e-file-providers/modernized-e-file-mef-s...)\n \nreply",
      "Interesting, I want to read more about this.\n \nreply",
      "https://news.ycombinator.com/item?id=44183845\n \nreply",
      "Sadly this program is being killed by the current admin. This repo looks great. The scala fact graph is super neat and there is clearly a lot of care that went into making the tutorial for it.\n \nreply",
      "I'm sure everyone working on this knew it was doomed before the first line of code was written, and that it would be killed as soon as the next (R) was in charge. It was a great accomplishment to get working software released before that happened, but I'm sure nobody was kidding themselves into thinking it would last. The pay-to-file tax lobby is too strong and corrupt.\n \nreply",
      "I don't know about that. Inertia is a strong force but it goes in both directions. Had this administration been a Democratic one four years might be long enough to establish it strongly enough that it would be very difficult to remove. Look at the Affordable Care Act. Imperfect though it was, Republicans have pledged over and over that they're going to get rid of it but when it power it seems they just can't.\n \nreply",
      "I hope you're right, but this administration so far has found almost no limit to the number of projects, lives, roles and institutions it can destroy or at least attempt to destroy. And the party that is supposed to be acting as the Opposition is basically letting them do whatever they want unhindered. Unless you consider \"holding up little signs and making frowny-faces\" to count as \"doing something.\"\n \nreply",
      "> Opposition is basically letting them do whatever they want unhinderedwhat do you propose they do, being entirely out of power?\n \nreply",
      "I think it's less about trying to get wins (most understand that congress dems are the minority) and more about resisting these anti-citizen acts as much as possible. Having 4 or 5 democrats vote on an R bill a few months back really put a nasty taste in people's mouths. Among all the crazy cabinet appointments that some voted onBasically, the democrats want fighters. We're well past bi-partisan tasks where we should just faciliate any kind of bill that comes up. Because very few are reasonable.\n \nreply"
    ],
    "link": "https://chrisgiven.com/2025/05/direct-file-on-github/",
    "first_paragraph": "The IRS has now published the vast majority of Direct File\u2019s code on GitHub as open-source software. As a work of the U.S. government, Direct File is in the public domain. And now everyone can check it out.Releasing Direct File\u2019s source code demonstrates that the IRS is fulfilling its obligations under the SHARE IT Act (three weeks ahead of schedule!). Now that Direct File has paved the way, I hope that more of the IRS\u2019s code, paid for with taxpayer dollars, will soon be available to all of us.Open sourcing Direct File has long been planned, and even longer desired. Explaining last May why open source is particularly important for Direct File, the team wrote:The IRS could take further steps to build public trust and enable independent assessment of its work. The Direct File product team was given the mandate to develop software that ensures every taxpayer receives the full benefit of any tax provisions for which they are eligible. Releasing components of Direct File as open-source soft"
  },
  {
    "title": "Ada and SPARK enter the automotive ISO-26262 market with Nvidia (adacore.com)",
    "points": 37,
    "submitter": "gneuromante",
    "submit_time": "2025-06-04T19:59:51 1749067191",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=44184861",
    "comments": [
      "Wow. I developed in Ada in aerospace 17 years ago. I thought the industry would move toward Rust.\n \nreply",
      "Ada/SPARK root of trust anchors Nvidia GPU market segmentation licensing, 50% profit margin and $3T market cap.\"Nvidia Security Team: \u201cWhat if we just stopped using C?\u201d, 170 comments (2022), https://news.ycombinator.com/item?id=42998383\n \nreply",
      "But why? The US military abandoned Ada for the F-35 and moved to C++ instead. Is it me or are they moving backwards?\n \nreply",
      "Everything I've heard about it was that it was pressure from contractors because they didn't like training or finding Ada talent.I get that there's more tools for C++ but first class formal verification support and a language that's generally designed to save you from yourself seems like something you would stand your ground on. Ada is supremely good at killing people and/or keeping them un-killed, there's a reason you still see it kicking around in defense and aerospace despite the common perception that it's a bygone relic.\n \nreply",
      "you mean the plane plagued with software development issues[1][2]?\neven ignoring the obnoxious yellow journalism, i'm not sure it's a shining beacon of best practices. or that replacing one ancient language with an even more ancient language is \"moving forwards.\"particularly when the language you're replacing was explicitly designed for your domain, and the language you're replacing it with is an entropic event horizon from which no coherent thoughtform can escape.[1] - https://www.dote.osd.mil/Portals/97/pub/reports/FY2024/other...[2] - https://www.gao.gov/products/gao-24-107177\n \nreply",
      "It was not for technical reasons. They needed more programmers and C++ had larger user base.Ada is technically better choice.\n \nreply",
      "It has more to do with tooling than programmers. C++ is used everywhere so there are many commercial tools to support it. Not so much for Ada.Ada's developmemt tools are fewer, less featued, and more costly due to low demand.\n \nreply",
      "> Ada's developmemt tools are fewer, less featuedSuch as?\n \nreply",
      "Regarding the number of options, C++ has quite a few IDEs: Visual Studio, Xcode, VSCode, CLion, and probably more (Oracle probably still sells the one they had for Solaris). For command-line compilers, C++ has: Visual Studio, Xcode, g++, clang++, IBM C++ compilers for their OSs, Oracle compilers for Solaris, etc.For Ada, is there anything other than AdaCore? Is that the same as GNATStudio?Edit* - fixed Ada capitalization\n \nreply",
      "The kind of tooling you need for avionics is not necessarily the kind of tooling you use for non-safety critical code.None of the tools mentioned, other than in limited level IDEs (for practical purposes in safety critical C++ expect some niche variation of Eclipse just like if you used Ada) are valid for the F-35 project."
    ],
    "link": "https://www.adacore.com/press/ada-and-spark-enter-the-automotive-iso-26262-market-with-nvidia",
    "first_paragraph": "Login to GNAT TrackerHigh-integrity software tooling experts, AdaCore, are delighted to announce the introduction of the Ada and SPARK programming languages into the automotive market. Together with their partner NVIDIA, they are set to publish an off-the-shelf reference process, allowing others to follow their lead.NVIDIA developed Drive\u00ae OS, the reference operating system and associated software stack designed specifically for developing and deploying autonomous vehicle applications on DRIVE AGX-based hardware.This system includes software components that comply with the highest levels of integrity of the automotive certification standard ISO-26262. To achieve that effort, NVIDIA selected these languages to develop some of the most critical components of its software stack. This required establishing a development process that takes advantage of formal methods and other safety characteristics of Ada and SPARK, thus fully leveraging their capabilities.AdaCore and NVIDIA have decided t"
  },
  {
    "title": "When memory was measured in kilobytes: The art of efficient vision (softwareheritage.org)",
    "points": 71,
    "submitter": "todsacerdoti",
    "submit_time": "2025-06-04T16:46:24 1749055584",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=44182698",
    "comments": [
      "I want to believe that however obsolete these old algorithms are today, at least some aspects of the underlying code and/or logic should prove useful to LLMs as they try to generate modern code.\n \nreply",
      "The idea that ML is the only way to do computer vision is a myth.Yes, it may not make sense to use classical algorithms to try to recognize a cat in a photo.But there are often virtual or synthetic images which are produced by other means or sensors for which classical algorithms are  applicable and efficient.\n \nreply",
      "I worked (as an intern) on autonomous vehicles at Daimler in 1991. My main project was the vision system, running on a network of transputer nodes programmed in Occam.The core of the approach was \u201cfind prominent horizontal lines, which exhibit symmetry about a vertical axis, and frame-to-frame consistency\u201d.Finding horizontal lines was done by computing variances in value. Finding symmetry about a vertical axis was relatively easy. Ultimately, a Kalman filter worked best for frame-to-frame tracking. (We processed video in around 120x90 output from variance algorithm, which ran on a PAL video stream.)There\u2019s probably more computing power on a $10 ESP32 now, but I really enjoyed the experience and challenge.This was our vehicle: https://mercedes-benz-publicarchive.com/marsClassic/en/insta...\n \nreply",
      "Any recommendations on background reading for classical CV for radar?\n \nreply",
      "I don\u2019t know anything about radar. I have a book called \u201cmachine vision\u201d (Shmuck, Jain, Kasturi) easy undergrad level, but also very useful. It\u2019s $6 on Amazon.\n \nreply",
      "Maybe\u2026 some of these algorithms from the 1980s struggled to do basic OCR, so they may need a lot of modification to be useful.\n \nreply",
      "That whole approach of \"find edges, convert to line drawing, process a line drawing\" in the 1980s struggled to do anything at all.\n \nreply",
      "There was a surprising amount of useful OCR happening in the 70\u2019s.High error rates and significant manual rescanning can be acceptable in some applications, as long as there\u2019s no better alternative.\n \nreply",
      "I find that modern OCR, audio transcription, etc... are beginning to have the opposite problem: they are too smart.It means that they make a lot fewer mistakes, but when they do, it can be subtle. For example, if the text is \"the bat escaped by the window\", a dumb OCR can write \"dat\" instead of \"bat\". When you read the resulting text, you notice it and using outside clues, recover the original word. An smart OCR will notice that \"dat\" isn't a word and can change it for \"cat\", and indeed \"the cat escaped by the window\" is a perfectly good sentence, unfortunately, it is wrong and confusing.\n \nreply",
      "Amazing. Wonder how fast it would be on a modern computer\n \nreply"
    ],
    "link": "https://www.softwareheritage.org/2025/06/04/history_computer_vision/",
    "first_paragraph": " June 4, 2025By Mathilde FichenIn the early days of computer vision, when memory was scarce and every byte counted, innovation thrived under constraint. \u201cAn Efficient Chain-Linking Algorithm,\u201d developed at Inria in the late 1980s, is a brilliant example of this spirit. Now preserved and shared by Software Heritage, this compact yet powerful piece of C code showcases how elegance and efficiency went hand in hand in outlining the future of image processing\u2014one pixel chain at a time.The code resulted from research work carried out between 1985 and 1991 at Inria, by G\u00e9rard Giraudon (research and principal investigator), Philippe Garnesson (a PhD student), and Patrick Cipi\u00e8re (software engineer). Down in sunny Sophia Antipolis, a tech park 20 minutes inland from Antibes, the team tackled computer vision with a distinctly local flavor. They called themselves PASTIS, a playful nod to the anise drink. Still, the acronym \u2013 Scene Analysis and Symbolic Image Processing Project (Projet d\u2019Analyse d"
  },
  {
    "title": "How big tech monopolies made the internet worse (cascadepbs.org)",
    "points": 3,
    "submitter": "Improvement",
    "submit_time": "2025-06-05T00:23:35 1749083015",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.cascadepbs.org/news/2025/06/how-big-tech-monopolies-made-internet-worse",
    "first_paragraph": "In an interview with the\u00a0On the Media\u00a0podcast, writer Cory Doctorow explains how a noncompetitive market has deteriorated the user experience.Micah Loewinger interviews author and technology activist Cory Doctorow for an episode of WNYC\u2019s On the Media podcast at the Cascade PBS Ideas Festival. (Christopher Nelson for Cascade PBS)Maybe you found this article through a Google search. If you did, chances are you had to scroll for longer \u2014 and past more spammy websites and advertisements disguised as results \u2014 than you might have had to 10 or even five years ago.\u00a0It\u2019s not just your imagination, or nostalgia \u2014 the online world is in fact getting worse. It\u2019s harder to use, and jammed with more AI-generated dreck you don\u2019t want.\u00a0Or, as Cory Doctorow puts it, \u201censh-ttified.\u201dListen to the full sessions on the weekly Cascade PBS Ideas Festival podcast.\u201cThe thesis of ensh-ttification is whether or not you\u2019re paying for the product, you\u2019re the product,\u201d said Doctorow, a journalist and tech activis"
  },
  {
    "title": "A practical guide to building agents [pdf] (cdn.openai.com)",
    "points": 120,
    "submitter": "tosh",
    "submit_time": "2025-06-04T15:21:22 1749050482",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=44181700",
    "comments": [
      "Thanks for sharing this. I'm actually starting to explore integrating an agent into one of my SaaS solutions, based on a client request.To be honest, my experience with agents is still pretty limited, so I\u2019d really appreciate any advice, especially around best practices or a roadmap for implementation. The goal is to build something that can learn and reflect the company\u2019s culture, answer situational questions like \u201cwhat to do in this case,\u201d assist with document error checking, and generally serve as a helpful internal assistant.All of this came from the client's desire to have a tool that aligns with their internal knowledge and workflows.Is something like this feasible in terms of quality and reliability? And beyond hallucinations, are there major security or roadblocks concerns I should be thinking about?\n \nreply",
      "Ingesting documents and using natural language to search your org docs with an internal assistant sounds more like a good use case for RAG[1]. Agents are best when you need to autonomously plan and execute a series of actions[2]. You can combine the two but knowing when depends on the use case.I really like the OpenAI approach and how they outlined the thought process of when and how to use agents.[1] https://www.willowtreeapps.com/craft/retrieval-augmented-gen...[2] https://www.willowtreeapps.com/craft/building-ai-agents-with...\n \nreply",
      "Interesting, and thanks for explanations.In this case, the agent would also need to learn from new events, like project lessons learned, for example.Just curious: can a RAG[1] system actually learn from new situations over time in this kind of setup, or is it purely pulling from what's already there?\n \nreply",
      "Especially with a client, consider the word choices around \"learning\". When using llms, agents, or rag, the system isn't learning (yet) but making a decision based on the context you provide. Most models are a fixed snapshot. If you provide up to date information, it will be able to give you an output based on that.\"Learning\" happens when initially training the llm or arguably when fine-tuning. Neither of which are needed for your use case as presented.\n \nreply",
      "Thanks for the clarification, really appreciate it. It helps frame things more precisely.In my case, there will be a large amount of initial data fed into the system as context. But the client also expects the agent to act more like a smart assistant or teacher, one that can respond to new, evolving scenarios.Without getting into too much detail, imagine I feed the system an instruction like: \u201cBox A and Box B should fit into Box 1 with at least 1\" clearance.\u201d   Later, a user gives the agent Box A, Box B, and now adds Box D and E, and asks it to fit everything into Box 1, which is too small. The expected behavior would be that the agent infers that an additional Box 2 is needed to accommodate everything.So I understand this isn't \"learning\" in the training sense, but rather pattern recognition and contextual reasoning based on prior examples and constraints.Basically, I should be saying \"contextual reasoning\" instead of \"learning.\"Does that framing make sense?\n \nreply",
      "You can ingest new documents and data into the RAG system as you need\n \nreply",
      "I'm a big fan of https://github.com/humanlayer/12-factor-agents because I think it gets at the heart of engineering these systems for usage in your app rather than a completely unconstrained demo or MCP-based solution.In particular you can reduce most concerns around security and reliability when you treat your LLM call as a library method with structured output (Factor 4) and own your own control flow (Factor 8). There should never be a case where your agent is calling a tool with unconstrained input.\n \nreply",
      "I guess I\u2019ve got some reading and research ahead of me. I definitely would rather support the idea of treating LLM calls more like structured library functions, rather than letting them run wild.Definitely bookmarking this for reference. Appreciate you sharing it.\n \nreply",
      "The complexity of an agent may range from something relatively simple to whatever level of complexity you want. So your projet sounds doable but you'll have to run some exploration to get proper answers. Regarding reliability, quality and security, it is as important to learn how to observe an agent system than learning how to implement an agent system. An agent/LLM-based solution is proven to work only if you observe that it actually works, experiments, tests and monitoring are not optional like eg in web development. As for security concerns, you'd want to take a look at the OWASP top 10 for LLMs: https://owasp.org/www-project-top-10-for-large-language-mode...\nLLMs/agents indeed have their own new set of vulnerabilities.\n \nreply",
      "That\u2019s sound advice, really appreciate the link. Regarding your point about continuous monitoring, that\u2019s actually the first thing I mentioned to the client.It\u2019s still highly experimental and needs to be observed, corrected, and tweaked constantly,  kind of like teaching a child, where feedback and reinforcement are key.I may share my experience with the HN community down the line. Thanks again!\n \nreply"
    ],
    "link": "https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf",
    "first_paragraph": ""
  },
  {
    "title": "Autonomous drone defeats human champions in racing first (tudelft.nl)",
    "points": 112,
    "submitter": "picture",
    "submit_time": "2025-06-04T20:03:43 1749067423",
    "num_comments": 93,
    "comments_url": "https://news.ycombinator.com/item?id=44184900",
    "comments": [
      "This is quite cool since past efforts in this direction have usually relied on crutches like outside-in imaging and positioning.A few details I picked up:* The drones are a spec drone across the league. It's a fairly large-footprint FPV racing drone (it's a 5\" propped drone, but it's very stretched out and quite heavy) with both a Betaflight flight controller and a Jetson Orin NX onboard. Teams were only allowed an IMU and a single forward camera.* It's unclear to me whether the teams were allowed to bypass the typical Betaflight flight controller which is present on the drone and use direct IMU input and ESC commands from the Jetson, or whether they were sending and receiving commands from the flight controller and relying on its onboard rate stabilization PID loop.DCL is kind of a weird drone racing league since it's made for TV; it's mostly simulator based with, more recently, only few real events a year. The spec DCL drone isn't very capable compared to the more open-specification drones in racing leagues like MultiGP, in large part to keep the events more spectator friendly. This probably makes it more amenable to AI, which is an interesting side effect.\n \nreply",
      "From near the bottom:> One of the core new elements of the drone\u2019s AI is the use of a deep neural network that doesn\u2019t send control commands to a traditional human controller, but directly to the motors.\n \nreply",
      "I saw that too - I'm assuming it means they're indeed using the DNN for stabilization. This has been done several times over the years, but generally with results which only rival PID and don't surpass it, so that's quite interesting. What's odd is that the physical architecture of the drone doesn't really make sense for this, so there must be some tweaks beyond the \"spec\" model. Hopefully some papers come soon instead of press releases.\n \nreply",
      "This is crazy, its dexterity and range of motion could potentially exceed all human modeled systems.\n \nreply",
      "I assume that they shave off milliseconds by doing so, and a gyroscope (or similar) sends back the position/angle of the drone. And like this does it bypass the 'limited' onboard computer and instead uses a much better/faster computer?\n \nreply",
      "I imagine the slower speed is a closer fit to combat drones (which have a payload and sometimes a fiber optic cable)? Also watching MultiGP they sorta move/accelerate too fast for me to fully appreciate the maneuvering.Feels kinda similar to the innovation around manned aircraft about 100 years ago when we went from toy/observation platform to killing machine in only a couple of decades. With the ardupilot news today, it was hard to not watch this and imagine the applications to a combat environment.\n \nreply",
      "> I imagine the slower speed is a closer fit to combat dronesA lot of comments are trying to draw connections to combat drones, but drone racing like this has been a hobby thing for a long time. The capabilities of the drones are set to have an even playing field, not to match combat drones or anything.These aren't meant to have any parallels to combat drones, drones that fly long distances, or drones that carry payloads.It's really just a special-purpose hobby thing for flying through a series of gates very quickly. Flight time measured in a couple minutes, no provisions for carrying weight.\n \nreply",
      "There's a few more details in the press release from the league itself. Sounds like they were really trying to put these things through their paces.> The course design pushed the boundaries of perception-based autonomy\u2014featuring wide gate spacing, irregular lighting, and minimal visual markers. The use of rolling shutter cameras further heightened the difficulty, testing each team\u2019s ability to deliver fast, stable performance under demanding conditionshttps://a2rl.io/press-release/9/artificial-intelligence-triu...\n \nreply",
      "This is only a few days after the massive drone attack in Russia.  Only a matter of time until we have drones smart enough to dodge bullets (or at least dodge out of where guns are pointing) while flying at breakneck speeds being controlled by AIs we don't fully understand.The tech industry is working hard to bring about the Terminator future.\n \nreply",
      "\u201cWhat hope can there be for mankind,\u201d I thought, \u201cwhen there are such men as Felix Hoenikker to give such playthings as ice-nine to such short-sighted children as almost all men and women are?\u201dAnd I remembered The Fourteenth Book of Bokonon, which I had read in its entirety the night before. The Fourteenth Book is entitled, \u201cWhat Can a Thoughtful Man Hope for Mankind on Earth, Given the Experience of the Past Million Years?\u201dIt doesn\u2019t take long to read The Fourteenth Book. It consists of one word and a period.This is it:\u201cNothing.\u201d--Kurt Vonnegut, Cat's Cradle\n \nreply"
    ],
    "link": "https://www.tudelft.nl/en/2025/lr/autonomous-drone-from-tu-delft-defeats-human-champions-in-historic-racing-first",
    "first_paragraph": "Back to (previous) overview\nTU Delft's latest news A team of scientists and students from TU Delft has taken first place at the A2RL Drone Championship in Abu Dhabi - an international race that pushes the limits of physical artificial intelligence, challenging teams to fly fully autonomous drones using only a single camera. The TU Delft drone competed against 13 autonomous drones and even human drone racing champions, using innovative methods to train deep neural networks for high-performance control. The gained knowledge on highly-efficient robust AI \u00a0will contribute to many robotics applications, from self-driving cars to humanoid robots.For the first time, a drone has beaten human pilots in an international drone racing competition, marking a new milestone in the development of artificial intelligence. On Saturday April 14, 2025, two drone racing events took place simultaneously: The Falcon Cup Finals for human pilots and the A2RL Drone Championship for AI-powered, autonomous drones"
  },
  {
    "title": "Show HN: GPT image editing, but for 3D models (adamcad.com)",
    "points": 103,
    "submitter": "zachdive",
    "submit_time": "2025-06-04T16:00:52 1749052852",
    "num_comments": 61,
    "comments_url": "https://news.ycombinator.com/item?id=44182206",
    "comments": [
      "I quite like the product and it has appeared just as I need it, I am struggling with designing a case for a project right now via fusion.I have noticed that the tool does not do quite what I want it to. It reminds me of MidJourney a couple years ago. I might have to build a workflow where I design parts in Adam and arrange them in other tools. As details I supply for arrangement appear to being applied to individual parts.Do you have any language that is getting better results than others?\n \nreply",
      "Could you share your conversation with Adam so I can take a look?\n \nreply",
      "Theres this one.https://app.adamcad.com/share/aa1a745c-a337-4ba6-a8c1-ca7298...Where I figured I had poisoned the context somehow and was getting nowhere.And herehttps://app.adamcad.com/share/8b3dd548-511f-4b96-a1dd-ed34d4...Where I got some traction but am working in the opposite direction. I am finding my language the barrier. I say \"Apart\" but how is it to know which way.Will it respond to X/Y coordinates properly?\n \nreply",
      "It does respond to X/Y. You can also try to describe vertical and horizontal. I just looked at the prompts. Are you trying to create an enclosure for a desktop computer? Would love to help!\n \nreply",
      "Its a modular project enclosure for messing about with electronics that have panel fit components. I need to be able to remove/replace pieces on the topside. I have a bunch of measurements that aren't with me right now, but 70x40mm panel sections worked out to be just right.My next move is probably going to be feeding it some cleaned up diagrams when I have access to them.\n \nreply",
      "Very cool! This reminds me of a use case I explored a few years ago\u2014customizing furniture with different fabrics, wood finishes, and design options. In physical showrooms, furniture stores can usually only display a single version of each piece, but customers often want to visualize how the same item would look in various configurations. That\u2019s where a digital tool could really shine.One concept I explored was creating an interactive app where users can experiment with different material options\u2014essentially a real-time configurator. There\u2019s a great example here [1], where if you model an object as a .obj file (possibly similar to Adam\u2019s parametric models), you can tweak its materials and colors dynamically. IKEA seems to have something similar in production for some of their products [2].I experimented with Adam as well, and it did a surprisingly good job. The only catch: if you try to iterate too much, it tends to alter the form of the object. My ideal version of this would involve a professional photographer capturing high-resolution images of, say, a couch. Then I\u2019d upload them into Adam, generate realistic renders with different fabrics or finishes, and download the final variants as high-quality images to use in catalogs or ecommerce.[1] https://angon.me/experiments/6/[2] https://www.ikea.com/gb/en/p/ektorp-2-seat-sofa-hakebo-grey-...[3] https://app.adamcad.com/share/2f1e68ad-2cdd-4613-8fdc-fc33f2...\n \nreply",
      "super interesting! thanks for sharing your generations :)in that case wouldn't you bypass the 3d aspect altogether?\n \nreply",
      "wdyt about zoo.dev? they seems getting a lot resources like funding\n \nreply",
      "I was briefly excited, because \"Can you take this file and make it solid\" is a use-case I have for turning wargaming terrain meant to be at one scale to be more generally scalable.Sadly, this appears to be a no go.\n \nreply",
      "could you help me I don't fully understand? you wanted to be able to upload a mesh and repair it using adam? thanks\n \nreply"
    ],
    "link": "https://www.adamcad.com/",
    "first_paragraph": "See it in ActionSee it in ActionPricingCareersGet In TouchSign InAdamCAD is an AI Powered CAD platform that generates 3D designs in secondsTry AdamCAD NowTry AdamCAD NowHow AdamCAD worksHow AdamCAD worksUse prompts in AdamCAD to describe and edit the 3Dmodel you want to create.AdamCAD generates the 3D Model and a list of parameters based on your design to refine it further.AdamCAD's creative mode turns any image into a 3D generation in seconds.AdamCAD is built to integrate with the CAD software professionals rely on.Build anything with natural languageBuild anything with natural languageWhether for industrial design or mechanical engineering, Adam brings ideas to life.Mockup a camshaft for a 4 stroke engine20-tooth spur gear with a 20\u00b0 pressure angle, 2.5 mm pitchGenerate a wall mounted key holder3D printable phone standSmall desktop plant pot for succulentsDesign a futuristic mugCreate an enclosure for a raspberry pi 4Make a toothbrush holderMockup a camshaft for a 4 stroke engineMock"
  }
]