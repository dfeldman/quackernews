[
  {
    "title": "Sizing chaos (pudding.cool)",
    "points": 278,
    "submitter": "zdw",
    "submit_time": "2026-02-18T21:18:20 1771449500",
    "num_comments": 155,
    "comments_url": "https://news.ycombinator.com/item?id=47066552",
    "comments": [
      "The issue is not the sizes, the issue is the obesity epidemic. According to CDC [1] the average woman in the US is 5'3\" weighing 172lbs. That's not just overweight but rather first degree of obesity. I guess you could argue that sizes should catch up to the demands when half of your population is straight up fat but I feel like a better angle would be educating people that 1500 kcal worth of Starbucks sugar for breakfast is not healthy.[1] https://www.cdc.gov/nchs/fastats/body-measurements.htmreply",
      "The article points out that the problem is deeper than this:> Once I compared my personalized sloper to commercial patterns and retail garments, I had a revelation: clothes were never made to fit bodies like mine. It didn\u2019t matter how much weight I gained or lost, whether I contorted my body or tried to buy my way into styles that \u201cflatter\u201d my silhouette, there was no chance that clothes would ever fit perfectly on their own.reply",
      "I do support addressing obesity (see my elsethreads), but duly noted that it\u2019s not a cure-all panacea for the problems faced by women. Obesity does not address the nine different U.S. body shapes; one can be obese and rectangular, or obese and spoon, or obese and triangle. Resolving obesity is a worthy cause, but will only reduce or remove the impact of size inflation on \u2018vanity\u2019 sizing as a whole, without addressing the significant disparity of sizes between manufacturers or the near-total lack of products for the eight non-hourglass body shapes.reply",
      "Despite the article highlighting only people of width as the \"millions of people who are excluded from standard size ranges\", sizing is also a problem in the other direction: it's practically impossible to find well-fitting clothes if you're tall and in decent shape. To your point, though, perhaps there was a time when \"large\" and \"x-large\" meant \"slightly tall\" and \"quite tall\" rather than \"slightly tall plus obese\" and \"quite tall plus very obese\".reply",
      "As a dude who is 6\u2019 1\u201d or thereabouts with a 32\u201d or thereabouts waist and a 34\u201d (or thereabouts) inseam: can confirm.Carhartts size up a waist size to account for shrinking, and I can almost reliably find a 34/34. Finding 32/34 in other pants is a challenge. On the subject of vanity sizing, I\u2019m 15 pounds heavier than I was 20 years ago, and I still wear a 32/34. Which is why all those measurements are qualified above.Finding shirts that fit is a similar challenge. Fitted shirts can usually be found in 16 34-35 with an athletic cut. Letter sizes are a total crapshoot. Sometimes I\u2019m a L, sometimes an M. If I\u2019m an M across the gut, frequently the shoulders are far too tight.Not that I\u2019m complaining as such, but I do agree that the sizes encompass too little information about body shape.reply",
      "I'll point out a statistical hazard here. While CDC lists the average height and weight at 5'3\" and 172 lbs, the medians appear to be 5'3\" and 161 lbs. That's a BMI of 28 and is considered overweight (25-30), not obese. Although I'll mention BMI is a pretty rough measure to begin with.reply",
      "To provide context for those still using BMI as the sole 'fat or not' discriminator \u2014 the CDC published an n=9894 longitudinal study* about ten years ago; generally summarizing figure 1, the median starts around 0.53 +/- gender variance at 20-29, peaks at 0.62 +/- at 70-79, and then begins decreasing from there; however, they found that BMI fails to represent the 'fat' levels improperly (to our detriment) for people in their 40s (figure 2), as older human bodies tend to lose fat in areas (i.e. arms) that have no significant bearing on overall health, but gain fat in the abdomen (which other studies have shown does correspond to increasing cardiovascular issues). They show median waist-to-height-ratio data as 'monotonically increasing abdominal adipose tissue throughout the years of adulthood but decreasing mass in non-abdominal regions', which bodes very poorly for clothing manufacturers \u2014 because not only do you have to account for nine body shapes in women, but you also have to account for age skewing the waist-to-length ratios of the body shape further.It would be particularly interesting to repeat this sizing study using the garment length to identify where it falls in 'height' median for women, and then identifying what 'age' median the garment's waistline is calibrated for. I can certainly guess what the results will be from personal experience on a per-retailer basis, and it would be a useful way to mathematically identify 'underserved niches' in today's market to target with appropriately-fit clothing (without a body scan).* doi:10.1371/journal.pone.0172245 (2017) https://stacks.cdc.gov/view/cdc/44820reply",
      "> I feel like a better angle would be educating people that 1500 kcal worth of Starbucks sugar for breakfast is not healthy.An even better angle is educating Starbucks to stop selling unhealthy garbage.The idea that all blame rests on individuals and corporations are blame-free is crazy. They have way more agency over what we consume than individuals do.reply",
      "That Starbucks probably saved my life after I made an unwise decision to bike 40 miles on an empty stomach. Bonking is real, and I\u2019m glad they are allowed to sell the sugary beverages to prevent me from bonking.Oh and I also fainted the first time I donated blood, because I did not know I should not donate blood while fasting. Again, sugary drinks helped.reply",
      "There's a lot of area on the spectrum between where we are today and \"sugary beverages are all banned\".For example, Starbucks could limit the sizes it sells and advertises\u2014you'd still be able to have as much sugar as you would like by buying multiple drinks, but it would raise the activation energy needed to do that. Making the healthier choice the path of least resistance works wonders.reply"
    ],
    "link": "https://pudding.cool/2026/02/womens-sizing/",
    "first_paragraph": "meet your typicalLike many girls her age, she loves to keep up with the latest fashion trends and explore new ways to express herself. Shopping is fun, but it won\u2019t always be this way.Scroll\u201cJunior\u2019s\u201d clothing lines often channel tweens\u2019 interests with youthful styles that fit young girls as they grow.\rFor now, our typical (or median) 11-year-old wears a size 9 in the junior\u2019s section, which is also considered a size Medium.But not all tweens wear the same size.\rIf we were to look at a sample of all 10- and 11-year-old girls in the U.S. from the  National Center for Health Statistics, here are the junior\u2019s sizes that match up with their waistline measurements.By age 15, most girls have gone through growth spurts and puberty, and they\u2019ve reached their adult height. \rMany have started to outgrow the junior\u2019s size section.This marks an important turning point as they shift into women\u2019s sizes.Girls who fall along the bottom 10th percentile can now wear an Extra Small in women\u2019s clothing, w"
  },
  {
    "title": "Ladybird: Closing this as we are no longer pursuing Swift adoption (github.com/ladybirdbrowser)",
    "points": 183,
    "submitter": "thewavelength",
    "submit_time": "2026-02-18T23:08:38 1771456118",
    "num_comments": 138,
    "comments_url": "https://news.ycombinator.com/item?id=47067678",
    "comments": [
      "Regardless of the language it is written in, one thing that I hope Ladybird will focus on when the time comes is a user-respecting Javascript implementation. Regardless of what the Web standards say, it is unacceptable that websites can (ab)use JS against the users for things such as monitoring presence/activity, disabling paste, and extracting device information beyond what is strictly necessary for an acceptably formatted website. One approach could be to report standardized (spoofed) values across the user base so that Ladybird users are essentially indistinguishable from each other (beyond the originating IP). This is more or less the approach taken by Tor, and where a project like Ladybird could make a real difference.reply",
      "There's just too many defense mechanisms on popular websites that would simply make Ladybird flagged as a bot and render the website unusable. I wouldn't mind a toggle to switch between this and normal behavior but having that as a default would be bad for wider adoption.reply",
      "If those \"popular websites\" are the likes of Facebook and Instagram, I don't see that as a big loss. That being said, I find that most of the Web works just fine on Tor, so it's certainly possible. Most of the issues seem related to the (known) the exit IP being overused or identified as Tor.reply",
      "Most of the web works with Tor, but to make tor successful at the things it is intended to do you have to disable JavaScript.This kills the internet.reply",
      "> If those \"popular websites\" are the likes of Facebook and Instagram, I don't see that as a big loss.Personally I wouldn't mind either but my point is that they probably want to cater to the average person, and not just security conscious tech savvy people, and if that's the case, then you really can't exclude FB/IG/YT and others from working properly in your browser.reply",
      "Only if there is not widespread adoption.reply",
      "The commit removing Swift has a little bit more detail:    Everywhere: Abandon Swift adoption\n\n    After making no progress on this for a very long time, let's acknowledge\n    it's not going anywhere and remove it from the codebase.\n\nhttps://github.com/LadybirdBrowser/ladybird/commit/e87f889e3...reply",
      "I remember mocking the switch to Swift back then.Swift is a poorly designed language, slow to compile, visibly not on path to be major system language, and they had no expert on the team.I am glad they are cutting their losses.reply",
      "Swift never felt truly open source either. That people can propose evolution points doesn\u2019t change the fact that Apple still holds all the keys and pushes whatever priorities they need, even if they\u2019re not a good idea (e.g. Concurrency, Swift Testing etc)Also funny enough, all cross platform work is with small work groups, some even looking for funding \u2026 anyway.reply",
      "> Swift never felt truly open source either.Apple has been always 'transactional' when it comes to OSS - they open source things only when it serves a strategic purpose. They open-sourced Swift only because they needed the community to build an ecosystem around their platform.Yeah, well, sure they've done some work around LLVM/Clang, WebKit, CUPS, but it's really not proportional to the size and the influence they still have.Compare them to Google, with - TensorFlow, k8s, Android (nominally), Golang, Chrome, and a long tail of other shit. Or Meta - PyTorch and the Llama model series. Or even Microsoft, which has dramatically reversed course from its \"open source is a cancer\" era (yeah, they were openly saying that, can you believe it?) to becoming one of the largest contributors on GitHub.Apple I've heard even have harshest restrictions about it - some teams are just not permitted to contribute to OSS in any way. Obsessively secretive and for what price? No wonder that Apple's software products are just horrendously bad, if not all the time - well, too often. And on their own hardware too.I wouldn't mind if Swift dies, I'm glad Objective-C is no longer relevant. In fact, I can't wait for Swift to die sooner.reply"
    ],
    "link": "https://github.com/LadybirdBrowser/ladybird/issues/933",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.List of issues preventing moving forward on moving Swift 6.0 support out of an experimental state:Swift issues: Please backport d8352e93c1c8042d9166eab3d76d6c07ef585b6d\u00a0swiftlang/llvm-project#8998Details: Swift's version of LLVM is missing the fix for [Clang] ICE in CheckPointerToMemberOperands passing decltype of lambda\u00a0llvm/llvm-project#53815. This means that any assertions build of llvm from the swift open source project cannot build our code. Snapshot builds are released with assertions on.Workaround: Build swift from source on Linux without llvm assertions, or use macOS.PR: \ud83c\udf52 [Clang] [Sema] Handle placeholders in '.*' expressions (#83103)\u00a0swiftlang/llvm-project#9038Fixed in Swift 6.0.0 release Interop: Compiler and C++ B"
  },
  {
    "title": "27-year-old Apple iBooks can connect to Wi-Fi and download official updates (reddit.com)",
    "points": 141,
    "submitter": "surprisetalk",
    "submit_time": "2026-02-18T20:54:31 1771448071",
    "num_comments": 70,
    "comments_url": "https://news.ycombinator.com/item?id=47066241",
    "comments": [
      "I reinstalled MacOS on a 2011 MacBook Air and it was actually shockingly hard. Thankfully, my machine booted and worked fine, so I didn't need to create a bootable USB stick. From memory:  - Network recovery boot cannot connect to your wifi because reasons. It'll see the SSID, but won't even prompt for password. It's totally unclear why nothing is working.\n  - Fall back to old IOT SSID with ancient protocols\n  - You cannot directly download or install High Sierra (the latest supported OS) for reasons I don't remember. \n  - I can't remember how, but somehow you can install Lion\n  - Launch beautiful Mac desktop. App store won't work because the certs are too old, or something. Safari won't work, because the supported SSL protocols are too old. \n  - Use a modern Mac to download a DMG installer for a slightly newer OS\n  - Copy it to a USB stick\n  - Find a USB stick big enough to hold it, try again\n  - Plug USB stick into target Mac, copy installer to desktop, run it\n  - Now you have a more modern OS that can actually connect to websites\n  - Also teh app store works, so you can upgrade to High Sierra using the app store.\n\nBut yeah. Man, the desktop was so beautiful and refreshing.reply",
      "> Man, the desktop was so beautiful and refreshing.I get the same feeling when doing a fresh install+boot of both OS X 10.9 Mavericks and Windows 7. They're just so much more pleasant than what we have now.It'd be nice if modern desktop operating systems took a lesson or two from their past selves.reply",
      "I feel the same way about Unix desktops. The newer stuff just.. looks gross? And it's difficult to use. I'm very thankful for Mate, especially the Alt+F2 behavior, but also the simple menu layout vs some horrible combination of search and popups.reply",
      "GNOME 2/MATE isn't quite to my taste for my personal use, but it is cozy in a way that post-3.0 versions aren't.reply",
      "For me it's the difference between \"this is a computer\" vs \"this is a computer trying to be a cell phone\". I think that's what everything from the last 15yr is trying to be--a phone. And not everything is a phone. On a computer we have a keyboard and a mouse, which are much, much more precise tools than vague gestures on a touchscreen.EDIT: I'm gonna go out on a limb here and say this is basically everything that's wrong with the computer(-adjacent) industry. We can appreciate the problem statement by asking \"why would anyone want to make a computer be a phone?\" The answer is a terminal case of a particularly defensive form of groupthink. It goes something like this:(1) \"everyone is talking about the iPhone\"\n(2) \"i need to feel relevant, ergo i must make phone noises too\"then they rub these two neurons together, and since it's the only two they got it isn't hard for them, and this process repeats a few generations and like a nuclear chain reaction soon enough the entire industry is trying to make everything be a fucking phone.It shouldn't be like that.EDIT2: As a species we don't play these games with other tools. Cars--some super early attempts had weird shit like tillers for steering but we quickly outgrew that idea and settled on the steering wheel, levers for the other hand, and pedals\nfor the feet. Same with airplanes and tracked vehicles (bulldozers, tanks, etc). Same with machine tools. This stupid game people are playing with computer interfaces these days is fundamentally inhuman.reply",
      "It's so obvious now that you wrote it, but it never occurred to me as such. New desktops, be it macOS, Gnome, Win.. they all look like damn phones and not computers.reply",
      "I did this and considered it the easy way of installing an OS on a Mac circa 2011 vs. DVD then messing around updating that ...>   Plug USB stick into target Mac, copy installer to desktop, run itApple has a whole page on making a bootable USB, it can save you a step: https://support.apple.com/en-us/101578reply",
      "OpenCore and MIST are two great tools for fans of obsolete Macs. https://github.com/ninxsoft/Mistreply",
      "My best guess is the macbook is freaking out over the combined 2.4 + 5ghz network. It used to be standard to have these with two different SSIDs. Or you have WPA3 required, though I'd think you'd experience issues with many devices doing that.reply",
      "My first thought was incompatible version of 802.11[a-z] as well.reply"
    ],
    "link": "https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/",
    "first_paragraph": ""
  },
  {
    "title": "Cosmologically Unique IDs (jasonfantl.com)",
    "points": 278,
    "submitter": "jfantl",
    "submit_time": "2026-02-18T18:37:22 1771439842",
    "num_comments": 83,
    "comments_url": "https://news.ycombinator.com/item?id=47064490",
    "comments": [
      "This analysis is not quite fair.  It takes into account locality (i.e. the speed of light) when designing UUID schemes but not when computing the odds of a collision.  Collisions only matter if the colliding UUIDs actually come into causal contact with each other after being generated.  So just as you have to take locality into account when designing UUID trees, you also have to take it into account when computing the odds of an actual local collision.  So a naive application of the birthday paradox is not applicable because that ignores locality.  So an actual fair calculation of the required size of a random UUID is going to be a lot smaller than the ~800 bits the article comes up with.  I haven't done the math, but I'd be surprised if the actual answer is more than 256 bits.(Gotta say here that I love HN.  It's one of the very few places where a comment that geeky and pedantic can nonetheless be on point. :-)reply",
      "This is the right critique. The whole article is a fun thought experiment but it massively overestimates the problem by ignoring causality. In practice, UUID collisions only matter within systems that actually talk to each other, and those systems are bounded by light cones. 128 bits is already overkill for anything humans will build in the next thousand years. 256 bits is overkill for anything that could physically exist in this universe.reply",
      "Reminds me of a time many years ago when I received a whole case of Intel NICs all with the same MAC address.It was an interesting couple of days before we figured it out.reply",
      "You must  consider both time and locality.From now until protons decay and matter does not exist anymore is only 10^56 nanoseconds.reply",
      "If protons decay. There isn't really any reason to believe they're not stable.reply",
      "And recent DESI data suggests that dark energy is not constant and the universe will experience a big crunch in a little more than double its current age, for a total lifespan of 33 billion years, no need to get wild with the orders of magnitude on years into the future. The infinite expansion to heat death over 10^100 years is looking less likely, 10^11 years should be plenty.https://www.sciencedaily.com/releases/2026/02/260215225537.h...reply",
      "not obvious to me this makes things better as opposed to worse? sure, the time bound helps but in the runup to a crunch won't we get vastly more devices in causal range at an asymptotically increasing rate?reply",
      "Protons can decay because the distinction between matter and energy isn't permanent.Two quarks inside the proton interact via a massive messenger particle. This exchange flips their identity, turning the proton into a positron and a neutral pion. The pion then immediately converts into gamma rays.Proton decayed!reply",
      "That's such an odd way to use units. Why would you do 10^56 * 10^-9 seconds?reply",
      "This was my thought.  Nanoseconds are an eternity.  You want to be using Planck units for your worst-case analysis.reply"
    ],
    "link": "https://jasonfantl.com/posts/Universal-Unique-IDs/",
    "first_paragraph": "We are an exploratory species, just past the solar system now, but perhaps one day we will look back and call our galaxy merely the first. There are many problems to solve along the way, and today we will look at one very small one. How do we assign IDs to devices (or any object) so the IDs are guaranteed to always be unique?Being able to identify objects is a fundamental tool for building other protocols, and it also underpins manufacturing, logistics, communications, and security. Every ship and satellite needs an ID for traffic control and maintenance history. Every radio, router, and sensor needs an ID so packets have a source and destination. Every manufactured component needs an ID for traceability. And at scale, the count explodes: swarms of robots, trillions of parts, and oceans of cargo containers moving through a civilization\u2019s supply chain.One of the key functions of an ID is to differentiate objects from one another, so we need to make sure we don\u2019t assign the same ID twice"
  },
  {
    "title": "Zero-day CSS: CVE-2026-2441 exists in the wild (googleblog.com)",
    "points": 253,
    "submitter": "idoxer",
    "submit_time": "2026-02-18T16:28:19 1771432099",
    "num_comments": 135,
    "comments_url": "https://news.ycombinator.com/item?id=47062748",
    "comments": [
      "\"Google Chromium CSS contains a use-after-free vulnerability that could allow a remote attacker to potentially exploit heap corruption via a crafted HTML page. This vulnerability could affect multiple web browsers that utilize Chromium, including, but not limited to, Google Chrome, Microsoft Edge, and Opera.\"That's pretty bad! I wonder what kind of bounty went to the researcher.reply",
      "> That's pretty bad! I wonder what kind of bounty went to the researcher.I'd be surprised if it's above 20K$.Bug bounties rewards are usually criminally low; doubly so when you consider the efforts usually involved in not only finding serious vulns, but demonstrating a reliable way to exploit them.reply",
      "Here is a comment that really helped me understand bug bounty payouts: https://news.ycombinator.com/item?id=43025038reply",
      "Everyone should read this comment, it does a really eloquent job explaining the situation.The fundamental thing to understand is this: The things you hear about that people make $500k for on the gray market and the things that you see people make $20k for in a bounty program are completely different deliverables, even if the root cause bug turns out to be the same.Quoted gray market prices are generally for working exploit chains, which require increasingly complex and valuable mitigation bypasses which work in tandem with the initial access exploit; for example, for this exploit to be particularly useful, it needs a sandbox escape.Developing a vulnerability into a full chain requires a huge amount of risk - not weird crimey bitcoin in a back alley risk like people in this thread seem to want to imagine, but simple time-value risk. While one party is spending hundreds of hours and burning several additional exploits in the course of making a reliable and difficult-to-detect chain out of this vulnerability, fifty people are changing their fuzzer settings and sending hundreds of bugs in for bounty payout. If they hit the same bug and win their $20k, the party gambling on the $200k full chain is back to square one.Vulnerability research for bug bounty and full-chain exploit development are effectively different fields, with dramatically different research styles and economics. The fact that they intersect sometimes doesn't mean that it makes sense to compare pricing.reply",
      "The bounty could be very high. Last year one bug\u2019s reporter was rewarded $250k. https://news.ycombinator.com/item?id=44861106reply",
      "Maybe google is an exception (but then again, maybe that payout was part marketing to draw more researchers).reply",
      "So is there anything that would actually satisfy crowd here?Offer $25K and it is \"How dare a trillion dollar company pay so little?\"Offer $250K and it is \"Hmm. Exception! Must be marketing!\"What precisely is an acceptable number?reply",
      "One is a lament that the industry average is so low, and the other is\u2026 a lament that the industry average is so low.  What's the problem?reply",
      "An increase in the average bug payout. Bounty programs pay low on average.reply",
      "A number better than what the exploit could be sold for on the black marketreply"
    ],
    "link": "https://chromereleases.googleblog.com/2026/02/stable-channel-update-for-desktop_13.html",
    "first_paragraph": "The Stable channel has been updated to 145.0.7632.75/76 for Windows/Mac\u00a0 and\u00a0144.0.7559.75 for Linux, which will roll out over the coming days/weeks. A full list of changes in this build is available in the\u00a0LogSecurity Fixes and RewardsNote: Access to bug details and links may be kept restricted until a majority of users are updated with a fix. We will also retain restrictions if the bug exists in a third party library that other projects similarly depend on, but haven\u2019t yet fixed.This update includes 1 security fix. Please see the Chrome Security Page for more information.Interested in switching release channels? Find out how\u00a0here. If you find a new issue, please let us know by\u00a0filing a bug. The\u00a0community help forum\u00a0is also a great place to reach out for help or learn about common issues.Srinivas Sista"
  },
  {
    "title": "Tailscale Peer Relays is now generally available (tailscale.com)",
    "points": 324,
    "submitter": "sz4kerto",
    "submit_time": "2026-02-18T16:46:12 1771433172",
    "num_comments": 172,
    "comments_url": "https://news.ycombinator.com/item?id=47063005",
    "comments": [
      "If you're sold on Tailscale due to them \"being open\" (as they semi-officially support the development of Headscale), keep in mind, that at the same time some of their clients are closed source and proprietary, and thus totally controlled by them and the official distribution channels, like Apple. Some of the arguments given for this stance are just ridiculous:> If users are comfortable running non-open operating systems or employers are comfortable with their employees running non-open operating systems, they should likewise be comfortable with Tailscale not being open on those platforms.https://github.com/tailscale/tailscale/issues/13717A solution like this can't really be relied in situations of limited connectivity and availability, even if technically it beats most of the competition. Don't ever forget it's just a business. Support free alternatives if you can, even if they underperform by some measures.reply",
      "I don't understand this attitude. Some humans have to eat and put a roof over their heads sometimes, and extracting consulting fees from open-source work (i.e. the Redhat model) is not always a paying business model. A hybrid model is often the best way to compromise.Disclaimer: I'm pursuing a similar solution on an app I'm working on. The CLI will be free and open-source (and will have feature parity with the GUI), but charging money for the GUI will also help support that development (and put my son through school etc.)And by \"feature parity\", I really mean it- The GUI will be translated into 22 languages... and so will the CLI. ;) (Claude initially argued against this feature. I made my arguments for it. It: \"You make a compelling argument. Let's do it.\" LOL)The lowest level of it is already available and fully open-source: https://github.com/pmarreck/validateI'm building something on top of that which will have a nice GUI, do some other data integrity stuff, and also have a CLI. And will be for sale in the Mac and Windows app stores.reply",
      "Personally, I understand people need to make money but this tends to be a death spiral (enshittification). So I tend to go for solutions without those incentives at all. Or at least use the free self hosted option.I wonder why you jumped into the mesh vpn market, it's so saturated. Theres literally hundreds of solutions out there (niche ones included for the mainstream ones it's probably 10 or so), many non profit options included. Is there really a niche you can offer that the others don't?Edit: ah by doing the same thing you didn't necessarily mean a mesh vpn? I don't really understand what your thing does but not vpn.I was just saying it because there's a new Show HN mesh VPN thing weekly now.reply",
      "Another way to counteract enshittification is to pay for things, then stop paying when they enshittify.reply",
      "just stop paying them, as though migrating to an alternative is free and easyreply",
      "In this case it really is easy. It's just wireguard with some NAT negotiation sauce and convenient auth layer.reply",
      "You're doing the Dropbox rsync comment.reply",
      "(Tailscalar here)\nTo be clear: it's only the GUIs that are closed source on selected platforms.reply",
      "I stand corrected.Although, the problem is not so single-layered. Do I understand the situation correctly, in case of iOS, to not be subject to additional limitations of the platform that restricts the distribution of your products to the extents that the laws of the countries where your business is registered require, all the user has to do is to fork the main repo (which is, thankfully, BSD), build a minimally acceptable GUI, pass Apple certification, publish the app in the app store, and Bob's your uncle?reply",
      "Thats actually a good way to split a project up into closed/open imho. Open the functional part so people can see you're not sending data to hq behind their backs and make the boring time consuming ui closed. I like it. Then make money out of a service rather than the software.  As we all know, tech people will see a piece if challenging software and go out of their way to replicate it and release it for free, for whatever reasons. So open sourcing that part takes the challenge away.reply"
    ],
    "link": "https://tailscale.com/blog/peer-relays-ga",
    "first_paragraph": "When Tailscale works best, it feels effortless, almost boring. Devices connect directly, packets take the shortest possible path, and performance ceases to be a pressing concern.But real-world networks aren\u2019t always that cooperative. Firewalls, NATs, and cloud networking constraints can block direct peer-to-peer connections. When that happens, Tailscale relies on relays (DERP) to keep traffic moving securely and reliably.Today, we\u2019re excited to announce that Tailscale Peer Relays is now generally available (GA). Peer relays bring customer-deployed, high-throughput relaying to production readiness, giving you a tailnet-native relaying option that you can run on any Tailscale node. Since their beta release, we\u2019ve shaped Tailscale Peer Relays to deliver major improvements in performance, reliability, and visibility.What started as a way to work around hard NATs has grown into a production-grade connectivity option. One that gives teams the performance, control, and flexibility they need t"
  },
  {
    "title": "DNS-Persist-01: A New Model for DNS-Based Challenge Validation (letsencrypt.org)",
    "points": 195,
    "submitter": "todsacerdoti",
    "submit_time": "2026-02-18T18:04:13 1771437853",
    "num_comments": 95,
    "comments_url": "https://news.ycombinator.com/item?id=47064047",
    "comments": [
      "Really happy to see this.In the meantime, if you use bind as your authoritative nameserver, you can limit an hmac-secret to one TXT record, so each webserver that uses rfc2136 for certificate renewals is only capable of updating its specific record:  key \"bob.acme.\" {\n    algorithm hmac-sha512;\n    secret \"blahblahblah\";\n  };\n  \n  key \"joe.acme.\" {\n    algorithm hmac-sha512;\n    secret \"blahblahblah2\";\n  };\n\n  zone \"example.com\" IN {\n   type master;\n   file \"/var/lib/bind/example.com.zone\";\n   update-policy {\n    grant bob.acme. name _acme-challenge.bob.acme.example.com. TXT;\n    grant joe.acme. name _acme-challenge.joe.acme.example.com. TXT;\n   };\n   key-directory \"/var/lib/bind/keys-acme.example.com\";\n   dnssec-policy \"acme\";\n   inline-signing yes;\n  };\n\nI like this because it means an attacker who compromises \"bob\" can only get certs for \"bob\". The server part looks like this:  export LE_CONFIG_HOME=\"/etc/acme-sh/\"\n  export NSUPDATE_SERVER=\"${YOUR_NS_ADDR}\"\n  export NSUPDATE_KEY=\"/var/lib/bob-nsupdate.key\"\n  export NSUPDATE_KEY_NAME=\"bob.acme.\"\n  export NSUPDATE_ZONE=\"acme.example.com.\"\n\n  acme.sh --issue --server letsencrypt -d 'bob.example.com' \\\n        --certificate-profile shortlived \\\n        --days 6 \\\n        --dns dns_nsupdatereply",
      "Is it false that DNS requests sent from LE to authoritatuve nameservers are unencryptedreply",
      "I think this is solving a real operational pain point, definitely one that I've experienced. My biggest hesitation here is the direct exposure of the managing account identity not that I need to protect the accounts key material, I already need to do that.While \"usernames\" are not generally protected to the same degree as credentials, they do matter and act as an important gate to even know about before a real attack can commence. This also provides the ability to associate random found credentials back to the sites you can now issue certificates for if they're using the same account. This is free scope expansion for any breach that occurs.I guarantee sites like Shodan will start indexing these IDs on all domains they look at to provide those reverse lookup services.reply",
      "CAA records including an accounturi already expose the account identity in the same manner, so I feel like that ship has already sailed somewhat (and I would prefer that the CAA and persist record formats match).reply",
      "Exactly. They should provide the user with a list of UUIDs(or any other randomish ID tied to the actual account) that can be used in the accounturi URL for these operations.reply",
      "The account is the same as you create in any acme client. I don't see potential for a reverse lookup.reply",
      "I think the previous post is talking about a search that will find the sibling domain names that have obtained certificates with the same account ID. That is a strong indication that those domains are in the same certificate renewal pipeline, most likely on the same physical/virtual server.reply",
      "Run ACME inside a Docker container, one instance (and credentials) for each domain name. Doesn't consume much resources. The real problem is IP addresses anyway, CT logs \"thankfully\" feed information to every bad actor in real time, which makes data mining trivially easy.reply",
      "you dont even need a docker container to do that.reply",
      "Agreed, that's just a personal preference thing of me. Harder to mess up and easier to route.reply"
    ],
    "link": "https://letsencrypt.org/2026/02/18/dns-persist-01.html",
    "first_paragraph": "When you request a certificate from Let\u2019s Encrypt, our servers validate that you control the hostnames in that certificate using ACME challenges. For subscribers who need wildcard certificates or who prefer not to expose infrastructure to the public Internet, the DNS-01 challenge type has long been the only choice. DNS-01 works well. It is widely supported and battle-tested, but it comes with operational costs: DNS propagation delays, recurring DNS updates at renewal time, and automation that often requires distributing DNS credentials throughout your infrastructure.We are implementing support for a new ACME challenge type, DNS-PERSIST-01, based on a new IETF draft specification. As the name implies, it uses DNS as the validation mechanism, but replaces repeated demonstrations of control with a persistent authorization record bound to a specific ACME account and CA. The draft describes this method as being \u201cparticularly suited for environments where traditional challenge methods are im"
  },
  {
    "title": "R3forth: A concatenative language derived from ColorForth (github.com/phreda4)",
    "points": 57,
    "submitter": "tosh",
    "submit_time": "2026-02-18T19:28:59 1771442939",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=47065179",
    "comments": [
      "Forth user here.Don't use AI, it writes Forth like it writes C.  It has got better at following Standard, in Gforth style, but it is awful at the spirit of Forth:   factoring programs into a vocabulary of tiny, reusable pieces.reply",
      "phreda4 has been doing cool stuff with ColorForth-likes for ages and for some reason barely gets any attention for it. Always brings a smile to my face to see it submitted herereply",
      "Inspired by this article, I tried to read some tutorials on Forth. My question is whether concatenative languages are AI-coding friendly. Apart from the training data availability, the question is also whether LLMs can correctly understand long flows of concatenated operations. Any ideas?reply",
      "They can produce idioms that resemble the flow of Forth code but when asked to produce a working algorithm, they get lost very quickly because there's a combination of reading \"backwards\" (push order) and forwards (execution order) needed to maintain context. At any time a real Forth program may inject a word into the stack flow that completely alters the meaning of following words, so reading and debugging Forth are nearly the same thing - you have to walk through the execution step by step unless you've intentionally made patterns that will decouple context - and when you do, you've also entered into developing syntax and the LLM won't have training data on that.I suggest using Rosetta Code as a learning resource for Forth idioms.reply",
      "Thanks for your reply.\nIn fact, I've grown tired of programming by myself \u2014 I do 95% of my coding with Claude Code. But the remaining 5% of bugs can't be solved by the AI agent, which forces me to step in myself. In those cases, I'm thrown into a codebase I've never touched before, and code readability becomes key. That's what drew me to this article and to Forth.\nI would look into the Rosetta.reply",
      "Any concatenative program can be reduced to a rho type, and AI are pretty good about combining properly typed abstractions.reply",
      "Then you get to definitions like \": open ( string -- handle 1 | 0) ... ;\" which describes returning algebraic type Maybe Handle unboxed on the stack. Algebraic types are fun, they can easily represent Peano arithmetic and get us into the realm Goedel incompleteness theorem very quickly.Or you can deduce signature for EXEC EXEC sequence. EXEC's stack effect can be described as ( \\alpha (\\alpha -- \\beta) -- \\beta), where \\greekletter is a placeholder for a stack part of arbitrary length. Notice that this type comment has nested brackets and does not adhere to Forth stack-effect comment convention.When I thought about this more than fifteen years ago, I've got at least two equally valid types for the EXEC EXEC: one where xt at top of stack consumes all its input and leaves no output ( \\alpha (\\alpha -- \\gamma) \\beta (\\beta -- ) -- \\gamma) and when first EXEC produces something for second to execute upon ( \\alpha \\beta (\\beta -- \\gamma (\\alpha \\gamma -- \\theta) -- \\theta).One can argue that second type of EXEC EXEC subsume first one, if greek-letter-named-stack-parts are allowed to be empty.Still it shows that typing Forth, at the very least, needs unification on the Peano's arithmetic level, implementing deduction from length zero to unbounded length.So, in my opinion, for LLM to dependably combine typed Forth/concatenative definitions, it needs to call external tool like Prolog to properly deduce type(s) of the sequence of Forth's (or concatenative language's) definitions.And here we enter a realm interesting in itself.Here it is: https://github.com/stassa/louiseThis is a Prolog system to learn programs in polynomial time. For one example, it can one-shot-learn a grammar, without being \"trained\" on millions of samples.So, should one use a LLM that either needs a paid access or just slow to run, or freely go where \"old school\" systems like Eurisco [1] and Cyc went?[1] https://en.wikipedia.org/wiki/EuriskoEurisco demonstrated superhuman abilities in 1982-83. It also demonstrated knowledge transfer at the time, where rules from VLSI place-and-route algorithms were used to design winning Traveler TCS fleet.reply",
      "Concatenation = composition in ultimate formreply",
      "That is a great looking tutorial. Can't wait to try it. Thanks!reply"
    ],
    "link": "https://github.com/phreda4/r3/blob/main/doc/r3forth_tutorial.md",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          "
  },
  {
    "title": "The Perils of ISBN (rygoldstein.com)",
    "points": 70,
    "submitter": "evakhoury",
    "submit_time": "2026-02-18T17:34:37 1771436077",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=47063663",
    "comments": [
      "This reminds me of MusicBrainz, whose database stores \"release groups\", e.g. the album Nevermind by Nirvana is one, which can have hundreds of \"releases\", as different media (tape, CD, LP, promo, ...), different countries, later re-issues, etc. [0]Sometimes these have different catalogue numbers or barcodes to distinguish them, sometimes they don't but they're still different. I've seen releases where the only difference is the label in the centre of the LP, or the back of the CD case has a two-column tracklisting vs a one-column tracklisting. Music publisher uses the same code and says it's identical and yet it's clearly not.Then there's the \"recordings\" on an album, which even if they're never re-recorded can still end up chopped up, bleeped or remastered. They're not the same sound. MusicBrainz likes to track when they are exactly the same recording (e.g. the LP recording of a song appearing on a compilation album verbatim) and when they're not (e.g. radio edits of the LP recording). And if we're going beyond recordings by one artist of \"their\" song, i.e. cover versions, or just plain standards, those are \"works\", with composers, lyricists, and can be recorded thousands of times by different artists...I greatly appreciate the pedantry and flexibility for noting down when creative works are the same versus where they differ, in relational database form.[0] https://musicbrainz.org/release-group/1b022e01-4da6-387b-865...reply",
      "They actually have a (very new, still alpha, probably not a ton of data yet) database for books:https://bookbrainz.org/aboutI haven't looked into what their schema is like, but if it's anything like Musicbrainz it will be pretty comprehensive and easy to pull the data you want out of!reply",
      "That's the post I made on r/plex a decade ago that pissed off a dumbass moderator and got me banned from there! I guess he hated books.I've recently been doing data entry on Open Library... sometimes even worldcat doesn't have an OCLC for an edition, and Open Lib is my fallback. Maybe I should be doing it on Bookbrainz instead.reply",
      "My favorite example of this sort of thing has been In My Tribe by 10000 Maniacs.  The UPC/Catalog Number remained the same between the 1987 release and the removal of Peace Train (track 7) in 1989.  I have this memory of sifting through the stock at a large used CD store in the mid-90s hoping to find the pre-removal version.reply",
      "https://musicbrainz.org/release-group/94d44c63-7dee-3921-aa6... all with the barcode 075596073820 and catalogue number 60738 / 60738-2 / 9 60738-2Interesting to read that the reason for the removal was Cat Stevens' apparent endorsement of the fatwa against Salman Rushdie. It seems it was the band themselves that requested it? https://www.rollingstone.com/music/music-news/cat-stevens-br...reply",
      "I know that for a book I've published via Kindle Press (the real ones, not digital) that there are at least 3 official revisions, and many many minor ones that as far as I know are only differentiated by the minor typos fixed, and MAYBE one of the numbers buried in the front matter. The ISBN has remained the same.reply",
      "Converse problem: ISBN re-use:\"Officially, ISBNs should never be reused. However, problems can happen if:- A publisher improperly reuses an ISBN- A small or self-publisher mis-registers a book- An ISBN agency error occurred- A book was published before 2007 and conversion from ISBN-10 to ISBN-13 created confusion\" [Source: ChatGPT]In 2009, I had plans to use ISBNs to distinguish the books in my personal library. But after scanning some ISBN bar codes with a MacBook app, I discovered some codes were associated with different books (the app also pulled the cover art, so it was easy to spot). Never had the time to find out if the bar code scanning was defective (=did not use the check sum) or these were cases of assignment errors, which \"shouldn't happen\" but have already happened.There is a certain type of ignorant developer who reused \"unique IDs\", I've even seen a database in production use where GUIDs were recycled (no joke).reply",
      "A good time to remember that the Open Library came to be thanks to the initial work of Brewster Kahle (founder of the Internet Archive) and Aaron Swartz (RIP) http://www.aaronsw.com/weblog/openlibraryreply",
      "Wikidata is a FRBR-compatible public database of books. I don't know if it's good enough for the kind of books the author wants, but in recent years the quality of wikidata greatly increased for the books that deal with (about 1000 items).BTW, they misunderstood their own example of \"Hotel Iris\" by Yoko Ogawa when they wrote \"the same work is duplicated four times.\" In fact, those four entries in the list point to distinct works.One of these is a French publication by the publisher Actes Sud. Translations are not the same work as the original. They are derived works.But it's true this list is a mess. Another entriy has 3 editions, one in English and two in Spanish, so it's obviously an error that mixes two distinct works.reply",
      "> Translations are not the same work as the original. They are derived works.Which adds yet another layer. Because you still want them to be considered as part of a larger single entity. If you're performing a search, you want to find the single main entity, and then have different translations listed the same way you have different editions listed.reply"
    ],
    "link": "https://rygoldstein.com/posts/perils-of-isbn",
    "first_paragraph": ""
  },
  {
    "title": "Making a font with ligatures to display thirteenth-century monk numerals (digitalseams.com)",
    "points": 38,
    "submitter": "a7b3fa",
    "submit_time": "2026-02-15T15:44:11 1771170251",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=47024585",
    "comments": [
      "Excellent! For a music project of mine I found MusGlyph [1] which is also all about ligatures, like typing ssss for 4 beamed sixteenth notes. There are some ligatures I need that are not in the font, I contacted the author and he encouraged me to add them myself. So now I\u2019m spending quality time with FOSS called FontForge. Also subsetting a ligature-heavy font for the web turns out an interesting challenge. Wrote up my experience here [2][1] https://www.notationcentral.com/product/musglyphs/\n[2] https://highperformancewebfonts.com/read/subsetting-and-liga...reply",
      "This is lovely.> Sometimes (not always), this makes addition visualI wonder how often - my suspicion would be rarely.reply",
      "Seems like it's only when adding 1 + 4, 6, or 8 in a place value or where one of them is 0. It doesn't seem like it'd ever hold across a carry, but I could be missing something.Edit: There's actually a few cases with adding 2 as well!reply",
      "Surprising there isn't a better way to do it than defining 10000 ligature config lines and 10000 glyphs. I guess dynamic combinations of subglyphs are a Unicode level thing?reply",
      "https://github.com/ctrlcctrlv/FRBCistercianThere is a compositional approach, used by this font.OP went with brute force because it's probably a heck of a lot easier up front, lol.reply",
      "Very cool use of a technology I wasn't even aware of!reply"
    ],
    "link": "https://digitalseams.com/blog/making-a-font-with-9999-ligatures-to-display-thirteenth-century-monk-numerals",
    "first_paragraph": "As the title implies, I just created a font that displays numbers in a compact format used by Cistercian monks. You can play with it on my demo site here:This project was inspired by Chris Heilmann\u2019s post about creating a generator for Cistercian numerals. His version creates PNG and SVG images, but I had the thought - why not use font ligatures instead?Font ligatures are a feature of digital typefaces, typically used to fix issues with certain combinations of letters. For example, overhanging \u201cf\u201d might collide with a following \u201ci\u201d or \u201cl\u201d; font creators can set a ligature that replaces the ugly collision with a prettier glyph.Image via Type Network: OpenType at Work | Standard LigaturesSome fonts designed for programming, like Fira Code, will use (abuse?) ligatures to combine sequences used for programming (like !== or =>) into their own glyphs. Animation via Scott Hanselman: Monospaced Programming Fonts With LigaturesThe neat thing about ligatures is that they are a visual enhancement"
  },
  {
    "title": "All Look Same? (alllooksame.com)",
    "points": 33,
    "submitter": "mirawelner",
    "submit_time": "2026-02-18T22:48:03 1771454883",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=47067498",
    "comments": [
      "Am asian myself, got 6/18 too.CJK people actually do look very similar anyway, which is not surprising as there are a lot of shared genetics.The way people tell them apart is going to be mostly based on current popular fashion, which is quite difficult to do with these bust shots and what I'm guessing are older picturesreply",
      ">are older picturesI am pretty sure it's 20+ years old. Just based on when I remember taking it.reply",
      "I got 12/18 on faces as an American-born Caucasian living in Japan for over 10 years. Since the subjects were photographed in New York City (and from the other comments, at least a decade ago), cues from fashion and makeup only helped me get about 4 of them, another 6 had pretty strong ethnic features. Of the remaining 8, it was a bit of a tossup and I did worse than guessing, getting only 2 correct.13/18 on food. Even with a lot of the same general types of food, the presentation and specific ingredients made a lot of them somewhat simple. I got tripped up on a few, though, where I overthought it (\"a Japanese X is usually not like this\") or ones where it was really a tossup for me between Chinese and Korean since I'm less familiar with those foods.reply",
      "11/18 :)What\u2019s really wild to me is having spent time in both Mexico and Thailand, I have seen some people in Mexico that could have a twin in Thailand.  That was really unexpected.reply",
      "From older version of the site:https://archive.ph/http://alllooksame.com/https://archive.ph/CeR00>this is what happens generally when you fight against anything out of anger. It\u2019s not that you have no justification for fighting; the real problem is that your efforts only make the situation worse, not just for others, but for yourself also.Problem is that it's hard to recognise that something is worth our moral efforts without feeling angry at the same time. Stoicism is constant work.reply",
      "I got 6/18 for the faces (\"Obviously, very bad.\") I thought I would get at least 50%. Interestingly, of the ones I felt very sure about, I did much better (got about 4 out of those 6).reply",
      "Obviously, random chance... \nIt's a bit ignorant/racist to expect people from different countries to look distinctly different (fashion notwithstanding), when genetics are so overlappingreply",
      "My recollection is that this website says that a 50% score is bad when the expected value of random chance of picking the correct option among 3 is 1/3. A 50% average score means there is some signal there. If it was impossible to guess, the average score should be 33%reply",
      "I'm not sure it's bad at all. China has around 1.5 billion people and officially recognizes 55 ethnicities.Some of them look more like non Chinese people than like I he Chinese ethnicities.reply",
      "9/18 and I lived in China & Japan for 10 years altogether.  It's a tough test!  There were only a couple that were obvious.reply"
    ],
    "link": "https://alllooksame.com/",
    "first_paragraph": "1These are photos taken in New York City. The people who appear in these pictures are 100% Chinese, Japanese or Korean; nobody is mixed.2These are images from the museum exhibit at Fondazione Sandretto Re Rebaudengo in Italy entitled \u201cAllLookSame?\u201d (Seriously, no kidding).3This is not as easy as one might think. If it's a Buddhist temple, for instance, it's a Buddhist temple regardless of where it is located.4These are photos taken by Matt McCoy as he traveled through Asia.5These are photos taken by Moon Lee as he traveled through Asia. You can check out more of his great photos at his site, moonlee.org.6This is a tricky one too since modernization tends to make everything look the same. I avoided photos that contained signs that are too obvious, like the letters that are unique to each culture.7Keep in mind that the food cultures are constantly being imported and exported. This is not a test of identifying the origin. The answers are based on where the pictures were taken. For instanc"
  },
  {
    "title": "Learning Lean: Part 1 (rkirov.github.io)",
    "points": 82,
    "submitter": "vinhnx",
    "submit_time": "2026-02-15T10:20:02 1771150802",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=47022604",
    "comments": [
      "> Prop vs Decidable - I vaguely understand the distinction, but can use more examples. Also would like to know does this relate to the noncomputable property.My path to this subject was tortured, so sorry if I don\u2019t account for Polysemy etc.With Prop, I think what you need to dig into is \u2018non-computational\u2019 not \u2018non-computable\u2019.Mere propositions is probably best viewed with Homotopy Type Theory[0]Two proofs (t1,t2) of the same proposition (p:Prop) which are definitionally equal are proof irrelevant, meaning that all they carry is the proof p is true.This paper [1] may be helpful but the difference between groupoids and subsingletons with classical mathematics is challenging for many of us.Hopefully this helps in your journey.Also remember that with classical set theory the internal and external proposition truths are different, the Curry\u2013Howard correspondence is to constructivist from lambda calculus, you don\u2019t have PEM etc\u2026Remember DGM[2] shows that finite indexing or projection is PEMGood luck and I hope you continue to share your journey.[0] https://homotopytypetheory.org/wp-content/uploads/2013/03/ho...[1] https://jesper.sikanda.be/files/definitional-proof-irrelevan...[2] https://ncatlab.org/nlab/show/Diaconescu-Goodman-Myhill+theo...reply",
      "> With Prop, I think what you need to dig into is \u2018non-computational\u2019 not \u2018non-computable\u2019.Here's another way to explain this:As you state, Prop has to do with proof-irrelevance. When doing constructive mathematics, proofs are programs (meaning they carry computational content), but sometimes it's useful to treat any two proofs of the same proposition as equal. As a consequence, proofs cannot be inspected or run as programs, and you get back the Law of Excluded Middle from classical mathematics.Decidable has to do with decidability. This means that given some proposition P, there is an algorithm that can either produce a proof of P, or a proof of ~P. This is usually useful when P is a predicate, so that at each x, P(x) either has a proof or a disproof.In classical mathematics, the Law of Excluded Middle holds for all propositions. In constructive mathematics, the Law of Excluded Middle only holds for decidable propositions. If P is decidable, it is safe to constructively assume P or ~P because an algorithm can produce the answer.reply",
      "I thought this was about Toyota's Lean when I clicked lol.That's catnip for my original IE brain.reply",
      "It's interesting to see the notes of someone tackling lean who's primary occupation is SWE but has a strong background in mathematics.reply",
      "Lean is great, but if someone's primary interest is SWE, I think there are better choices. The Lean community is primarily focused on formalizing mathematics right now. This might change in the future. Lean is nice to learn theorem proving, but once you learn the basics, you'll hit a roadblock when trying to move to software verification applications.For SWE, the most mature option is probably Isabelle. It's also a classical theorem prover, and it's perhaps easier to start with something that doesn't have dependent types. A cool thing is that the canonical Isabelle book [1] has been rewritten in Lean [2].[1] http://concrete-semantics.org[2] https://github.com/lean-forward/logical_verification_2025reply",
      "While I don't know the specifics of Lean, I know Rocq and will attempt to answer some of the remaining questions. I look forward to someone else telling me that my intuition from Rocq is completely wrong, so take this all with a grain of salt and read the comments replying to this one.1) rfl vs doing a proof:It depends on how your things are defined. For example, consider the function that appends two lists, a classic in functional programming (Here's a refresher: https://stackoverflow.com/a/35442915/2694054 )This is usually defined by recursion. But the details matter: The example in the link is defined by recursion on the first argument. That is, for a concrete first argument, it can evaluate. So it can e.g. evaluate `append [] ys` to `ys` just by unfolding the definition and resolving matches. But for `append xs []` you can not evaluate the `xs` any further because the remaining behavior depends on its concrete shape.  So to prove that `append xs [] = xs` you need a proof (by induction).2) Prop vs DecidableProp is a mathematical proposition. For example, the Riemann Hypothesis is a Prop. But a decidable Proposition is one for which you can write a program that knows if it is true or false. And you need to actually write this program, and prove it correct. So currently the Riemann Hypothesis is not decidable, because no one figured out how to write that program yet. (It will be a simple `return true` or `return false`, but which??) This mostly shows up for something like `forall x y, decidable (x = y)` which allows you to say that for any two numbers you can decide if they are equal or not. You can then use this when you actually do functional programming in Lean and actually want to run the program on concrete inputs.The remaining two questions are more specific to Lean's engineering so I won't even attempt to answer that.reply",
      "I never hijack non-ai threads to talk about AI, but can anybody share their experience using LLMs to code in Coq, Lean, etc.reply",
      "I\u2019ve never used them first hand, but crackpots sure do love claiming to solve Riemann hypothesis, P vs NP, Collatz conjecture etc and then peddle out some huge slop. My experience has solely been curiously following what the LLM\u2019s have been generating.You have to be very, VERY careful. With how predisposed they are to helping, they\u2019ll turn to \u201cdishonesty\u201d rather than just shut down and refuse. What I tend to see is they get backed into a corner, and they\u2019ll do something like prove something different under the guise of another:They\u2019ll create long pattern matching chains as to create labyrinths of state machines.They\u2019ll keep naming functions, values and comments to seem plausible, but you have to follow these to make sure they are what they say. A sneaky little trick is to drop important parameters in functions, they appear in the call but not in the actual body.They\u2019ll do something like taking a Complex value, but only working with the real projection, rounding a number, creatively making negatives not appear by abs etc etcSo even when it compiles, you\u2019ve got the burden of verifying everything is above board which is a pretty huge task.And when it doesn\u2019t work, introducing an error or two in formal proof systems often means you\u2019re getting exponentially further away from solving your problem.I\u2019ve not seen a convincing use that tactics or goals in the proof assistant themselves don\u2019t already providereply"
    ],
    "link": "https://rkirov.github.io/posts/lean1/",
    "first_paragraph": "I\u2019ve been captivated by the recent movement to popularize mathematics formalization through the Lean theorem prover, and this year I\u2019m diving deeper into learning it.For those unfamiliar with this revolution, I highly recommend watching Kevin Buzzard\u2019s talks on YouTube for an overview of why formal mathematics is generating such excitement in the mathematical community.The immediate benefits of formalization are well-documented: it helps catch errors in proofs and reduces the need for trust between collaborators since every step is mechanically verified. However, I believe there\u2019s another compelling advantage that\u2019s less frequently discussed: formalization enables a better separation of concerns in mathematical writing.When proofs are formally verified by tools like Lean, mathematicians can focus their written exposition on what truly matters to human understanding: the intuition, motivation, and creative journey that led to the proof. Instead of devoting pages to mechanical verificati"
  },
  {
    "title": "Roads to Rome (2015) (benedikt-gross.de)",
    "points": 11,
    "submitter": "robin_reala",
    "submit_time": "2026-02-15T12:42:26 1771159346",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://benedikt-gross.de/projects/roads-to-rome/",
    "first_paragraph": " 2015 Dec 486.713 routes to Rome, 120cm \u00d7 80cm, lambda print / interactive web map486.713 routes to Rome, 120cm \u00d7 80cm, lambda print / interactive web mapConcept/IdeaBenedikt Gro\u00df & Philipp SchmittProgramming/GISBenedikt Gro\u00df, Philipp Schmitt & Raphael ReimannDocumentation/TextRaphael Reimann & Philipp SchmittDataOpenStreetMapRouting EngineGraphHopperGeospatial Analysisturf.jsInteractive Mapsleaflet.js, Mapbox GL JS, tippecanoeBackend/Toolingnode.js, mongoDB\u201cRoads to Rome\u201d is a data visualization project that explores the idiom, \u201call roads lead to Rome\u201d. The outcome is both information visualization and data art and unveils mobility patterns at a very large scale. The visualizations were created using routing algorithms on existing street infrastructure from city to continent scale. The resulting images bring insights into the ways in which road infrastructure reflect regional, political and geographical situations.\nThe project page comes with detailed informations, interactive maps, a"
  },
  {
    "title": "What is happening to writing? Cognitive debt, Claude Code, the space around AI (resobscura.substack.com)",
    "points": 101,
    "submitter": "benbreen",
    "submit_time": "2026-02-18T14:59:16 1771426756",
    "num_comments": 73,
    "comments_url": "https://news.ycombinator.com/item?id=47061642",
    "comments": [
      "I won't ever put my name on something written by an LLM, and I will blacklist any site or person I see doing it. If I want to read LLM output I can prompt it myself, subjecting me to it and passing it off as your own is disrespectful.As the author says, there will certainly be a number of people who decide to play with LLM games or whatever, and content farms will get even more generic while having less writing errors, but I don't think that the age of communicating thought, person to person, through text is \"over\".reply",
      "some people might be better at prompting a LLM than youjust like when you go to a restaurant to have a chef cook for you when you can cook yourselfreply",
      "Axios got traction because it heavily condensed news into more scannable content for the twitter, insta, Tok crowd.So AI is this on massive steroids. It is unsettling but it seems a recurring need to point out that across the board many of \"it's because of AI\" things were already happening. \"Post truth\" is one I'm most interested in.AI condenses it all on a surreal and unsettling timeline. But humans are still humans.And to me, that means that I will continue to seek out and pay for good writing like The Atlantic. btw I've enjoyed listening to articles via their auto-generated NOA AI voice thing.Additionally, not all writing serves the same purpose. The article makes these sweeping claims about \"all of writing\". Gets clicks I guess, but to the point, most of why and what people read is toward some immediate and functional need. Like work, like some way to make money, indirectly. Some hack. Some fast-forwarding of \"the point\". No wonder AI is taking over that job.And then there's creative expression and connection. And yes I know AI is taking over all the creative industries too. What I'm saying is we've always been separating \"the masses\" from those that \"appreciate real art\".Same story.reply",
      "> Additionally, not all writing serves the same purpose.I think this is a really important point and to add on, there is a lot of writing that is really good, but only in a way that a niche audience can appreciate. Today's AI can basically compete with the low quality stuff that makes up most of social media, it can't really compete with higher quality stuff targeted to a general audience, and it's still nowhere close to some more niche classics.An interesting thought experiment is whether it's possible that AI tools could write a novel that's better than War and Peace. A quick google shows a lot of (poorly written) articles about how \"AI is just a machine, so it can never be creative,\" which strikes me as a weak argument way too focused on a physical detail instead of the result. War and Peace and/or other great novels are certainly in the training set of some or all models, and there is some real consensus about which ones are great, not just random subjective opinions.I kind of think... there is still something fundamental that would get in the way, but that it is still totally achievable to overcome that some day? I don't think it's impossible for an AI to be creative in a humanlike way, they don't seem optimized for it because they are completely optimized for the sort of analytical mode of reading and writing, not the creative/immersive one.reply",
      "> An interesting thought experiment is whether it's possible that AI tools could write a novel that's better than War and Peace. A quick google shows a lot of (poorly written) articles about how \"AI is just a machine, so it can never be creative,\" which strikes me as a weak argument way too focused on a physical detail instead of the result. War and Peace and/or other great novels are certainly in the training set of some or all models, and there is some real consensus about which ones are great, not just random subjective opinions.I am sure it could but then what is the point? Consider this, lets assume that someone did manage to use LLM to produce a very well written novel. Would you rather have the novel that the LLM generated (the output), or the prompts and process that lead to that novel?The moment I know how its made, the exact prompts and process, I can then have an infinite number of said great novels in 1000 different variations. To me this makes the output way, way less valuable compared to the input. If great novels are cheap to produce, they are no longer novel and becomes the norm, expectation rises and we will be looking for something new.reply",
      "> Today's AI can basically compete with the low quality stuff that makes up most of social media, it can't really compete with higher quality stuffBut compete in what sense? It already wins on volume alone, because LLM writing is much cheaper than human writing. If you search for an explanation of a concept in science, engineering, philosophy, or art, the first result is an AI summary, probably followed by five AI-generated pages that crowded out the source material.If you get your news on HN, a significant proportion of stories that make it to the top are LLM-generated. If you open a newspaper... a lot of them are using LLMs too. LLM-generated books are ubiquitous on Amazon. So what kind of competition / victory are we talking about? The satisfaction of writing better for an audience of none?reply",
      "> if you get your news on HN, significant portion that make it to the top are LLM-generated.You mean this anecdotally I assume.This makes me think of the split between people who read the article and people who _only_ read the comments. I'm in the second group. I'd say we were preemptive in seeking the ideas and discussion, less so achieving \"the point\" of the article.FWIW, AI infiltrates everything, i get that, but there's a difference between engagement with people around ideas and engagement with the content. it's blurry i know, but helps to be clear on what we're talking about.edit: in this way, reading something a particular human wrote is both content engagement and engagement with people around an idea. lovely. engaging with content only, is something else. something less satisfying.reply",
      "There are very few things worth reading submitted to this site. The only meaningful thing I'm glad to have read was the \"I sell onions on the internet\" blog post. Everything else I've forgotten, mostly VC marketing fluff or dev infighting in open source; hardly anything worth noting.This place is up there with reddit, it's all lowish calorie info; 90% forgettable, 10% meaningful but you have to dig quite quite deep to find it.reply",
      "To be fair, it has gotten harder, but when the meaningful stuff does happen, it is hard to beat. Some of the audience can have rather pointed takes. And if it is then somehow topped by 'off the beaten path' guy, it really makes it for me ( in the sense that maybe not all is lost quite yet ). I still sometimes reel from 'manifest bananas' guy.reply",
      ">The satisfaction of writing better for an audience of none?The satisfaction of writing for an engine. The last of what could still be recognized as a real human being writing. There\u2019s no competition with AI, but also no resignation and no fear of being limited compared to the vast knowledge of an LLM. Even in a context of an \"audience of none\", somewhere there will be a scraper tool interested in my writing. And if it gets hallucinated... wow!reply"
    ],
    "link": "https://resobscura.substack.com/p/what-is-happening-to-writing",
    "first_paragraph": ""
  },
  {
    "title": "How to Choose Between Hindley-Milner and Bidirectional Typing (thunderseethe.dev)",
    "points": 11,
    "submitter": "thunderseethe",
    "submit_time": "2026-02-15T18:07:44 1771178864",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://thunderseethe.dev/posts/how-to-choose-between-hm-and-bidir/",
    "first_paragraph": "This question is common enough you\u2019ve probably heard it posed countless times:\n\u201cShould my new programming language use a Hindley-Milner (HM) type system or a Bidirectional (Bidir) one?\u201d\nWhat\u2019s that?\nI need to understand friends don\u2019t just bring up type inference in casual conversation?OK, ouch, fair enough.\nBut\u2026whatever.\nThis is my blog.\nWe\u2019re doing it anyway!\nI don\u2019t know what you expected when you clicked on a programming languages blog.Picking a type system is a real barrier for would be language developers.\nEyes full of trepidation as they navigate the labyrinth of nuanced choice that goes into everything a programming language asks of them.\nWhich type system to choose is just another quandary in the quagmire as they trudge towards a working prototype.Its understandable they\u2019d want to make a quick decision and return to marching.\nBut this is the wrong question to ask.\nThe question presumes that HM and Bidir are two ends of a spectrum.\nOn one end you have HM with type variables and "
  },
  {
    "title": "Metriport (YC S22) is hiring a security engineer to harden healthcare infra (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2026-02-18T21:00:32 1771448432",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/metriport/jobs/XC2AF8s-senior-security-engineer",
    "first_paragraph": "Open-Source Platform for Healthcare Data IntelligenceMetriport is an open-source data intelligence platform that helps healthcare organizations access and exchange patient data in real-time. We integrate with all major US healthcare IT systems and tap into comprehensive medical data for 300+ million individuals.We've found product-market fit with multi-million ARR, 100+ customers (including Strive Health, Circle Medical, and Brightside Health), backing from top VCs, massive recent infusion of capital, and years of runway. We're ready to scale. We're a tight-knit, high-performing team of mostly former founders (including two YC alumni). We're engineering-heavy, operate with minimal bureaucracy and high autonomy, and hire based on competence, not prestige. We push hard\u2014founders work six days a week from our SF office\u2014but give everyone freedom to craft their schedule. We measure output and we're committed to sustainable intensity.The following points are an assortment of the most relevant"
  },
  {
    "title": "What Every Experimenter Must Know About Randomization (acm.org)",
    "points": 45,
    "submitter": "underscoreF",
    "submit_time": "2026-02-18T19:02:18 1771441338",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=47064845",
    "comments": [
      "I'm no statistician, but the part about halfway through that says not to use PRNGs for random assignment into bins seems wrong to me?Sure I can understand why for a research trial you might want just want to be totally safe and use a source of true randomness, but for all practical purposes a decent PRNG used for sorting balls into buckets is totally indistinguishable from true randomness is it not?I was half expecting this to have been written a few decades ago when really bad PRNGs were in common usage, but the article seems to be timestamped 2025.reply",
      "It reminded me an experiment where each subject was presented with a pseudorandomised sequence of trials, only that, unknown to the researchers, every time the experiment was running the same (default) seed was used, which resulted in all subjects being presented to the same \"random\" sequence of trials.reply",
      "\"If N = 300, even a 256-bit seed arbitrarily precludes all but an unknown, haphazardly selected, non-random, and infinitesimally small fraction of permissible assignments. This introduces enormous bias into the assignment process and makes total nonsense of the p-value computed by a randomization test.\"The first sentence is obviously true, but I'm going to need to see some evidence for \"enormous bias\" and \"total nonsense\".   Let's leave aside lousy/little/badly-seeded PRNGs.  Are there any non-cryptographic examples in which a well-designed PRNG with 256 bits of well-seeded random state produces results different enough from a TRNG to be visible to a user?reply",
      "The argument against PRNGs this paper makes isn't that the PRNG produces results that can be distinguished from TRNG, but that the 256-bit seed deterministically chooses a single shuffling. If you need 300 bits to truly shuffle the assignment but you only have 256 bits, then that's a lot of potential assignments that can never actually happen. With this argument it doesn't matter what the PRNG is, the fact that it's deterministic is all that matters. And this invalidates the p-value because the p-value assumes that all possible assignments are equiprobable, when in fact a lot of possible assignments have a probability of zero.I imagine you could change the p-value test to randomly sample assignments generated via the exact same process that was used to generate the assignment used by the experiment, and as you run more and more iterations of this the calculated p-value should converge to the correct value, but then the question becomes is the p-value calculated this way the same as the p-value you'd get if you actually went ahead and used equiprobable assignment to begin with?Ultimately, this all comes down to the fact that it's not hard to use true randomness for the whole thing, and true randomness produces statistically valid results, if you use true randomness for assignment then you can't screw up the p-value test, and so there's no reason at all to even consider how to safely use a PRNG here, all that does is open the door to messing up.reply",
      "If you have 300 bits of shuffling entropy, you have a lot of potential assignments that can never happen because you won't test them before the universe runs out. No matter how you pick them.Of course a PRNG generates the same sequence every time with the same seed, but that's true of every RNG, even a TRNG where the \"seed\" is your current space and time coordinates. To get more results from the distribution you have to use more seeds. You can't just run an RNG once, get some value, and then declare the RNG is biased towards the value you got. That's not a useful definition of bias.reply",
      "The number of possible assignments has to be effectively close to an integer multiple of the number of shuffles.It doesn't matter how many universes it would take to generate all of them, there are some assignments that are less likely.reply",
      "And why does it matter in the context of randomly assigning participants in an experiment into groups? It is not plausible that any theoretical \"gaps\" in the pseudorandomness are related to the effect you are trying to measure, and unlikely that there is a \"pattern\" created in how the participants get assigned. You just do one assignment. You do not need to pick a true random configuration, just one random enough.I assume that as long as p-values are concerned, the issue raised could very well be measured with simulations and permutations. I really doubt though that the distribution of p-values from pseudorandom assignments with gaps would not converge very fast to the \"real\" distribution you would get from all permuations due to some version of a law of large numbers. A lot of resampling/permutation techniques work by permuting a negligible fraction of all possible permutations, and the distribution of the statistics extracted converges pretty fast. As long as the way the gaps are formed are independent of the effects measured, it sounds implausible that the p-values one gets are problematic because of them.reply",
      "So here's how I would think about it intuitively:We can create a balanced partitioning of the 300 turkeys with a 300 bit random number having an equal number of 1's and 0's.Now suppose I randomly pick 300 bit number, still with equal 0's and 1's, but this time the first 20 bits are always 0's and the last 20 bits are always 1's.  In this scenario, only the middle 260 bits (turkeys) are randomly assigned, and the remaining 40 are deterministic.We can quibble over what constitutes an \"enormous\" bias, but the scenario above feels like an inadequate experiment design to me.As it happens, log2(260 choose 130) ~= 256.> Are there any non-cryptographic examples in which a well-designed PRNG with 256 bits of well-seeded random state produces results different enough from a TRNG to be visible to a user?One example that comes to mind is shuffling a deck of playing cards.  You need approximately 225 bits of entropy to ensure that every possible 52 card ordering can be represented.  Suppose you wanted to simulate a game of blackjack with more than one deck or some other card game with more than 58 cards.  256 bits is not enough there.reply",
      "It's an interesting observation and that's a nice example you provided but does it actually matter? Just because certain sequences can't occur doesn't necessarily mean the bias has any practical impact. It's bias in the theoretical sense but not, I would argue, in the practical sense that is actually relevant. At least it seems to me at first glance, but I would be interested to learn more if anyone thinks otherwise.For example. Suppose I have 2^128 unique playing cards. I randomly select 2^64 of them and place them in a deck. Someone proceeds to draw 2^8 cards from that deck, replacing and reshuffling between each draw. Does it really matter that those draws weren't technically independent with respect to the larger set? In a sense they are independent so long as you view what happened as a single instance of a procedure that has multiple phases as opposed to multiple independent instances. And in practice with a state space so much larger than the sample set the theoretical aspect simply doesn't matter one way or the other.We can take this even farther. Don't replace and reshuffle after each card is drawn. Since we are only drawing 2^8 of 2^64 total cards this lack of independence won't actually matter in practice. You would need to replicate the experiment a truly absurd number of times in order to notice the issue.reply",
      "By the definition of a cryptographically secure PRNG, no. They, with overwhelming probability, produce results indistinguishable from truly random numbers no matter what procedure you use to tell them apart.reply"
    ],
    "link": "https://spawn-queue.acm.org/doi/pdf/10.1145/3778029",
    "first_paragraph": ""
  },
  {
    "title": "When interfaces become disposable (chrisloy.dev)",
    "points": 20,
    "submitter": "chrisloy",
    "submit_time": "2026-02-15T12:43:05 1771159385",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=47023243",
    "comments": [
      "\"The street finds its own uses for things\" is getting to be vastly more true than it was! (And happy upcoming 40th birthday to Burning Chrome!!)I do hope that, even though individual software is not super valuable, that people find broader general systems that help them tie together their many disposable softwares, that their disposable softwares build off of. Just a hope. Seems like quick and dirty is winning, but having more platform underfoot that is reusable and durable and used frequently I think will be powerful. Pi as a self modifying platform is a very lo-fi version of this, is a self extending platform, which is amazing. https://github.com/badlogic/pi-mono https://lucumr.pocoo.org/2026/1/31/pi/The suggestion to have APIs for your product to remain relevant in the new world is extremely good. It still requires understanding the API world at least somewhat. I'd complement that suggestion here with a recommendation to try to lean in hard to webmcp, if you really want to help your users to find agentic success. WebMCP allows for contextual exposure of what the user is already seeing on the page, brings much more session context in that APIs typically do. That seamless blending of user experience & API/m2m control is enormously powerful for letting users go at things. https://webmachinelearning.github.io/webmcp/ https://news.ycombinator.com/item?id=47037501reply",
      "Interfaces strip away Identity. Lot of products are built around monetizing identity.reply",
      "I\u2019m confused by this commentreply",
      "I wrote this yesterday and it\u2019s apropos here:> Mark my words:\nThe era of \u201cPersonal computing\u201d is over\nLarge scale Capital is not gonna make any more investments into microelectronics going forward\nCapital is incentivized to make large data centers and very high speed private Internet, not public Internet, private Internet like starlink\nSo the same way in the 1970s it was the main frame era and server side computing, which turned into server side rendering, which then turned into client side rendering which culminated in the era of the private computer in your home and then finally in your pocket\nwe\u2019re going back to server side model communication and that\u2019s going to encompass effectively the gateway to all other information which will be increasingly compartmentalized into remote data centers and high-speed accesshttps://news.ycombinator.com/item?id=47042473reply",
      "A corollary of Right To Repair \u2014 ability to access an API or service layer allows real customization.For vendors offering a valuable product that provides data or info, this will be a massive boostFor vendors whose offerings are primarily lock-in to a particular interface, ummm, good luck with that...reply"
    ],
    "link": "https://chrisloy.dev/post/2026/02/14/when-interfaces-become-disposable",
    "first_paragraph": "I recently became a parent for the first time, as my wife and I welcomed a beautiful baby boy into our lives in January\nof this year. As I write this, we are still in the first weeks of adjusting to the maelstrom of caring for a fragile,\nprecious life that demands round-the-clock attention. In other words, we are not getting very much sleep.As a productivity nerd, I have in recent years been getting more data-driven about my health. I now wear a FitBit, which\nI use, among other things, to track my sleep, and so of course I was curious to see what the FitBit app could tell me\nabout my new sleep patterns. Unfortunately, I hit a limitation quite quickly: the UX largely expects you to have a\nsingle, unbroken sleep session each night. With a newborn that needs feeding every 2-3 hours, and sleep often coming in\nsnatched naps throughout the day, much of what I wanted to see was hidden behind design decisions that\ncatered to the median user rather than the edge case.But hey, it\u2019s 2026, AI codi"
  },
  {
    "title": "Portugal: The First Global Empire (2015) (historytoday.com)",
    "points": 56,
    "submitter": "Thevet",
    "submit_time": "2026-02-18T07:45:11 1771400711",
    "num_comments": 49,
    "comments_url": "https://news.ycombinator.com/item?id=47058368",
    "comments": [
      "When xenophobia is a useful social defence :> They were most successful in Japan, creating about 300,000 converts until their activities induced a wave of xenophobia and they were either expelled or killed.I am immensely glad that Japan was not colonised early on like the Philippines to their south unfortunately was.reply",
      "Technically, also the last global empire. Macau wasn't returned to China until 1999. https://en.wikipedia.org/wiki/Handover_of_Macaureply",
      "World powers change and shift with changes of technology, climate and needs for resources. Countries rise to power because they are in the right place at the right time, even if monarchs and nationalists will always attribute it to God preference or other self-serving reason.> The first century of Portuguese discoveries saw a successive stripping away of layers of medieval mythology about the world and the received wisdom of ancient authority \u2013 the tales of dog-headed men and birds that could swallow elephants \u2013 by the empirical observation of geography, climate, natural history and cultures that ushered in the early modern age.Technology brings societal change. The world has been becoming smaller with help of each new technological step. Societies can fight it, but it is unavoidable. So, I hope that we focus more on building a good world for us all using technology to improve all our lives.reply",
      "During the Napoleonic wars, the entire Portuguese court relocated to Rio de Janiro which became the new capital.\nhttps://en.wikipedia.org/wiki/Transfer_of_the_Portuguese_cou...reply",
      "For anyone interested in this \"They may have been the first to visit Australia.\", the comment refers to a wreck supposedly found in 1836 by whalers near Warrnambool.https://en.wikipedia.org/wiki/Mahogany_Shipreply",
      "Spain was the first globalization, not Portugal. The article forgets to mention two key elements:1) The Manila galeon[1], the first trading route connecting Europe, America and Asia. This was the first trully global trade route (Portugual never established a trans-Pacific route).2) The Real de a Ocho[2], the first global currency, used virtually everywhere including the US until the modern dollar replaced it in 1857. It still lives through the $ symbol, representing the Pillars of Hercules and the \"Plus Ultra\" script [3].It also downplays the role of Spain in the first circumnavigation. Sure, Magellan was born in Portugal, but he sailed for the Spanish Crown. The expedition was financed by Spain, sailed Spanish ships and finished its trip commanded by a Spanish sailor (Juan Sebasti\u00e1n Elcano).Finally, it is worth mentioning that the Spanish was not an empire of mere territorial possession, it was a civilization. Spain has currently 50 sites inscribed as UNESCO World Heritage [4], and from the ~150 sites in the Americas, ~50 were built by Spain. These includes entire cities, universities, hospitals, infrastructure, defenses and more [5].[1] https://www.youtube.com/watch?v=UGRn5qCAXBI[2] https://en.wikipedia.org/wiki/Spanish_dollar[3] https://en.wikipedia.org/wiki/Plus_ultra[4] https://en.wikipedia.org/wiki/List_of_World_Heritage_Sites_i...[5] https://greatbritainandtheusatheirtruehistory.quora.com/33-c...reply",
      "As a history enjoyer I have actually heard of this:> The Black Legend (Spanish: leyenda negra) or the Spanish Black Legend (Spanish: leyenda negra espa\u00f1ola) is a purported historiographical tendency which consists of anti-Spanish and anti-Catholic propagandahttps://en.wikipedia.org/wiki/Black_Legendreply",
      "Don\u2019t forget the Dutch who were the first to have colonies in North and South America, Africa and Asia.reply",
      ">This was the first trully global trade route (Portugual never established a trans-Pacific route).You're saying because Portugal traded with Asia through the wrong ocean, it wasn't global? Seems like an odd metric.reply",
      "No, I'm sayng that Portugal never closed the circuit that led to a global trade route. They built a line between Europe and Asia, but Asia and America remained economically disconnected. It was that loop that Spain closed that enabled a global economy.reply"
    ],
    "link": "https://www.historytoday.com/archive/first-global-empire",
    "first_paragraph": "\n                    This website is using a security service to protect\n                    itself from online attacks. We are checking your browser\n                    to establish a secure connection and keep you safe.\n                \n...\n\n\nPlease enable JavaScript to continue.\n\nPlease enable JavaScript to continue."
  },
  {
    "title": "Show HN: VectorNest responsive web-based SVG editor (ekrsulov.github.io)",
    "points": 69,
    "submitter": "ekrsulov",
    "submit_time": "2026-02-18T15:31:57 1771428717",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=47062096",
    "comments": [
      "I liked the simplistic UI, but it is not quite ready for serious use.I tried using it on a simple svg that i had (around 1KB, just few simple lines and shapes). But it did not rendered them properly. Colors were off (black-box instead of original colors), and in one place it was showing at triangle instead of an L shaped line.Also when I move an object, a single Cmd+Z wont undo the action. Have to repeat twice for object to go back (I am using Chrome on macbook)reply",
      "Thank you \u2014 this is exactly the kind of feedback I\u2019m looking for. If you\u2019re able to share the SVG that caused the issue, I\u2019d love to take a look and reproduce the problem.The undo behavior you described is a known issue \u2014 some interactions currently require multiple undo steps, and I\u2019ll be looking for a proper solution in the medium term. Really appreciate you taking the time to test and report this.reply",
      "How much time did it take you to build it? For the past several years I have been working full time on an SVG editor myself (https://boxy-svg.com) and I get a bit anxious when I see what LLMs are capable of nowadays.reply",
      "First of all \u2014 I\u2019ve seen Boxy before, and congratulations on the product you\u2019ve built. It\u2019s genuinely impressive, especially considering how much depth a serious SVG editor requires.This result is actually the fourth iteration of VectorNest. In previous versions I would build something, then restart from scratch \u2014 but always reusing pieces and, more importantly, the learning from the prior attempts. The big leap happened in the last few months, mainly due to an architectural decision (moving to a plugin-based core) and the noticeable improvement in LLM precision, which made iteration much more reliable.For me this is a side project, so I only dedicate a few hours per day. I started the first iteration less than a year ago, and the current iteration began about four months ago.I completely understand the anxiety around what LLMs can now produce \u2014 but I also think building something robust and production-ready still requires a lot of architectural thinking and long-term iteration.reply",
      "Wow! I'd never heard of it. Just checked it out and it's fantastic. Great job!reply",
      "Huge thanks for Boxy! I\u2019ve always wanted to know who made it - it\u2019s an impressive piece of software and has been for many years.Taught me a lot about SVG!reply",
      "Pen tool control points/curves are rendered in the wrong location: https://imgur.com/a/QXQoqOIreply",
      "I believe I\u2019ve been able to reproduce the issue and implemented a fix. Could you please try again and let me know if it\u2019s resolved now?reply",
      "Now it's good! Great job!reply",
      "Thanks for reporting this! Would you be able to share the SVG you\u2019re testing with and the browser/version you\u2019re using? That would really help me reproduce and investigate the issue.reply"
    ],
    "link": "https://ekrsulov.github.io/vectornest/",
    "first_paragraph": ""
  }
]