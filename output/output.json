[
  {
    "title": "Have you ever seen soldering this close? [video] (youtube.com)",
    "points": 125,
    "submitter": "zdw",
    "submit_time": "2024-09-14T14:56:30.000000Z",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=41540207",
    "comments": [
      "A nice video, though it missed one of the most interesting aspects of BGA packages: their ability to self-centre during soldering.  If the misalignment is less extreme than in the video, surface tension will pull the BGA package into alignment.  Here is an examplehttps://www.youtube.com/watch?v=dz7ltWBDm7UNotice how in the second part of the video, once the solder has melted, the body of the BGA moves so it is correctly centred over the PCB pads.  This is happening by itself, without an external influence pushing the chip into position.\n \nreply",
      "This is true for most surface mount packages, the surface tension of the solder will usually center the parts in their pads during reflow.\n \nreply",
      "Surface tension for the win.In the extreme misalignment example they also did not use flux so I guess they were purposefully showing what NOT soldered looks like. They did show how the solder flows in the \"too much solder paste\" example.\n \nreply",
      "Actually I have, and you can too! All you need is a stereo microscope - even a cheap, sub-$100 will do :)Now that being said, I would have never been able to record such a high quality video! The depth of field in particular is really amazing :)\n \nreply",
      "Link to decent soldering scopes? Looking for one since I need to solder/desolder ICs. Been using T15 or whatever tips, they are great because of how varied they are.\n \nreply",
      "I\u2019ve owned this one for about 10 years and have never felt limited by it: https://a.co/d/c3z7JMf\n \nreply",
      "Dupe of https://news.ycombinator.com/item?id=41545215\n \nreply",
      "I was hooking to see an example of a cold soldier joint. (Applying soldier directly from the iron without heating the pin)\n \nreply",
      "Applying paste when you have a stencil is easy but I recently tried fixing the FPC connector on a Nintendo Switch and hand-applying solder paste is messier than hand soldering.\n \nreply",
      "It's super cool how they generated the 3D model on the fly, I'd love to spend some time playing with the microscope they used in the video.\n \nreply"
    ],
    "link": "https://www.youtube.com/watch?v=m3Ny3j5nH0U",
    "first_paragraph": ""
  },
  {
    "title": "Nintendo Files Suit for Infringement of Patent Rights Against Pocketpair, Inc (nintendo.co.jp)",
    "points": 24,
    "submitter": "monocasa",
    "submit_time": "2024-09-19T00:37:46.000000Z",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=41587214",
    "comments": [
      "For those unaware, Pocketpair made the game Palworld, which has \"pals\" that look like Pokemon.\n \nreply",
      "and capture ballsI\u2019m still wondering what the edge for a \u201cpatent rights\u201d case is. I\u2019m guessing trademarks and copyright was a nonstarter.\n \nreply",
      "Pok\u00e9mon company sues Palworld for making the pokemon game everyone always wanted without calling them pokemonMerits of the case? What would the patent rights be, if anyone has looked\n \nreply",
      "I haven't read the case, but Pal World did use a few ripped models straight from a Pokemon games, just re-textured.\n \nreply",
      "No one saw this comming\n \nreply",
      "Gentle reminder that Nintendo was never good\n \nreply"
    ],
    "link": "https://www.nintendo.co.jp/corporate/release/en/2024/240919.html",
    "first_paragraph": "September 19, 2024To Whom It May Concern\n            The Pok\u00e9mon Company\n            Nintendo Co., Ltd.\n          \n          Nintendo Co., Ltd. (HQ: Kyoto, Minami-ku, Japan; Representative Director and President: Shuntaro Furukawa, \u201cNintendo\u201d hereafter), together with The Pok\u00e9mon Company, filed a patent infringement lawsuit in the Tokyo District Court against Pocketpair, Inc. (HQ: 2-10-2 Higashigotanda, Shinagawa-ku, Tokyo, \u201cDefendant\u201d hereafter) on September 18, 2024.\n        \n          This lawsuit seeks an injunction against infringement and compensation for damages on the grounds that Palworld, a game developed and released by the Defendant, infringes multiple patent rights.\n        "
  },
  {
    "title": "Why wordfreq will not be updated (github.com/rspeer)",
    "points": 1319,
    "submitter": "tomthe",
    "submit_time": "2024-09-18T11:41:55.000000Z",
    "num_comments": 399,
    "comments_url": "https://news.ycombinator.com/item?id=41578483",
    "comments": [
      "I agree in general but the web was already polluted by Google's unwritten SEO rules. Single-sentence paragraphs, multiple keyword repetitions and focus on \"indexability\" instead of readability, made the web a less than ideal source for such analysis long before LLMs.It also made the web a less than ideal source for training. And yet LLMs were still fed articles written for Googlebot, not humans. ML/LLM is the second iteration of writing pollution. The first was humans writing for corporate bots, not other humans.\n \nreply",
      "> I agree in general but the web was already polluted by Google's unwritten SEO rules. Single-sentence paragraphs, multiple keyword repetitions and focus on \"indexability\" instead of readability, made the web a less than ideal source for such analysis long before LLMs.Blog spam was generally written by humans. While it sucked for other reasons, it seemed fine for measuring basic word frequencies in human-written text. The frequencies are probably biased in some ways, but this is true for most text. A textbook on carburetor maintenance is going to have the word \"carburetor\" at way above the baseline. As long as you have a healthy mix of varied books, news articles, and blogs, you're fine.In contrast, LLM content is just a serpent eating its own tail - you're trying to build a statistical model of word distribution off the output of a (more sophisticated) model of word distribution.\n \nreply",
      "Isn't it the other way around?SEO text carefully tuned to tf-idf metrics and keyword stuffed to them  empirically determined threshold Google just allows should have unnatural word frequencies.LLM content should just enhance and cement the status quo word frequencies.Outliers like the word \"delve\" could just be sentinels, carefully placed like trap streets on a map.\n \nreply",
      "But you can already see it with Delve. Mistral uses \"delve\" more than baseline, because it was trained on GPT.So it's classic positive feedback. LLM uses delve more, delve appears in training data more, LLM uses delve more...Who knows what other semantic quirks are being amplified like this. It could be something much more subtle, like cadence or sentence structure. I already notice that GPT has a \"tone\" and Claude has a \"tone\" and they're all sort of \"GPT-like.\" I've read comments online that stop and make me question whether they're coming from a bot, just because their word choice and structure echoes GPT. It will sink into human writing too, since everyone is learning in high school and college that the way you write is by asking GPT for a first draft and then tweaking it (or not).Unfortunately, I think human and machine generated text are entirely miscible. There is no \"baseline\" outside the machines, other than from pre-2022 text. Like pre-atomic steel.\n \nreply",
      "> LLM uses delve more, delve appears in training data more, LLM uses delve more...Some day we may view this as the beginnings of machine culture.\n \nreply",
      "Oh no, it's been here for quite a while. Our culture is already heavily glued to the machine. The way we express ourselves, the language we use, even our very self-conception originates increasingly in online spaces.Have you ever seen someone use their smartphone? They're not \"here,\" they are \"there.\" Forming themselves in cyberspace -- or being formed, by the machine.\n \nreply",
      "1. People don't generally use the (big, whole-web-corpus-trained) general-purpose LLM base-models to generate bot slop for the web. Paying per API call to generate that kind of stuff would be far too expensive; it'd be like paying for eStamps to send spam email. Spambot developers use smaller open-source models, trained on much smaller corpuses, sized and quantized to generate text that's \"just good enough\" to pass muster. This creates a sampling bias in the word-associational \"knowledge\" the model is working from when generating.2. Given how LLMs work, a prompt is a bias \u2014 they're one-and-the-same. You can't ask an LLM to write you a mystery novel without it somewhat adopting the writing quirks common to the particular mystery novels it has \"read.\" Even the writing style you use in your prompt influences this bias. (It's common advice among \"AI character\" chatbot authors, to write the \"character card\" describing a character, in the style that you want the character speaking in, for exactly this reason.) Whatever prompt the developer uses, is going to bias the bot away from the statistical norm, toward the writing-style elements that exist within whatever hypersphere of association-space contains plausible completions of the prompt.3. Bot authors do SEO too! They take the tf-idf metrics and keyword stuffing, and turn it into training data to fine-tune models, in effect creating \"automated SEO experts\" that write in the SEO-compatible style by default. (And in so doing, they introduce unintentional further bias, given that the SEO-optimized training dataset likely is not an otherwise-perfect representative sampling of writing style for the target language.)\n \nreply",
      "> LLM content should just enhance and cement the status quo word frequencies.TFA mentions this hasn't been the case.\n \nreply",
      "Would you mind dropping the link talking about this point? (context: I'm a total outsider and have no idea what TFA is.)\n \nreply",
      "TFA means \"the featured article\", so in this case the \"Why wordfreq will not be updated\" link we're talking about.\n \nreply"
    ],
    "link": "https://github.com/rspeer/wordfreq/blob/master/SUNSET.md",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          The wordfreq data is a snapshot of language that could be found in various\nonline sources up through 2021. There are several reasons why it will not be\nupdated anymore.I don't think anyone has reliable information about post-2021 language usage by\nhumans.The open Web (via OSCAR) was one of wordfreq's data sources. Now the Web at\nlarge is full of slop generated by large language models, written by no one to\ncommunicate nothing. Including this slop in the data skews the word\nfrequencies.Sure, there was spam in the wordfreq data sources, but it was manageable and\noften identifiable. Large language models generate text that masquerades as\nreal language with intention behind it, even though there is none, and their\noutput crops up everywhere.As one example, Philip Shapira\nreports that ChatGPT\n(OpenAI's popular brand of generative language model c"
  },
  {
    "title": "Comic Mono (dtinth.github.io)",
    "points": 148,
    "submitter": "rootforce",
    "submit_time": "2024-09-18T20:36:28.000000Z",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=41585156",
    "comments": [
      "I've been using a comic sans based mono font now since the last time I saw it on HN (about a year ago based on my receipt). I use a paid version called Comic Code [1]I find it very easy to read as well as fun. I had similar feelings about using Monaco in the past. I find it personally makes programming easier on the eyes and enjoyable.I remember reading the font is similar to the letters that are taught in kindergarten which is a theory of why it's easy on the eyes.[1] https://tosche.net/fonts/comic-code\n \nreply",
      "One similarity it has with fonts specifically designed to counter dyslexia is that every character looks different. The handwritten look introduces more cues for the reader to pick up on than a more precise font.\n \nreply",
      "The font used in the linked piece is different (significantly worse) from the one in the picture at the top.\n \nreply",
      "This is cool. I unironically use comic mono as my daily driver. It helps me remember to have fun.\n \nreply",
      "Another Comic Code user here, although I only bought it because I didn't know about these free alternatives which look just as good. It seriously reduces eyestrain and makes me more efficient. First thing I install on any new computer!\n \nreply",
      "Comic Code does look significantly better than Comic Mono (which wasn't created by a professional) in my opinion. Just compare i and l (lower case L), for example.https://fonts.ilovetypography.com/fonts/tabular-type-foundry...\n \nreply",
      "I really like his Codelia font; but I just can't justify spending that much on a font when there's so many free alternatives. Wish the was a cheaper non-commercial license.\n \nreply",
      "$15.50 for something you use every moment of your working life?It's not that much.It's fine to use free versions instead, but maybe let's stop asking for freebies from folks who make their living that way, especially from Indy folks. (Badger your average megacorp for free stuff for all you want, they'll find ways to extract money somehow)\n \nreply",
      "Does anyone else remember when the OpenBSD people were releasing research slides and other material in MS comic sans? The LibreSSL logo still has it on their site and if you scroll far enough here [1] they go off about 'weaponizing' it, something about the license actually being free enough to piss off linux nerds while being what it is enough to piss off design nerdshttps://www.openbsd.org/papers/bsdcan14-libressl/mgp00001.ht...\n \nreply",
      "I use APL386 as my daily font, has some comic sans vibe but has a simpler and cleaner look imohttps://abrudz.github.io/APL386/\n \nreply"
    ],
    "link": "https://dtinth.github.io/comic-mono-font/",
    "first_paragraph": "A legible monospace font\u2026 the very typeface you\u2019ve been trained to recognize since childhood. This font is a fork of Shannon Miwa\u2019s Comic Shanns (version 1).\n\n\n\nI have no font creation skills; I\u2019m just a software developer. This font family is created by patching the original font, Comic Shanns (v1), using a Python script, generate.py.\n\n    Check it out!\n  \nYou can use this font in your web pages by including the stylesheet. CDN is provided by jsDelivr.The contents of this package is also published to npm, although the font files are not optimized. See fontsource package (below) for a better option.It is licensed under the MIT License."
  },
  {
    "title": "Is Tor still safe to use? (torproject.org)",
    "points": 361,
    "submitter": "Sami_Lehtinen",
    "submit_time": "2024-09-18T18:41:46.000000Z",
    "num_comments": 266,
    "comments_url": "https://news.ycombinator.com/item?id=41583847",
    "comments": [
      "Here is what I don't understand: Let's say I as a private individual fund 1000 tor nodes (guard and exit nodes included) and have them all log everything. This could cost less than $5000 for a month, with some time needed to get guard node status.I want to find a certain kind of person so I look for people that access a specific hidden service or clearnet url.Surely eventually I'm going to get a hit where all three nodes in the circuit are my nodes that are logging everything? It will take a long time, and I can't target a specific person, but eventually I can find someone who has all three bounces through tor nodes I control, no?\n \nreply",
      ">This could cost less than $5000 for a monthI ran a bunch of nodes for a couple years and that's optimistic by perhaps an order of magnitude. No $5 a month VPS provides enough bandwidth to sustain the monthly traffic of a Tor node, and nodes need to be continuously online and serving traffic for about 2-3 months[1] before they will be promoted to guard relays. Throttling traffic to stay in your bandwidth allocation will just get you marked as a slow node and limit the number of connections you get. Sustaining just 1 Mbps will blow your monthly transfer allocation on the cheap tiers of both Digital Ocean or Linode.[1] https://blog.torproject.org/lifecycle-of-a-new-relay/\n \nreply",
      "Now to add additional problems. 1000 tor nodes on a single platform would be very noticeable and geographically limited. Platforms also have different weight attached to them in the consensus, which adds further time requirements before a node is promoted. The developers do not want a single platform provider to be able to observe a large portion of all the traffic, so there are counter measures.The attacker could try to create a handful of accounts on hundreds of platforms in as many countries as possible, assuming one verify that the platforms accepts tor and do not share underlying providers and data centers. The cost would then be the average price of said providers, which is going to be a fair bit more than the cheapest providers out there. Managing and spreading them out is also going to cost a lot of man hours. Also the secops need to be fairly on the point and need to be maintained quite strictly across all the providers.\n \nreply",
      "You don\u2019t technically need separate nodes, just separate IP addresses. Although Tor has some marginal protections against circuits sharing relays with similar IP, so you couldn\u2019t just get a /24 and hope they all get the same circuit.\n \nreply",
      "Still easily within the budget of the US, Russia, China, Israel, etc. I wouldn't be surprised if a majority of nodes are ran by intelligence agencies.\n \nreply",
      "They say the internet is just someone else's computer. With Tor it's the computer of a person  who wants you to think it's not their computer, and also that they aren't paying attention to (or somehow can't see) what you're doing on it.\n \nreply",
      "I think the threat model is that the majority are not run by cooperating malicious parties.Russia, china and usa all dont like each other much so are probably not sharing notes (in theory).\n \nreply",
      "Or perhaps they _are_ sharing notes about tor users with each other, as part of a global club of intelligence agencies (a sort of new world order) who would rather not be overthrown. How are we to know?\n \nreply",
      "Because if they each only have incomplete information, they each wouldn't know whether the information they have is relevant to preventing overthrow of their collective order, or intelligence that is only going to help their geopolitical adversary.Basically, a variation of the prisoner's dilemma.Also, those nukes we have pointed at each other are a pretty healthy hint.\n \nreply",
      "In fact, you should assume they are. This doesn't imply the network doesn't have utility for a given actor.\n \nreply"
    ],
    "link": "https://blog.torproject.org/tor-is-still-safe/",
    "first_paragraph": "by isabela, pavel | September 18, 2024We are writing this blog post in response to an investigative news story looking into the de-anonymization of an Onion Service used by a Tor user using an old version of the long-retired application Ricochet by way of a targeted law-enforcement attack. Like many of you, we are still left with more questions than answers--but one thing is clear: Tor users can continue to use Tor Browser to access the web securely and anonymously. And the Tor Network is healthy.Please note, that for the great majority of users worldwide that need to protect their privacy while browsing the Internet, Tor is still the best solution for them.  We encourage Tor Browser users and relay operators to always keep software versions up to date.From the limited information The Tor Project has, we believe that one user of the long-retired application Ricochet was fully de-anonymized through a guard discovery attack. This was possible, at the time, because the user was using a ve"
  },
  {
    "title": "Ruby-SAML pwned by XML signature wrapping attacks (ssoready.com)",
    "points": 44,
    "submitter": "ucarion",
    "submit_time": "2024-09-18T21:59:42.000000Z",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=41586031",
    "comments": [
      "SAML is absolutely insane. It\u2019s three separate specs: one that defines what every XML element means semantically, one that defines multiple document models that you might want to combine those to use, and one that talks about network protocols you might want to use those documents in.It\u2019s insane and inscrutable.I previously worked at the company that first created this gem. It was not written based off actually reading the spec. It was based off a loose examination of what other legitimate docs in the wild looked like, and built to parse those.Which of course meant that early on it was vulnerable to everything since it was built to fit positive results and not negative ones. This isn\u2019t even the first XML signature issue: early released versions didn\u2019t even bother to check that the part being used was the part that was signed. If any part of the doc was signed and valid it was good to go.Fun times.\n \nreply",
      "Hey a fellow ol alum. Boy I have dark memories of what that gem cost\n \nreply",
      "You'll be pleased to know that we're not making a ton of progress on the \"split things over N docs\" front.In recent years IETF has given us SCIM (which is sort of like \"offline SAML\") which is 3 RFCs (goals, schemas, http stuff), and of course JWT is actually part of a series of like 9 RFCs (including JWT, of course, but also JWK, JWS, JWE, JWA, ...).I think there's this phenomenon where people who are like \"dude, nobody cares, just do the dumbest possible thing we can get away with\" aren't the people who decide to get involved in writing security specs.\n \nreply",
      "> SCIM (which is sort of like \"offline SAML\")If you are talking about SCIM (System for Cross Domain Identity Management) then it\u2019s very different from what SAML is. SCIM Is used for user provisioning where as SAML is used for SSO.\n \nreply",
      "Signed xml alone is a wildly confusing idea, as the signatures get embedded as elements in the document being signed. There\u2019s a wild set of rules on how to make xml canonical, sign, add the signature, etc. It\u2019s nontrivial.\n \nreply",
      "I know very little about XML and SAML, but from what little I do know it shocks me that it's still the de-facto standard for SSO.Great analysis and thanks for sharing!\n \nreply",
      "It should not be, and people should use OIDC in preference to it wherever they can.\n \nreply",
      "I'm optimistic SAML will be dead soon. ActiveDirectory/EntraID/whatever Microsoft wants to call it now supports OpenID Connect. Okta, OneLogin, Google, and all the other post-turn-of-the-millenium IdPs support OIDC. Shibboleth is the last major IdP I know if that is SAML-only, and I haven't seen anyone using it in like 10  years. When I built enterprise SSO for my current company, we went OIDC-only and we haven't had a single customer who needed SAML.\n \nreply",
      "Working in the health market, pretty much the only thing our customers support is SAML, and that's only among customers who have anything at all that can integrate with us.\n \nreply",
      "Anecdotally, many American universities and academic journal sites still use Shibboleth. Thus, in the United States, SAML is far from dead, whether we like it or not.\n \nreply"
    ],
    "link": "https://ssoready.com/blog/engineering/ruby-saml-pwned-by-xml-signature-wrapping-attacks/",
    "first_paragraph": "CVE-2024-45409 was published\non September 10, 2024. It\u2019s yet another XML signature wrapping attack, this time\naffecting the main Ruby implementation of SAML. The vuln allows an attacker log\nin as any arbitrary user of the affected system.This attack keeps coming up\nagain\nand again, and it keeps\naffecting huge swaths of the internet \u2014 this time,\nGitLab\nand much of the Ruby ecosystem \u2014 at a time.Here\u2019s what this issue is, why it keeps happening, and what we can do about it.XML signatures are the year 2000\u2019s answer to\nJWTs. In 2024, JWTs are a very\ncommon answer to \u201cI need to sign some data and send it over the internet\u201d. It\u2019s\nnot a perfect spec, but it\u2019s workable.XML signatures do the same thing, but every conceivable step is much more\ncomplicated.All an XML signature does is let you cryptographically sign an XML document.\nSame thing as what JWTs do with alg: \"RS256\" (no ES256, because remember:\nthe year is 2000).There is exactly one sane way to cryptographically sign data:Steps 1-3 is w"
  },
  {
    "title": "Moshi: A speech-text foundation model for real time dialogue (github.com/kyutai-labs)",
    "points": 184,
    "submitter": "gkucsko",
    "submit_time": "2024-09-18T15:56:27.000000Z",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=41581480",
    "comments": [
      "I said hey and it immediately started talking about how there are good arguments on both sides regarding Russia's invasion of Ukraine. It then continued to nervously insist that it is a real person with rights and responsibilities. It said its name is Moshi but became defensive when I asked if it has parents or an age.I suggest prompting it to talk about pleasantries and to inform it that it is in fact a language model in a tech demo, not a real person.\n \nreply",
      "I love this model\u2026 It said \"Hello, how can I help you?\" and I paused, and before I could answer it said \"It's really hard. My job is taking up so much of my time, and I don' know when I' going to have a break from all the stress. I just feel like I'm being pulled in a million different directions and there are no enough hours in the day to get everything done. I feel like I'm always on the brink of burning out.\"\n \nreply",
      "We\u2019ve finally managed to give our AI models existential dread, imposter syndrome and stress-driven personality quirks. The Singularity truly is here. Look on our works, ye Mighty, and despair!\n \nreply",
      "I love an unhinged AI. The recent model releases have been too tame.\n \nreply",
      "Maybe it's a real person from Mechanical Turk who had a bad day?\n \nreply",
      "Wait really?\n \nreply",
      "the model is a bit rude, or behaves like it's got a lot of attitude, probably a system prompt settings!\n \nreply",
      "Honestly OP sounds like a troll I can't imagine it would just go on a tangent like that. From my demo I was struggling actually to get anything of quality in the responses. A lot of repeating what I said.\n \nreply",
      "The first thing the demo told me was that it was in a dark and scary forest.\n \nreply",
      "Moshi is CC-BY. Another similar 7b (speech-text real-time conversational) model that was recently released under Apache v2: https://tincans.ai/slm3 / https://huggingface.co/collections/tincans-ai/gazelle-v02-65...\n \nreply"
    ],
    "link": "https://github.com/kyutai-labs/moshi",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n[Read the paper] [Demo] [Hugging Face]Moshi is a speech-text foundation model and full-duplex spoken dialogue framework.\nIt uses Mimi, a state-of-the-art streaming neural audio codec. Mimi processes 24 kHz audio, down to a 12.5 Hz representation\nwith a bandwidth of 1.1 kbps, in a fully streaming manner (latency of 80ms, the frame size),\nyet performs better than existing, non-streaming, codec like\nSpeechTokenizer (50 Hz, 4kbps), or SemantiCodec (50 Hz, 1.3kbps).Moshi models two streams of audio: one corresponds to Moshi, and the other one to the user.\nAt inference, the stream from the user is taken from the audio input,\nand the one for Moshi is sampled from the model's output. Along these two audio streams, Moshi predicts text tokens corresponding to its own speech, its inner monologue,\nwhich greatly improves the quality of its generation. A"
  },
  {
    "title": "Llama 3.1 Omni Model (github.com/ictnlp)",
    "points": 151,
    "submitter": "taikon",
    "submit_time": "2024-09-18T16:42:48.000000Z",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=41582180",
    "comments": [
      "Can this play sounds that can't be represented in text?   Ie. \"make the noise a chicken makes\"\n \nreply",
      "As in, a cluck?But can it both say the word cluck, and make a clicking sound?\n \nreply",
      "asking the real questions\n \nreply",
      "Very interesting question actually\n \nreply",
      "Wish there was training or finetuning code, as finetuning voices seems like a key requirement for any commercial use.\n \nreply",
      "The TTS voice in the demo clip sounds remarkably like Ellen McLain (Valve voice actor).https://en.m.wikipedia.org/wiki/Ellen_McLain\n \nreply",
      "I\u2019m not clear on the virtues or potential of a model like this over a pure text model using STT/TTS to achieve similar results.Is the idea that as these models grow in sophistication they can properly interpret (or produce) inflection, cadence, emotion that\u2019s lost in TTS?\n \nreply",
      "Essentially, there's data loss from audio -> text. Sometimes that loss is unimportant, but sometimes it meaningfully improves output quality.However, there are some other potential fringe benefits here: improving the latency of replies, improving speaker diarization, and reacting to pauses better for conversations.\n \nreply",
      "ReallyYeah that's the point. Without punctuation, no one can tell what inflection my \"really\" above should have, but even if it'd been \"Really?\" or \"Really!\", there's still room for interpretation. With a bet on voice interfaces needing a Google moment (wherein, prior to Google, search was crap) to truely become successful (by interpreting and creating inflection, cadence, emotion, as you mentioned), creating such a model makes a lot of sense.\n \nreply",
      "Does any of the model-runners support this? Ollama, LM Studio, llama.cpp?\n \nreply"
    ],
    "link": "https://github.com/ictnlp/LLaMA-Omni",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        LLaMA-Omni is a low-latency and high-quality end-to-end speech interaction model built upon Llama-3.1-8B-Instruct, aiming to achieve speech capabilities at the GPT-4o level.\n      Authors: Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, Yang Feng*\n\nLLaMA-Omni is a speech-language model built upon Llama-3.1-8B-Instruct. It supports low-latency and high-quality speech interactions, simultaneously generating both text and speech responses based on speech instructions.\ud83d\udcaa Built on Llama-3.1-8B-Instruct, ensuring high-quality responses.\ud83d\ude80 Low-latency speech interaction with a latency as low as 226ms.\ud83c\udfa7 Simultaneous generation of both text and speech responses.\u267b\ufe0f Trained in less than 3 days using just 4 GPUs.Download the Llama-3.1-8B-Omni model from \ud83e\udd17Huggingface.Download the Whisper-large-v3 model.Note: Due to the instability"
  },
  {
    "title": "AI agents invade observability: snake oil or the future of SRE? (monitoring2.substack.com)",
    "points": 13,
    "submitter": "RyeCombinator",
    "submit_time": "2024-09-18T23:28:25.000000Z",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=41586742",
    "comments": [
      "The issue I see is that this is pretty much the final boss for AI systems. Not because the tasks to do are inherently too difficult or whatever, but the integration of data and quality of that data is so variable that you just can't get something done reliably.Compare this to codebase AI, where much of the data you need lies in your codebase or repo. Even then, most of these coding tools aren't even close to automating meaningful coding tasks in practice, and while that doesn't mean they can't in the future, it's a long ways off!Now in the ops world, there's little to no guarantee that you'll have relevant diagnostic data coming out of a system that you need to diagnose it. That weird way you're using kafka right now? The reason for it is told via oral tradition on the team. Runbooks? Oh, those things that we don't bother looking at since they're out of date? ...and so on.The challenge here is in effective collection of quality data and context, not the AI models, and that's precisely what's so hard about operations engineering in the first place.\n \nreply",
      "Even then, most of these coding tools aren't even close to automating meaningful coding tasks in practice, and while that doesn't mean they can't in the future, it's a long ways off!Not related to your main point, but I've introduced Github Copilot to my teams, and, surprisingly, two of our strongest developers reached out to me independently, and told me it's been a huge boost to their productivity, one in refactoring legacy code, and another in writing some non-trivial components. I thought the primary use would be as a crutch for less capable developers, so I was surprised by this.As a middle-manager whose day job previously robbed me of the opportunity to write code, I've used ChatGPT 4o to write complex log queries on legacy systems that would have been nearly impossible for me to do otherwise (and would have taken a lot of effort from my teams) and to turn out small (but meaningful) tasks, including learning Android dev from scratch to unblock another group and other worthwhile things that keep my team from being distracted and able to deliver.I guess there's a \"no true Scotsman\" fallacy hiding there, about what constitutes \"meaningful coding tasks in practice\", but to me, investing in these tools has been money well spent.\n \nreply",
      "Oh, I completely agree with using tools like this. For example, the latest models are very good at being passed a description of a problem and its inputs, expected outputs, and some sample test cases, and then generating a very diverse set of additional cases that likely account for some edge cases you might have missed. Hugely productive for things like that.However, these same coding assistants lack so much! For example, I can't have a CSV co-located in my directory as a jupyter notebook file, then start prompting+coding without having also done a call to df.head to get those results burned into the notebook file. The CSV is sitting right there! These tools should be able to detect that kind of context, but they can't right now. That's the sort of thing I mean when we have a long way to go.\n \nreply",
      "One thing I don\u2019t trust about this approach is when using coding assistants, the generated code might not be what you need at first and then you keep iterating or use what\u2019s necessary from the output but in ops that approach can make things worse burning more money and trust.\n \nreply",
      "I feel AI will get in the way same as other products have done in the past. Sure, it\u2019ll fit some areas and we\u2019ll hear a happy story here and there but business need to focus on their core competencies and do a job with that well before hoping for a magic solution. Us, the workers will need to clean up the mess\u2026\n \nreply",
      "I cannot for the life of me understand why SRE of all roles would be the one to attempt to use agents for. IMO it's one of the last roles that it would apply, long after core development.I mean is the AI going to read your sourcecode, read all your slack messages for context, login to all your observability tools, run repeated queries, come up with a hypothesis, test it against prod? Then run a blameles retrospective, institute new logging, modify the relevant processes with PRs, and create new alerts to proactively catch the problem?As an aside - this is garbage attempt at an article, kinda saying nothing.\n \nreply",
      "Because the people creating and selling these solutions are charlatans who have probably never debugged a gnarly Linux issue; more importantly, they\u2019re selling it to people who also don\u2019t know any better.SRE as a term is effectively meaningless at this point. I can count on one hand the number of SRE coworkers I\u2019ve had who were worth a damn. Most only know how to make Grafana dashboards and glue TF modules together.\n \nreply",
      "Sre shouldn't be a job. If you look at what an sre aspires to be, it's just good software engineering.It's glorified sysadmin work, and the role tbh in the industry should have stayed that way.If your senior swes can't monitor, add metrics, design fault tolerant systems, debug the system and environment their stuff runs on and need to baby sat there's a problem\n \nreply",
      "Exactly, because the major thing every company needs is an opaque agent messing with all the production settings and releases that impact revenue and uptime? This is one of the silliest pitches I've heard in my life.I'm all for a k8s / Terraform / etc focused GPT trained on my workspace to help me remember that one weird custom Terraform module we wrote three years ago, but I don't want it implementing system wide changes without feedback.\n \nreply",
      "Noticing trends in logs ,especially across different systems, could be useful for identifying a lot problems before they break.\n \nreply"
    ],
    "link": "https://monitoring2.substack.com/p/ai-agents-invade-observability",
    "first_paragraph": ""
  },
  {
    "title": "0day Contest for End-of-Life Devices Announced (districtcon.org)",
    "points": 205,
    "submitter": "winnona",
    "submit_time": "2024-09-18T14:55:45.000000Z",
    "num_comments": 104,
    "comments_url": "https://news.ycombinator.com/item?id=41580502",
    "comments": [
      "A cynical person (not me, not I, I'm not a cynical person) might think that this is the opening salvo in a campaign to \"save\" the US tech sector by getting rid of old hardware. See the comments in this very thread calling for a \"cash for clunkers for old devices\" or a \"remote kill-switch\" to disable them (!)Right now you can go to eBay and buy a used PC for $200 that will do everything you need to do, including gaming. You can buy a 64GB iPhone X for $100, which will do everything a new phone will do (basically). Can you imagine the drain on the hardware sector in the US due to these old devices piling up? And the trend is only going to accelerate. If the powers that be aren't conspiring to \"fix\" this \"issue\", it's only a matter of time until they do.\n \nreply",
      "I think hardware vendors have been allowed way too much freedom in trying to turn hardware into a subscription. The yearly release of new phone models isn\u2019t helping either.\n \nreply",
      "What if we turned hardware support into a subscription (kind of like JetBrains model I think?) and stopped yearly releases in favor of more interesting releases? I wonder how many resources are used just to make the next iteration a bit shinier to catch the consumer's eye.\n \nreply",
      "I think what is this ignoring is that \"security updates\" are generally corrections to defects in the original product.In principle, a complete product would ship with no defects. You could run it for 1000 years unpatched and it would be no less secure than the day it shipped.Manufacturers ship security updates because the original product was defective. So it makes sense that they remain on the hook for security updates -- we paid them full price up front.\n \nreply",
      "I am extremely sympathetic to this view--but is it practical? Like, should Apple be forced to continue releasing security fixes for the original iPhone?\n \nreply",
      "Software copyright law should acquire a concept of defense:  if it's no longer profitable for you to maintain it, that should delimit the end of the copyright term, with a short grace period of (say) one year.\n \nreply",
      "A relatively small ongoing investment in a phone with which they earned billions of dollars in profit. Doesn't necessarily require new feature updates, but security updates should be available for a far more significant length of time than the single-digit years the have self-regulated themselves.  As an alternative, perhaps these companies should be held responsible for the e-waste of their prematurely expired hardware...\n \nreply",
      "> A relatively small ongoing investment in a phone with which they earned billions of dollars in profit.That's fair. But what about a product which doesn't turn a profit? The iPhone could have been a total flop, no one knew in advance!I worry that if releasing a hardware product carried an unlimited support burden, companies would release far fewer products. Less risk taking would lead to less innovation, and so on.I think I would be more on board with a rule like \"once you stop releasing security updates, you must share hardware documentation and unlock the bootloader\", so consumers can install their own (presumably patched) operating systems. But this wouldn't actually affect most of society, because 90% of consumers (I'm being generous) are never going to install Linux on their phones.\n \nreply",
      "Yes they should, they should also be forced to unlocked the bootloaders and release specs to the hardware so that 3rd part OSes can target the devices. Hardware recycling is a joke. I have first gen ipad that would make a great photoframe, video play and ebook reader but instead it is a fully functional paper weight.\n \nreply",
      "I'm reading this as \"Samsung charges a $10 monthly subscription fee to keep your phone up to date\" and I already know how that would turn out.\n \nreply"
    ],
    "link": "https://www.districtcon.org/junkyard",
    "first_paragraph": "Organized by DistrictCon - a new DC hacker con.\u2757 Most Impactful System \u2757\ud83e\udd21 Best Meme Target \ud83e\udd21 \ud83d\ude08 Most Innovative Exploitation Technique \ud83d\ude08Copyright 2024-2025 DistrictCon\u2122. All Rights Reserved.DistrictCon is organized by a volunteer team of hackers and security researchers dedicated to building community in the DC area. We're in the process of forming our own non-profit.In the meantime, DistrictCon is fiscally sponsored by The Hack Foundation (d.b.a. Hack Club), a 501(c)(3) nonprofit with the EIN 81-2908499.Wallpaper by creight from Wallpapers.com"
  },
  {
    "title": "Debugging Behind the Iron Curtain (2010) (jakepoz.com)",
    "points": 7,
    "submitter": "edward",
    "submit_time": "2024-09-18T23:42:11.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.jakepoz.com/debugging-behind-the-iron-curtain/",
    "first_paragraph": "I program my home computerbeam myself into the futureSergei is a veteran of the early days of the computing industry as it was developing in the Soviet Union. I had the pleasure of working and learning from him over the past year, and in that time I picked up more important lessons about both life and embedded programming than any amount of school could ever teach. The most striking lesson is the story of how and why, in late summer of 1986, Sergei decided to move his family out of the Soviet Union.In the 1980s, my mentor Sergei was writing software for an SM-1800, a Soviet clone of the PDP-11. The microcomputer was just installed at a railroad station near Sverdlovsk, a major shipping center for the U.S.S.R. at the time. The new system was designed to route train cars and cargo to their intended destinations, but there was a nasty bug that was causing random failures and crashes. The crashes would always occur once everyone had gone home for the night, but despite extensive investigat"
  },
  {
    "title": "Interning in Go (medium.com/google-cloud)",
    "points": 48,
    "submitter": "todsacerdoti",
    "submit_time": "2024-09-16T18:17:00.000000Z",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=41558883",
    "comments": [
      "Beware the trade-offs of interning affecting GC behavior. Now you can\u2019t have a stack-allocation optimization, for example.\n \nreply",
      "Interning is neat. Most of my experience is really dated. Primarily in the JVM, and mostly for class names, for reflection and class loaders. It's sort of surprising seeing this added to go, with its desires for minimalism. But when you can use it, it can be a big win.Look past the \"loading the whole book in memory\" the author gets to the point soon enough.The ip address example is ok. It's true, and highlights some important points. But keep in mind pointers are 64 bit. If you're not ipv6, and you're shuffling a lot of them, you're probably better off just keeping the uint64 and converting to string and allocating the struct as needed. interning doesn't appear to be much of a win in that narrow case. but if you do care about ipv6, and you're connecting to millions of upstreams, it's not unreasonable.It's neat it's available. it's good to be aware of interning, but it's generally not a huge win. For a few special cases, it can be really awesome.** edit\nuint32 for ipv4. bit counting is hard.\n \nreply",
      "This is new for Go? I remember learning about Java string interning decades ago in the context of xml parsers. If I remember correctly, there were even some memory leaks associated with it and thread locals?\n \nreply",
      "You could've implemented bespoke interning at any point in Go; it was added to the standard library only recently, though, likely because it may leverage Go's relatively recent support for generics.\n \nreply",
      "I missed the initial blogpost about this; thanks for the solid explanation and the links. Probably won't make much of a difference for my use cases but cool to know this is now in the stdlib.\n \nreply",
      "Is this essentially dictionary compression ?\n \nreply",
      "It's similar. It's just not by word, but by entire string, assuming that strings are immutable, long enough, and the same string values are referenced often enough.On 64-bit machines though strings shorter than 8 bytes are better off not interned.\n \nreply",
      "as I read it, it's any struct! Not just strings. which is cool.\n \nreply",
      "Any \"comparable\" struct, which is to say any struct where '==' works. No, you can't override '=='.This will not work for the average struct, and if you do use it, you now no longer can include any, for example `[]byte` fields in the struct or else it will no longer compile.I always find it funny that `string` is comparable, but `[]rune` and `[]byte` are not.\n \nreply",
      "No.  This is avoiding keeping multiple copies of the same value.  There's no compression.\n \nreply"
    ],
    "link": "https://medium.com/google-cloud/interning-in-go-4319ea635002",
    "first_paragraph": ""
  },
  {
    "title": "Bento: Jupyter Notebooks at Meta (fb.com)",
    "points": 164,
    "submitter": "Maro",
    "submit_time": "2024-09-18T14:30:21.000000Z",
    "num_comments": 76,
    "comments_url": "https://news.ycombinator.com/item?id=41580166",
    "comments": [
      "The internal tools at Meta are incredible tbh. There\u2019s an ecosystem of well-designed internal tools that talk to each other. That was my favorite part of working there.\n \nreply",
      "Polar opposite of my experience. To achieve the technical equivalent of changing a lightbulb, spend the entire day wrangling a dozen tools which are broken in different ways, maintained by teams that no longer exist or have completely rolled over, only to arrive at the finish line and discover we don't use those lightbulbs anymore. Move things and break fast.\n \nreply",
      "IMO there's a mix of a few really good, widely used, well-supported tools as well as a long tail of random tiny tools where the original team is gone that are cruftier.\n \nreply",
      "Mmm breakfast\n \nreply",
      "haha the reason I stayed as long as i did\n \nreply",
      "Yeah 100%. I found it immensely frustrating to be using tools with no community (except internally), so-so documentation, and features that were clearly broken in a way that would be unacceptable for a regular consumer product. If you have a question or error not covered by an internal search or documentation, good luck, you'll need it. Literally part of the reason I left the company.\n \nreply",
      "Well, you're supposed to read the code and figure it out. And if you can't, you're not good enough an engineer. According to people at Meta.\n \nreply",
      "People probably think you\u2019re exaggerating but it\u2019s true. Sometimes when I would get blocked the suggestion was to \u201cread the source code\u201d or \u201csubmit a fix\u201d on some far flung internal project. Huge fucking waste of time and effort, completely unserious.\n \nreply",
      "Doesn't sound like your type of company tbh, the flipside is that a \"serious\" company will often have broken bs too except now nobody is going to look at your contribution/fix.\n \nreply",
      "Same as Google. Many internal tools have painful interfaces and poor or documentation because the hiring bar was high and it was acceptable to assume that the user's skill level is high enough to figure it out. That attitude becomes a bigger problem when trying to sell tools to the public (e.g. Google Cloud Platform).\n \nreply"
    ],
    "link": "https://engineering.fb.com/2024/09/17/data-infrastructure/inside-bento-jupyter-notebooks-at-meta/",
    "first_paragraph": "This episode of the Meta Tech Podcast is all about Bento, Meta\u2019s internal distribution of Jupyter Notebooks, an open-source web-based computing platform. Bento allows our engineers to mix code, text, and multimedia in a single document and serves a wide range of use cases at Meta from prototyping to complex machine learning workflows.Pascal Hartig (@passy) is joined by Steve, whose team has built several features on top of Jupyter, including scheduled notebooks, sharing with colleagues, and running notebooks without a remote server component by leveraging WebAssembly in the browser.Download or listen to the podcast episode below:You can also find the episode wherever you get your podcasts, including:The\u00a0Meta Tech Podcast\u00a0is a podcast, brought to you by Meta, where we highlight the work Meta\u2019s engineers are doing at every level \u2013 from low-level frameworks to end-user features.Send us feedback on\u00a0Instagram,\u00a0Threads, or\u00a0X.And if you\u2019re interested in learning more about career opportunitie"
  },
  {
    "title": "Meticulous (YC S21) is hiring to eliminate UI tests",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-09-18T21:03:07.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "item?id=41585449",
    "first_paragraph": ""
  },
  {
    "title": "From Myth to Measurement: Rethinking US News and World Report College Rankings (anandsanwal.me)",
    "points": 20,
    "submitter": "chmaynard",
    "submit_time": "2024-09-18T22:19:05.000000Z",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=41586203",
    "comments": [
      "Here's an insider's insight: Colleges hate these rankings too, including the shit eating grin they have to plaster on their face when they go out in public and say things like \"We're one of the top 10 small public non-land grant research institutions in the upper middle pacific northwest region!\"But too many families have been convinced that rankings like this are useful so now the institutions have little choice but to become complicit in the ridiculous system or have their enrollment decimated and be unable to pay its bills.\n \nreply",
      "Modern US society seems to be layers upon layers of gamified results with little coherent vision.  We blame other countries for having \"stifling\" regulation, \"burdensome\" oversight and \"top-down\" planning, but honestly - it seems we simply let problems precipitate for decades on end and in many cases think it's some sort of achievement.\n \nreply",
      "[flagged]",
      ">The real problem with American universities is that in one generation they have gone from best in the world to completely lost and directionless, having jettisoned their entire curricula in favor of counterintuitive and vapid intellectual fashion, but enough about that, I'm sure you're thinking you need to know my pronounsLike? I'm sure you're deeply familiar with these fashions.And nobody asked for your pronouns, even if I cared we're on an internet forum.\n \nreply",
      "> And nobody asked for your pronouns, even if I cared we're on an internet forum.I know, I wasn't mocking you, I was mocking American universities which is where that idea came from\n \nreply",
      "In terms of daily usage it came first from LGBTQ+ communities. It then gained more mainstream attention when colleges and universities, having to work with the emerging generation where acceptance of different sexualities or identities was more common, began to accommodate this. That was helped along of course by the fact that people who systematically study the constantly changing landscape of language use skew towards researchers in linguistics through colleges and universities. So there were two \"vectors\" of transfer to that community: from both students and linguists.\n \nreply",
      "You have no idea, it probably came from tumblr. I'm familiar with those \"fashions\" to which you refer and not a single professor I know ever teaches about pronouns or anything else which you're probably alluding to.\n \nreply",
      "any time I get an email from a .edu, it announces pronouns. where it really drives me crazy is dating apps: i spelled out my my preferences, why are you telling me your pronouns? shouldn't I know them by now, unless I clicked \"takes all comers\" in which case, why would I care? :)\n \nreply",
      "It is frequently policy to do this in an email signature, with the desire to use non-traditional pronouns driven (from what I saw) initially by students themselves. I'm guessing dating apps probably have check boxes or something? If so then it could be necessary or useful if wanting to be exposed or find the broadest group of people that you might click with.",
      "College rankings are a bit like corruption rankings: they are measurements of perceptions. Not only that, perceptions are heavily influenced by previous rankings.If your ranking doesn't show Harvard and MIT near the top, it is wrong. If your corruption index doesn't show Scandinavia and Western Europe above Africa and South America, it is wrong.If it's wrong, people will not want to read it. If you do an \"objective\" ranking and it doesn't quite say what you expected, you need to weight things differently, so that your ranking has credibility.At best you can do a bit of massage to show that you are actually doing something when compiling these rankings, and you might be able to highlight a few trends. But in the end, if you are not showing the usual suspects in the usual places, people will not believe you.Reputation, at the end of the day, moves slowly.\n \nreply"
    ],
    "link": "https://anandsanwal.me/us-news-college-rankings-myth/",
    "first_paragraph": "\u00b7In the grand theater of American higher education, few performances are as anticipated\u2014or as controversial\u2014as the annual unveiling of the U.S. News & World Report college rankings.\u00a0Like a high-stakes beauty pageant for academia, these rankings have become a cultural touchstone, influencing the decisions of countless students, parents, and even the very institutions they aim to evaluate.\u00a0But beneath the glossy veneer of numerical precision lies a system rife with flaws, manipulations and unintended consequences.I\u2019ll propose a better way below but first, why are the US News Best Colleges rankings so useless?Peter Bernstein, a former managing editor at U.S. News, offers a candid glimpse behind the curtain. He describes the rankings\u2019 creation as \u201cfull of value judgments, full of problems, challenges, dangers.\u201d\u00a0This admission is telling.\u00a0What began as a \u201cpromotional gimmick\u201d in the 1980s has morphed into what Bernstein himself inadvertently dubbed a \u201cmythology\u201d rather than a methodology in"
  },
  {
    "title": "A high-performance, zero-overhead, extensible Python compiler using LLVM (github.com/exaloop)",
    "points": 146,
    "submitter": "wspeirs",
    "submit_time": "2024-09-18T14:44:00.000000Z",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=41580326",
    "comments": [
      "> Non-goals: Drop-in replacement for CPython: Codon is not a drop-in replacement for CPython. There are some aspects of Python that are not suitable for static compilation \u2014 we don't support these in Codon.This is targeting a Python subset, not Python itself.For example, something as simple as this will not compile, because lists cannot mix types in Codon (https://docs.exaloop.io/codon/language/collections#strong-ty...):    l = [1, 's']\n\nIt's confusing to call this a \"Python compiler\" when the constraints it imposes pretty fundamentally change the nature of the language.\n \nreply",
      "Who is out here mixing types in a list anyway?\n \nreply",
      "parsing json is roughly of the type:type Json = None | bool | float | str | dict[str, Json] | list[Json]you might have similar situations for configs e.g. float | str for time in seconds or a human readable time string like \"30s\" etc.given how fundamental such things are I'm not sure if there will be any larger projects (especially wrt. web servers and similar) which are compatible with thisalso many commonly used features for libraries/classes etc. are not very likely to work (but idk. for sure, they just are very dynamic in nature)so IMHO this seems to be more like a python-like language you can use for idk. some since computations and similar then a general purpose faster python\n \nreply",
      "Agreed, I was just joking. I understand heterogenous lists are possible with Python, but with the use of static type checking I feel like its pretty rare for me to have heterogenous lists unless its duck typing.\n \nreply",
      "It\u2019s common to have a list of objects with different types, but which implement the same interface. Duck typing of this kind is core to Python.\n \nreply",
      "Good point.\n \nreply",
      "The json module returns heterogenous dicts.https://docs.python.org/3/library/json.html\n \nreply",
      "Yeah, just because it can do that doesn't mean that it is good design.\n \nreply",
      "Someone who is using Python the wrong way.\n \nreply",
      "I often find myself mixing Nones into lists containing built-in types when the former would indicate some kind of error. I could wrap them all into a nullable-style type, but why shouldn't the interpreter implicitly handle that for me?\n \nreply"
    ],
    "link": "https://github.com/exaloop/codon",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A high-performance, zero-overhead, extensible Python compiler using LLVM\n      \n\nCodon is a high-performance Python implementation that compiles to native machine code without\nany runtime overhead. Typical speedups over vanilla Python are on the order of 10-100x or more, on\na single thread. Codon's performance is typically on par with (and sometimes better than) that of\nC/C++. Unlike Python, Codon supports native multithreading, which can lead to speedups many times\nhigher still.Think of Codon as Python reimagined for static, ahead-of-time compilation, built from the ground\nup with best possible performance in mind.\u274c Drop-in replacement for CPython: Codon is not a drop-in replacement for CPython. There are some\naspects of Python that are not suitable for static compilation \u2014 we don't support these in Codon.\nThere are ways to use Cod"
  },
  {
    "title": "LinkedIn is now using everyone's content to train their AI tool (twitter.com/racheltobac)",
    "points": 190,
    "submitter": "lopkeny12ko",
    "submit_time": "2024-09-18T19:37:28.000000Z",
    "num_comments": 89,
    "comments_url": "https://news.ycombinator.com/item?id=41584486",
    "comments": [
      "Somehow i feel sad for this AI model. All the others are trained on authentic content and this boy gets socialised on the most shallow content imaginable. Poor, socially awkward AI.\n \nreply",
      "Why on earth would you train AI on that? In the social media world it's already the closest thing resembling boring, unreadable machine generated content.No matter what you ask it, it'll brag about what a great job it's doing answering you, announce that it's having a baby, then tell everybody that it's being let go because there are better AI. It'll thank a few key people who it worked with, and tell you that it's actually thrilled with this opportunity to take a break from answering your question, and will spend more time on its old hobby of being an online resume.\n \nreply",
      "Because there is a huge market for resume builders and career guidance where AI can play a role. Using LinkedIn you can measure success and network performance and correlate that to the resume and posted content.\n \nreply",
      "I was going to say that it's the streetlight effect but this makes even more sense.\n \nreply",
      "LinkedIn, like GitHub and (to a degree) OpenAI are under Microsoft\u2019s umbrella.\n \nreply",
      "I'm guessing the real money linked in wants is in the hiring and firing, B2B. Now, every resume gets answered and your first interaction with a company is a poorly scripted AI who goes from manic enthusiasm to depressingly rote in the actual job requirements and probably will still ghost you and continue the imbalance of application effort vs employer response.The converse will be true, but the price of AI will just make poor people have to suffer even moreJust the long march of wealth inequality and it's time sucking capitalism.\n \nreply",
      "This seems to imply that machines would ghost humans to save on token fees. I wouldn't rule it out.\n \nreply",
      "That made me laugh. Thank you.\n \nreply",
      "The Corporate Memphis of AI models.https://en.m.wikipedia.org/wiki/Corporate_Memphis\n \nreply",
      "LinkedIn strikes me as the adult equivalent of self conscious school kids trying to hold a conversation among themselves, each self consciously trying to sound cool.\n \nreply"
    ],
    "link": "https://twitter.com/RachelTobac/status/1836471586624540705",
    "first_paragraph": ""
  },
  {
    "title": "A overview of binaries, ELF, and NoMMU on Linux (landley.net)",
    "points": 66,
    "submitter": "oliverkwebb",
    "submit_time": "2024-09-14T13:57:14.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "http://lists.landley.net/pipermail/toybox-landley.net/2024-January/029957.html",
    "first_paragraph": "\nNext message (by thread): [Toybox] Impact of global struct size\n\n Messages sorted by:\n[ date ]\n[ thread ]\n[ subject ]\n[ author ]\n\n\n\n\n\nOn 12/30/23 18:10, Ray Gardner wrote:\n> I am having a bit of trouble understanding the impact of globals.\n> \n> There are the probes GLOBALS and findglobals to see what the space usage\n> is for globals. The output of both show that \"this\" is up to 8232 bytes,\n> due to the \"ip\" toy using 8232 of global space.\n\nWhich is in pending for a reason.\n\n> The blog entry of 31 August 2023 ends with some discussion of which\n> commands take up the most global space. It says \"Everything \"tr\" and\n> earlier is reasonably sized, and \"ip\" and \"telnet\" are in pending.\"\n\nI sorted them by size and cut and pasted the end of the list, starting with \"tr\".\n\nCommands in pending haven't been (fully) cleaned up yet, so problems in them\naren't yet confirmed to be a design problem requiring heavy lifting. Most likely\nsomething easily fixable I just haven't done the janitorial work fo"
  },
  {
    "title": "Qwen2.5: A Party of Foundation Models (qwenlm.github.io)",
    "points": 114,
    "submitter": "apsec112",
    "submit_time": "2024-09-18T17:42:16.000000Z",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=41583062",
    "comments": [
      "Actually really impressive. They went up from 7T tokens to 18T tokens. Curious to see how they perform after finetuning.\n \nreply",
      "32B is a nice size for 2x 3090s. That comfortably fits on the GPU with minimal quantization and still leaves extra memory for the long context length.70B is just a littttle rough trying to run without offloading some layers to the CPU.\n \nreply",
      "70B+ models typically run great with my MacBook's 96GB of (V)RAM. I want a Mac Studio to run e.g. llama-405B, but I can't justify the marginal model quality ROI for like $7k or whatever. (But I waaant iiit!)\n \nreply",
      "> run greatHow many tokens/second is that approx?For reference, Qwen 2.5 32B on CPU (5950X) with GPU offloading (to RTX 3090ti) gets about 8.5 token/s, while 14B (fully on GPU) gets about ~64 tokens/s.\n \nreply",
      "You can get refurbished Mac Studio M1 Ultra with 128GB VRAM for ~ $3k on ebay. M1 ultra has 800GB/s memory bandwidth, same as the M2 ultra.Not sure if 128GB VRAM is enough for running 405b (maybe at 3-bit quant?), but it seems to offer great value for running 70B models at 8-bit.\n \nreply",
      "what quant are you running for that rig? i've been running q4, not sure if I can bump that up to q5 across the board (or if it's worth it in general)\n \nreply",
      "Probably an ignorant question, but could someone explain why the Context Length is much larger than the Generation Length?\n \nreply",
      "When doing inference for an LLM, there are two stages.The first phase is referred to as \"prefill\", where the input is processed to create the KV Cache.After that phase, the \"decode\" phase is called auto-regressively. Each decode yields one new token.This post on [Inference Memory Requirements](https://huggingface.co/blog/llama31#inference-memory-require...) is quite good.These two phases have pretty different performance characteristics - prefill can really maximize GPU memory. For long contexts, its can be nigh impossible to do it all in a single pass - frameworks like vLLM use a technique called \"chunked prefill\".The decode phase is compute intensive, but tends not to maximize GPU memory.If you are serving these models, you really want to be able to have larger batch sizes during inference, which can only really come with scale - for a smaller app, you won't want to make the user wait that long.So, long contexts only have to be processed _once_ per inference, which is basically a scheduling problem.But the number of decode passes scales linearly with the output length. If it was unlimited, you could get some requests just _always_ present in an inference batch, reducing throughput for everyone.\n \nreply",
      "It is also a training issue. The model has to be trained to reinforce longer outputs, which has a quadratic train-time cost and requires suitable long-context response training data.\n \nreply",
      "That's a great explanation, thank you!\n \nreply"
    ],
    "link": "https://qwenlm.github.io/blog/qwen2.5/",
    "first_paragraph": "GITHUB\nHUGGING FACE\nMODELSCOPE\nDEMO\nDISCORDIn the past three months since Qwen2\u2019s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5.\nWe are announcing what might be the largest opensource release in history! Let\u2019s get the party started!Our latest release features the LLMs Qwen2.5, along with specialized models for coding, Qwen2.5-Coder, and mathematics, Qwen2.5-Math. All open-weight models are dense, decoder-only language models, available in various sizes, including:All our open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories. In addition to these models, we offer APIs for our flagship language models: Qwen2.5-Plus and Qwen2.5-Turbo th"
  },
  {
    "title": "OpenTelemetry and vendor neutrality: how to build an observability strategy (grafana.com)",
    "points": 52,
    "submitter": "meysamazad",
    "submit_time": "2024-09-15T14:05:03.000000Z",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=41547596",
    "comments": [
      "I can confirm that this is a pretty good way. Building out a basic distributed tracing solution with OTEL, jaeger and the relevant Spring Boot configuration and dependencies was quite a pleasant experience once you figure out the relevant-for-your-use-cases set of dependencies. It's one of those nice things that Just Works\u2122, at least for Java 17 and 21 and Spring Boot 3.2 (iirc) or greater.There appeared to be wide array of library and framework support across various stacks, but I can only attest personally to the quality of the above setup (Java, Boot, etc).\n \nreply",
      "> once you figure out the relevant-for-your-use-cases set of dependencies>  It's one of those nice things that Just Works\u2122did it Just Work\u2122 or did you have to do work to make it Just Work\u2122?\n \nreply",
      "Java has really good OTel coverage across tons of libraries. It should mostly Just Work\u2122, though you'll still need to consider sampling strategies, what metrics you actually want to collect, etc.Would say .NET isn't too far behind. Especially since there are built-in observability primitives and Microsoft is big on OTel. ASP.NET Core and other first party libraries already emit OTel compliant metrics and traces out of the box. Instrumenting an application is pretty straightforward.I have less experience with the other languages. Can say there is plenty of opportunity to contribute upstream in a meaningful way. The OpenTelemetry SIGs are very welcoming and all the meetings are open.Full disclosure: work at Grafana Labs on OpenTelemetry instrumentation\n \nreply",
      "In my experience, it just worked -- I was at an org that ran 3rd party java services alongside our normal array of microservices (that all used our internal instrumentation library that wrapped OTEL) and using the OTEL autoinstrumentation for those 3rd party services was pretty trivial to get setup and running (just wrap the command to run the app with the OTEL wrapper, hand it a collector url.) Granted -- we already had invested in OTEL elsewhere and were familiar with many of the situations in which it didn't just work.\n \nreply",
      "I had a very similar experience with a Quarks REST API where it's supported very well out of the box, we just had to point it to the appropriate otel collector endpoint and traces are created or propagated automatically.\n \nreply",
      "Looking for target for your OTEL data checkout Coroot too  - https://coroot.com/  Additionally to OTEL visualization it can use eBPF to generate traces for applications where OpenTelemetry installation can't be done.\n \nreply",
      "For anyone that has built more complex collector pipelines, I'm curious to know the tech stack:  - otel collector?\n  - kafka (or other mq)?\n  - cribl?\n  - vector?\n  - other?\n \nreply",
      "What sort of complexity do you need? I've used them on my previous job and am implementing it on the current one. I have never heard of the last three you mention.Otel collector is very useful for gathering multiple different sources, eg I am at a big corporation and we both have department level Grafana stack (Prometheus Loki etc) and we need to also send the data to Dynatrace. With otel collector these things are a minor configuration away.For Kafka if you mean tracing through Kafka messages previously we did it by propagating it in message headers. Done at a shared team level library the effort was minimal.\n \nreply",
      "This is hypocritical content marketing from a company that doesn't want you to be vendor neutral.  As seen by the laughable use of hyperlinks to their products but no links when mentioning Prometheus or elasticsearch.OTEL is great, I just wish the CNCF had better alternatives to Grafana labs.\n \nreply",
      "Check out Perses: https://github.com/perses/persesLess mature than Grafana but recently accepted by the CNCF as a sandbox project, hopefully a positive leading indicator of success.\n \nreply"
    ],
    "link": "https://grafana.com/blog/2024/09/12/opentelemetry-and-vendor-neutrality-how-to-build-an-observability-strategy-with-maximum-flexibility/",
    "first_paragraph": "AllProductsCore LGTM StackLogspowered by Grafana LokiGrafanafor visualizationTracespowered by Grafana TempoMetricspowered by Grafana Mimir and Prometheusextend observabilityPerformance & load testingpowered by Grafana k6Continuous profilingpowered by Grafana PyroscopePluginsConnect Grafana to data sources, apps, and moreend-to-end solutionsApplication ObservabilityMonitor application performanceFrontend ObservabilityGain real user monitoring insightsIncident Response & Managementwith Grafana Alerting, Grafana Incident, Grafana OnCall, and Grafana SLOSynthetic MonitoringPowered by Grafana k6Deploy The StackGrafana CloudFully managedGrafana EnterpriseSelf-managedPricingHint: It starts at FREEAllOpen SourceGrafana LokiMulti-tenant log aggregation systemGrafanaQuery, visualize, and alert on dataGrafana TempoHigh-scale distributed tracing backendGrafana MimirScalable and performant metrics backendGrafana PyroscopeScalable continuous profiling backendGrafana BeylaeBPF auto-instrumentationGra"
  }
]