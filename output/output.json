[
  {
    "title": "The Storm Hits the Art Market (artnet.com)",
    "points": 60,
    "submitter": "onecommentman",
    "submit_time": "2025-09-08T23:48:14 1757375294",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=45175628",
    "comments": [
      "This is really the art as speculation fad coming home to roost. The people actually interested in art as art are now on things like Instagram buying direct from the artists with very little gatekeeping either by art schools or galleries.It would be tempting to see if there is a correlation in when the \u201clegit\u201d art world price going off a cliff and the NFT bubble, because that suckered in a lot of the idiot end of the speculator crowd.reply",
      "NFTs were about transparency in provenance, using public, tamper-proof records, that replace jurisdictions/authorities with the type of math that  enabled cross-border digital payments. Sure there was a spin-off scam industry but there remains a kernel of legitimacy in the concept.reply",
      "I agree - a lot of art is sold via artists having a relationship with a stable of a few hundred people via insta. Completely bypasses the galleries and is far more compelling for the buyer because they can develop an emotional connection with the creator. Galleries can\u2019t do that without having the artist present and even then they\u2019re overwhelmed and buyers can\u2019t make a connection. So this isn\u2019t necessarily an art thing, it\u2019s a gallery thing. Disintermediation strikes again.reply",
      "It's speculation, but tax avoidance too:\nhttps://naturalist.gallery/blogs/journal/understanding-the-f...reply",
      "And money laundering\u2026 don\u2019t forget the money launderingreply",
      "How much of this is due to anti-money laundering regulations enacted in the early 2020s [0], which much better vet the identities of the buyers/sellers?[0] https://www.lseg.com/en/risk-intelligence/financial-crime-ri...reply",
      "The vanity of owning rare things is all too much... NFTs are really the culmination of the phenomenon. I'm all for owning beautiful things and paying if it's something handmade. But once it becomes such an extreme out of wack status symbol kept alive by a global elite it's just feeding the monster. The worst is when it's an expensive status symbol and still mass produced I supposereply",
      "I collect rare books (and memorabilia related to them), and it is not about vanity at all. I consider them historical objects (in their niche, of course) in the sense they changed the industry, or affected a portion of society somehow. I consider myself a keeper and guardian of them.reply",
      "When you're rich enough to live in what's effectively a post-scarcity world, scarcity itself becomes a valuable commodity. After all, we are the product of natural selection, a process shaped by scarcity. It is therefore such a fundamental part of our lives that its absence drives some to artificially recreate it.reply",
      "I've purchased plenty of art over the past couple years, mostly original paintings with a few prints. Very few of those came from galleries and when I walked in the galleries, I already knew what I wanted to buy.Etsy and reddit are easy ways to find rather good art for what I can tell is a consistent price based on the size of the piece.The only time I 'browse' art offline is during special events, which have all been wander from house to house or small art studio to studio. I love the experience, it's much more personal as you know the artist will be there and you can discuss the piece with them.reply"
    ],
    "link": "https://news.artnet.com/market/intelligence-report-storm-2025-2684512",
    "first_paragraph": ""
  },
  {
    "title": "Signal Secure Backups (signal.org)",
    "points": 596,
    "submitter": "keyboardJones",
    "submit_time": "2025-09-08T16:43:39 1757349819",
    "num_comments": 296,
    "comments_url": "https://news.ycombinator.com/item?id=45170515",
    "comments": [
      "> alongside features that let you transfer your encrypted message history between Android, iOS, and Desktop devices.That's actually the feature I've been looking forward to. As I moved vom Android to iOS, I lost _all_ message histories from all messenger apps that use E2EE (Signal, WhatsApp, Threema, etc). The only one that \"just worked\" was Telegram due to not being encrypted. WhatsApp had a migration app that has to be done when setting up the iPhone, but it failed due to some bug. Signal had backups, but they didn't seem to be compatible between different OS versions.reply",
      "You already can, if you at least set up desktop, you can transfer also message history, though you won't have your media older than 45 days. Maybe it can work as a stopgap before they roll out encrypeted backups everywherereply",
      "That's a weird and crappy arbitrary limitation when I could move an arbitrary amount of data between the two devices otherwise. It's the worst part of Signal.reply",
      "doesn't signal also have a transfer to other device flow now?reply",
      "They have it between two Android phones next to each other for years, but probably not Android to iOSreply",
      "This looks brilliant. I just hope they make it easy to do test restores. In particular, I want to test restore without perturbing my main device. Let me restore using the secret key on a new device.When I install Signal on a computer it won't show me message history. Will backups allow me to view _all_ my message history on a computer? A big screen is very helpful for browsing lots of messages.reply",
      "Hi there, Signal dev here. You can sort of do this! You can restore on your new device, and while you will be unregistered on your old device, all of the data is still there. So if you see that something is amiss on the new device, you could re-register on your old device and you'd be right back where you started. This is actually one of the ways we test the feature with our own personal data.reply",
      "Hey, i have a related question about this:I have an old iPhone that has all my old Signal messages still on it that I wasnt able to move with me when I switched to Android. Is there any way that I can use these new tools to move the old conversations on my iPhone over to my android phone without losing all the new messages that are on my android now?That is, I want to merge the two histories.reply",
      "Using the new backup feature that we're discussing here (once it is available on iOS), you will probably need to transfer your old iPhone's data to an Android device first (either a secondary one or your current one, provided you have backed up its data to a backup file). Then follow https://news.ycombinator.com/item?id=45174779 .reply",
      "Multi-device would be a nice feature.And question: Will a backup taken today on Androis be able to be restored on iOS once released?reply"
    ],
    "link": "https://signal.org/blog/introducing-secure-backups/",
    "first_paragraph": ""
  },
  {
    "title": "Chat Control Must Be Stopped (privacyguides.org)",
    "points": 558,
    "submitter": "Improvement",
    "submit_time": "2025-09-08T20:07:22 1757362042",
    "num_comments": 177,
    "comments_url": "https://news.ycombinator.com/item?id=45173277",
    "comments": [
      "> Chat Control would make it mandatory for all service providers (text messaging, email, social media, cloud storage, hosting services, etc.) to scan all communications and all files (including end-to-end encrypted ones), in order to supposedly detect whatever the government deems \"abusive material.\"I wonder why there has been such silence on this, with the exception of a handful of well written blog posts. The scope of such a dragnet, the economic impact, the societal damage, all seems rather broad. Yet why don't any major operators in the EU take a stance? Is it really so below the radar, or being kept so below the radar?Just the network egress costs to whatever state sanctioned scanner gets built will in aggregate probably exceed a few hundred MEUR yearly.reply",
      "> I wonder why there has been such silence on thisYes, I would think that if there were any real journalism left, they would be all over this. For the sake of their profession, and the protection of their sources.reply",
      "Cory Doctorow points out a lot of things: https://pluralistic.net/But I don't think mainstream journalism points out computer nonsense because they're so intertwined with it all.I mean, \"we have a surveillance state\" first points to \"advertising\" which is their revenue stream.reply",
      "Quite the leap IMO, I actually think the strongest defense of the status quo is pointing out how much worse things could bereply",
      "Maybe it's safer not to say anything.reply",
      "Safer for whom?reply",
      "The news outletreply",
      "Having the service provider handle the encryption is very convenient for the users.  And, it turns out, the government.reply",
      "Sure, but the way this was written it also includes everything from Gmail to root access servers hosted by Hetzner. Gmail has been doing this for years, but (I assume) not Hetzner. If even hosting providers are dragged into this the scale grows dramatically. Can Hetzner really not even be bothered about having to comply with such ridiculous requirements?To give a simple example: imagine a script that constantly dumps /dev/urandom into JPG-like files nonstop onto a 16 TB disk, then repeats. I've seen enterprise systems that aren't so dissimilar. If indeed the EU commission wants all files scanned, then will Hetzner need to spy on all of their machines at least enough to check for compliance? I'm guessing their board members think it can't possibly be so dumb, or stand to gain handsomely and privately.reply",
      "Among many other reasons: because the proponents are using the usual \"think of the children\" tactics to impugn and libel the opposition.reply"
    ],
    "link": "https://www.privacyguides.org/articles/2025/09/08/chat-control-must-be-stopped/",
    "first_paragraph": "Illustration: Em / Privacy Guides | Photo: Ramaz Bluashvili / PexelsIf you've heard of Chat Control already, bad news: it's back. If you haven't, this is a pressing issue you should urgently learn more about if you value privacy, democracy, and human rights. This is happening this week, and we must act to stop it right now.Take a minute to visualize this: Every morning you wake up with a police officer entering your home to inspect it, and staying with you all day long.The agent checks your bathroom, your medicine cabinet, your bedroom, your closets, your drawers, your fridge, and takes photos and notes to document everything. Then, this report is uploaded to the police's cloud. It's \"for a good cause\" you know, it's to make sure you aren't hiding any child sexual abuse material under your bed.Every morning. Even if you're naked in bed. Even while you're having a call with your doctor or your lover. Even when you're on a date. Even while you're working and discussing your client's conf"
  },
  {
    "title": "Is OOXML Artifically Complex? (hsu.cy)",
    "points": 56,
    "submitter": "firexcy",
    "submit_time": "2025-09-05T23:07:00 1757113620",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=45144758",
    "comments": [
      "I love this screen that shows you exactly why they named it \u201cOffice Open\u201d XML: https://i.imgur.com/hnj3sdv.pngIt was a pretty big deal when OpenOffice.org's 2.0 release came with OpenDocument as the default file format. Very easy for someone to misread this MSOffice screen and click on OOXML expecting it to mean OO.o.reply",
      "https://m.slashdot.org/story/78708 (2007)reply",
      "It's as complex as it needs to be to losslessly convert old binary office files.A better format would have made us geeks a lot happier, but the average user just wants things to work the way they always have.reply",
      "Microsoft Office has many features. Each feature must be reflected in the file format somehow.(I wonder what the specification-pages-to-man-years ratio is...)reply",
      "I once digged through the 5000 page specification. There was a lot of useless stuff that only old Microsoft Word supported like WordArt items.reply",
      "Dead on.Microsoft is just dominant and exporting its 40 year old legacy codebase as a spec. LibreOffice team is frustrated that the for-profit model is beating the OSS model and crying foul over mostly necessary complexity. If LibreOffice started from scratch they\u2019d probably appreciate how much Microsoft serializes because a sufficiently complicated document saved to .docx basically provides a reference implementation.We do need for-profit alternatives to Word, and I\u2019m working on one in legal.[edit: I hope to put some real thoughts on this down soon, but most of the wonkiness emanates from evolving functionality and varying trends in best practices over the decades. I\u2019ve implemented a fair bit of the spec here: https://tritium.legal, but most of the hard part is providing for bidi language support, fonts, real-time editing and re-rendering, UI and annotations like spellchecking and grammar, not conforming to the markup spec. Spec conformance is just polish and testing. A performant modern word processor of any spec, however, is a technological achievement on the order of a web browser.]reply",
      "LibreOffice has versions that you pay for, with support. The most prominent is Collabora, which is a (if not the) biggest contributor to LibreOffice.reply",
      "Where does the article say it\u2019s a necessary complexity?> Thus, the primary goal for this new format wasn\u2019t to be elegant, universal, or easy to implement; it was to placate regulators while preserving Microsoft\u2019s technological and commercial advantages.That sounds quite anti-competitive to mereply",
      "Worth keeping in mind that the native MSO formats were using \"structured storage\", a horrible binary chunked serialization and metadata format from an era where binary embedding of document streams in other application documents via \"Object linking and embedding\" (OLE, see also Apple's OpenDoc format) was deemed desirable, with zero consideration given to third-party apps and segment formats tied to C++ data structures. Compared to that, OOXML is still a huge progress, and while it's complex I wouldn't say it's maliciously so.The Shakespeare example is a good one where the sentence is split into multiple spans to apply style rules yet the bare text content could be extracted by just removing all XML tags. Whereas the ODF variant is actually less recommendable as it relies on an unneccesarily complex formatting and text addressing language on top of XML.The article says> Even at a glance [ODF's markup] is more intelligible. Strip the text: namespaces and it\u2019s nearly valid HTML.\nThe only thing that needs explaining is that ODF doesn\u2019t wrap To be with a dedicated \u201cbold\u201d tag. Instead, it applies an auto-style named T1 to a <text:span>, an act of separating content and presentation that mirrors established web practices.but this definitely makes things more complex for data exchange compared to OOXML.reply",
      "> In my view, OOXML is indeed complex, convoluted, and obscure. But that\u2019s likely less about a plot to block third-party compatibility and more about a self-interested negligence: Microsoft prioritized the convenience of its own implementation and neglected the qualities of clarity, simplicity, and universality that a general-purpose standard should have.The author only provides arguments for \"self-interested negligence\". He provides no counterarguments to the claim that OOXML complexity was \"a plot to block third-party compatibility\". Therefore, he cannot compare \"negligence\" and \"a plot\". Therefore, his claim that \"negligence\" is a better explanation for OOXML complexity than \"a plot\" cannot follow.To restate:> If we dig into the context of OOXML\u2019s creation, it can be argued that harming competitors was not Microsoft\u2019s primary aim.The author provides no evidence to support this claim. At most, the evidence provided in this section at most supports the claim that \"negligence\" played a role in OOXML complexity. From this evidence alone, no conclusions can be drawn about the \"primariness\" of \"negligence\" vs \"harming competitors\".reply"
    ],
    "link": "https://hsu.cy/2025/09/is-ooxml-artificially-complex/",
    "first_paragraph": "A while ago, the official blog of LibreOffice published a provocative article: \u201cAn artificially complex XML schema as a lock-in tool.\u201d Its target is Microsoft\u2019s XML-based file formats \u2014 the Office Open XML (OOXML).The article alleges that, although Microsoft put its Office formats through standardization, the spec is engineered to be so complex that it obstructs interoperability with third-party software.\nMoreover, the complexity is allegedly gratuitous and disconnected from real-world needs; it\u2019s like advertising an \u201copen\u201d railway system while designing the signaling so only one manufacturer can run trains.\nUsers, the argument continues, often accept proprietary technology uncritically, which makes it easy for Microsoft to lock people into its ecosystem.A quick refresher: historically, Office used binary formats (.doc, .xls, and .ppt) whose contents weren\u2019t human-readable.\nStarting with Office 2007, Microsoft switched the defaults to .docx, .xlsx, and .pptx, where the \u201cx\u201d stands for X"
  },
  {
    "title": "NPM debug and chalk packages compromised (aikido.dev)",
    "points": 940,
    "submitter": "universesquid",
    "submit_time": "2025-09-08T15:37:57 1757345877",
    "num_comments": 502,
    "comments_url": "https://news.ycombinator.com/item?id=45169657",
    "comments": [
      "Hi, yep I got pwned. Sorry everyone, very embarrassing.More info:- https://github.com/chalk/chalk/issues/656- https://github.com/debug-js/debug/issues/1005#issuecomment-3...Affected packages (at least the ones I know of):- ansi-styles@6.2.2- debug@4.4.2 (appears to have been yanked as of 8 Sep 18:09 CEST)- chalk@5.6.1- supports-color@10.2.1- strip-ansi@7.1.1- ansi-regex@6.2.1- wrap-ansi@9.0.1- color-convert@3.1.1- color-name@2.0.1- is-arrayish@0.3.3- slice-ansi@7.1.1- color@5.0.1- color-string@2.1.1- simple-swizzle@0.2.3- supports-hyperlinks@4.1.1- has-ansi@6.0.1- chalk-template@1.1.1- backslash@0.2.1It looks and feels a bit like a targeted attack.Will try to keep this comment updated as long as I can before the edit expires.---Chalk has been published over. The others remain compromised (8 Sep 17:50 CEST).NPM has yet to get back to me. My NPM account is entirely unreachable; forgot password system does not work. I have no recourse right now but to wait.Email came from support at npmjs dot help.Looked legitimate at first glance. Not making excuses, just had a long week and a panicky morning and was just trying to knock something off my list of to-dos. Made the mistake of clicking the link instead of going directly to the site like I normally would (since I was mobile).Just NPM is affected. Updates to be posted to the `/debug-js` link above.Again, I'm so sorry.reply",
      "Hey, you're doing an exemplary response, transparent and fast, in what must be a very stressful situation!I figure you aren't about to get fooled by phishing anytime soon, but based on some of your remarks and remarks of others, a PSA:TRUSTING YOUR OWN SENSES to \"check\" that a domain is right, or an email is right, or the wording has some urgency or whatever is BOUND TO FAIL often enough.I don't understand how most of the anti-phishing advice focuses on that, it's useless to borderline counter-productive.What really helps against phishing :1. NEVER EVER login from an email link. EVER. There are enough legit and phishing emails asking you to do this that it's basically impossible to tell one from the other. The only way to win is to not try.2. U2F/Webauthn key as second factor is phishing-proof. TOTP is not.That is all there is. Any other method, any other \"indicator\" helps but is error-prone, which means someone somewhere will get phished eventually. Particularly if stressed, tired, or in a hurry. It just happened to be you this time.Good luck and well done again on the response!reply",
      "Or you know, get a password manager like the rest of us. If your password manager doesn't show the usual autofill, since the domain is different than it should, take a step back and validate everything before moving on.Have the TOTP in the same/another password manager (after considering the tradeoffs) and that can also not be entered unless the domain is right :)reply",
      "I mostly agree and I do use one.You only need read the whole thread however to see reasons why this would sometimes not be enough: sometimes the password manager does not auto-fill, so the user can think it's one of those cases, or they're on mobile and they don't have the extension there, or...As a matter of fact, he does use one, that didn't save him, see: https://news.ycombinator.com/item?id=45175125reply",
      "> sometimes the password manager does not auto-fillSo pick one that does? That's like its top 2 feature> he does use oneHe doesn't since he has no autofill installed, so loses the key security+ convenience benefit of automatchreply",
      "The fact that NPMs entire ecosystem relies on this not happening regularly is very scary.I\u2019m extremely security conscious and that phishing email could have easily gotten me. All it takes is one slip up. Tired, stressed, distracted. Bokm, compromisedreply",
      "From sindresorhus:You can run the following to check if you have the malware in your dependency tree:`rg -u --max-columns=80 _0x112fa8`Requires ripgrep:`brew install rg`https://github.com/chalk/chalk/issues/656#issuecomment-32668...reply",
      "Sorry, I am unfamiliar with ripgrep.  Is this simply scanning for the string `_0x112fa8`?  Could we do the same thing with normal grep -r?reply",
      "yes. ripgrep just does it faster, is all.reply",
      "But also respects .gitignore by default so I\u2019m not sure you want to use ripgrep to scan your node_modulesreply"
    ],
    "link": "https://www.aikido.dev/blog/npm-debug-and-chalk-packages-compromised",
    "first_paragraph": "Starting at September 8th, 13:16 UTC, our Aikido intel feed alerted us to a series packages being pushed to npm, which appeared to contains malicious code. These were 18 very popular packages,All together, these packages have more than 2 billion downloads per week. The packages were updated to contain a piece of code that would be executed on the client of a website, which silently intercepts crypto and web3 activity in the browser, manipulates wallet interactions, and rewrites payment destinations so that funds and approvals are redirected to attacker-controlled accounts without any obvious signs to the user.To avoid being compromised by packages like this, check out Aikido safe-chain!The above packages all started having new versions released, an example here being is-arrayish:We can see that the index.js file is modified, and contains obfuscated code:\u200dAfter applying a bit of deobfuscation to it, we get a fairly complex piece of code still:\u200dThis malware is essentially a browser-based"
  },
  {
    "title": "Contracts for C (Early Stages) (gustedt.wordpress.com)",
    "points": 9,
    "submitter": "joexbayer",
    "submit_time": "2025-09-05T06:36:49 1757054209",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://gustedt.wordpress.com/2025/03/10/contracts-for-c/",
    "first_paragraph": "programming in modern CC++ seems to finally converge with their contracts proposal, https://wg21.link/p2900. I decided to give it a try and come up with ideas how such a thing would look for C. This is in early stages, not a full proposal yet, and still would need implementation by some of the major compilers.In particular, the C++ feature is full of sidetracks that I don\u2019t like at all, such as user-defined global handlers and ignorability. But there is a core of ideas, syntax and semantics that I found interesting and that I think should be considered for C. The principal features areTo see how all of this works we first need understand two fundamental primitives, assertions and assumptions. Their syntax looks something likeHere, in both cases COND is some condition that is supposed to be fulfilled. The first, contract_assert, is quite similar to a feature that we already have in C namely the assert macro. Both test for the condition; if it is fulfilled nothing happens and execution g"
  },
  {
    "title": "Experimenting with Local LLMs on macOS (6nok.org)",
    "points": 249,
    "submitter": "frontsideair",
    "submit_time": "2025-09-08T14:43:17 1757342597",
    "num_comments": 167,
    "comments_url": "https://news.ycombinator.com/item?id=45168953",
    "comments": [
      "I agree that it's kind of magical that you can download a ~10GB file and suddenly your laptop is running something that can summarize text, answer questions and even reason a bit.The trick is balancing model size vs RAM: 12B\u201320B is about the upper limit for a 16GB machine without it choking.What I find interesting is that these models don't actually hit Apple's Neural Engine, they run on the GPU via Metal. Core ML isn't great for custom runtimes and Apple hasn't given low-level developer access to the ANE afaik. And then there is memory bandwidth and dedicated SRAM issues. Hopefully Apple optimizes Core ML to map transformer workloads to the ANE.reply",
      "I feel like Apple needs a new CEO, I've felt this way for a long time. If I had been in charge of Apple I would have embraced local LLMs and built an inference engine that optimizes models that are designed for Nvidia, I also would have probably toyed around with the idea of selling server-grade Apple Silicon processors and opening up the GPU spec so people can build against it. Seems like Apple tries to play it too safe. While Tim Cook is good as a COO, he's still running Apple as a COO. They need a man of vision, not a COO at the helm.reply",
      "I think if Cook had vision, he could have started something called Apple Enterprise and sold Apple Silicon as a server and made AI chips. I agree he\u2019s too conservative and has no product vision. Great manager though.reply",
      "I was pleasantly surprised Apple Silicon came out at all. Someone has their eye on long term vision at Apple at least, they just didn't do this on a whim.reply",
      "They did have Xserve back in the day. As great as Apple silicon is for running local llms along with being a general-purpose computing device, it\u2019s not clear that Apple silicon have enough of a differentiating advantage over a rack of nvidia gpus to make it worthwhile in enterprise\u2026reply",
      "Strange to be saying this about Apple products, but its advantage is that it's way, way cheaper.reply",
      "He's got people buying 800$+ phones and 100$+ earbuds when alternatives that do the same thing for a fraction of the cost exist and are readily available.He's a capitalist genius. Every intelligent person predicted that iPhones would not be able to maintain their profit margins - they vastly underestimated the power of appealing to 80 IQ.His vision is products for people with 80 IQ that they love. Everything you mentioned requires above 80 IQ and is thus not a market Apple is interested in.Look at Vision Pro - you need above 80 IQ - it's a flop. When they come out with glasses that are as useless and approachable as Apple Watch, aka that 80 IQ people love - it'll be a giant success.Look at iPad - it's a computer for 80 IQ, people love it.That's Apple's vision and they're sticking to it.reply",
      "Anyone who doesn\u2019t happen to do exactly what I do and have the same interests as me is \u201880 IQ\u2019 \u2014 whatever that means. Got it.reply",
      "Oh look, it's a poor, green-text Google apologist who thinks phones with preinstalled crapware, an energy management model that doesn't stop any app from saturating your bandwidth, CPU or battery draw, and a security model that ensures you stand a good chance of becoming part of a crypto farm or botnet just because you downloaded an emulator from a third-party app store, means you have above an 80 IQ! LOL, way to virtue-signal your poverty, bro. These are tough times, I get it... But the first 2 Android phones I ever tried, I crashed within 5 minutes just by... get this... turning on their fucking Bluetooth. WHAT QUALITY. More like \"what Chinese shovelware,\" amirite?(How does it feel? Literally turning around your inane opinion back onto you.)reply",
      "By calling everyone who buys Apple products 80 IQ, you are lowering the quality of the discourse here. Please don't do that.reply"
    ],
    "link": "https://blog.6nok.org/experimenting-with-local-llms-on-macos/",
    "first_paragraph": "September 08, 2025\u00a0\u00b7\u00a09 minutes to readSo, this blog post will be about LLMs, and everyone has opinions about that. To be upfront about it, I\u2019m a skeptic (bordering on hater), yet I like experimenting with stuff so I download and run them locally on my Mac. And I\u2019ll teach you how to do it too, if you\u2019d like!\n\n\n\n\nSome call them fancy autocomplete, some argue that they are sentient and should have rights. The truth is somewhere in between. Yes, they perform next word prediction, but it\u2019s so complex that there\u2019s nontrivial emergent behavior. No, they don\u2019t have creativity or a mind. I believe one day we can create sentient machines, but not in this current iteration, maybe not before we go extinct.Now that we\u2019re out of the science fiction territory, let\u2019s talk about their strengths. Laurie has a great post about it, which I highly recommend, but in summary they are generally good at summarizing text, regurgitating home maintenance advice from reddit, or telling you that you have cancer.I a"
  },
  {
    "title": "Liquid Glass in the Browser: Refraction with CSS and SVG (kube.io)",
    "points": 45,
    "submitter": "Sateeshm",
    "submit_time": "2025-09-08T21:32:49 1757367169",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=45174297",
    "comments": [
      "Impressive but also impressive in that scrolling down through the examples makes my fully-loaded M4-Max Macbook Pro judder. I hate to imaging the performance of a full UI leveraging this stuff. Apple can do it in the UI because they can optimize the hell out of it.reply",
      "Haha, I\u2019m the author of the post.I planned to fix the performance issues before posting here (since I knew HN would be quick to point that out), but somebody posted it first. You\u2019re absolutely right \u2014 it\u2019s pretty slow right now and needs optimization.And it\u2019s not just the refraction/displacement map: plenty of other parts, like visualisations, aren\u2019t optimized yet either.reply",
      "Yeah this site does not scroll like butter as it were.But I don\u2019t think css can leverage the gpu in most (any?) cases. Apple has almost certainly baked something into the silicon to help handle the ui.reply",
      "Same, very laggy on my machine. The spectacular border effects also didn't work for me.reply",
      "I forked a JS library for liquid-glass and patched it up with some positioning fixes. It's fun to use in presentations.See https://github.com/nkzw-tech/liquid-glassreply",
      "I made something similar to this with WebGL shaders (the benefit being it works across browsers): https://real-glass.vercel.app - The tricky thing for me was making it refract real HTML elements behindreply",
      "Very nice, I really like the vector animations :)One thing I'd say is to apply some anti-aliasing (MSAA, SMAA?)\u2014even on a 4K display with a pixel density of 64.3 px/cm, the jaggies are visible, especially because of the extreme contrast of the caustics behind the dark background.reply",
      "I'm not especially familiar with this, but I believe making the SVG element larger can increase its filter effects' resolution, and then using CSS transforms to scale the element's parent will return it to its original size, but with a higher resolution result. From there, additional changes to the filter effect (to incorporate a subtle blur for instance) may help it over the finish line.Regardless, this is a great writeup for changes I wish to never see in ordinary UI.reply",
      "I\u2019d be very interested to compare the power efficiency of this implementation versus the OS-native version of same over a 12-hour benchmark.reply",
      "this is the first one I've seen that isn't just feTurbulence. Thank you for doing it right! I've been thinking about it since the first liquid glass clones!reply"
    ],
    "link": "https://kube.io/blog/liquid-glass-css-svg/",
    "first_paragraph": "Apple introduced the Liquid Glass effect during WWDC 2025 in June\u2014a stunning UI effect that makes interface elements appear to be made of curved, refractive glass. This article is a hands\u2011on exploration of how to recreate a similar effect on the web using CSS, SVG displacement maps, and physics-based refraction calculations.Instead of chasing pixel\u2011perfect parity, we\u2019ll approximate Liquid Glass, recreating the core refraction and a specular highlight, as a focused proof\u2011of\u2011concept you can extend.We'll build up the effect from first principles, starting with how light bends when passing through different materials.Chrome\u2011only demoThe interactive demo at the end currently works in Chrome only (due to\nSVG filters as backdrop\u2011filter).You can still read the article and interact with the inline\nsimulations in other browsers.Refraction is what happens when light changes direction as it passes from one material to another (like from air into glass). This bending occurs because light travels at"
  },
  {
    "title": "Immich \u2013 High performance self-hosted photo and video management (github.com/immich-app)",
    "points": 438,
    "submitter": "rzk",
    "submit_time": "2025-09-08T07:58:38 1757318318",
    "num_comments": 155,
    "comments_url": "https://news.ycombinator.com/item?id=45165684",
    "comments": [
      "I would love to give this a try but its software supply chain story seems like a car crash, with dependency bumps needed every few days: https://github.com/immich-app/immich/commits/main/server/pac...I'm keen to use it as soon as the dependency story is mature (eg. it is packaged in Debian). This doesn't seem likely to happen any time soon.I'm sure many people won't care about this. But for me, it's a measure of quality. I expect to be able to deploy and not worry about it, except for security updates, for at least a couple of years, preferably more. Constantly moving dependencies spidering out to a multitude of other projects, and Docker Compose, provide no such confidence.Edit:Ironically, just after posting that I came across this, which I think proves why my concern is warranted: https://news.ycombinator.com/item?id=45169657Debian isn't immune to this, but it's much harder for such an attack to be successful when dependencies aren't constantly changing.reply",
      "Are there any/many applications that require a configured database (like PostgreSQL) and Redis/Valkey in Debian's package manager at all?Also, Docker-compose is pretty great in terms of getting complex applications up and running.reply",
      "> Ironically, just after posting that I came across this, which I think proves why my concern is warranted: https://news.ycombinator.com/item?id=45169657> Debian isn't immune to this, but it's much harder for such an attack to be successful when dependencies aren't constantly changing.Immich is more immune to this issue because they wait 5 days before raising PRs to bump dependencies, which is a good practice https://github.com/immich-app/.github/blob/main/renovate-con...reply",
      "I've been keeping my eye on Immich for a while and keep waiting for a stable release to try it out, but that hasn't happened yet. I'm also dreading having to setup proper backups if I were to switch to this over Google photos. My current solution is to backup critical homelab things to Google Drive automatically but I'd want a proper off-site backup if I were going to self host all my photos.reply",
      "To be fair, there\u2019s a massive banner on their front page warning users it\u2019s in beta. Until they settle on a proper release it\u2019ll continue to be a bit chaotic. All software development is like that.reply",
      "This looks like one of those projects that will never settle and have a stable slower release cycle.reply",
      "I don\u2019t think so. They are steadily approaching their defined and published goals for stable release. I\u2019m guessing it will come this year.reply",
      "v0.46.4_p3reply",
      "This looks like a project that\u2019s under heavy development (it is) responsibly keeping up  with dependencies. This gives me more confidence, not less.reply",
      "Why is docker compose a red flag? That feels like a benefit to me.reply"
    ],
    "link": "https://github.com/immich-app/immich",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n        High performance self-hosted photo and video management solution.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\n\n\n\n\n\n\n\n\n\nCatal\u00e0\nEspa\u00f1ol\nFran\u00e7ais\nItaliano\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nDeutsch\nNederlands\nT\u00fcrk\u00e7e\n\u4e2d\u6587\n\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430\n\u0420\u0443\u0441\u0441\u043a\u0438\u0439\nPortugu\u00eas Brasileiro\nSvenska\n\u0627\u0644\u0639\u0631\u0628\u064a\u0629\nTi\u1ebfng Vi\u1ec7t\n\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\nNoteYou can find the main documentation, including installation guides, at https://immich.app/.Access the demo here. For the mobile app, you can use https://demo.immich.app for the Server Endpoint URL.Read more about translations here.\n        High performance self-hosted photo and video management solution.\n       There was an error while loading. Please reload this page.\nTher"
  },
  {
    "title": "Will Amazon S3 Vectors kill vector databases or save them? (zilliz.com)",
    "points": 159,
    "submitter": "Fendy",
    "submit_time": "2025-09-08T15:35:46 1757345746",
    "num_comments": 85,
    "comments_url": "https://news.ycombinator.com/item?id=45169624",
    "comments": [
      "This is a good article and seems well balanced despite being written by someone with a product that directly competes with Amazon S3. I particularly appreciated their attempt to reverse-engineer how S3 Vectors work, including this detail:> Filtering looks to be applied after coarse retrieval. That keeps the index unified and simple, but it struggles with complex conditions. In our tests, when we deleted 50% of data, TopK queries requesting 20 results returned only 15\u2014classic signs of a post-filter pipeline.Things like this are why I'd much prefer if Amazon provided detailed documentation of how their stuff works, rather than leaving it to the development community to poke around and derive those details independently.reply",
      "> Things like this are why I'd much prefer if Amazon provided detailed documentation of how their stuff works, rather than leaving it to the development community to poke around and derive those details independently.Absolutely this. So much engineering time has been wasted on reverse-engineering internal details of things in AWS that could be easily documented. I once spent a couple days empirically determining how exactly cross-AZ least-outstanding-requests load balancing worked with AWS's ALB because the docs didn't tell me. Reverse-engineering can be fun (or at least I kinda enjoy it) but it's not a good use of our time and is one of those shadow costs of using the Cloud.It's not like there's some secret sauce here in most of these implementation details (there aren't that many ways to design a load balancer). If there was, I'd understand not telling us. This is probably less an Apple-style culture of secrecy and more laziness and a belief that important details have been abstracted away from us users because \"The Cloud\" when in fact, these details do really matter for performance and other design decisions we have to make.reply",
      ">It's not like there's some secret sauce here in most of these implementation details. If there was, I'd understand not telling us. This is probably less an Apple-style culture of secrecy and more laziness and a belief that important details have been abstracted away from us users because \"The Cloud\" when in fact, these details do really matter for performance and other design decisions we have to make.Having worked inside AWS I can tell you one big reason is the attitude/fear that anything we put in out public docs may end up getting relied on by customers. If customers rely on the implementation to work in a specific way, then changing that detail requires a LOT more work to prevent breaking customer's workloads. If it is even possible at that point.reply",
      "Right now, it is basically impossible to reliably build full applications with things like DynamoDB (among other AWS products), without relying on internal behaviour which isn't explicitly documented.reply",
      "I am also a former AWS employee.  What non public information did you need for DDB?reply",
      "Try ingesting the a complete WHOIS dump into DDB sometime.  This was before autoscaling worked at all when I tried... but it absolutely wasn't anything one can consider fun.In the end, after multiple implementations, finally had to use a Java Spring app on a server with a LOT of ram just to buffer the CSV reads without blowing up on the pushback from DDB.  I think the company spent over $20k in the couple months on different efforts in a couple different languages (C#/.Net, Node.js, Java) across a couple different routes (multiple queues, lambda, etc) just to get the initial data ingestion working a first time.The Node.js implementation was fastest, but would always blow up a few days in without the ability to catch with a debugger attached.  The queues and lambda experiments had throttling issues similar to the DynamoDB ingestion itself, even with the knobs turned all the way up.  I don't recall what the issue with the .Net implementation was at the time, but it blew up differently.I don't recall all the details, and tbh I shouldn't care, but it would have been nice if there was some extra guidance of trying to take in a few gb of csv into DynamoDB at the time.  To this day, I still hate ETL work.reply",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerg...reply",
      "Cool... though that would make it difficult to get the hundred or so CSVs into a single table, since it isn't supported  I guess stitching them before processing would be easy enough... also, no idea when that feature became available.reply",
      "It\u2019s never been a good idea to batch ingest of a lot of little single files using any ETL process on AWS, whether it be DDB, Aurora MySQL/Postgres using \u201cload into S3\u2026\u201d, Redshift batch import from S3, or just using Athena (yeah I\u2019ve done all of them).",
      "And yet \"Hyrum's Law\" famously says people will come to rely on features of your system anyway, even if they are undocumented. So I'm not convinced this is really customer-centric, it's more AWS being able to say: hey sorry this change broke things for you, but you were relying on an internal detail. I do think there is a better option here where there are important details that are published but with a \"this is subject to change at any time\" warning slapped on them. Otherwise, like OP says, customers just have to figure it all out on their own.reply"
    ],
    "link": "https://zilliz.com/blog/will-amazon-s3-vectors-kill-vector-databases-or-save-them",
    "first_paragraph": "Zilliz CloudFully-managed vector database service designed for speed, scale and high performance.MilvusOpen-source vector database built for billion-scale vector similarity search.DocumentationThe Zilliz Cloud Developer Hub where you can find all the information to work with Zilliz CloudLearn MoreWill Amazon S3 Vectors Kill Vector Databases\u2014or Save Them?By\u00a0James LuanNot too long ago, AWS dropped something new: S3 Vectors. It\u2019s their first attempt at a vector storage solution, letting you store and query vector embeddings for semantic search right inside Amazon S3.At a glance, it looks like a lightweight vector database running on top of low-cost object storage\u2014at a price point that is clearly attractive compared to many dedicated vector database solutions.\n\n\namazon s3 vectors.png\n\nNaturally, this sparked a lot of hot takes. I\u2019ve seen folks on social media and in engineering circles say this could be the end of purpose-built vector databases\u2014Milvus, Pinecone, Qdrant, and others included"
  },
  {
    "title": "Ex-WhatsApp cybersecurity head says Meta endangered billions of users (theguardian.com)",
    "points": 136,
    "submitter": "mdhb",
    "submit_time": "2025-09-08T21:26:07 1757366767",
    "num_comments": 70,
    "comments_url": "https://news.ycombinator.com/item?id=45174221",
    "comments": [
      "Given how WhatsApp is the de-facto way to communicate outside of the West and China, these security/data-handling \"weaknesses\" are most likely a feature, not a bug. An absolute bonanza for the certain intelligence services.Remember, kids: End to end encryption is useless if the \"ends\" are fully controlled by an (untrustworthy) third party.reply",
      "> outside of the Westyou probably mean outside of the USA, it's huge in Europe/UK(which doesn't contradict your main point)reply",
      "It is huge in Latin America.USA is special because it is the (only?) country where iPhone has more users than Android.reply",
      "iPhone has more users than Android in Canada and Japan as well. I think some Nordic countries too.reply",
      "It's crazy how an US company dominates the world's messaging market but not in the USreply",
      "It\u2019s not uncommon. Orkut back in the day was widely popular in Latin America and India. WhatsApp is the same. I think users in NA have a lot of high quality options as against those in Asia and LatAm who don\u2019t have much reliable options other than ones developed in NA.",
      "Without open source, end to end encryption is useless. It's not hard to hide a piece of code that defeats the encryption in closed source code.reply",
      "iMessage is end to end encrypted. Although Apple says it secure and the courts and FBI seem to not be able to get it in, it is still closed source.reply",
      "I can't tell if I'm being paranoid or just realistic, when I suspect that FBI/Apple fights over decrypting/unlocking iPhones or iMessage are just part of Apple's security theater.If I were Evil-Tim-Cook, I'd have a deal with the FBI (and other agencies) where I'd hand over some user's data, in return for them keeping that secret and occasionally very publicly taking Apple to court demanding they expose a specific user and intentionally losing - to bolster Apple's privacy reputation.reply",
      "> If I were Evil-Tim-Cook, I'd have a deal with the FBI (and other agencies) where I'd hand over some user's data, in return for them keeping that secret and occasionally very publicly taking Apple to court demanding they expose a specific user and intentionally losing - to bolster Apple's privacy reputation.The FBI wants its investigations to go to court and lead to convictions. Any evidence gained in this way would be exposed as coming form Apple; notwithstanding parallel construction:* https://en.wikipedia.org/wiki/Parallel_constructionAs for other agencies, I'm sure many have exploits to attack these devices and get spyware on them, and so may not need Apple's assistance.reply"
    ],
    "link": "https://www.theguardian.com/technology/2025/sep/08/meta-user-data-lawsuit-whatsapp",
    "first_paragraph": "Attaullah Baig, fired this year, said he had warned Mark Zuckerberg engineers had unaudited access to user dataA former top cybersecurity executive at WhatsApp filed a lawsuit on Monday alleging that parent company Meta disregarded internal flaws in the app\u2019s digital defenses and exposed billions of its users. He says the company systematically violated cybersecurity regulations and retaliated against him for reporting the failures.Attaullah Baig, who served as head of security for WhatsApp from 2021 to 2025, claims that approximately 1,500 engineers had unrestricted access to user data without proper oversight, potentially violating a US government order that imposed a $5bn penalty on the company in 2020.He also claimed the company failed to remedy the hacking and takeover of more than 100,000 accounts each day, ignoring his pleas and proposed fixes and choosing instead to prioritize user growth. The lawsuit, filed in US federal court in San Francisco, alleges Facebook-owner Meta fail"
  },
  {
    "title": "World Nuclear Association Welcomes Microsoft Corporation as Newest Member (world-nuclear.org)",
    "points": 61,
    "submitter": "gnabgib",
    "submit_time": "2025-09-08T21:01:18 1757365278",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=45173933",
    "comments": [
      "The Windows 98 license actually did forbid using Windows in nuclear power plants (along with other high risk areas). That was due to some interaction with the Java license and I always considered it a very fortunate fluke.reply",
      "Reference for those curious: https://archive.org/stream/microsoft-windows-98-second-editi...reply",
      "The same thing with QuickTime (remember QuickTime, and trailers.apple.com?)..Ah, where did that carefree time go, where we had the time to read licenses...reply",
      "\"It looks like you're trying to insert some control rods. Would you like help with that?\"reply",
      "funny, except now it will be Ani as the avatar to Grok Unhingedreply",
      "nuclear_power_run_book.docreply",
      "In all seriousness, it\u2019s only a matter of time before an LLM makes a critical error in language-translating (or even being used to write) a reference manual for an industrial process, and escapes the attention of regulators. One can only hope that that process is not nuclear\u2026reply",
      "K://nuclear_power_run_book FOR NEW JOINERS (v2 copy).docx (3) (SHARED)reply",
      "This is a big deal, not because Microsoft wants to build reactors but because it highlights the real bottleneck: nuclear fuel. There\u2019s already a growing uranium deficit, conversion and enrichment capacity are thin and geopolitically fragile, and next-gen reactors need HALEU, which barely exists today. Building new reactors is the easy part \u2014 scaling the fuel supply chain takes years.reply",
      "Yes, and ... restarting the fuel cycle under the current administration is, according to activity in the US uranium rich areas, kind of happening. I haven't seen anything \"official\" yet but driving around southern Utah shows signs of 'unexpected economic activity.' Speculation is that the USG is going to re-open a Uranium mine near Moab.reply"
    ],
    "link": "https://world-nuclear.org/news-and-media/press-statements/world-nuclear-association-welcomes-microsoft-corporation-as-newest-member",
    "first_paragraph": "LONDON, UK \u2013World Nuclear Association is proud to announce that Microsoft Corporation, one of the world's leading technology companies, has officially joined as our newest member. This landmark membership underscores the growing recognition of nuclear energy as an essential foundation for powering the digital economy and achieving ambitious climate goals.In order to meet growing electricity demand globally, the world not only needs to continue to invest in renewables, but also advanced carbon-free technologies. Microsoft has emerged as a leader in securing reliable, carbon-free electricity.\"Microsoft's membership with the Association is a game-changing moment for our industry,\" said Dr Sama Bilbao y Le\u00f3n, Director General of World Nuclear Association. \"When one of the world's most innovative technology companies recognizes nuclear energy as essential to their carbon-negative future, it sends a powerful signal to markets, policymakers, and industry leaders worldwide. This partnership wi"
  },
  {
    "title": "How RSS beat Microsoft (buttondown.com/blog)",
    "points": 264,
    "submitter": "vidyesh",
    "submit_time": "2025-09-08T10:50:01 1757328601",
    "num_comments": 151,
    "comments_url": "https://news.ycombinator.com/item?id=45166750",
    "comments": [
      "RSS isn't a format that's super-helpful for publishers. There are a variety of reasons why. But it's an absolute dream for consumers. And that's what makes it so awesome, so powerful.Case in point: I saw someone had unsubscribed from one of my email newsletters, and when I went to go read the \"reason why\" field, they'd filled out: \"subscribed to the RSS feed instead.\"That's right, my email newsletter has an RSS feed (thanks Buttondown!), and they prefer to receive the newsletter that way rather than via email. And can I blame them? Absolutely not! I love RSS. Is it better for my vanity to have their email address in my database instead, rather than some nebulous XML file going out to who-knows? Of course. But again, this format keeps on winning year after year because it's one of the best consumer-first features of the open web.reply",
      "The rise of email only newsletters has been irritating. Thankfully a lot of readers (I use inoreader) let you create mailboxes that just turn into entries in your readerreply",
      "That's an extremely rare edge case that fails to justify your point spectacularly.I thought the comment says \"it was an absolute dream for consumers\" but actually it says \"it's\". Sorry to burst your bubble, if you ask any normal person who does not spend 10 hours on HN per week, chances are that they have never heard of the term RSS in their life.reply",
      "I think there are a lot of us out there in fact. Managing a queue of feeds in an RSS reader is much more pleasant than having newsletters mixed with email. Separation of purpose is a good thing for most, imhoreply",
      "Does not help that the browsers killed native RSS support. To bring up the infamous example, many normal people were using Google Reader.reply",
      "- I still use RSS- Some major platform still provide RSS, which makes me use them (I do not use twitter, because it does not provide RSS- If not for RSS I would not be using Reddit- the moment platform drops RSS, I drop the platformLinks:[0] https://github.com/rumca-js/Django-link-archive - my own RSS readerreply",
      "I forgot Reddit ever had RSS, and I think they're doing their best to forget it, too.Viewing the source of a subreddit on old.reddit.com shows an RSS link; viewing it on the new domain does not.reply",
      "During the whole API debacle all the RSS feeds in my reader got rate limited or blocked, so I just stopped using Reddit. Maybe I'll give it another go if they actually started allowing RSS again.reply",
      "I've given up on Reddit, after all of their moves that seemed to be explicitly hostile to their users. I know some people still get value out of it, and I'm happy for them, but I'm not particularly interested anymore.reply",
      "Is there a community that you\u2019ve instead joined?reply"
    ],
    "link": "https://buttondown.com/blog/rss-vs-ice",
    "first_paragraph": "People like to tell the story of how VHS beat Betamax because adult film studios backed VHS. It\u2019s a clutch-your-pearls story that says nothing about why these multi-million-dollar businesses picked one format over the other. The real story is that while Betamax tapes had better resolution and fidelity, VHS was cheaper, offered longer recordings, and, most importantly, was the more open format.\u00a0Not many people talk about how or why RSS won the content syndication war because few people are aware that a war ever took place. Everyone was so fixated on the drama over RSS\u2019s competing standards (Atom vs RSS 2.0) that they barely registered the rise and fall of the Information and Content Exchange (ICE) specification, which had been created, funded, and eventually abandoned by Microsoft, Adobe, CNET, and other household names.\u00a0ICE was the Betamax to RSS\u2019s VHS. The Information and Content Exchange standard was more advanced, more expensive, less open, and unable to counter the overwhelming num"
  },
  {
    "title": "The key points of \"Working Effectively with Legacy Code\" (understandlegacycode.com)",
    "points": 81,
    "submitter": "lordleft",
    "submit_time": "2025-09-05T14:00:23 1757080823",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=45138695",
    "comments": [
      "I understand and I agree with the author points, specially looking to distance yourself from the dependencies these systems are usually entangled with, however:>But the code examples are in Java and C++ and I do python/JavaScript/ruby/The problem with real legacy code is that sometimes it's not even in those languages. It's VB.NET, COBOL, AS400, BASIC, FORTRAN...and these may not have a chance to \"wrap around your class\", or \"think about all the ORM code that you use\". None! I use none of that!. And I can't even call any tests because I can't extend a non existant class, there's no objects in here!.The author also says:>You need feedback. Automated feedback is the best. Thus, this is the first thing you need to do: write the tests.I don't need automated feedback. I need to untangle this deep business layer that everything is wrapped around in a 40 years old codebase, with practically no documentation and having to modify one of the core systems of the company. Sometimes I can't even trust the feedback the program outputs. In this kind of scenarios where the code is so messy and limited by technical factors, the best approach I have found is to debug. And that's it. Follow the trail, find the \"seam\", and then start your kingdom in the little space you can. Because if you tell your boss that you are implementing Unit tests in a 40 years old codebase, the first question he is gonna hit you with is \"Why?\", and the article doesn't give any compelling argument to answer this.reply",
      "I read the book shortly after it came out, when I was working on an enormous system of legacy code. Unfortunately, I didn't find the book particularly helpful in terms of strategies for understanding or modifying legacy code. Yes, tests are a good thing, but I expected the book to provide a lot more.I agree with the parent comment that it is useful to follow the \"trail\" through the code. It can be a big effort just to figure out how the pieces are connected. Figuring out the data structures and files is another important thing. Also, write documentation as you go; this will help others understand the big picture. If you can just jump in and start writing meaningful unit tests, your legacy system is kinda trivial :-)Overall, there are people who view testing as a useful tool and people who view testing as an ideology. This book falls into the latter category.reply",
      "Would you be so lucky if your legacy code was written in a well-known language. At a previous employer we had a big pile code in a \u201cmacro\u201d language. It could interface with the main C++ code base. It was similar to assembly language in that it had no loops (just gotos) and register-like local variables (no custom names, just L0, L1, L2,\u2026). The semantics were weird, something like unexpected pass-value-value versus pass-by-reference behavior when calling functions.One customer required lengthy qualification processes when changing the software. But \u201cconfiguration changes\u201d had a lower bar and somehow these \u201cmacros\u201d counted as config. So eventually all the interesting business logic and RPC processing end up in a giant macro file.reply",
      "> the article doesn't give any compelling argument to answer thisIs \"[w]hen code is not tested, how do you know you didn\u2019t break anything?\" not a compelling argument to your boss?reply",
      "> VB.NET*laughs in VBA*reply",
      "I got started professionally in ASP/JScript. That's right, ADO through JScript, backend programming in javascript circa 2001.(If you've never heard of JScript, be thankful. It was microsoft's very-slightly-modified ECMAscript variant)reply",
      "I've skimmed the source material and it doesn't either except for the general advice of putting it all in some kind of conceptual box and slowly move parts to modern or better managed solutions as you try to reverse engineer it all. The underlying subtext was that it is all probably worse than you think and there isn't much good news... But \"containment\" in various forms is the direction forward mostly unfortunately.reply",
      "Kind of depressing to read because it's so accurate, yet so likely to be ignored.I remember when Martin Fowler first published the book \"Refactoring\" - I was so relieved at the time because somebody with some clout that executives might actually listen to had not only identified what was wrong with software but identified a way to fix it and even gave it a name!  Boy was I wrong - now you have to be careful how and when you use the term \"refactor\" because the people who ought to be supportive hear \"wasting time\".reply",
      "> now you have to be careful how and when you use the term \"refactor\" because the people who ought to be supportive hear \"wasting time\".Programmers had a hand in this, sadly: https://martinfowler.com/bliki/RefactoringMalapropism.htmlNowadays people throw the term \"refactoring\" around quite loosely, usually meaning \"rewrite\" when that was not at all the original meaning of the term!A little bit like how \"vibe coding\" quickly went from one definition to another even though people should have known better.reply",
      "My feeling about it is that there is not a tension between refactoring and feature work but rather that a specific bit of feature work is best done with a refactoring.  For instance about 10 components on a certain page I was working on all had the same accessibility problem which involved changed the HTML and then changing the CSS so this is a good chance to make a single component which can be used to replace the problematic code in all 10 components and to rationalize the CSS.reply"
    ],
    "link": "https://understandlegacycode.com/blog/key-points-of-working-effectively-with-legacy-code/",
    "first_paragraph": "\u201cLegacy Code is code without tests\u201dIf you\u2019ve come across that definition, it\u2019s from Michael Feathers\u2019 book: Working Effectively with Legacy Code.While I have a slightly extended definition, this is a very valid and useful one!Feathers\u2019 book is from 2004. Yet, its content doesn\u2019t get outdated. There is a reason for that and this CommitStrip puts it best:This book is a reference. Probably THE reference.When there\u2019s a thread about Legacy Code, it doesn\u2019t take long for someone to drop a comment suggesting you read it.I didn\u2019t read it. I\u2019ve seen it\u2019s recommended. But what are the key points of that book?If that\u2019s you, I got your back!Here\u2019s my summary of the salient points of the book and how they can help you deal with your existing codebase.The challenge with changing existing code is to preserve the existing behavior.When code is not tested, how do you know you didn\u2019t break anything?You need feedback. Automated feedback is the best. Thus, this is the first thing you need to do: write the"
  },
  {
    "title": "Dark Academia Grows Up (publicbooks.org)",
    "points": 40,
    "submitter": "lermontov",
    "submit_time": "2025-09-05T16:18:33 1757089113",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=45140281",
    "comments": [
      "This is a review of Katabasis, R.F. Kuang's new book. I wonder if there are any fans of Kuang here who can convince me to give her another go; I did not much like Babel, and only got 20% of the way into it. (This despite being a recovering humanities scholar who still wears dark-academiaish tweed skirts to her programming job.) From what I remember, the characters were a bit flat, the plot didn't draw me in, and the writing style was a little formulaic. I can't help compare with Donna Tartt's The Secret History, which I absolutely adored.reply",
      "Babel is a story structured like a certain YA wizard series with very clear antagonists that are not remotely good people with very little in terms of shades of gray. Kuang has very clear themes and ideas that she wants to convey, and she leaves little to the imagination in terms of plot arcs and motivations and morality.On the other hand Babel has interesting characters that have a realistic representation of what it's like to have grown up in a cross-cultural context and how it can feel like you're betraying someone no matter what you do, which is not something I see in much media at all. She also starkly portrays and analyzes a trap that minorities can fall into where they are complicit in their own oppression.Also, I appreciate Kuang's refreshing approach to having the bad people be bad people as opposed to misguided or misunderstood people with good intentions. Some kinds of bad-ness really shouldn't be excused, or perhaps have been explored to death already.reply",
      "I read The Secret History at an impressionable age and arrived at university grossly misinformed. I love it anyway.reply",
      "My experience has been that a large majority of books that get hyped on social media and make the rounds as a hot book-club sort of read\u2026 are terrible. Whatever process causes a book to reach that point appears to have no connection to how good it is.Taste, ah, varies.reply",
      "This has been a thing for my whole life, though probably made worse by social media. But I remember as a teenager a mentor telling me to be skeptical of \"books that are read a lot by people who don't read a lot.\" But not to dismiss them entirely. There have been some good books on oprah's list or whatever and I'm sure there are some good ones coming across tiktok too.There is also non-literary value of doing the culturally resonant thing while it's relevant. It was fun to watch game of thrones when everyone was watching it, it was fun to play elden ring when everyone was playing it, it's fun to read acotar when everyone is reading it. Not everything has to stand alone on its own merits, social participation is a value too.reply",
      "Yeah, Babel had an interesting premise that didn't ultimately coalesce into a good book. I stuck with it, but only because I'm a compulsive book finisher.reply",
      "Did you like the rest of Donna Tartt's work as much as Secret History?reply",
      "These are the common complaints about all of kuang's books and I think even the people who like her work generally acknowledge they are well-founded. Her stuff clearly resonates with people despite those weaknesses, but if they don't for you then why push. There are a lot of books.reply",
      "> dark-academiaish tweed skirtsA fashion is merely a form of ugliness so absolutely unbearable that we have to alter it every six months~ https://ia801302.us.archive.org/20/items/ThePhilosophyOfDres...reply",
      "\u201cIf the devotion scholars feel toward their work is intense and sometimes irrational, it\u2019s because this is one of the last spaces of unalienated labor\u201dSpeaking as a former academic, I don\u2019t really agree with this \u2014 I think academia can make you believe wrongly that it\u2019s a kind of \u201cunalienated labor,\u201d but actually the alienation runs deep, all the deeper when it\u2019s invisible at first glance.Yes, you don\u2019t have to make something that is sold to customers or that fits in a JIRA ticket. But when you stop and think about it, you\u2019re going to be doing research based on topics and paradigms that other people have largely defined (advisors, peers); you have to publish in journals that are often for profit and pay you zero; when you teach you usually don\u2019t get paid all the tuition that your students are paying per course (the institution takes a big cut); you end up doing a lot of silly things to have a solid institutional position\u2026 TLDR, it has great moments of course, but it isn\u2019t unalienated.reply"
    ],
    "link": "https://www.publicbooks.org/dark-academia-grows-up/",
    "first_paragraph": ""
  },
  {
    "title": "Tesla market share in US drops to lowest since 2017 (reuters.com)",
    "points": 68,
    "submitter": "nabla9",
    "submit_time": "2025-09-08T23:43:20 1757375000",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=45175588",
    "comments": [
      "Political issues aside, Tesla has a variety problem. If we converge the split between midrange and premium, their lineup consists of a midsize sedan, a midsize SUV, and a niche truck.Early on this was in their favor, but with more automakers entering the fray with serious attempts to compete, they\u2019re going to have to add at least a couple more models to reman competitive: something in the vein of a Chevy Bolt on one end and an SUV that\u2019s the next step up in size from the Y/X on the other. A more conventional truck that more directly competes with the F-150 also couldn\u2019t hurt.reply",
      "Again, politics aside, I just find the lineup boring. Every model outside the Cybertruck looks like a ten year old car. Even the refresh of the model Y is still reskinning a design language started with the Model S, but with any interesting soul stripped out to reduce cost.The nerd in me loves the technology, particularly behind the scenes features of the Cybertruck like 48v architecture. In the end I want to drive something that feels like it has a soul and substance. Teslas lineup right now is not that.reply",
      "My wife thinks they all look like jellybeans. Tesla makes them like that to maximize efficiency, and they\u2019re still #1 on that metric, but people don\u2019t care about that.reply",
      "Still the best EVs on the market in terms of range, performance and price(in the US).   Chevy bolt competitor wouldn\u2019t make sense because Chevy couldn\u2019t even make the chevy bolt work and stopped production 2023.   EV trucks don\u2019t make sense at all,  they can\u2019t tow unless you want to recharge every 100 miles.   Also range is severely reduced offroading.  I don\u2019t get why auto manufacturers keep pushing them on the public.Tesla has image problem.  Y refresh is getting praise.  Edmunds says its the best car they\u2019ve driven in 2025.   It should be selling like hotcakes.  I don\u2019t think anything is fixing Tesla unless they fire musk.reply",
      "The Bolt is coming back. Chevy\u2019s reason for discontinuing it wasn\u2019t that it didn\u2019t sell, but that the platform was outdated. The new one is on their current platform and roughly analogous to the old EUV model.Pricing is another question though, and no CarPlay/AA while also coming from an old guard auto manufacturer means no sale from me.reply",
      "EV trucks do make sense, because most people (in the US) want a truck and don\u2019t tow anything.reply",
      "Battery tech is rapidly improving too, which will offset the hit to range that towing involves. Best to get the designs and manufacturing figured out to be ready for the day that battery tech arrives.reply",
      "The six-seater Model Y L, which has already been launched in China, fills in that \"SUV that's the next step up in size\" gap.https://en.m.wikipedia.org/wiki/Tesla_Model_Y_LBut yes, I fully agree that deprioritizing/stopping the low-cost \"Model 2\" was a major mistake. Particularly with tariffs keeping Chinese EVs out, Tesla could have pretty much owned the space in the US. (Although Elon's DOGE antics would still have alienated a large portion of the customer base.)reply",
      "As things stand, assuming that Nissan can manage to remain afloat, that \u201cModel 2\u201d market segment is going to be snapped up by the 2026 Leaf, which will start at ~$25k before incentives and is just a touch bigger than the Bolt EUV was and comes with NACS, fast charging, and a properly cooled battery. There\u2019s little in the US EV market that comes close to competing at that price range.Keep in mind that these things will be built in Japan and imported. I\u2019m sure Tesla could\u2019ve figured out some way to have an offering the same price or cheaper.reply",
      "They haven\u2019t released anything decent since the Model 3, which was a massive success. It\u2019s baffling to me that they\u2019re now betting the company on robotics, rather than improving and extending their existing lineup of cars.reply"
    ],
    "link": "https://www.reuters.com/business/autos-transportation/tesla-market-share-us-drops-lowest-since-2017-competition-heats-up-2025-09-08/",
    "first_paragraph": ""
  },
  {
    "title": "David Walker's Paper Clip Collection (presentandcorrect.com)",
    "points": 18,
    "submitter": "NaOH",
    "submit_time": "2025-09-05T02:21:56 1757038916",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=45134404",
    "comments": [
      "This reminded me of The Evolution of Useful Things [1] by Henry Petroski, which in one chapter explores the incremental development of the paper clip from its origin as a straight pin that pierced the set of papers.An aspect of paper clip design that\u2019s easy to overlook is that you also need to be able to manufacture them at scale, so the machinery to produce them also needs to evolve with the overall form of the product.[1] https://ia600104.us.archive.org/11/items/pdfy--rXmwRDB5uzq5M...reply",
      "I sometimes wonder what an archaeologist of the future would make of our rubbish and ephemera. A collection like this, for my imagined archaeologist, would be a rare treasure. What would they make of my uncle\u2019s marble collection: glass spheres in a leather pouch? Money? Unknown religious purposes?reply"
    ],
    "link": "https://www.presentandcorrect.com/blogs/blog/david-walkers-paper-clip-collection",
    "first_paragraph": "\n                Visit our Bloomsbury shop Monday to Saturday 11 \u2013 6.\n\n\n\nDavid Walker has been collecting paper clips from around the world for much of his life. He recently moved into a new home & his daughter has been sorting through his belongings. His clip collection couldn't make the move but was kindly donated to us. It was such a lovely thing to receive in the post, and document. You know we love a clip!\u00a0The annotation is beautiful, the locations of discovery and the overall aesthetic of the collation on cards and scraps of paper make us very happy indeed. They will be treasured.Sign up to our newsletter here. Thank you.12 Bury PlaceLondonWC1A 2JL020 72421421info@presentandcorrect.com"
  },
  {
    "title": "Learning the soroban rapid mental calculation as an adult (github.com/whacked)",
    "points": 44,
    "submitter": "vitalnodo",
    "submit_time": "2025-09-05T21:37:36 1757108256",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=45143927",
    "comments": [
      "Nice write up!I've got a soroban but never got around to learning it. Definitely should pick that up again.When I was at Uni (electrical engineering) I noticed that the Chinese students would reach for their calculator way less than western students and assumed that was because of the abacus.Side note: they tried to shape a control theory subject (everyone was getting 100%) by disallowing calculators in tests. It was rough having to do things like long division for the first time in decades. Still everyone got 100% haha.reply",
      "A nice anecdote from Richard Feynman vs a soroban userhttps://www.ee.torontomu.ca/~elf/abacus/feynman.htmlAnd HN discussion: https://news.ycombinator.com/item?id=27934887reply",
      "I used to learn abacus and had do it for 2 years..I am not sure if its me but I could've easily done 2 digit calculations in literal seconds... using the abacus modelThen our classes of abacus were shut and the thing is, that now I found myself a slight degradation because with abacus there was an assured confidence but then after abacus I used to blunder tiny things like 6+7=15 and 7*6 and other things.Maybe my personal experience with abacus was how it ended. I had it for 4 semesters and got 4 medals in abacus for getting full marks but I still believe that it made my common maths, which it meant to improve a little weaker because well now although I don't pull up an abacus in my mind, I still have all the learning of basic things like 6+7=13 and other stuff but for some utter magical reason, learning abacus made me confuse it.Overall it was a great experience though, seeing my cousins surprised that I was doing 2 digit numbers like 42x 37 calculation in 10 seconds in my mind while playing pass ball at 5th grade, maybe that was one of my peak achievements as I guess I have forgotten abacus completely...I also remember one of my cousin saying that abacus looks like a music instrument and he used to make a little music out of it. Maybe fun times indeed. I remember it was the same day that I discovered the badlands biome in minecraft, maybe I was in 4th grade. Gold is really common in badlands, I remmeber going into the cave if I am correct.Edit: also I meant not any hate towards abacus or maybe its just skill issue from my side and I just felt like saying it. I don't know why but for the most part although I still do these mistakes, I used to do it on a much higher frequency after stopping abacus and then covid hit and although I was a strong math student logically, I used to do some mathematical blunders.., for the most part they happen rarely now but still they sometimes do exist when my brain gets into too much auto pilot mode and I usually just prefer to solve calculations in the end because of this in any question or whatever, and maybe only just simplify things at most in intermediate steps so that I do least amount of mistakes possible. I usually don't think about this too much but I used to think that damn I am bad at this and used to remember the abacus past but then I saw some comments on reddit etc. saying that there were whole calculus phd teachers who sometimes did silly mistakes and maybe it was okay... felt really better about myself.reply",
      "Author never explains what soroban is or how it works, so I really have no idea what's going on here.reply",
      "They show an image of it, it's that vertical abacus. They say it's popular in Japan. They have made a digital version of it, used a big keyboard as input, and gamified the practice. They also record how fast they can do calculations and stuff like this.The point is for the author to understand how they learn (how many things become automatic in his brain and when, mental chunk formation, when performance plateaus). They just use the Japanese abacus as an excuse to do it.reply",
      "You didn't click all the details? The first section describes it in depth.reply",
      "Thank you! I didn\u2019t realize those were expandable until I read your comment.The black triangle bullets are expandable, folks, and worth expanding.reply",
      "Thank you. I went back and... never would have guessed in a million years they were expandable.They just look like summary bullets.Why would anyone write an article where the first paragraphs were collapsed anyways...? But then later paragraphs are expanded, so you think that's where the article starts?This may actually win the award for worst UX I've seen this year. Because it never gives any indication there's any UX at all. Just, wow.reply",
      "Aha, I thought they were just bullet points. Terrible UI!reply",
      "thanks, I thought I was just dumbreply"
    ],
    "link": "https://github.com/whacked/cow/blob/main/learning%20the%20soroban%20as%20an%20adult.md",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page."
  },
  {
    "title": "Seedship [Text-Based Game] (philome.la)",
    "points": 20,
    "submitter": "ntnbr",
    "submit_time": "2025-09-05T19:34:18 1757100858",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=45142704",
    "comments": [
      "I love this game and have had it on my phone for many years. Funny to see this come up - I just played it a couple of times a day ago for the first time in a few years.reply",
      "Feedback: I love text games but there is too much clicking and the font is too small.reply",
      "https://www.johnayliff.com/games/seedship/index.html?blcOndk...reply",
      "This was really fun. It had some FTL: Faster Than Light vibes.I wish it was much longer, like Zork. Would be cool to manage the colony on the planet too as a future feature.This was my result: https://www.johnayliff.com/games/seedship/index.html?bmhtrfj...reply",
      "The environmental parameters reminds me a little of the old space 4x, Stars!.Cute game. I break it out a couple times a year. There's been some great seed shop books (Watts's Freeze Frame Revolution, KSR's Aurora, Noumenon, Children of Time), and this is pretty different but has the spirit.reply"
    ],
    "link": "https://philome.la/johnayliff/seedship/play/index.html",
    "first_paragraph": ""
  },
  {
    "title": "AMD claims Arm ISA doesn't offer efficiency advantage over x86 (techpowerup.com)",
    "points": 99,
    "submitter": "ksec",
    "submit_time": "2025-09-08T14:36:00 1757342160",
    "num_comments": 183,
    "comments_url": "https://news.ycombinator.com/item?id=45168854",
    "comments": [
      "This is an entirely uncontroversial take among experts in the space. x86 is an old CISC-y hot mess. RISC-V is a new-school hyper-academic hot mess. Recent ARM is actually pretty good. And none of it matters, because the uncore and the fabrication details (in particular, whether things have been tuned to run full speed demon or full power sipper) completely dominate the ISA.In the past x86 didn't dominate in low power because Intel had the resources to care but never did, and AMD never had the resources to try. Other companies stepped in to full that niche, and had to use other ISAs. (If they could have used x86 legally, they might well have done so. Oops?) That may well be changing. Or perhaps AMD will let x86 fade away.reply",
      "I remember reading this Jim Keller interview:https://web.archive.org/web/20210622080634/https://www.anand...Basically the gist of it is that the difference between ARM/x86 mostly boils down to instruction decode, and:- Most instructions end up being simple load/store/conditional branch etc. on both architectures, where there's literally no difference in encoding efficiency- Variable length instruction has pretty much been figured out on x86 that it's no longer a bottleneckAlso my personal addendum is that today's Intel efficiency cores are have more transistors and better perf than the big Intel cores of a decade agoreply",
      "x86 decoding must be a pain - I vaguely remember that they have trace caches (a cache of decoded micro-operations) to skip decoding in some cases. You probably don't make such caches when decoding is easy.Also, more complicated decoding and extra caches means longer pipeline, which means more price to pay when a branch is mispredicted (binary search is a festival of branch misprediction for example, and I got 3x acceleration of linear search on small arrays when I switched to the branchless algorithm).Also I am not a CPU designer, but branch prediction with wide decoder also must be a pain - imagine that while you are loading 16 or 32 bytes from instruction cache, you need to predict the address of next loaded chunk in the same cycle, before you even see what you got from cache.As for encoding efficiency, I played with little algorithms (like binary search or slab allocator) on godbolt, and RISC-V with compressed instruction generates similar amount of code as x86 - in rare cases, even slightly smaller. So x86 has a complex decoding that doesn't give any noticeable advantages.x86 also has flags, which add implicit dependencies between instructions, and must make designer's life harder.reply",
      "I was an instruction fetch unit (IFU) architect on P6 from 1992-1995. And yes, it was a pain, and we had close to 100x the test vectors of all the other units, going back to the mid 1980's. Once we started going bonkers with the prefixes, we just left the pre-Pentium decoder alone and added new functional blocks to handle those. And it wasn't just branch prediction that sucked, like you called out! Filling the instruction cache was a nightmare, keeping track of head and tail markers, coalescing, rebuilding, ... lots of parallel decoding to deal with cache and branch-prediction improvements to meet timing as the P6 core evolved was the typical solution. We were the only block (well, minus IO) that had to deal with legacy compatibility. Fortunately I moved on after the launch of Pentium II and thankfully did not have to deal with Pentium4/Northwood.reply",
      "> x86 decoding must be a painSo one of the projects I've been working on and off again is the World's Worst x86 Decoder, which takes a principled approach to x86 decoding by throwing out most of the manual and instead reverse-engineering semantics based on running the instructions themselves to figure out what they do. It's still far from finished, but I've gotten it to the point that I can spit out decoder rules.As a result, I feel pretty confident in saying that x86 decoding isn't that insane. For example, here's the bitset for the first two opcode maps on whether or not opcodes have a ModR/M operand: ModRM=1111000011110000111100001111000011110000111100001111000011110000000000000000000000000000000000000011000001010000000000000000000011111111111111110000000000000000000000000000000000000000000000001100111100000000111100001111111100000000000000000000001100000011111100000000010011111111111111110000000011111111000000000000000011111111111111111111111111111111111111111111111111111110000011110000000000000000111111111111111100011100000111111111011110111111111111110000000011111111111111111111111111111111111111111111111I haven't done a k-map on that, but... you can see that a boolean circuit isn't that complicated. Also, it turns out that this isn't dependent on presence or absence of any prefixes. While I'm not a hardware designer, my gut says that you can probably do x86 instruction length-decoding in one cycle, which means the main limitation on the parallelism in the decoder is how wide you can build those muxes (which, to be fair, does have a cost).That said, there is one instruction where I want to go back in time and beat up the x86 ISA designers. f6/0, f6/1, f7/0, and f7/1 [1] take in an extra immediate operand whereas f6/2 and et al do not. It's the sole case in the entire ISA where this happens.[1] My notation for when x86 does its trick of using one of the register selector fields as extra bits for opcodes.reply",
      "> While I'm not a hardware designer, my gut says that you can probably do x86 instruction length-decoding in one cycleThat's some very faint praise there.  Especially when you're trying to chop up several instructions every cycle.  Meanwhile RISC-V is \"count leading 1s.  0-1:16bit 2-4:32bit 5:48bit 6:64bit\"reply",
      "The chopping up can happen the next cycle, in parallel across all the instructions in the cache line(s) that were fetched, and it can be pipelined so there's no loss in throughput.  Since x86 instructions can be as small as one byte, in principle the throughput-per-cache-line can be higher on x86 than on RISC-V (e.g. a single 32-byte x86 cache line could have up to 32 instructions where the original RISC-V ISA might only have 8).  And in any case, there are RISC-V extensions that allow variable-length instructions now, so they have to deal with the problem too.reply",
      "Intel\u2019s E cores decode x86 without a trace cache (\u03bcop cache), and are very efficient. The latest (Skymont) can decode 9 x86 instructions per cycle, more than the P core (which can only decode 8)AMD isn\u2019t saying that decoding x86 is easy. They are just saying that decoding x86 doesn\u2019t have a notable power impact.reply",
      "Does that really say anything about efficiency? Why can't they decode 100 instructions per cycle?reply",
      "> Why can't they decode 100 instructions per cycle?Well, obviously because there aren't 100 individual parallel execution units to which those instructions could be issued.  And lower down the stack because a 3000 bit[1] wide cache would be extremely difficult to manage.  An instruction fetch would be six (!) cache lines wide, causing clear latency and bottleneck problems (or conversely would demand your icache be 6x wider, causing locality/granularity problems as many leaf functions are smaller than that).But also because real world code just isn't that parallel.  Even assuming perfect branch prediction the number of instructions between unpredictable things like function pointer calls or computed jumps is much less than 100 in most performance-sensitive algorithms.And even if you could, the circuit complexity of decoding variable length instructions is superlinear.  In x86, every byte can be an instruction boundary, but most aren't, and your decoder needs to be able to handle that.[1] I have in my head somewhere that \"the average x86_64 instruction is 3.75 bytes long\", but that may be off by a bit.  Somewhere around that range, anyway.reply"
    ],
    "link": "https://www.techpowerup.com/340779/amd-claims-arm-isa-doesnt-offer-efficiency-advantage-over-x86",
    "first_paragraph": ""
  }
]