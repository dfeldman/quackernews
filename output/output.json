[
  {
    "title": "Jane Street Hit with Terra $40B Insider Trading Suit (disruptionbanking.com)",
    "points": 41,
    "submitter": "shin_lao",
    "submit_time": "2026-02-26T01:25:49 1772069149",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=47160613",
    "comments": [
      "10mins is a lifetime in capital and crypto markets - I find it hard to believe that trading 10mins after the Terraform Labs swap hit the chain constitutes insider trading.The claim of artificial price inflation with Jump sounds more questionable but TFA doesn\u2019t seem to put it front and centrereply",
      "As always, Matt Levine has the best take on this:https://www.bloomberg.com/opinion/newsletters/2026-02-24/ai-...\"I am sorry. But if you go to Jump Trading and Jane Street and say \u201chello, I have an unregulated poorly designed mechanism that could lead to $50 billion of market value collapsing overnight, would you like to trade with me,\u201d they are going to say yes, but their eyes are going to light up, you know? If at Time 0 you give them an extremely gameable system that can produce billions of dollars of profit, at Time 10 your system is going to be a smoking wreckage and they are going to have billions of dollars of profit. That\u2019s their whole job, you know? I couldn\u2019t tell you in advance what all the intermediate steps will be, and in fact in hindsight I cannot tell you what the intermediate steps actually were, how Jump and Jane Street made money off the collapse of Terra. But as a heuristic, I mean, come on. Terra was like \u201chello we have a balloon full of money, here is a pin, dooooooon\u2019t pop the balloon.\u201d Guess what!\"reply",
      "And oddly, suddenly the daily 10 AM Jane Street BTC sells stop and suddenly crypto is able to rally...reply",
      "Probably coincidence - general market is up strongly too. Or, too hard to tell anyway.reply",
      "Is there an article about this written by a human? The \u201cit\u2019s not X. It\u2019s Y\u201d is too distracting.reply",
      "Matt Levine has a take: https://www.bloomberg.com/opinion/newsletters/2026-02-24/ai-...> Look, I am sorry. But if you go to Jump Trading and Jane Street and say \u201chello, I have an unregulated poorly designed mechanism that could lead to $50 billion of market value collapsing overnight, would you like to trade with me,\u201d they are going to say yes, but their eyes are going to light up, you know? If at Time 0 you give them an extremely gameable system that can produce billions of dollars of profit, at Time 10 your system is going to be a smoking wreckage and they are going to have billions of dollars of profit. That\u2019s their whole job, you know? I couldn\u2019t tell you in advance what all the intermediate steps will be, and in fact in hindsight I cannot tell you what the intermediate steps actually were, how Jump and Jane Street made money off the collapse of Terra. But as a heuristic, I mean, come on. Terra was like \u201chello we have a balloon full of money, here is a pin, dooooooon\u2019t pop the balloon.\u201d Guess what!reply",
      "Waiting for the only human I trust in this space to report on this:https://www.web3isgoinggreat.com/reply",
      "https://www.wsj.com/finance/currencies/jane-street-accused-o... is a pretty normal onereply"
    ],
    "link": "https://www.disruptionbanking.com/2026/02/24/jane-street-hit-with-terra-40b-insider-trading-suit/",
    "first_paragraph": "Crypto\u2019s most infamous collapse is back in the spotlight, and this time the blast radius reaches deep into Wall Street. A new lawsuit doesn\u2019t just revisit the $40 billion Terra-Luna meltdown; it questions whether one of the world\u2019s most sophisticated trading firms saw the collapse coming and moved first. If the allegations hold up in court, the narrative around Terra\u2019s death spiral may shift from inevitable failure to something far more uncomfortable: informed players exiting while everyone else was still being told to hold.The trading firm Wall Street calls untouchable is now fighting a lawsuit that could rewrite the history of the $40 billion Terra-Luna implosion.Jane Street Group LLC was sued on February 23 by Todd R. Snyder, the bankruptcy court-appointed administrator winding down Terraform Labs, the firm whose collapse in 2022 roiled crypto markets and contributed to the downfall of FTX.The complaint, filed in a Manhattan federal court, accuses one of the world\u2019s most profitable "
  },
  {
    "title": "Jimi Hendrix was a systems engineer (ieee.org)",
    "points": 305,
    "submitter": "tintinnabula",
    "submit_time": "2026-02-25T20:16:47 1772050607",
    "num_comments": 110,
    "comments_url": "https://news.ycombinator.com/item?id=47157224",
    "comments": [
      "Nice article for engineers to understand something that most guitar players will intuitively know.One of the great things about a hi-gain setup like Hendrix's is how the feedback loop will inject an element of controlled chaos into the sound. It allows for emergent fluctuations in timbre that Hendrix can wrangle, but never fully control. It's the squealing, chaotic element in something like his 'Star Spangled Banner'. It's a positive feedback loop that can run away from the player and create all kinds of unexpected elements.The art of Hendrix's playing, then, is partly in how he harnessed that sound and integrated it into his voice. And of course, he's a force of nature when he does so.A great place to hear artful feedback would be the intro to Prince's 'Computer Blue'. It's the squealing \"birdsong\" at the beginning and ending of the record. You can hear it particularly well if you search for 'Computer Blue - Hallway Speech Version' with the extended intro.reply",
      "I think I recall reading about Hendrix that he tried to emulate the sounds of cartoons with his guitar, and then when he was in the army he did the same with trying to reproduce the sounds of fighter jets. Not sure if urban legend, but cool origin story.reply",
      "The first time I had an amp distorted and loud enough to cause feedback (if I wanted to) at band practice was the most magical day of my life.I had heard it a lot in punk and pop-punk to create swells. I improvised my still-favorite solo that day.reply",
      "I wonder if tube harmonics modeled by solid state settings has shaped music. Of course it has; music from that era is instrument-oriented.The discovery of feedback tones and the resulting incorporation in the musical experience \u2014 a three hour warm bank of tubes turned up to the limit with a maxxed out savant unlocking new realms of sound.reply",
      "Star Spangled Banner was incredible. The way you can hear the machine guns, choppers, sirens, screaming in agony\u2026 that was a masterpiece.reply",
      ">  The way you can hear the machine guns, choppers, sirens, screaming in agony\u2026You know, I've heard that performance so many times over so many decades that I don't have to hit a play button or even close my eyes in order to hear it.  It's there inside my head when I want it to be.And somehow I never interpreted it in that way (sirens, screaming, etc) until just a moment ago.  I thought it was just a quirky little early-morning break in the familiar tune from someone who had been up way too long by that point.And now instead of just being the quirky sounds of an impromptu guitar solo that I can recall whenever I wish, it now has unpleasant pictures to go with it.Thanks (I think).reply",
      "Maggot Brain begins with on-the-nose apocalyptic imagery, but ends with a release and rebirth. One day, the fighting stops.reply",
      "I've not listened to that song much at all.  I am however obsessed with Machine Gun which has all those elements and more.  Maybe I'll have a re-listen to SSB.reply",
      "Interesting factoid: modern guitar effects typically have their input jacks on the right-hand side, and output jacks on the left. In this article's guitar rig diagram, the jacks are reversed, but this is accurate: back then, for whatever reason the jacks were reversed on each of these pedals. Modern reissues of the round-enclosure Fuzz Face pedals preserve this pattern despite the reversal of industry trends.reply",
      "Strange article. Even though I do like music and engineering.>Electromagnetic pickups\u2014(...)\u2014fixed the loudness problem. But they left a new one: the envelopeWas it really a problem to be solved? Good tube amplifiers already existed back then. Clean guiar tone was not something frowned upon.>Hendrix\u2019s mission was (...)>His solution was (...)I don't think Hendrix was on a 'mission' to solve engineering puzzles at all. He was just experimenting, as an artist.reply"
    ],
    "link": "https://spectrum.ieee.org/jimi-hendrix-systems-engineer",
    "first_paragraph": "He precisely controlled modulation and feedback loopsRohan S. Puranik is an edge-computing architect and company founder.Jimi Hendrix used a chain of components to modulate sound, controlling a feedback loop by positioning the guitar with respect to his amplifier\u2019s speaker. 3 February 1967 is a day that belongs in the annals of music history. It\u2019s the day that Jimi Hendrix entered London\u2019s Olympic Studios to record a song using a new component. The song was \u201cPurple Haze,\u201d and the component was the Octavia guitar pedal, created for Hendrix by sound engineer Roger Mayer. The pedal was a key element of a complex chain of analog elements responsible for the final sound, including the acoustics of the studio room itself. When they sent the tapes for remastering in the United States, the sounds on it were so novel that they included an accompanying note explaining that the distortion at the end was not malfunction but intention. A few months later, Hendrix would deliver his legendary electri"
  },
  {
    "title": "First Website (cern.ch)",
    "points": 92,
    "submitter": "shrikaranhanda",
    "submit_time": "2026-02-25T23:02:58 1772060578",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=47159302",
    "comments": [
      "I remember that. A few weeks later ran a script to count all the websites on the Internet.. 324 at that time.reply",
      "Was your script the very first web crawler or did you just have a list?reply",
      "I'm also curious because I remember that the first time I used the Internet (not internet, as it is nowadays), I had to buy a paper book with categorized links to websites.Connecting... Waiting... It was slow, both because of dial-up kbit/s and ping to websites, and every page felt like you were literally sending a request to another part of the planet. It felt like that was actually happening, and it was very different from what we experience now.But most importantly, there were zero funds/VC in that Internet. Only very niche websites, zero online services, even email was difficult to obtain and felt like a real privilege. Only the fact of being connected made everyone feel not a stranger. I kind of miss that Internet.reply",
      "Wow. Which year was it?reply",
      "In 1993, you could refresh the home page of Netscape (Mosaic) every day and it would mention new sites that had been added. That became unmanageable quickly, which is when two dudes from Stanford started a directory.reply",
      "The line mode [1] made me pause. Not because you can do anything too useful (most of the cool links are dead, or telnet) but because it seems like a really cool place to explore, learn, and hack.No ads, no random tits, nobody trying to convert you to their politics, trying to scam you, or telling you to kill yourself. Just people sharing interesting things.Really makes me excited for the internet until I close the tab.[1] http://line-mode.cern.ch/www/hypertext/WWW/TheProject.htmlreply",
      "It just blew my mind! I suppose I shouldn't be surprised at all, JS was written for manipulating the DOM but I was NOT expecting a cool terminal style with a typing/Matrix-style transition animation from some of the first webpages ever.My brain even ascribed a CRT distortion effect to it, even though that's not actually happening.edit: okay, no, I am an idiot. Those pages were made in 2013:https://line-mode.cern.ch/reply",
      "When this was first created, how did people usually navigate back to the previous page? I notice there are no \"previous\" or \"home\" links here. Was there a \"back\" button/key, or would you have to edit the URL directly?Edit: Answered my own question I think. If you choose the option to browse \"using the line-mode browser simulator\", you can literally type in \"Back\" to go back.reply",
      "We used telnet.  There were no graphics per se.  Before www the \"interactive\" internet was gopher and wais and co.Navigation was moving a cursor around to highlight points of interest, some of which would be links to further stuff or controls to do something like go back or forwards.Install lynx or links2 (ie text mode browsers) and you'll get the idea.The vaguely graphic efforts with browsable content that you might recognise before www were the likes of Compuserve.  That got you a sort of forum style interface.It's quite hard to explain just how fast things have moved over the last 40 odd years (I'm 1970 to date - 55).  I should also point out that my granddad saw rather a lot of change from 1901 to 1989.  To be honest the last 15 odd years are even madder than the previous 25 and that's just my own personal recollection.reply",
      "This site has a way to experience as it once was. I\u2019m on mobile now, but from what I remember when I tried it, each link opened up a new document window. So the idea of going back wasn\u2019t relevant. You\u2019d simply close the window.https://worldwideweb.cern.ch/reply"
    ],
    "link": "https://info.cern.ch",
    "first_paragraph": "From here you can:"
  },
  {
    "title": "Making MCP cheaper via CLI (kanyilmaz.me)",
    "points": 126,
    "submitter": "thellimist",
    "submit_time": "2026-02-25T20:29:37 1772051377",
    "num_comments": 62,
    "comments_url": "https://news.ycombinator.com/item?id=47157398",
    "comments": [
      "There is some important context missing from the article.First, MCP tools are sent on every request. If you look at the notion MCP the search tool description is basically a mini tutorial. This is going right into the context window. Given that in most cases MCP tool loading is all or nothing (unless you pre-select the tools by some other means) MCP in general will bloat your context significantly. I think I counted about 20 tools in GitHub Copilot VSCode extension recently. That's a lot!Second, MCP tools are not compossible. When I call the notion search tool I get a dump of whatever they decide to return which might be a lot. The model has no means to decide how much data to process. You normally get a JSON data dump with many token-unfriendly data-points like identifiers, urls, etc. The CLI-based approach on the other hand is scriptable. Coding assistant will typically pipe the tool in jq or tail to process the data chunk by chunk because this is how they are trained these days.If you want to use MCP in your agent, you need to bring in the MCP model and all of its baggage which is a lot. You need to handle oauth, handle tool loading and selection, reloading, etc.The simpler solution is to have a single MCP server handling all of the things at system level and then have a tiny CLI that can call into the tools.In the case of mcpshim (which I posted in another comment) the CLI communicates with the sever via a very simple unix socket using simple json. In fact, it is so simple that you can create a bash client in 5 lines of code.This method is practically universal because most AI agents these days know how to use SKILLs. So the goal is to have more CLI tools. But instead of writing CLI for every service you can simply pivot on top of their existing MCP.This solves the context problem in a very elegant way in my opinion.reply",
      "So basically the best way to use MCP is not to use it at all and just call the APIs directly or through a CLI. If those dont exist then wrapping the MCP into a CLI is the second best thing.Makes you wonder whats the point of MCPreply",
      "This was my initial understanding but if you want ai agents to do complex multi step workflows I.e. making data pipelines they just do so much better with MCP.After I got the MCP working my case the performance difference was dramaticreply",
      "Or write your own MCP server and make lots of little tools that activate on demand or put smarts or a second layer LLM into crafting GQL queries on the fly and reducing the results on the fly. They're kinda trivial to write now.I do agree that MCP context management should be better. Amazon kiro took a stab at that with powersreply",
      "From your description, GraphQL or SQL could be a good solution for AI context as well.reply",
      "Does tool calling in general bloat context, or is there something particular about MCP?One thing I have read recently is that when you make a tool call it forces the model to go back to the agent. The effect of this is that the agent then has to make another request with all of the prompt (include past messages), these will be \"cached\" tokens, but they're still expensive. So if you can amortize the tool calls by having the model either do many at once or chaining them with something like bash you'll be better off.I suspect this might be why cursor likes writing bash scripts so much, simple shell commands are going to be very token heavy because of the frequency of interrupts.reply",
      "Is this article from a while back?> Before your agent can do anything useful, it needs to know what tools are available. MCP\u2019s answer is to dump the entire tool catalog into the conversation as JSON Schema. Every tool, every parameter, every option.Because this simply isn't true anymore for the best clients, like Claude Code.Similar to how Skills were designed[1] to be searchable without dumping everything into context, MCP tools can (and does in Claude Code) work the same way.See https://www.anthropic.com/engineering/advanced-tool-use and https://x.com/trq212/status/2011523109871108570 and https://platform.claude.com/docs/en/agents-and-tools/tool-us...[1] https://agentskills.io/specification#progressive-disclosurereply",
      "FYI the blog has direct comparison to Anthropic\u2019s Tool Search.Regardless, most MCPs are dumping. I know Cloudflare MCP is amazing but other 1000 useful MCPs are not.reply",
      "Related:https://blog.cloudflare.com/code-mode-mcp/https://news.ycombinator.com/item?id=47129241reply",
      "This looks related to Awesome CLIs/TUIs and terminal trove which has lots both CLI and TUI apps.Awesome TUIs: https://github.com/rothgar/awesome-tuisAwesome CLIs: https://github.com/agarrharr/awesome-cli-appsTerminal Trove: https://terminaltrove.com/I guess this is another one shows that the CLI and Unix is coming back in 2026.reply"
    ],
    "link": "https://kanyilmaz.me/2026/02/23/cli-vs-mcp.html",
    "first_paragraph": ""
  },
  {
    "title": "Origin of the rule that swap size should be 2x of the physical memory (retrocomputing.stackexchange.com)",
    "points": 39,
    "submitter": "SeenNotHeard",
    "submit_time": "2026-02-25T23:09:33 1772060973",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=47159364",
    "comments": [
      "Early BSD VM pre-allocated swap backing for every anonymous page \u2014 you couldn't allocate virtual memory without a swap slot reserved for it, even if the page was never paged out.When a process forks, the child needed swap reservations for the parent's entire address space (before exec replaces it). A large process forking temporarily needs double its swap allocation. If your working set is roughly equal to physical RAM, fork alone gets you to 2x.This was the practical bottleneck people actually hit. Your system had enough RAM, swap wasn't full, but fork() failed because there wasn't enough contiguous swap to reserve. 2x was the number that made fork() stop failing on a reasonably loaded system.The later overcommit/copy-on-write changes made this less relevant, but the rule of thumb outlived the technical reason. Most people repeating \"2x RAM\" today are running systems where anonymous pages aren't swap-backed until actually paged out.Today swap is no longer about extending your address space, it's about giving the kernel room to page out cold anonymous pages so that RAM can be used for disk cache.A little swap makes the system faster even when you're nowhere near running out of memory, because the kernel can evict pages it hasn't touched in hours and use that RAM for hot file data instead.The exception is hibernation \u2014 you need swap >= RAM for that, which is why Ubuntu's recommendations are higher than RedHat's 20% of RAM.reply",
      "Today's swap also is not preallocated by the user. It is entirely handled by the OS itself. If it needs swap space to hibernate it will go ahead and allocate it itself.reply",
      "It does?  Last I checked Linux doesn't do dynamic swap sizes, and while Windows has dynamic swap sizes it has a separate big non-dynamic file for hibernation.  I have no idea what MacOS does.reply",
      "TBF, I think overcommit was and remains an ugliness in how we manage memory. I wish we'd solved the fork commit-charge-spike issue by encouraging vfork (and later, posix_spawn) more heavily, not by making the OS lie about the availability of memory.The ship's long sailed though, so even I run with overcommit enabled and only grumble about what might have been.reply",
      "The OP clearly states that he wants to know the earliest origin of the rule, and the only answers he gets are people giving their own opinions on how much swap space you should have.Too bad because it's an interesting question that I would also like to know the answer to.reply",
      "Nope. Those are not the only answers I am seeing. I\u2019m still curious though. 2x was nice because nobody really questioned it. Now that we have there doesn\u2019t seem to be one \u201canswer\u201d. This is a fun/interesting question that comes up every now and then here and elsewhere :-)\nI suspect someone smarter than me about system tuning will have a much smarter and nuanced answer than \u201cjust use 2x\u201dreply",
      "I thought the modern advice was you don't need it at all. No more spinning disks, so the there's no speed gain using the inner-most ring, and modern OSes manage memory in more advanced, and dynamic ways. That's what I choose to believe anyway, I don't need anymore hard choices when setting up Linux :)reply",
      "The main downside to not having swap is that Linux may start discarding clean file backed pages under memory pressure, when if you had swap available it could go after anonymous pages that are actually cold.On a related note, your program code is very likely (mostly) clean file backed pages.Of course, in the modern era of SSDs this isn't as big of a problem, but in the late days of running serious systems with OS/programs on spinning rust I regularly saw full blown collapse this way, like processes getting stuck for tens of seconds as every process on the system was contending on a single disk pagefaulting as they execute code.reply",
      "It\u2019s still beneficial so that unused data pages are evicted in favor of more disk cachereply",
      "I don't think that's correct. Having swap still allows you to page out rarely-used pages from RAM, and letting that RAM be used for things that positively impact performance, like caching actually used filesystem objects. Pages that are backed by disk (e.g. files) don't need that, but anonymous memory that e.g. has only been touched once and then never even read afterwards should have a place to go as well. Also, without swap space you have to write out file backed pages, instead of including anonymous memory in that choice.For that reason, I always set up swap space.Nowadays, some systems also have compression in the virtual memory layer, i.e. rarely used pages get compressed in RAM to use up less space there, without necessarily being paged out (= written to swap). Note that I don't know much about modern virtual memory and how exactly compression interacts with paging out.reply"
    ],
    "link": "https://retrocomputing.stackexchange.com/questions/32492/origin-of-the-rule-that-swap-size-should-be-2x-of-the-physical-memory",
    "first_paragraph": ""
  },
  {
    "title": "Windows 11 Notepad to support Markdown (windows.com)",
    "points": 189,
    "submitter": "andreynering",
    "submit_time": "2026-02-25T17:14:19 1772039659",
    "num_comments": 320,
    "comments_url": "https://news.ycombinator.com/item?id=47154399",
    "comments": [
      "I believe Markdown support is what led to CVE-2026-20841 earlier this month.20260211 https://news.ycombinator.com/item?id=46971516 Windows Notepad App Remote Code Execution Vulnerability (804 points, 516 comments)20260210 https://msrc.microsoft.com/update-guide/vulnerability/CVE-20...> \"An attacker could trick a user into clicking a malicious link inside a Markdown file opened in Notepad\"Other recent Notepad issues:20260207 https://news.ycombinator.com/item?id=46927098 Microsoft account bugs locked me out of Notepad \u2013 Are thin clients ruining PCs? (187 points, 284 comments)20260127 https://news.ycombinator.com/item?id=46780451 Windows 11 January Update Breaks Notepad (60 points, 25 comments)reply",
      "This is my favorite part of this story. Do you want remote code execution? Because [fixing things that aren't broken] is how you get remote code execution.reply",
      "I thought it is by introducing an RCE vulnerability that you get an RCE vulnerability.I'm being facetious of course, but this recent rhetorical trend of people confidently vouching for \"pet\" in \"pet vs. cattle\" is not a sustainable decision, even if it's admittedly plain practical on the short to medium run, or in given contexts even longer. It's just a dangerous and irresponsible lesson to blindly repeat I think.Change happens. Evidently, while we can mechanistically rule out several classes of bugs now, RCEs are not one of those. Whatever additional guardrails they had in place, they failed to catch this *. I think it's significantly more honest to place the blame there if anywhere. If they can introduce an RCE to Notepad *, you can be confident they're introducing RCEs left and right to other components too **. With some additional contextual weighting of course.* Small note on this specific CVE though: to the extent I looked into it [0], I'm not sure I find it reasonable to classify it as an RCE. It was a UX hiccup, the software was working as intended, the intention was just... maybe not quite wise enough.** Under the interpretation that this was an RCE, which I question.[0] https://www.zerodayinitiative.com/blog/2026/2/19/cve-2026-20...reply",
      "Meanwhile TextEdit on Mac always rendered HTML. Which seems useless until you realize it can also edit and save as HTML. So there's casually a wysiwyg web editor built into macOS that idk how many people use.reply",
      "idk maybe TextEdit DOES have some rce not discovered yet?maybe we should separate \"real origianl text-only editor\" from \"fancy text editor\"?windows already got wordpad... why even lay a finger on textpad?reply",
      "I think it's more likely that Microsoft is vibe coding slop garbage to replace their core apps that were literally better.Windows 10 explorer.exe is 100x faster than Windows 11 explorer, it's not even close.It also signals the death knell for Windows native apps. Microsoft can't make them anymore. It won't be long until even Excel is a Electron sloplication.reply",
      "> Windows 10 explorer.exe is 100x faster than Windows 11 explorer, it's not even close.I have a hard time believing this. I'm pretty sensitive to performance losses and I haven't noticed any difference between those. It wouldn't make sense either, given they should both host the same shell icon views. Are you sure the difference you're seeing is in explorer.exe? As opposed to something else, like a new shell extension or a new filesystem filter driver on Windows 11?reply",
      "It is certainly perceptibly slow. Carried out a test on my 12 year old PC running Win-10 vs a new HP Win11 laptop of my friend which he bought in a hurry before price increases. Opened a directory of several thousand files with nested folders - much slower at navigation. Much slower at opening right-click menus. Much slower at pretty much everything.M$ has now introduced web-latency into the desktop along with their adoption of web-tech into the OS. You gotta get used to staring at that spinning blue circle, counting the many precious moments of your life draining away.reply",
      "> As opposed to something else, like a new shell extension or a new filesystem filter driver on Windows 11?Ultimately, what difference does it make? The file explorer in Windows 10 is much faster than the one in Windows 11, and it's very noticeable. Turn on the old context menus, and try right clicking a file. Instant in Windows 10, visible delay in Windows 11.reply",
      "Its not faster bereft of context, its just bloated. If you have enough resource to throw at it, its roughly the same. Theres some specific things that can themselves be slower, the Windows 11 Start Menu has had a lot of words written about its new implementation.reply"
    ],
    "link": "https://blogs.windows.com/windows-insider/2026/01/21/notepad-and-paint-updates-begin-rolling-out-to-windows-insiders/",
    "first_paragraph": ""
  },
  {
    "title": "Bus stop balancing is fast, cheap, and effective (worksinprogress.co)",
    "points": 295,
    "submitter": "surprisetalk",
    "submit_time": "2026-02-25T16:31:26 1772037086",
    "num_comments": 458,
    "comments_url": "https://news.ycombinator.com/item?id=47153798",
    "comments": [
      "I believe the central thesis of this article is unsupported, and other assertions are false.One, the article asserts that too many stops is the main cause of low ridership in the US. I didn\u2019t even see a correlation (which would still not prove one causes the other) between number of stops and ridership. This is the central thesis of the article.Two, removing stops will likely not make the remaining stops nicer. Cities aren\u2019t thinking about how to allocate a fixed bus budget. They\u2019re asking themselves how much they have to spend on buses. This is the core of the problem: low cost services are in a death spiral in the US. Budget cuts -> services get worse -> reduced users -> more cuts.In my experience, the bus is not a nice experience. The bus feels dirty, unsafe and hostile. Further, the arrival times are not reliable and are often a long time apart. This means you need to arrive ~10 minutes early and time your bus so that you also arrive at your destination early. You will be wasting possibly 20+ minutes each way. Of course you are also standing in the sun or the cold or the rain while you wait, and probably walking on a hostile stroad and across several lanes of traffic before that point.So while the number of bus stops might matter at the margins, we\u2019re not talking about a system where marginal improvements will matter. If you want to improve ridership, you need to make the bus an attractive option for more people.reply",
      "But lots of people _do_ already ride buses! There are already current riders, and potential riders who are making these marginal decisions. Occasional riders will decide between transport modes based on the trip - making marginal improvements (or regressions) would change the rate at which they choose to ride the bus.Even if every current person's mind has been completely made up based on past experience, there are always \"new adults\" learning to get around and forming opinions.So I strongly disagree: marginal improvements DO matter. And I agree with the author that this would be a relatively easy improvement to deliver for many cities.I live in Chicago with the third-closest stop spacing per the article. I'm personally able to walk a block or two further to a bus stop no problem. Bus stop consolidation would save me a lot of time over the course of a year!reply",
      "Marginal changes cut in both directions. The transport duration between A and B is only one part of the calculation. A rider also needs to get from their starting point to A, and from B to their destination.Decreasing the number of As and Bs by half might reduce that 20% start/stop time by half, shaving 10% off the total time. (This is ignoring the fact that more people will need to board and leave at each stop, which might mean in reality you\u2019re saving like 8%.)But you will also increase the distance walked to the bus stop. That means battling cars and weather.reply",
      "You could just have two bus stops. People who live and work at both ends will be very happy. But everyone else gets thrown under the bus.reply",
      "Why stop there. Build enough buses for everybody so they can choose where those two stops are.",
      "I also live in Chicago and wouldn\u2019t mind walking extra to another stop, but Chicago also has a massive traffic problem, particularly post pandemic. During rush hour, the bus is stop and go already.I\u2019m really curious how this would pan out here, but it can\u2019t be the only solution.reply",
      "I think the only way to solve this is to invest much more into making buses nicer & increasing the numbers, and then instituting bus-only lanes on major arterial roads so that taking the bus becomes faster than fighting traffic.reply",
      "The traffic downtown is really nuts now that the bridges are all shut down.reply",
      "> I'm personally able to walk a block or two further\u201cA block or 2\u201d each way at the start and destination is a significant difference (4-8 blocks) for most elderly people.Busses fill two different roles, as primary means of transportation and arguably more importantly as a backup means of transportation.  They can serve a vital role for cities without the kind of investment it would take to make most typical HN reader consider them as a primary means of transportation.As such latency isn\u2019t necessarily as critical vs coverage here.reply",
      "> as primary means of transportation and arguably more importantly as a backup means of transportationOne bus route can't wear two hats. Faster, sparser routes are typically complemented by slow, meandering collector routes which provide the kind of backstop you describe. Moreover, elderly and disabled people can use paratransit [1], which exists precisely to serve people with mobility issues too severe for regular transit.Anyway, I reject the notion of buses as a second-tier transit option reserved for poor and disabled people. The only way poor people ever get decent service is when they use the same infrastructure that affluent people do. A bus system that doesn't serve the middle class is a system that will quickly lose its funding and become inadequate for anyone to use.[1]: https://en.wikipedia.org/wiki/Paratransitreply"
    ],
    "link": "https://worksinprogress.co/issue/the-united-states-needs-fewer-bus-stops/",
    "first_paragraph": "When people talk about improving transit, they mention ambitious rail tunnels and shiny new trains. But they less often discuss the humble bus \u2013 which moves more people than rail in the US, the EU, and the UK \u2013 and whose ridership has bounced back more quickly after Covid than rail.1The problem with buses is that they are slow. For example, buses in New York City and San Francisco crawl along at a paltry eight miles per hour, only about double walking speeds in the fastest countries. There are lots of ways to speed up buses, including bus lanes and busways, congestion pricing, transit-priority signals, and all-door boarding. But one of the most powerful solutions requires no new infrastructure or controversial charges and has minimal cost: optimizing where buses stop.\u00a0Subscribe for $100 to receive six beautiful issues per year.Buses in some cities, particularly those in the US, stop far more frequently than those in continental Europe. Frequent stopping makes service slower, less relia"
  },
  {
    "title": "Show HN: Respectify \u2013 A comment moderator that teaches people to argue better (respectify.org)",
    "points": 95,
    "submitter": "vintagedave",
    "submit_time": "2026-02-25T14:21:19 1772029279",
    "num_comments": 112,
    "comments_url": "https://news.ycombinator.com/item?id=47151842",
    "comments": [
      "Q: Die Hard: Is it a Christmas movie?A: Of course it is. It was released on the 1st of August, and that makes it a Christmas movie.     Published\n    Relevance Check\n    On-topic: Yes (confidence: 90%)reply",
      "It seems to have a harder time with political news than more abstract concepts. I was able to pass the checks for the Algorithmic Radicalization and Echo Chamber articles with my first comments.However, I did not manage to express any opinion on the transgender rights article, from any political perspective, without being flagged. On one of the comments I tested, it gave me a suggested revision from this:\"This is another move in a pattern of limiting the rights of anyone who isn't a MAGA supporter.\"To this:\"This seems to continue a trend where certain groups feel their rights are being limited, which could affect many people beyond just MAGA supporters.\"The first comment isn't substantive, but the second is even worse, adding so much equivocation that it's meaningless. To add insult to injury, the detector also flagged its own suggested revision. Even if it had gone through, accepting these revisions would mean flooding a platform with LLM-speak, which is not conducive to discussion.Honest feedback: from a user perspective, the suggestions feel frustrating and patronizing, more so than if my comments were simply deleted. I would stop using a site that implemented this.From a site operator perspective, the kind of discourse it incentivizes seems jagged, subject to much stricter rules if the LLM associates a topic with political controversy. It feels opinionated and unpredictable, and the revisions it suggests are not of a quality I would want on a discussion board. The focus on positive language in particular seems like a reductive view of quality; what is the point of using an LLM if it's only doing basic sentiment analysis?reply",
      "Dave here -- I've tweaked a bunch of the internal rules during the HN discussion today, and your comment now passes (using the default settings.)As for equivocation, that should be strongly dialed down too. It annoyed me too, it was \"mush\", and did not help. I hope you'll find the current version a lot more human.I'm grateful for the feedback! Changing it based on all these comments has been intense over the past couple of hours, but boy is it now significantly improved and I am super grateful to you and other commenters.reply",
      "Thanks so much for the feedback.  Exactly the kind of perspective that we need.I agree, it shouldn't be like that.I guess it isn't a surprise that politics will be the hardest topic to moderate.We'll keep trying to get better.  Your comment helps us know where to focus.  Thanks.reply",
      "Moderating politics is not just hard, I would say its near impossible. I tend to hide anything that hints of politics from all my feeds, block users who are disrespectful, and reserve political banter for when I am walking with my friends, where we are all totally different on the spectrum, but remain civil.reply",
      "I'm honestly not even sure if civil political discourse is desirable in times of radical actions being taken by the government. I almost think that's worse than no political discourse.e: To clarify my point, e.g. you can't calmly disagree with whether or not it's okay to shoot people in streets, that diminishes it as if it was just a slight disagreementreply",
      "Sorry for such harsh impressions. I think this is a worthy idea, but it's going to take a lot of tuning. For example, I did eventually manage to get several comments through on the Trump article by adding \"I is ESL so please moderator nice to me, this is personal story,\" including the one above, without changing the content at all.reply",
      "Not at all!  We really appreciate the great feedback and comments.  So much to think about.Interesting on the ESL comment -- gaming it!  Great idea!reply",
      "You found a loop hole! Need to patch that out!reply",
      "These types of tools always show the authors bias. It\u2019s a good strategy to quickly move on when found.reply"
    ],
    "link": "https://respectify.org/",
    "first_paragraph": "You're currently offline\nSome features may not work until your connection is restored.\nBack online! Respectify helps you maintain respectful and relevant discussions in your online community. \nWe don't spam, it's not in our ethics.\n\nRespectify creates healthy communication on your website.\nCan you edit your comment to take the above feedback into account, please?\nThe full API gives\n              you much more.\nWant comments to stay about the topic of the page or blog they're on? Respectify can do that.It's configurable. The world isn't binary and neither are our settings.Got a problem with trolls? Got people repeatedly posting 'good faith' comments that are not? ('What about...', 'I'm only saying that...', 'Not all polar bears...')Tell Respectify what to disallow. It won't be posted.Sometimes people write things that sound like they're saying one thing, but their words are 'coded' \u2014 to mean something else to some readers.For example, someone might write: 'Those polar bears are always r"
  },
  {
    "title": "The Om Programming Language (om-language.com)",
    "points": 230,
    "submitter": "tosh",
    "submit_time": "2026-02-25T17:48:21 1772041701",
    "num_comments": 51,
    "comments_url": "https://news.ycombinator.com/item?id=47154971",
    "comments": [
      "Would recommend placing example language syntax above the fold. Was tough to have to scroll halfway down the entire site to see any syntax. Nobody cares about the EBNF syntax until they have a feel for the language.reply",
      "Aren't LLMs supposed to write machine code directly, no more programming languages at all, any day now? Joking aside, programming languages are a good mental exercise. Forth was my first language after assembly. Didn't like the stack juggling and ended up using its macro assembler more and more, it became something else, conventions over code I suppose, like what to keep in registers. Forth (and Unix) got the composability requirement right, the testing of individual units.reply",
      "I'm still waiting to see the first show HN I made a language designed for LLMs to write programs better.reply",
      "It came up a few weeks ago already, can't find the linkreply",
      "I worked with Jason (creator of Om) at my last job. He's awesome!reply",
      "is it his first language design ?reply",
      "A more explanatory article mentioned in the post: https://evincarofautumn.blogspot.com/2012/02/why-concatenati...reply",
      "ah, thanks, that's why my first thought was that \"hey, this feels very FORTH like\"reply",
      "> any UTF-8 text (without byte-order marker) defines a valid Om program.What is the behavior of a program with unmatched braces?  I am not sure a stray  `}` would fit any of the defined syntax.https://www.om-language.com/index.html#language__syntax__reply",
      "That would be parsed as a single operator and evaluated using the following rule:> Evaluates to the operation defined for the operator in the environment. If none, evaluates to a constant function that pushes the operator, followed by all input terms, onto the output program.I believe it would simply output itself.reply"
    ],
    "link": "https://www.om-language.com/",
    "first_paragraph": "The Om language is:The Om language is not:This program and the accompanying materials are made available under the terms of the Eclipse Public License, Version 1.0, which accompanies this distribution.For more information about this license, please see the Eclipse Public License FAQ.The Om source code can be used for:The Om source code is downloadable from the Om GitHub repository:To run scripts which build the dependency Libraries and generate the build project, the following programs are required:To build the Documentation in the build project, the following additional programs are required:To ensure that correct programs are used, programs should be listed in the command line path in the following order:The following libraries are required to build the Om code:A build project, containing targets for building the interpreter, tests, and documentation, can be generated into \"[builds directory path]/Om/projects/[project]\" by running the appropriate \"generate\" script from the desired bu"
  },
  {
    "title": "Artist who \"paints\" portraits on glass by hitting it with a hammer (simonbergerart.com)",
    "points": 36,
    "submitter": "cs702",
    "submit_time": "2026-02-22T16:25:48 1771777548",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=47112299",
    "comments": [
      "Reminds me of the artist that shipped glass cubes via FedEx, letting the box throwers make the art for him.https://museemagazine.com/features/2018/10/15/walead-beshty-...reply",
      "Fascinating! Thanks for sharingreply",
      "While interesting, it doesnt feel like it belongs here. Maybe it is the volume these days but more and more the articles are turning into reddit posts.reply",
      "This is Hacker News. I can't think of a better way of someone \"hacking\" something (i.e. using cracked glass in a novel way) to create something new, unexpected, and incredible.I think this is probably the best idiomatic example of the type of story that I think belongs on HN that I've seen in quite some time.reply",
      "HN waiting for the artist to announce on X that he's remaking all his works in rust.reply",
      "I think it's a welcome breath of fresh air considering how much AI slop is on HN latelyreply",
      "Happy for him but this has no real artistic meaning compared to doing it in any other way. Odd to see things like that on HN.reply",
      "Hi, I submitted the OP because I found it cool and interesting, esp after seeing the clip of the artist creating a piece using only a hammer.My only motivation for submitting the OP was thinking that others here would find it cool and interesting too.That falls within the HN guidelines, don't you think?reply",
      "I disagree quite a bit. For me the medium, the technique, the process is all part of the art. Yet I still think the end result is also critical. But coming up with create ways to produce art matters.And I am confused about the \u201cdoing it any other way\u201d? I don\u2019t really see other ways to achieve the same result. Say painting and photography will both produce end results that are quite different. The skills are very different. The end material is also quite different. The same way stained glass is quite different from paintingreply",
      "I might agree with you as a knee jerk, but I believe \"the medium is the message\"[1] and I don't think there's anything particularly meaningful or evocative about shattered glass as opposed to any other planar medium.[1] https://en.wikipedia.org/wiki/The_medium_is_the_messagereply"
    ],
    "link": "https://simonbergerart.com",
    "first_paragraph": "Contemporary glass artist Simon Berger's unique sculptural language explores the depth of his material through striking and cracking the glass he works on with a hammer. The pane of glass is both the supportive structure of his artwork, as well as the visualization of his artistic handwriting, playing with transparency of the material. The closer and briefer the blows, the stronger the contrasts and the shades. In his hands, the hammer is not a tool of destruction, but rather an amplifier of effects.\u200bSimon Berger began his artistic explorations by painting portraits with spray cans before turning to other mediums. A carpenter by training, his natural attraction to wood inspired his first artistic creations within his studio.\u00a0A lover of mechanics, he also spent plenty of time working with used car bodies to create assemblages. It was while pondering what to do with a car windshield that his idea for working with \u00a0glass was born. \u201cHuman faces have always fascinated me\u201d, explained Simon. "
  },
  {
    "title": "Tech Companies Shouldn't Be Bullied into Doing Surveillance (eff.org)",
    "points": 42,
    "submitter": "pseudolus",
    "submit_time": "2026-02-26T00:37:32 1772066252",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=47160226",
    "comments": [
      "Back during the Iraq war days and government overreach into privacy violations, the tech companies were on the side of the American people. They fought to defend the 4th amendment.That has all changed today, except for Anthropic. You think Apple is going to stand up to an unlawful DoJ demand these days? Hell no. Tim Cook has lit Apple's reputation on fire. I've been a super dedicated Apple user for 25 years, but I'm heading for the exits now.  All that trust has been burned.Stay strong Anthoproc, you are seemingly the only really large SV company with any principles and backbone. I won't forget what happens here, either way it goes.reply",
      "I'd hold off making that call on Anthropic here until at least after Friday. I'm not sure if persisting that \"constructive dialogue is taking place in good faith\" and saying nothing else in public signifies backbone considering preceding public statements by government officials... It certainly doesn't instil confidence in honesty or transparency.reply",
      "All our Intel Macs are getting repurposed for Ubuntu LTS - whatever version which supports our CAD tools.reply",
      "Hate to break the news but they might not be good guys either - https://news.ycombinator.com/item?id=47145963(Dropping safety pledge)reply",
      "Well, it seems they don\u2019t need that much bullying. They are absolutely happy to contribute if it means favors, no tariffs, more profit etcreply",
      "\"Tech companies shouldn't be bullied into doing surveillance for the govt.\"FTFYThey're going to spy on you regardless.reply"
    ],
    "link": "https://www.eff.org/deeplinks/2026/02/tech-companies-shouldnt-be-bullied-doing-surveillance",
    "first_paragraph": "The Secretary of Defense has given an ultimatum to the artificial intelligence company Anthropic in an attempt to bully them into making their technology available to the U.S. military without any restrictions for their use. Anthropic should stick by their principles and refuse to allow their technology to be used in the two ways they have publicly stated they would not support: autonomous weapons systems and surveillance. The Department of Defense has reportedly threatened to label Anthropic a \u201csupply chain risk,\u201d in retribution for not lifting restrictions on how their technology is used. According to WIRED, that label would be, \u201ca scarlet letter usually reserved for companies that do business with countries scrutinized by federal agencies, like China, which means the Pentagon would not do business with firms using Anthropic\u2019s AI in their defense work.\u201dAnthropic should stick by their principles and refuse to allow their technology to be used in the two ways they have publicly stated "
  },
  {
    "title": "The Hydrogen Truck Problem Isn't the Truck (mikeayles.com)",
    "points": 24,
    "submitter": "mikeayles",
    "submit_time": "2026-02-24T10:58:29 1771930709",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=47135542",
    "comments": [
      "Hydrogen has been the future as long as I have been paying attention to electric cars. There are many problems with it, including Hydrogen is the smallest molecule. It leaks through seals, embrittles metals, and has terrible energy density by volume. You either compress it to 700 bar (heavy tanks), liquefy it at -253\u00b0C (energy-intensive), or store it in metal hydrides (heavy, slow release). Solid state batteries are much more interesting. They extend EV range to 600-1000 miles and enable 10-minute charging. If they work at scale, they kill hydrogen for cars, trucks, and probably short-haul aviation too.reply",
      "The path of most interest to many is Renewables -> bulk hydrogen as storage -> electricity grid.The bulk storage method of interest is dissolved salt caverns: https://news.ycombinator.com/item?id=47160599reply",
      "While hydrogen fuel cell technology may not hold a distinct competitive advantage in the passenger vehicle market\u2014where battery electric vehicles have achieved greater maturity in infrastructure and cost reduction\u2014it retains significant merits in heavy-duty trucking and stationary power generation applications. This is particularly true when \"grey hydrogen\" (industrial by-product hydrogen derived from processes such as steam methane reforming or chlor-alkali production, rather than electrolysis powered by renewable energy) is readily available at competitive prices.Under such conditions, the total cost of ownership for fuel cell systems can achieve parity with, or even fall below, that of lithium-ion battery solutions. Furthermore, when accounting for the end-of-life considerations\u2014where fuel cells present fewer recycling challenges and material recovery complexities compared to the substantial battery waste stream associated with electrochemical energy storage\u2014hydrogen fuel cells emerge as a fundamentally more sustainable and economically viable long-term solution.reply",
      "Compressed hydrogen and cryogenic liquefaction also present explosive/BLEVE risks. Metal-hydride is probably the only reasonably safe-ish option. Other issues (like hydrogen embrittlement, leaking, slower flow-rate) are all very real challenges, but 'solvable'. Solving all of them at a price that consumers/businesses can stomach is quite debatable.Batteries are just too good nowadays to expect hydrogen to receive the level of R&D and infrastructure investment to become at all competitive.reply",
      "Good article, as always hydrogen for transport is dead. Unfortunately, as they say, what is dead can never die. And there will always be companies trying to siphon off public funds to do \u201ctrial runs\u201d.One thing that seems wrong is in the efficiency comparison: step 1 for hydrogen should be grid transmission, not electrolyzer.Also, how come the BEV price doesn\u2019t adjust in response to electricity prices (not that it would impact the result).reply",
      "This analysis does not account for side benefit of the oxygen. If you split water to get hydrogen, then for every kilogram of hydrogen you get, you also get 8 kg of oxygen. Liquid oxygen is not an expensive commodity, its market price is about $1/kg, but in this context this makes a difference. For example, in the first infographic, the cost of green hydrogen produced today is listed as \u00a316.97 which is about $23. If you can recoup $8 from this by selling the oxygen, or even only $5, then this makes a difference. If you select green H2 with 2030 assumptions, you get \u00a37.67, or about $10s. If you sell the oxygen at $5, you basically get the hydrogen at half price, and this makes the hydrogen powered truck slightly more economical than the battery powered one.reply",
      "The current market price is based on current supply and demand. Splitting water to create enough hydrogen for non-trivial fraction of the transportation sector would generate an enormous amount of oxygen. The price of oxygen would likely tank in that situation.reply",
      "Except you can make oxygen for pretty cheap using oxygen concentrators. The technology is simple enough that home versions exist for patients with lung problems can lug one around at all times to have a feed of oxygen rich air. Oxygen is almost 21% of the air we breathe, it's trivial to capture. Hydrogen counts for only 0.000055%.reply",
      "This is a very poor analysis, since it doesn\u2019t account for the capital costs.  Even if hydrogen is inefficient compared to batteries, it could win if the upfront investment was low enough to offset the additional fuel cost.  This is quite obvious, since that\u2019s why diesel trucks are winning today \u2014 the upfront cost of a diesel engine is cheap enough that it offsets the higher lifetime fuel costs.I do think that batteries will win, but the correct argument is one that shows that capital costs of batteries are going down faster than the cost of hydrogen production.reply",
      "As much as I find H2 fuel cell technology - which is a type of a gas based electric battery with no moving parts - fascinating, I can't help but wonder if we would be better off just running hybrids on e-fuels.e-fuels are just low quality gasoline, IIUC, made by (waves hands) ethical means from thin air using electricity. They still generate NOx gases, but ICEs just take them as is, and they're much more energy dense compared to long range batteries.The only real problem is that there don't seem to be many green and scalable means to produce them, but if we could, I think it can be an overall better alternative to seemingly unworkable hydrogen based EVs and/or unrecyclable battery based EVs.reply"
    ],
    "link": "https://www.mikeayles.com/blog/hydrogen-refuelling-road-freight/",
    "first_paragraph": ""
  },
  {
    "title": "Large-Scale Online Deanonymization with LLMs (simonlermen.substack.com)",
    "points": 193,
    "submitter": "DalasNoin",
    "submit_time": "2026-02-24T17:18:17 1771953497",
    "num_comments": 164,
    "comments_url": "https://news.ycombinator.com/item?id=47139716",
    "comments": [
      "I post under my real name here, pretty much the only place I post. It keeps me honest and straight in what I say when I choose to say it. I tried talking to my children about leaving as clean of a footprint on the internet as one can in anticipation of future people/systems taking that into consideration. I don't know what it will be but I would expect some adversarial stuff. Trying to keep clean is what I'd prefer for myself and my kids.On other hand, the Neal Stephenson's Fall or, Dodge in Hell book has an interesting idea in early phase of the book where a person agrees to what we now know \"flood the zone with sh*t\" (Steve Bannon's sadly very effective strategy) to battle some trolls. Instead of trying to keep clean, the intent is just to spam like crazy with anything so nobody understands the core. It is cleverly explored in the book albeit for too short of a time before moving into the virtual reality. I think there are a few people out here right now practicing this.reply",
      "I view posting online with a real name like getting a permanent tattoo.My values or priorities may significantly change over decades, especially as a child, so why would I want to jeopardize the reputation of a potential future identity with something I may post today?reply",
      "> I tried talking to my children about leaving as clean of a footprint on the internet as one can in anticipation of future people/systems taking that into consideration.I don\u2019t think you\u2019re wrong, but the fact that people consider it inevitable we\u2019ll all have an immutable social acceptance grade that includes everything from teenage shitposts to things you said after a loved one died, or getting diagnosed with cancer, makes me regret putting even a moment of my professional energies towards advancing tech in the US.reply",
      "I think he's wrong and I'm willing to say that. The ability for people to move beyond the fundamental attribution error is well known and takes major resources to correct that. For anyone that posts a comment, assuming you want to have easy attribution later is that you must future proof your words. That is not possible and it is extremely suppressive to express yourself.For example: \n\"Ellen Page is fantastic in the Umbrella Academy TV show\" \nInnocent, accurate, support, and positive in 2019.Same comment read after 1 Dec 2020 (Transition coming out): Insensitive, demeaning, in accurate.reply",
      "> That is not possible and it is extremely suppressive to express yourself.Also for the fact that you cannot predict how future powers will view past comments - for instance, certain benign political views 20 years ago could become \"terroristic speech\" tomorrow.I operate by a simple, general rule - I don't often say anything online I wouldn't say directly to someone's face in real life.reply",
      "This is very import: you don't know how the cancelation culture will be in 20 years.I like to use the example of a guy who did a blackface in a party back in 2000's. Although reprehensible, was not commom-sense racism back then. Today society sees it as completely unacceptable.Eventually that guy became prime minister of Canada and things went pretty bad when that photo surfaced decades later.Is it far to judge someone's actions by the lens of a different culture? When the popular opinion comes, they won't care about historical context.reply",
      "> I operate by a simple, general rule - I don't often say anything online I wouldn't say directly to someone's face in real life.More people should keep this same energy. I try to stress this to my kids and it feels like it's falling on deaf ears in regards to my teen. Alas.reply",
      "I can be a rude prick online sometimes, but I can be in real life too - basically though the reason I do this is I never want it to be some huge surprise IRL if someone sees what I write online and be like, \"wow, I didn't know that about him.\" I'm pretty much what I am online and IRL the same. For some reason this seems to matter for me, at least in the past when people have tried to like, send employers stuff I may have written online. The reaction is like \"oh, yea, we knew that already about him.\"Nothing terrible, maybe slightly embarrassing, but you know how online spaces can be. just be yourself basically, at least I try to be.reply",
      "Your framing is interesting. You may feel that you can\u2019t change who you are in real life, but people have a choice on how they behave online (or choose not to engage at all). So you could choose to be nice (or at least not a jerk); I\u2019m pretty sure you wouldn\u2019t get people writing to your employer complaining. I\u2019d argue that if you know you\u2019re sometimes a jerk, it\u2019d be less stressful for you and others if you didn\u2019t bring that energy online.reply",
      "I think the problem with this, especially amongst younger people, is having spent so much time online, they don't know where to draw this line anymore.reply"
    ],
    "link": "https://simonlermen.substack.com/p/large-scale-online-deanonymization",
    "first_paragraph": ""
  },
  {
    "title": "Learnings from 4 months of Image-Video VAE experiments (linum.ai)",
    "points": 71,
    "submitter": "schopra909",
    "submit_time": "2026-02-24T18:59:31 1771959571",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=47141107",
    "comments": [
      "As someone currently working on their own VAE, you reasoning for why you went with WAN 2.1 and your learnings for what you think you did wrong really resonated with me, specifically:> Looking back, we should have just filtered out these samples from the dataset and moved on.I hadn't even considered to look and see if poor data quality was resulting in an inability to recreate. This is a good gotchya to look out for. Appreciate the deep dive here!reply",
      "Hi HN, I\u2019m one of the two authors of the post and the Linum v2 text-to-video model (https://news.ycombinator.com/item?id=46721488). We're releasing our Image-Video VAE (open weights) and a deep dive on how we built it. Happy to answer questions about the work!reply",
      "No questions but I appreciate the write-up! Thank you for sharing.reply",
      "Very nice well written article!The kind that I like so much on HN. It tickle your mind but is still clear enough for an advanced beginner.reply",
      "its cool to see the iterative improvements to your model laid out, but for everything that workedm i imagine there were at least a million other things you also tried but didnt work out. whats your process of trying these different techniques/architectures? do you just wait for one experiment to finish and visually inspect the results everytime. seems hard since these take a while to train. how do you shorten the feedback loop in this space?reply",
      "honestly, it's really hard to shorten the feedback loop in this space. For this, we really just did run one experiment at a time and visually inspect the results everywhere. when you're going 0 -> 1, you're looking for \"signs of life\" to make sure the basic thing is working. when it comes to testing which (of the infinite levers) to the pull, a lot of it comes from intuition (which i know isn't the most fun answer). we spent a week or so just running experiments on the amount of compression we could squeeze out the VAE without significant degradation in the final results). In hindsight, spending a week on that seems like a waste, since we got the 8x spatial, 4x compression within the first 1-2 days. But in the moment, you're often unsure WHAT will be the key unlock. So, when you're in the middle of storm you're running a quick bayesian process in your head, measuring what you might learn from the outcome of the experiment vs. the time/money it would take to run the experiment. And you, hope that your intuitions become stronger over time, as you take more repetitions. More money, might help the problem (e.g. parallel experiments, more detailed explorations). But, I don't think money is a cure-all. At some point, you get lost in the sauce trying to tie the threads between all the empirical findings you have at your finger tips. Maybe one day AI models could help here integrating these all results. As it stands, they still struggle to reason about this stuff, in context of other research papers and findings (likely because all the context on arxiv is so noisy; you can't trust any particular finding and verifying findings is so hard to do, that it's hard to meta-reason about your experiments correctly).reply",
      "This seems like a great model to experiment fine tuning with original art, given it\u2019s relatively small and with open license. Is that a fair assessment?Thanks for the great write up and making it available to us all.reply",
      "yep, Apache 2.0! so anyone's welcome to download and hack awayreply",
      "Nice summary! I missed the mention of EQ-VAE when it comes to generation quality. Tiny trick, huge impact! Have you tried it?reply",
      "Hadn\u2019t seen that before! Seems very in line with what with the broader points about regularization. In table 4 they show faster convergence in 200 epochs when used alongside REPA. I\u2019d be curious to see if it ended up beating REPA by itself with full 800 epochs of training \u2014 or if something about this new latent space, leads to plateauing itself (learns faster but caps out on expressivity). We\u2019ve seen that phenomena before in other situations (eg UNET learns faster than DiT because of convolutions, but stops learning beyond a certain point).reply"
    ],
    "link": "https://www.linum.ai/field-notes/vae-reconstruction-vs-generation",
    "first_paragraph": "Learnings from 4 months of Image-Video VAE experimentsModern video generation relies on diffusion transformers, but attention scales quadratically so pixel space calculations are intractable. A VAE (Variational Autoencoder) solves this by compressing images and videos into a compact latent space for the diffusion model to operate in. Today we're open-sourcing our Image-Video VAE, our experiment logs, and a key finding: better compression doesn't always track with VAE stability or downstream generation quality.We spent July through November of 2024 training our own Image-Video VAE \u2014 fighting through months of NaNs, mysterious splotches, and co-training instability in the pursuit of better reconstruction quality, which (as it turns out) isn't as important as we thought.While we ended up using Wan 2.1's VAE for our most recent text-to-video model (more on that later), we still think there's a lot to learn from the process of building a VAE given how important they are to latent diffusion "
  },
  {
    "title": "Dissecting the CPU-memory relationship in garbage collection (OpenJDK 26) (norlinder.nu)",
    "points": 45,
    "submitter": "jonasn",
    "submit_time": "2026-02-24T13:49:39 1771940979",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=47137140",
    "comments": [
      "Hi HN, I'm the author of this post and a JVM engineer working on OpenJDK.I've spent the last few years researching GC for my PhD and realized that the ecosystem lacked standard tools to quantify GC CPU overhead\u2014especially with modern concurrent collectors where pause times don't tell the whole story.To fix this blind spot, I built a new telemetry framework into OpenJDK 26. This post walks through the CPU-memory trade-off and shows how to use the new API to measure exactly what your GC is costing you.I'll be around and am happy to answer any questions about the post or the implementation!reply",
      "Thank you for this interface! It will definitely help in tracking down GC related performance issues or in selecting optimal settings.One thing that I still struggle with, is to see how much penalty our application threads suffer from other work, say GC. In the blog you mention that GC is not only impacting by cpu doing work like traversing and moving (old/live) objects but also the cost of thread pauses and other barriers.How can we detect these? Is there a way we can share the data in some way like with OpenTelemetry?Currently I do it by running a load on an application and retaining its memory resources until the point where it CPU skyrockets because of the strongly increasing GC cycles and then comparing the cpu utilisation and ratio between cpu used/work.Edit: it would be interesting to have the GC time spent added to a span. Even though that time is shared across multiple units of work, at least you can use it as a datapoint that the work was (significantly?) delayed by the GC occurring, or waiting for the required memory to be freed.reply",
      "Thanks for reading! Your current method, pushing the load until the GC spirals and then comparing the CPU utilization, is exactly the painful, trial-and-error approach I'm hoping this new API helps alleviate.You've hit on the exact next frontier of GC observability. The API in JDK 26 tracks the explicit GC cost (the work done by the actual GC threads). Tracking the implicit costs, like the overhead of ZGC's load barriers or G1's write barriers executing directly inside your application threads, along with the cache eviction penalties, is essentially the holy grail of GC telemetry.I have spent a lot of time thinking about how to isolate those costs as part of my research. The challenge is that instrumenting those barrier events in a production VM without destroying application throughput (and creating observer effects) is incredibly difficult. It is absolutely an area of future research I am actively thinking about, but there isn't a silver bullet for it in standard HotSpot just yet.Something that you could look at there are some support to analyze with regards to thread pauses is time to safepoint.Regarding OpenTelemetry. MemoryMXBean.getTotalGcCpuTime() is exposed via the standard Java Management API, so it should be able to hook into this.reply",
      "Sorry if this is obvious to Java experts, but much as parallel GC is fine for batch workloads, is there a case for explicit GC control for web workloads? For example a single request to a web server will create a bunch of objects, but then when it completes 200ms later they can all be destroyed, so why even run GC during the request thread execution?reply",
      "Most web request cases where you care about performance probably have multiple parallel web requests, so there\u2019s no clean separation possible?reply",
      "At my work, one thing that I've often had to explain to devs is that the Parallel collector (and even the serial collector) are not bad just because they are old or simple.   They aren't always the right tool, but for us who do a lot of batch data processing, it's the best collector around for that data pipeline.Devs keep on trying to sneak in G1GC or ZGC because they hyper focus on pause time as being the only metric of value.  Hopefully this new log:cpu will give us a better tool for doing GC time and computational costs.  And for me, will make for a better way to argue that \"it's ok that the parallel collector had a 10s pause in a 2 hour run\".reply",
      "Every GC algorithm in HotSpot is designed with a specific set of trade-offs in mind.ZGC and G1 are fantastic engineering achievements for applications that require low latency and high responsiveness. However, if you are running a pure batch data pipeline where pause times simply don't matter, Parallel GC remains an incredibly powerful tool and probably the one I would pick for that scenario. By accepting the pauses, you get the benefit of zero concurrent overhead, dedicating 100% of the CPU to your application threads while they are running.reply",
      "Gotta be honest, I have a hard time arguing for G1 over ZGC.  It seems to me like any situation you'd want G1 you probably want ZGC instead.  That default 200ms target latency is already pretty long.  If you've made that tradeoff for G1 because you wanted lower latency, you probably are going to be happier with ZGC.I also find that the parallel collector is often better than G1, particularly for small heaps.  With modern CPUs, parallel is really fast.  Those 200ms pauses are pretty easy to achieve if you have something like a 4gb heap and 4 cores.The other benefit of the parallel collector is the off heap memory allocation is quiet low.  It was a nasty surprise to us with G1 how much off heap memory was required (with java 11, I know that's gotten a lot better).reply",
      "> This freed programmers from managing complex lifecycle management.It also deceived programmers into failing to manage complex lifecycles. Debugging wasted memory consumption is a huge pain.reply",
      "Are there plans to elucidate implicit GC costs as well?reply"
    ],
    "link": "https://norlinder.nu/posts/GC-Cost-CPU-vs-Memory/",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: I ported Tree-sitter to Go (github.com/odvcencio)",
    "points": 186,
    "submitter": "odvcencio",
    "submit_time": "2026-02-25T18:28:37 1772044117",
    "num_comments": 78,
    "comments_url": "https://news.ycombinator.com/item?id=47155597",
    "comments": [
      "Oh this is really neat for the Bazel community, as depending on tree-sitter to build a gazelle language extension, with Gazelle written in Go, requires you to use CGO.Now perhaps we can get rid of the CGO dependency and make it pure Go instead.\nI have pinged some folks to take a look at it.reply",
      "would also be nice to have this support gopackagesdriver backendreply",
      "thanks so much for the note! i really appreciate it. i built this precisely for folks like yourself with this specific pain, thanks again!reply",
      "Wouldn't `got` be confused with OpenBSD's Got: https://gameoftrees.org/index.htmlreply",
      "Why would people be confused with something that the vast majority never heard of. Naming shouldn't care about none mainstream project.reply",
      "oh wow! i really thought i was being too clever but i shouldve assumed nothing new under the sun. well im taking name suggestions now!reply",
      "Don't forget to check the new name with Claude:Are there revision control systems named \"got\"?Searched the web\nSearched the webYes! There is at least one notable revision control system named \"Got\" \u2014 specifically Game of Trees (Got), developed for OpenBSD.Game of Trees (Got) is a version control system which prioritizes ease of use and simplicity over flexibility. \nGameoftrees\n It's actively developed \u2014 version 0.123 was released on February 25, 2026 \nGameoftrees\n, just yesterday.Got uses Git repositories to store versioned data. At present, Got supports local version control operations only, and Git can be used for any functionality which has not yet been implemented in Got. It will always remain possible to work with both Got and Git on the same repository. \nLobstersSo \"Got\" is essentially a friendlier, simpler front-end to Git's underlying storage format, designed with OpenBSD's philosophy of simplicity and clean code in mind.reply",
      "Well, find and sed have modern \"fd\" and \"sd\" alternatives. Naming it \"gt\" allows you to claim that your version save 33% compared to typing \"git\".reply",
      "Goty McGotfacereply",
      "gotgit, gotchareply"
    ],
    "link": "https://github.com/odvcencio/gotreesitter",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Pure Go tree-sitter runtime\n      Pure-Go tree-sitter runtime \u2014 no CGo, no C toolchain, WASM-ready.Implements the same parse-table format tree-sitter uses, so existing grammars work without recompilation. Outperforms the CGo binding on every workload \u2014 incremental edits (the dominant operation in editors and language servers) are 90x faster than the C implementation.Every existing Go tree-sitter binding requires CGo. That means:gotreesitter is pure Go. go get and build \u2014 on any target, any platform.Tree-sitter's S-expression query language is supported, including predicates and cursor-based streaming. See Known Limitations for current caveats.After the initial parse, re-parse only the changed region \u2014 unchanged subtrees are reused automatically.Tip: Use grammars.DetectLanguage(\"main.go\") to pick the right grammar by filename \u2014 usefu"
  },
  {
    "title": "How to fold the Blade Runner origami unicorn (1996) (archive.org)",
    "points": 257,
    "submitter": "exvi",
    "submit_time": "2026-02-22T22:42:12 1771800132",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=47115574",
    "comments": [
      "I just read about the backstory. Gaff (Edward James Olmos) put the unicorn where Deckard would find it as a message. He was gently informing Deckard that he was a replicant. Deckard had just dreamed about the unicorn and told nobody, so the only way Gaff could know is if he knew which dreams Deckard had implanted in his memory.This came from Ridley Scott, not Philip K. Dick.I am impressed that Scott was so subtle about this for so long. It would have been a short-term boost to hit us over the head with it, as in The Sixth Sense. But being coy about it helped to make the movie a true classic.reply",
      "Better explanation\nhttps://geekydadcrafts.com/2019/01/10/the-blade-runner-unico...reply",
      "I'm having problems interpreting step 23, to arrive at 24.The archived Japanese instruction wants to unfold the paper entirely, and then ... what? I'm stumped.reply",
      "\"Diagrammed by Kenneth Thompson\".  The name sounds familiar...reply",
      "Thanks. These are waaay better instructions.reply",
      "I used to fold an origami unicorn design by Marc Kirschenbaum.  I can't find any instructions on the Modern Internet, but I used to fold it out of gum-wrappers while sitting in class.The unicorn from the film itself wasn't \"true\" origami, being a prop consisting of several pieces glued together, but it really popularized the idea of an origami unicorn and a number of the current designs were prompted by it.reply",
      "Are we losing old websites like that?I actually was unaware that this warranted a website. When I was young, I had one origami book. I completed it to about 40%; wasn't too bad but was far away from being really good. Origami is quite an art. These days I tend to watch youtube videos more than look at oldschool books but I loved that old handbook. Never folded a unicorn though.reply",
      "I also had a small handbook that was given to me by a big brother figure. I spent so much time with that handbook which was very beaten up, its cover fallen off etc. fun memories. Seeing origami reminds me of that time when I was 5-6 years oldreply",
      "I recently picked up an origami book and started practicing in dull moments. \nI highly recommend it for anyone struggling with phone addiction.reply",
      "Can you share any tips on good origami books for beginners?reply"
    ],
    "link": "https://web.archive.org/web/20011104015933/www.linkclub.or.jp/~null/index_br.html",
    "first_paragraph": ""
  },
  {
    "title": "Quasi-Zenith Satellite System (wikipedia.org)",
    "points": 3,
    "submitter": "teleforce",
    "submit_time": "2026-02-22T07:30:17 1771745417",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://en.wikipedia.org/wiki/Quasi-Zenith_Satellite_System",
    "first_paragraph": ""
  },
  {
    "title": "Following 35% growth, solar has passed hydro on US grid (arstechnica.com)",
    "points": 410,
    "submitter": "rbanffy",
    "submit_time": "2026-02-25T16:44:54 1772037894",
    "num_comments": 340,
    "comments_url": "https://news.ycombinator.com/item?id=47154009",
    "comments": [
      "The trump administration by refusing to admit the superior metrics of solar, they're just burying their heads in sand.As admitting that solar is now a superior and cost effective means of energy means admitting that the US is no longer top dog.As empires are built on mastering a source of energy.the Portuguese | Dutch - mastered wind to power their ships.the British mastered coal to power Industrial Revolution.America mastered oilnow the Chinese have Solar.even in places like Africa etc -- places were the grid was never available for $2k -- you can power your whole house with solar and lithium batteries. Panels are getting cheaper, same as batteries. Once the tipping point is reached for electric vehicles both personal and commercial - transition to fully electric mobility happensreply",
      "> The trump administration by refusing to admit the superior metrics of solar, they're just burying their heads in sand.I don't think I agree with this as it suggests they are doing it because they can't be bothered about it. Instead, they are doing it specifically because their (and/or their friend's) pockets are getting filled. To me, the latter is much more sinister.reply",
      "It such obvious corruption. Trump ordered the pentagon to buy coal power specifically.reply",
      "The crazy thing is coal mining is 40,000 jobs. I have never seen such a tiny industry given such preferential/oversized treatment.https://fred.stlouisfed.org/series/CES1021210001Lyft is 10% of the size of big coal and Amazon is over 20x larger.reply",
      "Meanwhile the areas where the coal jobs used to exist (or still do) just had their food stamps removed or reduced. It's such a charade.reply",
      "Chinese also have battery manufacturing, whose rapidly falling cost-curve is what is missing to enable 24/7 solar.American empire ruled with the petrodollar. Chinese will rule with the solaryuan if we don't get our shit together.reply",
      "You just slightly missed the crux of the issue here.The big \"problem\" with renewables like solar is that once you've installed enough for yourself you are done for like 30 years.  There is no monthly sun fee you need to keep paying.  There is no solardollar, because there's nothing that needs to be extracted, transported, and sold every single day.  A lot of billionaires are in an existential crisis over a world where fossil fuels are no longer the driving force of the economy.  That's why we have incessant propaganda against renewable energy.Even the solar panel market is self defeating.  Once there is enough installed power the demand will drop off sharply as the refresh cycle is too long.  The feedback loop of capitalism means we are likely to reach that point sooner than you would expect.That said, don't think I'm like the nuclear power guys of the 50s who claimed that electricity would be so abundant that we wouldn't even bother to meter it.  There are still costs with maintenance, repair, administration, debt servicing, and profits.  If you look at your power bill today it will probably list generation, distribution, and taxes.  Renewables only eliminate the generation costs, which are usually about half of the bill.reply",
      "> Even the solar panel market is self defeating. Once there is enough installed power the demand will drop off sharply as the refresh cycle is too long.If the average panel lifetime is 25 years, and it takes > 25 years to reach \"full capacity\" (whatever that might mean or whatever level that is at), then by definition there will be a continuous cycle of panel replacement taking place.It's not as if we get all the PV installed in 12 months and then it lasts for 25 years ...reply",
      "The 25 year thing comes from the 25 year warranties - they\u2019re generally warrantied to be at 80% power capability at 25 years. I don\u2019t know the real lifetime, but presumably it\u2019s a lot longer than 25 years. And by that point, maybe we\u2019ll have the deuteriumdollar\u2026reply",
      "The first set of PV panels I put on our van didn't even make their 10 year warranty @ 80%. Anecdata, sure, but still data.reply"
    ],
    "link": "https://arstechnica.com/science/2026/02/final-2025-data-is-in-us-energy-use-is-up-as-solar-passes-hydro/",
    "first_paragraph": "\n        Coal makes a bit of a comeback, if only by accident.\n      On Tuesday, the US Energy Information Administration released full-year data on how the country generated electricity in 2025. It\u2019s a bit of a good news/bad news situation. The bad news is that overall demand rose appreciably, and a fair chunk of that was met by additional coal use. On the good side, solar continued its run of astonishing growth, generating 35 percent more power than a year earlier and surpassing hydroelectric power for the first time.Overall, electrical consumption in the US rose by 2.8 percent, or about 121 terawatt-hours. Consumption had been largely flat for several decades, with efficiency and the decline of industry offsetting the effects of population and economic growth. There were plenty of year-to-year changes, however, driven by factors ranging from heating and cooling demand to a global pandemic. Given that history, the growth in demand in 2025 is a bit concerning, but it\u2019s not yet a clear "
  },
  {
    "title": "Access to a Shared Unix Computer (tilde.club)",
    "points": 47,
    "submitter": "TigerUniversity",
    "submit_time": "2026-02-22T11:07:25 1771758445",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=47110066",
    "comments": [
      "Multi-user Unix?  What will they think of next?This is cool, though. Gives people a taste of what it used to be like with everyone in the university logged into the big time-sharing machines all together.reply",
      "> Multi-user Unix?We could call it Multics!But yeah, I remember those glory days of everybody on the school's Sun 3/280, when an accidental fork bomb would ruin everyone's homework.reply",
      "I kinda wish it stayed that way, or rather something better replaced multi-user systems as they aren't well suited to personal computers like we have nowadays. Plus I like the added bonus of not needing to spend much to have access to the kind of compute power needed for a compsci course, it makes compiling a lot fasterreply",
      "You apparently never had to share a 3B20 (around 1 MIP) with 200 other CS1401 students desperately trying to get their Pascal project to compile before the midnight deadline.  15m for 'hello world'?  If you're lucky.reply",
      "I believe it runs Linux. [This](https://medium.com/message/tilde-club-i-had-a-couple-drinks-...) is the story behind tilde.club.reply",
      "The editorialized title brings back memories of logging into smashthestack almost two decades ago leaving and looking at graffiti on the walls.I didn't learn much following the link itself other than it is supposed to be pronounced TIL-dee and that some people have more money to spare than others.reply",
      "Apparently there are a few more similar communities like the one from the posthttps://tildeverse.org/members/reply",
      "Similar to SDF.org and envs.netreply",
      "https://tilde.team FTWreply",
      "It has been years since I have heard of this. Looks great!reply"
    ],
    "link": "http://tilde.club/",
    "first_paragraph": "Questions? See the official FAQ.Wishing everyone a warm, restful holiday season (or at least one with fewer surprises than a random package upgrade). Whether you\u2019re traveling, staying put, coding something wonderfully unnecessary, or just lurking with a mug of something hot, I hope you get a little peace and a lot of cozy.\n\nThanks for being part of what makes this place special: the creativity, the kindness, the weird little projects, and the steady reminder that the internet can still be human.\n\nMerry Christmas to those who celebrate, and happy holidays to everyone. See you on the other side of the calendar.\n\n~deependI will be upgrading the system to Fedora 43 on December 20, 2025. There is no specific start time. A 10-minute warning will be posted in the terminal before the upgrade begins. Expect brief downtime during the upgrade and reboot.\n\nIn preparation for this, IMAP will be upgraded during the next week due to breaking changes in Dovecot. There may be brief interruptions to mai"
  }
]