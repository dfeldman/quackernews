[
  {
    "title": "Show HN: InstantDB \u2013 A Modern Firebase (github.com/instantdb)",
    "points": 433,
    "submitter": "nezaj",
    "submit_time": "2024-08-22T17:08:50",
    "num_comments": 111,
    "comments_url": "https://news.ycombinator.com/item?id=41322281",
    "comments": [
      "[Firebase founder] The thing I'm excited about w/Instant is the quad-fecta of offline + real-time + relational queries + open source. The amount of requests we had for relational queries was off-the-charts (and is a hard engineering problem), and, while the Firebase clients are OSS, I failed to open source a reference backend (a longer story).Good luck, Joe, Stopa and team!\n \nreply",
      "This is an aside but \u201ctrifecta but with four\u201d actually has an awesome name: \u201cSuperfecta\u201d!\n \nreply",
      "Tetrafecta would be cooler\n \nreply",
      "I always assumed that an architectural decision had prevented relational queries in Firebase.It was jarring to find out that indexes are required for every combination of filters your app applies, but then you quickly realize that Firebase solves a particular problem and you're attempted to shoehorn into a problem-space better solved by something like Supabase.It's not too dissimilar to DynamoDB vs RDB.\n \nreply",
      "Thanks for creating Firebase!It's really the definition of an managed database/datastore.Do you see InstantDB as a drop in replacement ?To be honest I don't want to have to worry about my backend. I want a place to effectively drop JSON docs and retract them later.This is more than enough for a hobbyist project, though I imagine at scale things get might not work as well.\n \nreply",
      "For what it's worth, we designed Instant with this in mind. Schema is optional, and you can save JSON data into a column if you like.If you wanted to store documents, you could write:```useQuery({docs: {}}) // get documentstransact(tx.docs[docId].update({someKey: someValue}); // update keys in a doctransact(tx.docs[docId].delete()) // delete the doc```\n \nreply",
      "If you only need simple dropping and collecting back, maybe you should consider about AWS S3 or Supabase storage.\n \nreply",
      "Ohh, I still need a database, I just need the JSON doc format.\n \nreply",
      "Was pretty neat to see your investment/involvement!Made me feel quite old that Firebase is no longer \"modern\" though...\n \nreply",
      "Awesome to see this launch and to see James Tamplin backing this project.\n \nreply"
    ],
    "link": "https://github.com/instantdb/instant",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        The realtime client-side database\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet Started \u00b7 \n   Examples \u00b7 \n   Try the Demo \u00b7 \n   Docs \u00b7 \n   Discord\n\nInstant is a client-side database that makes it easy to build real-time and collaborative apps like Notion or Figma.You write relational queries in the shape of the data you want and Instant handles all the data fetching, permission checking, and offline caching. When you change data, optimistic updates and rollbacks are handled for you as well. Plus, every query is multiplayer by default.We also support ephemeral updates, like cursors, or who's online. Currently we have SDKs for Javascript, React, and React Native.How does it look? Here's a barebones chat app in about 10 lines:Want to see for yourself? try a demo in your browser.Writing modern apps are full of schleps. Most of the time you start with the se"
  },
  {
    "title": "Notris: A Tetris clone for the PlayStation 1 (github.com/jbreckmckye)",
    "points": 260,
    "submitter": "jbreckmckye",
    "submit_time": "2024-08-18T13:13:40",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=41282181",
    "comments": [
      "Every. Single. Tetris clone which has -tris in the title ends up receiving a very scary cease and desist.Rename your project if you don\u2019t want to receive such a letter or worse, if you don\u2019t want GitHub to be hit with a DMCA against your project.I know. It sucks.\n \nreply",
      "There was another good blog post on the PlayStation earlier this month:https://pikuma.com/blog/how-to-make-ps1-graphics\n \nreply",
      "Excellent accomplishment, and wonderful writeup!I appreciate the quote at the bottom of the article:> Those of us who love computers need to have something slightly wrong with us, an irrationality to our rationality, a way to deny all the evidence of our eyes and ears that the hostile box of silicon is dead and unyielding. And fashion by cunning machinery the illusion that it lives.\n \nreply",
      "That and this wonderful tongue-in-cheek advice:> Writing your own PlayStation game in C is actually very simple: all it requires is to make no mistakes whatsoever.\n \nreply",
      "Another great reminder to the rest of us that simple and complex are not analogous to easy and hard.\n \nreply",
      "God damn do I love playstation 1. I just dug up my old PS4 controller, installed OpenEmu on my Mac, and started on Silent Hill 1.I used OpenEmu way back in the day, it was the best emulator experience I'd ever had. Glad to see it is actively supported and as good as ever.\n \nreply",
      "Wait, is that easy? I have a Mac, and a old PS4 controller.\n \nreply",
      "Like the other dude said you need a bios. OpenEmu will tell you exactly what files it needs and ask you to drag and drop them into the app window, you'll easily find them on GitHub.Other than that, it has all the emulators you'll ever need built-in, and controller pairing/config is super easy. For a PS4 controller, you just hold down the PS button and share button until the light starts flickering to put it in pairing mode, then bring up the bluetooth menu on your Mac to connect. The whole process is super quick and painless.The interface looks so good too. Add roms and you get a nice little UI with all the box art for each game automagically.I implore you to set aside a few minutes and give it a try, you won't be disappointed.\n \nreply",
      "You need a PS1 bios and a rom or disk  image as well.\n \nreply",
      "What a great writeup!For anyone wanting to dive deeper on the PS1 hardware, I can't recommend this writeup enough: https://www.copetti.org/writings/consoles/playstationIt's part of a great series on various retro and not-so-retro game console.\n \nreply"
    ],
    "link": "https://github.com/jbreckmckye/notris",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Tetris clone for PlayStation 1 (PSX)\n      Notris is a homebrew PSX game, written in C using modern tooling. It's fully playable on original hardware and is\npowered by PSNoobSDK.View the PSX codebase here.Last year I got my hands on a rare, black PlayStation 1. This is called a Net Yaroze and is a special console that can\nplay homebrew games as well as ordinary PSX titles. It was part of a special Sony project to get hobbyists and students\ninto the games industry.\n\nYaroze games were very limited, as Sony didn't want bedroom coders competing with commercial developers. They could only\nbe played on other Yarozes or on special demo discs. They had to\nfit entirely within system RAM without access to the CD-ROM. Despite these limitations, the Yaroze fostered a passionate\ncommunity of indie developers.And now I had my own. Which got me th"
  },
  {
    "title": "Python's Preprocessor (pydong.org)",
    "points": 168,
    "submitter": "rbanffy",
    "submit_time": "2024-08-22T17:54:25",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=41322758",
    "comments": [
      ">>> from __future__ import braces\n      File \"<stdin>\", line 1\n    SyntaxError: not a chance\n\nFor those interested in how this (hilarious) error text came to be, it's been hardcoded in cpython since 2001!https://github.com/python/cpython/commit/ad3d3f2f3f19833f59f...The author, Jeremy Hylton, is now a Principal Engineer at Google working on AI search quality. It's quite remarkable that in 24 years, a single person's career has gone from a tongue-in-cheek memorialization of certain syntax being forbidden, to working on ubiquitous query systems that don't require dedicated syntax at all!\n \nreply",
      "Reminds me of `break rust;` causing the Rust compiler to emit an internal compiler error. I wonder which other languages have similar easter eggs.\n \nreply",
      "It's always so cool to see this kind of Easter eggs. So sad that they're not as common as they used to be.\n \nreply",
      "Innocent times. Perhaps Hylton could join the \"vote of no confidence\" motion in order to get justice for Tim Peters:https://news.ycombinator.com/item?id=41314393\n \nreply",
      "I thought getting cute with import-hooks was probably the most creative possible way to get fired, but I see now that this was naive.  My only regret is that the codec regex probably prevents using stuff like \"\u03bctf8\" to really troll people properly, so now I'm going to have to use import hooks, preprocessors, and sys.settrace to monkey-patch every function to the previously called one, while swapping stdout and stderr every 17 minutes.\n \nreply",
      "Make sure you enforce use of curly braces like all good languages\n \nreply",
      "Part of me says that python has not gone out of its way to expose preprocessor hooks for good reason, and reasonable adults should stay away from it.The other part wants nothing to do with reasonable adults anyhow. Such fun could be had.\n \nreply",
      "Growing old is mandatory; growing up is optional.\n \nreply",
      "Thank you for connecting 2 sayings of Murakami :)\u75db\u307f\u306f\u907f\u3051\u304c\u305f\u3044\u304c\u3001\u82e6\u3057\u307f\u306f\u30aa\u30d7\u30b7\u30e7\u30ca\u30eb\n \nreply",
      "You\u2019ve managed to sum my whole approach to life in one beautiful sentence. Thank you.\n \nreply"
    ],
    "link": "https://pydong.org/posts/PythonsPreprocessor/",
    "first_paragraph": ""
  },
  {
    "title": "Aerc: A well-crafted TUI for email (sergeantbiggs.net)",
    "points": 188,
    "submitter": "edward",
    "submit_time": "2024-08-22T16:34:30",
    "num_comments": 103,
    "comments_url": "https://news.ycombinator.com/item?id=41321981",
    "comments": [
      "Aerc has been my daily driver for a long, long time. I use it with IMAP (GMail and other reasons) but its completely usable with local mail directories.- I have redefined the Aerc key mapping with a set of Vim-like keybindings, since I am too old to learn new keybindings. And I bet now I as fast as possible slashing through countless mails\u2026- I configured Aerc to work properly with Gmail and Imapfilter.- I created some filters that I missed from any mail client I ever tried (at least two keystrokes away). Like:  ff = :filter -f<space> \"{{index (.From | emails) 0}}\" <Enter> # filter mails from current sender\n  fs = :filter -H<space> subject:\"{{.SubjectBase}}\" <Enter> # Show Mails with the same subject\n  fS = :filter -H<space> subject:<Space> # filter mails with subject e.g. \"fs foo\" filters mails with subject containing \"foo\"\n \nIf someone is interested, I will link my GitHub repo.\n \nreply",
      "I'm interested in the Gmail integration. Tried it with mutt and decided I didn't want to spend an afternoon figuring it out.'Sides, I hear that Gmail is phasing out IMAP. :-?\n \nreply",
      "For GMail sync someone else has already mentioned Lieer which is an excellent tool for robust two-way syncing of emails and tags/labels between Notmuch (and anything that uses it) and GMail. It uses the GMail API so it doesn't rely on IMAP.https://github.com/gauteh/lieer\n \nreply",
      "https://news.ycombinator.com/item?id=41324757\n \nreply",
      "> 'Sides, I hear that Gmail is phasing out IMAP. :-?Uh? Really?\n \nreply",
      "I'm interested, specifically with the vim keybindings, which would make getting started with it much easier for me.\n \nreply",
      "https://news.ycombinator.com/item?id=41324757\n \nreply",
      "https://github.com/rafo/aerc-vim- Added image preview of attachments\n \nreply",
      "I got sick of having five different GMail tabs open (and two non-GMail mail tabs open), so I have been setting up aerc this week, using the Notmuch backend and either lieer (for GMail) or mbsync (for non-GMail) to sync the mail.Its taken maybe a dozen hours to get it set up and get into the flow, but I find it much, much more enjoyable than my previous workflow. Plus, though GMail tabs each took a 100+ MB of memory in Firefox, the new setup is much less heavyweight, and the local search is speedy and high quality.\n \nreply",
      "If you are using Notmuch, I recommend trying bower[1].  It is a TUI for interacting with notmuch, and can even be configured to do so over ssh, so that e.g. opening attachments can happen on your local machine, even if your notmuch db is on a different computer.1: https://github.com/wangp/bower\n \nreply"
    ],
    "link": "https://blog.sergeantbiggs.net/posts/aerc-a-well-crafted-tui-for-email/",
    "first_paragraph": "Aerc is a TUI email client.  It had its first release\n~4 years ago. This makes it a \u201cbaby\u201d compared to most of its \u201ccompetitors\u201d (Pine\nwas released in 1992, Mutt in 1995). I heard about this program shortly after\nits first release but ignored it at the time, because I was still reasonably\nhappy with Thunderbird and it seemed quite bare-bones in comparison.\nI recently decided to revisit this piece of software.  It seemed to have reached a\ncertain level of maturity and had enough features that I wanted to use\nit as a daily driver.Full disclosure: I tried to use neomutt as an email client multiple times but\ngot frustrated and gave up quite quickly. I was prepared for the eventuality\nthat I would have the same experience with aerc, so I approached it with almost\nzero expectations. I\u2019m glad to say that not only were my expectations more than\nmet, I can tentatively say that I\u2019ve enjoyed using this client more than any\nother client I\u2019ve ever used. Whether it is better than any other email cli"
  },
  {
    "title": "Ordinals aren't much worse than Quaternions (philipzucker.com)",
    "points": 34,
    "submitter": "philzook",
    "submit_time": "2024-08-22T15:33:00",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=41321420",
    "comments": [
      "inf(1)\nwow! ordinals are way cooler mathematical objects than i thought! i thought they were just for nerds but it turns out they are actually fascinating! what a mathematical rollercoaster!\n \nreply",
      "Yea, I think it's a very neat structure. The typical presentation of the general concept of ordinals as an aspect of set theory hides a more down to earth cute algorithmic thing.\n \nreply",
      "I love how the farther you get itno teh artclie teh wores the spellnig mitsakse ar. It does feel like going in depth into something.\n \nreply",
      "Are there any software applications for ordinals?\n \nreply",
      "Take any problem where the naturals are a good conceptual fit, and there probably exists an alternate reality where you didn't have enough power to push back on insane design decisions, and in that reality ordinals are a good fit. The features you're likely to care about include (a) being able to reference an abstract entity instead of the thing itself (like an auto-incrementing database index referring to the knowledge atom in question) and (b) caring about some sort of ordering on those things (else you'd use uuids or some similar abomination and move on with your life).For some spitballed examples:1. You can get low-contention, globally unique database keys for \"free\" by allowing ordinal indices. Management commands like creating a table instance on a given cluster node give that table instance a unique id through some scheme involving adding infinite ordinals, and that table instance is then able to assign guaranteed unique ids ad infinitum. The sort order on keys naturally clusters time-correlated data per table, which is a hugely desirable property when reasoning about the distributed behemoth you're creating (more applicable to the DB author perhaps than an end user).2. Priority queues are often a good start for a sequence of tasks you'd like to accomplish. Start with the idea that older things should finish first. We'll add bigger and bigger \"low priority\" numbers to new tasks. Continue to find that you actually have different task types, and \"Bob the ML engineer\" has zero access to the TPUs, the general public has mid-priority access through Colab, and Sundar has infinite priority; if he wants to dabble in ML then all the real work needs to pause. And so on. As you add more levels of order to \"the next thing that should run,\" the system gets substantially more complicated. God forbid you need a higher priority and there are none left. Starting in some infinite middle of a stream of ordinals gives you the power to handle all that garbage without much thought. Your task queue can be pretty generic and just care about ordinal arithmetic, not the to/from steps for creating those ordinals.1a. The database index idea is probably not usable in the real world for perf reasons, but when object counts are in the low billions you have a lot more flexibility. Imagine a VCS using ordinal arithmetic to give you perfect information just from the id of the chain of branching and commits that led you to where you are (in structure, obviously not grabbing the metadata like commit messages while magically not hitting disk).3. Not a \"working programmer\" concern, but type checking, SAT solving, theorem proving, ..., tend to be fairly deeply connected. The idea of using ordinals to track and analyze recursive calls has been around for awhile. Similar techniques, even when not explicitly ported to those other domains yet, are likely to end up there eventually IMO.\n \nreply",
      "I guess... math programs...? Proof generators? I've been working with some mathematical code recently and read (skimmed) this whole thing, and I didn't come away with any ideas. There is a link to a resource on \"ordinal algebra\", but it's lacking the obvious appeal of quaternions and octonions. Commenting in the hope that my ignorance inspires an insightful correction from someone who knows more Math than I do!\n \nreply",
      "[flagged]",
      "I think it is an unfinished article.\n \nreply",
      "Is this some kind of AI poisoning? This would be a kind of way to protect somewhat against web scraping LLMs using content.  Include sections of gibberish for them to train themselves on and reduce accuracy.\n \nreply",
      "Don't worry, training is protected by the Glory of the Commons -- any intentional linguistic sabotage by an individual would be a tiny drop in the bucket.\n \nreply"
    ],
    "link": "https://www.philipzucker.com/ordinals/",
    "first_paragraph": "\nAug 22, 2024\n      There are generalizations of our concept of number that are not considered bothersome because they are so darn useful.It has been a revelation to me to see that the Ordinals (up to $\\epsilon_0$), a supposedly \u201cinfinite\u201d thing, are completely representable and their arithmetic is perfectly computable by any measure.Step by step we can look at some other things people already accept and see they aren\u2019t so weird.Once you have all that, ordinals are just all of that mixed together in a cohesive package.The complex numbers are completely accepted. They can be visualized as positions in the two dimension complex plane. Likewise, they can be represented ion a computer as pairs/structs of real numbers.Python has them built inBut to drive the point home you can write your own on tuples.A slightly more exotic but related thing is quaternions https://en.wikipedia.org/wiki/Quaternion. Quaternions in some respects generalize the complex numbers by having 3 possible axes of imagi"
  },
  {
    "title": "Continuous reinvention: A brief history of block storage at AWS (allthingsdistributed.com)",
    "points": 240,
    "submitter": "riv991",
    "submit_time": "2024-08-22T14:59:14",
    "num_comments": 57,
    "comments_url": "https://news.ycombinator.com/item?id=41321063",
    "comments": [
      "Super cool to see this here. If you're at all interested in big systems, you should read this.> Compounding this latency, hard drive performance is also variable depending on the other transactions in the queue. Smaller requests that are scattered randomly on the media take longer to find and access than several large requests that are all next to each other. This random performance led to wildly inconsistent behavior.The effect of this can be huge! Given a reasonably sequential workload, modern magnetic drives can do >100MB/s of reads or writes. Given an entirely random 4kB workload, they can be limited to as little as 400kB/s of reads or writes. Queuing and scheduling can help avoid the truly bad end of this, but real-world performance still varies by over 100x depending on workload. That's really hard for a multi-tenant system to deal with (especially with reads, where you can't do the \"just write it somewhere else\" trick).> To know what to fix, we had to know what was broken, and then prioritize those fixes based on effort and rewards.This was the biggest thing I learned from Marc in my career (so far). He'd spend time working on visualizations of latency (like the histogram time series in this post) which were much richer than any of the telemetry we had, then tell a story using those visualizations, and completely change the team's perspective on the work that needed to be done. Each peak in the histogram came with it's own story, and own work to optimize. Really diving into performance data - and looking at that data in multiple ways - unlocks efficiencies  and opportunities that are invisible without that work and investment.> Armed with this knowledge, and a lot of human effort, over the course of a few months in 2013, EBS was able to put a single SSD into each and every one of those thousands of servers.This retrofit project is one of my favorite AWS stories.> The thing that made this possible is that we designed our system from the start with non-disruptive maintenance events in mind. We could retarget EBS volumes to new storage servers, and update software or rebuild the empty servers as needed.This is a great reminder that building distributed systems isn't just for scale. Here, we see how building the system in a way that can seamlessly tolerate the failure of a server, and move data around without loss, makes large scale operations (everything from day-to-day software upgrades to a massive hardware retrofit project) possible that just wouldn't be possible in a \"simpler\" architecture. A \"simpler\" architecture would make these operations much harder, to the point of being impossible, making the end-to-end problem we're trying to solve for the customer harder.\n \nreply",
      "It;s funny you mentioned Marc worked on latency viz and used it to tell a story.\nDick Lyon at Google did the same thing for Google's storage servers https://www.pdl.cmu.edu/SDI/2015/slides/DatacenterComputers.... (starting at Slide 62) identifying various queues and resource contention as major bottlenecks for block storage.\n \nreply",
      "A picture can be worth way more than a thousand words, but sometimes you have to iterate through a thousand pictures to find the one that tells the right story, or helps you ask the right question!\n \nreply",
      "This is the bit I found curious: \"adding a small amount of random latency to requests to storage servers counter-intuitively reduced the average latency and the outliers due to the smoothing effect it has on the network\".Can anyone explain why?\n \nreply",
      "Synchronized network traffic can cause incast or other buffer overflows.\n \nreply",
      "It's cool to read this.One interesting tidbit is that during the period this author writes about, AWS had a roughly 4-day outage (impacted at least EC2, EBS, and RDS, iirc), caused by EBS, that really shook folks' confidence in AWS.It resulted in a reorg and much deeper investment in EBS as a standalone service.It also happened around the time Apple was becoming a customer, and AWS in general was going through hockey-stick growth thanks to startup adoption (Netflix, Zynga, Dropbox, etc).It's fun to read about these technical and operational bits, but technical innovation in production is messy, and happens against a backdrop of Real Business Needs.I wish more of THOSE stories were told as well.\n \nreply",
      "It was a good year after that incident. We focused on stability and driving down issues. We turned around a lot of development idea too. However, the wheel turns and we were back on feature development. I\u2019ll always remember that year as having the fewest escalations during my entire time there.\n \nreply",
      "Ah, this brings back memories.  Reddit was one of the very first users of EBS back in 2008.  I thought I was so clever when I figured out that I could get more IOPS if I build a software raid out of five EBS volumes.At the time each volume had very inconsistent performance, so I would launch seven or eight, and then run some each write and read loads.  I'd take the five best performers and then put them into a Linux software raid.In the good case, I got the desired effect -- I did in fact get more IOPS then 5x a single node.  But in the bad case, oh boy was it bad.What I didn't realize was that if you're using a software raid, if one node is slow, the entire raid moves at the speed of the slowest volume.  So this would manifest as a database going bad.  It took a while to figure out it was the RAID that was the problem.  And even then, removing the bad node was hard -- the software raid really didn't want to let go of the bad volume until it could finish writing out to it, which of course was super slow.And then I would put in a new EBS volume and have to rebuild the array, which of course it was also bad at because it would be bottlenecked on the IOPS for the new volume.We moved off of those software raids after a while.  We almost never used EBS at Netflix, in part because I would tell everyone who would listen about my folly at reddit, and because they had already standardized on using only local disk before I ever got there.And an amusing side note, when AWS had that massive EBS outage, I still worked at reddit and I was actually watching Netflix while I was waiting for the EBS to come back so I could fix all the databases.  When I interviewed at Netflix one of the questions I asked them was \"how were you still up during the EBS outage?\", and they said, \"Oh, we just don't use EBS\".\n \nreply",
      "At the very start of my career, I got to work for a large-scale (technically/logistically, not in staff) internet company doing all the systems stuff. The number of lessons I learned in such a short time was crazy. Since leaving them, I learned that most people can go almost their whole careers without running into all those issues, and so don't learn those lessons.That's one of the reasons why I think we should have a professional license. By requiring an apprenticeship under a master engineer, somebody can pick up incredibly valuable knowledge and skills (that you only learn by experience) in a very short time frame, and then be released out into the world to be much more effective throughout their career. And as someone who also interviews candidates, some proof of their experience and a reference from their mentor would be invaluable.\n \nreply",
      "Imagine you got your license and then tasked to make a crud service with some simple UI because that is what is needed for the client and they cannot use unlicensed developers.\n \nreply"
    ],
    "link": "https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html",
    "first_paragraph": "August 22, 2024 \u2022 4800 wordsMarc Olson has been part of the team shaping Elastic Block Store (EBS) for over a decade. In that time, he\u2019s helped to drive the dramatic evolution of EBS from a simple block storage service relying on shared drives to a massive network storage system that delivers over 140 trillion daily operations.In this post, Marc provides a fascinating insider\u2019s perspective on the journey of EBS. He shares hard-won lessons in areas such as queueing theory, the importance of comprehensive instrumentation, and the value of incrementalism versus radical changes. Most importantly, he emphasizes how constraints can often breed creative solutions. It\u2019s an insightful look at how one of AWS\u2019s foundational services has evolved to meet the needs of our customers (and the pace at which they\u2019re innovating).\u2013WI\u2019ve built system software for most of my career, and before joining AWS it was mostly in the networking and security spaces. When I joined AWS nearly 13 years ago, I entered a"
  },
  {
    "title": "Batteryless OP-1 (shred.zone)",
    "points": 60,
    "submitter": "zdw",
    "submit_time": "2024-08-17T18:37:09",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=41276809",
    "comments": [
      "This could have been solved by wiring the power lines of the working external usb charger to a hacked up plug and the data lines back to the PC. They even sell premade cables for this usecase where you have a car with android auto, but the car's USB port is incapable of delivering net+ power to the phone.\n \nreply",
      "I was thinking the same. Also, I would have liked if they discussed/measured the worst-case power draw of the device before starting to add capacitors, as it's possible a PC USB port just doesn't supply enough current to power it at steady state.\n \nreply",
      "> Considering the age of the battery, I decided not to charge it again, because there is a risk that it would swell and damage the OP-1.> Their suggestion was to keep using the original battery.> Would I have to throw away my OP-1 which is still looking almost pristine?Why not just charge it? Am I crazy to think, if it swells it swells? You turn it off, remove it then and get to the problem then. I've used a MacBook air from 2012 with a swollen battery since 2014, every day pretty much. It's not a nuclear bomb.\n \nreply",
      "Nuclear? Nah. Bomb? Pretty much.If it short circuits a cell, it will heat up. Once it reaches a critical temperature, it will start releasing it's stored energy in a self-sustaining reaction. (You need to dump water into it fast enough to carry away the heat faster than it can produce it.) You've now got a class D fire on your hands.Hope you've planned ahead and stored that laptop in an ammo crate or other fire-safe container, because it's gonna be burning long and hot and releasing toxic smoke the whole time.Low probability, high severity.\n \nreply",
      "Only one mainstream mass produced device in recent history ran into that as any sort of significant threat, and it was Samsung Note 7, of which there were ~95 reported fires, half as many reports of damaged property, and ~30 injuries.It takes catastrophic circumstances to bypass the fail-slow idiot proofing engineering that goes into consumer device batteries. The gassing off bubbles and warped laptop frames and all the stuff that goes into preventing things from catching fire and/or exploding makes it a vanishingly small probability, on the level of things like getting struck by lightning, attacked by a shark, or having space rocks land on your house. Such things happen, but it's not something you need to expend resources on. Sure, if you're a surfer, a golfer, live near an ISS dumping lane, or run a laptop repair shop, then take measures. If you're replacing a battery on your own laptop, once every couple years, it's not anything to waste concern on.Hoverboards and DIY vape rigs are honorable mentions, but problems with those tended to be caused by shoddy, third rate manufacturing of the electronics and batteries, and now even the third rate cheap crap is pretty darn safe. Apple isn't putting components teetering on the edge of catastrophic failure into their products, and judging by the absolute numbers, neither is anyone else. That there have been fewer than 10k incidents in the last decade of consumer device failure causing battery fires or explosions, and many many tens of billions of battery powered devices, paints a picture of humans successfully taming lightning.Big batteries in electric cars like Teslas and other higher powered uses do require a different safety mindset. The consequences are orders of magnitude higher, and rate of failure are higher as well, with frequent chain reactions and runaway, uncontrollable fires that can melt rock being a thing first responders and junkyard operators have to contend with.If you can explode a MacBook or laptop doing anything a normal consumer might plausibly attempt, the manufacturer would probably pay you a bounty. Batteries are safe.The odds of being struck by lightning over the course of your life are around 1 in 15k.The odds a device you own catches fire during your lifetime are around 1 in 1 million, assuming 20 such devices. Those odds get better year over year, and solid-state electrolytes, among other innovations, will make fires and explosions a thing of the past.\n \nreply",
      "Jump starters.  Some of the early ones, including at least one highly ranked on a review site, had the jumper cable socket directly connected to the battery.  Wild West stuff.  Get a UL listed one.\n \nreply",
      "I agree with all your points, but I think the risk changes somewhat when dealing with an already failing cell.\n \nreply",
      "Or you can have a dog gone bad day.https://youtu.be/kvbBLIhTr3E\n \nreply",
      "Synthesizer manufacturer here. It's really disappointing to see the message \"IF YOU CAN READ THIS YOUR WARRANTY IS GONE, SORRY\" printed on the inside of the OP-1... super helpful guys.On the inside of the synths we make, there's a link to a documentation repository. If people are desperate enough to remove screws, why not help them?\n \nreply",
      "What synths do you make?\n \nreply"
    ],
    "link": "https://shred.zone/cilla/page/503/batteryless-op-1.html",
    "first_paragraph": "Inari figuresSee the gallery The Teenage Engineering OP-1 is a portable synthesizer and sound recording system. I got one in 2016 because I was fascinated by its abilities that were shown in many YouTube videos. But to be honest, my musical talent is limited, and so it became more of a toy that was finally stowed away in a cupboard for several months.When I took it out again, I found the battery deeply discharged. Considering the age of the battery, I decided not to charge it again, because there is a risk that it would swell and damage the OP-1. Instead my plan was to remove the battery and use the OP-1 without it, since I don't make use of its mobility anyway. It would then need to be connected to USB power for using it, but that would be absolutely fine for me.I also found reports of people where the battery did not charge anymore, or was already depleted after a couple of minutes. There are many reasons for a battery change.Note that this article is about the classic OP-1, not abou"
  },
  {
    "title": "Show HN: A Ghidra extension for exporting parts of a program as object files (github.com/boricj)",
    "points": 208,
    "submitter": "boricj",
    "submit_time": "2024-08-22T08:54:44",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=41318133",
    "comments": [
      "Oh, great to see this here. I think this is an extremely cool project, and I helped to add MS COFF support. (P.S.: I will note that my initial PR was notably worse than the ELF support that was already present, so if you run into problems with it... probably my fault :P I can see it is being improved, though.) That said, I haven't done anything big with it yet. The most fun I had was delinking a Hello World executable compiled with Visual Studio 2003, relinking it to Linux x86 with GCC+glibc, and then relinking that to MinGW+msvcrt again. Doing anything larger than hello world is a bit beyond me yet, though, in part because I'm actually a pretty big n00b when it comes to Ghidra and haven't even really figured out a good way to select the ranges for delinking from a large binary. I should've probably asked someone by now, but oh well. :)Coincidentally, a derivation for this just got merged into Nixpkgs earlier today, so if you're using NixOS unstable it's possible to install it using ghidra.withExtensions; it is under ghidra-extensions.ghidra-delinker-extension. Only one problem: There was a new version released a few days ago and I didn't rebase my PR, so it is out of date. I will try to push an update soon.\n \nreply",
      "> I'm actually a pretty big n00b when it comes to Ghidra and haven't even really figured out a good way to select the ranges for delinking from a large binary.One way to keep track of things to delink is to use folders and fragments inside a program tree. For example, I have a Ghidra program where I've figured out the name and ranges of the various object files that originally made up the executable. These folders or fragments can then be selected as a whole with right-click > Select Addresses.The relocation synthesizer analyzer and the exporter can also be scripted, either independently or using the program's tree manager. This removes the need to select by hand the ranges you want as well as invoking manually the analyzer and the exporter.\n \nreply",
      "How much work is it to figure out which sections of the executable to export?Would it be realistic to be able to export a modern-ish (2008-2015) Win32 game into objects and then compile/link it into a full executable again with less than a few hours work?\n \nreply",
      "> How much work is it to figure out which sections of the executable to export?As long as you do not cut across a variable or a function, you can export pretty much however you want, you don't have to follow the original object file boundaries. What to export is a separate matter and requires some knowledge about the program. Having debugging symbols makes this much easier, otherwise by the time you've made the Ghidra database accurate enough for exportation you'll usually have an idea of where's what.> Would it be realistic to be able to export a modern-ish (2008-2015) Win32 game into objects and then compile/link it into a full executable again with less than a few hours work?About the user report in my submission, they first raised an issue in early July and by mid-August they got a fully working, functionally identical relinked executable. To be fair, the COFF exporter had a lot of bugs that needed to be fixed and the i386 analyzer needed some touch-ups, things that somebody else should hopefully won't stumble over now.I don't know how long it would take, but unless you have debugging symbols and are really lucky it will take more than a few hours of work. A skilled reverse-engineer can probably manage to get something executing in that timeframe (even if it crashes halfway during the first loading screen), but it's one of these tasks that you won't know when it will be done until it is done.\n \nreply",
      "> As long as you do not cut across a variable or a function, you can export pretty much however you want, you don't have to follow the original object file boundaries.Would it be possible to export basically the entire program at once and then slice off individual functions one by one?Do you have any guides/examples of the> Decompilation projects, by splitting a program into multiple object files and reimplementing these Ship of Theseus-stylestyle project?\n \nreply",
      "> Would it be possible to export basically the entire program at once and then slice off individual functions one by one?Yes. The exporters can handle whatever meaningful address selection you can throw at them, including multiple disjoint ranges within the same section. So you can keep carving holes inside your selection until nothing remains of the original program.> Do you have any guides/examples of the Ship of Theseus-style style project?Not quite. My own decompilation project is on a hiatus due to one version tracking session too many in a row, so I only have one article on this so far [1] and the way I've done it is a bit wonky.Another user has recently started a decompilation project [2] with a better framework than I've used in that article, but no actual decompilation has taken place there yet. Incidentally, that would also make for a good modding framework, if one decides to not write functionally identical replacement code.[1] https://boricj.net/tenchu1/2024/05/31/part-11.html (which is humorously titled \"A modding framework powered by the tears of CS101 teachers\")[2] https://github.com/widberg/FUELDecompilation\n \nreply",
      "> Yes. The exporters can handle whatever meaningful address selection you can throw at them, including multiple disjoint ranges within the same section. So you can keep carving holes inside your selection until nothing remains of the original program.Will this also work without painstakingly reversing things in the binary, say in the case of a giant game executable?If possible, I would be very interested in a simple tutorial that takes an arbitrary Windows executable, delinks it and replaces a single function, without all the extra steps necessary to run it on the PS1.It might even be preferable if it worked with MingW, since I'm on Linux as well.\n \nreply",
      "> Will this also work without painstakingly reversing things in the binary, say in the case of a giant game executable?You can get away with a Ghidra database that isn't accurate, as long as you know what you're doing. Basically, as long as the analyzers manage to identify all of the relocation spots inside your exportation, the rest doesn't matter that much. You can even get away with missing relocation spots inside your exportation, if you don't end up executing that code or accessing that data at run-time (if you do, then exotic undefined behavior ensues).The most important thing here is getting references right and addresses typed as pointers (the type itself doesn't matter). I'm not going to discuss this into more details than that, because it would require a deep understanding of the internal algorithms of the extension. Any shortfall between a less-than-accurate Ghidra database and experience will be filled in by luck.> If possible, I would be very interested in a simple tutorial that takes an arbitrary Windows executable, delinks it and replaces a single function, without all the extra steps necessary to run it on the PS1.It's essentially the same steps regardless of the platform. Select the bits you want in your object file, run the analyzer, invoke the exporter, use the linker to create a new program.I've made my Ghidra extension as user-friendly as possible, the rest is standard native development stuff (up to the point where you hit exotic undefined behavior and can't figure it out at a glance, hopefully you're well acquainted with your debugger if that happens).> It might even be preferable if it worked with MingW, since I'm on Linux as well.Actually, I've created a native port of a proprietary, statically-linked, Linux a.out i386 to Windows with MinGW [1] using my delinker. It was back when I didn't have a COFF object file exporter either, so it was the only toolchain for that target that could ingest ELF object files.That being said, MinGW and MSVC are reportedly only compatible at the C ABI level. Mixing and matching different toolchains can increase the odds of something going wrong, so you're probably better off using the toolchain that the program was originally built with (hopefully it runs on Wine).PS: remember that you are throwing your CS 101 handbook into the trashbin when you're using a delinker (and its teacher is unlikely to be of much help).[1] https://boricj.net/atari-jaguar-sdk/2024/01/02/part-5.html\n \nreply",
      "This sounds very interesting. And is tempting me to delve back into a game reverse engineering project I abandoned a few years back.Do you have a fully worked example of how to use this and then how to make use of its output? Would love to see an end-to-end walkthrough.\n \nreply",
      "There are links to various case studies on my blog inside the README of the repository.https://github.com/boricj/ghidra-delinker-extension/blob/mas...\n \nreply"
    ],
    "link": "https://github.com/boricj/ghidra-delinker-extension",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Ghidra extension for exporting relocatable object files\n      This Ghidra extension enables exporting parts of a program as object files. These object files have valid metadata (symbols, relocation tables\u2026) and as such can be reused directly by a toolchain for further processing.Use-cases include:Matrix of supported instruction set architectures and object files:The Ghidra extension archive will be created inside the dist/ directory.The reconstructed relocations can be viewed with Window > Relocation table (synthesized).Object files are made of three parts:When a linker is invoked to generate an executable from a bunch of object files, it will:Normally the relocation table is discarded after this process, as well as the symbol table if debugging symbols aren't kept, leaving only the un-relocatable section bytes.\nHowever, through car"
  },
  {
    "title": "What is an SBAT and why does everyone suddenly care (mjg59.dreamwidth.org)",
    "points": 322,
    "submitter": "todsacerdoti",
    "submit_time": "2024-08-22T09:11:10",
    "num_comments": 172,
    "comments_url": "https://news.ycombinator.com/item?id=41318222",
    "comments": [
      "Obviously people might screw up, but the spec included a way to revoke any signed components that turned out not to be trustworthy\"trustworthy\" according to who? Remember that dystopia does not appear spontaneously, but steadily advances little-by-little.What's the summary? Microsoft (understandably) didn't want it to be possible to attack Windows by using a vulnerable version of grub that could be tricked into executing arbitrary code and then introduce a bootkit into the Windows kernel during boot. Microsoft did this by pushing a Windows Update that updated the SBAT variable to indicate that known-vulnerable versions of grub shouldn't be allowed to boot on those systems.Who is Microsoft to decide what others do on their machines? Should they have the right to police and censor software they have no control of? In the spirit of Linus Torvalds: Microsoft, fuck you!We are seeing the scenario Stallman alluded to over 2 decades ago slowly become a reality. He wasn't alone either.https://www.gnu.org/philosophy/right-to-read.en.htmlhttps://www.cl.cam.ac.uk/~rja14/tcpa-faq.htmlThings like TPM and \"secure\" boot were never envisioned for the interests of the user. The fact that it incidentally protects against 3rd party attacks just happened to be a good marketing point.\"Those who give up freedom for security deserve neither.\"\n \nreply",
      "A recent Linux Unplugged episode went into how one can use the TPM to set up a secure and trusted chain of trust for the booting process on Linux [0] using Clevis [1], very interesting![0] https://linuxunplugged.com/572[1] https://fedoramagazine.org/automatically-decrypt-your-disk-u...\n \nreply",
      "I do something similar on all my laptops:- have custom secure boot platform  key- use a unified kernel image (UKI) which means I directly boot the kernel from efi (and place it in the efi partition)- sign the image with that platform key (I use sbctrl)- have every thing else including swap partition for hybernation fully disk encrypted, I could set it up to auto unlock using TPM2 but I would recommend using a long password. TPM2+password would be optimal. There had been too many cases of leaky TPMs and especially on a laptop you don't want to fully rely on it (through you in turn could decide to auto login if PCRs are unchanged, or login using only the (often not so secure) fingerprint reader etc.)- efi password, I mean if you don't set that you lose most secure boot benefits... EDIT: Not really most, there is still a bunch of ways it helps but it's anyway a bad idea to rely on secure boot and not have a efi passwordAs bonus tip:- include the vfat in your initramfs (i.e. `MODULES=(vfat)` in  `/etc/mkinitcpio.conf`) if your booting kernel and installed kernel modules ever mismatch that is nice to have to fix the issue\n \nreply",
      "> I could set it up to auto unlock using TPM2 but I would recommend using a long password. TPM2+password would be optimal.Personally, I trust LUKS with passphrases far more than I trust some random proprietary hardware implementation nobody can audit...It's also important to me to be able to recover the disk contents with the passphrase on another machine if the motherboard dies. Maybe that's what you meant (backup passphrase), but I think you meant requiring both?\n \nreply",
      "In case of systemd-cryptenroll (and other LUKS-related systemd infra, even without TPM) it's systemd that handles the passphrase to generate a key to unlock LUKS device - possibly combining with a PIN or passphrase or also a FIDO-compatible device or a smartcard.\n \nreply",
      "I meant:- I'm only using a long password- but it would be optimal to require PCR values and passwordNote that in any case where you use PCR values you always should setup a secondary way to unlock the partition. Or else you will lose your data if some of your hardware measured into a PCR breaks.Requiring both is optimal as it 1. doesn't rely on TPM/PCRs but 2. prevent certain attack vectors possible with password only but not possible with PCRs. Through you now also have   to manage a backup unlock method. Which is annoying. And the security benefits are negligible/irrelevant for most people. Which is why I don't use it.\n \nreply",
      "What are the details with a custom key?\n \nreply",
      "sbctl with package manager hook for automatically signing on updates etc.keys are just stored on the device, for the typical laptop use-case this is good enough (platform key only used by a single device, no MDA or anything like that)\n \nreply",
      "The \"new\" way of doing this would be using systemd-cryptenroll [0]. I did this recently on Ubuntu 24.04. I actually tried the default LUKS+TPM shipped with Ubuntu 24.04 at first [1], but it was a bit disappointing because it locks you into using snap-based kernels. This means you cannot install custom DKMS modules (which I needed). Although Clevis is very interesting software (you can even unlock based on some other computer in your network [2]), it's not absolutely required anymore for LUKS+TPM.[0] https://fedoramagazine.org/use-systemd-cryptenroll-with-fido...[1] https://ubuntu.com/blog/tpm-backed-full-disk-encryption-is-c...[2] https://docs.redhat.com/en/documentation/red_hat_enterprise_...\n \nreply",
      "> Microsoft's stated intention was that Windows Update would only apply the SBAT update to systems that were Windows-only, and any dual-boot setups would instead be left vulnerable to attack until the installed distro updated its grub and shipped an SBAT update itself.I wonder what went wrong here? If you would read the EFI boot order it would clearly say to boot shim first? Or were these dual boot setups where the user would use the firmware menu to select linux or windows?Anyway this comes at a time when I want to install linux on my work PC, since it has two nvme slots I think I'll go with installing it on a completely separate drive. Would have not prevented this issue though, which seems a legitimate fix from microsoft, just bad communication.\n \nreply"
    ],
    "link": "https://mjg59.dreamwidth.org/70348.html",
    "first_paragraph": "\nHello, you've been (semi-randomly) selected to take a CAPTCHA to validate\nyour requests. Please complete it below and hit the button!\nOther options:Copyright \u00a9 2009-2024 Dreamwidth Studios, LLC. Some rights reserved."
  },
  {
    "title": "When Serial Isn't RS-232, Geocaching with the Garmin GPS 95 (terinstock.com)",
    "points": 24,
    "submitter": "caust1c",
    "submit_time": "2024-08-19T00:33:24",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=41286753",
    "comments": [
      "I hope GPS receivers have addressed this rollover problem. Not that we will be using GPS receivers made now necessarily over 150 years from now. Garmin GPS 95 was made in 1995. Because it uses the older 10-bit week counter then it causes the device to display incorrect dates after the counter resets every 1024 weeks. Newer GPS systems with the 13-bit week counter don't have this problem because they can count weeks for a much longer period (157 years) before needing to reset.I am guessing when GPS was originally designed the designers didn\u2019t anticipate that the 10 but counter would be an issue and/or proliferation of these consumer devices.Since they could control their own equipment, maybe they thought the LNAV 10 bit counter was fine.\n \nreply",
      "Nice real world example of figuring out an unknown interface. There are plenty of different electrical interfaces for transmitting serial data, and they often use the same physical connector, so you definitely have to know what the device is outputting. For reverse-engineering an unknown device, you often will need a scope. If you're lucky, you'll be facing an interface that is documented, like RS-232, RS-422, or RS-485. If you're unlucky, you need to rely on experience, knowledge and familiarity like OP did. Only once you have the electrical signal figured out, you can move on to baud, word length, stop bits, parity, and so on, and then decoding the actual data.\n \nreply",
      "The thing I\u2019ve gotten most confused about historically is whether RS232 is voltage levels or a protocol\n \nreply",
      "https://www.optcore.net/difference-between-rs-232-rs-422-and....Serial is usually ascii, not always but usually. The different types of serial depend on things like duplexity and line noise, or lack thereof.232 is -15v to 15v, 422 is -6 to 6, and 485 is -7 to 12\n \nreply",
      "RS-232 defines the levels, the different signals, and the physical connectors. However, baud rate, parity, stop bits, and the like are at a \"level\" above RS-232, and so outside its scope.\n \nreply",
      "We also have to bear in mind that the computer industry often uses \"RS-232\" in reference to something that uses neither the connector nor the signaling levels defined by RS-232, something we have IBM to thank for.\n \nreply",
      "Nice! I just found my TFT color Garmin GPS yesterday that I was looking for during the pandemic but have now forgotten what project I was going to use it for. Maybe I should take it out caching for old times sake.\n \nreply",
      "I used to cache a ton; but, have only found a handful a year (outside of an annual guys trip with caching friends that has become more about chasing states where we\u2019ve found caches (49 for me; just missing AK)) and hitting breweries.Good times.\n \nreply",
      "My unit predates GPS units having street maps, but assuming your TFT version does, I believe there are exporters from Open Street Maps that might be cool to try out?\n \nreply"
    ],
    "link": "https://terinstock.com/post/2024/08/When-Serial-Isnt-RS-232-and-Geocaching-with-the-Garmin-GPS-95/",
    "first_paragraph": ""
  },
  {
    "title": "Launch HN: Arva AI (YC S24) \u2013 AI agents for instant global KYB onboarding",
    "points": 54,
    "submitter": "rhimshah",
    "submit_time": "2024-08-22T16:28:26",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=41321936",
    "comments": [
      "I'm not super familiar with KYB, so it could be that this would pass a human check as well, but I got in with a couple of mostly-blank utility bill / AOI templates and a pastebin saying 'this is the company x website, we do business y' as the official company website.In any case this is a cool demo, and the type of thing that I expect to be mostly solved as LLMs get better. Good luck!\n \nreply",
      "Thanks for playing, this is useful info!There's a big difference between the nature of fraudulent documents submitted to our customers today, and the ones that one might create knowing that the system is  build with LLMs - we've mostly optimised for the former so far.I'm also glad to see it took a number of attempts - security through obscurity is not something we want to rely on but the real system requires your identity and will offboard you without explanation at the first hint of misbehaviour.We'll continue to improve our system to be more vigilant and make fewer assumptions.\n \nreply",
      "Without explanation sounds like when platforms auto ban you without giving a reason or send you endless unsolvable captchas.\n \nreply",
      "It is not possible to give the reason for offboarding as it may tip off fraudsters on how they can evade detection next time. This is standard across the industry.\n \nreply",
      "This is very interesting. It calls to mind some advice I heard someone associated with YC (maybe PG?) offer a while ago: (approximately) the best ideas right now are the ones that will become more useful rather than less as AI gets better.I say that because although it's been about ten years since I worked at a financial institute, this task seems like it must be still barely too complex for AI alone in its present state to match human performance but perhaps within reach if we can soon expect further slight advances in consistency and attention across longer context. Does your experience line up with those suppositions? Or with the high structure/formalization you've mentioned, do you see this as an alternative to all those KYB analysts today?\n \nreply",
      "Yes, I would definitely put Arva in that category.For now, we limit the amount of decisioning that is made by an LLM and make as much of the business logic as we can concrete in code. It's mostly used to extract information from documents, crawl websites and identify specific fraud signals.\n \nreply",
      "So what you're saying is that you could've done this startup a decade ago without LLMs using traditional NLP and ML techniques. Or even just with straight up procedural code, OCR and a rules engine. Especially since as you say everything you're dealing with is highly structured.I work at a bank and everything you mentioned was solved many, many years ago.\nSo the more interesting question then is why are fintechs still using manual techniques despite having the capability to automate it.\n \nreply",
      "Not quite!Fintechs often still have humans review docs, websites, perform web due diligence etc. Efficacy has vastly improved at these validation steps with the assistance of LLMs.Interesting to hear that your previous bank has automated all of low/medium risk already, from what we have seen more traditional banks are far behind fintechs and are more risk averse in using new technologies. Nice to see that's not the case with all traditional banks.\n \nreply",
      "They do use automation, OP is just straight up clueless.\n \nreply",
      "Depends on the fintech! 99% don't have 100% automation for all low/medium risk.\n \nreply"
    ],
    "link": "item?id=41321936",
    "first_paragraph": ""
  },
  {
    "title": "Launch HN: AnswerGrid (YC S24) \u2013 Web research tool for lead generation",
    "points": 39,
    "submitter": "tife",
    "submit_time": "2024-08-22T17:51:01",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=41322730",
    "comments": [
      "Congratulations on the launch. We sell B2B crowd forecasting software, a fairly niche market, so something like this could be very interesting for us. We sell mostly into governments, think tanks, and other types of research institutions, with the occasional commercial application. A couple thoughts:Perhaps I didn't do something correctly, but I ended up with a list of companies. That saves me a little time, but I want to be able to then get to something role-based. Is this behind the upgrade or do I never see this information?Related, I want role to be one of my primary filtering mechanisms. Find me all Chief Risk Officers in x,y,z industry for companies larger than 500 people, for example. Then let me review their backgrounds, get their contact info, and contact them.I think this is how many sales people / founders operate to try and start to sell. What am I missing?\n \nreply",
      "Yes! you can add contacts based on job description. You'd do that by clicking on the far-right of the table to add a column, then select \"Find Employees\" and add as many key words and job descriptions as are relevant.Here's a list of Chief Risk officers for \"Companies in the insurance industry with more than 500 employees\": https://app.answergrid.ai/try-it?starting-grid=af8339c3-4619...You can interact with the contact cards, by clicking and visiting their linkedin (we didn't expose emails on the demo, but those are available aswell)\n \nreply",
      "Congratulations on the launch! I've done Python scripts to do this, which connect to APIs like Apollo, use proxies for Google scraping, and scraping libraries for URLs etc. It's no small feat to connect it all together. Your product looks next level, and with a gorgeous spreadsheet UI too. Very nice\n \nreply",
      "Thank you!Are you still using your stack for outbound :) ?\n \nreply",
      "How does this differ to a tool like Clay.com and their Claygent\n \nreply",
      "For lead generation, some users have found Clay to have a slightly steep learning curve, which makes sense if you were scaling GTM and want to set up elaborate waterfall mechanisms for contact enrichment.So we built this to be walk-up usable for people doing sales for the first time and want to focus on precise qualification and not just scale of outreach\n \nreply",
      "My first thought/question as well.\n \nreply",
      "Congratulations on the launch.I was about to build something similar to help someone dear to me on their job hunt.However, AnswerGrid looks very promising: https://app.answergrid.ai/try-it?starting-grid=9c5cf105-266e...\n \nreply",
      "Yes, we've been hearing a bit about the job search use case as well.I'll add that you could add a column for (linkedin) job postings too :)Here's the updated Grid: https://app.answergrid.ai/try-it?starting-grid=c4ad0d6c-2eca...(Most of them didn't have postings though, so I put a filter on the column to limit to those that do)\n \nreply",
      "Thank you. Appreciated!Btw: In a job hunt situation I don't actually care about the company's HQ. All I care about is, do they have a facility/office in this country/city.\n \nreply"
    ],
    "link": "item?id=41322730",
    "first_paragraph": ""
  },
  {
    "title": "DRAKON (wikipedia.org)",
    "points": 97,
    "submitter": "instagraham",
    "submit_time": "2024-08-19T17:10:13",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=41292757",
    "comments": [
      "I've used the (free) https://drakonhub.com service to create Drakon diagrams, and I've been able to express some pretty sophisticated workflows in a clear, intuitive way.One of the things I love about it is that it is orderly by design.  You've probably seen those scary flowcharts where arrows are pointing willy-nilly, criss-crossing each other,  and there's very little cohesion.  Drakon charts help avoid that by being relatively prescriptive about _where_ diagram components are placed.  For instance, the happy path should form a skewer shape.The \"Diagramming is different with DrakonHub\" section of https://drakonhub.com shows you the difference well.\n \nreply",
      "Impressive dogfooding \n> DrakonHub is written in DRAKON-JavaScript and DRAKON-Lua.https://github.com/stepan-mitkin/drakonhub\n \nreply",
      "I really like two things about the Drakon docs I've perused:1) They show how to draw a really clean flowchart, which is a bit of a lost art.  I'm not a big fan of flowcharts - IME it seems like the biggest fans are, weirdly, mechanical engineers of a certain age - but hey2) They show how to do a clean depiction of a state machine in flowchart format, which might come in handy for someone, somewhere\nhttps://drakonhub.com/files/lift.htmlAlso, as a sort of meta-comment, I noticed that we have two comments from people doing workflow programming (presumably Sharepoint or similar, which I'm 100% sure gets no love or respect, but is super useful if you need it) who found these approaches useful.  Workflow is a great use case for graphical programming; in a related vein, a hobby horse of an old friend is that a good graphical environment for devops would change the industry.The Russian wikipedia page (in auto-translate for me) has a ton more detail and history.  I'm not going to a deep dive but I'm also not so sure about the claim that there's no history of aerospace use.\n \nreply",
      "Once you start noticing this design language, you'll see it everywhere.For instance, in software/hardware audio processing you may load processing blocks. Typically they are arranged left-to-right, but MaxMSP can be top-to-bottom.\n \nreply",
      "After reading about this and the methodologies many years ago I implemented many of the precepts into my practice at the time (visual programming using a variety of workflow tools) and it really helped to prevent sprawl that is common with drag and drop workflow interfaces. It helps to enforce that discipline of not straying too far away from the desired path and handling exceptions. If it strays too far to the right then you know you need to look at things again and refactor. Worth reading.\n \nreply",
      "How different is something like this to say, a Jira workflow?I know there must be many like this but I think it might be particularly robust since it helped power the Buran launch and landing - which by all accounts was a bit of a miracle it all worked so well at first go.\n \nreply",
      "If you like diagrams that are also interactive via topological graph queries, See:https://schematix.com/video/depmap/\n \nreply",
      "Related:The DRAKON Language - https://news.ycombinator.com/item?id=36021495 - May 2023 (38 comments)Why aren't there more visual programming languages? (An ode to DRAKON) - https://news.ycombinator.com/item?id=35712086 - April 2023 (2 comments)Ask HN: Anyone knows a flowchart programming better/more modern than DRAKON? - https://news.ycombinator.com/item?id=26137769 - Feb 2021 (1 comment)The Human Revolution in Understanding Programs [pdf] - https://news.ycombinator.com/item?id=19160248 - Feb 2019 (6 comments)Drakon: a visual language for specifications from the Russian space program - https://news.ycombinator.com/item?id=12638032 - Oct 2016 (39 comments)DRAKON \u2013 An algorithmic visual programming language - https://news.ycombinator.com/item?id=10100932 - Aug 2015 (48 comments)Drakon \u2013 A visual language for specifications from the Russian space program - https://news.ycombinator.com/item?id=6429283 - Sept 2013 (28 comments)Use DRAKON to Automatically Generate Code from Flowcharts - https://news.ycombinator.com/item?id=5497668 - April 2013 (1 comment)\n \nreply",
      "Oh apologies, I thought submissions here were akin to Reddit where it flags you if the same link has been submitted before. I'd Googled Drakon too without finding  much discussion on it, but it didn't occur to me to search HN.\n \nreply",
      "Some topics are evergreen in part because their core principles ideas are deeply interesting and unique. In part because over time nobody can keep up with everything on HN and because there is a steady influx of new users by design. HN\u2019s \u201cEndless September\u201d is a feature not a bug.Drakon can be new to you, still intellectually interesting to many people who have seen it before, and make people with a deep affinity for Drakon feel less alone.\n \nreply"
    ],
    "link": "https://en.wikipedia.org/wiki/DRAKON",
    "first_paragraph": "DRAKON (Russian: \u0414\u0440\u0443\u0436\u0435\u043b\u044e\u0431\u043d\u044b\u0439 \u0420\u0443\u0441\u0441\u043a\u0438\u0439 \u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a, \u041a\u043e\u0442\u043e\u0440\u044b\u0439 \u041e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u0435\u0442 \u041d\u0430\u0433\u043b\u044f\u0434\u043d\u043e\u0441\u0442\u044c, lit.\u2009'Friendly Russian Algorithmic language Which Provides Clarity') is a free and open source algorithmic visual programming and modeling language developed as part of the defunct Soviet Union Buran space program[2] in 1986 following the need in increase of software development productivity. The visual language provides a uniform way to represent processes in flowcharts.\nThere are various implementation of the language specification that may be used to draw and export actual flowcharts. Notable examples include free and open source DRAKON Editor (September 2011).\nThe development of DRAKON started in 1986 to address the emerging risk of misunderstandings - and subsequent errors - between users of different programming languages in the Russian space program. Its development was directed by Vladimir Parondzhanov with the participation of the Russian Federal Space Agency (Academician Pilyugin Cent"
  },
  {
    "title": "Ethernet History Deepdive \u2013 Why Do We Have Different Frame Types? (lostintransit.se)",
    "points": 119,
    "submitter": "un_ess",
    "submit_time": "2024-08-22T08:29:45",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=41318013",
    "comments": [
      "I wish layer 2 and layer 3 were 'refactored' to force all links to be point to point, which they effectively are in the modern world.    When was the last time you saw ethernet frame collisions because you used a hub not a switch?We'd get rid of the idea of a broadcast domain.  We'd get rid of Mac address and ARP.  Switches and routers would become the same device.  We'd just use ip addresses for routing, and the 'next hop' would always be the opposite end of the link you sent a packet over.The world would be a simpler place, and no functionality would have been lost.\n \nreply",
      "All wifi is a giant collision domain. Also, each segment of a wired network is a collision domain.What you are describing is more in line with MPLS or Infiniband.I agree with you frustration. I prefer to design networks that start routing right at the access port or even using an agent, virtual network port, or VPN endpoint at the client or application (like QUIC), but that is very expensive from a resource standpoint.IPv6 is also another way to get closer to what you are describing.In my perfect world, we'd move to something like a mashup of MPLS and HIP (https://en.wikipedia.org/wiki/Host_Identity_Protocol)If want to study something more \"routed\" and more point to point, look at private mobile networks (5G).What we don't want is more layers of abstraction... that's making every slow, brittle and impossible to troubleshoot.\n \nreply",
      "> When was the last time you saw ethernet frame collisions because you used a hub not a switch?10base-T1S is just beginning to ramp up in the automotive industry, which modifies the super-successful 100base-T1 to be cheaper by (a) allowing cheaper PHYs; (b) allowing cheaper endpoints due to the lower data rate to handle; (c) allowing lower-spec single twisted pair wiring; and ... (d) allowing multi-drop. This is intended to allow Ethernet to push down into the space that CAN-FD is currently occupying, and looks likely to succeed, at least in some niches.\n \nreply",
      "A really interesting article covering this: \u201cThe world in which IPv6 was a good design\u201d   https://apenwarr.ca/log/20170810It talks about how when IPv6 was being designed, they wanted to do exactly that: drop most of the layer 2 stuff, abandon the idea of a bus network, make everything point-to-point, all switches would be L3 routers, etc.Search for \u201cWhat if instead the world worked like this?\u201d for the relevant part.My question though, is how would IP assignment work for each of the intermediary devices between me and (say) my ISP\u2019s gateway? My computer is plugged into a switch right now, which is plugged into another switch, which is plugged into my router, which has a point-to-point link to the ISP gateway. Would my router get a /64, then delegate a /68 to the next \u201crouter\u201d (ie. The physical thing I currently call a switch), which would delegate a /72 to the next one, etc? How would it determine the optimal IP allocation? What if there\u2019s a cycle? Aren\u2019t we sorta reinventing spanning tree at this point? (I\u2019m genuinely curious about this, because I don\u2019t really grok all of the implications of an \u201ceverything is L3\u201d world like this.)\n \nreply",
      "For v6-specific world, scoped addresses and scoped multicast are explicitly for that purpose. You do not need to hierarchically subnet each following router, you just need to be able to express \"next hop\" for the subnets you need to route towards.You use link-local autoconfiguration, and use appropriately-scoped multicast addresses to ask \"all-nodes\" or \"all-routers\", making autoconfiguration a breeze compared to v4 world. In v4 world a similar setup is also possible, though specific details of the setup differ, and you have to setup addresses manually for each p2p link.\n \nreply",
      "I mean sure, you\u2019d definitely use a scoped address to talk to the next hop, but it still doesn\u2019t solve how the router/switch knows which port to send the packet to for its next destination.Say I have a global unicast address on my desktop, 2 hops from my router, and I want to allow traffic to it. My router gets a packet sent to it over its link-local address, with a destination header of my desktop\u2019s IP. Say it has 4 ports (each going to another router/switch, each with its own link-local address.)How would it know which port to use as the next hop? It would need a routing table, and that would need to be configured automatically if we want to work as well as switches do today. What would be the protocol for this auto configuration? BGP or something like it? How do the routers know the available address space? Or are we just stipulating that we\u2019d invent a protocol for this, if it had ever happened?In ethernet we have the Spanning Tree Protocol for this, to discover the topology of an Ethernet network and know which links to use for which MAC addresses (including the ability to detect cycles.) I feel like something like spanning tree would still need to exist in an all-unicast, no-ethernet, L3-only world. Does such a thing exist already, or would we need to invent it in this counterfactual universe?\n \nreply",
      "If you force all links to be conceptually point to point, you probably make it harder to do some things. Already 1G and higher force full duplex, and 100base-TX full duplex is very common. I've still got a couple 10baseT half duplex devices though.I have redundant internet/nat routers at home (overkill!), and they communicate amongst each other to decide which is active and which isn't, but either way, the active one ARPs for the router address with 00-00-5E-00-01-01 as the mac address. The rest of the network just sends off-network packets to  00-00-5E-00-01-01, and failover happens because switches figure out what port is currently using that address.I share a different mac address for the upstream connection, which is PPPoE (sadly), but same deal --- when failover happens, the new computer starts using the address and everything figures it out, because stations are allowed to move to different ports by design.\n \nreply",
      "You can pretty much 1:1 what you describe in the redundancy case with IP, just replace the \"relearn which port the MAC address associated with that IP is on\" with \"relearn which port the next hop for that IP is on\".Things tend to get a little messier than people expect in figuring out the \"what values do I use for the point to point links and how do they get assigned\" step of things, though there are some clever answers there too.\n \nreply",
      "Ironically, this version of the header published in 1980 is what we still use to this day.IMHO Ethernet is one of the of great examples of backwards compatibility in the computing world. Even the wireless standards present frames to the upper layers like they're Ethernet. It's also a counterexample to the bureaucracy of standards bodies --- the standard that actually became widely used was the one that got released first. The other example that comes to mind is OSI vs DoD(TCP/IP).\n \nreply",
      "OSI was usable as a wide area network before TCP/IP was.\n \nreply"
    ],
    "link": "https://lostintransit.se/2024/08/21/ethernet-history-deepdive-why-do-we-have-different-frame-types/",
    "first_paragraph": ""
  },
  {
    "title": "What If Data Is a Bad Idea? (schmud.de)",
    "points": 108,
    "submitter": "surprisetalk",
    "submit_time": "2024-08-19T11:47:41",
    "num_comments": 70,
    "comments_url": "https://news.ycombinator.com/item?id=41290189",
    "comments": [
      "\"At the end of every seven years you shall grant a deletion of the data. And this is the form of the delete: Every data collector who has recorded anything on his neighbor shall delete it; he shall not maintain any of it about his neighbor or his brother, because it is called the Lord's delete\" [1][1] Deuteronomy 15-17\n \nreply",
      "This is a Neal Stephenson LLM-generated ebook waiting to happen.\n \nreply",
      "Also, you missed the chance to change to citation to \"Dataronomy\"\n \nreply",
      "Debt jubilee but for data is a great idea.\n \nreply",
      "The quote is here: https://news.ycombinator.com/item?id=11945722, from this memorable thread:Alan Kay has agreed to do an AMA today - https://news.ycombinator.com/item?id=11939851 - June 2016 (893 comments)\n \nreply",
      "The further I get into my career as a data scientist (formerly a software engineer) the more I think I see what Kay was getting at in this thread.I spend a huge chunk of my time swimming in a sea of data that people have carelessly amassed on the assumption that data is inherently valuable. And frequently I come to the conclusion that this data has negative value. The people who collected it failed to record enough information about the data's provenance. So I don't know what kind of processes produced it, how and why it was collected, any transformations that might have happened on the way to storing it, etc. Without that information, I simply cannot know for sure what any of it really means with enough precision to be able to draw valid conclusions from it.The best I can do is advise caution, say that this data can help us form hypotheses whose only true use is helping us form a plan to collect new data that will help us answer the question we have. What I'm typically asked to do instead is make a best guess and call it good.The former option is just the scientific method. The latter option is the very essence of pseudoscience.That second word in my job title gives me some anxiety. I'm keenly aware that most people are more fond of the word \"science\" than they are of the intellectual practice we call science. The people who like to amass data are no exception.\n \nreply",
      "I was the data and analytics part of a global team at HomeAway as we were struggling to finally release a \"free listing / pay per booking\" model to catch up with airbnb, I wired up tracking and a whole bunch of stuff for behavioral analysis including our GA implementation at the time.Before launch we kept seeing a step in the on-boarding flow where we saw massive drop-off and I kept redflagging it, eventually the product engineering team responsible for that step came back with a bunch of splunk logs saying they couldn't see the drop off and our analytics must be wrong \"because it's js\", which was just an objectively weird take.For \"silo\" reasons this splunk logging was used by product and no one else trusted it or did anything actionable with it as far as I could tell other than internally measuring app response times.I would not unflag the step and one PM in particular started getting very upset about this and saying our implementation was wrong and he roped a couple senior engineers in for support.I personally started regression testing that page based on our data and almost immediately caught that any image upload over ~1mb was not working and neither was mobile safari, turned out they had left their mvp code in place and it used flash or something stupid so it would break on image size and some browsers just wouldn't work at all.It was updated a couple weeks before launch and the go live was as good as could be expected.To this day I have no clue how this particular team had so misconfigured their server side logging that it HID the problem, but you see it all the time, if you don't know what you're doing and don't know how to validate things your data will actually sabotage you.\n \nreply",
      "You've accidentally described 100% of my experience with Splunk at every org I've worked at: it's so expensive no one is given access to it. It's hard to get logs into it (because of expense). And so you're experience of it is the annointed \"splunk team\" want something, but you never see how they're using it or really the results at all except when they have an edict they want to hand down because \"Splunk says it\".\n \nreply",
      "Did they just assume \"no errors on the server side, so the problem can't exist\"? That's bizarre.\n \nreply",
      "It was odder than no errors, they weren't seeing any funnel drop off at all.It wasn't worth investigating and fixing for them, at the time I figured they were excluding traffic incorrectly or didn't know how to properly query for \"session\" data... could have been any number of things though.\n \nreply"
    ],
    "link": "https://schmud.de/posts/2024-08-18-data-is-a-bad-idea.html",
    "first_paragraph": "What if \u201cdata\u201d is a really bad idea?Several years ago, esteemed computer scientist Alan Kay participated in an Ask Me Anything on Hacker News. Kay always sparks interesting conversation and I was happy to see Rich Hickey, a practitioner held in equally high regard, join the thread. The dialogue was kicked off by Kay\u2019s provocative musing on the legitimacy of data; not just some data, but the idea of data itself.Kay has made a career out of challenging conventional wisdom. Perhaps this is best embodied by one of his most famous quotes, \u201cA shift in perspective is worth 80 IQ points.\u201d \u00a0Alan Kay by Jean Baptiste\n\n It was a bit like a physicist asking, \u201cWhat if gravity is a bad idea?\u201dKay and Hickey largely talked past each other and never really got to engage the question. But I think it\u2019s one worth exploring. Not only in the technical dimensions of data, but also in the inescapable political dimensions of its automation.Data is inherently objectifying. This property is an asset when describ"
  },
  {
    "title": "Making PyPy's GC and JIT produce a sound [video] (youtube.com)",
    "points": 67,
    "submitter": "luu",
    "submit_time": "2024-08-22T16:36:34",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=41321998",
    "comments": [
      "There's a blog post from 6-8 years ago I can no longer find anywhere, where someone made node.js create similar ambient sounds and compared it to listening to hard drives doing mechanical work to understand what the computer was doing. Apparently it was actually useful to audibly hear when, for example, a request failed due to an authentication problem which had its distinct sound emerging from the pattern of code execution on that branch.Does anyone have a link to what I'm talking about?\n \nreply",
      "I recall something like that. You might be talking about baudio: https://www.npmjs.com/package/baudio . I also recall seeing some website demonstrating the idea of soundifying a stream of incoming metrics/events so you could listen to it and your brain could sift through and automatically pick our patterns, changes, and anomalies. I think there was a demo using the twitter firehose as an example? But for the life of me I can't remember anything more than that.\n \nreply",
      "Unrelated, but GPU coil whine can give unintentional ambience to LLM inference.In some llama.cpp versions on my home inference rig it would manifest as a sort of squelchy sound that would match the generation of characters on the screen, reminiscent of the effect often used during dialog in 8-bit era video games.I found it quite comfy.\n \nreply",
      "Just need a proof of concept for data exfiltration from a secure facility by using ultrasonic coil whine and you too can be a security researcher!\n \nreply",
      "This reminds me of one of the tweets that live inside my head and makes itself known every once in a while, about the Lion King movie: https://twitter.com/glowcoil/status/1204511618769588225 (\"when the gc sounds warmer\" https://www.fxguide.com/fxfeatured/how-virtual-production-wo... )\n \nreply",
      "Oh wow, that's just amazing, thanks for the link! From the article:'''\nInterestingly, there was a technical bump or glitch in the Unity Engine. A feature that no user would normally have any control over, that caused a bump that made it all the way from the D23 footage to the final film. Every once in awhile the Unity Engine clears our any unneeded data or assets from the Engine.Unfortunately, when this random function, deep in the code called \u2018garbage collection\u2019 ran it could cause a tiny pause in the smooth movement of the master Unity Camera. One such \u2018bump\u2019 happens in a shot in the D23 trailer. After the trailer was complete, the team discovered what the issue was and fixed it. But even when the shot was redone much later, this \u2018bump\u2019 is still in the final camera move, just because Rob Legato liked the feel of the recorded natural move, even with the bump. For the creative team, ironically given its causes, it just felt natural.\n'''https://www.fxguide.com/fxfeatured/how-virtual-production-wo...\n \nreply",
      "This is wonderful and I'd like to see more examples of multi-sensory experiences for working with computers or algorithms.\nAre brains, especially the subconscious are powerful at finding and learning patterns - so being able to utilize that seems so useful!\n \nreply",
      "Many related relevant thoughts:\u2022 This sounds very similar to what Calm Technology\u00b9 is trying to achieve.  Their first example was a freely dangling string tied to a motor, directly connected to the Ethernet.  The string then spun soothingly in a corner with some auditory and visual indication of network activity.\u2022 I seem to recall that the developers of the new computer in The Birth of a New Machine connected (old, analogue) oscilloscopes to the CPU pins for register content, directly driving the X and Y coordinates of the oscilloscope beam, and thereby got a visual image of how the execution usually looked in different situations, and when they looked \u201cwrong\u201d, it was time to slow down and analyze step-by-step.\u2022 Many older machines were not very well shielded against RF interference, so in olden times hackers used to turn on a radio to the most interfered channels to listen to the CPU executing their program, which helped with debugging.   (Later, many home computers were similarly not very good at compositing a TV signal, so the computer\u2019s activities would partly bleed into the sound and/or video output.  This also similarly aided debugging.)I was somewhat disappointed that the artificial sounds in the video were actually old game machine sound effects tied to events, not the actual sounds of the high-frequency events turned into audio.1. <https://en.wikipedia.org/w/index.php?title=Calm_technology&o...>\n \nreply",
      "This is super-interesting!You might be able to get even more information with a stereo signal, use the pan field for something.\n \nreply",
      "Good idea! There are various other extensions possible.Unfortunately there's a bit of an observer effect, the sounds are made with python code running in the same process, and the more complicated I made the sound generation logic, the more garbage it produces itself .\n \nreply"
    ],
    "link": "https://www.youtube.com/watch?v=drwJBzDM1jI",
    "first_paragraph": ""
  },
  {
    "title": "Climate policies that achieved major emission reductions (science.org)",
    "points": 31,
    "submitter": "barbazoo",
    "submit_time": "2024-08-22T18:13:40",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=41322955",
    "comments": [
      "Let's have a look at France's emissions :https://ourworldindata.org/co2/country/franceSomehow, from 1973 they went from 10t C02 per capita to 4.8 t per capita in 2022.Their GDP at the same went from 264 Bn to ~2779 Bnhttps://www.macrotrends.net/global-metrics/countries/FRA/fra...Whatever mysterious method of C02 reduction they used should be investigated.\n \nreply",
      "> These figures reflect \u2018production-based\u2019 emissions, so do not correct for traded goods.moving manufacturing overseas\n \nreply",
      "Nuclear power.\n \nreply",
      "Trying to summarize here w/quotes from article. . .\"Although we observed around 1500 policy adoptions and tightenings in our policy data, their impact on emissions has so far been highly uncertain. Our break detection DID analysis suggests that large emission reductions have materialized in only 69 cases\"\"In most cases, we found that effect sizes are larger if a policy instrument is part of a mix rather than implemented alone. . .  these comparisons of relative effect sizes provide suggestive evidence that some of the most widely used regulatory instruments and subsidy schemes may require complementary instruments to enable substantial emission reductions.\"\". . . scaling up good-practice policies identified in this study to each sector of other parts of the world can in the short term be a powerful climate mitigation strategy. However, even if all countries in our sample were able to replicate past success, more than four times (one and a half times) the effort witnessed so far would have to be exerted to close the emissions gap.\"\". . . This also highlights the need for research providing systematic evidence on which climate policy mixes are most powerful in spurring the necessary deployment and development of low-carbon technologies for a future net-zero economy \"\n \nreply",
      "Pure emissions measures that don't consider waste outputs seem to be woefully inadequate.  Having study periods that end in 2020 seems designed to arrive at incorrect outcomes.  And finally,  leaving out ocean based emissions and total amount of waste and electronic waste generated by new schemes seems thoughtless.\n \nreply",
      "Did anyone get the short list in a clear format? That article was the most handwavy, avoid the point, whitepaper I've read in a long time.\n \nreply",
      "My takeaways were1. Developed and developing countries reacted differently to different incentives. This to me is just another point where \u201cthe invisible hand\u201d only works in certain limited cases. Markets are identifiable measurable systems. Not magic2. It\u2019s a mix, it needs careful monitoring and good across the board policy and industrial involvement - yeah. We need good government and good private sector involvement. So there goes the paddle just as we enter the brown looking creek \u20263. Loving the graphics - Inwish Incoukd do that with matplotlib\n \nreply",
      "'This to me is just another point where \u201cthe invisible hand\u201d only works in certain limited cases.'There's a certain amount of irony and despair when referring to the invisible hand of a free market in the context of government and ngo interference therein...\n \nreply",
      "Do you mean \u201cThe Invisible Man\u201d?\n \nreply"
    ],
    "link": "https://www.science.org/doi/10.1126/science.adl6547",
    "first_paragraph": ""
  },
  {
    "title": "How Deep Can Humans Go? (mcgill.ca)",
    "points": 81,
    "submitter": "bookofjoe",
    "submit_time": "2024-08-19T15:48:23",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=41292049",
    "comments": [
      "The limits for other types of diving are interesting, too.All of the different gases you use for SCUBA diving have a maximum operating depth. At high pressures, various gases become toxic or cause side effects. Oxygen has a limit somewhere in the 1.2-1.6 bar range, and it becomes toxic. For deep dives, you need a lower percentage of oxygen in the mixture. Nitrogen causes nitrogen narcosis at higher pressures, so for deep dives, you need a lower percentage of nitrogen in the mixture. You need to add something else if you want to lower both nitrogen and oxygen, and that\u2019s helium. At high enough pressures, helium causes high-pressure nervous syndrome, a.k.a. helium tremors.That\u2019s normally the limit. Pretty much every gas is bad for you at these pressures. There is an experimental alternative, which is to use hydrogen gas for the deepest dives, but people are understandably cautious about breathing a mixture of oxygen and hydrogen.\n \nreply",
      "In principle liquid breathing is still an option. It's not great, but certainly better than oxygen + hydrogen.\n \nreply",
      "Have there been any human trials of total liquid breathing? Hydreliox breathing has been tested a few times, and has the advantage that it's fairly compatible with existing technology (compressed air bottles, gas regulators, etc.).Yeah, it's hydrogen, hazardous to mix and handle, but so is acetylene, and that can explode all by itself if it's under too much pressure. Mess up the mix and you die, but if you mess up the mix for any breathing gas, you're pretty effed (the wrong N2/O2 mix can cause any of hypoxia, narcosis, seizures at various depths).Not many people have done a pure hydrogen/oxygen dive (vs. hydrogen/helium/oxygen), but from one who has on a rebreather:> \u201cThe first cautious sip of hydrogen just to activate the ADV was satisfying,\u201d [Harris] said. Gas density was not subjectively improved, but Harris noticed an obvious benefit\u2014the HPNS-induced hand tremors he typically experienced after 180 meters/590 feet disappeared. [1][1]: https://indepthmag.com/n1-the-inside-story-of-the-first-ever...\n \nreply",
      "Office Of Naval Research did a human trial\nhttps://youtu.be/TIGCdA2YLyY?si=z_7RK06wLO7T926D&t=409\n \nreply",
      "> Have there been any human trials of total liquid breathing?It's used on a somewhat regular basis in clinical settings (e.g. liquid ventilation). I'm not aware of any diving experiments, though. There might have been some classified military tests, but nothing official AFAIK.\n \nreply",
      "Liquid ventilation has been used with some success as a salvage therapy for critically ill patients, especially preterm neonates. However, it carries serious risks of its own. And it requires an external mechanical pump to circulate the liquid; the human diaphragm isn't strong enough to move the liquid in and out.https://doi.org/10.5001%2Fomj.2011.02I think liquid breathing for human divers is going to remain science fiction forever. There just aren't any circumstances where it would make sense to do that instead of using an atmospheric diving suit (or an ROV).\n \nreply",
      "In the realm of science fiction that has science... Timemaster by Robert L. Forward (who writes science papers with a plot - and that's a good thing).The main character wanted to get out to the asteroid belt in an accelerated time frame (not relatively accelerated but rather faster than the current slow methods)> \"Sorry, sir,\" said Mary, the perky nose on her image twitching under her large glasses as she thought. \"I'd be glad to turn over all the cable we've been making for the Mars rotovator, but by the time Bull's division could turn it into a cable catapult, you'd be there using the existing system.\"> \"Can we push the gee limit higher?\" asked Randy.> \"Well... yes...\" admitted Bull. \"We have small express pods we use to send emergency cargo. We can accelerate those at ten times the normal three-gee launch acceleration and get them up to three hundred kilometers a second.\"> \"How soon would that get me there?\" asked Randy.> Bull, his fingers too large to operate a cuff-comp, pulled an hp pseudocray out of his shirt pocket, and did a short calculation. \"Three weeks,\" he said. \"But those capsules accelerate at thirty gees! You'd be squashed flat!\"> Randy paused for a while as he thought. \"I've read about deep-sea divers who survived at high pressures by breathing an oxygen-carrying liquid,\" he said. \"If I floated in a tank of that, I could handle thirty gees easily.\"> Tony, Mary, and Bull each thought for a while; then all three nodded, although reluctantly.> \"If I were you, I'd check with some medical and diving experts first,\" said Tony. \"It could be hard on your lungs.\"---Granted, that's science fiction and there are some real problems with that approach too...https://en.wikipedia.org/wiki/Liquid_breathing#Space_travel> Acceleration protection by liquid immersion is limited by the differential density of body tissues and immersion fluid, limiting the utility of this method to about 15g to 20g. Extending acceleration protection beyond 20g requires filling the lungs with fluid of density similar to water. An astronaut totally immersed in liquid, with liquid inside all body cavities, will feel little effect from extreme G forces because the forces on a liquid are distributed equally, and in all directions simultaneously. Effects will still be felt because of density differences between different body tissues, so an upper acceleration limit still exists. However, it can likely be higher than hundreds of G.> Liquid breathing for acceleration protection may never be practical because of the difficulty of finding a suitable breathing medium of similar density to water that is compatible with lung tissue. Perfluorocarbon fluids are twice as dense as water, hence unsuitable for this application.We're not at the point where we can launch at hundreds of Gs yet (and I'm not sure I would want them to be centripetal Gs initially either - that sounds very not fun) ... but when we get to the point were we want to launch a person into space in the direction of another planet at 10 or 20 or 30 Gs of initial acceleration, this is likely something to be revisited.\n \nreply",
      "Since we are talking about interactions with the nervous system, how do we go from the pressure at which these gasses were delivered to the alveoli to their behavior in the blood?I understand the practice GP is describing pretty well but when I try to simulate what is going on in my mind, it's not like these gasses are in gaseous form once they are in the blood. Are we just talking about the concentration that are reached because the pressure affects the diffusion gradient?\n \nreply",
      "You've got exactly the right image in mind: higher partial pressure of a gas in the lungs corresponds to a higher dissolved concentration of that species (H2, He, N2, O2, whatever) in the blood and tissue.\n \nreply",
      "> people are understandably cautious about breathing a mixture of oxygen and hydrogen.That would be one heck of a case of indigestion. Maybe they can pre-mix it with the pink stuff. Maybe that explains the pink tint to the fluids in The Abyss??\n \nreply"
    ],
    "link": "https://www.mcgill.ca/oss/article/student-contributors-did-you-know/how-deep-can-humans-really-go",
    "first_paragraph": "Up until the late 1960s, physiologists believed that the maximum depth a person could descend to was determined by the depth at which their total lung capacity (TLC) was compressed to the same volume as their residual volume (RV) which is the smallest lung volume a person can breathe to. The TLC was predicted to be reduced to RV at around 30 meters; below this depth, it was believed that total lung collapse would occur. However, in 1968, French FD Jacques Mayol proved the physiologist wrong when he dove to 70.4 meters. Then, on June 9, 2007, Herbert Nitsch set the current world record for the deepest free dive at 214 meters. This astonishing feat challenged everything physiologists thought they knew about the limits of the human body.Freediving is an extreme sport in which the freediver (FD) descends and returns using a single breath. There are five main subcategories of free diving. Of these, the no-limits category presents FDs with the greatest challenge as they descend using a loade"
  },
  {
    "title": "Faster Inverse BWT (2021) (cbloomrants.blogspot.com)",
    "points": 37,
    "submitter": "fanf2",
    "submit_time": "2024-08-22T17:42:02",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=41322655",
    "comments": [
      "The BWT based FM-index is one of my favorite data structures. It's used frequently for DNA mapping, where the 4 letter alphabet can be encoded in two bits and the occurrence function can use clever caching, bit bashing and the pop count function to get nice performance.\n \nreply",
      "BWT is one of those computer things that just seem like pure magic\n \nreply"
    ],
    "link": "http://cbloomrants.blogspot.com/2021/03/faster-inverse-bwt.html",
    "first_paragraph": "\n\nJump to the end for the punch line : speed results and source code.\n\n\n\nLet's briefly review the basic BWT to establish the baseline algorithm and notation.\n\n\n\nGiven an input string like \"inputstring\" add an EOF symbol so S = \"inputstring|\" then form all rotations :\n\n\ninputstring|\nnputstring|i\nputstring|in\nutstring|inp\ntstring|inpu\nstring|input\ntring|inputs\nring|inputst\ning|inputstr\nng|inputstri\ng|inputstrin\n|inputstring\n\n\nThen sort all the rows, this gives the matrix M :\n\n\ng|inputstrin\ning|inputstr\ninputstring|\nng|inputstri\nnputstring|i\nputstring|in\nring|inputst\nstring|input\ntring|inputs\ntstring|inpu\nutstring|inp\n|inputstring\n^          ^\nF          L\n\n\nthis is all the suffixes of the file, and we've sorted them all.  We've labeled the first column F\nand the last column L.\n\n\n\nThe last column L is the BWT, which is what we transmit.  (typically we don't transmit the EOF character,\ninstead we transmit its location and leave it out of the BWT string that is sent).  \n\n\n\nIn the matrix M e"
  },
  {
    "title": "GPU utilization can be a misleading metric (trainy.ai)",
    "points": 88,
    "submitter": "roanakb",
    "submit_time": "2024-08-21T17:25:00",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=41312335",
    "comments": [
      "> you can get 100% GPU utilization by just reading/writing to memory while doing 0 computationsIndeed! Utilization is a proxy for what you actually want (which is good use of available hardware). 100% GPU utilization doesn't actually indicate this.On the other hand, if you aren't getting 100% GPU utilization, you aren't making good use of the hardware.\n \nreply",
      "This sounds like the good old \"having high test coverage is bad because I can get to 100% just by calling functions and doing nothing, asserting nothing with them\".100% test coverage doesn't mean your tests are good, but having 50% (or pick your number) means they are bad / not sufficient.\n \nreply",
      "Yup, similar to SM efficiency in that sense too. If you aren't seeing >80%, there is certainly time left on the table. But getting a high SM efficiency value doesn't guarantee you're making good use of the hardware as well. (still a better proxy than GPU util though)\n \nreply",
      "Application-specific metrics are the way to go. For ML training this is one example:\nhttps://cloud.google.com/blog/products/ai-machine-learning/g...\n \nreply",
      "Nice, seems like ML Productivity Goodput is a pretty well thought-out metric to understand the overall efficiency of your cluster. I'll consider adding this into our cluster management platform. Only potential drawbacks I'd guess are it being somewhat difficult to compute since it relies on metrics like MFUs, and not something we can observe layer-by-layer to understand inefficient kernels, but I'll take a deeper look. Thanks!\n \nreply",
      "If you have a basic understanding of what your kernels are supposed to do, looking at pipe usage and roofline analysis in Nsight Compute is often helpful, since it will show you how hard you\u2019re saturating those.\n \nreply",
      "When understanding the performance of your model it's very helpful to look at a roofline plot [1].  The roofline plot will show you the floating-point performance as a function of arithmetic intensity for the various ops in your model.  The plot has two regimes: a memory-bound regime on the left and a compute-bound regime on the right.  This can help to identify memory-bound ops that are taking a significant fraction of compute time.[1]: https://en.wikipedia.org/wiki/Roofline_model\n \nreply",
      "Agreed, roofline plots would be quite powerful in this context. From a quick search, seems like the only way to create a roofline plot for your model would be to use Nsight [1]? Would be interested to know if there are any simpler tools, since one of the big benefits of SM efficiency is how easily the metric is accessed.[1]: https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s...\n \nreply",
      "running GPU models and maximizing utilization is pretty opaque to me as a layman coming into the scene.take this example: https://gist.github.com/sergiotapia/efc9b3f7163ba803a260b481... - running a fairly simple model that takes only 70ms per image pair, but because I have 300 images it becomes a big time sink.by using ThreadPoolExecutor, I cut that down to about 16 seconds. i wonder if there is a fairly obvious way to truly utlize my beefy L40S GPU! is it MPS? I haven't been successful at even running the MPS daemon on my linux server yet. very opaque for sure!\n \nreply",
      "Start with Nsight Systems and turn on GPU metrics. It\u2019s super easy and the plots will give you an immediate sense of your utilization, and low-hanging optimization opportunities.So using 10-wide parallel processing took your batch from 21 seconds down to 16 seconds, did I do the arithmetic correctly? That suggests the single-threaded version isn\u2019t too bad. I mean a 25% improvement is great and nothing to sneeze at, but batching might only be trimming the gaps in between image pairs, or queueing up your memory copies while the previous inference is running. You can verify this with nsys profiles.> i wonder if there is a fairly obvious way to truly utilize my beefy L40S GPU! is it MPS?No idea, it\u2019s not always easy (and generally speaking gets harder and harder as you approach 100%), but first profile to see what your utilization is before going down any big technical route. Maybe with your ThreadPoolExecutor, you\u2019re already getting max utilization and using MPS can\u2019t possibly help.\n \nreply"
    ],
    "link": "https://trainy.ai/blog/gpu-utilization-misleading",
    "first_paragraph": "The most common metric ML teams use to understand GPU usage is GPU Utilization, commonly accessed by running nvidia-smi from your terminal. Many integrated observability tools track GPU Utilization as their main performance metric as well. Somewhat to our surprise, it turns out that this is not always the best metric for understanding your GPU performance. In fact, you can get 100% GPU utilization by just reading/writing to memory while doing 0 computations! This article is about how we found that out, and what else we learned along the way.At Trainy, we work on infrastructure for managing GPU clusters, so we spend a lot of time thinking about this stuff. Last year we were working with a foundation model company to scale out and improve the efficiency of their LLM training. We went through the basic steps that are mentioned in practically every guide about performance tuning for Pytorch\u2014namely:These simple changes got us to 100% GPU utilization and significant power draw, which is grea"
  }
]