[
  {
    "title": "Veo 3 and Imagen 4, and a new tool for filmmaking called Flow (blog.google)",
    "points": 449,
    "submitter": "youssefarizk",
    "submit_time": "2025-05-20T17:46:36 1747763196",
    "num_comments": 263,
    "comments_url": "https://news.ycombinator.com/item?id=44044043",
    "comments": [
      "After doing some testing, Imagen 4 doesn't score any higher than Imagen 3 on my comparison chart, approximately ~60% prompt adherence accuracy.https://genai-showdown.specr.net\n \nreply",
      "I'm curious why you decide to declare victory after one successful attempt, but try many times for unsuccessful models. Are you trying to measure whether a model _can_ get it right, or whether it frequently _does_ get it right? I feel like success rate is a better metric here, or at least a fixed number of trials with some success rate threshold to determine model success.\n \nreply",
      "The winning image entry for \"The Yarrctic Circle\" by OpenAI 4o doesn't actually wields a cutlass. It's very aesthetically pleasing, even though it's so wrong in all fundamental aspects (perspective is nonsensical and anatomy is messed up, with one leg 150% longer than the other, ...).It's a very interesting resource to map some of the limits of existing models.\n \nreply",
      "In my own testing between the two this is what I\u2019ve noticed. Imagen will follow the instructions, and 4o will often not, but produces aesthetically more pleasing images.I don\u2019t know which is more important, but I would say that people mostly won\u2019t pay for fun but disposable images, and I think people will pay for art but there will be an increased emphasis on the human artist. However users might pay for reliable tools that can generate images for a purpose, things like educational illustrations, and those need to be able to follow the spec very well.\n \nreply",
      "People pay for digital sticker packs so their memoji in iMessage are customized. How much money they make on sticker packs is unknown to me, but image generation platform Midjourney seems to be doing alright.\n \nreply",
      "Good catch - that's on me I accidentally uploaded the wrong image for gpt-image-1. Fixed!\n \nreply",
      "Google Flow is remarkable as video editing UX, but Imagen 4 doesn't really stand out amongst its image gen peers.I want to interrupt all of this hype over Imagen 4 to talk about the totally slept on Tencent Hunyuan Image 2.0 that stealthily launched last Friday. It's absolutely remarkable and features:- millisecond generation times- real time image-to-image drawing capabilities- visual instructivity (eg. you can circle regions, draw arrows, and write prompts addressing them.)- incredible prompt adherence and qualityNothing else on the market has these properties in quite this combination, so it's rather unique.Release Tweet: https://x.com/TencentHunyuan/status/1923263203825549457Tencent Hunyuan had a bunch of model releases all wrapped up in a product that they call \"Hunyuan Game\", but the Hunyuan Image 2.0 real time drawing canvas is the real star of it all. It's basically a faster, higher quality Krea: https://x.com/TencentHunyuan/status/1924713242150273424More real time canvas samples: https://youtu.be/tVgT42iI31c?si=WEuvie-fIDaGk2J6&t=141 (I haven't found any other videos on the internet apart from these two.)You can see how this is an incredible illustration tool. If they were to open source this, this would immediately become the top image generation model over Flux, Imagen 4, etc. At this point, really only gpt-image-1 stands apart as having godlike instructivity, but it's on the other end of the [real time <--> instructive] spectrum.A total creative image tool kit might just be gpt-image-1 and Hunyuan Image 2.0. The other models are degenerate cases.More image samples: https://x.com/Gdgtify/status/1923374102653317545If anyone from Tencent or the Hunyuan team is reading this: PLEASE, PLEASE, PLEASE OPEN SOURCE THIS. (PLEASE!!)\n \nreply",
      "This is amazing, can\u2019t see how I\u2019ve missed it. Thank you!\n \nreply",
      "> but Imagen 4 doesn't really stand out amongst its image gen peers.In this AI rat race, whenever one model gets ahead, they all tend to reach parity within 3-6 months. If you can wait 6 months to create your video I'm sure Imagen 5 will be more than good enough.It's honestly kind of ridiculous the pace things are moving at these days. 10 years ago waiting a year for something was very normal, nowadays people are judging the model-of-the-week against last week's model-of-the-week but last week's org will probably not sleep and they'll release another one next week.\n \nreply",
      "I can't find the image you're talking about. Link pls?\n \nreply"
    ],
    "link": "https://blog.google/technology/ai/generative-media-models-io-2025/",
    "first_paragraph": "May 20, 2025[[read-time]] min read\n          Introducing Veo 3 and Imagen 4, and a new tool for filmmaking called Flow.\n        Today, we\u2019re announcing our newest generative media models, which mark significant breakthroughs. These models create breathtaking images, videos and music, empowering artists to bring their creative vision to life. They also power amazing tools for everyone to express themselves.Veo 3 and Imagen 4, our newest video and image generation models, push the frontier of media generation, with their groundbreaking new capabilities. We're also expanding access to Lyria 2, giving musicians more tools to create music. Finally, we\u2019re inviting visual storytellers to try Flow, our new AI filmmaking tool. Using Google DeepMind\u2019s most advanced models, Flow lets you weave cinematic films with more sophisticated control of characters, scenes and styles, to bring your story to life.We\u2019ve partnered closely with the creative industries \u2014 filmmakers, musicians, artists, YouTube c"
  },
  {
    "title": "The Value Isn't in the Code (jonayre.uk)",
    "points": 59,
    "submitter": "fragmede",
    "submit_time": "2025-05-20T23:33:27 1747784007",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=44046955",
    "comments": [
      "Couple months back, someone posted how they lost a days work due to hard drive crash and had to redo it. It took them roughly 30 minutes.Their point was the same as this article with a shorter time window. Knowing what to do, not how to do it, is 90% of the battle.But that is counterintuitive to the lay observer of software. They think they know what to do, because they\u2019ve got ideas, but feel inhibited because they don\u2019t yet know how to achieve them. So they assume that their immediate hurdle must be the hard part of software development.\n \nreply",
      "This is a very good point that has been proven over and over again in the industry. I recall being at Sun and having the argument over ONC and whether or not it should be \"open\" (which at the time meant everyone could get a copy of the code[1]) or \"closed\". Ed Zander was a big fan of keeping everything secret, after all anyone could reproduce it if they had the code right? And I used the same argument as the author, which is that if someone was a decent programmer and willing to invest the time, they could recreate it from scratch without the code so keeping the code secret merely slowed them down fractionally but letting our licensees read the code allowed them to better understand what worked and why and could release products that used it faster, which would contribute to its success in the marketplace.I lost that battle and ONC+ was locked behind the wall until Open Solaris 20 years later. So many people in tech cannot (or perhaps will not) distinguish between \"value\" and \"cost\". Its like people who confuse \"wealth\" and \"money\". Closely related topics that are fundamentally talking about different things.This is why you invest in people and expertise, not tools. Anyone can learn a new toolset, but only the people with expertise can create things of value.[1] So still licensed, but you couldn't use the trademark if you didn't license it and of course there was no 'warranty' because of course the trademark required an interoperability test.\n \nreply",
      "There is a feeling that releasing the code is \"giving it away for free\".  But, being able to compile and deploy it is not the whole story.  Enterprises need support from the people who built the thing, and so without that it is not a very attractive proposition. \n It could be true in some scenarios though.Microsoft doesn't open source Windows.  A big enough company could fork it and offer enterprise support at a fraction of the cost.  It would take them years to get there, and probably would be subpar to what large Windows customers get in support from Microsoft.  Yes I know y'all hate dealing with Microsoft support - imagine that but worse.  Still, the company with the forked distro would definitely take a bite out of Microsoft's Windows business, if only a small one.\n \nreply",
      "> Still, the company with the forked distro would definitely take a bite out of Microsoft's Windows business, if only a small one.That has not been shown to be the case. There is ample evidence that other companies would run this 'off market' or 'pirate' version, and zero evidence that if those choices had been unavailable that they would have legitimately licensed Windows.You are making a variant on the 'piracy losses' argument which has been shown is simply a pricing issue. If you \"ask\" for more than your product is \"valued\" then it won't be purchased but it may be stolen. And if you make it \"impossible\" to steal you will reduce its value to legitimate customers and have zero gain in revenue from those who had stolen it before (they still won't buy it).The \"value\" in Windows is the number of things that run on it and the fact that compatibility issues are \"bugs\" which get fixed by the supplier. We are rapidly reaching the point where it will add value to have an operating system for AMD64 hardware that is overtly governed (not Linux or FOSS) which allows you to get a copy of the source when you license it, and has an application binary interface (ABI) that other software developers can count on to exist, not change out from under them, and last for 10+ years.As Microsoft (and Apple) add more and more spurious features which enrich themselves and enrage their users the \"value\" becomes less and less. That calculus will flip and when it does enterprises will switch to the new operating system that is just an operating system and not a malware delivery platform.\n \nreply",
      ">>have an operating system for AMD64 hardware that is overtly governed (not Linux or FOSS)Not understanding this part, aren't Linux distros achieving this already without licence restrictions and various levels of stability depending on the distro selected?A huge amount of enterprise tooling is now being run on the cloud through the browser or via electron - for a large number of businesses, their staff would only need the equivalent of a Chromebook style GUI to perform their work.Native software is still essential for a small % of users.. is this what you're suggesting needs to be solved? A single alternative open source system (OS or VM?) that the software dev company can target.\n \nreply",
      "> And I\u2019d go further than that. I\u2019d suggest that, contrary to what intuition might tell you, refactoring might be better achieved by throwing the code away and starting again.I don't think this applies in most situations. If you have been part of the original core team and are rewriting the app in the same way, this might be true - basically a lost code situation, like the author was in.However, if you are doing so because you lack understanding of the original code or you are switching the stack, you will inevitably find new obstacles and repeat mistakes that were fixed in the original prototype. Also, in a real world situation, you probably also have to handle fun things like data import/migration, upgrading production instances and serving customers (and possibly fixing bugs) while having your rewrite as a side project. I'm not saying that a rewrite is never the answer, but the authors situation was pretty unique.\n \nreply",
      "Anyone truly considering this should weigh up this post with the timeless wisdom in Joel Spolsky's seminal piece, 'Things You Should Never Do'[1]. Rewriting from scratch can often be a very costly mistake. Granted, it's not as simple as \"never do this\" but it's not a decision one should make lightly.1: https://www.joelonsoftware.com/2000/04/06/things-you-should-...\n \nreply",
      "Fifteen years ago I agreed with his point. Today I do not.\n \nreply",
      "You've hit on an important point in the article:> if you are doing so because you lack understanding of the original codeAs I understood it, the key point of the article is that the understanding is the value. If you don't understand the code, then you've lost the value. That's why rebuilds by new folk who don't understand the solution don't work.\n \nreply",
      "Large sweeping software initiatives that go nowhere and are replaced by a product from a more agile team aspect isn't that unique, though the author being on both teams is.\n \nreply"
    ],
    "link": "https://jonayre.uk/blog/2022/10/30/the-real-value-isnt-in-the-code/",
    "first_paragraph": " Jon AyreBusiness, technology and strategic thinkingBusiness, technology and strategic thinkingOkay, I\u2019ll admit that calling your software worthless was a shameless clickbait tactic. However, I will assert that it\u2019s not as valuable or indispensable as you might think it is. You need two things to solve a problem using software, skill and time.Skill. Of course, you can create code using unskilled practitioners. The code might even work, but that\u2019s not the same thing as solving the problem. Bad code is an issue in itself, so even if you solve your original problem, you now have a new, possibly bigger one to deal with. Skill is essential if you\u2019re going to succeed and talent doesn\u2019t come cheap.Time. Coding can happen quickly, but solving problems is tricky. It takes time, and time is money. A software development team is not cheap to run, so any decent piece of software is going to incur significant cost. If you\u2019re going to put investment into your solution, you should expect it to be wor"
  },
  {
    "title": "Litestream: Revamped (fly.io)",
    "points": 222,
    "submitter": "usrme",
    "submit_time": "2025-05-20T19:58:27 1747771107",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=44045292",
    "comments": [
      "Looks like the code is here: https://github.com/benbjohnson/litestream/tree/v0.5Really nice to see this, I wrote this comment almost 2 years ago when I was a little miffed about trying to use litestream and litefs: https://news.ycombinator.com/item?id=37614193I think this solves most of the issues? You can now freely run litestream on your DB and not worry about issues with multiple writers? I wonder how the handoff is handled.The read replica FUSE layer sounds like a real nice thing to have.edit: Ah, it works like this: https://github.com/benbjohnson/litestream/pull/617> When another Litestream process starts up and sees an existing lease, it will continually retry the lease acquisition every second until it succeeds. This low retry interval allows for rolling restarts to come online quickly.Sounds workable!\n \nreply",
      "This post is like they read my mind and implemented everything I wanted from a new Litestream. So exciting.\n \nreply",
      "Is there anything like Livestream that can be just pip installed?\n \nreply",
      "Just a heads-up, the link in the \"Litestream is fully open source\" callout is malformed and leads to:https://http//litestream.io/\n \nreply",
      "Thanks!\n \nreply",
      "So fossil (which is built on top of sqlite) + this = SCM?\n \nreply",
      "I wish Fly would polish the developer experience on top of SQLite. They're close, but it's missing:1. A built-in UI and CLI that manages SQLite from a volume. Getting the initial database on a Fly Machine requires more work than it should.2. `fly console` doesn't work with SQLite because it spins up a separate machine, which isn't connected to the same volume where the SQLite data resides. Instead you have to know to run `fly ssh console \u2014pty`, which effectively SSH's into the machine with the database.The problem in general with SQLite web apps is they tend to be small apps, so you need a lot of them to make a decent amount of money hosting them.\n \nreply",
      "If you wanted to use litestream to replicate many databases (ideally, one or more per user), which is one of the use cases described here (and elsewhere), how do you tell litestream to add new databases dynamically? The configuration file is static and I haven't found an API to tell it to track a new db at runtime.\n \nreply",
      "I would expect this problem to get solved. It's tricky to detect new sqlites, but not impossible.In the meantime, it's pretty straightforward to use as a library.\n \nreply",
      "Is Litestream on a path to subsume LiteFS's capabilities? Re: PITR, would this be used to facilitate automated A/B testing of AI-generated code changes against live data subsets? I can imagine a lot of cool stuff in that direction. This is really cool Ben!\n \nreply"
    ],
    "link": "https://fly.io/blog/litestream-revamped/",
    "first_paragraph": "Litestream is an open-source tool that makes it possible to run many kinds of full-stack applications on top of SQLite by making them reliably recoverable from object storage. This is a post about the biggest change we\u2019ve made to it since I launched it.Nearly a decade ago, I got a bug up my ass. I wanted to build full-stack applications quickly. But the conventional n-tier database design required me to do sysadmin work for each app I shipped. Even the simplest applications depended on heavy-weight database servers like Postgres or MySQL.I wanted to launch apps on SQLite, because SQLite is easy. But SQLite is embedded, not a server, which at the time implied that the data for my application lived (and died) with just one server.So in 2020, I wrote Litestream to fix that.Litestream is a tool that runs alongside a SQLite application. Without changing that running application, it takes over the WAL checkpointing process to continuously stream database updates to an S3-compatible object st"
  },
  {
    "title": "Gemma 3n preview: Mobile-first AI (googleblog.com)",
    "points": 216,
    "submitter": "meetpateltech",
    "submit_time": "2025-05-20T18:03:32 1747764212",
    "num_comments": 81,
    "comments_url": "https://news.ycombinator.com/item?id=44044199",
    "comments": [
      "You can try it on Android right now:Download the Edge Gallery apk from github: https://github.com/google-ai-edge/gallery/releases/tag/1.0.0Download one of the .task files from huggingface: https://huggingface.co/collections/google/gemma-3n-preview-6...Import the .task file in Edge Gallery with the + bottom right.You can take pictures right from the app. The model is indeed pretty fast.\n \nreply",
      "Okay from some first tries with story writing, gemma-3n-E4B-it seems to perform between plain Gemma 3 4B and 12B. It definitely retains the strong instruction following which is good.Hint: You have to set the Max tokens to 32000 for longer conversations. The slider makes it look like it's limited to 1024, just enter it manually.\n \nreply",
      "Thanks for this guide it's great.Okay perhaps my phones not great and perhaps this isn't optimized/pruned for phone use but it's unusably slow. The answers are solid from my brief test.I wouldn't exactly say phone use, unless you have no internet and you don't mind a bit of a wait.Really impressive, regardless.\n \nreply",
      "What phone are you using?\n \nreply",
      "waiting for approval, is there a magnet?\n \nreply",
      "if you go into the app and click the first icon it directs you to a workflow to get approved after clicking on a button that is the same color as the background and jump through some hoops about providing user data and analytics etc then it will auto-approve you\n \nreply",
      "It reminds me of GPT3 quality answers. Kind of impressive.Although my entire usecase of local models is amoral questions, which it blocks. Excited for the abliterated version.\n \nreply",
      "And the libraries to embed Gemma-series in your iOS/Android app: https://ai.google.dev/edge/litertOr, run them on a microcontroller! https://github.com/tensorflow/tflite-micro\n \nreply",
      "I assume that \"pretty fast\" depends on the phone. My old Pixel 4a ran Gemma-3n-E2B-it-int4 without problems. Still, it took over 10 minutes to finish answering \"What can you see?\" when given an image from my recent photos.Final stats:15.9 seconds to first token16.4 tokens/second prefill speed0.33 tokens/second decode speed662 seconds to complete the answer\n \nreply",
      "I did the same thing on my Pixel Fold. Tried two different images with two different prompts: \"What can you see?\" and \"Describe this image\"First image ('Describe', photo of my desk)- 15.6 seconds to first token- 2.6 tokens/second- Total 180 secondsSecond image ('What can you see?', photo of a bowl of pasta)- 10.3 seconds to first token- 3.1 tokens/second- Total 26 secondsThe Edge Gallery app defaults to CPU as the accelerator. Switched to GPU.Pasta / what can you see:- It actually takes a full 1-2 minutes to start printing tokens. But the stats say 4.2 seconds to first token...- 5.8 tokens/second- 12 seconds totalDesk / describe:- The output is: while True: print(\"[toxicity=0]\")- Bugged? I stopped it after 80 seconds of output. 1st token after 4.1 seconds, then 5.7 tokens/second.\n \nreply"
    ],
    "link": "https://developers.googleblog.com/en/introducing-gemma-3n/",
    "first_paragraph": "Following the exciting launches of Gemma 3 and Gemma 3 QAT, our family of state-of-the-art open models capable of running on a single cloud or desktop accelerator, we're pushing our vision for accessible AI even further. Gemma 3 delivered powerful capabilities for developers, and we're now extending that vision to highly capable, real-time AI operating directly on the devices you use every day \u2013 your phones, tablets, and laptops.To power the next generation of on-device AI and support a diverse range of applications, including advancing the capabilities of Gemini Nano, we engineered a new, cutting-edge architecture. This next-generation foundation was created in close collaboration with mobile hardware leaders like Qualcomm Technologies, MediaTek, and Samsung System LSI, and is optimized for lightning-fast, multimodal AI, enabling truly personal and private experiences directly on your device.Gemma 3n is our first open model built on this groundbreaking, shared architecture, allowing d"
  },
  {
    "title": "\u201cZLinq\u201d, a Zero-Allocation LINQ Library for .NET (neuecc.medium.com)",
    "points": 65,
    "submitter": "cempaka",
    "submit_time": "2025-05-20T22:29:12 1747780152",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=44046578",
    "comments": [
      "In theory .Net 10 should make this obsolete, the headline features[1] are basically all about this. In practice, well, it's heuristics, I'm adding this to a particularly performance sensitive project right now :)Edit: what's also nice is that C# recognizes Linq as a contract. So long as this has the correct method names and signatures (it does), the Linq syntax will light up automatically. You can also use this trick for your own home-grown things (add Select, Join, Where, etc. overloads) if the Linq syntax is something you like.[1]: https://learn.microsoft.com/en-us/dotnet/core/whats-new/dotn...\n \nreply",
      "Could you elaborate? I don\u2019t see anything about improving the performance of enumerator. Zlinq appears to remove the penalty of allocating enumerators on the heap to be garbage collected. The link you sent mention improvements, but I don\u2019t see how they lead to linq avoiding heap allocations.\n \nreply",
      "Not just that but Zlinq also works across all C# environments it seems including versions embedded in game engines like Godot, Unity, .NET Standard, .NET 8 and 9.\n \nreply",
      "I believe they're referring to the stack allocation improvements, which would ideally allow all the LINQ temporary objects to live on the stack. I'm not sure whether it does in practice though.\n \nreply",
      "Unfortunately, those improvements don't work for Linq.Some notes on why this is so here: https://github.com/dotnet/runtime/blob/main/docs/design/core...\n \nreply",
      "This is great. I've worked on production .NET services and we often had to avoid LINQ in hot paths because of the allocations. Reimplementing functions with for-loops and other structures was time-consuming and error-prone compared to LINQ method chaining. Chaining LINQ methods is extremely powerful; like filter, map, and reduce in JS but with a bunch of other operators and optimizations. I wish more languages had something like it.\n \nreply",
      "This is cool - excited to try it - I would note that I've been a dotnet grunt for almost 15 years now. I am good at it, I know how to use the language, I know the ecosystem - this level of familiarity with the language is just not within my grasp. I can understand the code (mostly) reading it, but I never would have been able to conjure up, let alone implement this. Props to the author.\n \nreply",
      "What features does C# has that makes LINQ possible in it and not in other languages?\n \nreply",
      "It's part of the compiler - ast. Linq has two forms - one in the linq ordinary syntaxfrom x \nselect x.nameAnd other is just lambda with anonymous types and so on.For the lambda syntax, you can just do this:\nhttps://www.npmjs.com/package/linq\n \nreply",
      "I feel like pretty much every language with generics has a LINQ, like functools/itertools in Python, lodash for javascript. It\u2019s just a different expression of the same ideas.\n \nreply"
    ],
    "link": "https://neuecc.medium.com/zlinq-a-zero-allocation-linq-library-for-net-1bb0a3e5c749",
    "first_paragraph": ""
  },
  {
    "title": "Semantic search engine for ArXiv, biorxiv and medrxiv (arxivxplorer.com)",
    "points": 54,
    "submitter": "0101111101",
    "submit_time": "2025-05-20T21:49:36 1747777776",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=44046277",
    "comments": [
      "embedding search via https://searchthearxiv.com/ takes either a word vector, or an abs or pdf link to an arxiv paper.https://news.ycombinator.com/item?id=42519487I just did a spot check, I think searchthearxiv search results are superior.\n \nreply",
      "There\u2019s also the search and browsing on https://sugaku.net, it\u2019s more focused on math but does also have all of the arxiv on it\n \nreply",
      "Looks cool! \nYou can input either a search query or a paper URL on arxiv xplorer. You can even combine paper URLs to search for combinations of ideas by putting + or - before the URL, like `+ 2501.12948 + 1712.01815`\n \nreply",
      "This is really cool, and very relevant to something I'm working on. Would you be willing to do a quick explanation of the build?\n \nreply",
      "Sure! I first used openai embeddings on all the paper titles, abstracts and authors. When a user submits a search query, I embed the query, find the closest matching papers and return those results. Nothing too fancy involved!I'm also maintaining a dataset of all the embeddings on kaggle if you want to use them yourself: https://www.kaggle.com/datasets/tomtum/openai-arxiv-embeddin...\n \nreply",
      "Looks great! Could you add eprint.iacr.org (Cryptology ePrint Archive)?\n \nreply",
      "Do they have a public API/dataset?\n \nreply",
      "Oh god, there's a medrxiv?? TIL...Don't forget chemrXiv!\n \nreply",
      "Sadly I couldn't find a public API for chemrxiv, but would be happy to be proven wrong!\n \nreply"
    ],
    "link": "https://arxivxplorer.com/",
    "first_paragraph": "GuideScrollThank you to arXiv, bioRxiv and medRxiv for their open access interoperability.Built by ttumiel  with OpenAI's Embeddings."
  },
  {
    "title": "The NSA Selector (github.com/wenzellabs)",
    "points": 174,
    "submitter": "anigbrowl",
    "submit_time": "2025-05-20T18:30:18 1747765818",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=44044459",
    "comments": [
      "> if possible disable encryption, then you can profit from not only timing pattern (of white noise), but also listen in on the plaintext payload. the NSA loves plaintext.Haha, incredible.On a more serious note this is a really cool idea.  Would be interesting to listen to the same origin traffic in different network conditions to hear things like TCP rate control.\n \nreply",
      "SSL added and removed here! :-)https://www.agwa.name/blog/post/cloudflare_ssl_added_and_rem...\n \nreply",
      "This went over my head at first, but I really like it. So for those like me: it converts network traffic into audio output.YouTube explainer: https://youtu.be/vfgySTaM1TI\n \nreply",
      "Yes kinda, I would say network activity rather then traffic.  Audio signal is going to be in scale of 48Khz while measuring ethernet signal at scale of 100Mhz.  At that rate it wouldn't even get more then 1 sample from a full size packet.  So really it's polling 48Khz whether or not there was activity during that period.  The gimmick is that it uses some analog components.  Fully digital you could craft a meaningful audio signal that represents traffic.\n \nreply",
      "For those interested in hearing some beats, the terminal demo starts at 4:34 https://youtu.be/vfgySTaM1TI?t=274\n \nreply",
      "This is a fun demonstration of the principle.\"every website sounds different\" - that's super cool.The bitmap images sent at the end of the video also sound really cool.\n \nreply",
      "In NSA parlance, a \"selector\" primarily is a string that semi-uniquely identifies and addresses a persons intercepted data, such as- an IP address,- an email address,- a phone number,- a SIM card's MSIN- a person's social security number,- a national ID card number,- a passport number,- a social media handle etc.(elsewhere also known as \"accessor\", \"key\", \"handle\" or \"index\")\n \nreply",
      "That doesn't tell me why this is called selector.\n \nreply",
      "> In CSS, selectors are patterns used to match, or select, the elements you want to style. Selectors are also used in JavaScript to enable selecting the DOM nodes to return as a NodeList.From [1]. This nomenclature was also used way before CSS even existed; in SQL.[1] https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_selecto...\n \nreply",
      "They are interesting because combining and updating them is a non-trivial problem, as I've realized today while implementing a user ban system.\n \nreply"
    ],
    "link": "https://github.com/wenzellabs/the_NSA_selector",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        the NSA selector eurorack module\n      you can purchase the NSA selector at my little shop over at lectronz.  the NSA selector is a eurorack module with two ethernet jacks and one audio output.\nany bit on the network will be sent to the audio output.this is not an \"audio interface\". we do not play back any \"format\" such as RTP\nor MP3 or WAV ot the like. the eurorack module does not \"speak\" any protocol.\nall traffic is forwarded from one network jack to the other unmodified.\nit's just tapped, intercepted to convert it to audio.watch the NSA selector videoin the folder sequencer/ you find a very simple shell script, that mimics a\nsequencer by network pings of different size.if we transfer uncompressed, unencrypted images e.g. in the .bmp format,\nwe can hear the pixels. together with a small http server which is available\nin the filese"
  },
  {
    "title": "Deep Learning Is Applied Topology (theahura.substack.com)",
    "points": 348,
    "submitter": "theahura",
    "submit_time": "2025-05-20T13:54:54 1747749294",
    "num_comments": 155,
    "comments_url": "https://news.ycombinator.com/item?id=44041738",
    "comments": [
      "Since this post is based on my 2014 blog post (https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ ), I thought I might comment.I tried really hard to use topology as a way to understand neural networks, for example in these follow ups:- https://colah.github.io/posts/2014-10-Visualizing-MNIST/- https://colah.github.io/posts/2015-01-Visualizing-Representa...There are places I've found the topological perspective useful, but after a decade of grappling with trying to understand what goes on inside neural networks, I just haven't gotten that much traction out of it.I've had a lot more success with:* The linear representation hypothesis - The idea that \"concepts\" (features) correspond to directions in neural networks.* The idea of circuits - networks of such connected concepts.Some selected related writing:- https://distill.pub/2020/circuits/zoom-in/- https://transformer-circuits.pub/2022/mech-interp-essay/inde...- https://transformer-circuits.pub/2025/attribution-graphs/bio...\n \nreply",
      "Related to ways of understanding neural networks, I've seen these views expressed a lot, which to me seem like misconceptions:- LLMs are basically just slightly better `n-gram` models- The idea of \"just\" predicting the next token, as if next-token-prediction implies a model must be dumb(I wonder if this [1] popular response to Karpathy's RNN [2] post is partly to blame for people equating language neural nets with n-gram models. The stochastic parrot paper [3] also somewhat equates LLMs and n-gram models, e.g. \"although she primarily had n-gram models in mind, the conclusions remain apt and relevant\". I guess there was a time where they were more equivalent, before the nets got really really good)[1] https://nbviewer.org/gist/yoavg/d76121dfde2618422139[2] https://karpathy.github.io/2015/05/21/rnn-effectiveness/[3] https://dl.acm.org/doi/pdf/10.1145/3442188.3445922\n \nreply",
      "I guess I'll plug my hobby horse:The whole discourse of \"stochastic parrots\" and \"do models understand\" and so on is deeply unhealthy because it should be scientific questions about mechanism, and people don't have a vocabulary for discussing the range of mechanisms which might exist inside a neural network. So instead we have lots of arguments where people project meaning onto very fuzzy ideas and the argument doesn't ground out to scientific, empirical claims.Our recent paper reverse engineers the computation neural networks use to answer in a number of interesting cases (https://transformer-circuits.pub/2025/attribution-graphs/bio... ). We find computation that one might informally describe as \"multi-step inference\", \"planning\", and so on. I think it's maybe clarifying for this, because it grounds out to very specific empirical claims about mechanism (which we test by intervention experiments).Of course, one can disagree with the informal language we use. I'm happy for people to use whatever language they want! I think in an ideal world, we'd move more towards talking about concrete mechanism, and we need to develop ways to talk about these informally.There was previous discussion of our paper here: https://news.ycombinator.com/item?id=43505748\n \nreply",
      "1) Isn't it unavoidable that a transformer - a sequential multi-layer architecture - is doing multi-step inference ?!2) There are two aspects to a rhyming poem:a) It is a poem, so must have a fairly high degree of thematic coherenceb) It rhymes, so must have end-of-line rhyming wordsIt seems that to learn to predict (hence generate) a rhyming poem, both of these requirements (theme/story continuation+rhyming) would need to be predicted (\"planned\") at least by the beginning of the line, since they are inter-related.In contrast, a genre like freestyle rap may also rhyme, but flow is what matters and thematic coherence and rhyming may suffer as a result. In learning to predict (hence generate) freestyle, an LLM might therefore be expected to learn that genre-specific improv is what to expect, and that rhyming is of secondary importance, so one might expect less rhyme-based prediction (\"planning\") at the start of each bar (line).\n \nreply",
      "Absolutely, the first task should be to understand how and why black boxes with emergent properties actually work, in order to further knowledge - but importantly, in order to improve them and build on the acquired knowledge to surpass them. That implies, curbing \u00abparrot[ing]\u00bb and inadequate \u00abunderstand[ing]\u00bb.I.e. those higher concepts are kept in mind as a goal. It is healthy: it keeps the aim alive.\n \nreply",
      "Regardless of the mechanism, the foundational 'conceit' of LLMs is that by dumping enough syntax (and only syntax) into a sufficiently complex system, the semantics can be induced to emerge.Quite a stretch, in my opinion (cf. Plato's Cave).\n \nreply",
      "1000%. It's really hard to express this to non-engineers who never wasted years of their life trying to work with n-grams and NLTK (even topic models) to make sense of textual data... Projects I dreamed of circa 2012 are now completely trivial. If you do have that comparison ready-at-hand, the problem of understanding what this mind-blowing leap means, to which end I find writing like the OP helpful, is so fascinating and something completely different than complaining that it's a \"black box.\"I've expressed this on here before, but it feels like the everyday reception of LLMs has been so damaged by the general public having just gotten a basic grasp on the existence of machine learning.\n \nreply",
      "This has mirrored my experience attempting to \"apply\" topology in real world circumstances, off and on since I first studied topology in 2011.I even hesitate now at the common refrain \"real world data approximates a smooth, low dimensional manifold.\" I want to spend some time really investigating to what extent this claim actually holds for real world data, and to what extent it is distorted by the dimensionality reduction method we apply to natural data sets in order to promote efficiency. But alas, who has the time?\n \nreply",
      "Thanks for the follow up. I've been following your circuits thread for several years now. I find the linear representation hypothesis very compelling, and I have a draft of a review for Toy Models of Superposition sitting in my notes. Circuits I find less compelling, since the analysis there feels very tied to the transformer architecture in specific, but what do I know.Re linear representation hypothesis, surely it depends on the architecture? GANs, VAEs, CLIP, etc. seem to explicitly model manifolds. And even simple models will, due to optimization pressure, collapse similar-enough features into the same linear direction. I suppose it's hard to reconcile the manifold hypothesis with the empirical evidence that simple models will place similar-ish features in orthogonal directions, but surely that has more to do with the loss that is being optimized? In Toy Models of Superposition, you're using a MSE which effectively makes the model learn an autoencoder regression / compression task. Makes sense then that the interference patterns between co-occurring features would matter. But in a different setting, say a contrastive loss objective, I suspect you wouldn't see that same interference minimization behavior.\n \nreply",
      "> Circuits I find less compelling, since the analysis there feels very tied to the transformer architecture in specific, but what do I know.I don't think circuits is specific to transformers? Our work in the Transformer Circuits thread often is, but the original circuits work was done on convolutional vision models (https://distill.pub/2020/circuits/ )> Re linear representation hypothesis, surely it depends on the architecture? GANs, VAEs, CLIP, etc. seem to explicitly model manifolds(1) There are actually quite a few examples of seemingly linear representations in GANs, VAEs, etc (see discussion in Toy Models for examples).(2) Linear representations aren't necessarily in tension with the manifold hypothesis.(3) GANs/VAEs/etc modeling things as a latent gaussian space is actually way more natural if you allow superposition (which requires linear representations) since central limit theorem allows superposition to produce Gaussian-like distributions.\n \nreply"
    ],
    "link": "https://theahura.substack.com/p/deep-learning-is-applied-topology",
    "first_paragraph": ""
  },
  {
    "title": "Magic of software; what makes a good engineer also makes a good engineering org (moxie.org)",
    "points": 41,
    "submitter": "kiyanwang",
    "submit_time": "2025-05-19T05:26:32 1747632392",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=44026703",
    "comments": [
      "> The people who create software generally refer to themselves as software engineersEngineers follow processes and measure things. If a developer is not measuring things, most are not, then they are not engineering. If, at least, they are doing something new they are writing. Otherwise, they are typing. Software employs a lot of typists.If you want to select for excellent engineers you only need to identify two qualities: conscientiousness and persistence. Conscientiousness is awareness of the world outside yourself and is negatively correlated with intelligence. Conscientiousness is where you find disciplined industriousness. Add a little bit of intelligence and you also get people that can follow complex instructions and achieve high precision. Creativity is the child of persistence and high intelligence. Creativity is achieved by trying many variations of a process and carefully discerning quality against some audience or metric.If you hire a bunch of clowns you get a circus.\n \nreply",
      "> Software employs a lot of typists.We have many strong typists.\n \nreply",
      "> a filmmaker can use a camera to make great films without understanding in detail how to build the camera, and there is probably not much of a predictable relationship between knowledge of how the camera works and quality of the resulting film.This sentence conflates two different things to support a single point. One is true but the second is not. It's correct that a filmmaker does not need to know \"how to build a camera\" to make great films. However, a filmmaker certainly needs to know \"how a camera works\" to make a great film. And not just in the pedantic sense of where the Record button is but in the sense of a great violinist needing to know how their violin works (but not how to make a violin). The camera is the artistic instrument of cinema and using it well requires understanding how to leverage lens selection, aperture, shutter speed, exposure, focal plane, lighting, framing, etc to achieve the desired artistic outcome.Another possible confusion in this sentence is the catch-all term \"filmmaker\" when the actual example is more narrowly focused on operation of the camera. Deciding how to apply a camera toward making a motion picture is generally the responsibility of the cinematographer. Sometimes uniquely skilled directors can also act as their own cinematographer but the roles and skill sets are as divergent as composer and violinist.\n \nreply",
      "For anyone confused by this point perhaps watch some of the \u201cwhy [movie] still looks like a billion bucks\u201d videos on YouTube by Wolfcrow.The idea that a film artist doesn\u2019t need to know how a camera works is laughable. And not just the camera, but different film stock, processing methods, lenses, lighting, and the interaction of all those as well as the subject being shot. The level of attention to detail in great films is wild.\n \nreply",
      "I once heard a great question to illustrate this... When you are inspired by a painting, is this your first thought:\"I need to figure out what brand of paints and brushes this painter used so I can paint like this.\"Of course not! It's the painter that determines the quality of the painting and not the maker of the tools. A great painter (or filmmaker or cook or software engineer or ...) will understand the unique attributes of the tools available and leverage those to do great work, whether that's in oil or water, black & white or color, Python or Rust. A difference between experts and non-experts is that experts will find opportunity in the constraints of the tools they have available, and not only frustration.I believe this extends to engineering managers. I want managers who are good at spotting the unique advantages of each person on the team and are good at structuring the roadmap and individual projects to let the team get the most out of those advantages. And I want them to spot where advantages and disadvantages can be paired to enable learning. All this helps teams see how valuable their teammates can be and how much they can learn from each other. I've seen it result in teams achieving remarkable outcomes while having a ton of fun doing it.Scaling this up further... I want senior management that is in tune with the unique advantages of different parts of the org, and who then use that knowledge to make better decisions about direction. We can go in direction A, B, or C... that group over there is great at A, so let's pick that and give it to that group. Some see Conway's Law as a sort of \"doom\", but I've seen it used to the org's advantage with this kind of thinking.\n \nreply",
      "I call this the tension between discovery and requirement. Discovery is how you feel finding things in your garage, the inspiration to do something with that thing, now that you know you have it. Requirement, by contrast, is to work backward from what you want, and then you have to go out and research and obtain what you need to build it. Discovery is more pleasant, higher risk, and more rewarding (for individuals and society). Requirement is much more unpleasant. You don't get to grow from a solid starting point and get to see what it gives you, the delight of finding out what you can make. Instead, you start with specifications that punishes you if you miss them, never mind what your components themselves want to become. You are confronted with a vast array of options, which in combination are an N! set, and a misstep could tank the project.This is why discovery projects are often open-source and well-loved, and requirement projects are how we pay the bills.\n \nreply"
    ],
    "link": "https://moxie.org/2024/09/23/a-good-engineer.html",
    "first_paragraph": "The people who create software generally refer to themselves as software engineers, and yet if they graduate from university, it is typically with a degree in computer science. That has always felt a little strange to me, because science and engineering are two pretty different disciplines \u2013 yet we for the most part seem to take such an obvious contradiction for granted. However, I think there is something uniquely magical about software, and part of that magic might stem from this tension in how we define it.\nAt first glance, software seems like a straightforward engineering practice. After all, it exists within a fully known universe \u2013 the computer \u2013 of our own making. This is in stark contrast to disciplines that are more obviously science, like biology or physics, which exist within domains that we did not create and do not fully understand. Unlike computers, which are fully understood, those disciplines are in large part about the search for understanding. Science, in other words,"
  },
  {
    "title": "Red Programming Language (red-lang.org)",
    "points": 104,
    "submitter": "hotpocket777",
    "submit_time": "2025-05-20T18:14:02 1747764842",
    "num_comments": 48,
    "comments_url": "https://news.ycombinator.com/item?id=44044306",
    "comments": [
      "Languages that encourage making DSLs are a two-edged sword. On the one hand, you get to make a language that is more clear and fine-tuned to your use-case. On the other, you have an ad-hoc language with no support that you have to maintain along with the documentation (considering that you can't expect anyone else to know the DSL ahead of time). As I've gotten older, I've determined that well-designed APIs in a well-known language are a better alternative to DSLs.\n \nreply",
      "An API is just as much a DSL.\n \nreply",
      "No, an API uses existing rules, but a DSL uses its own ad hoc rules.GP is right. Don't make DSLs, make APIs, which are:* More composable* More reusable* More simple to reason about* More natively supported* More portable* More readable* More maintainable\n \nreply",
      "There are plenty of places where it makes more sense to use DSLs (or where they\u2019re flat-out required). SQL and regex both spring to mind. Pretty much any HTML templating language is simpler to use than concatenation a bunch of strings together. JSX is usually easier read than directly calling React functions directly. A line of a shell script can be much nicer (and more portable) than 20 lines of a more general purpose language.Those are things that spring to mind that I think are unequivocally DSLs, but if you\u2019re willing to consider   markup languages as DSLs, the list could get a lot longer.\n \nreply",
      "Most people think of DSLs as a language you can create within a language. All the things you named are either entire languages themselves like HTML, SQL, and the shell family, or formal extensions of existing languages like JSX. A DSL would be something you could create inside JS tagged literals or Ruby.\n \nreply",
      "I don't think of DSLs as strictly being embedded in another programming language. The term I would use for that is \"eDSL\", short for Embedded Domain Specific Language. See: https://wiki.haskell.org/Embedded_domain_specific_language\n \nreply",
      "Martin Fowler on DSLs, including internal and external DSLs:https://martinfowler.com/books/dsl.htmlhttps://martinfowler.com/dsl.htmlAlso see:https://en.m.wikipedia.org/wiki/Domain-specific_languageincluding the References section.\n \nreply",
      "Kind of, except that a non-DSL API doesn't create any new syntax. Which means that you get to keep all sorts of quality-of-life tools like syntax highlighting and correctness checking in the editor, autoformatting, possibly some amount of linting, etc.A few years ago I revisited Racket after a long hiatus, and that was maybe the biggest thing I noticed. I really don't like syntax macros as much as I did back in the day. Once I decide to use `define-syntax` I've then got to decide whether I also want to wade into dealing with also implementing a syntax colorer or an indenter as part of my #lang. And if I do decide to do that, then I've got a bunch more work, and am also probably committing to working in DrRacket (otherwise I'd rather stay in emacs) because that's the only editor that supports those features, and it just turns into a whole quagmire.And it's arguably even worse outside of Racket, where I might have to implement a whole language server and editor plugin to accomplish the same.Versus, if I can do what I need to do with a reasonably tidy API, then I can get those quality of life things without all the extra maintenance burden.None of this was a big deal 20 years ago. My expectations were different back then, because I hadn't been spoiled by things like the language server protocol and everyone (finally) agreeing that autoformatting is a Good Thing.\n \nreply",
      "Even without the new spiffs, I still don't see the point of DSLs.  From where I sit, I see exactly zero problems where I think that new syntax is what I need to be able to write a solution.\n \nreply",
      "I used to feel that way. I\u2019m still not a convert, but now I\u2019ve seen a lot more complexity papered over by a nice DSL.Standard math syntax is a DSL. I understand math a lot more quickly than I understand the same thing written in 20 lines of code.I think the language we use to express ourselves influence the quality of the product. If your language encapsulates complexity, then you can build more complicated things.I\u2019m not arguing in favor of specific (\u201cpointless\u201d) DSLs, but  there\u2019s a nice paper about making a video editing language in Racket [1] that makes a DSL seem pretty convincing.[1]: https://www2.ccs.neu.edu/racket/pubs/icfp17-acf.pdf\n \nreply"
    ],
    "link": "https://www.red-lang.org/p/about.html",
    "first_paragraph": "\n"
  },
  {
    "title": "My favourite fonts to use with LaTeX (2022) (lfe.pt)",
    "points": 57,
    "submitter": "todsacerdoti",
    "submit_time": "2025-05-17T02:53:34 1747450414",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=44011686",
    "comments": [
      "For LaTeX documents like technical papers...Something like Palatino (or even Computer Modern Roman) for body text.But for headings, humble Helvetica looks good, and a bit less \"academic\".  (I really dislike the default CMR at large point sizes.)For monospace bits, again I dislike the unusual-looking TeX default, so something serifed or otherwise clearly unambiguous (for \"1\" and \"l\", \"0\" and \"O\"), and thick enough to be legible (some Courier are too thin).  Inline, at a slightly smaller point size than body text, to look proportional, and maybe a little smaller in code blocks.For a book, I was thinking something slightly flashier for headings, at least on chapters, maybe Linux Biolinum.\n \nreply",
      "I would recommend most fonts that Michael Sharpe worked on. He did some nice refinements on already-decent fonts, often to bring copies closer to the original[1]. Heck, I\u2019d recommend them outside of LaTeX, too.Favorites out of those are XCharter, ScholaX, Etbb and Erewhon.Also have a look at Algol Revived, which is a remake of a font made for French Algol 60 books by famed type designer, Adrian Frutiger.[1]: https://ctan.org/author/sharpe\n \nreply",
      "I might pay the karma tax on this one but I've come to really appreciate \"Times New Roman\" (or TeX Gyre Termes + STIXTwoMath).Like the OP, I used to care a lot about fonts. Heck, at some point my Windows boot time got slowed down because of the sheer number of fonts it had to load!I used to think the default Latex font gives off a \"serious\" and \"scientific\" vibe. And I thought to myself: why would anyone ever use TNR when more \"soulful\" fonts exist?Now that I'm older (33), I resort back to TNR or TeX Gyre Termes but with one change: I add \"FakeBold\" to text to make it look like old papers and books: https://x.com/OrganicGPT/status/1920202649481236745/photo/1. I just want my text to convey my thoughts, and I don't want any fancy \"serifness\" get in the way (so no to Bembo and Palatinno).\n \nreply",
      "The Bembo variant I used for Dercuano, Derctuo, and Dernocua (without LaTeX) is Edward Tufte's ET Book, linked in the article, using the old-style numerals variant.  Unfortunately its Unicode coverage is very limited.  Fortunately, URW Palladio L (URW's freely-licensed version of Palatino) has fine Unicode coverage, so I used that and Palatino as fallbacks.  Unfortunately ET Book's metrics are not very comparable to URW Palladio L's, leading to letter size mismatches when letters mix; its x-height is especially different.So in, for example, where https://dercuano.github.io/notes/finite-function-circuits.ht... says \"S\u1d62 \u2208 \u03a3\", the \"S\" is noticeably shorter than the other full-height characters.  It looks a little bit better in the half-assed PDF rendering I produced with my hurriedly-written HTML-to-PDF renderer: http://canonical.org/~kragen/dercuano.20191230.pdf#page=1572The other big problem you can see on that PDF page is that I chose Latin Modern Typewriter Condensed (lmtlc) for fixed-width text so that I could get 80 columns onto the narrow cellphone screens I was targeting with the PDF, but lmtlc completely omits, for example, Greek, so the examples using Greek are totally screwed up.The formula display in that note is definitely worse than LaTeX would do, but I flatter myself to think that my half-assed Python script still produced better-looking math output than I usually see from Microsoft Word.\n \nreply",
      "Palatino with Microtype is my go to for all my LaTeX documents. It looks so good.\n \nreply",
      "Microtype is such an insanely good package, I love it\n \nreply",
      "My standard font package is \"mathpazo\", which is Palatino with maths support. I obviously like Palatino - and if it isn't available, Garamond is similar.If ever have to do much LaTeX again though, I'll check out the alternatives because the mess of partially compatible modules and the troubles with figure placement are still bad in LaTeX.\n \nreply",
      "It\u2019s hard to see Palatino and Garamond as similar, but perhaps that\u2019s just my typographic training at play. Palatino is much closer to its calligraphic origins than Garamond and has a darker color on the page (the seldom seen Palatino Book weight is a great improvement over the Palatino Medium that\u2019s the default Palatino weight for extended text).Note also that Palatino was originally designed for Linotype hot metal typesetting and has incorporated in its design the limitations of that system (which, in some ways is actually a bonus for na\u00efve digital setting where ligatures may be limited or non-existent). The most obvious case of this is the lack of character kerns\u2014that is, characters cannot extend beyond their typeset width. This makes the italics look cramped since, e.g., d, l and f cannot reach over the following letter with their ascenders.\n \nreply",
      "Palatino has been my favorite font since it first appeared on the Macintosh.\n \nreply",
      "I've heard so much praise for The Visual Display of Quantitative Information, but why, I'll never know. I bought it after hearing someone rave about it, and I'll be damned if I didn't hate every page of the book. It felt like a rebuff to The Design of Everyday Things.The former is currently sitting in my car, and I'll be trying to offload it to someone who actually wants it.\n \nreply"
    ],
    "link": "https://www.lfe.pt/latex/fonts/typography/2022/11/21/latex-fonts-part1.html",
    "first_paragraph": "Introduction\nFonts\n\u00a0\u00a0 Bembo\n\u00a0\u00a0 Palatino\n\u00a0\u00a0 Crimson\n\u00a0\u00a0 LibertineTeX and related systems are often associated with their default fonts, Donald Knuth\u2019s Computer Modern (CM) typefaces. While these fonts are excellent, they have become so ubiquitous in the scientific community that many LaTeX users have sought alternative fonts for their documents. As a result, a plethora of font packages and even OpenType-compatible engines (XeLaTeX and LuaLaTeX) have emerged in the past 20 years.A while ago, I set out to explore this extensive set of LaTeX font options. My goal was to select a small group of high-quality free fonts which I could later choose from when typesetting different documents. As I moved forward with my search, I also became interested in the history and rationale behind the design of different fonts and this led me to write some descriptive notes on each of my favourites.In the paragraphs below, I list and give a brief description of seven fonts which I personally like (where the "
  },
  {
    "title": "Show HN: 90s.dev \u2013 Game maker that runs on the web (90s.dev)",
    "points": 227,
    "submitter": "90s_dev",
    "submit_time": "2025-05-20T14:58:03 1747753083",
    "num_comments": 94,
    "comments_url": "https://news.ycombinator.com/item?id=44042371",
    "comments": [
      ">One day this February, I just got up at 2am and started writing code. I was tired of waiting. So I wrote and wrote and wrote.That's cool.  It makes me feel happy reading this, kind of inspiring :)>I found myself making an API for making game maker tools and a game engine and a game. It turns out I\u2019m an API designer at heart. I guess I always kinda knew this.I like hearing when someone finds their passion and goes for it full throttle.\n \nreply",
      "Thanks.I failed to capture the essenence of what my project is. The APIs are really what's exciting. But you can't see APIs, they're like the wind, you have to see them in action, through their side effects. I should have waited until I had made more things that show off how innovative these APIs are. I released it too soon.\n \nreply",
      "Hi, author here.Thank you everyone for the feedback. It's clear that I launched this far too soon.I'll work on the pain points you all noted, and come back in a few months.\n \nreply",
      "Absolutely not too soon! This is a great project; it's so amazingly, ridiculously overengineered, in the best way possible! The most beautiful bike shed I have ever seen. You even implemented your own reactivity system.I love it.\n \nreply",
      "Ha, thanks! Yeah the reactivity system was born out of trying to get stuff done quickly. I was tired of manually setting variables so I made it easy to \"chain\" and \"adapt\" them and stuff. Same thing with the (hacky?) auto-layout system. Shortcuts to get stuff done quick!\n \nreply",
      "It's just right for a Show HN. I was able to find the Hello World tour and grasp what's going on here, from previous PICO-8 and React experience. Looks pretty cool, and I think 16:9 is a good pick. PICO-8 being square is awkward.\n \nreply",
      "Thank you to you and the other replies. You're very encouraging. I'll keep iterating, making it more usable and its usage more clear. Maybe I can at least finish at least a few tasks while it's on HN's front page(!!!) based on everyone's feedback. Going to work on finishing the paint app right now, hopefully done within an hour.\n \nreply",
      "Thank you for launching early.Definitely launch often.10,000 iterations.  One down.  9,999 more to go.\n \nreply",
      "Thanks! That's an encouraging way to look at it, as it allows me to ship something that's not 100% perfect yet. But I guess no matter how hard we try, it never really is, is it?\n \nreply",
      "No sir, you didnt launch too soon.  You're doing great.\n \nreply"
    ],
    "link": "https://90s.dev/blog/finally-releasing-90s-dev.html",
    "first_paragraph": "90s.devI\u2019ve mentioned this a bunch on HN\nand I\u2019ve been working on it nonstop since about February.\nSo I\u2019m pretty excited to finally make it public.What it is in relation to other gamedev apps:* you are hereWhat it is:An API for making games, game engines, and game maker componentsAn API that wraps web technology up in an \u201coperating system\u201d metaphorAn API that shows you how fun it was to make GUI apps in the 90sWhat it\u2019s not:A game engineA game makerShort demo of the paint app (source code):Ever since I was a kid, I wanted to recreate Warcraft II or at least Warcraft I.\nBut for decades, I never got around to it.\nOne day this February, I just got up at 2am and started writing code.\nI was tired of waiting.\nSo I wrote and wrote and wrote.But instead of making a game,\nor even a game engine,\nor even the game maker tools,\nI found myself making an API for making game maker tools and a game engine and a game.It turns out I\u2019m an API designer at heart. I guess I always kinda knew this.Eventually "
  },
  {
    "title": "Show HN: A Tiling Window Manager for Windows, Written in Janet (agent-kilo.github.io)",
    "points": 200,
    "submitter": "agentkilo",
    "submit_time": "2025-05-20T15:08:42 1747753722",
    "num_comments": 68,
    "comments_url": "https://news.ycombinator.com/item?id=44042490",
    "comments": [
      "Custom windows shells (I know this is just a window manager, but still) in the year of our lord 2025? This takes me back to the days of installing bb4win and litestep in XP. I'm a kid again!\n \nreply",
      "flashback, 2001. I'm 25, sitting in the office with litestep installed (which honestly was the only alternative to Linux or resignation). My five years younger colleague steps up to my desk and says \"hey, cool desktop!\"I start explaining, very carefully, like I'm talking to a child, that this is an alternative shell, which replaces the standard Windows Explorer et cetera, und so weiter... it's very complicated you know...Guy says, \"cool... hey, why don't you check out this URL?\". I do. It's the litestep contributor page. His nick is on it. Near the top.Ow.\n \nreply",
      "First part of your story reminded me of when I once trolled a non-tech savvy friend by running Windows in a VM on Linux, and told him \"have you ever seen what's behind windows?\" then I exited full screen to reveal a desktop full of terminals running the matrix digital rain.\n \nreply",
      "TIL bbLean [1] still works in Windows 11! Currently digging through my archives for my old BB4Win styles repository[1] https://bb4win.sourceforge.net/bblean/\n \nreply",
      "Wow, bb4win and bblean take me way back. In fact, they were a huge inspiration for my shells[1] feature, which are just userland programs that happen to be able to manage panels (windows).[1] https://90s.dev/technical/architecture.html#shells\n \nreply",
      "Is it just me or did not a single one of those \"l33t haxker shells\" ever produceable a single ui innovation that lasted?I mean, I remember there being a whole ton of wildly customized windows shells with menus and floating terminals and so on, but not a single thing stuck around?\n \nreply",
      "I'm not sure about the Windows scene, since I only toyed around with one or two shells nearly twenty years ago, but the motivations for creating windows managers in Unix varied. Quite often they were about the appearance, customization, ease of customization, lack of customization, or low resource usage. I suspect that most of them were made for the learning experience or simply as a form of self-expression. They were never really about innovation. When there was innovation, it was usually in the form of small things like how we size and position windows (e.g. think about how it is possible to tile windows in Windows these days).Besides, the term innovation is used far too much with respect to software, in the sense that a lot of stuff can be traced back much further than the so-called innovators will suggest. Many ideas have deep roots, but it took several (often independent) attempts until the technology or its users were ready for it.\n \nreply",
      "Sloop manager for replacing progman.exe in windows 3.1, for me..\n \nreply",
      ">litestepBrings back memories !\n \nreply",
      "Oh shit yes: Rainlendar\n \nreply"
    ],
    "link": "https://agent-kilo.github.io/jwno/",
    "first_paragraph": "\nJwno is a highly customizable tiling window manager for Windows 10/11, built with Janet and \u2764\ufe0f. It brings to your desktop magical parentheses power, which, I assure you, is not suspicious at all, and totally controllable.\nJwno managing some Emacs frames, and its own REPL window.Note: This documentation is work-in-progress. Some links may return 404, because I have not finished writing those pages yet \ud83d\ude05.Jwno managing Sonic Pi.Using Jwno's UI hint feature to interact with UI elements.My cat coming at you.For new users:\nFor veterans:\nOther links:\n"
  },
  {
    "title": "New stem cell model sheds light on human amniotic sac development (crick.ac.uk)",
    "points": 16,
    "submitter": "gmays",
    "submit_time": "2025-05-17T00:18:40 1747441120",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.crick.ac.uk/news/2025-05-15_new-stem-cell-model-sheds-light-on-human-amniotic-sac-development",
    "first_paragraph": ""
  },
  {
    "title": "Robin: A multi-agent system for automating scientific discovery (arxiv.org)",
    "points": 113,
    "submitter": "nopinsight",
    "submit_time": "2025-05-20T16:21:03 1747758063",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=44043323",
    "comments": [
      "Someone has pointed out on X/Twitter that the \"novel discovery\" made by the AI system already has an entire review article written about the subject [0][0] https://x.com/wildtypehuman/status/1924858077326528991\n \nreply",
      "Not my subject area, but at least one other group looked at ABCA1, and judging from this abstract, it has been linked via GWAS already, and furthermore concludes it doesn\u2019t play a role (I haven\u2019t looked at the data though).I don\u2019t know, but if we were to reframe this as some software to take a hit from a GWAS, look up the small molecule inhibitor/activator for it, and then do some RNA-seq on it, I doubt it would gain any interest.https://iovs.arvojournals.org/article.aspx?articleid=2788418\n \nreply",
      "Wouldn't the fact that another group researched ABCA1 validate that the assistant did find a reasonable topic to research?Ultimately we want effective treatments but the goal of the assistant isn't to perfectly predict solutions. Rather it's to reduce the overall cost and time to a solution through automation.\n \nreply",
      "Not if (a) it misses a line of research has been refuted 1-2 years ago, (b) the experiments at recommends (RNA-Seq) are a limited resource that requires a whole lab to be setup to efficiently act based upon it, and (c) the result of the work is genetic upregulation of a gene, which could mean just about anything.Genetic regulation can at best let us know _involvement_ of a gene, but nothing about why. Some examples of why a gene might be involved: it's a compensation mechanism (good!), it modulates the timing of the actual critical processes (discovery worthy but treatment path neutral), it is causative of a disease (treatment potential found) etc...We don't need pipelines for faster scientific thinking ... especially if the result is experts will have to re-validate each finding. Most experts are anyway truly limited by access to models or access to materials. I certainly don't have a shortage of \"good\" ideas, and no machine will convince me they're wrong without doing the actual experiments. ;)\n \nreply",
      "This is, I think, what I've been struggling to get across to people: while some domains have problems that you can test entirely in code, there are a lot more where the bottleneck is too resource-conatrained in the physical world to have an experiment-free researcher have any value.There's practically negative utility for detecting archeological sites in South America, for example: we already know about far more than we could hope to excavate. The ideas aren't the bottleneck.There's always been an element of this in AI: RL is amazing if you have some way to get ground truth for your problem, and a giant headache if you don't. And so on. But I seem to have trouble convincing people that sometimes the digital is insufficient.\n \nreply",
      "This is a great framing - would you please expound on it a bit.  Software is almost exclusively gated by the \"thinking\" step, except for very large language models, so it would be helpful to understand the gates (\"access to models or access to materials\") in more detail.\n \nreply",
      "This is very cool.One question I have in these orchestration based multi agent systems is the out of domain generalization. Biotech and Pharma is one domain where not all the latest research is out there in public domain (hence big labs havent trained models on it). Then, there are many failed approaches (internal to each lab + tribal knowledge) which would not be known to the world outside. In both these cases, any model or system would struggle to get accuracy (because the model is guessing on things it has no knowledge of). In context learning can work but it's a hit and miss with larger contexts. And it's a workflow + output where errors are not immediately obvious like coding agents. I am curious as to what extent do you see this helping a scientist? Put another way, do you see this as a co-researcher where a person can brainstorm with (which they currently do with chatgpt) or do you expect a higher involvement in their day to day workflow? Sorry if this question is too direct.\n \nreply",
      "This approach is very interesting, and one attention-catching datum is that their proposed compound, ripasudil, is now largely out-of-patent with some caveats, via Google Patents and ChatGPT 03:> 1999 - D. Western Therapeutics Institute (DWTI) finishes the discovery screen that produced K-115 = ripasudil and files the first PCT on 4-F-isoquinoline diazepane sulfonamides. (Earliest composition-of-matter priority. A 20-year term from a 1999 JP priority date takes you to 2019 (before any extensions).> 2005 - Kowa (the licensee) files a follow-up patent covering the use of ripasudil for lowering intra-ocular pressure. U.S. counterpart US 8 193 193 issued 2012; nominal expiry 11 July 2026. (A method-of-use patent \u2013 can block generics in the U.S. even after the base substance expires).Scanning the vast library of out-of-patent pharmaceuticals for novel uses has great potential for curing disease and reducing human suffering, but the for-profit pipeline in academic/corporate partnerships is notoriously uninterested in such research because they want exclusive patents that justify profits well beyond a simple %-of-manufacturing cost margin.  Indeed they'd probably try to make random patentable derivatives of the compound in the hope that the activity of the public domain substance was preserved and market that instead (see the Prontosil/sulfanilimide story of the 1930s, well-related in Thomas Hager's 2006 book \"The Demon Under The Microscope).I suppose the user of these tools could restrict them to in-patent compounds, but that's ludicrously anti-scientific in outlook.  In general it seems the more constraints are applied, the worse the performance.Another issue is this is a heavily studied area and the result is more incremental than novel. I'd like to see it tackle a question with much less background data - propose a novel, cheap, easily manufactured industrial catalyst for the conversion of CO2 to methanol.\n \nreply",
      "Will we have AIs doing an increasing amount of the research, theory and even publication, with human scientists increasingly relegated to doing experiments under their direction?\n \nreply",
      "If so, it won't last long.  At some point AI will be able to use robots to do the experiments itself.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2505.13400",
    "first_paragraph": "Work on one of the world's most important websites and make an impact on open science.arXiv Is Hiring a DevOps EngineerHelp | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "AI's energy footprint (technologyreview.com)",
    "points": 110,
    "submitter": "pseudolus",
    "submit_time": "2025-05-20T10:07:55 1747735675",
    "num_comments": 115,
    "comments_url": "https://news.ycombinator.com/item?id=44039808",
    "comments": [
      "http://archive.today/mnHb8",
      "If you are old enough you remember posting to Usenet and the warning that would accompany each new submission:This program posts news to thousands of machines throughout the entire civilized world. Your message will cost the net hundreds if not thousands of dollars to send everywhere. Please be sure you know what you are doing. Are you absolutely sure that you want to do this? [ny]Maybe we meed something similar in LLM clients. Could be phrased in terms of how many pounds of atmospheric carbon the request will produce.\n \nreply",
      "Individual LLM requests are vanishingly small in terms of environmental impact; inference providers use a lot of batching to do lots of work at once. Furthermore, LLMs and diffusion models are not the only ML workload. While generative AI tickles investors, most of the ML actually being deployed is more mundane things, like recommendation systems, classifiers, and the like; much of which is used for adtech purposes adversarial to that of users. If LLMs and diffusers were the only thing companies used ML for, but efficiency gains from new hardware remained constant, we'd still be at the 2017 baseline for environmental impact of data centers.Likewise, I doubt that USENET warning was ever true beyond the first few years of the networks' lifetime. Certainly if everything was connected via dial-up, yes, a single message could incur hundreds of dollars of cost when you added the few seconds of line time it took to send up across the whole world. But that's accounting for a lot of Ma Bell markup. Most connections between sites and ISPs on USENET were done through private lines that ran at far faster speeds than what you could shove down copper phone wiring back then.\n \nreply",
      "If what you're saying is true, why are we hearing about AI companies wanting to build nuclear power plants to power new data centers they think they need to build?Are you saying all of that new capacity is needed to power non-LLM stuff like classifiers, adtech, etc? That seems unlikely.Had you said that inference costs are tiny compared to the upfront cost of training the base model, I might have believed it. But even that isn't accurate -- there's a big upfront energy cost to train a model, but once it becomes popular like GPT-4, the inference energy cost over time is dramatically higher than the upfront training cost.You mentioned batch computing as well, but how does that fit into the picture? I don't see how batching would reduce energy use. Does \"doing lots of work at once\" somehow reduce the total work / total energy expended?\n \nreply",
      "> If what you're saying is true, why are we hearing about AI companies wanting to build nuclear power plants to power new data centers they think they need to build?Well, partly because they (all but X, IIRC) have commitments to shift to carbon-neutral energy.But also, from the article:> ChatGPT is now estimated to be the fifth-most visited website in the worldThat's ChatGPT today. They're looking ahead to 100x-ing (or 1,000,000x-ing) the usage as AI replaces more and more existing work.I can run Llama 3 on my laptop, and we can measure the energy usage of my laptop--it maxes out at around 0.1 toasters. o3 is presumably a bit more energy intensive, but the reason it's using a lot of power is the >100MM daily users, not that a single user uses a lot of energy for a simple chat.\n \nreply",
      "A lot of us live in a country where \"rolling coal\" is a thing.  I fear your prompt may have an opposite of the intended effect.\n \nreply",
      "Taxing anything that can pollute (methane, gasoline, diesel) would let The Hand sort it out\n \nreply",
      "Carbon taxes are incredibly unpopular, because it makes easy and convenient things expensive.\n \nreply",
      "They're incredibly unpopular because even when they're made revenue-neutral (meaning, everyone gets a refund check) people don't realize most of them would make money if they actually reduced their carbon.\n \nreply",
      "They're incredibly unpopular because the ultraweathly use massive amounts of fossil fuels and thus lobby very, very hard against them...and make sure the public is often told just how evil they are and how expensive they'd hurt Johnny Everyday Worker, even car ownership, especially in a city (where much of the US populative lives) is not affordable to a large segment of the population.If memory serves Jet A is not taxed at all federally in the case of for-profit corporations (while non-commercial users DO pay a tax!) and many states also either do not tax it or tax it very litte.It's completely insane that we do not tax fuel usage for probably the most energy-intensive way to move people and/or goods and often that movement of people is entirely frivelous.\n \nreply"
    ],
    "link": "https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/",
    "first_paragraph": "The emissions from individual AI text, image, and video queries seem small\u2014until you add up what the industry isn\u2019t tracking and consider where it\u2019s\u00a0heading\u00a0next.AI\u2019s integration into our lives is the most significant shift in online life in more than a decade. Hundreds of millions of people now regularly turn to chatbots for help with homework, research, coding, or to create images and videos. But what\u2019s powering all of that?Today, new analysis by MIT Technology Review provides an unprecedented and comprehensive look at how much energy the AI industry uses\u2014down to a single query\u2014to trace where its carbon footprint stands now, and where it\u2019s headed, as AI barrels towards billions of daily users.This story is a part of MIT Technology Review\u2019s series \u201cPower Hungry: AI and our energy future,\u201d on the energy demands and carbon costs of the artificial-intelligence revolution.We spoke to two dozen experts measuring AI\u2019s energy demands, evaluated different AI models and prompts, pored over hun"
  },
  {
    "title": "The Dawn of Nvidia's Technology (dshr.org)",
    "points": 125,
    "submitter": "wmf",
    "submit_time": "2025-05-20T17:04:00 1747760640",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=44043687",
    "comments": [
      "That's from the period when there was no standardization of how the CPU talked to the graphics device. Triangles or quads? Shared memory or command queues? DMA from the CPU side or the graphics device side? Graphics as part of the CPU/memory system or as part of the display system? Can the GPU cause page faults which are serviced by the virtual memory system?Now we have Vulkan. Vulkan standardizes some things, but has a huge number of options because hardware design decisions are exposed at the Vulkan interface. You can transfer data from CPU to GPU via DMA or via shared memory. Memory can be mapped for bidirectional transfer, or for one-way transfer in either direction. Such transfers are slower than normal memory accesses. You can ask the GPU to read textures from CPU memory because GPU memory is full, which also carries a performance penalty. Or you can be on an \"integrated graphics\" machine where CPU and GPU share the same memory. Most hardware offers some, but not all, of those options.This is why a lot of stuff still uses OpenGL, which hides all that.(I spent a few years writing AutoCAD drivers for devices now best forgotten, and later trying to get 3D graphics to work on PCs in the 1990s. I got to see a lot of graphics boards best forgotten.)\n \nreply",
      "And that was an evolution of earlier 2D cards where you had a potential mixture of CPU-addressable framebuffer and various I/O ports to switch modes between text and raster graphics, adjust video modes in DACs, adjust color palette lookup tables, load fonts for text modes, and maybe address some 2D coprocessors for things like \"blitting\" (kind of like rectangular 2D DMA), line drawing, or even some basic polygonal rendering with funny options like dithering or stipple shading...\n \nreply",
      "This kind of retrospective from key people who were involved is invaluable from an historical perspective. I find hearing first-hand accounts of the context, assumptions, thought processes, internal debates, technical limitations, business realities and even dumb luck a good way to not only understand how we got here but how to do as well (or better) going forward.While the nitty gritty detail of recollections captured when still fresh in memory can be fascinating, I especially appreciate reflections written a few decades later as it allows putting the outcomes of key decisions in perspective, as well as generally enabling more frank assessments thanks to fewer business and personal concerns.\n \nreply",
      "I'm excited about this too, but it's a little concerning there's a brand in the title. There's no shortage of those from ati, intel, amd, apple, ibm, the game gaggle, etc to interview. The fact that nvidia succeeded where others failed is largely an artifact of luck.\n \nreply",
      "> largely an artifact of luck.I disagree with \"largely\". Luck is always a factor in business success and there are certainly some notable examples where luck was, arguably, a big enough factor that \"largely\" would apply - like Broadcast.com's sale to Yahoo right at the peak of the .com bubble. However, I'm not aware of evidence luck was any more of a factor in NVidia's success than the ambient environmental constant it always is for every business. Luck is like the wind in competitive sailing - it impacts everyone, sometimes positively, sometimes negatively.Achieving and then sustaining substantial success over the long run requires making a lot of choices correctly as well as top notch execution. The key is doing all of that so consistently and repeatedly that you survive long enough for the good and bad luck to cancel each other out. NVidia now has over 30 years of history through multiple industry-wide booms, downturns and fundamental technology transitions - a consistent track record of substantial, sustained success so long that good luck can't plausibly be a significant factor.That said, to me, this article didn't try to explain NVidia's long-term business success. It focused on a few key architectural decisions made early on which were, arguably, quite risky in that they could have wasted a lot of development on capabilities which didn't end up mattering. However, they did end up paying off and, to me, the valuable insight was that key team members came from a different background than their competitors and their experiences with multi-user, multi-tasking, virtualized mini and mainframe architectures caused them to believe desktop architectures would evolve in that direction sooner rather than later. The takeaway being akin to \"skate to where the puck is going, not where it is.\" In rapidly evolving tech environments, making such predictions is greatly improved when the team has both breadth and depth of experience in relevant domains.\n \nreply",
      "Nvidia\u2019s Cg language made developers prefer their hardware, I\u2019d say.\n \nreply",
      "I remember sitting next to David Rosenthal at a conference reception (must have been FAST, which makes sense given his involvement with LOCKSS) in San Jose some time around 2010 or 2011, not knowing up front who he was. He explained some of the innovations he had made at NVIDIA around making the hardware more modular and easier for parallel teams to work on, and we chatted about the rumors I had heard about SUN thinking about licensing the Amiga hardware, which he confirmed but said would have been a bad idea, because the hardware didn't support address space protection. I guess I didn't know enough about him or NVIDIA to be sufficiently impressed at the time, but he was a very friendly and down to earth person.\n \nreply",
      "> all an application could do was to invoke methods on virtual objects .. the application could not know whether the object was implemented in hardware or in the resource manager's software. The flexibility to make this decision at any time was a huge advantage. As Kim quotes Michael Hara as saying:> \u201cThis was the most brilliant thing on the planet. It was our secret sauce. If we missed a feature or a feature was broken, we could put it in the resource manager and it would work.\u201dAbsolutely brilliant. Understand the strengths and weaknesses of your tech (slow/updateable software vs fast/frozen hardware) then design the product so a missed deadline won\u2019t sink the company. A perfect combo of technically savvy management and clever engineering.\n \nreply",
      "I wanted to buy a Voodoo card, and due to PCI incompatible version, had to trade it back for a Riva TNT.Back then I was quite p***d not being able to keep the Voodoo, how little did I know how it was going to turn out.\n \nreply",
      "We hold you singularly responsible for the eventual failure of the Voodoo3/4/5 and Nvidia domination.\n \nreply"
    ],
    "link": "https://blog.dshr.org/2025/05/the-dawn-of-nvidias-technology.html",
    "first_paragraph": "I'm David Rosenthal, and this is a place to discuss the work I'm doing in Digital Preservation.\nThe design here is a good case of actually anticipating the future developments of technology (virtual FIFOs would seem to be genius from my point of view). I'm working on an NV3, eventually NV1/NV4, emulation (currently got 2D working which is a giant pain), and it's a pain in the ass to emulate this stuff. Did you work on NV2/the original 1996 QTM NV3/Riva 128? Also, was NV0 a set of VxDs that emulated the architecture?\n\nPost a Comment\n"
  },
  {
    "title": "Why does the U.S. always run a trade deficit? (newyorkfed.org)",
    "points": 177,
    "submitter": "jnord",
    "submit_time": "2025-05-20T11:42:24 1747741344",
    "num_comments": 367,
    "comments_url": "https://news.ycombinator.com/item?id=44040407",
    "comments": [
      "Using a national currency as the de facto global reserve guarantees a trade deficit for that country.No one else can manufacture USD's, so other countries have to acquire them by shaping their economies to supply goods and services demanded by the US. They can then use these earned dollars to transact with other countries, as the US itself insists they do.For the US, this is a simple trade off - gain massive political influence (and market intelligence - all USD transactions go through US institutions regardless of where those transacting partners are located), at the expense of hollowing out domestic industry and running a deficit in physical goods traded.The solution is a non-national global reserve, calculated on a basket of national currencies. This was Keynes argument at Bretton Woods, but the US would not have it then, and does not want it now.\n \nreply",
      "US doesn't just get political influence. It gets massive amounts of products and services enabling the US residents live well beyond their means.China for example, sends huge number of electronics and all kind of other consumer goods that Chinese produce by sweating in 12 hours shifts in 6 day work weeks in exchange for imaginary numbers.US is definitely not the victim here. There's the risk of this system stop working and that's when the US might have hard times due to being forced to live by its means and have no ability to kickstart its own production when that time comes.It makes sense to be worried for such an eventuality but US is definitely not being taken advantage here. The situation is more like selling your startup at young age and live a lavish lifestyle with the money without working and studying and risk becoming penniless and unemployable by the 50s.\n \nreply",
      "> It gets massive amounts of products and services enabling the US residents live well beyond their means.What does this mean really? That is their means.For a somewhat topical example, people of Australia get access to cheap medications (in part because they pay to subsidize the cost of them but also because) their government negotiates with pharmaceutical corporations to pay lower prices. This kind of negotiation would be completely out of reach of any private Australian person, but they are not living outside their means. Their means includes the means to elect governments to run the country for the benefit of its own people including doing things like securing lower prices for medications.> China for example, sends huge number of electronics and all kind of other consumer goods that Chinese produce by sweating in 12 hours shifts in 6 day work weeks in exchange for imaginary numbers.Until 1990, Kenya had a higher GDP per capia than China. It is absolutely not \"imaginary\". Work produces real value, just because you can represent or trade that for allegedly \"imaginary\" currency does not mean that the value created was imaginary.> US is definitely not the victim here. There's the risk of this system stop working and that's when the US might have hard times due to being forced to live by its means and have no ability to kickstart its own production when that time comes.US manufacturing output is double that of China's on a per-capita basis.> It makes sense to be worried for such an eventuality but US is definitely not being taken advantage here.Seems like that's the popular assertion but I don't see much solid reasoning behind it in this thread (not picking on you specifically), just handwaving about how USD's status as a global currency somehow makes trade deficits inevitable despite simple facts available that US had a surplus trade balance 50 years ago, when the USD has been considered the global / reserve currency for over 60 years.\n \nreply",
      "Indeed true - US is 4% of the global population but 25% of the global consumption, if not more. You can see this via the eyeball test - everything is bigger: cars, houses, even the people. US invented 'overproduction' to smear Chinese manufacturing, but did not consider the other side of the coin, which is US 'overconsumption'. Two to tango etc\n \nreply",
      "Wouldn't it be more apt to compare consumption vs production.  It looks like US is about 26% of global GDP from a quick google.  But I'm also suspect of comparing GDP's across nations that have very different methodologies.Also services are counted there, so it's likely we're exporting services in exchange for manufactured products.\n \nreply",
      "I see this tossed around but no one ever seems to point out that EU has similar numbers, which are about half, but still extremely higher than global average; and much more so in the richer countries of the EU (per capita of course)\n \nreply",
      "The EU complains about China Shock equally as much as the US [0][1][2][3][4][5][6]Just becuase Trump burnt bridges with the EU doesn't mean that EU member states will gladly allow job losses across Europe's industrial heartland to a country that is supplying a direct competitor (Russia) that has conducted grey zone warfare within the EUThe EU and it's member states are all working on building domestic capacity, and pushing Chinese manufacturers to manufacture WITHIN the EU [7][8], and further diversifying by making FTAs with ASEAN [9][10], Japan [11], SK [12], India [13], etc.In essence, nothing has materially changed in European policy with regards to China compared to under the Biden administration.Chinese overproduction is a global issue now, and most major economies and blocs have enacted barriers and will continue to do so unless China removes it's barriers to imports, subsidizes, and technology transfers.[0] - https://ecfr.eu/publication/electric-shock-the-chinese-threa...[1] - https://www.cer.eu/publications/archive/policy-brief/2025/ho...[2] - https://www.gmfus.org/news/watching-china-europe-may-2025[3] - https://www.chathamhouse.org/2021/07/eus-unsustainable-china...[4] - https://www.ecb.europa.eu/press/blog/date/2024/html/ecb.blog...[5] - https://www.iza.org/publications/dp/13259/hit-by-the-silk-ro...[6] - https://www.bloomberg.com/news/articles/2025-05-15/eu-econom...[7] - https://www.economist.com/europe/2024/09/19/near-shoring-is-...[8] - https://www.bruegel.org/sites/default/files/2024-07/The%20EU...[9] - https://www.reuters.com/world/china/sweden-propose-eu-member...[10] - https://www.ft.com/content/bee31826-012d-4bb1-a6eb-d6cc0d4ef...[11] - https://policy.trade.ec.europa.eu/eu-trade-relationships-cou...[12] - https://trade.ec.europa.eu/access-to-markets/en/content/eu-s...[13] - https://apnews.com/article/india-eu-modi-ursula-von-der-leye...\n \nreply",
      "Everyone gets something, or every country anyways.  The export-led growth Asian miracles are really just a part of this.  Maybe the Chinese worker gets screwed, but China maybe doesn't.  Something like that.\n \nreply",
      "Chinese workers got actual work vs being automated out of the farming industry and starving. China gets to import raw resources from other countries to feed both domestic production and foreign export.It was a rational strategy that\u2019s getting outdated as China\u2019s economy grew enough that exports can\u2019t ramp up any more.\n \nreply",
      "> China for example, sends huge number of electronics and all kind of other consumer goods that Chinese produce by sweating in 12 hours shifts in 6 day work weeks in exchange for imaginary numbersThat's more a function of subsidizes instead of foreign preference for USD.Chinese median household incomes (Yuan 34,000 or around $4,700) are much too low to spend on most goods at scale [0], and most of the money that could be spent on expanding the social safety net (and thus incentivizing Chinese consumers to spend instead of save) is spent on industrial subsidizes like tax holidays, a regressive income tax system comparable to the US, and subsidizing various redundant but provincially influential SOEs that can't compete with domestic private sector players (eg. traditional Chinese automotive players like SAIC and Chery versus BYD), and this is reflected in Chinese spending data as well as StatsChina's breakdown of spending by urban and rural Chinese [0].You are going to be dependent on foreign exports if your domestic consumers can only spend around Yuan 4000/$550 a year on transportation and telecom combined. Even factoring for PPP, it is difficult as these metrics are low in comparison to peer countries from a developmental standpoint.A lot of the \"overproduction\" that has made Chinese goods globally dominant is a result of that misalignment between consumers and production.Expanding the social safety net in China would dramatically enhance Chinese competitiveness over the long term, but top level leadership remains explicitly opposed to what they call \"Welfarism\" [1]:\"\u798f\u5229\u4e3b\u4e49\u5178\u8303\u56fd\u5bb6\uff0c\u4e2d\u4ea7\u584c\u9677\u3001\u8d2b\u5bcc\u5206\u5316\u3001\u793e\u4f1a\u6495\u88c2\u3001\u6c11\u7cb9\u55a7\u56a3\uff0c\u8fd9\u4e0d\u4e4f\u8b66\u793a\u2014 \u9632\u6b62\u843d\u5165\u201c\u798f\u5229\u4e3b\u4e49\u201d\u517b\u61d2\u6c49\u9677\u9631\"\"In countries that use a welfare model, the middle class is collapsing, the rich and the poor are polarizing, society is torn apart, and populism is rising. This is a warning - Prevent yourself from falling into the trap of \"welfarism\" to support lazy people\"[0] - https://www.stats.gov.cn/english/PressRelease/202501/t202501...[1] - http://theory.people.com.cn/n1/2021/1116/c40531-32283350.htm...\n \nreply"
    ],
    "link": "https://libertystreeteconomics.newyorkfed.org/2025/05/why-does-the-u-s-always-run-a-trade-deficit/",
    "first_paragraph": "At the New  York Fed, our mission is to make the U.S. economy stronger and the financial system more stable for all segments of society. We do this by executing monetary policy, providing financial services, supervising banks and conducting research and providing expertise on issues that impact the nation and communities we serve.Introducing the New York Innovation Center: Delivering a central bank innovation executionDo you have a request for information and records? Learn how to submit it.Learn about the history of the New York Fed and central banking in the United States through articles, speeches, photos and video.As part of our core mission, we supervise and regulate financial institutions in the Second District. Our primary objective is to maintain a safe and competitive U.S. and global banking system. The Governance & Culture Reform hub is designed to foster discussion about corporate governance and the reform of culture and behavior in the financial services industry. Need to f"
  },
  {
    "title": "LLM-D: Kubernetes-Native Distributed Inference at Scale (github.com/llm-d)",
    "points": 5,
    "submitter": "bbzjk7",
    "submit_time": "2025-05-20T23:55:18 1747785318",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://github.com/llm-d/llm-d",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        llm-d is a Kubernetes-native high-performance distributed LLM inference framework\n      \n\n\n\n\n\n\nLatest News \ud83d\udd25llm-d is a Kubernetes-native distributed inference serving stack - a well-lit path for anyone to serve large language models at scale, with the fastest time-to-value and competitive performance per dollar for most models across most hardware accelerators.With llm-d, users can operationalize GenAI deployments with a modular solution that leverages the latest distributed inference optimizations like KV-cache aware routing and disaggregated serving, co-designed and integrated with the Kubernetes operational tooling in Inference Gateway (IGW).Built by leaders in the Kubernetes and vLLM projects, llm-d is a community-driven, Apache-2 licensed project with an open development model.llm-d adopts a layered architecture on top of indus"
  },
  {
    "title": "The emoji problem (2022) (artofproblemsolving.com)",
    "points": 315,
    "submitter": "mtsolitary",
    "submit_time": "2025-05-20T10:18:15 1747736295",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=44039864",
    "comments": [
      "There is a great quora answer for this one: https://www.quora.com/How-do-you-find-the-positive-integer-s...\n \nreply",
      ">quora answer for this onethat quora article is written by Alon Amit and fwiw, TFA quotes Alon Amit, so his thoughts are ex post.\n \nreply",
      "now that's peak Quora right there!\n \nreply",
      "I really miss those years when Quora was a decent place. it went downhill when they decided to aggressively monetize it.the same happened to poe.\n \nreply",
      "I heard they use LLMs to generate questions and answers to those questions to help drive traffic, which is why no matter what insane bullshit you type into google, you will probably get a quora page about it.Something something dead internet theory.\n \nreply",
      "I saw a reddit thread from sub about {specific retailer} in {specific geographical region} where the OP ends with ChatGPT's classic \"Would you like some more variations of this text?\" response and the whole damn thread was other users happily replying on topic, totally missing that OP was clearly a bot because they were probably bots too. But hey, they all get karma, all get activity and look they're members of {geographic region} so their opinions on future political threads in {region} are totally legit right?But yeah, something something dead internet theory indeed.\n \nreply",
      "Yeah, yeah, they just mumble mumble diophantine mumble mumble elliptic curve mumble mumble Weierstrass form mumble mumble like your boring old uncle at Christmas. GET TO THE ANSWER!   154476802108746166441951315019919837485664325669565431700026634898253202035277999\n   36875131794129999827197811565225474825492979968971970996283137471637224634055579\n   4373612677928697257861252602371390152816537558161613618621437993378423467772036\n \nreply",
      "I tried giving this to ChatGPT. (Just by uploading the image to the base OpenAI interface.) I expected to either (a) have the model already know the question and give the right answer, (b) hallucinate an answer or (c) refuse to engage with the problem at all.Instead, this happened:https://chatgpt.com/share/682cce62-c53c-8003-be2c-2929395868...Basically, the model confidently outputs a guess, then calculates it, determines it to be incorrect, and repeatedly tries again, even repeating the same guesses over and over. It does not recognize any symmetry and acts like a completely unstructured agent. In the end, the model vehemently asserts there to be no solutions to this puzzle. I really did not expect this and will update my beliefs accordingly if the models behave as badly with future puzzles.\n \nreply",
      "Here's gemini https://g.co/gemini/share/ab287b25648fI also asked Chat GPT o3 and it thought for 11.5 minutes! \nhttps://chatgpt.com/share/682d0993-db4c-8004-a66c-3908ef7203...\n \nreply",
      "I'm impressed. There certainly are no \"reasonable\" solutions, where I am arbitrarily defining reasonable as numbers that humans can work or understand in their heads.Isn't there a version of ChatGPT that connects to Wolfram Alpha? Did you try that one?\n \nreply"
    ],
    "link": "https://artofproblemsolving.com/community/c2532359h2760821_the_emoji_problem__part_i?srsltid=AfmBOor9TbMq_A7hGHSJGfoWaa2HNzducSYZu35d_LFlCSNLXpvt-pdS",
    "first_paragraph": "Something appears to not have loaded correctly.Click to refresh."
  }
]