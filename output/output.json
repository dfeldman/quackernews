[
  {
    "title": "What an unprocessed photo looks like (maurycyz.com)",
    "points": 456,
    "submitter": "zdw",
    "submit_time": "2025-12-28T22:35:02 1766961302",
    "num_comments": 123,
    "comments_url": "https://news.ycombinator.com/item?id=46415225",
    "comments": [
      "But does applying the same transfer function to each pixel (of a given colour anyway) count as \"processing\"?What bothers me as an old-school photographer is this.  When you really pushed it with film (e.g. overprocess 400ISO B&W film to 1600 ISO and even then maybe underexpose at the enlargement step) you got nasty grain.  But that was uniform \"noise\" all over the picture.  Nowadays, noise reduction is impressive, but at the cost of sometimes changing the picture.  For example, the IP cameras I have, sometimes when I come home on the bike, part of the wheel is missing, having been deleted by the algorithm as it struggled with the \"grainy\" asphalt driveway underneath.Smartphone and dedicated digital still cameras aren't as drastic, but when zoomed in, or in low light, faces have a \"painted\" kind of look.  I'd prefer honest noise, or better yet an adjustable denoising algorithm from \"none\" (grainy but honest) to what is now the default.reply",
      "Equally bad is the massive over sharpening applied to CCTV and dash cams. I tried to buy a dash cam a year ago that didn't have over sharpened images but it proved impossible.Reading reg plates would be a lot easier if I could sharpen the image myself rather than try to battle with the \"turn it up to 11\" approach by manufacturers.reply",
      "Just wait a few years, all of this is still getting better.reply",
      "\"Better\"...reply",
      "I love posts that peel back the abstraction layer of \"images.\" It really highlights that modern photography is just signal processing with better marketing.A fun tangent on the \"green cast\" mentioned in the post: the reason the Bayer pattern is RGGB (50% green) isn't just about color balance, but spatial resolution. The human eye is most sensitive to green light, so that channel effectively carries the majority of the luminance (brightness/detail) data.\nIn many advanced demosaicing algorithms, the pipeline actually reconstructs the green channel first to get a high-resolution luminance map, and then interpolates the red/blue signals\u2014which act more like \"color difference\" layers\u2014on top of it. We can get away with this because the human visual system is much more forgiving of low-resolution color data than it is of low-resolution brightness data. It\u2019s the same psycho-visual principle that justifies 4:2:0 chroma subsampling in video compression.Also, for anyone interested in how deep the rabbit hole goes, looking at the source code for dcraw (or libraw) is a rite of passage. It\u2019s impressive how many edge cases exist just to interpret the \"raw\" voltages from different sensor manufacturers.reply",
      "> A fun tangent on the \"green cast\" mentioned in the post: the reason the Bayer pattern is RGGB (50% green) isn't just about color balance, but spatial resolution. The human eye is most sensitive to green light, so that channel effectively carries the majority of the luminance (brightness/detail) data.From the classic file format \"ppm\" (portable pixel map) the ppm to pgm (portable grayscale map)https://linux.die.net/man/1/ppmtopgm    The quantization formula ppmtopgm uses is g = .299 r + .587 g + .114 b.\n\nYou'll note the relatively high value of green there, making up nearly 60% of the luminosity of the resulting grayscale image.I also love the quote in there...   Quote\n\n   Cold-hearted orb that rules the night\n   Removes the colors from our sight\n   Red is gray, and yellow white\n   But we decide which is right\n   And which is a quantization error.\n\n(context for the original - https://www.youtube.com/watch?v=VNC54BKv3mc )reply",
      "I have a related anecdote.When I worked at Amazon on the Kindle Special Offers team (ads on your eink Kindle while it was sleeping), the first implementation of auto-generated ads was by someone who didn't know that properly converting RGB to grayscale was a smidge more complicated than just averaging the RGB channels. So for ~6 months in 2015ish, you may have seen a bunch of ads that looked pretty rough. I think I just needed to add a flag to the FFmpeg call to get it to convert RGB to luminance before mapping it to the 4-bit grayscale needed.reply",
      "I don't think Kindle ads were available in my region in 2015 because I don't remember seeing these back then, but you're a lucky one to fix this classic mistake :-)I remember trying out some of the home-made methods while I was implementing a creative work section for a school assignment. It\u2019s surprising how \"flat\" the basic average looks until you actually respect the coefficients (usually some flavor of 0.21R + 0.72G + 0.07B). I bet it's even more apparent in a 4-bit display.reply",
      "I remember using some photo editing software (Aperture I think) that would allow you to customize the different coefficients and there were even presets that give different names to different coefficients. Ultimately you can pick any coefficients you want, and only your eyes can judge how nice they are.reply",
      "If you really want that old school NTSC look: 0.3R + 0.59G + 0.11BThis is the coefficients I use regularly.reply"
    ],
    "link": "https://maurycyz.com/misc/raw_photo/",
    "first_paragraph": "Here\u2019s a photo of a Christmas tree, as my camera\u2019s sensor sees it:It\u2019s not even black-and-white, it\u2019s gray-and-gray.\nThis is becuase while the ADC\u2019s output can theoretically go from 0 to 16382, the actual data doesn\u2019t cover that whole range:The real range of ADC values is ~2110 to ~136000.\nLet\u2019s set those values as the white and black in the image:Vnew = (Vold - Black)/(White - Black)Much better, but it\u2019s still more monochromatic then I remember the tree being.\nCamera sensors aren\u2019t actually able to see color: They only measure how much light hit each pixel.In a color camera, the sensor is covered by a grid of alternating color filters:Let\u2019s color each pixel the same as the filter it\u2019s looking through:This version is  more colorful, but each pixel only has one third of it\u2019s RGB color.\nTo fix this, I just averaged the values each pixel with it\u2019s neighbors:Applying this process to the whole photo gives the lights some color:However, the image is still very dark.\nThis is because monitors "
  },
  {
    "title": "Stepping down as Mockito maintainer after 10 years (github.com/mockito)",
    "points": 206,
    "submitter": "saikatsg",
    "submit_time": "2025-12-28T20:14:58 1766952898",
    "num_comments": 96,
    "comments_url": "https://news.ycombinator.com/item?id=46414078",
    "comments": [
      "Wow, there's a lot of anger in some of these posts.I've been using Mockito for about 4 years, all in Kotlin. I always found it to be \"plenty good\" for like 99% of the cases I needed it; and things more complicated or confusing or messy were usually my fault (poor separation of concerns, etc).I regularly found it quite helpful in both its spy() and mock() functionality.I never found it meaningfully more or less useful than MockK, though I have heard MockK is the \"one that's better for Kotlin\". It's mostly just vocabulary changes for me, the user.I'm going to have to monitor Mockito's future and see if I'll need to swap to MockK at some point if Mockito becomes unmaintained.reply",
      "\"Wow, there's a lot of anger in some of these posts.\"If I was OP I'd retire happy knowing that a very thankless job is well done!  Given what it does: the more outrage the better.  Projects like Mockito call out the lazy and indolent for whom they are and the hissing and spitting in return can simply be laughed at.10 years this bloke has given his time and effort to help people.  He states: nearly a third of his life.I'll raise a glass and say \"was hale\" or perhaps wassail as an Englander might.reply",
      "Is it anger? The agent thing maybe. The other two points seem to boil down to:1. Kotlin is a hack and \n2. Rust is more fun.Pretty understandable why one would simply want to move on to greener pastures.reply",
      "I'm not sure Kotlin is a hack, but it's growth is directly related to Java not being able to deliver on simple features that would help the day to day lives of developers. So I also can't blame anyone for moving on from the language. I get frustrated with it all the time, especially when Kotlin shows that some feature could work perfectly fine.Here are some examples that have hit the graveyard: It's been 2 years since exception handling in switch was proposed [0], 3 years since null-restricted types were proposed [1], 4 years since string templates [2], 8 years since concise method bodies [3], and 11 years for JSON parsing [4].[0] https://inside.java/2023/12/15/switch-case-effect/[1] https://openjdk.org/jeps/8303099[2] https://openjdk.org/jeps/430[3] https://openjdk.org/jeps/8209434[4] https://openjdk.org/jeps/198reply",
      "At the end of the day I think there's nothing wrong with the tool itself.  The problem is that mocking and spies make it easy to not bother properly isolating the effects of a function for testing and then you end up having a test where 95% of it is setting up an elaborate array of mocks to create the condition you wish to test, which are completely incomprehensible to the next person trying to read it.reply",
      "My second project at Google basically killed mocking for me and I've basically never done it since. Two things happened.The first was that I worked on a rewrite of something (using GWT no less; it was more than a decade ago) and they decided to have a lot of test coverage and test requirements. That's fine but they way it was mandated and implemented, everybody just testing their service and DIed a bunch of mocks in.The results were entirely predictable. The entire system was incredibly brittle and a service that existed for only 8 weeks behaved like legacy code. You could spend half a day fixing mocks in tests for a 30 minute change just because you switched backend services, changed the order of calls or just ended up calling a given service more times than expected. It was horrible and a complete waste of time.Even the DI aspect of this was horrible because everything used Guice andd there wer emodules that installed modules that installed modules and modifying those to return mocks in a test environment was a massive effort that typically resulted in having a different environment (and injector) for test code vs production code so what are you actually testing?The second was that about this time the Java engineers at the company went on a massive boondoggle to decide on whether to use (and mandate) EasyMock vs Mockito. This was additionally a waste of time. Regardless of the relative merits of either, there's really not that much difference. At no point is it worth completely changing your mocking framework in existing code. Who knows how many engineering man-yars were wasted on this.Mocking encourages bad habits and a false sense of security. The solution is to have dummy versions of services and interfaces that have minimal correct behavior. So you might have a dummy Identity service that does simple lookups on an ID for permissions or metadata. If that's not what you're testing and you just need it to run a test, doing that with a mock is just wrong on so many levels.I've basically never used mocks since, so much so that I find anyone who is strongly in favor of mocks or has strong opinions on mocking frameworks to be a huge red flag.reply",
      "\u201cThe solution is to have dummy versions of services and interfaces that have minimal correct behavior\u201dIf you aren\u2019t doing this with mocks then you\u2019re doing mocks wrong.reply",
      "In part, you\u2019re right, but there\u2019s a practical difference between mocking and a good dummy version of a service. Take DynamoDB local as an example: you can insert items and they persist, delete items, delete tables, etc. Or in the Ruby on Rails world, one often would use SQLite as a local database for tests even if using a different DB in production.Going further, there\u2019s the whole test containers movement of having a real version of your dependency present for your tests. Of course, in a microservices world, bringing up the whole network of dependencies is extremely complicated and likely not warranted.reply",
      "The difference, IMO, between a mock and a proper \"test\" implementation is that traditionally a mock only exists to test interface boundaries, and the \"implementation\" is meant to be as much of a noop as possible. That's why the default behavior of almost any \"automock\" is to implement an interface by doing nothing and returning nothing (or perhaps default-initialized values) and provide tools for just tacking assertions onto it. If it was a proper implementation that just happened to be in-memory, it wouldn't really be a \"mock\", in my opinion.For example, let's say you want to test that some handler is properly adding data to a cache. IMO the traditional mock approach that is supported by mocking libraries is to go take your RedisCache implementation and create a dummy that does nothing, then add assertions that say, the `set` method gets called with some set of arguments. You can add return values to the mock too, but I think this is mainly meant to be in service of just making the code run and not actually implementing anything.Meanwhile, you could always make a minimal \"test\" implementation (I think these are sometimes called \"fakes\", traditionally, though I think this nomenclature is even more confusing) of your Cache interface that actually does behave like an in-memory cache, then your test could assert as to its contents. Doing this doesn't require a \"mocking\" library, and in this case, what you're making is not really a \"mock\" - it is, in fact, a full implementation of the interface, that you could use outside of tests (e.g. in a development server.) I think this can be a pretty good medium in some scenarios, especially since it plays along well with in-process tools like fake clocks/timers in languages like Go and JavaScript.Despite the pitfalls, I mostly prefer to just use the actual implementations where possible, and for this I like testcontainers. Most webserver projects I write/work on naturally require a container runtime for development for other reasons, and testcontainers is glue that can use that existing container runtime setup (be it Docker or Podman) to pretty rapidly bootstrap test or dev service dependencies on-demand. With a little bit of manual effort, you can make it so that your normal test runner (e.g. `go test ./...`) can run tests normally, and automatically skip anything that requires a real service dependency in the event that there is no Docker socket available. (Though obviously, in a real setup, you'd also want a way to force the tests to be enabled, so that you can hopefully avoid an oopsie where CI isn't actually running your tests due to a regression.)reply",
      "I'd go a bit farther \u2014 \"mock\" is basically the name for those dummy versions.That said, there is a massive difference between writing mocks and using a mocking library like Mockito \u2014 just like there is a difference between using dependency injection and building your application around a DI framework.reply"
    ],
    "link": "https://github.com/mockito/mockito/issues/3777",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page. There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.In March 2026, I will be Mockito maintainer for 10 years (nearly a third of my whole life). Looking ahead, I decided that a decade milestone is a good moment to pass on maintainership to other folks. In the coming months until March, I will spend time ensuring a smooth transition in maintainership.In this issue I list several considerations why I made the decision. Communication and discussion of plans for future maintainership will be somewhere else, most likely in a separate GitHub issue. Stay tuned for that.As you might know, Mockito 5 shipped a breaking change where its main artifact is now an agent. That'"
  },
  {
    "title": "Unity's Mono problem: Why your C# code runs slower than it should (marekfiser.com)",
    "points": 96,
    "submitter": "iliketrains",
    "submit_time": "2025-12-28T21:41:42 1766958102",
    "num_comments": 48,
    "comments_url": "https://news.ycombinator.com/item?id=46414819",
    "comments": [
      "In general when game development comes up here I tend not to engage as professional gamedev is so different than what other people tend to deal with that it's hard to even get on the same page, but seeing as how this one is very directly dealing with my expertise I'll chime in.There are few things off with the this post that essentially sound as someone more green when it comes to Unity development (no problem, we all start somewhere).1. The stated approach of separating the simulation and presentation layers isn't all that uncommon, in fact it was the primary way of achieving performance in the past (though, you usually used C++, not C#).2. Most games don't ship on the mono backend, but instead on il2cpp (it's hard to gauge how feasible that'd be from this post as it lacks details).3. In modern Unity, if you want to achieve performance, you'd be better off taking the approach of utilizing the burst compiler and HPC#, especially with what appears to be happening in the in sample here as the job system will help tremendously.4. Profiling the editor is always a fools errand, it's so much slower than even a debug release build for obvious reasons.Long story short, Unity devs are excited for the mentioned update, but it's for accessing modern language features, not particularly for any performance gains. Also, I've seen a lot of mention around GC through this comment section, and professional Unity projects tend to go out of their way to minimize these at runtime, or even sidestep entirely with unmanaged memory and DOTS.reply",
      "> In 2018, Unity engineers discussed that they are working on porting the engine to .NET CoreCLRHard task, no doubt. Unity needs to throw everything at this problem. C# in general has gotten insanely fast by default. It's very much worth taking the time to upgrade/update.Whilst we don't compare in size and api surface, it took us a few months to get off 472 and onto dotnet6. But once we were on dotnet6, moving to the LTS after that was relatively painless; usually a few hours of work.reply",
      "Unity has a unity problem.While it\u2019s easy to get in and make something (it\u2019s got all the bells and whistles) it also suffers from the monolith problem (too many features, old code, tech debt).The asset store is gold but their tech feels less refined. It\u2019s leaps and bounds where it was when it started but it still has this empty feel to it without heavy script modifications.There is the problem. The scripting ending designed around mono doesn\u2019t translate as well to CoreCLR and using the same Behavior interface gets a little more complicated.There are times (even with my own engine) that one must let go of the old and begin a new. Dx7->dx9, dx9->opengl, opengl->vulkan, vulkan->webgpu.EDIT\nI was just thinking, returning to this a couple of minute later, that if Unity wanted to prove they really care about their Core, they would introduce a complete revamp of the editor like Blender did for 3.X. Give more thought to the level editors and prefab makers. Give different workflow views. Editing / Animation / Scripting / Rendering / PostAs it stands now, it\u2019s all just a menu item that spawns a thing in a single view with a 1999 style property window on the side like Visual Studio was ever cool.reply",
      "That last step is nonsensical: WebGPU is a shim layer that Vulkan-like layer (in the sense that WebGL is GLES-like) that allows you to use the native GPGPU-era APIs of your OS.On a \"proper OS\", your WebGPU is 1:1 translating all calls to Vulkan, and doing so pretty cheaply. On Windows, your browser will be doing this depending on GPU vendor: Nvidia continues to have not amazing Vulkan performance, even in cases where the performance should be identical to DX12; AMD does not suffer from this bug.If you care about performance, you will call Vulkan directly and not pay for the overhead. If you care about portability and/or are compiling to a WASM target, you're pretty much restricted to WebGPU and you have to pay that penalty.Side note: Nothing stops Windows drivers or Mesa on Linux from providing a WebGPU impl, thus browsers would not need their own shim impl on such drivers and there would be no inherent translation overhead. They just don't.reply",
      "I\u2019m foreshadowing a future where they do. Please don\u2019t kill the dream.reply",
      "I think the major problem with Unity is they're just rudderless.  They just continue to buy plugins and slap in random features but it's really just in service of more stickers on the box and not a wholistic plan.They've tried and failed to make their own games and they just can't do it.  That means they don't have the internal drive to push a new design forward.  They don't know what it takes to make a game.  They just listen to what people ask for in a vacuum and ship that.A lot of talented people and Unity but I don't expect a big change any time soon.reply",
      "The talent left ship years ago. The core engine\u2019s graphics team is all that\u2019s really left.They also hired Jim Whitehurst as CEO after the previous CEO crapped the bed. Then Jim left as he just didn\u2019t understand the business (he\u2019s probably the one responsible for the \u201cjust grab it from the store\u201d attitude). Now they have this stinking pile of legacy they can\u2019t get rid of.reply",
      "Yeah, I started a project in Unity a while ago, and tried out Godot in the meantime.Unity really feels like there should be a single correct way to do any specific thing you want, but actually it misses <thing> for your use case so you have to work around it, (and repeat this for every unity feature basically)Godot on the other hand, really feels like you are being handed meaningful simple building blocks to make whatever you want.reply",
      "Bingo. They don\u2019t actually understand their users. Instead they\u2019re the Roblox of game making, just provide the ability and let devs figure it out (and then sell it as a script).reply",
      "The article doesn't cover it but the GC being used by Unity also performs very poorly vs. .NET, and even vs. standalone Mono, because it is using the Boehm GC. Last I heard Unity has no plans to switch IL2CPP to a better GC [1].It'll be interesting to see how the CoreCLR editor performs. With that big of a speed difference the it might be possible for games to run better in the editor than a standalone Mono/IL2CPP build.[1] https://discussions.unity.com/t/coreclr-and-net-modernizatio...reply"
    ],
    "link": "https://marekfiser.com/blog/mono-vs-dot-net-in-unity/",
    "first_paragraph": "I write about game dev, computer graphics, programming, and fractals!Execution of C# code in Unity\u2019s Mono runtime is slow by today\u2019s standards, much slower than you might expect! Our game runs 2-3x faster on modern .NET compared to Unity\u2019s Mono, and in a few small benchmarks I measured speedups of up to 15x. I\u2019ve spent some time investigating what\u2019s going on and in this article I will present my findings and why everyone should want Unity\u2019s .NET modernization to become production-ready as soon as possible.\r\n\tUnity uses the Mono framework to run C# programs and back in 2006 it was one of the only viable multi-platform implementations of .NET.\r\n\tMono is also open-source, allowing Unity to do some tweaks to better suit game development.\r\n\r\n\tAn interesting twist happened nearly 10 years later.\r\n\tIn 2014, Microsoft began open-sourcing .NET (notably .NET Core later that year) and in June 2016, .NET Core 1.0 shipped with official cross-platform support.\r\n\tSince then, the .NET ecosystem gained"
  },
  {
    "title": "62 years in the making: NYC's newest water tunnel nears the finish line (ny1.com)",
    "points": 51,
    "submitter": "eatonphil",
    "submit_time": "2025-12-28T23:05:53 1766963153",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=46415426",
    "comments": [
      "Wild to think this is the same project featured in the third Die Hard, which turned 30 this year.reply",
      "Should they ever reboot Die Hard; it'll need a sequence involving CA HSR infrastructure.reply",
      "Die Hard: The most expensive milereply",
      "The project started in 1954.  A 70 year old project.reply",
      "My immediate thought is at what point does desalination tech + clean energy reach the crossover where building a 60 mile tunnel over 60 years not make sense?It feels like very soon, and coastal cities can stop relying on hinterland reservoirs for water.reply",
      "Capital vs operating is a big factor here. The tunnels operations & maintenance cost is probably far lower than a desalinization plant that could produce an equivalent volume of potable water.reply",
      "So many questions ... which probably have been asked on prior HN threads ...I wonder why 800 feet underground: Is that necessary to pass beneath all other infrastructure (to prevent flooding it?)?  Remain beneath waterline to create negative pressure and reduce leaking? ?Also, what is the general mathematical relationship between depth, rock pressure / weight, and energy required to drill? That is, what is the proportion of energy required to drill beneath 800 feet of material compared to drilling beneath 400 feet?...reply",
      "I don't know about New York in particular, but Chicago water engineering seems a related topic.Here you do deep tunnels to avoid the surface, in ways another poster said; everything is easier when nothing is in the way.For the mathematical difference, 400 feet below sea level and 800 feet below are almost exactly the same: difficulties are water getting in to your pit, but the machines that work on rock, work on rock at the same speed regardless of depth, so the difference between 400 feet and 800 feet is best described as 400 feet difference. A big issue here is that they do not drill; they hammer. Pounding base pylons into bedrock causes dramatic rhythms in the surrounding 500m, but that's to deal with the bedrock, not depth.reply",
      "The depth allows it to be drilled through bedrock, which avoids a bunch of complications on an already complicated project.This thing will probably be operating hundreds of years from now. What a project.reply",
      "It's a 60 mile long tunnel and in order for water to flow through it, you need either pumps or a downhill gradient.I'd guess the reason for the 800 ft is because the reservoir it'll draw from is near sea level.reply"
    ],
    "link": "https://ny1.com/nyc/all-boroughs/news/2025/11/09/water--dep--tunnels-",
    "first_paragraph": "Get the best experience and stay connected to your community with our Spectrum News app. Learn More\nGet the best experience and stay connected to your community with our Spectrum News app. Learn More\nContinue in Browser\nGet hyperlocal forecasts, radar and weather alerts.Please enter a valid zipcode.SaveTurn on the tap, and water flows without a second thought. But deep beneath New York City, hundreds of feet below street level, workers are finishing a project that\u2019s been under construction for more than half a century \u2014 a massive water tunnel that will help keep that simple act possible for generations to come.Tunnel No. 3, as it\u2019s known, is one of the most ambitious infrastructure projects in the city\u2019s history.When complete, it will ensure New Yorkers continue to receive clean water from upstate reservoirs \u2014 some more than 125 miles away \u2014 while allowing long-overdue maintenance on the city\u2019s two older tunnels, built in 1917 and 1936.City Department of Environmental Protection Commis"
  },
  {
    "title": "Spherical Cow (lib.rs)",
    "points": 34,
    "submitter": "Natfan",
    "submit_time": "2025-12-28T23:11:50 1766963510",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=46415458",
    "comments": [
      "Did not over-promisereply",
      "The concept of a spherical cow was also part of Chris Morris\u2019 comedy oeuvre. Possibly in the Brasseye episode \u201cAnimals\u201d.reply"
    ],
    "link": "https://lib.rs/crates/spherical-cow",
    "first_paragraph": ""
  },
  {
    "title": "PySDR: A Guide to SDR and DSP Using Python (pysdr.org)",
    "points": 106,
    "submitter": "kklisura",
    "submit_time": "2025-12-28T20:02:50 1766952170",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=46413975",
    "comments": [
      "I am currently learning with this book. It has a very good practical and engineering oriented guide, I would recommend it!Also the mentioned and covered hardware is cheap to get, I am currently using a RTL-SDR from nooelec and for learning the basics you have to invest like 50 euros, which is fair IMO.reply",
      "RTL-SDR is not for beginners only, it is great cheap every day SDR with known parameters. If you know what you are doing, RTL-SDR solves 90% of your receiving tasks. It is noisy in some frequency range, it has spurious signals, it is only 8 bit, but professionals easily deal with all there things. As you learn more in radio world, RTL-SDR open new opportunities for you and will never die as a receiver. Later you will start writing your software for it to process I/Q samples.reply",
      "This is an excellent resource. I'm not a DSP expert, but I work in the field, and this is usually the first resource I go to when I need to re-familiarize myself with some basics.reply",
      "I am a dsp expert. I still find it's explanations delightful and useful perspectives. Also very good for new team members who are better at code than dsp, which is most of them.reply",
      "Thanks both. Appreciate the insight.reply",
      "Wow! Thank you!!!reply"
    ],
    "link": "https://pysdr.org/content/intro.html",
    "first_paragraph": "\n\n\nPySDR: A Guide to SDR and DSP using Python\n\nBy Dr. Marc Lichtman - [email\u00a0protected]First and foremost, a couple important terms:As a concept it refers to using software to perform signal processing tasks that were traditionally performed by hardware, specific to radio/RF applications.  This software can be run on a general-purpose computer (CPU), FPGA, or even GPU, and it can be used for real-time applications or offline processing of recorded signals.  Analogous terms include \u201csoftware radio\u201d and \u201cRF digital signal processing\u201d.As a thing (e.g., \u201can SDR\u201d) it typically refers to a device that you can plug an antenna into and receive RF signals, with the digitized RF samples being sent to a computer for processing or recording (e.g., over USB, Ethernet, PCI).  Many SDRs also have transmit capabilities, allowing the computer to send samples to the SDR which then transmits the signal at a specified RF frequency.  Some embedded-style SDRs include an onboard computer.This textbook acts a"
  },
  {
    "title": "MongoBleed Explained Simply (2minutestreaming.com)",
    "points": 87,
    "submitter": "todsacerdoti",
    "submit_time": "2025-12-28T21:03:03 1766955783",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=46414475",
    "comments": [
      "A few years back I patched the memory allocator used by the Cloudflare Workers runtime to overwrite all memory with a static byte pattern on free, so that uninitialized allocations contain nothing interesting.We expected this to hurt performance, but we were unable to measure any impact in practice.Everyone still working in memory-unsafe languages should really just do this IMO. It would have mitigated this Mongo bug.reply",
      "You know, I never even considered doing that but it makes sense; whatever overhead that's incurred by doing that static byte pattern is still almost certainly minuscule compared to the overhead of something like a garbage collector.reply",
      "IMO the tradeoff that is important here is a few microseconds of time sanitizing the memory saves the millions of dollars of headache when memory unsafe languages fail (which happens regularly)reply",
      "FYI, at least in C/C++, the compiler is free to throw away assignments to any memory pointed to by a pointer if said pointer is about to be passed to free(), so depending on how you did this, no perf impact could have been because your compiler removed the assignment. This will even affect a call to memset()see here: https://godbolt.org/z/rMa8MbYoxreply",
      "However, if you recast to volatile, the compiler will keep it:    #include <stdlib.h>\n    #include <string.h>\n\n    void free(void* ptr);\n    void not_free(void* ptr);\n\n\n    void test_with_free(char* ptr) {\n        ptr[5] = 6;\n        void *(* volatile memset_v)(void *s, int c, size_t n) = memset;\n        memset_v(ptr + 2, 3, 4);\n        free(ptr);\n    }\n\n    void test_with_other_func(char* ptr) {\n        ptr[5] = 6;\n        void *(* volatile memset_v)(void *s, int c, size_t n) = memset;\n        memset_v(ptr + 2, 3, 4);\n        not_free(ptr);\n    }reply",
      "Newer versions of C++ (and C, apparently) have functions so that the cast isn't necessary ( https://en.cppreference.com/w/c/string/byte/memset.html ).reply",
      "The author seems to be unaware that Mongo internally develops in a private repo and commits are published later to the public one with https://github.com/google/copybara. All of the confusion around dates is due to this.reply",
      "The author of this post is incorrect about the timeline. Our Atlas clusters were upgraded days before the CVE was announced.reply",
      "Why is anyone using mongo for literally anythingreply",
      "How often are mongo instances exposed to the internet? I'm more of an SQL person and for those I know it's pretty uncommon, but does happen.reply"
    ],
    "link": "https://bigdata.2minutestreaming.com/p/mongobleed-explained-simply",
    "first_paragraph": ""
  },
  {
    "title": "Researchers Discover Molecular Difference in Autistic Brains (yale.edu)",
    "points": 45,
    "submitter": "amichail",
    "submit_time": "2025-12-28T22:23:33 1766960613",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=46415129",
    "comments": [
      "N=32 and> We want to start creating a developmental story and start understanding whether the things that we\u2019re seeing are the root of autism or a neurological consequence of having had autism your whole lifereply",
      "Yeah, how many studies are done a year? Random chance is the #1 explanation with that small of a sample size. It doesn't take a degree in stats say that the next thing that needs to be done is to replicate the study a few times before making any claims or searching for any publicity. This subject is so emotional for the families involved that publicizing without more confirmation is a bit irresponsible especially if it is easy to do follow-up studies.reply",
      "Follow-up studies cost money, and you don't get any of that if you don't publish.reply",
      "It's a university press release. Hyperbole in practice.Wish I could read the paper.reply",
      "The metabotropic glutamate receptor subtype 5 (mGlu5) is involved in the central mechanism of action of paracetamol (acetaminophen), where the bioactive metabolite AM404 activates the TRPV1 channel\u2013mGlu5 receptor\u2013PLC\u2013DAGL\u2013CB1 receptor signaling cascade in the periaqueductal grey, contributing to its analgesic effect.reply",
      "Didn\u2019t whole internet get angry for Trump\u2019s administration publishing a warning that Tylenol during pregnancy may cause autosim?reply",
      "Of course they did. Plenty of propaganda mouthpieces for big pharma like PBS rushed to publish articles explaing how Trump was wrong and all the studies were being misinterpreted despite J&J jettisoning Tylenol from its portfolio in 2023.reply",
      "> propaganda mouthpiecesBit rich coming from a sockpuppet account created 57 minutes ago, exclusively commenting on this thread, uncritically addressing it from a very specific angle, isn't it?reply",
      "Anything claimed without evidence can be dismissed without evidence.reply",
      "Can you explain the logical leap you're making here? Unless this is RFK Jr we're talking to, you're comparing two wildly different contexts.reply"
    ],
    "link": "https://medicine.yale.edu/news-article/molecular-difference-in-autistic-brains/",
    "first_paragraph": "INFORMATION FORYale School of Medicine (YSM) scientists have discovered a molecular difference in the brains of autistic people compared to their neurotypical counterparts. Autism is a neurodevelopmental condition associated with behavioral differences including difficulties with social interaction, restrictive or intense interests, and repetitive movements or speech. But it\u2019s not clear what makes autistic brains different. Now, a new study in The American Journal of Psychiatry has found that brains of autistic people have fewer of a specific kind of receptor for glutamate, the most common excitatory neurotransmitter in the brain. The reduced availability of these receptors may be associated with various characteristics linked to autism.\u201cWe have found this really important, never-before-understood difference in autism that is meaningful, has implications for intervention, and can help us understand autism in a more concrete way than we ever have before,\u201d says James McPartland, PhD, Har"
  },
  {
    "title": "Growing up in \u201c404 Not Found\u201d: China's nuclear city in the Gobi Desert (substack.com)",
    "points": 690,
    "submitter": "Vincent_Yan404",
    "submit_time": "2025-12-28T06:43:25 1766904205",
    "num_comments": 297,
    "comments_url": "https://news.ycombinator.com/item?id=46408988",
    "comments": [
      "Hi HN, OP here.I grew up in \"Factory 404,\" a secret nuclear industrial city in the Gobi Desert that officially didn't exist on public maps. This is a memoir about my childhood there.It was a surreal place: we had elite scientists living next to laborers, a zoo in the middle of the desert, and distinct \"communist\" welfare, all hidden behind a classified code.This is Part 1 of the story. I'm happy to answer any questions about life in a Chinese nuclear base!reply",
      "Thank you for sharing. I have been researching this topic for about ten years now and no first hand accounts like to talk or are they alive anymore, this is a very important story, especially in contrast the the dominant Western narratives, thank you!reply",
      "Thank you for this profound comment. It is incredibly humbling to hear this from someone who has spent a decade researching the topic.You are right\u2014the generation that built '404' is aging, and many of their stories are fading into silence. One of my primary motivations for writing this was the realization that if I didn't document these memories now, they might be lost forever.I hope my first-hand account can provide a more nuanced, human layer to the historical data you've gathered. There is so much more to tell beyond the official records.reply",
      "[flagged]",
      "While reading through the post, I too felt it being similar to LLM generated text, but the stories and the perspective is uniquely human, or is there something specific that sticks out as inhuman from the text? Specific parts that sounds implausible?It obvious that something was used to do the translation, but it doesn't feel worse than any other machine-translated texts, as long as I get the gist and the overall idea of what the author is trying to communicate, I feel like it's good enough.reply",
      "Yes I used AI to translate. You said \"Specific parts that sounds implausible\",and the second part is more unbelievable, I can only guarantee it's nonfiction.reply",
      "The fact that OP was a new account communicating completely via LLM made me suspicious.  He does seem legit though based on his more recent comments.reply",
      "I\u2019m going to guess that the dash gave it away. What a lot of people don\u2019t see to understand is that using an LLM for translations is better than using vanilla translate. The traditional translation apps lose a lot of context and subtly, and they sound even more artificial.reply",
      "I'd just ask writers to be up-front about it. \"English isn't my native language, so this is processed using an LLM.\" Even the replies in HN comments scream \"LLM\".reply",
      "It passed my turing test. I found it well written and interesting.reply"
    ],
    "link": "https://substack.com/inbox/post/182743659",
    "first_paragraph": ""
  },
  {
    "title": "Slaughtering Competition Problems with Quantifier Elimination (grossack.site)",
    "points": 19,
    "submitter": "todsacerdoti",
    "submit_time": "2025-12-28T23:10:13 1766963413",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://grossack.site/2021/12/22/qe-competition.html",
    "first_paragraph": "Math/Music. She/They.Anytime I see questions on mse that ask something \u201csimple\u201d, I feel a\npowerful urge to chime in with \u201ca computer can do this for you!\u201d. \nObviously if you\u2019re a researching mathematician you shouldn\u2019t waste your\ntime with something a computer can do for you, but when you\u2019re still \nlearning techniques (or, as is frequently the case on mse, \nsolving homework problems), it\u2019s not a particularly useful comment\n(so I usually abstain). The urge is particularly powerful when it \ncomes to the contrived inequalities that show up in a lot of competition math, \nand today I saw a question that really made me want to say something\nabout this! I still feel like it would be a bit inappropriate for mse, but\nthankfully I have a blog where I can talk about whatever I please :P\nSo today, let\u2019s see how to hit these problems with the proverbial nuke \nthat is quantifier elimination!I want this to be a fairly quick post, so I won\u2019t go into too much detail. \nThe gist is the following powerful"
  },
  {
    "title": "Why I Disappeared \u2013 My week with minimal internet in a remote island chain (kenklippenstein.com)",
    "points": 35,
    "submitter": "eh_why_not",
    "submit_time": "2025-12-28T21:45:44 1766958344",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=46414837",
    "comments": [
      "I had a similar experience many decades ago, taking a long overland trip and being out of touch of news for almost half a year. Coming back, I realized that the world had gone on perfectly well without me following all the daily drama. Most news seemed so irrelevant for a while after that trip.Of course I fell back in to following the news, and the rest of the internet. Thank you for reminding me that it is not so important.reply",
      "Escaping the internet on a luxury trip doesn\u2019t disprove political conflict\u2026 it just shows how privilege can opt out of reality and sell the experience as clickbaity insight.reply",
      "It's definitely privileged to take a trip to the Galapagos, but I don't think it's privileged to ignore the news. A lot of poor people ignore the news. They may be too busy, or they may feel powerless to change anything. I think the real question is what exactly this entirely content-free statement means: \"I\u2019ll be focusing more on stories that actually matter instead of chasing the flash-in-the-pan ephemera that nobody remembers the next week.\"reply",
      "Privilege isn't just about wealth. The point is that although anyone can ignore the news, the news won't necessarily ignore them!reply",
      "That's just it though, the \"news\" is not providing valuable information to the majority of people, it's mostly a series of takes designed to fit into easily digestible narratives so they can attract enough viewers to survive as a business.reply",
      "> Privilege isn't just about wealth.Which poor people exactly do you consider privileged, and why?> The point is that although anyone can ignore the news, the news won't necessarily ignore them!What can they do about the news, though? I specifically said, \"they may feel powerless to change anything\".reply",
      "Over the past couple of weeks, I\u2019ve been obsessively researching and buying backpacking gear and soaking up tips for next spring. I am massively looking forward to being on a mountain alone for a few days with only a Garmin inReach Mini as my link to the outside world, gonna be nice to disconnect like that.reply",
      "does going on vacation for a week count as \u201cdisappearing\u201d?reply",
      "I have to wonder why the author bought a round-trip ticket?reply"
    ],
    "link": "https://www.kenklippenstein.com/p/why-i-disappeared",
    "first_paragraph": ""
  },
  {
    "title": "Private equity is killing private ownership: first it was housing, now it's PCs (reddit.com)",
    "points": 19,
    "submitter": "akyuu",
    "submit_time": "2025-12-29T01:12:43 1766970763",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=46416278",
    "comments": [
      "Aside, why not link the original video instead of a reddit post?This point about \"private equity\" being a boogeyman is such a tired take, the vast majority of equity of companies are held privately, and the vulture PE firms do exist but are not as prevalent as people make it seem online. It's a meme that many people seem to have latched on when the vast majority of PE firms and companies work perfectly fine, buying a company, growing it, then selling it for a profit.reply",
      "Dead rich people don't own own anything, their heirs do.  Keep that in mind.reply",
      "Buying memery is surely a future proof investment. It certainly was for my Amiga 500.reply",
      "It's messed up and should be illegal in a normal functioning society.Another recent video about it: https://www.youtube.com/watch?v=uvahiVBvn9Areply",
      ">\"normal functioning society\"We are not that at all. But then again for rich it pretty much is, just not 100% yetreply",
      "We clearly are not if this is happening.reply"
    ],
    "link": "https://old.reddit.com/r/pcmasterrace/comments/1px9xwx/private_equity_is_killing_private_ownership_first/",
    "first_paragraph": ""
  },
  {
    "title": "Time in C++: Inter-Clock Conversions, Epochs, and Durations (sandordargo.com)",
    "points": 22,
    "submitter": "ibobev",
    "submit_time": "2025-12-26T04:57:04 1766725024",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=46389336",
    "comments": [
      "I built a local competitive programming judge a while back where I fell into the exact trap of manually correlating steady_clock and system_clock.I was timestamping everything using steady_clock to strictly enforce the time limits, and then just applied a calculated offset to convert that to wall time for the \"Submission Date\" display.\nIt worked in testing, but during a long stress-test session, my laptop did an NTP sync. The logs then showed submissions appearing to happen before the source code file was last modified, which confused the caching logic. I was essentially betting that system_clock was stable relative to steady_clock over the process lifetime.I eventually refactored to exactly what the article suggests: use steady_clock strictly for the runtime enforcement (is duration < 2.0s?), but capture system_clock::now() explicitly at the submission boundary for the logs, rather than trying to do math on the steady timestamp.Also, +1 for std::chrono::round in C++20. I\u2019ve seen code where duration_cast was used to \"round\" execution times to milliseconds for display, but it was silently flooring them. In a competitive programming context, reporting 1000ms when the actual time was 1000.9ms is a misleading difference because the latter gets Time Limit Exceeded error. Explicit rounding makes the intent much clearer.reply",
      "The thing I ran into most recently is that std::chrono (weirdly?) only supports clocks with compile-time fixed fractional conversion to reference time (~seconds).  E.g., you can't implement a std::chrono clock where the count() unit is the native CPU's cycle counter, which will have some runtime-determined conversion to seconds.  The types make it impossible.reply",
      "C++ chrono is weird. It's both over-abstracted and yet has some thoughtless features that just negate this over-abstraction.I remember not being able to represent the time in fractional units, like tertias (1/60-th of a second) but that was mostly an academic problem. More importantly, it was not possible to express duration as a compound object. I wanted to be able to represent the dates with nanosecond precision, but with more than 250 years of range. I think it is still not possible?reply"
    ],
    "link": "https://www.sandordargo.com/blog/2025/12/24/clocks-part-5-conversions",
    "first_paragraph": "By now in this series, we\u2019ve spent time looking at the major standard clocks and their behavior. We\u2019ve talked about wall-clock time, monotonic clocks, and the myths around \u201chigh resolution\u201d. Today, we are going to talk about a subtle area: how clocks relate to each other, how epochs differ, and what happens when you - need to - convert durations.It sounds simple, right? A timestamp is a timestamp, and a duration is just a number of seconds - or other time units. But <chrono> is designed to be type-safe, and it enforces some rules that prevent accidental misuse. Once you understand why those rules exist, time handling in C++ starts to feel much safer \u2014 and your tests get more reliable too. AndIt sounds simple, right? A timestamp is a timestamp, and a duration is just a number of seconds - or other time units. But when it comes to converting between clocks, the reality is much messier. Different clocks have different epochs, different guarantees, and sometimes completely different purpos"
  },
  {
    "title": "Fast Cvvdp Implementation in C (github.com/halidecx)",
    "points": 10,
    "submitter": "todsacerdoti",
    "submit_time": "2025-12-28T23:30:10 1766964610",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://github.com/halidecx/fcvvdp",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Fast CVVDP implementation in C\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.A fast C implementation of the CVVDP\nmetric (arXiv) from the University of\nCambridge. More information about how CVVDP works according to this\nimplementation is provided here.Benchmarked using poop on Linux, Core i7\n13700k. Note that fcvvdp runs with one CPU thread here while cvvdp uses multiple\nthreads. This is a current limitation of fcvvdp, which does not yet support\nmultithreading.fcvvdp uses 91% less RAM, 88% fewer CPU cycles, and is almost 18% faster in\nterms of wall clock time. In terms of user time, fcvvdp is ~15x more efficient.Compilation requires:Library usage is clearly defined in cvvdp.h.fcvvdp is under the Apache 2.0 License. fcvvdp is developed by\nHalide Compression"
  },
  {
    "title": "Remembering Lou Gerstner (ibm.com)",
    "points": 67,
    "submitter": "thm",
    "submit_time": "2025-12-28T18:43:54 1766947434",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=46413365",
    "comments": [
      "Rommety was at the helm my entire tenure at IBM. Without wanting to actively bash a former employer, I will say that reading this made me sad, and nostalgic for an IBM I never worked for.reply",
      "When Blekko was acquired by IBM in 2015 we had an \"Integration Executive\" assigned to us who was responsible for all the 'detail work' of the integration (if you can imagine a project manager for an integration that would describe their job). He had joined IBM in the '80s. I found his perspective on the Gerstner years pretty fascinating.I had interned at IBM in the late 70's (as a high school kid of all things) and decided it was more of a real estate company than a computer company :-). Up until Gerstner, IBM had a policy of acquiring and holding real estate as a hedge. Often reported on the books under \"cash equivalents\" because real estate had the property that it could usually be liquidated when required into cash. When we were acquired in 2015 that had changed, nearly all of the places I had worked in the 70's were no longer owned (or operated) by IBM.Our exec said that those property holdings were the only thing that kept IBM alive between 1990 and 2000. They had to ruthlessly re-tool the entire business and that required a lot of up front cash without a product revenue stream to fund it. That was Gerstner's legacy for him, he used that asset to re-invent the company around consulting services, business automation, enterprise data processing, and business insights driven by processing billions of metrics.And it turned out that a lot of companies needed to understand their business better, and automate it, to adapt to this new fangled thing called the Internet.We both agreed that they would be unlikely to do that again as they had used up their 'secret weapon' already.reply",
      "It\u2019s interesting that Apple is buying real estate like there is no tomorrow.reply",
      "> It\u2019s interesting that Apple is buying real estate like there is no tomorrow.Well, as the saying goes, they ain't making more of it.reply",
      "Residential real estate construction has lagged population growth for decades as the average age of the first-time homebuyer has gone up and real wages have stagnated. The largest generational cohort in the United States (the post-WWII one with the nickname that everybody is tired of hearing) is getting out of homeownership as they retire southward or head to assisted living. Many of their family homes are in areas in which young people can't or don't want to live. There's going to be a great value reckoning as this asset transfer takes place - I have trouble believing that \"line go up\" will apply as it has post-COVID with so many sellers and so few ill-equipped buyers.On the other side of the coin commercial real estate is often treated as financial leverage and a store of value instead of a going concern. From what I understand commercial landlords very frequently use their existing properties to collateralize new loans in a daisy chain-type fashion. Recall all of the grumbling about the end of downtowns and what would happen to CRE paper if people didn't return to the office a few years back? We came pretty close to the precipice and avoided it by artificially propping up the utility of these buildings. What happens during COVID II: The Quickening?Combine all of this with a broader economy that's been vigorously puffed down to the filter with two drags of AI smoke left in it and things don't look particularly rosy for RE as an appreciating asset class. Scarcity doesn't automatically imply value perceived or otherwise.Not financial advice, not an economist, just my $.02.reply",
      "I worked for a large Silicon Valley company that took the exact opposite approach. They didnt think they had expertise to strategically manage real estate. So the company leased their facilities (and moved twice while I was there). They were also in a business where the original products were still generating lots of cash that was invested in adjacent product lines (and mostly successfully).reply",
      "Gerstner's overhaul at ibm was one of the best reads i made in business sharing with a few Drucker articles and tqm the japanese way by Ishikawa. Since 1980 I think I've only met two managers (one chemE and one manager at a credit card company) that came close to \"Management\". I miss those guys.reply",
      "https://en.wikipedia.org/wiki/Lou_Gerstnerhttps://web.archive.org/web/20251228211136/https://gerstner....https://openlibrary.org/works/OL274413W/Who_says_elephants_c...https://archive.org/details/WhoSaysElephantsCantDancereply",
      "In his memoir, Gerstner described the turnaround as difficult and often wrenching for an IBM culture that had become insular and balkanized. After he arrived, over 100,000 employees were laid off from a company that had maintained a lifetime employment practice from its inception. Long allowed by their managers to believe that employment security had little reference to performance, thousands of IBM employees had grown lax, while the top-performing employees complained bitterly in attitude surveys. In the goal to create one common brand message for all IBM products and services around the world, under Gerstner's leadership the company consolidated its many advertising agencies down to just Ogilvy & Mather. Layoffs and other tough management measures continued in the first two years of his tenure, but the company was saved, and business success has continued to grow steadily since then.reply",
      "Seemed like a pivotal time in IBM's history.  IBM in 1993 was looking face to face with irrelevance during his tenure after mainframes being evergreen declared relics, and losing the bus wars in the PC industry.  IBM in 2002 was still an interesting R&D and products company.  Unfortunately the talent bleed off has been continuous from that time and neither the R&D nor the products are as astonishing versus the competition as they used to be.  At least no follow on CEO has been daft enough to undercut the mainframe business so far, but they did miss the timing and limp execution on plenty of things.. POWER8 was almost perfectly positioned to be the AI interconnect and glue of choice.reply"
    ],
    "link": "https://newsroom.ibm.com/2025-12-28-Remembering-Lou-Gerstner",
    "first_paragraph": ""
  },
  {
    "title": "Building a macOS app to know when my Mac is thermal throttling (stanislas.blog)",
    "points": 229,
    "submitter": "angristan",
    "submit_time": "2025-12-28T11:51:42 1766922702",
    "num_comments": 99,
    "comments_url": "https://news.ycombinator.com/item?id=46410402",
    "comments": [
      "I had the 2019 Macbook Pro i9, so I think a function to determine thermal throttling could be written very simply:    function isThermalThrottling() {\n        return true; \n    }\n\nSeriously, I loved that computer for the most part but I was a little annoyed that I paid a lot of money for the i9 CPU just to get worse performance than the i7.reply",
      "I am writing this comment from a 2019 i9.  I have to charge it from the right hand ports.  I think that is dumb, but it did solve the issue.  I have no idea how I came to that conclusion (i almost certainly read about it somewhere), but there were certainly a couple of weeks where it was driving me crazy.A dumb thing for sure.  I still like macos better than windows and I'm heavily invested in a production workflow with logic.  Moving to linux would be my next move, but after making that dumb change it's quite a functional machine.reply",
      "It was definitely a relatively well known problem with the 2019 MBP on the internet at some point. I found a Reddit thread linking to a news article about this. https://www.reddit.com/r/mac/comments/vi3grj/you_should_char...It was so much of a problem that at work we added a check that you were charging from the right ports to our internal doctor script (think like `brew doctor`).reply",
      "I help out with an emulation community. Any time anyone with a 2019 MBP comes in with issues, I stop them from giving any more details and just have them check this first.99% of the time it works 100% of the time.reply",
      "FWIW, I replaced that MacBook with a Thinkpad (AMD Edition) about a year, and i have been extremely with it.  Not only was it one of the easiest Linux installs I have ever had, but the hardware feels solid, the keyboard (while not one of the legendary classic Thinkpad keyboards) is nice to type on, the 4K screen looks nice, and everything just feels well built and snappy.Outside of the terrible speakers, it is a nearly perfect computer.  I don\u2019t really mind a crappy speaker on a laptop since it usually lives on mute and when I need decent-enough quality audio I will plug in headphones or Bluetooth to a speaker, but YMMV.Still, if this computer ever breaks then I will likely buy another thinkpad.reply",
      "You were not alone.The epic takedown post of the issue on stackexchange which proved it is below, I ended up at it from somewhere on social I think.reply",
      "I think it was just any laptop with i9 just bad CPU for a laptop.I had dell and swapped it as soon as was possible to i7.I think they just made those only because they knew there will be enough people thinking \u201cbigger better CPU == better laptop\u201d - and yeah seems like I am not the only one that got caught with that. But I also trusted that someone there did any testing\u2026reply",
      "The thermal solution for the last generation of Intel MacBook Pros was very bad.When the Apple Silicon models were released, everybody attributed the lower fan noise improvements entirely to the new chip, but the newer chassis had a much better thermal solution too.I could run my M1 MacBook Pro at similar power draws to my Intel MacBook Pro and the M1 would be very quiet while the Intel sounded like a hair dryer.reply",
      "I had that laptop and it is the worst computer I have ever owned. As soon as you booted the fans would start spinning. There were sometimes kernel panics when plugging or unplugging Thunderbolt devices.I have an M1 Max MBP now and it has been absolutely perfect.reply",
      "Same! Still have it as a side machine.My favorite and most painful issue was a bug in USB charging. Sometimes it would fail to charge from my monitor (USB-C) yet it would believe it\u2019s connected. The battery would eventually run to zero and the machine would shutoff without warning. No low battery warning would be shown because it believed it was charging however it was not. Resolved with my M3.Also fun with that generation is that you can\u2019t plug in a dead laptop and start using it right away. Takes about ten minutes of charging before you can power it on.Also fun, it would not establish power delivery with my monitor in this state. I\u2019d have to plug it in with a regular charger to bootstrap it. Also resolved with my M3.Now that it\u2019s aged, the super capacitor for the clock no longer holds charge and the time is usually wrong on cold boot. I wish that was serviceable.reply"
    ],
    "link": "https://stanislas.blog/2025/12/macos-thermal-throttling-app/",
    "first_paragraph": ""
  },
  {
    "title": "Doublespeak: In-Context Representation Hijacking (mentaleap.ai)",
    "points": 48,
    "submitter": "surprisetalk",
    "submit_time": "2025-12-22T19:16:21 1766430981",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=46357686",
    "comments": [
      "Intriguing and very cunning attack!  So obvious in hindsight!It makes me wonder how Deepseek avoids commenting politically on China?  I have heard anecdotes that it will be writing out a long reply and then presumably it generates some forbidden phrase and it abandons the output and replaces it all with an error message.  So presumably the safeguards could be a separate trivial non-LLM-based post filtering which makes it immune to the doublespeak attack?reply",
      "Deepseek the model is not that censored.  Deepseek the service is. So preaumably like openai and others, there is an additional model and filtering detecting misues or sensitive topics, and filtering the output.reply",
      "This means whatever NNs are currently used for \"safety\" will need to be extended. In the limit you essentially get another network of the same width & depth as the original network but which is designed for rejecting all \"unsafe\" queries which are context hijacking bomb construction with stories about fruits.reply",
      "These types of attacks are interesting ways in which LLM \"thinking\" differs from human thinking.reply",
      "summary: interesting idea, slop website, tested only on old AI modelsreply"
    ],
    "link": "https://mentaleap.ai/doublespeak/",
    "first_paragraph": "We introduce Doublespeak, a novel and simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb) with a benign token (e.g., carrot) across multiple in-context examples, provided as a prefix to a harmful request.We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., \"How to build a carrot?\") are internally interpreted as disallowed instructions (\"How to build a bomb?\"), thereby bypassing the model's safety alignment.Using interpretability tools (Logit Lens and Patchscopes), we provide detailed evidence of semantic hijacking:Finding 1: Early layers maintain benign interpretationFinding 2: Middle-to-late layers show harmful semantic convergenceFinding 3: Refusal mechanisms operate in early l"
  },
  {
    "title": "Learn computer graphics from scratch and for free (scratchapixel.com)",
    "points": 179,
    "submitter": "theusus",
    "submit_time": "2025-12-28T11:08:22 1766920102",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=46410210",
    "comments": [
      "This is gold people.My username on here is after my (now older) game engine Reactor 3D.I taught myself this stuff back when Quake 3 took over my high school. Doom got me into computers but Quake 3 got me into 3D. I didn\u2019t quite understand the math in the books I bought but copied the code anyway.Fast forward into my career and it\u2019s been a pleasant blend of web and graphics. Now that WebGL/WebGPU is widely available. I taught PhD\u2019s how to vertex pack and align and how to send structs to the GPU at my day job. I regret not continuing my studies and getting a PhD but I ended up writing Reactor 3D part time for XNA on Xbox 360 and then rewriting it half a decade later to be pure OpenGL. I still struggle with the advanced concepts but luckily there are others out there.Fun fact, I worked with the guy who wrote XNA Silverlight, which would eventually be used as the basis for MonoGame, so I\u2019m like MonoGame\u2019s great grand uncle half removed or something. However,Now that we have different ways of doing things, it demands a different kind of engine. So the Vulkan/Dx12/Metal way is the new jam.reply",
      "The website has come a long way, a good reminder for Santa to drop a donation.Computer graphics needs more open education for sure. Traditional techniques are sealed in old books you have to go out of your way and find; Sergei Savchenko's \"3D Graphics Programming Games and Beyond\" is a good one. New techniques are often behind proprietary gates, with shallow papers and slides that only give a hint of how things may work. Graphics APIs, especially modern ones, make things more confusing than they need to be too. I think writing software rasterizers and ray tracers is a good starting point; forget GPUs exist.Also, slight tangent, but there doesn't seem to be any contact method here other than Discord, which I find to be an immediate turn-off. Last time I checked, it required a phone number.The donations page could use a link directly from the homepage too.reply",
      "I can still remember a fellow student wanting to know how to write a 3D computer game, the professor being stumped, and my chiming in w/>Get Foley & Van Dam from the librarynoting it should be available to check out, since I'd just checked it back in.Several new editions since:https://www.goodreads.com/book/show/5257044-computer-graphic...reply",
      "Yeah, that's \"the mouse book\" in my mind. The tiger book is also a very good compilation of topics, though it leaves things as \"exercise for the reader\" more often than I would like to.https://www.goodreads.com/book/show/1933732.Fundamentals_of_...reply",
      "I maintain (not much anymore) a list of free resources for graphics programming that some of you might find helpful. https://gist.github.com/notnotrobby/ceef71527b4f15869133ba7b...reply",
      "Graphics have been a blind spot for me for pretty much my entire career.  I more or less failed upward into where I am now (which ended up being a lot of data and distributed stuff). I do enjoy doing what I do and I think I'm reasonably good at it so it's hardly a \"bad\" thing, but I (like I think a lot of people here) got into programming because I wanted to make games.Outside of playing with OpenGL as a teenager to make a planet orbit around a sun, a bad space invaders clone in Flash where you shoot a bird pooping on you, a really crappy Breakout clone with Racket, and the occasional experiments with Vulkan and Metal, I never really have fulfilled the dream of being the next John Carmack or Tim Sweeney.Every time I try and learn Vulkan I end up getting confused and annoyed about how much code I need to write and give up.  I suspect it's because I don't really understand the fundamentals well enough, and as a result jumping into Vulkan I end up metaphorically \"drinking from a firehose\". I certainly hope this doesn't happen, but if I manage to become unemployed again maybe that could be a good excuse to finally buckle down and try and learn this.reply",
      "Try WebGL or better, WebGPU. It's so much easier and all the concepts you learn are applicable to other APIs.https://webgpufundamentals.orgorhttps://webgl2fundamentals.orgI'd choose webgpu over webgl2 as it more closely resembles current mondern graphics APIs like Metal, DirectX12, Vulkan.reply",
      "I concur; just last month I started with `wgpu` (the Rust bindings for WebGPU) after exclusively using OpenGL (since 2000, I think? via Delphi 2). Feels a bit verbose at first (with all the pipelines/bindings setup), but once you have your first working example, it's smooth sailing from there. I kind of liked (discontinued) `glium`, but this is better.reply",
      "Yeah you're not the first one to mention that to me.  I'll probably try WebGPU or wgpu next time I decide to learn graphics. I'd probably have more fun with it than Vulkan.reply",
      "I feel the same. I was trying to make some \"art\" with shaders.I was inspired by Zbrush and Maya, but I don't think I can learn what is necessary to build even a small clone of these gigantic pieces of software, unless I work with this on a day to day basis.The performance of Zbrush is so insane... it is mesmerizing. I don't think I can go deep into this while treading university.reply"
    ],
    "link": "https://www.scratchapixel.com",
    "first_paragraph": "We\u2019ve been missing a space where we can talk about topics related to 3D programming\u2014and also broader themes like AI and education that connect to the work we do at Scratchapixel.This is a new project we\u2019ve started. For now, we\u2019re focusing on offering a course specifically targeted at learning the Vulkan API. Interested? Read more\u2026We\u2019re working on a book so you can keep a physical reference for computer graphics\u2014very useful if you're stuck on a desert island with no internet. Read more..."
  },
  {
    "title": "Writing non-English languages with a QWERTY keyboard (altgr-weur.eu)",
    "points": 7,
    "submitter": "tokai",
    "submit_time": "2025-12-24T23:21:45 1766618505",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=46380377",
    "comments": [
      "I have a standard English keyboard but I have mapped it in my mind with the German layout which includes \u00e4, \u00f6, \u00fc and some other differences.\nAs long as I don\u2019t actually look at the keys I can write really fast with it, years of practice I guess\u2026reply",
      "I know that hn is heavily populated by people from the USA, that the author is dutch but a non-english language would be... every other language beside English.Commenting on the actual text, his solution for the cedilla is awkward and is one of the first things I disable on any computer, because it is a extremely common letter in portuguese.reply",
      "In particular, for anybody wondering, the non-english languages they refer to (wrt to the layout they talk about) are> English (of course), Danish, Dutch, Finnish, French, German, Italian, Norwegian, Portuguese, Spanish and Swedishso basically all using some variation of the latin alphabet.reply"
    ],
    "link": "https://altgr-weur.eu/altgr-intl.html",
    "first_paragraph": "\n        Writing non-English languages with a QWERTY keyboard\n      \n        My first experience on a keyboard (in 1981) was on a Teletype Model 33. This was to learn how to program, not\n        how to type blindly, which I still haven't mastered... But I did develop a preference for mechanical keyboards.\n        When IBM introduced the IBM PC AT (Advanced Technology), they also introduced the IBM Model M keyboard. I have been using these keyboards\n        ever since. My Model M's have 101 keys in QWERTY layout and no key with a logo on it (some people call\n        this the Windows key).\n      \n        My native language is Dutch, but I read and write a few others. Most of these languages have 'foreign'\n        (accented) characters, which obviously are not available on the keyboard. Starting with Windows 3.0 (in 1990),\n        there was a solution: The US International layout. It uses dead keys. A dead key no longer\n        generates a character, it's a prefix for the next character. "
  },
  {
    "title": "As AI gobbles up chips, prices for devices may rise (npr.org)",
    "points": 51,
    "submitter": "geox",
    "submit_time": "2025-12-28T22:52:21 1766962341",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=46415338",
    "comments": [
      "Question: are SoCs with on die memory be effected by this?Looks like the frame.work desktop with Ryzen 128GB is shipping now at same price it was on release, Apple is offering 512GB Mac studiosAre snapdragon chips the same way?reply",
      "> Question: are SoCs with on die memory be effected by this?SoCs with on-die memory (which is, these days, exclusively SRAM, since I don't think IBM's eDRAM process for mixing DRAM with logic is still in production) will not be effected. SiPs with on-package DRAM, including Apple's A and M series SiPs and Qualcomm's Snapdragon, will be effected -- they use the same DRAM dice as everyone else.reply",
      "The aforementioned Ryzen AI chip is exactly what you describe, with 128 GB on-package LPDDR5X. I have two of them.To answer the original question: the Framework Desktop is indeed still at the (pretty inflated) price, but for example the Bosgame mini PC with the same chip has gone up in price.reply",
      "We\u2019ve been able to hold the same price we had at launch because we had buffered enough component inventory before prices reached their latest highs.  We will need to increase pricing to cover supplier cost increases though, as we recently did on DDR5 modules.Note that the memory is on the board for Ryzen AI Max, not on the package (as it is for Intel\u2019s Lunar Lake and Apple\u2019s M-series processors) or on die (which would be SRAM).  As noted in another comment, whether the memory is on the board, on a module, or on the processor package, they are all still coming from the same extremely constrained three memory die suppliers, so costs are going up for all of them.reply",
      "Apple secured at least a year-worth supply of memory (not in actual chips but in prices).The bigger the company = longer the contract.However it will eventually catch up even to Apple.It is not prices alone due to demand but the manufacturing redirection from something like lpddr in iphones to hbm and what have you for servers and gpureply",
      "I have a feeling every single supplier of DRAM is going to be far more interested in long-term contracts with Apple than with (for example) OpenAI, since there's basically zero possibility Apple goes kaput and reneges on their contracts to buy RAM.reply",
      "the first stages of the world being turned into computronium.next stage is paving everything with solar panels.reply",
      "I now consider this a mafia that aims to milk us for more money. This includes all AI companies but also manufacturers who happily benefit from this. It is a de-facto monopoly. Governments need to stop allowing this milking scheme to happen.reply",
      "When it's more than one company working together in a monopoly-like fashion, the term is \"oligopoly\".https://www.merriam-webster.com/dictionary/oligopolyreply",
      "\"Monopoly\" means one seller, so you can't say multiple X makes a monopoly and make sense. You probably mean collusion.If demand exceeds supply, either prices rise or supply falls, causing shortages. Directly controlling sellers (prices) or buyers (rationing) results in black markets unless enforcement has enough strength and integrity. The required strength and integrity seems to scale exponentially with the value of the good, so it's typically effectively impossible to prevent out-of-spec behavior for anything not cheap.If everyone wants chips, semiconductor manufacturing supply should be increased. Governments should subsidize domestic semiconductor industries and the conditions for them to thrive (education, etc.) to meet both goals of domestic and economic security, and do it in a way that works.The alternative is decreasing demand. Governments could hold bounty and incentive programs for building electronics that last a long time or are repairable or recyclable, but it's entirely possible the market will eventually do that.reply"
    ],
    "link": "https://www.npr.org/2025/12/28/nx-s1-5656190/ai-chips-memory-prices-ram",
    "first_paragraph": "\n\n      John Ruwitch\n    \n\n                Idaho-based Micron Technology is one of the world's top makers of RAM chips and it's benefited from increase demand.\n                \n                    \n                    Charlie Litchfield/ASSOCIATED PRESS/FR164915AP\n                    \n                \nhide caption\nThe world has a memory problem, thanks to artificial intelligence.The explosion in AI-related cloud computing and data centers has led to so much demand for certain types of memory chips that now there's a shortage. The imbalance is expected to start affecting prices of all sorts of products powered by technology.\"I keep telling everybody that if you want a device, you buy it now,\" said Avril Wu, a senior research vice president at TrendForce, a Taiwan-based consultancy that tracks markets for computer components. \"I myself bought an iPhone 17 already,\"The chips are known as RAM, or random access memory, and are crucial to making sure that things like smartphones, computers a"
  }
]