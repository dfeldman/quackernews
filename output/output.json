[
  {
    "title": "Tiny JITs for a Faster FFI (railsatscale.com)",
    "points": 113,
    "submitter": "hahahacorn",
    "submit_time": "2025-02-12T22:20:19 1739398819",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=43030388",
    "comments": [
      "> Rather than calling out to a 3rd party library, could we just JIT the code required to call the external function?I am pretty sure this is the basis of the LuaJIT FFI: https://luajit.org/ext_ffi.htmlI think LuaJIT's FFI is very fast for this reason.\n \nreply",
      "Between Rails At Scale and byroot's blogs, it's currently a fantastic time to be interested in in-depth discussions around Ruby internals and performance! And with all the recent improvements in Ruby and Rails, it's a great time to be a Rubyist in general!\n \nreply",
      "Is it? To me it seems like Ruby is declining [1]. It's still popular for a specific niche of applications, but to me it seems like it's well past its days of glory. Recent improvements are nice, but is a JIT really that exciting technologically in 2025?[1]: https://www.tiobe.com/tiobe-index/ruby/\n \nreply",
      "Ruby will probably never again be the most popular language in the world, and it doesn't need to be for the people who enjoy it to be excited about the recent improvements in performance, documentation, tooling, ecosystem, and community.\n \nreply",
      "Rails is experiencing something of a renaissance in recent years. It\u2019s easily one of the most pleasant programming experiences I\u2019ve had in years.All my new projects will be Rails. (What about projects that don\u2019t lend themselves to Rails? I don\u2019t take on such projects ;)\n \nreply",
      "Stable mature technology trumps glory.\n \nreply",
      "\"write as much Ruby as possible, especially because YJIT can optimize Ruby code but not C code\"I feel like I'm not getting something. Isn't ruby a pretty slow language? If I was dipping into native I'd want to do as much in native as possible.\n \nreply",
      "There was a little drama that played out as Java was getting a proper JIT.In one major release, there was a bunch of Java code responsible for handling some UI element activities. It was found to be a bottleneck, and rewritten in C code for the next major release.Then the JIT became properly useful, and the FFI overhead was more than the difference between the hand-tuned C code and what the JIT would spit out on its own. So in the next major release, they rolled back to the all-Java implementation.Java had a fairly reasonably fast FFI for that generation of programming language, but they swapped for a better one a few releases after that. And by then I wasn't doing a lot of Java UI code so I had stopped paying attention. But around the same time they were also making a cleaner interface between the platform-specific and the general Java code for UI, so I'm not entirely sure how that played out.But that's exactly the sort of see-sawing you need to at least keep an eye out for when doing this sort of work. Would you be better off waiting a couple milestones and saving yourself a bunch of hand-tuning work, or do you need it right now for political or technical reasons?\n \nreply",
      "That is where a JIT enters the picture, ideally a JIT can re-optimize to an ideal state.While this is suboptimal when doing one shot execution, when an application is long lived, mostly desktop or server workloads, this work pays off versus the overall application.For example, Dalvik had a pretty lame JIT, thus it was faster calling into C for math functions, eventually with ART this was no longer needed, JIT could outperform the cost of calling into C.https://developer.android.com/reference/android/util/FloatMa...\n \nreply",
      "There's a phenomenal breakdown by JPCamara (https://jpcamara.com/2024/12/01/speeding-up-ruby.html) on why the Ruby#each method was rewritten in Ruby (https://bugs.ruby-lang.org/issues/20182). And bonus content from tender love: https://railsatscale.com/2023-08-29-ruby-outperforms-c/.TL;dr - JIT rules.\n \nreply"
    ],
    "link": "https://railsatscale.com/2025-02-12-tiny-jits-for-a-faster-ffi/",
    "first_paragraph": "\n        2025-02-12\n      \u2022 \n          \nAaron PattersonCan we have a faster FFI for CRuby?  Yes.I love programming in Ruby, and I advocate for people to write as much Ruby as possible.\nBut sometimes you really really must call out to native code.\nEven in those cases, I encourage people to write as much Ruby as possible, especially because YJIT can optimize Ruby code but not C code.Taken to its logical extreme, this guidance means that if you want to call a native library, you should write a native extension with a very very limited API\nwhere most work is done in Ruby.\nAny native code would be a very thin wrapper around the function we actually want to call that just converts Ruby types in to the types required by the native function.Of course such a simplistic API would be well suited to work with a library like FFI.Now, usually I steer clear of FFI, and to be honest the reason is simply that it doesn\u2019t provide the same performance as a native extension.Lets take a look at a very simpl"
  },
  {
    "title": "Leaking the email of any YouTube user for $10k (brutecat.com)",
    "points": 1216,
    "submitter": "brutecat",
    "submit_time": "2025-02-12T11:19:42 1739359182",
    "num_comments": 359,
    "comments_url": "https://news.ycombinator.com/item?id=43024221",
    "comments": [
      "I found this title confusing.  For those who didn't make it toward the end of the article: the leaked emails didn't cost them anything (except their time and ingenuity), and they received 10k as the bug bounty.\n \nreply",
      "I thought they meant providing the services to leak the email of any user for $10K, perhaps per user. :)\n \nreply",
      "Me too.I thought it meant they were offering this as a service for $10k.\n \nreply",
      "Yeah, I thought it was going to be about compute cost for brute forcing some hash or something\n \nreply",
      "The domain name kind of suggests this interpretation, too.\n \nreply",
      "Since every 3rd message on this thread (at the time I wrote this) is about how Google underpaid for this bug, some quick basic things about vulnerability valuations:* Valuations for server-side vulnerabilities are low, because vendors don't compete for them. There is effectively no grey market for a server-side vulnerability. It is difficult for a third party to put a price on a bug that Google can kill instantaneously, that has effectively no half-life once discovered, and whose exploitation will generate reliable telemetry from the target.* Similarly, bugs like full-chain Android/Chrome go for hundreds of thousands of dollars because Google competes with a well-established grey market; a firm can take that bug and sell it to potentially 6 different agencies at a single European country.* Even then, bounty vs. grey market is an apples-oranges comparison. Google will pay substantially less than the grey market, because Google doesn't need a reliable exploit (just proof that one can be written) and doesn't need to pay maintenance. The rest of the market will pay a total amount that is heavily tranched and subject to risk; Google can offer a lump-sum payment which is attractive even if discounted.* Threat actors buy vulnerabilities that fit into existing business processes. They do not, as a general rule, speculate on all the cool things they might do with some new kind of vulnerability and all the ways they might make money with it. Collecting payment information? Racking up thousands of machines for a botnet? Existing business processes. Unmasking Google accounts? Could there be a business there? Sure, maybe. Is there one already? Presumably no.A bounty payout is not generally a referendum on how clever or exciting a bug is. Here, it kind of is, though, because $10,000 feels extraordinarily high for a server-side web bug.For people who make their nut finding these kinds of bugs, the business strategy is to get good at finding lots of them. It's not like iOS exploit development, where you might sink months into a single reliable exploit.This is closer to the kind of vulnerability research I've done recently in my career than a lot of other vuln work, so I'm reasonably confident. But there are people on HN who actually full-time do this kind of bounty work, and I'd be thrilled to be corrected by any of them.\n \nreply",
      "I don't remember if I've ever thanked you for the dose or reality you bring to these discussions, but if not - thank you! Before I started reading your comments on bug bounty payouts I'd probably have made the typical thoughtless (in my case) remark that the bounties are tiny, without actually thinking through the realistic dollar value of bugs found.Not to mention not really thinking through how obviously stupid it is to immediately compare a legal activity to a highly illegal one, as if they're real alternatives for most people.\n \nreply",
      "https://www.youtube.com/watch?v=Y0pdQU87dc8\n \nreply",
      "I think your comment energy is more https://youtu.be/Pzpx9f5ByyA?t=110\n \nreply",
      "A friend generated a tag cloud from all my comments here like 10 years ago and it was just the word \"No\" like a supermassive black hole ringed by dozens of tiny little words I was saying \"no\" about.\n \nreply"
    ],
    "link": "https://brutecat.com/articles/leaking-youtube-emails",
    "first_paragraph": ""
  },
  {
    "title": "YouTube's New Hue (design.google)",
    "points": 9,
    "submitter": "xnx",
    "submit_time": "2025-02-13T00:55:26 1739408126",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=43031545",
    "comments": [
      "I was just looking at the YouTube brand stuff the other day and found it funny that their official red was solid #FF0000. A little relic of programmer art from the early days.\n \nreply",
      "There's a big opportunity to build apolitical uncensored clone of YouTube. Current alternatives lack funding and don't function well\n \nreply"
    ],
    "link": "https://design.google/library/youtube-new-red-color",
    "first_paragraph": "0 results  The story behind a brand-new brand paletteAnyone who\u2019s been online has been on YouTube. What began in 2005 as a quirky idea for a video-dating site has since exploded into a global powerhouse of a media platform. Today, billions tune in daily to watch everything from cat videos to cooking tutorials, gaming marathons to music performances.For most of its lifespan, YouTube\u2019s brand has been associated with its iconic red logo. But a few months ago, users began to notice a new, softer shade of red across the site \u2014 plus a surprising magenta gradient in hallmark features like the video progress bar. It\u2019s a subtle but meaningful shift.So why did YouTube change color? In honor of YouTube\u2019s 20th birthday, we asked key members of the art direction and design teams why and how they did it. Here, they discuss how to keep a beloved brand feeling fresh, the power of color in brand identity, the challenges of choosing the right shade, and how accessibility considerations and motion design"
  },
  {
    "title": "The average CPU performance of PCs and notebooks fell for the first time (cpubenchmark.net)",
    "points": 89,
    "submitter": "doener",
    "submit_time": "2025-02-12T20:34:41 1739392481",
    "num_comments": 57,
    "comments_url": "https://news.ycombinator.com/item?id=43029474",
    "comments": [
      "Could it be related to world-wide inflation and decreasing real wages? Salaries have not kept up, hence people can't afford as powerful hardware as they used to. It also seems that top-of-the-line hardware don't depreciate as fast as it used to. Many several year's old GPUs are still expensive, for example.\n \nreply",
      "> Many several year's old GPUs are still expensive,They are propped up by demand and the fact that most of the new GPUs are marginally better than previous ones.\n \nreply",
      "New GPUs are quite a bit better than previous ones but perf oer dollar has been flat for a while now.Also, if you're talking gaming GPUs old ones work fine given there has not been a new PlayStation or Xbox in many years. Min spec for many games is 5 years old tech\n \nreply",
      "As the chart is updated bi-weekly, but the data point that may change is for the current year. The first few days or weeks of a new year are less accurate compared to the end of a year.\n \nreply",
      "Really makes you wish the chart showed some measure of variance\n \nreply",
      "Or just have the last data point include everything from the full 12 month period before (as the title \"year on year\" would suggest) and maybe even put it in the correct place on the x-axis (e.g. for today, Feb 12, 2025, about 11.8% of the full year gap width from the (Dec 31) 2024 point).\n \nreply",
      "January 2024: https://web.archive.org/web/20240123174954/https://www.cpube...January 2023: https://web.archive.org/web/20230130185431/https://www.cpube...Still, both previous years were notably up after the first month already. This is different and perhaps notable regardless. Either the release schedule is skewed forward or the hardware is genuinely stagnating. Or perhaps the benchmark is hitting some other CPU unrelated bottleneck.\n \nreply",
      "The chart also shows that single-thread performance hasn\u2019t improved that much in the past twelve or so years, compared to Moore\u2019s law. And this is compounded by Wirth\u2019s law [0].[0] https://en.wikipedia.org/wiki/Wirth%27s_law\n \nreply",
      "I wonder why? Some possibilities:1. Extra silicon area being used by NPUs and TPUs instead of extra performance?2. Passmark runs under Windows, which is probably using increasing overhead with newer versions?3. Fixes for speculative execution vulnerabilities?4. Most computers are now \"fast enough\" for the average user, no need to buy top end.5. Intel's new CPUs are slower than the old ones\n \nreply",
      "> 1. Extra silicon area being used by NPUs and TPUs instead of extra performance?I'm not an expert in silicon design, but I recall reading that there's a limit to how much power can be concentrated in a single package due to power density constraints, and that's why they are adding fluff instead of more cores.>2. Passmark runs under Windows, which is probably using increasing overhead with newer versions?This is a huge problem as well. I have a 2017 15\" MacBook Pro and it's a decent computer, except it is horribly sluggish doing anything on it, even opening Finder. That is with a fresh install of macOS 13.7.1. If I install 10.13, it is snappy. If I bootcamp with Windows 11, it's actually quite snappy as well and I get 2 more hours of battery life somehow despite Windows in bootcamp not being able to control (power down) the dGPU like macOS can. Unfortunately I hate Windows, and Linux is very dodgy on this Mac.\n \nreply"
    ],
    "link": "https://www.cpubenchmark.net/year-on-year.html",
    "first_paragraph": ""
  },
  {
    "title": "The Prophet of Parking: A eulogy for the great Donald Shoup (worksinprogress.news)",
    "points": 182,
    "submitter": "herbertl",
    "submit_time": "2025-02-12T16:31:04 1739377864",
    "num_comments": 207,
    "comments_url": "https://news.ycombinator.com/item?id=43026920",
    "comments": [
      "Oregon eliminated burdensome parking regulations in most larger cities and: it's fine.Many home builders still add parking to new projects because there is market demand for it - and they are also competing for tenants or buyers against existing housing which has parking.But there is now the flexibility to do some projects without parking, which really helps at the affordable end of the spectrum, and is a good fit for more walkable locations.BTW, Nolan Gray, cited as the author, has a book out himself that's really approachable and good reading if you're interested in cities: https://islandpress.org/books/arbitrary-lines\n \nreply",
      "There's gotta be some middle ground.  I think of san francisco, where the streets are clogged with people circling the block and folks are double parked everywhere.\n \nreply",
      "The solution, as Donald Shoup advocated, is to raise (or in some cases, lower, and in general, have it be dynamic) parking rates to market-clearing prices for parking spots such that there are is always one (but not too much more) free spot available on the block.\n \nreply",
      "That's a necessity, but I'd also add legalizing construction of dedicated parking structures in more places. Land is at a premium in any desirable place and street parking is a lot less efficient usage of that than a multi-level parking structure. As a driver I also prefer them. Circling around blocks is a waste of time and annoying and my car is safer in a dedicated building that typically has some cameras\n \nreply",
      "This is not really because parking lots are not legalized (they are, and in fact are often required) but because structured parking is so expensive to build; $25-35k per space in an above ground garage and $35-50k per space underground. https://dcplm.com/blog/cost-of-building-a-parking-garage/The only place I'm aware of that bans new parking garages in the US is Manhattan, which has a general parking cap; but providing enough spaces for the actual car travel demand to Manhattan would necessitate leveling the whole island and then some, so the policy is there to get people to stop driving to a narrow, congested island.\n \nreply",
      "I am pretty sure that most areas here in Portland affected by the shortage of parking aren't zoned to allow building a parking structure. I haven't looked this up though. Edit: It's not downtown here that has this problem, but smaller, hip neighborhoods where you have a shopping street surrounded by an ocean of SFHsIf high cost is the issue, it seems like people are complaining about lack of parking but don't want it enough to pay the real, unsubsidized price. So why should everyone else pay for them?\n \nreply",
      "> $25-35k per spaceSeems unreasonably high.\n \nreply",
      "my solution is to look both ways before flipping the windshield wipers up on a double parked villain\n \nreply",
      "How does this benefit poor people who could barely afford their homes and can barely afford to commute to their job halfway across the city by car?If applied to an area that already is only middle-class people, then sure.Or resident parking permits.\n \nreply",
      "The solution to poor people not having enough money is to give them more money (if you really want to help them).It's not to make random consumer goods like parking free for all. If you do this, most of the goods will be used by people who are not poor, so it's very inefficient at helping you achieve your goal of helping poor people.In addition, many poor people won't want the thing you are making free. In the case of parking that could be because they don't own a car, so this plan doesn't help a portion of the population you are trying to help. Even more inefficient!When people think we should have free parking to help the poor, it's mostly just status quo bias at work. Most people would never say that we should make bread free. Or that we should make milk free. Parking isn't any different.\n \nreply"
    ],
    "link": "https://www.worksinprogress.news/p/the-prophet-of-parking",
    "first_paragraph": ""
  },
  {
    "title": "Record-breaking neutrino is most energetic ever detected (nature.com)",
    "points": 158,
    "submitter": "lnauta",
    "submit_time": "2025-02-12T16:52:57 1739379177",
    "num_comments": 89,
    "comments_url": "https://news.ycombinator.com/item?id=43027150",
    "comments": [
      "Ars Technica has an article, as well, with some additional context/explanation.https://arstechnica.com/science/2025/02/most-energetic-neutr...And an interesting, somewhat related, video from PBS Space Time exploring how supernovas act as particle accelerators (but don't quite explain particles like this one or the 'Oh My God' particle):https://www.youtube.com/watch?v=2sSNWIJbV3Q\n \nreply",
      "Neutrinos \u201cinteract with regular matter so rarely that it's estimated you'd need about a light-year of lead to completely block a bright source of them. Every one of us has tens of trillions of neutrinos passing through us every second, but fewer than five of them actually interact with the matter in our bodies in our entire lifetimes.\u201dThey have 1/500,000 the mass of electrons. They interact only through the super short range weak force (and gravity). Nearly 5% of fission energy is expressed in neutrinos.And, they may be their own antiparticles, meaning they can potentially annihilate each other.Wild that these things can carry so much energy!\n \nreply",
      ">Every one of us has tens of trillions of neutrinos passing through us every second, but fewer than five of them actually interact with the matter in our bodies in our entire lifetimes.\u201dOh, but those five...\n \nreply",
      "Next time I trip over nothing I am blaming those \"darn neutrinos messing up my knee!\"\n \nreply",
      "The article references the \"Oh-My-God particle, the most energetic particle yet encountered. Here's the late John Walker's excellent piece on that:https://fourmilab.ch/documents/OhMyGodParticle/\n \nreply",
      "> These calculations involve some elementary but easy to mess up algebra and some very demanding numerical calculations for which regular IEEE double precision is insufficient. If you'd like to double-check these results, be sure to use a multiple precision calculator with at least 30 significant digits of accuracy.So you're saying my iPhone built-in calculator app is going to have problems....?Time to whip out dc on the terminal.\n \nreply",
      "> So you're saying my iPhone built-in calculator app is going to have problems....?Your Android phone's built-in calculator app, however, will not. :^)https://dl.acm.org/doi/pdf/10.1145/3385412.3386037\n \nreply",
      "https://www.wolframalpha.com/\n \nreply",
      "Or ChatGPT to output Julia code\n \nreply",
      "Which defaults to float64, which is IEEE precision, exactly what the article warns about\u2026\u2026I mean, seriously?\n \nreply"
    ],
    "link": "https://www.nature.com/articles/d41586-025-00444-1",
    "first_paragraph": "Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\n            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\n            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\n            and JavaScript.AdvertisementYou can also search for this author in PubMed\n\u00a0Google Scholar\nEngineers prepare to add a KM3NeT module to the network of sea-floor detectors. Credit: Paschal Coyle/CNRSAstrophysicists have observed the most energetic neutrino ever. The particle \u2014 which probably came from a distant galaxy \u2014 was spotted by the Cubic Kilometre Neutrino Telescope (KM3NeT), a collection of light-detecting glass spheres on the floor of the Mediterranean Sea, on 13 February 2023. Researchers monitoring the telescope did not notice the detection until early 2024, when they completed the first analysis of their data. They unveiled it as"
  },
  {
    "title": "5G networks meet consumer needs as mobile data growth slows (ieee.org)",
    "points": 151,
    "submitter": "saigovardhan",
    "submit_time": "2025-02-12T17:04:22 1739379862",
    "num_comments": 208,
    "comments_url": "https://news.ycombinator.com/item?id=43027266",
    "comments": [
      "OK so someone has noticed that you \"only\" need 15MBs-1 for a 4K stream.  Jolly good.Now let's address latency - that entire article only mentioned the word once and it looks like it was by accident.Latency isn't cool but it is why your Teams/Zoom/whatevs call sounds a bit odd.  You are probably used to the weird over-talking episodes you get when a sound stream goes somewhat async.  You put up with it but you really should not, with modern gear and connectivity.A decent quality sound stream consumes roughly 256KBs-1 (yes: 1/4 megabyte per second - not much) but if latency strays away from around 30ms, you'll notice it and when it becomes about a second it will really get on your nerves.  To be honest, half a second is quite annoying.I can easily measure path latency to a random external system with ping to get a base measurement for my internet connection and here it is about 10ms to Quad9.  I am on wifi and my connection goes through two switches and a router and a DSL FTTC modem.  That leaves at least 20ms (which is luxury) for processing.\n \nreply",
      "There is no way that \"15MBs-1\" is more clear than \"15MB/s\", especially in an environment which lacks the ability to give text superscript styling.\n \nreply",
      "> Could maximum data speeds\u2014on mobile devices, at home, at work\u2014be approaching \u201cfast enough\u201d for most people for most purposes?That seems to be the theme across all consumer electronics as well. For an average person mid phones are good enough, bargain bin laptops are good enough, almost any TV you can buy today is good enough. People may of course desire higher quality and specific segments will have higher needs, but things being enough may be a problem for tech and infra companies in the next decade.\n \nreply",
      "The answer is yes. But it's not just about speed. The higher speeds drain the battery faster.I say this because we currently use an old 2014 phone as a house phone for the family. It's set to 2G to take calls, and switches to 4g for the actual voice call. We only have to charge it once every 2-3 weeks, if not longer. (Old Samsung phones also had Ultra Power Saving mode which helps with that)2G is being shutdown though. Once that happens and it's forced into 4G all the time, we'll have to charge it more often. And that sucks. There isn't a single new phone on the market that lasts as long as this old phone with an old battery.The same principle is why I have my modern personal phone set to 4G instead of 5G. The energy savings are very noticeable.\n \nreply",
      "Given that it's a house phone, have you tried setting the phone up to use Wi-Fi Calling (VoWiFi), and then putting the phone in Airplane Mode with wi-fi re-enabled?\n \nreply",
      "I actually miss the concept of house phones. Instead of exclusively person-to-person communication, families would call other families to catch up and sometimes even pass the phone around to talk to the grandparents, aunts and uncles, etc.\n \nreply",
      "That's a facinating obsevation! I hadn't consider the side effects of calling a common phone and interacting with other people rather than exclusively the one person you wanted to talk/text with. Probably distances you from (or never allows you to know) those adjacent to the person you already know.\n \nreply",
      "My siblings are much older than I am and when I was 10 my sister was starting her relationship with now-husband. One time he called our house phone, I picked up, and said \"yeah sister is home but she's busy at the moment so you can spend five minutes talking to me instead\"\n \nreply",
      "There are some more traditional home phones which work on 4/5G networks with a DECT handset which talks to a cellular base station. You might look into switching to that model to replace your \"cell phone as a home phone\" concept. It makes it a bit easier to add another handset to the DECT network and often means convenient cradles to charge the handsets while the base station stays in a good signal spot with plenty of power.Just a thought when it comes time to change out that device.\n \nreply",
      "That's not a bad idea, thank you! I always hate having to retire perfectly good working hardware just because a spec requirement change.\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/5g-bandwidth",
    "first_paragraph": "It\u2019s not more bandwidth that users needIs the worldwide race to keep expanding mobile bandwidth a fool\u2019s errand? Could maximum data speeds\u2014on mobile devices, at home, at work\u2014be approaching \u201cfast enough\u201d for most people for most purposes?\n\n\tThese heretical questions are worth asking, because industry bandwidth tracking data has lately been revealing something surprising: Terrestrial and mobile-data growth is slowing down. In fact, absent a dramatic change in consumer tech and broadband usage patterns, data-rate demand appears set to top out below 1 billion bits per second (1 gigabit per second) in just a few years.\n\n\tThis is a big deal. A presumption of endless growth in wireless and terrestrial broadband data rates has for decades been a key driver behind telecom research funding. To keep telecom\u2019s R&D engine rooms revving, research teams around the world have innovated a seemingly endless succession of technologies to expand bandwidth rates, such as 2G\u2019s \n\tmove to digital cell networ"
  },
  {
    "title": "Smuggling arbitrary data through an emoji (paulbutler.org)",
    "points": 411,
    "submitter": "paulgb",
    "submit_time": "2025-02-12T09:24:08 1739352248",
    "num_comments": 137,
    "comments_url": "https://news.ycombinator.com/item?id=43023508",
    "comments": [
      "This is cute but unnecessary - Unicode includes a massive range called PUA: the private use area. The codes in this range aren\u2019t mapped to anything (and won\u2019t be mapped to anything) and are for internal/custom use, not to be passed to external systems (for example, we use them in fish-shell to safely parse tokens into a string, turning an unescaped special character into just another Unicode code point in the string, but in the PUA area, then intercept that later in the pipeline).You\u2019re not supposed to expose them outside your api boundary but when you encounter them you are prescribed to pass them through as-is, and that\u2019s what most systems and libraries do. It\u2019s a clear potential exfiltration avenue, but given that most sane developers don\u2019t know much more about Unicode other than \u201calways use Unicode to avoid internationalization issues\u201d, it\u2019s often left wide open.\n \nreply",
      "I just tested and private use characters render as boxes for me (\ue000\ue001\udb80\udc00), the point here was to encode them in a way that they are hidden and treated as \"part of\" another character when copy/pasting.\n \nreply",
      "> the point here was to encode them in a way that they are hidden and treated as \"part of\" another character when copy/pastingAKA \"Steganography\" for the curious ones: https://en.wikipedia.org/wiki/Steganography\n \nreply",
      "Like when we used to encode the phone numbers of warez boards in GIFs.\n \nreply",
      "The difference is that PUA characters are usually rendered in some way that is rather visible, whereas the variation selectors aren\u2019t.\n \nreply",
      "Isn't this more what the designated noncharacters are for, rather than the private-use area?  Given how the private-use area sometimes gets for unofficial encodings of scripts not currently in Unicode (or for things like the Apple logo and such) I'd be worried about running into collisions with that if I used the PUA in such a way.Note that designated noncharacters includes not only 0xFFFF and 0xFFFE, and not only the final two code points of every plane, but also an area in the middle of Arabic Presentation Forms that was at some point added to the list of noncharacters specifically so that there would be more noncharacters for people using them this way!\n \nreply",
      "I'll be h\udb40\udd3e\udb40\udd5f\udb40\udd60\udb40\udd55\udb40\udd1c\udb40\udd10\udb40\udd5e\udb40\udd5f\udb40\udd10\udb40\udd63\udb40\udd55\udb40\udd53\udb40\udd62\udb40\udd55\udb40\udd64\udb40\udd63\udb40\udd10\udb40\udd58\udb40\udd55\udb40\udd62\udb40\udd55onest, I pasted this comment in the provided decoder thinking no one could miss the point this badly and there was probably a hidden message inside it, but either you really did or this website is stripping them.You can't invisibly watermark an arbitrary character (I did it to one above! If this website isn't stripping them, try it out in the provided decoder and you'll see) with unrecognized PUA characters, because it won't treat them as combining characters. You will cause separately rendered rendered placeholder-box characters to appear. Like this one: \ue000 (may not be a placeholder-box if you're privately-using the private use area yourself).\n \nreply",
      "Oh this is just the tip of the iceberg when it comes to abusing Unicode!  You can use a similar technique to this to overflow the buffer on loads of systems that accept Unicode strings.  Normally it just produces an error and/or a crash but sometimes you get lucky and it'll do all sorts of fun things! :)I remember doing penetration testing waaaaaay back in the day (before Python 3 existed) and using mere diacritics to turn a single character into many bytes that would then overflow the buffer of a back-end web server.  This only ever caused it to crash (and usually auto-restart) but I could definitely see how this could be used to exploit certain systems/software with enough fiddling.\n \nreply",
      "Yeah. Zalgo text is a common test for input fields on websites.  But it usually doesn't do anything interesting.  Maybe an exception trigger on some database length limit.  Doesn't typically even kill any processes.  The exception is normally just in your thread.  You can often trigger it just by disabling JS on even modern forms, but,, at best you're maybe leaking a bit of info if they left debug on and print the stack trace or a query.\nAnother common slip-up is failing to count \\n vs \\r\\n in text strings since JS usually usually counts a carriage return as 1 byte, but HTTP spec requires two.unescape(encodeURIComponent(\"\u00e7\")).length  is the quick and dirty way to do a JS byte length check.  The \\r\\n thing can be done just by cleaning up the string before length counting.\n \nreply",
      "For a real-world use case: Sanity used this trick[0] to encode Content Source Maps[1] into the actual text served on a webpage when it is in \"preview mode\". This allows an editor to easily trace some piece of content back to a potentially deep content structure just by clicking on the text/content in question.It has it's drawbacks/limitations - eg you want to prevent adding it for things that needs to be parsed/used verbatim, like date/timestamps, urls, \"ids\" etc - but it's still a pretty fun trick.[0] https://www.sanity.io/docs/stega[1] https://github.com/sanity-io/content-source-maps\n \nreply"
    ],
    "link": "https://paulbutler.org/2025/smuggling-arbitrary-data-through-an-emoji/",
    "first_paragraph": "Paul Butler \u2013 February 8, 2025This Hacker News comment by GuB-42 intrigued me:With ZWJ (Zero Width Joiner) sequences you could in theory encode an unlimited amount of data in a single emoji.Is it really possible to encode arbitrary data in a single emoji?tl;dr: yes, although I found an approach without ZWJ. In fact, you can encode data in any unicode character. This sentence has a hidden message\udb40\udd5f\udb40\udd58\udb40\udd10\udb40\udd5d\udb40\udd69\udb40\udd1c\udb40\udd10\udb40\udd69\udb40\udd5f\udb40\udd65\udb40\udd10\udb40\udd56\udb40\udd5f\udb40\udd65\udb40\udd5e\udb40\udd54\udb40\udd10\udb40\udd64\udb40\udd58\udb40\udd55\udb40\udd10\udb40\udd58\udb40\udd59\udb40\udd54\udb40\udd54\udb40\udd55\udb40\udd5e\udb40\udd10\udb40\udd5d\udb40\udd55\udb40\udd63\udb40\udd63\udb40\udd51\udb40\udd57\udb40\udd55\udb40\udd10\udb40\udd59\udb40\udd5e\udb40\udd10\udb40\udd64\udb40\udd58\udb40\udd55\udb40\udd10\udb40\udd64\udb40\udd55\udb40\udd68\udb40\udd64\udb40\udd11. (Try pasting it into this decoder)Unicode represents text as a sequence of codepoints, each of which is basically just a number that the Unicode Consortium has assigned meaning to.\nUsually, a specific codepoint is written as U+XXXX, where XXXX is a number represented as uppercase hexadecimal.For simple latin-alphabet text, there is a one-to-one mapping between Unicode codepoints and characters that appear on-screen. For example,\nU+0067 represents the character g.For other writing systems, some on-screen characters may be represented by multiple c"
  },
  {
    "title": "Show HN: Game Bub \u2013 open-source FPGA retro emulation handheld (lipsitz.net)",
    "points": 168,
    "submitter": "elipsitz",
    "submit_time": "2025-02-12T17:11:25 1739380285",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=43027335",
    "comments": [
      "Sadly that display controller MISO thing is notorious. I first ran into it a few years ago. The recommendation was to use a tristate buffer on the chip select line or, as you did, separate the buses.I also run into the power domain issue a lot. I didn't see a graphic about it in the article but essentially in the majority of devices the IO is like this:Vdd|Esd diode|I/O pin|Esd diode|GroundWhere the diodes are pointing towards. That way if the line goes too negative the lower one will conduct and clamp it to a diode drop + ground, same as if it goes too high to a diode drop + Vdd. The problem is if Vdd has a low impedance path to ground. This is common with power supply ICs either with output discharge resistors or transistors allowing current through to ground. When that happens, your io pin now has a diode in parallel to ground. If you're not careful and don't have current limiting resistors in the way I2C does (because it needs the pull ups), suddenly you're putting the max current your driver can deliver through that diode. Doesn't take much for the smoke to escape.Some IO pins are designed to be fine without VDD. You can usually check the absolute max ratings for someone like \"VDD + 0.3\" vs \"3.6\"\n \nreply",
      "Thank you for the nice project and write-up! I love stuff like this!I visited the comments earlier and was discouraged to find that most of them were of the flavor of, \"why does this exist?\" From people who have obviously never attempted 1% of an audacious project like this. It exists because it's cool sure looked to be a fun cross-domain learning experience, like what else do you need?BTW, I have submitted to Hack-A-Day's tip line so don't be surprised in there's an article there about it in a few days.\n \nreply",
      "I have an Analogue Pocket, which works really well, but the fact that it uses an FPGA is essentially meaningless to me. Is there really any significant difference compared to software emulation?I know how they're different, I understand the mechanics of it, I just don't understand why it's not possible to make a software emulation that is as good as FPGA emulation. Or maybe it is possible. I always felt like doing it in software would be a lot more flexible.\n \nreply",
      "I agree, and wrote a rant about this as part of the post: https://eli.lipsitz.net/posts/introducing-gamebub/#a-brief-r...IMO: the only real advantage is that it allows you to meet the precise timing needed to interface with physical hardware, like cartridges and other consoles (with link cables).I think they're also really fun to write, because you think more like a hardware designer than a software engineer.\n \nreply",
      "Am I correct in thinking that FPGAs have an advantage over software emulators when it comes to latency? In particular the latency from the host operating system before input has even been received by the software emulator? I.e. FPGAs don't have a host OS and will have the same latency as original hardware?This is very much a genuine question, I just want to know if my intuition about this is right or wrong.Your project looks amazing btw!\n \nreply",
      "Yeah, that's another advantage. Theoretically you can get down to <1 frame of input latency with an FPGA. I haven't found the latency on a software GBA emulator running on a computer to be noticeable, but some people might find this to be another advantage.I think with a dedicated emulation handheld (non-FPGA), you could probably pull some tricks to bring down the latency though.\n \nreply",
      "Consider that you can emulate an FPGA on a CPU, just much slower.\n \nreply",
      "Fantastic project and great writeup! The screen tradeoff with needing triple buffering but getting integer scaling was interesting to hear about - any feeling as to whether it adds human-noticeable latency vs. original hardware?\n \nreply",
      "Great question!In the absolute worst case (drawing an object at the very top of the screen, and the LCD output for the next frame started right before the current one finished), buffering adds a 2 frame delay (33 milliseconds). Probably noticeable for some people, but this worst case is uncommon.Average case I would expect ~0.5 to 1 frame delay, so 8 to 16 milliseconds. Probably not really noticeable.\n \nreply",
      "What is the total cost for a pcb populated with components? Probably around 60-70 qty 100?I appreciate the blog post and the writeup, it might be nice to include it in the repo.I have been toying with a similar design, with many of the same choices. Although for the system controller pair, I'd go with RP2350B and ESP32-C61 (I think). It would be nice if there was an optional chip and pad layout to support legacy or classic BT.Another option would be to have a USB port and support something like https://www.8bitdo.com/usb-wireless-adapter-2/ to enable legacy controllers.It looks like supporting legacy BT while noble, could be a project killing sidequest (if you didn't already have it done!) Another out, would be exposing an SPI connection internally so someone could hack in a controller of their choice.\n \nreply"
    ],
    "link": "https://eli.lipsitz.net/posts/introducing-gamebub/",
    "first_paragraph": "I\u2019m excited to announce the project I\u2019ve been working on for the last year and a half: Game Bub, an open-source FPGA based retro emulation handheld, with support for Game Boy, Game Boy Color, and Game Boy Advance games.Game Bub can play physical cartridges, as well as emulated cartridges using ROM files loaded from a microSD card. Game Bub also supports the Game Link Cable in both GB and GBA modes for multiplayer games. I designed the hardware with a number of bonus features, like video out (HDMI) via a custom dock, a rumble motor, real-time clock (for certain games). Additionally, the hardware is designed with extensibility in mind, allowing future software improvements to expand its capabilities.Game Bub has a custom-designed 6 layer PCB featuring a Xilinx XC7A100T FPGA with integrated memory,  display, speakers, rechargable battery, GB/GBA cartridge slot, all packaged up in a custom 3D-printed enclosure.This writeup is a detailed description of how I developed and built Game Bub. I "
  },
  {
    "title": "Imapsync (lamiral.info)",
    "points": 78,
    "submitter": "mooreds",
    "submit_time": "2025-02-12T18:54:35 1739386475",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=43028468",
    "comments": [
      "There\u2019s another great alternative called \u201cimap-backup\u201d [0], which I use to backup my mail accounts.It can sync, incrementally backup and/or restore e-mail accounts.It runs great as a container, does its job and exits. If anyone wants an ARM version, I maintain a container at [1].[0]: https://github.com/joeyates/imap-backup[1]: https://hub.docker.com/r/bayindirh/imap-backup\n \nreply",
      "If it's of interest, I have an automated container of it that builds whenever any of imapbackup's dependencies, or the base ruby:alpine image, are updated.https://github.com/cmsj/imap-backup\n \nreply",
      "HN discussion from 2022 (80 comments): https://news.ycombinator.com/item?id=29849762\n \nreply",
      "That website is delicious! Somehow it gives me more trust than if it were some overburdened react/bootcampDidn't know that tool but looks useful, thanks!\n \nreply",
      "Used this around 5 years ago to migrate a mid-sized mailserver, absolutely magnificient.I remember that back then you had to pay for the binaries, but could download the source code for free - you just had to figure out how to compile it.The page mentions Perl source code though, so I'm not sure if I'm mixing this up with another tool...\n \nreply",
      "I think the author packages an all-dependencies-included executable for windows that you're given access when you pay for support and all that, but it's in fact perl.I remember using this program a bunch of times during the years (and I think I've paid at least once) and remember using CPAN when configuring it.It has helped me a bunch of times!\n \nreply",
      "bought this a few years back to handle some cpanel imap <-> Gmail sync. Simply gets the job done. No nonsense tool that makes do without overhyped promo and flashy website, a rare breed these days. Highly recommended.\n \nreply",
      "Last I checked this was easiest way to migrate email accounts between Microsoft365 tenants. Somehow this is not a Microsoft feature.I have used imapsync quite a few times. Such a great tool.\n \nreply",
      "Microsoft has deprecated \u201clegacy\u201d auth methods, which makes this sort of thing very difficult to accomplish nowadays.I used to use IMAP/SMTP for Outlook Mail but they force you to use proprietary clients now, forcing OAuth. It makes scripting nearly impossible.https://learn.microsoft.com/en-us/exchange/clients-and-mobil...\n \nreply",
      "An alternative that works relatively well is to use thunderbird to synchronize all emails from the account to migrate, then drag them towards the new account.\n \nreply"
    ],
    "link": "https://imapsync.lamiral.info/",
    "first_paragraph": "\n\nImapsync presentation by Gilles\n\nImapsync is an IMAP transfer tool.\nThe purpose of imapsync is to migrate IMAP accounts or to backup IMAP accounts. \nIMAP, IMAP4 in fact (started December 1994), is one of the three current standard protocols used to access mailboxes,\nthe two other being POP3 (started November 1988) and HTTP (started May 1996) with webmails, \nwebmails are often tied to an IMAP server.\nThe latest imapsync published release 2.290 \nwas written on Tuesday, 20-Aug-2024 10:39:22 UTC\nImapsync is a command-line tool that allows incremental and\nrecursive IMAP transfers from one mailbox to another, both anywhere on the internet\nor in your local network.  Imapsync runs on Windows, Linux, Mac OS X.\n\n\"Incremental\" means you can stop the transfer at any time\nand restart it later efficiently, without generating duplicates.\n\n\"Recursive\" means the complete folders hierarchy can be copied, all folders, all subfolders etc.\n\n\"Command-line\" means it's not a graphical tool, on Windows you h"
  },
  {
    "title": "Is NixOS truly reproducible? (luj.fr)",
    "points": 104,
    "submitter": "pabs3",
    "submit_time": "2025-02-09T09:56:13 1739094973",
    "num_comments": 51,
    "comments_url": "https://news.ycombinator.com/item?id=42989666",
    "comments": [
      "> Our most important finding is that the reproducibility rate in nixpkgs has increased steadily from 69% in 2017 to about 91% in April 2023. The high reproducibility rate in our most recent revision is quite impressive, given both the size of the package set and the absence of systematic monitoring in nixpkgs. We knew that it was possible to achieve very good reproducibility rate in smaller package sets like Debian, but this shows that achieving very high bitwise reproducibility is possible at scale, something that was believed impossible by practitioners4I think people in this thread are focusing on the wrong thing. Sure, not all packages are reproducible, but the project is systematically increasing the percentage of projects that are reproducible while ALSO adding new projects and demonstrating conclusively that what was considered infeasible is actually readily achievable.> The interesting aspect of these causes is that they show that even if nixpkgs already achieves great reproducibility rates, there still exists some low hanging fruits towards improving reproducibility that could be tackled by the Nix community and the whole FOSS ecosystem.This work is helpful I think for the community to tackle the sources of unreproducible builds to push the percentage up even further. I think it also highlights the need for automation to validate that there aren't systematic regressions or regressions in particularly popular packages (doing individual regressions for all packages is a futile effort unless a lot of people volunteer to be part of a distributed check effort).\n \nreply",
      "I think this debate comes down to exactly what \"reproducible\" means. Nix doesn't give bit-exact reproducibility, but it does give reproducible environments, by ensuring that the inputs are always bit-exact. It is closer to being fully reproducible than most other build systems (including Bazel) -- but because it can only reasonably ensure that the inputs are exact, it's still necessary for the build processes themselves to be fully deterministic to get end-to-end bit-exactness.Nix on its own doesn't fully resolve supply chain concerns about binaries, but it can provide answers to a myriad of other problems. I think most people like Nix reproducibility, and it is marketed as such, for the sake of development: life is much easier when you know for sure you have the exact same version of each dependency, in the exact same configuration. A build on one machine may not be bit-exact to a build on another machine, but it will be exactly the same source code all the way down.The quest to get every build process to be deterministic is definitely a bigger problem and it will never be solved for all of Nixpkgs. NixOS does have a reproducibility project[1], and some non-trivial amount of NixOS actually is properly reproducible, but the observation that Nixpkgs is too vast is definitely spot-on, especially because in most cases the real issues lie upstream. (and carrying patches for reproducibility is possible, but it adds even more maintainer burden.)[1]: https://reproducible.nixos.org/\n \nreply",
      "> The quest to get every build process to be deterministic [...] will never be solved for all of Nixpkgs.Not least because of unfree and/or binary-blob packages that can't be reproducible because they don't even build anything. As much as Guix' strict FOSS and build-from-source policy can be an annoyance, it is a necessary precondition to achieve full reproducibility from source, i.e. the full-source bootstrap.\n \nreply",
      "you can slap a hash on a binary distribution and it becomes \"reproducible\" in the same trivial sense as any source tarball. after that, the reproducibility of whatever \"build process\" takes place to extract archives and shuffle assets around is no more or less fraught than any other package (probably less considering how much compilers have historically had to be brought to heel, especially before reproducibility was fashionable enough for it to enter much into compiler authors' consideration!!)\n \nreply",
      "Nixpkgs provides license[1] and source provenance[2] information. For legal reasons, Nix also defaults to not evaluating unfree packages. Not packaging them at all, though, doesn't seem useful from any technical standpoint; I think that is purely ideological.In any case, it's all a bit imperfect anyway, since it's from the perspective of the package manager, which can't be absolutely sure there's no blobs. Anyone who follows Linux-libre releases can see how hard it really is to find all of those needles in the haystack. (And yeah, it would be fantastic if we could have machines with zero unfree code and no blobs, but the majority of computers sold today can't meaningfully operate like that.)I actually believe there's plenty of value in the builds still being reproducible even when blobs are present: you can still verify that the supply chain is not compromised outside of the blobs. For practical reasons, most users will need to stick to limiting the amount of blobs rather than fully eliminating them.[1]: https://nixos.org/manual/nixpkgs/stable/#sec-meta-license[2]: https://nixos.org/manual/nixpkgs/stable/#sec-meta-sourceProv...\n \nreply",
      "> It is closer to being fully reproducible than most other build systems (including Bazel).How so? Bazel produces the same results for the same inputs.\n \nreply",
      "Bazel doesn't guarantee bit-exact outputs, but also Bazel doesn't guarantee pure builds. It does have a sandbox that prevents some impurities, but for example it doesn't prevent things from going out to the network, or even accessing files from anywhere in the filesystem, if you use absolute paths. (Although, on Linux at least, Bazel does prevent you from modifying files outside of the sandbox directory.)The Nix sandbox does completely obscure the host filesystem and limit network access to processes that can produce a bit-exact output only.(Bazel also obviously uses the system compilers and headers. Nix does not.)\n \nreply",
      "Uh, Either my understanding of Bazel is wrong, or everything you wrote is wrong.Bazel absolutely prevents network access and filesystem access (reads) from builds. (only permitting explicit network includes from the WORKSPACE file, and access to files explicitly depended on in the BUILD files).Maybe you can write some \u201crules_\u201d for languages that violate this, but it is designed purposely to be hermetic and bit-perfect reproducible.EDIT:From the FAQ[0]:> Will Bazel make my builds reproducible automatically?> For Java and C++ binaries, yes, assuming you do not change the toolchain.The issues with Docker's style of \"reproducible\" (meaning.. consistent environment; are also outlined in the same FAQ[1]> Doesn\u2019t Docker solve the reproducibility problems?> Docker does not address reproducibility with regard to changes in the source code. Running Make with an imperfectly written Makefile inside a Docker container can still yield unpredictable results.[0]: https://bazel.build/about/faq#will_bazel_make_my_builds_repr...[1]: https://bazel.build/about/faq#doesn\u2019t_docker_solve_the_repro...\n \nreply",
      "I believe your understanding of Bazel is wrong. I don't see any documentation that suggests the Bazel sandbox prevents the toolchain from accessing the network.https://bazel.build/docs/sandboxing(Actually, it can: that documentation suggests it's optionally supported, at least on the Linux sandbox. That said, it's optional. There's definitely actions that use the network on purpose and can't participate in this.)This may seem pointless, because in many situations this would only matter in somewhat convoluted cases. In C++ the toolchain probably won't connect to the network. This isn't the case for e.g. Rust, where proc macros can access the network. (In practical terms, I believe the sqlx crate does this, connecting to a local Postgres instance to do type inference.) Likewise, you could do an absolute file inclusion, but that would be very much on purpose and not an accident. So it's reasonable to say that you get a level of reproducibility when you use Bazel for C++ builds...Kind of. It's not bit-for-bit because it uses the system toolchain, which is just an arbitrary choice. On Darwin it's even more annoying: with XCode installed via Mac App Store, the XCode version can change transparently under Bazel in the background, entirely breaking the hermeticity, and require you to purge the Bazel cache (because the dependency graph will be wrong and break the build. Usually.)Nix is different. The toolchain is built by Nix and undergoes the same sandboxed build process with sandboxing and cryptographically verified inputs. Bazel does not do that.\n \nreply",
      "It does.There are mechanisms for opting out/breaking that, just as with Nix or any other system.> macOSWhat does nix do on these systems?\n \nreply"
    ],
    "link": "https://luj.fr/blog/is-nixos-truly-reproducible.html",
    "first_paragraph": "Build reproducibility is often considered as a de facto feature provided by functional package managers like Nix. Although the functional package manager model has important assets in the quest for build reproducibility (like reproducibility of build environments for example1), it is clear among practitioners that Nix does not guarantee that all its builds achieve bitwise reproducibility. In fact, it is not complicated to write a Nix package that builds an artifact non-deterministically:Despite this, build reproducibility has historically been used as a marketing argument by the NixOS community, with the catchphrase \u201cReproducible builds and deployments\u201d appearing as a headline of the nixos.org page until 20232. This situation has even occasionally created tensions with members of the reproducible-builds group who dedicate a lot of time contributing patches in compilers and downstream projects to make them bitwise reproducible for everyone, and prompted blog posts such as \u201cNixOS is not "
  },
  {
    "title": "Show HN: yknotify \u2013 Notify when YubiKey needs touch on macOS (github.com/noperator)",
    "points": 34,
    "submitter": "noperator",
    "submit_time": "2025-02-12T20:24:59 1739391899",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=43029385",
    "comments": [
      "Interesting. I haven\u201a\u00c4\u00f4t had this problem, because every time I need to touch the yubikey macOS pops a giant fucking dialog that requires 2-3 clicks before it lets me tap it.\n \nreply",
      "Interesting project, but I can't say I've ever wondered whether or not it's time to touch my Yubikey. Authenticating with a token is a very intentful process.\n \nreply",
      "I've found that when I'm cloning submoduled private repos via YK-backed SSH key, I'll need to touch multiple times but there's not always text in the terminal notifying me to do so. Easy to miss the small flashing green light.\n \nreply",
      "Is it possible to add it to ssh-agent once?\n \nreply",
      "I have because I use it for a ton of stuff. Password manager, sudo locally, ssh logins, sudo remotely, openpgp decrypt etc.It happens sometimes that I forget that's what it's waiting for. I'm no longer on Mac though. I have KDE. I don't always see the key flashing either because sometimes it's buried under the mess on my desk (I know...)It's a bit annoying that yubikeys don't just trigger a hid event or something, as far as i understand the only way to tell is by looking for some obscure log entries.\n \nreply",
      "Yubikey is an event based token. You tap it with explicit intent. If you aren't expecting to tap it, then the fail safe is you don't. It works that way by design.You can't use a screwdriver handle as a hammer then complain it doesn't work to your expectations.\n \nreply",
      "I just like to be notified when I need to tap something with explicit intent.\n \nreply",
      "The concern is that if you don't know how many times you should be tapping the YubiKey when you clone a git repo, then an attacker could slip in its own signing requests and you would dutifully tap the YubiKey to authorize them.  If you do know how many times to tap, do you still need the notification?(It's true that if an attacker slipped in a request right before I was expecting to tap my YubiKey, I would tap it a second time to get my operation to succeed under the assumption that it didn't detect my touch the first time.  But I would become suspicious if that kept happening.)\n \nreply",
      "For Linux, see: https://github.com/maximbaz/yubikey-touch-detector\n \nreply",
      "Shouldn't you only touch your YubiKey when you've just done something that you know requires you to touch your YubiKey? Otherwise, you're just authenticating anything that asks, including the virus.\n \nreply"
    ],
    "link": "https://github.com/noperator/yknotify",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Notify when YubiKey needs touch on macOS\n      yknotify watches macOS logs (via log stream CLI command) for events that I've determined, through trial and error, are heuristically associated with the YubiKey waiting for touch. I primarily use the FIDO2 and OpenPGP features and haven't tested other applications listed in ykman info (e.g., Yubico OTP, FIDO U2F, OATH, PIV, YubiHSM Auth).When waiting for FIDO2 touch, we'll see this message logged once (with example hex value):When waiting for OpenPGP touch, we'll see this message logged repeatedly:As soon as the YubiKey is touched, we'll get a new/different log message in the same category. So the strategy here is to check if either of the above messages are the last one logged in their respective categories, and if so, notify the user to touch the YubiKey.I've seen a few rare false pos"
  },
  {
    "title": "US and UK refuse to sign AI safety declaration at summit (arstechnica.com)",
    "points": 240,
    "submitter": "miohtama",
    "submit_time": "2025-02-12T09:33:29 1739352809",
    "num_comments": 492,
    "comments_url": "https://news.ycombinator.com/item?id=43023554",
    "comments": [
      "Something tells me aspects of living in the next few decades driven by technology acceleration will feel like being lobotomized while conscious and watching oneself the whole time. Like yes, we are able to think of thousands of hypothetical ways technology (even those inferior to full AGI) could go off the rails in a catastrophic way and post and discuss these scenarios endlessly... and yet it doesn't result in a slowing or stopping of the progress leading there. All it takes is a single group with enough collective intelligence and breakthroughs and the next AI will be delivered to our doorstop whether or not we asked for it.It reminds me of the time I read books in my youth and only 20 years later realized the authors of some of those books were trying to deliver a important life messages to a teenager undergoing crucial changes, all of which would be painfully relevant to the current adult me... and yet the whole time they fell on deaf ears. Like the message was right there but I did not have the emotional/perceptive intelligence to pick up on and internalize it for too long.\n \nreply",
      "> Like yes, we are able to think of thousands of hypothetical ways technology (even those inferior to full AGI) could go off the rails in a catastrophic way and post and discuss these scenarios endlessly... and yet it doesn't result in a slowing or stopping of the progress leading there.The problem is sifting through all of the doomsayer false positives to get to any amount of cogent advice.At the invention of the printing press, there were people with this same energy. Obviously those people were wrong. And if we had taken their \"lesson\", then human society would be in a much worse place.Is this new wave of criticism about AI/AGI valid? We will only really know in retrospect.\n \nreply",
      "> Is this new wave of criticism about AI/AGI valid? We will only really know in retrospect.All of the focus on AGI is a distraction. I think it's important for a state to declare it's intent with a technology. The alternative is arguing the idea that technology advances autonomously, independent of human interactions, values, or ideas, which is, in my opinion, an incredibly na\u00efve notion. I would rather have a state say \"we won't use this technology for evil\" than a state that says nothing at all and simply allows the businesses to develop in any direction their greed leads them.It's entirely valid to critique the uses of a technology, because \"AI\" (the goalpost shifting for marketing purposes to make that name apply to chatbots is a stretch honestly) is a technology like any other, like a landmine, like a synthetic virus, etc. In the same way, it's valid to criticize an actor for purposely hiding their intentions with a technology.\n \nreply",
      "But if the state approaches a technology with intent it is usually for the purposes of a military offence. I don't think that is a good idea in the context of AI! Although I also don't think there is any stopping it. The US has things like DARPA for example and a lot of Chinese investment seems to be done with the intent of providing capabilities to their army.The list of things states have attempted to deploy offensively is nearly endless. Modern operations research arguably came out of the British empire attempting (succeeding) to weaponise mathematics. If you give a state fertiliser it makes bombs, if you give it nuclear power it makes bombs, if you give it drones it makes bombs, if you give it advanced science or engineering of any form it makes bombs. States are the most ingenious system for turning things into bombs that we've ever invented; in the grand old days of siege warfare they even managed to weaponise corpses, refuse and junk because it turned out lobbing that stuff at the enemy was effective. The entire spectrum of technology from nothing to nanotech, hurled at enemies to kill them.We'd all love if states commit to not doing evil but the state is the entity most active at figuring out how to use new tech X for evil.\n \nreply",
      "> I think it's important for a state to declare it's intent with a technology. The alternative is arguing the idea that technology advances autonomously, independent of human interactions, values, or ideasThe sleight of hand here is the implication that human interactions, values, and ideas are only expressed through the state.\n \nreply",
      "The idea is that by its very nature as an agent that attempts to make the best action to achieve a goal, assuming it can get good enough, the best action will be to improve itself so it can better achieve its goal. In fact we humans are doing the same thing, we can't really improve our intelligence directly but we are trying to create AI to achieve our goals, and there's no reason that the AI itself wouldn't do so assuming it's capable and we don't attempt to stop it, and currently we don't really know how to reliably control it.We have absolutely no idea how to specify human values in a robust way which is what we would need to figure out to build this safely\n \nreply",
      "> The idea is that by its very nature as an agent that attempts to make the best action to achieve a goal, assuming it can get good enough, the best action will be to improve itself so it can better achieve its goal.I\u2019ve heard this argument before, and I don\u2019t entirely accept it.  It presumes that AI will be capable of playing 4D chess and thinking logically 10 moves ahead.  It\u2019s an interesting plot as a SF novel (literally the plot of the movie \u201cI Robot\u201d), but neural networks just don\u2019t behave that way.  They act, like us, on instinct (or training), not in some hyper-logical fashion.  The idea that AI will behave like Star Trek\u2019s Data (or Lore), has proven to be completely wrong.\n \nreply",
      "Despite what Sam Altman (a high-school graduate) might want to be true, human cognition is not just a massive pile of intuition; there are critical deliberative and intentional aspects to cognition, which is something we've seen come to the fore with the hubbub around \"reasoning\" in LLMs. Any AGI design will necessarily take these facts into account--hardcoded or no--and will absolutely be capable of forming plans and executing them over time, as Simon & Newell described the best back in '71:  The problem solver\u2019s search for a solution is an odyssey through the problem space, from one knowledge state to another, until\u2026 [they] know the answer.\n\nWith this in mind, I really don't see any basis to attack the intelligence explosion hypothesis. I linked a Yudkowsky paper above examining how empirically feasible it might be, which is absolutely an unsolved question at some level. But the utility of the effort itself is just downright obvious, even if we didn't have reams of internet discussions like this one to nudge any nascent agent in that direction.[1] Simon & Newell, 1971: Human Problem Solving https://psycnet.apa.org/record/1971-24266-001\n \nreply",
      "I think that is missing the point.  The AI's goals are what are determined by its human masters.  Those human masters can already have nefarious and selfish goals that don't align with \"human values\".  We don't need to invent hypothetical sentient AI boogeymen turning the universe into paperclips in order to be fearful of the future that ubiquitous AI creates.  Humans would happily do that too if they get to preside over that paperclip empire.\n \nreply",
      ">  The AI's goals are what are determined by its human masters.Imagine going to a cryptography conference and saying that \"the encryption's security flaws are determined by their human masters\".Maybe some of them were put there on purpose? But not the majority of them.No, an AI's goals are determined by their programming, and that may or may not align with the intentions of their human masters. How to specify and test this remains a major open question, so it cannot simply be presumed.\n \nreply"
    ],
    "link": "https://arstechnica.com/ai/2025/02/us-and-uk-refuse-to-sign-ai-safety-declaration-at-summit/",
    "first_paragraph": "\n        US stance is \"180-degree turnaround\" from Biden administration.\n      US Vice President JD Vance has warned Europe not to adopt \u201coverly precautionary\u201d regulations on artificial intelligence as the US and the UK refused to join dozens of other countries in signing a declaration to ensure that the technology is \u201csafe, secure and trustworthy.\u201dThe two countries held back from signing the communique agreed by about 60 countries at the AI Action summit in Paris on Tuesday as Vance vowed that the US would remain the dominant force in the technology.\u201cThe Trump administration will ensure that the most powerful AI systems are built in the US, with American-designed and manufactured chips,\u201d Vance told an audience of world leaders and tech executives at the summit.\u201cAmerica wants to partner with all of you...\u2009but to create that kind of trust, we need international regulatory regimes that foster the creation of AI technology rather than strangle it.\u201dThe summit declaration calls for \u201censurin"
  },
  {
    "title": "PgAssistant: OSS tool to help devs understand and optimize PG performance (github.com/nexsol-technologies)",
    "points": 168,
    "submitter": "justinclift",
    "submit_time": "2025-02-12T15:01:40 1739372500",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=43026036",
    "comments": [
      "Neat! Just added it to my \"Postgres Is Enough\" gist:https://gist.github.com/cpursley/c8fb81fe8a7e5df038158bdfe0f...\n \nreply",
      "> OpenAI helper: If you have an OpenAI account, the interface can query OpenAI to help you understand your query plan and optimize your SQL queriesHow good are LLMs at optimizing queries? Do they just give basic advices like \"try adding an index there\" or can they do more?I guess it would need to cross the results of the explain analyze with at least the DDLs of the table (including partitions, indexes, triggers, etc), and like the sizes of the tables and the rate of reads/writes of the tables in some cases, to be able to \"understand\" and \"reason\" about what's going on and offer meaningful advices (e.g. proposing design changes for the tables).I don't see how a generic LLM would be able to do that, but maybe another type of model trained for this task and with access to all the information it needs?\n \nreply",
      "In my experience, LLMs are a lot better than my backend engineers who don't even try but not that much better than someone who's skimmed the docs.Typically they suggest 1) adding indexes and 2) refactoring the query. If you only provide the query then the model isn't aware of what indexes already exist. LLMs make assumptions about your data model that often don't hold (i.e. 3NF). Sometimes you have to give the LLM the latest docs because it's unaware of performance improvements and features for newer versions.In my view, it's better to look at the query plan and think yourself about how to improve the query but I also recognize that most people writing queries aren't going to do that.There are other tools (RIP OtterTune) that can tune database parameters. Not something I really see any generative model helping with.\n \nreply",
      "They can be useful. I just recently used Claude to make a query 10x faster, though I did have to back-and-forth a bit, so you do still need to know what you're doing.\n \nreply",
      "Claude has been really good at all things Postgres related\n \nreply",
      "They\u2019re non-deterministic and YMMV by design. No one can answer that question. It might save you a hundred million with some genius optimization it lucked into or, more likely, it\u2019ll give you a seemingly equivalent query that actually returns subtly different results (easy to do with query optimizations when eg you rely on inherent/default sort order for joins with groups).\n \nreply",
      "LLM:s are really really good at creating queries. Shaving of the last ms from a complex query? yea, im sure an experienced query optimizer expert might beat it.But you get very far from letting the LLM run a few queries to gather info about the database and its use.\n \nreply",
      "They won\u2019t understand your schema and make the wrong assumptions _but_ as long as you are checking the results and can understand the final query they can be very helpful.Obviously there are tricks to let them better understand your schema but even using all of those it\u2019s going to make the wrong assumptions about some columns and how they are used.\n \nreply",
      "I've used them off and on for basic stuff like \"tell me what this query does\".  They're usually pretty good at that.  Sometimes it will make suggestions that are meaningful improvements, most of the time not.  Unless you specifically ask about something like \"would this be better done with a left join lateral/correlated subquery?\"  But you kinda have to already know what you're going for and then it can be helpful.\n \nreply",
      "The screenshot section in the README seems to be empty. Would've been interesting to see that. There's many tools that do similar things like https://github.com/ankane/pghero or some tools here: https://gitlab.com/postgres-ai\n \nreply"
    ],
    "link": "https://github.com/nexsol-technologies/pgassistant",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        PgAssistant is an open-source tool designed to help developers understand and optimize their PostgreSQL database performance.\n      \n\n\nA PostgreSQL assistant for developers\n      designed to help understand and optimize PostgreSQL database performance.\n\nPgAssistant is an open-source tool designed to help developers understand and optimize their PostgreSQL database performance. It provides insights into database behavior, identifies schema-related issues, and assists in correcting them. Additionally, PgAssistant makes it easy to manage a library of specific SQL queries useful for your project through a simple JSON file.pgAssistant needs the pg_stat_statements postgresql module to run.To activate this module on your database is very simple. Below, you will find an example of how to activate it if you are in a Docker environment, as we"
  },
  {
    "title": "What happens to your online accounts when you die? (digitalseams.com)",
    "points": 27,
    "submitter": "bobbiechen",
    "submit_time": "2025-02-09T15:09:41 1739113781",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=42991112",
    "comments": [
      "My Steam friends list is slowly turning into a memorial.\n \nreply",
      "I'm sorry to hear that.On the topic of Steam, according to their terms of service, you buy non-transferable licenses for access, and they will argue you can't inherit the games of a deceased person: https://arstechnica.com/gaming/2024/05/after-you-die-your-st... . This follows a tradition of digital media providers asserting deep control over things that you might think you \"own\" : https://www.nytimes.com/2009/07/18/technology/companies/18am...(I had a hard time fitting this thread into the blog post and eventually cut it)\n \nreply",
      "Damn that\u2019s sad.\n \nreply",
      "My son gets my 1pw master password and yubikey and inherits all my online accounts.\n \nreply",
      "That is my plan as well --- there's an envelope in the safe which has my e-mail password which should allow taking over the accounts, and maintain access to my GOG.com game library, my Amazon Kindle books, and my Amazon Music --- curious if there will be any case law in-between now and then.\n \nreply",
      "All will go to my daughter\n \nreply",
      "My brother passed a few years ago.  I was able to \"memorialize\" his Facebook account, or whatever they call that term.  Found a link on their web site, uploaded a scanned copy of the death certificate, and within a day or so the title to his page was changed to something like \"Remembering Joe Blow...\"  People could still post on his page, but nobody could log in under his name (just in case his account got hacked or something).  It was pretty easy to do.\n \nreply"
    ],
    "link": "https://digitalseams.com/blog/what-happens-to-your-online-accounts-when-you-die",
    "first_paragraph": "A few months ago, I heard of a work acquaintance's death via his company's social media. Shocking and abrupt - I had just met up with him (let\u2019s call him George) for the first time. We joked over beers in a cozy little alleyway cafe, me and a coworker and George.It made me queasy to see George\u2019s smiling LinkedIn profile photo, oblivious to the realities of flesh-and-blood. If you hadn\u2019t seen the company announcement (which didn\u2019t tag him), you might even still message him.LinkedIn has a clearly-defined process for what happens to the accounts of the deceased, so I filed the support ticket: \u201cRequest to memorialize\u201d, submitted to the bureaucracy of the living.Within two hours, a support team member replied, \u201cI'm sorry for not having a quick answer about your issue. I've forwarded your message to another group for additional review and advice.\u201d No problem - I\u2019m in no rush here. Over the next few days, my ticket bounced between a few more people; they asked me to re-file the ticket, and fi"
  },
  {
    "title": "Ask HN: What's the best implementation of Conway's Game of Life?",
    "points": 37,
    "submitter": "jakemanger",
    "submit_time": "2025-02-12T10:22:21 1739355741",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=43023875",
    "comments": [
      "Has to be the APL implementation [0] purely due to how small and arcane it is:    life \u2190 {\u22831 \u2375 \u2228.\u2227 3 4 = +/ +\u233f \u00af1 0 1 \u2218.\u2296 \u00af1 0 1 \u233d\u00a8 \u2282\u2375}\n\nThe video runs you through it, even if it doesn't make it any clearer for non-apl'ers.[0] https://aplwiki.com/wiki/Conway%27s_Game_of_Life\n \nreply",
      "This one is quite interesting:https://oimo.io/works/life/It was also featured on HN:https://news.ycombinator.com/item?id=39799755\n \nreply",
      "I think \"quite interesting\" is an understatement. This is the Game Of Life simulating the Game Of Life. I personally consider it to be one of the Great Wonders of software.\n \nreply",
      "Zooming out on that one is an absolutely amazing experience\n \nreply",
      "I made that a slowly panning and enlarging Mac OS screensaver from that site -  10 lines of source I\u2019ve long since lost to load a web view. [edit to make sense]\n \nreply",
      "Depends on your metric, but this recursive one is pretty good: https://oimo.io/works/life/Previously: https://news.ycombinator.com/item?id=39799755(Ope, ninja'd)\n \nreply",
      "Seems like that's a favourite of almost everyone!\n \nreply",
      "Not an implementation, but Bill Gosper's hashlife algorithm blew my mind when I first encountered it.https://en.wikipedia.org/wiki/HashlifeMy personal favorite has to be the one I made in x86 assembly ~30 years ago. I didn't know about hash life at the time, but still, it ran pretty fast. I seem to remember I used page flipping and moved the start of video memory each iteration. Or something like that.\n \nreply",
      "FWIW, Golly (mentioned elsewhere in this thread) implements Hashlife as one of its core algorithms. So you can use it to simulate enormous patterns at enormous speeds, as long as they are sparse and/or regular enough that the hash hit rate is high.https://golly.sourceforge.io/\n \nreply",
      "Game of life implemented as sets in Clojure:(defn neighbors [[x y]]\n  #{[(dec x) (dec y)] [(dec x) y] [(dec x) (inc y)]\n    [     x  (dec y)]             [     x  (inc y)]\n    [(inc x) (dec y)] [(inc x) y] [(inc x) (inc y)]})(defn count-neighbors [world cell]\n  (count (set/intersection (neighbors cell) world)))(def rules #{[true 2] [true 3] [false 3]})(defn live [world cell]\n  (contains? rules [(contains? world cell) (count-neighbors world cell)]))(defn evolve [world]\n  (into #{} (filter #(live world %) (reduce set/union (map neighbors world)))))Full source: https://github.com/twpayne/life/blob/master/clojure/src/life...\n \nreply"
    ],
    "link": "item?id=43023875",
    "first_paragraph": ""
  },
  {
    "title": "How Nissan and Honda's $60B merger talks collapsed (reuters.com)",
    "points": 87,
    "submitter": "comebhack",
    "submit_time": "2025-02-12T16:32:37 1739377957",
    "num_comments": 111,
    "comments_url": "https://news.ycombinator.com/item?id=43026934",
    "comments": [
      "At this point I feel like a rising stock price after a crisis should be a major red flag for long term corporate survival.Carlos Ghosn was able to \"turn Nissan around\", but it was at the expense of future product capabilities (in my opinion) [Disclosure I work for GM, this is solely my own opinion]Also, I must say that it is not clear to me that anyone could know what a long term winning play looked like 10-15 years ago when the damage was done (in my opinion). It takes a lot of effort and money to make a mediocre automobile, it takes a lot more to make a high quality automobile.\n \nreply",
      "This seems to be the classic way in which management types destroy technical companies. Same thing happened a decade back in electronics companies in Japan (Sony comes to mind).Technical competence is generally very hard to judge and often even harder replace. It's not surprising that the same management types are salivating at the thought of replacing people with AI.\n \nreply",
      "I don't know, the Renault Logan (and the B platform in general) was extremely good from the engineering standpoint. Cheap, reliable, easy to maintain, easy to fix. The technical competence in building gasoline cars was definitely there.They missed the electric wave sure, but as with any innovations the more competent you are in the previous wave of technology the harder it is to switch to the new one. But it's a different kind of problem.\n \nreply",
      "I kind of wonder if the endgame for all of this is one-product-per-company.The company comes into being to make widget x, and never cares / is able to make another product again.I mean, that's kind of how it all happens anyway.  The people who stick through things and make the thing go away anyway.  the ip is then acquired.\n \nreply",
      "That seems like the current model in tech, to the point that companies are eventually renamed for their only product (RIM -> BlackBerry, Sun -> JAVA, dotCloud -> Docker), but there are also a few Asian megacorps that have their fingers in seemingly everything, think of Yamaha and Mitsubishi.\n \nreply",
      "Under Goshn and his close early advisors, Renault-Nissan started working on EVs, launching the Leaf and Zo\u00e9. Early, he also managed to streamline production of the two companies, and started to implement management changes that let some workers have more autonomy.The issue is that power got to his head and truly believe he was the second coming of Jesus or something, and stopped improving his companies to rub shoulders with the Nepo CEO/aristocrat crowd. Had he continued the push toward affordable EV, Nissan could have been BYD, but R&D stopped, for no visible reason.My personal theory is that the fallout from his divorce estranged him from his early friends and his closest advisor (his wife) and idiotic sycophants made him believe he was above the law and deserved even more. I've heard a lot of good things about pre-2008 Goshn, from people who aren't usually glazing billionaires, so maybe I'm biased.\n \nreply",
      "> My personal theory is that the fallout from his divorce ...Yes, I've noticed that people having nasty public fights with family members can lead to extremely negative effects on decision making.\n \nreply",
      "> and idiotic sycophants made him believe he was above the law and deserved even moreWell in all fairness he is above the law. He walked out of Japan and is free in his country.\n \nreply",
      "> He walked out of JapanEven better: He escaped by hiding in a music equipment box that was carried onto a private jethttps://www.bbc.com/news/business-57760993\n \nreply",
      "Boy, that final paragraph sure looks like another automaker CEO\u2026\n \nreply"
    ],
    "link": "https://www.reuters.com/markets/deals/inside-collapse-nissan-hondas-60-billion-mega-deal-2025-02-12/",
    "first_paragraph": ""
  },
  {
    "title": "Automated Capability Discovery via Foundation Model Self-Exploration (arxiv.org)",
    "points": 38,
    "submitter": "f14t",
    "submit_time": "2025-02-12T18:14:34 1739384074",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=43028057",
    "comments": [
      "If you want to know more about open endedness I recommend Kenneth Stanley\u2019s book: why greatness cannot be planned\n \nreply",
      "The study focuses on evaluating GPT-4o, Claude 3.5, and Llama3-8B, but it might benefit a bit from testing across more architectures (like Mixtral, DeepSeek, Gemini). This would help show generalizing of ACD.\n \nreply",
      "What do you think that would help show? 4o and Llama are quite different; reportedly, the 4-series is a large MoE, whereas Llama is famously a dense model.\n \nreply",
      "Testing across more architectures helps to clarify if ACD uncovers failures tied to model scale, training data, or architectural differences like MoE or other desings.If failures are model-specific quirks or can generalize across LLMs, that would support claims about ACD\u2019s robustness and usefulness for broad AI evaluation.\n \nreply",
      "While I appreciate arxiv.org, I think there should be more peer reviewed work.\n \nreply",
      "Ideally we would see some peer review on arxiv itself. There are some... wrappers? of that kind of functionality on https://www.scienceopen.com/ and others, but it would be amazing to see those reviews closer to the source.\n \nreply",
      "I think the usual name is \"overlay\". At least, that's what Tim Gowers called the one he started :) https://gowers.wordpress.com/2015/09/10/discrete-analysis-an...\n \nreply",
      "Personally, I've come to think of the peer-review process as a big reinforcer of the  publish or perish culture in academia. Merit review committees are encouraged to rely on the count (and IF scores) of published peer-reviewed papers to measure impact, allowing them to depend on the peer-review publishing process to mint tokens signifying the value of a researcher. While this saves the committees time and gives them an excuse to not actually evaluate the content of the researcher's output, there are costs to researchers.For good and careful scientists, the peer review process rarely adds much value to the original submission, yet requires a lot of tedious work and energy responding to minor concerns. That time and energy could be spent doing more research. Peer-review adds its most value to bad manuscripts of bad research, where good reviewers coach the authors on how to do science better. This also takes up a lot of time.If I could do it my way, I'd rather publish to an archive and move on once I feel that the research is to my satisfaction.\n \nreply",
      "Is it the case that the authors of these ML papers frequently don't even try to get it into a peer reviewed manuscript?\n \nreply",
      "Yeah the field is moving so fast that I guess nobody wants to wait to publish in a journal. Even papers that are 100% guaranteed to be accepted don't wait for it. I think it was one of Deepseek's models or maybe some other company I saw who didn't even both uploading to Arxiv initially, they just had a PDF in their GitHub repo. I don't think any of this is a problem because the results speak for themselves and in many cases nobody has to guess if an idea is good or not because they can just try it themselves. That applies less to evaluation studies like the linked article though. Even in this case however the code is open source.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2502.07577",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Show HN: A no-build fullstack SSR TypeScript web framework (jsr.io)",
    "points": 54,
    "submitter": "thesephi",
    "submit_time": "2025-02-12T19:54:52 1739390092",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=43029089",
    "comments": [
      "I don't understand.You're authoring in TSX and serving JS to browsers, so there's clearly a build step.  How is your model different from e.g. vite serve?How does it get deployed?  Are the files translated to vanilla JS on request, or are they translated ahead of time and cached (in other words, a build step)?\n \nreply",
      "My apologies if the wording made it confusing :D By \"no build\" I only meant \"no bundling\".You can check the generated source code of the example deployment to see how it's deployed: https://fullsoak.onrender.com/But spoiler: it's like in the 90s: each page request results in a text/html response, and the HTML doc then links in any .js or .css file it needs.I elaborate more in this wiki: https://github.com/fullsoak/fullsoak/wiki/Concepts-&-Example...\n \nreply",
      "Not having all the code in-line that can be in-line is like the 90s. You had me until the 'no bundling' thing. Separate http requests for all resources is a non-starter for anyone worried about page speed. I do like the choice of Preact though!\n \nreply",
      "and that's a totally valid concern :) So far I only wish to check the concept in general (ie: \"does it work\"). As for \"does it work fast / at scale\", definitely a future topic.I do wish to elaborate: since we use JSX/TSX, in theory we can in-line as much as we'd like. Here I include CSS in the component itself: https://github.com/fullsoak/bun-examples/blob/main/src/compo...So it can be just 1 HTML resource + 1 js file for every unique path.\nAnd with preact-iso (or react router) we can make subsequent pages load on-the-fly without a hard browser refresh / hard resource reload. Like so: https://github.com/fullsoak/bun-examples/blob/main/src/compo...But ultimately: I do see rooms for improvements re. resource optimizations. My personal wish is to use standard web specs such as \"preload\", \"prefetch\", \"server push\", etc. to optimize as much as possible, without taking the \"bundling route\" - so: just exploring a different taste, not that I have anything against the existing \"build routes\" :)\n \nreply",
      "It works with bun or deno which are alternative node runtimes that support jsx and typescript without a build step.\n \nreply",
      "Deno does not support jsx and typescript 'without a build step'. It just runs a build/transform step for you under the hood - using the same tsc compiler, config, etc.https://docs.deno.com/runtime/reference/jsx/Saying this is not a 'build step' is like saying a shell script that builds and runs a C program from source has no build step.\n \nreply",
      "> a shell script that builds and runs a C program from source has no build stepI would say so. Ultimately build step or not isn't a meaningful thing to care about. What matters is the cold start/runtime impact. In the case of deno/bun, the impact seem minuscule/not meaningful.\n \nreply",
      "So the build happens implicitly via the runtimes, then.\n \nreply",
      "Bun handling the TSX at runtime is just its capabilities as an interpeter. Else we'd be saying all interpreted languages have a build step\n \nreply",
      "Browsers cannot parse jsx/tsx natively. Deno is transforming the code you write before it is executed in the browser. That transformation is typically referred to as a build step.\n \nreply"
    ],
    "link": "https://jsr.io/@fullsoak/fullsoak",
    "first_paragraph": "a no-build TypeScript fullstack SSR-first framework for developing fast web applications with a shallow learning curveFullS(tack)oak (FullSoak for short) is a modern (born 2025), no-build TypeScript\nfullstack framework for building fast web applications with a shallow learning\ncurve. At its core is the Oak http server framework\nwhich is inspired by Koa (one of the popular Node.js http frameworks).FullSoak is no-build. Zero, zip, zilch, nada. That means: no tsc, no\nwebpack (or any such equivalence). All files are served from where they\nare. No surprises. Still, optimizations such as minification and mangling are\nsupported.FullSoak supports both JSX and HTM (Hyperscript Tagged Markup) which boasts\nseveral enhancements over JSX -\nbut most importantly: both require no separate build step (back to point 1).\nJSX transformation is automatically applied on a per-file basis.FullSoak is Preact. So: the familiarity of React,\nbut as lean as we need it to be.FullSoak is SSR-first & SSR-optimized. S"
  },
  {
    "title": "Show HN: I made my own OS from scratch because I was bored (jotalea.com.ar)",
    "points": 56,
    "submitter": "Jotalea",
    "submit_time": "2025-02-12T20:55:12 1739393712",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=43029686",
    "comments": [
      "We need more bored people.\n \nreply",
      "There has perviously been a nice guide posted on HN on getting code to run on bare metal:https://johv.dk/blog/bare-metal-assembly-tutorial.html\n \nreply",
      "> I made it and I'm proud of thatAnd that's all that matters for a hobby project. Congrats!\n \nreply",
      "Great job, something like this is also a really great way to get your foot in the door with a job when the time comes.I like that you have the site in both Spanish and English!\n \nreply",
      "Yeah, I'm a native Spanish speaker, and since I'm planning to share this project in Hispanic communities too, it would be bad to not have my own language in it.Though following that logic, I should also make my OS in Spanish too... I guess I could add some kind of translation dictionary, I'm not sure. I guess I'll do it in the future.\n \nreply",
      "I'm impressed with your dedication and also amazed that I ran into this on HN. My 9-year-old just showed me your Geometry Dash remake in Scratch. You're like a legend to him and his friends!\n \nreply",
      "How can we download only the source code?\n \nreply",
      "The source code comes in a bundle that includes a compiler, an emulator, and some extra tools. While I don't see an advantage on downloading the source code alone, I won't deny it.Just note that there are three different \"branches\":- stable: This is the one being distributed in the website- neofetch: This one exists because I couldn't get the ASCII art to fit within 512 bytes along with the rest of features- multistage (untested): Here I'm trying to work with jumps in memory to handle different commands and features (and include them all in one release). I read online that I can compile them separately and then merge the binaries to get jumps to work. But I haven't finished it, and it probably won't compile as it is right now.That said, here is the source code for all of them: https://quickshare.samsungcloud.com/wB9kfq1umxW2edit: fixed missing newlines\n \nreply",
      "despite saying it's open source, there's no link to read source code e.g github.\n \nreply",
      "why do you think that is a requirement to being called open source?\n \nreply"
    ],
    "link": "https://jotalea.com.ar/misc/jotaleaos/",
    "first_paragraph": ""
  }
]