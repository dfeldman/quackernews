[
  {
    "title": "It's time to make computing personal again (vintagecomputing.com)",
    "points": 141,
    "submitter": "mariuz",
    "submit_time": "2025-01-19T23:11:22 1737328282",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=42763095",
    "comments": [
      "This is part of why I've been so excited about Genode/Sculpt https://genode.org/documentation/articles/sculpt-24-10It's tiny, clearly built with love for the user, doesn't do a heck of a lot, and has some interesting ideas that are just fun to mess around in. And unlike some of the similar retrocomputing OS's (which are also lovely but grounded in old fashioned design), genode feels like a glimpse into the good future.\n \nreply",
      "That looks like the most radical/unusual operating system thing I have seen in recent memory. Not sure how practical it is, but kudos for trying something so different.\n \nreply",
      "It's so cool, I could talk about it forever. It's practical enough for the devs to use it as a daily driver (though with linux in VirtualBox or Seoul for some things like running their builds) and theres a few businesses built on it.But nowhere near as practical as Linux at the moment of course\n \nreply",
      "Interesting, I didn't know anyone had tried to make seL4 on a desktop.I think it'd be very cool to have a fully verified kernel...\n \nreply",
      "> How many Nintendo Entertainment System games sustained themselves with in-app purchases and microtransactions? What more did the console ask of you after you bought a cartridge? Maybe to buy another one later if it was fun?True, but unlike the Apple II, the NES was not an open system. The NES had hardware DRM, which allowed Nintendo to control what games were published for the system and to charge licensing fees (much as Nintendo, or Apple, do today). Nintendo also tried (unsuccessfully) to shut down Game Genie.https://en.wikipedia.org/wiki/CIC_(Nintendo)#10NES\n \nreply",
      "There's plenty of Ed Zitron's opinions I don't agree with, but this is a really good quote:\"Our economy isn\u2019t one that produces things to be used, but things that increase usage.\"\n \nreply",
      "That's a side effect of the way we've educated the market to expect everything to be \"free.\" That leaves the only option available being indirect monetization through ads or in-app purchases or something similar to that.\n \nreply",
      "> \"Our economy isn\u2019t one that produces things to be used, but things that increase usage.\"...the quote, *AS A SOUNDBITE*, only sounds good on a surface level, but collapses under the slightest test. All products in some form or another increase the usage of resources in order to reach a certain goal.https://www.wheresyoured.at/the-anti-economy/The article, where the quote originates from, contextualizes the quote marking (a) the difference between products in service of an actual goal, (b) products that are only meant to look good on a balance sheet, and (c) how companies have morphed towards (b) in order to attract investor funds and increase share prices / company market values.The quote, BY ITSELF AND WITHOUT CONTEXT, is a twisted Neo-luddist version of its original self.\n \nreply",
      "I think it means increase usage of the thing itself, and I think it's a good insight. While there is a natural supply and demand curve, unscrupulous growth-focused businesses optimize their products (unhealthy food) and services (gambling, social media, mobile games) for high levels of consumption (at least for a portion of vulnerable users), irrespective of harmful effects. It's the tobacco industry model reborn.\n \nreply",
      "I think a more generous interpretation of this is simply one that is critiquing planned obsolescence and addicting algorithm. Some things need to increase usage by nature, but how many services have you used that really needed a subscription as a necessary model to work?\n \nreply"
    ],
    "link": "https://www.vintagecomputing.com/index.php/archives/3292/the-pc-is-dead-its-time-to-make-computing-personal-again",
    "first_paragraph": "How surveillance capitalism and DRM turned home tech from friend to foe.For a while\u2014in the \u201980s, \u201990s, and early 2000s\u2014it felt like nerds were making the world a better place. Now, it feels like the most successful tech companies are making it worse.Internet surveillance, the algorithmic polarization of social media, predatory app stores, and extractive business models have eroded the freedoms the personal computer once promised, effectively ending the PC era for most tech consumers.The \u201cpersonal computer\u201d was once a radical idea\u2014a computer an individual could own and control completely. The concept emerged in the early 1970s when microprocessors made it economical and practical for a person to own their very own computer, in contrast to the rise of data processing mainframes in the 1950s and 60s.At its core, the PC movement was about a kind of tech liberty\u2014\u2013which I\u2019ll define as the freedom to explore new ideas, control your own creative works, and make mistakes without punishment.The "
  },
  {
    "title": "TikTok says it is restoring service for U.S. users (nbcnews.com)",
    "points": 484,
    "submitter": "Leary",
    "submit_time": "2025-01-19T17:42:44 1737308564",
    "num_comments": 1473,
    "comments_url": "https://news.ycombinator.com/item?id=42759336",
    "comments": [
      "What's that saying? The best way to get a promotion is to cause a problem and then fix it?Political things aside, it's crazy to see so much of a flip-flop so quickly. Has there been any other behavior like this in the past where a company \"shut themselves down\" to make a big political statement and then almost immediately undid the shut down?\n \nreply",
      "> Has there been any other behavior like this in the past where a company \"shut themselves down\" to make a big political statement and then almost immediately undid the shut down?https://en.wikipedia.org/wiki/Government_shutdowns_in_the_Un...\n \nreply",
      "> a company \"shut themselves down\" to make a big political statementThey were following the law. Anything else is just promises by people who are not exactly known for following through with themShutting down because the law says it, and to prevent really big penalties, is not making \u201ca big political statement\n \nreply",
      "The law didn't require them to shut the service off for those who already had the app installed.  It just prevented new updates or downloads.  Shutting off the app immediately was just theater and reinstating the app with no changes to the law is just the second act.\n \nreply",
      "The law says that US cloud providers are fined if they continued to provide services to Bytedance.As far as we know, Tiktok is operated on US servers by Oracle. While it might have been possible to find another cloud provider and move all US data there, I can see them not wanting to do that given that there was no point if the app isn't distributed in the US anymore.\n \nreply",
      "There's currently no evidence pointing towards Oracle shutting down cloud service to them though.  TikTok appears to have just preemptively shut down the app before they were obligated to, complete with dramatic messages telling users what to blame and who to thank.\n \nreply",
      "Even without following the letter of the law it's entirely rational behaviour for a popular market leader to foment outrage by fully blacking out services. 150 million users (in the US alone) is a very powerful political influence. Politicians frequently fold for a few thousand vocal people complaining on the internet.It was a gambit used for net neutrality in 2014 https://en.wikipedia.org/wiki/Internet_Slowdown_Day\n \nreply",
      "I\u2019m not sure this is correct. I see where you\u2019re coming from, but there was a clear date that the law was going to be enacted by, and tiktok simply followed that date. Pretty much everybody expected tiktok to be required to shut down. The law is clear that there are penalties for tiktok continuing to operate past that date, so it\u2019s not really surprising.They were telling users who to blame and who to thank because in this specific case, the blame and the thank are pretty clear. The Biden administration approved the ban, and the Trump administration reversed it. Blaming one and thanking the other is also hardly surprising.\n \nreply",
      "The law was simply going to be enacted on the date, but there was evidence that the law wasn't going to be immediately enforced. The white house had released a statement two days ago, that they would leave the enforcement of the law up to the incoming administration.https://www.whitehouse.gov/briefing-room/statements-releases...",
      "They shut down and reopened without any changes to the law. They are open now, despite the law being in effect.\n \nreply"
    ],
    "link": "https://www.nbcnews.com/tech/tech-news/tiktok-says-restoring-service-us-users-rcna188320",
    "first_paragraph": "ProfileSectionsLocaltvFeaturedMore From NBCFollow NBC News news AlertsThere are no new alerts at this timeTikTok said Sunday that it would be restoring service to U.S. users after blocking it the evening before.In a statement, TikTok said its video platform was coming back online after President-elect Donald Trump provided the necessary assurances to the company\u2019s service providers.\u201cIn agreement with our service providers, TikTok is in the process of restoring service,\u201d TikTok said in a post on X. \u201cWe thank President Trump for providing the necessary clarity and assurance to our service providers that they will face no penalties providing TikTok to over 170 million Americans and allowing over 7 million small businesses to thrive.\u201d\u201cIt\u2019s a strong stand for the First Amendment and against arbitrary censorship. We will work with President Trump on a long-term solution that keeps TikTok in the United States,\u201d the company added.Just hours before TikTok blocked service to Americans, Trump pos"
  },
  {
    "title": "OpenAI funded FrontierMath Benchmarks and had access to the set (lesswrong.com)",
    "points": 146,
    "submitter": "wujerry2000",
    "submit_time": "2025-01-19T23:27:59 1737329279",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=42763231",
    "comments": [
      "\u201c\u2026 we have a verbal agreement that these materials will not be used in model training\u201dHa ha ha. Even written agreements are routinely violated as long as the potential upside > downside, and all you have is verbal agreement? And you didn\u2019t disclose this?At the time o3 was released I wrote \u201cthis is so impressive that it brings out the pessimist in me\u201d[0], thinking perhaps they were routing API calls to human workers.Now we see in reality I should\u2019ve been more cynical, as they had access to the benchmark data but verbally agreed (wink wink) not to train on it.[0: https://news.ycombinator.com/threads?id=agnosticmantis#42476... ]\n \nreply",
      "You can still game a test set without training on it, that\u2019s why you usually have a validation set and a test set that you ideally seldom use. Routinely running an evaluation on the test set can get the humans in the loop to overfit the data\n \nreply",
      "OpenAI doesn't respect copyright so why would they let a verbal agreement get in the way of billion$\n \nreply",
      "Can somehow explain to me how they can simply not respect copyright and get away with it? Also is this a uniquely open-ai problem, or also true of the other llm makers?\n \nreply",
      "A lot of people want AI training to be in breach of copyright somehow, to the point of ignoring the likely outcomes if that were made law. Copyright law is their big cudgel for removing the thing they hate.However, while it isn't fully settled yet, at the moment it does not appear to be the case.\n \nreply",
      "This has me curious about ARC-AGI.Would it have been possible for OpenAI to have gamed ARC-AGI by seeing the first few examples and then quickly mechanical turking a training set, fine tuning their model, then proceeding with the rest of the evaluation?Are there other tricks they could have pulled?It feels like unless a model is being deployed to an impartial evaluator's completely air gapped machine, there's a ton of room for shenanigans, dishonesty, and outright cheating.\n \nreply",
      "A co-founder of Epoch left a note in the comments:> We acknowledge that OpenAI does have access to a large fraction of FrontierMath problems and solutions, with the exception of a unseen-by-OpenAI hold-out set that enables us to independently verify model capabilities. However, we have a verbal agreement that these materials will not be used in model training.Ouch. A verbal agreement. As the saying goes, those aren't worth the paper they're written on, and that's doubly true when you're dealing with someone with a reputation like Altman's.And aside from the obvious flaw in it being a verbal agreement, there are many ways in which OpenAI could technically comply with this agreement while still gaining a massive unfair advantage on the benchmarks to the point of rendering them meaningless. For just one example, knowing the benchmark questions can help you select training data that is tailored to excelling at the benchmarks without technically including the actual question in the training data.\n \nreply",
      "What's even more suspicious is that these tweets from Elliot Glazer indicate that they are still \"developing\" the hold-out set, even though elsewhere Epoch AI strongly implied this already existed: https://xcancel.com/ElliotGlazer/status/1880809468616950187It seems to me that o3's 25% benchmark score is 100% data contamination.\n \nreply",
      "This was my assumption all along.\n \nreply",
      "The questions are designed so that such training data is extremely limited. Tao said it was around half a dozen papers at most, sometimes. That\u2019s not really enough to overfit on without causing other problems.\n \nreply"
    ],
    "link": "https://www.lesswrong.com/posts/cu2E8wgmbdZbqeWqb/meemi-s-shortform",
    "first_paragraph": ""
  },
  {
    "title": "Chopstick Sleeves as Emissaries of Japanese Typography and Culture (letterformarchive.org)",
    "points": 33,
    "submitter": "NaOH",
    "submit_time": "2025-01-16T00:53:59 1736988839",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://letterformarchive.org/news/this-just-in-chopstick-sleeves-as-emissaries-of-japanese-typography-and-culture/",
    "first_paragraph": ""
  },
  {
    "title": "Hacking the Yamaha DX9 to Turn It into a DX7 (2023) (ajxs.me)",
    "points": 38,
    "submitter": "elvis70",
    "submit_time": "2025-01-19T21:39:21 1737322761",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42762161",
    "comments": [
      "Author here: Thank you so much for posting the article! I'm happy to answer any questions about the project.If you find this interesting, I wrote an overview of how I approach these projects: https://ajxs.me/blog/Introduction_to_Reverse-Engineering_Vin...I'm currently messing around inside the Casio CZ101, another cool 80s digital synth.\n \nreply",
      "Fantastic read! Love that you did this.I am surprised Yamaha ever created the DX9 given the manufacturing\neconomics. Did they even save a few pennies here and there to create\nan inferior instrument? I guess even in the early 80s product\ndifferentiation was something.\n \nreply",
      "Thank you! I agree with you. I find it hard to believe that leaving out the sub-CPU, one RAM chip and removing the velocity sensitive keyboard would've made a big difference to the manufacturing cost. My guess is that they always planned to have a budget offering, but didn't have a good technical idea of how to implement it. My opinion is that the DX9 was too handicapped in the end to be good value. According to this contemporary review[1], at least one person thought the lower price made up for the handicap!1: https://www.muzines.co.uk/articles/nine-times-out-of-ten/791...\n \nreply",
      "Evidently, the answer is that the DX7 was already cheap to produce. Engineering new stuff and spinning up a new assembly line would probably cost more than they would save. Remember, market segmentation is often about price discrimination. It's wrong to assume that consumer goods are priced at cost-plus-margin.https://en.wikipedia.org/wiki/Price_discrimination\n \nreply",
      "For what it's worth, they're actually totally different machines under the hood. They share the same main CPU, sound chips, and D/A section, but those are about the only similarities. I think you and GP make great points about product differentiation/price discrimination though. I think this explains Yamaha's motivations.\n \nreply"
    ],
    "link": "https://ajxs.me/blog/Hacking_the_Yamaha_DX9_To_Turn_It_Into_a_DX7.html",
    "first_paragraph": "\n\tIn light of reaching the milestone of releasing \n\t Version 1.0\n\tof the DX9/7 firmware,\n\tI've made a few updates to this article.\n\tThe most significant update is the addition is about some interesting\n\tleftover data\n\tfound in the Yamaha DX9 ROM.\n\tAlso, a big thanks to the amazing Olli Niemitalo\n\tfor correcting an error in how I was calculating the DX7/9's voice update frequency!\n\n\tLike many vintage synth enthusiasts,\n\tI keep a keen eye on the local classified sites for the odd bargain that might pop up.\n\tLate last year, amidst the usual sea of second-hand Roland grooveboxes,\n\tand Korg Volca synths was something I'd never seen before: A Yamaha DX9.\n\n\tAfter my 2021 project creating a detailed\n\ttechnical analysis\n\tof the DX7, and my subsequent\n\tdisassembly\n\tof its firmware,\n\tI figured there wasn't too much more I needed to know about the inside of the DX7...\n\tSeeing the DX9 \u2014advertised at half the price of a DX7, no less\u2014\n\tan interesting thought crossed my mind:\n\tCould the DX9 firmware b"
  },
  {
    "title": "Ancient Phoenician Shipwreck Recovered, Sank 2.6k Years Ago Off Coast of Spain (smithsonianmag.com)",
    "points": 106,
    "submitter": "bookofjoe",
    "submit_time": "2025-01-16T20:25:36 1737059136",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=42730449",
    "comments": [
      "This is only tangentially related, but if you like history and ship wrecks and live near Kansas City, go to the Steamboat Arabia museum.They're digging up a steamboat that sunk, and they found after the river changed its course. It's super cool. When we went the last time we were driving across the states, one of the guys actually doing the excavating was there. He gave our kids a guided tour and talked about all the exhibits with them. It was super cool.\n \nreply",
      "Cool indeed but 1856CE is not quite as incredible as 300BCE.\n \nreply",
      "Agreed, it is a very cool museum. Go early on a weekend morning in the spring and hit the market while it\u2019s bustling, love that part of the city.\n \nreply",
      "Preserved ancient shipwrecks are why I don't believe when people say something \"degrades in X years\".\n \nreply",
      "FYI 2600 and 2.6k use the same number of characters.\n \nreply",
      "It doesn't mean the same though, 2.6k implies somewhere between 2550 and 2650 years ago and 2600 an exact number of years.\n \nreply",
      "That is intriguing I had never seen xk used in that sense. Is that a common convention?\n \nreply",
      "Popular in finance. Often you dont need the details.\n \nreply",
      "2600 only has 2 significant digits. It's not an exact number.\n \nreply",
      "Unless the precision (resolution) is known (stated), it is unclear whether the trailing zeroes are significant or not, one may only guess (while such a guess looks reasonable in this case). A convention for writing that unambiguously is to avoid insignificant trailing zeroes: e.g., writing it as 26e2 or 2.6e3. Then the written number carries along its precision.\n \nreply"
    ],
    "link": "https://www.smithsonianmag.com/smart-news/divers-recover-ancient-shipwreck-that-sank-2600-years-ago-off-the-coast-of-spain-180985778/",
    "first_paragraph": ""
  },
  {
    "title": "How do interruptions impact different software engineering activities (rdel.substack.com)",
    "points": 15,
    "submitter": "kiyanwang",
    "submit_time": "2025-01-19T22:56:41 1737327401",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://rdel.substack.com/p/rdel-75-how-do-interruptions-impact",
    "first_paragraph": ""
  },
  {
    "title": "Using your Apple device as an access card in unsupported systems (github.com/kormax)",
    "points": 152,
    "submitter": "ValentineC",
    "submit_time": "2025-01-19T18:01:42 1737309702",
    "num_comments": 49,
    "comments_url": "https://news.ycombinator.com/item?id=42759557",
    "comments": [
      "Oh and I only have to give my identity to Alipay and Chinese transit? And modify my phone so it consistently beacons out a trackable ID?\n \nreply",
      "It isn't beaconing out a trackable ID unless one manages to get into NFC range, which isn't gonna be read from miles away\n \nreply",
      "10 meters is plenty to covertly track you entering/leaving a building.> The distance from which an attacker is able to eavesdrop the RF signal depends on multiple parameters, but is typically less than 10 meters.https://en.wikipedia.org/wiki/Near-field_communication\n \nreply",
      "This is cool, do love the hacking ingenuity. And not that I want to give Apple extra credit, but they are slowly opening up NFC: https://www.macrumors.com/guide/apple-nfc-chip-ios-18-1/ - Is is very restrictive (probably) and very late - most certainly. But at least it's slowly coming.\n \nreply",
      "The fact that Apple even considered \"closing\" a general purpose standard like NFC is a testament to how much they are willing to drag their feet on any innovative customer-facing features.You'd think that it was common sense to avoid that, but we are talking about the company that invented DRM for the USB protocol...\n \nreply",
      "This is a common trope, and given that they don't speak about these things publicly people will take whatever negative opinion they think up and consider them completely valid.I'm aware of this myself, because I work in games, and sometimes we literally can't comment on things; which leads to the speculation being very often the absolute most negative possible interpretation of what is happening, and completely invalid.To take another more obvious example; imagine OpenBSD was, well, not open. Yet they didn't support Bluetooth. (they don't).From and outside perspective, is this: because they don't consider it open source? Because they're not capable? Because they hate bluetooth headphones or interfacing with audio devices? Is it due to security- maybe related to file sharing or something... it could be privacy.And if we thought they were just pure-through-and-through bastards, we'd think up more.There's another perspective, ironically that covers your point as well as OpenBSD's lack of bluetooth support:What if; it's actually much harder than we realise to make it easy to use and secure?It's totally possible that a standardised format has only hacky, rushed and dangerous implementations. It's not uncommon at all, actually.\n \nreply",
      "Well, it\u2019s not like anyone sued a Dutch university for finding security vulnerabilities in their NFC transport card solution\u2026NXP would never rush their implementation to market and then try to sue their way out of any vulnerabilities.https://freedom-to-tinker.com/2008/07/15/transit-card-maker-...\n \nreply",
      "Years after nfc was added to android I have yet to see anyone complain that I can write an android app that has raw NFC access.\n \nreply",
      "They're an absolutely staggeringly immense company and they fight tooth and nail to preserve their rent seeking and premium margins.  Nobody owes them a positive interpretation of apparently poor behaviour, especially not when discoverable materials often show the principals are absolutely doing the wrong thing (e.g., Jobs and the anti-labour wage fixing racket).If the company wants people to have positive thoughts, they should do positive things!  If indeed there are positive reasons for an apparently negative thing, they should explain them.If they can't or won't explain them, then people absolutely should assume if they feel like they're getting screwed, that they probably are.\n \nreply",
      "> and sometimes we literally can't comment on things; which leads to the speculation being very often the absolute most negative possible interpretationThis happens everywhere, it's not a problem exclusive to games. It's a problem fundamental to trust - when you stop communicating with your audience, what do they assume from your behavior? Gamedevs feel this pressure as a consequence of releases like Duke Nukem Forever and No Man's Sky that promised the world and came out with nothing. Apple feels this scrutiny for the same reason - the quality of software they make is markedly worse than what they put out 15 or 25 years ago. We have to hold them accountable, Apple's silence is an unacceptable response to the rift they've created with the software standards community.Apple has plenty of unexplained fees and double-standards, as developers we've learned better than to ask why. There is no reason Apple makes me pay $99 per year to improve their platform - they have no justification to arbitrarily deny third-party payment processing. The obvious reason, on the surface, is that offering another business the same full-fat feature access that you have threatens the service you intend to sell.When Apple creates institutions like the App Store, iMessage and Apple Pay, they are implicitly asking themselves how they will treat their competitors. When they design these institutions around features with no third-party parity, they are creating unfair businesses and sustaining them on a policy of secrecy. We are not stupid people, we can surmise why Apple cares about service revenue by looking at their quarterly financials.> What if; it's actually much harder than we realise to make it easy to use and secure?So what? HTTPS is hard to secure, but Apple's not going to stop supporting that over some unnamed ideological protest. NFC and USB are comparatively simple, and you can implement every feature that Apple uses without resorting to DRM or middleware. The only tangible explanation comes in the form of licensing fees for MFi and Apple-branded serial connectors. It's about control and money, two perfectly acceptable explanations for a business as greed-oriented as Apple.You have to be arguing in entirely bad faith to look at the rest of the technology market and assert that it's the standards forum's fault for not responding to issues that Apple won't even confirm they have in the first place. Not only is that one hell of a baseless assumption, it's an apologia based on a source that doesn't exist. Faith is not a virtue, in the business world.\n \nreply"
    ],
    "link": "https://github.com/kormax/apple-device-as-access-card",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Using your Apple device as an access card in unsupported systems\n      \n\nThis repository describes general status on the topic and a possible solution to allow you to use your device as an access card in UID-based access systems (without cloning, consent on behalf of maintainer will be required).\nIn case you've already tried researching this topic, feel free to skip the Introduction section straight to The Solution.Many people have been beating their heads against the wall trying to use their Apple device as an access card.Sadly, due to closed NFC and Wallet ecosystem, such a thing is not possibe to do on your own without any explicit support on the behalf of Apple and access system provider.\nThankfuly, as of now this market request is starting to be fulfilled by the likes of Google, Apple with cooperation of access system manufactu"
  },
  {
    "title": "All Lisp indentation schemes are ugly (aartaka.me)",
    "points": 37,
    "submitter": "birdculture",
    "submit_time": "2025-01-19T21:30:09 1737322209",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=42762077",
    "comments": [
      "This post misses the IMO best indentation scheme for lisp, which I used for my college class where we had to use MIT scheme:    (define (match:element variable restrictions)\n        (define (ok? datum)\n            (every \n                (lambda (restriction)\n                    (restriction datum)\n                )\n                restrictions\n            )\n        )\n        (define (element-match data dictionary succeed)\n            (and \n                (ok? data)\n                (let ((vcell (match:lookup variable dictionary)))\n                    (if vcell\n                        (and \n                            (equal? (match:value vcell) data)\n                            (succeed dictionary)\n                        )\n                        (succeed (match:bind variable data dictionary))\n                    )\n                )\n            )\n        )\n        element-match\n    )\n\nIt may not be densest or most compact indentation scheme, but damn is it readable for someone without a lisp/scheme background!\n \nreply",
      "That is blasphemy.\n \nreply",
      "Nicely indented easy to read blasphemy is the most dangerous kind!\n \nreply",
      "I love a Lisp-themed article, but this is such a yawn in 2025.It seems pretty clear that a threading macro which allows the user to place the \"hole\" being filled wherever they want is the answer.Example: cl-arrowshttps://github.com/nightfly19/cl-arrows?tab=readme-ov-file\n \nreply",
      "> Common Lisp in particular is extremely unfriendly to threading macros. Arrows imply a consistent thread-first or thread-last functions. But CL's standard lib is too inconsistent for that to work. So we're left with picking an indentation style we don't necessarily like.Diamond arrows exist, you know.    (-<> foo\n      bar\n      (baz)\n      (quux 1 2 3)\n      (fred 4 5 6 <>)\n      (frob 7 <> 8 <> 9))\n\nAll in all, this post reads like a rant, and I realized that upon reading \"now what I'm about to suggest is likely not to your taste\". That style of indentation is something I use often when writing calls to long-named functions like COMPUTE-APPLICABLE-METHODS and I haven't ever thought of it being not to my taste, or even of it being ugly as the author suggests.\n \nreply",
      "never had any real issue using emacs indentation engine (not even sure lisp modes use smie..)\n \nreply",
      "> (tree-transform-if predicate\n    >                   transformer\n    >                   (second tree)\n    >                   depth)\n\n> A problematic function indentation\n> Nineteen! Nineteen spaces of indentation! It's getting unruly. Such an indent, when used in deeply nested code, makes it too wide and unreadable. If you add the strict one-per-line alignment of arguments, it's also painfully long line-wise.I think this is more a problem with languages that encourage (or don't discourage) deeply nested function calls. I face the same issue in Python, although Python kinda discourages me to do:    foo(\n        bar(\n            baz(\n                far(\n                    faz(...)))))\n\n\nInstead, Python and Pythonic idioms encourage a \"step by step\" approach:    a = faz(...)\n    b = far(a)\n    ...\n\n\nI don't know which is \"better\"\u2014the first approach requires thinking ahead about the chain of functions we're about to call while the second one is more like a gradual development of the solution, albeit being more verbose than the former.\n \nreply",
      "You can have the same step-by-step in Lisp via LET*. The problem outlined by the author happens when you have a function call where 1) the operator has a long name, 2) there are many arguments. The TREE-TRANSFORM-IF is such a case and you can't solve it by just \"going step by step\".\n \nreply",
      "Yes, I think for whatever reason, Lisp languages tend to use long names for funcs and vars whereas Haskell and other FP languages prefer symbols like >>=, <$>, etc. Personally, I prefer somewhere in the middle.> The TREE-TRANSFORM-IF is such a case and you can't solve it by just \"going step by step\".Yes, this is a good point.\n \nreply",
      "I prefer a third approach - chaining method calls via `.method()`. C#'s LINQ extension methods and Rust's iterator methods do it and I find it to be the best of both worlds most of the time.\n \nreply"
    ],
    "link": "https://aartaka.me/lisp-indent.html",
    "first_paragraph": "\nOnce you get used to Lisp, you stop noticing the parentheses and rely on indentation instead.\nThat's partially why there are several alternative syntaxes based solely on indentation.\nLike Wisp,\nor sweet expressions.\nBut then, the question stands: how to indent the code, actually?\nEspecially so\u2014in Lispy syntax.\n\n\nA solution that will likely satisfy a proponent of any indentation style:\n\"Just put it all on one line lol.\"\nOf course, lines are not infinitely readable and there's a column cap,\n(whether natural or enforced.)\nSo this line (adapted from\ncl-blc,)\nwhile devoid of indentation problems, is unreadable:\n\n\n(list (tree-transform-if predicate transformer (first tree) depth) (tree-transform-if predicate transformer (second tree) depth))\nAbsurdly long line of a Lisp code\n\nThat's the problem statement: some forms need multiple lines and indentation.\nBut what kind of indentation?\n\n\nThere's an established style of indentation: align the function arguments on the same column:\n\n\n(list (tree-"
  },
  {
    "title": "JTAG 'Hacking' the Original Xbox in 2023 (ret2.io)",
    "points": 10,
    "submitter": "transpute",
    "submit_time": "2025-01-16T17:01:34 1737046894",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://blog.ret2.io/2023/08/09/jtag-hacking-the-original-xbox-2023/",
    "first_paragraph": "\n\n        \n        August 9, 2023\n      \n    \n      \n        \n          /\n        Markus Gaasedelen\nReleased in November 2001, the original Xbox was Microsoft\u2019s first venture into the game console industry. With its hardware closely resembling a cheap but versatile PC of the early 2000s, the device came under the immediate scrutiny of technical hobbyists looking to run \u2018homebrew\u2019 code and alternative operating systems on it.Over the course of its lifetime, the Xbox was unilaterally hacked through a broad range of both hardware and software attacks. But at 20 years old, this Intel-based Pentium III system holds up as an amazing platform to learn or explore a plethora of security and computer systems engineering topics that are still relevant to this day.As a fun departure from our usual content, this post exploits some nostalgia to tackle a challenge put forth by some of the earliest musings of the original Xbox researchers: \u201chacking\u201d the original Xbox via Intel\u2019s x86 CPU JTAG.Hardware "
  },
  {
    "title": "The surprising struggle to get a Unix Epoch time from a UTC string in C or C++ (berthub.eu)",
    "points": 83,
    "submitter": "PascalW",
    "submit_time": "2025-01-19T16:10:11 1737303011",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=42758257",
    "comments": [
      "Is it a struggle though?They needed to have a locale matching the language of the localised time string they wanted to parse, they needed to use strptime to parse the string, they needed to use timegm() to convert the result to seconds when seen as UTC. The man pages pretty much describe these things.The interface or these things could certainly be nicer, but most of the things they bring up as issues aren't even relevant for the task they're trying to do. Why do they talk about daylight savings time being confusing when they're only trying to deal with UTC which doesn't have it?\n \nreply",
      "It is not.  int main(void) {\n    struct tm tm = {0}; \n    const char *time_str = \"Mon, 20 Jan 2025 06:07:07 GMT\"; \n    const char *fmt = \"%a, %d %b %Y %H:%M:%S GMT\"; \n\n    // Parse the time string\n    if (strptime(time_str, fmt, &tm) == NULL) {\n        fprintf(stderr, \"Error parsing time\\n\");\n        return 1;\n    }\n\n    // Convert to Unix timestamp (UTC)\n    time_t timestamp = timegm(&tm);\n    if (timestamp == -1) {\n        fprintf(stderr, \"Error converting to timestamp\\n\");\n        return 1;\n    }\n\n    printf(\"Unix timestamp: %ld\\n\", timestamp);\n    return 0;\n  }\n\nIt is a C99 code snippet that parses the UTC time string and safely converts it to a Unix timestamp and it follows best practices from the SEI CERT C standard, avoiding locale and timezone issues by using UTC and timegm().You can avoids pitfalls of mktime() by using timegm() which directly works with UTC time.Where is the struggle? Am I misunderstanding it?Oh by the way, must read: https://www.catb.org/esr/time-programming/ (Time, Clock, and Calendar Programming In C by Eric S. Raymond)\n \nreply",
      "I can't find `timegm` neither in the C99 standard draft nor in POSIX.1-2024.The first sentence of your link reads:>The C/Unix time- and date-handling API is a confusing jungle full of the corpses of failed experiments and various other traps for the unwary, many of them resulting from design decisions that may have been defensible when the originals were written but appear at best puzzling today.\n \nreply",
      "https://man7.org/linux/man-pages/man3/timegm.3.htmlIt's not posix, but it's pretty available\n \nreply",
      "Yeah, you're correct that `timegm` is neither part of the C99 standard nor officially specified in POSIX.1-2024 but it is widely supported in practice on many platforms, including glibc, musl, and BSD systems which makes it a pragmatic choice in environments where it is available. Additionally, it is easy to implement it in a portable way when unavailable.So, while `timegm` is not standardized in C99 or POSIX, it is a practical solution in most real-world environments, and alternatives exist for portability, and thus: handling time in C is not inherently a struggle.As for the link, it says \"You may want to bite the bullet and use timegm(3), even though it\u2019s nominally not portable.\", but see what I wrote above.\n \nreply",
      "timegm() is even available on Haiku\n \nreply",
      "What's a man page? [cit]\n \nreply",
      "\"manual pages\", type \"man man\" in your terminal.https://man7.org/linux/man-pages/man1/man.1.html\n \nreply",
      "My personal rule for time processing: use the language-provided libraries for ONLY 2 operations: converting back and forth between a formatted time string with a time zone, and a Unix epoch timestamp. Perform all other time processing in your own code based on those 2 operations, and whenever you start with a new language or framework, just learn those 2.I've wasted so many dreary hours trying to figure out crappy time processing APIs and libraries. Never again!\n \nreply",
      "13 more years to go until the 2038 problem.Surely we'll have everything patched up by then..\n \nreply"
    ],
    "link": "https://berthub.eu/articles/posts/how-to-get-a-unix-epoch-from-a-utc-date-time-string/",
    "first_paragraph": "Bert Hubert's writingsbert@hubertnet.nlSo how hard could it be. As input we have something like Fri, 17 Jan 2025 06:07:07 in UTC, and we\u2019d like to turn this into 1737094027, the notional (but not actual) number of seconds that have passed since 1970-01-01 00:00:00 UTC.Trying to figure this out led me to discover many \u2018surprise features\u2019 and otherwise unexpected behaviour of POSIX time handling functions as implemented in various C libraries & the languages that build on them. There are many good things in the world of C and UNIX, but time handling is not one of them.There is a narrow path of useful behaviour however. But first some context.The tl;dr: as long as you never call setlocale(), you can use strptime() to parse a UTC time string. Do not use %z or %Z. Pass the struct tm calculated by strptime() to the pre-standard function timegm() (mkgmtime() on Windows) to get the correct UNIX epoch timestamp for your UTC time string. Do read on for solutions for if you do use locales. C++ ha"
  },
  {
    "title": "Physicists have shown that an idealized form of magnetism is heatproof (quantamagazine.org)",
    "points": 45,
    "submitter": "headalgorithm",
    "submit_time": "2025-01-16T17:55:13 1737050113",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42728513",
    "comments": [
      "Interesting. It would be huge if this can help room temperature superconductivity. It seems like nobody knows what to do with it yet though.\n \nreply",
      "Heatproof might have a limit.  Every material has a failure point, some temperature when all the particles decide to part ways.   Those arrows mean little once elections start moving around as individuals.\n \nreply",
      "Isn't this exactly what the article says is not true according to the research? The whole point is that given the ideal setup they describe, the prediction is that the structure would remain orderly regardless of temperature.\n \nreply",
      "I believe they predicted something slightly different. That the magnetism would remain until structural failure. .Simply put physics says if you put enough energy on an atom in a solid, it will eventually leave the solid on the solid/vacuum boundary from raw kinetic energy alone. Now, if you're trapped in an infinitely large solid, there would be no boundary and the magnetism could remain. For example the example of the early universe would be an example of one of these near infinite structures where magnetism remains. Large astronomical objects with lots of gravity may be able to reproduce these conditions somewhat like this.With all that said, it should give us means of maintaining magnetism in an ordered manner in structures we'd consider hot as a human.\n \nreply",
      "It seems a lot like their \"structure\" is a model, and not composed of atoms, or anything physical at all. It's still very interesting, but it's not something one could go out and build, even in theory.\n \nreply"
    ],
    "link": "https://www.quantamagazine.org/heat-destroys-all-order-except-for-in-this-one-special-case-20250116/",
    "first_paragraph": "\nAn editorially independent publication supported by the Simons Foundation.\n\nGet the latest news delivered to your inbox.\nCreate a reading list by clicking the Read Later icon next to the articles you wish to save.Type search term(s) and press enterPopular\n                                    SearchesJanuary 16, 2025Kristina Armitage/Quanta MagazineStaff WriterJanuary 16, 2025Sunlight melts snowflakes. Fire turns logs into soot and smoke. A hot oven will make a magnet lose its pull. Physicists know from countless examples that if you crank the temperature high enough, structures and patterns break down.Now, though, they\u2019ve cooked up a striking exception. In a string of results over the past few years, researchers have shown that an idealized substance resembling two intermingled magnets can \u2014 in theory \u2014 maintain an orderly pattern no matter how hot it gets. The discovery might influence cosmology or affect the quest to bring quantum phenomena to room temperature.Several physicists expr"
  },
  {
    "title": "Two Hard Things (2009) (martinfowler.com)",
    "points": 3,
    "submitter": "mooreds",
    "submit_time": "2025-01-20T00:18:48 1737332328",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://martinfowler.com/bliki/TwoHardThings.html",
    "first_paragraph": "ArchitectureRefactoringAgileDeliveryMicroservicesDataTestingDSLAboutBooksFAQVideosContent IndexBoard GamesPhotographyInsightsCareersRadarRSSMastodonLinkedInX (Twitter)BGG14 July 2009Martin FowlerAPI designThere are only two hard things in Computer Science: cache\n    invalidation and naming things.-- Phil KarltonLong a favorite saying of mine, one for which I couldn't find a\n  satisfactory URL.Like many good phrases, it's had a host of riffs on it. A few of them I feel are worth\n  adding to the pageThere are 2 hard problems in computer science: cache invalidation, naming things, and\n    off-by-1 errors.-- Leon BambrickThere are only two hard problems in distributed systems: 2. Exactly-once delivery 1.\n    Guaranteed order of messages 2. Exactly-once delivery-- Mathias Verraesthere's two hard problems in computer science: we only have one joke and it's not funny.-- Phillip Scott BowdenThere are so many variations on the \u201cthere are only two hard problems in\n    computer programming...\u201d jo"
  },
  {
    "title": "Invisible Epidemic: The Loneliness Epidemic (pudding.cool)",
    "points": 17,
    "submitter": "kamikazeturtles",
    "submit_time": "2025-01-19T23:28:51 1737329331",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=42763240",
    "comments": [
      "Can't exactly ask coworkers for an ugly crying hug in the middle of a workday and then just act like nothing happened like you can with a best friend :(I'll also admit it: I'm lonely and the magic of life seems to have disappeared since this over-the-top loneliness began around 2020. I too want a life full of loving people.Also, fantastic work on that website. That's good stuff.\n \nreply",
      "> After lunch, he decided to watch TV again. He's been around other people today, like at the grocery store. But he hasn't done anything with other people.Now there's something to make you feel even _more_ alone -- being around people.Just waiting for life to expire at this point.\n \nreply",
      "Brilliant work\n \nreply"
    ],
    "link": "https://pudding.cool/2023/09/invisible-epidemic/",
    "first_paragraph": "This is a 62-year-old Hispanic male \u2013 let's call him Martin.\r  Martin is trying to sleep. But he's rolling around in bed, sleepless.\r  Keep scrolling.He couldn't sleep. So he got out of bed and watched TV.\r  Martin's a regular guy in his 60s, but he's pretty isolated. He doesn't have a job. He doesn't have a wife or partner, nor does he have children in his home. He just doesn't interact with many people.In this story, we'll go through 24 hours of a typical weekend day in 2021. We know what people did \u2013 and who they did it with \u2013 because, since 2003, the American Time Use Survey has asked people to track how they use their time.\r  By the end of the day, we'll learn that Martin's isolation isn't unique. In fact, loneliness has become a far more common experience in the last few decades \u2013 and it was supercharged by the pandemic.\r  We'll follow a handful of people, including Martin. Let's meet everyone else!Click a person for details.The average human can name about 1,500 people, and we p"
  },
  {
    "title": "Build a tiny CA for your homelab with a Raspberry Pi (smallstep.com)",
    "points": 125,
    "submitter": "timkq",
    "submit_time": "2025-01-19T15:50:51 1737301851",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=42758070",
    "comments": [
      "This being raspberry pi absolves you from needing to buy a separate hardware noise generator: it has plenty of GPIO. For example, one can obtain entropy by sampling random noise generated by reverse-biasing a junction in a cheap pn transistor. Here is an example: http://holdenc.altervista.org/avalanche/. Bonus \u2014 maybe it will get you hooked on electrical engineering!Btw, some versions of raspberry pi already have hardware random number number generator accessible at \n/dev/hwrng.\n \nreply",
      "How does the disconnected audio input of any random PC or thinclient compare?I continue to find it a bit silly to see \"with a raspberry pi\" when people just mean \"with any random linux box that doesn't need to be very powerful\".It's like listening to NPR, where every smartphone is an iPhone even if it's an Android, you know?\n \nreply",
      ">How does the disconnected audio input of any random PC or thinclient compare?That will give you RF noise, which isn't really random.\n \nreply",
      "Electrical noise (including RF noise) is really random, as in it is impossible to predict exact value.It does have non-flat spectrum, meaning some values are more probable than others, but that only means you need to whiten it.  (A rough analogy might be a 6-sided die labeled with 1,1,1,2,3,4 - yes, number 1 is much more likely to come out. No, this does not make it \"not really random\", and some trivial math can produce ideal random stream out of it)The only problem with audio input is that you may end up with non-random value - like all-zero output. But properly implemented whitener should detect this and stop outputting any value at all.\n \nreply",
      "> That will give you RF noise, which isn't really random.what does \"really\" random even mean in this context? does it actually matter?given 3 hypothetical devices in a homelab:a) does no specialized hardware entropy collection, and instead relies entirely on the standard Linux kernel mechanismsb) does entropy collection based on the RF noise that you're saying isn't \"really\" randomc) does entropy collection based on whatever mechanism you have in mind that generates \"real\" randomness (hand-carving bits of entropy out of quantum foam, or whatever)even if your threat model includes \"the NSA tries to break into my homelab\"...device A will almost certainly be fine, they'll have ways of getting access that are much simpler than compromising the entropy pool.I suppose device B has a theoretical vulnerability that if the NSA had physical access to your homelab, they could monitor the RF environment, and then use that to predict what its inputs to the entropy pool were. but...that's assuming they have physical access, and can plant arbitrary equipment of their own design. at that point, they don't need to care about your entropy pool, you're already compromised.\n \nreply",
      "I'd do this then three years later realize that something broke and it's just been feeding zeroes for the last 18 months.\n \nreply",
      "good point, the project immediately after building the CA is to build a decent monitoring/alerting setup.\n \nreply",
      "Could still have been randomhttps://xkcd.com/221/https://web.archive.org/web/20011027002011/http://dilbert.co...\n \nreply",
      "I love this idea!\n \nreply",
      "I'm running smallstep CA in my homelab. While it's nicely done and clearly focuses to the containerized enterprise market, its defaults are very harsh.Take for example the maximum certificate duration. While from a production/security perspective short-lived certificates are great, you don't want to renew certs in your homelab every 24-48hrs. Also, many things just don't support ACME but still benefit from a valid certificate, e.g. router/firewall/appliance web interfaces. Out of the box, the limit for traditionally issued certificates using the CLI is very low, too.The default prevents expired certificates to be renewed. If your homelab does not offer a couple of nines behind the comma, you'll pretty much have to intervene on a regular basis UNLESS you adjust the defaults. You can't set the max duration to years, months or days but only hours:  \"claims\": {\n     \"minTLSCertDuration\": \"24h\",\n     \"maxTLSCertDuration\": \"26400h\",\n     \"defaultTLSCertDuration\": \"9000h\"\n    },\n\nIf the goal of hour homelab is to design/test/experiment with a fault-tolerant high availability k8s infra, e.g. for your job, it's great.CAVE: macOS enforces duration limits even for trusted enterprise CAs, e.g. Safari won't accept your 1000 days certificate anymore.\n \nreply"
    ],
    "link": "https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/",
    "first_paragraph": "Carl TashianTL;DR In this tutorial, we're going to build a tiny, standalone, online Certificate Authority (CA) that will mint TLS certificates and is secured with a YubiKey. It will be an internal ACME server on our local network (ACME is the same protocol used by Let's Encrypt). The YubiKey will securely store the CA private keys and sign certificates, acting as a cheap alternative to a Hardware Security Module (HSM). We'll also use an open-source True Random Number Generator, called Infinite Noise TRNG, to spice up the Linux entropy pool.Now that you have good time synchronization and a stable hostname, we can proceed.Now, insert your YubiKey. Let's install the yubikey-manager (and dependency pcscd) and make sure you can connect to the YubiKey:You'll need Go in order to build the step-ca server.You'll need to install both step-ca (the CA server software) and step (the command used to configure and control step-ca).First, download the source for step-ca and build it with experimental "
  },
  {
    "title": "UK's elite hardware talent is being wasted (josef.cn)",
    "points": 64,
    "submitter": "sebg",
    "submit_time": "2025-01-19T23:52:13 1737330733",
    "num_comments": 69,
    "comments_url": "https://news.ycombinator.com/item?id=42763386",
    "comments": [
      "This is spot on. All the smart and ambitious people I know who studied (non-software) Engineering at university in the UK have ended up going into software engineering via self-teaching or finance/consulting because the only hardware engineering career paths seem to be working for Rolls Royce in the middle of nowhere with terrible pay, or alternatively working at Jaguar Land Rover in the middle of nowhere with terrible pay\n \nreply",
      "Was a MechE for 10 years here in the US and now I\u2019m a SWE. Even here, no one cares about hardware engineers. Don\u2019t get me wrong, you can make enough to be \u201ccomfortable\u201d. But anecdotally, maybe 10% of MechE do design. 10% of that are paid handsomely to be in tech and are \u201cProduct Designers\u201d. Even then, almost every tech company want to be a predominantly software company. They just happen to need hardware to execute their product. Admittedly, it\u2019s really hard to do hardware in this economy when one country has 60% of the global manufacturing output and can copy your design, make it cheaper, and make it better. Ironically, the biggest dividing line that makes a hardware product better is good software.\n \nreply",
      "Preach. My friend is a gifted passionate Aerospace engineer (top in his specific stream at Cambridge) and basically is withering away working for the above 2 firms. The location is grim being far from others and generally far from other young exciting people. Additionally in his org, there just isn't a sense of excitement/ urgency which leaves him with little to do. Prioritising career for a career that's not thereWhilst others working in software (myself included) can have a far greater quality of life and salary working in London.\n \nreply",
      "To some extent, this also applies to software. Except for DeepMind and a few other select places like Altos Labs, getting past \u00a3100k is hard, especially outside London. Unless you go into finance, of course. But then, you have to stick to London. Finance is like a black hole that sucks a big chunk of the mathematical, CS and statistical UK talent. They have very proactive recruiters trying to connect with Oxbridge grads when they are approaching graduation.\n \nreply",
      "As a Brit, when I was raising the seed round for my startup, UK and European VCs would consistently try to haggle you down on price while the American VC's were exclusively focussed on trying to figure out whether this could be a billion dollar business or not (in the end we raised a $5m seed led by Spark, and have done extremely well and raised more since).The UK lost Deep Mind - which could have been OpenAI!! -- to Google. I think part of the issue is cultural - the level of ambition in the UK is just small compared to the US. Individual founders like Demis or Tom Blomfield may have it but recruiting enough talent with the ambition levels of early Palantir or OpenAI employees is so hard because there are so few. Instead, a lot of extremely smart people in the UK would rather get the 'safe' job at Google, or McKinsey than the 'this will never work but can you imagine how could it would be if it did' job at a startup.There are probably political reasons as well. Unfortunately the UK has not been well governed for 20 years or so, and hence economic outcomes as a whole have been abysmal.\n \nreply",
      "Similar in France / Paris where some American players can easily pay 100K+ euros for SWE. Rest of France salaries are half or even less.\n \nreply",
      "Fully agree on all except this point:> \"UK's small market limits growth.\"(followed by a list of companies founded before we put up trade barriers with our largest and closest single market)\n \nreply",
      "Not that I disagree that Brexit was an awful idea, but this was a problem even before Brexit. The reach of European companies just isn't what it used to be in the face of American and Chinese giants, and the EU is failing to be a truly single market where companies can grow to that scale.\n \nreply",
      "Ad services, once it was monopolized by Google and Facebook really warped the value of the software engineering profession over other areas.Software is incredibly valuable, but there are other technology areas that are much harder and equally as valuable (if not more so when augmented with good software).A lot of software engineers who only know the last 20 years have inflated egos as results.How many technology experts suddenly became public health experts overnight when COVID-19 hit? And how many of these same people continue to parrot the same bullshit after over 1 million American deaths?\n \nreply",
      "Now we all also get to enjoy measles and rotting teeth as they shut off the fluoride treatments. \nScrewing over your neighbor in service of individualism is the new American way. In the 1960s, people were excited to find ways not to have your neighbors die a premature death. Now those people are too old to give a fuck, and they raised generations of individualist don\u2019t give a fuckers. Free state live free or die trying to fight massive corporate coffers of lawyers.Acquiesced to arbitration agreements, click wrapped with no choice.\n \nreply"
    ],
    "link": "https://josef.cn/blog/uk-talent",
    "first_paragraph": "Back HomeSaturday Aug 3, 2024By Josef ChenImperial, Oxford, and Cambridge produce world-class engineers. Yet post-graduation, their trajectory is an economic tragedy - and a hidden arbitrage opportunity.The Stark Reality:Top London hardware engineer graduates: \u00a330,000-\u00a350,000Silicon Valley equivalent: $150,000+The reality for most graduates is even grimmer:\u00a325,000 starting salaries at traditional engineering firmsExodus to consulting or finance just because it's compensated betterMeanwhile computer science graduates land lucrative jobs in big tech or quant trading, often starting at \u00a3100,000+Examples of wasted potential:Sarah: Built a fusion reactor at 16. Now? Debugging fintech payment systems.James: 3D-printed prosthetic limbs for A-levels. Today? Writing credit risk reports.Alex: Developed AI drone swarms for disaster relief at 18. Graduated with top honours from Imperial. His job? Tweaking a single button's ergonomics on home appliances.These aren't outliers. They're a generation o"
  },
  {
    "title": "The Fuzzing Book (fuzzingbook.org)",
    "points": 161,
    "submitter": "chautumn",
    "submit_time": "2025-01-19T11:57:18 1737287838",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42756286",
    "comments": [
      "I was just reading about these whacky German's yesterday, after investigating a particularly undescriptive (and still available 20 years after it's last update) PKG in pacman. It was the tool/framework/concept that these guys used in the mid to late 90s to fuzz those 2k+ bugs out of Netscape Navigator like they mention in the article. \nRather ingenious that it's fuzzing technique allowed them to not only isolate bugs without really knowing what behavior triggered it, but it would automatically narrow it down to the specific line of code at fault. I'm not really doing it justice, even less so because I can't remember the name of that tool/framework/technique.... Definitely made me rethink my understanding of how under lying systems architecture and code actually interact, and what a \"bug\" truly is. \nSo anyway, tl;dr, a coincidence happened to me.\n \nreply",
      "My 5 cents on fuzzing compilers, and, actually, finding issues [1], also share another work we have done using black box/pentesting security techniques, including fuzzing in Fireblocks [2].[1] https://www.coinfabrik.com/blog/why-the-fuzz-about-fuzzing-c...[2] https://www.coinfabrik.com/blog/fireblocks-api-black-box-rev...\n \nreply",
      "More on compiler correctness in general: https://github.com/MattPD/cpplinks/blob/master/compilers.cor... and fuzzing in particular: https://github.com/MattPD/cpplinks/blob/master/compilers.cor...\n \nreply",
      "Great content, excited to see the book grow :)\n \nreply",
      "Agreed - the authors are the top fuzzing experts.\n \nreply"
    ],
    "link": "https://www.fuzzingbook.org/",
    "first_paragraph": "Welcome to \"The Fuzzing Book\"! \nSoftware has bugs, and catching bugs can involve lots of effort.  This book addresses this problem by automating software testing, specifically by generating tests automatically.  Recent years have seen the development of novel techniques that lead to dramatic improvements in test generation and software testing.  They now are mature enough to be assembled in a book \u2013 even with executable code.You can use this book in four ways:You can read chapters in your browser.  Check out the list of chapters in the menu above, or start right away with the introduction to testing or the introduction to fuzzing.  All code is available for download.You can interact with chapters as Jupyter Notebooks (beta).  This allows you to edit and extend the code, experimenting live in your browser.  Simply select \"Resources \u2192 Edit as Notebook\" at the top of each chapter. Try interacting with the introduction to fuzzing.You can use the code in your own projects.  You can download"
  },
  {
    "title": "Why is Git Autocorrect too fast for Formula One drivers? (gitbutler.com)",
    "points": 167,
    "submitter": "birdculture",
    "submit_time": "2025-01-19T19:20:23 1737314423",
    "num_comments": 100,
    "comments_url": "https://news.ycombinator.com/item?id=42760620",
    "comments": [
      "> Originally, if you typed an unknown command, it would just say \"this is not a git command\".Back in the 70s, Hal Finney was writing a BASIC interpreter to fit in 2K of ROM on the Mattel Intellivision system. This meant every byte was precious. To report a syntax error, he shortened the message for all errors to:    EH?\n\nI still laugh about that. He was quite proud of it.\n \nreply",
      "> EH?I feel like that would also make a good response from the text parser in an old-school interactive fiction game.Slightly related, but I remember some older variants of BASIC using \"?\" to represent the PRINT statement - though I think it was less about memory and more just to save time for the programmer typing in the REPL.\n \nreply",
      "How wasteful, ed uses just ? for all errors,  a 3x saving\n \nreply",
      "Ed also uses \"?\" for \"Are you sure?\" If you're sure, you can type the last command a second time to confirm.The story goes that ed was designed for running over a slow remote connection where output was printed on paper, and the keyboard required very firm presses to generate a signal. Whether this is true or folklore, it would explain a lot.GNU Ed actually has optional error messages for humans, because why not.\n \nreply",
      "/bin/ed did in fact evolve on very slow teletypes that used roll paper.It made the option to print file content with line numbers very useful (personally only used very dumb terminals instead of physical teletype, but experience is a bit similar just with shorter scrollback :D)",
      "I've been sorely tempted to do that with my compiler many times.\n \nreply",
      "I run a wordle spinoff, xordle, which involves two wordle puzzles on one board. This means you can guess a word and get all 5 letters green, but it isn't either of the target words. When you do this it just says \"Huh?\" on the right. People love that bit.\n \nreply",
      "It'd be interesting and amusing if he'd made the private key to his part of Bitcoin a variation on that.RIP.\n \nreply",
      "This seems like really quite bad design.EDIT: 1) is the result of my misreading of the article, the \"previous value\" never existed in git.1) Pushing a change that silently break by reinterpreting a previous configuration value (1=true) as a different value (1=0.100ms confirmation delay) should pretty much always be avoided. Obviously you'd want to clear old values if they existed (maybe this did happen? it's unclear to me), but you also probably want to rename the configuration label..2) Having `help.autocorrect`'s configuration argument be a time, measured in a non-standard (for most users) unit, is just plainly bad. Give me a boolean to enable, and a decimal to control the confirmation time.\n \nreply",
      "For point 1, I think you're misunderstanding the timeline. That change happened in 2008, during code review of the initial patch to add that option as a boolean, and before it was ever committed to the main git tree.\n \nreply"
    ],
    "link": "https://blog.gitbutler.com/why-is-git-autocorrect-too-fast-for-formula-one-drivers/",
    "first_paragraph": "\n\t\t\t\t\tWhy does Git's autocorrect wait 0.1s before executing a mistyped command? Let's dig in.\n\t\t\t\tA while ago, I happened to see a tweet from @dhh where he mistyped a Git command as git pushy and was surprised to notice that Git figured out that he probably meant git push and then gave him 0.1 seconds to verify if that's what he wanted to run before it ran it anyways.As David is a semi-professional race car driver in addition to being a fellow Ruby programming nerd, he naturally noticed that the amount of time that Git afforded him to react was impossible for even Formula One drivers.Of course this seems like a ludicrous bit of Git functionality, but I figured if this was surprising to David, you too might wonder why Git gave him (and possibly gives you) about the length of time that it takes a human eye to blink in order to:What could possibly be the reason to wait 100ms? So little time is essentially equivalent to simply running the command.Well, it's a combination of a misunderstand"
  },
  {
    "title": "Why Twitter is such a big deal (2009) (paulgraham.com)",
    "points": 36,
    "submitter": "Olshansky",
    "submit_time": "2025-01-19T18:38:36 1737311916",
    "num_comments": 56,
    "comments_url": "https://news.ycombinator.com/item?id=42760054",
    "comments": [
      "In early 2000-2007 I felt technology optimism (things like Digg, slashdot) about new websites and there was a hopefulness about new technology (file sharing) The spirit of new technology that \"there is something new\" and the \"this is how things work from now on\" (WAP websites, floppy disks, guest books, simple 1megabyte web hosting, geocities, fan sites, myspace, WhatsApp on cheap phones).In other words, every new thing was something that may have been before but it was \"this is how things work from now on\". The platform defines and upholds the character of interaction. Twitter and Reddit do that and as pg highlights how twitter recipients is by algorithm. (From OP: \"where you don't specify the recipients.\")I have fond memories of writing HTML from magazines and in the eras before me it was handwriting text games into BASIC interpreters.\n \nreply",
      "Around when Elon bought twitter he said (paraphrased) that twitter was the realtime news platform. It\u2019s something I feel like is true in a way that should be true for other social media platforms but isn\u2019t.For example, say I\u2019m in traffic on the highway. Searching 401 might in this example surface tweets from other drivers on the highway talking about traffic and/or posts about an accident they came across.Nothing about this sort of interaction is baked into the protocol as far as I can tell yet FB insta snap etc don\u2019t work this way.\n \nreply",
      "Twitter can\u2019t be a news platform when tweets with links are suppressed\n \nreply",
      "You mean it can't be a link platform.Maybe thats a good thing. It forces content to be posted to X directly instead of click baiting you into ad infested, paywalled, dark pattern websites.The only losers here are legacy media.\n \nreply",
      "> Nothing about this sort of interaction is baked into the protocol as far as I can tell yet FB insta snap etc don\u2019t work this way.Neither does Twitter.Its search is frequently broken to push whatever the new version of their algorithm decides to push. If Musk so wishes your entire feed will be just his rants (something I experienced a few weeks ago).Pre-Musk and pre-algorithm Twitter was a good source of news, as it was near-realtime, and relevant to you. Now? No.\n \nreply",
      "Can you see profiles without logging in again?Something that really pissed me off is how much of a \"support channel\" it became for things like my internet provider. If the internet went down their twitter was often the only place you could get info.\n \nreply",
      "Yeah I hate that kind of \"Well, everyone uses it\", whether it's Twitter or WhatsApp or anything. Even POTS and email are pretty shit in their own way\n \nreply",
      "[flagged]",
      "Who said they were driving?\n \nreply",
      "> say I\u2019m in traffic on the highway. Searching 401Takes a lot of good faith to not think the above implies they\u2019re searching twitter while sitting in traffic in a car they\u2019re driving. They never said \u201cI can ask a passenger to search\u2026\u201d or anything of the sort.\n \nreply"
    ],
    "link": "https://paulgraham.com/twitter.html",
    "first_paragraph": ""
  },
  {
    "title": "Build a Database in 3000 Lines with 0 Dependencies (build-your-own.org)",
    "points": 304,
    "submitter": "not_a_boat",
    "submit_time": "2025-01-16T13:59:34 1737035974",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=42725163",
    "comments": [
      "If you want 1 dependency rather than 0, there are various low-level libraries designed for building full-featured databases.For one of these, RocksDB, one reference point is how CockroachDB was built on top of them for many years and many successful Jepsen tests (until they transitioned to an in-house solution).https://www.cockroachlabs.com/blog/cockroachdb-on-rocksd/https://www.cockroachlabs.com/blog/pebble-rocksdb-kv-store/Another possibility is Apple's FoundationDB, with an interesting discussion here: https://news.ycombinator.com/item?id=37552085\n \nreply",
      "A similar resource I recently discovered and it\u2019s not that popular: https://github.com/pingcap/talent-plan/tree/master/courses/r...Writing a Bitcask(KV wal) like db in Rust. Really cool and simple ideas. The white paper is like 5 pages.\n \nreply",
      "I looked into this at one point, I was typing out entire codebases for didactic purposes: SQLite 3 was 120,000 lines of code, but SQLite 2 was 12,000.So for a bit more effort you get a battle tested real world thing!\n \nreply",
      "The proprietary test suite for SQLite3 is much much larger still.  The battle-testedness comes in great part from that.\n \nreply",
      "Is that where the 10x more lines came from? Writing more \"testable\" code?\n \nreply",
      "Oh no, SQLite3 is a lot more featureful than SQLite2.  The proprietary test suite is what makes SQLite3 so solid.\n \nreply",
      "This makes me wonder. Is anyone practicing TDD with genAI/LLMs? If the true value is in the tests, might as well write those and have the AI slop be the codebase itself. TDD is often criticized for being slow. I'd seriously like to compare one vs the other today. I've also heard people find it challenging to get it to write good tests.\n \nreply",
      "Really puts the auto- in didact! Very curious to hear how this worked for you; it\u2019s almost directly the opposite of the copilot approach.I learned assembler by typing in listings from magazines and hand dis-assembling and debugging on paper. Your approach seems similar in spirit, but who has the times these days?\n \nreply",
      "I program in neovim with no plugins, no autocomplete and no syntax highlighting. I type everything myself (though I will use copy and paste from time to time). There is a discipline to it that I find very beneficial. As a language designer, it also makes me think very carefully about the syntactic burden of languages that I design. It keeps my languages tight. One of the nice things about typing all of my own code without suggestions is that it eliminates many distractions. I may get some things wrong from time to time, but then I only have myself to blame. And I never waste time messing around with broken plugin configs or irritating syntax highlighting nits.It's not for everyone but I love it.\n \nreply",
      "I learned this from Zed Shaw's Learn X The Hard Way books. He says this approach is mainstream in other disciplines, like music, languages, or martial arts.I also heard the philosopher Ken Wilber spent a few years (in what kids today call Monk Mode) writing out great books by hand.The main effect I noticed is that I rapidly gain muscle memory in a new programming language, library or codebase.The other effect is that I'm forced to round-trip every token through my brain, which is very helpful as my eyes tend to glaze over \u2014 often I'll be looking right at an obvious bug without seeing it.\n \nreply"
    ],
    "link": "https://build-your-own.org/blog/20251015_db_in_3000/",
    "first_paragraph": ""
  }
]