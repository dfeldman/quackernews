[
  {
    "title": "Telephone Exchanges in the UK (telephone-exchanges.org.uk)",
    "points": 91,
    "submitter": "petecooper",
    "submit_time": "2025-06-15T19:33:43 1750016023",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=44284466",
    "comments": [
      "If anyone is interested in telephone exchange technology at all, I highly recommend checking out the Connections Museum in Seattle. They have multiple eras of electromechanical switching equipment up and running, and a huge collection of cool old phones, teletypes and payphones. They also have a great YouTube channel with very knowledgeable people.https://www.telcomhistory.org/ConnectionsSeattle.htmlhttps://m.youtube.com/@ConnectionsMuseumI feel like they're not well known and there's no place like it!\n \nreply",
      "Another excellent museum is the Kodiak Military History Museum at Fort Abercrombie, on Kodiak Island, Alaska.It has some old working telephone and teletype systems. You can watch the physical switching equipment do its magic. It is truly awesome. The raw speed and accuracy of the mechanical systems is almost unbelievable.\n \nreply",
      "Don't miss https://www.youtube.com/watch?v=kfOzyIib7wU for some young folk urban exploring what turns out to be a still active exchange, full of ancient and modern tech\n \nreply",
      "I know one guy who make something cool with old Telephone and electronic stuff https://this-museum-is-not-obsolete.com/\n \nreply",
      "You can go and play with an old branch exchange, with all the whistles and er, bells at \"This Museum is (not) Obsolete\". Run by Sam from Look Mum No Computer. If you're ever near Ramsgate in the UK.https://this-museum-is-not-obsolete.com/\n \nreply",
      "This is an impressive feat of cataloguing!Considering the telecom system is at the bedrock of almost all modern technologies, it really doesn't get enough love or attention in the public mind.The dull derelict-looking, and often graffitied, buildings that house the system doesn't reflect just how cool the infrastructure is.\n \nreply",
      "My physics teacher in the 1980s (sadly RIP a few years ago[1]) told me that the location of telephone exchanges was a UK state secret.  The theory was that the Russians would nuke them destroying the country's ability to communicate, but as their location was a secret that outcome could be prevented.  40+ years on, I wonder if any of that was actually true?[1] https://johnchess.blogspot.com/2019/11/david-welch-1945-2019...\n \nreply",
      "The dullness is eerily consistent. Even in the age of privatisation, when everything is a brand, these buildings are devoid of markings. So it might well be true, we just stopped worrying about it once the cold war was officially over (once we realized the Russians already knew everything they needed anyway).\n \nreply",
      "In hindsight, that does seem a little ridiculous; yet it was indeed the thinking.  One could see where the exchanges were by simple dint of visiting a place.  Soviet spies would just have had to walk around a bit.Of course, nuclear weapons wouldn't even have had to specifically target exchanges in order to disrupt electronic communications as they already were by the 1980s.It was amusing to learn a decade ago that the U.S.S.R. military had far more complete maps of many parts of the U.K. than Ordnance Survey published.  Apparently down to Soviet spies just walking around a bit, playing tourist.\n \nreply",
      "Sort of, yes. https://en.wikipedia.org/wiki/BT_Tower#Secrecy\n \nreply"
    ],
    "link": "https://telephone-exchanges.org.uk/",
    "first_paragraph": "Use the drop-down numerical list to find your local STD Telephone Area code:\u00a0Having launched in February 2022 the Telephone Exchanges website is now celebrating it\u2019s third anniversary. The website is an off-shoot of my well established Telephones UK website which was first published in February 2002 and is now over twenty years old. This new Telephone Exchanges website is also built on a WordPress platform offering a fresh new mobile friendly style,\u00a0 higher quality images and with SSL security.The site consists of pages containing photographs and information on telephone exchanges within the UK. I appreciate all the invaluable help and support from many contributors. I am particularly indebted to David Hillas, Simon Cowper-Smith, John Cranston, Ian Duffy, Denver Whiting, Lisa Kinch, Damon Wilkinson, Jaggery, Neil White, Tony Willgoose, Craig Gonsalves, John Howard, Peter Laird, Peter Barrington, David Edge, Toby Lucas, Ian Richardson, Alex Presland, Jake Thomas, David Jones, Chris Mill"
  },
  {
    "title": "First-Ever Supercritical CO2 Circuit Breaker Debuts (ieee.org)",
    "points": 25,
    "submitter": "rbanffy",
    "submit_time": "2025-06-15T23:01:38 1750028498",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44285521",
    "comments": [
      "> The work may be niche, but the impact could be high. About 1 percent of SF6 leaks from electrical equipment. In 2018, that translated to 8,200 tonnes of SF6 emitted globally, accounting for about 1 percent of the global-warming value that year.This figure is for the electricity sector only, not overall global emissions. Still, considering the sheer volume of C02 puffing up from power stations, it's impressive that the normal operation of SF6 breakers accounts for an integer percentage of their GHG impact.----Global emissions were 53 Gt CO2 equivalent in 2023 [0]. 38% of C02 emissions are attributed to the electricity sector in 2023. [1] This figure seems to be strictly C02, not including other GHG, and I can't quickly find a sector-by-sector breakdown for that year. Per IPCC reports in 2022, electricity production and heating accounted for 34% of global GHG in 2019 [2], so for back-of-the-envelope math, it's reasonable.Per the article, the GHG impact of SF6 is 25k CO2, so 8.2k tons SF6 emitted annually is 205 million tons CO2e. This is 0.39% of 53 Gt CO2e (the global value), or nearly exactly 1% of the electricity sector's 38% share.[0] https://www.statista.com/statistics/1285502/annual-global-gr...\n[1] https://www.statista.com/statistics/1129656/global-share-of-...\n[2] https://www.epa.gov/ghgemissions/global-greenhouse-gas-overv...\n \nreply",
      "Thank you for doing these calculations! 0.39% of anthropogenic global warming is surprisingly large, but it doesn't sound like a big impact to me.  I mean, it sounds like about the same greenhouse effect impact as the San Bernardino metropolitan area.\n \nreply",
      "The article's title doesn't seem to be accurate.- It doesn't exist (there is mention of a prototype - but that's not really a debut, all the images in the article are generic).- The $3.9M funding ended May 18th https://arpa-e.energy.gov/programs-and-initiatives/search-al... and there doesn't seem to be a related paper yet (other than this IEEE \"maybe\" from April)- The project is (unfortunately) called TESLA: Tough and Ecological Supercritical Line Breaker for AC 2022: https://www.sf6andalternativescoalition.org/wp-content/uploa...\n \nreply",
      "At first I thought that would be wildly impractical but CO2 goes supercritical at just shy of 90\u00b0F and a little over 1kpsi. Those kinds of pressures are pretty well unknown in a substation, but occur all the time in refineries and the like. It's basically just a pressure vessel with a relatively mild heater requirement. Eminently doable. I think we will probably see these deployed in the next few decades. SF6 is an amazing material but really, really horrible as a greenhouse gas.\n \nreply",
      "For years they had an SF6 tank in my building because somebody had a wind tunnel (the first photo)https://batl.mae.cornell.edu/facilities/the machine is still there but they took out the SF6 tank.\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/sf6-gas-replacement",
    "first_paragraph": "A new high-voltage breaker can clear grid-scale faults without greenhouse gasEmily Waltz is the power and energy editor at IEEE Spectrum.Georgia Tech\u2019s high-voltage circuit breaker quenches arcs with supercritical CO2.Researchers this month will begin testing a high-voltage circuit breaker that can quench an arc and clear a fault with supercritical carbon dioxide fluid. The first-of-its-kind device could replace conventional high-voltage breakers, which use the potent greenhouse gas sulfur hexafluoride, or SF6. Such equipment is scattered widely throughout power grids as a way to stop the flow of electrical current in an emergency.\u201cSF6 is a fantastic insulator, but it\u2019s very bad for the environment\u2014probably the worst greenhouse gas you can think of,\u201d says Johan Enslin, a program director at U.S. Advanced Research Projects Agency\u2013Energy (ARPA-E), which funded the research. The greenhouse warming potential of SF6 is nearly 25,000 times as high as that of carbon dioxide, he notes.If succe"
  },
  {
    "title": "Modifying an HDMI dummy plug's EDID using a Raspberry Pi (downtowndougbrown.com)",
    "points": 183,
    "submitter": "zdw",
    "submit_time": "2025-06-15T16:00:43 1750003243",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=44282998",
    "comments": [
      "Minor note for those wanting to try this at home - these cheap dummy plugs only have a 256 byte eeprom, which is not enough for storing the various extended EDID blocks needed to specify high-refresh high-resolution configs. If you just want 1080p60 they're fine, but you won't be able to simulate a 4k240 monitor with them.Also, some of them have the write-protect line pulled high (or low? don't remember) and you'll need a bit of surgery to actually write to them.\n \nreply",
      "WP high vs low might depend on which chip is used.\n \nreply",
      "One caveat of these dummy plugs is that they don't do HDCP. They handle the typical use case of forcing a specific resolution output for headless machines rather well, but fail for the use case that you need to run something that expects HDCP.This seems a good place to ask: does anyone know of a good solution like this HDMI dummy plug, but that negotiates HDCP? I need to test video streaming apps that require HDCP to play at full resolution, but it is inconvenient to have a full TV for every test.The one solution I've found is an HDMI multiviewer, which seems to negotiate HDCP to each port individually.\n \nreply",
      "I use this HDMI splitter. It lets you either set a preprogrammed EDID, or learns an EDID from whatever you plug into HDMI output 1, and then shows up as a connected monitor for as long as the splitter's plugged in without having to connect anything to the outputs. I believe it negotiaties HDCP between the computer/console/whatever and the splitter, then sends the signal to the output monitor without HDCP. https://www.amazon.com/dp/B07VP37KMB\n \nreply",
      "Aliexpress sells things that claim to terminate hdcp and forward hdmi.  Caveat emptor.\n \nreply",
      "I find it crazy that the signal between our monitors and desktop computers is encrypted when these exist.\n \nreply",
      "Terminating HDCP is difficult, you\u2019d have to downgrade it to HDCP 1.4 and then have a 1.4 \u2018compliant\u2019 (see: device on the end for it to be a dummy monitor. If you need anything newer than HDCP 1.4, it\u2019s likely not possible.\n \nreply",
      "I did a tear down of this Monoprice dongle: https://tomverbeure.github.io/2023/11/26/Monoprice-Blackbird....It terminates as an HDCP 2.0 endpoint and converts to HDCP 1.4. You\u2019d still need an HDCP 1.4 sink to make it work though.\n \nreply",
      "I'm using the Monoprice multiviewer. It negotiates HDCP without a display attached. Other than being a bit big and expensive, and being unable to strip HDCP, it's a good solution.I found the same device in generic packaging on AliExpress, but haven't had the chance to order that version, yet.There are lots of professional SDI converters and such, but they are either $3k+ or \"call for price\".\n \nreply",
      "That was written by you?I don't agree with this section:> The HDCP converter simply announces itself as a final video endpoint\u2026 yet still repeats the content to its output port. Without a very expensive HDMI protocol analyzer, we can\u2019t check if the source is tagging the content as type 0 or type 1, but there is no reason now to think that it\u2019s not type 1.There's no magic in the HDMI protocol that says type 1 vs type 0. Its just another HDCP message over DDC, but it is only sent to repeaters. In this case, since the HDCP Repeater is lying about not being a repeater, it isn't getting sent the StreamID Type information.\n \nreply"
    ],
    "link": "https://www.downtowndougbrown.com/2025/06/modifying-an-hdmi-dummy-plugs-edid-using-a-raspberry-pi/",
    "first_paragraph": "I recently found myself needing to change the monitor that a cheap HDMI \u201cdummy plug\u201d pretended to be. It was a random one I had bought on Amazon several years ago that acted as a 4K monitor, and I needed it to be something simpler that didn\u2019t support a 4K resolution. The story behind why is a long one that I\u2019m still figuring out and might eventually become a separate blog post in the future.If you\u2019re not familiar with dummy plugs, here\u2019s a quick primer: they are tiny dongles you can plug into an HDMI, DVI, etc. port that don\u2019t actually do anything with the video signal. They simply have the minimum circuitry needed for a video source device, like a computer, to think that a monitor is hooked up. In general this entails a pull-up resistor on pin 19 (HPD) to +5V, as well as a little I2C EEPROM chip containing the Extended Display Identification Data (EDID). This is useful for headless machines to force the OS to think a monitor is attached.The EDID contains all the info about the monitor"
  },
  {
    "title": "Twin \u2013 A Textmode WINdow Environment (github.com/cosmos72)",
    "points": 38,
    "submitter": "kim_rutherford",
    "submit_time": "2025-06-15T20:07:27 1750018047",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=44284657",
    "comments": [
      "Reminds me ofhttps://en.m.wikipedia.org/wiki/DESQview and https://en.m.wikipedia.org/wiki/IBM_TopViewIt'd be interesting to try this concept again on the wildly different computers we have now compared to 40 years ago.4k monitors, high speed networks, dozens of cores, things are significantly different - might open some wildly exciting and new possibilities\n \nreply",
      "Also reminds me of https://en.wikipedia.org/wiki/AlphaWindowsAlthough I\u2019ve never succeeded in locating a copy of the spec, any implementations, even a screenshot\u2026 would be great if any of them turned up some day\n \nreply",
      "This should have enough to build either an emulator or a window manager.https://bitsavers.org/pdf/displayIndustryAssociation/AlphaWi...\n \nreply",
      "and Borland Turbovision\n \nreply",
      "I'm a fan of the tiles/patterns from DESQView/Xhttps://news.ycombinator.com/item?id=16044021\n \nreply",
      "I got Desqview/X running about 25 years ago on some Everex.  An X Terminal that ran win16 software - Useless but fascinating.IIRC there was some additional minimal runtime stuff like perl/awk/sed that came with it kinda like MinGW later on\n \nreply",
      "Same experience here. It blew my mind but it wasn't really useable.\n \nreply"
    ],
    "link": "https://github.com/cosmos72/twin",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Text mode window environment. A \"retro\" program for embedded or remote systems, that doubles as X11 terminal and text-mode equivalent of VNC server\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Version 0.9.0Twin is text-based windowing environment with mouse support, window manager,\nterminal emulator, networked clients and the ability to attach/detach\nmode displays on-the-fly.It supports a variety of displays:Currently, twin is tested on Linux (i386, x86_64, ARM, ARM64, PowerPC, Alpha, Sparc),\non Mac OS X (x86_64) and on FreeBSD (i386, x86_64).\nI had yet no chance to seriously test it on other systems.The following screenshot shows an example of twin with various clients:\nTutorial\nA quite complete tour of twin features: the user interface,\nhow to use twin "
  },
  {
    "title": "Canyon.mid (canyonmid.com)",
    "points": 230,
    "submitter": "LorenDB",
    "submit_time": "2025-06-15T13:23:55 1749993835",
    "num_comments": 137,
    "comments_url": "https://news.ycombinator.com/item?id=44282177",
    "comments": [
      "Yes, I know it's just a \"retro looking computer\" to frame a YouTube video but...I had to look up the Tandy 1000 RSX, because it seemed very wrong to have 16-color VGA graphics coming out of a computer labeled as \"Tandy 1000\".Tandy 1000 RSX was the last model from 1991, and it had Super VGA rather than the famous \"Tandy graphics\" that originated with the IBM PCJr.  It did not come with an Adlib or Sound Blaster card, which is what was depicted in the YouTube video.  But the computer did have one ISA slot, and an Adlib or Sound Blaster compatible card could have been installed.It also had a 386 processor rather than the 286 normally found on Tandy 1000 computers, and 1MB of RAM.\n \nreply",
      "VGA on a Tandy 1000 wasn't all that unusual.  Most if not all of the earlier Tandy 1000 models that had ISA slots could take a VGA card in them.  The hardware worked fine (it's just memory bus accesses under 1mb and I/O port instructions), it just depended on software support to do anything with it.  Tandy's magazine PCM often listed and rated add-in VGA cards.  I remember reading of a later version of DeskMate that supported VGA resolution.\n \nreply",
      "The late Lonnie Falk would have been happy to see that PCM did such a good job of covering Radio Shack's computers that it is thought of as Tandy's magazine. Falsoft's line of magazines covering that area probably added at least a few million to Tandy's bottom line.\n \nreply",
      "Why did we kill all that beautiful minimalism? Computers had enough gaming, entertainment and productivity back then. But the definition of \"enough\" kept changing. Like a carrot tied   to stick attached to an animal.\n \nreply",
      "This is more commentary on the nature of personality and taste than of computers.It\u2019s human nature to think of familiar things from our youth as the height of achievement. That was the time of the best music, the best movies, the best culture, the best sports, the best everything. No matter if you were born in 1950 or 1990.\n \nreply",
      "To be fair, the quality of software has dramatically dropped, apps now take 10 seconds to load, memory usage is maxed, games crash and people needed to reinstall their OS so frequently that Microsoft literally added a \"reset PC\" option..You can argue that software does much more than before, sure I agree but no one asked for so much bloat and features in every day apps. My note taking app doesn't need AI.\n \nreply",
      "> To be fair, the quality of software has dramatically dropped, apps now take 10 seconds to load, memory usage is maxed, games crash and people needed to reinstall their OS so frequently that Microsoft literally added a \"reset PC\" option..Are you talking about the 90s or now?  Because those were all at least as true then as now.  Everything took forever.  You needed more RAM every month.  Everything crashed constantly.  I had to reinstall Win98SE so many fucking times that I can still type F73WT-WHD3J-CD4VR-2GWKD-T38YD from memory.The amount of suck in commercial software is constant.  Companies always prioritize adding the shiny-looking features that sell software to rubes over improving things like memory use, response time, and general quality of life until the quality of life is actually bad enough to drive customers to another vendor, so it's perpetually bad enough to keep the average customer right on the edge of \"oh fuck this, I'm switching to something else.\"\n \nreply",
      "Software crashed all the time back then.  Hitting control-S was one of those things you just did by habit so as to not lose too much work when it happened.\n \nreply",
      "Apps take 10 seconds to load? Which apps, on what system?My whole machine reboots in less than 10 seconds. I haven\u2019t seen a blue screen of death in a decade. I haven\u2019t had significant data loss from a failed drive or a corrupt machine in.. I can\u2019t remember. Even DaVinci Resolve is ready to run in a few short seconds.This is all on a machine I bought 6-8 years ago. I reboot my phone and watch and laptop when I think to, not because I have to. I run half a dozen browsers and hundreds of tabs and play YouTube while waiting for a remote machine to deploy to an immutable temp instance that gets destroyed after every test cycle.I speak to my AIs and I can live and work anywhere on this planet that legally allows me.There are problems in our world and on our machines and in our governments but apps don\u2019t take 10 seconds to load.Except ServiceNow. I\u2019ll give you that one.\n \nreply",
      "Gimp. On any system.\n \nreply"
    ],
    "link": "https://canyonmid.com/",
    "first_paragraph": ""
  },
  {
    "title": "Childhood leukemia: how a deadly cancer became treatable (ourworldindata.org)",
    "points": 157,
    "submitter": "surprisetalk",
    "submit_time": "2025-06-15T13:12:42 1749993162",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=44282143",
    "comments": [
      "My son was diagnosed with B-ALL (RUNX1) in 2020.I don't want to go deep into it today (Fathers Day here), but he's alive and well now. We rang the bell just over two years ago.He is enrolled in a study through the Children's Oncology Group mentioned here, and underwent an experimental modification specifically for male patients.Specifically, the current state of treatment protocols (when he started) was that males received an extra ~6 months of treatment as there was thought that the testes could serve as a repository for the cancer. The data says that is likely not true, and that the tradeoff for the longer chemo is worse than any risk present.We were fortunate that he hit every single \"best case\", from him being diagnosed very early, to all of the best possible results from his blood tests at every point.Many of my comments talking about my experience are buried, but there is plenty that I've said here on HN.\n \nreply",
      "Im going to think about this comment a lot today. Not in a \u201cthoughts & prayers\u201d way, just because this makes me so happy & proud for humanity, that we can do actually worthwhile things like this together. That\u2019s the kind of stuff I want to think about on my silly made-up Dad holiday. Cheers!\n \nreply",
      "CONGRATS to you and your son. And Happy Father's Day! :)\n \nreply",
      "My dad started his work as a Pediatric Hematologist Oncologist in the late 60s. He had a firm belief that cure rates could and would climb as a result of research and better clinical care. He spent his life pursuing both.When people would ask him how he managed to stay so positive - he was one of the happiest people I\u2019ve ever known - he\u2019d reference the trends highlighted in this article.That didn\u2019t change how hard it was when he lost a patient, but I know he always had his eyes and his mind on the future.This is an incredible example of science and medicine. Thanks OP for posting it.\n \nreply",
      "Sounds like you already know this, but your dad's a hero. Infinite respect for the folks who dedicate their lives to helping others.\n \nreply",
      "Thanks for saying that.Here's a bit more about him from the obituary my sister wrote: https://www.northjersey.com/obituaries/pnys1147090\n \nreply",
      "what a guy, your dad was a great person\n \nreply",
      "Thank you.\n \nreply",
      "It's hard to imagine the emotional weight of working in pediatric oncology back then, when outcomes were so bleak\n \nreply",
      "I don't fully understand how he did it. I know he took a lot from the line in the Talmud that said \"whoever saves a life, it is considered as if he saved an entire world.\"My mom says that his baseline was incredibly high and that he was incredibly resilient. He also had a big rebellious streak, an analytical mind, and endless compassion.\n \nreply"
    ],
    "link": "https://ourworldindata.org/childhood-leukemia-treatment-history",
    "first_paragraph": ""
  },
  {
    "title": "How to modify Starlink Mini to run without the built-in WiFi router (olegkutkov.me)",
    "points": 255,
    "submitter": "LorenDB",
    "submit_time": "2025-06-15T12:40:07 1749991207",
    "num_comments": 70,
    "comments_url": "https://news.ycombinator.com/item?id=44282017",
    "comments": [
      "Fascinating that they chose to use modulated board-to-board Ethernet instead of just running RGMII from MAC to MAC.\n \nreply",
      "Ethernet seems far easier to prototype with. There's almost no off the shelf stuff for talking to RGMII whereas Ethernet you can just plug into your laptop for testing. If it's two different teams building things it seems like it would be a lot easier to just agree on Ethernet as the interface and then delay integration testing or release earlier.\n \nreply",
      "RGMII is not some obscure competitor to Ethernet, but rather, Ethernet was designed to be a modular two-part design with \"MAC\" and \"PHY\" chips connected via \"MII\" interface. RGMII is simply the latest version of it.Many Ethernet-supported SoCs still use various MII style interfaces because it makes more sense to outsource the physical layer to some external chip especially if not everyone is going to use Ethernet.It's perhaps like the difference between using Thunderbolt vs raw PCIe. You technically shouldn't need Thunderbolt if you're just permanently connecting two things inside a same machine.Is it smarter to do it proper and make it silicon efficient than just shipping the darn thing ASAP? idk. We'll see.\n \nreply",
      "RGMII requires way more work to run board-to-board (heaps of signals, quite precise length matching, impedance control, etc. on the boards, better board-to-board connectors etc.) and at the end of all that will likely be less robust than just running Ethernet. I'd much rather use SGMII just because it's far fewer signals to match (even if it runs way faster) instead of RGMII.The chips they're using might already have Ethernet PHYs built in anyway which might also be part of the reason they're using Ethernet.\n \nreply",
      "A $3 breakout PCB with an RGMII PHY and MagJack on it would solve this problem without resorting to analog communication.\n \nreply",
      "Assembly isn't free, either an engineer or the PCB fabricator has to put that together. Also the design isn't free and it's certainly not necessarily going to match the behavior of the device on the other side.But your laptop's Ethernet adapter comes free with your laptop (both in terms of money and waiting to get it since it's already on your desk) and possibly even more importantly you know the laptop manufacturer and users have QAed it for you so it's absolutely going to behave the way you expect which is important when the device you're designing isn't behaving.\n \nreply",
      "> Assembly isn't free, either an engineer or the PCB fabricator has to put that together> your laptop's Ethernet adapterThe device as-designed likely wouldn't work with your laptop's ethernet adapter - hence why the author of TFA placed an isolation transformer and jack ...on a breakout board.\n \nreply",
      "Heh I didn't notice it didn't have the isolation transformer. That is odd.\n \nreply",
      "A lot of this is pretty POC-y. Agree digital to analog to analog to digital is kinda inefficient, and in the abstract MAC to PHY (which is probably what you mean when you say MAC to MAC) with RGMII is probably better. My off the cuff guess is that it is likely the written-up interface is easier to access or requires less diving into internals. Not sure where the RGMII lines are, and depending on the design of the Starlink mini itself (I am ignorant of this) the lines might have been buried deeper and less accessible, who knows.\n \nreply",
      "Sure, then you get to write an MDIO emulator so that it actually detects link and since it's a proprietary system you can't exactly comment out the SMI code.. fun!\n \nreply"
    ],
    "link": "https://olegkutkov.me/2025/06/15/how-to-modify-starlink-mini-to-run-without-the-built-in-wifi-router/",
    "first_paragraph": " The Starlink Mini terminal is designed as a compact, all-in-one solution with an integrated Wi-Fi router. While this design is ideal for typical consumer use, certain applications\u2014such as custom networking setups, embedded installations, or power-constrained environments\u2014may benefit from removing the internal router entirely. In this article, I\u2019ll detail the process of physically removing the built-in Wi-Fi router board from the Starlink Mini, allowing the terminal to operate solely via Ethernet and offering greater flexibility for advanced users.Please note that this modification applies only to the Starlink Mini 1 (as of June 14, 2025). Hardware changes in future models, such as the expected Mini 2, may render this process invalid.The disassembly process requires patience and accuracy. I recommend using metal spudgers and a plastic prying tool.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Additionally, you will need a thin, flexible knife or a thin metal wire to remove the router\u2019s PCB.I prepared a video manual about the"
  },
  {
    "title": "First 2D, non-silicon computer developed (psu.edu)",
    "points": 66,
    "submitter": "giuliomagnifico",
    "submit_time": "2025-06-12T18:26:43 1749752803",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=44261118",
    "comments": [
      "> at frequencies up to 25 kilohertzHow high could this technique go?\n \nreply",
      "https://arstechnica.com/science/2019/08/16-bit-risc-v-proces...Modern microprocessor built from complementary carbon nanotube transistors\nhttps://www.nature.com/articles/s41586-019-1493-8\n \nreply",
      "> molybdenum disulfide for n-type transistors and tungsten diselenide for p-type transistors  \n\nIsn't this rather unusual?\n \nreply",
      "Yes? But it\u2019s been in research for a decade or two, based on a quick search.It\u2019s confusing to me because moly d is a very common lubricant, even for home uses.\n \nreply",
      "A small step towards Sophons\n \nreply",
      "Well with all the sabre-rattling by Kratsios on space time control, Sophons is not that far fetched.\n \nreply",
      "Well\u2014I, for one, welcome our new Trisolaran overlords!\n \nreply",
      "WTF is up with that illustration at the top of the article?\n \nreply",
      "Some attempt to visually represent molybdenum disulfide and tungsten diselenide with the keys of a QWERTY keyboard.\n \nreply",
      "Which if it was done properly would have WSe2 and MoS2 rather than seemingly random keys\n \nreply"
    ],
    "link": "https://www.psu.edu/news/research/story/worlds-first-2d-non-silicon-computer-developed",
    "first_paragraph": "This conceptual illustration of a computer based on 2D molecules displays an actual scanning electron microscope\u00a0image of the computer fabricated by a team by researchers at Penn State. The\u00a0keyboard features highlighted keys labeled with the abbreviations for molybdenum disulfide\u00a0and tungsten diselenide,\u00a0representing the two 2D materials used to develop the transistors in the computer.\u00a0\u00a0Credit: Krishnendu Mukhopadhyay/Penn State. All Rights Reserved.June 11, 2025By Ashley WennersHerronUNIVERSITY PARK, Pa. \u2014 Silicon is king in the semiconductor technology that underpins smartphones, computers, electric vehicles and more, but its crown may be slipping, according to a team led by researchers at Penn State. In a world first, they used two-dimensional (2D) materials, which are only an atom thick and retain their properties at that scale, unlike silicon, to develop a computer capable of simple operations.The development, published today (June 11) in\u00a0Nature, represents a major leap toward the"
  },
  {
    "title": "Why SSL was renamed to TLS in late 90s (2014) (dierks.org)",
    "points": 154,
    "submitter": "Bogdanp",
    "submit_time": "2025-06-15T14:10:37 1749996637",
    "num_comments": 67,
    "comments_url": "https://news.ycombinator.com/item?id=44282378",
    "comments": [
      "The situation is additionally confused by the fact that the version numbers do not give a good clue to how different the protocols were. Specifically:SSLv2 was the first widely deployed version of SSL, but as this post indicates, had a number of issues.SSLv3 is a more or less completely new protocolTLS 1.0 is much like SSLv3 but with some small revisions made during the IETF standardization process.TLS 1.1 is a really minor revision to TLS 1.0 to address some issues with the way block ciphers were used.TLS 1.2 is a moderately sized revision to TLS 1.1 to adjust to advances in cryptography, specifically adding support for newer hashes in response to weaknesses in MD5 and SHA-1 and adding support for AEAD cipher suites such as AES-GCM.TLS 1.3 is mostly a new protocol though it reuses some pieces of TLS 1.2 and before.Each of these protocols has been designed so that you could automatically negotiate versions, thus allowing for clients and servers to independently upgrade without loss of connectivity.\n \nreply",
      "Curious, when you tell someone they need to access a website securely (or any other case where you might use the term TLS or SSL), do you:1. Say SSL or TLS?2. How old are you (or did you start working before 1999?)I'll reply with my answer too.\n \nreply",
      "1. SSL.  For a long time I didn't even know TLS was the \"same thing\", but even now that I know it is, I still say SSL 9 times out of 10.2. 38 - Started working in 2011, but my first forays into network programming was in something like 2004-2005.Looked over onto my other screen and sure enough the function I'd literally minutes before added an if statement to went        public Builder sslCertNotBefore(Instant sslCertNotBefore) {\n            if (sslCertNotBefore.isAfter(MAX_UNIX_TIMESTAMP)) {\n                sslCertNotBefore = MAX_UNIX_TIMESTAMP;\n            }\n            this.sslCertNotBefore = sslCertNotBefore;\n            return this;\n        }\n\nI think possibly part of the problem is that we as programmers typically don't deal with TLS directly.  The code above is part of a system I wrote that extracts detailed certificate information from HTTPS connections, and man was it ever a hassle to wrestle all the information I was interested in out of the java standard library.Sure on the one hand it's easier to not mess up if it's all automatic and out of sight, but at the same time, it's not exactly beneficial to the spread of deeper awareness of how TLS actually works when it's always such a black box.\n \nreply",
      "I usually say SSL, because it has a greater chance of being understood than the more correct TLS (nobody uses SSL 3.0 anymore). It's also in the name of many SSL (I mean, TLS) libraries, like the classic OpenSSL.But yeah, I learned about SSL back in the crypto wars days of the 1990s, back when you had to pirate the so-called \"US only\" version of Netscape if you wanted decent SSL encryption, so I might be just using the old term out of habit.\n \nreply",
      "These days I tend to say \"TLS\" more and more, but until just a year or two ago it was almost always \"SSL\". And \"SSL\" still slips out occasionally.I'm 51, started working in IT in the mid 90's.\n \nreply",
      "I say \"https\" because sometimes even regular people know what that means.\n \nreply",
      "Reflex is to say SSL but usually correct myself to TLS. Started in IT in 2006 (was a nerd a few years before that though)\n \nreply",
      "I second this, started around the same time.\n \nreply",
      "(1) SSL(2) 37. I've been an Internet user since ~1995 and been working in tech since 2004.\n \nreply",
      "TLS. 1989.Even today, people and marketing pages promote \"SSL\" term. Unless you specifically google, \"What is the deference between SSL and TLS?\" most people would have no idea what TLS is.\n \nreply"
    ],
    "link": "https://tim.dierks.org/2014/05/security-standards-and-name-changes-in.html",
    "first_paragraph": "\nThe Netscape/Microsoft browser wars in the mid-90's were really vicious and competitive. They really had it out for each other.Netscape had developed the SSL protocol. The initial version had cryptographic flaws and was broken pretty quickly, and never released. The first production version was SSL 2, which was in use for a few years. (I don't know the exact versions of Navigator it shipped in.)SSL 2 had some flaws, both cryptographic and practical; not dramatic enough to make replacing it a crisis, but it clearly needed some work from early on.As a part of the cutthroat competition, Microsoft decided to revise the SSL 2 protocol with some additions of their own, and specified a protocol called \"PCT\" that was derived from SSL 2. It was only supported in IE and IIS.Netscape also wanted to address SSL 2 issues, but wasn't going to let Microsoft take leadership/ownership in the standard, so they developed SSL 3.0, which was a more significant departure.Various people in the industry & co"
  },
  {
    "title": "Datalog in miniKanren (deosjr.github.io)",
    "points": 80,
    "submitter": "deosjr",
    "submit_time": "2025-06-15T16:16:08 1750004168",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=44283093",
    "comments": [
      "Seems like interest in Datalog is high this week, so I thought I'd share a write-up of a minimal Datalog implementation I did a while ago.Runs in the browser using Hoot (https://spritely.institute/hoot/) which compiles Guile Scheme to WebAssembly.\n \nreply",
      "Datalog is a syntactic subset of Prolog[1], which this is... not.I think the most misunderstood thing about Prolog (and Datalog, the functor-free subset of pure Prolog) is that the syntax is really, really important.It's like, the whole gimmick of the language. It is designed to efficiently and elegantly query and transform itself.  If you lose the syntax you lose all of intermediate and advanced Prolog (and Datalog).[1]: https://en.m.wikipedia.org/wiki/Datalog\n \nreply",
      "Semantics are more important than syntax.  Prolog's flexible syntax is a nice-to-have rather than essential when you're in Lisp.  And Datalog is purely first-order, so the advanced Prolog you're talking about doesn't exist in it.However, syntax does matter, and this is not acceptable    (dl-find \n     (fresh-vars 1 \n      (lambda (?id) \n       (dl-findo dl\n        ((,?id reachable ,?id)))))))\n\nas a way to ask    reachable(Id, Id).\n\nI think you could, however, write a bit more Scheme and be able to ask    (?id reachable ?id)\n\nwhich would be acceptable.However, the ordering betrays a deeper semantic difference with orthodox Datalog, which is about distinct N-ary relations, like a relational database, not binary relations.  This implementation seems to be specific to binary relations, so it's not really Datalog for reasons that go beyond mere syntax.On the other hand, this (from the initial goal) would be perfectly fine:    (dl-rule! dl (reachable ,?x ,?y) :- \n                     (edge ,?x ,?z) (reachable ,?z ,?y))\n\nThe orthodox Datalog syntax is:    reachable(X, Y) :- edge(X, Z), reachable(Z, Y).\n \nreply",
      "Shouldn\u2019t lisp macros make it easy to present such a nice syntax? Perhaps the author could easily implement that bit, if not the wide rows. Or is that the point you\u2019re making?There is a dl-rule here: https://github.com/deosjr/deosjr.github.io/blob/15b5f7e02153...\n \nreply",
      "I don't think you need Lisp macros for it; you could use just a regular Lisp function. I don't think the standard R5RS macros are powerful enough to grovel over the query expression to make a list of the free variables, but then, standard Scheme also doesn't have records.  I think Guile has a procedural macro system that you could use, but I don't think it would be a good idea.Yes, I think the semantic divergence is more fundamental. Triple stores and graph databases and binary relations are awesome, but they aren't what Datalog is.\n \nreply",
      "What scheme is this?\n \nreply",
      "Guile Scheme. See https://github.com/deosjr/deosjr.github.io/blob/master/dynam... for more.\n \nreply"
    ],
    "link": "https://deosjr.github.io/dynamicland/datalog.html",
    "first_paragraph": "\n      A browser with Wasm GC and tail call support is required for\n      this demo.  We recommend using either Mozilla Firefox or Google\n      Chrome.\n    Having access to an embedded logical programming language makes some tasks really easy.\n       One prerequisite for RealTalk is some form of Datalog, and I built one in Scheme using miniKanren so that I had access to all of the internals.\n       This page explains the naive Datalog implementation I did before modifying some of it to fit my version of Dynamicland.As a typical example we can look at a directed graph with five vertices labeled a through e.\n    Using Datalog, we introduce these vertices as separate records, establish facts about directed edges between them, and then introduce rules about what it means for a vertex to be reachable from another vertex.\n    In the end we manually trigger fixpoint analysis and run a query against the resulting database of initial and derived facts.This is roughly the syntax that I want to g"
  },
  {
    "title": "Simplest C++ Callback, from SumatraPDF (kowalczyk.info)",
    "points": 72,
    "submitter": "jandeboevrie",
    "submit_time": "2025-06-15T17:26:38 1750008398",
    "num_comments": 60,
    "comments_url": "https://news.ycombinator.com/item?id=44283614",
    "comments": [
      "Don't know about the code subtilities, but SumatraPDF is a gift for viewing PDF on MS Windows.\nSo big thanks to the author !\n \nreply",
      "Out of curiosity, what's your use case for it?  Years ago I preferred Sumatra/Foxit to Adobe, but every major browser has supported rendering PDFs for at least a decade and I haven't had needed or wanted a dedicated PDF reader in all that time.\n \nreply",
      "It's smaller, lighter and much faster than launching a web browser to view a PDF.  I can configure it to open a new instance for each PDF which is nice if you need to have several docs open at once.  Again, nothing that you can't do with a browser and dragging tabs, but I prefer this.\n \nreply",
      "Part of why I use SumatraPDF is that it automatically reloads its view when the files change (at least for PDFs, I haven't tested on the other file types it supports).\n \nreply",
      "I'm not a C++ programmer, but I was under the impression that closures in c++ were just classes that overload the function call operator `operator()`. So each closure could also be implemented as a named class. Something like:    class OnListItemSelected {\n        OnListItemSelectedData data;\n\n        void operator()(int selectedIndex) { ... }\n    }\n\nPerhaps I'm mistaken in what the author is trying to accomplish though?\n \nreply",
      "Indeed, that is exactly the case, lambdas are essentially syntax sugar for doing this.The one thing the author's solution does which this solution (and lambdas) does not is type erasure: if you want to pass that closure around, you have to use templates, and you can't store different lambdas in the same data structure even if they have the same signature.You could solve that in your case by making `void operator()` virtual and inheriting (though that means you have to heap-allocate all your lambdas), or use `std::function<>`, which is a generic solution to this problem (which may or may not allocate, if the lambda is small enough, it's usually optimized to be stored inline).I get where the author is coming from, but this seems very much like an inferior solution to just using `std::function<>`.\n \nreply",
      "> though that means you have to heap-allocate all your lambdasI think whether or not you have to allocate from the heap depends on the lifetime of the lambda. Virtual methods also work just fine on stack-allocated objects.\n \nreply",
      "Exactly! And if you need type erasure, you can just store it in a std::function.> OnListItemSelectedData data;In this case you can just store the data as member variables. No need for defining an extra class just for the data.As I've written elsewhere, you can also just use a lambda and forward the captures and arguments to a (member) function. Or if you're old-school, use std::bind.\n \nreply",
      "Main issue author had with lambdas is autogenerated names in crash reports\n \nreply",
      "I don\u2019t really understand what problem this is trying to solve and how the solution is better than std::function. (I understand the issue with the crash reports and lambdas being anonymous classes but not sure how the solution improved on this or how std::function has this problem?)I haven\u2019t used windows in a long time but back in the day I remember installing SumatraPDF to my Pentium 3 system running windows XP and that shit rocked\n \nreply"
    ],
    "link": "https://blog.kowalczyk.info/a-stsj/simplest-c-callback-from-sumatrapdf.html",
    "first_paragraph": "\nFeedback about page:\n\n\nFeedback:\n\nOptional: your email if you want me to get back to you: \n\n\n\n                Send Feedback\n            \n\n                Cancel\n            \n"
  },
  {
    "title": "Datalog in Rust (github.com/frankmcsherry)",
    "points": 237,
    "submitter": "brson",
    "submit_time": "2025-06-15T11:18:40 1749986320",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=44281727",
    "comments": [
      "It's funny seeing this as the top story.I'm in the middle of putting together a realtime strategy game using Differential Datalog[1] and Rust, with DDL managing the game's logic. Mostly as an excuse to expose myself to new ideas and engage in a whole lot of yak shaving.[1] https://github.com/vmware-archive/differential-datalog\n \nreply",
      "On, nice!I'll be interested in reading how this goes!\n \nreply",
      "Very cool, I'm curious to see what the state of that implementation is and how far you get, since DDLog is not being actively maintained anymore.\n \nreply",
      "I made some progress porting mangle datalog to Rust https://github.com/google/mangle/tree/main/rust - it is in the same repo as the golang implementation.It is slow going, partly since it is not a priority, partly because I suffer from second system syndrome. Mangle Rust should deal with any size data through getting and writing facts to disk via memory mapping. The golang implementation is in-memory.This post is nice because it parses datalog and mentions the LSM tree, and much easier to follow than the data frog stuff.There are very many datalog implementations in Rust (ascent, crepe) that use proc-macros. The downside is that they won't handle getting queries at runtime. For the static analysis use case where queries/programs are fixed, the proc macro approach might be better.\n \nreply",
      "It is nice to see a core group of Datalog enthusiasts persist, even though the current Datalog revival seems to be on the decline. The recent Datalog 2.0 conference was quite small compared to previous years and the second HYTRADBOI conference was very light on Datalog as well, while the first one had a quarter of submissions with Datalog connection.I'm encouraged by the other commenters sharing their recent Datalog projects. I am currently building a set of data quality pipelines for a legacy SQL database in preparation of a huge software migration.We find Datalog much more useful in identifying and looking for data quality issues thatn SQL, as the queries can be incredibly readable when well-structured.\n \nreply",
      "No offense, but I wouldn't take Datalog 2.0's small attendance as an exemplar of Datalog's decline, even if I agree with that high-level point. Datalog 2.0 is a satellite workshop of LPNMR, a relatively-unknown European conference that was randomly held in Dallas. I myself attended Datalog 2.0 and also felt the event felt relatively sparse. I also had a paper (not my primary work, the first author is the real wizard of course :-) at the workshop. I myself saw relatively few folks in that space even attending that event--with the notable exception of some European folks (e.g., introducing the Nemo solver).All of this is to say, I think Datalog 2.0's sparse attendance this year may be more indicative of the fact that it is a satellite workshop of an already-lesser-prestigious conference (itself not even the main event! That was ICLP!) rather than a lack of Datalog implementation excitement.For what it's worth, none of what I'm saying is meant to rebut your high-level point that there is little novelty left in implementing raw Datalog engines. Of course I agree, the research space has moved far beyond that (arguably it did a while ago) and into more exotic problems involving things like streaming (HydroFlow), choice (Dusa), things that get closer to the general chase (e.g., Egglog's chase engine), etc. I don't think anyone disagrees that vanilla Datalog is boring, it's just that monotonic, chain-forward saturation (Horn clauses!) are a rich baseline with a well-understood engineering landscape (esp in the high-performance space) to build out more interesting theories (semirings, Z-sets, etc..).\n \nreply",
      "I like the author's datalog work generally, but I really wish his introductory material did not teach using binary join, which I found to get very messy internally as soon as you get away from the ideal case. I found the generic join style methods to be much, much simpler to generalize in one's head (see https://en.wikipedia.org/wiki/Worst-case_optimal_join_algori...).\n \nreply",
      "related: McSherry's preceding blog post was all about demonstrating how binary joins can achieve worst-case optimal runtime, given suitable adjustments to the query plan.- https://github.com/frankmcsherry/blog/blob/master/posts/2025...\n \nreply",
      "For materialization-heavy workloads (program analysis, etc.), we often find that optimized binary join plans (e.g., profile-optimized, hand-optimized, etc.) beat worst-case optimal plans due to the ability to get better scalability (less locking) without the need to use a trie-based representation. Within the space of worst-case optimal plans, there are still lots of choices: but a bad worst-case optimal plan can often beat a bad (randomly-chosen) binary plan. And of course (the whole point of this exercise), there are some queries where every binary plan explodes and you do need WCOJ. There's also some work on making more traditional binary joins robust (https://db.in.tum.de/people/sites/birler/papers/diamond.pdf), among other interesting work (https://arxiv.org/html/2502.15181v1). Effectively parallelizing WCOJs is still an open problem as far as I am aware (at least, this is what folks working on it tell me), but there are some exciting potential directions in tackling that that several folks are working on I believe.\n \nreply",
      "\"I, a notorious villain, was invited for what I was half sure was my long-due comeuppance.\" -- Best opening line of a technical blog post I've read all year.The narrator's interjections were a great touch. It's rare to see a post that is this technically deep but also so fun to read. The journey through optimizing the aliasing query felt like a detective story. We, the readers, were right there with you, groaning at the 50GB memory usage and cheering when you got it down to 5GB.Fantastic work, both on the code and the prose.\n \nreply"
    ],
    "link": "https://github.com/frankmcsherry/blog/blob/master/posts/2025-06-03.md",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page."
  },
  {
    "title": "DARPA program sets distance record for power beaming (darpa.mil)",
    "points": 7,
    "submitter": "gnabgib",
    "submit_time": "2025-06-15T22:40:40 1750027240",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.darpa.mil/news/2025/darpa-program-distance-record-power-beaming",
    "first_paragraph": "In a series of recent tests in New Mexico, the Persistent Optical Wireless Energy Relay (POWER) program achieved several new records for transmitting power over distance. The team recorded more than 800 watts of power delivered during a 30-second transmission from a laser 8.6 kilometers (5.3 miles) away. Over the course of the test campaign, more than a megajoule of energy was transferred.Previously, the greatest reported distance records for an appreciable amount of optical power (>1 microwatt) were 230 watts of average power at 1.7 kilometers for 25 seconds and a lesser (but undisclosed) amount of power at 3.7 kilometers.\u00a0\u201cIt is beyond a doubt that we absolutely obliterated all previously reported optical power beaming demonstrations for power and distance,\u201d said POWER Program Manager Paul Jaffe after the results were confirmed. The DARPA-led team brought together industry and government, including the U.S. Naval Research Laboratory and the High Energy Laser Systems Test Facility (HE"
  },
  {
    "title": "David Attenborough at 99: 'I will not see how the story ends' (thetimes.com)",
    "points": 100,
    "submitter": "herbertl",
    "submit_time": "2025-06-15T21:21:28 1750022488",
    "num_comments": 49,
    "comments_url": "https://news.ycombinator.com/item?id=44285054",
    "comments": [
      "https://archive.md/CFCFn",
      "I watched David Attenborough's recent film 'Ocean' on a big screen. The footage of bottom trawling was really shocking. I don't understand how that has been allowed to continue in UK coastal waters, let alone to be subsidised in marine protected areas. Madness. It's like napalming a forest to get a few deer. Thankfully things may be changing:https://www.gov.uk/government/news/government-proposes-to-ex...Don't know how much of that was due to the film.\n \nreply",
      "Greenpeace used to drop boulders into the ocean to prevent bottom trawling circa 2021-2022. Unsure if they still do. Fairly straightforward to solve for if you\u2019re willing to drop chunks of rock and concrete in the ocean at the right spots.Bans are nice, destructive force against adversaries works better though. Hard to take the selfish out of the human, so you have to engineer systems accordingly.https://www.greenpeace.org.uk/news/live-greenpeace-boulders-...\n \nreply",
      "The relevant excerpt. [0][0] https://youtu.be/IzG9AwlypaY?feature=shared\n \nreply",
      "Watch it in a cinema, to get the full effect.There are some before and after scenes of the sea bed, which are pretty shocking as well.I'm not sure how that got that footage. Surely fisherman would not want that to be seen?\n \nreply",
      "Found this:\"Technically, probably the hardest thing was trying to film bottom trawling because it's never been filmed before and we didn't know if it was possible. You have to film the wonder but you also have to film the destruction. Capturing that was absolutely essential and it took a lot of research to find some scientists planning bottom trawling experiments who decided that adding cameras would help their research and also help to share it with the world.\"At:https://www.arksen.com/blogs/news/ocean-with-david-attenboro...\n \nreply",
      "I watched this film last night, and it was stunning and horrifying all at once. It really brings home the impact of industrial-scale trawling on the marine environment. It's literally like bulldozing a garden to harvest the fruit.\n \nreply",
      "> subsidised in marine protected areasWhat do you mean?\n \nreply",
      "https://en.wikipedia.org/wiki/Marine_protected_area> A marine protected area (MPA) is a protected area of the world's seas, oceans, estuaries or in the US, the Great Lakes. These marine areas can come in many forms ranging from wildlife refuges to research facilities. MPAs restrict human activity for a conservation purpose, typically to protect natural or cultural resources. Such marine resources are protected by local, state, territorial, native, regional, national, or international authorities and differ substantially among and between nations. This variation includes different limitations on development, fishing practices, fishing seasons and catch limits, moorings and bans on removing or disrupting marine life. MPAs can provide economic benefits by supporting the fishing industry through the revival of fish stocks, as well as job creation and other market benefits via ecotourism. The value of MPA to mobile species is unknown.\n \nreply",
      "I think that not seeing how the story ends will be a blessing in disguise.(I do not share his optimism that we fix this, the forces of Line Must Go Up are going to win... at least until we all rapidly lose)\n \nreply"
    ],
    "link": "https://www.thetimes.com/life-style/celebrity/article/david-attenborough-book-extract-age-99-lj3rd2fg7",
    "first_paragraph": "Q: My earliest memory of the ocean is of a tropical lagoon. Ammonites rose and fell in the warm water column, occasionally propelling themselves forwards, their curled ram\u2019s horn shells surprisingly streamlined in the water. This tropical lagoon was in fact in my imagination, fired as I explored the old limestone quarry near my childhood home in Leicester, some 60 miles from the coast. For a small boy in the 1930s this was a marvellous place for adventures, and the knowledge that millions of years ago it would have been a warm and wild lagoon only increased its appeal. Here I could spend days searching for treasure buried in rocks laid down in ancient tropical seas. Holding the fossils of long-dead sea creatures that I had chipped out of the rock, knowing my eyes were the first ever to see them, ignited my curiosity. I would spend much of the rest of my life wondering what lived below the surface of the ocean.I have been fortunate enough to live for nearly 100 years. During this time w"
  },
  {
    "title": "Chemical knowledge and reasoning of large language models vs. chemist expertise (nature.com)",
    "points": 9,
    "submitter": "bookofjoe",
    "submit_time": "2025-06-14T10:22:06 1749896526",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44275471",
    "comments": [
      "Nothing to see here unless you have some kind of unsatisfied interest in the future of AI :\\This is all highly academic, and I'm highly industrial so take this with a grain of salt.  Sodium salt or otherwise, your choice ;)If you want things to be accomplished at the bench, you want any simulation to be made by those who have not been away from the bench for that many decades :)Same thing with the industrial environment, some people have just been away from it for too long regardless of how much familiarity they once had.  You need to brush up, sometimes the same plant is like a whole different world if you haven't been back in a while.\n \nreply",
      "BASF Group - will they speak in public? probably not, given what is at stake  IMHO\n \nreply"
    ],
    "link": "https://www.nature.com/articles/s41557-025-01815-x",
    "first_paragraph": "Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\n            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\n            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\n            and JavaScript.Advertisement\nNature Chemistry\n\n                         (2025)Cite this article\n14k Accesses78 AltmetricMetrics detailsLarge language models (LLMs) have gained widespread interest owing to their ability to process human language and perform tasks on which they have not been explicitly trained. However, we possess only a limited systematic understanding of the chemical capabilities of LLMs, which would be required to improve models and mitigate potential harm. Here we introduce ChemBench, an automated framework for evaluating the chemical knowledge and reasoning abilities of state-of-the-art LLMs against the expertise of ch"
  },
  {
    "title": "It\u2019s nearly impossible to buy an original Bob Ross painting (2021) (thehustle.co)",
    "points": 111,
    "submitter": "rmason",
    "submit_time": "2025-06-15T20:21:52 1750018912",
    "num_comments": 100,
    "comments_url": "https://news.ycombinator.com/item?id=44284723",
    "comments": [
      "I don't know why the article and everyone here is coming away with the conclusion that Bob Ross didn't want his art to be sold.A simpler reasoning is that there wasn't any demand for his paintings while he was alive. His show ran from 1983-1994 and he died in 1995. He was reasonably popular at that time, sure, but Bob Ross as we know him only blew up in the 2010s in the internet/YouTube/streaming age.Now there is a trove of 1,165 paintings which are no doubt valuable, but cannot all be sold because they would flood the market and decrease their own value. So Bob Ross, Inc. is cleverly keeping them under lock and key and letting the scarcity drive prices up.\n \nreply",
      "> Bob Ross as we know him only blew up in the 2010s in the internet/YouTube/streaming age.No, he was just as well-known when his show was on the air. He was a household name, his paintings and style was known, and people talked about him enough to have opinions on whether he was an \"artist\" or just a TV show host.\n \nreply",
      "I was going to call this anecdotal evidence based on it never appearing in the top 100 (or so) Nielson rated TV shows for a year, based on the lists for 1984-1995 here[0].However, it looks like PBS never signed up for Nielson until 2009, so we have limited/no public data on viewership of The Joy of Painting (or Sesame Street, etc for that matter).http://www.thetvratingsguide.com/2020/02/tvrg-ratings-histor...\n \nreply",
      "There's a lot of TV shows out there, even in the 80s and 90s, and plenty of ways for celebrities to have their image and reputation bolstered. Ratings aren't reliable in trying to measure someone's notoriety.Growing up in the late 80s/90s, and mostly outside of the US, I can't remember a time when I didn't know who Bob Ross was.\n \nreply",
      "My sense is that he was known to frequent PBS viewers (I remember him from before 2010) \u2014 but the whole Chia-fro thing and \"happy clouds\" or whatever meme-like thing that comes to mind definitely took him to the mainstream crowd with the internet.\n \nreply",
      "Bob Ross was known in my country (in Europe) due to his show at the time. Not quite universally, but probably closer to a household name than any other living painter was at the time. Dunno how it was in other countries in Europe, but still. The man was relatively well known for paintings,  paintings that were regarded well by the general audience (experts: dunno).So while maybe he couldn't be selling his paintings for 1000s to the decently-off, there clearly was ample demand. If he truly wanted to make a boatload, he easily could have.Related: the treasure trove could easily be sold 1 painting at a time. Just don't make it regular - not once a year,  but sometimes 2 in 2 months,  and then 5 years nothing. That really wouldn't spurs the value that much,  if at all.\n \nreply",
      "I think a lot of the responses to this are ignoring the things that were popular in the 90s that don't see a big spike of demand more recently.Bob Ross was popular. Thomas Kinkade was popular. IMO it's doubtful Ross would've been as popular at retail in the 90s as Kinkade. One was a nice cute little educational show. One was \"the painter of light\" with a marketing engine around him. Both also had plenty of detractors from the \"serious\" art scene.Why did Ross get positive associations through 2000s internet culture that Kinkade never did?Which would you rather go buy now?Was it just nostalgia, since he was relevant much more to the lives of the kids that grew up to create a lot of the internet culture of the time? Probably a big chunk of it.But there's also just a certain right-place-right-time. Like, nobody seems to be  going nuts about re-buying their childhood Pogs or even Beanie Babies. Ok, those  were readily available at retail; Bob Ross wasn't. But Pokemon cards were too...\n \nreply",
      "> He was reasonably popular at that time, sure, but Bob Ross as we know him only blew up in the 2010s in the internet/YouTube/streaming age.He remained popular after his death. I can remember seeing memes of Bob Ross as early as 2008.\n \nreply",
      "Yeah, I grew up watching reruns of his show on PBS in the early 00's. It was much more fun to watch when home sick than Antiques Roadshow.\n \nreply",
      "What, The Price Is right not good enough for you?!\n \nreply"
    ],
    "link": "https://thehustle.co/why-its-nearly-impossible-to-buy-an-original-bob-ross-painting",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Seastar \u2013 Build and dependency manager for C/C++ with Cargo's features (github.com/ai314159)",
    "points": 38,
    "submitter": "AI314159",
    "submit_time": "2025-06-15T19:36:32 1750016192",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=44284493",
    "comments": [
      "I wouldn't recommend Cargo as something to copy for a real project, even though I've a fan of and have been using Rust exclusively lately. It suffers from not being able to handle global features without manually/conditionally propagating features to dependencies, as well as not being able to propagate metadata to dependencies without abusing the links functionality.Why is that important? Well that's useful if you want something like json/serde or not in all transitive dependencies for a particular artifact you are generating like a library or a binary. That applies for other configurability that C/C++ developers bake into their libraries too.Is this an educational learning experience as part of Hackclub which is a linked organization on your GitHub profile? Whether or not if so, trying to build this will be a good learning experience.Think beyond just C/C++ and maybe Rust...The entire set of ideas of things to implement is just to look at the feature set of Bazel and Buck 2 (which happens to also be written in Rust). Those offer functionality to build complete products in any language, locally or distributed across a build farm of servers, and glue them all together in any format. For example you can't build a multi-arch OCI/Docker container image for a Rust-binary server in a single command with Cargo.Except for the initial learning curve, using them could be as simple as including their \"build\" files in your published git repo. No central repository needed.https://github.com/hackclub\nhttps://bazel.build/about/why\nhttps://buck2.build/docs/about/why/\n \nreply",
      "Highly recommend you rename because of a name clash with an existing famous C++ framework: https://seastar.io/\n \nreply",
      "Thank you for the tip! Yeah, I was probably gonna rename either way, funny I didn't find that when Googling...\n \nreply",
      "kinda surprising lol, seastar runs all sorts of popular stuff like ScyllaDB (a nosql performant db meant to be a replacement to Cassandra)\n \nreply",
      "Dependency management is a solved problem in the C/C++ world.It all should be done from within CMake using Hunter. They handle diamond dependencies correctly (everyone else just yolos it), they handle \"package registry\" correctly, ie git repos + hash verification. They handle tool chain files and forwarding of compilation flags correctly. I had a library building for like a dozen targets with it (including crazy stuff like iOS 9)\n \nreply",
      "Solved problem is a strong statement. I've never heard of Hunter before. And as far as I can remember the most popular way of solving this is having a list of dependencies in a README somewhere so you can install them and their headers with your os/distro package manager\n \nreply",
      "That's not a serious solution. You don't control dependency versions if you use a package manager and you can't build full static builds or full debug builds etc. Some targets don't have package managers (ex an embedded device)\n \nreply",
      "It is not, but it is the standard. So I would this is _not_ a solved problem",
      "libfftw3 is one the most widely used Fast Fourier Transform open source libraries.It can be built to be used in a single-threaded environment, or a multi-threaded one. It can be built to use 32 bit or 64 bit floating point as its internal data type.Any build system that cannot handle or allow me to express \"I depend on the single-threaded 64 bit version of libfftw3\" doesn't get my attention.Does yours?\n \nreply",
      "I think this was maybe true a decade ago...Having to contact them to get a license is a pain, and in benchmarks there are on-par or better libraries available. The tuning steps are baroque and if you really care about performance that much.. You'd probably look at GPU solutionsAre you expecting to do the tuning step during the build?Last I checked their arm support was terrible.. But maybe that's changed\n \nreply"
    ],
    "link": "https://github.com/AI314159/Seastar",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A simple, unified build system inspired by Cargo for C, C++, and maybe Rust\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Seastar is a fast, extensible build system for C, C++, and maybe soon, Rust and Zig as well.\nI believe that it should be easy to make, prototype, and iterate upon designs.\nWhile C is still one of our most widely used languages, it makes it hard to create\nprograms easily, especially for beginners. Instead, Seastar aims to be more like\nRust's tooling with cargo, but supporting seamless compilation across more languages.Seastar is very simple to build and run. Assuming you have Cargo and Rust installed,\nclone the repository, cd into the example folder, and run cargo run -- build\nto run Seastar and build the example project, or cargo run --"
  },
  {
    "title": "How fast can the RPython GC allocate? (pypy.org)",
    "points": 29,
    "submitter": "todsacerdoti",
    "submit_time": "2025-06-15T19:55:09 1750017309",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=44284592",
    "comments": [
      "My summary is that it's about one or two allocations per nanosecond on CF Bolz's  machine, an AMD Ryzen 7 PRO 7840U, presumably on one core, and it's about 11 instructions per allocation.This is about 2\u20134\u00d7 faster than my pointer-bumping arena allocator for C, kmregion\u2020, which is a similar number of instructions on the (inlined) fast path. But possibly that's because I was testing on slower hardware.  I was also testing with 16-byte initialized objects, but without a GC.  It's about 10\u00d7 the speed of malloc/free.I don't know that I'd recommend using kmregion, since it's never been used for anything serious, but it should at least serve as a proof of concept.______\u2020 http://canonical.org/~kragen/sw/dev3/kmregion.h http://canonical.org/~kragen/sw/dev3/kmregion.c http://canonical.org/~kragen/sw/dev3/kmregion_example.c\n \nreply",
      "I simulated yours vs the UPB arena fast path:https://godbolt.org/z/1oTrv1Y58Messing with it a bit, it seems like yours has a slightly shorter dependency chain due to loading the two members separately, where UPB loads them as a pair (as it needs both in order to determine how much size is available). Also seems to have less register pressure. I think that's because yours bumps down. UPB's supports in place forward extension, so it needs to bump up.If you added branch hints to signal to the compiler that your slow path is not often hit, you might seem some improvement (although if you have PGO it should already do this). These paths could also be good candidates for the `preserve_most` calling convention.However, there is an unfortunate compiler behavior here for both implementations - it doesn't track whether the slow path (which is not inlined, and clobbers the pointers) was actually hit, so it reloads on the hot path, for both approaches. Unfortunately this means that a sequence of allocations will store and load the arena pointers repeatedly, when ideally they'd keep the current position in a register on the hot path and refill that register after clobbering in the cold path.\n \nreply",
      "Thank you very much!  I vaguely remember that it did that, and the failure to keep the pointer in registers might explain why PyPy's version is twice as fast (?).\n \nreply",
      "I don't know much about language internals or allocation but am learning. why this could be significantly faster than a bump/arena allocator?And is the speed up over malloc/free due to large block allocation as opposed to individual malloc?\n \nreply",
      "It is a bump allocator.  I don't know why it's so much faster than mine, but my hypothesis was that CF Bolz was testing on a faster machine. The speedup over malloc/free is because bumping a pointer is much faster than calling a subroutine.\n \nreply",
      "The reason why their allocator is faster than Boehm isn't because of conservative stack scanning.You can move objects while using conservative stack scanning. This is a common approach. JavaScriptCore used to use it.You can have super fast allocation in a non-moving collector, but that involves an algorithm that is vastly different from the Boehm one. I think the fastest non-moving collectors have similar allocation fast paths to the fastest moving collectors. JavaScriptCore has a fast non-moving allocator called bump'n'pop. In Fil-C, I use a different approach that I call SIMD turbosweep. There's also the Immix approach. And there are many others.\n \nreply",
      "> Every allocation takes 110116790943 / 10000000000 \u2248 11 instructions and 21074240395 / 10000000000 \u2248 2.1 cyclesI don\u2019t believe this in even the slightest. That is not a meaningful metric for literally any actual workload in the universe. It defies common sense.A few years ago I ran some benchmarks on an old but vaguely reasonable work load. I came up with a p95 or just 25nanoseconds but p99.9 on the order of tens of microseconds. https://www.forrestthewoods.com/blog/benchmarking-malloc-wit...Of course \u201c2% of time in GC\u201d is doing a lot of heavy lifting here. But I\u2019d really need to see a real work load for me to start to believe.\n \nreply"
    ],
    "link": "https://pypy.org/posts/2025/06/rpython-gc-allocation-speed.html",
    "first_paragraph": "\nCF Bolz-Tereick\n\n\n2025-06-15 15:48\n Comments\nWhile working on a paper about allocation profiling in\nVMProf I got curious\nabout how quickly the RPython GC can allocate an object. I wrote a small\nRPython benchmark program to get an idea of the order of magnitude.The basic idea is to just allocate an instance in a tight loop:The RPython type inference will find out that instances of A have a single\ni field, which is an integer. In addition to that field, every RPython object\nneeds one word of GC meta-information. Therefore one instance of A needs 16\nbytes on a 64-bit architecture.However, measuring like this is not good enough, because the RPython static\noptimizer would remove the allocation since the object isn't used. But we can\nconfuse the escape analysis sufficiently by always keeping two instances alive\nat the same time:(I confirmed that the allocation isn't being removed by looking at the C code\nthat the RPython compiler generates from this.)This is doing a little bit more work tha"
  },
  {
    "title": "Lisp-stat: Lisp environment for statistical computing (lisp-stat.dev)",
    "points": 3,
    "submitter": "oumua_don17",
    "submit_time": "2025-06-14T11:23:55 1749900235",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://lisp-stat.dev/about/",
    "first_paragraph": "Lisp-Stat is conceptually similar to R and will be familiar to most\npeople from that ecosystem.  It is suitable for both exploratory data\nanalysis as well as front-line production deployments.  Common Lisp is\ncurrently used at Google in several high-availability, high-volume\ntransactional systems.We had a few requirements when evaluating options.  Specifically the\nsystem had to:Common Lisp was the only framework that met all these requirements.Probably the most important reasons though are given in the paper by\nRoss Ihaka, one of the originators of the R language, Lisp as a Base\nfor a Statistical Computing\nSystem\nabout the deficiencies in R and the inability to compile to\nmachine code (among other issues). The same is true of Python. In that\npaper he argues for Lisp as a replacement for R.Lisp-Stat provides support for vectorized mathematical operations and\na comprehensive set of statistical methods that are implemented using\nthe latest numerical algorithms.  In addition, Common Lisp p"
  },
  {
    "title": "Cure Dolly's Japanese Grammar Lessons (kellenok.github.io)",
    "points": 47,
    "submitter": "agnishom",
    "submit_time": "2025-06-14T05:38:07 1749879487",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=44274435",
    "comments": [
      "I live in Japna and have a high-intermediate level of Japanese. I wish I had been able to read and think about these well-expressed observations when I was just starting out as it would have saved me from having to intuit things over time.Like Cure Dolly writes, no one tells you what you really need to know when you're learning Japanese (all languages?)\n \nreply",
      "For people not in the loop, Cure Dolly was a youtuber who taught basic Japanese grammar lessons using a digital avatar. She had (and continues to have) a cult-like following (in a good way) among a section of the online Japanese learning community. She was also ill at the time and eventually passed away.\n \nreply",
      "I found Cure Dolly's guide very helpful as a launching point for Japanese. Just don't take her \"all you need\" marketing too seriously, it is more an introductory level than a complete guide.\n \nreply",
      "I'm so glad someone is taking these forward. Her videos were a god send when I started out learning this language. Besides the sometimes strange rants about text-books, her videos are top notch.This looks like it took a lot of effort to transcribe with all her helpful graphics too.Thank you for doing this!R.I.P Cure Dolly\n \nreply",
      "Another great source is https://imabi.org/\n \nreply",
      "> \u3060 = the one-way equals signWhat on earth did I just read?https://i.imgur.com/Z6OUVSp.png\n \nreply",
      "I did some digging into the identity behind the Cure Dolly character after finding the videos quite helpful. There is what I recall being a fairly credible trail of evidence that the proprietor is formerly associated with the Silver Sisterhood, AKA the \"Lesbian Spanking Cult\", which has popped up on HN a number of times due to their involvement in the early text adventure video game industry.\n \nreply",
      "i randomly stumbled upon the connection as well while reading about the St. Bride's School. such a random connection between two completely different interests of mine that i joked the universe is a simulation with limited RAM and reuses assetsCure Dolly has _some_ connection to this group which, to me, just adds even more mystique to an already fascinating story[1][1] https://if50.substack.com/p/1992-silverwolf\n \nreply",
      "Could you provide some sources for this? I'm worried that this could be considered as spreading rumors.\n \nreply",
      "This is what I could find from a few minutes of searching, although it's not really much evidence either way: https://pastebin.com/eHtG45pfI have no skin in this game, but I wouldn't be surprised that someone with a brilliant mind would also have, to put it bluntly, \"a bunch of weird shit.\"\n \nreply"
    ],
    "link": "https://kellenok.github.io/cure-script/",
    "first_paragraph": "Appearancein MarkdownRead all Dolly's japanese course on a handy and fast websiteCreated by KellenOriginally transripted by nunko/dinuz."
  }
]