[
  {
    "title": "Photos taken inside musical instruments (dpreview.com)",
    "points": 266,
    "submitter": "worik",
    "submit_time": "2025-05-30T20:32:19 1748637139",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=44139626",
    "comments": [
      "A little game for all of you: On Firefox or Chrome, go to Developer Tools (F12) > Console and execute:  document.querySelectorAll('p').forEach(e => e.style.opacity = 0)\n\nNow without the text visible anymore, try and guess which musical instrument each picture represents.  Then reload the page, enjoy the article, and check how many you got right.  What's your score out of 8?  I scored 5.\n \nreply",
      "No need to reload, you can hover over the image and read its filename to see the answers right after you guessed.\n \nreply",
      "Many more fantastic photos at his website:https://www.charlesbrooks.info/\n \nreply",
      "I thought dpreview closed down.  Happy to see it didn't.  Who saved them?\n \nreply",
      "https://www.dpreview.com/site-news/8298318614/dpreview-com-l...https://www.gearpatrol.com/about/a44214660/gear-patrol-dprev...https://www.gearpatrol.com/about/about-gear-patrol/\n \nreply",
      "Gear Patrol: https://www.gearpatrol.com/about/a44214660/gear-patrol-dprev...\n \nreply",
      "The violins looks like the lower deck of a galleon.  cf https://en.wikipedia.org/wiki/HMS_Victory#/media/File:Victor...\n \nreply",
      "It's cool to see how much the older double bass and violin have been repaired. Those square/flat pyramid pieces of wood are cleats, which were added by a luthier to repair and secure cracks.\n \nreply",
      "Beautiful photos! I'd love to see a concert hall designed to look like something from this gallery.\n \nreply",
      "I wonder if he would have been better off making a device to hold a small mirror steady and used a telephoto lens pointed at it from one of the f holes.It says he had a 5 mm hole to work with. That would pass an 8 gauge wire with plenty of room to maneuver. Mount a mirror to the end, thread a two or three foot wire through the hole from the inside out, clamp it to a surface the instrument is sitting in to keep it from moving, and set up your camera from a low angle and the light positioned to not cast a shadow.Alternately you could J hook a long, large diameter scope, and composite two shots with the cable visible on opposite sides of each picture.\n \nreply"
    ],
    "link": "https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments",
    "first_paragraph": ""
  },
  {
    "title": "Surprisingly Fast AI-Generated Kernels We Didn't Mean to Publish (Yet) (stanford.edu)",
    "points": 184,
    "submitter": "mfiguiere",
    "submit_time": "2025-05-30T20:03:12 1748635392",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=44139454",
    "comments": [
      "\"FP32 is less common in modern ML workloads and often less optimized on recent hardware compared to FP16 or BF16, which may partly explain why it\u2019s easier to achieve performance gains over PyTorch with FP32 kernels.\"People haven't spent time optimizing the fp32 versions of these kernels in years. This will be much more interesting if they can improve the kernels where developer effort has gone and that are actually used.\n \nreply",
      "I wonder if it's using known improvements from the fp16/bf16 kernels that are transferable to fp32?\n \nreply",
      ">People haven't spent time optimizing the fp32 versions of these kernels in years.Wow, so, you're basically saying the AI created new algos in a domain with no pre-existing solutions? Awesome!\n \nreply",
      "No one said the AI created new algorithms nor that there weren\u2019t pre-existing solutions.The implication was that the FP32 versions of these kernels have lagged behind the more popular versions. There was opportunity to translate the advancements from other kernels into these. Someone would need to look closely to see exactly what was done, but it\u2019s premature to suggest anything like \u201cnew algos\u201d or \u201cno pre-existing solutions\u201dThis is a great use case for LLMs, though. I often do something similar where I make improvements to something I use most frequently and ask an LLM to translate that pattern to other similar parts of the code.\n \nreply",
      ">The implication was that the FP32 versions of these kernels have lagged behind the more popular versions.Help me understand this 'cause I'm a bit slow these days ...Does that mean optimized FP32 versions of these kernels were already there or not?\n \nreply",
      "The hype cycle in action, folks. Pay heed.\n \nreply",
      "My takeaway - from this article, from Google\u2019s AlphaEvolve [1], and the recent announcement about o3 finding a zero day in the Linux kernel [2] - is that Gemini Pro 2.5 and o3 in particular have reached a new level of capability where these ideas that were tried unsuccessfully with other models, suddenly just work.[1] https://deepmind.google/discover/blog/alphaevolve-a-gemini-p...[2] https://sean.heelan.io/2025/05/22/how-i-used-o3-to-find-cve-...\n \nreply",
      "In my opinion, I wouldn\u2019t say so much that they are suddenly working. Rather we\u2019ve reached a point where they can iterate and test significantly faster than humans are capable of doing and have the ability to call on significantly more immediately available information that it can make sense of, and as a result, the combination information, advancement and intelligently applied brute force seems to be having success in certain applications.\n \nreply",
      "Good points. I suspect that o3 is able to reason more deeply about different paths through a codebase than earlier models, though, which might make it better at this kind of work in particular.\n \nreply",
      "I was blown away by some debugging results I got from o3 early on and have been using it heavily since. The early results that caught my attention were from a couple cases where it tracked down some problematic cause through several indirect layers of effects in a way where you'd typically be tediously tracing step-by-step through a debugger. I think whatever's behind this capability has some overlap with really solid work it'll do in abstract system design, particularly in having it think through distant implications of design choices.\n \nreply"
    ],
    "link": "https://crfm.stanford.edu/2025/05/28/fast-kernels.html",
    "first_paragraph": "We have some very fast AI-generated kernels in pure CUDA-C without using libraries and DSLs such as CUTLASS and Triton. They are performing close to or in some cases even beating the standard expert-optimized production kernels shipped in PyTorch. Some of our highlighted results:(Our results are benchmarked on an Nvidia L40S GPU, and % performance is defined as reference time divided by generated kernel time)\n\u201cUntiled\u201d by DALL\u00b7E (2025). (Digital pigment on virtual canvas)From the MMA collectionWe started with the goal of generating synthetic data to train better kernel generation models. Somewhere along the way the unexpected happened: the test-time only synthetic data generation itself started producing really good kernels beating or performing close to human expert optimized PyTorch baselines, utilizing advanced optimizations and hardware features, which were previously thought to be challenging. As a result, we decided to write this blog post early and share our findings. The point "
  },
  {
    "title": "Mary Meeker's first Trends report since 2019, focused on AI (bondcap.com)",
    "points": 66,
    "submitter": "kjhughes",
    "submit_time": "2025-05-30T19:53:59 1748634839",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44139403",
    "comments": [
      "Here\u2019s the PDF.\nhttps://www.bondcap.com/report/pdf/Trends_Artificial_Intelli...\n \nreply",
      "Did self driving (assumed Waymo) really exceed rideshare by number of rides per week in SF already?! Wow I need to visit more often\n \nreply"
    ],
    "link": "https://www.bondcap.com/reports/tai",
    "first_paragraph": ""
  },
  {
    "title": "Beating Google's kernelCTF PoW using AVX512 (anemato.de)",
    "points": 245,
    "submitter": "anematode",
    "submit_time": "2025-05-30T16:19:50 1748621990",
    "num_comments": 76,
    "comments_url": "https://news.ycombinator.com/item?id=44137715",
    "comments": [
      "Cool stuff! This method is very similar to how AVX-512-optimized RSA implementations work too, as they also have to do Very Large Exponentiations. This paper[1] covers how RSA does its windowing, which includes the formula showing how the window size can be arbitrary. One additional thing AVX-512 RSA implementations do, though, is store the results of the multiplications for the range [0..2^{window-size}) in a table; then, for each window, that result is retrieved from the table[2] and only the shifting/repositioning has to be done.1. https://dpitt.me/files/sime.pdf (hosted on my domain because it's pulled from a journal)2. https://github.com/aws/aws-lc/blob/9c8bd6d7b8adccdd8af4242e0...\n \nreply",
      "Ooh interesting, I should have looked at this while developing.... Looks like that code could definitely use another version for e.g. Zen 5 where using zmm registers would lead to a 2x multiplication throughput. Also the mask registers are bounced to GPRs for arithmetic but that's suboptimal on Zen 4/5.Separately, I'm wondering whether the carries really need to be propagated in one step. (At least I think that's what's going on?) The chance that a carry in leads to an additional carry out beyond what's already there in the high 12 bits is very small, so in my code, I assume that carries only happen once and then loop back if necessary. That reduces the latency in the common case. I guess with a branch there could be timing attack issues though\n \nreply",
      "ymms were used here on purpose! With full-width registers, the IFMA insns have a deleterious effect on frequency, at least in the Icelake timeframe.\n \nreply",
      "zen5 can run avx512 at near full boost clocks: https://chipsandcheese.com/p/zen-5s-avx-512-frequency-behavi...\n \nreply",
      "Ye, hence a separate version for CPUs which don't have that problem. Although, maintaining so many of these RSA kernels does seem like a pain. Didn't realize u wrote that code; super cool that it's used in practice!\n \nreply",
      "I am not the original author\u2014this is adapted from an implementation by Shay Gueron, the author of that paper I linked, but I do agree that it's cool!\n \nreply",
      "There is something off with>  ...and despite supporting it [AVX512] on consumer CPUs for several generations...I dunno. Before Rocket Lake (11th gen) AVX512 was only available in those enthusiast cpu, xeon cpu or in some mobile processors (which i wouldn't really call consumer cpu).With the 12th gen (and that performance/efficiency core concept), they disabled it after a few months in those core and it was never seen again.I am pretty sure tho, after AMD has some kind of success with AVX512 Intel will reintroduce it again.btw. I am still rocking an Intel i9-11900 cpu in my setup here. ;)\n \nreply",
      "> With the 12th gen (and that performance/efficiency core concept), they disabled it after a few months in those core and it was never seen again.The 12th gen CPUs with performance cores didn't even advertise AVX512 support or have it enabled out of the box. They didn't include AVX512 on the efficiency cores for space reasons, so the entire CPU was considered to not have AVX512.It was only through a quirk of some BIOS options that you could disable the efficiency cores and enable AVX512 on the remaining CPU. You had to give up the E-cores as a tradeoff.\n \nreply",
      "IMO this is more a sign of complete dysfunction at Intel. They definitely could have made avx512 instructions trigger a switch to p-cores, and honestly probably could have supported them completely (if slightly slowly) by splitting the same way AMD does on Zen4 and Zen5 C cores. The fact that they shipped P cores and E cores that had different assembly sets is what you get when you have separate teams competing with each other rather than working together to make a good product.\n \nreply",
      "> They definitely could have made avx512 instructions trigger a switch to p-coresNot really, no. OS-level schedulers are complicated as is with only P vs E cores to worry about, let alone having to dynamically move tasks because they used a CPU feature (and then moving them back after they don't need them anymore).> and honestly probably could have supported them completely by splitting the same way AMD does on Zen4 and Zen5 C cores.The issue with AVX512 is not (just) that you need a very wide vector unit, but mostly that you need an incredibly large register file: you go up from 16 * 256 bit = 4096 bits (AVX2) to 32 * 512 bit = 16384 bits (AVX512), and on top of that you need to add a whole bunch of extra registers for renaming purposes.\n \nreply"
    ],
    "link": "https://anemato.de/blog/kctf-vdf",
    "first_paragraph": "In May 2025, my Crusaders of Rust teammates William Liu (FizzBuzz101) and Savy Dicanosa (Syst3mFailure) discovered and developed an exploit of a use-after-free bug in Linux's packet scheduler. The bugfix patch contains additional details. William found this bug while fuzzing Linux for his master's thesis, which I will link here upon its publication. (Congratulations, William!)They wanted to submit the bug to Google's kernelCTF competition for an anticipated $51,000 bounty.1 Unfortunately, finding the bug and writing the exploit was only the first part of the battle. This post documents my small but unique contribution to our ultimately winning the bounty.To avoid paying out lots of money, kernelCTF organizers limit the number of submissions eligible for a bounty. Every two weeks at noon UTC, the submission window opens. Only the first team who is able to connect to and exploit the server, and submit the flag to a Google Form, receives a payout; any subsequent submissions are marked as "
  },
  {
    "title": "Valkey Turns One: How the Community Fork Left Redis in the Dust (gomomento.com)",
    "points": 34,
    "submitter": "cebert",
    "submit_time": "2025-05-30T22:24:22 1748643862",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44140379",
    "comments": [
      "I run Redis across dozens of applications. So when Valkey became available for a discounted price on AWS I was excited. We finally got around to trying it out about 2 months ago and all was going well. No noticeable difference in performance. Until Valkey just died. It died in such a way that AWS still thought it was running happily but it was completely offline. It took 12+ hours for it to come up again and then it happened again... AWS researched the issue for 2 weeks and couldn't figure it out. It will be a long time before we attempt to use Valkey for anything critical in the future. We since have replace that Valkey with Redis under the same workload and have no issues.\n \nreply",
      "We need to get valkey into the default distro package managers.  It sucks having to add keys and update the distro get valkey installed in say a GitHub Action runner.\n \nreply",
      "What distro doesn't have it that you need? Skimming https://repology.org/project/valkey/versions it seems to be in nixpkgs, Arch, Ubuntu, Fedora, Debian, and EPEL. Of those, the only caveat I immediately see is that Debian only got it in 13 or 12+backports.\n \nreply",
      "If I ask ChatGPT it says I have to install it this way - which is a lot more work/time than just \"sudo apt-get install valkey-server\":---# Add the Valkey APT repository keycurl -fsSL https://packages.valkey.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/valkey-archive-keyring.gpg# Add the Valkey repository to your APT sourcesecho \"deb [signed-by=/usr/share/keyrings/valkey-archive-keyring.gpg] https://packages.valkey.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/valkey.list > /dev/null# Update your package indexsudo apt-get update# Install Valkeysudo apt-get install valkey-server---\n \nreply",
      "GitHub Actions runners are based on Ubuntu, and from the Repology link it seems ValKey is in the \"universe\" repository which is a community supported repository and therefore might not be enabled by default.\n \nreply"
    ],
    "link": "https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Icepi Zero \u2013 The FPGA Raspberry Pi Zero Equivalent (github.com/cheyao)",
    "points": 81,
    "submitter": "Cyao",
    "submit_time": "2025-05-28T13:31:04 1748439064",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=44115853",
    "comments": [
      "I checked out his post on Reddit [0]. OP (cyao12) wrote a CPU in Verilog at the age of 13 and is now only 16. Mind. Blown.  cyao12: I'm going to try and put the old cpu I made in verilog when I was 13 on it! The sdram is okay, the traces are short enough that the distance difference doesn't matter :D\n\n  Collez_boi: You made a freaking CPU in Verilog when you were 13?! That's crazy.\n\n  cyao12: Yeaaah, but tbh the design wasnt really good lol. Im 16 now so Im quite happy about my progress\n\n[0] https://www.reddit.com/r/FPGA/comments/1kwxvk8/ive_made_my_f...\n \nreply",
      "If I understood correctly, the ECP5 FPGA can be designed for with open source tooling [0][1], which makes this even more awesome.OP, if you are planning to commercialize these, try to confirm compatibility, that will definitely make it more attractive![0] https://hackernoon.com/getting-started-using-open-source-fpg...[1] https://github.com/YosysHQ/prjtrellis\n \nreply",
      "Fpga is kind of like the final frontier in my embedded trajectory, haven\u2019t made the leap yet mainly because microcontrollers or fixed arch cpus are fast enough for most consumer tech. I\u2019d love to give it a shot one day though\n \nreply",
      "Another project like FPGA in \"Pi Zero\" format was fleaFPGA_Ohm (http://fleasystems.com/fleaFPGA_Ohm.html).\n \nreply",
      "Yep - unfortunately, that never appears to have been available for purchase outside the Indiegogo campaign.\n \nreply",
      "What would it take for something like this to output eDP over one of the USB C ports? What are the design and hardware requirements?\n \nreply",
      "Neat stuff! Is this similar to the Arduino MKR Vidor?\n \nreply",
      "Can you sell these assembled? Using Tang nano for learning right now, but the tooling and docs situation is not great\n \nreply",
      "The icebreaker is great and easy to get up and running, plus it has pmod headers which makes it easy to add add-ons.\n \nreply",
      "Yep, and I love it. That's a much smaller (and lower pin count) FPGA than this ECP5, though.\n \nreply"
    ],
    "link": "https://github.com/cheyao/icepi-zero",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        ECP5 Development Board in the Raspberry Pi Zero form\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Icepi Zero is an economical FPGA development board with a Raspberry Pi Zero form factor. It carries a Lattice ECP5 25F, enabling powerful designs while keeping a small portable size. It also has a HDM- port allowing easy digital video output.View PCB onlineCurrently most powerful FPGA boards on the market are expensive and bulky.I've always wanted a low-cost portable FPGA with video output to make my own CPU, but there isn't any on the market.The Icepi Zero aims to fix this. Carrying a powerful ECP5 FPGA on a small Raspberry Pi Zero form factor, it is the ultimate portable solution for FPGA development. Additionally packing a HDM- mini port and 3 USB-C ports,"
  },
  {
    "title": "Java Virtual Threads Ate My Memory: A Web Crawler's Tale of Speed vs. Memory (dariobalinzo.medium.com)",
    "points": 26,
    "submitter": "dariobalinzo",
    "submit_time": "2025-05-29T13:08:17 1748524097",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=44125734",
    "comments": [
      "> Without any back-pressure, the program kept stuffing memory with pending results.The key phrase in the whole article.This is the reason I am not fully confident in the \"pervasive async\" mindset overtaking web development. In JavaScript, pervasive async makes sense, because all functions eventually return control to a central event loop anyway. In environments with real threading, the move from OS threads to \"green threads and async everywhere\" implicitly gives up the tried-and-tested scheduler of your OS and its mechanisms to handle back-pressure. Developers must now take full responsibility for avoiding buffer bloat.> they fundamentally change how we think about concurrency limits and resource management.Virtual threads make concurrency cheaper, but nothing about them (or any other concurrency abstraction) eliminates the need for flow control. By switching to virtual threads, you traded off the safeguards once provided by your (bounded!) thread pool and OS scheduler. It's now your responsibility to put safeguards back in.As an aside: in Clojure's core.async, channel limits (like 1024 pending puts) are intentionally there as a warning sign that your async pipeline isn't handling back-pressure correctly. This is exactly the \"hyperactive downloader with no brakes\" problem described in the article. With core.async, you would have eventually hit an exception related to excessive puts on a channel, rather than silently expanding buffers until resource exhaustion.\n \nreply",
      "At my last job I made a thing that took in tens of millions of records from Kafka, did some processing, plopped stuff into Postgres, and moved along.  It wasn't hard, but getting throughput was difficult to achieve my throughput goals with regular blocking JDBC.Eventually I moved to an async-non-blocking model, and for my initial tests it went great, stuff returned almost immediately and I could grab the next one.  The problem is that, sort of by definition, non-blocking stuff doesn't really know when it's going to complete right away, so it just kept enqueuing stuff until it ate all my memory.I figured out pretty early on what was happening, but the other people on my team really seemed to struggle with the concept of backpressure, I think because they weren't really used to a non-blocking model. You kind of get backpressure for free with blocking code.Eventually I was able to solve my problem using a BlockingQueue,\n \nreply",
      "> By switching to virtual threads, you traded off the safeguards once provided by your (bounded!) thread pool and OS scheduler.The OS scheduler provides no safeguards here (you can use the thread-per-task executor with OS threads - you'll just get an OOME much sooner), and as the virtual thread adoption guide says, you should replace bounded thread pools with a semaphore when needed because it's just a better construct. It allows you to limit concurrency for different resources in an easy and efficient way even when different threads make use of different resources.In this case, by switching to virtual threads you get to use more appropriate concurrency constructs without needing to manage threads directly, and without sharing them among tasks.\n \nreply",
      "It's not really any different to the limitations of memory that we have always dealt with, it just consumes it faster than we might expect. The techniques we use for ensuring we don't run out of memory also apply to anything that can make threads and we have to put limitations on how many can exist, especially since they consume quite a bit of memory even in their virtual thread variant.\n \nreply",
      "Everything, always, returns to a main event loop, even the actual hardware threads on your CPU. So JavaScript developers don't get a pass on this.The problem is you keep creating async requests, they need to be stored somewhere, and that's in memory. Thread or no thread, it's in memory.And backpressure is essential, yes. Except for basic tasks.But there is another solution, which is coalescing requests. I.e. in my current project, I combine thousands of operations, each of which could've been its own async request, into a single \"unit of work\" which then results in a single async request, for all operations combined.If we treat IO seriously, we won't just fire off requests into the dark by the hundreds. We'd think about it, and batch things together. This can be done manually, but it's best done at a lower level, so you don't have to think about it at the business logic level.\n \nreply",
      "> Virtual Threads Ate My MemoryFrom what I see, It's a developer trade off.If you want to save on memory then you need to drop down to callbacks, promise chains, or async-await sugar that is a compiler transform to a state machine.But if you do that, then you will write a blog complaining about the boilerplate!I think Zig had an ability to kind of give you the best of both worlds. A zig expert would need to correct me on this.\n \nreply",
      "I used VThreads recently with good results I should say. In my case I limited concurrency by a blocking queue of concurrent tasks. So number of virtual threads does not keep rising based of millions of input records I was processing.I found for me keeping VThreads about ~100X of concurrent call to external resources was kind of sweet spot for overall throughput.\n \nreply",
      "Good timing. I was actually reading the official docs the other day on virtual threads, and it has a big section decidicated to rate limiting virtual threads:\"The hardest thing to internalize about virtual threads is that, while they have the same behavior as platform threads they should not represent the same program concept.\"...\"Sometimes there is a need to limit the concurrency of a certain operation. For example, some external service may not be able to handle more than ten concurrent requests. Because platform threads are a precious resource that is usually managed in a pool, thread pools have become so ubiquitious that they're used for this purpose of restricting concurrency, \"...\"But restricting concurrency is only a side-effect of thread pools' operation. Pools are designed to share scarce resources, and virtual threads aren\u2019t scarce and therefore should never be pooled!\"When using virtual threads, if you want to limit the concurrency of accessing some service, you should use a construct designed specifically for that purpose: the Semaphore class.\"...\"Simply blocking some virtual threads with a semaphore may appear to be substantially different from submitting tasks to a fixed thread pool, but it isn't. Submitting tasks to a thread pool queues them up for later execution, but the semaphore internally (or any other blocking synchronization construct for that matter) creates a queue of threads that are blocked on it that mirrors the queue of tasks waiting for a pooled thread to execute them. Because virtual threads are tasks, the resulting structure is equivalent\"\"Even though you can think of a pool of platform threads as workers processing tasks that they pull from a queue and of virtual threads as the tasks themselves, blocked until they may continue, the underlying representation in the computer is virtually identical. _Recognizing the equivalence between queued tasks and blocked threads will help you make the most of virtual threads._\" (emphasis mine)So it was cool seeing him arrive at the same conclusion (or he read the docss). Either way it was a nice timely article.Link to the docs: https://docs.oracle.com/en/java/javase/21/core/virtual-threa...\n \nreply"
    ],
    "link": "https://dariobalinzo.medium.com/virtual-threads-ate-my-memory-a-web-crawlers-tale-of-speed-vs-memory-a92fc75085f6",
    "first_paragraph": ""
  },
  {
    "title": "The 'white-collar bloodbath' is all part of the AI hype machine (cnn.com)",
    "points": 258,
    "submitter": "lwo32k",
    "submit_time": "2025-05-30T13:38:21 1748612301",
    "num_comments": 425,
    "comments_url": "https://news.ycombinator.com/item?id=44136117",
    "comments": [
      "I worked at two different $10B+ market cap companies during ZIRP. I recall in most meetings over half of the knowledge workers attending were superfluous. I mean, we hired someone on my team to attend cross functional meetings because our calendars were literally too full to attend. Why could we do that? Because the company was growing and hiring someone to attend meetings wasn't going to hurt the skyrocketing stock. Plus hiring someone gave my VP more headcount and therefore more clout. The market only valued company growth, not efficiency. But the market always capitulates to value (over time). When that happens all those overlay hires will get axed. Both companies have since laid off 10K+. AI was the scapegoat. But really, a lot of the knowledge worker jobs it \"replaces\" weren't providing real value anyway.\n \nreply",
      "This is so true. We had a (admittedly derogatory) term we used during the rise in interest rates, \"zero interest rate product managers\". Don't get me wrong, I think great product managers are worth their weight in gold, but I encountered so many PMs during the ZIRP era who were essentially just Jira-updaters and meeting-schedulers. The vast majority of folks I see that were in tech that are having trouble getting hired now are in people who were in those \"adjacent\" roles - think agile coaches, TPMs, etc. (but I have a ton of sympathy for these folks - many of them worked hard for years and built their skills - but these roles were always somewhat \"optional\").I'd also highlight that beyond over-hiring being responsible for the downturn in tech employment, I think offshoring is way more responsible for the reduction in tech than AI when it comes to US jobs. Video conferencing tech didn't get really good and ubiquitous (especially for folks working from home) until the late teens, and since then I've seen an explosion of offshore contractors. With so many folks working remotely anyway, what does it matter if your coworker is in the same city or a different continent, as long as there is at least some daily time overlap (which is also why I've seen a ton of offshoring to Latin America and Europe over places like India).\n \nreply",
      "Off-shoring is pretty big right now but what shocks me is that when I walk around my company campus I see obscene amounts of people visibly and culturally from, mostly, India and China. The idea that literally massive amounts of this workforce couldn't possibly be filled by domestic grads is pretty hard to engage with. These are low level business and accounting analyst positions.Both sides of the aisle retreated from domestic labor protection for their own different reasons so the US labor force got clobbered.\n \nreply",
      "I am VERY pro-immigration. I do have concerns about the H1B program though. IMO it's not great for both immigrant workers, as well as non-immigrant workers because it creates a class of workers for whom it's harder to change employers which weakens their negotiation position. If this is the case for enough of the workforce it artificially depresses wages for everyone. I want to see a reform that makes it much easier for H1B workers to change employers.\n \nreply",
      "In context of tech, H1B is great for the money people in the US and India. It suppresses wages in both countries and is a powerful plum for employee \u201cloyalty\u201d. There\u2019s a whole industry of companies stoking the pipeline of cheap labor and corrupting the hiring process.In big dollar markets, the program is used more for special skills. But when a big bank or government contractor needs marginally skilled people onshore, they open an office in Nowhere, Arizona, and have a hard time finding J2EE developers. So some company from New Jersey will appear and provide a steady stream of workers making $25/hr.The calculus is that more H1=less offshore.The smart move would be to just let skilled workers from India, China, etc with a visa that doesn\u2019t tie them to an employer. That would end the abusive labor practices and probably reduce the number of lower end workers or the incentive to deny entry level employment to US nationals.\n \nreply",
      "H1-B also makes CS masters programs a cash cow for US schools.\n \nreply",
      "How does H1B suppress wages in India?\n \nreply",
      "All those people skilled enough to get hired in the US (for massive increase in wages) don\u2019t try to get similar positions in India, thus, nobody has to compete to pay for them.\n \nreply",
      "Because it surpresses wages in the US, so Indian employers do not need to offer as much compensation to keep local workers who are considering emigrating.\n \nreply",
      "I want to use you as a bit of a sounding board, so don't take this as negative feedback.The problem is that the left, which was historically pro-labor, abdicated this position for racial reasons, and the right was always about maximizing the economic zone.\n \nreply"
    ],
    "link": "https://www.cnn.com/2025/05/30/business/anthropic-amodei-ai-jobs-nightcap",
    "first_paragraph": "Markets \n\n\nHot Stocks \n\n\nFear & Greed Index \n\n\n\n            Latest Market News \n\n\n\n            Hot Stocks \n\n\nFollow:\nA version of this story appeared in CNN Business\u201a\u00c4\u00f4 Nightcap newsletter. To get it in your inbox, sign up for free here.\n\n            If the CEO of a soda company declared that soda-making technology is getting so good it\u201a\u00c4\u00f4s going to ruin the global economy, you\u201a\u00c4\u00f4d be forgiven for thinking that person is either lying or fully detached from reality.\n    \n            Yet when tech CEOs do the same thing, people tend to perk up.\n    \n            ICYMI: The 42-year-old billionaire Dario Amodei, who runs the AI firm Anthropic, told Axios this week that the technology he and other companies are building could wipe out half of all entry-level office jobs \u201a\u00c4\u00b6 sometime soon. Maybe in the next couple of years, he said.\n    \n            He reiterated that claim in an interview with CNN\u201a\u00c4\u00f4s Anderson Cooper on Thursday.\n    \n            \u201a\u00c4\u00faAI is starting to get better than humans a"
  },
  {
    "title": "Reverse engineering of Linear's sync engine (github.com/wzhudev)",
    "points": 8,
    "submitter": "flashblaze",
    "submit_time": "2025-05-29T04:29:30 1748492970",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://github.com/wzhudev/reverse-linear-sync-engine",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A reverse engineering of Linear's sync engine for learning purposes. Endorsed by Linear's CTO.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.ImportantThis is a pretty awesome (and correct) write-up of our sync engine. (link)...probably the best documentation that exists - internally or externally. (link)-- Tuomas Artman (Co-founder and CTO of Linear)ImportantDisclaimer: This research is conducted solely for learning purposes. Readers should not use the findings to develop software that competes with Linear or attempt to use the information provided to disrupt or compromise Linear\u2019s systems. If the Linear team requests, I will be more than happy to remove this repository.ImportantCheck out the SUMMARYMy friends found this too long, so I wrote a summary high"
  },
  {
    "title": "Systems Correctness Practices at Amazon Web Services (acm.org)",
    "points": 295,
    "submitter": "tanelpoder",
    "submit_time": "2025-05-30T12:43:13 1748608993",
    "num_comments": 103,
    "comments_url": "https://news.ycombinator.com/item?id=44135638",
    "comments": [
      "> Deterministic simulation.  Another lightweight method widely used at AWS is deterministic simulation testing, in which a distributed system is executed on a single-threaded simulator with control over all sources of randomness, such as thread scheduling, timing, and message delivery order. Tests are then written for particular failure or success scenarios, such as the failure of a participant at a particular stage in a distributed protocol. The nondeterminism in the system is controlled by the test framework, allowing developers to specify orderings they believe are interesting (such as ones that have caused bugs in the past). The scheduler in the testing framework can also be extended for fuzzing of orderings or exploring all possible orderings to be tested.Any good open source libraries that do this that are language agnostic? Seems doable - spin up a container with some tools within it. Said tools require some middleware to know when a test is going to be run, when test is run, tools basically make certain things, networking, storage, etc \"determinstic\" in the context of the test run.This is more-or-less what antithesis does, but haven't seen anything open source yet.You of course, could write your tests well, such that you can stub out I/O, but that's work and not everyone will write their tests well anyway (you should do this anyway, but it's nicer imo if this determinism is on a layer higher than the application).as a slight sidebar - I'm not really bullish on AI, but I think testing is one of the things where AI will hopefully shine, because the feedback loop during prompting can be driven by your actual application requirements, such that the test implementation (driven by AI), requirements (driven by you as the prompt) and \"world\" (driven by the actual code being tested) can hopefully help drive all three to some theoretical ideal. if AI gives us anything, I'm hoping it can make software a more rigorous discipline by making formal verification more doable.\n \nreply",
      "There have historically been two giant adoption challenges for DST.(1) Previously, you had to build your entire system around one of the simulation frameworks (and then not take any dependencies).(2) It\u2019s way too easy to fool yourself with weak search/input generation, which makes all your tests look green when actually you aren\u2019t testing anything nontrivial.As you say, Antithesis is trying to solve both of these problems, but they are very challenging.I don\u2019t know of anybody else who has a reliable way of retrofitting determinism onto arbitrary software. Facebook\u2019s Hermit project tried to do this with a deterministic Linux userspace, but is abandoned. (We actually tried the same thing before we wrote our hypervisor, but found it didn\u2019t work well).A deterministic computer is a generically useful technology primitive beyond just testing. I\u2019m sure somebody else will create one someday, or we will open-source ours.\n \nreply",
      "I suspect you can relatively easily obtain a completely deterministic machine by running QEMU in 100% emulation mode in one thread. But what you are after is controlled deterministic execution, and it's far harder. That is, making your multiple processes to follow a specific dance that triggers an interesting condition must be very involved, when seen from the level as low as CPU and OS scheduler. Hence a language-agnostic setup is hard to achieve and especially hard to make it do your bidding. It may drown you in irrelevant details.I once built a much, much simpler thing that allowed to run multiple JVM threads in a particular kind of lockstep, by stubbing and controlling I/O operations and the advance of the system time. With that, I could run several asynchronously connected components in particular interaction patterns, including not just different thread activation order but also I/O failures, retries, etc. It was manageable, and it helped uncover a couple of nasty bugs before the code ever ran in prod.But that was only possible because I went with drastic simplifications, controlling not the whole system but only particular synchronization points. It won't detect a generic data race where explicit synchronization would be just forgotten.\n \nreply",
      "https://apple.github.io/foundationdb/testing.html\nhttps://www.youtube.com/watch?v=4fFDFbi3toc\n \nreply",
      "Trust me, I love FDB, but that's not the same thing. The FDB team IIRC had to write their own programming language to do this. It's not a agnostic layer above the application.The problem with coupled tooling is that no one will use it. That's what is cool about antithesis. If they're able to complete their goal, that's basically what will be achieved.\n \nreply",
      "I guess you meant to say \"only the people working on the software coupled to the tooling will use it\". It's not just FDB & Amazon that are using something like this, and it is a ridiculously powerful type of tool for debugging distributed systems.\n \nreply",
      "ah, yes.\n \nreply",
      "Fiar point. I was thinking about antithesis, but it's not open source (yet?). Turns out I also didn't read your comment well enough. Back to lurking I go.\n \nreply",
      "https://rr-project.org/ for languages that can be debugged by gdb.\n \nreply",
      "There was a talk from Joe Armstrong about using property testing to test Dropbox.\n \nreply"
    ],
    "link": "https://cacm.acm.org/practice/systems-correctness-practices-at-amazon-web-services/",
    "first_paragraph": ""
  },
  {
    "title": "Microsandbox: Virtual Machines that feel and perform like containers (github.com/microsandbox)",
    "points": 243,
    "submitter": "makeboss",
    "submit_time": "2025-05-30T13:20:04 1748611204",
    "num_comments": 122,
    "comments_url": "https://news.ycombinator.com/item?id=44135977",
    "comments": [
      "This is great!I'd like to see a formal container security grade that works like:  1) Curate a list of all known (container) exploits\n  2) Run each exploit in environments of increasing security like permissions-based, jail, Docker and emulator\n  3) The percentage of prevented exploits would be the score from 0-100%\n\nUnder this scheme, I'd expect naive attempts at containerization with permissions and jails to score around 0%, while Docker might be above 50% and Microsandbox could potentially reach 100%.This might satisfy some of our intuition around questions like \"why not just use a jail?\". Also the containers could run on a site on the open web as honeypots with cash or crypto prizes for pwning them to \"prove\" which containers achieve 100%.We might also need to redefine what \"secure\" means, since exploits like Rowhammer and Spectre may make nearly all conventional and cloud computing insecure. Or maybe it's a moving target, like how 64 bit encryption might have once been considered secure but now we need 128 bit or higher.Edit: the motivation behind this would be to find a container that's 100% secure without emulation, for performance and cost-savings benefits, as well as gaining insights into how to secure operating systems by containerizing their various services.\n \nreply",
      "The issue, at least with multitenant workloads, isn't \"container vulnerabilities\" as such; it's that standard containers are premised on sharing a kernel, which makes every kernel LPE a potential container escape --- there's a long history of those bugs, and they're only rarely flagged as \"container escapes\"; it's just sort of understood that a kernel LPE is going to break containers.\n \nreply",
      "You cannot build a secure container runtime (against malicious containers) because underlying it is the Linux kernel.The only way to make Linux containers a meaningful sandbox is to drastically restrict the syscall API surface available to the sandboxee, which quickly reduces its value. It's no longer a \"generic platform that you can throw any workload onto\" but instead a bespoke thing that needs to be tuned and reconfigured for every usecase.This is why you need virtualization. Until we have a properly hardened and memory safe OS, it's the only way. And if we do build such an OS it's unclear to me whether it will be faster than running MicroVMs on a Linux host.\n \nreply",
      "You cannot build a secure virtualization runtime because underlying it is the VMM. Until you have a secure VMM you are subject to precisely the same class of problems plaguing container runtimes.The only meaningful difference is that Linux containers target partitioning Linux kernel services which is a shared-by-default/default-allow environment that was never designed for and has never achieved meaningful security. The number of vulnerabilities resulting from, \"whoopsie, we forgot to partition shared service 123\" would be hilarious if it were not a complete lapse of security engineering in a product people are convinced is adequate for security-critical applications.Present a vulnerability assessment demonstrating a team of 10 with 3 years time (~10-30 M$, comparable to many commercially-motivated single-victim attacks these days) can find no vulnerabilities in your deployment or a formal proof of security and correctness otherwise we should stick with the default assumption that software if easily hacked instead of the extraordinary claim that demands extraordinary evidence.\n \nreply",
      "> You cannot build a secure virtualization runtime because underlying it is the VMMThere are VMMs (e.g. pKVM) with small SLoC that are isolated by silicon support for nested virtualization. This can be found on recent Google Pixel phones/tablets with strong isolation of untrusted Debian Linux \"Terminal\" VM.A similar architecture was shipped a decade ago by Bromium and now on millions of HP business laptops, including firmware isolation by hypervisor, \"Hypervisor Security : Lessons Learned \u2014 Ian Pratt, Bromium \u2014 Platform Security Summit 2018\", https://www.youtube.com/watch?v=bNVe2y34dnMChristian Slater, HP cybersecurity (\"Wolf\") edutainment on nested virt hypervisor in printers, https://www.youtube.com/watch?v=DjMSq3n3Gqs\n \nreply",
      "While VMs do have an attack surface, it is vastly different than containers, which as you pointed out are not really a security system, but simply namespaces.Seacomp, capabilities, selinux, apparmor, etc.. can help harden containers, but most of the popular containers don't even drop root for services, and I was one of the people who tried to even get Docker/Moby etc.. to let you disable the privileged flag...which they refused to do.While some CRIs make this easier, any agent that can spin up a container should be considered a super user.With the docker --privlaged flag I could read the hosts root volume or even install efi bios files just using mknod etc, walking /sys to find the major/minor numbers.Namespaces are useful in a comprehensive security plan, but as you mentioned, they are not jails.It is true that both VMs and containers have attack surfaces, but the size of the attack surface on containers is much larger.\n \nreply",
      "One can definitely build a container runtime that uses virtualization to protect the hostFor example there is Kata containershttps://katacontainers.io/This can be used with regular `podman` by just changing the container runtime so there\u2019s no even need for any extra toolingIn theory you could shove the container runtime into something like k8s\n \nreply",
      "> ... drastically restrict the syscall API surface available to the sandboxee, which quickly reduces its value ...Depends I guess as Android has had quite a bit of success with seccomp-bpf & Android-specific flavour of SELinux [0]> Until we have a properly hardened and memory safe OS ... faster than running MicroVMs on a Linux host.Andy Tanenbaum might say, Micro Kernels would do just as well.[0] https://youtu.be/WxbOq8IGEiE\n \nreply",
      "You also have gVisor, which runs all syscall through some Go history that's supposedly safe enough for Google.\n \nreply",
      "Importantly I'd like to see the configurations of the machines. There's a lot you can do to docker or systemd spawns that greatly vary the security levels. This would really help show what needs to be done and what configurations lead to what risks.Basically I'd love to see a giant ablation\n \nreply"
    ],
    "link": "https://github.com/microsandbox/microsandbox",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Self-Hosted Plaform for Secure Execution of Untrusted User/AI Code\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Ever needed to run code you don't fully trust? Whether it's AI-generated code, user submissions, or experimental code, the traditional options all have serious drawbacks:microsandbox combines the best of all worlds:Get started in few easy steps:[ASCIINEMA \u2192]Tipmicrosandbox server is also an MCP server, so it works directly with Claude, Agno and other MCP-enabled AI tools and agents.For more information on setting up the server, see the self-hosting guide.NoteThere are SDKs for other languages as well! Join us in expanding support for your favorite language.microsandbox offers a growing list of sandbox environment types optimized for different ex"
  },
  {
    "title": "Ray Tracing in J (nprescott.com)",
    "points": 38,
    "submitter": "todsacerdoti",
    "submit_time": "2025-05-28T17:44:43 1748454283",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=44118646",
    "comments": [
      "A few important bits this left out (as far as I read, which wasn't too far). Note: I am not a J expert, just a dabbler.J's tacit syntax can generally transparently take either a single argument on the right, or two arguments, one on the left and one on the right.In addition to the fork described in the article, J defines a \"Hook\" for two verbs (instead of the fork's three). A hook applies the right verb to the right argument, then applies the left verb with the original argument on the left and the result from the right verb on the right. Meaning:   this gets the largest item from a list >./ \n   this divides the left by the right %\n   so this scales every item in a list, so the largest becomes 1 and everything else becomes its ratio to the largest: %>./\n\nJ allows arbitrarily long strings of verbs: these get forked and hooked until you go insane trying to track it all in your brain.Defining a function longer than a fork and using the same code inline can (often?) not give the same results. I think that's why the caps are needed in the magnitude function in the article.I think the article is missing a trick on the magnitude-of-a-vector bit: J has a marvelous conjunction called \"under\" which, when applied to two verbs, first applies the first (right) verb, then applies the second verb to the result, and then unapplies the first verb.So when you have the need to \"sqrt the sum of the squares\" you should immediately be noticing that sqrt and square are opposites, and be thinking \"under\".   Under is &.:\n   Sum is +/\n   Square is *:\n\nSo magnitude can be expressed more succinctly (and I think more idiomatically, but again I'm not an expert) as:magnitude =: +/&.:*:magnitude 3 4   5\n\nmagnitude 3 4 5   7.07107\n \nreply",
      "\"[verbs] get forked and hooked until you go insane trying to track it all\"J comes with a Qt IDE, which has a function \"dissect\" that displays a graphical parse tree of an expression.load 'debug/dissect'dissect '(+/ % #) ? 10 $ 100'\n \nreply",
      "Could someone write a script to find out how many times \"<problem> in <single letter>\" has been posted to HN?\n \nreply",
      "Yea but ray tracing is super fun. It's hard to be irritated with fun.\n \nreply",
      "what if i write one in J and then post it on HN?\n \nreply",
      "Only if it counts itself\n \nreply"
    ],
    "link": "https://idle.nprescott.com/2020/ray-tracing-in-j.html",
    "first_paragraph": "I've been reading up on J and decided another small project was in\norder, this time I've written a minimal ray tracer.J is, perhaps infamously, a terse language. I think it may sometimes\nget unfairly discounted for this brevity because of the flavor of the\nsyntax (namely, ASCII). It can be a little hard to describe, but the\nsyntax isn't the hard part of learning J, the jarring parts are a\nresult of the paradigm differences in array programming. By the time\nI've started to wrap my head around the very functional, array\noriented style of programming the syntax has started to fall away. At\nleast in the case of my own code I don't find the syntax being an\nobstacle, I'm curious how this would hold up in the face of someone\nelse's code.One piece of syntax that bears explaining is J's means of tacit\nprogramming (that is, functions which make no reference to their\narguments). While there is a bit more depth to it than I bother using,\nI've limited myself to using mostly trains of three verbs, s"
  },
  {
    "title": "Jerry Lewis's \"The Day the Clown Cried\" discovered in Sweden after 53 years (thenationalnews.com)",
    "points": 127,
    "submitter": "danso",
    "submit_time": "2025-05-30T20:27:43 1748636863",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=44139592",
    "comments": [
      "> I want to sell it to a serious producer who either restores it or keeps it locked away, or restores it and shows it to people for studying purposes.How is someone who \"keeps it locked away\" even an option if he believes it \"must be seen\"?This seems like the perfect candidate for going on archive.org, if the goal is for it to be preserved and for people to see it.I also find it odd that he's been screening it for friends since the 80s, yet has only shown it to 24 people.\n \nreply",
      "You can find old film reels from random places. I used to work at a film post house where people would bring us film they purchased without really knowing what it was. One person scored several features from an old broadcaster from when they would actually broadcast straight from the telecine. Some of these might be in the public domain now, but most were not. A lot of them reeked of vinegar and beyond saving.\n \nreply",
      "What is the minimum number of people you would expect any arbitrary person to screen a film for?? If he started with a handful and met a new one every couple years, that's \"odd\" to you? Why do you even have an internal concept of what that number should be?\n \nreply",
      "If the movie was really good (as implied by saying they people need to see it), I\u2019d expect the initial batch to tell some friends, and then those people to tell some friends. Some organic growth at least in his local area where people are pushing him for more showing to show friends.I don\u2019t know what I was expected, but it was higher than 25 over 45 years.\n \nreply",
      "Organic growth? For inviting people into his home to watch a movie he obviously kept a secret?\n \nreply",
      "If he stole it, how does he have the right to sell it?\n \nreply",
      "he wouldn't. but eventually, it'll become part of the public domain. at that point, he could release his \"work output\" and own the copyright on that. that new work could be sold. i worked with someone that did this very thing of restoring copies of old films and released them on DVD\n \nreply",
      "In the us, unpublished works created before 1978 are copyrighted until 70 years after the death of the author (Sweeden might be different though).So you might be waiting a long time.> he could release his \"work output\" and own the copyright on thatProbably not in the united States, but other countries (i know UK at the very least) this would be true. The united states requires \"creative decisions\" to grant copyright. Work output by itself doesn't count.You could still distribute it, you just couldn't copyright it.\n \nreply",
      "Just putting the film up on a telecine and transferring it requires creative decisions, so I'm not really sure what you're on about. You're commenting like you know what you're talking about, but you clearly are not familiar with the process. This is something I've absolutely worked on projects to do this very thing. When scanning a film print/negative, there are many decisions to be made that would make yours different than the originals. How far do you zoom in/out on each frame. Does it need pan&scan. Was it shot 4:3, but now you're transferring it to 16:9?? The color decisions will also be unique. Was it B&W, the same applies to the grade. Were there film scratches, dirt, etc that you've now removed/restored? Every single one of these decisions is a creative decisions.\n \nreply",
      "https://en.wikipedia.org/wiki/Bridgeman_Art_Library_v._Corel... is what i'm \"on about\".While all those decisions may feel creative to you, its highly questionable whether they are \"creative\" according to the law (in usa anyways)\n \nreply"
    ],
    "link": "https://www.thenationalnews.com/arts-culture/film-tv/2025/05/29/jerry-lewis-day-the-clown-cried-discovered/",
    "first_paragraph": "CultureFilm & TVMay 29, 2025One of cinema's most sought-after lost films has been discovered after having been kept secretly in the collection of a Swedish actor for 45 years. Comedian Jerry Lewis's controversial holocaust film The Day the Clown Cried, shot in 1972 but never released, was thought to not exist in finished form.But Hans Crispin, star of the beloved 1980s Swedish TV series Angne & Svullo, claims he stole a complete workprint of the film from the archives of its production studio in 1980 \u2013 and has been screening it for guests in his apartment ever since. \u201cI have the only copy,\u201d Crispin told Swedish state news broadcaster SVT. \u201cI stole it from Europafilm in 1980 and copied it to VHS in the attic where we copied other films at night.\u201cI've kept the copy in my bank vault,\u201d Crispin added.Crispin recently screened a full copy to journalists from SVT and Sweden's Icon magazine to prove his claim was true. \u201cYou're the 23rd and 24th people I've shown it to,\u201d he told Icon and SVT. T"
  },
  {
    "title": "Revenge of the Chickenized Reverse-Centaurs (pluralistic.net)",
    "points": 40,
    "submitter": "GreenWatermelon",
    "submit_time": "2025-05-28T16:55:08 1748451308",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=44118055",
    "comments": [
      "Reading this made my blood boil up a little> In labor circles, \u201cchickenization\u201d refers to exploitative working arrangements that resemble the plight of the American poultry farmer. The U.S. poultry industry has been taken over by three monopolistic packers, who have divided the nation up into exclusive territories, so that each chicken farmer has only one buyer for their birds.> Farmers are \u201cindependent small businesspeople\u201d who nominally run their own operations, but because all their products must be sold through a single poultry processor, that processor is able to exercise enormous control over the operation. The processor tells the farmer which birds to raise, as well as what the birds are to be fed, how much, and on what schedule. The processor tells the farmer how to build their coops and when the lights are to go on and off. The processor tells the farmer which vets to use, and tells the vets which medicines to prescribe.> The processor tells the farmer everything\u2026except how much they\u2019ll be able to sell their birds for. That is determined unilaterally when the farmer brings their birds to market, and the payout is titrated to the cent, to represent exactly enough money for the farmer to buy birds and feed and vet services through the processor\u2019s preferred suppliers, and to service the debts on the coops and light and land, but not one penny more.This amount of scumminess is mind boggling.\n \nreply",
      "I've seen this referred to as a treadmill before. They get the farmers on a treadmill (loans for co-ops, equipment inputs) and once they're waking, they don't let them stop.The craziest thing is this is well known trick, historically. In the 1800s there was a company run by a man that was both wholesale buyer of fish from fishermen and also supplied the mortgages for fishing boats. He was the only one for both in many small fishing communities and was universally hated for it.\n \nreply",
      "This is monopsony, right? Effectively?\n \nreply",
      "Yes, for some reason the term \"monopsony\" is little known, so much that the autocorrect tried to turn it into \"monopoly\". Maybe for not having a famous board game named after it.But here it looks even worse as workers have to invest into equipment from the company that is of little use besides working for that company, making it borderline slavery.\n \nreply",
      "Unfortunately, unionization is not \u201ccoming back with a vengeance\u201d in the United States and it is not allowed to by our laws.Our laws ban: sector unionization, sympathy and general strikes, and secondary boycotts.On top of that, we have a very narrow definition of employee, an employers can permanently replace striking workers. The right to strike can even be taken away with a mandated cooling off period.Even having one of those factors can hamstring unionization in a country, so they\u2019re pretty much never going to \u201ccome back with a vengance\u201d here\n \nreply",
      "> Our laws ban ... secondary boycottsI never understood this. How can boycotts be banned? Seems inherently unconstitutional to me.Not that it really matters to me in the sense that it's definitely an area where I'd do as I pleased regardless of the law. But I've always found the headlines that I see from time to time odd.\n \nreply",
      "You lost me at \u201c\u2026 such as when a human chess master and a chess-playing computer program collaborate to smash their competition.\u201dThe ideia that a chess IA needs a human to be able to win is laughable. No human being even close to be capable of playing chess in the level of alpha zero.\n \nreply",
      "You'd better start laughing, because humans plus computers beat computers, albeit with a very high draw rate, when large amounts of time are allowed.https://new.uschess.org/edwards-32nd-ICCF-ch\n \nreply",
      "I think the autor reference is from a chess master bing helped by an IA.> (\u2026) that\u2019s not the kind of centaur that we talk about when it\u2019s a chess master paired with a chess program. That chess master is being augmented by the machine, and the machine is the junior partner in the relationship. The human is the head, and the AI is the body.\n \nreply",
      "stockfish + human beats stockfish OR there are no new commits to stockfish that increase its ELO OR all new commits to stockfish are coded by AI.currently we are in option 1\n \nreply"
    ],
    "link": "https://pluralistic.net/2022/04/17/revenge-of-the-chickenized-reverse-centaurs/",
    "first_paragraph": "Pluralistic: Daily links from Cory DoctorowNo trackers, no ads. Black type, white background. Privacy policy: we don't collect or retain any data at all ever period.In AI circles, a \u201ccentaur\u201d describes a certain kind of machine/human collaboration, in which \u201cdecision-support\u201d systems (which the field loves to call \u201cAI\u201ds) are paired with human beings for results that draw upon the strengths of each, such as when a human chess master and a chess-playing computer program collaborate to smash their competition.In labor circles, \u201cchickenization\u201d refers to exploitative working arrangements that resemble the plight of the American poultry farmer. The U.S. poultry industry has been taken over by three monopolistic packers, who have divided the nation up into exclusive territories, so that each chicken farmer has only one buyer for their birds.Farmers are \u201cindependent small businesspeople\u201d who nominally run their own operations, but because all their products must be sold through a single poult"
  },
  {
    "title": "StackAI (YC W23) Is Looking for SWR and Tailwind Wizards (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-05-30T21:02:07 1748638927",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/stackai/jobs/C1rOopy-frontend-engineer",
    "first_paragraph": "AI Agents for the EnterpriseAbout Stack AIResponsibilities:We need 10X engineers to make this happen! True wizards that can deliver on a daily basis in the high-paced environment.In this role you will...Required Skills and Qualifications:You will be a great fit if you have...Preferred Qualifications:You will be an exceptional fit if you also have...Beyond these skills, what we value the most is your ability to remain curious and open to learning new skills. As a early-stage startup joiner, you may need to wear many hats.Stack AI is a no-code drag-and-drop tool to quickly design, test, and deploy AI workflows that leverage Large Language Models (LLMs), such as ChatGPT, to automate any business process.Our core value is to make it extremely easy to build arbitrarily complex AI pipelines using a visual interface that allows you to connect different data sources with different AI models.Our customers use Stack AI to build applications such as:\u00a9 2025 Y Combinator"
  },
  {
    "title": "Silicon Valley finally has a big electronics retailer again: Micro Center opens (microcenter.com)",
    "points": 96,
    "submitter": "modeless",
    "submit_time": "2025-05-30T22:24:09 1748643849",
    "num_comments": 56,
    "comments_url": "https://news.ycombinator.com/item?id=44140378",
    "comments": [
      "The Micro Center in Cambridge, MA, has improved a lot over the years.When they have the thing that I want, I'd prefer to go there, rather than order online.Also, I've never seen opened returns re-shinkwrapped and sold again as new at Micro Center, unlike in stories about Fry's.  There are some wire shelves where opened returns are sold at a small discount, clearly labeled.Incidentally, would be nice to also have a good surplus and e-cycling store browsing adventure in town.  But I guess the economics are difficult, when real estate is so expensive, and most of the few customers for unusual stuff are online.  (That local hobbyists could save a lot of money on shipping cost of decommissioned corporate and lab gear, or make impulse purchases they wouldn't online, probably isn't enough, I'd guess.)\n \nreply",
      ">When they have the thing that I want, I'd prefer to go there, rather than order online.Worth mentioning that they price match AmazonI bought a CPU cooler there a few months back - the guy at checkout told me to pull up the Amazon listing on my phone so he could knock some cash offhttps://community.microcenter.com/kb/articles/6-do-you-price...\n \nreply",
      "How is that possible? Amazon has to have lower overheads than a brick and mortar.\n \nreply",
      "Amazon prices what they can get away with, not what their costs are. Jeff Bezos\u2019s rocketry hobby is a testament to Amazon\u2019s ability to extract surplus.\n \nreply",
      "I don't recall ever asking for a price match at a brick&mortar, even though I'm aware it's available at some stores.  I'd guess most people don't.The store gets some mileage out of being known for price-matching competitors (even online competitors, where that'd be a bit much).(Well, occasionally I have questioned in-store, when a major chain shows one price on the Web, available at a specific brick&mortar location, but when you get to that location, there's a much higher price on the shelf.  Now I tend to order for pickup at those stores, which is more work for them, just to lock in the Web-advertised price, rather than the switcheroo price.)\n \nreply",
      "They hope you'll buy a soda and a magazine or whatever. They always try to upsell protection plans.\n \nreply",
      "Best Buy will price match via online chat.\n \nreply",
      "That store is dangerous. The last few times I went \"just to browse\" but I came home with an ultra wide monitor and a new PC build.I've started buying parts retail instead of online just because of how much I enjoy Microcenter. The interior does need a bit of a renovation though, it looks almost identical to how it did in the 90s.\n \nreply",
      "I was literally just there an hour ago, buying $800 worth of gear for a Wi-Fi mesh buildout.Could I have gotten it cheaper online? Probably. But when you have 36 hours notice that you need to build out Wi-Fi, you can't beat Micro Center.\n \nreply",
      "Not quite the same as storefront surplus, but: https://w1mx.mit.edu/flea-at-mit/\n \nreply"
    ],
    "link": "https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx",
    "first_paragraph": "We are using cookies to give you the best experience on our website. You can view our Privacy Policy and information regarding our use of cookies here for more information about cookies.\nAccept Accept Privacy Policy and Terms\nStore Locator:After years of waiting, the ribbon has been cut and Micro Center Silicon Valley is officially open.\u00a0On a sunny Friday morning in Santa Clara, with hundreds of fans queued in a line wrapping down the block and around the corner, we welcomed the Silicon Valley community to our newest store, at 5201 Stevens Creek Blvd.\u00a0If you're a DIY PC builder, a serious gamer, a creator, a maker, or just someone who gets excited over the latest CPUs, GPUs, and 3D printers, then you already know what Micro Center is aboutThe Bay Area sets a high bar for all things tech, and here you'll find aisles stacked high with components, knowledgeable staff who actually know what they're talking about, and a hands-on experience you just can't replicate online.\u00a0The grand opening "
  },
  {
    "title": "The radix 2^51 trick (2017) (chosenplaintext.ca)",
    "points": 388,
    "submitter": "blobcode",
    "submit_time": "2025-05-30T03:55:37 1748577337",
    "num_comments": 76,
    "comments_url": "https://news.ycombinator.com/item?id=44132673",
    "comments": [
      "With AVX512 (and to a lesser extent with AVX2) one can implement 256 bit addition pretty efficiently with the additional benefit of fitting more numbers in registers.It looks more or less like this:  __m256i s = _mm256_add_epi64(a, b);\n  const __m256i all_ones = _mm256_set1_epi64x(~0);\n  int g = _mm256_cmpgt_epu64_mask(a, s);\n  int p = _mm256_cmpeq_epu64_mask(s, all_ones);\n  int carries = ((g << 1) + p) ^ p;\n\n  __m256i ret = _mm256_mask_sub_epi64(s, carries, s, all_ones);\n\nThe throughput even seems to be better: https://godbolt.org/z/e7zETe8xYIt's trivial to change this to do 512 bit addition where the improvement will be even more significant.\n \nreply",
      "Note that, especially on certain Intel architectures, using AVX512 instructions _at all_ can result in the whole processor downclocking, and thus ending up resulting in inconsistent / slower overall performance.https://stackoverflow.com/questions/56852812/simd-instructio...\n \nreply",
      "> using AVX512 instructions _at all_This isn't correct. AVX512 provides both a bunch of extra instructions, zmm (512 bit) registers, and an extra 16 (for a total of 32) vector registers. The donwnclocking only happens if you use 512 bit registers (not just avx512 instructions). The difference here matters a bunch since there are a bunch of really useful instructions (e.g. 64 bit integer multiply) that are added by avx512 that are pure upside.Also none of this is an issue on Zen4 or Zen5 since they use much more sensible downlclocking where it will only downclock if you've used enough instructions in a row for it to start spiking power/temp.\n \nreply",
      "Ah yes, you\u2019re completely correct :)General idea was just to highlight some of the dangers of vector registers. I believe the same is true of ymm (256) to a lesser extent.\n \nreply",
      "> Aside: Why 13 bits instead of 12? For our purposes, we\u2019re going to ignore the carries in the most significant limb, allowing numbers to wrap when they overflow past 2256 - 1 (just like how unsigned addition works in C with normal size integer types). As a result, we can assign 52 bits to the most significant limb and ignore the fact that it will run out of room for carries before the other limbs do.Why not give the top limb 64 bits and the other four limbs 48 bits each, then? You can accumulate more additions before normalization, you can take advantage of word alignment during splitting and normalization if your instruction set has anything useful there, and your overflow properties are identical, no?\n \nreply",
      ">> Why not give the top limb 64 bits and the other four limbs 48 bits each, then?I think one goal is to use 5 64 bit registers to do 256 bit math. That means using 256/5 = 51.2 bits of each word. That's probably some kind of ideal if you want 256bit math, but not optimal if you're writing a generic big-int library. In the old days you'd want to use exactly one byte for the carry(s) because we didn't have barrel shifters to do arbitrary bit shifts efficiently. In that case I'd use 56 bits of the 64 to get nice byte alignment.This is all quite relevant for RISC-V since the ISA does not have flags.\n \nreply",
      "Even with this explanation a 64 + 48*4 is clearly superior. You can go longer without overflow (since you have 16 bits of carry space per pseudo-digit), and the amount of carry space is aligned even more nicely.\n \nreply",
      ">That means using 256/5 = 51.2 bits of each word.Why must each word have the same amount? Why not 64 bits on the top word, and 48 bits on the other 4 words?\n \nreply",
      "Evenly distributing the number of bits per word lets you chain more additions/subtractions before having to normalize.\n \nreply",
      "Sure, but the point is that for the most significant limb, there is no point in having redundant bits because whatever you put in them will be discarded after normalization.\n \nreply"
    ],
    "link": "https://www.chosenplaintext.ca/articles/radix-2-51-trick.html",
    "first_paragraph": "Faster addition and subtraction on modern CPUsDo you remember how to do long addition on paper?Starting from the \u201cones\u201d position, we add 6 + 6 = 12, write down a 2 and carry a 1.\nWe proceed to the left, one position at a time, until there are no more digits\nto add.When implementing addition for large integers (e.g. 264 and above), it\u2019s common to write\ncode that looks quite similar to this algorithm.\nInterestingly, there\u2019s a straightforward trick that can speed up this\nprocess enormously on modern CPUs.But first, a question: why do we start long addition with the \u201cones\u201d?\nWhy not start on the left?The answer, of course, is the carries.\nWe can\u2019t figure out for sure what a given digit of the answer will be\nuntil we\u2019ve completed all of the additions to the right of that digit.Imagine if we tried to add left-to-right instead:6 + 3 = 9. So the first digit is 9.\n8 + 4 = 12. OK, the second digit is 2\u2026 but carry a 1, so the first digit\nwas actually 9 + 1 = 10\u2026 now carry back that 1\u2026For mental ma"
  },
  {
    "title": "Copy Excel to Markdown Table (and vice versa) (thisdavej.com)",
    "points": 72,
    "submitter": "thisdavej",
    "submit_time": "2025-05-30T00:32:11 1748565131",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=44131669",
    "comments": [
      "Worked on something very similar recently at work.If you want to respect more of Excels formatting, like multi column cells, you can get the text/html from the paste event instead of the text/plain.You can parse the HTML using DOMParser: parseFromString()\n \nreply",
      "FWIW I built a streamlit app to extrapolate tribal knowledge in excel trackers into markdown wikis for vector database ingestion. Instead of uploading raw tables, it maps sheet headers to real headings to wrap each section in wiki-type format context pages. The UI lets you pick out QA sections from local files, but I\u2019m stuck on how to persist selections and configs for repeat runs. Curious how others would tackle the issue of repeatable settings.Code\u2019s here: https://github.com/devin-liu/excel-to-markdown\n \nreply",
      "This is a very handy tool.For those of us who sometimes have to do presentations but don\u2019t want to (learn) do full blown slidedecks I have a feature request:If you could add googlesheets to mermaid.js table or googlesheets straight to png of a markdown table that would be so nice. As I recall Mermaid.js renders to png.Easy way to build tables, I can just insert the image of the table.\n \nreply",
      "Nice. There's also a good VS Code plugin for doing this: \nhttps://marketplace.visualstudio.com/items?itemName=csholmq....And of course, markdowntools (multiple conversion tools):\nhttps://www.markdowntools.com/\n \nreply",
      "I prefer https://tableconvert.com/\n \nreply",
      "I use this one https://www.tablesgenerator.com/\n \nreply",
      "Thanks, never seen that one. It's quite slick.\n \nreply",
      "I made a version[0] of this years ago inspired by something similar in MailChimp using a pasted spreadsheet. Mine converts to Markdown and simpler text. I used it to send table data as plain text emails[0] http://mirrodriguezlombardo.com/Tablas-simples/\n \nreply",
      "If you have to go from an HTML table to Markdown, you can use Table Capture: https://chromewebstore.google.com/detail/table-capture/iebpj...\n \nreply",
      "Cool tool!Anyone have recommendations for a command line solution on Linux? :)\n \nreply"
    ],
    "link": "https://thisdavej.com/copy-table-in-excel-and-paste-as-a-markdown-table/",
    "first_paragraph": ""
  },
  {
    "title": "How to Run CRON Jobs in Postgres Without Extra Infrastructure (wasp.sh)",
    "points": 40,
    "submitter": "Liriel",
    "submit_time": "2025-05-28T10:14:11 1748427251",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=44114436",
    "comments": [
      "Tangential since it's not PG related but I'm more and more moving away from cron and I prefer using systemd timers (I'm on RHEL at work). I just find the interface to list and manager timers better and I can just handle everything like a systemd service anyways.\n \nreply",
      "What is the systemd equivalent for `service crond stop` and later `service crond start`?In other words, I want to disable all jobs for some time (for benchmarking) and then bring them back up.\n \nreply",
      "Maybe you could make a target unit file like \u201cjobs.target\u201d and in your timer unit files do \u201cWantedBy=jobs.target\u201d. Then you could do \u201csystemctl start/stop jobs.target\u201d\n \nreply",
      "First, list and save the currently active timers:\n```bash\nsystemctl list-timers --state=active --no-legend | awk '{print $NF}' > /tmp/active_timers.txt\n```Stop all active timers:\n```bash\nsudo systemctl stop $(cat /tmp/active_timers.txt)\n```Later, restart the previously active timers:\n```bash\nsudo systemctl start $(cat /tmp/active_timers.txt)\n```\n \nreply",
      "Cron isn't an acronym; it's not normally written in all caps.Cron's name originates from Chronos, at least according to Wikipedia.\n \nreply",
      "I have nothing against pg_boss[0] from the articel (I don't know anything about it), but there are plenty of queues and crons and schedulers for PGSome others:* https://github.com/LaunchPlatform/bq* https://github.com/cybertec-postgresql/pg_timetable* https://github.com/pgmq/pgmq* https://github.com/riverqueue/river* https://github.com/oban-bg/oban* https://github.com/pgadmin-org/pgagent* https://github.com/citusdata/pg_cronetc. There are plenty of options to choose from.0: https://github.com/timgit/pg-boss\n \nreply",
      "Gonna toss my own hat in the ring there for the python+postgres ecosystem :)https://github.com/tktech/chancy> As a rule of thumb, if you're processing less than 1000 jobs per day or your jobs are mostly lightweight operations (like sending emails or updating records), you can stick with this solution.This seems... excessively  low? Chancy is on the heavier side and happily does many millions of jobs per day. Postgres has no issue with such low throughput, even on resource constrained systems (think a $5 vps). Maybe they meant 1000 per second?\n \nreply",
      "Also worth mentioning: https://www.pgflow.dev/\n \nreply",
      "No mention of pg_cron?\n \nreply",
      "apples and oranges?pg_cron is for pg specific cron tasks. You use pg_cron to truncate a table, compute pg views, values, aggregates, etc. Basically just running PG queries on a CRON schedule.pg_cron itself won't run an external script for you. Like you can't do    SELECT cron.schedule('0/30 * * * *', $$ ./sendEmails.sh $$);\n\n\nyou can use pg_cron to insert a job-row in a jobs table that you have some consumer that runs a `select * from jobs where status = 'pending' limit 1;`. Then you're on the hook to handle the pg updates for dispatching and handling updates, job status, etc. You could even call that implementation pg-boss if it's not taken.\n \nreply"
    ],
    "link": "https://wasp.sh/blog/2025/05/28/how-to-run-cron-jobs-in-postgress-without-extra-infrastructure",
    "first_paragraph": ""
  },
  {
    "title": "Cap: Lightweight, modern open-source CAPTCHA alternative using proof-of-work (capjs.js.org)",
    "points": 114,
    "submitter": "tiagorangel",
    "submit_time": "2025-05-30T16:36:10 1748622970",
    "num_comments": 82,
    "comments_url": "https://news.ycombinator.com/item?id=44137867",
    "comments": [
      "Did no-one click through to the technical white paper?https://www.researchgate.net/publication/374638786_Proof-of-...\"Proof-of-Work CAPTCHA with password cracking functionality\"The \"work\" is \"to use the distributed power of webusers\u2019 computers\" to \"obtain suspects\u2019 passwords in order to access encrypted evidence\" and \"support law enforcement activities\".Funny how that isn't mentioned anywhere in the linked site.\n \nreply",
      "> Normally, it is undesirable for users\u2019 passwords to be cracked. However, in the case of law enforcement, we often need to obtain suspects\u2019 passwords in order to access encrypted evidence. The obvious solution is to build powerful (and expensive) dictionary cryptanalysis computers. A less obvious approach is to use the distributed power of web users\u2019 computers, as has been done in the Seti@Home (https://setiathome.berkeley.edu/ \u2014 suspended project) or Folding@Home projects (https://foldingathome.org/). The proposed approach can therefore support law enforcement activities while providing the desired functionality to the web community\"You're not allowed to visit this website unless you submit your computer to being part of the fed's password cracking botnet\" that's a whole fresh hell. A better use case is right there in their own description! I'd love my captchas to be little Folding@Home problems.\n \nreply",
      "Can't we just submit bogus hashes?\n \nreply",
      "That is shady as hell. Welp this is dead on the vine\n \nreply",
      "Definitely concerning, although I'm having trouble finding anything in the codebase to support this.This paper even seems to contradict aspects of the project's no tracking stance. If someone told me this paper was for a different (but similar) project, I'd believe it after looking at the two side by side.Would definitely want this to be addressed before I'd consider using it.\n \nreply",
      "There are two binaries commited to the repo (cap_wasm_bg.wasm) but from what I can tell, it doesn't seem to be making any network calls or what have you. They still should get rid of them and add a Rust build step for their browser/node packages.\n \nreply",
      "Interesting discovery. This research sounds creepy and ill-advised, but my intuition suggests to me this is an innocent attempt to do something useful rather than waste energy on a PoW algorithm. My intuition also tells me that if this project became popular enough, attackers would break the algorithm fairly easily and the project would just revert to a more conventional PoW algorithm that doesn't try to be smart.\n \nreply",
      "I think there's a good chance they just linked to the paper for technical background, unrelated to the paper's mention of law enforcement usage. The website mentions self-hosted, no third-party requests, etc. Unless they're flat-out lying.\n \nreply",
      "> @cap.js/solver is a standalone library that can be used to solve Cap challenges from the server. Doesn't this defeat the purpose of Cap? Not really. Server-side solving is a core use case of proof-of-work CAPTCHAs like Cap or altcha. It's about proving effort, not necessarily involving a human.I like this. Allows for reasonable bots like IA without the mindless wasteful AI scrappers.\n \nreply",
      "Isn't IA's architecture pretty strained already without this?\n \nreply"
    ],
    "link": "https://capjs.js.org/",
    "first_paragraph": "AppearanceCap is a lightweight, modern open-source CAPTCHA alternative using SHA-256 proof-of-workCap's widget library is extremely small, only ~20kb minified (including WASM)Cap's usage of proof-of-work eliminates the need for any tracking, fingerprinting or data collectionCap is self-hostable so you can customize both the backend & frontend (or you can just use CSS variables)Cap uses PoW instead of complex puzzles, making it easier for humans and harder for botsCap offers a standalone mode with Docker, allowing you to use it with languages other than JSCap can run invisibly in the background using a simple JS APIFloating mode keeps your CAPTCHA hidden until it's neededCompletely open source under the Apache 2.0 licenseCap is a lightweight, modern open-source CAPTCHA alternative using SHA-256 proof-of-work. It's fast, private, and extremely simple to integrate. Learn more about proof-of-work here.Cap is built into 2 main parts:@cap.js/widget: A small JavaScript library that renders th"
  }
]