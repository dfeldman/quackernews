[
  {
    "title": "Infinite Grid of Resistors (mathpages.com)",
    "points": 86,
    "submitter": "niklasbuschmann",
    "submit_time": "2025-06-14T22:12:12 1749939132",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=44279181",
    "comments": [
      "I don't get why EE education emphasizes problems of this sort. The infinite grid is an extreme example, but solving weirdly complicated problems involving Kirchoff's laws and Thevenin's theorem was a common way to torture students back in my day...Here, I don't think it's even useful to look at this problem in electronic terms. It's a pure math puzzle centered around an \"infinite grid of linear A=B/C equations\". Not the puzzle I ever felt the need to know the answer to, but I certainly don't judge others for geeking out about it.\n \nreply",
      "If all you mind is the EE curriculum then ok. Or else there is an interesting work of Gerald Westendorp on the web [1] on how allowing other classical passive components (Ls & Cs) you can get discretizations (and hence alternative views) of a very wide class of iconic Physics partial differential equations (to the point that the question is more what cannot be fit to this technique). G. W. is alive and kicking in mathstodon.[1] https://westy31.nl/Electric.html\n \nreply",
      "There are two parts to education. One is to impart knowledge, the other is to filter the students.\n \nreply",
      "Not entirely wrong but it's a little too easy to use that argument for squashing any criticism for education content.\n \nreply",
      "I was about to say \"they still torture students this way\" but stopped myself when I remembered I took Circuits 1 and 2 back in 2007. So maybe my knowledge is dated too...It's a weird butterfly effect moment in my career though. I had an awesome professor for circuits 1, and ended up switching majors to EE after that. Then got two more degrees on top of the bachelor's\n \nreply",
      "I'm a bit mathematician and a bit electrical engineer.The electrical engineer suggests it's not measurable unless you apply current and also asks \"when\" after the current is applied referring to the distributed inductive and capacitive element and the speed of field propagation. The mathematician goes to a bar and has a stiff drink after hearing that.\n \nreply",
      "Eventually you need to pullin a physicist too who will point out that at an appropriate distance quantum effects will dominate - because eventually at a far enough distance  the number of electrons moving per second (ie current flow) will be either 0 or 1 at some nodes\n \nreply",
      "Intuitively I knew this class of problem was theoretical only BS when it came up in college...I hadn't considered that sort of strange effect though!  Makes me feel not so bad for 'never really getting it' because I just couldn't wrap my mind around the problem description's obvious inanity and the infinite edges.\n \nreply",
      "Given an infinite grid of resistors... would you expect planets to form?\n \nreply",
      "They say hydrogen is an odorless colorless gas which, in sufficient quantities, given enough time, turns into people. I\u2019m sure the same could be true of resistors.\n \nreply"
    ],
    "link": "https://www.mathpages.com/home/kmath668/kmath668.htm",
    "first_paragraph": ""
  },
  {
    "title": "I have reimplemented Stable Diffusion 3.5 from scratch in pure PyTorch (github.com/yousef-rafat)",
    "points": 331,
    "submitter": "yousef_g",
    "submit_time": "2025-06-14T13:56:46 1749909406",
    "num_comments": 61,
    "comments_url": "https://news.ycombinator.com/item?id=44276476",
    "comments": [
      "If you are interested in this: Flux reference implementation is very minimalistic: https://github.com/black-forest-labs/flux/tree/main/src/fluxThe minRF project is very easy to start with training small diffusion models with rectified flow: https://github.com/cloneofsimo/minRFAlso, the reference implementation of SD 3.5 is actually minimalistic too: https://github.com/Stability-AI/sd3-ref\n \nreply",
      "Reference implementations are unmaintained and buggy.For example https://github.com/huggingface/transformers/issues/27961 OpenAI's tokenizer for CLIP is buggy, it's a reference implementation, it isn't the one they used for training, and the problems with it go unsolved and get copied endlessly by other projects.What about Flux? They don't say it was used for training, it wasn't, there are bugs with it that break cudagraphs or similar that aren't that impactful. On the other hand, it uses CLIP reference, and CLIP reference is buggy, so this is buggy...\n \nreply",
      "Congrats on finding a bug!However, the keyword here is training / inference divergence. Unfortunately, nobody is going to spend multi-million to retrain a model, so our reimplementation needs to be bug-to-bug correct to use the trained weights properly. That's why the reference implementations are essential because it is from the original model trainers so you have the best \"bet\" on matching the training code properly.To give you some concrete example of bugs we needs to maintain:1. In SDXL, they use OpenClipG for text encoding, but wrongfully uses 0 as padding tokens (corresponding to symbol \"!\") whereas even for OpenClipG its own training, the endoftext token was used as padding token. However, if you switching SDXL to use endoftext token as padding token, due to training / inference divergence, you get subpar generated images.2. In FLUX, we mainly use T5 as text encoder. However, T5 usually used as encoder with mask to exactly the same input length, to avoid extended impact of padding tokens. In FLUX, we don't apply mask for T5 text encoding, hence intuitively causing padding token to take more effect than it should. Again, \"fixing\" this bug without retraining you will get subpar generated images.There are many examples like this, some are easier to fix some are not (HiDream uses a different ODE solver that is different than what we usually do for rectified flow, hence you need to negate its prediction to be compatible with existing samplers, but this is \"easier to fix\").TL;DR: Yes, there are bugs in software, but we better to maintain bug-to-bug compatibility than trying to \"fix\" it, hence highlight the importance of a \"done\" reference implementation, rather than a usual \"active\" implementations in software industry otherwise.(I maintain the most complete reimplementation of SoTA media generation models in Swift: https://github.com/drawthingsai/draw-things-community/tree/m.... So I tend to think that I know one or two about \"reimplementation from scratch\".)\n \nreply",
      "I think if you read the issue carefully you would understand that the CLIP implementation in transformers and as published by OpenAI is wrong and does not match their trained model code; and that doing the fix I suggest, empirically for me and in theory, improves results.\n \nreply",
      "You can disable clip l on flux without a loss in quality. You are also making an elephant out of a fly. CLIP is used everywhere.\n \nreply",
      "Consider another interpretation: CLIP L in Flux can be disabled without a loss in quality because the way it is used is buggy!\n \nreply",
      "It shouldn't take a lot of effort to fix a tokenizer...\n \nreply",
      "People are a little too blinded by the insight porn of matching buggy behavior to just read and comprehend the issue. They can\u2019t engage with the simpler and more pornographic insight porn that the reference implementations are buggy and do not match the trained artifacts.\n \nreply",
      "I'm not sure what this means. If it means the Stable Diffusion 3.5 model, why is  it fetching that here: https://github.com/yousef-rafat/miniDiffusion/blob/main/enco...The training dataset is very small, only including fashion-related pictures: https://github.com/yousef-rafat/miniDiffusion/tree/main/data...\n \nreply",
      "The dataset is for trying out fine-tuning of the diffusion model. It's a reimplementation of SD3 by writing the code from scratch again, but the weights are taken from HuggingFace due to hardware constraints on my part.\n \nreply"
    ],
    "link": "https://github.com/yousef-rafat/miniDiffusion",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A reimplementation of Stable Diffusion 3.5 in pure PyTorch\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.miniDiffusion is a reimplementation of the Stable Diffusion 3.5 model in pure PyTorch with minimal dependencies. It's designed for educational, experimenting, and hacking purposes.\nIt's made with the mindset of having the least amount of code necessary to recreate Stable Diffusion 3.5 from scratch, with only ~2800 spanning from VAE to DiT to the Train and Dataset scripts.-Files: The main Stable Diffusion model code is located in dit.py, dit_components.py, and attention.py. The dit.py file contains the main model, dit_components.py contains the embedding, normalization, patch embedding, and help functions for the DiT code, and attention.py contains the J"
  },
  {
    "title": "Q-learning is not yet scalable (seohong.me)",
    "points": 4,
    "submitter": "jxmorris12",
    "submit_time": "2025-06-15T00:56:54 1749949014",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://seohong.me/blog/q-learning-is-not-yet-scalable/",
    "first_paragraph": "\n                Over the past few years,\n                we've seen that next-token prediction scales, denoising diffusion scales, contrastive learning scales,\n                and so on, all the way to the point where we can train models with billions of parameters\n                with a scalable objective that can eat up as much data as we can throw at it.\n                Then, what about reinforcement learning (RL)?\n                Does RL also scale like all the other objectives?\n\n                Apparently, it does.\n                In 2016, RL achieved superhuman-level performance in games like Go and Chess.\n                Now, RL is solving complex reasoning tasks in math and coding with large language models (LLMs).\n                This is great. However, there is one important caveat:\n                most of the current real-world successes of RL have been achieved with on-policy RL algorithms\n                (e.g., REINFORCE, PPO, GRPO, etc.),\n                which always req"
  },
  {
    "title": "Inside the Apollo \"8-Ball\" FDAI (Flight Director / Attitude Indicator) (righto.com)",
    "points": 126,
    "submitter": "zdw",
    "submit_time": "2025-06-14T15:43:03 1749915783",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=44277051",
    "comments": [
      "Author here for your Apollo questions :-)\n \nreply",
      "Great article. I'd never thought about a spacecraft ADI having a third axis. Sadly, a nitpick - Bill Lear's F-5 autopilot was not, as far as I can tell, in any way connected to the Northrop F-5 fighter jet.\n \nreply",
      "Thanks. You are correct about the F-5 autopilot, so I fixed that. It turns out that it was used in planes such as the C-47, C-60, C-45, and B-26, but is unrelated to the F-5.\n \nreply",
      ">The Command Module for Apollo used a completely different FDAI (flight director-attitude indicator) that was built by Honeywell.That's surprising. Was there any requirement that necessitated them to be different parts, or it's just because different suppliers were chosen by Grumman/North American?\n \nreply",
      "It's probably a combination of different suppliers being chosen, and everyone wanted a piece of the pie. But it's annoying when I figure out how something works in the Lunar Module and then discover that the Command Module is completely different. Not to mention that the Saturn V is a whole different world.\n \nreply",
      "I remember a similar thing from the, IIRC, F-104.\n \nreply",
      "I mainly remember this because he refers to it as the 'frappin 8 ball' in the Apollo 13 movie, if my memory serves.\n \nreply",
      "Yes, in the movie, Lovell says \"What's the frappin' attitude?\" as the 8-ball rolls out of control. The actual Apollo 13 transcript has nothing like that, interestingly enough.Links: https://archive.org/details/apollo1319959231994/page/n92/mod...\nhttps://www.nasa.gov/wp-content/uploads/static/history/alsj/...\n \nreply",
      "same here, he sure does\n \nreply",
      "Back in the day, this would be have been a good homework assignment for an EE analog controls class.\n \nreply"
    ],
    "link": "https://www.righto.com/2025/06/inside-apollo-fdai.html",
    "first_paragraph": "Computer history, restoring vintage computers, IC reverse engineering, and whateverDuring the Apollo flights to the Moon, the astronauts observed the spacecraft's orientation on a special instrument\ncalled the FDAI (Flight Director / Attitude Indicator).\nThis instrument showed the spacecraft's attitude\u2014its orientation\u2014by rotating a ball.\nThis ball was nicknamed the \"8-ball\" because it was black (albeit only on one side).\nThe instrument also acted as a flight director, using three yellow needles to indicate how the astronauts should maneuver\nthe spacecraft. Three more pointers showed how fast the spacecraft was rotating.An Apollo FDAI (Flight Director/Attitude Indicator) with the case removed. This FDAI is on its side to avoid crushing the needles.Since the spacecraft rotates along three axes (roll, pitch, and yaw), the ball also rotates along three axes.\nIt's not obvious how the ball can rotate to an arbitrary orientation while remaining attached.\nIn this article, I look inside an FDAI"
  },
  {
    "title": "Have a damaged painting? Restore it in just hours with an AI-generated \"mask\" (news.mit.edu)",
    "points": 19,
    "submitter": "WithinReason",
    "submit_time": "2025-06-12T17:59:23 1749751163",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=44260659",
    "comments": [
      "The restoration method is more interesting than the AI, but claiming it uses AI is probably necessary to get funding for it.\n \nreply",
      "Yes, it seems like the same method (a thin polymer mask of which a computer record exists) could be used in a completely human-driven process. Just allowing restorers to try a few different things to see what works would probably speed up their process immensely.There's probably some other grant money they can get down the line for a completely AI-less process, but obviously there's a lot more funding for AI than for art restoration.\n \nreply",
      "It's conceptually similar to current restoration techniques, but, the real innovation is in doing the whole thing as one piece ahead of time rather than requiring somebody to get in there with tiny brushes after a transparent base layer is completed.It's also worth noting that this doesn't help with the first half of most restorations, which is removing crud and wear and often painstakingly undoing previous alterations or low-quality restorations. There's actually a pretty long history of paintings being altered to fit current fashions or otherwise mucked with in ways that make modern art historians cringe.\n \nreply",
      "They could probably just tell people they're \"using AI\" and then just use whatever methods work best. It's not like investors are doing much more DD than listening for that phrase.\n \nreply",
      "Funny aside, they have an odd ligature to for \u201cfi\u201d on that site. It shows up in iOS Safari at least.> Kachkine acknowledges that, as with any restoration project, there are ethical issues to consider, in terms of whether a restored version is an appropriate representation of an artist\u2019s original style and intent. Any application of his new method, he says, should be done in consultation with conservators with knowledge of a painting\u2019s history and origins.A sort of interesting thought, \u201cwhat was the artist\u2019s intent, should be recover the painting,\u201d is a well known question nowadays. It would be sort of funny if current artists would just write down what sort of restoration plans they are ok with. I wonder how many would say \u201cjust do what you will do people can enjoy it.\u201dAlthough, one could argue maybe that the damage which occurs to artwork as it ages also tells a historical story. Perhaps that story doesn\u2019t just belong to the artist, and so restoring the work could be questionable even with their permission. I\u2019m sure this is well-trod ground.\n \nreply",
      "I hope the AI will not generate hands with seven fingers ...\n \nreply",
      "It\u2019s a perfect example for thinking about the limitations of these things. Locally fingers appear next to other fingers, with a high probability. It requires a level operation to count how many fingers exist on a component and understand their relative position.\n \nreply",
      "That was a problem a year or two ago. New AI models have gotten past that point now.\n \nreply",
      "openai native 4o image generation generates 4 fingers instead.\n \nreply",
      "Is that a joke? Because 4o image generation (assuming you click \"Image Generate\" which uses gpt-image-1) EASILY handles rendering hands with the proper number of fingers even without specifying something like that.If anything, it's actually MORE difficult to generate hands with an improper number of fingers. Apologies to Count Rugen.https://imgur.com/a/hIp5DQO\n \nreply"
    ],
    "link": "https://news.mit.edu/2025/restoring-damaged-paintings-using-ai-generated-mask-0611",
    "first_paragraph": "Suggestions or feedback?\n    Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \n    Creative Commons Attribution Non-Commercial No Derivatives license.\n    You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided \n    below, credit the images to \"MIT.\" \n  \n\n\n\n\n\n\n\n\n\n\n\n\nPrevious image\nNext image\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt restoration takes steady hands and a discerning eye. For centuries, conservators have restored paintings by identifying areas needing repair, then mixing an exact shade to fill in one area at a time. Often, a painting can have thousands of tiny regions requiring individual attention. Restoring a single painting can take anywhere from a few weeks to over a decade.In recent years, digital restoration tools have opened a route to creating virtual representations of original, restored works. These tools apply te"
  },
  {
    "title": "Chicken Eyeglasses (wikipedia.org)",
    "points": 48,
    "submitter": "thomassmith65",
    "submit_time": "2025-06-11T06:13:49 1749622429",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=44244675",
    "comments": [
      "The fact that this article doesn\u2019t include any images of chickens wearing said goggles is an injustice.\n \nreply",
      "We owned this game growing up:\nhttps://magisterrex.wordpress.com/2010/04/13/the-best-classi...It has those goggles in it.  Still remember fondly to this day (not the game, the chicken goggles).\n \nreply",
      "And contact lenses too.  A HBS case study I remember from grad school:\"Optical Distortion, Inc\"\nA new product, contact lenses for chickens, is to be introduced by a small firm formed to market the product. An entry strategy must be planned including price, sales force, size, and location. Allows data for computation of economic benefit to farmers. Includes state-by-state chicken population data for planning a rollout sales program.https://www.hbs.edu/faculty/Pages/item.aspx?num=17120\n \nreply",
      "Looks like he actually tried it:  But some ideas cannot be crushed by bankruptcy and the dream of providing lenses to all of America\u2019s hens was carried on by the son of one of Vision Control Inc.\u2019s founders, a young Mr. Randall Wise. Wise, a Harvard Business school graduate and former nautical shipping consultant, used the millions he made from selling his software company to establish Animalens, Inc.\n\n Instead of pecking at each other (success!), the hens were now pecking at the air, rubbing their eyes repeatedly on their wings, and suffering from corneal ulcers and ruptured eyes.\n\nhttps://www.vice.com/en/article/chickens-wore-sunglasses-ind...\n \nreply",
      "I looked this up after hearing about them on ABC's \nIf You're Listening podcast: https://abc.net.au/listen/programs/if-youre-listening/skunk-...\n \nreply",
      "Well, I know more about abnormal injurious behavior in birds than I did an hour ago\n \nreply",
      "I can not find pictures of chicken wearing those particular glasses depicted in this Wikipedia article.\n \nreply",
      "A friend had a job in the 70's at a research lab, and one of their duties was to use a hot iron to curl the beaks of each incoming batch of chicks, to help prevent pecking.  They called the tasking \"giving the chickens lips\".  I like the glasses solution a bit better.\n \nreply",
      "I don't get it though - how does this help prevent pecking? The only reasoning seems to be in the 1911 article, where it suggests they're made to protect the chickens' eyes.\n \nreply",
      "end of the first paragraph> the coloring was thought to prevent a chicken wearing them from recognizing blood on other chickens, which may increase the tendency for abnormal injurious behavior\n \nreply"
    ],
    "link": "https://en.wikipedia.org/wiki/Chicken_eyeglasses",
    "first_paragraph": "Chicken eyeglasses, also known as chicken specs, chicken goggles, generically as pick guards, and under other names,[2] were small eyeglasses made for chickens intended to prevent feather pecking and cannibalism. They differ from blinders in that they allow the bird to see forward, whereas blinders do not. One variety used rose-colored lenses, as the coloring was thought to prevent a chicken wearing them from recognizing blood on other chickens, which may increase the tendency for abnormal injurious behavior. They were mass-produced and sold throughout the United States as early as the beginning of the 20th century.[3][4]\nChicken eyeglasses were often made from celluloid or aluminum[5] and typically consisted of \"two oval panels that fit over the upper beak of the chicken. A pin is put through the nostril to hold the oval pieces in place.\"[2] Different designs were produced that attached to the chicken's head in different ways. Some were held in place by a strap,[3] some by small hooks"
  },
  {
    "title": "Solar Orbiter gets world-first views of the Sun's poles (esa.int)",
    "points": 178,
    "submitter": "sohkamyung",
    "submit_time": "2025-06-11T23:00:50 1749682850",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=44252717",
    "comments": [
      "I didn't even realize that we've never seen the sun's poles before as I just assumed we already scanned our star many times over.A nice reminder of how patchy and limited our knowledge is despite the impression of the opposite.Keep up the great work, humans!\n \nreply",
      "\u2018World First\u2019 is a poor choice of words. \u2018First Ever\u2019?\n \nreply",
      "well, they are the first time they're seen on this world so I think it's fine.\n \nreply",
      "It's our world's first -- maybe the others already got it.Or better,  \"humanity's first\".\n \nreply",
      "Happened outside our world though!\n \nreply",
      "There was a previous mission (Ulysses aka International Solar Polar mission) that sent back a lot of data but for whatever reason, they didn't have it send visual images.  Big bright ball = no surprise, maybe.\n \nreply",
      "This slightly tilted view of the poles is a teaser. I didn't know they'd managed to incorporate late in the mission gravity assists into the cheaper plan B to slightly tweak out of the ecliptic while dropping close to the sun. That's pretty cool. https://upload.wikimedia.org/wikipedia/commons/6/66/Animatio...But we could've had so much more. The original proposal A for the ESA Solar Orbiter was a highly inclined orbit relative to the ecliptic plane to truly get full polar views of the sun. But this was too expensive. So they went with the cheaper proposal B which was mostly just a spectroscopic platform. Similar to SDO AIA, except in a solar orbit (almost completely within the ecliptic plane) instead of SDO AIA's Earth based sun synchronous orbit.\n \nreply",
      "They plan to get a more polar orbit each time they get close to Venus: https://www.esa.int/ESA_Multimedia/Images/2020/01/Solar_Orbi...Not sure if 33\u00b0 angle in 2029 is the final \"polarity\" or if they'll keep tilting after that.\n \nreply",
      "Wouldn't the tilt affect the gravity assist of Venus?\n \nreply",
      "The planning of sure, you've gotta make sure you're crossing the plane at the time, but gravity assist itself is otherwise the same though.\n \nreply"
    ],
    "link": "https://www.esa.int/Science_Exploration/Space_Science/Solar_Orbiter/Solar_Orbiter_gets_world-first_views_of_the_Sun_s_poles",
    "first_paragraph": "Thanks to its newly tilted orbit around the Sun, the European Space Agency-led Solar Orbiter spacecraft is the first to image the Sun\u2019s poles from outside the ecliptic plane. Solar Orbiter\u2019s unique viewing angle will\u00a0change our understanding of the Sun\u2019s magnetic field, the solar cycle and the workings of space weather.\u00a0Any image you have ever seen of the Sun was taken from around the Sun\u2019s equator. This is because Earth, the other planets, and all other operational spacecraft orbit the Sun within a flat disc around the Sun called the ecliptic plane. By tilting its orbit out of this plane, Solar Orbiter reveals the Sun from a whole new angle.\u00a0\u00a0The video above compares Solar Orbiter\u2019s view (in yellow) with the one from Earth (grey), on 23 March 2025. At the time, Solar Orbiter was viewing the Sun from an angle of 17\u00b0 below the solar equator, enough to directly see the Sun\u2019s south pole. Over the coming years, the spacecraft will tilt its orbit even further, so the best views are yet to c"
  },
  {
    "title": "Wrong ways to use the databases, when the pendulum swung too far (luu.io)",
    "points": 18,
    "submitter": "luuio",
    "submit_time": "2025-06-12T18:19:13 1749752353",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.luu.io/posts/2025-database-pendulum",
    "first_paragraph": "\nDisclaimer: This story took place when I was a junior developer for a previous employer ages ago. Some details might be fuzzy, and goes without saying this is entirely from my own POV so take everything here with humor and a grain of salt. \nI like to share these stories, because I believe learning what not to do is just as valuable as learning what to do.\nThe team I joined was part of a larger org. This org inherited a highly critical pipeline from an offshore team. When I say critical, I mean the-entire-company-bottom-line-and-financials-depended-on-them critical. I guess the company just wanted to move that particularly branch of development back to the US. These systems were complex with a lot of business rules and many moving parts, multiple large code bases, and definitely ancient.\nThe development experience wasn\u2019t great either. Even though there were separate codebases, the projects were somehow setup in a way that changing one small file required building almost the entire thin"
  },
  {
    "title": "AMD's AI Future Is Rack Scale 'Helios' (morethanmoore.substack.com)",
    "points": 15,
    "submitter": "rbanffy",
    "submit_time": "2025-06-14T20:51:11 1749934271",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://morethanmoore.substack.com/p/amds-ai-future-is-rack-scale-helios",
    "first_paragraph": ""
  },
  {
    "title": "Clinical knowledge in LLMs does not translate to human interactions (arxiv.org)",
    "points": 50,
    "submitter": "insistent",
    "submit_time": "2025-06-14T22:18:46 1749939526",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=44279209",
    "comments": [
      "I've seen that LLMs hallucinate in very subtle ways when guidng you through a course of treatment.Once when having to administer eyedrops to a parent, and I saw redness and was being conservative, it told me the wrong drop to stop. The doctor saw my parent the next day so it was all fixed but did lead to me freaking out.Doctors behave very differently from how we normal humans behave. They go through testing that not many of us would be able to sit through let alone pass. \nAnd they are taught a multitude of subjects that are so far away from the subjects everyone else learns that we have no way to truly communicate to them.And this massive chasm is the problem, not that the LLM is the wrong tool.Thinking probabilistically (mainly basyesia) and understanding the initial first two years of medschool will help you use an LLM much more effectively for your health.\n \nreply",
      "This is just a random anecdote but ChatGPT (when given many, many details with 100% honesty) has essentially matched exactly what doctors told me in every case where I've tested it. This was across several non-serious situations (what's this rash) and one quite serious situation, although the last is a decently common condition.The two times that ChatGPT got a situation even somewhat wrong, were:- My kid had a rash and ChatGPT thought it was one thing. His symptoms changed slightly the next day, I typed in the new symptoms, and it got it immediately. We had to go to urgent care to get confirmation, but in hindsight ChatGPT had already solved it. \n- In another situation my kid had a rash with somewhat random symptoms and the AI essentially said \"I don't know what this is but it's not a big deal as far as the data shows.\" It disappeared the next day.It has never gotten anything wrong other than these rashes. Including issues related to ENT, ophthalmology, head trauma, skincare, and more. Afaict it is basically really good at matching symptoms to known conditions and then describing standard of care (and variations).I now use it as my frontline triage tool for assessing risk. Specifically ChatGPT says \"see a doctor soon/ASAP\" I do it, if it doesn't say to go see a doctor, I use my own judgment ie I won't skip a doctor trip if I'm nervous just because AI said so. This is all 100% anecdotes and I'm not disagreeing with the study, but I've been incredibly impressed by its ability to rapidly distill medical standard of care.\n \nreply",
      "I sincerely hope your credulity doesn't swing around to bite you in the ass with this.\n \nreply",
      "Interesting quote from the venturebeat article linked:> \u201cThere is also a reason why clinicians who deal with patients on the front line are trained to ask questions in a certain way and a certain repetitiveness,\u201d Volkheimer goes on. Patients omit information because they don\u2019t know what\u2019s relevant, or at worst, lie because they\u2019re embarrassed or ashamed.In order for an LLM to really do this task the right way (comparable to a physician), they need to not only use what the human gives them but be effective at extracting the right information from the human, the human might not know what is important or they might be disinclined to share, and physicians can learn to overcome this. However, in this study, this isn't actually what happened - the participants were looking to diagnose a made-up scenario, where the symptoms were clearly presented to them, and they had no incentive to lie or withhold embarrassing symptoms since they weren't actually happening to them, it was all made up - and yet, it still seemed to happen, that the participants did not effectively communicate all the necessary information.\n \nreply",
      "As a patient, I am responsible for sharing information to my doctor. I wouldn't hold it against them if they didn't extract information from me.\n \nreply",
      "Sure, but think of a good help desk tech: if they waited for users to accurately report useful information, nothing would ever get fixed.\n \nreply",
      "Sure. But as a patient, you are also not expected to know what is or isn't important. Omitting unimportant information (to you) because your brain does a low pass filter is partially what the doctor is trying to bypass.\n \nreply",
      "Its as if every single person had to be an expert in every field to be able to function, that's really not a thing and we expect the actual experts to know how to extract the needed information.That's one of the main differences between mediocre and incredible engineers, being able to figure out what the problem that needs to be solved is and not work on whatever a stakeholder asks them to build.\n \nreply",
      "Yeah, there's a lot of agency on both sides of the equation when it comes to any kind of consultant.  You're less likely to have bad experiences with doctors if you're self aware and thoughtful about how you interact with them.\n \nreply",
      "You don\u2019t want to make systems that require people to be as diligent as you because those systems will have bad outcomes.\n \nreply"
    ],
    "link": "https://arxiv.org/pdf/2504.18919",
    "first_paragraph": ""
  },
  {
    "title": "Seven replies to the viral Apple reasoning paper and why they fall short (garymarcus.substack.com)",
    "points": 208,
    "submitter": "spwestwood",
    "submit_time": "2025-06-14T19:52:38 1749930758",
    "num_comments": 152,
    "comments_url": "https://news.ycombinator.com/item?id=44278403",
    "comments": [
      "> 1. Humans have trouble with complex problems and memory demands. True! But incomplete. We have every right to expect machines to do things we can\u2019t. [...] If we want to get to AGI, we will have to better.I don't get this argument. The paper is about \"whether RLLMs can think\". If we grant \"humans make these mistakes too\", but also \"we still require this ability in our definition of thinking\", aren't we saying \"thinking in humans is a illusion\" too?\n \nreply",
      "Agreed. But also his point about AGI is incorrect. AI that will perform on the level of average human in every task is AGI by definition.\n \nreply",
      "the average human is good at something, and sucks at almost everything. Human performance at chess and average performance at chess differ by 7 orders of magnitude.\n \nreply",
      "That very much depends on which AGI definition you are using. I imagine there are a dozen or so variants out there. See also \"AI\" and \"agents\" and (apparently) \"vibe coding\" and pretty much every other piece of jargon in this field.\n \nreply",
      "I think it's very widely accepted definition and there's really no competing definitions either as far as I know. While some people might think AGI means superintelligence, it's only because they've heard the term but never bothered to look up \nwhat it means.\n \nreply",
      "OpenAI: https://openai.com/index/how-should-ai-systems-behave/#citat...\"By AGI, we mean highly autonomous systems that outperform humans at most economically valuable work.\"AWS: https://aws.amazon.com/what-is/artificial-general-intelligen...\"Artificial general intelligence (AGI) is a field of theoretical AI research that attempts to create software with human-like intelligence and the ability to self-teach. The aim is for the software to be able to perform tasks that it is not necessarily trained or developed for.\"DeepMind: https://arxiv.org/abs/2311.02462\"Artificial General Intelligence (AGI) is an important and sometimes controversial concept in computing research, used to describe an AI system that is at least as capable as a human at most tasks. [...] We argue that any definition of AGI should meet the following six criteria: We emphasize the importance of metacognition, and suggest that an AGI benchmark should include metacognitive tasks such as (1) the ability to learn new skills, (2) the ability to know when to ask for help, and (3) social metacognitive abilities such as those relating to theory of mind. The ability to learn new skills (Chollet, 2019) is essential to generality, since it is infeasible for a system to be optimized for all possible use cases a priori [...]\"The key difference appears to be around self-teaching and meta-cognition. The OpenAI one shortcuts that by focusing on \"outperform humans at most economically valuable work\", but others make that ability to self-improve key to their definitions.",
      "Agree. Both sides of the argument are unsatisfying. They seem like quantitative answers to a qualitative question.\n \nreply",
      "\"Have we created machines that can do something qualitatevely similar to that part of us that can correlate known information and pattern recognition to produce new ideas and solutions to problems -- that part we call thinking?\"I think the answer to this question is certainly \"Yes\". I think the reason people deny this is because it was just laughably easy in retrospect.In mid-2022 people were like. \"Wow this GPT3 thing generates kind of coherent greentexts\"Since then really only we got: larger models, larger models, search, agents, larger models, chain-of-thought and larger models.And from a novelty toy we got a set of tools that at the very least massively increase human productivity in a wide range of tasks and certainly pass any Turing test.Attention really was all you needed.But of course, if you ask a buddhist monk, he'll tell you we are attention machines, not computation machines.He'll also tell you, should you listen, that we have a monkey in our mind that is constantly producing new thoughts. This monkey is not who we are, it's an organ. It's thoughts are not our thoughts. It's something we perceive. And that we shouldn't identify with.Now we have thought-genrating-monkeys with jet engines and adrenaline shots.This can be good. Thought-genrating-monkeys put us on the moon and wrote Hamlet and the Oddesy.The key is to not become a slave to them. To realize that our worth consists not in our ability to think. And that we are more than that.\n \nreply",
      ">I think the answer to this question is certainly \"Yes\".It is unequivocally \"No\". A good joint distribution estimator is always by definition a posteriori and completely incapable of synthetic a priori thought.\n \nreply",
      "That doesn't seem true to me at all. Let's say you fit y=c+bx+ax^2 on the domain -10,10 with 1000 data points uniformly distributed along x and with no more than 1% noise in observed y. Your model will be pretty damn good and absolutely will be able to generate \"synthetic a priori\" y outputs for any given x within the domain.Now let's say you didn't know the true function and had to use a neural network instead. You would probably still get a great result in the sense of generating \"new\" outputs that are not observed in the training data, as long as they are within or reasonably close to the original domain.LLMs are that. With enough data and enough parameters and the right inductive bias and the right RLHF procedure etc, they are getting increasingly good at estimating a conditional next token distribution given the context. If by \"synthetic\" you mean that an LLM can never generate a truly new idea that was not in it's training data, then that becomes the question of what the \"domain\" of the data really is.I'm not convinced that LLMs are strictly limited to ideas that they have \"learned\" in their data. Before LLMs, I don't think people realized just how much pattern and structure there was in human thought, and how exposed it was through text. Given the advances of the last couple of years, I'm starting to come around to the idea that text contains enough instances of reasoning and thinking that these models might develop some kind of ability to do something like reasoning and thinking simply because they would have to in order to continue decreasing validation loss.I want to be clear that I am not at all an AI maximalist, and the fact that these things are built largely on copyright infringement continues to disgust me, as do the growing economic and environmental externalities and other problems surrounding their use and abuse. But I don't think it does any good to pretend these things are dumber than they are, or to assume that the next AI winter is right around the corner.\n \nreply"
    ],
    "link": "https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple",
    "first_paragraph": ""
  },
  {
    "title": "Debunking HDR [video] (yedlin.net)",
    "points": 57,
    "submitter": "plastic3169",
    "submit_time": "2025-06-11T15:56:00 1749657360",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=44248968",
    "comments": [
      "Solid overview of applied color theory for video, so worth watching.As to what was to be debunked, the presentation not only fails to set out a thesis in the introduction, it doesn't even beg a question, so you've got to watch hours to get to the point: SDR and HDR are two measurement systems which when correctly used for most cases (legacy and conventional content) must produce the visual result. The increased fidelity of HDR makes it possible to expand the sensory response and achieve some very realistic new looks that were impossible with SDR, but the significance and value of any look is still up to the creativity of the photographer.This point could be more easily conveyed by this presentation if the author explained in the history of reproduction technology, human visual adaptation exposes a moment by moment contrast window of about 100:1, which is constantly adjusting across time based on average luminance to create an much larger window of perception of billions:1(+) that allows us to operate under the luminance conditions on earth. But until recently, we haven't expected electronic display media to be used in every condition on earth and even if it can work, you don't pick everywhere as your reference environment for system alignment.(+)Regarding difference between numbers such as 100 or billions, don't let your common sense about big or small values phase your thinking about differences: perception is logarithmic; it's the degree of ratios that matter more than the absolute magnitude of the numbers. As a famous acoustics engineer (Paul Klipsch) said about where to focus design optimization of response traits of reproduction systems: \"If you can't double it or halve it, don't worry about it.\"\n \nreply",
      "Regardless of whether it is HDR or SDR, when processing raw data for display spaces one must throw out 90%+ of information of what was captured by the sensor (which is often a small amount of what was available at the scene already). There can simply be no objectivity, it is always about what you saw and what you want others to see, an inherently creative task.\n \nreply",
      "90% really ? What color information get ejected exactly ? For the sensor part are you talking about the fact that the photosites don't cover all the surface ? Or that we only capture a short band of wavelength ? Or that the lens only focuses rays unto specific exact points and make the rest blurry and we loose 3D ?\n \nreply",
      "Cameras capture linear brightness data, proportional to the number of photons that hit each pixel. Human eyes (film cameras too) basically process the logarithm of brightness data. So one of the first things a digital camera can do to throw out a bunch of unneeded data is to take the log of the linear values it records, and save that to disk. You lose a bunch of fine gradations of lightness in the brightest parts of the image. But humans can't tell.Gamma encoding, which has been around since the earliest CRTs was a very basic solution to this fact. Nowadays it's silly for any high-dynamic image recording format to not encode data in a log format. Because it's so much more representative of human vision.\n \nreply",
      "Ok so similar to the other commentator then, thanks. According to that metric its much more than 90% we\u2019re throwing out then (:\n \nreply",
      "well technically there's a bunch of stuff that happens after the sensor gets raw data. (also excluding the fact that normal sensors do not capture light phase)demosaicing is a first point of loss of data (there is a tiling of monochrome small sensors, you reconstruct color from little bunches with various algorithms)there is also a mapping to a color space of your choosing (probably mentioned in the op video, i apologize for i have not watched yet...). sensor color space do not need to match that rendered color space...note of interest being that sensors actually capture some infrared light (modulo physical filters to remove that). so yeah if you count that as color, it gets removed. (infrared photography is super cool!)then there is denoising/sharpening etc. that mess with your image.there might be more stuff i am not aware of too. i have very limited knowledge of the domain...\n \nreply",
      "But even before sensor data we go from 100 bits of photons data to 42 bits counted by photosites. Mh well maybe my calculations are too rough\n \nreply",
      "Presumably they're referring to the fact that most cameras capture ~12-14 bits of brightness vs the 8 that (non-hdr) displays show.\n \nreply",
      "Oh that's normal then. There are mandatory steps of dynamic range reduction in the video editing / color grading pipeline (like a compressor in audio production). So the whole information is not lost but the precision / details can be yes. But that's a weird definition, there are so many photons in daylight capture that you could easily say we really need minimum 21 bits per channel minimum (light intensity of sun / light intensity of moon)\n \nreply",
      "But that\u2019s not seen at the sensor - at least not at once - look at the sun and then look immediately at the dark sky moon (if it were possible) - the only reason you get the detail on the moon is the aperture in front. You couldn\u2019t see the same detail if they were next to each other. The precision is the most dark in the scene next to the most bright, as opposed to the most dark possible next to the most bright. That\u2019s the difference.\n \nreply"
    ],
    "link": "https://yedlin.net/DebunkingHDR/index.html",
    "first_paragraph": ""
  },
  {
    "title": "Waymo rides cost more than Uber or Lyft and people are paying anyway (techcrunch.com)",
    "points": 230,
    "submitter": "achristmascarl",
    "submit_time": "2025-06-12T14:19:43 1749737983",
    "num_comments": 416,
    "comments_url": "https://news.ycombinator.com/item?id=44258139",
    "comments": [
      "As a Waymo-booster on HN for a while now, here's my latest anecdote. I tried to figure out how to take Waymo to LAX even though it's not actually in their territory yet just because I value the experience so much. I was borderline going to take it within walking distance (about half a mile), but got lazy at the last minute. I took Lyft instead, and, as if the universe cursed my laziness, I booked a \"comfort\" car for $3 more than the base level Lyft. At first I was going to get a Tesla Model Y to take me, but that cancelled. Instead, what must have been a first generation Honda Pilot picked me up, suspension creaking and muffler that had seen better days. Did Lyft recognize what they sent instead of the \"comfort\" they promised and therefore charge me $3 less? Of course not. When I tried to contact customer service I ran into what I'm sure plenty of HN people have, which is a dead end where you report the issue and they (programmatically?) adjudicate the complaint on the spot. Their determination? I wasn't entitled to a $3 refund. Ironic that the rideshare app with human drivers doesn't allow me to contact their customer service whereas Waymo has no problem with it (yeah, yeah, I get it, \"we'll see once they reach a huge scale.\" But today the experience is so much better than Uber or Lyft that while it lasts I will bask in its driverless glory).\n \nreply",
      "> Their determination? I wasn't entitled to a $3 refund.Frustratingly, Lyft\u2019s position on this is that if you don\u2019t like the car that arrives you should reject it when it arrives, otherwise you\u2019re not entitled to a (even partial) refund, even when they know on their end that the car they sent doesn\u2019t match what you paid extra for.\n \nreply",
      "Uber has done that to me.  You pick a class but what you get seems unrelated.I need more space for luggage and such and  ... some \"mid-sized\" SUV picks me up that has about as much space a regular sedan anyway ... often the same type of vehicle that picked me up the previous day as a regular vehicle.\n \nreply",
      "I paid extra and scheduled an Uber with a child seat.  After waiting 30 minutes, when the car showed up, there was no car seat so the driver canceled right away and drove off.  Lesson learned.\n \nreply",
      "I'm pretty sure by now the various \"classes\" of service offered by Lyft and Uber are instead just ways for the customer to donate money to Lyft and Uber. There's no difference in what kind of yahoo shows up in what kind of beater.\n \nreply",
      "I pretty much just use it to book black cars these days - at least in my local city where those require licensed livery drivers.  Good experience there for the most part.  Most of the time I\u2019m using Uber it\u2019s either a business expense to the airport or I\u2019m booking for a large party anyways.That and I guess UberXL - otherwise it\u2019s pretty fungible.The interesting bit is that black is often pretty much the same price a UberX about a third of the time.\n \nreply",
      "It's also impossible to book an Uber with 2 child seats so, i guess i'm effed then.\n \nreply",
      "search \"mifold grab and go booster\" on amazon\n \nreply",
      "Uber operates in 71 countries. That booster seat is available in 1 country. So it solves 1.4% of the problem.Also that's a booster seat, not a child care seat, so can't be used if your kids are under 4.\n \nreply",
      "Assuming 71 countries of equal Uber using population.\n \nreply"
    ],
    "link": "https://techcrunch.com/2025/06/12/waymo-rides-cost-more-than-uber-or-lyft-and-people-are-paying-anyway/",
    "first_paragraph": "\n\n\t\tLatest\t\n\n\n\t\tAI\t\n\n\n\t\tAmazon\t\n\n\n\t\tApps\t\n\n\n\t\tBiotech & Health\t\n\n\n\t\tClimate\t\n\n\n\t\tCloud Computing\t\n\n\n\t\tCommerce\t\n\n\n\t\tCrypto\t\n\n\n\t\tEnterprise\t\n\n\n\t\tEVs\t\n\n\n\t\tFintech\t\n\n\n\t\tFundraising\t\n\n\n\t\tGadgets\t\n\n\n\t\tGaming\t\n\n\n\t\tGoogle\t\n\n\n\t\tGovernment & Policy\t\n\n\n\t\tHardware\t\n\n\n\t\tInstagram\t\n\n\n\t\tLayoffs\t\n\n\n\t\tMedia & Entertainment\t\n\n\n\t\tMeta\t\n\n\n\t\tMicrosoft\t\n\n\n\t\tPrivacy\t\n\n\n\t\tRobotics\t\n\n\n\t\tSecurity\t\n\n\n\t\tSocial\t\n\n\n\t\tSpace\t\n\n\n\t\tStartups\t\n\n\n\t\tTikTok\t\n\n\n\t\tTransportation\t\n\n\n\t\tVenture\t\n\n\n\t\tEvents\t\n\n\n\t\tStartup Battlefield\t\n\n\n\t\tStrictlyVC\t\n\n\n\t\tNewsletters\t\n\n\n\t\tPodcasts\t\n\n\n\t\tVideos\t\n\n\n\t\tPartner Content\t\n\n\n\t\tTechCrunch Brand Studio\t\n\n\n\t\tCrunchboard\t\n\n\n\t\tContact Us\t\nA central premise of robotaxis is that high usage and lower labor costs will ultimately make it a cheap transportation option. That is still far from true, but now there\u2019s some data that gives us an idea of by how much.Obi, an app that aggregates real-time pricing and pick-up times across multiple ride-hailing services, has just published what it\u2019s calling the "
  },
  {
    "title": "Unsupervised Elicitation of Language Models (arxiv.org)",
    "points": 118,
    "submitter": "kordlessagain",
    "submit_time": "2025-06-14T12:32:20 1749904340",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=44276041",
    "comments": [
      "> However, as tasks and model behaviors grow more complex, human supervision becomes increasingly unreliable: LMs can learn to mimic mistakes in demonstrations or exploit flaws in feedback. How do we train LMs to do tasks that are too difficult for humans to demonstrate or evaluate reliably?I didn't read the whole paper but it seems important that you still need real ground truth to measure improvement, so you still need to get real labels at some point. The task they focus on where LLMs have \"superhuman\" performance is guessing the gender of blog authors. While humans are bad at this, humans are decent as remembering their gender, and a bunch of them are willing to write a blog post, so there's obviously a better way to get supervised examples than asking humans to guess labels: you collect posts in from authors whose gender is known. i.e. \"human generated labels are low quality\" should not be taken to mean \"good labels are not available so we should go fully unsupervised\".So since you already need some real ground truth to know whether your algorithm accomplished anything, I think it's fair to ask: when would you commit to using _all_ your labeled data for evaluation and none for fine tuning, as described in this work? Logical consistency seems valuable, sure, but it seems like really you'd want to use both consistency and some (small?) amount of labeled examples, and a perhaps larger amount of self-labeled examples. In their loop where they revise labels to be more coherent, it seems natural to imagine that pre-provided labels should be stickier than self-generated ones, but not immutable, because there's always some chance of noise in your upstream data generation process.\n \nreply",
      "Yeah this was my thought after skimming the paper as well. The core of the paper seems to be a way to use LLMs' latent knowledge to create a labeled dataset that can then be used for fine-tuning. They even acknowledge that this can only work to reinforce knowledge that the model already knows, e.g. helping convert pass@k to maj@k.My immediate thought is how this differs from just naively asking each each question individually to the LLM multiple times and taking the ground truth as consensus majority. The search algorithm probably implicitly does this, though I guess there is some benefit in providing it other related questions as well. I thought I remember similar papers dealing with LLM self-interrogation, using the idea that \"true\" statements must be logically consistent and so the same underlying explanation should hold for perturbations or related questions as well.The flaw seems to be that it's still beholden to the training data. Any misconceptions that are internalized during pretraining won't actually be fixed, and in fact they'll only be propagated further.\n \nreply",
      "I skimmed mostly, but was trying to understand how they came up with \"superhuman\" as a description, and it seems like a stretch?This might seem like a nit but the term \"superhuman\" is a VERY strong one to my mind. It doesn't suggest \"better than the average human off the street at a particular random task\" but instead suggests \"better than humans are capable of getting with training, at a high percentile-level\".One of the biggest advantages of LLMs as a tool are that they are generally quite good against a broad variety of things without needing a ton of further domain-specific training. Humans tend to be the opposite.It doesn't seem like they gave much training to the human annotators they recruited. Whereas an LLM trained on the internet has been trained on a LOT of blog posts + associated metadata. And nobody has ever really bothered figuring out \"how would we best train humans to identify gender of blog post authors\" - there's very little economic incentive for it. It's not like we generally train people to write in gender-specific ways in school either, so we haven't been formally instructed on potential differences. We'd have to rely on broad-brush generalizations if not given an opportunity to deep dive to try to find more specific tendencies.But if you pay people to study a big majority chunk of the corpus they're using for this for a couple years, focusing consciously on the post style, contents, and the gender both, and then test them on stuff from the ones you held out... how well could they do?\n \nreply",
      "\"Superhuman\" refers to abilities, qualities, or powers that exceed those naturally found in humans. It implies being greater than normal human capabilities.The term is often used in fiction, particularly in superhero comics and fantasy, but it can also be used metaphorically to describe extraordinary effort or achievement in real life (e.g., \"It took a superhuman effort to finish the marathon\").(Definition from Gemini)It seems reasonable to use the term to me simply to say the abilities on a benchmark of the model were greater than the human annotated data. Computers have always been superhuman at many tasks, even before llms.\n \nreply",
      "On a separate note, using an LLM for a definition is a bit funny, when there are expert-curated sources easily available. The LLM didn't get it wrong here, but...https://en.wikipedia.org/wiki/SuperhumanFirst line: \"The term superhuman refers to humans, humanoids or other beings with abilities and other qualities that exceed those naturally found in humans.\"Golly, I wonder what that model based its first sentence on.\n \nreply",
      "> \"Superhuman\" refers to abilities, qualities, or powers that exceed those naturally found in humans. It implies being greater than normal human capabilities.How do you know what normal human capabilities are for an unusual task that humans have not trained for? Is identifying the gender of the author of a blog post 80% of the time \"extraordinary\"? How do I know what a human is capable of doing for that with training?If a person with no programming experience asked Claude or ChatGPT to produce some code, they'd get better code than their \"normal\" human capability could produce. So: superhuman coders?But also today, I have asked Claude and ChatGPT to do coding tasks for me that both models got stuck on. Then I fixed them myself because I've had a lot of training and practice. So: not superhuman? But wait, the model output the broken code faster than I would've. So: superhuman again?Extraordinary shouldn't be so easily disputable.LLMs have superhuman breadth and superhuman speed. I haven't seen superhuman depth in any capabilities yet. I've seen them have \"better than untrained median person\" and often \"better than hobbyist\" depth. But here the authors claim \"superhuman capabilities\" which is pretty specificly not just meaning the breadth or speed.\n \nreply",
      "I was intrigued that one of the researchers was listed as \"independent\", so I checked her out:https://lindapetrini.comIt looks like she's a science communicator rather than a scientist herself. That's interesting... I'm not used to seeing academic papers that include an author devoted entirely to the writing aspect. (Then again, maybe I just haven't noticed?)\n \nreply",
      "The fact that she's a scientist communicator doesn't imply that she only did the communication part, I think\n \nreply",
      "> our goal is to fine-tune a pretrained model on its own generated labelsHaven't all the big labs been doing this for a couple years now? It's a good idea, with great execution, but it's far from novel.https://en.wikipedia.org/wiki/Weak_supervision\n \nreply",
      "I think this removes any amount of human-labeled data: no RLHF and stuff like that. You can use their technique to create an unsupervised reward model, and use that model to RL your way to having a useful assistant LLM.The paper is very accessible (it's mostly written by Anthropic researchers), and Section 4 summarises their findings really well. They were themselves really surprised by the results:> We were initially very skeptical of these findings, because they seemed clearly too good to be true, and suspiciously close to training with actual labels. To ensure we didn\u2019t accidentally train on the labels, (1) we re-ran the experiment several times on different datasets, (2) we copied the dataset into a new file, excluding any labels before re-running our algorithm with that file, and (3) one coauthor independently replicated the findings on the Claude 3.5 Haiku base model using a different codebase.(emphasis mine)\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2506.10139",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Last fifty years of integer linear programming: Recent practical advances (hal.science)",
    "points": 172,
    "submitter": "teleforce",
    "submit_time": "2025-06-14T06:15:08 1749881708",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=44274567",
    "comments": [
      "Could someone maybe give a high-level explanation into why commercial ILP solvers (e.g. Gurobi) are that much better than free/open-source ones? Is it because ILP is inherently that difficult to solve (I know it's NP-hard), that the best solvers are just a large ensemble of heuristics for very specific sub-problems and thus no general \"good\" strategy has made it's way into the public domain?\n \nreply",
      "It\u2019s mostly that they work closely with clients in a very hands on way to implement problem-specific speedups. And they\u2019ve been doing this for 10-20 years. In the MILP world this means good heuristics (to find good starting points for branch & bound, and to effectively prune the B&B tree), as well as custom cuts (to cut off fractional solutions in a way that effectively improves the objective and solution integrality).It\u2019s common that when researchers in Operations Research pick a problem, they can often beat Gurobi and other solvers pretty easily by writing their own cuts & heuristics. The solver companies just do this consistently (by hiring teams of PhDs and researchers) and have a battery of client problems to track improvements and watch for regressions.\n \nreply",
      "> the best solvers are just a large ensemble of heuristics for very specific sub-problemsThe big commercial solvers have the resources (and the clients interested in helping) to have invested a lot of time in tuning everything in their solves to real-world problems. Heuristics are part of that; recognizing simpler sub-problems or approximations that can be fed back into the full problem is also part.I think a big part is that the OSS solvers are somewhat hamstrung by the combination of several issues: (1) the barrier to entry in SoTA optimizer development is very high, meaning that there are very few researchers/developers capable of usefully contributing both the mathematical and programming needed in the first place, (2) if you are capable of (1), the career paths that make lots money lead you away from OSS contribution, and (3) the nature of OSS projects means that \"customers\" are unlikely to contribute back to kind of examples, performance data, and/or profiling that is really needed to improve the solvers.There are some exceptions to (2), although being outside of traditional commercial solver development doesn't guarantee being OSS (e.g. SNOPT, developed  at Stanford, is still commercially licensed). A lot of academic solver work happens in the context of particular applications (e.g. Clarabel) and so tends to be more narrowly focused on particular problem classes. A lot of other fields have gotten past this bottleneck by having a large tech company acquire an existing commercial project (e.g. Mujoco) or fund an OSS project as a means of undercutting competitors. There are narrow examples of this for solvers (e.g. Ceres) but I suspect the investment to develop an entire general-purpose solver stack from scratch has been considered prohibitive.\n \nreply",
      "> solvers are just a large ensemble of heuristics for very specific sub-problemsIsn't that statement trivially applicable to anything NP-Hard (which ILP is, since it's equivalent to SAT)?\n \nreply",
      "No, good algorithms for NP hard problems can be more than just heuristics.Modern SAT solvers are a good example of this. CDCL is elegant.\n \nreply",
      "NP-hard is really hard, but it is hard for (a) polynomial running time, (b) for exact solutions, (c) on worst case problems.One might suspect that fast enough on specific problems for approximate solutions that still make/save a lot of money might also be welcome.  Ah, perhaps not!E.g., in NYC, two guys had a marketing resource allocation problem, tried simulated annealing, and ran for days before giving up.They sent me the problem statement via email, and in one week I had the software written and in the next week used the IBM OSL (Optimization Subroutine Library) and some Lagrangian relaxation.    In 500 primal-dual iterations with600,000 variables40,000 constraintsfound a feasible solution within 0.025% of\noptimality.So, I'd solved their problem (for practical purposes, the 0.025% has to count as a solving) for free.They were so embarrassed they wanted nothing to do with me.  We never got to where I set a price for my work.The problem those two guys had was likely that, if they worked with me, then I would understand their customers and, then, beat the guys and take their customers.  There in NYC, that happened a second time.If a guy is in, say, the auto business, and needs a lawyer, the guy might want the best lawyer but will not fear that the lawyer will enter the auto business as a powerful competitor.  Similarly for a good medical doctor.For an optimization guy saving, say, 5% of the operating costs of a big business, say, $billion in revenue a year, all the management suite will be afraid of the guy getting too much power and work to get him out -- Goal Subordination 101 or just fighting to retain position in the tribe.After having some grand successes in applied math where other people had the problem but then being afraid that I would be too powerful, I formulated:If some technical, computing, math, etc. idea you have is so valuable, then start your own business exploiting that idea -- of course, need a suitable business for the idea to be powerful.\n \nreply",
      "scale and speed. for example, most quant trading firms run huge optimizations as often as possible. open-source solver often cannot even solve the problems (OOM exceptions, etc)\n \nreply",
      "In most MILP domains, the underlying engineering know-how is more critical than mathematical formulations or CS coding: (that's why most OR groups operate independently of math or CS departments).OSS never took off among professional engineers because they've have \"skin in the game\", unlike math and CS folks who just reboot, and pretend nothing is wrong.\n \nreply",
      "I vaguely recall building a resource allocation tool using IBM's \"ILOG\" mixed integer linear programming library and we realised that if we'd built the tool about 20 years earlier it would have still been running for the same problems we were now solving within 5 minutes.As I recall it the raw computer power had increased by a factor of around a thousand and the algorithms had improved by about the same, giving us a factor of a million improvement.Worth pondering when trying to predict the future!The \"resources\" in question were diamonds by the way...\n \nreply",
      "It feels like there\u2019s a significant lack of \u201cML/AI\u201d based approaches applied to these kinds of problems. I\u2019ve seen a lot of example of RL/GNN papers that do attempt to solve smaller problems but it always feels like the best option is to just pay for a gurobi license and have at it. I\u2019ve been doing some scheduling optimisation recently (close to job shop scheduling) and while there\u2019s some examples of using RL they just don\u2019t seem to cut it. I\u2019ve resorted to evolutionary algorithms to get reasonable solutions to some big problems. Maybe it\u2019s just always more efficient to using OR type approaches when you can formulate the problem well.\n \nreply"
    ],
    "link": "https://inria.hal.science/hal-04776866v1",
    "first_paragraph": "\nMixed-integer linear programming (MILP) has become a cornerstone of operations research. This is driven by the enhanced efficiency of modern solvers, which can today find globally optimal solutions within seconds for problems that were out of reach a decade ago. The versatility of these solvers allowed successful applications in many areas, such as transportation, logistics, supply chain management, revenue management, finance, telecommunications, and manufacturing. Despite the impressive success already obtained, many challenges remain, and MILP is still a very active field.This article provides an overview of the most significant results achieved in advancing the MILP solution methods. Given the immense literature on this topic, we made deliberate choices to focus on computational aspects and recent practical performance improvements, emphasizing research that reports computational experiments. We organize our survey into three main parts, dedicated to branch-and-cut methods, Dantzi"
  },
  {
    "title": "How the Final Cartridge III Freezer Works (pagetable.com)",
    "points": 61,
    "submitter": "ingve",
    "submit_time": "2025-06-14T16:57:22 1749920242",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=44277431",
    "comments": [
      "One important detail the article omits: it is possible for the software running on the Commodore 64 to prevent being \"frozen\" by this technique. Because the processor needs the NMI interrupt to run the freeze routine, the software can pre-empt it by pulling down the NMI line on its own. This can be done using the second CIA I/O chip, whose interrupt output pin is connected to the NMI line. By making the CIA chip generate the NMI and never acknowledging it, the software will ensure that the NMI line is always pulled low and the freezer will not work.\n \nreply",
      "Clarification question, having read OP yet having missed some fine details: I presume you mean that software could set some CIA-II register to keep the NMI line to the CPU down indefinitely. Since what triggers the handling of an NMI is the transition from high to low, that means no other handler (in particular the Freeze Frame's) would get executed. And one'd also need to redirect the NMI vector, which normally is in Kernel ROM, to a dummy handler consisting of an RTI or little more. Correct?\n \nreply",
      "That's a clever trick, but giving up all the useful types of NMIs seems like a steep price to pay.\n \nreply",
      "Commodore 64 software hardly ever uses the NMI interrupt, because there is also the maskable IRQ interrupt which can be generated by the first CIA chip, and also, unlike NMI, by the video chip VIC for the raster interrupt. It is also much more useful because it is maskable - the software can temporarily turn it off when it is not desirable to be interrupted. The only thing the NMI can do that IRQ cannot is detecting that the Restore key was pressed, but that is seldom useful.\n \nreply",
      "I had forgotten what a big deal 'freezer' cartridges were on the C64!A friend's uncle manufactured these himself using an EPROM burner and God knows where he got the casings, and sold them to us kids. Worked great. I had no idea about the amount of hacking that went into making them work.\n \nreply",
      "I liked the interface of the FC3, but I always seemed to have trouble with it on NTSC systems, likely the fastloader. I ended up using ICEPIC or Super Snapshot most of the time instead.\n \nreply",
      "I always found Action Replay to be superior and easier to use than Final Cartridge.\n \nreply",
      "Very interesting. I was m always amazed by the code complexity behind C/64. Also, all this logic was squeezed in so few bytes!\n \nreply",
      "I love how the Action Replay FLOPPY disk for Amiga 1200 loads into the high 1MB RAM space. No need for hardware. (note: Amiga 1200 has 2MB RAM by default).\n \nreply",
      "I presume that the lack of an Ultimax-style bypass is why, as I understand it, there are no freezer cartridges for Atari 8-bit. (The piracy problem was just as bad, however, because of Happy Computers <https://en.wikipedia.org/wiki/Happy_drives>.)On the other hand, Atari 8-bit's design allows for FujiNet to work without the workarounds/disadvantages a Commodore equivalent would have. <https://news.ycombinator.com/item?id=37424773>\n \nreply"
    ],
    "link": "https://www.pagetable.com/?p=1810",
    "first_paragraph": "\npagetable.com\nSome Assembly Requiredby Dani\u00ebl MantioneDani\u00ebl contributed the commented disassembly of the FC3 freezer functionality to the reverse engineering effort at github.com/mist64/final_cartridge. Thanks to Eric Schlaepfer for his input on 6502 timing.One key reason why the Commodore 64 was so successful in the 80s was that it was able to do things it wasn\u2019t designed for. Freezer cartridges, which allowed stopping any running program or game, applying cheat codes and resuming, or saving the complete computer\u2019s state to disk so it could be continued from later, were one of those clever innovations: They were possible on the Commodore 64, but not on many other computers. A Commodore 64 with a good cartridge was a significantly more capable computer than a Commodore 64 without, this did contribute to the longetivity of the computer and is one reason why the Commodore 64 could remain in production for more than a decade.However, because the Commodore 64 wasn\u2019t designed at all to su"
  },
  {
    "title": "How to Bring Back Oddly Shaped App Icons in macOS 26 Tahoe (simonbs.dev)",
    "points": 3,
    "submitter": "Bogdanp",
    "submit_time": "2025-06-11T11:11:58 1749640318",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://simonbs.dev/posts/how-to-bring-back-oddly-shaped-app-icons-on-macos-26-tahoe/",
    "first_paragraph": "Simon B. St\u00f8vringJune 11, 2025macOS 26 Tahoe replaces the oddly shaped app icons that once brought joy and personality to the Dock with the familiar squircle icons from iOS.Here's what that looks like. The lovely Things icon now sits awkwardly on a glass background \ud83d\ude22I strongly dislike this change. Part of the Mac\u2019s soul was in its expressive, varied app icons, a case of character over conformity.Fortunately, there's a way to bring back the personality and charm that custom icon shapes add to macOS. This post shows how to do it both as a macOS user and as an app developer.When you manually assign an app icon through Finder, macOS respects the shape and doesn't force the glass background. Interestingly, this also works if you just replace the icon with the same one.Here's how to replace the app icon:Here's a short video showing the steps:App developers will likely start shipping new squircle-style icons soon. If you prefer the current look, it's a good idea to back up their .icns files n"
  },
  {
    "title": "The Many Sides of Erik Satie (mitpress.mit.edu)",
    "points": 130,
    "submitter": "anarbadalov",
    "submit_time": "2025-06-08T13:36:11 1749389771",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=44216921",
    "comments": [
      "Satie is fascinating, and I don't know of any composer who had so much variety in what he attempted. The Gymnopedies and Gnossiennes are by far his best known pieces, but once you get away from that it gets strange and wonderful. He threw off ideas which seem to have led to different musical movements years later. Minimalism, for instance, was a term first coined in 1968, but some people point to Satie's Vexations of 1893 (https://www.youtube.com/watch?v=sKKxt4KacRo&list=RDsKKxt4Kac...) - to be played 840 times. One puzzle (at least for me) is to work out whether he had the piano or organ in mind for some pieces. While the instruments look similar, some of the held notes will fade away on the piano, losing harmonies which would otherwise be present.\n \nreply",
      "Hate to nit, but there is actually a seventh gnossienne. Satie didn't publish 4-6, or even label them \"gnossiennes\", whereas the seventh was explicitly referred to as a gnossienne by Satie.\n \nreply",
      "In addition to writing the music and drama mentioned in the article, Satie also wrote about his own (rather eccentric) life. An excerpt about optimizing that stood out to me:> An artist must regulate his life. Here is a time-table of my daily acts. I rise at 7.18; am inspired from 10.23 to 11.47. I lunch at 12.11 and leave the table at 12.14. A healthy ride on horse-back round my domain follows from 1.19 pm to 2.53 pm. Another bout of inspiration from 3.12 to 4.7 pm. ... My only nourishment consists of food that is white: eggs, sugar, shredded bones, the fat of dead animals, veal, salt, coco-nuts, chicken cooked in white water, mouldy fruit, rice, turnips, sausages in camphor, pastry, cheese (white varieties), cotton salad, and certain kinds of fish (without their skin).  [1][1] M\u00e9moires d'un amn\u00e9sique (1912). An english translation of the excerpt: https://en.wikisource.org/wiki/A_Day_in_the_Life_of_a_Musici....\n \nreply",
      "This is obviously a piece of satire, very typical of Satie.\n \nreply",
      "I am surprised the article didn\u2019t touch upon \u2018furniture music\u2019 - https://en.m.wikipedia.org/wiki/Furniture_musichttps://aeon.co/videos/background-music-was-the-radical-inve...\n \nreply",
      "Indeed. He came up with the concept of \"background music\" 100 years ago, it's impressive!\n \nreply",
      "In the spirit of recommending favourite pieces, one of his that I love is Je Te Veux: https://www.youtube.com/watch?v=r1J_lxbaQxQ It's perhaps more obvious in terms of its tunefulness than some of his pieces, but I think it's like a perfectly-cut jewel and somehow quintessentially French.\n \nreply",
      "Thanks for sharing; I didn't expect to see Erik Satie on HN :-)It's a lovely little vignette of Satie's work and life. If you haven't already, give a listen to his Gnossiennes and Gymnop\u00e9dies. Beautiful melodies with a lot of harmonic variation.\n \nreply",
      "I think his most underrated and unknown piece is _Danses de travers_.https://www.youtube.com/watch?v=9x6nuiNN3JI&list=RD9x6nuiNN3...\n \nreply",
      "What a beautiful piece. For me it evokes a river: not knowing where it's going, but sounding exactly right in the moment\n \nreply"
    ],
    "link": "https://thereader.mitpress.mit.edu/the-many-sides-of-erik-satie/",
    "first_paragraph": "Contemplating this book, I asked a selection of young people: \u201cHave you ever heard of Erik Satie?\u201d Across the board, his name registered nothing. The moment I played the Gnossiennes and Gymnop\u00e9dies however \u2014 \u201cOh, I LOVE those!\u201d Everyone seems to know one or two, as if they\u2019re now part of some collective audio memory.\u00a0There are three Gymnop\u00e9dies and six Gnossiennes, all composed between 1887 and 1895. Most people seem to know either \u201cGymnop\u00e9die #1\u201d or \u201cGnossienne #1.\u201d They are familiar props in TV adverts, film soundtracks, chill-out compilations. They are both just over three minutes \u2014 i.e., pop single length, not grand classical excursion.In the Gymnop\u00e9dies, Roger Shattuck writes, Satie \u201ctakes one musical idea and \u2026 regards it briefly from three different directions.\u201d \u201cGymnop\u00e9die #1\u201d is probably Satie\u2019s best-known piece. You reach for words like stately, unhurried, spacious, melancholy, poignant \u2026 but the music\u2019s ineluctable strangeness remains. It is like a painting by Vel\u00e1zquez, whe"
  },
  {
    "title": "SIMD-friendly algorithms for substring searching (2018) (0x80.pl)",
    "points": 193,
    "submitter": "Rendello",
    "submit_time": "2025-06-14T03:31:18 1749871878",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=44274001",
    "comments": [
      "The \"AVX2 (generic)\" approach is roughly what ripgrep uses (via Rust's `regex` crate) to accelerate most searches. Even something like `\\w+\\s+Sherlock\\s+\\w+` will benefit since ripgrep will pluck `Sherlock` out of the regex and search that.The actual implementation is here: https://github.com/BurntSushi/memchr?tab=readme-ov-file#algo...The main difference with the algorithm presented here is that instead of always using the first and last bytes of the needle, a heuristic is used to try to pick two bytes that occur less frequently according to a background frequency distribution.It ends up being quite a bit faster than just plain Two-Way or even GNU libc's implementation of `memmem`. From the root of the `memchr` repository:    $ rebar rank benchmarks/record/x86_64/2023-12-29.csv -e '^rust/memchr/memmem/(oneshot|prebuilt|twoway)' -e '^libc/memmem/oneshot'\n    Engine                       Version  Geometric mean of speed ratios  Benchmark count\n    ------                       -------  ------------------------------  ---------------\n    rust/memchr/memmem/prebuilt  2.7.1    1.07                            57\n    rust/memchr/memmem/oneshot   2.7.1    1.39                            54\n    libc/memmem/oneshot          unknown  3.15                            54\n    rust/memchr/memmem/twoway    2.7.1    5.26                            54\n\nThe \"prebuilt\" benchmarks also demonstrate the API deficiencies of routines like `memmem`. Because of their API, they need to rebuild the state necessary to execute a search for the needle given. But it is often the case that the needle is invariant across repeated calls with different haystacks. So you end up doing that repeated work for no gain.\n \nreply",
      "Interesting. How do you know the background distribution of bytes? Won't a scan of the haystack to get the distribution take up a significant amount of time?\n \nreply",
      "It's a _background_ distribution. That is, a guess. A heuristic. It doesn't change with the input.\n \nreply",
      "I implemented these in my attempt to SIMD optimize Wasm/WASI libc.https://github.com/ncruces/go-sqlite3/tree/main/sqlite3/libcFor known haystack lengths, and large needles, I found it useful to augment the approach with Quick Search.https://igm.univ-mlv.fr/~lecroq/string/node19.html\n \nreply",
      "C# has implemented some SIMD for IndexOf: https://github.com/dotnet/runtime/pull/63285\n \nreply",
      "I implemented many different algorithms for searching and splitting strings using SMID methods as well:\nhttps://github.com/naver/tamgu/blob/06aedf2f14895925d7b5a8e2...\nI used a different algorithm than the ones presented here.\n \nreply",
      "Could you summarize your algorithm? Like a high-level description?Like algorithm 1 (from the article) checks N possible positions at a time by matching the first and last character; algorithm 2 instead tests the 4 leading characters.\n \nreply",
      "I tried to implement a modified version for LZ77 window search with Zig's generic SIMD a few years ago:https://news.ycombinator.com/item?id=44273983\n \nreply",
      "Reminds me of https://github.com/nikneym/hparseWhich uses SIMD algos for fast HTTP parsing.\n \nreply",
      "The swar algorithm has UB because it casts the 1 byte aligned data to 8 byte aligned. Perhaps it suffers some performance issues from unaligned loads?\n \nreply"
    ],
    "link": "http://0x80.pl/notesen/2016-11-28-simd-strfind.html",
    "first_paragraph": "Popular programming languages provide methods or functions which locate a\nsubstring in a given string. In C it is the function strstr, the C++\nclass std::string has the method find, Python's string has methods\npos and index, and so on, so forth. All these APIs were designed for\none-shot searches.  During past decades several algorithms to solve this\nproblem were designed, an excellent page by Christian Charras and\nThierry Lecroq lists most of them (if not all). Basically these\nalgorithms could be split into two major categories: (1) based on\nDeterministic Finite Automaton, like Knuth-Morris-Pratt, Boyer Moore, etc.,\nand (2) based on a simple comparison, like the Karp-Rabin algorithm.The main problem with these standard algorithms is a silent assumption\nthat comparing a pair of characters, looking up in an extra table and\nconditions are cheap, while comparing two substrings is expansive.But current desktop CPUs do not meet this assumption, in particular:ContentsThis article shows two ap"
  },
  {
    "title": "Endometriosis is an interesting disease (owlposting.com)",
    "points": 320,
    "submitter": "crescit_eundo",
    "submit_time": "2025-06-13T22:40:22 1749854422",
    "num_comments": 223,
    "comments_url": "https://news.ycombinator.com/item?id=44272933",
    "comments": [
      "I'm always struck by stories of how difficult it is to get a condition diagnosed. Endometriosis is a great example as the OP author notes.The New York Times used to run a series of medical case studies in their magazine, and over and over again the story was essentially that the patient sought care from primary care and specialists, without success, and were generally miserable, until a miraculous event happened, like a friend's aunt knowing someone at Johns Hopkins, and that doctor having the time to think about it. The problem was incredibly evident for female patients.I don't know whether this is a result of doctors being burned out by the system that they work in, a certain doctorial arrogance that diminishes their listening skills, over-reliance on heuristic diagnoses, some kind of ignorance of womens' conditions, or even a kind of medical misogyny. But it pushes people pretty quickly towards Dr Google and sometimes, sadly, into quackery, and that can't be a good thing.\n \nreply",
      "\"I don't know whether this is a result of...\"The cause is pretty easy. Patients aren't treated like mysteries, they're treated like BAU Jira tickets - just get it done so you're on to the next one. The system is built to handle the 90%. If you fall into that other 10%, it won't work well for you. If you have provider companies and insurances pressing you to hit some metric, that's what you have to do. If you are concerned about malpractice, then you have to just read from the Epic system. No surprise we're in this situation.\n \nreply",
      "Well, that, and deviation from the established practices can make it difficult to get paid by the insurance company and/or open you up to legal risk (particularly if something goes wrong). Or so I understand from those in the system in the US.\n \nreply",
      "Up to 25% of women have endo, in some communities at least.We need more doctors. The nation has grown, our medical professionals and courts must scale up. Automation isn't going to solve everything.\n \nreply",
      "Indeed.Imagine if asking a specialist the time meant they had 3 broken clocks on the wall and  picked one.That\u2019s pretty much how initial medical diagnoses are done.Insurance companies then limit what\u2019s types of care/investigation can be done for various conditions.Doctor may know that medicine X will work best but insurance demands that Y and Z be first tried before covering X.   Same with tests.I\u2019m tired of it.\n \nreply",
      "What would you propose as an alternative? Healthcare costs are already high. Imposing step therapy requirements to try cheaper treatments first is one of the few ways that insurers have to control costs. And the cheaper treatments do work well for many patients (or they recover on there own just due to time).\n \nreply",
      "Amusingly this is why people say LLMs will beat doctors. It\u2019s because the 90% of cases is so easy that a motivated guy with Google can get there and a smart NP can get there too.It isn\u2019t that it\u2019s easy to do all a doctor does. But their training and knowledge shines in the 99th percentile case except they never exercise it there so you can usually get there with Google.\u201cOh but an LLM will guess the common case and never think of the rare!\u201dYeah but so will a doctor given 10 minutes on it. They\u2019re not exactly going to House MD you. You\u2019re gonna die.\n \nreply",
      "I discovered a friend\u2019s chronic medical issue that two VA specialists and a PCP couldn\u2019t figure out using an LLM health project that had been posted here. Works when it works, n=1. Certainly, don\u2019t trust the robot, but it doesn\u2019t hurt to rubber duck debug with it to find blind spots. Fancy search engine sometimes is right (although it can lie too!).(Bone tuberculosis)\n \nreply",
      "Language models are really good at free association tasks, such as semantic fuzzy search. Next token prediction is among the worst possible ways to use them (although if there's no other obvious way of getting the information out of the model, it works in a pinch).Which project did you use?\n \nreply",
      "https://news.ycombinator.com/item?id=42974851https://github.com/OpenHealthForAll/open-health\n \nreply"
    ],
    "link": "https://www.owlposting.com/p/endometriosis-is-an-incredibly-interesting",
    "first_paragraph": ""
  }
]