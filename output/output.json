[
  {
    "title": "School smartphone ban results in better sleep and improved mood (york.ac.uk)",
    "points": 133,
    "submitter": "jonatron",
    "submit_time": "2024-12-14T23:51:44 1734220304",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=42420352",
    "comments": [
      "I threw away my smartphone 4-ish months ago, and I have 50% more motivation, 50% more headspace to focus on things, and 50% better moods most days. The attention economy, especially in social media, is a plague.It wasn't always this way. I did digital detoxes every few years for about two decades. For most of that time, there were always subtle positive impacts of switching off. But nowadays, the positive effects are not subtle at all. They are very significant.I am impressed we let a few companies commodify and commercialized human attention and human connection to this degree. Humanity has been done a great disservice in both areas by them. One day, this period of mass harm to humanity will be in history books.\n \nreply",
      "In Ontario/Canada schools banned cell phones with much of any issue at all this year.My friends in the US seems shocked at the fact that kids couldn't have a phone during class hours.  When I asked why their main issue was that if kids cell phones were in their lockers, how would they text their parents to say they were ok when their school had a shooting.Which just goes to show how much your environment affects your thinking.  I've never once thought or even considered there could be a school shooting at a school here.\n \nreply",
      "As an American, I'm at a loss trying to figure out when phones started being allowed in school.Back in my day, all electronics had to stay in your locker except your calculator, including pagers, personal organizers and for the very few kids who were wealthy enough to have one, phones as well. This would be about the time that Motorola and Nokia were selling giant bricks that they called phones.Edit: and no, school shootings had nothing to do with the change. We'd gone through Columbine not long before, and despite the media spamming everyone they're very rare even today.\n \nreply",
      "Do people really think this way? In good faith? They have such a high sense of risk from school shootings that they have to organize their daily activities around that possibility, regardless of whether it may be ruining the learning environment, the entire nominal purpose of school?\n \nreply",
      "According to my daughter, the Ontario ban had little effect on her high school.   Before the ban, some teachers allowed phones, some didn't.   After the ban, some teachers allow phones and some don't.   Many teachers use internet resources in their classroom, and phones are how the students access those.   There are Chromebooks available, but not enough for everybody, and they are in rough shape.And the other daughter's middle school is still the same as before the ban.  They previously had a ban stricter than the provincial government's mandate so nothing changed.\n \nreply",
      "this sounds possibly like a whole lot of things but \u201cban\u201d is not one of them - maybe something like Ontario \u201cyou can do whatever teacher says\u201d thing\n \nreply",
      "> their main issue was that if kids cell phones were in their lockers, how would they text their parents to say they were ok when their school had a shooting.If the main purpose is just texting with their parents in case of emergency, they could get an old-fashioned pager for that.  I heard that these devices are a pretty severe explosion hazard though.\n \nreply",
      "You can't send messages with an old-fashioned pager. (There are two-way pagers, but I don't think those were ever common).\n \nreply",
      "Replace shooting with fire, flood, earthquake, tornado, etcIMO the objection is dumb regardless, but maybe that will help translate.\n \nreply",
      "yep, my kids HS has a cloth rack hanging on the door to place phones in so the kids can grab them when a shooting is happening but doesn't have them during class\n \nreply"
    ],
    "link": "https://www.york.ac.uk/news-and-events/news/2024/research/school-smartphone-ban-better-sleep/",
    "first_paragraph": ""
  },
  {
    "title": "IRATA.ONLINE: A Community for Retro-Computing Enthusiasts (irata.online)",
    "points": 120,
    "submitter": "Bluestein",
    "submit_time": "2024-12-14T19:42:50 1734205370",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=42418982",
    "comments": [
      "This is really cool. \nExcept that a hub for retro computing should\nnot point people to Facebook to keep up with \nnews.\"\"\"\nNew developments on IRATA.ONLINE happen fast! Check the Facebook group for everything from new PLATOTerm ports, scheduling workshops and events, to announcements of new content on the service itself!\n\"\"\"\n \nreply",
      "Hi. I run IRATA.ONLINE. Where should I post news? :)\n-Thom\n \nreply",
      "On the website, of course (ideally provide an RSS or Atom feed, too).\n \nreply",
      "That's really odd, if they did it for privacy reason they missed some news.\n \nreply",
      "What are the specific reasons that this is a problem?Asking without any bias, just curious if there\u2019re any other reasons than what I already know\u2026\n \nreply",
      "Facebooks spies on you, and makes reading the news without an account hard.\n \nreply",
      "Not everyone has a facebook account.\n \nreply",
      "Platform lock in.\n \nreply",
      "Thom did great work on the Plato stuff. Also be sure to check out his Fujinet work, really nicely done. Ported to many platforms too.\n \nreply",
      "This is amazing.Are there any Windows 3.11 programs that support PLATO?I'd love to add this by default on my virtual PC at pieter.com.\n \nreply"
    ],
    "link": "https://irata.online/",
    "first_paragraph": "The Web terminal (aka the big orange button) can now be installed as a web application. Simply click on the\n\t      orange button, and select Install... from your browser's menu.The latest #FujiNet version of PLATOTERM has been released for Atari 8-bit\n\t\tusers with FujiNet adapters! You can run the latest copy by using TNFS server\n\t\tatari-apps.irata.online, and loading plato.atr from inside the Comms folder.The source code can be found here: Source on GitHubPLATOTERM 1.3 LITE for Atari 8-bit Computers has been released. This version fits in a 16K cartridge ROM by\n\t\tremoving preferences support, but otherwise is the complete terminal program. New features in this version\n\t\tinclude a 2048 byte input buffer, support for higher baud rates, and all of the various supported pointing\n\t\tdevices. The disk version will be updated soon to match.Release page on Github.ZIP containing all ROM variantsThe Android app wrapper for the JavaScript version of PLATOTERM has been released!\n\t\tYou can get it o"
  },
  {
    "title": "Humans are unreliable models of mouse disease (cell.com)",
    "points": 177,
    "submitter": "XzetaU8",
    "submit_time": "2024-12-14T17:23:13 1734196993",
    "num_comments": 76,
    "comments_url": "https://news.ycombinator.com/item?id=42418272",
    "comments": [
      "The first \u2014 and maybe only, or at least sufficient \u2014 use of tissue engineering at scale is not going to be growing organs for implantation, growing meat in a lab for human consumption, or anything quite that flashy.The first one is going to be the ability to grow liver slices at scale for tox screening in pharma.animal rights issues entirely aside: we spend entirely too much time and money killing a whole lot of mice all the while knowing perfectly well they\u2019re barely semi-decent models for toxicology.if I can fill the lab with constant flow vats of slices of liver, they\u2019ll read out fast, they\u2019ll be a massive savings in time and money, and they\u2019ll give us much more accurate results.\n \nreply",
      "Sounds like a worthwhile undertaking. Do you know someone who is actively working on this problem?\n \nreply",
      "Many are, it is just not a trivial problem to create or copy living organs.\n \nreply",
      "It it meaningfully easier than growing livers for transplantation?\n \nreply",
      "Almost definitely: livers to implant need to be full sized and last years to decades. A test liver could be 1cm2, fit on a microscope slide, and have a two week shelf life.Not to mention that there is a much higher bar, ethically, regulatorily, and economically for testing a synthetic implantable liver vs a 'liver-on-a-chip'.\n \nreply",
      "The research has been complicated in the US because so many of the cell lines pass through fetal tissue. But IIUC it's ongoing (and not so encumbered in other countries). Not to say all are intertwined with fetal cell research, only that it's one of the cheapest sources of stem cells and law has made using it complicated.\n \nreply",
      "Do you (or a GP) know how I can donate already-extracted stem cells for this?\n \nreply",
      "Balancing safety and innovation in human health is incredibly difficult.This isn't a criticism of the FDA, but rather an observation of facts underscoring both the challenges and opportunities.Some foods consumed safely by humans are toxic/harmful in mice, due to how mice react differently to key compounds.Common examples:1. Chocolate - theobromine2. Coffee - caffeine3. Chili peppers - capsaicin\n \nreply",
      "All those are \"pesticides\" that plants produce to try to poison predators. Other examples are cocaine and nicotine.\n \nreply",
      "Poor little bastards can't eat anything fun\n \nreply"
    ],
    "link": "https://www.cell.com/cell/abstract/S0092-8674(24)00897-3",
    "first_paragraph": ""
  },
  {
    "title": "They See Your Photos (theyseeyourphotos.com)",
    "points": 174,
    "submitter": "vladyslavfox",
    "submit_time": "2024-12-14T21:09:51 1734210591",
    "num_comments": 103,
    "comments_url": "https://news.ycombinator.com/item?id=42419469",
    "comments": [
      "This is just an ad for their photo service. Which presumably has terrible search features, if it doesn't use AI to analyse them. That's one of the best features in Google Photos!\n \nreply",
      "Hey, one of the folks working on the said photo service here.Ente has reasonably good search[1] powered by on-device machine learning[2].[1]: https://ente.io/blog/machine-learning[2]: https://ente.io/ml\n \nreply",
      "Terrible search without AI is a bit of a stretch. Also Google does not have a monopoly on object/face recognition in photos. There are self-hosted solutions that readily provide you with that without feeding a faceless AI with your photos while boiling the ocean.\n \nreply",
      "Could you please tell some? I am about to switch to a self-hosted solution.\n \nreply",
      "https://github.com/meichthys/foss_photo_libraries\n \nreply",
      "I am very very happy with immich. See the other commenter's response for many more.\n \nreply",
      "It is a weird advertisement when Ente (the name of the service) will also see your photos.\n \nreply",
      "The service is end to end encrypted with local AI for indexing.I tried it a few months ago however and the upload/encryption was so slow from their desktop app it would have taken weeks to migrate my photos to the service.\n \nreply",
      "Try uploading some CSAM and then we'll see if they really don't see your photos.Privacy and Safety are a spectrum, you can't have both simultaneously.\n \nreply",
      "Apple Photos in iCloud can also be E2E encrypted (though not by default: you have to explicitly enable that), are indexed locally, and Apple's pricing for storage is about half of this service.\n \nreply"
    ],
    "link": "https://theyseeyourphotos.com/",
    "first_paragraph": ""
  },
  {
    "title": "Man ran 700 miles to make 'insanely impressive' art on GPS fitness app (washingtonpost.com)",
    "points": 41,
    "submitter": "bookofjoe",
    "submit_time": "2024-12-11T22:42:41 1733956961",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=42393867",
    "comments": [
      "Related:World\u2019s largest (7 countries, 7,237km) Strava art is finally finished (2 points, 2 years ago, no comments) https://news.ycombinator.com/item?id=32523588Strava artist draws pictures with his bike and GPS (2 points, 9 years ago, no comments) https://news.ycombinator.com/item?id=11138751US teacher tracks his bike rides by GPS - and creates some stunning artwork (14 points, 13 years ago, 4 comments) https://news.ycombinator.com/item?id=3518331Swedish artist ships GPS to make world's biggest sketch (4 points, 17 years ago, 3 comments) https://news.ycombinator.com/item?id=201026\n \nreply",
      "Can someone who is familiar with Toronto say if this feels realistic to them? I understand the bits where the lines stick to the street grid, but some of the lines appear to be crossing blocks in a very clean and straight way.Not saying it is not real but it is just a bit too clean to me. Even with extremely good planning i would expect that the lines would need to compromise sometimes to avoid buildings.\n \nreply",
      "For the diagonal lines the runner turned his GPS off at some points, and turned back on at another, causing Strava to fill in the missing data with a diagonal line\n \nreply",
      "from his twitter/x acct, https://x.com/mccabedj/status/1860191515349504467 and https://x.com/mccabedj/status/1860192166561349753    People were generally understanding when I walked through homes. But when it got inconvenient I just stoped/started the app to get Strava to draw B-line between the points\n\n    Strava requires a run to be continuous. So I do the full run. And in post I trim off the extra bits to get the hat in the air\n \nreply",
      "https://archive.ph/ocmAK\n \nreply",
      "Direct link to the TikTok video: https://www.tiktok.com/@duncan77mccabe/video/743836674477690...\n \nreply",
      "Also on youtube: https://youtu.be/fO--TM4_6hw\n \nreply",
      "I need to meet this individual \u2014 amazing!\n \nreply",
      "I used to, back in the better days whence I ran, keep gps records of most of my trajectories.None ever made significant art, but it's not an unpleasant exercise in itself to mentally revisit the paths while attempting to remember as many details as possible.There's so much think-time on distance runs, so much to take in, if the discomfort doesn't lower the proverbial brow beneath the shoe into the gutters of the mind, which it often did for me.\n \nreply",
      "Sometimes I enjoy doing this, too. Makes one wonder about memory..\n \nreply"
    ],
    "link": "https://www.washingtonpost.com/lifestyle/2024/12/02/strava-art-run-toronto-mccabe/",
    "first_paragraph": ""
  },
  {
    "title": "NYC wants you to stop taking traffic cam selfies, but here's how to do it anyway (pcmag.com)",
    "points": 209,
    "submitter": "gnabgib",
    "submit_time": "2024-12-13T02:49:01 1734058141",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=42405559",
    "comments": [
      "The referenced project is open source https://github.com/wttdotm/traffic_cam_photoboothTIL NYC traffic cams have a live feed on the web\"NYC DOT traffic cameras only provide live feeds and do not record any footage.\nThere are 919 cameras available via the NYCTMC.org website.\"[1]random traffic camera\nhttps://webcams.nyctmc.org/api/cameras/a8f2d065-c266-4378-ac...[1] https://webcams.nyctmc.org/about\n \nreply",
      "What is their use if they don't record anything? Just to measure current traffic levels? I assumed they were all used as ALPRs. I've seen some cameras sprouting up in my small town and it worries me.\n \nreply",
      "Yes, and they predate the internet.They are essentially a public live traffic report so that the news agencies are not running helicopters amok to get the same footage; and many of the cameras are in tight locations where it would be hard to fly helicopters or drones without irritating neighbors or being a danger to public safety.\n \nreply",
      "Just because NYCTMC doesn't record doesn't mean NYC Police, or any other group doesn't... Could have been intentionally coordinated or not at the beginning, but it almost certainly is recorded by several players now\n \nreply",
      "These traffic cams are a tiny portion of all the cams in NYC, and most have abysmal resolution and fps (~1 fps via mjpeg).The NYPD has its own much more vast network of cams, but unfortunately these are not accessible to the public. This article from 2021 puts it at 16k nypd cams, I bet it\u2019s more now. https://www.amnesty.org/en/latest/news/2021/06/scale-new-yor...\n \nreply",
      "You should be able to get that information:https://www.nyc.gov/site/nypd/about/about-nypd/policy/post-a...Although:https://www.brennancenter.org/our-work/analysis-opinion/nypd...\n \nreply",
      "Google NYPD Viper Unit\n \nreply",
      "Video Interactive Patrol Enhancement ResponseIt sounds like they install their own cameras though.\n \nreply",
      "The small(ish) town I grew up in started using cameras a dozen or more years ago.They get used with computer vision to control and coordinate traffic lights (sometimes with the help of inductive loops in the pavement, and sometimes without).In this particular case:  They don't record anything, and their ISM 900MHz backhauls don't have enough bandwidth for centralized video anyway.(Sources:  Background in RF, and I used to hang out with the city employee who took care of this system along with most other things relating to traffic lights there.)\n \nreply",
      "In 2020-2021, the NYC Mesh team ran a project called streetwatch.live (dead link) to archive footage from traffic cameras.\n \nreply"
    ],
    "link": "https://www.pcmag.com/articles/nyc-wants-you-to-stop-taking-traffic-cam-selfies-but-heres-how-to-do-it",
    "first_paragraph": ""
  },
  {
    "title": "Hacking physics from the back of a napkin (hapax.github.io)",
    "points": 87,
    "submitter": "g0xA52A2A",
    "submit_time": "2024-12-14T17:31:02 1734197462",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42418309",
    "comments": [
      "MIT OCW has a free textbook \"The Art of Insight in Science and Engineering: Mastering Complexity\", which I usually point folks to on the subject of estimating physical systems.  I think this is a real gem.  Another work of the author is referenced at the bottom of the article.  I'll also vouch for the referenced Guesstimation (though I hate the title).https://ocw.mit.edu/courses/res-6-011-the-art-of-insight-in-...\n \nreply",
      "A feel for reasonable values, and rough-quantitative understanding, is widely thought important for expertise. Despite the limited attention it usually gets in education. But even when instruction is relatively well resourced, as with college seminars devoted entirely to estimation, the content available seems less than inspiring. \"Estimate turkey cooking time from first principles, without recourse to wrapper or google\"... um, yay? Either we're profoundly confused about utility, or there seems an under-addressed opportunity to gather content which revels in that utility.If beyond call-and-response, memorize-and-regurgitate, plug-and-chug, problem-based-reasoning, and the-equation-is-the-phenomena, there is something more to aspire to, where quantitative understanding in content is pervasive and foundational, something which illuminates, simplifies, scaffolds, integrates and exercises understanding... then maybe we've a whole lot of content-creation work for the collective todo list?\n \nreply",
      "For biology, bionumbers[1] is a fun resource.[1] https://bionumbers.hms.harvard.edu/Search.aspx?task=searchby...\n \nreply",
      "I saw all of these hacks in my undergrad physics education and yes, these were the highlights of the whole experience. They should be spread as far and wide as possible.\n \nreply",
      "Is this good book for beginners?\n \nreply"
    ],
    "link": "https://hapax.github.io/physics/teaching/hacks/napkin-hacks/",
    "first_paragraph": "QML researcherFebruary 24, 2020. The computational power of a humble napkin is\n  awesome. I discuss three napkin algorithms \u2014 dimensional\n  analysis, Fermi estimates, and random walks \u2014 and use them to\n  figure out why rain falls, the length of the E. coli genome, and the\n  mass of a proton, among other things. These examples suggest a\n  napkin-based approach to teaching physics.Hacker spirit. Nowadays, the word \u201chacker\u201d conjures up visions of\nRussian trolls, Julian Assange, and Angelina Jolie\u2019s 90s pixie cut.\nBut a nobler usage predates this.\nHacker culture, in the original sense, grew out of places like MIT\nin the 60s, with its tradition of highbrow silliness and elaborate technical pranks.\nAlthough associated with programming, hacker spirit can be viewed as\na broader ethos about play, understanding and creativity.\nIn the words of open-source gnuru Richard Stallman,\nWhat [hackers] had in common was a love of excellence and\nprogramming. They wanted to make the programs that they used "
  },
  {
    "title": "Antimatter Production, Storage, Control, Annihilation Applications in Propulsion (sciencedirect.com)",
    "points": 100,
    "submitter": "belter",
    "submit_time": "2024-12-14T17:21:58 1734196918",
    "num_comments": 83,
    "comments_url": "https://news.ycombinator.com/item?id=42418264",
    "comments": [
      "I don't have references in front of me, (EDIT: I do!) but IIRC it takes about a kilogram of mass-energy to accelerate a kilogram mass to about 0.85c.  But that kilogram would have to be carrying another kilogram of matter/anti-matter fuel to decelerate again.  So there is a kind of relativistic Tsiolkovsky equation for mass-energy propulsion vehicles that carry their own fuel.Zipping around the galaxy at 0.95c, stopping at destinations and then zipping off again will require carrying a lot of antimatter with you.EDIT: Thanks to Wolfram Alpha I was able to see that it the kinetic energy of 1 kg at 0.87c is very close to the mass energy of 1kg of matter.\n \nreply",
      "Even at .99c, everything interesting is years, decades, centuries, and millennia apart. All interstellar solutions include maintenance over millennia. Once you're doing that, relativistic velocity is just a hazard, not a boon. If we do conquer interstellar travel it'll probably be at 1% c. The factor of 10-20x scale won't matter to such long lived civilizations.\n \nreply",
      "From the perspective of someone on the ground, yes. But due to time dilation, it could be just a few days for the people on the ship.\n \nreply",
      ".99c only dilates time by a factor of 7, so one year of time passed on a vehicle would yield ~7 light years.Heck, you have to get better than 5 nines to even compress a year into a day, .9999963c, which would take a freakish amount of energy to accelerate a KG to (3.3 \u00d7 10^19 J).\n \nreply",
      "On top of this at 1g it takes ~1 year accelerating to that speed and another year decelerating.  So even the nearest star is going to be a multi year subjective journey or a really unpleasant trip.\n \nreply",
      "This is assuming that you have to be conscious during the trip. Antimatter storage is a lot more scifi than human hibernation.\n \nreply",
      "The problem with human hibernation is that it is almost impossible to ethically test.What medical ethics board is going to approve a research project \"put healthy experimental subject in coma for 5 years, observe what they are like when they wake up?\"Reminds me of https://en.wikipedia.org/wiki/Deep_sleep_therapy and https://en.wikipedia.org/wiki/Chelmsford_Royal_CommissionI think this is an example of a technology which, if it is ever developed, is most likely to be developed by some kind of totalitarian regime which has no ethical qualms about human experimentation.\n \nreply",
      "Heinlein story time! Door into Summer\nhttps://en.wikipedia.org/wiki/The_Door_into_Summer\nProtagonist signs their life away for a 'cold sleep' that will allow them to wake up many years later. Tech is iffy. They do it because they are desperate, and the interest gained over time should make them rich (doesn't happen of course).\n \nreply",
      "That's not really how hibernation works in mammals, they have to raise their body temperature and come out of hibernation every couple weeks, the leading theory is this is to (ironically) catch up on sleep.Inducing torpor would have major medical uses in surgical and emergency medicine, it's not just useful for passing the time.\n \nreply",
      "At .99c, the Lorentz factor is only about 7, so 1 year of ship time for 7 years of earth time.\n \nreply"
    ],
    "link": "https://www.sciencedirect.com/science/article/pii/S2666202724004518",
    "first_paragraph": ""
  },
  {
    "title": "Should Programming Languages Be Safe or Powerful? (lambdaland.org)",
    "points": 10,
    "submitter": "soegaard",
    "submit_time": "2024-12-14T22:58:03 1734217083",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=42420091",
    "comments": [
      "For those skipping to comments based on title alone, this isn\u2019t really much discussion about a trade off. It starts that way and then goes \u201cbut with my favorite language you can have both!\u201d And spends 3/4 of the article telling you about it.The most value from this article is the clickbait title yielding some interesting discussion from people on that question outside of the actual article contents.This article isn\u2019t bad but the name is terrible and a bit clickbaity\n \nreply",
      "Since we're avoiding clickbait: the language in question is Racket.\n \nreply",
      "I think the emergence of the LLM-integrated IDE can be leveraged to transform a powerful yet unsafe language like c into a safe language.\n \nreply",
      "Programming languages should be safe by default, except in clearly delimited sections of code (as small as a single expression) where the programmer requests less safety.Safety is the property that erroneous situations are intercepted and diagnosed, rather than running off with incorrect results or behaving unpredictably. It can be a continuum based on the number and kinds of situations that are treated safely.\n \nreply",
      "tldr - should your language be Lisp or Haskell?\n \nreply",
      "> If you want to write device drivers, then C is great for you.No, it really isn't. It's 2024, people who should know better should stop saying this.\n \nreply",
      "Alright, I'll assume you know better; what should we use? Preferably on a variety of OSs; ex. I imagine you can write Windows or Haiku drivers in C++, but I'm only aware of C being an option in any BSD, and Linux support for anything but C is spotty (obviously Rust is making headway, but last I heard you could only write certain kinds of drivers using it as the underlying support is built out).\n \nreply",
      "If your platform is supported by LLVM, Zig is worth serious consideration. It is able to target any of the operating systems you mention, from any of the operating systems you mention.  Linking to the syscall interface / libc / C libraries is straightforward, exporting functions which respect the local ABI is quite simple.\n \nreply"
    ],
    "link": "https://lambdaland.org/posts/2024-11-21_powerful_or_safe_languages/",
    "first_paragraph": "Should a programming language be powerful and let a programmer do a lot, or should it be safe and protect the programmer from bad mistakes? Contrary to what the title insinuates, these are not diametrically opposed attributes. Nevertheless, this is the mindset that underlies notions such as, \u201cmacros, manual memory management, etc. are power tools\u2014they\u2019re not supposed to be safe.\u201d If safety and power are not necessarily opposed, why does this notion persist?The problem\u2014I think\u2014is that historically you did have to trade safety for certain kinds of power: if you wanted to write a high-performance device driver, C\u2014with all its unsafe behavior\u2014was your only option. This founded the idea that the \u201cpower tools\u201d of the industry were fundamentally dangerous.There\u2019s a few things wrong with this though:Power is relative to the domain of interest. Both Haskell and C are powerful, but in completely different ways. So, when judging whether an aspect of a language is powerful or not, consider its app"
  },
  {
    "title": "Using Libc for GPUs (llvm.org)",
    "points": 103,
    "submitter": "hochmartinez",
    "submit_time": "2024-12-11T15:29:21 1733930961",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=42388724",
    "comments": [
      "Very cool to see my project posted here!The motivation behind a lot of this was to have community LLVM implementations of runtime functions normally provided by the vendor libraries (C math, printf, malloc), but if you implement one function you may as well implement them all.Realistically, the infrastructure behind this project is more relevant than the C library calls themselves. The examples in the linked documentation can be used for any arbitrary C/C++ just as well as the LLVM C library, it's simply statically linking. This is what allowed me to compile and run more complicated things like libc++ and DOOM on the GPU as well. The RPC interface can also be used to implement custom host services from the GPU, or used to communicate between any two shared memory processes.\n \nreply",
      "I wonder how GPU is going to access an unknown size NULL terminated string in system RAM, the strchr() source looks like normal C++. In my minimal Vulkan GPGPU experience the data need to be bound to VkDeviceMemory to be accessible through PCI bus with compute shader, is LLVM libc runtime doing similar set-ups in the background, and if so, is it faster than glibc's hand-tuned AVX implementation?\n \nreply",
      "This is libc running on the GPU, not a libc spanning CPU and GPU. The primary obstruction to doing that is persuading people to let go of glibc. The spec of \"host runs antique glibc, GPU runs some other thing that interops transparently with glibc\" is a nightmare of hacks and tragedy.What would be relatively easy to put together is llvm libc running on the host x64 and also llvm libc running on the GPU. There's then the option to do things like malloc() on the GPU and free() the same pointer on the CPU. Making it genuinely seamless also involves persuading people to change what function pointers are, do some work on the ABI, and preferably move to APUs because PCIe is unhelpful.There's an uphill battle to bring people along on the journey of \"program the things differently\". For example, here's a thread trying to drum up enthusiasm for making function pointers into integers as that makes passing them between heterogenous architectures far easier https://discourse.llvm.org/t/rfc-function-pointers-as-intege....\n \nreply",
      ">making function pointers into integers as that makes passing them between heterogenous architecturesThis is interesting, though function pointers are long expected to be address on binary, C-brained people like me would probably adapt to the concept of \"pointer to a heterogeneous lambda object\" or \"shared id across heterogeneous runtimes\" easier.\n \nreply",
      "Yeah, I might need to take another angle at the branding / sales part, possibly with a prototype in hand.I was hopeful that wasm was sufficient prior art. Whether function pointers are absolute addresses, or relative to the start of text, or resolved through trampolines filled in by the loader, or are offsets into some table is all (nominally) invisible to the language.Integer in [0, k) has the nice feature that multiple GPUs can each index into their own lookup table containing pointers to the local code section. Or for calling into another machine - it's essentially a cheap serialisation / perfect hash done at linker time. It makes indirect calls slower, but indirect calls are slow anyway so hopefully saleable.\n \nreply",
      "If the host has an antique glibc, surely it has an antique llvm libc? Does llvm just have a more stable ABI or something?\n \nreply",
      "It seems like unified memory has to be the goal. This all just feels like a kludgy workaround until that happens (kind of like segmented memory in the 16-bit era).\n \nreply",
      "Is unified memory practical for a \"normal\" desktop/server configuration though? Apple has been doing unified memory, but they also have the GPU on the CPU die. I would be interested to know if a discrete GPU plugged into a PCIe slot would have enough latency to make unified memory impractical.\n \nreply",
      "It\u2019s clearly not practical now, but that doesn\u2019t mean it won\u2019t be at some point.\n \nreply",
      "Running normal c directly on gpu has been the dream for a long time. this looks excellent\n \nreply"
    ],
    "link": "https://libc.llvm.org/gpu/using.html",
    "first_paragraph": "Table of ContentsUsing the GPU C libraryOffloading usageOpenMP Offloading exampleBinary formatDirect compilationBuilding for AMDGPU targetsBuilding for NVPTX targetsOnce you have finished building the GPU C library it\ncan be used to run libc or libm functions directly on the GPU. Currently, not\nall C standard functions are supported on the GPU. Consult the list of\nsupported functions for a comprehensive list.The GPU C library supports two main usage modes. The first is as a supplementary\nlibrary for offloading languages such as OpenMP, CUDA, or HIP. These aim to\nprovide standard system utilities similarly to existing vendor libraries. The\nsecond method treats the GPU as a hosted target by compiling C or C++ for it\ndirectly. This is more similar to targeting OpenCL and is primarily used for\nexported functions on the GPU and testing.Offloading languages like CUDA, HIP, or OpenMP work by compiling a single source\nfile for both the host target and a list of offloading devices. In order to\n"
  },
  {
    "title": "How big data created the modern dairy cow (worksinprogress.co)",
    "points": 12,
    "submitter": "surprisetalk",
    "submit_time": "2024-12-13T15:41:47 1734104507",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=42409496",
    "comments": [
      "There is no way the 6x yield increase per cow did not negatively affect the taste, nutrition, and quality of the milk. Indeed, most of the \"milk\" you buy in large industrial countries barely deserves the term - it's got no taste or subtle flavor. It's even worse regarding derived butters and cheeses.Yield is a great and easy thing to optimize becuase it's easy to measure. And if you increase yield at the expense of taste at suffiently slow rate, the change is impercetible to the customer. Happens slowly over decades so that a nice glass of milk is now like drinking dirty wash water. And a nice fat steak becomes ground up crickets and synthetics.Moral of the story. Get your own low efficiency cows.\n \nreply",
      "I've had milk from a cow not bred for increased yield (Hereford). It was unremarkable.\n \nreply",
      "those cows probably still exist right? you can still get milk from them. Do they taste better?\n \nreply",
      "Yes, but because of the way that the milk industry is shaped in the US it\u2019s exceedingly hard to get and many people are afraid of it.\n \nreply",
      "What is this typically called? Are you referring to raw milk? I think this is a different thing.\n \nreply",
      "Fantastic article. I didn't realize dairy cows lactated ~60lbs/day or ~3.5% of their body weight. Totally insane. Chickens appear to be this way too -- from some quick research Rhode Island Reds are ~3kg, lay ~300 eggs/year and each egg is ~62g.The graph at the end (\"US milk yield continues to grow, but falls short of its genetic potential\") is interesting. If I saw that graph, I would interpret it as dairy farmers overfitting on the genetic yield potential measure, not as something that needs explanations like climate change.\n \nreply",
      "I think the second paragraph is poorly written> America\u2019s cows are now extraordinarily productive. In 2024, just 9.3 million cows will produce 226 billion pounds of milk (about 100 million tons) \u2013 enough milk to provide ten percent of 333 million insatiable Americans\u2019 diets, and export for good measure.Is that all the cows in the US?  Why tell us how many cows produce 10 percent of demand?\n \nreply",
      "What he means is that 10M cows, all the cows in the US, produce dairy products that are 10% of calories in US diets.\n \nreply"
    ],
    "link": "https://worksinprogress.co/issue/how-big-data-created-the-modern-dairy-cow/",
    "first_paragraph": "What do cryogenics, butterfat tests, and genetic data have in common? They\u2019re some of the reasons behind the world\u2019s most productive dairy cows. Here\u2019s how it all started.No matter where you are in the world, there\u2019s a good chance the milk or cheese you\u2019re buying is the product of the US dairy industry. Even if it didn\u2019t come from American cattle, the cow that produced the milk could well have been inseminated by an American bull.The United States has been the world\u2019s largest supplier of cattle genetics since at least 1992. In 2022, the US exported $295 million in bovine semen, 47 percent of the world\u2019s exports. Few countries even come close to this market share: the next biggest exporters are Canada at 14 percent and the Netherlands at seven.America\u2019s cows are now extraordinarily productive. In 2024, just 9.3 million cows will produce 226 billion pounds of milk (about 100 million tons) \u2013 enough milk to provide ten percent of 333 million insatiable Americans\u2019 diets, and export for good"
  },
  {
    "title": "Occasional Paper: Four Hidden Species of Portuguese Man-O'-War (crookedtimber.org)",
    "points": 38,
    "submitter": "akkartik",
    "submit_time": "2024-12-14T19:53:12 1734205992",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=42419036",
    "comments": [
      "These are called blue bottles in Australia, and we get them fairly regularly where I live (Bondi) but also up and down the east coast. I'm sure they are on the west coast as well.I had thought they were 3 distinct parts, not 5. It is a fascinating bit of symbiosis.\n \nreply",
      "I've been stung by one of these on my foot while surfing. It was painful. Painful enough for me to get out of the water and call it a day.The man in the photograph with his limb covered must have had an awful day.\n \nreply",
      "I was stung on the stomach and side ... It bent me over in the water then somehow I crawled up the beach and laid down.\n \nreply",
      "More than a bad day. We should assume that those marks left permanent scars.I had spotted visible marks on a leg, five years after a mild encounter.\n \nreply",
      "Fascinating read. Never knew about these 'aggregate' organisms.\n \nreply",
      "We get lots of these in the Bay of Biscay for some reason. Not really warm waters, it's the North-is Atlantic. I guess it's mild in summer, that's when we get them.\n \nreply",
      "> a single Portuguese man-o\u2019-war is composed of four or five separate animals.Sorry, no.  Just because they are not physically connected to each other doesn't make them separate animals.  They are a single animal made of parts that happen not to be physically attached to one another.  This is not uncommon in nature.  Colony insects like ants and bees and termites are even more extreme examples of this.  An individual ant (or bee or termite) is not an organism any more than (say) your spleen is.  Most ants (or bees or termites) are sterile.  They cannot reproduce.  It is the colony that is the organism, not the individual insect.\n \nreply",
      "I'm no expert but going off of Wikipedia, I don't see the issue with the passage you quoted?The nomenclature for colonial species seems to be that even if the colony can be a single organism, the parts are still referred to as animals (e.g. \"A zooid is a single animal that is part of a colonial animal.\")But anyway, is there even a strict definition for what an organism is?\n \nreply"
    ],
    "link": "https://crookedtimber.org/2024/11/11/occasional-paper-four-hidden-species-of-portuguese-man-o-war/",
    "first_paragraph": "by Doug Muir on November 11, 2024There\u2019s been a a certain amount of negativity floating around lately. So, let\u2019s talk about a toxic, venomous freak of nature and the parasite that afflicts it.Biology warning, this gets slightly squicky.Let\u2019s start with the toxic, venomous freak of nature:\u00a0 the Portuguese man-o\u2019-war.If you\u2019ve spent a lot of time in warm ocean waters, you\u2019ve probably encountered one of these guys.\u00a0 They\u2019re hard to miss!\u00a0 They come in a variety of colors \u2014 pink, blue, purple \u2014 and they\u2019re pretty prominent, floating on the surface of the ocean like discarded party balloons.\u00a0 And if you\u2019ve ever been stung by one, well, you probably remember that.\u00a0 Their stings aren\u2019t lethal to humans, but they\u2019re welt-inducing and painful.So it\u2019s a jellyfish.\u00a0 Except it isn\u2019t really: it\u2019s several jellyfish, smooshed together.\u00a0 And here\u2019s where the \u201cfreak of nature\u201d part kicks in.I mean, yeah, strictly speaking nature has no freaks; every species that exists, belongs; everything is a product"
  },
  {
    "title": "Array Languages for Clojurians (2020) (appliedscience.studio)",
    "points": 40,
    "submitter": "simonpure",
    "submit_time": "2024-12-14T12:33:32 1734179612",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42416681",
    "comments": [
      "Jeeze, the overlap of people who are interested in Clojure and array langs couldn't exceed the low double digits.Well, if you are among that sort of masochist and you like Clojure and APL, you could play with libapl-clj: https://github.com/jjtolton/libapl-cljI haven't touched it in awhile because honestly whether your coworkers do APL or your coworkers do Clojure they will hate you if you use this.  Ye be warned.God forbid there's interest in it we could add more support... but there are like 75 significant systems engineering and architecture questions you should be prepared to either answer or blithely ignore before using this Frankenstein's monster.\n \nreply",
      "I feel bad for companies that hire \u201cClojure data scientists\u201d.\n \nreply",
      "why?\n \nreply",
      "Yeah? Like poor little mom and pop shops like Walmart?\n \nreply"
    ],
    "link": "http://www.appliedscience.studio/articles/array-programming-for-clojurists.html",
    "first_paragraph": "A discussion in the Clojure data science Zulip led me to Slobodan Blazeski\u2019s enlightening article Array languages for Lisp programmers in the journal of the British APL Association. He quotes Alan Perlis:As a lisp, Clojure of course qualifies as one such mind-altering substance. (It arguably qualifies again on the basis of its focus on immutability.) But Blazeski\u2019s article points out that array-based languages such as J, its predecessor APL (\"A Programming Language\"), and the proprietary q are equally mind-expanding. Let\u2019s see what was \u2013 and remains \u2013 so compelling about the array programming approach to problems, and compare it to Clojure\u2019s approach.Array languages make it both simple and easy to work on whole containers the same way as you work on scalars. Let\u2019s look at J first. (Our array programming examples will be one-liners, with the result on the following lines.) We\u2019ll start by adding the scalar 2 to an array of 2, 3, and 4:The same + operator works on multiple arrays element-"
  },
  {
    "title": "Byte Latent Transformer: Patches Scale Better Than Tokens (meta.com)",
    "points": 281,
    "submitter": "zxexz",
    "submit_time": "2024-12-14T06:36:35 1734158195",
    "num_comments": 71,
    "comments_url": "https://news.ycombinator.com/item?id=42415122",
    "comments": [
      "The summer that BERT came out I was working at a startup that was using character-based CNN models for classification.  We were thinking a lot about alternate representations,  other members of the team were keen on word vectors but I wasn't,  particularly because it seemed the documents were were working on frequently had out-of-dictionary words,  because those words were important,  and because discarding them would lead to failure.(We were working on \"foundation models\" too,  so it's not just being out-of-dictionary in the final model that's a problem but being out-of-dictionary in the foundation model which is more expensive to train.)We were doing OK with character based models for classification but people believed that storing the \"dictionary\" inside the neural net was not a good use of the neural net so there was a lot of enthusiasm for tokens.Meanwhile I felt so sure that schemes like Word2Vec were doomed that I had left an earlier project using RNNs where the goal was text understanding with a foundation model made by training an RNN to write fake abstracts for case reports from PubMed.When byte-pair encoding was introduced I remember telling people in a meeting that it was the first tokenization scheme we'd looked at that I could endorse.I have to admit though that I wish we could work at the character label.\n \nreply",
      "I was really excited for CANINE [1] but it never really went anywhere.  Tokens are a hack.  They work for the most part, but it\u2019s clear when they don\u2019t.[1] https://arxiv.org/abs/2103.06874\n \nreply",
      "Do you mean that all produced output must be a chain or words found in a dictionary?The real-world for humans has them creating and using non-dictionary words to communicate daily. A good example is \"notify\", defined in the dictionary. \"notifier\", which is not and is used to describe \"a means to notify someone\".  The code to send an email notification is an \"email notifier\", then there is text message, voice call, call center call back notifiers ....All industries and organizations have jargon, custom defined words not found in a dictionary and use non distinctive acronyms.How would a ML output be useful if it cannot handle real world commutation and  only lab based sanitization of in-dictionary only responses?\n \nreply",
      "(Author here)If I understand your question right, this is one of the reasons BPE is nice and the parent liked it. For any character sequence, provided the characters are in the alphabet used to create the BPE vocab, there are no unknown words/sequences. One downside of some previous tokenization methods is you could have unknown/UNK tokens, EG dictionary based methods.In our paper with bytes, we also avoid the UNK issue, since we can have an embedding for every possible byte, since it\u2019s not that many (and for sequences of bytes we use hash embedding, although we did test n-gram lookups for the top K frequent byte n-grams in the training data).\n \nreply",
      "Nice work. Thank you for commenting on HN!Did you guys try using an RNN or some other kind of DNN to encode the patches?\n \nreply",
      "I don't believe so, or at least if someone tried it didn't work well enough that I remember :). Some of the motivation for the architecture changes in encoding patches stemmed from finding FLOP efficient ways to express relationships between byte sequences. E.G., having a long context window makes sense when dealing with tokens, but you don't need as long as an attention window if you're attending byte sequences to make patch representations, since the patch representations will implicitly be part of a longer context window in terms of number of patches.\n \nreply",
      "Thanks for the quick reply!Interesting. I would have thought one of those \"minimum viable\" RNNs (like https://arxiv.org/abs/2410.01201) would have been ideal for this. I might tinker a bit with this :-)\n \nreply",
      "That's the OP's point. At the time, the community was split between word-level, which has the shortcomings you're describing, and byte-level which is uselessly compute intensive. BPE was the first reasonable in-between. BLT improves on BPE by having the the compression learnable rather than precomputed\n \nreply",
      "I really hope this works out. Death to tokenizers!Interesting that it's a hierarchical structure but only two levels of hierarchy. Stacking more levels seems like an obvious direction for further research.Note: I posted this comment on another related story[1] and the author replied:\"Author here :), I do think it\u2019s a good direction to look into! That said, aside from it being a bit too much to do at once, you\u2019d also have to be careful about how you distributed your FLOP budget across the hierarchy. With two levels, you can make one level (bytes/local encoder) FLOP efficient and the other (patches/global encoder) FLOP intensive. You\u2019d also need to find a way to group patches into larger units. But ya, there are many directions to go from here!\"[1] https://news.ycombinator.com/item?id=42413430\n \nreply",
      "To create a patch, a small model is used to predict the likelihood for the next character in the input string. Input string: 'Lazy dog jumped over a fence.' Use the model to predict the likelihood of each character.For example:    100% sure the next character is 'a'.\n    Or maybe it's 10% sure it's 'a', 10% sure it's 'b', and so on.\n\nThen we chunk character estimates together.\nHow many characters?\nEnough characters so that the total uncertainty (entropy) in each chunk is about the same.\nAnd there you have your 'patch' (or 'token').\n \nreply"
    ],
    "link": "https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/?_fb_noscript=1",
    "first_paragraph": "December 12, 2024We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented dynamically based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first flop controlled scaling study of byte-level models up to 8B parameters with 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed-vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling th"
  },
  {
    "title": "Show HN: A simple web game to help learn chords and basic progressions (yottanami.com)",
    "points": 41,
    "submitter": "yottanami",
    "submit_time": "2024-12-14T11:05:14 1734174314",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=42416258",
    "comments": [
      "Congrats on launching!Limiting this to work only with folks who have MIDI keyboards attached to their machines probably cuts your audience down by 99.9999%. Especially here on hacker news.I think you\u2019re going to find it tough going to get any real feedback with that requirement. You might find better luck launching in a community of folks who are more likely to meet your hardware requirements.\n \nreply",
      "I disagree. I believe it is a good idea to focus in your \"customer\" and ignore the rest.This is the best advice I was given. People are always asking for you to unfocus and disperse.Most people that is serious about learning chords have a MIDI device. It will make your life way harder to add non MIDI devices and the people that will use it are not really that committed. MIDI devices are so cheap today that anyone that wants one could get one.BTW I have several MIDI devices because I create my own games too.\n \nreply",
      "Just got my MPK61 hooked up this week\u2026 I\u2019ll have to give it a try.The set of people that have MIDI capable keyboards and want to improve musically is vanishingly small compared to those without, true, but it\u2019s clearly a lucrative market if you do it right: there are tons of (music) keyboard classes out there online.And a lot of the hardware and software are pricey enough that I imagine ads could be pretty lucrative too.\n \nreply",
      "They're a minor investment now a days with some being under $30. It's just buttons at the end of the day\n \nreply",
      "okay, but.. any path to connecting this to phone mics so that it can function anywhere? whats the state of the art in tone and chord recognition models?\n \nreply",
      "No need for any ML models here, you can \"just\" do an FFT on a conventional signal and pick out the peaks\n \nreply",
      "Agree - this is something I'd be interested in trying out, but I don't have a MIDI keyboard.\n \nreply",
      "I do think it would be nice to let me back up to even a janky keyboard input?not sure if anyone has good mapping for that.\n \nreply",
      "No drop down options exist for MIDI keyboard?\n \nreply",
      "You need to both have existing midi devices and allow browser access.\n \nreply"
    ],
    "link": "https://chords.yottanami.com/",
    "first_paragraph": "Learn and practice piano chords and common chord progressions in both major and minor keys!Select your MIDI keyboard and key:"
  },
  {
    "title": "Show HN: Svader \u2013 Create GPU-rendered Svelte components (github.com/sockmaster27)",
    "points": 161,
    "submitter": "sokmastr",
    "submit_time": "2024-12-14T11:00:49 1734174049",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=42416230",
    "comments": [
      "This reminds me a little of the <shader-doodle> web component, which also works in any framework or plain HTML:https://github.com/halvves/shader-doodle\n \nreply",
      "Really cool idea with doing it as a web component.There's also Pocket Shader which is pure JS:\nhttps://github.com/braebo/pocket-shader\n \nreply",
      "This is very cool. However, I do wonder about your use case for making things like sliders.CSS and HTML already have several decades of work on accessibility and cross platform support. It's far from trivial to recreate this.For things like hero pages, I can see the benefit. But for basic UI like sliders, are you reinventing the wheel here?\n \nreply",
      "Talking about the slider specifically, I'm of course not advocating for going out and rewriting all sliders using shaders.This is just an example of a place where you can use a shader to create a cool, interactive visual effect.On the example site I've implemented the slider as a regular HTML slider element, and using a canvas to replace its visual appearance. Doing something like this, I don't believe there's anything in the way of getting all the accessibility benefits of a regular slider element, though I could be wrong, I'm no expert by any means.\n \nreply",
      "None off he examples work on Safari iOS 18.1.1\n \nreply",
      "Even the the example at svader.vercel.app/hello-world/webgl/?\nTesting on the Safari browsers is currently a TODO, and the library is in general still experimental when it comes to wider browser support, but I would have expected at least the standard WebGL examples to work.\nDoes it at least display the \"WebGL not supported\" message?\n \nreply",
      "WebGL works very well on iOS.If in doubt try https://luduxia.com/whichwayround/ ;P\n \nreply",
      "Just a white page.\n \nreply",
      "Strange. I'll have to look into this.Thanks for letting me know!\n \nreply",
      "If you don\u2019t have access to Apple devices, I find Epiphany/GNOME Web to be a pretty good proxy\u2014it uses WebKit, unlike most things these days that use Blink. Most times, Safari issues appear in Epiphany as well. This seems to hit a TypeError in https://svader.vercel.app/_app/immutable/chunks/entry.B0k4a9... line 1, column 9932 (Xe(ht,e)), and I have no time to investigate further.\n \nreply"
    ],
    "link": "https://github.com/sockmaster27/svader",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Create GPU-rendered Svelte components\n      \n\n\n\n\nCreate GPU-rendered Svelte components with WebGL and WebGPU fragment shaders.Supports Svelte 4 and Svelte 5.In short, a fragment shader can be written as a program that takes the coordinates of a pixel on the screen and returns the color that this pixel should have.\nThis program can be executed on the GPU, ensuring massive parallelism and speed.To learn more about how to write fragment shaders, check out The Book of Shaders.The following is a collection of examples all made using Svader. The live version of all of these can be previewed on svader.vercel.app,\nand the source code can be found in the src/routes/ directory.To use a fragment shader component, you first need to decide whether to use WebGL or WebGPU.\nIf you're unsure about what to use, see the WebGL vs. WebGPU section.The fo"
  },
  {
    "title": "What Is Entropy? A Measure of Just How Little We Know (quantamagazine.org)",
    "points": 234,
    "submitter": "nsoonhui",
    "submit_time": "2024-12-14T07:49:10 1734162550",
    "num_comments": 94,
    "comments_url": "https://news.ycombinator.com/item?id=42415386",
    "comments": [
      "Interesting to read this, 27 years after my PhD* (theoretical physics), in which I did compare the view WITH and the view WITHOUT \u2018unknowns\u2019 causing entropy as a driver.* My PhD was about how to treat a (quantum mechanical) system inside a cavity: a cavity with one perfect mirror and one 99.999999% perfect mirror. The (one dimensional) universe was made whole by another perfect mirror at the other side of the non-perfect mirror (in ASCII art:[100%] \u2014l\u2014 [100-epsilon] \u2014\u2014L\u2014\u2014\u2014 [100%]With L >> l.\nThe \u2018whole universe\u2019 solution was simple (using standard quantum mechanics techniques), the \u2018lossy\u2019 \u2018small universe\u2019 was not. But they needed to be the same (physically).\nThus using the exact solution for the \u2018complete\u2019 (l+L) universe and comparing it to possible \u2018small\u2019 (l) universe models in which some non-linear term accounted for loss.\nThe connection between how a lossy system (in which entropy exists/is a driving \u2018force\u2019) and a losless system (in which everything is conserved) is thus not a new insight;-0\n \nreply",
      "I read you're comment with interest, but ultimately I can't understand the point being made because I don't know what kind of mirror you're referring to (optical?), I don't know what 'l' or 'L' represent (lateral spacing of mirrors?, vacuum energy desnities?), and the last sentence I think maybe the word 'how' should be deleted?\n \nreply",
      "The imperfect mirror means that epsilon% of the time the light goes through to a much larger \"back room\" whereas (1-epsilon)% of the time the light just reflects like normal. The point being made is that this is an extension of an ordinary ideal cavity to include unavoidable (but weak) interaction with the much larger system outside of it (aka the whole universe). It just so happens the much larger external system is also being modeled as a simple 1d cavity.In other words, entropy is equivalent to bits of information needed to specify the complete state of the system leaking outside of the confines of where those bits are being observed by an experiment (eg tunneling through an imperfect mirror).Entropy is an accounting tool to keep track of how many bits are missing, and how far this ignorance has percolated into what you can safely predict about the system.\n \nreply",
      "Answers to your questions: \n1): all the way to the left, a mirror with a reflectivity|r| of 1 (or a 100%). In the middle an |r| of slightly below 1. Yes, optical, system with photons (a and a^dagger with [a,a^dagger]=1).\n2) distance between mirrors 1 and 2: l. Distance mirror 2 and 3:L. (Later taking the limit L/l ==>> infinity) \n3) the how is actually correct, I guess the word behaves is missing twice: .... how .... behaves and a .... behaves.\n \nreply",
      "Interesting. Could you share a link to your thesis?\n \nreply",
      "So i am curiouys to know how did u invent new knowledge without computer science\n \nreply",
      "Happy to see the article being discussed here!\nI was responsible for the technical implementation of the interactives. If you are interested in the source code, you can find it here: https://github.com/jnsprnw/mip-entropy\nIt\u2019s built in Svelte 5 with Tailwind.\n \nreply",
      "May I ask why you chose svelte 5 rather than something else? I\u2019ve noticed that a lot of \u201cone off\u201d interactions are being built with svelte nowadays. What are the benefits of using it?\n \nreply",
      "First, when I joined the project, it wasn\u2019t clear how it would be published. Svelte, which outputs compiled JavaScript, can fit into many CMS workflows that newspapers/publishers use.I believe Svelte was developed by Rich Harris at the NY Times for this very reason.We ended up using iFrames, so other frameworks like React could have been used.Second, Svelte is very well suited for these small interactives because it has built-in state, transitions, and reactivity with low overhead.Third, it was a personal choice, as I now do most of my work in Svelte.\n \nreply",
      "> We ended up using iFrames, so other frameworks like React could have been used.Wait, wasn't one of the original selling points of React that it could be embedded piecewise to enhance interactivity of the parts of pages that needed it? It should certainly not need a separate page!\n \nreply"
    ],
    "link": "https://www.quantamagazine.org/what-is-entropy-a-measure-of-just-how-little-we-really-know-20241213/",
    "first_paragraph": "\nAn editorially independent publication supported by the Simons Foundation.\n\nGet the latest news delivered to your inbox.\nCreate a reading list by clicking the Read Later icon next to the articles you wish to save.Type search term(s) and press enterPopular\n                                    SearchesDecember 13, 2024Kristina Armitage/Quanta MagazineContributing WriterDecember 13, 2024Life is an anthology of destruction. Everything you build eventually breaks. Everyone you love will die. Any sense of order or stability inevitably crumbles. The entire universe follows a dismal trek toward a dull state of ultimate turmoil.To keep track of this cosmic decay, physicists employ a concept called entropy. Entropy is a measure of disorderliness, and the declaration that entropy is always on the rise \u2014 known as the second law of thermodynamics \u2014 is among nature\u2019s most inescapable commandments.I have long felt haunted by the universal tendency toward messiness. Order is fragile. It takes months o"
  },
  {
    "title": "I made a free Figma library packed with components for fast prototyping (veryfront.com)",
    "points": 193,
    "submitter": "kojiwakayama",
    "submit_time": "2024-12-14T08:54:03 1734166443",
    "num_comments": 53,
    "comments_url": "https://news.ycombinator.com/item?id=42415637",
    "comments": [
      "I'm really into audio production, and I would re-record the video without using a mechanical keyboard, and ideally not in an echo-y room. It's not very watchable right now. In addition to using a noisy keyboard not appropriate for video recording, you type very hard / angrily. Switching to a quieter keyboard with low key travel might help you prevent RSI in the future.\n \nreply",
      "Thanks Steve for the feedback!\n \nreply",
      "\"Angrily?\" Really?\n \nreply",
      "Like he has a grudge against his keyboard, angry and dwarflike.\n \nreply",
      "Who's downvoting those who object to unwarranted projection of emotions on a stranger?Go back to Reddit.\n \nreply",
      "Hi HN, I\u2019m Koji, software engineer and co-founder of Veryfront, where we offer tools to streamline web app development. Happy to share insights or answer questions about design, development, or anything related to building apps. AMA!\n \nreply",
      "Hi Koji, this looks like a fantastic tool! I think it will pair nicely with Locofy (https://locofy.ai) for handoff from design to AI-generated code to really simplify the frontend process! \nI\u2019ll be sharing this with my design team!\n \nreply",
      "Veryfront is amazing. I've known Koji for several years and he and his team have built a lot of great tech that now covers everything from rapid design iteration, to developing with their web IDE, with a live preview (it actually continually builds your website). and deploying using their cloud hosting solution.The technology stack is very flexible. You can add your own components, libraries, styling solutions, backends, etc. And there is also an export option if you want to leave their platform that gives you everything in code form.The big benefit of veryfront is that you can get started and have a webapp live in no time at all. You don't need anyone working on devops because they do it for you. You don't need to setup a lot of tools because it all runs in the browser. You don't need to figure out how to set up these tools either because they do it for you. You just code your webapp.\n \nreply",
      "Thanks so much Jilles. I really appreciate it!\n \nreply",
      "I'm going to be very naive here as this isn't my space at all. Are these open sourced? If not these, are there react components I can grab for basic front end projects that won't run afoul of licensing?\n \nreply"
    ],
    "link": "https://veryfront.com/figma-kit",
    "first_paragraph": ""
  },
  {
    "title": "Cyborg Insect Factory (arxiv.org)",
    "points": 3,
    "submitter": "slow_typist",
    "submit_time": "2024-12-11T18:43:24 1733942604",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://arxiv.org/abs/2411.13164",
    "first_paragraph": "The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy.Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Llama.cpp Now Supports Qwen2-VL (Vision Language Model) (github.com/ggerganov)",
    "points": 8,
    "submitter": "BUFU",
    "submit_time": "2024-12-14T21:15:12 1734210912",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://github.com/ggerganov/llama.cpp/pull/10361",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \nHave a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.\n  By clicking \u201cSign up for GitHub\u201d, you agree to our terms of service and\n  privacy statement. We\u2019ll occasionally send you account related emails.\n    Already on GitHub?\n    Sign in\n    to your account\n  This PR implements the Qwen2VL model as requested at #9246 .\nThe main changes include:TODO:Steps to convert model and inferenceDownload the official Qwen/Qwen2-VL-2B-Instruct checkpoint, then convert the LLM part of the model to GGUF format using convert_hf_to_gguf.py:Convert the vision encoder to GGUF format with qwen2_vl_surgery.py:Build the llama-qwen2vl-cli in the same way you would build llama-llava-cli.Run the command: (It's recommended to resize the image to a resolution below 640x640, so it won't take"
  }
]