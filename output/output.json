[
  {
    "title": "Starship Flight 7 (spacex.com)",
    "points": 252,
    "submitter": "chinathrow",
    "submit_time": "2025-01-16T21:25:24 1737062724",
    "num_comments": 216,
    "comments_url": "https://news.ycombinator.com/item?id=42731091",
    "comments": [
      "Looks like second stage broke up over Caribbean, videos of the debris (as seen from ground):https://x.com/deankolson87/status/1880026759133032662?t=HdHF...https://x.com/realcamtem/status/1880026604472266800https://x.com/adavenport354/status/1880026262254809115Moment of the breakup:https://www.instagram.com/reel/DE52_hVSeQz/\n \nreply",
      "What a strangely beautiful sight. While I was excited to see ship land, I'm also happy I get to see videos of this!\n \nreply",
      "As long as the debris has no effect wherever it lands, I agree with you\n \nreply",
      "A lot of flights seem to be diverting to avoid it...https://bsky.app/profile/flightradar24.com/post/3lfvhpgmqqc2...\n \nreply",
      "Does SpaceX bother with NOTAM for its launches?https://en.wikipedia.org/wiki/NOTAMIt seems like the flights should have been planned around it so no diversion would be needed.\n \nreply",
      "They do but its not clear to me whether the area where it broke up was actually included in the original NOTAM. The NOTMAR definitely does not according to the graphic shown on the NASASpaceflight stream. They are still live so I can't link a time code but something like -4:56 in this stream as of posting: https://www.youtube.com/watch?v=3nM3vGdanpw\n \nreply",
      "They do issue debris warnings, the area is in the Indian Ocean and caused Qantas to delay flights to avoid it: https://www.abc.net.au/news/2025-01-17/spacex-launch-to-go-a...\n \nreply",
      "Understandable, but an over reaction. Any debris not burning up is falling down after minutes.\n \nreply",
      "Would you bet hundreds of lives and millions of dollars on that?\n \nreply",
      "Yes. Space debris at orbiting speeds doesn't fall straight down, it's simple physics. If anything, planes much further downrange should be diverted, not right under the re-entry point.EDIT: it's crazy how much HN is falling for social media engagement bait.\n \nreply"
    ],
    "link": "https://www.spacex.com/launches/mission/?missionId=starship-flight-7?submit",
    "first_paragraph": "\n        On its flight to the International Space Station, Dragon executes a series of burns that position the vehicle\n        progressively closer to the station before it performs final docking maneuvers, followed by pressurization of\n        the vestibule, hatch opening, and crew ingress.\n      \n        On its flight to the International Space Station, Dragon executed a series of burns that positioned the vehicle\n        progressively closer to the station before it performed final docking maneuvers, followed by pressurization of\n        the vestibule, hatch opening, and crew ingress.\n      \n        On its flight to the International Space Station, Dragon executes a series of burns that position the vehicle\n        progressively closer to the station before it performs final docking maneuvers, followed by pressurization of\n        the vestibule, hatch opening, and crew ingress.\n      \n        On its flight to the International Space Station, Dragon executed a series of burns that po"
  },
  {
    "title": "No Calls (keygen.sh)",
    "points": 1009,
    "submitter": "ezekg",
    "submit_time": "2025-01-16T14:17:31 1737037051",
    "num_comments": 348,
    "comments_url": "https://news.ycombinator.com/item?id=42725385",
    "comments": [
      "I'm a CTO who makes purchasing decisions.  There are numerous products I likely would have purchased, but I either find a substitute or just go without because I won't play the stupid \"let's get on a call\" game.If your website doesn't give me enough information to:1.  Know enough about your product to know that it will (generally speaking) meet my needs/requirements.2.  Know that the pricing is within the ballpark of reasonable given what your product does.Then I will move on (unless I'm really desparate, which I assure you is rarely the case).  I've rolled-my-own solution more than once as well when there were no other good competitors.That's not to say that calls never work or don't have a place, because they definitely do.  The key to using the call successfully (with me at least) is to use the call to get into true details about my needs, after I know that you're at least in the ballpark.  Additionally, the call should be done efficiently.  We don't need a 15 minute introduction and overview about you.  We don't need a bunch of small talk about weather or sports.  2 minutes of that is ok, or when waiting for additional people to join the call, but beyond that I have things to do.I know what my needs are.  I understand you need some context on my company and needs in order to push useful information forward, and I also understand that many potential customers will not take the lead in asking questions and providing that context, but the sooner you take the temperature and adjust, the better.  Also, you can get pretty far as a salesperson if you just spend 5 minutes looking at our website before the call!  Then you don't have to ask basic questions about what we do.  If you're willing to invest in the time to get on a call, then it's worth a few minutes of time before-hand to look at our website.\n \nreply",
      "Oh I might add another huge thing:  Have a way to justify/explain your pricing and how you came to that number.  When you have to \"learn about my company\" in order to give me pricing info, I know you're just making the price up based on what you think I can pay.  That's going to backfire on you because after you send me pricing, I'm going to ask you how you arrived at those numbers.  Is it by vCPU?  by vRAM?  by number of instances?  by number of API calls per month?  by number of employees?  by number of \"seats\"?  If you don't have some objective way of determining the price you want to charge me, you're going to feel really stupid and embarrassed when I drill into the details.\n \nreply",
      ">you're just making the price up based on what you think I can payIt should be based on the email address used.  If, for example, your email ends in @google.com, you get charged more.  If it ends in @aol.com, then they take pity on you and you get a discount.My co-worker's grandfather owned a TV repair business.  The price was entirely based on the appearance of the person and had nothing to do with the actual problem.  This way rich people subsidize the repairs of poor people.\n \nreply",
      "> This way rich people subsidize the repairs of poor people.tbh I have no problem with this as long as the work was done well.\n \nreply",
      "More like the people who appear rich subsidize the repairs of the people who appear poor. Probably usually fairly accurate but it's amusing to think about the edge cases where the truly rich don't feel the need to dress wealthy anymore and get their TV repaired for cheap.\n \nreply",
      "One of the big benefits of wealth is that everything costs less. This is just an extension of that.\n \nreply",
      "Don't want to be a hater but the parent of my previous post was literally about charging more for rich people. That is the entire point of enterprise plans too.\n \nreply",
      "You know it might be also priced on \u201cthis guy feels like a pain to work with after the way he asks questions, let\u2019s put the price up\u201d. There is no way to objectively explain that without having person offended - so I am going to put a price I think will cover me dealing with BS questions or attitude of the customer and if he walks it is still a good deal for me.We might think that companies need every single sale - well no sometimes you want to fire a customer or not take one on.\n \nreply",
      "I'm confused by this, why would sales team know in detail the vRAM contribution to sales price, and how is it relevant to your purchase decision? I've never heard of enterprise/SAAS pricing to be based primarily using cost plus pricing.\n \nreply",
      "Some products (especially infrastructure) still bill based on (outdated and often irrelevant) core counts and memory count.  A few years ago I talked to a seller of a PDF library/toolkit who wanted to know my production and staging core count before they would quote me a price.  Explaining to them that it runs in a serverless function on-demand was fun, especially because they would say things like, \"well, what's your average?\"  I would often reply and say my average is defined by a function where you take the number of active users (which itself is highly elastic) and calculate for average runtime at 4 cores per user for approximately 50 ms per page (which page count is highly elastic too) and sum to get \"average core use per month\".  Needless to say it was like pushing a rope.More common now with SaaS seems to be employee count or some other poor proxy measurement for usage.  I love actual usage based billing, but some of the proxies people pick are ridiculous.  Like, if I have 5 seats or 500 employees, but 2 users spend 6 hours a day in the software and then 10 others maybe look at it once a quarter, paying the same for those is absurd and is not usage-based billing at all.\n \nreply"
    ],
    "link": "https://keygen.sh/blog/no-calls/",
    "first_paragraph": "Thursday, January 16th 2025Zeke Gabrielse, Founder of KeygenWhen I first started Keygen, I had this idea in my head that I could create a\ncompany where I never had to get on a sales call \u2014 or any call. Being an\nintrovert, I absolutely hated calls. They're not only awkward, but a 30 minute\ncall takes up hours of my headspace. I quickly learned that I didn't want to\ndo them, and so I decided that I wouldn't.I instituted a bonkers 'no calls' policy at work.(Even I thought I was being crazy.)However, as Keygen grew, I wanted to start to move more up-market. I wanted\nlarger customers, because the bigger a customer got, the less support they\ntypically needed, and of course, the more they paid me. And the bigger\nlogos help gain trust in a market.The first thing I tried was taking sales calls. I threw up a big 'book a call'\nbutton on my enterprise pricing page \u2014 but I obviously hated it.(What about for non-enterprise leads? Well, I can't justify a call for $49/mo,\nI'll tell you that much, espe"
  },
  {
    "title": "Solving the first 100 Project Euler problems using 100 languages (github.com/jaredkrinke)",
    "points": 71,
    "submitter": "todsacerdoti",
    "submit_time": "2025-01-16T21:59:38 1737064778",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=42731460",
    "comments": [
      "I solved the first 100 problems a long time ago before learning the best practices we learn as professional engineers.It\u2019s alien code now.\n \nreply",
      "Worth emphasising: Project Euler is a mathematical computation challenge, not a programming challenge like Advent of Code. The principal skill is algorithm design and as the problems progress you need to do more and more maths before you start coding.\n \nreply",
      "I find it rather annoying and irrespectful for Project Euler. They specifically ask not to publish solutions.Some other collection could be used for demonstration of skills and languages.\n \nreply",
      "It's ChatGPT answers anyway.\n \nreply",
      "I know they ask, but it's really easy to find solutions online.For what it's worth, when I was doing Project Euler myself, I never had a problem _not_ looking up solutions.\n \nreply",
      "Just because a bus stop already has a broken window does not make it ok to smash in another.\n \nreply",
      "The first 100 problems are fair game to publish, I can't find it on their website anymore, but it's generally accepted that this is the case. You can check out the project euler discord for more authoritative information on this\n \nreply",
      "From Project Euler homepage:> the rule about sharing solutions outside of Project Euler does not apply to the first one-hundred problems\n \nreply",
      "fwiw this only shows up if you're logged in (at least for me), which confused me for a bit\n \nreply",
      "Oops, you are right!\n \nreply"
    ],
    "link": "https://github.com/jaredkrinke/100-languages",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Solving the first 100 Project Euler problems using 100 different programming languages!\n      This repository tracked a misguided quest to write code in 100 different programming languages, specifically by solving the first 100 Project Euler problems, using a different--and, ideally, new to me--language for each problem.I started this journey in March of 2024 and (after taking several breaks) finally completed it in January of 2025. Was this a good use of my time? Well, learning new programming languages is always a good use of one's time, so yes, it was.\n        Solving the first 100 Project Euler problems using 100 different programming languages!\n      "
  },
  {
    "title": "Some Things to Expect in 2025 (lwn.net)",
    "points": 40,
    "submitter": "signa11",
    "submit_time": "2025-01-16T22:55:37 1737068137",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=42731962",
    "comments": [
      "> A major project will discover that it has merged a lot of AI-generated codeMy friend works at a well-known tech company in San Francisco. He was reviewing his junior team member's pull request. When asked what a chunk of code did, the team member matter-of-factly replied \"I don't know, chatgpt wrote that\"\n \nreply",
      "> A major project will discover that it has merged a lot of AI-generated code, a fact that may become evident when it becomes clear that the alleged author does not actually understand what the code does.Not to detract from this point, but I don\u2019t think I understand what half the code I have written does if it\u2019s been more than a month since I wrote it\u2026\n \nreply",
      "I am confident that you do understand it at time of writing.> We depend on our developers to contribute their own work and to stand behind it; large language models cannot do that. A project that discovers such code in its repository may face the unpleasant prospect of reverting significant changes.At time of writing and commit, I am certain you \"stand behind\" your code.  I think the author refers to the new script kiddies of the AI time.  Many do not understand what the AI spits out at time of copy/paste.\n \nreply",
      "Sounds a lot like bashing copy pasting from StackOverflow. So also like old argument \u201ckids these days\u201d.No reasonable company pipes stuff directly to prod you still have some code review an d QA. So doesn\u2019t matter if you copy from SO without understanding or LLM generates  code that you don\u2019t understand.Both are bad but still happen and world didn\u2019t crash.\n \nreply",
      "LLM can generate a larger chunk of code then you'll find on SO, so I think it's a larger issue to have LLM code then copy-pasted SO code.\n \nreply",
      "Ignoring all the points made, this was a very pleasant reading experience.Not ignoring the points made, I cannot put my finger on where LLMs land in 2025. I do not think any sort of AGI type of phenomenon will happen.\n \nreply",
      "Yes, it was a good read. As someone with no direct connection to Linux or open-source development, I was surprised to find myself reading to the end. And near the end I found this comment particularly wise:> The world as a whole does not appear to be headed in a peaceful direction; even if new conflicts do not spring up, the existing ones will be enough to affect the development community. Developers from out-of-favor parts of the world may, again, find themselves excluded, regardless of any personal culpability they may have for the evil actions of their governments or employers.\n \nreply",
      "> A major project will discover that it has merged a lot of AI-generated code, a fact that may become evident when it becomes clear that the alleged author does not actually understand what the code does.\"Oh Machine Spirit, I call to thee, let the God-Machine breathe half-life unto thy data flow and help me comprehend thy secrets.\"\n \nreply",
      "That's how ye' get yerself Tzeench'd\n \nreply"
    ],
    "link": "https://lwn.net/Articles/1003780/",
    "first_paragraph": "\nSubscribers to LWN.net made this article \u2014 and everything that\n       surrounds it \u2014 possible.  If you appreciate our content, please\n       buy a subscription and make the next\n       set of articles possible.\n\nThe extensible scheduling class (sched-ext) will be a game changer.\nAlready we have seen, in 2024, how the ability to load a CPU scheduler from\nuser space as a set of BPF programs has unleashed a great deal of\ncreativity; that was before sched-ext was part of a released kernel.  In\n2025, this feature will start showing up in more distributions, and more\npeople will be able to play with it.  The result will be a flood of new\nscheduling ideas, each of which can be quickly tested (and improved) on\nreal systems.  Some of those ideas will result in specialty schedulers\nincluded with focused distributions (systems for gaming, for example);\nothers, hopefully, will eventually find their way into the kernel's EEVDF\nscheduler.\n\nCode written in Rust will land in the kernel at an increasi"
  },
  {
    "title": "Learn Yjs Interactively (yjs.dev)",
    "points": 72,
    "submitter": "paulgb",
    "submit_time": "2025-01-16T22:13:22 1737065602",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=42731582",
    "comments": [
      "Hey HN! I'm the developer at Jamsocket who made this. In case you're not familiar with Yjs, it's a CRDT library for building collaborative and local-first apps.The thing is, if you're not used to working with distributed state there's definitely a learning curve. Even simple things like incrementing a counter \u2014 the \"hello world\" of JavaScript framework demos \u2014 get tricky when dealing with multiple clients. Worse, a lot of tutorials are just like \"install this library and text editor integration and boom you have an app\", which doesn't give you a good mental model for what's actually happening.So we made Learn Yjs! It's an interactive Yjs tutorial. I wanted it to be really intuitive for people just getting their feet wet with local-first development, so there are lots of explorable demos and coding exercises. The idea is to use interactive examples to build an understanding from the ground up.Hope you like it :)\n \nreply",
      "Woah! Beautifully done! I wonder if I will finally understand CRDT with this.\nWill go through it over the weekend.Congratulations!\n \nreply",
      "I used Yjs in a small side project a while ago. The client side was fairly easy to learn and use. On the server side though, I wanted to deploy a single binary, but examples in languages other than Node (e.g. for the Rust port Yrs) seemed close to nonexistent, so I ended up slightly adapting https://github.com/yjs/y-websocket (Node) with persistence based on LevelDB. That's suboptimal for me. I wonder if there are good resources for implementing the server-side with persistence in, say, Rust.Btw, IIRC existing resources for the client side are heavily focused on text editor integrations, so this is really helpful. I had to fumble a bit myself because I wasn't using a text editor integration.\n \nreply",
      "The interactive demos are beautiful! Is there a library you used to build it?\n \nreply",
      "Thank you! The site as a whole is built with Astro and the demos are \u201cislands of interactivity\u201d built with React.The demos are really running Yjs under the hood \u2014 each \u201cclient\u201d has its own document, and when the user clicks I set a timeout to simulate the latency and then manually merge the documents into each other. (There\u2019s a third document too for the timeline in the center.)Other than that, I\u2019m using dnd kit [1] for the drag and drop functionality in the todos demo, Motion [2] for the animations and CodeMirror [3] for the text editor.[1] https://dndkit.com[2] https://motion.dev[3] https://codemirror.net\n \nreply",
      "The banner image game is silly but fun\n \nreply",
      "is Yjs a platform like Convex? are they competing? would love a comparison page potentially! :pray:\n \nreply",
      "You can think of Yjs as a protocol for data synchronization. It gives you a way to describe a JSON-like data structure (i.e. nested lists and maps), and keep them in sync across multiple devices.Yjs itself doesn't provide a platform, but it's an open protocol so there there are service providers (like ourselves[1]) that offer Yjs backends as a service (other notable providers are TipTap/Hocusocus and Liveblocks).[1] https://jamsocket.com/y-sweet\n \nreply"
    ],
    "link": "https://learn.yjs.dev/",
    "first_paragraph": "Welcome to Learn Yjs \u2014 an interactive tutorial series on building realtime collaborative applications using the Yjs CRDT library.This very page is an example of a realtime collaborative application.\nEvery other cursor in the garden above is a real live person reading the page right now.\nClick one of the plants to change it for everyone else!Learn Yjs starts with the basics of Yjs, then covers techniques for handling state in distributed applications.\nWe\u2019ll talk about what a CRDT is, and why you\u2019d want to use one.\nWe\u2019ll get into some of the pitfalls that make collaborative applications difficult and show how you can avoid them.\nThere will be explorable demos and code exercises so you can get a feel for how Yjs really works.Here\u2019s an example of an explorable demo.\nEach box below represents a client \u2014 a separate computer running an app that uses Yjs.\nWhen you interact with either client, the changes are automatically synced to the other one.\nYou can control the latency with the slider on "
  },
  {
    "title": "Nepenthes is a tarpit to catch AI web crawlers (zadzmo.org)",
    "points": 413,
    "submitter": "blendergeek",
    "submit_time": "2025-01-16T13:57:43 1737035863",
    "num_comments": 149,
    "comments_url": "https://news.ycombinator.com/item?id=42725147",
    "comments": [
      "Haha, this would be an amazing way to test the ChatGPT crawler reflective DDOS vulnerability [1] I published last week.Basically a single HTTP Request to ChatGPT API can trigger 5000 HTTP requests by ChatGPT crawler to a website.The vulnerability is/was thoroughly ignored by OpenAI/Microsoft/BugCrowd but I really wonder what would happen when ChatGPT crawler interacts with this tarpit several times per second. As ChatGPT crawler is using various Azure IP ranges I actually think the tarpit would crash first.The vulnerability reporting experience with OpenAI / BugCrowd was really horrific. It's always difficult to get attention for DOS/DDOS vulnerabilities and companies always act like they are not a problem. But if their system goes dark and the CEO calls then suddenly they accept it as a security vulnerability.I spent a week trying to reach OpenAI/Microsoft to get this fixed, but I gave up and just published the writeup.I don't recommend you to exploit this vulnerability due to legal reasons.[1] https://github.com/bf/security-advisories/blob/main/2025-01-...\n \nreply",
      "I am not surprised that OpenAI is not interested if fixing this.\n \nreply",
      "Their security.txt email address replies and asks you to go on BugCrowd. \nBugCrowd staff is unwilling (or too incompetent) to run a bash curl command to reproduce the issue, while also refusing to forward it to OpenAI.The support@openai.com waits an hour before answering with ChatGPT answer.Issues raised on GitHub directly towards their engineers were not answered.Also Microsoft CERT & Azure security team do not reply or care respond to such things (maybe due to lack of demonstrated impact).\n \nreply",
      "why try this hard for a private company that doesn't employ you?\n \nreply",
      "Ego, curiosity, potential bug bounty & this was a low hanging fruit: I was just watching API request in Devtools while using ChatGPT. It took 10 minutes to spot it, and a week of trying to reach a human being. Iterating on the proof-of-concept code to increase potency is also a nice hobby.These kinds of vulnerabilities give you good idea if there could be more to find, and if their bug bounty program actually is worth interacting with.With this code smell I'm confident there's much more to find, and for a Microsoft company they're apparently not leveraging any of their security experts to monitor their traffic.\n \nreply",
      "Make it reflective, reflect it back onto an OpenAI API route.\n \nreply",
      "While others (and OP) give good reasons, beyond passion and interest, those I see are typically doing this without a bounty to a build public profile to establish reputation that helps with employment or building their devopssec consulting practices.Unlike clear cut security issues like RCEs, (D)DoS and social engineering few other classes of issues are hard to process for devopssec, it is a matter of product design, beyond the control of engineering.Say for example if you offer but do not require 2FA usage to users,  having access to known passwords for some usernames from  other leaks then with a rainbow table you can exploit poorly locked down accounts.Similarly many dev tools and data stores for ease of adoption of their cloud offerings may be open by default, i.e. no authentication, publicly available or are easy to misconfigure poorly that even a simple scan on shodan would show. On a philosophical level these security issues in product design perhaps, but no company would accept those as security vulnerabilities, thankfully this type of issues is reducing these days.When your inbox starts filling up with reporting items like this to improve their cred, you stop engaging because the product teams will not accept it and you cannot do anything about it, sooner or later devopsec teams tend to outsource initial filtering to bug bounty programs and they obviously do not a great job of responding especially when it is one of the grayer categories.\n \nreply",
      "Maybe it's wrecking a site they maintain or care about.\n \nreply",
      "Some people have passion.\n \nreply",
      "Nice find, I think one of my sites actually got recently hit by something like this. And yea, this kind of thing should be trivially preventable if they cared at all.\n \nreply"
    ],
    "link": "https://zadzmo.org/code/nepenthes/",
    "first_paragraph": "This is a tarpit intended to catch web crawlers. Specifically, it's targetting crawlers that scrape data\nfor LLM's - but really, like the plants it is named after, it'll eat just about anything that finds it's\nway inside.It works by generating an endless sequences of pages, each of which with dozens of links, that simply go\nback into a the tarpit. Pages are randomly generated, but in a deterministic way, causing them to appear\nto be flat files that never change. Intentional delay is added to prevent crawlers from bogging down your\nserver, in addition to wasting their time. Lastly, optional Markov-babble can be added to the pages, to\ngive the crawlers something to scrape up and train their LLMs on, hopefully accelerating model collapse.You can take a look at what this looks like, here. (Note: VERY slow page loads!)THIS IS DELIBERATELY MALICIOUS SOFTWARE INTENDED TO CAUSE HARMFUL ACTIVITY.\nDO NOT DEPLOY IF YOU AREN'T FULLY COMFORTABLE WITH WHAT YOU ARE DOING.LLM scrapers are relentless a"
  },
  {
    "title": "I ditched the algorithm for RSS (joeyehand.com)",
    "points": 425,
    "submitter": "DearNarwhal",
    "submit_time": "2025-01-16T12:18:32 1737029912",
    "num_comments": 178,
    "comments_url": "https://news.ycombinator.com/item?id=42724284",
    "comments": [
      "My buddy will soon offer an RSS reader. I will post it here.Yes, you can create an RSS feed from a Youtube Channel. \nYou can can create an RSS feed from Reddit.You can't to my best knowledge create an RSS feed anymore from TwitterNewsletter to RSS: https://kill-the-newsletter.com/More stuff:Blogs & RSS\nhttps://rssfeedasap.com/\nhttps://code.rosaelefanten.org/rssparser.lisp/dir?ci=tipThis one you have to pay. I am considering it. Some RSS feeds don't work on my TinyTinyRSS. I think cloudflare, like always, is killing it:https://politepol.com/en/pricesPS: If you have an idea for a RSS reader domain, please suggest.\n \nreply",
      "Rsshub can give you a RSS feed for Twitter but you have to give it a web session cookie which kinda freaks me out and probably violates the current TOS.\n \nreply",
      "I've always described (the old) Twitter as an RSS feed but for people, which I loved. Is there a way to recreate this without all the slop?\n \nreply",
      "Here are the instructions if you want to try RSSHub. You have to self-host it somewhere to really send in the authorization confighttps://docs.rsshub.app/routes/social-media#x-twitter\n \nreply",
      "I used RSS bridge but AFAIK is has not been working a long time.Maybe is does but looks complicated: https://rss-bridge.github.io/rss-bridge/Bridge_Specific/Twit...\n \nreply",
      "I'm building a web app which would extract blogs and their RSS feeds from all HN stories you've commented, upvoted or added to favorites \u2013 so that you could easily extract exactly the content you want. I plan on expanding it to handle content you've interacted from other social networks too.\n \nreply",
      "A good algorithm is a good thing. However what a good algorithm is for me is often different from what it is for those who maintain them. Outrage gets attention and sometimes it is needed, but there is a level of too much, and also a lot of outrage unfairly represents the issues and so it makes me mad even though if I understood the details I wouldn't be mad just concerned.I want an algorithm that surfaces things of interest to me, then says \"you have seen it all, go outside\" (with an option of if I'm confined to a hospital bed to go on).  Algorithm maintainers want me to keep scrolling for more ad dollars.\n \nreply",
      "You should read about OPML blogrolls [1], they are gaining traction in this space. Personally, I like the idea of manually exploring recommendations, so I built a browsable index [2]. But you can crawl the these as well and build all sorts of recommendations engines.[1] https://opml.org/blogroll.opml[2] https://alexsci.com/rss-blogroll-network/discover/feed-c550c...\n \nreply",
      "> you have seen it all, go outsideOr \"you've seen it all. Bored? Click here to let your friends know you're looking for something to do/see who else is bored\". Or \"Bored? X needs volunteers!\" Or some other positive suggestion to try to prevent a \"eh guess I'll doomscroll something else\" reaction.\n \nreply",
      "That would be such a killer feature. And it could find other friends in the area and that are also free. Not like Meta doesn't have some kind of model for all that data already probably.\n \nreply"
    ],
    "link": "https://joeyehand.com/blog/2025/01/15/i-ditched-the-algorithm-for-rssand-you-should-too/",
    "first_paragraph": ""
  },
  {
    "title": "Porting the GNAT Ada compiler to macOS/aarch64 (briancallahan.net)",
    "points": 49,
    "submitter": "ingve",
    "submit_time": "2025-01-13T10:14:06 1736763246",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=42681917",
    "comments": [
      "Already done here https://github.com/simonjwright/building-gcc-macos-native\nSimon's the man when it comes to GNAT on macOS.\n \nreply",
      "True though from what he has said early on in it being more of a hobby and liking building tools then I am sure he would welcome any help or input.\n \nreply"
    ],
    "link": "https://briancallahan.net/blog/20250112.html",
    "first_paragraph": "academic, developer, with an eye towards a brighter techno-social lifeAfter getting a port of GDC working on my new MacBook Pro, there are still two languages left in the GCC suite that I don't have: Ada and Go. Some searching around makes it seem pretty clear that Gccgo is not yet really on the table to macOS. But there should not be any reason we can't add Ada to our GCC suite, seeing as there is already support for macos/aarch64 in the repository. I wanted to get a macOS/aarch64 native Ada through the GNAT compiler in GCC.But try as I might, I could not find any precompiled packages for it. I guess part of the issue is that macOS/aarch64 support is not fully upstreamed into GCC propre; instead, Iain Sandoe has a GitHub repository that includes the necessary changes for full support. I used his gcc-14-branch repository to build GDC.So let's get to work.I don't have Rosetta on my machine. I know it takes all of two seconds to install it, but I also wanted an excuse to play around with"
  },
  {
    "title": "Five years of React Native at Shopify (shopify.engineering)",
    "points": 177,
    "submitter": "onnnon",
    "submit_time": "2025-01-13T22:08:51 1736806131",
    "num_comments": 89,
    "comments_url": "https://news.ycombinator.com/item?id=42690114",
    "comments": [
      "I agree with most of the other comments here, and it sounds like Shopify made sound tradeoffs for their business. I'm sure the people who use Shopify's apps are able to accomplish the tasks they need to.But as a user of computers and occasional native mobile app developer, hearing \"<500ms screen load times\" stated as a win is very disappointing. Having your app burn battery for half a second doing absolutely nothing is bad UX. That kind of latency does have a meaningful effect on productivity for a heavy user.Besides that, having done a serious evaluation of whether to migrate a pair of native apps supported by multi-person engineering teams to RN, I think this is a very level-headed take on how to make such a migration work in practice. If you're going to take this path, this is the way to do it. I just hope that people choose targets closer to 100ms.\n \nreply",
      "I would read the <500ms screen loads as follows:When the user clicks a button, we start a server round-trip and fetch the data and do client-side parsing, layout, formatting and rendering and then less than 500ms later, the user can see the result on his/her screen.With a worst-case ping of 200ms for a round-trip, that leaves about 200ms for DB queries and then 100ms for the GUI rendering, which is roughly what you'd expect.\n \nreply",
      "Since the post is about the benefits of react, I'm sure if requests were involved they would mention it.Also, even if it was involved, 200ms for round-trip and DB queries is complete bonkers. Most round-trips don't take more than 100ms, and if you're taking 200ms for a DB query on an app with millions of users, you're screwed. Most queries should take max 20-30ms, with some outliers in places where optimization is hard taking up to 80ms.\n \nreply",
      "I do not understand this thinking at all, a parsed response into whatever rendering engine, even if extremely fast is going to be a large percentage of this 500ms page load. Diminishing it with magical thinking about pure database queries under load with no understanding of the complexity of Shopify is quite frankly ridiculous, next up you\u2019ll be telling everyone to roll there own file sharing with rsync or something\u2026\n \nreply",
      "If you are good those numbers are an order of magnitude off. In truth it is probably mostly auth or something. If you simply avoid json you can radically attack these things fast.RTT to nearest major metro DC should be up to 20ms (where I am it is less than half that), your DB calls should not be anything like 200ms (and in the event they are you need to show something else first), and 10-20ms is what you should assume for rendering budget of something very big. 60hz means 16ms per frame after all.\n \nreply",
      "What percentile? Topics like these don't talk about the 5G connected iphone 16 pro max, but have to include low-end phones with old OS versions and bad connectivity (e.g. try the same network connectivity in the London metro, where often there is no receiption whatsoever).As you reach for higher percentiles, RTT and such start growing very fast.Edit: other commenter mentioned 75% as percentile.\n \nreply",
      "> What percentile?There's no argument that starts this way which doesn't end either with \"support working offline\", or defining when you consider that a user has stepped out of bounds with respect to acceptable parameters, which then raises the question what do you do in that event?If all you're trying to do is say 75% of users have a good experience, and in your territory 75% means a 150ms and that's too long then the network cannot be in your critical path, and you have to deal with it. If you're on a low end phone any I/O at all is going to kill you, including loading too much code, and needs to be out of the way.If you can tell the UX is going to be bad you will need to abort and tell them that, though they really will not like it, it's often better to prevent such users ever getting your app in the first place.I come from mobile games, and supported titles with tens of millions of players around the world back in the early 4G era. All I can tell you is not once did mobile ping become a concern - in fact those networks are shockingly good compared to wifi.\n \nreply",
      "Independent of connectivity, UI rendering should be well under the device refresh rate. Consider the overhead of a modern video game that runs 60fps without a hiccup. It\u2019s ludicrous that a CRUD app which usually only populates some text fields and maybe a small image or two can\u2019t do the same\n \nreply",
      "> RTT to nearest major metro DC should be up to 20ms (where I am it is less than half that)over a mobile network?  My best rtt to azure or aws over tmobile or verizon is 113ms vs 13ms over my fiber conection.\n \nreply",
      "With times like that you'd be better off with Starlink!I'm not joking:\nhttps://www.pcmag.com/news/is-starlink-good-for-gaming-we-pu...Are you doing the 113 test from the actual device, or something tethered to it? For example, you don't want a bluetooth stack in the middle.\n \nreply"
    ],
    "link": "https://shopify.engineering/five-years-of-react-native-at-shopify",
    "first_paragraph": ""
  },
  {
    "title": "Uncovering Real GPU NoC Characteristics: Implications on Interconnect Arch. (ubc.ca)",
    "points": 7,
    "submitter": "matt_d",
    "submit_time": "2025-01-13T21:41:12 1736804472",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://people.ece.ubc.ca/aamodt/publications/papers/realgpu-noc.micro2024.pdf",
    "first_paragraph": ""
  },
  {
    "title": "Test-driven development with an LLM for fun and profit (yfzhou.fyi)",
    "points": 136,
    "submitter": "crazylogger",
    "submit_time": "2025-01-16T15:30:19 1737041419",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=42726584",
    "comments": [
      "One trend I've noticed, framed as a logical deduction:1. Coding assistants based on o1 and Sonnet are pretty great at coding with <50k context, but degrade rapidly beyond that.2. Coding agents do massively better when they have a test-driven reward signal.3. If a problem can be framed in a way that a coding agent can solve, that speeds up development at least 10x from the base case of human + assistant.4. From (1)-(3), if you can get all the necessary context into 50k tokens and measure progress via tests, you can speed up development by 10x.5. Therefore all new development should be microservices written from scratch and interacting via cleanly defined APIs.Sure enough, I see HN projects evolving in that direction.\n \nreply",
      "> 3. If a problem can be framed in a way that a coding agent can solve...This reminds me of the South Park underwear gnomes. You picked a tool and set an expectation, then just kind of hand wave over the hard part in the middle, as though framing problems \"in a way coding agents can solve\" is itself a well-understood or bounded problem.Does it sometimes take 50x effort to understand a problem and the agent well enough to get that done? Are there classes of problems where it can't be done? Are either of those concerns something you can recognize before they impact you? At commercial quality, is it an accessible skill for inexperienced people or do you need a mastery of coding, the problem domain, or the coding agent to be able to rely on it? Can teams recruit people who can reliable achieve any of this? How expensive is that talent? etc\n \nreply",
      ">as though framing problems \"in a way coding agents can solve\" is itself a well-understood or bounded problem.It's not, but if you can A) make it cheap to try out different types of framings - not all of them have to work and B) automate everything else then the labor intensity of programming decreases drastically.>At commercial quality, is it an accessible skill for inexperienced peopleI'd expect the opposite, it would be an extremely inaccessible skill requiring high skill and high pay. But, if 2 people can deliver as much as 15 people at a higher quality and they're paid triple, it's still way cheaper overall.I would still expect somebody following this development pattern to routinely discover a problem the LLM can't deal with and have to dive under the hood to fix it - digging down below multiple levels of abstraction. This would be Hard with a capital H.\n \nreply",
      "We've had failed projects since long before LLMs. I think there is a tendency for people to gloss over this (3.) regardless, but working with an LLM it tends to become obvious much more quickly, without investing tens/hundreds of person-hours. I know it's not perfect, but I find a lot of the things people complain about would've been a problem either way - especially when people think they are going to go from 'hello world' to SaaS-billionaire in an hour.I think mastery of the problem domain is still important, and until we have effectively infinite context windows (that work perfectly), you will need to understand how and when to refactor to maximize quality and relevance of data in context.\n \nreply",
      "well according to xianshou's profile they work in finance so it makes sense to me that they would gloss over the hard part of programming when describing how AI is going to improve it\n \nreply",
      "Working in one domain does not preclude knowledge of others. I work in cybersec but spent my first working decade in construction estimation for institutional builds. I can talk confidently about firewalls or the hospital you want to build.No need to make assumptions based on a one-line hacker news profile.\n \nreply",
      "> 5. Therefore all new development should be microservices written from scratch and interacting via cleanly defined APIs.Not necessarily. You can get the same benefits you described in (1)-(3) by using clearly defined modules in your codebase, they don't need to be separate microservices.\n \nreply",
      "Agreed. If the microservice does not provide any value from being isolated, it is just a function call with extra steps.\n \nreply",
      "I think the argument is that the extra value provided is a small enough context window for working with an LLM.  Although I'd suggest making it a library if one can manage, that gives you the desired context reduction bounded by interfaces without taking on the complexities of adding an additional microservice.I imagine throwing a test at an LLM and saying:> hold the component under test constant (as well as the test itself), and walk the versions of the library until you can tell me where they're compatible and where they break.If you tried to do that with a git bisect and everything in the same codebase, you'd end up varying all three (test, component, library) which is worse science than holding two constant and varying the third would be.\n \nreply",
      "> I think the argument is that the extra value provided is a small enough context window for working with an LLM.I'm not sure moving something that could work as function to a microservice would save much context.  If anything, I think you are adding more context, since you would need to talk about the endpoint and having it route to the function that does what you need. When it is all over, you need to describe what the input and output is.\n \nreply"
    ],
    "link": "https://blog.yfzhou.fyi/posts/tdd-llm/",
    "first_paragraph": ""
  },
  {
    "title": "MuJoco Playground (mujoco.org)",
    "points": 25,
    "submitter": "kzakka",
    "submit_time": "2025-01-16T22:08:46 1737065326",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=42731527",
    "comments": [
      "An open-source library for GPU-accelerated robot learning and sim-to-real transfer.\n \nreply",
      "This is great - I haven't looked at the documentation yet but do you support soft body simulation?\n \nreply",
      "MuJoCo does support soft bodies:https://mujoco.readthedocs.io/en/latest/modeling.html#deform...The GPU part of MuJoCo only supports soft collisions rather than fully deformable bodies.\n \nreply"
    ],
    "link": "https://playground.mujoco.org/",
    "first_paragraph": "\n        We introduce MuJoCo Playground, a fully open-source framework for robot\n        learning built with MJX, with the express goal of streamlining simulation,\n        training, and sim-to-real transfer onto robots.\n        With a simple installation process (pip install playground), researchers can train policies in minutes on a single GPU.\n        Playground supports diverse robotic platforms, including quadrupeds,\n        humanoids, dexterous hands, and robotic arms, and enables zero-shot\n        sim-to-real transfer from both state and pixel inputs. This is achieved\n        through an integrated stack comprising a physics engine, batch renderer,\n        and training environments. MuJoCo Playground was a community effort involving multiple groups, and we hope it proves valuable to researchers and developers alike.\n      \n      All videos on this page are shown at 1x speed (real-time).\n    "
  },
  {
    "title": "2k-year-old wine and the uncanny immediacy of the past (resobscura.substack.com)",
    "points": 145,
    "submitter": "benbreen",
    "submit_time": "2025-01-15T21:35:41 1736976941",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=42717393",
    "comments": [
      "Nice article about artifacts that make the past more immediate, that allow us to connect our experiences to people hundreds or thousands of years ago.My favorite example is the writings of Onfim, who was a little boy in the 1200s in present day Russia whose scribbling and homework were exquisitely preserved on birch bark fragments. It\u2019s so immediately recognizable as a little boy\u2019s endearing doodles about knights and imaginary beasts, yet its 800 years old.https://en.m.wikipedia.org/wiki/Onfim\n \nreply",
      "Similarly, when I read Meditations by Marcus Aurelius I was struck by how normal everything seemed. While he was an Emperor the everyday banality of what he talked about going through 2,000 years ago was amazing.Humans really haven't changed that much at all.\n \nreply",
      "> Humans really haven't changed that much at all.\n\nI guess you are quoting Woland from The Master and Margarita [1], the words he said in a show at the Variety theater.[1] https://en.wikipedia.org/wiki/The_Master_and_MargaritaWoland is the Satan in the novel. What he said has a deeper meaning, but superficial one is most probably wrong.\n \nreply",
      "I think they\u2019re just saying that humans haven\u2019t really changed much at all, if I had to guess they weren\u2019t referencing any one quote. The only thing that\u2019s really changed is the tools we can use, but we\u2019ve made little (some would say backwards compared to certain reference points) progress in why and what we use them for.\n \nreply",
      "I\u2019ve had similar feeling when realizing that the bells that we sometimes hear in old cities of Europe are exact same bells producing exact same sound as 1000 years ago\n \nreply",
      "Wow, haven't seen this before, thank you. Amazing that the writing can still be read by a modern reader (that said, really helps to know what it's supposed to say though). The note I found most relatable is the one with greetings to his classmate.\n \nreply",
      "Do some eyebrows convey emotion or coincidence? (near bottom of page)Edit: I'm also curious how much time (thousands of years) for there to be noticeable difference in the capability of the brain like abstract thinking. Language may be the real problem.\n \nreply",
      "I like the adorable animal shaped sippy cups for babies:https://www.nytimes.com/2019/09/25/science/prehistoric-baby-...\n \nreply",
      "I agree these objects (just like colourised photos) help bridge the distance to the past. But I've had the same experience purely with text. If you read Cicero's letters and diaries, and then just imagine him - suffering writer's block, wracked with anxiety and self-doubt, desperate for his friends to cheer him up - as a neurotic, terminally-online Twitter user, it fits perfectly and breathes life into his every word.\n \nreply",
      "The bakers stamp on the bread is interesting brandinghttps://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_...\n \nreply"
    ],
    "link": "https://resobscura.substack.com/p/2000-year-old-wine-and-the-uncanny",
    "first_paragraph": ""
  },
  {
    "title": "Continue (YC S23) Is Hiring a Software Engineer in San Francisco (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-01-16T21:00:25 1737061225",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/continue/jobs/smcxRnM-software-engineer",
    "first_paragraph": "The leading open-source AI code assistantContinue is seeking an outstanding software engineer to help us build state-of-the-art autocomplete and codebase retrieval, who thinks rigorously and pays attention to the smallest details. In this role, you will work on fundamental, but highly open-ended problems where deliberate measurement, rapid experimentation, and empathy for users push forward the product.About youPlease keep in mind that we are describing the background that we imagine would best fit the role. If you don\u2019t meet all the requirements, but you are confident that you are up for the task, we absolutely want to get to know you!What you will doWe\u2019re a startup, so you\u2019ll have to be ready to do whatever is required to accomplish our mission. However, you can definitely expect to:We believe there is an opportunity to create a future where developers are amplified, not automated. This is why we are building the leading open-source AI code assistant and commercializing an enterprise"
  },
  {
    "title": "Physically Based Rendering: From Theory to Implementation (pbr-book.org)",
    "points": 113,
    "submitter": "ahamez",
    "submit_time": "2025-01-13T14:28:33 1736778513",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=42683731",
    "comments": [
      "Every time I see stuff like this it makes me think about optical design software.There are applications (Zemax, for example) that are used to design optical systems (lens arrangements for cameras, etc). These applications are eye-wateringly expensive-- like similar in pricing to top-class EDA software licenses.With the abundance GPU's and modern UI's, I wonder how much work would be involved for someone to make optical design software that blows away the old tools. It would be ray-tracing, but with interesting complications like accounting for polarization, diffraction, scattering, fluorescence, media effects beyond refraction like like birefringence and stuff like Kerr and Pockels, etc.\n \nreply",
      "This, very much this!I do research in a subfield of optics called nonimaging optics (optics for energy transfer, e.g. solar concentrators or lighting systems). We  typically use these optical design applications, and your observations are absolutely correct. Make some optical design software that uses GPUs for raytracing, reverse-mode autodiff for optimization, sprinkle in some other modern techniques you may blow these older tools out of the water.I am hoping to be able to get some projects going in this direction (feel free to reach out if anyone are interested).PS: I help organize an academic conference my subfield of optics. We run a design competition this year [1,2]. Would be super cool if someone submits a design that they made by drawing inspiration from modern computer graphics tools (maybe using Mitsuba 3, by one of the authors of this book?), instead of using our classical applications in the field.[1] https://news.ycombinator.com/item?id=42609892[2] https://nonimaging-conference.org/competition-2025/upload/\n \nreply",
      "> I am hoping to be able to get some projects going in this direction (feel free to reach out if anyone are interested).This does sound interesting! I\u2019ve just finished a Masters degree, also in non-imaging optics (in my case lidar systems). I have experience in raytracing for optical simulation, though not quite in the same sense as optical design software. How should I contact you to learn more?\n \nreply",
      "I've been working on something similar, although I'm more interested in replicating the effects of existing lenses than designing new ones: https://x.com/dearlensform/status/1858229457430962318PBRT 3rd edition actually has a great section on the topic but it's one of the parts that wasn't implemented for the GPU (by the authors, anyway): https://pbr-book.org/3ed-2018/Camera_Models/Realistic_Camera...\n \nreply",
      "You\u2019d be surprised! Everywhere I\u2019ve worked, academic or industry, typically writes their own simulation software. Sometimes it\u2019s entirely handwritten (i.e., end-to-end, preprocessing to simulation to evaluation), sometimes it\u2019ll leverage a pre-existing open source package. I imagine this will become more and more common if, for no other reason, you can\u2019t back-propagate an OpticStudio project and open source automatic differentiation packages are unbeatable.\n \nreply",
      "If you're interested in the equivalent of \"backprop through zemax\" there are a few projects going on to jointly optimize optical designs with the image processing, e.g. check out: https://vccimaging.org/Publications/Wang2022DiffOptics/\n \nreply",
      "I'd imagine there is fairly wide gap between having a simulation engine core and an useful engineering applicationFrom academic side, I've found the work of Steinberg in this area extremely impressive. They are pushing the frontier to include more wave-optical phenomenon in the rendering. E.g. https://ssteinberg.xyz/2023/03/27/rtplt/\n \nreply",
      "I once saw a youtube video of a guy who first modeled a pinhole camera in something like Blender3D and then went on to design and simulate an entire SLR camera.\n \nreply",
      "https://youtu.be/YE9rEQAGpLw\n \nreply",
      "Thanks, but it was a different video.I remember he had a lot of problems with the pinhole camera because the small size of the pinhole meant that rays had trouble going into the box, so to speak, and thus he needed an insane amount of rays.\n \nreply"
    ],
    "link": "https://pbr-book.org",
    "first_paragraph": ""
  },
  {
    "title": "Diffusion training from scratch on a micro-budget (github.com/sonyresearch)",
    "points": 61,
    "submitter": "lnyan",
    "submit_time": "2025-01-13T09:55:24 1736762124",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=42681793",
    "comments": [
      "The differently styled images of \"astronaut riding a horse\" are great, but that has been a go-to example for image generation models for a while now. The introduction says that they train on 37 million real and synthetic images. Are astronauts riding horses now represented in the training data more than would have been possible 5 years ago?If it's possible to get good, generalizable results from such (relatively) small data sets, I'd like to see what this approach can do if trained exclusively on non-synthetic permissively licensed inputs. It might be possible to make a good \"free of any possible future legal challenges\" image generator just from public domain content.\n \nreply",
      ">  Are astronauts riding horses now represented in the training data more than would have been possible 5 years ago?\n\nYes.Though I'm a bit confused why this became the goto. If I remember correctly the claim was about it being \"out of distribution\" but I have high confidence that astronauts riding horses are within the training dataset prior to DALL-E. The big reason everyone should believe this is because astronauts have always been compared to cowboys. And... what do we stereotypically associate with cowboys?The second reason, is because it is the main poster for the 2006 movie The Astronaut Farmer:\nhttps://en.wikipedia.org/wiki/The_Astronaut_FarmerBut here's some other ones I found that are timestamped. It's kinda hard to find random digital art that is timestamped. Looks like even shutterstock doesn't... And places like deviantart don't have great search. Hell... even Google will just flat out ignore advanced search terms (the fuck is even the point of having them?). The term is so littered now that this makes search difficult, but I found two relatively quickly.2014: https://www.behance.net/gallery/18695387/Space-Cowboy#2016: https://drawception.com/game/DZgKzhbrhq/badass-space-cowboy-...But even if the samples did not exist, I do not think this represents a significantly out of distribution, if at all, image. Are we in doubt that there's images like astronauts riding rockets? I think certainly there exists \"astronaut riding horse\" along the interpolation between \"person riding horse\" and \"astronaut riding <insert any term>\". Mind you, generating samples in distribution but not in training (or test) is still a great feat and impressive accomplishment. This should in no way be underplayed at all! But there is a difference in claiming out of distribution.  > I'd like to see what this approach can do if trained exclusively on non-synthetic permissively licensed inputs\n\nOne minor point. The term \"synthetically generated\" is a bit ambiguous. It may include digital art. It does not necessarily mean generated by a machine learning generative model. TBH, I find the ambiguity frustrating as there is some important distinctions.\n \nreply",
      "It wasn't because it was \"out of distribution\" (although that's a reasonable assumption and it is at least _somewhat_ out of distribution, given the scarcity of your examples).Like the avocado armchair before it, the real reason was simply that it \"looked cool\". It scratched some particular itch for people.For me, indeed it's correlated with \"imagination\". An avocado armchair output had a particular blending of concepts that matched (in my mind) the way humans blend concepts. With the \"astronaut riding a horse on the moon\", you are hitting a little of that; but also you're effectively addressing criticism about text-to-image models with a prompt that serves as an evaluation for a couple of things:1.) t2i is bad at people (astronaut)2.) t2i struggles with animal legs (horse)3.) t2i struggles with costumes, commonly putting the spacesuit on both the astronaut _and_ the horse - and mangling that in the process (and usually ruining any sense of good artistic aesthetics).4.) t2i commonly gets confused with the moon specifically, frequently creating a moon _landscape_ but also doing something silly like putting \"another\" moon in the \"night sky\" as well.There are probably other things. And of course this is subjective. But as someone who followed these things as they happened, which was I believe the release of DALL-E 2 and the first Stable Diffusion models, this is why I thought it was a good evaluation (at the time).edit: I truly despise HN comment's formatting rules.\n \nreply",
      "> although that's a reasonable assumption and it is at least _somewhat_ out of distribution, given the scarcity of your examples\n\nThis isn't what \"out of distribution\" means. There can be ZERO images and it wouldn't mean something is OOD. OOD means not within the underlying distribution. That's why I made the whole point about interpolation.Is it scarce? Hard to tell. But I wouldn't make that assumption based on my examples. Search is poisoned now.I think there's a lot of things that are assumed scarce which are not. There's entire datasets that are spoiled because people haven't bothered to check.  > edit: I truly despise HN comment's formatting rules.\n\n  Add 2 spaces\n  on a new\n  line and \n\n  you can do whatever you want because it is a quote block\n  That's also why I quote people this way\n \nreply",
      ">  This isn't what \"out of distribution\" means. There can be ZERO images and it wouldn't mean something is OOD. OOD means not within the underlying distribution. That's why I made the whole point about interpolation.Sure. I'll concede to that, although it's a bit pedantic.> Is it scarce? Hard to tell. But I wouldn't make that assumption based on my examples. Search is poisoned now.I was more referring to why it originally was used, not why it would still be used. In any case, I maintain that it was _not_ used for being OOD, which I mentioned in my first comment.>  Add 2 spaces\n>  on a new\n>  line and \n>  ...Yeah, I still hate it. Sorry. Give me markdown support and I'll be happy.edit: I'll leave my mistake here as an example of why it's non-intuitive.\n \nreply",
      ">The estimated training time for the end-to-end model on an 8\u00d7H100 machine is 2.6 days.That's a $250,000 machine for the micro budget. Or if you don't want to do it locally ~$2,000 to do it on someone else's machine for the one model.\n \nreply",
      "From the abstract  Finally, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with only $1,890 economical cost and achieve a 12.7 FID in zero-shot generation on the COCO dataset. Notably, our model achieves competitive FID and high-quality generations while incurring 118\u00d7 lower cost than stable diffusion models and 14\u00d7 lower cost than the current state-of-the-art approach that costs $28,400.\n\nFigure 1  Qualitative evaluation of the image generation capabilities of our model (512\u00d7512 image resolution). Our model is trained in 2.6 days on a single 8\u00d7H100 machine (amounting to only $1,890 in GPU cost) without any proprietary or billion image dataset. \n\nEnd of intro under the key contributions bullet points  - Using a micro-budget of only $1,890, we train a 1.16 billion parameter sparse diffusion transformer on 37M images and a 75% masking ratio that achieves a 12.7 FID in zero-shot generation on the COCO dataset. The wall-clock time of our training is only 2.6 days on a single 8\u00d7H100 GPU machine, 14\u00d7 lower than the current state-of-the-art approach that would take 37.6 training days ($28,400 GPU cost).\n\nI'm just saying, the authors are not trying to hide this point. They are making it abundantly clear.I should also mention that this is the most straightforward way to discuss pricing. It is going to be much more difficult if they do comparisons including the costs of the machines as then there needs to be an amortization cost baked in and that's going to have to include costs of electricity, supporting hardware, networking, how long the hardware is used for, at what percentage utility the hardware is, costs of employees to maintain, and all that fun stuff. Which... you can estimate by... GPU rental costs... Since they are in fact baking those numbers in. They explain their numbers in the appendix under Table 5. It is estimated at $3.75/H100/hr.Btw, they also state a conversion to A100s\n \nreply",
      "I've been collecting papers on straining models on small numbers of GPU's. What I look for is (a) type of GPU, (b) how many, and (c) how long it ran. I can quickly establish a minimum cost from that.I say minimum because there's pre-processing data, setting up the machine configuration, trial runs on small data to make sure it's working, repeats during the main run if failures happened, and any time to load or offload data (eg checkpoints) from the GPU instance. So, the numbers in the papers are a nice minimum rather than the actual cost of a replication which is highly specific to one's circumstances.\n \nreply",
      "So, a kilo-budget.\n \nreply",
      "You can do it on one single GPU but you would need to use gradient accumulation and the training would probably last 1-2 months on a consumer GPU.\n \nreply"
    ],
    "link": "https://github.com/SonyResearch/micro_diffusion",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Official repository for our work on micro-budget training of large-scale diffusion models.\n      This repository provides a minimalistic implementation of our approach to training large-scale diffusion models from scratch on an extremely low budget. In particular, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with a total cost of only $1,890 and achieve an FID of 12.7 in zero-shot generation on the COCO dataset. Please find further details in the official paper: https://arxiv.org/abs/2407.15811.The current codebase enables the reproduction of our models, as it provides both the training code and the dataset code used in the paper. We also provide pre-trained model checkpoints for off-the-shelf generation.Clone the repository and install micro_diffusion as a Python p"
  },
  {
    "title": "Firebase bill is usually $50, but I was surprised to see a $70k bill in one day (twitter.com/tamarajtran)",
    "points": 20,
    "submitter": "plainOldText",
    "submit_time": "2025-01-17T00:30:27 1737073827",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=42732714",
    "comments": [
      "Firebase is pretty simple to use, and possibly because of this, I've seen quite a few terrible implementations. I think it attracts people who only know app development, where security, scalability, authentication, etc, are not really a concern because you have exactly one user on one device.It's easy to accidentally make a bunch of information world-readable, and if a malicious user gets hold of the right details they can simply read all your content out, or even start writing bad content in, racking up a bill in the process.Whether this is what happened here or not I have no idea. And I don't think this is even really a problem with Firebase, just an indicator of the sort of developer who ends up using Firebase and their background.\n \nreply",
      "Does Firebase allow you to set up a billing limit or is it one of those exciting cloud services where no matter what you do  you're always one mistake away from losing your house?\n \nreply",
      "Apparently she had a $20 limit, but, if my calculations are correct, $70k is more than that, which seems odd.\n \nreply",
      "Somebody else pointed out that it's likely just an alert, not a hard limit, which checks out given Firebase documentation (https://firebase.google.com/docs/projects/billing/avoid-surp...), which has no mention of hard limits and explicitly warns you that an alert won't stop anything.\n \nreply",
      "Ah, oops...\n \nreply",
      "They have a documentation page titled \u201cAvoid surprise bills\u201d[1] but I imagine it\u2019s easy for some developers to skip over that.[1]: https://firebase.google.com/docs/projects/billing/avoid-surp...\n \nreply",
      "> A budget alert sends an email whenever your project's spending level hits a threshold that you've set. Budget alerts do NOT turn off services or usage for your app.It's not an automatic hard-stop so you could still screw yourself over pretty badly with runaway spending.\n \nreply",
      "https://firebase.google.com/docs/projects/billing/avoid-surp...> We don't turn off services and usage because although you might have a bug in your app causing an increase in spend, you might just be experiencing unexpected positive growth of your app. You don't want your app to shut down unexpectedly when you need it to work the most.Frankly, I don't see anything on that page that would actually prevent a surprise bill.\n \nreply",
      "eg. OpenAI enforces strict limits until you spend a significant amount of money.\n \nreply"
    ],
    "link": "https://twitter.com/tamarajtran/status/1880036092906467841",
    "first_paragraph": ""
  },
  {
    "title": "David Lynch has died (variety.com)",
    "points": 871,
    "submitter": "wut42",
    "submit_time": "2025-01-16T18:20:49 1737051649",
    "num_comments": 287,
    "comments_url": "https://news.ycombinator.com/item?id=42728862",
    "comments": [
      "\"My childhood was elegant homes, tree-lined streets, the milkman, building backyard forts, droning airplanes, blue skies, picket fences, green grass, cherry trees. Middle America as it's supposed to be. But on the cherry tree there's this pitch oozing out \u2013 some black, some yellow, and millions of red ants crawling all over it. I discovered that if one looks a little closer at this beautiful world, there are always red ants underneath. Because I grew up in a perfect world, other things were a contrast.\"David Lynch\n \nreply",
      "\"What a heavy load Einstein must've had. Fuckin' morons, everywhere.\"David Lynch\n \nreply",
      "I have never seen a single one of his movies but I love watching interviews with him, he had an amazing presence and so much energy.\n \nreply",
      "I'm also ashamed to say I've also never seen any of his movies and TV series but this still hits hard because of his influence on some my most cherished fictional properties. These are Alan Wake/Control, Silent Hill 1&2, Returnal and Disco Elysium.Actually, his influence on how surrealist fiction is presented throughout all media cannot be understated. I was surprised to read even the original Zelda has him as an influence. Majora's Mask does feel particularly Lynchian.It would not surprise me if the Souls games and at least the later Berserks (late 90s/early 2000s forward) were either directly or 1-step indirectly influenced by Lynch.\n \nreply",
      "I think it was less the original Zelda than it was Link's Awakening that had the Lynch influence, specifically influence by Twin Peakshttps://www.nintendolife.com/news/2019/12/feature_how_david_...\n \nreply",
      "An (un)obvious connection between Eraserhead and Bloodborne (spoiler!):https://www.reddit.com/r/bloodborne/comments/xgu21c/eraserhe...\n \nreply",
      "I'm gonna say start with Blue Velvet. It still has the backbone of a classical noir, but it is completely run through with the character of his work. Mulholland Drive reflects the apex of his vision and talents, but there's a learning curve to appreciating it.\n \nreply",
      "Other than the 1980's Dune movie he directed, I think it was either Lost Highway or Mulholland Drive that made me want to know more about David Lynch.I had to watch Mulholland Drive at least 5 times to get a sense of what it's even about, and I think I must have been the audience for which he made that film, if it wasn't indeed just art to make himself happy (which is the BEST kind).Anyway, it kind of endears another person to you when you connect with their work. So this one hit kind of hard.I lost a fellow weirdo, and he'll be missed!\n \nreply",
      "Mulholland Drive was my first Lynch movie and led me to watch pretty much everything else he released.  I'd still start with Mulholland Drive if I started over again I think.\n \nreply",
      "Wild at heart. Very approachable, but gory and brutal. The angst seep trough\n \nreply"
    ],
    "link": "https://variety.com/2025/film/news/david-lynch-dead-director-blue-velvet-twin-peaks-1236276106/",
    "first_paragraph": "\n\n\t\t\t\t\t\t\t\tBy \n\n\tChris Morris\nMusic Reporter\n\tDirector-writer David Lynch, who radicalized American film with with a dark, surrealistic artistic vision in films like \u201cBlue Velvet\u201d and \u201cMulholland Drive\u201d and network television with \u201cTwin Peaks,\u201d has died. He was 78.\n\tLynch revealed in 2024 that he had been diagnosed with emphysema after a lifetime of smoking, and would likely not be able to leave his house to direct any longer. His family announced his death in a Facebook post, writing, \u201cThere\u2019s a big hole in the world now that he\u2019s no longer with us. But, as he would say, \u2018Keep your eye on the donut and not on the hole.'\u201d\n\tThe \u201cTwin Peaks\u201d TV show and films such as \u201cBlue Velvet,\u201d \u201cLost Highway\u201d and \u201cMulholland Drive\u201d melded elements of horror, film noir, the whodunit and classical European surrealism. Lynch wove tales, not unlike those of his Spanish predecessor Luis Bunuel, which proceeded with their own impenetrable logic.\n\n\t\n\n\n\n\n\n\n\n\n\n\n\t\t\tPopular on Variety\t\t\n\n\n\n\n\n\n\n\t\n\t\n\t\t\n\t\t\t\t\tRelate"
  },
  {
    "title": "Device uses wind to create ammonia out of air (ieee.org)",
    "points": 102,
    "submitter": "rbanffy",
    "submit_time": "2025-01-16T14:43:20 1737038600",
    "num_comments": 92,
    "comments_url": "https://news.ycombinator.com/item?id=42725823",
    "comments": [
      "The comments here are focused on how much energy it would take to turn this into fuel. The real story here is decentralized fertilizer production, buried at the end of the article:> this innovation could fundamentally reshape fertilizer manufacturing by providing a more sustainable, cost-effective alternative to centralized productionThe high energy cost of Haber-Bosch, plus the additional cost of transportation from manufacturer to farmer could potentially be eliminated by distributed, passive fertilizer generators scattered around in the fields.I'm no expert, but assuming sufficient local production, low concentration could potentially be overcome by continuous fertilization with irrigation throughout the growing season.Let's find out. Some quick fiddling with a molarity calculator and an almanac:-- 100 uM ammonia -> 1.7 mg / L ammonia-- 82% nitrogen -> 1.4 mg / L nitrogen-- My lawn needs around 1 lb / 1000 sq ft, or around 5 g / m2-- So my lawn needs about 3500 L / m2 of fertilized irrigation total for the season-- Ballpark farming irrigation is around 0.2 inches per day, or around 5L/m2I would need to water my lawn about 700 days in the year, or more realistically up my irrigation rate by about a factor of 4, AND source all of the water from the fertilizer box.I'm a little skeptical that I can allocate space for enough production and still have a lawn left to fertilize. The tech probably isn't ready for the big time on an industrial farm yet, but for research demo, this seems like a promising direction! Much more than concentrating it for fuel.\n \nreply",
      "Interesting idea.So, farms are definitely setup already to accomplish this.  Most farms have moved to central pivots for irrigation, and they already inject fertilizer into the pivot [1].  If fertilization could be generated onsite, then you could theoretically have everything plumbed together to \"just work\" without much intervention or shipping of chemicals.[1] https://www.farmprogress.com/farming-equipment/chemical-fert...\n \nreply",
      "Rain will wash nitrogen away (down to streams, rivers, and then the ocean creating lots of problems) so you want to apply nitrogen with an eye on when it will rain so your fertilizer stays on the field where you want it.  Your link doesn't specify what fertilizer is being applied, I would guess nitrogen is not one.Ammonia should be applied to the soil - in the air it is a hazard that can kill people and harm the plants (farmers wear lots of protective gear when working with ammonia, with more other things they don't bother).As such I'm not convinced that is the right answer.  You want a system that will apply nitrogen\n \nreply",
      ">  I would guess nitrogen is not one.It's the main fertilizer applied.Here's another site talking about common problems with this technique (from a farmer's perspective). [1][1] https://www.valleyirrigation.com/blog/valley-blog/2022/06/13...\n \nreply",
      "Farmers already do keep an eye when it will rain before applying fertilisers. So, this is already part of their calculation. Although, yes , this means they will not apply it everyday. Depending on their location this means that a lot of weeks are out of the picture.\n \nreply",
      "A somewhat passive fertiliser generator scattered around your fields is also known as a \"cow\" and a \"chicken\".\n \nreply",
      "Cows and chickens cannot fix nitrogen from the air. They eat the nitrogen-fixing plants. So in a sense they don't \"generate\" fertiliser, they only concentrate it.\n \nreply",
      "Until big fertilizer lobbies to make decentralized fertilizer illegal. Insert national security, wrong hands blah blah\n \nreply",
      "> Insert national security, wrong hands blah blahThat isn't a big reach.Ammonium nitrate is already controlled in several parts of the worldhttps://en.wikipedia.org/wiki/ANFO\n \nreply",
      "ANFO is explosives made with ammonium nitrate(Ammonium Nitrate Fuel Oil), however ammonium nitrate is by itself rather energetic and will explode when store improperly. The most recent memorable incident would be 2020 Beirut: https://en.wikipedia.org/wiki/2020_Beirut_explosionImagine one of these units left somewhere, slowly filling a tank that has not been sealed, water evaporating back out leaving a nice ammonium nitrate powder behind....\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/ammonia-fuel-2670794408",
    "first_paragraph": "The process requires no external power to produce the green energy fuelAlfred Poor is the former editor of Health Tech Insider and a contributor to IEEE Spectrum.Stanford University researchers tested their ammonia production device in different spots, including on campus.Lots of green energy experiments in the lab publish impressive results\u2014but what\u2019s more impressive is when those results come from an actual on-site pilot demonstration in the real world. That\u2019s the case with new research that is able to generate ammonia out of thin air, without requiring an external power source.This technique was developed by a team of researchers from Stanford University and King Fahd University of Petroleum and Minerals in Saudi Arabia. It relies on a fine mesh coated with catalysts that combine atmospheric nitrogen with hydrogen from water vapor to create ammonia (NH3).Ammonia has long-been a key component of fertilizer for agricultural crops, but it recently has taken on increased importance as a"
  },
  {
    "title": "Mathematicians discover new way for spheres to 'kiss' (quantamagazine.org)",
    "points": 143,
    "submitter": "isaacfrond",
    "submit_time": "2025-01-16T09:57:59 1737021479",
    "num_comments": 62,
    "comments_url": "https://news.ycombinator.com/item?id=42723406",
    "comments": [
      "I'd really love to know what the mathematicians are actually doing when they work this stuff out? Is it all on computers now? Can they somehow visualize 24-dimensional-sphere-packings in their minds? Are they maybe rigorously checking results of a 'test function' that tells them they found a correct/optimal packing? I would love to know more about what the day-to-day work involved in this type of research actually would be!\n \nreply",
      "> Is it all on computers now?Most modern math is certainly not \"all on computers\" and in general not even \"mostly on computers\".  There are definitely proofs for things like testing large spaces exhaustively which are sped up by computers (see the https://en.wikipedia.org/wiki/Four_color_theorem) and definitely for things like visualization (probably one of the oldest uses of computers for math), but usually the real work goes into how math has always been done: identifying patterns and abusing symmetries.For this one explicitly, if you read through the paper you'll find the statement that the main theorem presented here \"does not depend on any computer calculations. However, we have made available files with explicit coordinates for our kissing configurations\"\n \nreply",
      "It really depends though. Even in something like knot theory, that one might consider to be a very \"pure\" area, there's still a lot of computation involved that can be automated by computers.\n \nreply",
      "They definitely don't visualize 24 dimensional spheres. When I did my PhD in pure math I found that gradually I just became comfortable working without any visual or spatial intuition and instead relying on the (algebraic and topological) machinery that had been put in place before me. Terence Tao had a nice essay [1] talking about how the final stage in becoming a professional mathematician is developing the intuition to know what is likely true or not in these very abstract spaces.I also never used a computer for anything other than latex.[1] https://terrytao.wordpress.com/career-advice/theres-more-to-...\n \nreply",
      "The kind of intuition you gain for higher dimension tends not to be visual. It is more that you learn a bunch of tools and these in turn build intuition. For example high dimensional spheres are \"pointy\" and most of their volume are near their surface. These ideas can be defined rigorously and are important and useful. For medium dimension there are usually specific facts that you exploit. In my own work stuff like \"How often do you expect random walks to intersect\" is very important (and dependent on dimension).\n \nreply",
      "> For example high dimensional spheres are \"pointy\" and most of their volume are near their surfaceI had a visceral reaction to this. In what sense can a sphere be considered pointy? Almost by definition, it is the volume that minimizes surface area, in any number of dimensions.I can see how in higher dimensions e.g. a hypersphere has much lower volume than a hypercube. But that's not because the hypersphere became pointy, it's because the corners of the hypercube are increasingly more voluminous relative to the volume of the hypersphere, right?\n \nreply",
      "There is a standard thought experiment where you start with a hypercube of side-length 2, centered at the origin. You then place a radius 1 sphere on each vertex of this hypercube. The question then becomes: what is the largest sphere you can place at the origin so that it is \"contained\" by the other spheres. As it turns out in like dimension 6 or so the radius of the center sphere exceeds 1. It will actually poke out arbitrarily far (while still being restricted by the corner spheres).\n \nreply",
      "I hear this point parroted all of the time, but I think it is a misunderstanding and a poor visualization. Consider the same situation, but instead of focusing on the radius of the center sphere, focus on the distance between the spheres on the corners to the origin. For 1-dimension, these 'spheres' are unit intervals and so the distance is 1 (Central radius is 0). For 2-dimensions, these are circles at a distance of root(3) (Central radius is root(2)-1). 3-D: root(3) (Central radius is root(3)-1). Etc. So, it isn't the central circle getting more 'pointy' allowing the central radius to increase, but rather that the corner circles are getting further from the origin, allowing larger N-spheres (increasing proportional to the root of N). Thus, pointy is not the right way to conceptualize these spheres. For the more visual folk, I would recommend drawing this out and you can see this in action.\nMore clearly, if a sphere became 'spikey' then the distance on the surface of the spike should be further than a neighboring point, which is NOT the case.\nNot trying to attack you, I just see this same point over and over and think that this warrants more thought\n \nreply",
      "Yes, but that can be better understood as the hypercube becoming more pointy, not the sphere. And it's true; the cube's vertices get arbitrarily far from the origin, while the centers of its faces stay at \u00b11.There are other ways in which a hypersphere can be considered \"pointy\", though; for example, consider a point lying on the surface being moved some epsilon distance to a random direction. As the dimension increases, the probability that the point ends up inside the sphere approaches zero \u2013 the sphere spans a smaller and smaller fraction of the \"sky\".\n \nreply",
      "Specifically, of course, d = sqrt(N), where N is dimension and d is distance of a vertex of the unit hypercube from the origin.\n \nreply"
    ],
    "link": "https://www.quantamagazine.org/mathematicians-discover-new-way-for-spheres-to-kiss-20250115/",
    "first_paragraph": "\nAn editorially independent publication supported by the Simons Foundation.\n\nGet the latest news delivered to your inbox.\nCreate a reading list by clicking the Read Later icon next to the articles you wish to save.Type search term(s) and press enterPopular\n                                    SearchesJanuary 15, 2025How many spheres can you squeeze around a central sphere?Michele Sclafani for\u00a0Quanta MagazineContributing WriterJanuary 15, 2025In May of 1694, in a lecture hall at the University of Cambridge, Isaac Newton and the astronomer David Gregory started to contemplate the nature of the stars, only to end up with a math puzzle that would persist for centuries.The details of their conversation were poorly recorded and are possibly apocryphal \u2014 it had something to do with how stars of varying sizes would orbit a central sun. What\u2019s remembered today is the more general question it inspired: Given a central sphere, how many identical spheres can be arranged so that they touch it withou"
  }
]