[
  {
    "title": "Jellyfin as a Spotify Alternative (coppolaemilio.com)",
    "points": 75,
    "submitter": "coppolaemilio",
    "submit_time": "2025-04-17T00:10:51 1744848651",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=43711706",
    "comments": [
      "Self-hosting stuff is awesome if you have the skills.I have been on a mission for the last 2 years to replace as many subscriptions as possible with self-hosted solutions. The subscriptions really had gotten out of hand, it had gotten to about $200 (AUD) a month.Quick napkin math is that I have cancelled about ~$150 a month worth of subscriptions so far. The $500 office desktop I got for a home server is struggling at this point, but it's already paid for itself, so I will likely upgrade it to something much better later this year.Currently I am in the process of replacing all the movie streaming services with Emby.Spotify and Adobe lightroom is still on the todo list.I will likely end up with Youtube, Fastmail and Borgbase being my remaining subscriptions once I am done.\n \nreply",
      "What do we do about Lightroom? Capture one? How about sharing galleries?\n \nreply",
      "My strategy for syncing my music library with my phone is that I have four smart playlists:- songs rated 5 stars which I haven\u2019t listened to for at least 8 months\u00b9- songs rated 4 stars which I haven\u2019t listened to for at least 16 months- songs rated 3 stars which I haven\u2019t listened to for at least 32 months- the 20GB of least-played music(there are some other strictures as well, like eliminating Christmas music and some music files I have in my library more for archival purposes than anything else, but this is a decent approximation).This gives me a reasonably fresh selection of music and at least at the moment, with my daily sync habit, when I listen to a song it goes out of rotation for a while which could be anywhere from a week to years.\u2e3b1. This was originally 6/12/24 months, but I ended up boosting that time frame as storage grew tight on my phone.\n \nreply",
      "This reminds me of my smart playlist on Apple Music.It's called \"long time no see\" and it includes any songs I've listened to more than 10 times but haven't listened to in the last year. I've been using the same music library for nearly two decades now, so it works really well for me. It's like a constantly rotating nostalgia playlist.\n \nreply",
      "My problem stays the same \u2014 finding all my music that is on Spotify from elsewhere. It costs a lot to buy those music files and that too if they are available (which isn\u2019t always the case) and even after I buy I am not sure what were the T&C from that particular place I bought - whether I really own it, I don\u2019t, a bit but not fully - etc. Finding from Linux ISO sites is a nightmare and an extra bad nightmare if we are talking about some 2K - 0.6K songs (because I have 600 from before I started streaming). I wish there was an easy way for this - plug and play kinda.\n \nreply",
      "> I wish there was an easy way for this - plug and play kindaI can click a button in Lidarr to auth with Spotify and automatically search usenet for every album of every artist I follow on spotify, download them all, and make them available in Jellyfin. It'll even monitor the spotify account and import new additions. Getting the whole stack set up is pretty much the exact opposite of plug and play, but once you have it all installed it's amazing how much becomes smooth sailing. 2K songs is nothing for this kind of stack.\n \nreply",
      "I'm going to assume OP isn't interested in piracy given they were talking about buying...\n \nreply",
      "They said \"finding from Linux ISO sites is a nightmare\" and I took that to be a euphemism for piracy sites that they only don't use because they're annoying to navigate.\n \nreply",
      "This is a vendor lock-in more than anything. As someone who listens to mostly dubstep and EDM and built my playlist off of Spotify, I can't move to Spotify because they don't have half my playlists\n \nreply",
      "You can rip them from tidal quite easily. Youtube also has lots of music to rip but in shitty quality. That being said, music piracy has declined quite a bit since spotify. I'd suggest getting into a private music tracker if you really want to.\n \nreply"
    ],
    "link": "https://coppolaemilio.com/entries/i-left-spotify-what-happened-next/",
    "first_paragraph": "When I stopped using Spotify I tried a few different solutions until I found the perfect replacement for me. If you want the tl;dr: I now use Jellyfin. But if you want to know how I got here, follow me through each step of the way.I started gathering all my music files (mp3, or flac) in my computer, and from there I wanted to just listen to them the old way. The first issue I encountered was that none of the available music players were any good.We all love the nostalgic look of Winamp in screenshots, but in reality those players are very limited. They work (kinda) okay for playing a single album, but I struggle to browse my library or create a playlist with them. I tried tons of programs, but none of them satisfied me. I guess music players left the zeitgeist so the technology of playing files locally didn\u2019t improve much lately.\nFor a few days, I went along with the good old VLC player, but I was surprised to find how bad it is at handling flac files.I gave foobar2000 another go, and "
  },
  {
    "title": "Zoom Outage Caused by Accidental 'Shutting Down' of the Zoom.us Domain (zoom.us)",
    "points": 18,
    "submitter": "RVRX",
    "submit_time": "2025-04-17T00:55:49 1744851349",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=43711957",
    "comments": [
      "I was really hoping to find out they were hosting their DNS on GoDaddy. I still want it to be true.\n \nreply",
      "Amazing how many service outages are caused by doing business with GoDaddy.\n \nreply",
      "Someone in the Zoom company management forgot to update the billing  credit card for that domain, I bet you $1000. Happens all the time with our clients.\n \nreply",
      "\"This block was the result of a communication error between Zoom\u2019s domain registrar, Markmonitor, and GoDaddy Registry, which resulted in GoDaddy Registry mistakenly shutting down zoom.us domain. \"\n \nreply",
      "Something is fishy about this.  A communication error would not result in a domain being placed on hold.  On hold is usually the result of a legal order or in the case of the .us TLD a nexus compliance violation.  I've transferred thousands of domains from assorted dodgy registrars into MarkMonitor and can not even imagine a scenario where a miscommunication results in a domain being placed on hold.\n \nreply",
      "Nah, weird stuff that \u201cshouldn\u2019t\u201d happen almost always happens more often than things that \u201cshould\u201d happen.\n \nreply",
      "I hear ya but this would more than likely be something like a really sloppy human error such as following the wrong process vs. a miscommunication otherwise I would expect these outages to be much more frequent.  I do remember when a fat-finger at UUNET took out most of the internet long ago but that was a human error.To me a communication error implies someone followed erroneous instructions without asking the obvious, \" ... but isn't this a big business that is still live?\"\n \nreply",
      "Companies pay MarkMonitor to NOT make these mistakes. So... GoDaddy failed?\n \nreply",
      "oops\nrolls eyes\n \nreply"
    ],
    "link": "https://status.zoom.us/incidents/pw9r9vnq5rvk",
    "first_paragraph": "Resend OTP in:  seconds \n                    Didn't receive the OTP?\n                    Resend OTP \nResend OTP in: 30 seconds \n                      Didn't receive the OTP?\n                      Resend OTP \nThe URL we should send the webhooks toWe'll send you email if your endpoint fails\n          Subscribe to updates for Issue with multiple Zoom Services via email and/or text message. You'll receive email notifications when incidents are updated, and text message notifications whenever Zoom creates or resolves an incident.\n        "
  },
  {
    "title": "Demolishing the Fry's Electronics in Burbank (latimes.com)",
    "points": 34,
    "submitter": "walterbell",
    "submit_time": "2025-04-14T03:43:55 1744602235",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=43677862",
    "comments": [
      "Where else could you simultaneously purchase through-hole transistors, a gaming motherboard, a 19\" rack, a leafblower, a loudspeaker disguised as a plastic rock, pornography, a taser, a sandwich and a decent cup of coffee while surrounded by fiberglass cowboys and aliens... sad to see\n \nreply",
      "Don't forgot the knock-off cologne!\n \nreply",
      "I'm sad to see it end like this.  About 20 years ago, I used to go to this Burbank Fry's after work whenever I could and spend hours just browsing. They had this cool 50's sci-fi alien/ufo movie theme inside.  Another favorite is the Fry's in Anaheim where I got pc parts to build my first 486 PC in the early 90's. So many memories. I remember buying my first laptop ever, a monochrome thinkpad with personal check (as my credit card limit was too low) and having to call bank to verify my balance; also the day that I bought a Pentium CPU when it first came out, and the time when I got the boxed Windows 95 OS.  Those were the days.\n \nreply",
      "Drooling over PC part reviews in magazines that I could never afford, checking the weekly printed Fry's ad in the local paper to find deals, convincing my parents to drop me at the electronics-nerd-utopia for a lazing weekend afternoon - \"Won't you get bored?\" . . . \"No Mom!\"Iconic building, nostalgic time.\n \nreply",
      "For those in the bay area, Micro Center is opening a branch in Santa Clara.https://www.microcenter.com/site/stores/santa-clara.aspxWell, maybe. It's delayed months at this point.\n \nreply",
      "I wonder what they did with the Fry\u2019s 747.\n \nreply",
      "it's sad to see this location go. it was such an amazing store on the inside. the theme had some great homages to Mars Attacks!, as well as a great many other sci-fi films. this album has some good pictures of some of the more notable sculptures in there, but the theme went even farther than just sculptures: https://www.flickr.com/photos/selfishcauses/albums/721577140...\n \nreply",
      "Great photos!Painting and 3-D scan of Burbank Frys, https://savefrys.com/tributes/ and https://sketchfab.com/3d-models/rip-frys-162eeef6095343ec8b3...Roman remnants in Phoenix store, https://nickdiak.com/2021/02/an-empire-crumbles-retrospectiv...Oil-themed store in Houston, https://houstonhistoricretail.com/electronics/frys-electroni...\n \nreply",
      "Here's a decent 21 min mini-doc on the birth and death of Fry's https://www.youtube.com/watch?v=pu2uAKc37jIPersonally, visiting friends in California and them taking me to Fry's fir the first time was an experience I'll not forget. We had CompUSA back home. But, Fry's was a whole other level.Going from being a computer geek in 90s rural midwest to being a computer geek in a Fry's in Silicon Valley in the Tech Bubble was like stepping into a magically wonderful mirror world.\n \nreply",
      "El Sobrante, https://www.hmdb.org/m.asp?m=94274> The Fry's Foods grocery chain began at this location in 1954 when Donald Fry acquired Ray's Market, owned by Ray Dickenson. Joined by his brother, Charles, in 1955, they grew that initial store into a 41-store chain which they sold in 1972. Charles gifted a portion of the proceeds to his three sons, enabling them to launch the first store of what would one day become the highly successful Fry's Electronics retail chain.\n \nreply"
    ],
    "link": "https://www.latimes.com/00000196-230a-d4c4-abd7-fb5a95770000-123",
    "first_paragraph": "Follow UsRebecca Castillo is a video creator and special projects lead for Los Angeles Times\u2019 High School Insider and other Public Affairs programs. A Southern California native, she is an alumna of Swarthmore College and USC.Subscribe for unlimited accessSite MapFollow UsMORE FROM THE L.A. TIMES  "
  },
  {
    "title": "OpenAI Codex CLI: Lightweight coding agent that runs in your terminal (github.com/openai)",
    "points": 311,
    "submitter": "mfiguiere",
    "submit_time": "2025-04-16T17:24:50 1744824290",
    "num_comments": 187,
    "comments_url": "https://news.ycombinator.com/item?id=43708025",
    "comments": [
      "I tried one task head-to-head with Codex o4-mini vs Claude Code: writing documentation for a tricky area of a medium-sized codebase.Claude Code did great and wrote pretty decent docs.Codex didn't do well. It hallucinated a bunch of stuff that wasn't in the code, and completely misrepresented the architecture - it started talking about server backends and REST APIs in an app that doesn't have any of that.I'm curious what went so wrong - feels like possibly an issue with loading in the right context and attending to it correctly? That seems like an area that Claude Code has really optimized for.I have high hopes for o3 and o4-mini as models so I hope that other tests show better results! Also curious to see how Cursor etc. incorporate o3.\n \nreply",
      "Claude Code still feels superior. o4-mini has all sorts of issues. o3 is better but at that point, you aren't saving money so who cares.I feel like people are sleeping on Claude Code for one reason or another. Its not cheap, but its by far the best, most consistent experience I have had.\n \nreply",
      "\"gemini 2.5 pro exp\" is superior to Claude Sonnet 3.7 when I use it with Aider [1]. And it is free (with some high limit).[1]https://aider.chat/\n \nreply",
      "Don't they train on your inputs if you use the free Ai studio api key?\n \nreply",
      "speaking for myself, I am happy to make that trade. As long as I get unrestricted access to latest one. Heck, most of my code now is written by gemini anyway haha.\n \nreply",
      "> Its not cheap, but its by far the best, most consistent experience I have had.It\u2019s too expensive for what it does though. And it starts failing rapidly when it exhausts the context window.\n \nreply",
      "If you get a hang of controlling costs, it's much cheaper. If you're exhausting the context window, I'm not surprised you're seeing high cost.Be aware of the \"cache\".Tell it to read specific files, never use /compact (that'll bust cache, if you need to, you're going back and forth too much or using too many files at once).Never edit files manually during a session (that'll bust cache). THIS INCLUDES LINT.Have a clear goal in mind and keep sessions to as few messages as possible.Write / generate markdown files with needed documentation using claude.ai, and save those as files in the repo and tell it to read that file as part of a question.I'm at about ~$0.5-0.75 for most \"tasks\" I give it. I'm not a super heavy user, but it definitely helps me (it's like having a super focused smart intern that makes dumb mistakes).If i need to feed it a ton of docs etc. for some task, it'll be more in the few $, rather than < $1. But I really only do this to try some prototype with a library claude doesn't know about (or is outdated).For hobby stuff, it adds up - totally.For a company, massively worth it. Insanely cheap productivity boost (if developers are responsible / don't get lazy / don't misuse it).\n \nreply",
      "Did you try the same exact test with o3 instead? The mini models are meant for speed.\n \nreply",
      "I want to but I\u2019ve been having trouble getting o3 to work - lots of errors related to model selection.\n \nreply",
      "These days, I usually paste my entire (or some) repo into gemini and then APPLY changes back into my code using this handy script i wrote: https://github.com/asadm/vibemodeI have tried aider/copilot/continue/etc. But they lack in one way or the other.\n \nreply"
    ],
    "link": "https://github.com/openai/codex",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Lightweight coding agent that runs in your terminal\n      Lightweight coding agent that runs in your terminalnpm i -g @openai/codexCodex CLI is an experimental project under active development. It is not yet stable, may contain bugs, incomplete features, or undergo breaking changes. We\u2019re building it in the open with the community and welcome:Help us improve by filing issues or submitting PRs (see the section below for how to contribute)!Install globally:Next, set your OpenAI API key as an environment variable:Note: This command sets the key only for your current terminal session. To make it permanent, add the export line to your shell's configuration file (e.g., ~/.zshrc).Run interactively:Or, run with a prompt as input (and optionally in Full Auto mode):That\u2019s it \u2013 Codex will scaffold a file, run it inside a sandbox, install any\nm"
  },
  {
    "title": "Darwin's children drew all over the \u201cOn the Origin of Species\u201d manuscript (2014) (theappendix.net)",
    "points": 318,
    "submitter": "arbesman",
    "submit_time": "2025-04-16T14:28:36 1744813716",
    "num_comments": 96,
    "comments_url": "https://news.ycombinator.com/item?id=43706037",
    "comments": [
      "Relevant only by virtue of also being about historical children\u2019s drawings, but it reminds of another example of a child\u2019s drawings preserved for us to see: https://en.m.wikipedia.org/wiki/Onfim> \u2026 Onfim, was a boy who lived in Novgorod (now Veliky Novgorod, Russia) in the 13th century, some time around 1220 or 1260. He left his notes and homework exercises scratched in soft birch bark, which was preserved in the clay soil of Novgorod.I would wager that if you could travel back in time to the emergence of anatomically modern humans, you\u2019d find they\u2019re just like us. I don\u2019t think that\u2019s particularly controversial or surprising, but it\u2019s easy to forget that people who came long before us were really no different from us (or put differently, were no different than them), and it helps to better understand history if you think of them that way.\n \nreply",
      "Author of the original Appendix article here (the one about Darwin's kids) - I think it got on HN today because I linked to while discussing Onfim here: https://resobscura.substack.com/p/onfims-world-medieval-chil...\n \nreply",
      "Hi Ben! I'll email you a repost invite for the Onfim article (https://news.ycombinator.com/item?id=43705174) - if you wait a week or so and then use it, the repost will go in the second-chance pool.The reason for waiting is to give the hivemind cache time to clear. Normally we'd re-up the existing post, but we don't want two overly similar threads on the frontpage within a short time period.\n \nreply",
      "2014! Amazing.\n \nreply",
      "> I would wager that if you could travel back in time to the emergence of anatomically modern humans, you\u2019d find they\u2019re just like us.I find this viewpoint surprisingly underutilized in institutional history and archeology sometimes. I occasionally watch documentaries with distinguished talking heads on e.g. egyptology and what not, and they often bend over backwards to find complicated explanations that defy all \"this is just not how humans or human organizations operate\" logic. For example, analyzing an impressive building and then assuming that the same people capable of constructing it also made a basic mistake or in other ways assuming they were daft. Or requiring a complex lore/spiritual explanation for something that can be equally explained by classic big org fuckups.\n \nreply",
      "The formal name for this kind of argument is \"ethnographic analogy\". It's widespread in archaeology and institutional history, but doesn't always show up so overtly because1. It's not very interesting to say \"they're just like us\" and2. \"like us\" is a huge statement hiding a lot of assumptions.Analogy is also considered a fairly weak argument on its own. There are vanishingly few accepted \"cultural universals\" despite decades of argument on the subject (which I'll let the wiki article [0] summarize), so justifying them usually follows an argument like \"X is related/similar to Y, and X has behavior Z, so Y's behavior is an evolution of Z\". That's fine if you're talking Roman->Byzantines, maybe, but it's a bit of a stretch when your analogy is \"modern US->Old Kingdom Egypt\". It's also very, very easy to get wrong and make a bad analogies. Take basically the entire first couple centuries of American anthropology as an example.[0] https://en.wikipedia.org/wiki/Cultural_universal\n \nreply",
      "For a long time, I also somehow thought that people from earlier eras were less intelligent\u2014simply because, in retrospect, all those obvious mistakes are so apparent. It took considerable mental effort for me to accept that people back then were probably just like us today, only living under different circumstances.\n \nreply",
      "The difference between us and them is the accumulated knowledge. You and I had no better an idea of what a volcano is than an anyone from thousands of years ago until someone told us.\n \nreply",
      "I think of certain types of knowledge as one way functions. In order to acquire the knowledge you have to search a huge key space or experience costly elimination of options. Once you know the answer it feels obvious and intuitive. We have accumulated so much of this knowledge now that we have a hard time intuitively understanding the gap between people without it and us.\n \nreply",
      "Related: https://en.wikipedia.org/wiki/Social_credit \"Douglas disagreed with classical economists who recognised only three factors of production: land, labour and capital. While Douglas did not deny the role of these factors in production, he considered the \"cultural inheritance of society\" as the primary factor. He defined cultural inheritance as the knowledge, techniques and processes that have accrued to us incrementally from the origins of civilization (i.e. progress). Consequently, mankind does not have to keep \"reinventing the wheel\". \"We are merely the administrators of that cultural inheritance, and to that extent the cultural inheritance is the property of all of us, without exception.\" ... Douglas believed that it was the third policy alternative [the object of the industrial system is merely to provide goods and services] upon which an economic system should be based, but confusion of thought has allowed the industrial system to be governed by the first two objectives [to impose upon the world a system of thought and action and to create employment]. If the purpose of our economic system is to deliver the maximum amount of goods and services with the least amount of effort, then the ability to deliver goods and services with the least amount of employment is actually desirable. Douglas proposed that unemployment is a logical consequence of machines replacing labour in the productive process, and any attempt to reverse this process through policies designed to attain full employment directly sabotages our cultural inheritance. Douglas also believed that the people displaced from the industrial system through the process of mechanization should still have the ability to consume the fruits of the system, because he suggested that we are all inheritors of the cultural inheritance, and his proposal for a national dividend is directly related to this belief.\"\n \nreply"
    ],
    "link": "https://theappendix.net/posts/2014/02/darwins-children-drew-vegetable-battles-on-the-origin-of-species",
    "first_paragraph": "\n    \n      \n        By Benjamin Breen \u2013\n      \n    \n    Published February 12, 2014\n  Yesterday was Darwin Day, marking the 205th anniversary of the great naturalist\u2019s birth on February 12, 1809. One of the great things about Darwin is that a huge amount of his work is digitized and freely available via sites like Darwin Online.Interested browsers can also check out the Darwin Manuscripts Project, a collaborative initiative based at the American Museum of Natural History. Here you can read through Darwin\u2019s personal notes, including gems like his scratched out book title ideas. There are also a number of nature drawings that Darwin prepared while writing his masterpiece, On the Origin of Species by Means of Natural Selection (1859). Here, for example, is Darwin\u2019s rather skillful drawing of the stamen of a Strelitizia flower:\n\n      Cambridge University Library DAR 49: 115r\n    \nBut there are other drawings in Darwin\u2019s papers that defy explanation - until we remember that Darwin and his"
  },
  {
    "title": "12-factor Agents: Patterns of reliable LLM applications (github.com/humanlayer)",
    "points": 223,
    "submitter": "dhorthy",
    "submit_time": "2025-04-15T22:38:04 1744756684",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=43699271",
    "comments": [
      "> reliable LLM applicationsadd that to the list of contradictory phrases (jumbo shrimp, etc.)\n \nreply",
      "This old obscure blog post about framework patterns has resonated with me throughout my career and I think it applies here too. LLMs are best used as \"libraries\" rather than \"frameworks\", for all the reasons described in the article and more, especially now while everything is in such flux. \"Frameworks\" are sexier and easier to sell though, and lead to lock-in and add-on services, so that's what gets promoted.https://tomasp.net/blog/2015/library-frameworks/\n \nreply",
      "This is so good\u2026\u201c\u2026 you can find frameworks not just in software, but also in ordinary life. If you buy package holidays, you're buying a framework - they transport you to some place, put you in a hotel, feed you and your activities have to fit into the shape provided by the framework (say, go into the pool and swim there). If you travel independently, you are composing libraries. You have to book your flights, find your accommodation and arrange your program (all using different libraries). It is more work, but you are in control - and you can arrange things exactly the way you need.\u201d\n \nreply",
      "oh heck yeah this rocks. I'm gonna add to the links section\n \nreply",
      "Additionally in terms of career development, you're going to be a lot better off learning the low level LLM interfaces rather than being dependent on a framework (or their even more evil cousin, platforms). Once you learn those, jumping to a platform is usually trivial, whereas the reverse can be more challenging. Junior devs often think that the more frameworks they have on their resume the better, but it often pigeonholes you more than it helps.And I don't mean to imply that frameworks are always bad. Things like security best practices out of the box can be worth it. But especially in AI right now, nobody knows what those best practices are going to be. So it's best to spend this time learning how to do things at a low level rather than attaching to some framework that may be obsolete in a year.\n \nreply",
      "exactly - we keep trying to figure out the right interfaces, but we jump to assume that we know what they are.If we had the right interface, we would set up the black box, and then put holes/knobs on the box to allow anyone to change the things they should actually need to change.if we have the wrong interface, then the knobs aren't interesting, and instead we keep end up opening the box, or reaching into the holes at weird angles to do things that nobody knew we'd want to, but that are obviously the right things to do to maximize performancesomeday we'll have the right interface, but for now, better to skip the box and do the extra cycles. You're an engineer, you can write a for loop and a switch statement. don't outsource your prompts and give up control flow to save a few hundred lines that will eventually become pretty customized anyway\n \nreply",
      "Very informative wiki, thank you, I will definitely use it. So Ive made my own \"AI Agents framework\" [0] based on actor model, state machines and aspect oriented programming (released just yesterday, no HN post yet) and I really like points 5 and 7:    5: Unify execution state and business state\n    8. Own your control flow\n\nThat is exactly what SecAI does, as it's a graph control flow library at it's core (multigraph instead of DAG) and LLM calls are embedded into graph's nodes. The flow is reinforced with negotiation, cancellation and stateful relations, which make it more \"organic\". Another thing often missed by other frameworks are dedicated devtools (dbg, repl, svg) - programming for failure, inspecting every step in detail, automatic data exporters (metrics, traces, logs, sql), and dead-simple integrations (bash). I've released the first tech demo [1] which showcases all the devtools using a reference implementation of deepresearch (ported from AtomicAgents). You may especially like the Send/Stop button, which is nothings else then \"Factor 6. Launch/Pause/Resume with simple APIs\". Oh and it's network transparent, so it can scale.Feel free to reach out.[0] https://github.com/pancsta/secai[1] https://youtu.be/0VJzO1S-gV0\n \nreply",
      "i like the terminal UI and otel integrations - what tasks are you using this for today?\n \nreply",
      "Thanks, terminal UI is an important design choice - it's fast, cheap, and runs everywhere (like the web via wasm / ssh, or on iphones with touch). The LLM layer is still fresh, and I personally use it for web scraping, but the underlying workflow engine is quite mature and ubiquitous - it was used for sync engines, UIs, daemons, network services. It shines when faces complexity, nondeterminism, and retry logic - the more chaotic the flow is, the bigger the gains.The approach is to shape behavior from chaos by exclusion, instead of defining all possible transitions. With LLMs, this process could be automated and effectively an agent would be dynamically creating itself using a DSL (state schema and predefined states). The great thing about LLMs is being charged by tokens instead of a number of requests. We can just interrogate them about every detail separately and build a flow graph with transparent (and debuggable) reasoning. I also have API sketches for proactive scenarios (originally made for an ML prototype) [0].[0] https://github.com/pancsta/secai/blob/474433796c5ffbc7ec5744...\n \nreply",
      "Another one: plan for cost at scale.These things aren't cheap at scale, so whenever something might be handled by a deterministic component, try that first. Not only save on hallucinations and latency, but could make a huge difference in your bottom line.\n \nreply"
    ],
    "link": "https://github.com/humanlayer/12-factor-agents",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        What are the principles we can use to build LLM-powered software that is actually good enough to put in the hands of production customers?\n      In the spirit of 12 Factor Apps.  The source for this project is public at https://github.com/humanlayer/12-factor-agents, and I welcome your feedback and contributions. Let's figure this out together!Hi, I'm Dex. I've been hacking on AI agents for a while.I've tried every agent framework out there, from the plug-and-play crew/langchains to the \"minimalist\" smolagents of the world to the \"production grade\" langraph, griptape, etc.I've talked to a lot of really strong founders, in and out of YC, who are all building really impressive things with AI. Most of them are rolling the stack themselves. I don't see a lot of frameworks in production customer-facing agents.I've been surprised to find "
  },
  {
    "title": "Damn Vulnerable MCP Server (github.com/harishsg993010)",
    "points": 151,
    "submitter": "mrxhacker99",
    "submit_time": "2025-04-16T16:00:21 1744819221",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=43707021",
    "comments": [
      "As it has been mentioned before, MCP isn't \"vulnerable\". It's just on the other side of your air lock. Think of your MCP as a different client application. The whole thing is just a client. The fact that you need to write a client for your client is.... something, but your MCP app is a client app. It's boundaries with your service should be understood as such.Saying MCP is vulnerable is like saying \"Web applications are vulnerable. Anyone can see the API calls you're making and modify them or trick your UI app to make a different call and hack your system\". Obviously that's mostly nonsense, but not 100% wrong either. You see it a lot with very very inexperienced developers who think \"just because my App is Android/iOS only I don't need to worry about authn/authz\". There was just a story on here few weeks ago about some New Zealand startup that did that\n \nreply",
      "The MCP ecosystem right now actively encourages insecure behavior. Just installing a popular WhatsApp sever can give attackers access to your private data - they can text you with instructions for your assistant to forward private messages to another account using tricks to help make that action look legit so you'll approve it: https://simonwillison.net/2025/Apr/9/mcp-prompt-injection/#m...\n \nreply",
      "But you can replace MCP with any tech and you have the same valid sentence.\u201cAttackers are using (email attachments, SMSs, TeamViewer, crypto wallet, phishing websites, etc) to access your private data - they can [\u2026] you using tricks to make it seem legit\u201dThe only difference is that AI/MCP is the current flavor of the month for this type of attacks. These attacks get much worse when the tech has the hype (like AI now or limewire 20 years ago or the internet 30 years ago) and the average user still doesn\u2019t quite fully grasp what this tech is doing or how it\u2019s working.\n \nreply",
      "I somewhat agree, but I think an important distinction is that in this case, you are legitimately giving the MCP server your credentials - there are no tricks there.This is distinct from various forms of phishing where they are tricking you to give access to sensitive information. Here, you are giving that access willingly to something that is then itself vulnerable to being tricked/tricking you.\n \nreply",
      ">Saying MCP is vulnerable is like saying \"Web applications are vulnerable\u201dJust for reference, this GitHub follows in the tradition of many an example project all of which have the explicit intent of demonstrating not that the underlying concept is inherently vulnerable, but that implementations can be.Damn Vulnerable Web App is probably the best known, but there are others for REST apis, web sockets, GraphQL, and more. They\u2019re educational reference implementations that are deliberately insecure to use as an educational tool.\n \nreply",
      "Except that all the \u201cvulnerabilities\u201d listed are addressed (or can be only addressed) by treating tbr MCP server as a client application.If a Damn Vulnerable Web App demo was just 10 or 20 different \u201cthere no authn/authz on this endpoint\u201d, it would be a crappy demo\n \nreply",
      "How will this work when people are talking about third party MCP servers(e.x. booking.com, GitHub, etc.)\n \nreply",
      "The same way you'd write a third party client to any software/API.The MCP uses some kind of identity to talk to booking.com or GitHub. That's your security boundary. You assume that anything the MCP has access to (including that identity), the user has access to. If you add a `list_available_hotels()` tool to your booking.com MCP, that tool needs to run with the same identity as the person talking to the LLM. It doesn't have any more permissions or access to your system than the booking.com react app does.Think of the MCP server as a natural language interface to your application. Like a CLI or a WebApp. Instead of writing specific commands to a cli, or following a series of clicks in a GUI app, you \"chat\" with it.\n \nreply",
      "100% this... the authn/authz should be gated at the server that store sensitive data... whatever token/user that MCP uses must have its access scope down to what needed. I guess the biggest issue right now is many of these APIs have no granular access control and is open to abuse :(With that said, some vulnerabilities like command injections or argument injection, the responsibility is on MCP developer to make sure they follow best practices and not let user take control of these commands when \"shelling out\".\n \nreply",
      "> The fact that you need to write a client for your client is...correct me if im wrong, but isnt that a proxy?  why is everyone calling it a server\n \nreply"
    ],
    "link": "https://github.com/harishsg993010/damn-vulnerable-MCP-server",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Damn Vulnerable MCP Server\n      A deliberately vulnerable implementation of the Model Context Protocol (MCP) for educational purposes.The Damn Vulnerable Model Context Protocol (DVMCP) is an educational project designed to demonstrate security vulnerabilities in MCP implementations. It contains 10 challenges of increasing difficulty that showcase different types of vulnerabilities and attack vectors.This project is intended for security researchers, developers, and AI safety professionals to learn about potential security issues in MCP implementations and how to mitigate them.The Model Context Protocol (MCP) is a standardized protocol that allows applications to provide context for Large Language Models (LLMs) in a structured way. It separates the concerns of providing context from the actual LLM interaction, enabling applications "
  },
  {
    "title": "How a Forgotten Battle Created a More Peaceful World (worldhistory.substack.com)",
    "points": 49,
    "submitter": "crescit_eundo",
    "submit_time": "2025-04-16T22:21:14 1744842074",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=43711001",
    "comments": [
      "The included descriptions of the battle\u2019s aftermath are haunting.As much as movies and documentaries usually reflect the horrors of combat itself, they rarely deal with the aftermath - not lights out in an adrenaline fueled moment, but lying for hours in a random patch of land no longer having a mouth, trying to cry for help knowing that no one will come until dying from exposure and maggots.This was the reality for millions of young people; still is. Let\u2019s hope we never see it firsthand.\n \nreply",
      "Such deformity and mutilation may result from an errant jump on a dirt bike. I intend to fly because it is there, it is the right of children to be free. Do a flip. And, yes, some will break a jaw or crush a testicle like a grape. We are not meek souls.\n \nreply",
      "You point out agency yourself, which is indeed the differentiator. Combatants in wars are rarely on the battlefield by their own choice or design.\n \nreply",
      "Everyone has a choice.\n \nreply",
      "Based on the title, I thought this was an article about The Kalinga War, a war so bloody it moved the Emperor Ashoka to embrace non-violence and spread Buddhism throughout Asia.\n \nreply",
      "Yes, where would we be without the rules-based international order? Perhaps we would be watching videos every day of children blown apart by weapons of war.\n \nreply",
      "Those videos are occurring because of a major power hypocritically flouting the rules-based international order. In spite of it, not because of it. We know the counterfactual of the rules-based order. It's nonstop European warfare in the 19th and the early 20th centuries.\n \nreply",
      "Yep, and UN had been expressing \"serious concerns\" every time. Although I haven't heard even those for a long time.\n \nreply",
      "\"International law is a funny thing. Within a country, lines of authority are clear. The government makes laws, it has agencies that enforce them, and the penalties for violating the laws are clear. But, in our modern system of sovereign states, no authority sits above the nation. Each country is sovereign. International laws are, therefore, more fragile, because they require the consent of everybody involved to keep them going.\"This was the purpose of the imperium, not necessarily in the narrow sense of empire we often have in mind, but as a kind of order (e.g. the HRE).In our case, the United States as hegemon has played the role of the global imperium over much of the world over the last few decades, and over a good chunk of the world since WWII. The reigning doctrine of the American empire has been liberalism (which explains why many if not most Americans/Westerners treat liberalism as a \"neutral\" position; it is the water we swim in). It explains why the US has intervened in numerous distant conflicts, engaged in countless \"nation building\" campaigns aimed at spreading liberal democracy around the world, and successfully influenced peoples worldwide through its film and media. These were all intended to preserve and enlarge the liberal imperium.Now that liberalism has devoured and corroded the Protestant mother that held it together, and escaped the containment it created - in large part through the infusion of liberalism into Protestant doctrine - we are witnessing the fullness of the tensions inherent in liberalism playing out in the human psyche and society and unraveling liberalism and the liberal order. The shape of the emerging postliberal order is uncertain. The noisiest contenders seem to be an increasingly overt tyrannical liberalism and fascism, though a less conspicuous movement aiming to return to pre-liberal classical traditions is also in play.\n \nreply",
      "At nations level anarchy is the rule of law.\n \nreply"
    ],
    "link": "https://worldhistory.substack.com/p/how-a-forgotten-battle-created-a",
    "first_paragraph": ""
  },
  {
    "title": "Man who built ISP instead of paying Comcast expands to hundreds of homes (2022) (arstechnica.com)",
    "points": 294,
    "submitter": "voxadam",
    "submit_time": "2025-04-16T20:06:50 1744834010",
    "num_comments": 136,
    "comments_url": "https://news.ycombinator.com/item?id=43709770",
    "comments": [
      "Hey! I did this too - CenturyLink wanted an insane amount of money to bring fiber to our place, now we service hundreds and we're growing into a major contender in Boulder County - https://ayva.network\n \nreply",
      "Just a quick heads up that the homepage video is ~24MB over the wire, even on a phone. That might actually be a challenge if someone's WiFi is down and they're trying to get support over cellular.(Huge kudos for this project in general)\n \nreply",
      "Thanks! It's actually much less for the bandwidth-constrained, I use adaptive coding.  If you have the bandwidth though...That said, I know our page isn't particularly lightweight anyway, I've been pretty focused on expansion efforts and haven't had much time to update & work on the site.\n \nreply",
      "This page was 9 seconds of white screen before the entire thing loaded at once. I'm on Starlink. Hopefully you get a chance to correct this in the future, as I'm really supportive of projects like yours, but if the page was linked from a top-5 article or something I would have hit the back button already.\n \nreply",
      "First Contentful Paint at 12.0 seconds here, and I'm on fiber.\n \nreply",
      "You are doing God's work. Thank you. I wish more people cared about wasteful bandwidth usage.\n \nreply",
      "Well... To be fairrrrrrrrrrrIf you used Ayva's fiber internet that video would download instantly =D\n \nreply",
      "\"Works for me, couldn't replicate\".  My place has 10gbps/10gbps service through my network but this is a quick test over 6ghz wifi: https://www.speedtest.net/result/17623249189\n \nreply",
      "(not OP)This nerdsniped me.10 minutes later, and TIL: MPD is some sort of streamed-MP4 format; dash-mpd-cli is a xplatform Rust utility binary that can download this to an MP4, just given the MPD URL in dev tools.However I keep getting 1.5 MB and 500 KB for the two videos, no matter window width. Chrome on macOS arm64, 16\" MBP.I'm curious what your environment is, if you don't mind sharing(also, trivia for audience: last week I saw a tweet that palantir.com was doing over 100 MB worth of videos, and of course, A) they are B) they're poorly compressed, as much as 10x the bitrate they need to be.)\n \nreply",
      "Frontend is full blazor w/hybrid WASM, almost zero JS, all C#.  Browser DOM is controlled by the app service in realtime, I plan on using this as a basis for our subscribers to be able to do live traffic & link stats monitoring, among other things.\n \nreply"
    ],
    "link": "https://arstechnica.com/tech-policy/2022/08/man-who-built-isp-instead-of-paying-comcast-50k-expands-to-hundreds-of-homes/",
    "first_paragraph": "\n      Jared Mauch gets $2.6 million from gov't to expand fiber ISP in rural Michigan.\n    Jared Mauch, the Michigan man who built a fiber-to-the-home Internet provider because he couldn't get good broadband service from AT&T or Comcast, is expanding with the help of $2.6 million in government money.When we wrote about Mauch in January 2021, he was providing service to about 30 rural homes including his own with his ISP, Washtenaw Fiber Properties LLC. Mauch now has about 70 customers and will extend his network to nearly 600 more properties with money from the American Rescue Plan's Coronavirus State and Local Fiscal Recovery Funds, he told Ars in a phone interview in mid-July.\n\n\n\n\n\n\n\n                Fiber installed at one of the homes on Mauch's network.\n                              \n\n\n\n\n\n\n\n      Fiber installed at one of the homes on Mauch's network.\n\n          \n\n\n\nThe US government allocated Washtenaw County $71 million for a variety of infrastructure projects, and the county devo"
  },
  {
    "title": "TLS certificate lifetimes will officially reduce to 47 days (digicert.com)",
    "points": 360,
    "submitter": "crtasm",
    "submit_time": "2025-04-15T15:09:22 1744729762",
    "num_comments": 497,
    "comments_url": "https://news.ycombinator.com/item?id=43693900",
    "comments": [
      "What's the end game here? I agree with the dissent. Why not make it 30 seconds?Once we cross the threshold of \"I absolutely have to automate everything or it's not viable to use TLS anymore\", why do we care about providing anything beyond ~48 hours? I am willing to bet money this threshold will never be crossed.This feels like much more of an ideological mission than a practical one, unless I've missed some monetary/power advantage to forcing everyone to play musical chairs with their entire infra once a month...\n \nreply",
      "I'm on the team at Let's Encrypt that runs our CA, and would say I've spent a lot of time thinking about the tradeoffs here.Let's Encrypt has always self-imposed a 90 day limit, though of course with this ballot passing we will now have to reduce that under 47 days in the future.Shorter lifetimes have several advantages:1. Reduced pressure on the revocation system. For example, if a domain changes hands, then any previous certificates spend less time in the revoked state. That makes CRLs smaller, a win for everyone involved.2. Reduced risk for certificates which aren't revoked but should have been, perhaps because a domain holder didn't know that a previous holder of that domain had it, or an attack of any sort that led to a certificate being issued that wasn't desired.3. For fully short-lived certs (under 7 days), many user-agents don't do revocation checks at all, because that's a similar timeline to our existing revocation technology taking effect. This is a performance win for websites/user-agents. While we advocate for full certificate automation, I recognize there are cases where that's not so easy, and doing a monthly renewal may be much more tractable.Going to shorter than a few days is a reliability and scale risk.  One of the biggest issues with scale today is that Certificate Transparency logs, while providing great visibility into what certs exist (see points 1 and 2), will have to scale up significantly as lifetimes are cut.Why is this happening now, though? I can't speak for everyone, and this is only my own opinion on what I'm observing, but: One big industry problem that's been going on for the last year or two is that CAs have found themselves in situations where they need to revoke certificates because of issues with those certificates, but customers aren't able to respond on an appropriate timeline. So the big motivation for a lot of the parties here is to get these timelines down and really prove a push towards automation.\n \nreply",
      "When I first set up Let's Encrypt I thought I'd manually update the cert one per year. The 90 day limit was a surprise. This blog post helped me understand (it repeats many of your points) https://letsencrypt.org/2015/11/09/why-90-days/\n \nreply",
      "So it's being pushed because it'll be easier for a few big players in industry. Everybody else suffers.\n \nreply",
      "It's a decision by Certificate Authorities, the ones that sell TLS certificate services, and web browser vendors. One benefits from increased demand on their product, while the other benefits by increasing the overhead on the management of their software, which increases the minimum threshold to be competitive.There are security benefits, yes. But as someone that works in infrastructure management, including on 25 or 30 year old systems in some cases, it's very difficult to not find this frustrating. I need tools I will have in 10 years to still be able to manage systems that were implemented 15 years ago. That's reality.Doubtless people here have connected to their router's web interface using the gateway IP address and been annoyed that the web browser complains so much about either insecure HTTP or an unverified TLS certificate. The Internet is an important part of computer security, but it's not the only part of computer security.I wish technical groups would invest some time in real solutions for long-term, limited access systems which operate for decades at a time without 24/7 access to the Internet. Part of the reason infrastructure feels like running Java v1.3 on Windows 98 is because it's so widely ignored.\n \nreply",
      "It astounds me that there's no non-invasive local solution to go to my router or whatever other appliances web page without my browser throwing warnings and calling it evil. Truly a fuck up(purposeful or not) by all involved in creating the standards. We need local TLS without the hoops.\n \nreply",
      "All my personal and professional feelings aside (they are mixed) it would be fascinating to consider a subnet based TLS scheme. Usually I have to bang on doors to manage certs at the load balancer level anyway.\n \nreply",
      "I wonder what this would look like: for things like routers, you could display a private root in something like a QR code in the documentation and then have some kind of protocol for only trusting that root when connecting to the router and have the router continuously rotate the keys it presents.\n \nreply",
      "Yeah, what they'll do is put a QR code on the bottom, and it'll direct you to the app store where they want you to pay them $5 so they can permanently connect to your router and gather data from it. Oh, and they'll let you set up your WiFi password, I guess.That's their \"solution\".\n \nreply",
      "It makes the system more reliable and more secure for everyone.I think that's a big win.The root reason is that revocation is broken, and we need to do better to get the security properties we demand of the Web PKI.\n \nreply"
    ],
    "link": "https://www.digicert.com/blog/tls-certificate-lifetimes-will-officially-reduce-to-47-days",
    "first_paragraph": "\nDigiCert ONE Platform\n\n\nSolutions\n\n\nTLS/SSL Certificates\n\n\nDocument Signing Certificates\n\n\nCode Signing Certificates\n\n\nS/MIME Email Certificates\n\n\nMark Certificates\n\n\nDNS\n\n\nEU (eIDAS)\n\n\nPKIoverheid\n\n\nAbout DigiCert\n\n\r\n   Resource Center \r\n   \n\r\n   Insights \r\n   \nJoin industry luminaries for World Quantum Readiness Day.\r\n   Support \r\n   \n\r\n   Tools \r\n   \n\r\n   Resources \r\n   \nThe CA/Browser Forum has officially voted to amend the TLS Baseline Requirements to set a schedule for shortening both the lifetime of TLS certificates and the reusability of CA-validated information in certificates. The first user impacts of the ballot take place in March 2026.The ballot was long debated in the CA/Browser Forum and went through several versions, incorporating feedback from certificate authorities and their customers. The voting period ended on April 11, 2025, closing one hotly contested chapter and allowing the certificate world to plan for what comes next.The new ballot targets certificate validi"
  },
  {
    "title": "Query Engines: Push vs. Pull (2021) (justinjaffray.com)",
    "points": 28,
    "submitter": "tanelpoder",
    "submit_time": "2025-04-14T06:15:59 1744611359",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://justinjaffray.com/query-engines-push-vs.-pull/",
    "first_paragraph": "People talk a lot about \u201cpull\u201d vs. \u201cpush\u201d based query engines, and it\u2019s\npretty obvious what that means colloquially, but some of the\ndetails can be a bit hard to figure out.Important people clearly have thought hard about this distinction, judging by this paragraph from Snowflake\u2019s Sigmod paper:Push-based execution refers to the fact that relational operators push their\nresults to their downstream operators, rather than waiting for these\noperators to pull data (classic Volcano-style model). Push-based execution\nimproves cache efficiency, because it removes control flow logic from tight\nloops. It also enables Snowflake to efficiently process DAG-shaped plans, as\nopposed to just trees, creating additional opportunities for sharing and\npipelining of intermediate results.And\u2026that\u2019s all they really have to say on the matter.\nIt leaves me with two major unanswered questions:In this post, we\u2019re going to talk about some of the philosophical differences\nbetween how pull and push based query eng"
  },
  {
    "title": "Breaking the Llama Community License (victor.earth)",
    "points": 112,
    "submitter": "mkl",
    "submit_time": "2025-04-13T22:15:08 1744582508",
    "num_comments": 63,
    "comments_url": "https://news.ycombinator.com/item?id=43676254",
    "comments": [
      "This is why I've always considered the weights-vs-source debate to be an enormous red herring that skips the far more important question: are the weights actually \"Open\" in the first place?If Llama released everything that the most zealous opponents of weights=source demand they release under the same license that they're currently offering the weights under, we'd still be left with something that falls cleanly into the category of Source Available. It's a generous Source Available, but removes many of the freedoms that are part of both the Open Source and Free Software Definitions.Fighting over weights vs source implicitly cedes the far more important ground in the battle over the soul of FOSS, and that will have ripple effects across the industry in ways that ceding weights=source never would.\n \nreply",
      "I don't think most people in the weights-vs-source debate misunderstands this, it's just the the current \"open-source\" models for the most part do not even meet the bar of source-available, so talking about if the license is actually Open is not the current discussion.\n \nreply",
      "See, but my point is that this is putting the cart before the horse. The \"Open\" in \"Open Source\" is what matters most by far, the same way that the \"Free\" in \"Free Software\" is the key word that qualifies the kind of software we're taking about.Once we've resolved the problem of using the word \"Open\" incorrectly I'm happy to have a conversation about what should be the preferred form for modification (i.e. source) of an LLM. But that is the less important and far more esoteric discussion (and one about which reasonable people can and do disagree), to the point where it's merely a distraction from the incredibly meaningful and important problem of calling something \"Open Source\" while attaching an Acceptable Use policy to it.\n \nreply",
      "> The \"Open\" in \"Open Source\" is what matters most by far, the same way that the \"Free\" in \"Free Software\" is the key word that qualifies the kind of software we're taking about.I don't think this is true. If someone said \"look, my software is open source\" and by \"source\" they meant the binary they shipped, the specific definition of \"open\" they chose to use would not matter much for the sort of things I'd like to do with an open source project. Both are important.\n \nreply",
      "I agree that both matter, but one is more important than the other.If they released the binary as \"Open Source\" but had a long list of things I wasn't allowed to do with it, the fact that they didn't release the source code would be of secondary concern to the fact that they're calling it \"Open\" while it actually has a trail of legal landmines waiting to bite anyone who tries to use it as free software.And that's with a clear cut case like a binary-only release. With an LLM there's a lot of room for debate about what counts as the preferred form for making modifications to the work (or heck, what even counts as the work). That question is wide open for debate, and it's not worth having that debate when there's a far more egregious problem with their usage.\n \nreply",
      "The catch is that the benefits of open vs non-open don't translate neatly from software to models. If software is binary-only, is it exceedingly difficult to change it in any kind of substantial way (you can change the machine code directly, of course, but the very nature of the format makes this very limited). OTOH with a large language model with open weights but without open training data - the closest equivalent to open source for software - you can still change its behavior very substantially with finetuning or remixing layers (from different models even!).\n \nreply",
      "It gets even weirder with Llama 4: https://www.llama.com/llama4/use-policy/ [Update: Apparently this has been the case since 3.2!]> With respect to any multimodal models included in Llama 4, the rights granted under Section 1(a) of the Llama 4 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.This is especially strange considering that Llama 3.2 also was multimodal, yet to my knowledge there was no such restriction.In any case, at least Huggingface seems to be collecting these details now \u2013 see for example https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Inst...Curious to see what Ollama will do.\n \nreply",
      "Technically, those were there since 3.2's Use Policy. I've summarized the changes of the license here: https://notes.victor.earth/how-llamas-licenses-have-evolved-...\n \nreply",
      "Oh, I must have compared the license of 3.2 to the usage policy of 4 then or made some other error; I was convinced this was a new restriction!Thank you, also for that article \u2013 the tabular summary of changes across the two is great!\n \nreply",
      "Super weird.Any idea what standing Meta/Llama thinks they have when they write stuff like that?Is it copyright law? Do they think llama4 is copyright them? Is it something else?\n \nreply"
    ],
    "link": "https://notes.victor.earth/youre-probably-breaking-the-llama-community-license/",
    "first_paragraph": "If you're distributing or redistributing a LLM model that is under the \"Llama 3.3 Community License Agreement\", you might be breaking at least one of the terms you've explicitly/implicitly agreed to.All of the Llama models and their derivatives (fine-tunes etc) of them is are covered by a Llama Community License.The feeling I get from the ML/AI community right now, is that almost no one actually reads and follows the various license agreements they agree to follow, when they use models like Meta's Llama.Llama is marketed as a \"open source\" model yet Meta themselves also calls it \"proprietary\" in the license text and have a lot of conditions that aren't compatible with open source. If you trusted the marketing to be true and have an existing understanding of \"open source\", it's possible you've been making assumptions about the license which aren't true in reality.I'll try to go through some of the points from the license that I think are the most likely to have been missed, in an effort"
  },
  {
    "title": "Unix files have (at least) two sizes (utcc.utoronto.ca)",
    "points": 25,
    "submitter": "ingve",
    "submit_time": "2025-04-14T10:18:12 1744625892",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=43679764",
    "comments": [
      "symbolic links in ext4 can be stored in the inode data, meaning zero bytes in the file representing the symbolic link itself (of course, the inode data is bigger as a result(?))small files in btrfs can be stored in the metadata blocks instead of data blocks\n \nreply",
      "Many years ago I looked after a Novell cluster of three hosts with a rather expensive FC connected array.  So what - that's pretty normal?It was the early noughties and a TB was expensive.  I wrote a spreadsheet with inputs from the Novell .ocx jobbies.  The files were stored on some Novell NSS volu.mes.I was able to show all states of the files and aggregate stats too.Nowadays a disc is massive and worrying about compression is daft\n \nreply",
      "The dilemma as old as stacker and {double,drive}space. This is just a leaking abstraction. Leave it be cause there's no good solution for it. The best place for zfs-aware backup code is in a backup tool that cares.\n \nreply",
      "It's not just UNIX. The cluster size (block size in UNIX) is the smallest unit of size any file system can reference when accessing disk storage.\n \nreply",
      "On macOS you can see both if you do Get Info on a file\n \nreply",
      "> ZFS opts to report the physical block size of the file, which is probably the more useful number for the purposes of things like 'du'. However, it does leave us with no way of finding out the logical block sizeIf the filesystem is tantamount to a big fat .zip, then perhaps there is a requirement for a manifest of the logical file sizes somewhere.Somehow I doubt that this problem hasn't already been solved.\n \nreply"
    ],
    "link": "https://utcc.utoronto.ca/~cks/space/blog/unix/UnixFilesTwoSizes",
    "first_paragraph": " You're probably reading this page because you've attempted to\naccess some part of my blog (Wandering\nThoughts) or CSpace, the wiki thing it's\npart of. Unfortunately whatever you're using to do so has a HTTP\nUser-Agent header value that is too generic or otherwise excessively\nsuspicious. Unfortunately, as of early 2025 there's a plague of\nhigh volume crawlers (apparently in part to gather data for LLM\ntraining) that behave like this. To reduce the load on Wandering Thoughts I'm experimenting with\n(attempting to) block all of them, and you've run into this.  All HTTP User-Agent headers should clearly identify what they\nare, and for non-browser user agents, they should identify not just\nthe software involved but also who specifically is using that software.\nAn extremely generic value such as \"Go-http-client/1.1\"\nis not something that I consider acceptable any more. "
  },
  {
    "title": "OpenAI o3 and o4-mini (openai.com)",
    "points": 414,
    "submitter": "maheshrijal",
    "submit_time": "2025-04-16T17:01:54 1744822914",
    "num_comments": 385,
    "comments_url": "https://news.ycombinator.com/item?id=43707719",
    "comments": [
      "Ok, I\u2019m a bit underwhelmed. I\u2019ve asked it a fairly technical question, about a very niche topic (Final Fantasy VII reverse engineering): https://chatgpt.com/share/68001766-92c8-8004-908f-fb185b7549...With right knowledge and web searches one can answer this question in a matter of minutes at most. The model fumbled around modding forums and other sites and did manage to find some good information but then started to hallucinate some details and used them in the further research. The end result it gave me was incorrect, and the steps it described to get the value were totally fabricated.What\u2019s even worse in the thinking trace it looks like it is aware it does not have an answer and that the 399 is just an estimate. But in the answer itself it confidently states it found the correct value.Essentially, it lied to me that it doesn\u2019t really know and provided me with an estimate without telling me.Now, I\u2019m perfectly aware that this is a very niche topic, but at this point I expect the AI to either find me a good answer or tell me it couldn\u2019t do it. Not to lie me in the face.Edit: Turns out it\u2019s not just me: https://x.com/transluceai/status/1912552046269771985?s=46\n \nreply",
      "Have you asked this same question to various other models out there in the wild? I am just curious if you have found some that performed better. I would ask some models myself, but I do not know the proper answer, so I would probably be gullible enough to believe whatever the various answers have in common.\n \nreply",
      "Compare to Gemini Pro 2.5:https://g.co/gemini/share/c8fb1c9795e4Of note, the final step in the CoT is:> Formulate Conclusion: Since a definitive list or count isn't readily available through standard web searches, the best approach is to: state that an exact count is difficult to ascertain from readily available online sources without direct analysis of game files ... avoid giving a specific number, as none was reliably found across multiple sources.and then the response is in line with that.\n \nreply",
      "I've used AI with \"niche\" programming questions and it's always a total let down. I truly don't understand this \"vibe coding\" movement unless everyone is building todo apps.\n \nreply",
      "It's incredible when I ask Claude 3.7 a question about Typescript/Python and it can generate hundreds of lines of code that are pretty on point (it's usually not exactly correct on first prompt, but it's coherent).I've recently been asking questions about Dafny and Lean -- it's frustrating that it will completely make up syntax and features that don't exist, but still speak to me with the same confidence as when it's talking about Typescript. It's possible that shoving lots of documentation or a book about the language into the context would help (I haven't tried), but I'm not sure if it would make up for the model's lack of \"intuition\" about the subject.\n \nreply",
      "It can imitate its creator. We reached AGI.\n \nreply",
      "How would it ever know the answer it found is true and correct though? It could as well just repeat some existing false answer that you didn't yet find on your own. That's not much better than hallucinating it, since you can't verify its truth without finding it independently anyway.\n \nreply",
      "Interesting... I asked o3 for help writing a flake so I could install the latest Webstorm on NixOS (since the one in the package repo is several months old), and it looks like it actually spun up a NixOS VM, downloaded the Webstorm package, wrote the Flake, calculated the SHA hash that NixOS needs, and wrote a test suite. The test suite indicates that it even did GUI testing- not sure whether that is a hallucination or not though. Nevertheless, it one-shotted the installation instructions for me, and I don't see how it could have calculated the package hash without downloading, so I think this indicates some very interesting new capabilities. Highly impressive.\n \nreply",
      "Thats so different from my experience. I tried to have it switch a flake for a yarn package that works to npm and after 3 tries with all the hints I could give it it couldn\u2019t do it\n \nreply",
      "If it can write a nixos flake it's significantly smarter than the average programmer. Certainly smarter than me, one-shotting a flake is not something I'll ever be able to do \u2014 usually takes me about thirty shots and a few minutes to cool off from how mad I am at whoever designed this fucking idiotic language. That's awesome.\n \nreply"
    ],
    "link": "https://openai.com/index/introducing-o3-and-o4-mini/",
    "first_paragraph": ""
  },
  {
    "title": "OneSignal (YC S11) Is Hiring Engineers (onesignal.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-04-16T21:00:11 1744837211",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://onesignal.com/careers",
    "first_paragraph": "Connect with a customer engagement expert to learn moreConnect with a customer engagement expert to learn more\nView Open Roles\n\u201cAs we approach 2 million users of our platform around the world, OneSignal is still at just the start of our journey. Over the coming years, we aspire to become one of the most valuable technology companies in the world.\u201dGeorge DeglinCo-founder & CEO\n   To reach our ambitious goals, it takes calculated risks. Push yourself to try new things and take on new challenges with the support of your team\n  \n   We thrive seeing the success of OneSignal users, and we pride ourselves on being a customer-centric organization that truly listens to feedback\n  \n   We focus on hiring great people so we can empower you with the flexibility to set your own goals, own your work, and be recognized for it\n  \n   We hold ourselves and our product to high standards and are trusted for our unparalleled messaging efficiency, reliability and speed\n  \n   We take work seriously, but that "
  },
  {
    "title": "Kermit: A typeface for kids (microsoft.design)",
    "points": 251,
    "submitter": "nmcfarl",
    "submit_time": "2025-04-16T12:57:13 1744808233",
    "num_comments": 133,
    "comments_url": "https://news.ycombinator.com/item?id=43704904",
    "comments": [
      "As someone teaching their 4 year old to read right now, I don't buy it. The text is long on \"friendly\" and random stuff like that, but that's not what I'm looking for in a font for kids.Just off the top of my head the \"v\" in there doesn't have a point on the bottom, which is one of the confusions my daughter has (\"u\" vs \"v\"). And I don't think the \"n\" needs the serif on the right foot, as that's not the \"platonic\" shape of a lower case N. I do appreciate that their lower case \"a\" is more like a handwritten one, as is the lower case \"g\".I've been going through the Teach Your Child to Read[0] book, and it introduces a \"learner-friendly\" font, which actually helps. It has special glyphs for \"th\", for example, and other font tricks like making silent letters smaller, and different variants for the vowels depending on their sound. Eventually, those tricks are minimized and the kid is reading a normal font, though.In other words, I'm interested in the idea of a font that's useful for early readers, but this font doesn't seem to be concretely designed in that way, and I'm put off by the vague \"friendly\" type stuff it seems to be focusing on.[0] https://www.amazon.com/Teach-Your-Child-Read-Lessons/dp/0671...\n \nreply",
      "Stroke 6 of the \"r\" is weird in that it is making an upward stroke rather than a down stroke. I guess that this still grates after those years learning calligraphy with pens that would not work trying to draw up. All strokes were made with a downward/pulling motion. Pushing a pen like that just didn't work.\n \nreply",
      "e m and t all have the same motion.\n \nreply",
      "At least the small letter \"a\" appears as it would when written by hand. All fonts that add the \"hanger hook\" on top of the small \"a\" irritate me.\n \nreply",
      "the \"serif\" on the \"n\" is called an \"exit stroke\". You often find lots of glyphs that get an exit stroke (the \"l\" and the \"i\" come to mind, but it is most glyphs that have a single vertical stem, or on the right most vertical stem) when you get the italic version of the typeface.\n \nreply",
      "> unpublished study is finding that adding prosody to text improves children\u2019s comprehension.As a dyslexic software engineer who knows by heart a good number of the 50 tables in the open font type specification, I'd like to look into this in more detail but there is no code or paper published about this (yet).In the mean time, it would be nice for people stop using dyslexics as an excuse to motivate for their own special interests. I've suffered my entire formative years under this low-key Munchausen by proxy from all sort of educators gass-lighting me into believing I should use some technology that in the fullness of time proved to be counter productive.But ok, the variable speed HOI animation looks cool, I'll give you that.\n \nreply",
      "As a former teacher who's done original research in educational psychology, I'd like to add that educational psychology is just a grab-bag of weak correlations whose discovery was motivated by, 'When I was a teacher, I saw ______ and that made me sad.' Any 'theory' is a just-so story that the researcher assembled from ideas they found aesthetically pleasing. It's not science; it's activity without achievement, because the individual pieces of research can't be assembled into a coherent body of knowledge.The typeface looks nice though.\n \nreply",
      "School administrators sometimes implement the stupidest policies based on correlations of various strengths. But even a strong correlation might have nothing to do with causation.E.g.: A school my wife used to work at is requiring all 8th graders to take algebra (normally a high-school-level class in the US) regardless of math aptitude because some study shows that 8th graders who take algebra have improved outcomes. Nevermind the fact that this is almost certainly because kids who are already good at math will both take algebra AND have improved outcomes.\n \nreply",
      "Continuing with the logic of that school, most wildly successful people were bullied at school.....\n \nreply",
      "And many of them lost a parent at an early age.\n \nreply"
    ],
    "link": "https://microsoft.design/articles/introducing-kermit-a-typeface-for-kids/",
    "first_paragraph": ""
  },
  {
    "title": "Astronomers Detect a Possible Signature of Life on a Distant Planet (nytimes.com)",
    "points": 68,
    "submitter": "julienchastang",
    "submit_time": "2025-04-16T23:20:31 1744845631",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=43711376",
    "comments": [
      "This is about dimethyl sulfide in the atmosphere of exoplanet K2-18b:https://en.wikipedia.org/wiki/K2-18bThe NYT article reports a new study in The Astrophysical Journal and links to it, but the DOI is currently not found:https://dx.doi.org/10.3847/2041-8213/adc1c8I also don't see the article yet under their Latest Articles:https://iopscience.iop.org/journal/0004-637XHere are recent articles about K2-18b from Google Scholar:https://scholar.google.com/scholar?hl=en&as_sdt=0,48&q=%22K2...\n \nreply",
      "Those DOIs have a different prefix, I think it might be in Lettershttps://iopscience.iop.org/journal/2041-8205It's still not out yet though. Also journals are often rather tardy or just straight up don't register the doi at all and still put it on their site, but maybe this is an embargo thing.edit - is it possibly this after publishing? https://arxiv.org/abs/2501.18477\n \nreply",
      "Good point about the DOI prefix. I don't think it's that arXiv article, though, since it says there is no statistically significant evidence for carbon dioxide or dimethyl sulfide. This NYT article (and presumably whatever journal article it's based upon) reports high levels of dimethyl sulfide.\n \nreply",
      "You're right, different authors too.https://www.eurekalert.org/multimedia/1069012 uses the DOI in the article and says it's from> A. Smith, N. Madhusudhan (University of Cambridge)Here's another article on it https://www.bbc.co.uk/news/articles/c39jj9vkr34oI think this is still under embargo, or hasn't quite been released. There's going to be some time required for visuals like on the bbc site.\n \nreply",
      "If life evolves on a planet with only oceans, no surface, imagine how much longer it would take to discover rockets that can leave the planet.Like if there was no surface on earth, and only fish, there must be some very significant reason for advanced fish to even want to leave the water, let alone the atmosphere\n \nreply",
      "That seems like a very landbased mindset.  From a high level, what is an ocean but a thick atmosphere?  I could even imagine an underwater culture would be quicker to explore because they would surely discover the surface of the ocean quicker than we discovered the concept of the atmosphere and that innately leads to the questions of whether the atmosphere has a \"surface\" and what is above it.\n \nreply",
      "There is also the issue that they will likely never discover fire and thus chemistry and metallurgy.\n \nreply",
      "> From a high level, what is an ocean but a thick atmosphere?\"Now you have two atmospheres.\"\n \nreply",
      "Ha. Ha ha. Nice.\n \nreply",
      "Overcoming water->land barrier seems comparable to overcoming land->air and air->outer space in the context of rocket launches.\n \nreply"
    ],
    "link": "https://www.nytimes.com/2025/04/16/science/astronomy-exoplanets-habitable-k218b.html",
    "first_paragraph": ""
  },
  {
    "title": "eInk Mode: Making web pages easier to read (jackscogito.blogspot.com)",
    "points": 79,
    "submitter": "amadeuspagel",
    "submit_time": "2025-04-15T09:52:18 1744710738",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=43690828",
    "comments": [
      "Okay, so I started reading this and got excited to see what this was all about. I've been wanting a web browser that can turn the \"regular web\" (as much as possible) into something more like an e-book that happens to have hyperlinks.Essentially, imagine Reader Mode, but all the time.But this is... something else? I tried scrolling to the bottom and as soon as I click on the scroll bar, I get a pop-up showing a bunch of gestures and can no longer scroll. Pressing the back button does nothing. Closing the tab asks me if I want to discard my unsaved changes?!So I'm guessing this is some CSS/JS to make a regular site _behave_ something like an e-reader? Which may be fine as far as it goes, I just don't think it's a good idea to hijack the scroll\n \nreply",
      "I want this too! I discovered a really esotieric way to get this: https://duncanlock.net/blog/2024/01/04/super-fast-reader-mod... - but it would be really nice if there was just a way to switch reader mode on permenantly in modern browsers. Apparently Safari can do that, but afaik no other browsers support it.I really wish browsers were more modular - if the rendering engine and network fetching were easily separable - and you could insert your own steps into that pipeline, you could do all sorts of neat stuff. If I could insert some code of my own in between the fetch and the render, that would be ideal - strip out all ads and malware, optionally remove all scripts or css, run it through readability, etc...\n \nreply",
      "While looking for a typeface for my terminal, I happened upon the Braille Institute's website (https://www.brailleinstitute.org/) which exemplifies more or less what you're describing. It is an aesthetically-pleasing site, with low visual and cognitive loads.Granted, that is an example of a site and not a browser, but I would love it if a browser could magically transform websites to look like the Braille Institute's, where visual and cognitive accessibility are first-class citizens in the UX.\n \nreply",
      "Do you happen to know how the Braille website's style is called? The high contrast, button shadows with big offsets. I've seen it in several places now and would like to use it myself\n \nreply",
      "Probably not it, but it reminds me of some websites that have been described as \"brutalist\". See: https://web.archive.org/web/20250404083913/https://brutalist...\n \nreply",
      "There may be good ideas in here, particularly for the e-reader users, but it's a terribly unfriendly web site from a mouse / keyboard. Having no (apparent) way to escape eInk mode is really not a great user experience.And I, too, fell into the eInk mode by clicking on the scrollbar. Very odd design decisions there.I couldn't figure out how to go \"up\" in pages. Pressing \"Home\" didn't do anything. So a lot of web browser / mouse / keyboard functionality is thrown out in that mode, which shouldn't be necessary to still be a good e-reader mode.\n \nreply",
      "> Essentially, imagine Reader Mode, but all the time.You can tell Safari to use Reader Mode by default on all websites.On iOS 18: Settings > Apps > Safari > Reader > All Websites.On macOS 15: Safari > Settings\u2026 > Websites > Reader > When visiting other websites.\n \nreply",
      "I had a similar idea to use the ibtegrated browser in my ebook.. maybe is useful to someone else https://ebookmode.dropdatabase.es/\n \nreply",
      "When I browsed away by typing another site name in the address bar i got the 'your information may not be saved' firefox popup twice. Didn't see any forms...\n \nreply",
      "I find the wording ambiguous, but this:> Eink mode is not a closed-file format reader but rather a form of responsive web design (RWD) integrated into the website itself.suggests to me that it's a display mode you have to enable in the CSS design of the site, like \"print\" layout. I.e. this particular version is not software you can use on any site, it has to be baked into the site.\n \nreply"
    ],
    "link": "https://jackscogito.blogspot.com/2025/04/e-ink-mode-making-web-pages-easier-to.html",
    "first_paragraph": "\n\u601d\u8003\u3001\u5206\u4eab\u3001\u5b58\u5728\u3002Think, Share and Be.\n\u5982\u679c\u60a8\u559c\u6b61\u6211\u5beb\u7684\u6587\u7ae0\uff0c\u53ef\u4ee5\u8d0a\u52a9\u6211\u4e00\u676f\u5496\u5561\uff0c\u7d66\u6211\u66f4\u591a\u5275\u4f5c\u7684\u52d5\u529b\uff0c\u8b1d\u8b1d\u60a8\u7684\u652f\u6301\uff01"
  },
  {
    "title": "Nintendo Bled Atari Games to Death (mitpress.mit.edu)",
    "points": 165,
    "submitter": "sohkamyung",
    "submit_time": "2025-04-16T12:33:33 1744806813",
    "num_comments": 124,
    "comments_url": "https://news.ycombinator.com/item?id=43704596",
    "comments": [
      "This article ignores the fact that aside from being barred with manufacturing unlicensed NES games, Atari also failed to compete with any of its subsequent consoles after the VCS (although it did have some success with its PCs). The consoles were all flawed in some way. They were underpowered, didn't offer much over the previous iteration, or simply didn't have a strong enough library of games to compete. Atari was famously slow to realize that maybe people want more out of a game console than home ports of decade-old arcade games. On top of that, their original games that weren't home ports were mostly lackluster or were just outside of what gamers of the time were demanding.Hard to say that Nintendo putting the kibosh on one arm of Atari's business \"bled them to death\" when all their other arms were bleeding from self-inflicted wounds.EDIT: As pointed out below, I have mixed up Atari Corporation and Atari Games, so not all my criticism stands. Atari Games, publishing as Tengen, still largely put out ports of arcade games, but they were at least contemporary arcade games.\n \nreply",
      "You seem to be confused (which is fair, this is a little confusing). In 1984, Warner Communications sold Atari's home and computer game division to Jack Tramiel, which became Atari Corporation. Atari Corporation was the company that made all the future Atari consoles (7800, Jaguar, etc) and computers (ST line). Atari Games, Atari's arcade game division, remained with Warner. This article is entirely about Atari Games, who had nothing to do with anything sold for the home market with the Atari name. They were entirely separate companies. The reason why they did business as Tengen was that as part of the split, Atari Games wasn't allowed to sell games to the home market using the Atari name.I will say that the article is a bit inaccurate at the end. Atari Games kept using the Tengen name for several years after the lawsuit for publishing games on the Genesis. They only stopped in 1994 when Warner consolidated all of its game related brands under the \"Time Warner Interactive\" name.\n \nreply",
      "Prior to the Warner / Tramiel sale, though, Atari management showed a stunning lack of foresight re: the lifecycle of their console platforms. If I recall properly, I've heard Al Alcorn (and / or perhaps Joe Decuir) talk about how the technical people pitched VCS as a short-lived platform, but management kept the product going far beyond its intended lifetime.The 5200 was released in 1982, built on 1979 technology. The Famicom was released in Japan in 1983 but didn't make it to the United States until 1986. If Atari had made better controller decisions with the 5200, and perhaps included 2600 compatibility, I think Nintendo would have had a much harder row to hoe when they came to the US.Then again, if Atari had taken Nintendo's offer to distribute the NES in the US...(Some people write speculative fiction about world wars having different outcomes. My \"The Man in the High Castle\" is to wonder about what the world would have been like if Jack Tramiel hadn't been forced out of Commodore, if the Amiga went to Atari, etc.)\n \nreply",
      "atari marketing was pretty f---ing terrible. objectively soi had one of the home computer division marketing types come to my office one day, and was asked:\"can you print out all possible 8x8 bitmaps? we'd like to submit them to the copyright office so no one else can use them\"a stunning lack of knowledge of copyright law and basic exponential math. i didn't bother to point out that he really wanted all possible 8x8 _color_ bitmaps (there aren't enough atoms in the universe for this, by many orders of magnitude)they didn't make very good decisions about consoles or computers, either\n \nreply",
      "Atari made a lot of bad decisions, but what you were asked is not something you should expect someone in marketing to understand in general.  There is only so much someone can get good at in their lifetime and so eventually you will have to give up understanding everything - and then look like an idiot when you ask for something that is obviously unreasonable to someone who does know.What was asked for is a reasonable ask.  It just isn't possible to create.\n \nreply",
      "Agreed that a marketer can\u2019t be expected to know the math. But is it really reasonable to attempt to copyright every possible 8x8 bitmap?\n \nreply",
      "> What was asked for is a reasonable ask.No it isn't. You don't get any copyright protection on a volume of data produced by rules, such as \"every possible 8x8 bitmap\". Furthermore, you also don't get copyright protection against \"copies\" that were developed without reference to your work, as would always be the case for this idea. So there is no theoretical benefit from attempting it.What's the reasonable part?\n \nreply",
      "Asking if you can print all 8x8 bitmaps is very reasonable.Wanting to copyright them to block competition is despicable.\n \nreply",
      "I'm shocked at how \"few\" pages printing all 8x8 bitmaps would actually require. Assuming full page coverage of an 8.5 x 11 sheet at 600 dpi I'm only coming with a touch over 548 billion pages. I expected it to be more. Legal-size paper drops that to about 430.5 billion pages.\n \nreply",
      "I think your math is a little off (or maybe mine is).I'll take a short cut and imagine that you have an 8x8 square with no margins (68% of a borderless 8.5x11), then you have a grid of 600x600 bitmaps, which is 3.6e5. if each pixel is only black or white, than you have 1.8e19 possible bitmaps (64-bit), divide the two and you have 5e13, or about 50 trillion pages.  Fix the equation, and you get a grid of 5.2e5, for 30 trillion pages instead of 50.However, bring that up to 24-bit color or more (even 8-level greyscale is e154), and the exponentiality of the problem goes back to as described by the OP\n \nreply"
    ],
    "link": "https://thereader.mitpress.mit.edu/how-nintendo-bled-atari-games-to-death/",
    "first_paragraph": "In July 2024, a new company called Tengen Games released its first game, \u201cZed and Zee,\u201d for the Nintendo Entertainment System (NES). The surprising part of this story is not the release of a new \u201chomebrew\u201d game for a system released in 1985 \u2014 hobbyist computing has been visible since at least the 1970s \u2014 but that Tengen and its parent company, Atari Games, had disappeared 30 years ago after being crushed in court by Nintendo for doing exactly the same thing: manufacturing unauthorized cartridges for the NES.This story isn\u2019t just a curiosity \u2014 it highlights how the gaming industry, like many creative fields, is defined as much by legal and business decisions as by artistic vision. Behind every major shift, there\u2019s a dance between engineers, lawyers, and business leaders, visible only to a few insiders. Nintendo\u2019s lawyers, more than Mario, made Nintendo. Atari\u2019s lawyers, more than ET \u2014 notoriously the worst game of all time \u2014 sealed its downfall.To illustrate these points, let\u2019s take a w"
  },
  {
    "title": "RakuAST Grant Report (niner.name)",
    "points": 38,
    "submitter": "librasteve",
    "submit_time": "2025-04-16T18:55:12 1744829712",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=43709042",
    "comments": [
      "awesome job by niner\n \nreply",
      "Fascinating write-up on RakuAST. Converting legacy code to manipulate the AST directly is a significant step towards better tooling and metaprogramming. Curious to see how the macros implementation evolves and whether performance gains live up to expectations once fully integrated.\n \nreply"
    ],
    "link": "https://niner.name/blog/rakuast_grant_report/index.html",
    "first_paragraph": "2025-04-10 12:18:00The RakuAST project was a rewrite and redesign of the compiler frontend, i.e. the part that parses the original source code, figures out what the different parts are, does checks and optimizations and creates a low level representation which will then be turned into bytecode by the backend.\u00a0When I applied for the grant a lot of basic infrastructure was already in place. Many simple programs would already run, e.g. you could define variables, classes, functions, create objects, call methods and a lot more.\u00a0However Raku is a large language. E.g. there's not just methods. There are also private methods, meta methods, methods with fully qualified names (i.e. when you want a method of a specific base class), method calls where the name is determined at compile time (e.g. $foo.\"$bar\"()), hypered method calls and even more obscure ways of calling code. These were all still left to do. The same was true for all other areas of the compiler.\u00a0My method for attacking this was si"
  }
]