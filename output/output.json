[
  {
    "title": "I want everything local \u2013 Building my offline AI workspace (instavm.io)",
    "points": 448,
    "submitter": "mkagenius",
    "submit_time": "2025-08-08T18:19:05 1754677145",
    "num_comments": 137,
    "comments_url": "https://news.ycombinator.com/item?id=44840013",
    "comments": [
      "I'm constantly tempted by the idealism of this experience, but when you factor in the performance of the models you have access to, and the cost of running them on-demand in a cloud, it's really just a fun hobby instead of a viable strategy to benefit your life.As the hardware continues to iterate at a rapid pace, anything you pick up second-hand will still deprecate at that pace, making any real investment in hardware unjustifiable.Coupled with the dramatically inferior performance of the weights you would be running in a local environment, it's just not worth it.I expect this will change in the future, and am excited to invest in a local inference stack when the weights become available. Until then, you're idling a relatively expensive, rapidly depreciating asset.reply",
      "I think the local LLM scene is very fun and I enjoy following what people do.However every time I run local models on my MacBook Pro with a ton of RAM, I\u2019m reminded of the gap between local hosted models and the frontier models that I can get for $20/month or nominal price per token from different providers. The difference in speed and quality is massive.The current local models are very impressive, but they\u2019re still a big step behind the SaaS frontier models. I feel like the benchmark charts don\u2019t capture this gap well, presumably because the models are trained to perform well on those benchmarks.I already find the frontier models from OpenAI and Anthropic to be slow and frequently error prone, so dropping speed and quality even further isn\u2019t attractive.I agree that it\u2019s fun as a hobby or for people who can\u2019t or won\u2019t take any privacy risks. For me, I\u2019d rather wait and see what an M5 or M6 MacBook Pro with 128GB of RAM can do before I start trying to put together another dedicated purchase for LLMs.reply",
      "I agree and disagree. Many of the best models are open source, just too big to run for most people.And there are plenty of ways to fit these models! A Mac Studio M3 Ultra with 512 GB unified memory though has huge capacity, and a decent chunk of bandwidth (800GB/s. Compare vs a 5090's ~1800GB/s). $10k is a lot of money, but that ability to fit these very large models & get quality results is very impressive. Performance is even less, but a single AMD Turin chip with it's 12-channels DDR5-6000 can get you to almost 600GB/s: a 12x 64GB (768GB) build is gonna be $4000+ in ram costs, plus $4800 for for example a 48 core Turin to go with it. (But if you go to older generations, affordability goes way up! Special part, but the 48-core 7R13 is <$1000).Still, those costs come to $5000 at the low end. And come with much less token/s. The \"grid compute\" \"utility compute\" \"cloud compute\" model of getting work done on a hot gpu with a model already on it by someone else is very very direct & clear. And are very big investments. It's just not likely any of us will have anything but burst demands for GPUs, so structurally it makes sense. But it really feels like there's only small things getting in the way of running big models at home!Strix Halo is kind of close. 96GB usable memory isn't quite enough to really do the thing though (and only 256GB/s). Even if/when they put the new 64GB DDR5 onto the platform (for 256GB, lets say 224 usable), one still has to sacrifice quality some to fit 400B+ models. Next gen Medusa Halo is not coming for a while, but goes from 4->6 channels, so 384GB total: not bad.(It sucks that PCIe is so slow. PCIe 5.0 is only 64GB/s one-direction. Compared to the need here, it's no-where near enough to have a big memory host and smaller memory gpu)reply",
      "> Many of the best models are open source, just too big to run for most people.You can find all of the open models hosted across different providers. You can pay per token to try them out.I just don't see the open models as being at the same quality level as the best from Anthropic and OpenAI. They're good but in my experience they're not as good as the benchmarks would suggest.> $10k is a lot of money, but that ability to fit these very large models & get quality results is very impressive.This is why I only appreciate the local LLM scene from a distance.It\u2019s really cool that this can be done, but $10K to run lower quality models at slower speeds is a hard sell. I can rent a lot of hours on an on-demand cloud server for a lot less than that price or I can pay $20-$200/month and get great performance and good quality from Anthropic.I think the local LLM scene is fun where it intersects with hardware I would buy anyway (MacBook Pro with a lot of RAM) but spending $10K to run open models locally is a very expensive hobby.reply",
      "The game changer technology that'll enable full 1TB+ LLM models for cheap is  Sandisk's High Bandwidth Flash. Expect devices with that in about 3-4 years, maybe even on cellphones.reply",
      "> Many of the best models are open source, just too big to run for most peopleI don't think that's a likely future, when you consider all the big players doing enormous infrastructure projects and the money that this increasingly demands. Powerful LLMs are simply not a great open source candidate. The models are not a by-product of the bigger thing you do. They are the bigger thing. Open sourcing a LLM means you are essentially investing money to just give it away. That simply does not make a lot of sense from a business perspective. You can do that in a limited fashion for a limited time, for example when you are scaling or it's not really your core business and you just write it off as expenses, while you try to figure yet another thing out (looking at you Meta).But with the current paradigm, one thing seems to be very clear: Building and running ever bigger LLMs is a money burning machine the likes of which we have rarely or ever seen, and operating that machine at a loss will make you run out of any amount of money really, really fast.reply",
      "https://pcisig.com/pci-sig-announces-pcie-80-specification-t...From 2003-2016, 13 years, we had PCIE 1,2,3.2017 - PCIE 4.02019 - PCIE 5.02022 - PCIE 6.02025 - PCIE 7.02028 - PCIE 8.0Manufacturing and vendors are having a hard time keeping up. And the PCIE 5.0 memory is.. not always the most stable.reply",
      "Are you conflating GDDR5x with PCIe 5.0?reply",
      "No.I'm saying we're due for faster memory but seem to be having trouble scaling bus speeds as well (in production) and reliable memory. And the network is changing a lot, too.It's a neverending cycle I guess.reply",
      "One advantage of Apple Silicon is the unified memory architecture. You put memory on the fabric instead of on PCIe.reply"
    ],
    "link": "https://instavm.io/blog/building-my-offline-ai-workspace",
    "first_paragraph": "I want everything local \u2014 no cloud, no remote code execution.That\u2019s what a friend said. That one-line requirement, albeit simple, would need multiple things to work in tandem to make it happen.What does a mainstream LLM (Large Language Model) chat app like ChatGPT or Claude provide at a high level?With so many LLMs being open source / open weights, shouldn't it be possible to do all that locally? But just local LLM is not enough, we need a truely isolated environment to run code as well.So, LLM for chat, Docker to containerize code execution, and finally a browser access of some sort for content.We wanted a system where:The idea was to perform tasks which require privacy to be executed completely locally, starting from planning via LLM to code execution inside a container. For instance, if you wanted to edit your photos or videos, how could you do it without giving your data to OpenAI/Google/Anthropic? Though they take security seriously (more than many), it's just a matter of one slip"
  },
  {
    "title": "Little-known leguminous plant can increase beef production by 60% (2022) (embrapa.br)",
    "points": 46,
    "submitter": "littlexsparkee",
    "submit_time": "2025-08-08T23:20:56 1754695256",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=44842700",
    "comments": [
      "If this has been known for years and nobody's trying to implement it, surely there's a catch?reply",
      "100% accurate. Livestock feed is (like most agriculture) hyper optimized.If they could increase production by 60% with any additive at all, it would immediately see widespread use.People still have this weird view of farming that it's like Johnny Goodguy and his family taking care of a small herd. While that exists still, Johnny is also tracking every input and outcome and optimizing daily.The data collection and use in Ag would astound people.reply",
      "Agreed. Farming is incredibly data driven/cost conscious. For the entire industry to not make this an overnight priority says something about the analysis is missing.reply",
      "the other people are right that there is crazy data collection and optimization from them... however blind spots can exist and outliers could be possible in that kind of environment.reply",
      "\u201cWorked once in a specific tropical pasture\u201d. AFAIK,  most beef production is not in the tropics.reply",
      "South of Brazil is in the tropic of capricorn and it's a beef producing regionreply",
      "Due to direct benefit to herders, this is easier to reinforce than methane reductions with supplement e.g. seaweed as has been posted here before, since those benefits are not captured (without a carbon market or climate-friendly consumer branding to command a premium). Even if beef was methane-free, it would still have a larger impact than chicken (without regenerative agriculture, silviculture etc) but projections show that consumption is going to grow steadily as the global middle class grows so adopting these efficiencies will be important.reply",
      "\"There is resistance among farmers not only because the seeds are expensive, but also because the species used so far, especially Stylosanthes, do not persist when associated with Brachiaria grasses\", Boddey explains. After some time in the field the leguminous plant wanes or dies, and it is necessary to renew the pasture, which entails further costs and work.\"So the farmers did the math and the money doesn't work.Scientists in this article seem very focused on the climate aspect of it while the farmers themselves are going to be focused on the bottom line. Farmers are not going to entail extra costs if they don't have to much the same as any other business owner.reply",
      "You might underestimate the ability to market lower-carbon beef though. Where you see a disincentive, others may see the next \"organic.\"reply",
      "I certainly don't speak for everyone but I don't see that catching on at the individual level. People eat the organic because its objectively healthier. I don't think most people that eat beef would care if its low carbon. I eat a good deal of beef, probably 3 steaks a week personally and another 3 in total for my wife and kids. I'm not going to pay x% more for low carbon. With that said I could absolutely see Europe mandating this and forcing everyone to just pay more for beef. So you're right, if they can target this at governments then they could force it to catch on.reply"
    ],
    "link": "https://www.embrapa.br/en/busca-de-noticias/-/noticia/75361634/little-known-leguminous-plant-can-increase-beef-production-by-60",
    "first_paragraph": " Enter multiple e-mails separated by comma.  Photo: Cl\u00e1udia Rezende  Desmodium ovalifolium is a perennial forage legume of Asian origin. Picture of desmodium flower. \u00a0A study conducted for four years has revealed that the intercropping of Brachiaria brizantha cv. Marandu, also known as marandu grass, with the forage legume\u00a0Desmodium ovalifolium (desmodium) increases the weight of grazing cattle in 60% in comparison with pastures without the use of nitrogen fertilization. \"The introduction of the legume had the same impact of an annual application of 150 kilos of nitrogen fertilizer per hectare in the pasture\", explains\u00a0Robert Boddey, a researcher at\u00a0Embrapa Agrobiology.Newly published in the journal\u00a0Grass & Forage Science, one of the most important in the area of forage cultivation, the study also points out that the use of\u00a0Desmodium ovalifolium can reduce time to slaughter by 30%, which represents less costs for breeders. \"Reducing time to slaughter also entails less enteric methane e"
  },
  {
    "title": "Ultrathin business card runs a fluid simulation (github.com/nicholas-l-johnson)",
    "points": 858,
    "submitter": "wompapumpum",
    "submit_time": "2025-08-08T11:41:04 1754653264",
    "num_comments": 171,
    "comments_url": "https://news.ycombinator.com/item?id=44835879",
    "comments": [
      "Advantages of a business card sized hollow box partially filled with water:  * more realistic fluid motion\n  * cheaper, easier build\n  * easier to debug\n\nDisadvantages:  * risk of wet butt when you sit down\n  * less joy of doing hard thingsreply",
      "Disadvantages:  * excessively fast fluid motion at card scalereply",
      "Very nice, but probably a bit too expensive to just hand out.I knew a chap that had a similar hardware business card (I don't remember exactly what it did, but it wasn't as cool as this one).I remember that his card was pretty scuffed up, and he insisted I give it back, after he handed it to me. Bit weird.reply",
      "Maybe you wouldn\u2019t give em to just anybody, but anybody who gets one is guaranteed to remember you!I\u2019d probably even keep it in my desk to play with. After a few weeks I\u2019d accidentally have this guy\u2019s email/linkedin memorized for eternityreply",
      "I'm just imagining interviewing a candidate for a job involving embedded systems and this dude pulls this out at the end and says \"If you have any other questions, my email's right on there.\"What an absolute baller this guy is.reply",
      "Still has to respond \"no\" on the post interview scorecard because the given solution didn't use the optimal ObscureLeetCodeAlgorithmreply",
      "(This assumes that the guy passed through the resume filters and advanced to the in-person stage, which is not that easy. Should work on a video call though.)reply",
      "I am thinking how most of the people I get business cards from are people I've already invited in to my office and are discussing some potential business relationship. They've often flown to my city, staying in a hotel, paying for transport, meals, etc. The impression they make during that 1 hour meeting is paramount and I think this is certainly going to leave a lasting impression. Most of those business cards just get tossed into a drawer or trash bin, I bet people keep this one on their desk a play around with it.reply",
      "This matches my experience fairly accurately except for the one guy I met at a housewarming who handed out cards to everyone. It was so weird - I haven\u2019t seen anyone do that in real life. He had a shop that repairs chipped windshields.And you know what? About 8 months later my windshield got sprayed by gravel. That guy got the business (he\u2019s a friend of a friend after all, and I had his number in my wallet).I\u2019d say the issue isn\u2019t that cards are outdated. It\u2019s that people aren\u2019t using them correctly.reply",
      "This is true.I remember a story linked from here, recently, where a designer submitted a CV that was a custom-made widget of some kind.He got the job.reply"
    ],
    "link": "https://github.com/Nicholas-L-Johnson/flip-card",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.This repo contains all files related to the flip-card project, which is a business card that runs a fluid-implicit-particle(FLIP) simulation.The PCB design files are in the \"kicad-pcb\" folder. The flip-card project is inspired by mitxela's fluid simulation pendant project https://mitxela.com/projects/fluid-pendantThe fluid simulation logic is contained in a standalone crate, which is in the \"fluid_sim_crate\" folder. This is based off the work by Matthias M\u00fcller (https://github.com/matthias-research) and his excelent demonstrations on his youtube channel \"Ten Minute Physics\"One of the more difficult features was the rechargable battery.  I found a design for a board edge usb-c port from cnlohr's tiny touch lcd project https://"
  },
  {
    "title": "What makes a SuperAger? (northwestern.edu)",
    "points": 9,
    "submitter": "hhs",
    "submit_time": "2025-08-09T00:58:21 1754701101",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44843186",
    "comments": [
      "The title says what makes a superager. But the article does not say why someone has one of the two things that makes them a superager.reply",
      ">\u201cWhat we realized is there are two mechanisms that lead someone to become a SuperAger,\u201d Weintraub said. \u201cOne is resistance: they don\u2019t make the plaques and tangles. Two is resilience: they make them, but they don\u2019t do anything to their brains.\u201dreply"
    ],
    "link": "https://news.northwestern.edu/stories/2025/08/what-makes-a-superager/",
    "first_paragraph": "For 25 years, scientists at Northwestern Medicine have been studying individuals aged 80 and older \u2014 dubbed \u201cSuperAgers\u201d \u2014 to better understand what makes them tick.These unique individuals, who show outstanding memory performance at a level consistent with individuals who are at least three decades younger, challenge the long-held belief that cognitive decline is an inevitable part of aging.Over the quarter-century of research, the scientists have seen some notable lifestyle and personality differences between SuperAgers and those aging typically \u2014 such as being social and gregarious \u2014 but \u201cit\u2019s really what we\u2019ve found in their brains that\u2019s been so earth-shattering for us,\u201d said Dr. Sandra Weintraub, a professor of psychiatry and behavioral sciences and neurology at Northwestern University Feinberg School of Medicine.By identifying biological and behavioral traits associated with SuperAging, the scientists hope to uncover new strategies to promote cognitive resilience and delay or pr"
  },
  {
    "title": "Tor: How a military project became a lifeline for privacy (mitpress.mit.edu)",
    "points": 240,
    "submitter": "anarbadalov",
    "submit_time": "2025-08-08T15:45:27 1754667927",
    "num_comments": 126,
    "comments_url": "https://news.ycombinator.com/item?id=44838378",
    "comments": [
      "I used Tor for surveillance.  But an appropriate kind, IMHO.I used Tor as a small part of one of the capabilities of a supply chain integrity startup.  I built a fancy scraper/crawler to discreetly monitor a major international marketplace (mainstream, not darknet), including selecting appropriate Tor exit nodes for each regional site, to try to ensure that we were seeing the same site content that people from those regions were seeing.Tor somehow worked perfectly for those needs.  So my only big concern was making sure everyone in the startup knew not to go bragging about this unusually good data we had.  Since we were one C&D letter away from not being able to get the data at all.(Unfortunately, this had to be a little adversarial with the marketplace, not done as a data-sharing partnership, since the marketplace benefited from a cut of all the counterfeit and graymarket sales that we were trying to fight.  But I made sure the scraper was gentle yet effective, both to not be a jerk, and also to not attract attention.)(I can talk about it now, since the startup ran out of runway during Covid investor skittishness.)reply",
      "This is not a good way to do this. Tor exit nodes are public and may be marked for special behavior by the marketplace you are surveying. There is no reason to believe you are getting good information this way.The right way to do this would be through a VPN/tor + Residential proxy to hide your intentions from everyone involved.reply",
      "> There is no reason to believe you are getting good information this way.Spot checks checked out.  And it was a perfectly fine way to do it.You are correct that Tor exit nodes often get special handling (at the moment, including by Cloudflare, and by Google Recaptcha).  And the idea of poisoning of data is starting to propagate, due do anti-AI-scraper sentiment.reply",
      "Poisoning the data has been around for at least a decade now. I used to work for a very large pricing analytics company that would track product pricing for Fortune 500 manufacturers.We found on several occasions that some shady retailers would find the CIDR of the manufacturers corp networks and comply with the MAP policies on pricing if traffic came from them. Then when our bot went through with obviously generic AWS / Proxy ips we would see a much lower price that broke their agreement. That one was a fun realization for the manufacturers as to the level of shadiness some retailers would go through.reply",
      "> selecting appropriate Tor exit nodes for each regional siteSo, a proxy? Onion routing doesn't really play a role for this use case.reply",
      "> So, a proxy? Onion routing doesn't really play a role for this use case.The onion routing obscured our identity from the \"proxy\" exit nodes.Separately, Tor was also a convenient way to get a lot of arbitrary country-specific \"proxies\", without dealing with the sometimes sketchy businesses that are behind residential IP proxies.(Counterfeiting/graymarket operations can be organized crime.  I'd rather just fire up Tor, and trust math a little, than to try to vet the legitimacy and intentions of a residential IP broker.)reply",
      "The Tor exit nodes are public.reply",
      "They were concerned about the exit node identifying them, not the site identifying that a Tor exit node is connecting.reply",
      "Why would you need to obscure your identity from the exit nodes?reply",
      "So that the exit node can't go to the site they were scraping and say \"this is the person scraping your site\".reply"
    ],
    "link": "https://thereader.mitpress.mit.edu/the-secret-history-of-tor-how-a-military-project-became-a-lifeline-for-privacy/",
    "first_paragraph": "I\u2019m sitting in a cold, scuffed, and dirty plastic chair on a crowded train, watching freezing fog stream past the window \u2014 one of the many unpleasant but strangely enjoyable everyday experiences of life in the United Kingdom. Despite the train carriage hailing from the mid-1980s, there is something resembling Wi-Fi service, and so I connect, hoping to sneak in a few hours of PhD research. I load up a website \u2014 or so I think \u2014 but instead reach a block page courtesy of the train\u2019s Wi-Fi provider.Sighing, I load up the Tor Browser and type in the address. The website loads instantly.Tor is mostly known as the Dark Web or Dark Net, seen as an online Wild West where crime runs rampant. Yet it\u2019s partly funded by the U.S. government, and the BBC and Facebook both have Tor-only versions to allow users in authoritarian countries to reach them.At its simplest, Tor is a distributed digital infrastructure that makes you anonymous online. It is a network of servers spread around the world, accesse"
  },
  {
    "title": "Efrit: A native elisp coding agent running in Emacs (github.com/steveyegge)",
    "points": 81,
    "submitter": "simonpure",
    "submit_time": "2025-08-08T19:20:51 1754680851",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=44840654",
    "comments": [
      "I'm fascinated by this and the recent claude-code-ide package: https://github.com/manzaltu/claude-code-ide.elBut, I cannot seem to get past this error when I run claude-code-ide: \"Symbol\u2019s function definition is void: project-root\" I know this is defined in project.el, but claude has been surprisingly unhelpful at fixing this issue.I'm feeling a bit frustrated by the state of emacs packages lately. I've used emacs for 30 years and it feels like things are getting worse.reply",
      "I'm sorry it's not working. I've used emacs for almost 40 years and I'm definitely contributing to it being worse, by uploading efrit in its current state. But people were asking me for it. Damned if you do and all that.I'm more than happy to work with you to get it working, with the caveat that it actually kind of sucks right now. It's no Claude Code. But I am quickly evolving it in that direction.reply",
      "This project does not work for me either. Drat.  This is turn 4. Focus on any remaining tasks that haven't been completed yet.   Don't repeat work that was already done in previous turns.\n  Assistant: I notice from the context that we're in a directory that might be related to a xxx project. Let me try to find and open the yyyy.ts file.                                                               \n  [Result: Error: Unknown tool 'resolve_path']reply",
      "Yeah it really kind of sucks right now, it's more of a proof of concept.I'm working on evolving it into something that's not so transactional -- it will work more like claude code. Didn't realize it was going to hit the front page today. I'll poke at it this weekend and send an update.reply",
      "Ok, I might have to frame this comment. I'm a big fan.After reading your wikipedia page, I didn't realize we were both at UWash CS at the same time. Small world.reply",
      "I wonder if this could be updated to use OpenRouter in a similar way to Emigo[1] was aiming to do.(I use the past tense, because Emigo has not been updated in a quarter of a year, which seems as if it may as well be decades in the timeline of this sort of stuff.)[1] https://github.com/MatthewZMD/emigoreply",
      "Clever name. For thos curious Efrit is Genie in Arabic and possibly the same in neighboring languages such as Persian and Turkish..reply",
      "> For thos curious Efrit is Genie in ArabiMore precisely, as I understand it, \u201cgenie\" is  an anglicization of its Arabic equivalent, \u201cjinn\"; Efrit is a specific kind of jinn.reply",
      "Genie comes from Latin via French, it's not (just) a transliteration.https://www.etymonline.com/word/geniehttps://en.wikipedia.org/wiki/Genius_(mythology)reply",
      "I think it might be 'e' for 'emacs' (there's precedent: eglot, eldoc, eshell, erc, emms) combined with 'ifrit', since \"efrit\" is a much less common spelling.reply"
    ],
    "link": "https://github.com/steveyegge/efrit",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A native elisp coding agent running in Emacs\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.A sophisticated AI coding agent that leverages Emacs' native programmability through direct Elisp evaluation.Efrit is a conversational AI assistant that integrates seamlessly with Emacs, providing multiple interfaces for different types of tasks:Clone the repository:Add to your Emacs configuration (~/.emacs.d/init.el):Configure your API key in ~/.authinfo:Restart Emacs and test with M-x efrit-chatEmergency Emacs Setup (for quick testing):Using straight.el:Multi-Buffer Creation with efrit-doStarting with a simple request:Conversational Context - Making Modificationsefrit-do maintains context, so you can refine previous work:This demonstrates efrit-do's key feature: co"
  },
  {
    "title": "Jim Lovell, Apollo 13 commander, has died (nasa.gov)",
    "points": 363,
    "submitter": "LorenDB",
    "submit_time": "2025-08-08T19:12:18 1754680338",
    "num_comments": 73,
    "comments_url": "https://news.ycombinator.com/item?id=44840582",
    "comments": [
      "Trivia: Jim Lovell is the only person to fly to the moon twice without landing on it (scheduled \"test flight\" on Apollo 8, unscheduled emergency on Apollo 13).12 people flew to the moon without landing on it, now only 1 is still alive (Fred Haise).12 people walked on the moon, 4 are still alive (Buzz Aldrin, David Scott, Charles Duke, Harrison Schmitt).(Conclusion: walking on the moon is healthy?)reply",
      "> (Conclusion: walking on the moon is healthy?)NASA vetted the Apollo astronauts for those who did not have medical problems, so it would be more accurate to say they walked on the moon because they were healthy.reply",
      "They also vetted the people who didn't walk on the moon (because they apply the same testing to all astronauts).reply",
      "I also didn't walk on the moon, so therefore I'm healthy.reply",
      "The average lifespan of the astronauts that did not walk on the moon might be even longer than the ones that did.I mean, you don't have any data (or evidence) on that one way or the other; do you?ADDED. Oops: the comment 3 levels above gives data. I regret wading into this thread.reply",
      "What about backup crews as a control?reply",
      "For Apollo, the backup crew became the main crew two missions later (politics intervened on the last couple of missions, but this generally held true for all the manned Apollo missions).reply",
      "Incredibly small sample size, but aren\u2019t they allreply",
      "Do the moon viewers skew slightly older as a group because the earlier missions did not include space walking, and the walkers skew slightly younger?Could also be impacted by fame / levels of personal income from being in one of the two distinct groups.reply",
      "Long flights without getting up to stand are dangerous after all.reply"
    ],
    "link": "https://www.nasa.gov/news-release/acting-nasa-administrator-reflects-on-legacy-of-astronaut-jim-lovell/",
    "first_paragraph": "2 min readThe following is a statement from acting NASA Administrator Sean Duffy on the passing of famed Apollo astronaut Jim Lovell. He passed away Aug. 7, in Lake Forest, Illinois. He was 97 years old.\u201cNASA sends its condolences to the family of Capt. Jim Lovell, whose life and work inspired millions of people across the decades. Jim\u2019s character and steadfast courage helped our nation reach the Moon and turned a potential tragedy into a success from which we learned an enormous amount. We mourn his passing even as we celebrate his achievements.\u201cFrom a pair of pioneering Gemini missions to the successes of Apollo, Jim helped our nation forge a historic path in space that carries us forward to upcoming Artemis missions to the Moon and beyond.\u201cAs the Command Module Pilot for Apollo 8, Jim and his crewmates became the first to lift off on a Saturn V rocket and orbit the Moon, proving that the lunar landing was within our reach. As commander of the Apollo 13 mission, his calm strength und"
  },
  {
    "title": "Ask HN: How can ChatGPT serve 700M users when I can't run one GPT-4 locally?",
    "points": 274,
    "submitter": "superasn",
    "submit_time": "2025-08-08T19:27:28 1754681248",
    "num_comments": 189,
    "comments_url": "https://news.ycombinator.com/item?id=44840728",
    "comments": [
      "I work at Google on these systems everyday (caveat this is my own words not my employers)). So I simultaneously can tell you that its smart people really thinking about every facet of the problem, and I can't tell you much more than that.However I can share this written by my colleagues! You'll find great explanations about accelerator architectures and the considerations made to make things fast.https://jax-ml.github.io/scaling-book/In particular your questions are around inference which is the focus of this chapter\nhttps://jax-ml.github.io/scaling-book/inference/Edit:\nAnother great resource to look at is the unsloth guides. These folks are incredibly good at getting deep into various models and finding optimizations, and they're very good at writing it up. Here's the Gemma 3n guide, and you'll find others as well.https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-...reply",
      "Same explanation but with less mysticism:Inference is (mostly) stateless. So unlike training where you need to have memory coherence over something like 100k machines and somehow avoid the certainty of machine failure, you just need to route mostly small amounts of data to a bunch of big machines.I don't know what the specs of their inference machines are, but where I worked the machines research used were all 8gpu monsters. so long as your model fitted in (combined) vram, you could job was a goodun.To scale the secret ingredient was industrial amounts of cash. Sure we had DGXs (fun fact, nvidia sent literal gold plated DGX machines) but they wernt dense, and were very expensive.Most large companies have robust RPC, and orchestration, which means the hard part isn't routing the message, its making the model fit in the boxes you have. (thats not my area of expertise though)reply",
      "> So I simultaneously can tell you that its smart people really thinking about every facet of the problem, and I can't tell you much more than that.\"we do 1970s mainframe style timesharing\"there, that was easyreply",
      "Doesn't google have TPU's that makes inference of their own models much more profitable than say having to rent out NVDIA cards?Doesn't OpenAI depend mostly on its relationship/partnership with Microsoft to get GPUs to inference on?Thanks for the links, interesting book!reply",
      "Yes. Google is probably gonna win the LLM game tbh. They had a massive head start with TPUs which are very energy efficient compared to Nvidia Cards.reply",
      "The only one who can stop Google is Google.They\u2019ll definitely have the best model, but there is a chance they will f*up the product / integration into their products.reply",
      "It would take talent for them to mess up hosting businesses who want to use their TPUs on GCP.But then again even there, their reputation for abandoning products, lack of customer service, condescension when it came to large enterprises\u2019 \u201clegacy tech\u201d lets Microsoft who is king of hand holding big enterprise and even AWS run rough shod over them.When I was at AWS ProServe, we didn\u2019t even bother coming up with talking points when competing with GCP except to point out how they abandon services.  Was it partially FUD? Probably.  But it worked.reply",
      ">It would take talent for them to mess up hosting businesses who want to use their TPUs on GCP.there are few groups as talented at losing a head start as google.reply",
      "Google employees collectively have a lot of talent.reply",
      "But they\u2019re ASICs so any big architecture changes will be painful for them right?reply"
    ],
    "link": "item?id=44840728",
    "first_paragraph": ""
  },
  {
    "title": "Hacking Diffusion into Qwen3 for the Arc Challenge (matthewnewton.com)",
    "points": 43,
    "submitter": "mattnewton",
    "submit_time": "2025-08-05T14:43:12 1754404992",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.matthewnewton.com/blog/arc-challenge-diffusion",
    "first_paragraph": ""
  },
  {
    "title": "Unmasking the Sea Star Killer (biographic.com)",
    "points": 28,
    "submitter": "sohkamyung",
    "submit_time": "2025-08-05T13:10:17 1754399417",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44797566",
    "comments": [
      "My coworkers worked on this research! I wasn't involved in this project, but I'm so stoked to see this making the rounds. Everyone involved is deservedly proud of this one.A big part of what makes me happy is that this seems quite niche to me, but it's getting a ton of attention which suggests people care about this more than I realized. That feels good! This is a huge deal for biodiversity in a huge swath of the Pacific Ocean.reply",
      "It\u2019s been really sad to watch the decline of sea stars the past 10+ years. Tide-pooling just hasn\u2019t been the same at Cannon Beach or the San Juan Islands. It\u2019s very encouraging to pin down the culprit.reply",
      "Well that\u2019s fun. The seastars are ravaged by Vibrio pectenicida in the same genus that includes human pathogens like the bacteria that causes cholera and various flesh eating bacteria that can infect through undercooked seafood. The latter have even begun to eat plastic and multiply in sargassum [1] so watch those cuts when swimming in Florida![1] https://news.ycombinator.com/item?id=36129757reply",
      "Study linkhttps://www.nature.com/articles/s41559-025-02797-2reply",
      "Couldn't they just do the initial test with 5? Maybe 10.. Why we gotta disintegrate 19 more of these fuzzy little guys? :|reply",
      "[dead]"
    ],
    "link": "https://www.biographic.com/unmasking-the-sea-star-killer/",
    "first_paragraph": ""
  },
  {
    "title": "How we replaced Elasticsearch and MongoDB with Rust and RocksDB (radar.com)",
    "points": 188,
    "submitter": "j_kao",
    "submit_time": "2025-08-08T12:57:50 1754657870",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=44836463",
    "comments": [
      "Bit thin on details and not looking like they\u2019ll open source it, but if someone clicked the post because they\u2019re looking for their \u201creplace ES\u201d thing:Both https://typesense.org/ and https://duckdb.org/ (with their spatial plugin) are excellent geo performance wise, the latter now seems really production ready, especially when the data doesn\u2019t change that often. Both fully open source including clustered/sharded setups.No affiliation at all, just really happy camper.reply",
      "These are great projects, we use DuckDB to inspect our data lake and for quick munging.We will have some more blog posts in the future describing different parts of the system in more detail. We were worried too much density in a single post would make it hard to read.reply",
      "These are great. I am eternally grateful that projects like this are open source, I do however find it hard to integrate them into your own projects.A while ago I tried to create something that has duckdb + its spatial and SQLite extensions statically linked and compiled in. I realized I was a bit in over my head when my build failed because both of them required SQLite symbols but from different versions.reply",
      "DuckDB does not have any kind of sharding or clustering? It doesn't even have a server (unless you count the HTTP Server Extension)?reply",
      "Typesense is an absolute beast, and it has a pretty great dev experience to boot.reply",
      "Not sure what they'll opensource. The rust code? They're calling it a DB, but they described an entire stack.reply",
      "Typsense as a product has been great (hosted cluster). Customer support has been awesome as well.reply",
      "Slightly meta, but I find its a good sign that we're back to designing and blogging about in-house data storage systems/ Query engines again. There was an explosion of these in the 2010's which seemed to slow down/refocus on AI recently.reply",
      "It slowed down not because of AI, but because it turned out it was mostly pointless. Highly specialized stacks that could usually be matched in performance by tweaking an existing system or scaling a different way.In-house storage/query systems that are not a product being sold by itself are NIH syndrome by a company with too much engineering resources.reply",
      "Is it good? What's left to innovate on in this space? I don't really want experimental data stores. Give me something rock solid.reply"
    ],
    "link": "https://radar.com/blog/high-performance-geocoding-in-rust",
    "first_paragraph": "Geocoding, search, routing, and base mapsLearn why product and digital leaders choose RadarThe cost-effective, all-in-one Google Maps alternative, with geocoding, search, routing, and mapsLearn why product and digital leaders choose RadarbyJeff KaoonAugust 7, 2025At Radar, performance is a feature. Our platform processes over 1 billion API calls per day from hundreds of millions of devices worldwide. We provide geolocation infrastructure and solutions, including APIs for:But as our products and data scale, so do our engineering challenges.To support this growth, we developed HorizonDB, a geospatial database written in Rust that consolidates multiple location services into a single, highly performant binary. With HorizonDB, we are able to power all of the above use cases with excellent operational footprint:Before HorizonDB, we split geocoding across Elasticsearch and microservices for forward geocoding, and MongoDB for reverse. Operating and scaling this stack was costly: Elasticsearch"
  },
  {
    "title": "The surprise deprecation of GPT-4o for ChatGPT consumers (simonwillison.net)",
    "points": 285,
    "submitter": "tosh",
    "submit_time": "2025-08-08T18:04:26 1754676266",
    "num_comments": 255,
    "comments_url": "https://news.ycombinator.com/item?id=44839842",
    "comments": [
      "Edit to add: according to Sam Altman in the reddit AMA they un-deprecated it based on popular demand. https://old.reddit.com/r/ChatGPT/comments/1mkae1l/gpt5_ama_w...I wonder how much of the '5 release was about cutting costs vs making it outwardly better. I'm speculating that one reason they'd deprecate older models is because 5 materially cheaper to run?Would have been better to just jack up the price on the others. For companies that extensively test the apps they're building (which should be everyone) swapping out a model is a lot of work.reply",
      "The vibe I'm getting from the Reddit community is that 5 is much less \"Let's have a nice conversation for hours and hours\" and much more \"Let's get you a curt, targeted answer quickly.\"So, good for professionals who want to spend lots of money on AI to be more efficient at their jobs. And, bad for casuals who want to spend as little money as possible to use lots of datacenter time as their artificial buddy/therapist.reply",
      "Well, good, because these things make bad friends and worse therapists.reply",
      "The number of comments in the thread talking about 4o as if it were their best friend the shared all their secrets with is concerning. Lotta lonely folks out therereply",
      "Where do they all come from? Where do they all belong?reply",
      "You win today.reply",
      "No this isn't always the case.Perhaps if somebody were to shut down your favourite online shooter without warning you'd be upset, angry and passionate about it.Some people like myself fall into this same category, we know its a token generator under the hood, but the duality is it's also entertainment in the shape of something that acts like a close friend.We can see the distinction, evidently some people don't.This is no different to other hobbies some people may find odd or geeky - hobby horsing, ham radio, cosplay etc etc.reply",
      "People were saying they'd kill themself if OpenAI didn't immediately undeprecate GPT-4o. I would not have this reaction to a game being shut down.reply",
      "I'm kind of in your side but there's definitely people out there who would self harm if they invested a lot of time in an mmo that got shut down",
      "Lack of third-place to exist and make friends.reply"
    ],
    "link": "https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/",
    "first_paragraph": "8th August 2025I\u2019ve been dipping into the r/ChatGPT subreddit recently to see how people are reacting to the GPT-5 launch, and so far the vibes there are not good. This AMA thread with the OpenAI team is a great illustration of the single biggest complaint: a lot of people are very unhappy to lose access to the much older GPT-4o, previously ChatGPT\u2019s default model for most users.A big surprise for me yesterday was that OpenAI simultaneously retired access to their older models as they rolled out GPT-5, at least in their consumer apps. Here\u2019s a snippet from their August 7th 2025 release notes:When GPT-5 launches, several older models will be retired, including GPT-4o, GPT-4.1, GPT-4.5, GPT-4.1-mini, o4-mini, o4-mini-high, o3, o3-pro.If you open a conversation that used one of these models, ChatGPT will automatically switch it to the closest GPT-5 equivalent. Chats with 4o, 4.1, 4.5, 4.1-mini, o4-mini, or o4-mini-high will open in GPT-5, chats with o3 will open in GPT-5-Thinking, and cha"
  },
  {
    "title": "Build durable workflows with Postgres (dbos.dev)",
    "points": 92,
    "submitter": "KraftyOne",
    "submit_time": "2025-08-08T19:24:07 1754681047",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=44840693",
    "comments": [
      "So we do this exact thing in our software, and I implement it (along with other devs), and I was still entranced enough to read through the end.  No differences between ours and theirs (this is a fairly common practice anyway) but article is written in succinct, informative chunks with \"images\" (of code) in between.This is how you write a technical article.  Thanks to the author for the nice read :)reply",
      "Recently moved some of the background jobs from graphile worker to DBOS. Really recommend for the simplicity. Took me half an hour.I evaluated temporal, trigger, cloudflare workflows (highly not recommended), etc and this was the easiest to implement incrementally. Didn't need to change our infrastructure at all. Just plugged the worker where I had graphile worker.The hosted service UX and frontend can use a lot of work though but it's not necessary for someone to use. OTEL support was there.reply",
      "Agree on the UI - I wish it was improvedreply",
      "Why would you not recommend Cloudflare workflows? Was thinking of using them in my current project..reply",
      "Interesting!What made you opt for DBOS over Temporal?reply",
      "Temporal required re-architecting some stuff, their typescript sdk and sandbox is bit unintuitive to use so would have been an additional item to grok for the team, and additional infrastructure to maintain. There was a latency trade off too which in our case mattered.Didn't face any issue though. Temporal observability and UI was better than DBOS. Just harder to do incremental migration in an existing codebase.reply",
      "What was the reason for the transition?reply",
      "Needed checkpoints in some of our jobs wrapping around the AI agent so we can reduce cost and increase reliability (as workflow will start from mid step as opposed to a complete restart).We already check pointed the agent but then figure it's better to have a generic abstraction for other stuff we do.reply",
      "Some other lightweight solutions around:https://github.com/iopsystems/durablehttps://github.com/maxcountryman/underwayreply",
      "I've been using https://www.pgflow.dev for workflows which is built on pgmq and am really impressed so far. Most of the logic is in the database so I'm considering building an Elixir adapter DSL.reply"
    ],
    "link": "https://www.dbos.dev/blog/why-postgres-durable-execution",
    "first_paragraph": "Meet the team simplifying reliability.Demos, deep dives, and more DBOS.Launch your first workflow in minutes.Step-by-step guides & real-world use cases.Ready-to-run code to spark your project.Lightweight Durable TypeScript WorkflowsLightweight Durable Python WorkflowsDemo applications for DBOS TransactWhen we started building a durable workflows library, the most critical architectural decision we faced was what data store to use for workflow metadata. The core durable workflow operations are simple\u2013checkpointing workflow state and recovering an interrupted workflow from its latest checkpoint. Almost any data store can handle these operations, but choosing the right one is critical to ensure workflows are scalable and performant.In this blog post, we\u2019ll dive deep into why we chose to build on Postgres. While there are good nontechnical reasons for the decision (Postgres is popular and open-source with a vibrant community and over 40 years of battle-testing), we\u2019ll focus on the technica"
  },
  {
    "title": "Astronomy Photographer of the Year 2025 shortlist (rmg.co.uk)",
    "points": 162,
    "submitter": "speckx",
    "submit_time": "2025-08-08T14:29:54 1754663394",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=44837434",
    "comments": [
      "This is the kind of discovery that I love to see on HN. Regardless of who wins the competition, we all win by getting to see all of the entries.Absolutely gorgeous shots. Made my day.reply",
      "It\u2019s also lovely to see the exhibition IRL when it comes, if you\u2019re lucky enough.When I saw it, there were narratives about the people behind the shots which made it extra special.reply",
      "Nice images. I personally prefer more of Space and less of Earth in astro.How did they do solar eruption? Must be some filters as Sun doesn't look like this to naked eye. I mean this image:https://www.rmg.co.uk/sites/default/files/styles/large/publi...reply",
      "I believe it's an H-alpha filter.  https://en.wikipedia.org/wiki/Hydrogen-alphareply",
      "> Into the Past by Jim HildrethThe area in this photo -- the Caineville Mesa, Factory Butte, \"Long Dong Silver\" (I'm not aware of a more polite name) -- is some of the strangest land in America. It really is that lunar blue gray. The Temples of the Sun and Moon (enormous natural sandcastles) are also nearby, and are similarly eerie in the evening.The closest I've ever felt to being in space. Recommend!reply",
      "Agree. Found his site, but that shot\u2019s not listed: https://www.hildreth-photographer.com/portfolio.html?folio=F...reply",
      "Seems their IG has some of these, including the shot from the article: https://www.instagram.com/tripodtales/p/C8hRS0ctzCp/Factory Butte: https://www.instagram.com/tripodtales/p/C6gg-wpS-tr/\"Long Dong Silver\": https://www.instagram.com/tripodtales/p/C-yLCskOGiC/reply",
      "The one of M33 (Triangulum Galaxy) really blew me away, so many nebulae!reply",
      "Shanghai blood moon reminded me of Blade Runner. Who knew that a 1982 imagination of LA Chinatown would look so similar to Shanghai in fool Moon.I know it's partly because of the color pallette, but stillreply",
      "> Gateway to the Galaxy by Yujie ZhangAre the \"geometric buildings\" real or just something she put up for the picture?reply"
    ],
    "link": "https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/2025-shortlist",
    "first_paragraph": "Explore some of the stunning images shortlisted in the world\u2019s biggest astrophotography competitionThe shortlist for the ZWO Astronomy Photographer of the Year 2025 competition has been unveiled.From a blood moon hanging over Shanghai to a family portrait of the Solar System and a close-up of a comet's streaming tails, distant astronomical wonders are photographed in magnificent detail for all to admire.Now in its 17th year, in 2025 the competition received a record number of entries, with just over 5,880 photographs submitted from 68 different countries.See a small selection of shortlisted images below, and stay tuned to discover this year's full shortlist, winners and runners-up at a special online awards ceremony on 11 September.Sign up to our space newsletter for exclusive astronomy news,\u00a0guides and events, and be among the first to see this year's Astronomy Photographer of the Year winnersJiading District, Shanghai, ChinaThis photograph captures a red Full Moon rising beside Shang"
  },
  {
    "title": "Fire hazard of WHY2025 badge due to 18650 Li-Ion cells (why2025.org)",
    "points": 75,
    "submitter": "fjfaase",
    "submit_time": "2025-08-06T06:29:21 1754461761",
    "num_comments": 71,
    "comments_url": "https://news.ycombinator.com/item?id=44808423",
    "comments": [
      "I'm confused that noone is pointing out most protected 18650 cells won't even fit in those holders, since protected cells are generally in the 18690\u202618700 pseudo size range.  That's too long to get into those holders.Source: the holders are likely Keystone 1042 [https://www.keyelco.com/product.cfm/product_id/918], which I've worked with before.  For a protected cell, cf. for example https://imrbatteries.com/products/panasonic-ncr18650b-3350ma... - note 69.41mm length.[ed.: it's the China equivalent of a Keystone 1042, https://www.lcsc.com/product-detail/C2988620.html - I can't confirm but am 95% confident a protected cell won't fit; if it would, the hold on an unprotected 18650 cell would be quite loose.]reply",
      "That is mentioned in the article, although could perhaps be more emphasized since it does mean the \"obvious\" fix is not possible:Commonly available protected 18650 cells don't fit in the badge's cell holders because they are slightly longer.reply",
      "FWIW, I have battery holders that deathgrip unprotected cells, so even finding a proper holder for those is a crapshoot.reply",
      "I did indeed totally miss that, thanks for pointing it out!reply",
      "I\u2019ve worked with the Keystone holders and can confirm that those will hold protected cells, at least some I got from nkon.nlreply",
      "Sure, that means the idea of using unprotected cells was already there when the holders were selected :)reply",
      "Yeah, I'm just saying, you can't even buy regular protected cells and put them in, because they won't fucking fit.  I do think \"actual\" 18650 protected cells exist, but they would be rare and expensive because you can't build them out of mass manufactured bare 18650 cells (for obvious reasons of where do you put the damn protection circuit.)reply",
      "Ah yea then I misunderstood. That's right you can't easily switch out the cells for protected cells yourself :(reply",
      "The Keystone holders are nice but expensive, but they do not fit most protected 18650 cells, and I don't like the PCB mounting options.I designed my own 3D printed 18650 holder for my project, including a positive battery tab cut-out to prevent reverse battery insertion. I get to decide how big the battery can be, and protected cells are 100% the way to go.I've never had a problem with a short with the protected cells, and my circuit also cuts off power to the load using a mosfet, if a short ever occurs. It's been working great for years.reply",
      "Funny, I'm also going with a 3D printed 18650 battery module... for a lepton (FLIR) project.reply"
    ],
    "link": "https://wiki.why2025.org/Badge/Fire_hazard",
    "first_paragraph": "This document was originally posted in two places:\nA response was published by IFCAT:\nAdditional writeups from other people:\nMore discussions:\n\nThe WHY2025 badge is a fire hazard when used with unprotected cells. Unprotected cells themselves are intrinsically unsafe and require additional safety measures which are not provided by the badge. In fact, the badge makes it worse.\nVisitors of WHY2025 can get a badge, a fun electronic gadget that is a true work of art. These badges have become a tradition at this kind of conference/festival.\nDesigning a badge is a large effort, and in this edition it was done in just a few months. Unfortunately, with this edition's design and intended use, the only thing preventing a fire is, basically, a thin layer of paint (actually resin (solder mask)).\nThe WHY2025 badge was designed to be powered by 2 Li-Ion 18650 battery cells connected in parallel. The cells provided to visitors are of the \"unprotected\" kind and thus capable of providing a very large sh"
  },
  {
    "title": "Why building a self-hosted SaaS is harder (getlago.com)",
    "points": 18,
    "submitter": "FinnLobsien",
    "submit_time": "2025-08-05T15:58:02 1754409482",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44799727",
    "comments": [
      "> Even low-risk fixes like renaming a database column can break a billing run when a job is currently using that data.You should tell me how you rename database columns in AWS without breaking anything.I\u2019m not really sure what the point of this article is, it just seems to promote the company\u2019s migration method with a misleading title. But I highly disagree that self-hosted is harder. With many self-hosted BaaS systems I\u2019d argue it\u2019s easier.reply",
      "Agreed on both counts - absolutely awful article, and self-hosting is not hard. You seemingly need a phd to do the most basic things on AWSreply",
      "I didnt read the article but how you do is, you should create new column without deleting the old one, and your code should be updated to use the new column, once you phase out the old version of the code, you backfill data from old column to new one, and delete the old column.reply",
      "Or you rewrite queries on the fly with ProxySQL or similar to use the new column name, and then deploy the new code. Can even insert a momentary pause at the proxy layer so no queries hitting the old name sneak through while you do the rename.This method doesn\u2019t work as well with distributed DBs, but to be fair they\u2019re a terrible idea for most use cases.reply",
      "Some vendors I work with have transitioned to SaaS-only models and it's truly painful. I have a perfectly good enterprise datacenter, but I also rent some Windows VMs in Amazon's cloud apparently that I still have to manage the application updates myself, but have to put in a support ticket for the .NET system dependencies if they're missing, because you know, it's in the cloud now so I'm not supposed to access the underlying infrastructure.I don't always have a choice, but if I do, I will always choose the vendor which will give me an on-premise product. And I guarantee you the companies that do will outlast the SaaS-only ones.reply"
    ],
    "link": "https://www.getlago.com/blog/self-hosted-saas",
    "first_paragraph": "Table of contentIn the 90s, we\u00a0flew in technicians to install Oracle databases in server basements. Today, Supabase spins up a backend, in seconds, for free.Over the past 30 years, software has gotten faster, cheaper and easier in almost every way.Some engineers might miss 24-month cycles of tranquil coding, but nobody wants to do code reviews over email or contort software to run on a 10 year-old server rack your eighth-biggest customer is still using.As an open source SaaS startup, we need to be able to do both: Ship quickly while also offering a self-hosted version.This makes shipping updates harder because customer instances are a black box. We have no way to know what jobs a customer has running. This is especially difficult because no two customers use Lago the same way. One customer might aggregate usage data in real-time while another does so monthly. Even low-risk fixes like renaming a database column can break a billing run when a job is currently using that data. But as a st"
  },
  {
    "title": "My DIY modular charging station (arun.is)",
    "points": 15,
    "submitter": "surprisetalk",
    "submit_time": "2025-08-06T13:47:26 1754488046",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=44811993",
    "comments": [
      "What I really want is for all of my outlets to have one A/C and one retractable usb-c cable.  So the cables are exposed when in use, but coiled back into the wall otherwise. When will my dream come true?reply",
      "I was expecting a fire resistant cabinet.I keep my chargers and batteries in another building for anything with removable batteries.reply",
      "I gather a burning battery tends to spray flaming debris in all directions.  Even if it it's not full on flameproof, some sort of enclosure might contain debris and avoid spot fires.reply",
      "I'm as paranoid as anyone but this is too much even for me. How do you two charge your phones?reply",
      "Phones charge on fire resistant tile. I only use the other building for stuff with removable batteries which phones sadly aren't.  If the phones had removable batteries I'd just get an extra set an swap with the ones on the charger when they run low.I've been thinking of finding some kind of fire resistant box for phone charging though, so that's what I was hoping to find at the link. :)reply",
      "There's bat-safe boxes ( https://www.bat-safe.com ) , but can't comment on effectiveness...reply"
    ],
    "link": "https://arun.is/blog/diy-modular-charging-station/",
    "first_paragraph": " 5 min read \u2002  \u2002 Aug 4, 2025   The cables crept into our lives gradually. First, we had a USB charger in our\nbedroom to charge my wife\u2019s iPad or my Kindle. Then we added another near our\ndining table, pulling up a chair to use as a base when charging the kids\u2019 iPad.\nAdd in frequent visits by family and most of our available outlets have a\ncharger coming out of it in one form or another.Aside from the fact that we now had cables all over the house, there was the\nannoyance of walking around the house to find a charger, only to find them all\nalready taken by someone else\u2019s devices.This challenge seemed like the perfect chance to consolidate. So, I made my own\nmodular charger from simple materials. Leica CL \u00b7 60mm \u00b7 f/5.6 \u00b7 1/640 \u00b7\nISO 100But first, I started looking around in the market for off-the-shelf options that\ncould charge multiple USB-C devices all at once.I love the concept behind the Scosche BaseLynx 2.0,\nbut was concerned by the low 60W available across the three ports. Further"
  },
  {
    "title": "Getting good results from Claude Code (dzombak.com)",
    "points": 239,
    "submitter": "ingve",
    "submit_time": "2025-08-08T13:45:57 1754660757",
    "num_comments": 107,
    "comments_url": "https://news.ycombinator.com/item?id=44836879",
    "comments": [
      "I\u2019m just today after having my first real success with Claude (and generally with coding agents). I\u2019ve played with Cursor in the past but am now trying Claude and others.As mentioned in the article, the big trick is having clear specs. In my case I sat down for 2 hours and wrote a 12 step document on how I would implement this (along with background information). Claude went through step by step and wrote the code. I imagine this saved me probably 6-10 hours. I\u2019m now reviewing and am going to test etc. and start adjusting and adding future functionality.Its success was rooted in the fact I knew exactly how to do what it needed to do. I wrote out all the steps and it just followed my lead.It makes it clear to me that mid and senior developers aren\u2019t going anywhere.That said, it was amazing to just see it go through the requirements and implement modules full of organised documented code that I didn\u2019t have to write.reply",
      "> As mentioned in the article, the big trick is having clear specsI've been building a programming language using Claude, and this is my findings, too.Which, after discovering this, makes sense. There are a LOT of small decisions that go into programming. Without detailed guidance, LLMs will end up making educated guesses for a lot of these decision, many of which will be incorrect. This creates a compounding effect where the net effect is a wrong solution.reply",
      "Yeah. Read \u201cProgramming as Theory Building\u201d by Naur [1] to understand why you need to still need to develop a theory of the problem and how to model it yourself lest the LLM concoct (an incorrect) one for you.[1] https://gwern.net/doc/cs/algorithm/1985-naur.pdfreply",
      "Thanks for sharing this article.reply",
      "I get excellent results and don\u2019t do anything like that. Basically I ask Claude to write code as I do. A small step at a time. I literally prompt it to do the next step I\u2019d do and so on and so forth. I accept all changes immediate and then commit after every change and then review the diff. If Claude did some badness then I ask it to fix that. I typically also give references to existing code that I want it to model or functions to use.This gives me excellent results with far less typing and time.reply",
      "Can you (or anyone) share an example of such a specification document?  As an amateur programmer experimenting with CC, it would be very helpful to understand the nature and depth of the information that is helpful.reply",
      "I have multiple system prompts that I use before getting to the actual specification.1.  I use the Socratic Coder[1] system prompt to have a back and forth conversation about the idea, which helps me hone the idea and improve it. This conversation forces me to think about several aspects of the idea and how to implement it.2. I use the Brainstorm Specification[2] user prompt to turn that conversation into a specification.3. I use the Brainstorm Critique[3] user prompt to critique that specification and find flaws in it which I might have missed.4. I use a modified version of the Brainstorm Specification user prompt to refine the specification based on the critique and have a final version of the document, which I can either use on my own or feed to something like Claude Code for context.Doing those things improved the quality of the code and work spit out by the LLMs I use by a significant amount, but more importantly, it helped me write much better code on my own because I know have something to guide me, while before I used to go blind.As a bonus, it also helped me decide if an idea was worth it or not; there are times I'm talking with the LLM and it asks me questions I don't feel like answering, which tells me I'm probably not into that idea as much as I initially thought, it was just my ADHD hyper focusing on something.[1]: https://github.com/jamesponddotco/llm-prompts/blob/trunk/dat...[2]: https://github.com/jamesponddotco/llm-prompts/blob/trunk/dat...[3]: https://github.com/jamesponddotco/llm-prompts/blob/trunk/dat...reply",
      "Good stuff. A minor observation:> I use the Socratic Coder[1] system prompt to have a back and forth conversation about the idea. (prompt starts with: 1. Ask only one question at a time)Why only 1? IMHO it's better to write a long prompt explaining yourself as much as possible (exercises your brain and you figure out things), and request as many questions to clarify as possible, review, and suggestions, all at once. This is better because:  1. It makes you think deeper and practice writing clearly.\n  2. Even though each interaction is quite slower, since you are more active and engaged it feels shorter (try it), and you minimize interactions significantly.\n  3. It's less wasteful as going back and forth \n  4. You converge in much shorter time as your misconceptions, misunderstandings, problems expressing yourself, or confusion on the part of the LLM are all addressed very early.\n  5. I find it annoying to wait for the replies.\n\nI guess if you use a fast response conversational system like ChatGPT app it would make more sense. But I don't think that way you can have deep conversations unless you have a stellar working memory. I don't, so it's better for me to write and read, and re-write, and re-read...reply",
      "I do one question at a time so I don't feel overwhelmed and can answer questions with more details.I start with an idea between <idea> tags, write as much as I possibly can between these tags, and then go one question at a time answering the questions with as much details as I possibly can.Sometimes I'll feed the idea to yet another prompt, Computer Science PhD[1], and use the response as the basis for my conversation with the socratic coder, as the new basis might fill in gaps that I forgot to include initially.[1]: https://github.com/jamesponddotco/llm-prompts/blob/trunk/dat...[2]: Something like \"Based on my idea, can you provide your thoughts on how the service should be build, please? Technologies to use, database schema, user roles, permissions, architectural ideas, and so on.\"reply",
      "Thanks for sharing these prompts. Will certainly help with improving my LLM coding workflow.reply"
    ],
    "link": "https://www.dzombak.com/blog/2025/08/getting-good-results-from-claude-code/",
    "first_paragraph": "\n                    \ud83d\udc4b\ud83c\udffb Currently looking for work!\n                    Please see my LinkedIn profile and get in touch, or see ways to support me in the interim.\n                I've been experimenting with LLM programming agents over the past few months. Claude Code has become my favorite.It is not without issues, but it's allowed me to write ~12 programs/projects in relatively little time, and I feel I would not have been able to do all this in the same amount of time without it. Most of them, I wouldn't even have bothered to write without Claude Code, simply because they'd take too much of my time. (A list is included at the end of this post.)I'm still far from a Claude Code expert, and I have a backlog of blog posts and documentation to review that might be useful. But \u2014 and this is critical \u2014 you don't have to read everything that's out there to start seeing results. You don't even need to read this post; just type some prompts in and see what comes out.That said, because I just "
  },
  {
    "title": "Overengineering my homelab so I don't pay cloud providers (ergaster.org)",
    "points": 198,
    "submitter": "JNRowe",
    "submit_time": "2025-08-04T23:14:15 1754349255",
    "num_comments": 164,
    "comments_url": "https://news.ycombinator.com/item?id=44792419",
    "comments": [
      "> But my server could be shut down because of a power outage or another reason. I might be at work or even on holidays when it happens, and even wireguard can\u2019t solve this.A 'power outage' incident doesn't seem to have been mitigated. My homelab has had evolving mitigations: I cut a hole in the side of a small UPS so I could connect it to a larger (car) battery for longer uptime, which got replaced by a dedicated inverter/charger/transfer-switch attached to a big-ass AGM caravan battery (which on a couple of occasions powered through two-to-three hour power outages), and has now been replaced with these recent LiFePo4 battery power station thingies.Of course, it's only a homelab, there's nothing critically important that I'm hosting, but that's not the dang point, I want to beat most of \"the things\", and I don't like having to check that everything has rebooted properly after a minor power fluctuation (I have a few things that mount remote file stores and these mounts usually fail upon boot due to the speed at which certain devices boot up - and I've decided not to solve that yet).reply",
      "For anyone thinking of doing this, please please don't. A car battery is probably never a sealed deep cycle battery, and the UPS's charging circuitry is not intended to charge a battery of this size (this is assuming you're using a lead based battery, and not something even more crazy and dangerous like Li-Po or LiFePO4). God forbid you have a cell fail on a car battery and that charger starts cooking the battery. I've had actual car lead acid batteries explode because of poor choices someone else made trying to do something like this, and man when they go, they're dangerous and scary. You really need to pick hardware that's all properly specced and sized for the job...there's a reason Eaton and APC charge what they do.reply",
      "I agree entirely, and wouldn't do it again.reply",
      "To each their own. I'd personally sleep far more soundly with even a car battery UPS under my bed than with one of those consumer ready lithium ion portable power station batteries they sell on Amazon.But if you can't explain the difference between voltage and current, or know what \"short circuit\" means, then this isn't something to poke at.reply",
      "Author here, indeed I didn't install a UPS. I've tried to keep my setup fairly minimal, and I'm consciously accepting that if there's a power outage my services will be down. I self-host exclusively for myself, not for others.What I don't want though is a power outage putting my server offline while I'm on holidays, and not be able to access my services at all.My ISP-provided router supports Wireguard, so I can use that to connect to my KVM and send the Wake on LAN packages.reply",
      "Out of curiosity, did you look through the BIOS options on your computer? Mine is much less capable than yours (it is a used mini-pc) but it has options to boot itself up upon resuming power.reply",
      "I use UPS for my internet and then remote access such as intel AMT will get you back into your systems if you've specced your hardware to have such features.reply",
      "You REALLY should not expose AMT to the internet.reply",
      "> I have a few things that mount remote file stores and these mounts usually fail upon boot due to the speed at which certain devices boot up - and I've decided not to solve that yetIf your OS is using systemd, you can fix that pretty easily by adding an After=network-online.target (so the ExecStart doesn't even try to check if there is no networking yet) and an ExecCondition shell script [1] to actually check if nfs / smb on the target host is alive as an override to the fs mounts.Add a bunch of BindsTo overrides to the mounts and the services that need the data, and you have yourself a way to stop the services automatically when the filesystem goes away.I've long been in the systemd hater camp, but honestly, not having to wrangle with once-a-minute cronjobs to check for issues is actually worth it.[1] https://forum.manjaro.org/t/for-those-who-use-systemd-servic...reply",
      "Here's a deeper article on ordering things around network startup: https://systemd.io/NETWORK_ONLINE/It doesn't conflict with anything you've said, just a very handy document.reply"
    ],
    "link": "https://ergaster.org/posts/2025/08/04-overegineering-homelab/",
    "first_paragraph": ""
  },
  {
    "title": "GPT 5 vs. Opus 4.1 for Vibe-Coded Apps (instantdb.com)",
    "points": 3,
    "submitter": "stopachka",
    "submit_time": "2025-08-08T23:11:24 1754694684",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.instantdb.com/essays/gpt_5_vs_opus_4",
    "first_paragraph": ""
  }
]