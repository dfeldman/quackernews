[
  {
    "title": "AI will make formal verification go mainstream (kleppmann.com)",
    "points": 301,
    "submitter": "evankhoury",
    "submit_time": "2025-12-16T21:14:49 1765919689",
    "num_comments": 162,
    "comments_url": "https://news.ycombinator.com/item?id=46294574",
    "comments": [
      "I think formal verification shines in areas where implementation is much more complex than the spec, like when you\u2019re writing incomprehensible bit-level optimizations in a cryptography implementation or compiler optimization phases. I\u2019m not sure that most of us, day-to-day, write code (or have AI write code) that would benefit from formal verification, since to me it seems like high-level programming languages are already close to a specification language. I\u2019m not sure how much easier to read a specification format that didn\u2019t concern itself with implementation could be, especially when we currently use all kinds of frameworks and libraries that already abstract away implementation details.Sure, formal verification might give stronger guarantees about various levels of the stack, but I don\u2019t think most of us care about having such strong guarantees now and I don\u2019t think AI really introduces a need for new guarantees at that level.reply",
      "> to me it seems like high-level programming languages are already close to a specification languageThey are not. The power of rich and succinct specification languages (like TLA+) comes from the ability to succinctly express things that cannot be efficiently computed, or at all. That is because a description of what a program does is necessarily at a higher level of abstraction than the program (i.e. there are many possible programs or even magical oracles that can do what a program does).To give a contrived example, let's say you want to state that a particular computation terminates. To do it in a clear and concise manner, you want to express the property of termination (and prove that the computation satisfies it), but that property is not, itself, computable. There are some ways around it, but as a rule, a specification language is more convenient when it can describe things that cannot be executed.reply",
      "TLA+ is not a silver bullet, and like all temporal logic, has constraints.You really have to be able to reduce your models to: \u201cat some point in the future, this will happen,\" or \"it will always be true from now on\u201dHave probabilistic outcomes? Or even floats [0] and it becomes challenging and strings are a mess.> Note there is not a float type. Floats have complex semantics that are extremely hard to represent. Usually you can abstract them out, but if you absolutely need floats then TLA+ is the wrong tool for the job.TLA+ works for the problems it is suitable for, try and extend past that and it simply fails.[0] https://learntla.com/core/operators.htmlreply",
      "There are excellent probabilistic extensions to temporal logic out there that are very useful to uncover subtle performance bugs in protocol specifications, see e.g. what PRISM [1] and Storm [2] implement. That is not within the scope of TLA+.Formal methods are really broad, ranging from lightweight type systems to theorem proving. Some techniques are fantastic for one type of problem but fail at others. This is quite natural, the same thing happens with different programming paradigms.[1] https://www.prismmodelchecker.org[2] https://www.stormchecker.orgreply",
      "> To give a contrived example, let's say you want to state that a particular computation terminates. To do it in a clear and concise manner, you want to express the property of termination (and prove that the computation satisfies it), but that property is not, itself, computable. There are some ways around it, but as a rule, a specification language is more convenient when it can describe things that cannot be executed.Do you really think it is going to be easier for the average developer to write a specification for their program that does not terminatevsGiving them a framework or a language that does not have for loop?Edit: If by formal verification you mean type checking. That I very much agree.reply",
      "Maybe it's difficult for the average developer to write a formal specification, but the point of the article is that an AI can do it for them.reply",
      "Yes. I feel like people who are trying to push software verification have never worked on typical real-world software projects where the spec is like 100 pages long and still doesn't fully cover all the requirements and you still have to read between the lines and then requirements keep changing mid-way through the project... Implementing software to meet the spec takes a very long time and then you have to invest a lot of effort and deep thought to ensure that what you've produced fits within the spec so that the stakeholder will be satisfied. You need to be a mind-reader.It's hard even for a human who understands the full business, social and political context to disambiguate the meaning and intent of the spec; to try to express it mathematically would be an absolute nightmare... and extremely unwise. You would literally need some kind of super intelligence... And the amount of stream-of-thought tokens which would have to be generated to arrive at a correct, consistent, unambiguous formal spec is probably going to cost more than just hiring top software engineers to build the thing with 100% test coverage of all main cases and edge cases.Worst part is; after you do all the expensive work of formal verification; you end up proving the 'correctness' of a solution that the client doesn't want.The refactoring required will invalidate the entire proof from the beginning. We haven't even figured out the optimal way to formally architect software that is resilient to requirement changes; in fact, the industry is REALLY BAD at this. Almost nobody is even thinking about it. I am, but I sometimes feel like I may be the only person in the world who cares about designing optimal architectures to minimize line count and refactoring diff size. We'd have to solve this problem first before we even think about formal verification of 'most software'.Without a hypothetical super-intelligence which understands everything about the world; the risk of misinterpreting any given 'typical' requirement is almost 100%... And once we have such super-intelligence, we won't need formal verification because the super-intelligence will be able to code perfectly on the first attempt; no need to verify.And then there's the fact that most software can tolerate bugs... If operationally important big tech software which literally has millions of concurrent users can tolerate bugs, then most software can tolerate bugs.reply",
      "Software verification has gotten some use for smart contracts. The code is fairly simple, it's certain to be attacked by sophisticated hackers who know the source, and the consequence of failure is theft of funds, possibly in large amounts. 100% test coverage is no guarantee that an attack can't be found.People spend gobs of money on human security auditors who don't necessarily catch everything either, so verification easily fits in the budget. And once deployed, the code can't be changed.Verification has also been used in embedded safety-critical code.reply",
      "If the requirements you have to satisfy arise out of a fixed, deterministic contract (as opposed to a human being), I can see how that's possible in this case.I think the root problem may be that most software has to adapt to a constantly changing reality. There aren't many businesses which can stay afloat without ever changing anything.reply",
      "The whole perspective of this argument is hard for me to grasp.  I don't think anyone is suggesting that formal specs are an alternative to code, they are just an alternative to informal specs.  And actually with AI the new spin is that they aren't even a mutually exclusive alternative.A bidirectional bridge that spans multiple representations from informal spec to semiformal spec to code seems ideal.  You change the most relevant layer that you're interested in and then see updates propagating semi-automatically to other layers.  I'd say the jury is out on whether this uses extra tokens or saves them, but a few things we do know.  Chain of code works better than chain of thought, and chain-of-spec seems like a simple generalization.  Markdown-based planning and task-tracking agent workflows work better than just YOLOing one-shot changes everywhere, and so intermediate representations are useful.It seems to me that you can't actually get rid of specs, right?  So to shoot down the idea of productive cooperation between formal methods and LLM-style AI, one really must successfully argue that informal specs are inherently better than formal ones.  Or even stronger: having only informal specs is better than having informal+formal.reply"
    ],
    "link": "https://martin.kleppmann.com/2025/12/08/ai-formal-verification.html",
    "first_paragraph": "\nSkip to content\nPublished by Martin Kleppmann on 08 Dec 2025.Much has been said about the effects that AI will have on software development, but there is an\nangle I haven\u2019t seen talked about: I believe that AI will bring formal verification, which for\ndecades has been a bit of a fringe pursuit, into the software engineering mainstream.Proof assistants and proof-oriented programming languages such as Rocq,\nIsabelle, Lean,\nF*, and Agda have been around for a long\ntime. They make it possible to write a formal specification that some piece of code is supposed to\nsatisfy, and then mathematically prove that the code always satisfies that spec (even on weird\nedge cases that you didn\u2019t think of testing). These tools have been used to develop some large\nformally verified software systems, such as an operating system kernel,\na C compiler, and a\ncryptographic protocol stack.At present, formal verification is mostly used by research projects, and it is\nuncommon for industrial software\nengineers t"
  },
  {
    "title": "alpr.watch (alpr.watch)",
    "points": 614,
    "submitter": "theamk",
    "submit_time": "2025-12-16T16:54:19 1765904059",
    "num_comments": 317,
    "comments_url": "https://news.ycombinator.com/item?id=46290916",
    "comments": [
      "For years I've thought about doing an \"art project\" to make people more aware of the fact they are being observed \u2013 but I never actually got up and did it.The idea was to seek spots in the city where public web cams are pointed at, and paint QR codes on the ground at those spots (using a template), linking to the camera stream. So when curious passerbys scan the code, they see themselves in a camera stream and feel \"watched\".reply",
      "I had thought about creating a larger roadside banner with the faces (pulled from voters guide) of the city council members who approved Flock, along with the face of the Sheriff with something along the lines of \"These people want to know where your wife and daughter are at all times - deflock.me\" and place it right next to the Flock camera.Gotta tag some political organization on the banner which makes it illegal to remove.reply",
      "I wonder if it\u2019s legal to modify the images to look more sinister. Otherwise, someone passing by might not read the text, making it free advertising for council/sherrif.reply",
      "Not exactly the same, but Massive Attack had some facial recognition software running in the background during a concert to illustrate how pervasive modern day surveillance is: https://petapixel.com/2025/09/17/band-massive-attack-uses-li...reply",
      "That's not face recognition. That's face detection. It just detects faces and sticks a label from a pre-selected list. Come on, this doesn't even pass the basic smell test. \"Facial recognition\" my ass. It doesn't recognize anyone. I could build this in a cave with scraps. There's a huge difference between the two: recognition means you have found a known person, detection means you found a person.That's about the difference between eating sodium chloride and eating sodium.reply",
      "Belgian artist Dries Depoorter has something that comes close, where he tried to match public webcams against Instagram photos. See https://driesdepoorter.be/thefollower .reply",
      "What, are those streams publicly accessible?I'm only aware of boring rooftop weather webcams where obv you can't see yourself.Any examples for what you speak of?reply",
      "I don't mean these Flock cameras, I mean what you refer to as \"boring rooftop weather webcams\". Some of those show people fairly close up and even if you can't recognize your face in the stream, you will recognize the place and realize that it's you, standing there right now in that video stream.Just search for \"<your city> webcam\" and see what you can find.reply",
      "Some places have them available. For example, every highway camera in California (and in some places like Oakland there's plenty of cameras that show crosswalks): https://cwwp2.dot.ca.gov/vm/iframemap.htmQuality isn't great, but you could likely see yourself recognizably.reply",
      "Many are! I live in NY and 511ny.org has a great view of all traffic cams in the state (and some beyond it, but I don't understand how they got on the list...)reply"
    ],
    "link": "https://alpr.watch/",
    "first_paragraph": "Your local government might be discussing surveillance tech like Flock cameras, facial recognition, or automated license plate readers right now. This map helps you find those meetings and take action.Why this matters: \u00a0Municipalities across the US are quietly adopting surveillance technologies in rapidly growing numbers with over 80,000 cameras already out on the streets. These systems track residents' movements, collect biometric data, and build massive databases of our daily lives.alpr.watch scans meeting agendas for keywords like \"flock,\" \"license plate reader,\" \"alpr,\" and more. Each pin on the map shows where these conversations are happening so that you can make a difference.Enter your email below and we'll send you a login link. After logging in, you can set your notification preferences.You're logged in! Update your notification settings to receive alerts.Automated License Plate Recognition (ALPR) systems use cameras and artificial intelligence to capture, read, and store lice"
  },
  {
    "title": "No Graphics API (sebastianaaltonen.com)",
    "points": 407,
    "submitter": "ryandrake",
    "submit_time": "2025-12-16T19:20:17 1765912817",
    "num_comments": 73,
    "comments_url": "https://news.ycombinator.com/item?id=46293062",
    "comments": [
      "This is a fantastic article that demonstrates how many parts of vulkan and DX12 are no longer needed.I hope the IHVs have a look at it because current DX12 seems semi abandoned, with it not supporting buffer pointers even when every gpu made on the last 10 (or more!) years can do pointers just fine, and while Vulkan doesnt do a 2.0 release that cleans things, so it carries a lot of baggage, and specially, tons of drivers that dont implement the extensions that really improve things.If this api existed, you could emulate openGL on top of this faster than current opengl to vulkan layers, and something like SDL3 gpu would get a 3x/4x boost too.reply",
      "I'm surprised he made no mention of the SDL3 GPU API since his proposed API has pretty significant overlap with it.reply",
      "DirectX documentation is on a bad state currently, you have the Frank Lunas's books, which don't cover the latest improvements, and then is hunting through Learn, Github samples and reference docs.Vulkan is another mess, even if there was a 2.0, how are devs supposed to actually use it, especially on Android, the biggest consumer Vulkan platform?reply",
      "No longer needed is a strong statement given how recent the GPU support is. It's unlikely anything could accept those minimum requirements today.But soon? Hopefullyreply",
      "Those requirements more or less line up with the introduction of hardware raytracing, and some major titles are already treating that as a hard requirement, like the recent Doom and Indiana Jones games.reply",
      "Only if you're ignoring mobile entirely. One of the things Vulkan did which would be a shame to lose is it unified desktop and mobile GPU APIs.reply",
      "Eh, I think the jury is still out on whether unifying desktop and mobile graphics APIs is really worth it. In practice Vulkan written to take full advantage of desktop GPUs is wildly incompatible with most mobile GPUs, so there's fragmentation between them regardless.reply",
      "It's quite useful for things like skia or piet-gpu/vello or the general category of \"things that use the GPU that aren't games\" (image/video editors, effects pipelines, compute, etc etc etc)reply",
      "I suppose that's true, yeah. I was focusing too much on games specifically.reply",
      "Doom was able to drop it and is now Steam Deck verified.reply"
    ],
    "link": "https://www.sebastianaaltonen.com/blog/no-graphics-api",
    "first_paragraph": "My name is Sebastian Aaltonen. I have been writing graphics code for 30 years. I shipped my first 3d accelerated game in 1999. Since then I have been working with almost every gaming console generation (Nokia N-Gage, Nintendo DS/Switch, Sony Playstation/Portable, Microsoft Xbox) and every PC graphics API (DirectX, OpenGL, Vulkan). For the last 4 years I have been building a new renderer for HypeHype targeting WebGPU, Metal (Mac & iOS) and Vulkan (Android). During my career I have been building several Ubisoft internal engines, optimizing Unreal Engine 4 and leading the Unity DOTS graphics team. I am a member of the Vulkan Advisory Panel and an Arm Ambassador.This blog post includes lots of low level hardware details. When writing this post I used \u201cGPT5 Thinking\u201d AI model to cross reference public Linux open source drivers to confirm my knowledge and to ensure no NDA information is present in this blog post. Sources: AMD RDNA ISA documents and GPUOpen, Nvidia PTX ISA documents, Intel PR"
  },
  {
    "title": "Announcing the Beta release of ty (astral.sh)",
    "points": 300,
    "submitter": "gavide",
    "submit_time": "2025-12-16T20:52:45 1765918365",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=46294289",
    "comments": [
      "Hopefully it gets added to this comparison:https://htmlpreview.github.io/?https://github.com/python/typ...If that table is anything to go by, Pyright is not to be underestimated.I have briefly tried ty (LSP) in Emacs and it seems to work well so far. The only questionable thing I've encountered is that when the signature of a method is shown, the type annotations of some parameters seem to be presented in a particularly verbose form compared to what I'm used to - maybe they're technically correct but it can be bit much to look at.Anyway, odds are pretty good that ty is what I will end up using long-term, so thanks and congrats on releasing the first beta!reply",
      "Note: while spec conformance is important, I don't recommend using it as the basis for choosing a type checker. It is not representative of the things that most users actually care about (and is not meant to be).(I was on the Python Typing Council and helped put together the spec, the conformance test suite, etc)reply",
      "Can you add some examples of the things users care about that aren't well covered by this? I empathize with everyone who wants a feature comparison chart so they can be confident switching without unknowingly losing important safety checks.reply",
      "I think the idea is not that there are features that aren\u2019t listed, but rather that if a typechecker supports 10 features people care about and is missing 10 that people don\u2019t really use, it will look a lot worse on a list like this than a typechecker with 100% compliance, when in practice it may not really be worse at all.Edit: Based on this other comment, the point was also about things not covered by the spec. \u201cThe spec mostly concerns itself with the semantics of annotations, not diagnostics or inference.\u201d\nhttps://news.ycombinator.com/item?id=46296360reply",
      "The chart does not describe speed (either in general or in any particular case). Speed/performance/latency is a thing users care about that is not included in the feature list.reply",
      "We'll be adding ourselves to that table soon. We'll have some work to catch up with pyright on conformance, but that's what the time between now and stable release is for.reply",
      "pyright is very good, but there is also https://docs.basedpyright.com/latest/ which improves on it further.That said I'm very happy user of uv, so once Ty becomes ready enough will be happy to migrate.reply",
      "Basedpyright plus any AI generated python is a hellscape unless you use hooks and have a lot of patience.reply",
      "Pyright has been great. But it\u2019s slow. Speed of a LSP does matter for UX. Excited to see how much ty improves on this.reply",
      "Pyright is really really good. Anyone that doubts that 10x engineers exist, just go and look at Eric Traut. He's pretty much written it single handedly. Absolute machine.Mypy is trash. Nice to have a table to point to to prove it.reply"
    ],
    "link": "https://astral.sh/blog/ty",
    "first_paragraph": "\n    December \n    16, \n    2025TL;DR: ty is an extremely fast Python type checker and\nlanguage server, written in Rust, and designed as an alternative to tools like mypy, Pyright, and\nPylance.Today, we're announcing the Beta release of ty. We now use ty\nexclusively in our own projects and are ready to recommend it to motivated users for production use.At Astral, we build high-performance developer tools for the Python ecosystem. We're best known for\nuv, our Python package manager, and\nRuff, our linter and formatter.Today, we're announcing the Beta release of the next tool in the Astral toolchain: ty, an\nextremely fast Python type checker and language server, written in Rust.Type checking the home-assistant project on the command-line, without caching (M4).ty was designed from the ground up to power a language server. The entire ty architecture is built\naround \"incrementality\", enabling us to selectively re-run only the necessary computations when a\nuser (e.g.) edits a file or modifies"
  },
  {
    "title": "GPT Image 1.5 (openai.com)",
    "points": 309,
    "submitter": "charlierguo",
    "submit_time": "2025-12-16T18:07:07 1765908427",
    "num_comments": 164,
    "comments_url": "https://news.ycombinator.com/item?id=46291941",
    "comments": [
      "Okay results are in for GenAI Showdown with the new gpt-image 1.5 model for the editing portions of the site!https://genai-showdown.specr.net/image-editingConclusions- OpenAI has always had some of the strongest prompt understanding alongside the weakest image fidelity. This update goes some way towards addressing this weakness.- It's leagues better at making localized edits without altering the entire image's aesthetic than gpt-image-1, doubling the previous score from 4/12 to 8/12 and the only model that legitimately passed the Giraffe prompt.- It's one of the most steerable models with a 90% compliance rateUpdates to GenAI Showdown- Added outtakes sections to each model's detailed report in the Text-to-Image category, showcasing notable failures and unexpected behaviors.- New models have been added including REVE and Flux.2 Dev (a new locally hostable model).- Finally got around to implementing a weighted scoring mechanism which considers pass/fail, quality, and compliance for a more holistic model evaluation (click pass/fail icon to toggle between scoring methods).If you just want to compare gpt-image-1, gpt-image-1.5, and NB Pro at the same time:https://genai-showdown.specr.net/image-editing?models=o4,nbp...reply",
      "This showdown benchmark was and still is great, but an enormous grain of salt should be added to any model that was released after the showdown benchmark itself.Maybe everyone has a different dose of skepticism. Personally I'm not even looking at results for models that were released after the benchmark, for all this tells us, they might as well be one-trick ponies that only do well in the benchmark.It might be too much work, but one possible \"correct\" approach for this kind of benchmark would to periodically release new benchmarks with new tests (that are broadly in the same categories) and only include models that predate each benchmark.reply",
      "Yeah that\u2019s a classic problem, and it's why good tests are such closely guarded secrets: to keep them from becoming training fodder for the next generation of models. Regarding the \"model date\" vs \"benchmark date\" - that's an interesting point... I'll definitely look into it!I don't have any captcha systems in place, but I wonder if it might be worth putting up at least a few nominal roadblocks (such as Anubis [1]) to at least slow down the scrapers.A few weeks ago I actually added some new, more challenging tests to the GenAI Text-to-Image section of the site (the \u201cangelic forge\u201d and \u201covercrowded flat earth\u201d) just to keep pace with the latest SOTA models.In the next few weeks, I\u2019ll be adding some new benchmarks to the Image Editing section as well~~[1] - https://anubis.techaro.lolreply",
      "I think training image models to pass these very specific tests correctly will be very difficult for any of these companies. How would they even do that?reply",
      "GPT Image 1.5 is the first model that gets close to replicating the intricate detail mosaic of bullets in the \"Lord of War\" movie poster for me. Following the prompt instructions more closely also seems better compared to Nano Banana Pro.I edited the original \"Lord of War\" poster with a reference image of Jensen and replaced bullets with GPU dies, silicon wafers and electronic components.https://x.com/singhkays/status/2001080165435113791reply",
      "This leaderboard feels incredibly accurate given my own experience.reply",
      "So when you say \"X attempts\" what does that mean? You just start a new chat with the same exact prompt and hope for a different result?reply",
      "All images are generated using independent, separate API calls. See the FAQ at the bottom under \u201cWhy is the number of attempts seemingly arbitrary?\u201d and \u201cHow are the prompts written?\u201d for more detail, but to quickly summarize:In addition to giving models multiple attempts to generate an image, we also write several variations of each prompt. This helps prevent models from getting stuck on particular keywords or phrases, which can happen depending on their training data. For example, while \u201chippity hop\u201d is a relatively common name for the ball-riding toy, it\u2019s also known as a \u201cspace hopper.\u201d In some cases, we may even elaborate and provide the model with a dictionary-style definition of more esoteric terms.This is why providing an \u201cX Attempts\u201d metric is so important. It serves as a rough measure of how \u201csteerable\u201d a given model is - or put another way how much we had to fight with the model in order for it to consistently follow the prompt\u2019s directives.reply",
      "Z-image was released recently and that's what /r/StableDiffusion all talks about these days. Consider adding that too. It is very good quality for its size (Requires only 6 or 8 gigs of ram).reply",
      "I've actually done a bit of preliminary testing with ZiT. I'm holding off on adding it to the official GenAI site until the base and edit models have been released since the Turbo model is pretty heavily distilled.https://mordenstar.com/other/z-image-turboreply"
    ],
    "link": "https://openai.com/index/new-chatgpt-images-is-here/",
    "first_paragraph": ""
  },
  {
    "title": "Midjourney is alemwjsl (aadillpickle.com)",
    "points": 56,
    "submitter": "aadillpickle",
    "submit_time": "2025-12-11T00:25:57 1765412757",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=46226068",
    "comments": [
      "> Turns out that somehow Midjourney is so commonly searched for, that Google has started serving them in search results for a meaningless English phrase that just means a Korean forgot to switch off of their English keyboard when searching for.BTW, this happens all the time in Korea, because it's extremely common for someone to type something while forgetting to switch to the correct input method.  Try these, for example:    \ucd94\u315c\n    gozjsbtm\n    elwmsl\n    vkdlTjsreply",
      "It gets even better! EBS (\ud55c\uad6d\uad50\uc721\ubc29\uc1a1 Educational Broadcasting System) is using \"\ub4c4 dyun\" as one of its brands, which is one of such mis-transliterated words. Tistory, a popular blog service in Korea, once went by \"\ucd78\uc7ac\uae45 chyojaeging\" in the similar way. (Both words have absolutely no meaning in Korean, suggesting that it was transliterated from QWERTY to a Hangul keyboard layout.)reply",
      "I think you meant Cyworld instead of Tistory.reply",
      "> It's actually insane the levels of understanding the algorithms that are responsible for serving us information have and how little we, the creators of said algorithms, understand what's going on in said algorithms.Keyboard layout mismatches are common enough that I assume Google has a layout detection stage hardcoded just like they have typo correction hardcoded. And the creators of said algorithms probably understand very well how they work. (The na\u00efve way would be to convert from every possible layout to every other layout, but I think you could build something more lightweight using Hidden Markov Models.)reply",
      "Typos could be automatically discovered and indexed one word at a time by watching users search the wrong word (wrong input method) and then search again with the correct input method.reply",
      "This is actually pretty common. It's less obvious with Chinese or Japanese, as the input method there usually matches the transliteration based on how the word is spoken (romaji in Japanese, pinyin in Chinese), which of course does not look unusual.For example, you wouldn't think twice about it if for the Japanese word for washing machine, you not only saw \"\u6d17\u6fef\u6a5f\" (which is how it's written in Kanji), but also \"sentakuki\" or \"sentakki\" in the search results, because even to non-Japanese speakers it's pretty clear that that's probably the Japanese word for washing machine written with latin character transliteration, and pretty much exactly what you'd say.With Korean, it looks more jarring, as the input method is apparently very different, and seems to map the keys for unrelated latin letters to Hangul letters? (I have no idea, I don't know anything about Hangul other than it's based on syllables, kind of like Hiragana/Katakana, and apparently very logical.)reply",
      "> With Korean, it looks more jarring, as the input method is apparently very different, and seems to map the keys for unrelated latin letters to Hangul letters?More or less, yes. Each Hangul character represents a syllable, and is composed of two or more components (jamo) representing individual phonemes (like vowels or consonants) which make up the syllable. The keys on a Korean keyboard are mapped to those jamo.Further details: https://en.wikipedia.org/wiki/Korean_language_and_computersreply",
      "More specifically, since Korean syllables are of the form CV(C) where C is a consonant and V is a vowel, almost all Hangul keyboard layouts divide the entire keyboard into two or three sections (consonant-vowel or initial-medial-final). The standard KS X 5002 layout is the former, a \"bipartite\" method (\ub450\ubc8c\uc2dd), while I'm using one of the latter, \"tripartite\" methods (\uc138\ubc8c\uc2dd).reply",
      "> I scroll up a bit to reread ChatGPT's analysis, and I realize it mentions \"transliteration\". I have no idea what that word means, so I look it up.How?reply",
      "Everyone has gaps in their knowledge which can be things that \"should be obvious\" to others. If someone doesn't know something, they either forgot or haven't learned it yet! I appreciated the author's honesty here.reply"
    ],
    "link": "https://www.aadillpickle.com/blog/midjourney-is-alemwjsl",
    "first_paragraph": ""
  },
  {
    "title": "Sei AI (YC W22) Is Hiring (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-12-17T01:00:21 1765933221",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/sei/jobs/TYbKqi0-llm-engineer-mid-senior",
    "first_paragraph": " AI Agents for Financial InstitutionsWe are Sei, an agentic AI platform for financial services. Since launching, we're live with large enterprises across the US, Europe, and APAC and growing at double digits per month.We are backed by world-class investors, including Y Combinator, Tribe Capital, PayPal, Picus Capital, & Hashed. Pranay (CEO) and Ram (CTO) are the founders. We have combined experience of 20+ years of building fintech and tech products for businesses & customers across the world at companies such as Deutsche Bank, Cloud Kitchens, PayPal, TransferWise, and Amazon, among others.We are looking for an LLM engineer to help shape the company's tech, product, and culture. We are currently working with a bunch of enterprise customers and banks, and are experiencing rapid growth. We are looking to hire mid to senior engineers who can take our V1 into a more scaleable, robust platform as we prepare for more growth.You can expect to do all of the following:Continuous 360 feedback: E"
  },
  {
    "title": "No AI* Here \u2013 A Response to Mozilla's Next Chapter (waterfox.com)",
    "points": 84,
    "submitter": "MrAlex94",
    "submit_time": "2025-12-16T22:07:49 1765922869",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=46295268",
    "comments": [
      "> Large language models are something else entirely*. They are black boxes. You cannot audit them. You cannot truly understand what they do with your data. You cannot verify their behaviour. And Mozilla wants to put them at the heart of the browser and that doesn't sit well.Am I being overly critical here or is this kind of a silly position to have right after talking about how neural machine translation is okay? Many of Firefox's LLM features like summarization afaik are powered by local models (hell even Chrome has local model options). It's weird to say neural translation is not a black box but LLMs are somehow black boxes that we cannot hope to understand what they do with the data, especially when viewed a bit fuzzily LLMs are scaled up versions of an architecture that was originally used for neural translation. Neural translation also has unverifiable behavior in the same sense.I could interpret some of the data talk as talking about non local models but this very much seems like a more general criticism of LLMs as a whole when talking about Firefox features. Moreover, some of the critiques like verifiability of outputs and unlimited scope still don't make sense in this context. Browser LLM features except for explicitly AI browsers like Comet have so far had some scoping to their behavior, either in very narrow scopes like translation or summarization. The broadest scope I can think of is the side panels that show up which allow you to ask about a web page with context. Even then, I do not see what is inherently problematic about such scoping since the output behavior is confined to the side panel.reply",
      "No, it is disqualifyingly clueless. The author defends one neural network, one bag of effectively-opaque floats that get blended together with WASM to produce non-deterministic outputs which are injected into the DOM (translation), then righteously crusades against other bags of floats (LLMs).From this point of view, uBlock Origin is also effectively un-auditable.Or your point about them maybe imagining AI as non-local proprietary models might be the only thing that makes this make sense. I think even technical people are being suckered by the marketing that \"AI\" === ChatGPT/Claude/Gemini style cloud-hosted proprietary models connected to chat UIs.reply",
      "> Machine learning technologies like the Bergamot translation project offer real, tangible utility. Bergamot is transparent in what it does (translate text locally, period), auditable (you can inspect the model and its behavior), and has clear, limited scope, even if the internal neural network logic isn\u2019t strictly deterministic.This really weakens the point of the post. It strikes me as a: we just don't like those AIs. Bergamot's model's behavior is no more or less auditable or a black box than an LLM's behavior. If you really want to go dig into a Llama 7B model, you definitely can. Even Bergamot's underlying model has an option to be transformer-based: https://marian-nmt.github.io/docs/The premise of non-corporate AI is respectable but I don't understand the hate for LLMs. Local inference is laudable, but being close-minded about solutions is not interesting.reply",
      "It's not necessarily close minded to choose to abstain from interacting with generative text, and choose not to use software that integrates it.I could say it's equally close minded not to sympathize with this position, or various reasoning behind it. For me, I feel that my spoken language is effected by those I interact with, and the more exposed someone is to a bot, the more they will speak like that bot, and I don't want my language to be pulled towards the average redditor, so I choose not to interact with LLMs (I still use them for code generation, but I wouldn't if I used code for self expression. I just refuse to have a back and forth conversation on any topic. It's like that family that tried raising a chimp alongside a baby. The chimp did pick up some human like behavior, but the baby human adapted to chimp like behavior much faster, so they abandoned the experiment.)reply",
      "I\u2019m not too worried about starting to write like a bot. But, I do notice that I\u2019m sometimes blunt and demanding when I talk to a bot, and I\u2019m worried that could leak through to my normal talking.I try to be polite just to not gain bad habits. But, for example, chatGPT is extremely confident, often wrong, and very weasely about it, so it can be hard to be \u201cnice\u201d to it (especially knowing that under the hood it has no feelings). It can be annoying when you bounce the third idea off the thing and it confidently replies with wrong instructions.Anyway, I\u2019ve been less worried about running local models, mostly just because I\u2019m running them CPU-only. The capacity is just so limited, they don\u2019t enter the uncanny valley where they can become truly annoying.reply",
      "Sure, I am more referring to advocating for Bergamot as a type of more \"pure\" solution.I have no opinion on not wanting to converse with a machine, that is a perfectly valid preference. I am referring more to the blog post's position where it seems to advocate against itself.reply",
      "You can't really dig into a model you don't control. At least by running locally, you could in theory if it is exposed enough.The focused purpose, I think, gives it more of a \"purpose built tool\" feel over \"a chatbot that might be better at some tasks than others\" generic entity. There's no fake persona to interact with, just an algorithm with data in and out.The latter portion is less a technical and more an emotional nuance, to be sure, but it's closer to how I prefer to interact with computers, so I guess it kinda works on me... If that were the limit of how they added AI to the browser.reply",
      "Yes I agree with this, but the blog post makes a much more aggressive claim.> Large language models are something else entirely. They are black boxes. You cannot audit them. You cannot truly understand what they do with your data. You cannot verify their behaviour. And Mozilla wants to put them at the heart of the browser and that doesn\u2019t sit well.Like I said, I'm all for local models for the exact reasons you mentioned. I also love the auditability. It strikes me as strange that the blog post would write off the architecture as the problem instead of the fact that it's not local.The part that doesn't sit well to me is that Mozilla wants to egress data. It being an LLM I really don't care.reply",
      "This feature can be easily disabled with policies:https://mozilla.github.io/policy-templates/#generativeaihttps://mozilla.github.io/policy-templates/#preferenceshttps://searchfox.org/firefox-main/source/browser/app/profil...https://searchfox.org/firefox-main/source/modules/libpref/in...reply",
      "On Windows Mozilla can't even handle disabling hardware acceleration, a.k.a. the GPU, from its settings page. Sure you can toggle the button but it doesn't work as verified in the task manager. What hope is there that they can be trusted to disable AI then? It's a feature that I'd never want enabled. When that \"feature\" comes out users will be forced to find a fork without the feature.reply"
    ],
    "link": "https://www.waterfox.com/blog/no-ai-here-response-to-mozilla/",
    "first_paragraph": ""
  },
  {
    "title": "Pricing Changes for GitHub Actions (resources.github.com)",
    "points": 484,
    "submitter": "kevin-david",
    "submit_time": "2025-12-16T17:12:02 1765905122",
    "num_comments": 560,
    "comments_url": "https://news.ycombinator.com/item?id=46291156",
    "comments": [
      "It is us, developers, who convinced our management to purchase GitHub Enterprise to be our forge. We didn't pay any heed to the values of software freedom. A closed source, proprietary software had good features. We saw that and convinced our management to purchase it. Never mind what cost it would impose in the future when the good software gets bad owners. Never mind that there were alternatives that were inferior but were community-developed, community-maintained and libre.The writing is in the wall. First it was UX annoyances. Then it was GitHub Actions woes. Now it is paying money for running their software on your own hardware. It's only going to go downhill. Is it a good time now to learn from our mistakes and convince our teams and management to use community-maintained, libre alternatives? They may be inferior. They may lack features. But they're not going to pull user hostile tricks like this on you and me. And hey, if they are lacking features, maybe we should convince our management to let us contribute time to the community to add those features? It's a much better investment than sinking money into a software that will only grow more and more user hostile, isn't it?reply",
      "> alternatives that were inferiorActually there were alternatives that were far superior (seriously, no way to group projects?) but also more than 2x as expensive.  If GH \"fixes the glitch\" then it will be plan B time.reply",
      "It's not my money man.  It's still fine.reply",
      "Seriously. I can't really get worked up about businesses raising the prices other businesses pay.reply",
      "it's just software.it changes and you move on.reply",
      "Have any suggestions to those community-developed and maintained options?reply",
      "Gitea. Gitlab (ish?).reply",
      "GitLab actually implemented Actions first back in the day (called CI/CD). I remember GitHub was following their lead.reply",
      "Takes like these do not account for the value you gained by using the software in the meantime. Here are 2 scenarios:1) company uses exclusively free software, spends more time dealing with the shortcomings of said software than developing product, product is half baked and doesn't sell well, company dies.2) company uses proprietary but cheap/free (as in beer) software that does the job really well, focuses on developing product, product is good and sells well, company how has a ton of money they could use to replicate the proprietary product from scratch if they wanted to.A purist approach like in scenario 1 leaves everyone poor. A pragmatic approach like scenario 2 ends up earning enough money that can be used to recreate the proprietary software from scratch (and open-source it if you wanted to).In this case the problem isn't even the proprietariness of the software, it's the fact that companies are reliant on someone else hosting the software (GH being FOSS wouldn't actually change anything here - whoever is hosting it can still enforce whatever terms they want).FOSS alternatives already exist, it's just that our industry is so consumed by grifters that nobody knows how to do things anymore (because it's more profitable for every individual not to); running software on a server (what used to be table stakes for any shop and junior sysadmin) is nowadays lost knowledge. Microsoft and SaaS software providers are capitalizing on this.reply",
      "> A purist approach like in scenario 1 leaves everyone poor.That depends, not always. Sometimes the employees of said company manages to contribute back upstream, on the dime of the company. If the \"free software\" they used and contributed to have a lot of users, it's certainly not \"leaves everyone poor\" but rather \"helps everyone, beyond monetary gain\".Sure, you can make the argument that it isn't that great for the company, and you may be right. But the world is bigger than companies making money, killing a few companies along the way to make small iterative steps on making free software for absolutely everyone is probably a worthwhile sacrifice, if you zoom out a bit.reply"
    ],
    "link": "https://resources.github.com/actions/2026-pricing-changes-for-github-actions/",
    "first_paragraph": "December 15, 2025 // 7 min readTLDR - On January 1, 2026, we are lowering the price of hosted runners, and beginning March 1, 2026, we are charging $0.002 per-minute across self-hosted runners. The vast majority of customers will not see a change to their bill. Actions will remain free in public repositories. We\u2019re announcing updates to our pricing and product models for GitHub Actions. Historically, self-hosted runner customers were able to leverage much of GitHub Actions\u2019 infrastructure and services at no cost. This meant that the cost of maintaining and evolving these essential services was largely being subsidized by the prices set for GitHub-hosted runners. By updating our pricing, we\u2019re aligning costs more closely with usage and the value delivered to every Actions user,  while fueling further innovation and investment across the platform. The vast majority of users, especially individuals and small teams, will see no price increase.We will have a GitHub Actions pricing calculato"
  },
  {
    "title": "I ported JustHTML from Python to JavaScript with Codex CLI and GPT-5.2 in hours (simonwillison.net)",
    "points": 58,
    "submitter": "pbowyer",
    "submit_time": "2025-12-16T22:48:56 1765925336",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=46295771",
    "comments": [
      "I think the most interesting thing about this is how it demonstrates that a very particular kind of project is now massively more feasible: library porting projects that can be executed against implementation-independent tests.The big unlock here is https://github.com/html5lib/html5lib-tests - a collection of 9,000+ HTML5 parser tests that are their own independent file format, e.g. this one: https://github.com/html5lib/html5lib-tests/blob/master/tree-...The Servo html5ever Rust codebase uses them. Emil's JustHTML Python library used them too. Now my JavaScript version gets to tap into the same collection.This meant that I could set a coding agent loose to crunch away on porting that Python code to JavaScript and have it keep going until that enormous existing test suite passed.Sadly conformance test suites like html5lib-tests aren't that common... but they do exist elsewhere. I think it would be interesting to collect as many of those as possible.reply",
      "This is one of the reasons I'm keeping tests to myself for a current project. Usually I release libraries as open source, but I've been rethinking that, as well.reply",
      "Oddly enough my conclusion is the opposite: I should invest more of my open source development work in creating language-independent test suites, because they can be used to quickly create all sorts of useful follow-on projects.reply",
      "I\u2019ve idly wondered about this sort of thing quite a bit. The next step would seem to be taking a project\u2019s implementation dependent tests, converting them to an independent format and verifying them against the original project, then conducting the port.reply",
      "This is an interesting case. It may be good to feed it to other model and see how they do.Also: it may be interesting to port it to other languages too and see how they do.JS and Py are but runtime-typed and very well \"spoken\" by LLMs. Other languages may require a lot more \"work\" (data types, etc.) to get the port done.reply",
      "Few know that Firefox's HTML5 parser was originally written in Java, and only afterward semi-mechanically translated (pre-LLMs) to the dialect of C++ used in the Gecko codebase.This blog post isn't really about HTML parsers, however.  The JustHTML port described in this blog post was a worthwhile exercise as a demonstration on its own.Even so, I suspect that for this particular application, it would have been more productive/valuable to port the Java codebase to TypeScript rather than using the already vibe coded JustHTML as a starting point.  Most of the value of what is demonstrated by JustHTML's existence in either form comes from Stenstr\u00f6m's initial work.reply",
      "Whoa... it looks like the Firefox HTML5 parser is still maintained as Java to this day!Here's the relevant folder:https://github.com/mozilla-firefox/firefox/tree/main/parser/...  make translate        # perform the Java-to-C++ translation from the remote\n                        # sources\n\nAnd active commits to that javasrc folder - the last was in November: https://github.com/mozilla-firefox/firefox/commits/main/pars...reply",
      "I have secretly held the belief for a while that the Java implementation should be mechanically translated to TypeScript and then fixed up, annotated, and maintained not just primarily but entirely in that form; the requisite R&D/tooling should be created to:(a) permit a fully mechanical, on-the-fly rederivation of the canonical TypeScript sources into Java, for Java consumers that need it (a lot like the ts->js step that happens for execution on JS engines), and(b) compiler support that can go straight from the TypeScript subset used in the parser to a binary that's as performant as the current native implementation, without requiring any intermediate C++ form to be emitted or reviewed/vetted/maintained by hand(Hejlsberg is being weird/not entirely forthcoming about the overall goals wrt the announcement last year about porting the TypeScript computer to Go.  We're due for an announcement that they've done something like lifted the Go compilers' backends out of the golang.org toolchain, strapped the legacy tsc frontend on top, allowing the TypeScript compiler to be continued to be developed and maintained in TypeScript while executing with the performance previously seen mostly with tools written in Go.)I agree with the overall conclusion of the post that what is demonstrated there is a good use case for LLMs.  It might even be the best use for them, albeit something to be undertaken/maintained as part of the original project.  It wouldn't be hugely surprising if that turned out to be the dominant use of LLM-powered coding assistants when everything shakes out (all the other promises that have been made for and about them notwithstanding).They could also play a significant role in the project I outlined above.reply",
      "There are certainly dozens of better ways to do what I did here.I picked JustHTML as a base because I really liked the API Emil had designed, and I also thought it would be darkly amusing to take his painstakingly (1,000+ commits, 2 months+ of work) constructed library and see if I could port it directly to Python in an evening, taking advantage of everything he had already figured out.reply",
      "> How much better would this library be if an expert team hand crafted it over the course of several months?i think the fun conclusion would be: ideally no better, and no worse. that is the state you arrive it IFF you have complete tests and specs (including probably for performance). now a human team handcrafting would undoubtedly make important choices not clarified in specs, thereby extending the spec. i would argue that human chain of thought from deep involvement in building and using the thing is basically 100% of the value of human handcrafting, because otherwise yeah go nuts giving it to an agent.reply"
    ],
    "link": "https://simonwillison.net/2025/Dec/15/porting-justhtml/",
    "first_paragraph": "15th December 2025I wrote about JustHTML yesterday\u2014Emil Stenstr\u00f6m\u2019s project to build a new standards compliant HTML5 parser in pure Python code using coding agents running against the comprehensive html5lib-tests testing library. Last night, purely out of curiosity, I decided to try porting JustHTML from Python to JavaScript with the least amount of effort possible, using Codex CLI and GPT-5.2. It worked beyond my expectations.I built simonw/justjshtml, a dependency-free HTML5 parsing library in JavaScript which passes 9,200 tests from the html5lib-tests suite and imitates the API design of Emil\u2019s JustHTML library.It took two initial prompts and a few tiny follow-ups. GPT-5.2 running in Codex CLI ran uninterrupted for several hours, burned through 1,464,295 input tokens, 97,122,176 cached input tokens and 625,563 output tokens and ended up producing 9,000 lines of fully tested JavaScript across 43 commits.Time elapsed from project idea to finished library: about 4 hours, during which I"
  },
  {
    "title": "40 percent of fMRI signals do not correspond to actual brain activity (tum.de)",
    "points": 392,
    "submitter": "geox",
    "submit_time": "2025-12-16T13:46:57 1765892817",
    "num_comments": 170,
    "comments_url": "https://news.ycombinator.com/item?id=46288415",
    "comments": [
      "My previous job was at a startup doing BMI, for research. For the first time I had the chance to work with expensive neural signal measurement tools (mainly EEG for us, but some teams used fMRI). and quickly did I learn how absolute horrible the signal to noise ratio (SNR) was in this field.And how it was almost impossible to reproduce many published and well cited result. It was both exciting and jarring to talk with the neuroscientist, because they ofc knew about this and knew how to read the papers but the one doing more funding/business side ofc didn't really spend much time putting emphasis on that.One of the team presented a accepted paper that basically used Deep Learning (Attention) to predict images that a person was thinking of, from the fMRI signals. When I asked \"but DL is proven to be able to find pattern even in random noise, so how can you be sure this is not just overfitting to artefact?\" and there wasn't really any answer to that (or rather the publication didn't take that in to account, although that can be experimentally determined). Still, a month later I saw tech explore or some tech news writing an article about it, something like \"AI can now read your brain\" and the 1984 implications yada yada.So this is indeed something probably most practitioners, masters and PhD, realize relatively early.So now that someone says \"you know mindfulness is proven to change your brainwaves?\" I always add my story \"yes, but the study was done with EEG, so I don't trust the scientific backing of it\" (but anecdotally, it helps me)reply",
      "There are lots of reliable science done using EEG and fMRI; I believe you learned the wrong lesson here. The important thing is to treat motion and physiological sources of noise as a first-order problem that must be taken very seriously and requires strict data quality inclusion criterion. As far as deep learning in fMRI/EEG, your response about overfitting is too sweepingly broad to apply to the entire field.To put it succinctly, I think you have overfit your conclusions on the amount of data you have seenreply",
      "I would argue in fact almost all fMRI research is unreliable, and formally so (test-retest reliabilities are in fact quite miserable: see my post below).https://news.ycombinator.com/item?id=46289133EDIT: The reason being, with reliabilities as bad as these, it is obvious almost all fMRI studies are massively underpowered, and you really need to have hundreds or even up to a thousand participants to detect effects with any statistical reliability. Very few fMRI studies ever have even close to these numbers (https://www.nature.com/articles/s42003-018-0073-z).reply",
      "That depends immensely on the type of effect you're looking for.Within-subject effects (this happens when one does A, but not when doing B) can be fine with small sample sizes, especially if you can repeat variations on A and B many times. This is pretty common in task-based fMRI. Indeed, I'm not sure why you need >2 participants expect to show that the principle is relatively generalizable.Between-subject comparisons (type A people have this feature, type B people don't) are the problem because people differ in lots of ways and each contributes one measurement, so you have no real way to control for all that extra variation.reply",
      "Precisely, and agreed 100%. We need far more within-subject designs.You would still in general need many subjects to show the same basic within-subject patterns if you want to claim the pattern is \"generalizable\", in the sense of \"may generalize to most people\", but, precisely depending on what you are looking at here, and the strength of the effect, of course you may not need nearly as much participants as in strictly between-subject designs.With the low test-retest reliability of task fMRI, in general, even in adults, this also means that strictly one-off within-subject designs are also not enough, for certain claims. One sort of has to demonstrate that even the within-subject effect is stable too. This may or may not be plausible for certain things, but it really needs to be considered more regularly and explicitly.reply",
      "Between-subject heterogeneity is a major challenge in neuroimaging. As a developmental researcher, I've found that in structural volumetrics, even after controlling for total brain size, individual variance remains so large that age-brain associations are often difficult to detect and frequently differ between moderately sized cohorts (n=150-300). However, with longitudinal data where each subject serves as their own control, the power to detect change increases substantially\u2014all that between-subject variance disappears with random intercept/slope mixed models. It's striking.Task-based fMRI has similar individual variability, but with an added complication: adaptive cognition. Once you've performed a task, your brain responds differently the second time. This happens when studies reuse test questions\u2014which is why psychological research develops parallel forms. But adaptation occurs even with parallel forms (commonly used in fMRI for counterbalancing and repeated assessment) because people learn the task type itself. Adaptation even happens within a single scanning session, where BOLD signal amplitude for the same condition typically decreases over time.These adaptation effects contaminate ICC test-retest reliability estimates when applied naively, as if the brain weren't an organ designed to dynamically respond to its environment. Therefore, some apparent \"unreliability\" may not reflect the measurement instrument (fMRI) at all, but rather highlights the failures in how we analyze and conceptualize task responses over time.reply",
      "Yeah, when you start getting into this stuff and see your first dataset with over a hundred MRIs, and actually start manually inspecting things like skull-stripping and stuff, it is shocking how dramatically and obviously different people's brains are from each other. The nice clean little textbook drawings and other things you see in a lot of education materials really hide just how crazy the variation is.And yeah, part of why we need more within-subject and longitudinal designs is to get at precisely the things you mention. There is no way to know if the low ICCs we see now are in fact adaptation to the task or task generalities, if they reflect learning that isn't necessarily task-relevant adaptation (e.g. the subject is in a different mood on a later test, and this just leads to a different strategy), if the brain just changes far more than we might expect, or all sorts of other possibilities. I suspect if we ever want fMRI to yield practical or even just really useful theoretical insights, we definitely need to suss out within-subject effects that have high test-retest reliability, regardless of all these possible confounds. Likely finding such effects will involve more than just changes to analysis, but also far more rigorous experimental designs (both in terms of multi-modal data and tighter protocols, etc).FWIW, we've also noticed a lot of magic can happen too when you suddenly have proper longitudinal data that lets you control things at the individual level.reply",
      "Yes on many of those fronts, although not all those papers support your conclusion. The field did/does too often use tasks with to few trials, with to few participants. That always frustrated me as my advisor rightly insisted we collect hundreds of participants for each study, while others would collect 20 and publish 10x faster than us.reply",
      "Yes, well \"almost all\" is vague and needs to be qualified. Sample sizes have improved over the past decade for sure. I'm not sure if they have grown on median meaningfully, because there are still way too many low-N studies, but you do see studies now that are at least plausibly \"large enough\" more frequently. More open data has also helped here.EDIT: And kudos to you and your advisor here.EDIT2: I will also say that a lot of the research on fMRI methods is very solid and often quite reproducible. I.e. papers that pioneer new analytic methods and/or investigate pipelines and such. There is definitely a lot of fMRI research telling us a lot of interesting and likely reliable things about fMRI, but there is very little fMRI research that is telling us anything reliably generalizable about people or cognition.reply",
      "I remember when resting-state had its oh shit moment when Power et al (e.g. https://pubmed.ncbi.nlm.nih.gov/22019881/) showed that major findings in the literature, many of which JD Power himself helped build, was based off residual motion artifacts. Kudos to JD Power and others like him.reply"
    ],
    "link": "https://www.tum.de/en/news-and-events/all-news/press-releases/details/40-percent-of-mri-signals-do-not-correspond-to-actual-brain-activity",
    "first_paragraph": "If you use one of the color modes, the TUM website and its elements will be displayed in either dark or light.The settings are stored on your computer and not transferred to the server.40 percent of MRI signals do not correspond to actual brain activityFor almost three decades, functional magnetic resonance imaging (fMRI) has been one of the main tools in brain research. Yet a new study published in the renowned journal Nature Neuroscience fundamentally challenges the way fMRI data have so far been interpreted with regard to neuronal activity. According to the findings, there is no generally valid coupling between the oxygen content measured by MRI and neuronal activity.Researchers at the Technical University of Munich (TUM) and the Friedrich-Alexander-University Erlangen-Nuremberg (FAU) found that an increased fMRI signal is associated with reduced brain activity in around 40 percent of cases. At the same time, they observed decreased fMRI signals in regions with elevated activity. Fi"
  },
  {
    "title": "Mozilla appoints new CEO Anthony Enzor-Demeo (blog.mozilla.org)",
    "points": 414,
    "submitter": "recvonline",
    "submit_time": "2025-12-16T13:53:14 1765893194",
    "num_comments": 637,
    "comments_url": "https://news.ycombinator.com/item?id=46288491",
    "comments": [
      "Having worked at Mozilla a while ago, the CEO role is one I wouldn't wish on my worst enemy. Success is oddly defined: it's a non-profit (well, a for-profit owned by a non-profit) that needs to make a big profit in a short amount of time. And anything done to make that profit will annoy the community.I hope Anthony leans into what makes Mozilla special. The past few years, Mozilla's business model has been to just meekly \"us-too!\" trends... IoT, Firefox OS, and more recently AI.What Mozilla is good at, though, is taking complex things the average user doesn't really understand, and making it palpable and safe. They did this with web standards... nobody cared about web standards, but Mozilla focused on usability.(Slide aside, it's not a coincidence the best CEO Mozilla ever had was a designer.)I'm not an AI hater, but I don't think Mozilla can compete here. There's just too much good stuff already, and it's not the type of thing Mozilla will shine with.Instead, if I were CEO, I'd go the opposite way: I'd focus on privacy. Not AI privacy, but privacy in general. Buy a really great email provider, and start to own \"identity on the internet\". As there's more bots and less privacy, identity is going to be incredibly important over the years.. and right now, Google defacto owns identity. Make it free, but also give people a way to pay.Would this work? I don't know. But like I said, it's not a job I envy.reply",
      "Fully agree with this.- Mozilla SSL Certs - for corporations that don't want Let's Encrypt- Mozilla Mail - a reliable Exchange/Google Mail alternative (desperately needed imo)- Thunderbird for iOS - why is this not a thing yet?- Mozilla Search - metasearch that isn't based on Bing/DDG/GoogleAll seemingly low-hanging fruit with brand alignment.reply",
      "> Mozilla Mail - a reliable Exchange/Google Mail alternative (desperately needed imo)Thunderbird Pro was announced a while back, still not GA thoughreply",
      "I'm still sad they shelved Mozilla Persona due to low adoption. There is a hole in the market around privacy and identity, and Mozilla would be a natural choice to fill it, but it's going to be an uphill battle to get major sites and end users on board. Not a job to be envious about indeed.reply",
      "It's such a hard position, I think, because the community and their own users have a huge stick of their ass.It's damned if you do, damned if you don't. Basically every product Mozilla releases is immediately met with extreme scourn and scepticism. While everyone else seems to get the benefit of the doubt, including the likes of Google, Mozilla seems to get the exact opposite of that. This is despite them having a much, much better track record when it comes to privacy.And then, when the products inevitably fail, people will lose their minds. As if any one of them was actually using it. Where were you guys earlier?With pocket, all I heard about was how annoying the little button was and how it was a slap on users faces. Stupid complaints, to be sure. But then it goes away and oh no! Actually, it was the best thing ever and we will all miss it dearly and grrrr damn you Mozilla!reply",
      "You don't really seem to be trying to fairly describe the problem.With Pocket, Mozilla forced it on everyone, then two years later they bought the service, then many years later they eventually killed it for everyone. They didn't even try the approach of making it an opt-in extension that users could install if they desired. The unoffensive strategy was obvious all along, and they just didn't choose that route. The concerns of Mozilla partnering with and promoting a proprietary service were easily anticipated, and the solution (buying Pocket) was clearly an option since they did that step eventually.Yes, Mozilla may be in a hard place trying to diversify and find success with their other ventures. But they're clearly making plenty of unforced errors along the way.reply",
      "That unforced error was particularly egregious considering that tab containers and Facebook containers are optional addons that are well integrated into the browser.reply",
      "> With Pocket, Mozilla forced it on everyone,It was ridiculously easy to turn off. Making a fairly non-obtrusive service opt-out instead of opt-in is not forcing it on everyone.reply",
      "They literally forced every user to either accept the invasion of the proprietary service, or have to take extra steps to disable it on each of their devices. Neither of those is actually a reasonable, respectful way to treat your users.reply",
      "I just never used Pocket. I don\u2019t think I had to change my habits or settings to do so.reply"
    ],
    "link": "https://blog.mozilla.org/en/mozilla/leadership/mozillas-next-chapter-anthony-enzor-demeo-new-ceo/",
    "first_paragraph": ""
  },
  {
    "title": "Dafny: Verification-Aware Programming Language (dafny.org)",
    "points": 24,
    "submitter": "handfuloflight",
    "submit_time": "2025-12-16T22:50:59 1765925459",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=46295792",
    "comments": [
      "This might be a stupid question, but why a separate programming language rather than aiming to verify/synthesize invariants in languages people use?reply",
      "The semantics of Dafny is carefully designed to make verification efficient.Dafny can compile to and interface with a few languages, including C#.reply",
      "Not a stupid question at all.  There are two reasons verification tends to happen in these specialized languages: the languages we usually use are often not expressive enough to write things like specifications, and a bit too expressive in the sense of letting people write program logic that is insanely difficult to verify (think untyped pointers into a dynamically allocated heap for example).  So these verification related languages often are more expressive on the spec side and more restrictive in terms of what kind of code you can write.reply",
      "> \u2026 verify/synthesize invariants in languages people use?Good question. This is the holy grail. This is what everyone in PL research would love. This is where we want to get to.Turns out a language as \u201csimple\u201d as C has sufficiently complicated semantics as to limit rigorous analysis to the basics. One example is loop analysis: it\u2019s very useful to know that a loop will terminate eventually; if a loop is modifying some state and\u2014worse\u2014if the iteration variable gets modified\u2014kiss your analysis goodbye because mechanically synthesizing strong pre- and post-conditions becomes insurmountable. It\u2019s not an engineering challenge. It\u2019s a math/pure CS theory challenge.reply",
      "Most existing mainstream languages aren\u2019t expressive enough to encode these invariants. For languages outside of the mainstream, Lean 4 is a language supporting verification, and it\u2019s also a full programming language, so you can write your proofs/theorems in the same language that you program in.reply",
      "Dafny has been around for a while and people do in fact use it. People also apply contract languages to C and all matter of other things, so really question boils down to \"Why arent you doing what I expect of you?\"reply",
      "Looks interesting. I saw some C# files, from which it seems it is implemented in C#. Is there going to be an implementation in Dafny?reply",
      "Reminds me of Eiffel, in a good way. Looks awesome. Is there anything close to this in Scala by chance?reply",
      "Dafny is quite different from Scala in that it is a formal language that can be compiled to a number of different targets such as Go or Python or C#. This allows an algorithm to be formally verified while still producing executable code.You could add Scala as a compilation target or you could just use the Java output and call formally verified Java functions from Scala. Even if you do get an implementation that produces Scala, don't expect the full power of idiomatic Scala to be available in the code you formally verify. To verify code, you have to write the code in Dafny with associated assertions to be proven. Since there are multiple compilation targets multiple formal constraints on what can usefully be verified, the data types available will not match the data types that you would use natively from Scala.reply",
      "It's similar in spirit, but in Dafny one can express much more complicated and complex invariants which get checked at build time -- compared to eiffel where pre/post conditions are checked at runtime (in dev builds mostly).reply"
    ],
    "link": "https://dafny.org/",
    "first_paragraph": "\n\nDafny is a verification-aware programming language that has native support for recording specifications\nand is equipped with a static program verifier.\nBy blending sophisticated automated reasoning with familiar programming idioms and tools,\nDafny empowers developers to write provably correct code (w.r.t. specifications).\nIt also compiles Dafny code to familiar development environments such as C#, Java, JavaScript, Go and Python (with more to come) so Dafny can integrate with your existing workflow.\nDafny makes rigorous verification an integral part of development,\nthus reducing costly late-stage bugs that may be missed by testing.\n\nIn addition to a verification engine to check implementation against specifications,\nthe Dafny ecosystem includes several compilers,\nplugins for common software development IDEs,\na LSP-based Language Server,\na code formatter,\na reference manual,\ntutorials,\npower user tips,\nbooks,\nthe experiences of professors teaching Dafny,\nand the accumulating expertise"
  },
  {
    "title": "Show HN: TheAuditor v2.0 \u2013 A \u201dFlight Computer\u201c for AI Coding Agents (github.com/theauditortool)",
    "points": 7,
    "submitter": "ThailandJohn",
    "submit_time": "2025-12-16T13:50:15 1765893015",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://github.com/TheAuditorTool/Auditor",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Antidote to VibeCoding\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Database-First Static Analysis and Code Context IntelligenceMulti-language security analysis platform with strict data fidelity guarantees for Python, JavaScript/TypeScript, Go, Rust, Bash, and Terraform/HCL projects\ud83d\udd12 Privacy-First: All code analysis runs locally. Your source code never leaves your machine.Network Features (fully optional - use --offline to disable):Default mode includes network calls. Run aud full --offline for air-gapped operation.TheAuditor is a database-first code intelligence platform that indexes your entire codebase into a structured SQLite database, enabling:Key Differentiator: While most SAST tools re-parse files for every query, TheAuditor indexes incrementally "
  },
  {
    "title": "Thin desires are eating life (joanwestenberg.com)",
    "points": 305,
    "submitter": "mitchbob",
    "submit_time": "2025-12-16T00:50:41 1765846241",
    "num_comments": 126,
    "comments_url": "https://news.ycombinator.com/item?id=46283276",
    "comments": [
      "I can't help but feel that this article was written in a format that is the textual equivalent of thin desires\u2026Every sentence is separated into its own paragraph, like each one is supposed to be revelatory (or maybe tweet-worthy). It's pretty common design knowledge that if you try to emphasize everything, you end up emphasizing nothing. The result is that reading the article feels choppy, and weirdly unsatisfying, since the larger arc of each point is constantly being interrupted.Why choose such an antithetical form, to what is otherwise an important and deep message?The only answer that comes to mind is that the author's livelihood, or at least their internal gauge of success, is tied to manipulating readers' thin desires.reply",
      "Reading, I knew someone would comment on it. I actually prefer the style - maybe because my attention span is shot. But I think it\u2019s more because the author made sure each sentence was content heavy. No verbose paragraphs. And paragraphs made of dense sentences are themselves dense and become harder to read.Reflect on the structure of your own comment. I suspect you were not intentionally trying to be ironic.Edit: revisiting the article, I\u2019ll allow that the author may have over-done it in some parts. But I think the bias was in the right direction.reply",
      "It's not just you. I've read this person's stuff before. Every sentence comes off as if they are presenting the results of a major epiphany.You can write things which sound pretty. It's the equivalent of wordy sugar. It's much harder to to write things you've learned from life experience or thought deeply about.Subject your beliefs to the Socratic method. If they've survived your own criticism to the fullest extent and can be validated by your own lived experience, then maybe they've got an inkling of truth and they're worth writing about.reply",
      "I agree with the general sentiment of your comment, but not this:> then maybe they've got an inkling of truth and they're worth writing about.Ideas don't have to be infallible to be worth writing about. It's a slippery slope to not writing at all.reply",
      "Same reaction - I could immediately tell this person had learned to write on Twitter (or Linkedin), not real meaty writing. I had an English professor who wrote \"FORM = CONTENT\" on the chalkboard; this article would send him into a fury.reply",
      "This type of layout - short or 1 sentence paragraphs - has been around since the early days of the web.An early proponent was the BBC news website, and you can see they still adopt this style.The BBC found that breaking up text in this way made it easier to read on a web page.reply",
      "News is the ultimate in thin writing, by definition.I think the article would've been improved by varying sentence structure and paragraph length.  There is a time and place for short paragraphs, and they do make things easier to read.  However, the whole point the article is making is that many things that are worth doing are not easy, and many things that are easy are not worth doing.  It's explicitly advocating for people to engage with the world around them, even if that means they have to face the possibility of changing themselves.Long-form paragraphs are exactly that: harder to read, but they invite you to grapple with the material that's being written.reply",
      "Interspersed single sentence and denser paragraphs, seem to get the most bang out of both.My reply was prompted by both the substance and style of your comment. :)reply",
      "I quite like that this is a more unique writing style and in fact would encourage people to write \"unusually\".reply",
      "It sounds like a Ted Talk with unnecessarily long poses to let sentences sink in. For some reason I just can't digest this sort of writing.reply"
    ],
    "link": "https://www.joanwestenberg.com/thin-desires-are-eating-your-life/",
    "first_paragraph": "The defining experience of our age seems to be hunger.\u00a0We're hungry for more, but we have more than we need.\u00a0We're hungry for less, while more accumulates and multiplies.We're hungry and we don't have words to articulate why.We're hungry, and we're lacking and we're wanting.We are living with a near-universal thin desire: wanting something that cannot actually be gotten, that we can't define, from a source that has no interest in providing it.The distinction between thick and thin desires isn't original to me.Philosophers have been circling this territory for decades, from Charles Taylor's work on frameworks of meaning to Agnes Callard's more recent writing on aspiration.But the version I find most useful is simple:A thick desire is one that changes you in the process of pursuing it.A thin desire is one that doesn't.The desire to understand calculus versus the desire to check your notifications are both real desires, and both produce (to a degree) real feelings of satisfaction when ful"
  },
  {
    "title": "Chat-tails: Throwback terminal chat, built on Tailscale (tailscale.com)",
    "points": 48,
    "submitter": "nulbyte",
    "submit_time": "2025-12-16T21:16:35 1765919795",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=46294592",
    "comments": [
      "Looking forward to hearing Tailscale getting forced to do age verification and banning use of this to people under 16 years old in Australia under their new social media rules...reply",
      "Don't worry, VPNs are next in line to be banned anyway due to their big evasion potential of all this age verification nonsensereply",
      "ps just as I wrote this the English had to prove my point: https://news.ycombinator.com/item?id=46294780reply",
      "Ssh and ssl/tls without licence will be next :)reply",
      "Things like this make me wish there was a sort of public version of Tailscale where everyone got a routable IP address to everyone else no matter what kind of firewall they were behind. Like the old days of the internet, I guess.reply",
      "https://yggdrasil-network.github.io/ for the most part enables this - otherwise I2P and Tor for the most part facilitate this with the bonus encryption element.We might one day have it natively with ipv6 adoption increasing.reply",
      "This is great, I've been looking for an easy to use local chat app for me and my kids, and Adium on Bonjour has been flaky with my VLAN setup at home. Will have to give this a try...reply",
      "I'm gonna need an app for that . No, I just tried it. Works as advertised. Thank you for the dockerfile. Using putty, the formatting is messed up.on the banner/help. Must be some dynamic end of line thing. Still works so.reply",
      "this reminds me of gopher chatsreply",
      "Enough with the tailscale spamreply"
    ],
    "link": "https://tailscale.com/blog/chat-tails-terminal-chat",
    "first_paragraph": "To find a safe space for his kid to chat with friends while playing Minecraft, Brian Scott had to go back to the future.The chat went back, that is, to an IRC-like interface, run through a terminal. The connection and setup remain futuristic, because Scott used Tailscale, and tsnet, to build chat-tails.Chat-tails is the opposite of everything modern chat apps are offering. Nobody can get in without someone doing some work to invite them. All the chats are ephemeral, stored nowhere easy to reach, unsearchable. There are no voice chats, plug-ins, avatars, or images at all, really, unless you count ASCII art. And that\u2019s just the way Brian wanted it.\u201cIt\u2019s about, hey, you have this private space, across your friends\u2019 tailnets, where you can chat, about the game you\u2019re playing or whatever you\u2019re doing,\u201d Scott told me. \u201cIt\u2019s supposed to be more like the days where you were all on the same LAN, you would bring your computers together and have a gaming session. Now you can kind of have that sam"
  },
  {
    "title": "MIT professor shot at his Massachusetts home dies (bbc.com)",
    "points": 178,
    "submitter": "mosura",
    "submit_time": "2025-12-16T21:52:26 1765921946",
    "num_comments": 73,
    "comments_url": "https://news.ycombinator.com/item?id=46295071",
    "comments": [
      "American MSM has carefully avoided mentioning a critically important fact pointing towards the motives of the killer: the professor was Jewish and openly pro-Israel.https://www.theyeshivaworld.com/news/general/2487170/jewish-...reply",
      "You\u2019re trying too hard to make that conspiratorial take: most responsible outlets don\u2019t speculate on motives until there\u2019s some evidence of a connection. For example, the stories I\u2019ve read quoted his neighbors wondering whether there\u2019s a connection to what happened at Brown, which is just an hour away and still has the killer at large. If there\u2019s any evidence of an anti-Jewish motive, I will be shocked if it\u2019s not an NYT headline within minutes.reply",
      "My prediction is that it was a random home invasion robbery committed by someone with multiple previous felonies who had no idea that the person living in the house they were trying to rob was a MIT professor.But I have no more information than anyone else does, I'm making a low-confidence educated guess, and at some point in the near future it's very likely that the professionals whose job it is to investigate serious crimes will have a better idea of what actually happened than anyone posting in this thread.reply",
      "It's a reasonable guess, but 8:30p seems like a dumb time for a home robbery.  Usually they're committed during the day when people are at work, and if not that then deep in the night for maximum cover.  8:30 is almost like the ideal time if you actually want someone to be there and answer the door at an hour where it wouldn't cause enough alarm for them to answer the door with a weapon.reply",
      "When it comes to small-scale crime like this, the smartest thing is typically not to do it at all. So the people who do it will generally not be very smart.reply",
      "Indeed, 8:30p is no different from 2p or 10a for the act.It's most likely a matter of happenstance. It happened to be the warmest time of the day (even though it was evening). Maybe the thinking was someone was home to help them find the valuables, maybe not.>  8:30p seems like a dumb time for a home robbery.The assertion that there is some optimization for some specific imagined motivation, is literal fantasy.reply",
      "> Correction 16 December: An earlier version of this story incorrectly defined the kind of plasma that Professor Loureiro researched.If I get shot and someone writes some libelous bullshit about how I worked with hygienic macro systems, someone kindly jump on that shit ASAP. Thanks in advance!reply",
      "Here's the local Boston news reporting on it:https://www.youtube.com/watch?v=CmbmBNre5SQreply",
      "Could this be related to the Brown shooting?reply",
      "From ABC -\"Authorities have investigated whether his death could be connected to this weekend's Brown University shooting and, at this point, a senior law enforcement official briefed on both cases told ABC News there is nothing to suggest they\u2019re connected.\"https://abcnews.go.com/US/mit-professor-shot-killed-home-bos...reply"
    ],
    "link": "https://www.bbc.com/news/articles/cly08y25688o",
    "first_paragraph": "A Massachusetts university professor who was shot at his home has died, campus officials say. Nuno F Gomes Loureiro, 47, a nuclear science and engineering professor from Portugal, was shot \"multiple times\" on Monday and died on Tuesday morning in hospital, according to Brookline police and Massachusetts Institute of Technology (MIT) officials.Police said officers responded to a call for gunshots at an apartment at about 8:30pm local time. Loureiro was taken by ambulance to a Boston hospital, where he died on Tuesday morning. No one is in custody and police are treating the incident as \"an active and ongoing homicide investigation\",  the Norfolk County District Attorney's Office said.CBS News, the BBC's US media partner, reported that a neighbour said he heard \"three loud bangs\" Monday evening and thought somebody in the apartment building was kicking in a door. Long-time resident Anne Greenwald told CBS that the professor had a young family and went to school nearby.Loureiro majored in"
  },
  {
    "title": "Writing a blatant Telegram clone using Qt, QML and Rust. And C++ (kemble.net)",
    "points": 82,
    "submitter": "tempodox",
    "submit_time": "2025-12-16T15:41:47 1765899707",
    "num_comments": 48,
    "comments_url": "https://news.ycombinator.com/item?id=46289918",
    "comments": [
      "> I believe they have put the most love into their user interfaces out of all the chat programs I have seenAbsolutely true.Telegram: Best UI. Signal: Best privacy. WhatsApp: Largest userbase.It's interesting to think about these three dimensions. I could theoretically pinpoint everything that make Telegram's UI the best, and copy it. I could do the same with Signal's privacy. Both of these are technical problems. There's a process for becoming the best at UI, and there's a process for becoming the best at privacy. I don't know a process for becoming the one with the largest userbase.Other than the 3 big ones, I recently found Jami [1]Good UI, though not as good as Telegram. Arguably better privacy than Signal - you don't even need an account if you don't want. Zero userbase. Free software.[1] https://jami.net/reply",
      "Telegram is also the best at first class support of all the platforms it runs on. In addition to the Qt-based app that's popular on Windows and Linux, the predominant client on macOS/iOS is AppKit/UIKit-based, and there exist numerous other native clients (such as UWP/WinAppSDK on Windows, GTK on Linux, and CLI for anything with a command line).In comparison everything else puts reasonable effort into the mobile clients and phones in the rest with bloated, half-baked web apps or if you're lucky an iOS Catalyst port.Along with UI/UX quality, this stuff matters and impacts adoption, even if most users can't put their reasoning into words.reply",
      "> I don't know a process for becoming the one with the largest userbase.Easy: Be at the right spot in the right time and be lucky to be noticed.WhatsApp had one smart idea: tying accounts to phone number, which solved detectability, while SMS where expensive in many regions. When ICQ/AIM still missed the mobile market and before Apple made iMessage.Easy to replicate, as we can see with Facebook messenger or Google's different attempts, who invested quite a few resources into that.reply",
      "Jami is so useless that never recieved messages after few hours. And there is no way you can make it work properly for longer duration.reply",
      "> I don't know a process for becoming the one with the largest userbase.I was at WhatsApp from 2014 - 2019. Growing a large userbase from scratch doesn't happen by any one factor. You have to do a lot of things well. (and probably get lucky)a) potential users need a compelling reason to join. Messaging at data rates was significant, but not in the US were many people had large messaging allowances. Works better than SMS/MMS was compelling for some.b) existing users need to be satisfied enough to stay: service has to work consistently, client has to work, etc.c) signup flow needs to work well. Doesn't matter if people want to use the app if they can't. You need to help users understand their phone number (or other identification). You need multiple methods of verification, because SMS doesn't always work. Giving someone a several digit code over the phone is a cognitive task for the user, and it's harder with disjointed speech generation, so you need to spend some time on that too. You need multiple providers because if you can't get verification codes to users, some of those people will give up and never come back. Since you have multiple providers, you need to figure out how to pick one based on current conditions which you also need to figure out how to track. Also --- you need some money, sending all these codes gets expensive. Phone numbers as ids is a blessing because \"everyone has one\" and you can use the system address book for contacts, but verification costs add up; usernames or email as id make contact discovery messy and a surprising amount of people in the developing world don't have an email address or don't know what it is.d) users get new phones, a lot, you need to make it easy to move their account. Or they will likely drop your service when they get a new phone.e) you need to be prepared for and handle large events. If some big news happens, people will want to talk about it. If some similar service has an outage, you will get more traffic --- if you also fall over, that's a lost opportunity.f) things need to work well on the devices people actually have. Which might not be the ones you would prefer to use. Worldwide, most people don't have flagship phones. If you want a large number of users, having good experiences only on recent flagships is self limiting. Working well (or at least better than alternatives) on low end and older devices is a path towards addressing users that others miss.There's probably more. Most of these require sustained consistent effort to deliver. It's not a one time thing. And it's not quick. Sustained consistent effort is easy enough as a one product start-up, but it's very hard as a big-corp.Userbase can be a positive feedback loop: once you have enough users, that becomes its own reason to join ... and having no one to talk to is a reason to leave. There's not really a way to jump start it, unless you've already got a large user base somewhere else that you can use to seed your service.reply",
      "> things need to work well on the devices people actually haveUntil 2025, WhatsApp was even on KaiOSreply",
      "That's great, thank you.reply",
      "I tried Jami for a bit with a friend. For both me and my friend, Jami was very unreliable about delivering notifications about new messages. So my friend would send me a message but because I didn\u2019t get any notification about the message it would go days before I opened the app and saw that he had said something, and I\u2019d respond to it and it would be days before he would happen to open the app again because he also didn\u2019t get any notification.reply",
      "This is sadly a ubiquitous problem with FOSS phone software. Google's and Apple's notification systems are anti-FOSS. You can use your own on Google phones, but then your app will have to wake up periodically to check it, and the system will detect your app as a battery waster, tell the user your app is a battery waster, and automatically prevent your app waking up to prevent battery waste. And on Apple I believe you simply can't do that because they user has to open the app to wake it.reply",
      "Not just FOSS. Many corporate apps have exactly the same problems with notification delivery. Those systems barely work.reply"
    ],
    "link": "https://kemble.net/blog/provoke/",
    "first_paragraph": "Writing code, having fun.This was a fun project for a couple of days, but I will probably shelve it for now so I can continue what I was already working on before. Read on to follow along with my journey.Spoilers: I didn\u2019t get very far.Get ready for an Opinion Dump of an intro:I have fond memories from 10 years ago of using Qt, and especially QML. It\u2019s just so easy to think of a design and make it happen with QML. I chose to use Svelte for building Web Apps\u2122 while it was still very beta just because it was the closest experience to QML I came across.Rust is pretty great. Also been a huge fan of that since like 2012 when I saw an example on the front page where the compiler validated pointer usage at compile time without adding any extra work at run time. Basically black magic after trying to fix the worst kind of bugs in C++.I became a full time Full Stack Web Developer against all my intentions. I like making user interfaces and solving hard problems though, so it\u2019s not so bad. I have"
  },
  {
    "title": "Japan to revise romanization rules for first time in 70 years (japantimes.co.jp)",
    "points": 110,
    "submitter": "rgovostes",
    "submit_time": "2025-12-16T08:54:58 1765875298",
    "num_comments": 89,
    "comments_url": "https://news.ycombinator.com/item?id=46286292",
    "comments": [
      "Curiously enough, Hepburn romanization fixes some ambiguities in Japanese (Japanese written in kana alone) while introducing others.The \u014d in Hepburn could correspond to \u304a\u3046 or \u304a\u304a or \u30aa\u30fc. That's an ambiguity.Where does Hepburn disambiguate?In Japanese, an E column kana followed by I sometimes makes a long E, like in \u5148\u751f (sen + sei -> sens\u0113). The \"SEI\" is one unit.  But in other situations it does not, like in a compound word ending in the E kana, where the second word starts with I. For instance \u9152\u8272 (sake + iro -> sakeiro, not sak\u0113ro).Hepburn distinguishes these; the hiragana spelling does not!This is one of the issues that makes it very hard to read Japanese that is written with hiragana only, rather than kanji.   No word breaks and not knowing whether \u305b\u3044 is supposed to be s\u0113 or sei.There are curiosities like karaage which is \"kara\" (crust) + \"age\" (fried thing). A lot of the time it is pronounced as kar\u0101ge, because of the way RA and A come together. Other times you hear a kind of flutter in it which articulates two A's.I have no idea which romanization to use. Flip a coin?reply",
      "What's interesting is that they address this problem where the latin alphabet introduces the ambiguity (Is genin \u3052\u3093\u3044\u3093 or \u3052\u306b\u3093? Hepburn goes with gen'in for the former to avoid ambiguity), so they could have extended that to sake'iro and applied the same strategy when the ambiguity comes from kana itself.reply",
      "Use Ruby text alongside kanji, maybe?reply",
      "The language school I attended all but banned romanization. The idea was to learn, practice, and finally internalize kana and kanji as quickly as possible. Hepburn is just a band-aid when it comes to language study.For people not interested in learning Japanese, however, a unified romanization could have its benefits. It just never struck me as particularly inconsistent to begin with, even after so many years living there.reply",
      "Kunrei-shiki is intended for domestic Japanese use. That's why it results in spellings that don't make logical sense for any Latin-based phonology. It's too focused on round trip unambiguity at the cost of phonetic clarity for non-Japanese. My big peeve is the company Mitutoyo using K-S, which everyone mispronounces because they don't know it's a poor transcription of \"Mitsutoyo\".reply",
      "Yeah my impression was the Orthography is pretty consistent compared to English.From what I understand this isn't the first time they've made some kind of change to orthography, I remember reading something about updating offical use of certain kana to reflect more modern pronunciations. It wasn't a dramatic change.It's interesting to see some countries just have this centralised influence over something like how their language is written as they're the main ones speaking it, as opposed to English.reply",
      "Hepburn is poorly supported in some input methods, like on Windows. If you want to type k\u014den or whatever, you really have to work for that \u014d. It's better now on mobile devices and MacOS (what I'm using now): I just long-pressed o and picked \u014d from a pop-up.reply",
      "That's one aspect I really love about macOS. I'm from a small country so nearly no one makes hardware with our exact layout, but with macOS I can always just long press to fill in the gaps. I just wish all apps used native inputs, not some weird half-baked solution they built themselves.reply",
      "I rarely miss Linux, but I liked being able to have compose keys, most of which were very logical and fast to type. Now on MacOS, I either have to know the option (alt) combination or long press, which makes my writing with accents way slower.https://en.wikipedia.org/wiki/Compose_keyreply",
      "If you frequently write the same characters, it's straightforward to create your own keyboard layout that matches your usage, using https://software.sil.org/ukelele/reply"
    ],
    "link": "https://www.japantimes.co.jp/news/2025/08/21/japan/panel-hepburn-style-romanization/",
    "first_paragraph": "SubscribeToday's print editionHome DeliveryThe Agency for Cultural Affairs submitted a recommendation to education minister Toshiko Abe on Wednesday to replace the government\u2019s romanization system for the Japanese language, the first such overhaul in 70 years.The agency is recommending replacing the government\u2019s long-standing Kunrei system with more widely used Hepburn-style spellings. The changes are expected to be approved within the current fiscal year and gradually rolled out in textbooks and other materials.Under the Kunrei system, codified by Cabinet notification in 1954, phonemes for \u3061 and \u3075 are written as ti and hu. Most Japanese schools still teach the style in romanization studies.But the Hepburn system, which renders them as chi and fu, has become dominant both in Japan and abroad, making the impact of the change likely most evident in educational materials such as school textbooks.The council\u2019s recommendation also adopts Hepburn spellings for \u3057, \u3058 and \u3064 as shi, ji, and tsu,"
  },
  {
    "title": "Sega Channel: VGHF Recovers over 100 Sega Channel ROMs (and More) (gamehistory.org)",
    "points": 218,
    "submitter": "wicket",
    "submit_time": "2025-12-16T13:07:14 1765890434",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=46288024",
    "comments": [
      "One of the most interesting things in this release IMO is the internal documents from Sega Channel management. For example, in this binder https://archive.gamehistory.org/item/ef6246e4-79be-4b02-b262... there's a bunch of research documents trying to figure out how to turn the service around after it began underperforming their expectations.It seems like the main problem they ran into was that the service appealed mainly to the small minority of \"heavy players\" (who they defined as playing more than 14 hours per week). Their original projections were that they could target cable subscribers who own Genesis systems and play games more than 4 hours a week, but they found that most people who weren't gaming fanatics preferred to own a few games and rent games as needed rather than subscribe to Sega Channel.The other big problem they ran into was parental resistance. A large amount of parents they talked to viewed Sega Channel as an \"open tap\" that would increase their child's time spent playing games. An ongoing subscription also was only a one-time \"give\" from the parent to the child, whereas buying/renting games was one \"give\" per occasion, which was more psychologically attractive to the parents.reply",
      "The document includes musings about a PC version of Sega Channel. So, kind of like Steam, but in 1996.reply",
      "It was impossible to compete with blockbuster at the time. They had walls of games to rent and rent we did.reply",
      "\"Sega broke ground in the late 90s with one of the first digital game distribution systems for consoles.\"By the time this came on to the scene the idea was already 14 years old. Intellivision was doing it in 1980: https://en.wikipedia.org/wiki/PlayCableThe idea blows people's minds if they think of a TV channel as just a channel for delivering TV, but the concept is not that hard if you realize it's just a way to broadcast data, most of which happens to be television video signals. The problem is making it cost-effective for a console to have an amount of RAM normally associated with a cartridge. For most of console gaming's lifespan cart size completely outclassed RAM size so storing a full cartridge image in RAM was expensive for what was generally the low end of the market. Plus the RAM you could stick in the receiver put a firm upper limit on how large a cart you could broadcast, and in an era still undeniably ruled by Moore's Law the size of the more desirable carts tended to outrun the RAM put in these things so they tended to become rapidly unable to keep up with the cart sizes.reply",
      "That's got to be a good part of it. And to top it off, there wasn't persistent storage available locally, so you couldn't build up a little library of playable content you received from such a service, having to sacrifice the old stuff to get something new, and if they didn't rebroadcast that item, you would never see it again.I'm sure to some kinds of people that was fine, but I think people kind of don't like having to delete something they like even a little -- even if they won't play it again, they'd rather know they can.reply",
      "some months they had General Chaos and some they didnt.  perhaps the coolest thing was only my one friend had it so we had to go over to his house to play it not like today where I would just passively consume it in a sad room all alone.reply",
      "Interested in the Lost World: Jurassic park variants. I was 5 when that came out in 1997 and it was an odd release since barely anyone had a Megadrive anymore but we had one and my parents wouldn\u2019t buy me a PlayStation, so they relented and bought me that as I loved dinosaurs. It is honestly such a great game and was really underappreciated since it was so late in the release cycle for that console.reply",
      "This is awesome! I used to have sega channel as a kid. I never had a game console and a Sega is the only one my mom ever bought us. We were fortunate enough to have cable TV from TCI cable and they had Sega channel. Every month we'd get new games and eventually they changed it to 2 weeks.reply",
      "We were also on TCI when I was a kid. I begged for Sega Channel. My mom tried to sign us up, but being across the river from the city meant we got half the TV channels, no PPV, and no Sega Channel.reply",
      "If it was Nintendo instead of Sega, everyone would be slapped with a cease-and-desist backed up with the threat of lawsuits.reply"
    ],
    "link": "https://gamehistory.org/segachannel/",
    "first_paragraph": "Our project to preserve the history of Sega Channel \u2014 including over 100 new Sega Channel ROMs.Sega broke ground in the late 90s with one of the first digital game distribution systems for consoles. Sega Channel offered access to a rotating library of Sega Genesis titles, along with game tips, demos, and even a few exclusive games that never came out in the United States in any other format. In an era of dial-up internet, Sega Channel delivered game data over television cable \u2014 a novel approach that gave the service its name.In the years since, Sega Channel has been shrouded in a bit of mystery. The service was discontinued in 1998, and the lack of retrievable game data and documentation around Sega Channel has led to decades of speculation about it. We\u2019ve mostly been left with magazine articles and second-hand accounts. Once in a while, one or two Sega Channel ROMs will show up online. How do you preserve a service like Sega Channel?For the last two years, we\u2019ve been working on a larg"
  }
]