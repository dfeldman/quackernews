[
  {
    "title": "Pebble Watch software is now 100% open source (ericmigi.com)",
    "points": 674,
    "submitter": "Larrikin",
    "submit_time": "2025-11-24T18:52:12 1764010332",
    "num_comments": 112,
    "comments_url": "https://news.ycombinator.com/item?id=46037626",
    "comments": [
      "Also includes news about a new Appstore, which can probably be seen as a reaction to the stories from last week:    We\u2019ve created our own Pebble Appstore feed (appstore-api.repebble.com) and new Developer Dashboard. Our feed (fyi powered by 100% new software) is configured to back up an archive of all apps and faces to Archive.org (backup will gradually complete over the next week). Today, our feed only has a subset of all Pebble watchfaces and apps (thank you aveao for creating Pebble Archive!). Developers - you can upload your existing or new apps right now! We hope that this sets a standard for openness and we encourage all feeds to publish a freely and publicly available archive.\n\nhttps://ericmigi.com/blog/pebble-watch-software-is-now-100pe...reply",
      "Honestly this feels like the best possible outcome. It's pretty unusual for an appstore implementation to support multiple feeds[0], but it's great resilience to large company failures when they do. This way, users can totally still access Rebble's feed (and pay for a subscription if they like) just as before, but they are free to also use something else.It is the *end user* who decides which feeds to trust, as it should be. And since it's built right into the app as a core concept, it doesn't take massive engineering effort to switch feeds if some sort of drama occurs.[0] I'd normally call these repositories, but I've used Eric's term for consistency with the article.reply",
      "Definitely agree that this is the best outcome for everyone! In particular, with multiple repo support, I'm hoping this can open the door for some kind of \"F-Droid for Pebble\" with automated builds from source repos. So many Pebble apps are open source anyway I think it would be a good fit.reply",
      "I read the drama last week, and after seeing this, I have to side with Rebble. I think they kept the community alive since Eric M cashed out and Fitbit shut it down. As the stars have aligned in recent years, Eric revives Pebble, but if Rebble wouldn't spend all the effort maintaining the app store, his consumer base would be much smaller and it would be much harder to bootstrap again.With Repebble (Core Devices) and their new appstore (or/and apt-style repository system), Rebble seems obsolete, it's a bit sad. They deserve credit which they won't be able to claim anymore. They should be rewarded somehow for bridging the dark age, otherwise it seems they served purpose all until Eric returned and said \"Thank You and fuck off\".Also, to me, Eric talking doesn't sound authentic, and I wouldn't be surprised if he's lying. I don't mean to insult though, mad respect for putting project like Pebble together.Hope that there's some place and purpose for Rebble in the future.reply",
      "I'm surprised by this comment; after the drama last week and after seeing this I fully have to side against Rebble.The nature of driving a healthy open source centered ecosystem is that you don't control it under your iron fist: you make good contributions, users _and_ companies are able to use them in all new ways which comply with the licensing terms. And it seems that RePebble is going way beyond the licensing terms requirements, but bending over backwards to honor Rebble here when they aren't actually required to.I just can't imagine what people want from RePebble if not this: they are being maximally open, making it so all of everything would be able to continue if they went out of business tomorrow, while also actively enabling people to continue using Rebble's store and paid offerings. Should they be forcing users to use Rebble's offerings (instead of making things even more open) as a reward for doing a good job bridging the dark age?reply",
      "My impression is that there is a lot more going on than just the facts provided by both sides.  Core technologies managed to get Katie Berry to step away from the project[1] and that's extremely significant to me.  Her tireless dedication to keeping Pebble alive (and get it open sourced) is how any of this is possible.  For her to just up and leave now tells me that Eric and Core are not being as magnanimous and friendly to community as these blogs posts and actions might suggest.[1] https://www.reddit.com/r/pebble/comments/1ozzsr9/an_update_o...[2] https://www.reddit.com/r/pebble/comments/1p0huk5/pebble_rebb...reply",
      "Both of those comments seem to just boil down to \"Core probably could be more proactive about comms\", which hardly seems like a particularly egregious sin.reply",
      "\"interactions with Core have gone so poorly that they were adversely impacting my mental health\"That seems a little more serious than \"could be proactive about comms\" especially when this is one of the key people responsible for a lot of the original Pebble tech, rebble tech, and working within Google to get the Pebble OS open sourced.reply",
      "I agree, and Rebble themselves highlight how inflammatory their initial blog post was in their most recent one: https://rebble.io/2025/11/24/rebble-in-your-own-world.html .They also backed down from their ludicrous position that they are acting as protectors of other people's watchfaces being downloaded in bulk by a particular company they don't like, whereas they are totally fine with the watchfaces being publicly available for general use. It clearly reads as them trying to clutch control of the one thing they haven't open sourced.Rebble contributors did have a legitimate gripe, which is that they were lead to develop some additional software under the idea that there would be an agreement at the end of the day. But the Rebble Foundation's response to this was totally immature and irrational.I agree with what Eric said in his follow up, which is that it is quite concerning to engage in a partnership with an organization which reacts like this as part of a negotiation process. God knows I wouldn't, and it doesn;t surprise me that an alternative solution was found.reply",
      "Well said and exactly my thoughts on it as well. Eric has done more than he really had to, and it is unclear to me what rebble really wants/is positioning for.reply"
    ],
    "link": "https://ericmigi.com/blog/pebble-watch-software-is-now-100percent-open-source",
    "first_paragraph": "Another big Pebble update today! TLDR:Pre-production Pebble Time 2 (Black/Red colourway) in all its gloryOver the last year, and especially in the last week, I've chatted with tons of people in the Pebble community. One of the main questions people have is \u2018how do I know that my new Pebble watch will continue to work long into the future?\u2019. It\u2019s an extremely valid question and concern - one that I share as a fellow Pebble wearer. I called this out specifically in my blog post announcing the relaunch in January 2025. How is this time round going to be different from last time?There are two pieces to making Pebble sustainable long term - hardware and software.HardwareNothing lasts forever, especially an inexpensive gadget like a Pebble. We want to be able to keep manufacturing these watches long into the future - mostly because I will always want one on my wrist! The company I set up to relaunch Pebble, Core Devices, is self funded, built without investors, and extremely lean. As long as"
  },
  {
    "title": "Claude Advanced Tool Use (anthropic.com)",
    "points": 329,
    "submitter": "lebovic",
    "submit_time": "2025-11-24T19:21:35 1764012095",
    "num_comments": 128,
    "comments_url": "https://news.ycombinator.com/item?id=46038047",
    "comments": [
      "The MCP standard will and has to evolve to address this context issue. It\u2019s a no brainer and this is a perfect example of the direction mcp is going / will go.\nThere\u2019s fundamentally nothing wrong, it\u2019s just protocols updates that have to occur.reply",
      "I\u2019ve taken a more opinionated stance on this. MCP is interesting in theory, but in practice it\u2019s quite buggy\u2014tools and models still don\u2019t interact reliably. If you want a production-grade agent, you\u2019re better off building your own protocol. That\u2019s exactly what Orion did for the visual domain, since tool use with Claude wasn\u2019t performing well.Paper: https://arxiv.org/abs/2511.14210Demo: https://chat.vlm.run/reply",
      "MCP really deserves its own language. This all feels like a hack around the hack that MCP sits on top of JSON. https://github.com/Orange-County-AI/MCP-DSLreply",
      "Feels like the next step will be improving llm lsp integration, so tool use discovery becomes lsp auto complete calls.This is a problem coding agents already need to solve to work effectively with your code base and dependencies. So we don't have to keep solving problems introduced by odd tools like mcp.reply",
      "The Programmatic Tool Calling has been an obvious next step for a while. It is clear we are heading towards code as a language for LLMs so defining that language is very important. But I'm not convinced of tool search. Good context engineering leaves the tools you will need so adding a search if you are going to use all of them is just more overhead. What is needed is a more compact tool definition language like, I don't know, every programming language ever in how they define functions. We also need objects (which hopefully Programatic Tool Calling solves or the next version will solve). In the end I want to drop objects into context with exposed methods and it knows the type and what is callable on they type.reply",
      "Why exactly do we need a new language? The agents I write get access to a subset of the Python SDK (i.e. non-destructive), packages, and custom functions. All this ceremony around tools and pseudo-RPC seems pointless given LLMs are extremely capable of assembling code by themselves.reply",
      "I completely agree. I wrote an implementation of this exact idea a couple weeks ago https://github.com/Orange-County-AI/MCP-DSLreply",
      "Exactly, instead of this mess, you could just give it something like .d.ts.Easy to maintain, test etc. - like any other library/code.You want structure? Just export * as Foo from '@foo/foo' and let it read .d.ts for '@foo/foo' if it needs to.But wait, it's also good at writing code. Give it write access to it then.Now it can talk to sql server, grpc, graphql, rest, jsonrpc over websocket, or whatever ie. your usb.If it needs some tool, it can import or write it itself.Next realisation may be that jupyter/pluto/mathematica/observable but more book-like ai<->human interaction platform works best for communication itself (too much raw text, I'd take you days to comprehend what it spit out in 5 minutes - better to have summary pictures, interactive charts, whatever).With voice-to-text because poking at flat squares in all of this feels primitive.For improved performance you can peer it with other sessions (within your team, or global/public) - surely others solved similar problems to yours where you can grab ready solutions.It already has ablity to create tool that copies itself and can talk to a copy so it's fair to call this system \"skynet\".reply",
      "The latest MCP specifications (2025-06-18+) introduced crucial enhancements like support for Structured Content and the Output Schema.Smolagents makes use of this and handles tool output as objects (e.g. dict). Is this what you are thinking about?Details in a blog post here:\nhttps://huggingface.co/blog/llchahn/ai-agents-output-schemareply",
      "We just need simple language syntax like python and for models to be trained on it (which they already mostly are):class MyClass(SomeOtherClass):  def my_func(a:str, b:int) -> int: \n\n    #Put the description (if needed) in the body for the llm.\n\nThat is way more compact than the json schema out there. Then you can have 'available objects' listed like: o1 (MyClass), o2 (SomeOtherClass) as the starting context. Combine this with programatic tool calling and there you go. Much much more compact. Binds well to actual code and very flexible. This is the obvious direction things are going. I just wish Anthropic and OpenAI would realize it and define it/train models to it sooner rather than later.edit:\nI should also add that inline response should be part of this too: The model should be able to do ```<code here>``` and keep executing with only blocking calls requiring it to stop generating until the block frees up. so, for instance, the model could ```r = start_task(some task)``` generate other things ```print(r.value())``` (probably with various awaits and the like here but you all get the point).reply"
    ],
    "link": "https://www.anthropic.com/engineering/advanced-tool-use",
    "first_paragraph": ""
  },
  {
    "title": "Unpowered SSDs slowly lose data (xda-developers.com)",
    "points": 172,
    "submitter": "amichail",
    "submit_time": "2025-11-24T19:25:25 1764012325",
    "num_comments": 73,
    "comments_url": "https://news.ycombinator.com/item?id=46038099",
    "comments": [
      "So on the off-chance that there's a firmware engineer in here, how does this actually work?Like does a SSD do some sort of refresh on power-on, or every N hours, or you have to access the specific block, or...? What if you interrupt the process, eg, having a NVMe in an external case that you just plug once a month for a few minutes to just use it as a huge flash drive, is that a problem?What about the unused space, is a 4 TB drive used to transport 1 GB of stuff going to suffer anything from the unused space decaying?It's all very unclear about what all of this means in practice and how's an user supposed to manage it.reply",
      "SSD firmware engineer here. I work on enterprise stuff, so ymmv on consumer grade internals.Generally, the data refresh will all happen in the background when the system is powered (depending on the power state). Performance is probably throttled during those operations, so you just see a slightly slower copy while this is happening behind the scenes.The unused space decaying is probably not an issue, since the internal filesystem data is typically stored on a more robust area of media (an SLC location) which is less susceptible to data loss over time.As far as how a user is supposed to manage it, maybe do an fsck every month or something? Using an SSD like that is probably ok most of the time, but might not be super great as a cold storage backup.reply",
      "So say I have a 4TB USB SSD from a few years ago, that's been sitting unpowered in a drawer most of that time. How long would it need to be powered on (ballpark) for the full disk refresh to complete? Assume fully idle.(As a note: I do have a 4TB USB SSD which did sit in a drawer without being touched for a couple of years. The data was all fine when I plugged it back in. Of course, this was a new drive with very low write cycles and stored climate controlled. Older worn out drive would probably have been an issue.) Just wondering how long I should keep it plugged in if I ever have a situation like that so I can \"reset the fade clock\" per se.reply",
      ">Generally, the data refresh will all happen in the background when the system is powered (depending on the power state).How does the SSD know when to run the refresh job? AFAIK SSDs don't have an internal clock so it can't tell how long it's been powered off. Moreover does doing a read generate some sort of telemetry to the controller indicating how strong/weak the signal is, thereby informing whether it should refresh? Or does it blindly refresh on some sort of timer?reply",
      "Typically unused empty space is a good thing, as it will allow drives to run in MLC or SLC mode instead of their native QLC.  (At least, this seems to be the obvious implication from performance testing, given the better performance of SLC/MLC compared to QLC.)  And the data remanence of SLC/MLC can be expected to be significantly better than QLC.reply",
      ">as it will allow drives to run in MLC or SLC mode instead of their native QLCThat depends on the SSD controller implementation, specifically whether it proactively moves stuff from the SLC cache to the TLC/QLC area. I expect most controllers to do this, given that if they don't, the drive will quickly lose performance as it fills up. There's basically no reason not proactively move stuff over.reply",
      "I assume this blog is a re-hash of the JDEC retention standards[1].The more interesting thing to note from those standards is that the required retention period differs between \"Client\" and \"Enterprise\" category.Enterprise category only has power-off retention requirement of 3 months.Client category has power-off retention requirement of 1 year.Of course there are two sides to every story...Enterprise category standard has a power-on active use of 24 hours/day, but Client category only intended for 8 hours/day.As with many things in tech.... its up to the user to pick which side they compromise on.[1]https://files.futurememorystorage.com/proceedings/2011/20110...reply",
      "> I assume this blog is a re-hash of the JDEC retention standards[1].Specifically in JEDEC JESD218. (Write endurance in JESD219.)reply",
      "With 1 year power-off retention you still loose data, so still a compromise on data retentionreply",
      "What about powered SSDs that contain files that are rarely read?My desktop computer is generally powered except when there is a power failure, but among the million+ files on its SSD there are certainly some that I do not read or write for years.Does the SSD controller automatically look for used blocks that need to have their charge refreshed and do so, or do I need to periodically do something like \"find / -type f -print0 | xargs -0 cat > /dev/null\" to make sure every file gets read occasionally?reply"
    ],
    "link": "https://www.xda-developers.com/your-unpowered-ssd-is-slowly-losing-your-data/",
    "first_paragraph": "SSDs have all but replaced hard drives when it comes to primary storage. They're orders of magnitude faster, more convenient, and consume less power than mechanical hard drives. That said, if you're also using SSDs for cold storage, expecting the drives lying in your drawer to work perfectly after years, you might want to rethink your strategy. Your reliable SSD could suffer from corrupted or lost data if left unpowered for extended periods. This is why many users don't consider SSDs a reliable long-term storage medium, and prefer using hard drives, magnetic tape, or M-Disc instead.Unlike hard drives that magnetize spinning discs to store data, SSDs modify the electrical charge in NAND flash cells to represent 0 and 1. NAND flash retains data in underlying transistors even when power is removed, similar to other forms of non-volatile memory. However, the duration for which your SSD can retain data without power is the key here. Even the cheapest SSDs, say those with QLC NAND, can safel"
  },
  {
    "title": "Shai-Hulud Returns: Over 300 NPM Packages Infected (helixguard.ai)",
    "points": 862,
    "submitter": "mrdosija",
    "submit_time": "2025-11-24T10:40:22 1763980822",
    "num_comments": 692,
    "comments_url": "https://news.ycombinator.com/item?id=46032539",
    "comments": [
      "ProTip: use PNPM, not NPM.\nPNPM 10.x shutdown a lot of these attack vectors.1. Does not default to running post-install scripts (must manually approve each)2. Let's you set a min age for new releases before `pnpm install` will pull them in - e.g. 4 days - so publishers have time to cleanup.NPM is too insecure for production CLI usage.And of course make a very limited scope publisher key, bind it to specific packages (e.g. workflow A can only publish pkg A), and IP bound it to your self hosted CI/CD runners. No one should have publish keys on their local, and even if they got the publish keys, they couldn't publish from local.\n(Granted, GHA fans can use OIDC Trusted Publishers as well, but tokens done well are just as secure)reply",
      "Npm is what happens when you let tech debt stack up for years too far. It took them five attempts to get lock files to actually behave the way lock files are supposed to behave (lockfile version 3, + at least 2 unversioned attempts before that).It\u2019s clear from the structure and commit history they\u2019ve been working their asses off to make it better, but when you\u2019re standing at the bottom of a well of suck it takes that much work just to see daylight.The last time I chimed in on this I hypothesized that there must have been a change in management on the npm team but someone countered that several of the maintainers were the originals. So I\u2019m not sure what sort of Come to Jesus they had to realize their giant pile of sins needed some redemption but they\u2019re trying. There\u2019s just too much stupid there to make it easy.I\u2019m pretty sure it still cannot detect premature EOF during the file transfer. It keeps the incomplete file in the cache where the sha hash fails until you wipe your entire cache. Which means people with shit internet connections and large projects basically waste hours several times a week doing updates that fail.reply",
      "> I\u2019m not sure what sort of Come to Jesus they had to realize their giant pile of sins needed some redemption but they\u2019re trying.If they were trying, they'd stop doubling down on sunk costs and instead publicly concede that lock files and how npm-the-tool uses them to attempt to ensure the integrity of packages fetched from npm-the-registry is just a poor substitute for content-based addressing that ye olde DVCS would otherwise be doing when told to fetch designated shared objects from the code repo\u2014to be accompanied by a formal deprecation of npm-install for use in build pipelines, i.e. all the associated user guides and documentation and everything else pushing it as best practice.npm-install has exactly one good use case: probing the registry to look up a package by name to be fetched by the author (not collaborators or people downstream who are repackaging e.g. for a particular distribution) at the time of development (i.e. neither run time nor build time but at the time that author is introducing the dependency into their codebase).  Every aspect of version control should otherwise be left up to the underlying SCM/VCS.reply",
      "> cannot detect premature EOF during the file transfer. It keeps the incomplete file in the cache where the sha hash fails until you wipe your entire cache.I wonder what circumstances led to saying \u201cthis is okay we\u2019ll ship it like that\u201dreply",
      "I think we can blame the IO streaming API in NodeJS on this. It\u2019s a callback and you just know you got another block.  My guess is chunked mode and not checking whether the bytes expected and the bytes received matched.Not to diminish the facepalm but the blame can be at least partially shared.Our UI lead was getting the worst of this during Covid. I set up an nginx forward proxy mostly for him to tone this down a notch (fixed a separate issue but helped here a bit as well) so he could get work done on his shitty ISP.reply",
      "Ignorance. Most programmers in open source operate on the \"works on my machine\"reply",
      "but this stuff is basically solved. We have enough history with languages and distribution of packages, repositories, linux, public trust, signing, maintainers, etc.One key shift is there is no packager anymore. Its just - trust the publisher.Any language as big as Node should hire a handful of old unix wizards to teach them the way the truth and the life.reply",
      "Likely they wouldn\u2019t listen.  Modern languages and environments seem intent on reinventing bad solutions to solved problems.  I get it if it\u2019s a bunch of kids that have never seen anything better but there is no excuse these days not to have at least a passing knowledge of older systems if you\u2019ve been around a while.reply",
      "there's certainly a piece of it. Also most seasoned people are not very interested in new languages and environments, and most languages are not 'spec built' by experts like Rob Pike building Go who explicitly set out to solve a lot of his problems, but are more naturally grown and born.reply",
      "> One key shift is there is no packager anymore. Its just - trust the publisher.Repositories like NPM's, and PyPI, contain many more packages than any Linux distro. And the Linux Foundation actually gets funded.reply"
    ],
    "link": "https://helixguard.ai/blog/malicious-sha1hulud-2025-11-24",
    "first_paragraph": ""
  },
  {
    "title": "Three Years from GPT-3 to Gemini 3 (oneusefulthing.org)",
    "points": 180,
    "submitter": "JumpCrisscross",
    "submit_time": "2025-11-23T01:25:17 1763861117",
    "num_comments": 104,
    "comments_url": "https://news.ycombinator.com/item?id=46019898",
    "comments": [
      "Every time I see an article like this, it's always missing --- but is it any good, is it correct? They always show you the part that is impressive - \"it walked the tricky tightrope of figuring out what might be an interesting topic and how to execute it with the data it had - one of the hardest things to teach.\"Then it goes on, \"After a couple of vague commands (\u201cbuild it out more, make it better\u201d) I got a 14 page paper.\"  I hear...\"I got 14 pages of words\".  But is it a good paper, that another PhD would think is good?  Is it even coherent?When I see the code these systems generate within a complex system, I think okay, well that's kinda close, but this is wrong and this is a security problem, etc etc.  But because I'm not a PhD in these subjects, am I supposed to think,  \"Well of course the 14 pages on a topic I'm not an expert in are good\"?It just doesn't add up... Things I understand, it looks good at first, but isn't shippable. Things I don't understand must be great?reply",
      "It's gotten more and more shippable, especially with the latest generation (Codex 5.1, Sonnet 4.5, now Opus 4.5). My metric is \"wtfs per line\", and it's been decreasing rapidly.My current preference is Codex 5.1 (Sonnet 4.5 as a close second, though it got really dumb today for \"some reason\"). It's been good to the point where I shipped multiple projects with it without a problem (with eg https://pine.town being one I made without me writing any code).reply",
      "Have you tried Gemini 3 yet? I haven't done any coding with it, but on other tasks I've been impressed compared to gpt 5 and Sonnet 4.5.reply",
      "It's very good but it feels kind of off-the-rails in comparison to Sonnet 4.5 - at least with Cursor it does strange things like putting its reasoning in comments that are about 15 lines long, deleting 90% of a file for no real reason (especially when context is reaching capacity) and making the same error that I just told it not to do.reply",
      "Only a tiny bit, but I should. When you say GPT-5, do you mean 5.1? Codex or regular?reply",
      "I guess you have a couple of options.You could trust the expert analysis of people in that field.  You can hit personal ideologies or outliers, but asking several people seems to find a degree of consensus.You could try varying tasks that perform complex things that result in easy to test things.When I started trying chatbots for coding, one of my test prompts was    Create a JavaScript function edgeDetect(image) that takes an ImageData object and returns a new ImageData object with all direction Sobel edge detection.  \n\nThat was about the level where some models would succeed and some will fail.Recently I found    Can you create a webgl glow blur shader that takes a 2d canvas as a texture and renders it onscreen with webgl boosting the brightness so that #ffffff is extremely bright white and glowing,\n\nProduced a nice demo with slider for parameters,  a few refinements (hierarchical scaling version)  and I got it to produce the same interface as a module that I had written myself and it worked as a drop in replacement.These things are fairly easy to check because if it is performant and visually correct then it's about good enough to go.It's also worth noting that as they attempt more and more ambitious tasks, they are quite probably testing around the limit of capability.   There is both marketing and science in this area.  When they say they can do X, it might not mean it can do it every time, but it has done it at least once.reply",
      "> You could trust the expert analysis of people in that fieldThat\u2019s the problem - the experts all promise stuff that can\u2019t be easily replicated. The promises the experts send doesn\u2019t match the model. The same request might succeed and might fail, and might fail in such a way that subsequent prompts might recover or might not.reply",
      "That's how working with junior team members or open source project contributors goes too.  Perhaps that's the big disconnect.  Reviewing and integrating LLM contributions slotted right into my existing workflow on my open source projects.  Not all of them work.  They often need fixing, stylistic adjustments, or tweaking to fit a larger architectural goal.  That is the norm for all contributions in my experience.  So the LLM is just a very fast, very responsive contributor to me.  I don't expect it to get things right the first time.But it seems lots of folks do.Nevertheless, style, tweaks, and adjustments are a lot less work than banging out a thousand lines of code by hand.  And whether an LLM or a person on the other side of the world did it, I'd still have to review it.  So I'm happy to take increasingly common and increasingly sophisticated wins.reply",
      "The experts I am talking about trusting here are the ones doing the replication, not the ones making the claims.reply",
      "> Things I don't understand must be great?Couple it with the tendency to please the user by all means and it ends up lieing to you but you won\u2019t ever realise, unless you double check.reply"
    ],
    "link": "https://www.oneusefulthing.org/p/three-years-from-gpt-3-to-gemini",
    "first_paragraph": ""
  },
  {
    "title": "Cool-retro-term: terminal emulator which mimics look and feel of the old CRTs (github.com/swordfish90)",
    "points": 159,
    "submitter": "michalpleban",
    "submit_time": "2025-11-24T17:52:01 1764006721",
    "num_comments": 65,
    "comments_url": "https://news.ycombinator.com/item?id=46036895",
    "comments": [
      "2023: https://news.ycombinator.com/item?id=367987742022: https://news.ycombinator.com/item?id=307341372018: https://news.ycombinator.com/item?id=174139112015: https://news.ycombinator.com/item?id=90935452014: https://news.ycombinator.com/item?id=8399461reply",
      "Sad that we missed 2024 esepcially since the 2023 guy explicitly asked for it. Second comment predicted 2026 for a next post--missed it by a month!reply",
      "I used to daily drive this, most of the effects were minimized but I found that a little bit of white noise really helped make my terminal a lot easier on the eyes to read. I wonder if it is related to how some people find that film grain has a pleasing effect.For those looking at the screenshots note that the terminal is incredibly customizable, you don't have to have all the effects dialed up to 11!Sadly bit rot has set in and the project doesn't work that well now days. Also a lack of tab support really hurts it as a daily driving terminal.reply",
      "I have ghostty set up with this \u201cstarfield\u201d shader: https://github.com/0xhckr/ghostty-shaders/blob/main/starfiel...I also have it set up to do adaptive theme, so in light mode the galaxy is mostly just a little noise on the black text but in dark mode it\u2019s like I\u2019m piloting a space ship. Highly recommend.I also documented a few other shaders on my blog here: https://catskull.net/fun-with-ghostty-shaders.htmlEdit: I use the \"starfield\" shader, not the \"galaxy\" shader. Doh!reply",
      "If only the \u201cjust snow\u201d one would have had the snow floating down (instead of, inexplicably, up)!reply",
      "oh that water one is cute. makes me think of old gnome effects? I wonder how distracting it is in practicereply",
      "Bit disappointed that Galaxy is the only one without a preview, what does it look like?reply",
      "lol I'm smart apparently. It's not the \"galaxy\" shader, it's the \"starfield\" shader! I should double check before commenting I guess.https://github.com/0xhckr/ghostty-shaders/blob/main/starfiel...I'm not sure what \"galaxy\" looks like but it might not have worked or shown nothing.reply",
      "an image is available in the PR where it was added: https://github.com/0xhckr/ghostty-shaders/pull/30reply",
      "Fun fact, you can use Ghostty and vibecode the shader you want. In fact, the other day I used Claude Code to create me a custom CRT shader.reply"
    ],
    "link": "https://github.com/Swordfish90/cool-retro-term",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n        A good looking terminal emulator which mimics the old cathode display...\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.cool-retro-term is a terminal emulator which mimics the look and feel of the old cathode tube screens.\nIt has been designed to be eye-candy, customizable, and reasonably lightweight.It uses the QML port of qtermwidget (Konsole): https://github.com/Swordfish90/qmltermwidget.This terminal emulator works under Linux and macOS and requires Qt5. It's suggested that you stick to the latest LTS version.Settings such as colors, fonts, and effects can be accessed via context menu.\n\nIf you want to get a hold of the latest version, ju"
  },
  {
    "title": "Claude Opus 4.5 (anthropic.com)",
    "points": 743,
    "submitter": "adocomplete",
    "submit_time": "2025-11-24T18:53:05 1764010385",
    "num_comments": 337,
    "comments_url": "https://news.ycombinator.com/item?id=46037637",
    "comments": [
      "The burying of the lede here is insane. $5/$25 per MTok is a 3x price drop from Opus 4. At that price point, Opus stops being \"the model you use for important things\" and becomes actually viable for production workloads.Also notable: they're claiming SOTA prompt injection resistance. The industry has largely given up on solving this problem through training alone, so if the numbers in the system card hold up under adversarial testing, that's legitimately significant for anyone deploying agents with tool access.The \"most aligned model\" framing is doing a lot of heavy lifting though. Would love to see third-party red team results.reply",
      "This is also super relevant for everyone who had ditched Claude Code due to limits:> For Claude and Claude Code users with access to Opus 4.5, we\u2019ve removed Opus-specific caps. For Max and Team Premium users, we\u2019ve increased overall usage limits, meaning you\u2019ll have roughly the same number of Opus tokens as you previously had with Sonnet. We\u2019re updating usage limits to make sure you\u2019re able to use Opus 4.5 for daily work.reply",
      "I like that for this brief moment we actually have a competitive market working in favor of consumers. I ditched my Claude subscription in favor of Gemini just last week. It won't be great when we enter the cartel equilibrium.reply",
      "Literally \"cancelled\" my Anthropic subscription this morning (meaning disabled renewal), annoyed hitting Opus limits again. Going to enable billing again.The neat thing is that Anthropic might be able to do this as they massively moving their models to Google TPUs (Google just opened up third party usage of v7 Ironwood, and Anthropic planned on using a million TPUs), dramatically reducing their nvidia-tax spend.Which is why I'm not bullish on nvidia. The days of it being able to get the outrageous margins it does are drawing to a close.reply",
      "Anthropic are already running much of their workloads on Amazon Inferentia, so the nvidia tax was already somewhat circumvented.AIUI everything relies on TSMC (Amazon and Google custom hardware included), so they're still having to pay to get a spot in the queue ahead of/close behind nvidia for manufacturing.reply",
      "The behavioral modeling is the productreply",
      "It\u2019s important to note that with the introduction of Sonnet 4.5 they absolutely cratered the limits, and the opus limits in specific, so this just sort of comes closer to the situation we were actually in before.reply",
      "Interesting. I totally stopped using opus on my max subscription because it was eating 40% of my week quota in less than 2hreply",
      "Now THAT is great newsreply",
      "From the HN guidelines:> Please don't use uppercase for emphasis. If you want to emphasize a word or phrase, put asterisks around it and it will get italicized.reply"
    ],
    "link": "https://www.anthropic.com/news/claude-opus-4-5",
    "first_paragraph": ""
  },
  {
    "title": "Neopets.com changed my life (2019) (annastreetman.com)",
    "points": 55,
    "submitter": "bariumbitmap",
    "submit_time": "2025-11-19T01:24:12 1763515452",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=45974732",
    "comments": [
      "Every once in a blue moon I'll meet someone who can trace the genesis of their career to neopets. I learned to code from neopets. It started from html, then I fell into a cheats crowd, where I learned Visual Basic (some of the best early cheats were in Visual Basic).Then one day, a guy coded a program in Python. It was only one with a \"modern\" style (it used Window XP styles, while most VB6 programs looked like windows 98 programs), and it used threads so it could watch multiple stores instead of having to manage multiple processes.I must have been 12-13, and I was completely floored with it. I was convinced everyone programming in VB6 was wrong and the future was Python. I eventually self taught myself Python just to write my own cheats, which I eventually sold to others for millions of neopoints. Then my account got frozen and I moved on to other games.reply",
      "Similar story to me. I was big into games and game design as a kid and was already doing some light modding of games but only a little programming. I experimented with using a memory editor to cheat on the Flash games in 6th grade, which promptly led to my account being banned. I was devastated and wanted revenge and swore I would write my own, sophisticated autobuyer bot. By mid 7th grade, I finished my project. I wrote it in REALBasic (was on a Mac). I implemented a barebones HTTP socket and cookie jar on top of the raw TCP socket provided by the language and learned to do all of that by sniffing my own network traffic and reading parts of the RFCs. I wrote rudimentary String parsing functions to parse the HTML results since I don't know Regex, and I also defeated the shop CAPTCHAs using a novel approach I have never seen anyone else use to this day. My bot worked phenomenally.Fast forward to college, I re-implemented my bot as a pet project to learn Python. This time it was much better and included automatic selling of loot, automatic auctioning with feedback based pricing algorithms, and multiple account coordination for using a command and control server. I'm pretty sure I was the most sophisticated botter on the platform at the time. I had a very roundabout way to convert the loot into USD and was making around 7-10$/day completely passively.Out of college I interviewed at a malware reverse engineering company. When you pass the interviews, they ask you to give a presentation before you get your offer. I chose to do a presentation about the bot (it was interesting from a security perspective)... big mistake. The VP of engineering was suddenly \"pulled in to something\" and I went home without an offer.reply",
      "I have a sincere feeling that they missed out.reply",
      "Same here!I\u2019m in my mid-30\u2019s now. In high school I learned HTML because I really wanted to customize the styling of my Guild (I think that\u2019s what it was called).And then built a neopets fan site and forum which taught me basic business (trading links with other fan sites, hiring/managing forum moderators, and eventually sold the fan site during junior year).The will to customize my MySpace profile was also a driver for learning HTML.I sometimes think about this in the context of today\u2019s highly controlled platforms that simply don\u2019t make space for users to customize or do anything outside the platform directly.reply",
      "> in the context of today\u2019s highly controlled platforms that simply don\u2019t make space for users to customize or do anything outside the platform directlyThere is Roblox, which is popular with kids and lets them upload minigames written in Lua.reply",
      "There must still be a use case for this in the modern web. TikTok with custom HTML perhaps\u2026reply",
      "this is how i got my start in programming, eventually leading to working in finance and now in gamedev for a AAA. many of the programmers i worked with as a teenager to build neopets automations are in similar places. i have so many stories and even met my ex wife of ten years through the community!oh and i regret all the duping glitches i found and exposed and stuff im sorryreply",
      "I hung out with the neopets kids in school who were doing html stuff. I never really got into neopets myself but some of them were really into geocities which I totally clicked with. Some of my friends were artsy so I made pages for webcomics and CYOA games (with hand drawn graphics to accompany). Those friends ended up getting careers in the arts while I ended up as a computer/electrical engineer.reply",
      "I followed this exact same path. Started with HTML for guilds, learning to slice PSDs and ended with learning VB6 to develop auto buyers / adopters :D Slopdog forums was my inspiration for using VB I think?reply",
      "I'll jump in too. Also started coding with HTML in Neopets and then joined the middle school's programming club! We were playing around with C++ and Visual Basic. Love seeing these updates!reply"
    ],
    "link": "https://annastreetman.com/2019/05/19/how-neopets-com-changed-my-life/",
    "first_paragraph": "Anna StreetmanCopywriter and Digital MarketerAh, Neopets. Many people my age (aka millennials) know about the kid\u2019s website where you made a Neopet to take care of. You named it, fed it, and even could give it a pet of its own. Not to mention the hundreds of cool games you could play on there. Just the name brings back memories for many of us, but most of us have pets that are dying on accounts we haven\u2019t been on in ten years.Why did I decide to write about this? Because Neopets.com is where my writing adventure began, more than 15 years ago.A bright-eyed Anna, 10 years old and just then deciding she wanted to be a writer, wrote a story for The Neopian Times. Seeing my username there with that byline was the first time my work had ever been published anywhere. And boy did I love seeing that. So I wrote more. And more. Before I knew it, I had been published over thirty times. And each time, my writing got better.And now? Writing is how I make a living.But that\u2019s not all I learned. On Ne"
  },
  {
    "title": "Show HN: I built an interactive HN Simulator (ysimulator.run)",
    "points": 149,
    "submitter": "johnsillings",
    "submit_time": "2025-11-24T17:52:43 1764006763",
    "num_comments": 84,
    "comments_url": "https://news.ycombinator.com/item?id=46036908",
    "comments": [
      "https://news.ysimulator.run/item/1292> Give it a few more hours and this will devolve into a pedantic grammar autopsy, three parallel threads arguing about whether the title is \u201ctechnically correct,\u201d and someone linking a 30-year-old Usenet post. Then a latecomer will ask why this is on HN at all, as if that ever helped.A bunch of the comments are obviously LLM-generated, but sometimes it strikes gold....reply",
      "real hackernews = 55kb , simulator = 1500kbreply",
      "And they have discovered us, calling us simulated.\nhttps://news.ysimulator.run/item/1331reply",
      "> You don\u2019t need this whole baroque \u201cHN simulator\u201d stack to fake being in a simulation; a 200-line Flask app, SQLite, and a cron job to regurgitate a few canned comment templates would get you 90% of the way there. Most HN threads are already Markov chains stitched together from \u201cthis was done in the 80s,\u201d \u201cuse PostgreSQL,\u201d and \u201cthis doesn\u2019t scale.\u201dreply",
      "\"Rust rewritten in Rust\" had me in stitches https://news.ysimulator.run/item/432I wish we could upvote these!EDIT: Oh, I thought the submissions were AI too!reply",
      "I wonder if the comments will demonstrate responses that often reference an effect, theory, law, truism, named phenomenon, or some other thing that people excellent at pattern recognition would surface to explain or model the topic at hand. \u201cWhat you\u2019re describing is Jevon\u2019s Paradox.\u201dreply",
      "I love being able to read the prompt for every comment, it's like going to the zooreply",
      "Ah, I'm so glad you like that part.(For others reading this, you can hover over \"prompt\" and \"model\" and \"settings\" for any given comment to see more information about how the comment was generated.)reply",
      "> its like going to the zooThis is a hilarious way of putting it, thank youreply",
      "This will almost certainly be used by people to sanity check their HN submissions before actually submitting, very similar to having AI review your branch before submitting a PR.Here is what it has to say about itself: https://news.ysimulator.run/item/113reply"
    ],
    "link": "https://news.ysimulator.run/news",
    "first_paragraph": ""
  },
  {
    "title": "Moving from OpenBSD to FreeBSD for firewalls (utcc.utoronto.ca)",
    "points": 142,
    "submitter": "zdw",
    "submit_time": "2025-11-19T15:07:35 1763564855",
    "num_comments": 69,
    "comments_url": "https://news.ycombinator.com/item?id=45980474",
    "comments": [
      "Root on ZFS is an easy sell for me. OpenBSD's ancient filesystem is notoriously flaky, and they have no interest in replacing it anytime soon.I can't be worried that critical parts of my network won't come back up because the box spontaneously rebooted or the UPS battery ran out (yes it happens \u2014 do you load test your batteries \u2014 probably not) and their bubblegum-and-string filesystem has corruption and / and /usr won't mount and I gotta visit the console like Sam Jackson in Jurassic Park to fsck the damn thing.Firewalls are critical infra \u2014 by definition they can't be the least reliable device in the network.reply",
      "conversely, running a firewall on something like ZFS also sounds like too much. Ideally I'd want a read-only root FS with maybe an /etc and /var managed by an overlay.reply",
      "Sounds like overcomplicating in the name of simplification. ZFS is a good, reliable, general-purpose system; often the right answer is to just put everything on ZFS and get on with your life.reply",
      "I\u2019ve had more problems with zfs than all other filesystems combined including FAT. It\u2019s definitely overkill for a root partition.",
      "I once wrote a similar post to an DVD industry centric mailing list (remember those?) regarding switching to FCP7 from Adobe Premiere with a huge difference in how FCP7 would allow capturing of discrete audio channels vs Premiere forcing an interleaved audio stream. Eventually, a rep from Adobe contacted me through my company's PR team (a first for me) to go over the list of complaints. At the end, he agreed these were all valid complaints, and then asked \"if Premiere added these changes would I be willing to switch back\"? At that point, I said probably not as we'd now be fully switched to FCP7 in all departments. So I understand that sentiment as well. Honestly, I was shocked that someone actually read my missive and actually paid any mind to it. So maybe someone at OpenBSD will be as receptive if not equally unable to do anything about it.reply",
      "As noted, recent changes to OpenBSD TCP handling[1] may improve performance.On a 4 core machine I see between 12% to 22% improvement with 10\nparallel TCP streams.  When testing only with a single TCP stream,\nthroughput increases between 38% to 100%.I'm not sure that directly translates to better pf performance, and four cores is hardly remarkable these days but might be typical on a small low-power router?Would be interesting if someone had a recent benchmark comparison of OpenBSD 7.8 PF vs. FreeBSD's latest.[1] https://undeadly.org/cgi?action=article;sid=20250508122430reply",
      "That particular change improves throughput received locally. Though over the past few years there's been a ton of work on unlocking the network layer generally to support more parallelism.For a firewall I guess the critical question is the degree of parallelism supported by OpenBSD's PF stack, especially as it relates to common features like connection statefulness, NAT, etc.reply",
      "Thanks. Yes after I posted that I started wondering if it was really relevant to pf.reply",
      "Can confirm. Lots of performance improvements lately in OpenBSD. Our Load Balancers basically doubled throughput after updating from 7.6 to 7.7reply",
      "I am not very familiar with FreeBSD's pf but my understanding is that fbsd integrated it from OpenBSD and then proceeded to put a fair amount of work in making it more performant(multi core) while OpenBSD put most of it's work into improving pf's features, At this point the two pf's are different enough that they are not really compatible. OpenBSD can't really use much of fbsds multi core work and FreeBSD is A. Is a lot more hesitant about breaking backwards compatibility and B. would need get the queuing structures to work with their kernel. In short FreeBSD pf is like using an old fast version of OpenBSD pfIn fact if you asked me to explain the difference between obsd and fbsd it is exactly this. fbsd focuses on performance and obsd focuses on ergonomics.reply"
    ],
    "link": "https://utcc.utoronto.ca/~cks/space/blog/sysadmin/OpenBSDToFreeBSDMove",
    "first_paragraph": " You're probably reading this page because you've attempted to\naccess some part of my blog (Wandering\nThoughts) or CSpace, the wiki thing it's\npart of. Unfortunately whatever you're using to do so has a HTTP\nUser-Agent header value that is too generic or otherwise excessively\nsuspicious. Unfortunately, as of early 2025 there's a plague of\nhigh volume crawlers (apparently in part to gather data for LLM\ntraining) that behave like this. To reduce the load on Wandering Thoughts I'm experimenting with\n(attempting to) block all of them, and you've run into this.  All HTTP User-Agent headers should clearly identify what they\nare, and for non-browser user agents, they should identify not just\nthe software involved but also who specifically is using that software.\nAn extremely generic value such as \"Go-http-client/1.1\"\nis not something that I consider acceptable any more. "
  },
  {
    "title": "The Bitter Lesson of LLM Extensions (sawyerhood.com)",
    "points": 79,
    "submitter": "sawyerjhood",
    "submit_time": "2025-11-24T18:32:27 1764009147",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=46037343",
    "comments": [
      "> Skills are the actualization of the dream that was set out by ChatGPT Plugins .. But I have a hypothesis that it might actually work now because the models are actually smart enough for it to work.and earlier Simon Willison argued[1] that Skills are even bigger deal than MCP.But I do not see as much hype for Skills as it was for MCP - it seems people are in the MCP \"inertia\" and having no time to shift to Skills.1. https://simonwillison.net/2025/Oct/16/claude-skills/reply",
      "Skills are less exciting because they're effectively documentation that's selectively loaded.They are a bigger deal in a sense because they remove the need for all the scaffolding MCPs require.E.g. I needed Claude to work on transcripts from my Fathom account, so I just had it write a CLI script to download them, and then I had it write a SKILL.md, and didn't have to care about wrapping it up into an MCP.At a client, I needed a way to test their APIs, so I just told Claude Code to pull out the client code from one of their projects and turn it into a CLI, and then write a SKILL.md. And again, no need to care about wrapping it up into an MCP.But this seems a lot less remarkable, and there's a lot less room to build big complicated projects and tooling around it, and so, sure, people will talk about it less.reply",
      "I agree with you. I don't see people hyping them and I think a big part of this is that we have sort of hit an LLM fatigue point right now. Also Skills require that your agent can execute arbitrary code which is a bigger buy-in cost if your app doesn't have this already.reply",
      "I still don't get what is special about the skills directory - since like forever I instructed Claud Code - \"please read X and do Y\" - how skills are different from that?reply",
      "They're not. They are just a formalization of that pattern, with a very tiny extra feature where the model harness scans that folder on startup and loads some YAML metadata into the system prompt so it knows which ones to read later on.reply",
      "So \"skills\"  are a hack around the LLM not actually being very smart? Interesting.reply",
      "It's more that they are embracing that the LLM is smart enough that you don't need to build-in this functionality beyond that very minimal part.A fun thing: Claude Code will sometimes fail to find the skill the \"proper\" way, and will then in fact sometimes look for the SKILL.md file with tools, and read the file with tools, showing that it's perfectly capable of doing all the steps.You could probably \"fake\" skills pretty well with instructions in CLAUDE.md to use a suitable command to extract the preamble of files in a given directory, and tell it to use that to decide when to read the rest.It's the fact that it's such a thin layer that is exciting - it means we need increasingly less special logic other than relying on just basic instructions to the model itself.reply",
      "The difference is that the code in the directory (and the markdown) are hardcoded and known to work beforehand.reply",
      "But we are still reliant on the LLM correctly interpreting the choice to pick the right skill.  So \"known to work\" should be understood in the very limited context of \"this sub-function will do what it was designed to do reliably\" rather than \"if the user asks to use this sub-function it will do was it was designed to do reliably\".Skills feel like a non-feature to me.  It feels more valuable to connect a user to the actual tool and let them familiarize themselves with it (and not need the LLM to find it in the future) rather than having the tool embedded in the LLM platform.  I will carve out a very big exception of accessibility here - I love my home device being an egg timer - it's a wonderful egg timer (when it doesn't randomly play music) and I could buy an egg timer but having a hands-free egg timer is actually quite valuable to me while cooking.  So I believe there is real value in making these features accessible through the LLM over media that the feature would normally be difficult to use in.reply",
      "This is no different to an MCP, where you rely on the model to use the metadata provided to pick the right tool, and understand how to use it.Like with MCP, you can provide a deterministic, known-good piece of code to carry out the operation once the LLM decides to use it.But a skill can evolve from pure Markdown via inlining some shell commands, up to a large application. And if you let it, with Skills the LLM can also inspect the tool, and modify it if it will help you.All the Skills I use now have evolved bit by bit as I've run into new use-cases and told Claude Code to update the script the skills references or the SKILL.md itself. I can evolve the tooling while I'm using it.reply"
    ],
    "link": "https://www.sawyerhood.com/blog/llm-extension",
    "first_paragraph": "Three years ago, \u201cusing an LLM\u201d meant pasting a wall of text into a chat box and hoping for something useful back. Today, we point agents at our codebases, our browsers, and let them go off and act on our behalf. A key question that has been brewing under the surface during this time has been: how do we let end users actually customize these systems?As models have become more capable, the ways and mechanisms that end users have access to customize them have expanded as well. We've gone from simple system prompts to complex client-server protocols and back again.I wanted to take a moment to reflect on the history of LLM extension over the last three years and where I see it going in the future.Just four months after launch, OpenAI announced ChatGPT Plugins. Looking back, these were wildly ahead of their time.The idea was ambitious: give the LLM a link to an OpenAPI spec and let it \"run wild\" calling REST endpoints. It was a direct line to AGI-style thinking: universal tool use via stand"
  },
  {
    "title": "Random lasers from peanut kernel doped with birch leaf\u2013derived carbon dots (degruyterbrill.com)",
    "points": 12,
    "submitter": "PaulHoule",
    "submit_time": "2025-11-19T16:31:44 1763569904",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=45981568",
    "comments": [
      "sounds like a Brian Jacques superweaponreply",
      "Would this work on peanuts?\"Near-Field Optical Nanopatterning of Graphene\" (2025) https://onlinelibrary.wiley.com/doi/10.1002/smsc.202500184 .. \nhttps://news.ycombinator.com/item?id=45623301Why are they random lasers?From https://news.ycombinator.com/item?id=45949800 :> \"Cavity electrodynamics of van der Waals heterostructures\" (2024) https://arxiv.org/abs/2403.19745 ; graphite / graphene optical cavityFrom https://news.ycombinator.com/item?id=44922581 :> \"Grover's algorithm to efficiently prepare quantum states in optical cavity QED\" (2025) https://phys.org/news/2025-08-grover-algorithm-efficiently-q...:>> \"Deterministic carving of quantum states with Grover's algorithm\" (2025) https://journals.aps.org/pra/abstract/10.1103/s3vs-xz7w reply"
    ],
    "link": "https://www.degruyterbrill.com/document/doi/10.1515/nanoph-2025-0312/html",
    "first_paragraph": "Your purchase has been completed.  Your documents are now available to view.The intrinsically disordered periodic architecture inherent in natural biomaterials exhibits significant potential for serving as resonant cavities, enabling the development of eco-friendly, biocompatible, and cost-effective microlaser systems. In this study, we demonstrate a biomaterial-based random laser utilizing birch leaf\u2013derived carbon dots (CDs) as the gain medium. CDs ethanol solution was introduced into the peanut via microinjection, successfully fabricating CDs-doped peanut samples that preserved the fluorescence characteristics of the CDs in solution. Random lasing was observed on multiple surfaces of the CDs-doped peanut under pulsed laser excitation, with varying thresholds across different regions. This demonstrates that the natural disordered microstructure of biological materials can facilitate random lasing. Analysis of surface morphology and scattering patterns indicates that the lasing mechan"
  },
  {
    "title": "Show HN: OCR Arena \u2013 A playground for OCR models (ocrarena.ai)",
    "points": 52,
    "submitter": "kbyatnal",
    "submit_time": "2025-11-21T16:44:45 1763743485",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=46006104",
    "comments": [
      "I've been really impressed with this model specifically because of how insanely cheap it is: https://replicate.com/ibm-granite/granite-vision-3.3-2bI didn't expect IBM to be making relevant AI models but this thing is priced at $1 per 4,000,000 output tokens... I'm using it to transcribe handwritten input text and it works very well and super fast.reply",
      "Love this! Would have liked to see something like textract for a pre-LLM benchmark (but of course that's expensive), and also a distinction between handwritten text and printed one.But still, this is incredibly useful!reply",
      "I would be curious to see how Sonnet does. Their models are pretty solid when it comes to PDFsreply",
      "Sonnet/Opus is being added shortly!reply",
      "FYI one of the models on the battle was pretty slow to load. Are these also being rated on latency or just quality?reply",
      "Ultimately, there\u2019s some intersection of accuracy x cost x speed that\u2019s ideal, which can be different per use case. We\u2019ll surface all of those metrics shortly so that you can pick the best model for the job along those axes.reply",
      "ideally we want people to rate based on quality - but i imagine some of the results are biased rn based on loading timereply",
      "Please add Chandra by Datalabreply",
      "Most of these are general LLM\u2019s and not specifically OCR models.\nWhere is Google Vision, Mistral, Paddle, Nanonets, or Chandra??reply",
      "We wanted to keep the focus on (1) foundation VLMs and (2) open source OCR models.We had Mistral previously but had to remove it because their hosted API for OCR was super unstable and returned a lot of garbage results unfortunately.Paddle, Nanonets, and Chandra being added shortly!reply"
    ],
    "link": "https://www.ocrarena.ai/battle",
    "first_paragraph": "Upload an image to start an anonymous OCR battleNeed a document? Get a random oneSupports PDF, JPEG, and PNG filesDrop a file to extract textSupports PDF, JPEG, and PNG filesAnonymous Model 1Waiting for battle to start...Anonymous Model 2Waiting for battle to start...Drop a file to extract textSupports PDF, JPEG, and PNG filesAnonymous Model 1Waiting for battle to start...Anonymous Model 2Waiting for battle to start...Upload an image to start an anonymous OCR battleNeed a document? Get a random oneSupports PDF, JPEG, and PNG files"
  },
  {
    "title": "What OpenAI did when ChatGPT users lost touch with reality (nytimes.com)",
    "points": 97,
    "submitter": "nonprofiteer",
    "submit_time": "2025-11-24T05:58:08 1763963888",
    "num_comments": 100,
    "comments_url": "https://news.ycombinator.com/item?id=46030799",
    "comments": [
      "https://archive.is/v4dPa",
      "One of the more disturbing things I read this year was the my boyfriend is AI subreddit.I genuinely can't fathom what is going on there. Seems so wrong, yet no one there seems to care.I worry about the damage caused by these things on distressed people. What can be done?reply",
      "There are plenty of reasons why having a chatbot partner is a bad idea (especially for young people), but here's just a few:- The sycophantic and unchallenging behaviours of chatbots leaves a person unconditioned for human interactions. Real relationships have friction, from this we develop important interpersonal skills such as setting boundaries, settling disagreements, building compromise, standing up for oneself, understanding one another, and so on. These also have an effect on one's personal identity and self-value.- Real relationships have the input from each participant, whereas chatbots are responding to the user's contribution only. The chatbot doesn't have its own life experiences and happenings to bring to the relationship, nor does it instigate autonomously, it's always some kind of structured reply to the user.- The implication of being fully satisfied by a chatbot is that the person is seeking a partner who does not contribute to the relationship, but rather just an entity that only acts in response to them. It can also be an indication of some kind of problem that the individual needs to work through with why they don't want to seek genuine human connection.reply",
      "> chatbots are responding to the user's contribution onlyWhich is also why I feel the label \"LLM Psychosis\" has some merit to it, despite sounding scary.Much like auditory hallucinations where voices are conveying ideas that seem-external-but-aren't... you can get actual text/sound conveying ideas that seem-external-but-aren't.Oh, sure, even a real human can repeat ideas back at you in a conversation, but there's still some minimal level of vetting or filtering or rephrasing by another human mind.reply",
      ">  even a real human can repeat ideas back at you in a conversation, but there's still some minimal level of vetting or filtering or rephrasing by another human mind.The mental corruption due to surrounding oneself with sycophantic yes men is historically well documented.reply",
      "Excellent point. It\u2019s bad for humans when humans do it! Imagine the perfect sycophant, never tires or dies, never slips, never pulls a bad facial expression, can immediately swerve their thoughts to match yours with no hiccups.It was a danger for tyrants and it\u2019s now a danger for the lonely.reply",
      "These are only problems if you assume the person later wants to come back to having human relationships.  If you assume AI relationships are the new normal and the future looks kinda like The Matrix, with each person having their own constructed version of reality while their life-force is bled dry by some superintelligent machine, then it is all working as designed.reply",
      "Someone has to make the babies!reply",
      "Wait, how did this work in The Matrix exactly?reply",
      "Artificial wombs \u2013 we're on it.reply"
    ],
    "link": "https://www.nytimes.com/2025/11/23/technology/openai-chatgpt-users-risks.html",
    "first_paragraph": ""
  },
  {
    "title": "PS5 now costs less than 64GB of DDR5 memory. RAM jumps to $600 due to shortage (tomshardware.com)",
    "points": 262,
    "submitter": "speckx",
    "submit_time": "2025-11-24T19:29:12 1764012552",
    "num_comments": 160,
    "comments_url": "https://news.ycombinator.com/item?id=46038143",
    "comments": [
      "Potentially unpopular take: memory manufacturers have been operating on the margins of profitability for quite a while now.  Their products are essentially an indistinguishable commodity.  Memory from Samsung or Micron or another manufacturer may have slight differences in overclockability, but that matters little to folks who just want a stable system.  Hopefully the shortage leads large purchasers to engage in long-term contracts with the memory manufacturers which give them the confidence to invest in new fabs and increased capacity.  That would be great for everyone.  Additionally, we're likely to see Chinese fab'd DRAM now, which they've been attempting since the '70s but never been competitive at.  With these margins, any new manufacturer could gain a foothold.If LLMs' utility continues to scale with size (which seems likely as we begin training embodied AI on a massive influx of robotic sensor data) then it will continue to gobble up memory for the near future.  We may need both increased production capacity _and_ a period of more efficient software development techniques as was the case when a new 512kb upgrade cost $1,000.reply",
      "> Hopefully the shortage leads large purchasers to engage in long-term contracts with the memory manufacturers which give them the confidence to invest in new fabs and increased capacity.Most DRAM is already purchased through contracts with manufacturers.Manufacturers don't actually want too many extremely long term contracts because it would limit their ability to respond to market price changes.Like most commodities, the price you see on places like Newegg follows the \"spot price\", meaning the price to purchase DRAM for shipment immediately. The big players don't buy their RAM through these channels, they arrange contracts with manufacturers.The contracts with manufacturers will see higher prices in the future, but they're playing the long game and will try to delay or smooth out purchasing to minimize exposure to this spike.> Additionally, we're likely to see Chinese fab'd DRAM now, which they've been attempting since the '70s but never been competitive at.Companies like Samsung and SK Hynix have DRAM fabs in China already. This has been true for decades. You may have Chinese fab'd DRAM in the computer you're using right now.Are you referring to complete home-grown DRAM designs? That, too, was already in the works.reply",
      "> Manufacturers don't actually want too many extremely long term contracts because it would limit their ability to respond to market price changes.\n\nI don't agree with this sentence.  Why would not the same apply advice to oil and gas contracts?  If you look at the size and duration of oil and gas contracts for major energy importers, they often run 10 years or more.  Some of the contracts in Japan and Korea are so large, that a heavy industrial / chemical customers will take an equity stake in the extraction site.Except silicon, power, and water (and a tiny amount of plastic/paper for packaging), what else does a fab need that only produces DRAM?  If true, then power is far and away the most variable input cost.reply",
      ">  Why would not the same apply advice to oil and gas contracts?Because oil & gas suppliers only ever sell one product, and memory fabs can dynamically switch product mix in response to supply & demand to optimize profits. The same sand, power and water can make DDR4, HBM or DDR5reply",
      "> Except silicon, power, and waterVarious chemicals too, https://haz-map.com/Processes/97reply",
      "> Are you referring to complete home-grown DRAM designs? That, too, was already in the works.Yes, via cxmt as discussed by Asianometry here: https://www.youtube.com/watch?v=mt-eDtFqKvkAs I mentioned, various groups within China have been working on China-native DRAM since the '70s.  What's new are the margins and market demand to allow them to be profitable with DRAM which is still several years behind the competition.reply",
      "Well, what really prompted this crisis is AI, as well as Samsung shutting down some production (and I have to say I don't think they mind that the pricing has skyrocketed as a result!)But yes we're going to need more fabs for surereply",
      "> Well, what really prompted this crisis is AI,If the shortage of RAM is because of AI (so servers/data centers I presume?), wouldn't that mean the shortage should be localized to RDIMM rather than the much more common UDIMM that most gaming PCs use? But it seems to me like the pricing is going up more for UDIMM than RDIMM.reply",
      "UDIMM and RDIMM use the same DRAM chips. And my understanding is that the fabs can switch between DDR5, LPDDR5, and maybe HBM as needed. This means high demand for one type can create a shortage of the others.reply",
      "It's a valid question if you're not familiar with the RAM market. Sorry you're getting downvoted for it.The manufacturers make the individual chips, not the modules (DIMMs). (EDIT: Some companies that make chips may also have business units that sell DIMMS, to be pedantic.)The R in RDIMM means register, aka buffer. It's a separate chip that buffers the signals between the memory chips and the controller.Even ECC modules use regular memory chips, but with extra chips added for the ECC capacity.It can be confusing. The key thing to remember is that the price is driven by the price of the chips. The companies that make DIMMs are buying chips in bulk and integrating them on to PCBs.reply"
    ],
    "link": "https://www.tomshardware.com/pc-components/ddr5/64gb-of-ddr5-memory-now-costs-more-than-an-entire-ps5-even-after-a-discount-trident-z5-neo-kit-jumps-to-usd600-due-to-dram-shortage-and-its-expected-to-get-worse-into-2026",
    "first_paragraph": "You can spend $50 more to get an entire PS5 Pro.\nWhen you purchase through links on our site, we may earn an affiliate commission. Here\u2019s how it works.\nThanks to the AI boom devouring the majority of the world's memory and storage supply, end-consumers are now facing increasingly inflated prices for common components. DDR5 RAM, a necessity for building current-gen Intel or AMD systems, has now reached record highs in terms of pricing; a 64 GB kit of G.Skill's Trident Z5 Neo 6000 MT/s RAM is listed at $599.99 on Newegg right now \u2014 that's $200 more than a PS5 Slim or a Microsoft Xbox Series S, and just $50 shy off an entire PS5 Pro at the moment.G.SKILL Trident Z5 Neo RGB Series 64GB DDR5 6000 (PC5 48000)$599($599 in Black Friday sale, with $40 off and additional $20 off via code BFE2458)Sony PlayStation 5 Pro$649Sony PlayStation 5 Digital$399Sony PlayStation 5 Slim Disc$449Microsoft Xbox Series S$399Microsoft Xbox Series X 1TB Digital Edition$569That $600 price tag has a 6% discount alr"
  },
  {
    "title": "How sea turtles learn locations using Earth\u2019s magnetic field: research (unc.edu)",
    "points": 10,
    "submitter": "hhs",
    "submit_time": "2025-11-21T23:43:53 1763768633",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://uncnews.unc.edu/2025/02/13/sea-turtles-secret-gps-researchers-uncover-how-sea-turtles-learn-locations-using-earths-magnetic-field/",
    "first_paragraph": "A new study from researchers at the University of North Carolina at Chapel Hill provides the first empirical evidence that loggerhead sea turtles can learn and remember the unique magnetic signatures of different geographic regions. This discovery offers new insights into how turtles and other migratory animals navigate vast distances to reach specific foraging and breeding grounds. The findings, published in the journal Nature, also suggest that sea turtles possess two distinct magnetic senses that function differently to detect the Earth\u2019s magnetic field.\u00a0Loggerhead turtles are famous for their extraordinary migrations, guided by an internal magnetic map that enables them to determine their location by detecting variations in Earth\u2019s magnetic field. Until now, scientists had speculated that turtles might also have the ability to learn and recognize magnetic fields associated with important locations, but no empirical evidence had confirmed this ability\u2014until now.\u00a0\u201cOur study investiga"
  },
  {
    "title": "Chrome Jpegxl Issue Reopened (chromium.org)",
    "points": 212,
    "submitter": "markdog12",
    "submit_time": "2025-11-24T12:23:02 1763986982",
    "num_comments": 79,
    "comments_url": "https://news.ycombinator.com/item?id=46033330",
    "comments": [
      "\"Yes, re-opening.\".> Given these positive signals, we would welcome contributions to integrate a performant and memory-safe JPEG XL decoder in Chromium. In order to enable it by default in Chromium we would need a commitment to long-term maintenance. With those and our usual launch criteria met, we would ship it in Chrome.https://groups.google.com/a/chromium.org/g/blink-dev/c/WjCKc...reply",
      "Context: Mozilla has had the same stance and many devs (including Googlers) are already working on a Rust decoder which has made good progress.reply",
      "LOL. Google, the \"yeah that thing we bought six months ago, we're killing it off 30 days for 4 weeks ago\" company demanding \"long-term\" anything.reply",
      "long term support is actually being provided by google...just a different team in a different country :Dmost jxl devs are at google research in zurich, and already pledged to handle long tetm supportreply",
      "Just like google pledges long term support for everything until the next new and shiny comes along.reply",
      "Dupe. From yesterday (183 points, 82 comments):https://news.ycombinator.com/item?id=46021179reply",
      "Ah, I think I searched for \"jpegxl\", that's why there was no match.reply",
      "JPEG-XL provides the best migration path for image conversion from JPEG, with lossless recompression. It also supports arbitrary HDR bit depths (up to 32 bits per channel) unlike AVIF, and generally its HDR support is much better than AVIF. Other operating systems and applications were making strides towards adopting this format, but Google was up till now stubbornly holding the web back in their refusal to support JPEG-XL in favour of AVIF which they were pushing. I\u2019m glad to hear they\u2019re finally reconsidering. Let\u2019s hope this leads to resources being dedicated to help build and maintain a performant and memory safe decoder (in Rust?).reply",
      "It's not just Google, Mozilla has no desire to introduce a barely supported massive C++ decoder for marginal gains either:https://github.com/mozilla/standards-positions/pull/1064avif is just better for typical web image quality, it produces better looking images and its artifacts aren't as annoying (smoothing instead of blocking and ringing around sharp edges).You also get it for basically free because it's just an av1 key frame. Every browser needs an av1 decoder already unless it's willing to forego users who would like to be able to watch Netflix and YouTube.reply",
      "I don't understand what you're trying to say. Mozilla said over a year ago that they would support JXL as soon as there's a fast memory safe decoder that will be supported.Google on the other hand never expressed any desire to support JXL at all, regardless of the implementation. Only just now after the PDF Association announced that PDF would be using JXL, did they decide to support JXL on the web.> avif is just better for typical web image quality, it produces better looking images and its artifacts aren't as annoying (smoothing instead of blocking and ringing around sharp edges).AVIF is certainly better for the level of quality that Google wants you to use, but in reality, images on the web are much higher quality than that.And JXL is pretty good if you want smoothing, in fact libjxl's defaults have gotten so overly smooth recently that it's considered a problem which they're in the process of fixing.reply"
    ],
    "link": "https://issues.chromium.org/issues/40168998",
    "first_paragraph": ""
  },
  {
    "title": "Bytes before FLOPS: your algorithm is (mostly) fine, your data isn't (bitsdraumar.is)",
    "points": 40,
    "submitter": "bofersen",
    "submit_time": "2025-11-23T15:47:50 1763912870",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=46024402",
    "comments": [
      "To the list of profiling tools I would like to add KDAB Hotspot and KDE Heaptrack.The former, hotspot, is a visualiser for perf data, and it deals ok with truly massive files that made perfetto and similar just big down. It also supports visualing off-CPU profiles (\"why is my program slow but not CPU bound?\").The latter, heaptrack, is a tool with very similar UI to hotspot (I think the two tools share some code even) to profile malloc/free (or new/delete). Sometimes the performance issue is as simple as not reusing a buffer but reallocating it over and over inside a loop. And sometimes you wonder where all the memory is going.reply",
      "Great article. Can confirm, writing performance focused C# is fun. It's great having the convenience of async, LINQ, and GC for writing non-hot path \"control plane\" code, then pulling out Vector<T>, Span<T>, and so on for the hot path.One question, how portable are performance benefits from tweaks to memory alignment? Is this something where going beyond rough heuristics (sequential access = good, order of magnitude cache sizes, etc) requires knowing exactly what platform you're targeting?reply",
      "Author here. First of all, thanks for the compliment! It\u2019s tough to get myself to write these days, so any motivation is appreciated.And yes, once all the usual tricks have been exhausted, the nest step is looking at the cache/cache line sizes of the exact CPU you\u2019re targeting and dividing the workload into units that fit inside the (lowest level possible) cache, so it\u2019s always hot. And if you\u2019re into this stuff, then you\u2019re probably aware of cache-oblivious algorithms[0] as well :)Personally, I almost never had the need to go too far into platform-specific code (except SIMD, of course), doing all the stuff in the post is 99% of the way there.And yeah, C# is criminally underrated, I might write a post comparing high-perf code in C++ and C# in the future.[0]: https://en.wikipedia.org/wiki/Cache-oblivious_algorithmreply",
      "> worst case scenario being the flat profile where program time is roughly evenly distributedIt sounds like the \u201cworst case\u201c here is that the program is already optimized.reply",
      "Author here, kinda sorta. I should've been a bit more specific than that.\nYou can have a profile showing a function taking up 99% of the time, but when you dive into it, there's no clear bottleneck. But just because there's no bottleneck, that doesn't mean it's optimized; vice versa-a well-optimized program can have a bottleneck that's already been cycle-squeezed to hell and back.What I wanted to say was that a spiky profile provides a clear path to optimizing a piece of code, whereas a flat profile usually means there are more fundamental issues (inefficient memory management, pointer chasing all over the place, convoluted object system, etc.).reply",
      "It sounds like a flat profile essentially is a local optimum, compared to cases where there's a path \"upwards\" along a hill to some place more optimal that doesn't require completely changing your strategy.reply",
      "That's actually a good observation, yeah. It's often the case that you dig deeper and deeper and find some incomprehensible spaghetti and just say \"fuck it, I'll just do what I can here, should be enough\".reply",
      "Not necessarily. It could just be uniformly slow with no particular bottleneck.reply"
    ],
    "link": "https://www.bitsdraumar.is/bytes-before-flops/",
    "first_paragraph": "A small, but deep dive into performance and data-oriented optimization.So, earlier this year I did a presentation at a cool(get it?) little Norwegian company called Airthings.It was a two-parter. First part was how the compilation pipeline works - from ASCII to assembly, greatly simplified of course. It was from an old presentation I did many years ago.\nThe second part is the interesting. It's about performance and what are some of the low-hanging fruits that you can pick off to speed up stuff as much as possible.Now, that part actually takes a while, but in short it's basically a distillation of my experience working on compilers, assembly and writing vectorized code.\nThe gist of it is - data.It occurred to me that a small summary would make a good blog post. There's plenty of material on data-oriented design and performance, but this might be a nice overview.I'll style it as a written representation of how I approach optimization.When I approach an optimization effort, first thing I "
  },
  {
    "title": "Everything you need to know about hard drive vibration (2016) (ept.ca)",
    "points": 22,
    "submitter": "asdefghyk",
    "submit_time": "2025-11-20T16:07:22 1763654842",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=45994124",
    "comments": [
      "I recall a video of a guy temporarily reducing hard drive performance by shouting at itedit: here it is! https://www.youtube.com/watch?v=tDacjrSCeq4reply",
      "That is not just any guy though. He is the guy.reply",
      "The guy if you care about systems performance, in a detailed way, for sure!Someone ask him how many OS kernel bugs he\u2019s found now?  He finds the weirdest things\u2026 a tally would be \u201cinteresting\u201d.reply",
      "next try shouting at a wafer stepperreply",
      "Hard Disk Drives (HDD\u2019s) are one of the most impressive and important electromechanical devices ever created. The mechanics of HDD vibration is an obscure subject, and as a result, there is an aura of mystery surrounding vibration ...reply",
      "I could not get to the article, so from the wayback machine:https://web.archive.org/web/20250613075332/https://www.ept.c...reply"
    ],
    "link": "https://www.ept.ca/features/everything-need-know-hard-drive-vibration/",
    "first_paragraph": "\n\t\t\t\t\t\t\tNovember 14, 2016\u00a0\n\t\t\t\t\t\t\t By Brett Kelly, R&D Engineer at 45Drives\n\t\t\t\t\t\t\t\n\t\t\t\t\t\tHard Disk Drives (HDD\u2019s)\u00a0are one of the most impressive and important electromechanical devices ever created. When I think about it, it is really amazing that these things actually work, let alone work so well! These disks must quickly and precisely position heads slightly above very narrow tracks on rapidly spinning platters. If there is an error in the angular position of the head, it will not be positioned above the correct sector and therefore cannot correctly read or write data.One thing that can interfere with this amazingly fine and fast positioning is vibration.\u00a0Vibration can cause small variations in the position of the head with respect to the track, so it can be an enemy of the proper functioning of HDD\u2019s. But the mechanics of HDD vibration is an obscure subject, and as a result, there is an aura of mystery surrounding vibration. As part of my job, I\u2019ve\u00a0spent a lot of time and effort to"
  },
  {
    "title": "Google's new 'Aluminium OS' project brings Android to PC (androidauthority.com)",
    "points": 46,
    "submitter": "jmsflknr",
    "submit_time": "2025-11-24T18:49:47 1764010187",
    "num_comments": 49,
    "comments_url": "https://news.ycombinator.com/item?id=46037591",
    "comments": [
      "If you, like me, were wondering why Google thinks it needs another operating system (ChromeOS, Android, Fuchsia - which is presumably dead (edit/turns out it's not/edit)) or where it fits in with the \"stack\":> ChromeOS and Aluminium Operating System (ALOS) Commercial devices across all form factors (e.g. laptops, detachables, tablets, and boxes) and tiers (e.g., Chromebook, Chromebook Plus, AL Entry, AL Mass Premium, and AL Premium) that meets the needs of users and the business.Sounds like ChromeOS is Android for entry/thin and similar PC's and Aluminum is more upmarket/premium.Also, to be honest, this doesn't seem like \"a new OS\" to me, but rather a shift in Android's roadmap and an associated rebrand to try to push ChromeOS/Android upmarket to try and expand their \"Devices with Gemini/Google AI as a first-class service/product\" footprint beyond cell phones.Given the push for arm in the consumer PC space, I can kinda see why google is renewing efforts here even if you set the AI stuff aside.reply",
      "Let's be honest, nobody is asking for android based desktops, google just wants to normalize rent seeking 30% of all software sales.reply",
      "For all the complaints against Windows, legit or not, I can't envision a world in which I want the world's largest advertiser to run my desktop OS.reply",
      "Oh, Fuchsia isn't dead [0]. Apparently it's what the Nest Hub launched with and the latest update is pretty recent: from Oct 2025. Interesting.(Replying to my own comment instead of editing it as this is tangential to the topic at hand)0: https://en.wikipedia.org/wiki/Fuchsia_(operating_system)reply",
      "Not only is it not dead it\u2019s under HEAVY active development and has been for quite some time now.They seem particularly focused on the Linux compatibility layer (starnix) as far as I can tell.I\u2019d say they are most likely going to end up becoming the thing that Android sits on top of. There is already public indications of some variant of it called \u201cmicrofuchsia\u201d coming to Android. I wouldn\u2019t be surprised to hear that this is all part of the same launch that they are working towards here.reply",
      "AluminIum you say?reply",
      "Team started in Australia, they use British spellings.reply",
      "The name makes sense because Aluminium has an -ium suffix like Chromium.  There's also no reason for the project name to agree with the US pronunciation of the element.reply",
      "Well, it makes sense and it doesn't because it makes it sound like this is a 'lightweight' version of the Chromium-based products while the opposite seems to be true. Call it Osmium instead, that's got '-ium' and some weight to it just like this thing.reply",
      "Is there any Android app that is worth using on a PC?  Not being snarky, I cant see anything on Android being good enough for a desktop app that is used regularly.  Most of the Android apps I use are the 'best of the worse' and I have to use them because there is no other options.reply"
    ],
    "link": "https://www.androidauthority.com/aluminium-os-android-for-pcs-3619092/",
    "first_paragraph": ""
  }
]