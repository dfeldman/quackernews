[
  {
    "title": "AI solves International Math Olympiad problems at silver medal level (deepmind.google)",
    "points": 776,
    "submitter": "ocfnash",
    "submit_time": "2024-07-25T15:29:41",
    "num_comments": 376,
    "comments_url": "https://news.ycombinator.com/item?id=41069829",
    "comments": [
      "So I am extremely hyped about this, but it's not clear to me how much heavy lifting this sentence is doing:> First, the problems were manually translated into formal mathematical language for our systems to understand.The non-geometry problems which were solved were all of the form \"Determine all X such that\u2026\", and the resulting theorem statements are all of the form \"We show that the set of all X is {foo}\". The downloadable solutions from https://storage.googleapis.com/deepmind-media/DeepMind.com/B... don't make it clear whether the set {foo} was decided by a human during this translation step, or whether the computer found it. I want to believe that the computer found it, but I can't find anything to confirm. Anyone know?\n \nreply",
      "The computer did find the answers itself. I.e., it found \"even integers\" for P1, \"{1,1}\" for P2, and \"2\" for P6. It then also provided provided a Lean proof in each case.\n \nreply",
      "Can you elaborate on how it makes guesses like this? Does it do experiments before? Is it raw LLM? Is it feedback loop based on partial progress?\n \nreply",
      "\"AlphaProof is a system that trains itself to prove mathematical statements in the formal language Lean. It couples a pre-trained language model with the AlphaZero reinforcement learning algorithm, which previously taught itself how to master the games of chess, shogi and Go.\"\n \nreply",
      "Huh, so MCTS to find the \u2018best\u2019 token using a (relatively) small, quick language model? Sounds like an interesting approach to small model text generation too\u2026\n \nreply",
      "Yeah I am not clear the degree to which this system and LLMs are related. Are they related? Or is AlphaProof a complete tangent to CHatGPT and its ilk?\n \nreply",
      "It's not an English LLM (Large Language Model).It's a math Language Model. Not even sure it's a Large Language Model. (Maybe shares a foundational model with an English LLM; I don't know)It learns mathematical statements, and generates new mathematical statements, then uses search techniques to continue. Similar to Alpha Go's neural network, what makes it new and interesting is how the NN/LLM part makes smart guesses that drastically prune the search tree, before the brute-force search part.(This is also what humans do to solve math probrems. But humans are really, really slow at brute-force search, so we really almost entirely on the NN pattern-matching analogy-making part.)\n \nreply",
      "My reading of it is that it uses the same architecture as one of the Gemini models but does not share any weights with it. (i.e it's not just a finetune)\n \nreply",
      "These kind of LLMs are also very interesting for software engineering. It's just a matter of replacing Lean with something that is more oriented towards proving software properties.For example, write a formal specification of a function in Dafny on Liquid Haskell and get the LLM to produce code that is formally guaranteed to be correct. Logic-based + probability-based ML.All GOFAI ideas are still very useful.\n \nreply",
      "formal definition of first theorem already contain answer of the problem \n\"{\u03b1 : \u211d | \u2203 k : \u2124, Even k \u2227 \u03b1 = k}\" (which mean set of even real numbers).if they say that they have translated first problem into formal definition then it is very interesting how they initially formalized problem without including answer in it\n \nreply"
    ],
    "link": "https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/",
    "first_paragraph": "Latest company postsLatest technology postsLatest postsResearchAlphaProof and AlphaGeometry teamsBreakthrough models AlphaProof and AlphaGeometry 2 solve advanced reasoning problems in mathematicsArtificial general intelligence (AGI) with advanced mathematical reasoning has the potential to unlock new frontiers in science and technology.We\u2019ve made great progress building AI systems that help mathematicians discover new insights, novel algorithms and answers to open problems. But current AI systems still struggle with solving general math problems because of limitations in reasoning skills and training data.Today, we present AlphaProof, a new reinforcement-learning based system for formal math reasoning, and AlphaGeometry 2, an improved version of our geometry-solving system. Together, these systems solved four out of six problems from this year\u2019s International Mathematical Olympiad (IMO), achieving the same level as a silver medalist in the competition for the first time.The IMO is the"
  },
  {
    "title": "Jacek Karpi\u0144\u015bki, the computer genius the communists couldn't stand (2017) (culture.pl)",
    "points": 167,
    "submitter": "janisz",
    "submit_time": "2024-07-25T18:50:04",
    "num_comments": 65,
    "comments_url": "https://news.ycombinator.com/item?id=41072026",
    "comments": [
      "Officials weren't keen on these bespoke machines because there was a Comecon push to standardize on unified system based on IBM 360, with compatible peripherals manufactured in many Eastern bloc countries: https://en.wikipedia.org/wiki/ES_EVMK-202 was later developed into MERA 400, which was somewhat more successful. https://mera400-pl.translate.goog/Strona_g%C5%82%C3%B3wna?_x...\nOne of interesting things from modern perspective is that these machines didn't have synchronous clocks, but cycles were timed by RC delay circuits which were tuned differently for various groups of instructions. One notable operating system developed for it was CROOK, which was significantly more unixy compared to other contemporary mainframe based systems.\nThere's modern emulator for it https://github.com/jakubfi/em400, and they have excellent YouTube channel (in Polish though): https://www.youtube.com/@MERA400/videos\n \nreply",
      "It was incredibly expensive to have dozens of research institutes work on esoteric architectures at their pleasure. The majority of the population worked their asses of in poverty to make thousands of tanks and dozens of submarines, not entertain some computer scientists.\n \nreply",
      "> It\u2019s believed that Karpi\u0144ski paved the way for today\u2019s common use of paging in computer memory systems.So, I didn't know anything about the K-202, so this is very interesting to me.However, are we sure that it influenced anything, given that only about 230 of them were ever made, and those were destroyed at the factory? How would knowledge of his team's innovations, let alone specific information, have leaked out to western computer designers from within Soviet Poland? If it did, was the mechanism... espionage, published research, what?\n \nreply",
      "From the article,>Thus in 1970, the Microcomputers Plant was established. Located in Warsaw, it employed Polish workers but used British components and financing.The British were involved in every step.\n \nreply",
      "As an aside, it's really kind of nuts how much backing and investment the Communists have received from the West, including Wall Street, all the way to the Russian Revolution, and continuing to the present day.\n \nreply",
      "The Wikipedia page says it was actually segmented memory rather than paging as it is defined today.\n \nreply",
      "More like both, there was a block address register (BAR) and when it was 0..63 that was selecting a core memory board. So that's sort of like segmentation but core could be paged out from and in to tape, disc, or drum. For example the OS (OPSYS) could be paged out to free up block 0 of core memory. So that's paging and even neater it was basically a page per program so independent programs could run concurrently, paging core in and out as needed. The paging was handled by controllers (disc, tape, drum) with very little involvement of the CPU (executive).See 3.1.1, 3.3.2, 3.4, 3.5, 5, & 5.2: http://www.zenker.poznan.pl/k-202/dokumentacja/k-202-reklama...\n \nreply",
      "the article mentions one other Perceptron of its kind in the U.S., and that has another really interesting story (this one got downplayed by Marvin Minsky but the inventor died young in an accident): https://news.cornell.edu/stories/2019/09/professors-perceptr...\n \nreply",
      "I don't have anything to add on the subject of the article, but just want to mention I really like the site it's published on. Alongside przekroj.org (which recently started dipping its toes into publishing also in English) it is one of my favorite places on the web. No clickbait, no quantity over quality, just (mostly) interesting, well researched content. I wish there were more places like these around the internet.\n \nreply",
      "Really good story. I recommend reading the whole thing.A couple things that I think add some useful context, having spent some of my life in other post-soviet countries:- In Communist theory (at least as Marx and Lenin saw it) business competition was a destructive force and one of the \"inherent contradictions\" of capitalism. So if another project was deemed to be better, that was justification enough for shutting down other enterprises.-  Also, during this time under soviet communism, the most common measure of manufacturing was in kilograms output. Karpi\u0144ski working on a small run of small computers would not have looked impressive to officials in the least.- Importing western materials and parts was not expressly forbidden (though certainly not politically popular). But Poland (like a lot of Soviet countries) was undergoing a currency crunch. Possession of foreign currency was illegal and importing materials did not do favorable things for their \"fake\" exchange rates. The operation was probably contingent on generating more foreign dollars than they spent.- Computers in general were viewed skeptically as Western excesses that either wasted time or stole jobs. So making boring calculating machines for accounting or scientific research could be seen as acceptable - but small, cheap, Western style micro-computers were another matter.- The fact that Karpi\u0144ski spent a lot of time in the West and knew people and understood English and was not a party member makes it shocking he even got to spin up the enterprise in the first place. Had he not won the UNESCO award, he probably wouldn't have even made it as far as he did.- Getting banning from your vocation and getting a visa was unfortunately a very common method of getting dissapeared. He probably also lost housing privilege as well - hence moving out to the country and raising pigs.\n \nreply"
    ],
    "link": "https://culture.pl/en/article/jacek-karpinski-the-computer-genius-the-communists-couldnt-stand",
    "first_paragraph": ""
  },
  {
    "title": "Reverse Engineering for Everyone (0xinfection.github.io)",
    "points": 378,
    "submitter": "udev4096",
    "submit_time": "2024-07-25T14:41:26",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=41069256",
    "comments": [
      "I'd like to add that reverse engineering can also be done without any peeking at the thing you're trying to reverse-engineer.Andrew Tridgell explaining how he reverse engineered Microsoft's SMB protocol with the \"French cafe technique\": https://www.samba.org/ftp/tridge/misc/french_cafe.txtTridge also reverse engineered BitKeeper, the proprietary software that Linus foolishly used to host Linux kernel development for a while. He noticed that if you telnet to the BitKeeper address:port rather than use its proprietary client, you can type \"help\" and it then spits out a list of commands to try...You can then interrogate the repository with these commands and get a complete understanding of all the internal data structures, without ever using the proprietary software, let alone having to disassemble it.The fact that Tridge did this reverse-engineering led BitKeeper's owner, Larry McVoy, to rescind the Linux community's use of his software, so Linus wrote git.\n \nreply",
      "An undergrad highlight for me was hearing the bitkeeper/git story from Tridge one afternoon that he happened to be in the faculty lunch room :)\n \nreply",
      "So Andrew saved us not only once, but twice!It goes to show that yes, you need someone to score, but you also need someone to make that critical pass of the ball.\n \nreply",
      "I never thought of reversing as something you pick up a book for. Everything I learned was through application from a young age.1. Learning how to use Cheat Engine to scan video game process memory and modify games.2. Learned how to read/replay packets in an MMO to try an cheat.3. Learned how to craft DLLs, hooks and inject them in processes.4. Learned how create patches for executables to solve some crackme challenges.5. Mess with real world software that requires a license key, to suddenly not require a license key (or accept any key).6. Mess with binary formats to try an reverse how game saves worked to.. you guessed it, cheat.7. Get a real job and make money with the skills and knowledge I acquired.\n \nreply",
      "Same. I learned reverse engineering by staring at CE/IDA for entirely too many hours as a kid, which means whenever someone asks me for advice on how to learn reverse engineering I don't really have any good answers :)I think in reality it's the type of thing you do just have to try and spend some time on. The OP tutorial comes across as very sparse, both trying to cover too much and also not really teaching reverse engineering skills more than most people would be able to pick up in a few hours of messing around. beginners.re in contrast is massive, but also much more in-depth and goes step-by-step; on the other hand crackmes are probably better hands on challenges to try.\n \nreply",
      "Wow, did you really have access to IDA as a kid? Even with adult money it seems expensive to me.\n \nreply",
      "Most people used a cracked old version of IDA. I actually just used the freeware version, which was ancient and didn't come with any decompiler. Which was definitely difficult, and people having access to Ghidra for free these days is definitely a lot better!\n \nreply",
      "Everyone pirated IDA as a young reverse engineer, that's just a rite of passage.\n \nreply",
      "Going straight for reverse-engineering is doable, but it's significantly harder without some engineering background, either formal or self-taught.I have an ongoing reverse-engineering project for a video game and I ended up getting in contact with a self-taught modder of the game, who doesn't know how to program. He learned more in a couple of evening Discord calls with me showing him around the reverse-engineered Ghidra project, explaining the basics of computer program engineering as we went, than he did flipping bits with Cheat Engine.He then proceeded to recreate a fairly ambitious mod that was showcased in a Youtube video 15 years ago but never released, something that was bugging him for years but was unable to recreate. I steered him throughout, but by seeing how the pieces fit together he then managed to do the same mod on the sequel (which was never done before) all by himself.Experience with engineering gives you perspective when reverse-engineering.\n \nreply",
      "I don't think this is true, or at least I'm not convinced by a single anecdote. The majority of good reverse engineers I know picked up reverse engineering first and programming second (and a lot of them are still frankly not great programmers), and likewise I know plenty of good programmers who would be completely lost reverse engineering. Reverse engineering is a very different skillset than programming.\n \nreply"
    ],
    "link": "https://0xinfection.github.io/reversing/",
    "first_paragraph": "\n\n\n\n\n\n\n\n\n\n\n    Wikipedia defines it as:\n    \n        Reverse engineering, also called backwards engineering or back engineering, is the process by which an artificial object is deconstructed to reveal its designs, architecture, code, or to extract knowledge from the object. It is similar to scientific research, the only difference being that scientific research is conducted into a natural phenomenon.\n    \n    Whew, that was quite a mouthful, wasn't it? Well, it is one of the main reasons why this tutorial set exists. To make reverse engineering\n    \n        as simple as possible.\n    \n\n\n\n    This comprehensive set of reverse engineering tutorials covers x86, x64 as well as 32-bit ARM and 64-bit architectures. If you're a newbie looking to learn reversing, or just someone looking to revise on some concepts, you're at the right place. As a beginner, these tutorials will carry you from nothing upto the mid-basics of reverse engineering, a skill that everyone within the realm of cyber-secu"
  },
  {
    "title": "Show HN: Haystack \u2013 an IDE for exploring and editing code on an infinite canvas (haystackeditor.com)",
    "points": 153,
    "submitter": "akshaysg",
    "submit_time": "2024-07-25T13:54:14",
    "num_comments": 70,
    "comments_url": "https://news.ycombinator.com/item?id=41068719",
    "comments": [
      "I'm not sure about the AI features but the concept of working with code on a canvas and having some sort of flowchart UI has been a dream of mine for some time. I mainly do data processing and draw lots of flowcharts for communicating pipelines and also for myself. Having a development UI which goes in that direction somewhat would be great for handling unwieldy codebases written by non-CS folk.\n \nreply",
      "Kind of like what BlueJ does with Java?\n \nreply",
      "Is a tool like LabView what you have in mind?https://www.ni.com/en/shop/labview.html\n \nreply",
      "Curious if the problem you're encountering is visualizing the data flow vs. the control flow?\n \nreply",
      "Congrats on the launch. I've actually been going in a totally separate direction lately, it helps with my focus and ADHD tendencies to jump around and get overwhelmed.1. One window.\n2. Minimal everything.\n3. Most importantly, no tree navigation.If I need to jump to a file, I either use recents menu in Intellij or use file markers to jump to it, means I have to remember the filemarker and have intent to go there (I'm using IdeaVim plugin for vim hotkeys). This, to me, make programming much more focused and pleasurable. This is part of my evolution from multi screen to widesreen and then back to a single screen.For large and mature codebases like you mention, I just use LLM to guide me around.\n \nreply",
      "> For large and mature codebases like you mention, I just use LLM to guide me around.How? What do you call large mature codebases? In my experience LLMs already make many mistakes and assumptions just in one file?\n \nreply",
      "continue.dev with anthropic's API key https://docs.continue.dev/how-to-use-continue#ask-questions-...\n \nreply",
      "Any idea if there's a way to use this with vim/nvim? It doesn't look like it, but wanted to double check\n \nreply",
      "Makes sense! Curious how you use LLMs to guide you around?In my own experience, I ask the LLM where a functional flow is and it does a great job of creating an entrypoint, but I frequently have to figure out the finer details myself and sometimes the LLM points to something related, but not quite what I was looking for.\n \nreply",
      "I've been wanting to try something like this for a long time! Is there any way to sign up for updates?(In particular, I'm waiting for the option to turn off the AI stuff, at least for when I am trying things out.)I especially love the ability to zoom into a function and hide the rest of a file. I've wanted that sort of thing a surprising number of times. I'm thinking one could also use this to virtually rearrange a single file by pulling the functions in that file that you are working on next to each other easily?In addition to the editing interface, I think there are probably a lot of visualization opportunities in editors:- what's the interaction between my code and third-party dependencies?\n- what are the dependencies/call graph of my code? or perhaps just a portion of it?\n- call graphs can be nested \u2014 perhaps you care for a dependency graph at the class or module level, or perhaps you want to \"drill down\" into the call graph inside an objectIn all these cases, being able to seamlessly transition between the visualization and editing \u2014 like you seem to be aiming towards \u2014 seems like the killer feature.And, of course, there are lot of other interesting visualization opportunities once you have any sort of graphics in your editor:- heatmaps of code churn\n- heatmaps of code performance\n- tracing variable usage\n- and, of course, lots more (https://adamtornhill.com/articles/crimescene/codeascrimescen...)One last thought: if you haven't read _The Programmer's Brain_, you should at least listen to this podcast. https://se-radio.net/2021/06/episode-462-felienne-on-the-pro... There are a bunch of editing ideas related to how our brain works that I haven't seen well supported in an editor yet. You took one step in that direction \u2014 maybe there are some more opportunities?Looking forward to what you come up with!\n \nreply"
    ],
    "link": "https://haystackeditor.com/",
    "first_paragraph": "HomeDemoFAQsDownloadDownloadCan I keep my settings from VS Code?Does Haystack log or send my code anywhere?How do I navigate more easily inside of editors?I don't see the extension I want in the sidebarWhat languages do you support?Why does the Python language server not work?What platforms do you support?"
  },
  {
    "title": "Launch HN: Undermind (YC S24) \u2013 AI agent for discovering scientific papers",
    "points": 130,
    "submitter": "jramette",
    "submit_time": "2024-07-25T15:36:57",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=41069909",
    "comments": [
      "I'm a CS academic who _should_ be working on finalizing a new submission, so when I saw this on HN I decided to give it a try and see if it could find anything in the literature that I'd missed. Somewhat to my surprise, it did - the top 10 results contained two items that I really ought to have found myself (they're from my own community!), but that I'd missed. There were also some irrelevant results mixed in (and lots of things I was already aware of), but overall I'm very impressed with this and will try it out again in the future. Nice work :)\n \nreply",
      "Very cool, and very relevant to my life -- I am currently writing a meta-analysis and finishing my literature search.I gave it a version of my question, it asked me reasonable follow-ups, and we refined the search to:> I want to find randomized controlled trials published by December 2023, investigating interventions to reduce consumption of meat and animal products with control groups receiving no treatment, measuring direct consumption (self-reported outcomes are acceptable), with at least 25 subjects in treatment and control groups (or at least 10 clusters for cluster-assigned studies), and with outcomes measured at least one day after treatment begins.I just got the results back: https://www.undermind.ai/query_app/display_one_search/e5d964....It certainly didn't find everything in my dataset, but:* the first result is in the dataset.* The second one is a study I excluded for something buried deep in the text.* The third is in our dataset.* The fourth is excluded for something the machine should have caught (32 subjects in total), but perhaps I needed to clarify 25 subjects in treatment and control each.* The fifth result is a protocol for the study in result 3, so a more sophisticated search would have identified that these were related.* The sixth study was entirely new to me, and though it didn't qualify because of the way the control group received some aspect of treatment, it's still something that my existing search processes missed, so right away I see real value.So, overall, I am impressed, and I can easily imagine my lab paying for this. It would have to advance substantially before it was my only search method for a meta-analysis -- it seems to have missed a lot of the gray literature, particularly those studies published on animal advocacy websites -- but that's a much higher bar than I need for it to be part of my research toolkit.\n \nreply",
      "For a meta-analysis, you might want to try the \"extend\" feature. It sends the agent to gather more papers (we only analyze 100 carefully initially), so if your report might say \"only 55% discovered\", could be useful.(Also, if you want, you can share your report URL here, others will be able to take a look.)\n \nreply",
      "Thanks, I added my URL\n \nreply",
      "Awesome! I just took you up on your offer and compared roughly similar questions using Claude 3.5 Sonnet and Undermind.Claude 3.5 is reluctant to provide references\u2014-although it will if coaxed by prompting.Undermind solves this particular problem. A great complement for my research question \u2014- the evidence that brain volume is reduced as a function of age in healthy cognitively normal humans. In mice we see a steady slow increase that averages out to a gain of 5% between the human equivalents of 20 to 65 years of age. This increase is almost linear as a function of log of age.Here is the question that was refined with Undermind\u2019s help:>I want to find studies on adult humans (ages 20-100+) that have used true longitudinal repeated measures designs to study variations in brain volume over several years, focusing on individuals who are relatively healthy and cognitively functional.I received 100 ranked and lightly annotated set of 100 citations in this format:>[1] Characterization of Brain Volume Changes in Aging Individuals With Normal Cognition Using Serial Magnetic Resonance Imaging\nS. Fujita, ..., and O. Abe\nJAMA Network Open\n2023 - 21 citations - Show abstract -  Cite - PDF\n99.6% topic match\nProvides longitudinal data on brain volume changes in aging individuals with normal cognition.\nAnalyzes annual MRI data from 653 adults over 10 years to observe brain volume trajectories.\nExcludes populations with neurodegenerative diseases; employs true longitudinal design with robust MRI techniques.\n \nreply",
      "It's worth highlighting that first result is exactly what you asked for, given all 4 of your criteria:1. It's on adults.2. It's longitudinal over multiple years.3. It studies variations in brain volume.4. It focuses on healthy individuals.You can see the full results for that search text here: https://undermind.ai/query_app/display_one_search/e1a3805d35...\n \nreply",
      "And that first hit in JAMA Open is a fabulous paper. Ten or mire yearly MRI scans for 650 subjects.\n \nreply",
      "Ref to prior art: https://en.wikipedia.org/wiki/Meta_(academic_company)One anecdote that I heard from the team developing it: turned out that researchers more readily sourced material from their social networks, notably twitter at the time. Meta's search functionality didn't receive enough traffic and eventually was shut down.Perhaps LLMs will make the search capability more compelling. I guess we'll see.\n \nreply",
      "I have only tried one search, but so far it's impressive. I have been using elicit.com, but they seem to be taking a different approach that is less AI-heavy. I would definitely give this a shot for a few months.\n \nreply",
      "We're trying to bias the system toward more autonomous execution, rather than a \"copilot\"-like experience where you iterate back and forth with the system. That lets us run more useful subroutines in parallel in the backend, as long as you specified your complex goal clearly.\n \nreply"
    ],
    "link": "item?id=41069909",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Wat \u2013 Deep inspection of Python objects (github.com/igrek51)",
    "points": 136,
    "submitter": "igrek51",
    "submit_time": "2024-07-25T16:18:34",
    "num_comments": 49,
    "comments_url": "https://news.ycombinator.com/item?id=41070428",
    "comments": [
      "Waaat :D. This is so nice. I used to use python-ls[0] for similar, but something about it that I can't recall broke for me and it's no longer maintained. Adding to my debugging arsenal which primarily consists of snoop[1] and pdbpp. Only thing I'd like for wat now is maybe an ipy widget for even easier object exploration in Jupyter.I'm also really appreciating the base64 exec hack. All my years in Python I never thought of or came across it until now. I'll totally be using it for some things :).[0] https://github.com/gabrielcnr/python-ls\n[1] https://pypi.org/project/snoop/\n \nreply",
      "Ah this looks fun! I use \"dir\" all the time with python, and find it more useful than official documentation in some cases where documentation is not great.Surprised there isn't more innovation and new tools like this around python's interactive shell given it's one of the real strong points the language has.\n \nreply",
      "There\u2019s the help() function as well! Super helpful!\n \nreply",
      "Looks like a fancy version of good old icecream.\nhttps://github.com/gruns/icecreamIf you never heard about it, scroll to the bottom to \nhttps://github.com/gruns/icecream#icecream-in-other-language...\n \nreply",
      "Neat. I built something similar but web-based for Java years ago: https://scg.unibe.ch/wiki/projects/DoodleDebug\n \nreply",
      "> from wat import watGiven the cool nature of this project, I'm surprised they don't offer simply \"import wat\" with identical usage syntax.  Thus inviting curious users to wat/wat in order to discover the trick...\n \nreply",
      "\"import wat\" would be great, but Python has some restrictions about modules not being callable. That's why I ended up with longer `from wat import wat`. Not sure but maybe this would be more convenient `import wat; wat.wat / object`\n \nreply",
      "Python has code execution at import time. Just grab the global context, and overwrite the module with a callable.\n \nreply",
      "Afaik the import sets the module in the importing module's context only after the code in the imported module is run.Edit: Oh, you don't mean in the importing module's globals, you mean `sys.modules`. Yeah that works!\n \nreply",
      "actually, no you were right about what I meant but i'll claim credit for helping you realize there was a better way XD\n \nreply"
    ],
    "link": "https://github.com/igrek51/wat",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Deep inspection of Python objects\n      Deep inspection of Python objects.WAT is a powerful inspection tool\nthat allows you to delve into and examine unknown objects at runtime.\"Wat\" is a variant of the English word \"what\" that is often used to express confusion or disgustIf you find yourself deep within the Python console, feeling dazed and confused,\nwondering \"WAT? What's that thing?\",\nthat's where the wat inspector comes in handy.Start the Python Interpreter (or attach to one) and execute wat / object on any object\nto investigate its\ntype, formatted value, variables, methods, parent types, signature,\ndocumentation, and even its source code.\nAlternatively, you can use wat(object) syntax.If you want to debug something quickly without rerunning,\nyou don't even need to install anything to use this inspector.Load it on the fly by past"
  },
  {
    "title": "Real-Time Procedural Generation with GPU Work Graphs [pdf] (gpuopen.com)",
    "points": 36,
    "submitter": "ibobev",
    "submit_time": "2024-07-22T14:49:36",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=41035084",
    "comments": [
      "Related: https://gpuopen.com/learn/work_graphs_mesh_nodes/work_graphs...\n \nreply",
      "GPUs are Turing complete computers. Why is it necessary to add a GPU programming design pattern like this as an extension to Vulkan? Can I just implement this design pattern in an existing GPU programming language right now, without waiting for Vulkan extensions to be implemented for all the relevant GPU models? If this isn't possible right now, then what's missing from existing GPU languages that prevents this kind of flexibility?\n \nreply",
      "They are Turing complete in the context of a single thread. But, a single thread does not control the entire GPU. For example: In multi-vendor APIs, a thread cannot spawn another thread.The execution model of GPUs is complex and only specified at the high level in order to give manufacturers freedom in their implementations. Work Graphs are an extension of the execution model.\n \nreply",
      "Because every GPU on the market has a very different internal organization and instruction set.They are able to execute any shader using standardized programming constructs (Vulkan, DirectX, Metal) that both the OS and driver understand. While the API and OS manage device/app context, the driver manages the device itself and the job of compiling shaders or standard calls to their GPUs specific instructions or layout.\n \nreply",
      "I\u2019m wondering the same. Maybe it\u2019d be easier if we could upload our own \u2018sequential program\u2019 to a little CPU inside the GPU that would be code making calls to the real GPU. This way delays to native GPU code would be minimized and we wouldn\u2019t need to mimick creating new features / GPU paradigms\n \nreply",
      "Work graphs are in Vulkan (pg. 3)?! Since when? Where can I learn about this? Excitement\n \nreply",
      "IIRC they're an AMD extension (VK_AMDX_shader_enqueue) that only works with a specific beta driver currently.I wouldn't really say they're usable in vulkan at this point.\n \nreply",
      "I think Vulkan support is still AMD-only.https://gpuopen.com/gpu-work-graphs-in-vulkan/\n \nreply",
      "Does this work on Metal\n \nreply"
    ],
    "link": "https://gpuopen.com/download/publications/Real-Time_Procedural_Generation_with_GPU_Work_Graphs-GPUOpen_preprint.pdf",
    "first_paragraph": ""
  },
  {
    "title": "A Clone of Deluxe Paint II Written in Python (github.com/mriale)",
    "points": 69,
    "submitter": "luismedel",
    "submit_time": "2024-07-25T20:53:50",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=41073264",
    "comments": [
      "It's funny to see this presented as a retro \"pixel art\" program. I mean I suppose it is given that it is based on a program nearly 40 years old, but the example image that is always shown with Deluxe Paint (and on this clone version's page too) is the famous picture of King Tut's mask. When I first saw that picture back in the day it was the first example of computer graphics that I had to look at twice to determine that it wasn't a photo -- the resolution, and the colors made it seem so real at first.\n \nreply",
      "If you want to see what a master of Deluxe Paint palette cycling can do check out Mark Ferrari'shttp://www.effectgames.com/demos/canvascycle/ (hit \"Show Options)andhttps://www.youtube.com/watch?v=aMcJ1Jvtef0\n \nreply",
      "The art is great on its own, but knowing the constraints it runs under and how one image can look so different, or even animated, based solely on the palette, elevates the art to a whole new level.\n \nreply",
      "About 2-3 years ago, I was writing a clone of Deluxe Paint in C/Raylib (referencing the released source code). I got about 50% done and lost my incentive.It's a big program with a lot of features and caveats, so seeing a completed version is awesome. Congrats.\n \nreply",
      "I didn't know the code had been released!I had to look up how they did the palleted color blending and found...https://github.com/historicalsource/DeluxePaint/blob/8493bb3...\n \nreply",
      "I'd love to know how long the original artist took to paint that image!I imagine many hours of squinting at a CRT screen, with absolutely nothing better to do. No internet, no mobile phones, apps, social medias, zit.\n \nreply",
      "Also shoutout to DPaint.js, which runs in the Browser: https://github.com/steffest/DPaint-js/(Though I will say that Aseprite has taken over as my preferred Pixel Art editor.)\n \nreply",
      "Deluxe Paint!Fond memories as I found it coming along with some game I got way back in the 90s for my Amiga 600.Those were good times~\n \nreply",
      "This is a very accurate recreation, impressive.Be sure to try the color cycle mode by hitting tab.\n \nreply",
      "Thanks! I shall use this to create pictures of hot dogs :)\n \nreply"
    ],
    "link": "https://github.com/mriale/PyDPainter",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A usable pixel art paint program written in Python\n      A usable pixel art program written in PythonPyDPainter, pronounced \"Pied Painter\" (like Pied Piper), is an attempt to create a usable pixel art program in Python using PyGame.\nThe original inspiration came from the Commodore Amiga\nversion of Deluxe Paint released by Electronic Arts in 1985.\nBack then, Deluxe Paint helped define the user interface of a paint program with tool bars, menus, and the\nnovel use of left and right mouse buttons for painting and erasing. After pixel art gave way to photo-realism and\nhigh-resolution 24 bit color, Deluxe Paint was largely forgotten for artistic work -- left behind\nin the ever-progressing march of technology.Recently, with a resurgence of all things \"retro,\" low-resolution pixel art and limited color palettes\nhave become popular once agai"
  },
  {
    "title": "Applied Machine Learning for Tabular Data (aml4td.org)",
    "points": 24,
    "submitter": "sebg",
    "submit_time": "2024-07-25T19:48:35",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://aml4td.org/",
    "first_paragraph": "Max Kuhn   Kjell Johnson  2024-06-17Welcome! This is a work in progress. We want to create a practical guide to developing quality predictive models from tabular data. We\u2019ll publish materials here as we create them and welcome community contributions in the form of discussions, suggestions, and edits.We also want these materials to be reusable and open. The sources are in the source GitHub repository with a Creative Commons license attached (see below).Our intention is to write these materials and, when we feel we\u2019re done, pick a publishing partner to produce a print version.The book takes a holistic view of the predictive modeling process and focuses on a few areas that are usually left out of similar works. For example, the effectiveness of the model can be driven by how the predictors are represented. Because of this, we tightly couple feature engineering methods with machine learning models. Also, quite a lot of work happens after we have determined our best model and created the f"
  },
  {
    "title": "My Favorite Algorithm: Linear Time Median Finding (2018) (rcoh.me)",
    "points": 224,
    "submitter": "skanderbm",
    "submit_time": "2024-07-25T09:16:53",
    "num_comments": 113,
    "comments_url": "https://news.ycombinator.com/item?id=41066536",
    "comments": [
      "10-15 years ago, I found myself needing to regularly find the median of many billions of values, each parsed out of a multi-kilobyte log entry. MapReduce was what we were using for processing large amounts of data at the time. With MapReduce over that much data, you don't just want linear time, but ideally single pass, distributed across machines. Subsequent passes over much smaller amounts of data are fine.It was a struggle until I figured out that knowledge of the precision and range of our data helped. These were timings, expressed in integer milliseconds. So they were non-negative, and I knew the 90th percentile was well under a second.As the article mentions, finding a median typically involves something akin to sorting. With the above knowledge, bucket sort becomes available, with a slight tweak in my case. Even if the samples were floating point, the same approach could be used as long as an integer (or even fixed point) approximation that is very close to the true median is good enough, again assuming a known, relatively small range.The idea is to build a dictionary where the keys are the timings in integer milliseconds and the values are a count of the keys' appearance in the data, i.e., a histogram of timings. The maximum timing isn't known, so to ensure the size of the dictionary doesn't get out of control, use the knowledge that the 90th percentile is well under a second and count everything over, say, 999ms in the 999ms bin. Then the dictionary will be limited to 2000 integers (keys in the range 0-999 and corresponding values) - this is the part that is different from an ordinary bucket sort. All of that is trivial to do in a single pass, even when distributed with MapReduce. Then it's easy to get the median from that dictionary / histogram.\n \nreply",
      "Did you actually need to find the true median of billions of values? Or would finding a value between 49.9% and 50.1% suffice? Because the latter is much easier: sample 10,000 elements uniformly at random and take their median.(I made the number 10,000 up, but you could do some statistics to figure out how many samples would be needed for a given level of confidence, and I don't think it would be prohibitively large.)\n \nreply",
      "The kind of margin you indicate would have been plenty for our use cases. But, we were already processing all these log entries for multiple other purposes in a single pass (not one pass per thing computed). With this single pass approach, the median calculation could happen with the same single-pass parsing of the logs (they were JSON and that parsing was most of our cost), roughly for free.Uniform sampling also wasn't obviously simple, at least to me. There were thousands of log files involved, coming from hundreds of computers. Any single log file only had timings from a single computer. What kind of bias would be introduced by different approaches to distributing those log files to a cluster for the median calculation? Once the solution outlined in the previous comment was identified, that seemed simpler that trying to understand if we were talking about 49-51% or 40-50%. And if it was too big a margin, restructuring our infra to allow different log file distribution algorithms would have been far more complicated.\n \nreply",
      "> the latter is much easier: sample 10,000 elements uniformly at random and take their medianDo you have a source for that claim?I don't see how could that possibly be true... For example, if your original points are sampled from two gaussians of centers -100 and 100, of small but slightly different variance, then the true median can be anywhere between  the two centers, and you may need a humungous number of samples to get anywhere close to it.True, in that case any point between say -90 and 90 would be equally good as a median in most applications.  But this does not mean that the median can be found accurately by your method.\n \nreply",
      "> this does not mean that the median can be found accurately by your method.You can do dynamic sampling: e.g. take double the samples, see what decimal in your result budges. Adjust.\n \nreply",
      "the key word is \u201cuniformly\u201d. If your data is not uniformly distributed, then you just have to pick random values non-uniformly. There are many ways to do that, and once you have your algo you\u2019ll be able to reliably find an approximation of the median much faster than you would find the actual median.https://en.m.wikipedia.org/wiki/Non-uniform_random_variate_g...\n \nreply",
      "I was thinking the same thing.In all use-cases I've seen a close estimate of the median was enough.\n \nreply",
      "I\u2019m not sure why you use a dictionary with keys 0\u2026999, instead of an array indexed 0\u2026999.\n \nreply",
      "That's just a dict/map with less flexibility on the keys :D\n \nreply",
      "Around 4 years ago I compared lots of different median algorithms and the article turned out to be much longer than I anticipated :)https://danlark.org/2020/11/11/miniselect-practical-and-gene...\n \nreply"
    ],
    "link": "https://rcoh.me/posts/linear-time-median-finding/",
    "first_paragraph": "\n          POSTS\n        Finding the median in a list seems like a trivial problem, but doing so in linear time turns out to be tricky. In this post I\u2019m going to walk through one of my favorite algorithms, the median-of-medians approach to find the median of a list in deterministic linear time. Although proving that this algorithm runs in linear time is a bit tricky, this post is targeted at readers with only a basic level of algorithmic analysis.The most straightforward way to find the median is to sort the list and just pick the median by its index. The fastest comparison-based sort is \\(O(n \\log n)\\), so that dominates the runtime.12Although this method offers the simplest code, it\u2019s certainly not the fastest.Our next step will be to usually find the median within linear time, assuming we don\u2019t get unlucky. This algorithm, called \u201cquickselect\u201d, was devevloped by Tony Hoare who also invented the similarly-named quicksort. It\u2019s a recursive algorithm that can find any element (not just"
  },
  {
    "title": "Defense of Lisp macros: The automotive field as a case in point (mihaiolteanu.me)",
    "points": 128,
    "submitter": "molteanu",
    "submit_time": "2024-07-25T09:18:13",
    "num_comments": 131,
    "comments_url": "https://news.ycombinator.com/item?id=41066544",
    "comments": [
      "I was a bit baffled the first time I was introduced to Simulink Coder. I get the value proposition: simulate your model so that you have confidence it's doing the right thing, and then essentially just run that model as code instead of implementing it yourself and possibly introducing mistakes. What I didn't understand, as a software guy, not an engineering guy, was why on earth you'd want a graphical programming language to do that. Surely it would be easier to just write your model in a traditional text-based language. Hell, the same company even makes their own language (MATLAB) that would be perfect for the task.I did a little digging, and it turns out I had it backwards. It's not that Simulink invented a new graphical programming language to do this stuff. Control systems engineers have been studying and documenting systems using Simulink-style block diagrams since at least the 1960s. Simulink just took something that used to be a paper and chalkboard modeling tool and made it executable on a computer.\n \nreply",
      "Bingo.  Additionally, control theory block diagrams are pretty rigorous, with algebraic rules for manipulating them you'll find in a controls textbook.  You can pretty much enter a block diagram from Apollo documentation into Simulink and \"run\" it, in much the same way you could run a spice simulation on a schematic from that era.\n \nreply",
      "Yeah, there's a really interesting phenomenon that I've observed that goes with that. If what you're describing was the main way people used it, I'd be very satisfied with that and have actually been chewing on ways to potentially bring those kinds of concepts into the more \"mainstream\" programming world (I'm not going to go way off on that tangent right now)What I've seen many times though in my career that awkwardly spans EE and CS is that people forget that you can still... write equations instead of doing everything in Simulink. As an example I was looking at a simplified model of an aircraft a couple months ago.One of the things it needed to compute was the density of air at a given altitude. This is a somewhat complicated equation that is a function of altitude, temperature (which can be modelled as a function of altitude), and some other stuff. These are, despite being a bit complicated, straightforward equations. The person who had made this model didn't write the equations, though, they instead made a Simulink sub-model for it with a bunch of addition, multiplication, and division blocks in a ratsnest.I think the Simulink approach should be used when it brings clarity to the problem, and should be abandoned when it obscures the problem. In this case it took a ton of mental energy to reassemble all of the blocks back into a simple closed-form equation and then further re-arranging to verify that it matched the textbook equation.\n \nreply",
      "I had an intern do something similar with an electromechanical machine.  I'd all but written the equations out for him but he found drawing it out in Simulink in the most fundamental, primitive blocks to help him understand what was going on . I don't get it either.A related phenomenon seems to be people who don't want to \"wire up\" schematics but  attach named nets to every component and then draw boxes around them.\n \nreply",
      "> people who don't want to \"wire up\" schematics but attach named nets to every component and then draw boxes around themLol there are a few things that I am highly intolerant of and this is one of them. The only place where I'm generally ok with that approach is at the input connectors, the microcontroller, and the output connectors. Same philosophy as I said before though: \"if it brings clarity draw it as a wired up schematic; if it turns into a ratsnest use net labels to jump somewhere else\". Having every component in a separate box with net labels will generally obscure what's going on and just turns it into an easter egg hunt.\n \nreply",
      "Also, part of the style seems to be to contort any actual schematic to fit in the box.  I like to ask \"if you wouldn't put it on its own page so you could draw it clearly, why did you draw a box around it?\".\n \nreply",
      "I don't really get this point of view. Take an I2C bus, for example. Isn't it easier to read the schematic if all the components on the bus have two pins connected to wires labeled 'SDA' and 'SCL'?\n \nreply",
      "Yeah, there\u2019s a balance here. There are common nets like SDA, SCL, GND, VCC that make sense to have as global nets. And then there\u2019s actual circuits where it makes more sense (for understandability) to not do it like that.Here\u2019s an example that I think takes it too far: https://cdn.sparkfun.com/datasheets/Dev/LilyPad/LilyPad-MP3-...- The audio jack (line level) could have just sat to the right of the codec and been directly connected to it to make it clear that it\u2019s just a direct output- the speaker amplifier could be directly connected to the audio jack as well. I spent a lot of time looking around to figure out where it actually connects to; I figured it would have its input connected to the MP3 codec and didn\u2019t realize that it would be connected through the micro switches inside the headphone jack (which makes sense now that I see it)- the SD card slot could either be connected directly to the codec or could have better names on the nets. When I see MOSI, MISO, and SCK I assume that they\u2019re connected to the main MCU. Even more confusing, there\u2019s an SD-CS that is connected to the MCU but not the rest of the SPI pinsOverall I\u2019d probably give this one a B. It\u2019s by no means the most egregious I\u2019ve seen but it also obscures the signal flow pretty badly.Other things I would probably consider doing:- better clustering of pins on the MCU by function. The switch inputs and LED outputs that connect to the rotary encoder are all over the place. Most tools allow you to move the pins on microcontrollers around at will- the FTDI block could move onto the first page. The DTR pin could use a bit of explanation that it\u2019s tied to the reset circuit. Not their fault in this one but the naming inconsistency between \nFTDI-RXI and TRIG4/TXO is confusing until you figure out that the pin is meant to be either a GPIO or a UART pin. The name doesn\u2019t make it clear that it goes to the FTDI chip because it\u2019s inconsistent with the rest of the FTDI-xxx names but that\u2019s because most ECAD programs handle net names poorly anywayEdit: one more thing that really bugs me about this schematic: the Vbatt net leaving J1. It\u2019s labelled but not with any kind of flag or loose/unterminated line that would indicate that it\u2019s connected to the net leaving U2. And further with that, the fact that VIN on U2 comes only from the FTDI connector on the second page (I think\u2026)\n \nreply",
      "I think using the drawing it out process to understand initially is ... not how my brain would do it, but seems like a perfectly valid approach.But once you -do- understand it's time to stick that in a reference file out of the way and write it again properly for actual use.\n \nreply",
      "One cool thing about, for instance, density altitude calculations or runway length calculations, is that you can break the parts of the algorithm down graphically do pilots can trace through datapoints to get an answer without even having a calculator. See many pilot operating handbooks for examples.\n \nreply"
    ],
    "link": "https://mihaiolteanu.me/defense-of-lisp-macros",
    "first_paragraph": ""
  },
  {
    "title": "Charge Robotics (YC S21) is hiring MechEs to build robots that build solar farms (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-07-25T22:12:57",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/charge-robotics/jobs/ml4f9l4-senior-mechanical-engineer",
    "first_paragraph": "Robots that build solar farmsCharge Robotics is a Series A startup building robots that build solar farms.Demand for new solar projects is booming (1/5th of all the solar that exists in the US was installed last year!), but today\u2019s construction companies can\u2019t keep up due to limited labor resources.We thought this was insane, so we started working on robots to directly address this bottleneck and speed up the world\u2019s transition to renewables.Charge is a fast-moving company which means constant opportunities for learning and growth. You\u2019ll have a large impact on the direction of our company and our product, which will be reflected in significant equity compensation. And you get to work with \ud83e\udd16 giant robots \ud83e\udd16.If you are excited to work on interesting technical problems with direct climate impact, you\u2019re going to fit right in at Charge Robotics.Read more about Charge in recent press:Charge\u2019s funding:We\u2019re MIT-founded and backed by some of Silicon Valley\u2019s top investors, including Lux, YC ("
  },
  {
    "title": "Generating sudokus for fun and no profit (tn1ck.com)",
    "points": 208,
    "submitter": "todsacerdoti",
    "submit_time": "2024-07-24T21:02:09",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=41062072",
    "comments": [
      "Answering the object-level problem \"my grandma wanted to play some sudokus on her computer\", I really enjoyed the Cracking the Cryptic-affiliated game \"Classic Sudoku\", which is available on Steam, although some of the puzzles are really hard. The puzzles are all handmade, and many of them have some specific reason to exist: e.g. there's at least one which is clearly intended to teach you the swordfish pattern, and there are a few which appear to be built around some beautiful one-off ideas.\n \nreply",
      "Cracking the cryptic is fantastic. The beauty in the setting of some of their showcased puzzles is truly wonderful to watch get solved.\n \nreply",
      "And it is such a fantastic resource to learn! You want to get into puzzles like they do on the channel? Just open a video, pause it, and click the link in the description. Stare at the puzzle, trying to figure it out. The first time you do this, you probably won't. Once you lose patience (but give yourself some time!), watch the video until Simon or Mark get to the first deduction and explain it. Pause the video again, and try to continue using your new knowledge. When you get stuck again, what (or skim) through the video until they do enter a deduction that you have not found. Rinse and repeat.Do this a couple of times with different videos, and you will start to build a repertoire of techniques yourself. At some point, you will be capable of solving puzzles on your own. (And if you get stuck, the video is there to help you.)\n \nreply",
      "Normally I get a bit tired or skip ahead when technical topics or gameplay centered strategies are explained over and over again on the same channel, but I can watch them prove the Phistomefel ring or break down the logic on disqualifying candidates with their shaders every time.\n \nreply",
      "Very reminiscent of agadmator and his chess videos, but without the app\n \nreply",
      "I love the feeling of finding the beauty of how a puzzle resolves. The hand set puzzles are as much a challenge to player as they are a demonstration of the setter's own skill and cleverness.\n \nreply",
      "Cracking The Cryptic also has a \"Genuinely Approachable Sudoku\" community (Discord is the only public feed I'm aware of), where a handmade puzzle using all sorts of Sudoku variants is posted daily.\n \nreply",
      "There\u2019s also the self-titled \u201cCracking the Cryptic\u201d app that has a bunch of handmade Sudoku & friends puzzles that range in difficulty from pretty tricky up to fiendish.https://crackingthecryptic.com/#apps\n \nreply",
      "Hi! I'm the author, didn't expect this to be posted here yet. I was still somewhat working on it, so please bear with me when you find anything weird. You can give me any feedback here.\n \nreply",
      "Such a beautifully written blog! I have no comments so far, but I will read it through tonight again.I wrote a Sudoku solver using a SAT solver compiled to wasm (it is just a simple exercise TBH):https://www.nhatcher.com/hats/sudoku.html\nhttps://www.nhatcher.com/post/on-hats-and-sats/\n \nreply"
    ],
    "link": "https://tn1ck.com/blog/how-to-generate-sudokus",
    "first_paragraph": ""
  },
  {
    "title": "Memory Mapping an FPGA from an STM32 (serd.es)",
    "points": 99,
    "submitter": "hasheddan",
    "submit_time": "2024-07-25T14:21:36",
    "num_comments": 43,
    "comments_url": "https://news.ycombinator.com/item?id=41068995",
    "comments": [
      "I recommend checking out SpinalHDL generally - I do a ton of this very same kind of work with these same chips (7 series, US+) and would never look back to Verilog!AXI (and all memory-mapped bus protocol schemes) becomes very very pleasant. SV interfaces get you 5% of the way there, though!Also - I was under the impression that S1000-2M is a higher-end material, not cost-optimized? (But not Rogers, of course.)\n \nreply",
      "S1000-2 is quite cheap and lossy (Df 0.016), slightly better than Isola 370HR (0.021) but nowhere near the stuff I usually use. At my usual Chinese board house it's one of the lowest cost substrates available for prototypes since it's always in stock and there's no need to special order.For higher end digital work I typically reach for Taiwan Union TU872SLK (Df 0.009) which also has a better range of prepregs and glass styles available to help minimize fiber weave effect. Still quite a bit lossier than e.g. RO4350B but far less expensive and if you have decent equalizers on your SERDES the difference is typically not significant unless you're making some kind of humongous backplane. I get wide open eyes with just a tiny bit of post-cursor emphasis on the TX FFE at 10.3125 Gbps on TU872SLK for my typical shortish high speed tracks (FPGA to SFP+ cage).\n \nreply",
      "Also S1000-2 is not rated/controlled past 1GHz.  It shouldn't vary that much so for small runs the risk is minimal.  But for volume production that's exactly the sort of thing you never want to have to investigate in hindsight.\n \nreply",
      "Curious who you are using in CN for higher-speed FPGA boards, if you can share!I haven't seen these as directly-advertised options at any of my usual suspects.\n \nreply",
      "Multech (multech-pcb.com) is my preferred manufacturer these days for high end stuff. I've done six layer HDI any-layer via stackups, ten layers with filled via-in-pad, RO4350B, TU872SLK, flex, 75 micron trace/space, etc. And that's nowhere near the limit of their capabilities, I just haven't needed higher end yet.I have some 25/100G stuff in the pipe for probably some time next year that I plan to make with them too.Their website undersells, I get the impression most of the actual sales contacts are word of mouth. I talk to my sales rep by skype mostly (the alternatives are expensive international phone calls or wechat).The really cool thing is that you get a 10+ page QA report with every order including measured copper/dielectric/soldermask thicknesses, hole sizes, ionic contamination measurements, and a ton of other metrics. And they send the TDR strips and polished cross section with every order as their way of saying \"look, we actually did the QA, double check our measurements if you don't trust us\". (I actually have repeated some of the measurements to spot-check and got results within a few percent of their QA department, no surprises there).And they don't make silent gerber changes or anything. They do a full CAM review and send you working gerbers and a list of suggested DFM tweaks for you to sign off before beginning manufacture. If something doesn't look right you have a chance to say \"wait there's a problem\".For example, one time they wanted to make a really large width adjustment for impedance on some RF traces that I had carefully modeled in an EM solver. But they didn't make a bad board without telling me, they flagged it on the CAM review and we went back and forth before realizing the mistake was on their end (they had calculated impedance assuming solder mask over the traces, while they were actually exposed copper). They re-ran the numbers which then closely matched my simulations, I signed off on the modified design, and the board was manufactured without issue.\n \nreply",
      "Be veeeery careful. STM32H QSPI peripheral is FULL OF very nasty bugs, especially the second version (supports writes) that you find in STM32H0B chips . You are currently avoiding them by having QSPI mapped as device memory, but the minute you attempt to use it with cache or run code from it, or (god help you) put your stack, heap, and/or vector table on a QSPI device, you are in for a world of poorly-debuggable 1:1,000,000 failures. STM knows but refuses to publicly acknowledge, even if they privately admit some other customers have \"hit similar issues\". Issues I've found, demonstrated to them, and wrote reliable replications of:* non-4-byte-sized writes randomly lost about 1/million writes if QSPI is writeable and not cached* non-4-byte-sized writes randomly rounded up in size to 2 or 4 bytes with garbage, overwriting nearby data about 1/million writes if QSPI is writeable and cached* when PC, SP, and VTOR all point to QSPI memory, any interrupt has about a 1/million chance of reading garbage instead of the proper vector from the vector table if it interrupts a LDM/STM instruction targeting the QSPI memory and it is cached and misses the cacheSome of these have workarounds that I found (contact me). I am refusing to disclose them to STM until they acknowledge the bugs publicly.I recommend NOT using STM32H7 chips in any product where you want QSPI memory to work properly.\n \nreply",
      "What the hell is going on at ST? Every STM uC I've tried to use in the past few years has had showstopper bugs with loads of very similar complaints online dating back to the release of the part. Bugs that have been in the wild for years and still exist in the current production run.After burning enough company time chasing bugs through ST's crappy silicon, I've had to just swear them off entirely. We're an Atmel house now. Significantly fewer (zero) problems, and some pretty nifty features like UPDI.\n \nreply",
      "It seems endemic with embedded devices. Only big customers get the true list of errata, and of course the errata are random PDFs rather than a useful format. Even just having them on an ftp site with all the errata in one spot would save so much pain!\n \nreply",
      "In college, our SoC design instructor told us that to pass the class, our modules should be better than ST's \"which is not that high of a bar\" :P\n \nreply",
      "They churn out new parts and don't bring in fixes. See all the chips in their lineup that have a USB host controller. Every one of them (they use Synopsys IP) will fail with multiple LS devices through a hub. \nWe talked to our FAE about this and they have no plans to fix it. The bug has existed for years and the bad IP is being baked into all the new chips still.\nSolution? Just use yet another chip for its host controller, and don't use a hub.\n \nreply"
    ],
    "link": "https://serd.es/2024/07/24/Memory-mapping-an-FPGA-from-a-STM32.html",
    "first_paragraph": "I teased at this a bit in my previous posts and finally have a setup I\u2019m happy with, so I thought I\u2019d do a more\nin-depth writeup.To recap, the planned architecture for most of my future large-scale embedded projects is a fairly large (AMD Xilinx\nKintex-7 or Artix / Kintex UltraScale+) FPGA for the high speed data plane paired with a STM32H735 for the control\nplane with a memory mapped interface between them.This is somewhat reminiscent of SoC FPGAs like Xilinx\u2019s Zynq / Versal platforms, but with a few important differences\nthat make it suit my needs and preferences better:After several false starts using quad SPI, I\u2019ve settled on using the Flexible Memory Controller (FMC) as the preferred\nMCU-side bridge between the AXI on the STM32 and the FPGA\u2019s internal interconnect. This is a highly configurable module\nwhich can be used to interface to old school (PC133 etc) SDRAM, asynchronous or synchronous SRAM/PSRAM, parallel\nNOR/NAND flash, etc.Most importantly, unlike the OCTOSPI peripheral o"
  },
  {
    "title": "Mapping Hacker News to find who knows what in the HN community (blog.wilsonl.in)",
    "points": 150,
    "submitter": "robg",
    "submit_time": "2024-07-25T15:04:24",
    "num_comments": 94,
    "comments_url": "https://news.ycombinator.com/item?id=41069527",
    "comments": [
      "This implementation doesn\u2019t really get there, but\u2026I think this is an extremely cool idea both on HN specifically and generally on the Internet. Bluesky does a bit of the thing where you can mix and match your content to your ranker/recommender system.I hope you folks keep working on it, this is a refreshingly cool hack in the space.\n \nreply",
      "I remember there was a (rather controversial) tool posted here a few years ago, which used textual analysis and stylometry to find \u201csimilar users.\u201d So you could type in someone\u2019s username and find their likely alt accounts. It was creepily accurate - or at least that\u2019s what I heard from a friend who has an alt :)Could this tool be repurposed for that? Presumably the \u201cmap\u201d rendered in each user\u2019s avatar could be encoded as a vector and then compared to that of another user.EDIT: Wait, I just realized it already does this\u2026 (or at least I think so - it\u2019s not immediately obvious if \u201cExplore More Users\u201d is ranked by similarity.)\n \nreply",
      "> Could this tool be repurposed for that? Presumably the \u201cmap\u201d rendered in each user\u2019s avatar could be encoded as a vector and then compared to that of another user.It's probably less likely to work because people often use alts to participate in discussions that they wouldn't want to associate the primary identity with. Whether that be discussions about their employer, about politics, or something else, the subjects an alt particulates in will likely be different.Style works because few people are capable of fundamentally altering their style even if they tried.\n \nreply",
      "My final year project in uni was a tool to detect plagiarism by analyzing whether parts of an assignment had been outsourced to anyone else (basically did one author write the document on their own). Or if two projects by the same student were actually written by one person. For what was at the time an extremely naive implementation of various stylometric methods it worked surprisingly well. Even I was shocked when I managed to demonstrate the tool working. People should read about this branch of studies if they care at all about anonymity online.\n \nreply",
      "I believe this was it: https://news.ycombinator.com/item?id=33755016\n \nreply",
      "Sad, the website is not more.\n \nreply",
      "Interesting! Seems creepy to me too, as you can find it would love to understand how we can do better.\n \nreply",
      "Personally, I like how HN focuses on content and discussions rather than individual users. If I wanted to follow experts, I'd probably curate a selection on a social network like Mastodon, or kludge together some RSS feeds.Also, I feel like this tool selects for active commenters, not for knowledgeable experts. Not to mention throwaway accounts.Still a cool project.\n \nreply",
      "Thanks! Those are fair points. We're thinking we could uplevel the social layer so you can connect with people of similar interests for deeper connections. In this way we compute not just your contributions but how they relate to others.\n \nreply",
      "The social web died, all you're doing it making pitchfork and torch 2.0 for mobs.If you want to add value and not bloody public spectacle rank comments instead of users.I have a bunch of low quality posts here when idiots piss me off, but also share world first research and breakthroughs I've been involved with the rare time the counter party is worth talking to.\n \nreply"
    ],
    "link": "https://blog.wilsonl.in/hackerverse-2/#",
    "first_paragraph": ""
  },
  {
    "title": "Five Little Languages and How They Grew: Talk at HOPL (1993) (bell-labs.com)",
    "points": 65,
    "submitter": "fanf2",
    "submit_time": "2024-07-22T08:42:03",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=41032097",
    "comments": [
      "Interesting that other than C only pascal has survived (kind of).I have fond memories of Pascal, but when I think about it is actually Turbo Pascal I have great memories of and not so much the language itself.Turbo C/C++ was surprisingly a worse environment for me despite being much more fluent in C, which shows the important of a responsive IDE.\n \nreply",
      "The original title was   The Development of the C Language\n   or,\n   Five Little Languages and How They Grew\n \nreply",
      "> C's own descendants, by which I mainly mean C++, may very well be even livelier in the next few years. Aside from languages that are directly descended from C, (particularly C++ but also some others) [...]Does anyone know what the other languages descended from C they might have been referring to circa 1993?\n \nreply",
      "Earlier than or in 1993:- C++: 1979-- Objective C: 1984- Split-C: 1993Later than 1993:- C--: 1997- C#: 2000- D: 2001See also: https://en.wikipedia.org/wiki/List_of_C-family_programming_l... (but this list goes a bit too far IMHO)\n \nreply",
      "There was also this extension of C called Concurrent C, which has its own book called The Concurrent C Programming Language, originally published in 1989. I don't see it on the Wikipedia page, which is why I'm saying it here.\n \nreply",
      "I would consider Rust to be informed enough by C, that it can be considered a descendent\u2014as much as D, and more so than C# (which is more of a Java-descendent; and while Java's syntax is based on C, that's where it ends).\n \nreply",
      "C# was also at least somewhat informed by Delphi, the Object Pascal variant the same architect created just prior to MS luring him away from Borland.\n \nreply",
      "Objective-C?\n \nreply",
      "> Computer languages exist to perform useful things that affect the world in some way, not just to express algorithms, and so their success depends in part on their utility.Their all-in utility.  That is, their utility after taking into account usability, but also availability, availability and robustness of libraries, perhaps portability, and I'm sure there are more variables that affect net utility.\n \nreply",
      "This was recently brought into sharp focus while learning a new-to-me C descendant. After working with it for a couple months, I've found a lot to like in the language itself, but the many paper cuts and bad ergonomics of its standard library have considerably drained my enthusiasm for continuing to use it. What a pity.\n \nreply"
    ],
    "link": "https://www.bell-labs.com/usr/dmr/www/hopl.html",
    "first_paragraph": "\nAlthough I have the introductory remarks by the session chair,\nBrent Hailpern, and also the transcript of the Q&A session that\nfollowed, I've omitted these parts. I'll leave the parts others\nsaid for the book (which I recommend).\n\n\nThe transcript below is quite close to what I intended to say\naccording to my notes, though there were some on-the-fly\nadditions (especially in the opening--not surprisingly, there\nwere more than a few barbs thrown).\n\n\nIn cooperation with the volume's editors,\nparticularly Tim Bergin (to whom great thanks are due), my own language\nglitches have been cleaned up well, but it still retains some informality,\nas well as showing some of the time pressure on the presentation.\n\n\nDMR, March 2002\n\nThe transcript below is quite close to what I intended to say\naccording to my notes, though there were some on-the-fly\nadditions (especially in the opening--not surprisingly, there\nwere more than a few barbs thrown).\n\n\nIn cooperation with the volume's editors,\nparticularly "
  },
  {
    "title": "Show HN: Tiny Moon \u2013 Swift library to calculate the moon phase (github.com/mannylopez)",
    "points": 74,
    "submitter": "mannylopez",
    "submit_time": "2024-07-25T15:17:01",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=41069681",
    "comments": [
      "I know a lot of people into fishing that would love this as an app to quickly check the moon phase on a given date when planning a multi-day fishing trip or deciding when to go out. Apex predators are typically less hungry around and during a full moon due to the extra light making hunting at night easy.\n \nreply",
      "I don't really have use for this, but I must compliment the author on a job well-done. The code is well-structured, well-documented, well-tested, and well-designed.\n \nreply",
      "Thank you! I was a technical writer before learning how to program, so it's important for me to write well-documented and well-tested code.Doing so also allowed me to completely remove and replace my first attempt at implementing the moon phase algorithm when it turned out to be not-accurate enough (the values were off by about 4 hours).\n \nreply",
      "Is there a plan to add this for linux / windows? Either way, super cool project\n \nreply",
      "Good news! According to Swift Package Index [0], Tiny Moon is already compatible with Linux environments.0. https://swiftpackageindex.com/mannylopez/TinyMoon/\n \nreply",
      "The following formula will return the closest moon phase emoji in Excel:=LET(phase,MOD(ROUND(MOD(NOW(),29.5275)/3.691,0)-2,8)+1,UNICHAR(127760+phase)))\n \nreply",
      "nice! I have needed this before. I think you're using the Meeus algorithm from Astronomical Algorithms? it's a classic, great choice\n \nreply",
      "Yes, pretty much every good library I came across referenced Meeus' Astronomical Algorithms [0] book and formulas. Those were a bit dense for me to fully parse, but Dr. Louis Strous` page [1] on finding the position of the moon helped simplify it for me (although that implementation is less accurate, I think).The algorithm in Moontool for Windows [2], which Tiny Moon is based off of, is based off of Meeus' algorithm.0. https://www.agopax.it/Libri_astronomia/pdf/Astronomical%20Al...1. https://aa.quae.nl/en/reken/hemelpositie.html#42. https://www.fourmilab.ch/moontoolw/\n \nreply",
      "Nice work. Just FYI for anybody else - it adds an icon to status bar (if that's what it's called? - top right icons). I thought it was going to open an app window and didn't notice the icon, so thought it wasn't working. (In hindsight, it's clear from screenshots in app store).\n \nreply",
      "I think it's called the menu bar, and apps that don't open windows (with an icon in the menu bar) are known as \"menu bar apps\". Until recently, these were a PITA to implement with Apple's SDK. Thankfully they provided API support sometime I think starting with macOS 13... before that it was pretty kludgy.\n \nreply"
    ],
    "link": "https://github.com/mannylopez/TinyMoon",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A tiny Swift library to calculate the moon phase for any given date, works super fast, and works completely offline.\n      \n\nA tiny Swift library to calculate the moon phase for any given date, works super fast, and works completely offline.Compatible with iOS and MacOS.Now that Tiny Moon is added to your project, import it and simply pass in the the Date and TimeZone for which you'd like to know the Moon phase for. If no date is passed in, then your system's current Date will be used.An example app using Tiny MoonThe main entry point into the library is the TinyMoon name space.From there, you can access func's calculateMoonPhase and calculateExactMoonPhaseThe Moon object prioritizes the major phases (new moon, first quarter, full moon, last quarter) if it happens to land within a specific day.The ExactMoon object always returns the"
  },
  {
    "title": "Node.js adds experimental support for TypeScript (github.com/nodejs)",
    "points": 1000,
    "submitter": "magnio",
    "submit_time": "2024-07-25T02:57:37",
    "num_comments": 457,
    "comments_url": "https://news.ycombinator.com/item?id=41064351",
    "comments": [
      "One thing to note is that it is impossible to strip types from TypeScript without a grammar of TypeScript. Stripping types is not a token-level operation, and the TypeScript grammar is changing all the time.Consider for example: `foo < bar & baz > ( x )`. In TypeScript 1.5 this parsed as (foo<bar) & (baz > (x)) because bar&baz wasn\u2019t a valid type expression yet. When the type intersection operator was added, the parse changed to foo<(bar & baz)>(x) which desugared to foo(x). I realise I\u2019m going back in time here but it\u2019s a nice simple example.If you want to continue to use new TypeScript features you are going to need to keep compiling to JS, or else keep your node version up to date. For people who like to stick on node LTS releases this may be an unacceptable compromise.\n \nreply",
      "It looks like the team has already considered this in one regard> There is already a precedent for something that Node.js support, that can be upgraded seperately, its NPM.\nNode bundles a version of npm that can upgraded separately, we could do the same with our TypeScript transpiler.> We could create a package that we bundle but that can also be downloaded from NPM, keep a stable version in core, but if TypeScript releases new features that we don't support or breaking changes, or users want to use the new shiny experimental feature, they can upgrade it separately.\nThis ensures that users are not locked, but also provides support for a TypeScript version for the whole 3 years of the lifetime of Node.js release.https://github.com/nodejs/loaders/issues/217\n \nreply",
      "As long as Node understands to use the project-specific version of TypeScript (i.e., the one in node_modules or the PNP equivalent), that should be fine.But it would be a step backward to need to globally upgrade TypeScript (as you do with npm), since some older projects will not be compatible with newer versions of TypeScript.Ask me how I know. ;)\n \nreply",
      "I think Node\u2019s strategy here is to not let perfect be the enemy of useful for some people.\n \nreply",
      "> As long as Node understands to use the project-specific version of TypeScriptIt won't, but in such a scenario, typescript would only be a type checker, wich is a entirely different endeavor than running typescript.\n \nreply",
      "The syntax from the perspective of type stripping has been relatively stable for more versions of Typescript than it was unstable. You had to reach all the way back to 1.5 in part because it's been very stable since about 2.x. The last major shift in syntax was probably Conditional Types in 2.8 adding the ternary if operator in type positions. (The type model if you were to try to typecheck rather than just type-strip has changed a lot since 2.x, but syntax has been generally stable. That's where most of Typescript's innovation has been in the type model/type inferencing rather than in syntax.)It's still just (early in the process) Stage 1, but the majority of Typescript's type syntax, for the purposes of type stripping (not type checking), is attempting to be somewhat standardized: https://github.com/tc39/proposal-type-annotations\n \nreply",
      "They did just add a new keyword, satisfies, in 5.4. That would be a breaking change if you can\u2019t upgrade the type stripper separately.\n \nreply",
      "This is true, but in other cases they added keywords in ways that could work with type stripping. For example, the `as` keyword for casts has existed for a long time, and type stripping could strip everything after the `as` keyword with a minimal grammar.When TypeScript added const declarations, they added it as `as const` so a type stripping could have still worked depending on how loosely it is implemented.I think there is a world where type stripping exists (which the TS team has been in favor of) and the TS team might consider how it affects type stripping in future language design. For example, the `satisfies` keyword could have also been added by piggy-backing on the `as` keyword, like:    const foo = { bar: 1 } as subtype of Foo\n\n(I think not using `as` is a better fit semantically but this could be a trade-off to make for better type stripping backwards compatibility)\n \nreply",
      "I don't know a lot about parser theory, and would love to learn more about ways to make parsing resilient in cases like this one. Simple cases like \"ignore rest of line\" make sense to me, but I'm unsure about \"adversarial\" examples (in the sense that they are meant to beat simple heuristics). Would you mind explaining how e.g. your `as` stripping could work for one specific adversarial example?    function foo<T>() {\n        return bar(\n            null as unknown as T extends boolean\n            ? true /* ): */\n            : (T extends string\n                ? \"string\"\n                : false\n            )\n            )\n    }\n\n    function bar(value: any): void {}\n\nAny solution I can come up with suffers from at least one of these issues:- \"ignore rest of line\" will either fail or lead to incorrect results\n- \"find matching parenthesis\" would have to parse comments inside types (probably doable, but could break with future TS additions)\n- \"try finding end of non-JS code\" will inevitably trip up in some situations, and can get very expensiveI'd love a rough outline or links/pointers, if you can find the time![0] TS Playground link: https://www.typescriptlang.org/play/?#code/AQ4MwVwOwYwFwJYHs...\n \nreply",
      "Most parsers don't actually work with \"lines\" as a unit, those are for user-formatting. Generally the sort of building blocks you are looking for are more along the lines of \"until end of expression\" or \"until end of statement\". What defines an \"expression\" or a \"statement\" can be very complex depending on the parser and the language you are trying to parse.In JS, because it is a fun example, \"end of statement\" is defined in large part by Automatic Semicolon Insertion (ASI), whether or not semicolons even exist in the source input. (Even if you use semicolons regularly in JS, JS will still insert its own semicolons. Semicolons don't protect you from ASI.) ASI is also a useful example because it is an ancient example of a language design intentionally trying to be resilient. Some older JS parsers even would ignore bad statements and continue on the next statement based on ASI determined statement break. We generally like our JS to be much more strict than that today, but early JS was originally built to be a resilient language in some interesting ways.One place to dive into that directly (in the middle of a deeper context of JS parser theory): https://developer.mozilla.org/en-US/docs/Web/JavaScript/Refe...\n \nreply"
    ],
    "link": "https://github.com/nodejs/node/pull/53725",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \nHave a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.\n  By clicking \u201cSign up for GitHub\u201d, you agree to our terms of service and\n  privacy statement. We\u2019ll occasionally send you account related emails.\n    Already on GitHub?\n    Sign in\n    to your account\n  It is possible to execute TypeScript files by setting the experimental flag --experimental-strip-types.\nNode.js will transpile TypeScript source code into JavaScript source code.\nDuring the transpilation process, no type checking is performed, and types are discarded.Refs: nodejs/loaders#217I believe enabling users to execute TypeScript files is crucial to move the ecosystem forward, it has been requested on all the surveys, and it simply cannot be ignored. We must acknowledge users want to run node foo.ts wi"
  },
  {
    "title": "Unfashionably secure: why we use isolated VMs (thinkst.com)",
    "points": 174,
    "submitter": "mh_",
    "submit_time": "2024-07-25T17:00:03",
    "num_comments": 134,
    "comments_url": "https://news.ycombinator.com/item?id=41070870",
    "comments": [
      "As a permanent \"out of style\" curmudgeon in the last ~15 years, I like that people are discovering that maybe VMs are in fact the best approach for a lot of workloads and the LXC cottage industry and Docker industrial complex that developed around solving problems created by themselves or solved decades ago might need to take a hike.Modern \"containers\" were invented to make things more reproducible ( check ) and simplify dev and deployments ( NOT check ).Personally FreeBSD Jails / Solaris Zones are the thing I like to dream are pretty much as secure as a VM and a perfect fit for a sane dev and ops workflow, I didn't dig too deep into this is practice, maybe I'm afraid to learn the contrary, but I hope not.Either way Docker is \"fine\" but WAY overused and overrated IMO.\n \nreply",
      "As the person who created docker (well, before docker - see https://www.usenix.org/legacy/events/atc10/tech/full_papers/... and compare to docker), I argued that it wasn't just good for containers, but could be used to improve VM management as well (i.e. a single VM per running image - seehttps://www.usenix.org/legacy/events/lisa11/tech/full_papers...)I then went onto built a system with kubernetes that enabled one to run \"kubernetes pods\" in independent VMs - https://github.com/apporbit/infranetes (as well as create hybrid \"legacy\" VM / \"modern\" container deployments all managed via kubernetes.)- as a total aside (while I toot my own hort on the topic of papers I wrote or contributed to), note the reviewer of this paper that originally used the term Pod for a running container - https://www.usenix.org/legacy/events/osdi02/tech/full_papers... - explains where Kubernetes got the term from.I'd argue that FreeBSD Jails / Solaris Zones (Solaris Zone/ZFS inspired my original work) really aren't any more secure than containers on linux, as they all suffer from the same fundamental problem of the entire kernel being part of one's \"tcb\", so any security advantage they have is simply due lack of bugs, not simply a better design.\n \nreply",
      "> As the person who created docker (well, before docker - see https://www.usenix.org/legacy/events/atc10/tech/full_papers/... and compare to docker)I picked the name and wrote the first prototype (python2) of Docker in 2012. I had not read your document (dated 2010). I didn't really read English that well at the time, I probably wouldn't have been able to understand it anyways.https://en.wikipedia.org/wiki/Multiple_discoveryMore details for the curious: I wrote the design doc and implemented the prototype. But not in a vacuum. It was a lot work with Andrea, J\u00e9r\u00f4me and Gabriel. Ultimately, we all liked the name Docker. The prototype already had the notion of layers, lifetime management of containers and other fundamentals. It exposed an API (over TCP with zerorpc). We were working on container orchestration, and we needed a daemon to manage the life cycle of containers on every machine.\n \nreply",
      "I'd note I didn't say you copied it, just that I created it first (i.e. \"compare paper to docker\".  also, as you note, its possible someone else did it too, but at least my conception got through academic peer-review / patent office, yeah, there's a patent, never been attempted to be enforced though to my knowledge).when I describe my work (I actually should have used quotes here), I generally give air quotes when saying it, or say \"proto docker\", as it provides context for what I did (there's also a lot of people who view docker as synonymous with containerization as a whole, and I say that containers existed way before me).  I generally try to approach it humbly, but I am proud that I predicted and built what the industry seemingly needed (or at least is heavily using).people have asked me why I didn't pursue it as a company, and my answer is a) I'm not much of an entrepreneur (main answer), and b) I felt it was a feature, not a \"product\", and would therefore only really profitable for those that had a product that could use it as a feature (which one could argue that product turned out to be clouds, i.e. they are the ones really making money off this feature).  or as someone once said a feature isn't necessarily a product and a product isn't necessarily a company.\n \nreply",
      "I understood your point. I wanted to clarify, and in some ways connect with you.At the time, I didn't know what I was doing. Maybe my colleagues did some more, but I doubt that. I just wanted to stop waking up at night because our crappy container management code was broken again. The most brittle part was the lifecycle of containers (and their filesystem). I recall being very adamant about the layered filesystem, because it allowed to share storage and RAM across running (containerized) processes. This saves in pure storage and RAM usage, but also in CPU time, because the same code (like the libc for example) is cached across all processes. Of course this only works if you have a lot of common layers. But I remember at the time, it made for very noticeable savings. Anyways, fun tidbits.I wonder how much faster/better it would have been if inspired by your academic research. Or maybe not knowing anything made it so we solved the problems at hand in order. I don't know. I left the company shortly after. They renamed to Docker, and made it what it is today.\n \nreply",
      "they did it \"simpler\", i.e. academic work has to be \"perfect\" in a way a product does not.  so (from my perspective), they punted the entire concept of making what I would refer to as a \"layer aware linux distribution\" and just created layers \"on demand\" (via RUN syntax of dockerfiles).From an academic perspective, its \"terrible\", so much duplicate layers out in the world, from a practical perspective of delivering a product, it makes a lot of sense.It's also simpler from the fact that I was trying to make it work for both what I call \"persistent\" containers (ala pets in the terminology) that could be upgraded in place and \"ephemeral\" containers (ala cattle) when in practice the work to enable upgrading in place (replacing layers on demand) to upgrade \"persistent\" containers I'm not sure is that useful (its technologically interesting, but that's different than useful).My argument for this was that this actually improves runtime upgrading of systems.  With dpkg/rpm, if you upgrade libc, your systems is actually temporarily in a state where it can't run any applications (in the delta of time when the old libc .so is deleted and the new one is created in its place, or completely overwrites it), any program that attempts to run in that (very) short period time, will fail (due to libc not really existing).  By having a mechanism where layers could be swapped in essentially an atomic manner, no delete / overwrite of files occurs and therefore there is zero time when programs won't run.In practice, the fact that a real world product came out with a very similar design/implementation makes me feel validated (i.e. a lot of phd work is one offs, never to see the light of day after the papers for it are published).\n \nreply",
      "Would you say approaches like gvisor or nabla containers provide more/enough evolution on the security front? Or is there something new on the horizon that excites you more as a prospect?\n \nreply",
      "GVisor basically works by intercepting all Linux syscalls, and emulating a good chunk of the Linux kernel in userspace code. In theory this allows lowering the overhead per VM, and more fine-grained introspection and rate limiting / balancing across VMs, because not every VM needs to run it's own kernel that only interacts with the environment through hardware interfaces. Interaction happens through the Linux syscall ABI instead.From an isolation perspective it's not  more secure than a VM, but less, because GVisor needs to implement it's own security sandbox to isolate memory, networking, syscalls, etc, and still has to rely on the kernel for various things.It's probably more secure than containers though, because the kernel abstraction layer is separate from the actual host kernel and runs in userspace - if you trust the implementation... using a memory-safe language helps there. (Go)The increased introspectioncapabiltiy  would make it easier to detect abuse and to limit available resources on a more fine-grained level though.Note also that GVisor has quite a lot of overhead for syscalls, because they need to be piped through various abstraction layers.\n \nreply",
      "I actually wonder how much \"overhead\" a VM actually has.  i.e. a linux kernel that doesn't do anything (say perhaps just boots to an init that mounts proc and every n seconds read in/prints out /proc/meminfo) how much memory would the kernel actually be using?So if processes in gvisor map to processes on the underlying kernel, I'd agree it gives one a better ability to introspect (at least in an easy manner).It gives me an idea that I'd think would be interesting (I think this has been done, but it escapes me where), to have a tool that is external to the VM (runs on the hypervisor host) that essentially has \"read only\" access to the kernel running in the VM to provide visibility into what's running on the machine without an agent running within the VM itself.  i.e. something that knows where the processes list is, and can walk it to enumerate what's running on the system.I can imagine the difficulties in implementing such a thing (especially on a multi cpu VM), where even if you could snapshot the kernel memory state efficiently, it be difficult to do it in a manner that provided a \"safe/consistent\" view.  It might be interesting if the kernel itself could make a hypercall into the hypervisor at points of consistency (say when finished making an update and about to unlock the resource) to tell the tool when the data can be collected.\n \nreply",
      "https://github.com/Wenzel/pyvmidbg  LibVMI-based debug server, implemented in Python. Building a guest aware, stealth and agentless full-system debugger.. GDB stub allows you to debug a remote process running in a VM with your favorite GDB frontend. By leveraging virtual machine introspection, the stub remains stealth and requires no modification of the guest.\n\nmore: https://github.com/topics/virtual-machine-introspection\n \nreply"
    ],
    "link": "https://blog.thinkst.com/2024/07/unfashionably-secure-why-we-use-isolated-vms.html",
    "first_paragraph": "Would your rather observe an eclipse through a pair of new Ray-Bans, or a used Shade 12 welding helmet? Undoubtably the Aviators are more fashionable, but the permanent retinal damage sucks. Fetch the trusty welding helmet.We\u2019ve made a number of security choices when building Canary that have held us in pretty good stead. These choices are interesting in that they don\u2019t involve the purchase of security products, they don\u2019t get lots of discussion in security engineering threads, and they verge on being unfashionable. One major unsexy architectural choice has proved itself: complete customer isolation.Fundamentally, Canary relies on two components: the Canary devices (hardware or virtual) that are deployed in customer infrastructure, and the Console (which we run) that these Canaries report into. Very broadly this is identical to most cloud-managed device or appliance products: appliances send telemetry to the cloud. It\u2019s typical for cloud-managed devices to report to a single endpoint ("
  },
  {
    "title": "Veles: Open-source tool for binary data analysis (codisec.com)",
    "points": 63,
    "submitter": "LorenDB",
    "submit_time": "2024-07-21T17:12:59",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=41026480",
    "comments": [
      "The GitHub repo [1] was made a read-only archive on Sep 30, 2020. Are there forks picking up where the original author left off?[1]: https://github.com/codilime/veles\n \nreply",
      "Since this is an old tool and it fails to deal with large files, I made a completely new version from scratch a while ago, using a completely different rendering approach that's more like how you'd render a volumetric data set like an MRI scan. It loads the file and processes it into a 256x256x256 volumetric data set, which is then rendered using shaders. As a result, the file size doesn't matter for rendering, only the loading time depends on the file size. Unlike the original Veles, it also doesn't need any subsampling for huge files, but you need a powerful enough graphics card.The source code is on github and unlike the original Veles, it doesn't have countless dependencies and build problems on modern systems: https://github.com/hackyourlife/veles\n \nreply",
      "See by yourself.\nThere is only one fork wich fixed the msgpack encoding breakage: https://github.com/0xBattleSong/veles\n \nreply",
      "The main page of codisec [0] lists that Veles has been retired. They since have a new project, CodiLime [1], which seems like is a general technology consulting/outsourcing company.[0] https://codisec.com/[1] https://codilime.com/\n \nreply",
      "ImHex has some similar data pattern analysis tools. And as it is actively maintained, might be worthy to take a look at.\n \nreply",
      "Not sure how new the tool is because it was new in 2017...https://news.ycombinator.com/item?id=15966021\n \nreply",
      "There was a major push for development back in 2015-2016, back before it was open sourced, but AFAIK it went dead pretty soon as internal funding dried up.. Looking at github commit log, it seems it died by mid 2018.First time I had to setup CI on a Mac, and what a horrible experience it was (Windows was comparatively easy, though finding out that starting with Windows Core wasn't best option took some time to learn)\n \nreply",
      "Did you work for the parent company?\n \nreply",
      "I am honestly surprised this still shows up...learnt so much CMakeFile pain...\n \nreply",
      "Their linux build is in ubuntu 16.04\n...\n \nreply"
    ],
    "link": "https://codisec.com/veles/",
    "first_paragraph": ""
  }
]