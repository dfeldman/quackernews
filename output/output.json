[
  {
    "title": "Xfinity using WiFi signals in your house to detect motion (xfinity.com)",
    "points": 287,
    "submitter": "bearsyankees",
    "submit_time": "2025-06-30T19:03:50 1751310230",
    "num_comments": 192,
    "comments_url": "https://news.ycombinator.com/item?id=44426726",
    "comments": [
      "> Subject to applicable law, Comcast may disclose information generated by your WiFi Motion to third parties without further notice to you in connection with any law enforcement investigation or proceeding, any dispute to which Comcast is a party, or pursuant to a court order or subpoena.Sounds like, at least in some limited circumstances (using the provided WiFi AP, having this feature turned on, etc), ISPs are going to be able to tell law enforcement/courts whether anyone was home at a certain time or not.reply",
      "The solution here shouldn't be technical; it should be legal.If we rely on the technical path, Comcast can achieve the same by how many active IPv6 addresses are in use. Even if you aren't using your phone, the device is going to be constantly pinging services like email, and your ISP can use that to piece together how many people are at home.If we rely on legal protection, then not only Comcast, but all ISPs will be prohibited from spying on their customers. Ideally the legislation would be more broad and stop other forms of commercial/government surveillance, but I can't imagine a world where Congress could actually achieve something that widely helpful for regular citizens.reply",
      "\"The solution here shouldn't be technical; it should be legal.\"Laws can be broken. Laws of physics cannot. Best to utilize both a legal and physical defense.reply",
      "> The solution here shouldn't be technical; it should be legal.I disagree. Solutions should be technical whenever possible, because in practice, laws tend to be abused and/or not enforced. Laws also need resources and cooperation to be enforced, and some laws are hard to enforce without creating backdoors or compromising other rights.\"ISPs will be prohibited from spying on their customers\" doesn't mean ISPs won't spy on their customers.reply",
      "There is no technical solution for this unless you want to invest billions/trillions in building new computing and networking platforms created with privacy in mind.ISPs will always have the ability to at least deduce whether a connection was used, the MAC address, and it there is WiFi, unfortunately whether people are physically present.If we look at the roadmap for WiFi/phones/etc, they will soon gain the ability to map out your home, including objects, using consumer radios.reply",
      "We need more funding for open-source WiFi Sensing counter-measures, e.g. EU research, https://ans.unibs.it/projects/csi-murder/> this paper addressed passive attacks, where the attacker controls only a receiver, but exploits the normal Wi-Fi traffic. In this case, the only useful traffic for the attacker comes from transmitters that are perfectly fixed and whose position is well known and stable, so that the NN can be trained in advance, thus the obfuscator needs to be installed only in APs or similar \u2018infrastructure\u2019 devices. Active attacks, where the attacker controls both the transmitter and the receiver are another very interesting research area, where, however, privacy protection cannot be based on randomization at the transmitter.https://github.com/ansresearch/csi-murder/> The experimental results obtained in our laboratory show that the considered localization method (first proposed in an MSc thesis) works smoothly regardless of the environment, and that adding random information to the CSI mess up the localization, thus providing the community with a system that preserve location privacy and communication performance at the same time.reply",
      "Technical and legal solutions are for different classes of problems.Encryption is a technical solution trying to solve the problem of people being able to steal your data/money without your knowledge.The law/police are the solution to the 5 dollar wrench problem, where you are very aware of the attack but unable to physically stop itreply",
      "It might make it a bit harder to use the information obtained through spying, though. Both is good.reply",
      "> The solution here shouldn't be technical; it should be legal.The parent commenter was highlighting that law enforcement can compel them to provide the data.The customer has to opt-in to WiFi motion sensing to have the data tracked. If you see something appear in an app, you should assume law enforcement can compel the company to provide that data. It's not really a surprise.> If we rely on legal protection, then not only Comcast, but all ISPs will be prohibited from spying on their customers.To be clear, the headline on HN is editorialized. The linked article is instructions for opting in to WiFi motion sensing and going through the setup and calibration. It's a feature they provide for customers to enable and use for themselves.reply",
      "\u201cPlease accept our new terms of service to continue using your internet connection\u201dYour honor, they clearly opted in to us spying on absolutely everything they do or think.reply"
    ],
    "link": "https://www.xfinity.com/support/articles/wifi-motion",
    "first_paragraph": "Don't worry, this sounds complicated but it's just a simple browser setting. You can usually find JavaScript options in your browser's Settings, Preferences or Internet Options menu.Here are the instructions how to enable JavaScript in your web browser."
  },
  {
    "title": "The new skill in AI is not prompting, it's context engineering (philschmid.de)",
    "points": 366,
    "submitter": "robotswantdata",
    "submit_time": "2025-06-30T20:53:55 1751316835",
    "num_comments": 205,
    "comments_url": "https://news.ycombinator.com/item?id=44427757",
    "comments": [
      "I wrote a bit about this the other day: https://simonwillison.net/2025/Jun/27/context-engineering/Drew Breunig has been doing some fantastic writing on this subject - coincidentally at the same time as the \"context engineering\" buzzword appeared but actually unrelated to that meme.How Long Contexts Fail - https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-ho... - talks about the various ways in which longer contexts can start causing problems (also known as \"context rot\")How to Fix Your Context - https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.... - gives names to a bunch of techniques for working around these problems including Tool Loadout, Context Quarantine, Context Pruning, Context Summarization, and Context Offloading.reply",
      "Providing context makes sense to me, but do you have any examples of providing context and then getting the AI to produce something complex? I am quite a proponent of AI but even I find myself failing to produce significant results on complex problems, even when I have clone + memory bank, etc. it ends up being a time sink of trying to get the ai to do something only to have me eventually take over and do it myself.reply",
      "First, you pay a human artist to draw a pelican on a bicycle.Then, you provide that as \"context\".Next, you prompt the model.Voila!reply",
      "How to draw an owl.1. Draw some circles.2. Prompt an AI to draw the rest of the fucking owl.reply",
      "And then the AI doesn\u2019t handle the front end caching properly for the 100th time in a row so you edit the owl and nothing changes after you press save.reply",
      "And if you want 2 owls?reply",
      "Hire a context engineer to define the task of drawing an owl as drawing two owls.reply",
      "This hits too close to home.reply",
      "Oh, and don't forget to retain the artist to correct the ever-increasingly weird and expensive mistakes made by the context when you need to draw newer, fancier pelicans. Maybe we can just train product to draw?reply",
      "Drew Breunig's posts are a must read on this.  This is not only important for writing your own agents, it is also critical when using agentic coding right now.  These limitations/behaviors will be with us for a while.reply"
    ],
    "link": "https://www.philschmid.de/context-engineering",
    "first_paragraph": "Context Engineering is new term gaining traction in the AI world. The conversation is shifting from \"prompt engineering\" to a broader, more powerful concept: Context Engineering. Tobi Lutke describes it as \"the art of providing all the context for the task to be plausibly solvable by the LLM.\u201d and he is right.With the rise of Agents it becomes more important what information we load into the \u201climited working memory\u201d. We are seeing that the main thing that determines whether an Agents succeeds or fails is the quality of the context you give it. Most agent failures are not model failures anyemore, they are context failures.To understand context engineering, we must first expand our definition of \"context.\" It isn't just the single prompt you send to an LLM. Think of it as everything the model sees before it generates a response.The secret to building truly effective AI agents has less to do with the complexity of the code you write, and everything to do with the quality of the context yo"
  },
  {
    "title": "I write type-safe generic data structures in C (danielchasehooper.com)",
    "points": 225,
    "submitter": "todsacerdoti",
    "submit_time": "2025-06-30T16:55:20 1751302520",
    "num_comments": 87,
    "comments_url": "https://news.ycombinator.com/item?id=44425461",
    "comments": [
      "For your level 2 code, `uint64_t data[];` is wrong for types whose alignment is greater than `uint64_t`, and also wasteful for types whose alignment is smaller (for example, under an ilp32 ABI on 64-bit architectures).For your level 3 code, it should be `int main() { List(Foo) foo_list = {NULL};`Note that working around a lack of `typeof` means you can't return anything. Also, your particular workaround allows `const`ness errors since `==` is symmetrical.You can't safely omit `payload` since you need it to know the correct size. Consider a `List(int64_t)` and you try to add an `int32_t` to it - this should be fine, but you can't `sizeof` the `int32_t`. Your code is actually lacking quite a bit to make this work.=====There are 2 major limitations to generics in C right now:* Delegating to a vtable (internal or external) is limited in functionality, since structs cannot contain macros, only functions.* Delegating to an external vtable (mandatory to avoid overhead) means that you have to forward-declare all of the types you'll ever use a vtable with. So far the best approach I've found is to declare (but not define) static functions in the same forwarding header I declare the typedefs in; note that GCC and Clang differ in what phase the \"undefined static\" warning appears in for the case where you don't actually include that particular type's header in a given TU.(think about writing a function that accepts either `struct SizedBuffer {void *p; size_t len;};` or `struct BoundedBuffer {void *begin; void *end;};`, and also const versions thereof - all from different headers).reply",
      "> Delegating to an external vtable (mandatory to avoid overhead) means that you have to forward-declare all of the types you'll ever use a vtable with.We went down the rabbit hole of writing a compiler for this as part of a project I used to work on (Apache Clownfish[1], a subproject of the retired Apache Lucy project).  We started off parsing .h files, but eventually it made sense to create our own small header language (.cfh \"Clownfish Header\" files).Here's some generated code for invoking the CharBuf version of the \"Clone\" method defined in parent class \"Obj\":    typedef cfish_CharBuf*\n    (*CFISH_CharBuf_Clone_t)(cfish_CharBuf* self);\n\n    extern uint32_t CFISH_CharBuf_Clone_OFFSET;\n\n    static inline cfish_CharBuf*\n    CFISH_CharBuf_Clone(cfish_CharBuf* self) {\n        const CFISH_CharBuf_Clone_t method\n            = (CFISH_CharBuf_Clone_t)cfish_obj_method(\n                self,\n                CFISH_CharBuf_Clone_OFFSET\n            );\n        return method(self);\n    }\n\nUsage:    cfish_CharBuf *charbuf = cfish_CharBuf_new();\n    cfish_CharBuf *clone = CFISH_CharBuf_Clone(charbuf);\n\nWe had our reasons for going to these extremes: the point of Clownfish was to provide a least-common-denominator object model for bindings to multiple dynamic languages (similar problem domain to SWIG), and the .cfh files also were used to derive types for the binding languages.  But there was truly an absurd amount of boilerplate being generated to get around the issue you identify.This is why almost everybody just uses casts to void* for the invocant, skipping type safety.[1] https://github.com/apache/lucy-clownfishreply",
      "i am firmly of the opinion that compiling to c is a better route than doing clever c tricks to sort of get what you want. the compiler can be pretty minimal and as you note it pays for itself.reply",
      "> it should be `int main() { List(Foo) foo_list = {NULL};`In C `int main()` means the function takes an unknown number of arguments. You need `int main(void)` to mean it doesn't take any arguments. This is a fact frequently forgotten by those who write C++.reply",
      "This is incorrect. In a function definition, an empty list means it takes no parameters. 6.7.5.3 Function declarators> 14. An empty list in a function declarator that is part of a definition of that function specifies that the function has no parameters.reply",
      "As you surely know if you're quoting the standard, it depends on which standard!reply",
      "I believe that since C23 foo() is now a nullary function. As this is the last approved standard and it supersedes all previous standards, it is technically correct to say that de-jure this is what the (unqualified) C standard mandates.Of course de-facto things are more nunanced.reply",
      "C23 does not change anything in this situation, because we are talking about the definition of main(), not a forward declaration. More details here:https://news.ycombinator.com/item?id=38729278#38732366reply",
      "Quote a different standard.reply",
      "That had been harmonized with C++ in C23 (e.g. func() is equivalent with func(void) now).It's not really relevant for main() though, even in older C versions main() works fine and simply means \"I don't need argc and argv\".reply"
    ],
    "link": "https://danielchasehooper.com/posts/typechecked-generic-c-data-structures/",
    "first_paragraph": "June 25, 2025\u30fb7 minute readI write type safe generic data structures in C using a technique that I haven\u2019t seen elsewhere1. It uses unions to associate type information with a generic data structure, but we\u2019ll get to that. My approach works for any type of data structure: maps, arrays, binary trees\u2026 but for this article I illustrate the ideas by implementing a basic linked list. Since many people aren\u2019t aware you can do C generics at all, I figured I\u2019d start simple and build up to this:I hesitate to even mention this, because I do not like it2, but its worth comparing to the technique at the end of this article. It works like this: you write your data structure in a header, using macros for your types, and then #include the header multiple times; once for each type the data structure will be used with.list.hmain.cWhile it is generic and type safe, it has downsides:Another way to make a data structure generic is to use void *. It\u2019s not type safe but we\u2019ll get to that.Note: malloc is use"
  },
  {
    "title": "There are no new ideas in AI only new datasets (jxmo.io)",
    "points": 304,
    "submitter": "bilsbie",
    "submit_time": "2025-06-30T14:43:46 1751294626",
    "num_comments": 161,
    "comments_url": "https://news.ycombinator.com/item?id=44423983",
    "comments": [
      "What John Carmack is exploring is pretty revealing.\nTrain models to play 2D video games to a superhuman level, then ask them to play a level they have not seen before or another 2D video game they have not seen before. The transfer function is negative. So, in my definition, no intelligence has been developed, only expertise in a narrow set of tasks.It\u2019s apparently much easier to scare the masses with visions of ASI, than to build a general intelligence that can pick up a new 2D video game faster than a human being.reply",
      "He is not using appropriate models for this conclusion and neither is he using state of the art models in this research and moreover he doesn't have an expensive foundational model to build upon for 2d games. It's just a fun project.A serious attempt at video/vision would involve some probabilistic latent space that can be noised in ways that make sense for games in general. I think veo3 proves that ai can generalize 2d and even 3d games, generating a video under prompt constraints is basically playing a game. I think you could prompt veo3 to play any game for a few seconds and it will generally make sense even though it is not fine tuned.reply",
      "Veo3's world model is still pretty limited. That becomes obvious very fast once you prompt out of distribution video content (i.e. stuff that you are unlikely to find on youtube). It's extremely good at creating photorealistic surfaces and lighting. It even has some reasonably solid understanding of fluid dynamics for simulating water. But for complex human behaviour (in particular certain motions) it simply lacks the training data. Although that's not really a fault of the model and I'm pretty sure there will be a way to overcome this as well. Maybe some kind of physics based simulation as supplement training data.reply",
      "Is any model currently known to succeed in the scenario that Carmack\u2019s inappropriate model failed?reply",
      "No monolithic models but us ng hybrid approaches we've been able to beet humans for some time now.reply",
      "To confirm: hybrid approaches can demonstrate competence at newly-created video games within a short period of exposure, so long as similar game mechanics from other games were incorporated into their training set?reply",
      "What you're thinking of is much more like the Genie model from DeepMind [0]. That one is like Veo, but interactive (but not publically available)[0] https://deepmind.google/discover/blog/genie-2-a-large-scale-...reply",
      "> I think veo3 proves that ai can generalize 2d and even 3d games, generating a video under prompt constraints is basically playing a game.In the same way that keeping a dream journal is basically doing investigative journalism, or talking to yourself is equivalent to making new friends, maybe.The difference is that while they may both produce similar, \"plausible\" output, one does so as a result of processes that exist in relation to an external reality.reply",
      "> generating a video under prompt constraints is basically playing a gameBesides static puzzles (like a maze or jigsaw) I don't believe this analogy holds? A model working with prompt constraints that aren't evolving or being added over the course of \"navigating\" the generation of the model's output means it needs to process 0 new information that it didn't come up with itself \u2014 playing a game is different from other generation because it's primarily about reacting to input you didn't know the precise timing/spatial details of, but can learn that they come within a known set of higher order rules. Obviously the more finite/deterministic/predictably probabilistic the video game's solution space, the more it can be inferred from the initial state, aka reduce to the same type of problem as generating a video from a prompt), which is why models are still able to play video games. But as GP pointed out, transfer function negative in such cases \u2014 the overarching rules are not predictable enough across disparate genres.> I think you could prompt veo3 to play any game for a few secondsI'm curious what your threshold for what constitutes \"play any game\" is in this claim? If I wrote a script that maps button combinations to average pixel color of a portion of the screen buffer, by what metric(s) would veo3 be \"playing\" the game more or better than that script \"for a few seconds\"?edit: removing knee-jerk reaction languagereply",
      "It's not ideal, but you can prompt it with an image of a game frame, explain the objects and physics in text and let it generate a few frames of gameplay as a substitute for controller input as well as what it expects as an outcome. I am not talking about real interactive gameplay.I am just saying we have proof that it can understand complex worlds and sets of rules, and then abide by them. It doesn't know how to use a controller and it doesn't know how to explore the game physics on its own, but those steps are much easier to implement based on how coding agents are able to iterate and explore solutions.reply"
    ],
    "link": "https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only",
    "first_paragraph": ""
  },
  {
    "title": "The hidden JTAG in a Qualcomm/Snapdragon device\u2019s USB port (linaro.org)",
    "points": 117,
    "submitter": "denysvitali",
    "submit_time": "2025-06-30T18:34:27 1751308467",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=44426428",
    "comments": [
      "This is a much better experience than the previous Qualcomm debug experience, which was a hand-rolled set of read/write/execute primitives exposed over USB. It was hilariously undersecured, allowing a few of us to continually get root on various Qualcomm models.In seriousness, these debug ports are seriously lacking in most mobile chipsets. MediaTek still has the old-style approach in many of their devices, requiring some incantations which expose serial over USB, but not in the way you think -- it's serial over USB pins!I've done tonnes of work with mobile chipsets and security and this seems like they've finally started down the road to making this functionality accessible. Don't be surprised if you don't see this supported out of the box in most places, though. Most OEMs will certainly disable this once they've adapted their bootloaders to it. The big G doesn't like debuggability in end user devices.reply",
      "MediaTek still has the old-style approach in many of their devices, requiring some incantations which expose serial over USB, but not in the way you think -- it's serial over USB pins!Wasn't that only in their old feature-phone (MT62xx) SoCs? All the smartphone ones AFAIK (at least since the MT657x days) use USB CDC in their BootROM and preloader.reply",
      "Most of those boards have a separate physical JTAG connector (at least in development kits, this article indicates JTAG over USB is disabled in production systems anyways so no difference there) which is what they are expecting you to use for low-level debugging. It only costs like 1,000 $ for a JTAG probe which is like 1 fully-burdened engineer-day of cost. Even fully featured probes enabling hardware trace and time-travel debugging only cost like 1 engineer-week.reply",
      "> Most of those boards have a separate physical JTAG connector (at least in development kits, this article indicates JTAG over USB is disabled in production systems anyways so no difference thereThere's generally an entire phase of prototyping where engineers will be using production boards but still need JTAG, which is why it's fused and why these kinds of features exist. It's a lot easier to have your lower-level software team (drivers/BSP, perf, etc.) sitting with production-ready units provisioned with engineering keys and debug enabled than to have them having to use some kind of case-off JTAG header setup, cost aside.reply",
      "The probes cost enough to exceed individual purchasing limits at hardware companies, which means you need to go through the requisition process. That takes long enough that you have to plan ahead and you don't order more as your needs increase. Then everyone's fighting for the limited probes right before a ship date and they get jealously guarded like priceless jewels.JTAG also isn't usually exposed through enclosures, so using the probe on a field unit might require destructive entry depending on the application.reply",
      "Well the problem there is companies who are too stupid to invest in cheap tooling with massive ROI for their developers. A pretty constant problem in software development.And I am not knocking JTAG over USB. It is certainly convenient and beneficial since you can enable it in production or deployed units. I was commenting on how the GP (and even article) was making it out to be missing capability. They just do not have the cheap tools that are the intended way to access that capability.edit: The article even mentions how the \"Qualcomm Landing Team at Linaro\", which seems to be the team that works with pre-production hardware to get them working on launch day, has a development process where \"debuggers have never been a staple of our work for all the typical reasons you'd expect (cost and complexity being the main ones)\". That is literally the team that should have pre-production units in the lab which will have debug connectors and where JTAG probes should be par for the course, yet they are apparently hardly using them partly because of \"cost\".reply",
      "> this article indicates JTAG over USB is disabled in production systems anywaysWell, should be. I bet there've been screwups now and then...reply",
      "Google exposes serial Serial over the SBU pins on all the Pixel devicesreply",
      "What are the effective implications of this?reply",
      "It's just a UART; you can use the UART to debug the device in various ways.On Pixel devices, the UART is not configured or brought up by default in locked production mode (as things should be), but by unlocking the device and then using `fastboot oem uart enable` you can flip the bits to turn it on. On early Pixel devices it was on the headphone jack and on newer ones it's on the SBU pins.By default I think it's still configured as the kernel console in the kernel command line, so once it's enabled it will show the kernel debug output and present a TTY. But of course you can subsequently configure it to do whatever you'd want a UART for: kgdb for kernel-debugging, earlier stuff in the bootloader, and so on.So, the implications are just: there's a convenient debugging interface available to you that turns on if you unlock the device and ask for it.On Chromebook devices there's a more complicated and fancy debugging system where the SBU pins can be muxed to the security processor's USB host interface by presenting a debug cable called a SuzyQ, which presents a whole suite of debugging facilities. This used to be used quite frequently for unbricking purposes.reply"
    ],
    "link": "https://www.linaro.org/blog/hidden-jtag-qualcomm-snapdragon-usb/",
    "first_paragraph": " Casey Connolly  Monday, June 30, 20259 min read     Please enter Mastodon Instance:   Submit Cancel                              \n         Back in February of this year, Qualcomm quietly published the source code for interacting with EUD. This is perhaps one of the most exciting things they\u2019ve done lately - especially if you spend a lot of time debugging the kernel or U-Boot - let\u2019s talk about it.EUD stands for Embedded USB Debug: essentially, this is a debug interface built right into almost every Qualcomm SoC since ~2018. Internally it hooks deep into the SoC, providing debug facilities for not just the CPUs but also the myriad of Hexagon co-processor/DSPs; many of the exciting details can be found in this patent from way back in 2014.In practise, for a non-production device (like a dev board, though some production devices seem to work too), EUD can be enabled by writing a few registers and then starting up the USB phy (though the details vary by generation). Instead of whatever ty"
  },
  {
    "title": "Melbourne man discovers extensive model train network underneath house (sbs.com.au)",
    "points": 88,
    "submitter": "cfcfcf",
    "submit_time": "2025-06-30T23:53:42 1751327622",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=44429182",
    "comments": [
      "This is fantastic. But wow, the home inspector was really phoning it in that day!reply",
      "It's an omission so huge you could drive a train through it.reply",
      "Our inspector missed obvious asbestos in the basement \u2014 I would have preferred the model trains!reply",
      "Good news, everyone! It'll be all legal again soon:https://www.ishn.com/articles/114790-trump-administration-to...reply",
      "The finger curls. You get model trains carrying carloads of asbestos.reply",
      "Our home inspector missed the front door failing to latch!reply",
      "Philip K. Dick's \"Small Town\" is found!reply",
      "How? It was accessible through a door. Nobody - not the seller, agent, himself or any other prospective buyers, or the building inspector he presumably engaged to check the place over before signing contracts - thought to look behind the door?How can you buy a house without checking out the foundations/basement yourself or by a pro?reply",
      "Not uncommon for Australia. The housing market is very competitive so being a nuisance as a buyer, such as hiring someone for a thorough inspection, could hurt your chances.What inspectors actually do also depends on who is engaging them and how much they get paid. For example, in the ACT it's mandatory for sellers to have an inspection done. This will generally go to the lowest bidder and they will put in minimum effort, e.g. the report will have things like \"Roof inspected as far as can see from ladder placed against the house\" and \"furniture present, unable to inspect area\". If you were the buyer and engaging an inspector, and the seller cooperated, you could have them inspect as much as you were willing to pay them for.reply",
      "All the home inspectors I looked at (Victoria, where this house is, plus Tasmania) were all quite clear that they would only access areas they could find a way in.  Closed up areas, wouldn't be inspected by default.reply"
    ],
    "link": "https://www.sbs.com.au/news/article/i-was-shocked-melbourne-mans-unbelievable-find-after-buying-house/m4sksfer8",
    "first_paragraph": "MoreLoginSign upSearchThe model train setup Daniel Xu found beneath the home he just purchased is impressive, but it has proven an even greater delight, given he is a train engineer and train enthusiast. Source: SBS NewsPublished 29 June 2025 1:28pmUpdated 30 June 2025 1:59pmShare this with family and friendsMorning (Mon\u2013Fri)Afternoon (Mon\u2013Fri)WeekendBy subscribing, you agree to SBS\u2019s terms of service and privacy policy including receiving email updates from SBS.SBS World News"
  },
  {
    "title": "People Keep Inventing Prolly Trees (dolthub.com)",
    "points": 31,
    "submitter": "lifty",
    "submit_time": "2025-06-28T20:01:54 1751140914",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44407708",
    "comments": [
      "Haha, this is funny. I've been obsessed with rolling-hash based chunking since I read about it in the dat paper. I didn't realize there was a tree version, but it is a natural extension.I have a related cryptosystem that I came up with, but is so obvious I'm sure someone else has invented it first. The idea is to back up a file like so: first, do a rolling-hash based chunking, then encrypt each chunk where the key is the hash of that chunk. Then, upload the chunks to the server, along with a file (encrypted by your personal key) that contains the information needed to decrypt each chunk and reassemble them. If multiple users used this strategy, any files they have in common would result in the same chunks being uploaded. This would let the server provider deduplicate those files (saving space), without giving the server provider the ability to read the files. (Unless they already know exactly which file they're looking for, and just want to test whether you're storing it.)Tangent: why is it that downloading a large file is such a bad experience on the internet? If you lose internet halfway through, the connection is closed and you're just screwed. I don't think it should be a requirement, but it would be nice if there was some protocol understood by browsers and web servers that would be able to break-up and re-assemble a download request into a prolly tree, so I could pick up downloading where I left off, or only download what changed since the last time I downloaded something.reply",
      "I think the cost of processing stuff that way would far exceed the cost of downloading the entire file again. You can already resume downloads from a byte offset if the server supports it, and that probably covers 99% of the cases where you would actually want to resume a download of a single file. Partial updates are rarely possible for large files anyway, as they are often compressed. If the host wants to make partial updates make sense then they could serve over rsync.reply",
      "This article does not mention Jumbostore (Kave Eshghi, Mark Lillibridge, Lawrence Wilcock, Guillaume Belrose, and Rycharde Hawkes) which used content defined chunking recursively on the chunk list of a content defined chunked file in 2007. This is exactly what a Prolly Tree is.reply",
      "I was aware of this kind of structure when I coined 'prolly tree'. It's the same thing bup was doing, which I referenced in our design docs:https://github.com/attic-labs/noms/blob/master/doc/intro.md#...The reason I thought a new name was warranted is that a prolly tree stores structured data (a sorted set of k/v pairs, like a b-tree), not blob data. And it has the same interface and utility as a b-tree.Is it a huge difference? No. A pretty minor adaptation of an existing idea. But still different enough to warrant a different name IMO.reply",
      "Amazing! all these people reinvented my SuperMegaTree!reply"
    ],
    "link": "https://www.dolthub.com/blog/2025-06-03-people-keep-inventing-prolly-trees/",
    "first_paragraph": "Multiple Discovery refers to when a scientific discovery is made independently by multiple individuals around the same time. The most well-known examples are Isaac Newton and Gottfried Leibniz's independent invention of calculus, and Charles Darwin and Alfred Russel Wallace's independent formulation of the theory of evolution.\n\n\n\n\n(Source: https://xkcd.com/626/)There's even a hypothesis1 that multiple discovery is the norm, rather than an exception: that invention is an result of social conditions rather than any single \"great thinker\", and that once the conditions for a discovery are met, that discovery is often made multiple times in rapid succession.It's not always immediately obvious when multiple discovery happens. There's several reasons why a case of multiple discovery might go unnoticed:But if we see multiple discovery happening, it's a strong indication that the thing being discovered is in enough demand that it's going to keep being discovered, at least until knowledge of it "
  },
  {
    "title": "Donkey Kong Country 2 and Open Bus (jsgroth.dev)",
    "points": 192,
    "submitter": "colejohnson66",
    "submit_time": "2025-06-30T15:01:12 1751295672",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=44424194",
    "comments": [
      "I have to say as a 6502 assembly programmer I have wasted many hours of my life tracking down the same issue in my code (forgetting to put an # in front of an immediate value and thus accidentally doing a memory access instead).  Often it's like this case too where things might accidentally work some of the time.Worse than the floating-bus in this example is when it depends on uninitialized RAM which is often consistent based on DRAM so the code will always work on your machine/emulator but won't on someone else's machine with different DRAM chips (invariably you catch this at a demoparty when it won't run on the party machine and you only have 15 minutes to fix it before your demo is about to be presented)reply",
      "Was there ever an architecture that used dynamic memory with a 6502 CPU?  In my (limited?) experience, that platform always had static RAM.reply",
      "The mid-1980s Acorn 8-bit range all used dynamic RAM for the onboard memory.The BBC Micro range all had 250 ns DRAM, with the CPU getting 2e6 accesses and the video getting the other 2e6 (taking advantage of the 6502's predictable RAM access rate). The display memory fetches served to refresh the RAM.I don't know much about the Acorn Electron, which was very different internally, but it had dynamic RAM as well. I expect the video refresh was used to refresh the DRAM in this case too - as the display memory layout was the same, and so over every 640 microsec it would touch every possible address LSB.The 6502 second processor had DRAM as well, refreshed by a circuit that ran on a timer and stole the occasional cycle from the CPU at some rate.Though static RAM was quite common for RAM upgrade boards (of one kind or another), presumably cheaper for this case than the alternative.reply",
      "Most of them.  Static RAM was (and still is) more expensive since it needs more transistors and chip area per bit stored.  It it, however, also much easier to interface since it doesn't need refresh circuitry.  This is why you see it in the earliest designs, and also why you see it in so many hobbyist designs.  It's also why you tend to see it in the video systems even if the rest of the machine uses DRAM.  Dealing with DRAM refresh while reading out the whole memory chip sequentially (while also having a second port to read/write from the CPU!) starts making things very complicated.But still DRAM is what you would use for a \"real\" system.  Wozniak's design for the Apple II used a clever hack where the system actually runs at 2 MHz with an effective CPU rate of 1 MHz.  Any read from a DRAM row will refresh the entire row.  Approximately every other cycle the video system steps incrementally through memory, refreshing as it goes.reply",
      "Same with the VIC-II and the 6510 in the Commodore 64. The video chip is given the main character role for the bus, stopping the CPU from moving forward if it needs cycles for video generation or DRAM refresh.reply",
      "Well, the SNES - if that counts, it's a 65816 - uses DRAM. This is especially noteworthy because the DRAM refresh is actually visible on-screen on some units:https://www.retrorgb.com/snesverticalline.htmlreply",
      "I think you'll find more systems used DRAM than SRAM.The Apple II was one of the first 6502 systems to use DRAM (in 1977) and Woz was incredibly clever in getting the refresh for free as a side effect of the video generationreply",
      "There must have been computers with 6502 and DRAM.For higher memory capacities, e.g. 32 kB, 48 kB or 64 kB, static RAM would have been too expensive and too big, even if 6502 did not have an integrated DRAM controller, like Zilog Z80.Using SRAM instead of DRAM meant using 4 times more IC packages, e.g, 32 packages instead of 8. The additional DRAM controller required by DRAM would have needed only 1 to 4 additional IC packages. Frequently the display controller could be used to also ensure DRAM refresh.reply",
      "Are you thinking of SDRAM (a type of DRAM)?reply",
      "I appreciate all of the responses.  I did development on a KIM-1 and I owned a SYM-1.  Both of these used static RAM.  I expanded the RAM in my SYM-1 from 4K to 8K (with eight 2114 static RAM chips).  I never owned any other 6502 based computers.reply"
    ],
    "link": "https://jsgroth.dev/blog/posts/dkc2-open-bus/",
    "first_paragraph": "Donkey Kong Country 2 has a pretty well-known bug in the old SNES emulator ZSNES where some stages have spinning barrels that don\u2019t work properly. One of the earliest pictured here, in the first stage of Krem Quay (third world):Barrel BayouAfter you jump into the barrel, you\u2019re supposed to be able to completely control its rotation by pressing left and right on the d-pad, with the barrel only rotating while you\u2019re holding left or right. In ZSNES, this is horribly bugged. Tapping left or right makes the barrel spin forever in that direction, until you press the opposite direction\u2026which simply makes it spin forever in the opposite direction.This is more than just annoying - it makes these stages significantly more difficult than the developers intended, since later on the spinning barrels show up over spikes and other hazards:Klobber KarnageThis used to be somewhat documented in threads on the ZSNES forums, but those unfortunately seem to have gone offline since last I looked at them, an"
  },
  {
    "title": "End of an Era (erasmatazz.com)",
    "points": 77,
    "submitter": "marcusestes",
    "submit_time": "2025-06-30T19:17:11 1751311031",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=44426845",
    "comments": [
      "Crawford's work is worthy of study, as is the causation for why he experienced external failure. It embodies the \"simulationist\" aesthetic of game design: given enough modelled parameters, something emergent and interesting will happen. This was a trend of the 20th century: computers were new and interesting, and simulations did work when you asked them to solve physics problems and plan logistics. Why wouldn't it work for narrative?But then you play the games, and they're all so opaque. You have no idea what's going on, and the responses to your actions are so hard to grasp. But if you do figure it out, the model usually collapses into a linear, repeatable strategy and the illusion of depth disappears. You can see this happening from the start, with Gossip. Instead of noticing that his game didn't communicate and looking for points of accessibility, he plunged further forward into computer modelling. The failure is one of verisimilitude: The model is similar to a grounded truth on paper, but it's uninteresting to behold because it doesn't lead to a coherent whole. It just reflects the designer's thoughts on \"this is how the world should work\", which is something that can be found in any comments section.Often, when Crawford lectured, he would go into evo-psych theories to build his claims: that is, he was confident that the answers he already accepted about the world and society were the correct ones, and the games were a matter of illustration. He was likewise confident that a shooting game would be less thoughtful than a turn-based strategy game because the moment-to-moment decisions were less complex, and the goal should be to portray completeness in the details.I think he's aware of some of this, but he's a stubborn guy.reply",
      "This is evident in his description of programming in his later years:Time and time again I would send my friend Dave Walker an email declaring that Javascript (or something else) was utterly broken, incapable of executing the simplest program without errors. Dave would ask to see the source code and I would present it to him with detailed notes proving that my code was perfect and Javascript was broken. He\u2019d call me, we\u2019d discuss it, and eventually he\u2019d say something like, \u201cWhere did you terminate the loop beginning at line 563?\u201d There would be a long silence, followed by the tiniest \u201cOh\u201d from me. I\u2019d thank him for his help and hang up. A week later, I\u2019d be fuming again about another fundamental flaw in Javascript.Many of us are stubborn and will work hard and long, without much positive external feedback, under the assumption that our vision is correct and the audience, if one even exists, is wrong. Much fundamental progress has been made this way: Faraday, Einstein, Jobs, etc. But of course many times one simply is wrong and refusing to see it means throwing years away, and whatever else with it (money, relationships, etc.). It's a hard balance, especially for the monomaniacal without much interest in balance. Finding out how to make solid (public, peer-reviewed, evidence-based, whatever) incremental progress towards the paradigm shift seems to be the way if one can manage.reply",
      "That quote about JavaScript is... huh. I do not understand how you can even begin coming to the conclusion of \"JavaScript [is] utterly broken, incapable of executing the simplest programs without errors\" when obviously, JavaScript (which I do not like, by the way) is productively used on a large scale (even back then), and constantly under scrutiny from programmers, computer scientists, language designers... it's just baffling.It reminds me of when I was around 10 years old or so, maybe slightly older, and playing around with Turbo C (or maybe Turbo C++) on DOS. I must have gotten something very basic about pointers (which were new to me at the time) wrong, probably having declared a char* pointer but not actually allocated any memory, leaving it entirely uninitialized, and my string manipulation failed in weird and interesting ways (since this was on DOS without memory protection, you wouldn't get a program crash like a segmentation fault very easily, instead you'd often see \"more interesting\" corruption).Hilariously, at the time I concluded that the string functions of Turbo C(++) must be broken and moved away \"string.h\" so I wouldn't use it. But even then I shortly after realized how insane I was: Borland could never sell Turbo C(++) if the functions behind the string.h API were actually broken, and it became clear that my code must be buggy instead. And remember, I was 10 years old or so, otherwise I don't think I would have come to that weird conclusion in the first place.Nowadays, I do live in this very tiny niche where I actually encounter not only compiler bugs, but actual hardware/CPU bugs, but even then I need a lot of experiments and evidence for myself that that's what I'm actually hitting...reply",
      "\"Am I so out of touch? No, it's the audience who's wrong!\"reply",
      "Crawford's work that I'm most familiar with is a game called Balance of Power -https://en.wikipedia.org/wiki/Balance_of_Power_(video_game)I played it as a cold war kid and was fascinated by it. Mid 80's, post War-Games, this game blew my mind. It simulated the world.The lesson I remember was that conflict in the Cold War was not zero-sum. One side would win and one side would lose. There were (in this game) no win-win outcomes. But - and this is the key point - the value of each win or loss was unequally felt. For the US to back down in Indonesia was disappointing. To back down in West Germany was fatal.Oh - and also the notion of graduated escalation & de-escalation. Playing the game well requried using escalation wisely. Sometimes you escalate (a bit) to see how they respond & judge the value of a conflict to your opponent. Sometimes you escalate (a lot) to signal to your opponent that a given conflict is very serious to you.I don't know if I ever had _fun_ playing the game - but of the hundreds of games I played as a kid this one stuck with me.All this with something like 64k of memory - brilliant!reply",
      "I was not familiar with Chris Crawford other than vaguely being aware of the name. Reading this post and others on the website (like https://web.archive.org/web/20180820035048/http://www.erasma...), it\u2019s hard to not get the overall picture of \u201cperson says everyone else is doing it wrong, without having done it right themselves\u201d.What I mean by that is that there are game designers like Jonathan Blow who have their own theories on what is a great game and are extremely critical of the industry not following those theories, and then have released games that succeed at demonstrating those theories. In Jonathan Blow\u2019s case, you can disagree with the man, but you can\u2019t disagree with the fact that The Witness is a wildly original, successful game (1M+ copies sold) that has a cult following.That does not seem to be the case for Crawford\u2019s work. Lots of theories, lots of indictment for the industry doing it wrong, but no actual demonstration of what \u201cdoing it right\u201d would mean.Saying that no one gets it and civilization won\u2019t be ready for many centuries (as the article I linked above does) feels like kind of a cheap rhetorical cop out.For what it\u2019s worth, I disagree with his indictment of the video game landscape as being narratively poor. Lots of video games with great interactive narratives out there, and there are many players who have been deeply moved by such games (of course, which games that might be varies from person to person).I think a good antidote when one finds themselves in those thinking patterns is to listen to what others have to say, and not dismiss them as not getting it because they don\u2019t follow your particular (unproven) theories.reply",
      "> I disagree with his indictment of the video game landscape as being narratively poor.I think he would say they are narratively poor by his defintion that the narrative must be generated by the game/player combo and not just pre-programmed. People love \"The Last of Us\" for it's narrative but that narritive is something that can arguably be conveyed via book or movie. Crawford wanted something where the narrative itself was generated.And no, he wouldn't count the choices players make in the average game. Whether to get go west or east. Whether to get the a sword first or the arrow. He wanted the story and character dialog to change. Few if any games do that. Of course today with LLMs it's likely some games will soon / have already done it to some degree and will do better in the future.Going back to his older work, you'd need to feed a context to the LLM about each characters motivations and then update that context based on player actions so that as the game progresses the way each NPC interacts with the player, and other NPCs, changes in a way that's consistent with each character's intrisict motivations and their interactions with others.reply",
      "This feels quite sad.Someone who clearly wanted to make a difference, but mostly seems to have not just made games.He made game tools, but then didn't actually use them to make games. And then he blamed everyone else for not being ready for what he was making.Giving up after only one released work just seems like such a shame.reply",
      "The posts author and site operator is Chris Crawford [0]. He said so in the post but Wikipedia confirms that he was mostly active from 1980s to 1990s with at least 15 titles to his name, not including other tools that he built and not including game design books that he authored or wrote for.[0] https://en.wikipedia.org/wiki/Chris_Crawford_(game_designer)reply",
      "This feels quite sad.A whole person -- flattened into little bits gleaned from some text, glued together with assumptions and world-building -- dismissed as \"blaming\" and \"giving up\" \"after one game\"The YouTube link in the other post has a top comment of \"The best speech in all of gaming history delivered by what must be considered the Socrates of gaming.\", to give you a sense of there may be more depth to this person than \"giving up after 1 game\".If nothing else, it indicates the crowd perceives more depth, which will be enough to make you ponder if you missed something.I suggest re-reading the article with a different set of assumptions -- when faced with a contradiction, first, check your premises -- it's likely the guy worried about declining programming skills and pointing out the ease at which he was dismissing JavaScript due to simple errors, is being self-aware and sarcastic.Once you're freed via engaging with your own thinking, instead of rushing to do public judgement, it is a quite beautiful meditation of working on something that fails to get the mindshare you hoped for, and a all-too-familiar to all of us reminder of the cognitive dissonance required to be okay with that, even when you'll never be okay.reply"
    ],
    "link": "https://www.erasmatazz.com/personal/self/end-of-an-era.html",
    "first_paragraph": ""
  },
  {
    "title": "GPEmu: A GPU emulator for rapid, low-cost deep learning prototyping [pdf] (vldb.org)",
    "points": 18,
    "submitter": "matt_d",
    "submit_time": "2025-06-30T22:37:06 1751323026",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://vldb.org/pvldb/vol18/p1919-wang.pdf",
    "first_paragraph": ""
  },
  {
    "title": "The Original LZEXE (A.K.A. Kosinski) Compressor Source Code Has Been Released (clownacy.wordpress.com)",
    "points": 56,
    "submitter": "elvis70",
    "submit_time": "2025-06-30T19:19:52 1751311192",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=44426864",
    "comments": [
      "Whow, another well-known piece of software that was written by Fabrice Bellard. He's also the original author of qemu, tinyemu, tcc, ffmpeg and many more.https://bellard.orgreply",
      "One of the most influential programmers of our time, if not the most.reply",
      "His track record is exceptional, he must be a Godlike programmer!reply"
    ],
    "link": "https://clownacy.wordpress.com/2025/05/24/the-original-lzexe-a-k-a-kosinski-compressor-source-code-has-been-released/",
    "first_paragraph": "Clownacy's CornerLast year, I discovered that the Kosinski compression format is actually LZEXE, which was used for compressing DOS executables back in the 90s and the late 80s. Its developer catalogues three versions on his website: v0.90, v0.91, and v0.91e. While only binaries of v0.91 and v0.91e can be found on the website, v0.90 can be found mirrored on various other websites.I got in touch with LZEXE\u2019s developer, Fabrice Bellard, and he was able to release LZEXE\u2019s source code, untouched since 1990! It is released under the terms of the MIT licence, allowing it to be freely used in other projects. To maximise performance, the compression logic was written in x86 assembly, while its frontend was written in Pascal. This particular source code appears to be for v0.91.Back in 2021, I made my own Kosinski compressor which produced identical data to what could be found in the Mega Drive Sonic games. At the time, I noticed that it did not accurately reproduce the Mega CD BIOS\u2019s compressed"
  },
  {
    "title": "Jim Boddie codeveloped the first successful DSP at Bell Labs (ieee.org)",
    "points": 16,
    "submitter": "jnord",
    "submit_time": "2025-06-30T22:04:53 1751321093",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://spectrum.ieee.org/dsp-pioneer-jim-boddie",
    "first_paragraph": "He codeveloped the first successful DSP at Bell LabsSteve Walters worked closely with Jim Boddie on the Bell Labs DSP1, designing the I/O circuitry as well as the DSP development system.Pat Hays enjoyed a long career as a lead developer of computer chips at Bell Labs in Holmdel, N.J., and is the author of Silicon Planet: My Life in Computer Chips.While working as an architect and designer at AT&T Bell Laboratories in Holmdel, N.J., Jim Boddie codeveloped a new type of semiconductor: the DSP. James R. \u201cJim\u201d Boddie, a pioneer of the programmable, single-chip digital signal processor, died on 2 December at his home in Canton, Ga., following a long illness. The IEEE senior member was 74.While working as an architect and designer at AT&T Bell Laboratories in Holmdel, N.J., Boddie applied his expertise in signal processing\u00a0algorithms to develop a new type of semiconductor: the DSP. The integrated circuit, which Bell Labs called DSP1, was announced at the 1980 International Solid-State Circui"
  },
  {
    "title": "Show HN: TokenDagger \u2013 A tokenizer faster than OpenAI's Tiktoken (github.com/m4thyou)",
    "points": 245,
    "submitter": "matthewolfe",
    "submit_time": "2025-06-30T12:33:58 1751286838",
    "num_comments": 67,
    "comments_url": "https://news.ycombinator.com/item?id=44422480",
    "comments": [
      "Kudos, I think (in the short term at least) there is a large amount of perf. optimization to be found by coding parts of the whole AI/ML infrastructure in C++ like this one, not as a rewrite (god no!) but drop in and fix key bottlenecks. Anytime I see someone (seems Chinese engineers are good at this) put something out in C++, good chance some solid engineering tradeoffs have been made and dramatic improvement will be seen.reply",
      "Agreed. A former mentor of mine told me a nice way of viewing software development:1. Make it work.\n2. Make it fast.\n3. Make it pretty.Transformers & LLMs have been developed to a point where they work quite well. I feel as though we're at a stage where most substantial progress is being made on the performance side.reply",
      "Heh, seems people I've been learning from been biased away from beauty, as I know that as \"Make It Work, Make It Right, Make It Fast\".reply",
      "I've usually heard/said it as  1. Make it\n  2. Make it work\n  3. Make it work better\n\n(different circumstances have different nuances about what \"better\" means, it isn't always performance optimization; some do substitute \"faster\" for \"better\" here, but I think it loses generality then).reply",
      "I've always heard it (and said it) as:  1. Make it work\n  2. Make it correct\n  3. Make it fastreply",
      "I always heard the \"Make it Right\" as \"Make it Beautiful\", where Right and Beautiful would mean \"non-hacky, easily maintainable, easily extendable, well tested, and well documented\"reply",
      "Fair chance I'm remembering it wrong :Dreply",
      "What's the difference between make it work and make it right? Aren't they the same thing?reply",
      "> make it work and make it right?My mentor used say it is the difference between a screw and glue.You can glue some things together and prove that it works, but eventually you learn that anytime you had to break something to fix it, you should've used a screw.It is trade off in coupling - the glue binds tightly over the entire surface but a screw concentrates the loads, so needs maintenance to stay tight.You only really know which is \"right\" it if you test it to destruction.All of that advice is probably sounding date now, even in material science the glue might be winning (see the Tesla bumper or Lotus Elise bonding videos - every screw is extra grams).reply",
      "Making it work can be a hacky, tech debt laden implementation. Making it right involves refactoring/rewriting with an eye towards maintainability, testability, etc etcreply"
    ],
    "link": "https://github.com/M4THYOU/TokenDagger",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        High-Performance Implementation of OpenAI's TikToken.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\nA fast, drop-in implementation of OpenAI's TikToken, designed for large-scale text processing. 2x Throughput and 4x faster on code sample tokenization.Performed on an AMD EPYC 4584PX - 16c/32t - 4.2 GHz.And optionally for running the tests:\n        High-Performance Implementation of OpenAI's TikToken.\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page."
  },
  {
    "title": "Entropy of a Mixture (cgad.ski)",
    "points": 28,
    "submitter": "cgadski",
    "submit_time": "2025-06-30T21:11:30 1751317890",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=44427929",
    "comments": [
      "Really nice presentation of divergences. I made a similar attempt (math + interactive JS visualizations) a long time ago for the Bregman divergences: https://mark.reid.name/blog/meet-the-bregman-divergences.htm...These visualizations are much nicer than mine though.Curious fact, the Bregman divergences are a different class of divergences to the f-divergences that intersect at the KL divergence. That is, KL is (essentially) the only divergence that is both an f-divergence and a Bregman divergence. This is basically because log turns a ratio into a difference.reply",
      "Forgive the meta comment, but is the source for your blog template available? I admire the presentation.reply",
      "Looks custom with simple HTML & CSS, KaTex for LaTex rendering, and Haxe 4.3.6 for the interactive charts. Never thought to use Haxe in such a way!reply"
    ],
    "link": "https://cgad.ski/blog/entropy-of-a-mixture.html",
    "first_paragraph": "Given a pair (p0,p1)(p_0, p_1)(p0\u200b,p1\u200b) of probability density functions and an interpolation factor \u03bb\u2208[0,1],\\lambda \\in [0, 1],\u03bb\u2208[0,1], consider the mixture p\u03bb=(1\u2212\u03bb)p0+\u03bbp1.p_\\lambda = (1 - \\lambda) p_0 + \\lambda p_1.p\u03bb\u200b=(1\u2212\u03bb)p0\u200b+\u03bbp1\u200b. How does the entropy\nH(p\u03bb)=\u2212E\u2061X\u223cp\u03bbln\u2061p\u03bb(X)H(p_\\lambda) = -\\E_{X \\sim p_\\lambda} \\ln p_\\lambda(X)H(p\u03bb\u200b)=\u2212EX\u223cp\u03bb\u200b\u200blnp\u03bb\u200b(X)\nof this mixture vary as a function of \u03bb\\lambda\u03bb? The widget below shows what this situation looks like for a pair of discrete distributions on the plane. (Drag to adjust \u03bb.\\lambda.\u03bb.)It turns out that entropy is concave as a function of probabilities. This explains the \"upward bulge\" of our curve. It might also be intuitive to you that, when p0p_0p0\u200b and p1p_1p1\u200b are less similar, this curve should bulge upwards more. What exactly does its shape tell us? In this short post, we'll see how questions about H(p\u03bb)H(p_\\lambda)H(p\u03bb\u200b) lead to information about the relationship between p0p_0p0\u200b and p1.p_1.p1\u200b. For example, we'll see how JSD dive"
  },
  {
    "title": "They don't make 'em like that any more: Sony DTC-700 audio DAT player/recorder (kevinboone.me)",
    "points": 74,
    "submitter": "naves",
    "submit_time": "2025-06-30T18:03:26 1751306606",
    "num_comments": 62,
    "comments_url": "https://news.ycombinator.com/item?id=44426171",
    "comments": [
      "\"In the the late 80s it wasn\u2019t easy to copy a CD onto DAT, because of the different sampling rates.\"At that point nobody would have worried about using the analog inputs to do the copy. The quality was such a leap from cassette that nobody would quibble about an analog stage.As usual, the record companies' and Congress's behavior in the DAT case ripped off everyone and ruined an entire product. The lie of \"perfect digital copies causing piracy\" was gobbled up by a legislature of out-of-touch geezers eager to serve corporate interests, when everyone with a brain knew that all \"piracy\" was taking place on double-cassette boom boxes in dorm rooms. Statistically nobody copying music gave a shit about quality.And sure enough, when MP3 came along it further proved the point by being a glaringly IMperfect digital copy. So all the audiophiles, home musicians, and indie bands who would have built the market for DAT got screwed by media conglomerates' lies and Congress.And oh yeah, that asinine tax on blank media: I would have then made the argument that by paying it, I paid for a license to copy whatever I wanted.Anyone old enough might remember that Best Buy and Circuit City advertised \"any CD $10.99 or less\" when they were typically $16. Then, all of a sudden, that deal disappeared... to the point that employees even feigned ignorance. Why?It turns out that record companies had colluded and strong-armed retailers into rescinding this pricing. They were later found to have illegally ripped consumers off for $400 million (if I remember correctly), which coincidentally was the exact amount they were whining that Napster cost them. I still have a copy of the $13 check I received from these lowlives.DAT stands testament to the relentless ripping-off of the American consumer, under the cover of absurd lies.reply",
      "> ... VHS players rapidly became throw-away items \u2013 eventually nobody really cared if they only lasted a year or two.I don't know if I'm losing my marbles, but I don't ever recall a time growing up when my family (or anyone else I knew) were buying a new VCR every year or two.reply",
      "Early 2000s. My family used VHS until after the switch to digital TV. Not that we would buy one new, but if we found one at a garage sale for a couple bucks we would take it. Used to have a stock of 2 or 3 on hand at a time. They were all late 90's / early 2000s models that everyone was dropping in favor of DVDs, made as cheap as possible, and would quit working in about 8-10 months. Which meant I got to take apart the broken one - I recall taking apart around a dozen, but some of those were already broken and found in the trash.Meanwhile, the \"basement\" VCR my dad bought new in '85 still works to this day, but that one was less programmable, so we always used the cheap ones to record off the air.reply",
      "I vividly remember the day when at age 10 my grandfather let me disassemble a broken VCR. It is the day I learned to treat electronics with large capacitors with respect.reply",
      "My lesson was a disposable film camera... The flash cap gave quite a joltreply",
      "That is unfortunately my experience. My household between ~2005 and ~2015 acquired a VCR every year or so, keeping pace with the rate at which they would pack up. These were second-hand machines at the end of their life, so although I wouldn't say we \"didn't care\" when disposing of them, it was with a sense of resignation as we knew that repairing them was beyond our collective skill and equipment.At an ambient relative humidity of 90%, the tapes themselves would become mouldy at an alarming rate. We did therefore check for mould before playing them, as this could have rubbed off onto the VCRs and then might have spread to other tapes.reply",
      "All of our VCRs lasted a very long time. My parents had a Toshiba VCR from the late 1980s as well as a Sony Hi-Fi model VCR from 1995, both of which lasted for years and years, even in spite of damage and neglect from use (and misuse) by young children.reply",
      "I think this is plausible as we got into the late 90s and they became cheap enough to not warrant repairs. Prior to that, though, it definitely was not the case. I had a neighbor who made quite a comfortable living repairing VCRs in the 80s and early 90s. I also saw the web for the first time in his shop in 93/94.reply",
      "Today I think of VHS as ideal for people who want to get into an obsolete format.  I often see decks for sale for $12 that work great at our reuse center and prerecorded tapes with great moves up to 2005 or so are $1-2 there or the Salvation Army.  The decks I see are late models which have automatic tracking and VHS HiFi and are highly reliable -- commercial movies are usually encoded in Dolby Pro Logic and often sound more cinematic than many DVDs because the average DVD has a NERFed 5.1 track because they assume you're going to play it on a two-channel system.reply",
      "Obsolete formats (especially with high performance mechanics) are fun, but VHS picture quality isn't. My idea of fun would be to try to get the best picture quality possible by throwing appropriate digital encoding + error correction + compression at the problem - the more anachronistic, the better.We have crazy powerful DSPs (like a low end GPU), advances in coding and error correction codes, and highly advanced lossy compression algorithms now 8)Previously on HN: film on vinyl LP (pretty terrible, not much to work with), super high quality VHS reading by hooking up ADCs directly to the video heads + software, and VHS tape streamers (IIRC 1-2 GB with circa 1993 cheap hardware).reply"
    ],
    "link": "https://kevinboone.me/dtc-700.html",
    "first_paragraph": "Don\u2019t let anyone tell you otherwise: DAT players were fantastic. They\noffered all the advantages of an audio cassette, but with the sound\nquality of a CD. The compact audio cassette was a marvellous invention,\nin its own way; but this technology struggled to provide audio fidelity\nthat would satisfy discerning listeners. Its frequency response was\nlimited, and the unavoidable background hiss was very obvious in quiet\nenvironments. Still, in the 1970s audio cassettes were the way\nmost people listened to music, and I still have a stack of them.One thing that made cassettes so popular was that you could record on\nthem. Setting aside the legal issues, you could record from FM radio, or\nfrom vinyl records, or even from microphones. It was easy to make \u2018mix\ntapes\u2019 of you favourite tracks, and share them with friends. Cassettes\nwere everywhere \u2013 from portable players like the Walkman, to serious\nhardware in hi-fi racks; they were even in cars.\nThere were shops that sold nothing but cassettes,"
  },
  {
    "title": "The Email Startup Graveyard: Why 80%+ of Email Companies Fail (forwardemail.net)",
    "points": 6,
    "submitter": "skeptrune",
    "submit_time": "2025-07-01T00:41:04 1751330464",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail",
    "first_paragraph": "While countless email startups have burned through millions solving non-existent problems, we at Forward Email have quietly built actual email infrastructure from scratch since 2017\u2014proving that sustainable email services require engineering, not just venture capital.  This is an exhaustive analysis of email startup failures, acquisitions, and the fundamental misunderstanding of what email actually is.WarningTL;DR: Stop building email apps. Email isn't broken. This post documents why 80%+ of email startups fail and why you shouldn't be the next one.NoteKey Insight: Most email startups don't build actual email infrastructure from scratch. They're just glue on top of Amazon SES or leveraging existing open-source solutions like Cyrus/Postfix (Fastmail, Resend, etc.). The infrastructure works perfectly - the problem is thinking it needs \"improvement.\"Here's every major email startup failure we could find, organized by accelerator, funding, and outcome:CautionFailure Rate: Techstars alone h"
  },
  {
    "title": "Price of rice in Japan falls below \u00a54k per 5kg (japantimes.co.jp)",
    "points": 71,
    "submitter": "PaulHoule",
    "submit_time": "2025-06-30T19:54:11 1751313251",
    "num_comments": 99,
    "comments_url": "https://news.ycombinator.com/item?id=44427147",
    "comments": [
      "The fact the average Japanese person won't even consider trying imported Japonica rice from Australia or USA is madness if budget is a consideration.But as someone who's tried many varieties of Japonica, there is a difference between the best Japan-grown rice and non-speciality rice grown elsewhere, as well as a difference between fresh (Japanese enjoy eating new rice, which is different from many rice-eating cultures) and old rice.I pay somewhere around AUD$14/kg for Japanese rice in Australia, but I also don't eat it that often and I'm not that price sensitive.But also, the average Japanese eats around 1kg of uncooked rice per week. That's \u00a5800 at the rates in the article (~USD$300/year). Japan's cost of living is generally pretty low, but I doubt +/- $100/year is effecting many people.reply",
      "From what I've seen at the supermarket, cal-rose, rice from California does sell out and get restocked with reasonable quantity. While I can't be sure it's \"average Japanese people\" buying it I see no reason to believe otherwise. It's been shown on TV a few times too, the sure-fire average marketing in Japan.reply",
      "At least for supermarkets around me they _only_ sell Japanese rice. You\u2019d have to buy online or drive out which doesn\u2019t work for vast majority of old population.Also as an East Asian I can somewhat understand reluctance to change rice. It\u2019s just such a staple in your daily life. If I had eaten one type of rice for my entire life (and the price of the rice has remained stable for the last 40 years) and suddenly I can\u2019t afford that type of rice, it would be a shock.reply",
      "There are so many restaurants that talk about using the best rice in Japan.On asking them where it comes from and it's always either local to the restaurant, or to the prefecture the owner grew up in.There's a lot of local patriotism for rice in Japan, I even find it admirable most of the time.Rice price is obviously important and probably linked pretty highly with inflation numbers in Japan. It's price is currently being artificially manipulated higher by JA, and that sucks. But I think ultimately most of these articles and even the local discourse with other Japanese is just a socially acceptable topic to grumble about.reply",
      "Rice patriotism in Japan is just result of politically motivated and generally unpopular gen-tan policy spectacularly backfiring. The price is not artificially inflated, at least not by JA, it's just result of forcing rice to be uneconomical in the market where people just buys rice no matter what. As supply is decreased artificially and price increases in response, unit price rose and so did quality. As result of quality increased artificially although indirectly, imports unaffected by local market further lost competitiveness.I bet Japan's going to be internally forced to expand rice exports, not imports. Farmers must pay bills or else they go away, and people aren't going to eat imports. So farmers has to stay by either completely subsidized a la defense production, or by scaling out at paces permanently exceeding inflation. Maybe both. Extra production has to go somewhere but population is on decline, so it has to go somewhere beyond the seas.Unless iPhone moment happens and the country's hit by vastly superior rice, which iOS was - iPhone had worse hardware with worse integration than any of local phones back then, but UI/UX was literally 5-10 years ahead of everything else, and it completely replaced the entire Japanese phone market helped by blatantly illegal marketing tactics. That's not happening so far with rice.reply",
      "I think it's only Aeon that is pushing imported rice. While I see them prop up everywhere lately, indeed if none in the area I guess it's not readily available. Where it is though it doesn't seem unpopular, at least it isn't stuck on shelves. When I first saw it disappear, I thought it may have been a failed experiment but was happy to see it restocked.reply",
      "> The fact the average Japanese person won't even consider trying imported Japonica rice from Australia or USA is madness if budget is a consideration.I live in Japan, and my girlfriend is an atypical Japanese that doesn't like rice that much. For her, the madness is that people here won't even consider other sources of carbs like pasta, potatoes, or bread.reply",
      "I am not Japanese, and don\u2019t eat a lot of rice (twice a week maybe), but I don\u2019t think it\u2019s madness.If you told me to eat rice instead of bread I would probably be just as horrifiedreply",
      "Japan forces this to be the case with extremely high tariffs on rice imports. It's not that they won't consider it, they literally can't.reply",
      "I see cal-rose for shy above 3000 yen. While I didn't grasp the details, I saw a news report which made it seems they've set up a quota (xx tons or something) of rice to be imported with almost no tariffs.reply"
    ],
    "link": "https://www.japantimes.co.jp/news/2025/06/24/japan/japan-rice-price-falls-below-4000/",
    "first_paragraph": "SubscribeToday's print editionHome DeliveryThe average price of rice sold at about 1,000 supermarkets across Japan in the week through June 15 stood at \u00a53,920 per 5 kilograms, slipping below \u00a54,000 for the first time since the week that ended on March 2, the agriculture ministry said Monday.The average price was down by \u00a5256 from the previous week, declining for the 4th straight week and marking the first drop exceeding \u00a5100 since March 2022, when the ministry started releasing weekly rice prices.Prime Minister Shigeru Ishiba had aimed to push down rice prices to the \u00a53,000-\u00a54,000 range by as early as mid-June.The significant drop likely reflected the distribution of government stockpiled rice released under discretionary contracts.Despite the fall, the average rice price was still \u00a51,772 higher than that of a year earlier.Agriculture minister Shinjiro Koizumi told reporters on Monday that the country has \"taken a step forward on a new stage toward curbing abnormally high rice prices.\""
  },
  {
    "title": "Creating fair dice from random objects (arstechnica.com)",
    "points": 30,
    "submitter": "epipolar",
    "submit_time": "2025-06-28T06:11:38 1751091098",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=44402618",
    "comments": [
      "It does not seem to be so useful and practical to use strange shapes for dice; the common shapes, with numbers (or other symbols that are applicable for the game you are playing) on each side, will probably be more useful, anyways. However, it might be interesting.Another reason to use dice for tabletop games is so that the game can be played without the use of a computer.When I play GURPS, I generally use different dice with each dice roll in order to try to mitigate some of the bias. (I don't know quite how much effective this really is, though.)reply",
      "The question I have is how stable are the probabilities over time? My guess is traditional dice are more physically robust to wear and degrade more gracefully.reply",
      "The Roman rock crystal icosahedron die in the Louvre would be nice:https://archimedes-lab.org/2021/07/15/amazing-roman-rock-cry...reply",
      "How to create a fair coin from an arbitrarily biased coin:1. Toss the coin and remember the answer.2. Toss the coin again, if it is different from your previous toss then your result from #1 is fair. Otherwise, go back to step 1.If p is the probability of getting heads, there are four possible outcomes with their associated probabilities:    TT -> (1 - p)^2   (rejected)\n    HT -> p * (1 - p)\n    TH -> (1 - p) * p\n    TT -> p^2         (rejected)\n\nNeedless to say, p * (1 - p) and (1 - p) * p have an equal probability, so if we don't reject our two tosses, we have a fair outcome.reply",
      "\"arbitrarily\" is doing some heavy lifting!I'm not sure that two concurrent harmonious answers constitutes a \"fixed\" coin or a diagnosis of a fixed coin.This scheme will be rubbish with a one sided coin ie the limit for \"arbitrary fixed coin\".reply",
      "Reference: https://en.wikipedia.org/wiki/Randomness_extractor#Von_Neuma...reply",
      "That's cute. intuitively, if two flips give different outcomes, it's fifty/fifty which would be first.reply",
      "Hey hey, it's Keenan Crane again :)reply",
      "the title is a classic quant interview problemthe basic idea is that, because multiplication commutes, probability of A then B is the same as probability of B then A, so long as they are independent events (rolling objects typically meets this criteria)so instead of using just A or just B, which might neither have 0.5 probability, you only count \"A then B\" and \"B then A\" as rollsand this trivially extends to constructing a fair N-sided die out of any arbitrarily biased die for any Nreply",
      "That isn't what the article is about at all. It's not even what the first paragraph is about.What they are doing is designing physical shapes that will have a specified probability of falling in different positions.What you are talking about is post processing a biased random signal to get a less biased signal.reply"
    ],
    "link": "https://arstechnica.com/science/2025/05/your-next-gaming-dice-could-be-shaped-like-a-dragon-or-armadillo/",
    "first_paragraph": "\n        Statistically, \"the real behavior of a rolling object is largely a function of its geometry.\"\n      Most people are familiar with conventional cubical six-sided dice, but there are also polyhedral versions like the 20-sided dice used in ancient Rome and to play Dungeons and Dragons. Researchers have figured out how to design dice with even more exotic shapes, like a kitten, a dragon, or an armadillo. And they are \"fair\" dice: Experiments with 3D-printed versions produced results that closely matched predicted random outcomes, according to a forthcoming paper currently in press at the journal ACM Transactions on Graphics.Dice are examples of so-called \"rigid bodies,\" broadly defined as shapes that move as one solid piece, with no need for bending or twisting. Such shapes \"are of scientific interest because they model so many of the phenomena we encounter in our daily lives: anything from the way your dishes roll around on the floor when you drop them, to how the gears in your w"
  },
  {
    "title": "Show HN: New Ens\u014d \u2013 first public beta (sonnet.io)",
    "points": 218,
    "submitter": "rpastuszak",
    "submit_time": "2025-06-30T11:02:55 1751281375",
    "num_comments": 81,
    "comments_url": "https://news.ycombinator.com/item?id=44421776",
    "comments": [
      "Very nice, I've been wanting to build something like this myself but haven't gotten to it. The coffee shop mode is great! My biggest feature request would be changing the font and cursor. The blinking cursor is both distracting and unnecessary as you should assume that you are at the end anyway (since you shouldn't edit)!reply",
      "I really want a fixed-width font. I know most people dislike writing prose with monospace fonts. But I'm a developer, and proportional fonts always feel wrong.reply",
      "Well, talk to a script writer, they only write on Courier typefacereply",
      "noted, thanks!I'm VERY conservative with adding new UI elements, especially those introducing new possible sources of distractions, so I might hide it behind a bunch of menus. That said, I've spent ages yak shaving / working on those problems already :)reply",
      "I've always felt that the best part of writing on a computer is the ability to edit while you write, however, I also understand that doesn't work at all for a lot of people, so I think this app is neat even though I personally wouldn't use it.reply",
      "Sometimes forcing yourself \"not to edit\" allows you to bring out things which are hard to catch and hide in the nooks and crannies of your mind.Brain dumping also works the same way. You write whatever you have in your mind, without even correcting spelling errors. It really brings out things you don't know they are there and bothering you or taking space.You should at least try once. Takes an hour or so.I also use a similar method for drafting my blog posts if I have the idea, but can't bring out the rest of the text.reply",
      "I've always felt the best part of writing on a computer is legibility :)reply",
      "Although, I don't think that Enso as a whole will work for me (I have a very different approach to writting); I love the idea of the coffee shop mode. Want to implement something like this for Obsidian now.reply",
      "This might be useful for a whole browser coffee shop mode:https://addons.mozilla.org/en-US/firefox/addon/obfuscator/reply",
      "I like that idea, too.When I\u2019m at home, I do most of my writing now with voice input. Would somebody please invent a sound cancellation device that will enable me to talk to my devices in coffee shops and on public transportation without being heard by others?reply"
    ],
    "link": "https://untested.sonnet.io/notes/new-enso-first-public-beta/",
    "first_paragraph": "Hi there,Look!The new version of Ens\u014d (codename: Occult Vampire Keanu) is available for public testing!Download it here\nThis is a temporary icon I used for testing. I am considering creating a simplified version of it. PS. here's the original image (on potato.horse, of course)Following MISS, my focus is on removing distractions over adding new features. This can be surprisingly challenging (e.g. how do I tell users about feature X or Y without breaking their flow?) but also gives me time to focus on polishing the app.(we will discuss these in more detail in future posts)Most of the UI has been moved to the application menu bar for easier discoverability and shortcut access. So far no one has missed the old inline UI, but you can read more about it towards the end of this note.We have 6 5\u00bd predefined themes focussed on accessibility and specific use patterns based on feedback I've collected over the years.5\u00bd and not 6 because one theme still needs some work. Is there a specific use case"
  },
  {
    "title": "Claude Code now supports Hooks (anthropic.com)",
    "points": 80,
    "submitter": "ramoz",
    "submit_time": "2025-07-01T00:01:15 1751328075",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=44429225",
    "comments": [
      "Really excited to see this implemented.Hooks will be important for \"context engineering\" and runtime verification of an agent's performance. This extends to things such as enterprise compliance and oversight of agentic behavior.Nice of Anthropic to have supported the idea of this feature from a github issue submission: https://github.com/anthropics/claude-code/issues/712reply",
      "It is indeed. I don't use Claude Code. I use Cline which is a VS Code extension (cline.bot).This is a pretty killer feature that I would expect to find in all the coding agents soon.reply",
      "Exit Code 2 Behavior\n    PreToolUse - Blocks the tool call, shows error to Claude\n\nThis is great, it means you can set up complex concrete rules about commands CC is allowed to run (and with what arguments), rather than trying to coax these via CLAUDE.md.E.g. you can allow    docker compose exec django python manage.py test\n\nbut prevent    docker compose exec django python manage.py makemigrationsreply",
      "You can already do this in .Claude/settings.jsonreply",
      "Ah you\u2019re right, but for more complex logic it\u2019s useful to be able to run it through a custom scriptreply",
      "This closes a big feature gap. One thing that may not be obvious is that because of the way Claude Code generates commits, regular Git hooks won\u2019t work. (At least, in most configurations.)We\u2019ve been using CLAUDE.md instructions to tell Claude to auto-format code with the Qlty CLI (https://github.com/qltysh/qlty) but Claude a bit hit and miss in following them. The determinism here is a win.It looks like the events that can be hooked are somewhat limited to start, and I wonder if they will make it easy to hook Git commit and Git push.reply",
      "Would love to see this in Cursor. My workaround right now is using a bunch of rules that sort of work some of the time.reply",
      "So, form my limited understanding, this doesn't take up context, it's something auto where you can configure per tool use, and not MCP that Claude decides \"when\" to run it?!reply",
      "This needs a way to match directories for changes in monorepos. E.g. run this linter only if there were changes in this directory.reply",
      "An abstraction via a script should work, right? They document that it pipes the JSON data to your command's stdin,  ```lint-monorepo.sh\n\n  # read that data\n  json_input=$(cat)\n\n  # do some parsing here with jq, get the file path (file_path)\n\n  if [$file_path\" == \"$dir1\"*]\n    run lint_for_dir1\n  ```reply"
    ],
    "link": "https://docs.anthropic.com/en/docs/claude-code/hooks",
    "first_paragraph": ""
  }
]