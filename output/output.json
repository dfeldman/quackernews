[
  {
    "title": "Google can keep its Chrome browser but will be barred from exclusive contracts (cnbc.com)",
    "points": 437,
    "submitter": "colesantiago",
    "submit_time": "2025-09-02T20:26:47 1756844807",
    "num_comments": 282,
    "comments_url": "https://news.ycombinator.com/item?id=45108548",
    "comments": [
      "By the way, a pet peeve of mine right now is that reporters covering court cases (and we have so many of public interest lately) never seem to simply paste the link to the online PDF decision/ruling for us all to read, right in the story.  (and another user here kindly did that for us below: https://storage.courtlistener.com/recap/gov.uscourts.dcd.223... )It seems such a simple step (they must have been using the ruling PDF to write the story) yet why is it always such a hassle for them to feel that they should link the original content?  I would rather be able to see the probably dozens of pages ruling with the full details rather than hear it secondhand from a reporter at this point.  It feels like they want to be the gatekeepers of information, and poor ones at that.I think it should be adopted as standard journalistic practice in fact -- reporting on court rulings must come with the PDF.Aside from that, it will be interesting to see on what grounds the judge decided that this particular data sharing remedy was the solution. Can anyone now simply claim they're a competitor and get access to Google's tons of data?I am not too familiar with antitrust precedent, but to what extent does the judge rule on how specific the data sharing need to be (what types of data, for what time span, how anonymized, etc. etc.) or appoint a special master?   Why is that up to the judge versus the FTC or whoever to propose?reply",
      "> By the way, a pet peeve of mine right now is that reporters covering court cases never seem to simply paste the link to the online PDF decision/ruling for us all to read right in the story.I presume that this falls under the same consideration as direct links to science papers in articles that are covering those releases. Far as I can tell, the central tactic for lowering bounce rate and increasing 'engagement' is to link out sparsely, and, ideally, not at all.I write articles on new research papers, and always provide a direct link to the PDF,; but nearly all major sites fail to do this, even when the paper turns out to be at Arxiv, or otherwise directly available (instead of having been an exclusive preview offered to the publication by the researchers, as often happens at more prominent publications such as Ars and The Register).In regard to the few publishers that do provide legal PDFs in articles, the solution I see most often is that the publication hosts the PDF itself, keeping the reader in their ecosystem. However, since external PDFs can get revised and taken down, this could also be a countermeasure against that.reply",
      "Articles about patent infringement are similarly annoying when the patent numbers aren't cited. This is basic 21st century journalism 101. We aren't limited to what fits on a broadside anymore.We need an AI driven extension that will insert the links. This would be a nice addition to Kagi as they could be trusted to not play SEO shenanigans.reply",
      "I think they're called broadsheets unless you're in the headline!reply",
      "I think one of the lessons of Wikipedia, is the more you link out the more they come back.People come to your site because it is useful. They are perfectly capable of leaving by themselves. They don't need a link to do so. Having links to relavent information that attracts readers back is well worth the cost of people following links out of your site.reply",
      "Interesting example, as Google used to link to Wikipedia much more prominently, then stopped doing that, which dropped Wikipedia's visitor counts a lot.  A very large percentage of Wikipedia's visits are Google referrals.Google shifted views that used to go to Wikipedia first to their in-house knowledge graph (high percentages of which are just Wikipedia content), then to the AI produced snippets.All to say, yes...Wikipedia's generosity with outbound links is part of the popularity.  But they still get hit by this \"engagement\" mentality from their traffic sources.reply",
      "Wikipedia eventually failed.reply",
      "What do you mean? It's one of the most popular sitesreply",
      "I won't call it dead, but it is declining.  Their various sources of traffic are now regurgitating Wikipedia Content (and other 3rd party sources) via uncited/unlinked AI \"blurbs\"...instead of presenting snippets of Wikipedia contents with links to Wikipedia to read more.It's not the only reason their traffic is declining, but it seems like a big one.reply",
      "Who cares if the traffic is declining? I don\u2019t find Wikipedia useful because it gets lots of visits, I find it useful for the information it contains.reply"
    ],
    "link": "https://www.cnbc.com/2025/09/02/google-antitrust-search-ruling.html",
    "first_paragraph": ""
  },
  {
    "title": "%CPU utilization is a lie (brendanlong.com)",
    "points": 39,
    "submitter": "BrendanLong",
    "submit_time": "2025-09-03T00:01:02 1756857662",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=45110688",
    "comments": [
      "Utilization is not a lie, it is a measurement of a well-defined quantity, but people make assumptions to extrapolate capacity models from it, and that is where reality diverges from expectations.Hyperthreading (SMT) and Turbo (clock scaling) are only a part of the variables causing non-linearity, there are a number of other resources that are shared across cores and \"run out\" as load increases, like memory bandwidth, interconnect capacity, processor caches. Some bottlenecks might come even from the software, like spinlocks, which have non-linear impact on utilization.Furthermore, most CPU utilization metrics average over very long windows, from several seconds to a minute, but what really matters for the performance of a latency-sensitive server happens in the time-scale of tens to hundreds of milliseconds, and a multi-second average will not distinguish a bursty behavior from a smooth one. The latter has likely much more capacity to scale up.Unfortunately, the suggested approach is not that accurate either, because it hinges on two inherently unstable concepts> Benchmark how much work your server can do before having errors or unacceptable latency.The measurement of this is extremely noisy, as you want to detect the point where the server starts becoming unstable. Even if you look at a very simple queueing theory model, the derivatives close to saturation explode, so any nondeterministic noise is extremely amplified.> Report how much work your server is currently doing.There is rarely a stable definition of \"work\". Is it RPS? Request cost can vary even throughout the day. Is it instructions? Same, the typical IPC can vary.Ultimately, the confidence intervals you get from the load testing approach might be as large as what you can get from building an empirical model from utilization measurement, as long as you measure your utilization correctly.reply",
      "This is bang on, you can't count the hyperthreads as double the performance, typically they are actually in practice only going to bring 15-30% if the job works well with it and their use will double the latency. Failing to account for loss in clockspeed as the core utilisation climbs is another way its not linear and in modern software for the desktop its really something to pay careful attention to.It should be possible from the information you can get on a CPU from the OS to better estimate utilisation involving at the very least these two factors. It becomes a bit more tricky to start to account for significantly going past the cache or available memory bandwidth and the potential drop in performance to existing threads that occurs from the increased pipeline stalls. But it can definitely be done better than it is currently.reply",
      "This hits so close to home. I once tried to explain to a manager that a server at 60% utilization had zero room left, and they looked at me like I had two heads. I wish I had this article back then!reply",
      "You also want to hit him with queueing theory.Up to a hair over 60% utilization the queuing delays on any work queue remain essentially negligible. At 70 they become noticeable, and at 80% they've doubled. And then it just turns into a shitshow from there on.The rule of thumb is 60% is zero, and 80% is the inflection point where delays go exponential.The biggest cluster I ran, we hit about 65% CPU at our target P95 time, which is pretty much right on the theoretical mark.reply",
      "A big part of this is that CPU utilization metrics are frequently averaged over a long period of time (like a minute), but if your SLO is 100 ms, what you care about is whether there's any ~100 ms period where CPU utilization is at 100%. Measuring p99 (or even p100) CPU utilization can make this a lot more visible.reply",
      "The way they refer to cores in their system is confusing and non-standard. The author talks about a 5900X as a 24 core machine and discusses as if there are 24 cores, 12 of which are piggybacking on the other 12. In reality, there are 24 hyperthreads that are pretty much pairwise symmetric that execute on top of 12 cores with two sets of instruction pipeline sharing same underlying functional units.reply",
      "Years ago, when trying to explain hyper threading to my brother, who doesn't have any specialized technical knowledge, he came up with the analogy that it's like 2-ply toilet paper. You don't quite have 24 distinct things, but you have 12 that are roughly twice as useful as the individual ones, although you can't really separate them and expect them to work right.reply",
      "Nah, it's easier than that. Putting two chefs in the same kitchen doesn't let you cook twice the amount of food in the same amount of time, because sometimes the two chefs need to use the same resource at the same time - e.g. sink, counter space, oven. But, the additional chef does improve the utilization of the kitchen equipment, leaving fewer things unused.reply",
      "How many times has hyperthreading been an actual performance benefit in processors? I cannot count how many times an article has come out saying you'll get better performance out of your <insert processor here> by turning off hyperthreading in the BIOS.It's gotta be at least 2 out of every 3 chip generations going back to the original implementation, where you're better off without it than with.reply",
      "For whatever it\u2019s worth, operational database systems (many users/connections, unpredictable access patterns) are beneficiaries of modern hyperthreading.I\u2019m familiar with one such system where the throughput benefit is ~15%, which is a big deal for a BIOS flag.IBM\u2019s POWER would have been discontinued a decade ago were it not for transactional database systems, and that architecture is heavily invested in SMT, up to 8-way(!)reply"
    ],
    "link": "https://www.brendanlong.com/cpu-utilization-is-a-lie.html",
    "first_paragraph": "   By Brendan Long on   September 02, 2025 I deal with a lot of servers at work, and one thing everyone wants to know about their servers is how close they are to being at max utilization. It should be easy, right? Just pull up top or another system monitor tool, look at network, memory and CPU utilization, and whichever one is the highest tells you how close you are to the limits.For example, this machine is at 50% CPU utilization, so it can probably do twice as much of whatever it's doing.And yet, whenever people actually try to project these numbers, they find that CPU utilization doesn't quite increase linearly. But how bad could it possibly be?To answer this question, I ran a bunch of stress tests and monitored both how much work they did and what the system-reported CPU utilization was, then graphed the results.SetupI vibe-coded a script that runs stress-ng in a loop, first using a worker for each core and attempting to run them each at different utilizations from 1% to 100%, the"
  },
  {
    "title": "This blog is running on a recycled Google Pixel 5 (2024) (ctms.me)",
    "points": 46,
    "submitter": "indigodaddy",
    "submit_time": "2025-09-02T22:58:23 1756853903",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=45110209",
    "comments": [
      "I like the idea of using old phones for infrastructure-lite applications, taking advantage of their low power requirements and built-in UPS (which, yes, has its own drawbacks which can only be mitigated to a certain degree).I have a number of old android devices that I'd like to use for ...something cool like this, but my existing homelab infra could just add an extra VM or container to do this without any likely additional power draw. It's still cool and I want to do it though.My only query about this cool project is why not wifi? Whilst I'm sure there's a good reason for the author (and I can understand having esoteric specific requirements because I have my own \"things\"), but it would negate the need for a docking->Ethernet device, which feels to me like unnecessary addition of a device that requires power. Also, bandwidth / throughput probably isn't much of a limitation given the device that's being used. I think I'm mainly interested in the author's specific reason for this requirement (I'm a BA, these questions are my bread and butter).Comment to author: Gotta add the Pixel 5 to your homelab inventory! Also, nice site, layout and information.reply",
      "Pedantry: A _reused_ Google Pixel 5. It was not broken down and reconstituted.Reduce, Reuse, Recycle is in order of environmental impact, so reusing is an upgrade!reply",
      "My number one concern is how do I avoid the spicy pillow problem... If I could have phones run off of USB without a battery, I would love to do that.reply",
      "https://www.instructables.com/Power-an-Android-Phone-Without...You can also buy dummy batteries for certain models online.reply",
      "Is a phone plugged in 24/7 actually more power-efficient than a slice of a mega-optimized cloud server?reply",
      "I had a web server running on my jailbroken iPod touch back in 2009/2010. It\u2019s neat but not entirely novel, or practical.reply",
      "Pretty wild how readily available compute has become. Sure not AI level compute but between modern consumer hardware and the mountain of free tier stuff out there I\u2019ve always got more than ability to use it effectivelyreply",
      "Can you stresstest it ? How many simultaneous connections can it handle ?reply",
      "I'd argue that's happening right now.  But we don't know if it's still running on a phone as the post is from September 2024.reply",
      "Title needs \"(2024)\". It's been over a year and whether the blog is still running on a recycled Pixel 5 is unclear.reply"
    ],
    "link": "https://blog.ctms.me/posts/2024-08-29-running-this-blog-on-a-pixel-5/",
    "first_paragraph": "If you glance over this blog, you will see that I am an avid Android fan. After setting up numerous Linux proot desktops on phones, I wanted to see if I use a phone as a server and run my blog from an Android phone. Since you are reading this, I was successful.I was inspired my a few Mastodon posts earlier this week to give it a go. First, I stumbled on a post from @kaimac who is running a site from an ESP32 microcontroller. In the comments of that post, I saw a mention to compost.party created by user @computersandblues that runs completely on an Android device and a solar panel. Last, @stevelord who is essentially running a homelab on a TP-Link router with OpenWRT installed.I think a lot about power consumption of my homelab and I also love using old hardware for random projects to give them new life. I was truly inspired by the above works, so I got right down to business.I looked through the devices I had laying around and I chose a Google Pixel 5 my brother-in-law gave me after he"
  },
  {
    "title": "Introduction to Ada: a project-based exploration with rosettas (adacore.com)",
    "points": 138,
    "submitter": "jaypatelani",
    "submit_time": "2025-09-02T17:32:08 1756834328",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=45106314",
    "comments": [
      "I would love to get a list of language features that are \"free to use with GNAT\" and those that are \"AdaCore license required.\"  The last time I did Ada (admittedly, back in the 90s) it wasn't all that clear what language features I could use for free.  And since we're on lists of things, a list of zero-cost abstractions and non-zero-cost abstractions would be nice.I'm pretty sure these aren't big issues these days, but there's still a lot of people walking around thinking \"I can't use Ada on this project, I don't have budget for a commercial compiler.\"  Maybe a \"project manager's introduction to Ada.\"  I would write it myself, but I've forgotten most everything I learned about the language and it's development community.[Apart from that... young engineers should definitely check out Ada, even if you don't eventually use it.  Why it was considered a good idea to create a new language, the problems language designers were trying to solve and how developers used the language to build code that was more bullet-proof than C++ is kind of an interesting story.]reply",
      "> I would love to get a list of language features that are \"free to use with GNAT\" and those that are \"AdaCore license required.\"All Ada language features are present in the free/open source version of the compiler. The proprietary version of GNAT is just updated more frequently I think and has commercial support - they periodically copy their changes into the main GCC source tree.They have proprietary tools for some kinds of static analysis, but those wouldn\u2019t be considered language features. GNATprove (the theorem prover tool for verifying SPARK programs) is open source.reply",
      "I think AdaCore stopped supporting GNAT community in 2022 and recommended to use Alire community, no?https://blog.adacore.com/a-new-era-for-ada-spark-open-source...reply",
      "GNAT Community was just a version of the GNAT toolchain/IDE provided by AdaCore. GNAT is still open source and still updated as part of GCC, it is now just recommended to install it using the Alire package manager. Builds of GNAT are also provided on some distros since it is part of GCC.reply",
      "Alire is just a package and toolchain manager that AdaCore wrote in the style of Cargo. It still runs GNAT under the hood.reply",
      "I don't think AdaCore was involved in the creation of alire:https://zaguan.unizar.es/record/79726/files/texto_completo.p...AdaCore does directly contribute to GNAT which is important.GNAT was chosen for alire since it is free software:  \"This work presents a working prototype tailored to the Ada compiler available to open source enthusiasts, GNAT.\"Other Ada implementations typically use their own build systems which are different from GNAT's and so they would probably need changes to work with alire.reply",
      "The last time I bought a license for AdaCore it was to get the unit testing, static analysis and dynamic analysis tools IIRC but, that was about 8-9 years ago. We also paid for training from AdaCore and their main guy came out.reply",
      "There is a special place in my programming heart for Ada since it was my first professional language (back in 1996). It is interesting to see it coming back a bit. Ignoring the language, the culture and history of Ada is a great one. I was around for the fall of the Ada mandate and I got to see the play 'Lady Ada and Castle Mandate'. The community that put that on was passionate and, obviously, quirky.reply",
      "I did a student project at uni using Ada, building a compiler. it was a pretty nice language to work in, felt similar to Pascal. I don't remember particularly disliking anything. Declaring function parameters as in, or out, or in out, was cool.reply",
      "I've always liked generative art and I am getting up to speed on Ada 2022, so this is a fitting article. Currently using SPARK2014/Ada 2022 for a high-integrity, safety-critical automation control software project.reply"
    ],
    "link": "https://blog.adacore.com/introduction-to-ada-a-project-based-exploration-with-rosettas",
    "first_paragraph": "This practical walkthrough, designed as a short tutorial, was created upon joining AdaCore as a Field Engineer. In this new role, I\u2019ll be working directly with customers to help them succeed with Ada. Although I was first introduced to the language nearly two decades ago, this new position inspired me to revisit its fundamentals, and I used the excellent https://learn.adacore.com portal as a quick refresher.While that platform takes a concept-based approach, I chose to complement it with a project-based method by developing a small, end-to-end Ada program that generates animated rosettas in the form of SVG files. These are technically hypotrochoid curves, producing patterns that many will recognize from the classic Spirograph\u2122 toy.In this walkthrough, we\u2019ll show that Ada can be fun and easy to learn. Although the language is famous for safety-critical systems, we will use it as a modern, general-purpose programming language and try out some new features from Ada 2022 along the way.Let'"
  },
  {
    "title": "Making a Linux home server sleep on idle and wake on demand (2023) (dgross.ca)",
    "points": 123,
    "submitter": "AgaoAnar",
    "submit_time": "2025-09-02T19:42:48 1756842168",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=45108066",
    "comments": [
      "20 years ago, I used to have a Linux server running Slackware at home, that would wake up the two PCs we had at home to back up their data if they were turned off.\nIf they were already turned on, they would send a WOL packet to the Linux server to turn it on in case the that server was off, and then start the backup routine. And the last one would tell the Linux server to turn-off itself. It used to work really great, good times.reply",
      "Well done. I really enjoy blog posts that dive into topics that probably cross the minds of most home lab-ers at one point or another.reply",
      "This reminds me of this old plugin for Windows Home Server: https://www.green-it-software.com/products/lights-out-for-wh...You could set a calendar schedule for waking up itself and backing up the clients, and at night the server would go into standby only if no clients were running anymore since X minutes.reply",
      "Well done. But note: you can grab an ATX control board, or configure the RPi as a USB Gadget to wake the machine via power button or keyboard.reply",
      "I did this recently as I was struggling to get WoL to work with my consumer PC. It seems like this ultra low-level stuff is a total crapshoot so if you can dodge it by just wiring up the power button, that's a good option.In in the end I just went the whole hog and set up a PiKVM, so now if I mess up the machine's networking (or even completely break the OS) I can still recover it remotely even though it doesn't have a proper BMC or anything like that.In general this approach seems ugly in principle but I really like it in practice. It lets you retrofit solid remote capabilities onto consumer hardware. That way you have such a broader market to buy from.reply",
      "(2023), discussed at the time: https://news.ycombinator.com/item?id=35627107reply",
      "Thanks! Macroexpanded:Making a Linux home server sleep on idle and wake on demand \u2013 the simple way - https://news.ycombinator.com/item?id=35627107 - April 2023 (237 comments)reply",
      "All this complexity to save a few bucks per year on your electricity bill? This is ridiculous, the Pi costs far more than what you can be expected to save.reply",
      "I think it turned out a lot more complicated than the author expected, but that their solution they kindly wrote up will be pragmatic for someone.(For example, imagine a big home GPU server that is needed only intermittently, and you want it to spin up automatically on network traffic from family's various devices that you can't modify.)Of course, if you have simpler needs, and you're willing to send a WOL magic packet from the using devices, you can do in a few lines of shell script.  It's a 1-line ssh-to-something-that-can-etherwake-on-that-vlan script, then wait in a loop for the service you need to appear, then 1-line ssh-to-server-to-shutdown when you're done.reply",
      "In many European countries electricity is quite expensive. In the U.K. for example, running 20 watts nonstop for a year will cost you around $65 on a typical tariff. If you have more than one home server the savings can quickly add up.reply"
    ],
    "link": "https://dgross.ca/blog/linux-home-server-auto-sleep",
    "first_paragraph": ""
  },
  {
    "title": "A staff engineer's journey with Claude Code (sanity.io)",
    "points": 200,
    "submitter": "kmelve",
    "submit_time": "2025-09-02T19:34:24 1756841664",
    "num_comments": 138,
    "comments_url": "https://news.ycombinator.com/item?id=45107962",
    "comments": [
      "Preventing garbage just requires that you take into account the cognitive limits of the agent. For example ...1) Don't ask for large / complex change. Ask for a plan but ask it to implement the plan in small steps and ask the model to test each step before starting the next.2) For really complex steps, ask the model to write code to visualize the problem and solution.3) If the model fails on a given step, ask it to add logging to the code, save the logs, run the tests and the review the logs to determine what went wrong. Do this repeatedly until the step works well.4) Ask the model to look at your existing code and determine how it was designed to implement a task. Some times the model will put all of the changes in one file but your code has a cleaner design the model doesn't take into account.I've seen other people blog about their tricks and tips. I do still see garbage results but not as high as 95%.reply",
      "I've found that an effective tactic for larger, more complex tasks is to tell it \"Don't write any code now.  I'm going to describe each of the steps of the problem in more detail.  The rough outline is going to be 1) Read this input 2) Generate these candidates 3) apply heuristics to score candidates 4) prioritize and rank candidates 5) come up with this data structure reflecting the output 6) write the output back to the DB in this schema\".  Claude will then go and write a TODO list in the code (and possibly claude.md if you've run /init), and prompt you for the details of each stage.  I've even done this for an hour, told Claude \"I have to stop now.  Generate code for the finished stages and write out comments so you can pick up where you left off next time\" and then been able to pick up next time with minimal fuss.reply",
      "FYI: You can force \"Plan mode\" by pressing shift-tab. That will prevent it from eagerly implementing stuff.reply",
      "> That will prevent it from eagerly implementing stuff.In theory. In practice, it's not a very secure sandbox and Claude will happily go around updating files if you insist / the prompt is bad / it goes off on a tangent.I really should just set up a completely sandboxed VM for it so that I don't care if it goes rm-rf happy.reply",
      "Plan mode disabled the tools, so I don\u2019t see how it would do that.A sandboxed devcontainer is worth setting up though. Lets me run it with \u2014dangerously-skip-permissionsreply",
      "how can it plan if it does not have access to file read, search, bash tools to investigate things? If it has access to bash tools then it's going to write code, via echo or sed.",
      "I don't know either but I've seen it write to files in plan mode. Very confusing.reply",
      "How does a token predictor \u201capply heuristics to score candidates\u201d? Is it running a tool, such as a Python script it writes for scoring candidates? If not, isn\u2019t it just pulling some statistically-likely \u201cscore\u201d out of its weights rather than actually calculating one?reply",
      "I feel like I do all of this stuff and still end up with unusable code in most cases, and the cases where I don't I still usually have to hand massage it into something usable. Sometimes it gets it right and it's really cool when it does, but anecdotally for me it doesn't seem to be making me any more efficient.reply",
      "The key is prompting. Prompt to within an inch of your life. Treat prompts as source code - edit them in files, use @ notation to bring them into the console. Use Claude to generate its own prompts - https://github.com/wshobson/commands/ and https://github.com/wshobson/agents/ are very handy, they include a prompt-engineer persona.I'm at the point now where I have to yell at the AI once in a while, but I touch essentially zero code manually, and it's acceptable quality. Once I stopped and tried to fully refactor a commit that CC had created, but I was only able to make marginal improvements in return for an enormous time commitment. If I had spent that time improving my prompts and running refactoring/cleanup passes in CC, I suspect I would have come out ahead. So I'm deliberately trying not to do that.I expect at some point on a Friday (last Friday was close) I will get frustrated and go build things manually. But for now it's a cognitive and effort reduction for similar quality. It helps to use the most standard libraries and languages possible, and great tests are a must.Edit: Also, use the \"thinking\" commands. think / think hard / think harder / ultrathink are your best friend when attempting complicated changes (of course, if you're attempting complicated changes, don't.)reply"
    ],
    "link": "https://www.sanity.io/blog/first-attempt-will-be-95-garbage",
    "first_paragraph": "Content operationsContent backendUse CasesUsersBuild and ShareInsight\u00a9 SANITY 2025OSL, NOR (CET)SFO, USA (PST)This started as an internal Sanity workshop where I demoed how I actually use AI. Spoiler: it's running multiple agents like a small team with daily amnesia.Vincent QuigleyVincent Quigley is a Staff Software Engineer at SanityPublished September 2, 2025Until 18 months ago, I wrote every line of code myself. Today, AI writes 80% of my initial implementations while I focus on architecture, review, and steering multiple development threads simultaneously.This isn't another \"AI will change everything\" post. This is about the messy reality of integrating AI into production development workflows: what actually works, what wastes your time, and why treating AI like a \"junior developer who doesn't learn\" became my mental model for success.The backstory: We run monthly engineering workshops at Sanity where someone presents what they've been experimenting with. Last time was my turn, and"
  },
  {
    "title": "<template>: The Content Template element (developer.mozilla.org)",
    "points": 149,
    "submitter": "palmfacehn",
    "submit_time": "2025-09-02T17:16:42 1756833402",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=45106049",
    "comments": [
      "I'm using the <template> tag heavily in Timelinize[0], which has a fairly sophisticated UI that I'm writing in vanilla JS -- not even jQuery. I use a few libraries (for mapping, datetime, and Bootstrap+Tabler for some nice styles), but that's it.I wrote some boilerplate JS, yes, but I have complete control over my frontend, and I understand how it works, and I don't need to compile it.Anyway, it works well so far! The <template> tag is a great way to lay out components and then fill them in with simple JS functions.One nuance is documented in my code comments:    // Ohhhhhh wow, we need to use firstElementChild when cloning the content of a template tag (!!!!):\n    // https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/template#avoiding_documentfragment_pitfalls\n    // I spent way too long on this.\n    const elem = $(tplSelector);\n    if (!elem) return;\n    return elem.content.firstElementChild.cloneNode(true);\n\nOnce I figured that out, it's been smooth sailing and very powerful.Oh, but I haven't cleaned up my code at all yet because I was just experimenting/hustling, so it's a bit spaghetti :) If you go looking, don't mind the mess.---[0]: https://github.com/timelinize/timelinize - a self-hosted application for bringing together all your data on your own computer and organizing it onto a single timeline: social media accounts, photo libraries, text messages, GPS tracks, browsing history, and more. Almost ready for its debut...reply",
      "You should use document.importNode() to clone templates.Template contents are in a separate document from the main document, which is what makes them inert. importNode() adopts the nodes into the document so they have the correct prototype immediately (which includes custom elements). Otherwise the adopt steps are run when the elements are first attached to the document, which adds another tree walk to the insert/append operation that costs some time.So:    document.importNode(elem.content, true);\n\nThen you'll have a DocumentFragment you can pull nodes out of. Or just append the whole fragment.reply",
      "Awesome -- gonna try this. Thanks for the tip! And lit-html looks cool btw.Update: I'm using importNode() now -- works great too.reply",
      "I considered vanilla template initially for building my ActivityPub frontend[1], but in the end the sugar that lit-js adds on top of web components seemed worth the file size overhead.[1] https://github.com/mariusor/onireply",
      "Author of lit-html here.Yeah, Lit's tagged template literals and render() method are basically a shorthand for making a <template> element, marking the spots where expressions go, cloning the template, then filling in those sports.reply",
      "Timelinize, just wow. Been wishing for something like this forever. Love the idea!Regarding the gpx support... Don't want to lead you astray, but, have you seen https://www.sethoscope.net/heatmap/?reply",
      "Thanks! Hopefully it delivers.I haven't seen that, actually -- looks very nice. We do render a heatmap already, but I'm learning that heatmaps are very tricky to tune to be most useful/interesting. I will try to learn from that project.reply",
      "Dude, cool project!  I considered building something like this a couple years ago as the only way in hell I'd ever use it is if it's open source and self-hosted (thanks so much for using AGPLv3!).  I ended up stuck in analysis paralysis, so I'm super impressed :-)reply",
      "Ha, thanks. It's hard, so no shame for being \"stuck\". I've iterated on this for over a decade at this point. Still more evolution to come... but I really want this for my family.reply",
      "What's Tabler?reply"
    ],
    "link": "https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/template",
    "first_paragraph": "\nHTML: Markup language\n\nCSS: Styling language\n\nJavaScript: Scripting language\n\nWeb APIs: Programming interfaces\n\nAll web technology\n\nLearn web development\nDiscover our toolsGet to know MDN better\nThis feature is well established and works across many devices and browser versions. It\u2019s been available across browsers since \u2068November 2015\u2069.\n* Some parts of this feature may have varying levels of support.The <template> HTML element serves as a mechanism for holding HTML fragments, which can either be used later via JavaScript or generated immediately into shadow DOM.This element includes the global attributes.Creates a shadow root for the parent element.\nIt is a declarative version of the Element.attachShadow() method and accepts the same enumerated values.Exposes the internal shadow root DOM for JavaScript (recommended for most use cases).Hides the internal shadow root DOM from JavaScript.Note:\nThe HTML parser creates a ShadowRoot object in the DOM for the first <template> in a node with "
  },
  {
    "title": "You're Not Interviewing for the Job. You're Auditioning for the Job Title (idiallo.com)",
    "points": 53,
    "submitter": "foxfired",
    "submit_time": "2025-09-02T21:31:37 1756848697",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=45109324",
    "comments": [
      "It's been 10 years since I did an interview and I think I would rather retire and grow rare lizards than jump through the interview hoops at a new company. I am 90% sure I couldn't pass the interview for my current position but I'm the one who designed the whole thing. -staff level backend engineerreply",
      "I was 14 years in a job at senior level and got laid off.And yeah, it's pretty much as bad as you describe.reply",
      "Yep.  I didn't even \"pass\" some of the coding challenges despite hiring, firing, and writing coding challenges myself!What I often saw was many of those jobs were sitting on the market for months and months just getting relisted every few weeks.  The ones that actually led to interviews, I had one dipshit proudly tell me about his 2-hour commute five days a week.  Another told me they wanted to vomit-out mini-MVPs and were replacing the new guy who wasn't doing it fast enough.  Just an ocean of fake jobs and awful jobs lol.reply",
      "I had a coding challenge where if you didn\u2019t implement what was essentially a Set in the exact way that they expected, you couldn\u2019t proceed since the evaluation assumed you\u2019d match their ordering (which was arbitrary). One of those automated test things and there was no way to contact a human and say, your process is broken. I\u2019m eagerly counting down the days until I can retire. Between broken hiring processes and the rise of LLM coding, I\u2019m left wondering if this is how I want to spend my precious time.reply",
      "In the past 10 years and I was already 40 years old in 2014, I\u2019ve interviewed:- at a company where they launched a new division in a satellite office in another city to separate the team from the old guard to create  a \u201ctiger team\u201d of experienced developers.  I was the second hire.  I just spoke to the manager as an experienced professional and we talked about how I solved real world problems- a new to the company director who needed a lead software engineer to integrate systems of acquisitions that the PE owner was buying.- the new to the company CTO after the founders found product market fit and wanted to bring technology leadership into the company from a third party consulting company.  I was eventually tasked with making everything cloud native, scalable, resilient etc.  I was his second technical hire. Our customers were large health care companies where one new contract could bring in 10K new users and even more ETL integrations.  He knew I didn\u2019t have any practical AWS experience.  He later told me I seemed like a smart guy and I could figure it out.- AWS itself in the ProServe division - 5 round behavioral interview where I walked through my implementations.- (2024) third party cloud consulting company in a staff role.  They asked how would I architect something and I made sure I hit all of the \u201cpillars\u201d of AWS Well Architected and talked through 12 Factor Apps.I\u2019m 51 and I stay interview ready. My resume and my career documents are updated quarterly and I keep my network warm.I believe right now if I were looking for a job, someone would hire me quickly if not for a permanent position, at least I could hustle up on a contract.reply",
      "I had almost this exact interview experience recently with a popular AI startup. The exercise was to build a search UI over a static array of dictionary terms. It was a frontend role so I wired it up with filter and startsWith and spent more time polishing the UI and UX.The final interview question was: \u201cOkay, how do you make this more performant?\u201d My answer was two-tiered:- Short term: debounce input, cache results.\n- Long term: use Algolia / Elastic, or collaborate with a backend engineer to index the data properly.I got rejected anyway (even with a referral). Which drove home OP's point: I wasn't being judged on problem solving, but auditioning for the \"senior eng\" title.With candidate interview tools and coding aids increasingly hard to detect in interviews, this gap between interview performance and delivering in the role is only going to widen. Curious how many of these \"AI-assisted hires\" will start hitting walls once they're outside of the interview sandbox.reply",
      "In general:- At a large tech company, a referral can help you get an interview; it rarely affects the actual hiring decision or the offer.- As an interviewee, I might feel like I did great, but I don\u2019t know what signal the interviewer wanted or what their bar is for that level.My son\u2019s school uses an adaptive test three times per year (MAP Growth). It\u2019s designed so each student answers about 50% of the math questions correctly. Most students walk out with a similar perception of:- how hard the test was, and- how well they did.Those perceptions aren\u2019t strongly related to differences in their actual performance.Interviews are similar. A good interviewer keeps raising the difficulty and probing until you hit an edge. Strong candidates often leave feeling 50/50. So \u201cI crushed it\u201d (or \u201cthat was brutal\u201d) isn\u2019t a reliable predictor of the outcome. What matters are the specific signals they were measuring for that role and level, which may not be obvious from the outside, especially when the exercise is intentionally simple.Many years ago, when I interviewed at an investment bank for a structuring role, I answered all of their questions correctly, even though some of them were about things I'd never heard of (like a 'swaption'). I answered at what I thought was a reasonable pace, and only for one or two questions did I need a minute or two to work out the answer on paper. At the time, I thought I'd done well. I didn't get the job. I now know more about what they were looking for, and I'd say my performance was somewhere between 'meh' and 'good enough'. I'm sure they had better candidates.When I interviewed at Google (back in 2014), I was asked the classic https://github.com/alex/what-happens-when question. I didn't know it was a common question, and hadn't specifically prepared for it. Nonetheless, I thought I crushed it. I explained a whole bunch of stuff about DNS, TCP, ARP, subnet masks, HTTP, TLS etc.I said nothing about equally important things that were much less familiar to me: e.g. keyboard interrupts, parsing, rendering, ...Luckily I passed that interview, but at the time I thought I'd covered everything important, when in reality my answer helped show the interviewer exactly where the gaps were in my understanding.reply",
      "> Next, you cover the whiteboard in boxes, arrows, and at least one redundant Kubernetes cluster. Add a message queue, Kafka obviously, regardless of whether you need one. Sprinkle in some microservices because monoliths are for peasants, and draw load balancers like protective talismans around every component.Loved this. TFA is so true: interviewing is unfortunately a performance (for both sides, but mainly the interviewee).reply",
      "Ha. I like to give a systems design scenario that rewards simplicity. Candidates who complexify it (usually in very predictable ways) get rejected. The few who see the simple path have been great hires. Because they also asked the right questions.reply",
      "Isn't that just another kind of trick question? It seems like that relies a lot on the interviewee guessing that you aren't looking for a standard complex solution.reply"
    ],
    "link": "https://idiallo.com/blog/performing-for-the-job-title",
    "first_paragraph": "I once had a job interview for a backend position. Their stack was Node.js, MySQL, nothing exotic. The interviewer asked: \"If you have an array containing a million entries, how would you sort the data by name?\"My immediate thought was: If you have a JavaScript array with a million entries, you're certainly doing something wrong.The interviewer continued: \"There are multiple fields that you should be able to sort by.\"This felt like a trick question. Surely the right answer was to explain why you shouldn't be sorting millions of records in JavaScript. Pagination, database indexing, server-side filtering. So I said exactly that.I was wrong. He wanted me to show him Array.prototype.sort().My crime? Prioritizing real-world efficiency over theatrical scale. The interviewer didn't see a practical engineer, he saw a candidate who \"lacked vision.\"I once read that \"a complex system usually reflects an absence of good design.\" It's brilliant. True. And if you're prepping for a system design inte"
  },
  {
    "title": "The Little Book of Linear Algebra (github.com/the-litte-book-of)",
    "points": 276,
    "submitter": "scapbi",
    "submit_time": "2025-09-02T14:17:07 1756822627",
    "num_comments": 79,
    "comments_url": "https://news.ycombinator.com/item?id=45103436",
    "comments": [
      "It's crazy that Linear Algebra is one of the deepest and most interesting areas of mathematics, with applications in almost every field of mathematics itself plus having practical applications in almost every quantitative field that uses math.But it is SOOO boring to learn the basic mechanics. There's almost no way to sugar coat it either; you have to learn the basics of vectors and scalars and dot products and matrices and Gaussian elimination, all the while bored out of your skull, until you have the tools to really start to approach the interesting areas.Even the \"why does matrix multiplication look that way\" is incredibly deep but practically impossible to motivate from other considerations. You just start with \"well that's the way it is\" and grind away until one day when you're looking at a chain of linear transformations you realize that everything clicks.This \"little book\" seems to take a fairly standard approach, defining all the boring stuff and leading to Gaussian elimination. The other approach I've seen is to try to lead into it by talking about multi-linear functions and then deriving the notion of bases and matrices at the end. Or trying to start from an application like rotation or Markov chains.It's funny because it's just a pedagogical nightmare to get students to care about any of this until one day two years later it all just makes sense.reply",
      "> Even the \"why does matrix multiplication look that way\" is incredibly deep but practically impossible to motivate from other considerations. You just start with \"well that's the way it is\" and grind awayIn my experience it need not be like that at all.One can start by defining and demonstrating linear transformations. Perhaps from graphics -- translation, rotation, reflection etc. Show the students that these follow the definition of a linear transformation. That rotating a sum is same as summing the rotated(s).[One may also mention that all differentiable functions (from vector to vector) are locally linear.]Then you define adding two linear transformations using vector addition. Next you can define scaling a linear transformation. The point being that the combination can be expressed as linear transformations themself. No need to represent the vectors as R^d, geometric arrows and parallelogram rule would suffice.Finally, one demonstrates composition of linear transformations and the fact that the result itself is a linear transformation.The beautiful reveal is that this addition and composition of linear transformations behave almost the same as addition and multiplication of real numbers.The addition asociates and commutes. The multiplication associates but doesn't necessarily commute. Most strikingly, the operations distributes. It's almost like algebra of real numbers !Now, when you impose a coordinate system or choose a basis, the students can discover that matrix multiplication rule for themselves over a couple of days of playing with it -- Look, rather than maintaining this long list of linear transformations, I can store it as a single linear transformation in the chosen basis.reply",
      "> Perhaps from graphics -- translation, rotation, reflectionMaybe ... but the fact that you included translation in the list of linear operations seems like a big red flag. Translation feels very linear but it is emphatically not [1]. This is not intended to be a personal jab; just that the intuitions of linear algebra are not easy to internalize.Adding linear transformations is similarly scary territory. You can multiply rotations to your heart's content but adding two rotations gives you a pretty funky object that does not have any obvious intuition in graphics.[1] I wouldn't jump into projective or affine spaces until you have the linear algebra tools to deal with them in a sane way, so this strikes me as a bit scary to approach it this way.reply",
      "Mea culpa about translation.For a moment I was thinking in homogeneous coordinates - that's not the right thing to do in the introductory phase.Thanks for catching the error and making an important point. I am letting my original comment stand unedited so that your point stands.About rotations though, one need not let the cat out of the bag and explain what addition of rotation is *.One simply defines addition of two linear operators as the addition of the vectors that each would have individually produced. This can be demonstrated geometrically with arrows, without fixing coordinates.* In 2D it's a scaled rotation.reply",
      "If anybody is aware of materials that teach linear algebra via graphics as suggested here, I would be interested to hear about them. As someone who learns best through practical application, maths have been by far among my greatest weak points, despite having written software for upwards of a decade. It\u2019s limiting in some scenarios and pure imposter syndrome fuel.reply",
      "3Blue1Brown [Essense of linear algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2x...)reply",
      "codingthematrix.comreply",
      "I didn't think any part of linear algebra was boring. I was hooked from the moment I saw Ax=b => x = b/A. Gaussian elimination is a blast, like an actually-productive Sudoku puzzle, and once you have it down you can blaze through the first 2/3rds of an undergrad linear algebra course. I don't consciously try to gain automaticity with math subjects, but matrix-column multiplication I got pretty quickly and now I just have it.I learned from Strang, for what it's worth, which is basically LU, spaces, QR, then spectral.I am really bad at math, for what it's worth; this is just the one advanced math subject that intuitively clicked for me.reply",
      "MIT OCW is an amazing resource -- anyone can learn from Strang, which is a goldmine.He also created a course on using Linear Algebra for machine learning:> Linear algebra concepts are key for understanding and creating machine learning algorithms, especially as applied to deep learning and neural networks. This course reviews linear algebra with applications to probability and statistics and optimization\u2013and above all a full explanation of deep learning.- MIT OCW Course: Matrix Methods in Data Analysis, Signal Processing, and Machine Learning (https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-an...)- The text book website: Linear Algebra and Learning from Data (2019)\nhttps://math.mit.edu/~gs/learningfromdata/- The Classic Linear Algebra Course: https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010...reply",
      "> I didn't think any part of linear algebra was boring.My formal linear algebra course was boring as hell, to me. The ~4 lectures my security prof dedicated to explaining just enough to do some RSA was absolutely incredible. I would pay lots of money for a hands-on what-linalg-is-useful-for course with practical examples like that.reply"
    ],
    "link": "https://github.com/the-litte-book-of/linear-algebra",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        There is hardly any theory which is more elementary than linear algebra, in spite of the fact that generations of professors and textbook writers have obscured its simplicity by preposterous calculations with matrices. \u2014Jean Dieudonne\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.A concise, beginner-friendly introduction to the core ideas of linear algebra.A scalar is a single numerical quantity, most often taken from the real numbers, denoted by $\\mathbb{R}$. Scalars are\nthe fundamental building blocks of arithmetic: they can be added, subtracted, multiplied, and, except in the case of\nzero, divided. In linear algebra, scalars play the role of coefficients, scaling factors, and entries of larger\nstructures such as vectors and matrices. They provide the we"
  },
  {
    "title": "Physically based rendering from first principles (imadr.me)",
    "points": 132,
    "submitter": "imadr",
    "submit_time": "2025-09-02T18:07:22 1756836442",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=45106846",
    "comments": [
      "I'm not a fan of how people talk about \"first principles\" as I think it just leads to lots of confusion. It's a phrase common in computer science that makes many other scientific communities cringe. First principles are things that cannot be reduced and you have to have very good justifications for these axioms. The reason the other scientific communities cringe is because either (most likely case) it's being used improperly and someone is about to demonstrate their nativity, or they know they're about to dive into a pedantic nightmare of nuances and they might never escape the rabbit holes that are about to follow.In fact, I'd like to argue that you shouldn't learn things from first principles, at least in the beginning. Despite the article not being from first principles, it does illustrate some of the problems of first principles: they are pedantic. Everything stems from first principles so they have to be overly pedantic and precise. Errors compound so a small error in one's first principles becomes enormous by the time you look at what you're actually interested in. Worst of all, it is usually subtle, making it difficult to find and catch. This makes them a terrible place to begin, even when one already has expertise and is discussing with another expert. But it definitely should not be the starting place for an expert to teach a non-expert.What makes it clear that the author isn't a physicist is that they don't appear to understand the underlying emergent phenomena[0]. It's probably a big part of why this post feels so disordered. All the phenomena they discussed are the same, but you need to keep digging deeper to find that (there's points where even physicists know they are the same but not how or why). It just feels like they are showing off their physics knowledge, but it is well below that which is found in an undergraduate physics degree[1]. This is why you shouldn't start at first principles, its simplicity is too complex. You'd need to start with subjects more complicated than QED. The rest derive out of whatever a grand unified theory is.But as someone who's done a fair amount of physical based rendering, I'm just uncertain what this post has to do with it. I would highly recommend the book \"Physically Based Rendering:\nFrom Theory To Implementation\" by Pharr, Jakob, and Humphreys that the author says the post is based on. It does a much better job at introducing the goals and focusing on getting the reader up to speed. In particular, they define how the goal of PBR is to make things indistinguishable from a real photograph, which is a subtle but important distinction from generating a real photograph.That said, I still think there's nice things about this post and the author shouldn't feel ashamed. It looks like they put a lot of hard work in and there are some really nice animations. It's clear they learned a lot and many of the animations there are not as easy as they might appear. I'm being critical but I want them to know to keep it up, but that I think it needs refinement. Finding the voice of a series of posts can be quite hard and don't let stumbles in the beginning prevent you from continuing.[0] Well that and a lack of discussion of higher order interference patterns because physicists love to show off {Hermite,Laguerre}-Gaussian mode simulations https://en.wikipedia.org/wiki/Gaussian_beam#Higher-order_mod...[1] In a degree you end up \"learning physics\" multiple times. Each time a bit deeper. By the end of an undergraduate degree every physicist should end up feeling like they know nothing about physics.reply",
      "Thanks for the constructive criticism! A few points I'd like to discuss:Let's suppose the aim of the article was indeed to learn PBR from first principles, what would it look like? Quantum electrodynamics?I think there is merit in exploring different physical models for fun and scientific curiosity (like I mentioned in the first chapter).\nI (personally) feel that it's boring to just dump equations like Snell's law without exploring the deeper meaning behind it. I also feel that it's easier to grasp if you have some surface knowledge about more complex physical models.I agree however that I probably made many mistakes since I didn't study physics, I'd appreciate any feedback to improve that.I dislike \"Physically Based Rendering: From Theory To Implementation\", I personally think that the literate programming approach of the book is way too confusing and disorganized. I prefer the SIGGRAPH talk by Naty Hoffman[0][0] https://www.youtube.com/watch?v=j-A0mwsJRmkreply",
      "> I dislike \"Physically Based Rendering: From Theory To Implementation\", I personally think that the literate programming approach of the book is way too confusing and disorganized.Interesting. Personally it's by far the best programming related book I've read. I didn't mind the literal programming, and I loved how it dove fairly deep into the math and physics but also into the details of implementing the math.The latter being important as there are can be so many gotchas when implementing math.reply",
      "> Let's suppose the aim of the article was indeed to learn PBR from first principles, what would it look like?Well given the title I at least expected the article to explain or derive things like why and how metals and their alloys have the color (wavelenght-dependent complex index of refraction) that they do, why and how say quartz crystals have different colors, birefringence, fluorescence (makes T-shirts appear extra bright) etc.And there is no mention of the recording process. Are we simulating good old film with its silver crystals of various sizes? Different film stock is known to have very different looks due to their chemistry.Or a digital camera sensor with its quantum and thermal noise, bayer filter and a rolling shutter causing those funny-looking propeller pictures?Not knocking the article, but given the title it fell well short of my expectations going in. That is, I was wondering how on earth anyone had managed to do all that.reply",
      "Yes I didn't have time to write a chapter about camera sensors, the human retina, and the whole image formation process. I'll definitely expand on this later on.reply",
      "> Let's suppose the aim of the article was indeed to learn PBR from first principles, what would it look like? Quantum electrodynamics?Something like that, yes. A truly from-first-principles treatment of photon-surface interactions would involve an extremely deep dive into quantum numbers, molecular orbitals, solid state physics and crystal lattices (which are metals), including a discussion about how electron waves superpose to produce various conduction/valence bands with various band gaps, and then discuss how photons interact with these bands.reply",
      "I might be a stupid question but how hard would that be to explain, and to understand?If you had to teach an alien from another universe physically based rendering:- In an exhaustive manner and,- You're only allowed to explain something if it derives from something more \"fundamental\" until we reach the most comprehensive physical models we haveHow hard would be the math behind it for example? Because realistically in my article the hardest piece of math is a very basic integralCould I for example start reading these Feynman lectures[0] and get up to speed on everything there is to know about photon-surface interaction?[0] https://www.feynmanlectures.caltech.edu/reply",
      "The raw mathematics isn't the hardest; most of this is settled by the end of the second year of undergraduate physics\u2014things like Taylor series, ODEs, PDEs, special functions, a bit of linear algebra (no proofs needed, just use the results); perhaps complex analysis which leads to Fourier transforms and all. Maybe a treatment of tensors.The issue is the sheer complexity of micro systems, and the unintuitive nature of going deeper than 'EM wave reflects off electrons'.Consider metal-light interaction. Exactly how does a visible-light EM wave interact with a conduction band of superposed free valence electrons? How does the massive superposition elevate each valence electron up energy levels? Why do only metallic and semi-metallic crystals have no band gap? Why are electrons filled in the order of s, p, d, f, g, h orbitals? Why do these orbitals have these shapes? Why are electrons so much less massive than protons and neutrons? Why does the nucleus not tear itself apart since it only contains positive and neutral particles? Why are protons and neutrons made of three quarks each, and how does the strong interaction appear? Why are the three quarks' mass defect so much more than the individual masses of each quark? How does the mass-energy equivalence appear? Why does an accelerating electric charge produce and interact with a magnetic field, and thus emit EM radiation? What is mass, charge, and magnetism in the first place?Each question is more 'first principles' than the last, and the answers get more complex. In these questions we have explored everything from classical EM, to solid state physics, to quantum electro- and chromodynamics, to particle physics and the Standard Model, and are now verging on special and general relativity.reply",
      "Sure! And I appreciate the response. I hope I didn't come off as too mean, it can be hard to find that balance in text, especially while criticizing. I really do not want to discourage you, and I think you should keep going. Don't let mistakes stop you.  > Let's suppose the aim of the article was indeed to learn PBR from first principles, what would it look like?\n\nI think you shouldn't go that route, but the most honest answer I can give is that such a beginning doesn't exist in physics knowledge. You could start with something like String Theory, Supergravity, Loop Quantum Gravity, or some other proposition for a TOE. Physicists are still on the search for first principles.All this is well beyond my expertise btw, despite having worked in optics. If you want to see some of this complexity, but at a higher level, I'd highly recommend picking up Jackson's Elecrtodynamics book. That's that canonical E&M book for graduate level physics, Griffith's is the canonical version for undergraduate (Junior/Senior year). Both are very well written. I also really like Fowles's \"Introduction to Modern Optics\", and it is probably somewhere in between (I read it after Griffiths).I am in full agreement with you that having deep knowledge makes a lot of more shallow topics (and even many other deep topics) far easier to grasp. But depth takes time and it is tricky to get people to follow deep dives. I'm not trying to discourage you here, I actually do encourage going deep, but just noting how this is a tricky line and that's why it is often avoided. Don't just jump into the deepend. Either wade people in or the best option is to lead them in so they don't even recognize they're going deep until they're already there.  > I dislike <PBR Book>, I personally think that the literate programming approach of the book is way too confusing and disorganized\n\nThis is very understandable and I think something you should hone in on and likely where you can make something very successful. But an important thing to note about his SIGGRAPH talk is his audience. His talk is aimed at people who are experts in computer graphics, but likely computer scientists and not physicists. So his audience knows a fair amount of rendering to begin with and can already turn much of what's being said into the code already. But if you listen to it again I think you'll pick up on where he mentions they'll ignore a bunch of things[0]. There's no shame in ignoring some things and working your way forward. I actually like what Hoffman said at 22:25 \"and we know that's an error. But we'll live with it for now.\" That's the mark of good scientific and engineering thinking: acknowledge errors and assumptions, triage, but move forward. A common mistake looks similar, dismissing those errors as inconsequential. That's putting them in the trash rather than tabling for later. Everything is flawed, so the most important thing is keeping track of those flaws, least we have to do extra work to rediscover them.So, who is your audience?This is just my opinion, so you have to be the real judge; but I think you should leverage your non-expertise. One of the hard things when teaching is that once you understand something you quickly forget how difficult it was to learn those things. We quickly go from \"what the fuck does any of this mean\" to \"well that's trivial\" lol. You referenced Feynman in your blog post and most important thing I learned from him is one of the best tools for learning is teaching (I've given way too many lectures to my poor cat lol). It forces you to answer a lot more questions, ones you normally would table and eventually forget about. But at your stage it means you have an advantage, that the topics you are struggling with and have overcome are much more fresh. When learning things we often learn from multiple sources (you yourself shared overlapping material), and that's because multiple perspectives give us lots of benefits. But at this point, don't try to be a physicist. If you want to work towards that direction, great! If you don't, that's okay too. But let your voice speak from where you are now.Reading your blog post and poking through others, there's a \"you\" that's clear in there. Lean into it, because it is good. I like your attention to detail. Like in your Ray Marching post how you just color code everything. Not everyone is going to like that, but I appreciate it and find it very intuitive. I'm a huge fan of color coding equations myself and make heavy use of LaTeX's annotate-equations package when I make slides.But I think looking at this post in isolation the biggest part to me is that it is unclear where you're going. This is a problem I suffer from a lot in early drafts. An advisor once gave me some great advice that works really well for any formal communication. First, tell \"them\" what you're going to tell them, then tell them, then tell them what you told them. It's dumb, but it helps. This is your intro, it is your hook. I think there's places for these ideas but early on they almost feel disconnected. This is really hard to get right and way too easy to overthink. I think I can help with a question: \"What is your thesis?\"/\"What is your main goal?\" Is it \"learn how our human eyes capture light and how our brains interpret it as visual information\"? Or is it \"Physically based rendering from first principles\". Or even \"learn how to create physically realistic renderings of various materials.\" These are not exactly the same thing. When I'm struggling with this problem it is because I have too much to say. So my process is to create a \"vomit draft\" where I just get all the things out and it's going to be a mess and not in the right order. But once out of my head they are easier to put together and in the right order. After your vomit draft check your alignment. What is most important and what can be saved? What's the most bare bones version of what you need to communicate? Build out of that.I do really think there's a good blog post in here and I can see a lot of elements that suggest a good series may come. So I do really encourage you to keep going. Look at what people are saying they like and what they dislike. But also make sure to not take them too literally. Sometimes when we complain about one thing we don't know our issue is something else. What I'm saying is don't write someone else's perfect post, write your post, but find best how to communicate what you want. I know I've said a lot, and I haven't exactly answered all your questions, but I hope this helps.[0] There's a side note here that I think is actually more important than it appears. But the thing is that there's a weird relationship between computation and accuracy. I like to explain this looking at a Taylor Series as an example. Our first order approximation is usually easy to calculate and can usually get us a pretty good approximation (not always true btw). Usually much more than 50% accurate. Second order is much more computationally intensive and it'll significantly increase your accuracy but not as much as before. The thing is accuracy converges much like a log-like curve (or S-curve) while computation increases exponentially. So you need to make these trade-offs between computational feasibility and accuracy. The most important part is keeping track of your error. Now, the universe itself is simple and the computational requirements for it are lower than it takes us to simulate but there's a much deeper conversation about this that revolves around emergence. (The \"way too short\" version is there's islands of computational reducibility) But the main point here is this is why you should typically avoid going too low quickly, because you end up introducing too much complexity all at once and the simplicity of it all is masked by this complexity.reply",
      "Again, thanks for the thorough and constructive answer, it doesn't come off as mean, on the contrary I appreciate it :)I strongly agree that teaching is absolutely the best tool for learning. I wrote this article in part because I got inspired by the \"What I cannot create, I do not understand\" quote by Feynman.I agree that the article is disorganized, and it's not only a feeling: it literally is! I had to shuffle around parts of the chapter about radiometry because I couldn't find the right place for it. I was kind of in a rush because I submitted this article as part of 3blue1brown's summer of math exposition.I find it interesting that between the 3rd and 4th edition of pbr book, chapters have been reorganized too. Monte Carlo Integration has been moved to an earlier chapter before radiometry, reflection models..etc which I found confusing, but I suppose the authors had a good reason to do that.\nSo I have a lot to learn on how to improve my writing and how to organize ideas into something coherent.[0] https://some.3b1b.co/reply"
    ],
    "link": "https://imadr.me/pbr/",
    "first_paragraph": "In this interactive article, we will explore the physical phenomena that create light and the fundamental laws governing its interaction with matter. We will learn how our human eyes capture light and how our brains interpret it as visual information. We will then model approximations of these physical interactions and learn how to create physically realistic renderings of various materials.We are all familiar with light: it\u2019s the thing that allows us to see the world, distinguish colors and textures, and keeps the universe from being a dark, lifeless void. But precisely defining what light is has proven to be a tricky question.\nThroughout history, many philosophers (and later, physicists) studied light in an effort to demystify its nature. Some ancient Greeks considered it to be one of the four fundamental elements that composed the universe: beams of fire emanating from our eyes.\nDescartes proposed that light behaved like waves, while Newton thought that it consisted of tiny particle"
  },
  {
    "title": "Vijaye Raji to become CTO of Applications with acquisition of Statsig (openai.com)",
    "points": 126,
    "submitter": "tosh",
    "submit_time": "2025-09-02T18:18:10 1756837090",
    "num_comments": 48,
    "comments_url": "https://news.ycombinator.com/item?id=45106981",
    "comments": [
      "Hats off to Statsig. They built a stellar product. Superior to many of their industry competitors like Optimizely. Back when I was on an internal Experimentation platform, we were impressed how they balanced dev velocity & stat rigor https://www.statsig.com/updates These guys ship.Business-wise, I think getting acquired was the right choice. Experimentation is too small & treacherous to build a great business, and the broader Product Analytics space is also overcrowded. Amplitude (YC 2012), to date, only has a 1.4B market cap.Joining the hottest name next door gives Statsig a lot more room to explore. I look forward to their evolution.reply",
      "At peak amplitude's market cap was 10Breply",
      "Amplitude is on track to be delisted lolreply",
      "Imagine working for 13 years and creating something only worth 1.4B dollars. What pathetic losersreply",
      "lol, lmao eventhanks for the laughreply",
      "Initial Show HN four years ago: https://news.ycombinator.com/item?id=26629429Congrats to the Statsig team!reply",
      "1 comment - 13 points. Guess you should never feel bad if your own Show HN doesn't take off, it's not the end of the world!reply",
      "Are we supposed to post every blog/news post of OpenAI and keep fueling the AI hype? I think at this point people should know that OpenAI is just like any other company.reply",
      "Statsig is big enough that its acquisition is interesting in its own right. Or maybe I'm biased because I spent quite some time setting up a Statsig integration at $FORMER_EMPLOYER. But if I've done that, odds are that a lot of the other people here have too...reply",
      "This seems like a big shift for OpenAI into an enterprise applications vendor to me.reply"
    ],
    "link": "https://openai.com/index/vijaye-raji-to-become-cto-of-applications-with-acquisition-of-statsig/",
    "first_paragraph": ""
  },
  {
    "title": "Static sites enable a good time travel experience (hamatti.org)",
    "points": 141,
    "submitter": "speckx",
    "submit_time": "2025-09-02T15:25:44 1756826744",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=45104303",
    "comments": [
      "I dunno -- generally speaking, the Wayback Machine is a much better time travel experience than trying to recover a website from an old git commit.Especially since it's not limited to only sites I've created...And in this particular case, all the creator was looking for was old badge images, and they'd generally be in an images directory somewhere no matter whether the site was static or dynamic.reply",
      "Was wondering the same thing. Couldn't they just load https://web.archive.org/save/{site_url} once a month in their Github action instead of managing the storage of these images?reply",
      "Could be useful for company internals sites?reply",
      "Not strictly the topic, but don't miss or sleep on the blog (self-)gamification links[1][2], excellent whimsy.1: https://hamatti.org/posts/i-gamified-my-own-blog/2: https://varunbarad.com/blog/blogging-achievementsreply",
      "Can I mention HN's custom time travel experience?https://news.ycombinator.com/front?day=2025-08-31(available via 'past' in the topbar)reply",
      "Wow, to be honest I never knew that was there - just got back from the 2017s. Such a cool feature to support on-site without going through third parties, more sites should have official archival support like this!Thanks for the share.reply",
      "I have a bit of an internal struggle here. I use a site generator too but I struggle with the question, should I? I recently wrote about why I\u2019m writing pure html and CSS in 2025.https://joeldare.com/why-im-writing-pure-html-and-css-in-202...reply",
      "I'm not sure how using a static site generator would run counter to any of those points. You can simply generate the same website that you've written by hand.EDIT: Well perhaps the \"build steps\" one, but building my Hugo site just involves me running \"hugo\" in my directory.reply",
      "The answer is no. You can do this instead: https://community.qbix.com/t/qbix-websites-loading-quickly/2...See \u201cStatic Sites\u201d section. And realize that DNS caching your pages is essentially making your site \u201cstatic\u201d.reply",
      "Plain text files and version control win again.reply"
    ],
    "link": "https://hamatti.org/posts/static-sites-enable-a-good-time-travel-experience/",
    "first_paragraph": "\n  Varun wrote about\n  gamifying blogging and personal website maintenance\n  which reminded me of the time when\n  I awarded myself some badges for blogging.\n\n  I mentioned this to Varun who asked if I had any screenshots of what it looked\n  like on my website. My initial answer was \u201cno\u201d, then I looked at Wayback\n  Machine but there were not pictures of the badges.\n\n  Then, a bit later it hit me. I don\u2019t need any archived screenshots: my website\n  is built with Eleventy and it's static so I can check out a git commit from\n  the time I had those badges up, fire up Eleventy and see the website \u2014 as it\n  was in the spring of 2021.\n\n  That\u2019s a beauty of a static site generator combined with my workflow of\n  fetching posts from CMS before build time so each commit contains a full\n  snapshot of the website.\n\n  Comparing this to a website that uses a database for posts (like WordPress) or\n  a flow where posts from CMS are not stored in version control but rather\n  fetched at build time only, m"
  },
  {
    "title": "Launch HN: Datafruit (YC S25) \u2013 AI for DevOps",
    "points": 44,
    "submitter": "nickpapciak",
    "submit_time": "2025-09-02T16:08:31 1756829311",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=45104974",
    "comments": [
      "As someone who's been doing Infra stuff for two decades, this is very exciting. There is a lot of mindless BS we have to deal with due to shitty tools and services, and AI could save us a lot of time that we'd rather use to create meaningful value.There is still benefit for non-Infra people. But non-Infra people don't understand system design, so the benefits are limited. Imagine a \"mechanic AI\". Yes, you could ask it all sorts of mechanic questions, and maybe it could even do some work on the car. But if you wanted to, say, replace the entire engine with a different one, that is a systemic change and has farther reaching implications than an AI will explain, much less perform competently. You need a mechanic to stop you and say, uh, no, please don't change the engine; explain to me what you're trying to do and I'll help you find a better solution. Then you need a real mechanic to manage changing the tires on the moving bus so it doesn't crash into the school. But having an AI could make the mechanic do all of that smoother.Another thing I'd love to see more AI use of, is people asking the AI for advice. Most devs seem to avoid asking Infra people for architectural/design advice. This leads to them putting together a system using their limited knowledge, and it turns out to be an inferior design to what an Infra person would have suggested. Hopefully they will ask AI for advice in the future.reply",
      "Glad you find it interesting. A surprising way people are using us right now has been people who are technical but don\u2019t have deep infrastructure expertise, asking datafruit questions about how stuff should be done.Something we\u2019ve been dealing with is trying to get the agents to not over-complicate their designs, because they have a tendency to do so. But with good prompting they can be very helpful assistants!reply",
      "Yeah it's def gonna be hard. So much of engineering is an amalgam of contexts, restrictions, intentions, best practice, and what you can get away with. An agent honed by a team of experts to keep all those things in mind (and force the user to answer important questions) would be invaluable.Might be good to train multiple \"personalities\": one's a startup codebro that will tell you the easiest way to do anything; another will only give you the best practice and won't let you cheat yourself. Let the user decide who they want advice from.Going further: input the business's requirements first, let that help decide? Just today I was on a call where somebody wants to manually deploy a single EC2 instance to run a big service. My first question is, if it goes down and it takes 2+ days to bring it back, is the business okay with that? That'll change my advice.reply",
      "Yes definitely! That's why we do believe the agents, for the time being, will act as great junior devs that you can offload work onto, while as they get better they can slowly get promoted into more active roles.The personalities approach sounds fun to experiment with. I'm wondering if you could use SAEs to scan for a \"startup codebro\" feature in language models. Alas this is not something we get to look into until we think that fine-tuning our own models is the best way to make them better. For now we are betting on in-context learning.Business requirements are also incredibly valuable. Notion, Slack, and Confluence hold a lot of context, but it can be hard to find. This is something that I think the subagents architecture is great for though.reply",
      "I can see the value, but to do the things you're describing, the AI needs to be given fairly highly-privileged credentials.> Right now, Datafruit receives read-only access to your infrastructure> \"Grant @User write access to analytics S3 bucket for 24 hours\"\n>    -> Creates temporary IAM role, sends least-privilege credentials, auto-revokes tomorrowThese statements directly conflict with one another.So it needs \"iam:CreateRole,\" \"iam:AttachPolicy,\" and other similar permissions. Those are not \"read-only.\" And, they make it effectively admin in the account.What safeguards are in place to make sure it doesn't delete other roles, or make production-impacting changes?reply",
      "Ahh. To clarify, changes like granting users access would be done by our agent modifying IaC, so you would still have to manually apply the changes. Every potentially destructive change being an IaC change helps allow the humans to always stay in the loop. This admittedly makes the agents a little more annoying to work with, but safer.reply",
      "So you\u2019re modifying Terraform? How is your tool better than just using an AI-enabled IDE and asking it to apply the change?How is the auto-revoke handled? Will it require human intervention to merge a PR/apply the Terraform configuration, or will it do it automatically?reply",
      "Lots of people have asked us this! We try to do more than just being an AI-enabled IDE by giving the agent access to your infrastructure and observability tools. So you can query over your AWS, get information about metrics over the past few days, etc etc. We also plan to integrate with more DevOps tools as our customers ask for them. We also try to be less like an IDE, and more like an autonomous agent. We've noticed that DevOps engineers actually like being engineers, and enjoy some infrastructure tasks, while there are others that they would rather automate away. Not sure if you have experienced this sentiment?Also, auto-revoke right now can be handled by creating a role in Terraform that can be assumed and expires after a certain time. But we\u2019re exploring deeper integrations with identity providers like Okta to handle this better.reply",
      "IMO it is a smart decision to implement this as a self-hosted system, and have the AI make PRs against the IaC configuration - for devops matters, human-in-the-loop is a high priority. I'm curious how well this would work if I'm using Pulumi or the AWS CDK (both are well-known to LLMs).I consulted for an early stage company that was trying to do this during the GPT-3 era. Despite the founders' stellar reputation and impressive startup pedigree, it was exceedingly difficult to get customers to provide meaningful read access to their AWS infrastructure, let alone the ability to make changes.reply",
      "LLMs are pretty awesome at Terraform, probably because there is just so much training data. They are also pretty good at the AWS CDK and Pulumi to a bit of a lesser extent, but I think giving them access to documentation is what helps make them the most accurate. Without good documentation the models start to hallucinate a bit.And yeah, we are noticing that it\u2019s difficult to convince people to give us access to their infrastructure. I hope that a BYOC model will help with that.reply"
    ],
    "link": "item?id=45104974",
    "first_paragraph": ""
  },
  {
    "title": "'World Models,' an old idea in AI, mount a comeback (quantamagazine.org)",
    "points": 136,
    "submitter": "warrenm",
    "submit_time": "2025-09-02T16:53:29 1756832009",
    "num_comments": 49,
    "comments_url": "https://news.ycombinator.com/item?id=45105710",
    "comments": [
      "I\u2019ve been working on board game ai lately.Fwiw nothing beats \u2018implement the game logic in full (huge amounts of work) and with pruning on some heuristics look 50 moves ahead\u2019. This is how chess engines work and how all good turn based game ai works.I\u2019ve tried throwing masses of game state data at latest models in pytorch. Unusable. It\nMakes really dumb moves. In fact one big issue is that it often suggests invalid moves and the best way to avoid this is to implement the board game logic in full to validate it. At which point, why don\u2019t i just do the above scan ahead X moves since i have to do the hard parts of manually building the world model anyway?One area where current ai is helping is on the heuristics themselves for evaluating  best moves when scanning ahead. You can input various game states and whether the player won the game or not in the end to train the values of the heuristics. You still need to implement the world model and look ahead to use those heuristics though! When you hear of neural networks being used for go or chess this is where they are used. You still need to build the world model and brute force scan ahead.One path i do want to try more: In theory coding assistants should be able to read rulebooks and dynamically generate code to represent those rules. If you can do that part the rest should be easy. Ie. it could be possible to throw rulebooks at ai and it play the game. It would generate a world model from the rulebook via coding assistants and scan ahead more moves than humanly possible using that world model, evaluating to some heuristics that would need to be trained through trial and error.Of course coding assistants aren\u2019t at a point where you can throw rulebooks at them to generate an internal representation of game states. I should know. I just spent weeks building the game model even with a coding assistant.reply",
      "You probably know this, but things heavily depend on the type of board game you are trying to solve.In Go, for instance, it does not help much to look 50 moves ahead.  The complexity is way too high for this to be feasible, and determining who's ahead is far from trivial.  It's in these situations where modern AI (reinforcement learning, deep neural networks) helps tremendously.Also note that nobody said that using AI is easy.reply",
      "Alphago (and stockfish that another commenter mentioned) still has to search ahead using a world model. The AI training just helps with the heuristics for pruning and evaluation of that search.The big fundamental blocker to a generic \u2018can play any game\u2019 ai is the manual implementation of the world model. If you read the alphago paper you\u2019ll see \u2018we started with nothing but an implementation of the game rules\u2019. That\u2019s the part we\u2019re missing. It\u2019s done by humans.reply",
      "Note that MuZero did better than AlphaGo, without access to preprogrammed rules: https://en.wikipedia.org/wiki/MuZeroreply",
      "Minor nitpick: it did not use preprogrammed rules for scanning through the search tree, but it does use preprogrammed rules to enforce that no illegal moves are made during play.reply",
      "During play, yes, obviously you need an implementation of the game to play it. But in its planning tree, no:> MuZero only masks legal actions at the root of the search tree where the environment can be queried, but does not perform any masking within the search tree. This is possible because the network rapidly learns not to predict actions that never occur in the trajectories\nit is trained on.https://arxiv.org/pdf/1911.08265reply",
      "That is exactly what the commenter was saying.reply",
      "The more detailed clarification on what \"preprogrammed rules\" actually means in this case made the entire discussion significantly more clear to me. I think it was helpful.reply",
      "Implementing a world model seems to be mostly solved by LLMs.  Finding one that can be evaluated fast enough to actually solve games is extremely hard, for humans and AI alike.reply",
      "What are you talking about?reply"
    ],
    "link": "https://www.quantamagazine.org/world-models-an-old-idea-in-ai-mount-a-comeback-20250902/",
    "first_paragraph": "\nAn editorially independent publication supported by the Simons Foundation.\n\nGet the latest news delivered to your inbox.\nCreate a reading list by clicking the Read Later icon next to the articles you wish to save.Type search term(s) and press enterPopular\n                                    SearchesSeptember 2, 2025A world model is like a computational snow globe \u2014 a miniature representation of reality.Nash Weerasekera for\u00a0Quanta MagazineContributing WriterSeptember 2, 2025The latest ambition of artificial intelligence research \u2014 particularly within the labs seeking \u201cartificial general intelligence,\u201d or AGI \u2014 is something called a world model: a representation of the environment that an AI carries around inside itself like a computational snow globe. The AI system can use this simplified representation to evaluate predictions and decisions before applying them to its real-world tasks. The deep learning luminaries Yann LeCun (of Meta), Demis Hassabis (of Google DeepMind) and Yoshua Ben"
  },
  {
    "title": "Show HN: LightCycle, a FOSS game in Rust based on Tron (github.com/tortured-metaphor)",
    "points": 6,
    "submitter": "DavidCanHelp",
    "submit_time": "2025-09-03T00:08:54 1756858134",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=45110748",
    "comments": [
      "This readme could really benefit from a screenshot.reply",
      "Came here to say the exact same thing. At a glance, is it 3D or 2D? Console game like snake or with color and sprites? Etc.Either screenshots or a gif would be nice.reply"
    ],
    "link": "https://github.com/Tortured-Metaphor/LightCycle",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A Rust LightCycle Game\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.A classic TRON-inspired light cycle game built with Rust and ggez.Navigate your light cycle around the arena, leaving a trail behind you. Avoid crashing into walls, your own trail, or your opponent's trail. The last cycle standing wins!Use your boost strategically - it doubles your speed but drains energy quickly. Energy regenerates when not boosting.Built with:This project is open source and available under the MIT License.\n        A Rust LightCycle Game\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page."
  },
  {
    "title": "The staff ate it later (wikipedia.org)",
    "points": 150,
    "submitter": "gyomu",
    "submit_time": "2025-09-02T15:24:48 1756826688",
    "num_comments": 81,
    "comments_url": "https://news.ycombinator.com/item?id=45104289",
    "comments": [
      "> Then Pharaoh also called for the wise men and the sorcerers, and they also, the magicians of Egypt, did the same with their secret arts. For each one threw down his staff and they turned into serpents. But Aaron\u2019s staff swallowed up their staffs.- Exodus 7:1-12 (NIV)Many moons ago I had a girlfriend who worked on an nationally broadcast afternoon show where they often had guest chefs demonstrating dishes, so I would come home from my thankless PhD work to eat Michelin-starred food from a lunchbox. Overall not so bad.reply",
      "Cool story, I upvoted because the downvotes felt a bit harsh. But what does the first part have to do with the second part?reply",
      "\"staff\" meaning either the crew filming a TV show, or meaning a magical staffreply",
      "I get it now. More staff engineers than I expected in the Bible.reply",
      "pretty much everything is in the Bible if you look, even automobiles: \"and G-d drove Adam and Eve from the Garden of Eden in His Fury\"https://duckduckgo.com/?t=ffab&q=plymouth%20fury&ia=images&i...reply",
      "\u201cJesus and his disciples were all in one Accord\u201dreply",
      "And don't forget that Moses received the Ten Commandments on tablets, although it doesn't say whether he used iPads or something running Android.reply",
      "It's rumored that God was wary of humans having apple products.reply",
      "I gave up tablets and pills of all kinds a long time ago now, but, at the time, if god himself had popped up to tell me something, it wouldn't have been a huge surprise.reply",
      "And that Revelation 5/6 contain the original doomscroll.reply"
    ],
    "link": "https://en.wikipedia.org/wiki/The_staff_ate_it_later",
    "first_paragraph": ""
  },
  {
    "title": "Still Asking: How Good Are Query Optimizers, Really? [pdf] (vldb.org)",
    "points": 16,
    "submitter": "matt_d",
    "submit_time": "2025-08-31T05:53:19 1756619599",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=45080683",
    "comments": [
      "Adapting the plan at runtime seems like the most universal solution for optimizer edge cases and is already implemented in the big 3.If you think about it, the adaptive mechanism doesn't have to be perfect to have a lot of uplift. Even coarse grain detection and starting from zero can make a huge difference if the alternative would be a query burning up the server for hours and a failed batch job.reply",
      "What are some more information on the state of the art in runtime adaptation? I confess I do not feel like I possess such a thing from the databases I regularly use.Adaptation sounds very compelling; if instead of emitting a plan based on a cardinality estimate, we emit a plan and a range of reasonable intermediate cardinalities together with expected time, and interrupt the original plan when the expectations are exceeded by an order of magnitude, and perform alternative plans based on newly gathered physical information, it sounds like it would be greatly beneficial. Are there concrete reasons that this has not been done (e.g. cost, complexity)?reply",
      "> Adapting the plan at runtime seems like the most universal solution for optimizer edge cases and is already implemented in the big 3But the issues frequently aren't edge cases and frequently are at runtime (i.e. new query), it's just the best the optimizers can do with limited info (cardinality) and a limited duration to evaluate alternatives.reply"
    ],
    "link": "https://www.vldb.org/pvldb/vol18/p5531-viktor.pdf",
    "first_paragraph": ""
  },
  {
    "title": "What Happens During Startup? (eclecticlight.co)",
    "points": 11,
    "submitter": "colinprince",
    "submit_time": "2025-08-30T02:35:00 1756521300",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://eclecticlight.co/2025/08/29/what-happens-during-startup/",
    "first_paragraph": "With careful observation and a little knowledge of the startup sequence of an Apple silicon Mac, you can learn a lot about what can and can\u2019t happen during that sequence. This article explains how, with examples from the log of a Mac mini M4 Pro.In broad terms, startup of an Apple silicon Mac consists of the following sequence of events:The opening entry in the log is the boot announcement of\n=== system boot:\nfollowed by the boot UUID. There\u2019s then a gap of 5 seconds or more before the next entry, which marks the start of kernel boot. Those seconds are the silent phase during which the LLB and iBoot are doing their thing. They don\u2019t write to the Unified log, but leave fragments of cryptic information known as breadcrumbs, which you can\u2019t make use of. The kernel then writes its usual welcome of\nkprintf initialized\nand the following four seconds or so are filled by log entries from the kernel.During this phase, the system clock is synchronised, and wallclock time adjusted, usually twice "
  },
  {
    "title": "Indices, not Pointers (joegm.github.io)",
    "points": 10,
    "submitter": "vitalnodo",
    "submit_time": "2025-09-02T23:21:30 1756855290",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=45110386",
    "comments": [
      "This is a very tempting and commonly used strategy in Rust to bypass the borrow checker. I've used it to implement tries/DFAs with great success (though I can't find the code anymore)reply",
      "> There is a pattern I\u2019ve learned while using Zig which I\u2019ve never seen used in any other language.I've done this in small applications in C (where nodes were already being statically allocated) and/or assembly (hacking on an existing binary).No idea about the effect on speed in general; I was trying to save a few bytes of storage in a place where that mattered.reply",
      "Reinventing malloc to some degree.reply",
      "malloc with NEAR ad FAR pointers (as was used in MSDOS on processors with segmented memory).reply"
    ],
    "link": "https://joegm.github.io/blog/indices-not-pointers/",
    "first_paragraph": "There is a pattern I\u2019ve learned while using Zig which I\u2019ve never seen used in any other language. It\u2019s an extremely simple trick which - when applied to a data structure - reduces memory usage, reduces memory allocations, speeds up accesses, makes freeing instantaneous, and generally makes everything much, much faster. The trick is to use indices, not pointers.This is something I learned from a talk by Andrew Kelley (Zig\u2019s creator) on data-oriented design. It\u2019s used in Zig\u2019s compiler to make very memory-efficient ASTs, and can be applied to pretty much any node-based data structure, usually trees.So what does this mean exactly? Well, to use indices means to store the nodes of the data structure in a dynamic array, appending new nodes instead of individually allocating them. Nodes can then reference each other via indices instead of pointers.\nA comparison of memory layouts with different storage methodsPretty simple, right? But this strategy has some major performance benefits.A pointer"
  },
  {
    "title": "Untangling the myths and mysteries of Dvorak and QWERTY (2023) (aresluna.org)",
    "points": 43,
    "submitter": "kens",
    "submit_time": "2025-08-29T13:59:33 1756475973",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=45064241",
    "comments": [
      "The real problem with current keyboards is the physical arrangement of the keys. Staggered rows instead of columns make them less ergonomic, the oversized spacebar wastes much of the most valuable space on the keyboard. The thumb as one of the strongest fingers has almost nothing to do, with both thumbs mostly sharing a single key while typing text. While the weak pinky finger has to cover more keys than the others. These things are more significant than qwerty vs dvorak.Need to type faster? Spend some time practising every day and you will gain more speed within weeks than from just switching layouts. Most people don't as speed often isn't actually that important. I myself am bottlenecked by my brain, not my typing speed. Need less hand movement? Placing symbols, arrow keys etc as secondary function onto the central keys with a programmable keyboard helps with that, changing to dvorak doesn't as much because on a modern keyboard you can reach all letters without hand movement either way.reply",
      "\"The thumb as one of the strongest fingers has almost nothing to do, with both thumbs mostly sharing a single key while typing text.\"To be fair, that single key is used rather excessively compared to the rest.reply",
      "That key makes up about 15% in English text, and it could be covered by 10% of fingers but instead it's 20%. Meanwhile every use of shift, return, backspace, ctrl etc is done with the weakest fingers and often include some hand stretching to reach those keys. Altough I haven't looked at actual keypress stats and how those are distributed across fingers. Might be interesting to look into.On my keyboard I cover six keys with my two thumbs. It eliminates almost all hand movement and guess what, I feel a difference in my pinky fingers but not in the thumbs. I'm not saying every keyboard should be like this, but I think on a large scale you can probably improve wirst and hand health in the population by making a few small tweaks in how keys are arranged.reply",
      "> Meanwhile every use of shift, return, backspace, ctrl etc is done with the weakest fingers and often include some hand stretching to reach those keys.As previously stated, this is only an issue for non-practiced typers. With practice, it no longer feels like stretching. It just becomes muscle memory. It's like a novice golfer complaining that it hurts their hips when they rotate through the swing, or a tennis player complaining that trying to add spin with a wrist twist feels weird, or any millions of other example of \"feels weird without practice\". Hell, most people can't do the most basic of yoga poses without practice. Muscles need to be stretched and trained into doing what you want them to do. Once they are, all of the complaints go away and things feel normal.Is QWERTY the most efficient, no. But as someone else commented, speed is not my issue. Thinking what needs to be typed is definitely my speed regulator. If I were to just do basic text dictation or re-typing while reading a direct source, my speeds increase dramatically.I find that most typing complaints are from those that never had formal typing instruction and are self taught with games or similar. I was fortunate to have one full year of typing while in high school, and it is by far the most used class instruction I've ever had.reply",
      "Nah dawg. I was a practiced touch typist for years. We had oodles of typing in elementary school. Pain in my pinkies didn't get better. Only worse because keys like {}| are far more important in programming than writing English and add to pinkie stress. Switching to a Kinesis Advantage with thumb keys and layers to put keys like {} under better fingers resolved my problem.reply",
      "I had never considered until now that my left thumb never touches the keyboard.reply",
      "I still miss a Compaq keyboard I had with a split spacebar. You could choose various options; I had the left side as backspace.I don't agree with your statement about switching though. I was a very good Qwerty typist, but I'm much faster and more accurate on Dvorak. Switching was one of the most useful things I have ever done.reply",
      "I switched to Dvorak around five years ago now. I decided to switch at the same time as switching to a columnar split keyboard (specifically the ZSA Moonlander, which is still my primary keyboard (my secondary is the ZSA Voyager for travel)). I did this switch at the same time, because I felt like it would be a sufficiently different keyboard that I wouldn't have as concrete of muscle memory, and I wouldn't be fighting my QWERTY instincts as much.A big part of why I wanted to make the switch at all is because I was experiencing fatigue in my hands, and I felt that it could be due to my improper typing habits that I developed from mostly learning to type through playing videogames. I wanted to properly type from the home row, and the split columnar keyboard basically enforces that, and Dvorak makes it even easier.I will say though, I type at about the same speed that I did before I made the switch. Switching layouts almost certainly will not enable you to type faster. Switching layouts encourages you to deliberately practice typing on that layout (I did lots of typing challenges while learning) which will make you faster. The biggest benefit for me has actually been in my back! The split keyboard allows me to rotate my shoulders back a lot more, which makes me feel way better at the end of the day. My hands are less fatigued too, but I don't feel like that was as big of a deal for me.reply",
      "Yeah but with QWERTY you can type out the word 'typewriter' using only the top row of the keyboard; so if you're a typewriter salesman QWERTY is the way to go.(Been using Dvorak for 25 years now. Doesn't matter what the physical keyboard layout is - currently I'm using a Swiss layout)reply",
      "There are no more typewriter salesmen.reply"
    ],
    "link": "https://aresluna.org/the-primitive-tortureboard/",
    "first_paragraph": "\n\t\t\t\tMarcin Wichary\n\t\t\t\n\t\t\t\tDecember\u00a02023\u00a0/ 8,000\u00a0words\u00a0/ 33\u00a0photos\n\t\t\t\n\t\t\tThis essay was originally published in December 2023 as sixth chapter of the book Shift Happens.\n\t\t\n\t\t\tThere weren\u2019t many who hated QWERTY more. To his credit, there was a lot to hate. The layout seemed random, with letters strewn around without rhyme or reason. Watching someone type on it felt painful: fingers flailed wildly all over the place, common letters far away from the home row necessitated more travel, and there were long stretches of one hand doing all the work while the other sat idle. Adding insult to injury, it wasn\u2019t even really that difficult to spot someone using a QWERTY-wearing typewriter. The layout was already so ubiquitous, it was known as \u201cthe universal keyboard.\u201d\n\t\t\n\t\t\tBut he felt it didn\u2019t deserve to be. \u201cIt would be difficult to design a key-board over which the hands must travel farther,\u201d he said. There was nothing smart about it; QWERTY \u201cwas not arranged on any principle, but resulted"
  }
]