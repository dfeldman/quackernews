[
  {
    "title": "Kalman Filter Tutorial (kalmanfilter.net)",
    "points": 121,
    "submitter": "ColinWright",
    "submit_time": "2025-01-18T21:50:15 1737237015",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=42751690",
    "comments": [
      "Always telling this whenever the topic of Kalman Filters come up:If you're learning the Kalman Filter in isolation, you're kind of learning it backwards and missing out on huge \"aha\" moments that the surrounding theory can unlock.To truly understand the Kalman Filter, you need to study Least Squares (aka linear regression), then recursive Least Squares, then the Information Filter (which is a different formulation of the KF).\nThen you'll realize the KF is just recursive Least Squares reformulated in a way to prioritize efficiency in the update step.This PDF gives a concise overview:[1] http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/...\n \nreply",
      "I appreciate you taking the time to help people understand higher level concepts.From a different perspective... I have no traditional background in mathematics or physics. I do not understand the first line of the pdf you posted nor do I understand the process for obtaining the context to understand it.But I have intellectual curiosity. So the best path forward for me understanding is a path that can maintain that curiosity while making progress on understanding. I can reread the The Six (Not So ) Easy Pieces and not understand any of it and still find value in it. I can play with Arnold's cat and, slowly, through no scientific rigor other than the curiosity of the naked ape, I can experience these concepts that have traditionally been behind gates of context I do not possess keys to.http://gerdbreitenbach.de/arnold_cat/cat.html\n \nreply",
      "There is no royal road. You only need to put in a fraction of the effort people before you did. You may be just too dumb to ever understand what brilliant people before you devoted their life to. And that's okay. Just stop whining about it in public.\n \nreply",
      "I think the easiest way depends on your background knowledge. If you understand the linearity of the Gaussian distribution and the Bayesian posterior of Gaussians, the Kalman filter is almost trivial.For (1D) we get the prior from the linear prediction X'1 = X0*a + b, for which mean(X'1) = mean(X0)*a + b and var(X'1) = var(X0)*a, where a and b give the assumed dynamics.The posterior for Gaussians is the precision weighted mean of the prior and the observation likelihood: X1 = (1 - K)*X'1 + Y*K, where the weighting K = (1/var(X'1))/(1/var(X'1) + 1/var(Y)), where Y is the Gaussian observation (with zero mean for exposition).Iterating this gives the Kalman filter. Generalizing this to multiple dimensions is straightforward given the linearity of multidimensional Gaussians.This is how (after I understood it) it makes it really simple to me, but things like linearity of (multidimensional) Gaussians and Gaussian posterior as such probably are not.\n \nreply",
      "You can keep telling this, but this \u201cesoteric\u201d math is often too much for the people actually implementing the filters.\n \nreply",
      "It's bread and butter math for physics, Engineering (trad. Engineering), Geophysics, Signal processing etc.Why would anyone have people implementing Kalman filters who found the math behind them \"esoteric\"?Back in the day, in my wet behind the ears phase, my first time implementing a Kalman Filter from scratch, the application was to perform magnetic heading normalisation for on mag data from an airborne geophysical survey - 3 axis nanotesla sensor inputs on each wing and tail boom requiring a per survey calibration pattern to normalise the readings over a fixed location regardless of heading.This was buried as part of a suite requiring calculation of the geomagnetic reference field (a big paramaterised spherical harmonic equation), upward, downward and reduce to pole continuations of magnetic field equations, raw GPS post processing corrections, etc.where \"etc\" goes on for a shelf full of books with a dense chunk of applied mathematics\n \nreply",
      "I understood it as reestimation with a dynamic weight factor based on the perceived error factor. I know it\u2019s more complex than that but this simplified version I needed at one point and it worked.\n \nreply",
      "That\u2019s the one should learn any subject\u2014-be it physics, chemistry, math, etc. However, textbooks don\u2019t follow that technique.\n \nreply",
      "I strongly recommend Elements of Physics by Millikan and Gale for anyone who wants to learn pre-quantum physics this way.\n \nreply",
      "You are probably right, but many folks following your advice will give up halfway through and never get to KF.\n \nreply"
    ],
    "link": "https://www.kalmanfilter.net/default.aspx",
    "first_paragraph": "\"If you can't explain it simply, you don't understand it well enough.\"\r\n                                    The Kalman Filter algorithm is a powerful tool for estimating and predicting system states in the presence of uncertainty and is widely used as a fundamental component in applications such as target tracking, navigation, and control.\r\n                                \r\n                                    Although the Kalman Filter is a straightforward concept, many resources on the subject require extensive mathematical background and fail to provide practical examples and illustrations, making it more complicated than necessary.\r\n                                \r\n                                    Back in 2017, I created an online tutorial based on numerical examples and intuitive explanations to make the topic more accessible and understandable. The online tutorial provides introductory material covering the univariate (one-dimensional) and multivariate (multidimensional) Kalma"
  },
  {
    "title": "Pharaoh's Tomb HD \u2013 A Remake Made in JavaScript with Kaplay (iocaihost.me)",
    "points": 47,
    "submitter": "JSLegendDev",
    "submit_time": "2025-01-18T22:44:58 1737240298",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=42752023",
    "comments": [
      "This was made with the JavaScript Library Kaplay. Check it out here : https://kaplayjs.comGame was made by Misanthrope. His twitter can be found here : https://x.com/petergencur\n \nreply",
      "Great job making this work intuitively on a mobile touch screen.\n \nreply",
      "I was thinking that I haven't yet experienced this particular control mechanic for a video game. It's not perfect but it works on a mobile touch screen.\n \nreply",
      "Controls hint! \u2193You can press R to reset the level. And L to load the latest autosave. I didn't expect game to be shared this early so there is missing info about these controls :). (Also, touch users would be out of luck in this one, sorry..)Btw, there are just 5 levels so far (original has 60).\n \nreply",
      "Nice. I didn't detect any measurable input lag - which is good because these old-school games can be tough as nails.I briefly flipped through the docs - it seems like Kaplay shares a lot in common with PhaserJS. I'd be interested in a comparison between the two frameworks.\n \nreply",
      "I made a video explaining why I prefer to use Kaplay. It's 12 min long.It's not a direct comparison with Phaser but I think it could give you a clear picture of where Kaplay stands and what it offers compared to Phaser.Link here : https://www.youtube.com/watch?v=o8a6jXRnYfc\n \nreply",
      "Why is it better than just doing it in WebAssembly?With WebAssembly you'll get native builds as well as web builds, higher performance, and you're not restricted in the language you use.\n \nreply",
      "Without going into the Web/WebAssembly debate,\nKAPLAY is just too good as a library to pass up. It's intuitive, easy and comes with a lot built-in. It makes making games actually fun with it's ECS-like API.If KAPLAY existed as a WASM runtime or in another language, I would use it.\n \nreply",
      "Why would you compare a game framework to wasm?\n \nreply",
      "Because WebAssembly brings every game framework to the web.\n \nreply"
    ],
    "link": "https://pt-hd.iocaihost.me/",
    "first_paragraph": ""
  },
  {
    "title": "Nation-scale Matrix deployments will fail using the community version of Synapse (matrix.org)",
    "points": 32,
    "submitter": "zaik",
    "submit_time": "2025-01-18T23:52:23 1737244343",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=42752402",
    "comments": [
      "At the end of the day, the money has to come from somewhere. There's only a handful of companies that make the open source profit game work and last I checked, most of them make their money from B2B deals with support, SLAs, etc. If you want to deploy something at nation scale, surely you can afford to pay for it.\n \nreply",
      "I find this comment interesting: https://mastodon.matrix.org/@element/113848890455666110 where IMO it is stated that Deutsche Telekom (with HessenConnect 2.0) did the free-riding and \"directly contributed to layoffs at Element in 2022 and 2023\". It sounds as if Telekom changed its mind after an initial contract or something. Or was Telekom one of many companies that did a similar thing?I know from my own experience that making money based on an open source project is extremely challenging.However, I don't understand why they frame their Pro version in this way and point to failure instead of success. I can't think of a better way (they will have to do the work :)). But it's already a good thing if they've won them as \"Matrix believers\", and they shouldn't scare them to get there.\n \nreply",
      "I don't like this. I only have a handful of users but I could still benefit from the additional vulnerability proofing of Rust. It's not just about the scaling. My data security matters too. I have not exposed it to the internet for a while now because I already had my doubts.This really sounds like \"haha you community members are stuck with a sucky version of our software and you're never getting the good one!\". It's not exactly making me feel valued as a community member. And we shouldn't forget this is what put matrix on the map. This feels very bait-and-switch'ey.Also pointing out the flaws in one's own software doesn't inspire a lot of confidence. I think it would have been a sounder approach to paywall real enterprise features rather than the core product. Such as SSO, auditing, legal holds, that stuff. Basically all the stuff that Microsoft Teams has in Purview.Ps I thought dendrite was supposed to be the next big thing? It feels like element is often jumping on new things like big rewrites such as element X instead of improving the existing products.\n \nreply",
      "Dendrite was officially cancelled late last year. The repo was archived.https://github.com/matrix-org/dendrite\n \nreply",
      "The worst part is the article reads like someone is nervous that the nation states are busting hosting services from somebody doing horizontal scaling via multiple server instances rather than doing the Proper rust implementation. The classic tension of oss vs paid version.\n \nreply",
      "Perhaps this should point instead to the actual blog post this link references: https://element.io/blog/scaling-to-millions-of-users-require...\n \nreply",
      "So matrix is commercialized now and the \"real\" software version is as expensive as Slack/Rocket? That's really disappointing...\n \nreply",
      "Is there an alternative? Can you set up your own server and it's free (as in beer)?\n \nreply",
      "Yes. The article is about how a particular open source Matrix server implementation isn't suitable for millions of simultaneous users, and you should pay for the non-open-source version.But if you are merely handling tens of thousands of users, no problem. And if you have the budget to handle millions of users, maybe you have the budget to pay development as well as operations staff?\n \nreply",
      "Side note: why do people still say free as in beer? Do people not assume free automatically means price, unless there\u2019s a qualifier?\n \nreply"
    ],
    "link": "https://mastodon.matrix.org/@element/113842786942364269",
    "first_paragraph": ""
  },
  {
    "title": "VS Code Pets (github.com/tonybaloney)",
    "points": 218,
    "submitter": "vortex_ape",
    "submit_time": "2025-01-18T18:17:59 1737224279",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=42750195",
    "comments": [
      "This has reminded me of an anecdote. I work on a corporate social network. One day a colleague from the parent company comes to us scared because instead of seeing the people photos and the attached images, he saw strange images. As in the past we had some scare with xss reflected, we immediately got scared and went straight to investigate the matter. It turned out that the colleague had a Firefox extension installed that changed his images for Nicholas Cage's faces. He didn't remember having done it, but we did remember his blunder hahaha\n \nreply",
      "At university, we used this extension to teach our classmates about good security practices, such as locking their computers when left unattended. It was fun, especially when professors didn't lock their computers. And my former classmates did learn to lock their computers :)\n \nreply",
      "violating security policies in order to \u201cteach a lesson\u201d is a sure fire way to get people to lose trust in you.Accessing someone\u2019s computer and manipulating the software was instant termination at my old company. Some new security guy joined and tried to do what you did. Find unlocked computers and mess with them to prove a point. He lasted a week.\n \nreply",
      "It all depends on the company of course.I worked at a place where if you left your laptop unlocked, anyone could use your slack account to announce you were buying breakfast for the team tomorrow. That was more effective than any training video they could have made us watch. But I obviously wouldn't do something like that as a lone wolf.\n \nreply",
      "Ironic, given that a ton of the security dogma these days is \"don't trust anyone\" --- you can guess why that started happening; precisely because of people like him.\n \nreply",
      "That's hilarious. Sounds like someone was pranking your colleague.Was this the extension? https://addons.mozilla.org/en-US/firefox/addon/niccage/\n \nreply",
      "Damn, I was half hoping it was doing some deepfake face swapping rather than just totally replacing the whole image. Part of me would love to install a \"Being John Malkovich\" style face replacement plugin onto someone's machine.\n \nreply",
      "Yes, it was that one!\n \nreply",
      "I would like to be able to feed my pets, ideally feeding them obsolete parts of my code.\n \nreply",
      "\"Your pet feed on comments so be aware of that!\"\n \nreply"
    ],
    "link": "https://github.com/tonybaloney/vscode-pets",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Adds playful pets \ud83e\udd80\ud83d\udc31\ud83d\udc36 in your VS Code window\n      \n    Puts a small, bored cat, an enthusiastic dog, a feisty snake, a rubber duck, or Clippy \ud83d\udcce in your code editor to boost productivity.\n    \n\nReport a Bug\n    \u00b7\n    Request feature\n\n\nInstall this extension from the VS Code marketplace.ORWith VS Code open, search for vscode-pets in the extension panel (Ctrl+Shift+X on Windows/Linux or Cmd(\u2318)+Shift+X on MacOS) and click install.ORWith VS Code open, launch VS Code Quick Open (Ctrl+P on Windows/Linux or Cmd(\u2318)+P on MacOS), paste the following command, and press enter.ext install tonybaloney.vscode-petsCongrats on installing joy! Enjoy interacting with these cute pixelated pets. Read below to get a full understanding of this extension. Not convinced? Watch our extension spotlight on Visual Studio Code.After installing, open the command "
  },
  {
    "title": "Show HN: Interactive systemd \u2013 a better way to work with systemd units (isd-project.github.io)",
    "points": 357,
    "submitter": "kai-tub",
    "submit_time": "2025-01-18T16:22:03 1737217323",
    "num_comments": 121,
    "comments_url": "https://news.ycombinator.com/item?id=42749402",
    "comments": [
      "This looks neat. I have to look up the very fiddly and unintuitive systemd commands all the time. service start? service.foo start? start foo.service? Oh right, sudo systemctl start service.fooAnd the feedback is so bad. It should know everything in its own config dir and tell me how to do what I want to do. Was it enabled? I forget. How do I look at logs? Oh right journalctl. Also the layout of things with lots of symlinks and weird directories in places that annoy my 90's linux sysadmin brain. Why am I looking at /lib/systemd/systemI am annoyed by the redundant \"systemd/system\" directory name every time I have to go there. At this point, just promote it to /etc/systemd and build a better CLI.As a very occasional linux sysadmin just trying to make things work, the \"typing at a console\" systemd interfaces are not fun to work with. Maybe nobody should be doing that. In an enterprise, sure that's different. I think interfaces should be human, and linux should still be fun.\n \nreply",
      "> I have to look up the very fiddly and unintuitive systemd commands all the time. service start? service.foo start? start foo.service? Oh right, sudo systemctl start service.fooI don't get this complaint. It's the same order as almost every other command-line utility that has subcommands: <command> <subcommand> <thing to operate on>. To me, that kind of consistency is very intuitive.    systemctl stop my-service\n    systemctl status my-service\n    git add my-file\n    git remote remove upstream\n    apt install my-package\n    docker run my-container\n    adb push local-file remote-file\n \nreply",
      "I feel the same way. The big part for me is that it tells us that owyn doesn't use tab completion if they're forgetting about the \".service\" part. Sure, I don't remember either, I don't have to.I'll add the abstraction for anyone confused  program [command [subcommand]] [flags] [object]\n  e.g.s\n  systemctl status sshd.service\n  systemctl enable --now sshd.service\n  touch -c test.sh\n  echo 'Hello World'\n  echo \"Hello ${USER}\"\n  \nAnything in brackets is optional and might not appear or be available. By command I mean a category of commands. Such as 'pip install' vs 'pip uninstall', which are sub-programs inside the main program. But this can have layers such as 'uv pip install'.  Often flags can be used in any order because you'll just loop over all the arguments but this is still the standard order.There's also the two actor pattern  program [command [subcommand]] [flags] source destination\n  e.g.s\n  cp /foo/bar/baz.txt \"${HOME%/}/\"\n  scp -i \"${HOME}/.ssh/foo\" ${HOME}/to_upload.sh user@remote:~/\n  dd if=/dev/urandom of=/dev/diskToDestroy\n  rsync /mnt/hdd1/ /mnt/hdd2/\n \nreply",
      "Also the `.service` part is optional anyway for most commands (including the start, stop, restart etc. ones they use in their examples). Only commands that can operate on services and other non-service units require it.\n \nreply",
      "For me:* /etc/init.d/my-service stopand Ubuntu\u2019s:* service my-service stopboth lurk in my brain.\n \nreply",
      "Sure, it's different from the old way, but I don't think \"unintuitive\" is the right word for that. systemd forced people to change their habits so that it could be more intuitive. Of course, people are going to disagree about whether it was worth it - it's the age-old question about breaking backwards compatibility for the sake of minor improvement. Personally, I got used to it pretty quickly and I like it more than the old commands now.\n \nreply",
      "The way I remember this is that the old way didn't allow for applying the verb to multiple units. Now you can \"restart\" multiple units, i.e. `systemctl restart nginx webapp`, etc.\n \nreply",
      "Yeah, I gave up resisting and I\u2019m rolling with it. Ok, fine, this is the way now.And yet my fingers still want to type it the other way.\n \nreply",
      "Yet, the \"ease\" of use is a joke.  Every try to stop and start a service at different points?  At points you choose?  Have fun!Ah well, it's all been said before.\n \nreply",
      "When it comes to starting and stopping services I want the verb to go last. Way easier to press up, backspace backspace backspace o p to change service ssh start to service ssh top. This is a frequent pattern I follow as I start/stop/restart/reload. Having to go back at least one word adds keystrokes that aren\u2019t necessary.\n \nreply"
    ],
    "link": "https://isd-project.github.io/isd/",
    "first_paragraph": "isd \u2014 a better way to work with systemd unitsSimplify systemd management with isd!\nisd is a TUI offering fuzzy search for units, auto-refreshing previews,\nsmart sudo handling, and a fully customizeable interface\nfor power-users and newcomers alike.NoteIf you prefer an example terminal recording over text,\njump to the next section.If you ever became frustrated while typing:This tool is for you!But isd is not only designed for systemd power-users!\nisd lowers the bar to interact with systemd units and provides a unified\ninterface that only shows relevant information and commands.\nIf you only ever run systemctl status <unit>, then isd is might still be for you,\nsince isd will auto-refresh the output.If you are interested, read on and take a look at the recorded terminal session.isd can currently be installed in three different ways:An AppImage is a single self-containing executable, similar to a Windows\n.exe and MacOS .dmg file. This should make it easy to run isd\non any Linux distribution"
  },
  {
    "title": "Unix Spell Ran in 64kB RAM (codingconfessions.com)",
    "points": 6,
    "submitter": "madmax108",
    "submit_time": "2025-01-19T00:31:38 1737246698",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://blog.codingconfessions.com/p/how-unix-spell-ran-in-64kb-ram",
    "first_paragraph": ""
  },
  {
    "title": "Beating cuBLAS in Single-Precision General Matrix Multiplication (salykova.github.io)",
    "points": 28,
    "submitter": "skidrow",
    "submit_time": "2025-01-15T09:20:03 1736932803",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://salykova.github.io/sgemm-gpu",
    "first_paragraph": "\nJan 12, 2025\n      \u2022 Aman SalykovThis project is inspired by the outstanding works of Andrej Karpathy, George Hotz, Scott Gray, Horace He, Philippe Tillet, Jeremy Howard, Lei Mao and the best CUDA hackers from the GPU MODE community (Discord server). A special thanks to Mark Saroufim and Andreas K\u00f6pf for running GPU MODE and all you\u2019ve done for the community.The code is available at sgemm.cu. This article complements my blog post, which covers the implementation of FP32 matrix multiplication that outperforms BLAS libraries on modern Intel and AMD CPUs. Today we\u2019ll walk through a GPU implementation of SGEMM (Single-precision GEneral Matrix Multiply) operation defined as C := alpha*A*B + beta*C. The blog delves into benchmarking code on CUDA devices and explains the algorithm\u2019s design along with optimization techniques. These include inlined PTX, asynchronous memory copies, double-buffering, avoiding shared memory bank conflicts, and efficient coalesced storage through shared memory. I\u2019"
  },
  {
    "title": "Skymont: Intel's E-Cores reach for the Sky (chipsandcheese.com)",
    "points": 66,
    "submitter": "ksec",
    "submit_time": "2025-01-18T19:27:58 1737228478",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=42750734",
    "comments": [
      "Triple decoder is one unique effect. The fact that Intel managed to get them lined up for small loops to do 9x effective instruction issue is basically miraculous IMO. Very well done.Another unique effect is L2 shared between 4 cores. This means that thread communications across those 4 cores has much lower latencies.I've had lots of debates with people online about this design vs Hyperthreading. It seems like the overall discovery from Intel is that highly threaded tasks use less resources (cache, ROPs, etc. etc).Big cores (P cores or AMD Zen5) obviously can split into 2 hyperthread, but what if that division is still too big? E cores are 4 threads of support in roughly the same space as 1 Pcore.This is because L2 cache is shared/consolidated, and other resources (ROP buffers, register files, etc. etc.) are just all so much smaller on the Ecore.It's an interesting design. I'd still think that growing the cores to 4way SMT (like Xeon Phi) or 8way SMT (POWER10) would be a more conventional way to split up resources though. But obviously I don't work at Intel or can make these kinds of decisions.\n \nreply",
      "While the frontend of Intel Skymont, which includes instruction fetching and decoding, is very original and unlike to that of any other CPU core, the backend of Skymont, which includes the execution units, is extremely similar to that of Arm Cortex-X4 (which is a.k.a. Neoverse V3 in its server variant and as Neoverse V3AE in its automotive variant).This similarity consists in the fact that both Intel Skymont and Arm Cortex-X4 have the same number of execution units of each kind (and there are many kinds of execution units).Therefore it can be expected that for any application whose performance is limited by the CPU core backend, the CPU cores Intel Skymont and Arm Cortex-X4 (or Neoverse V3) should have very similar performances.Moreover, Intel Skymont and Arm Cortex-X4 have the same die area, i.e. around 1.7 square mm (including with both cores 1 MB of L2 cache in this area). Therefore the 2 cores not only should have about the same performance for backend-limited applications, but they also have the same cost.Before Skymont, all the older Intel Atom cores had been designed to compete with the medium-size Arm Cortex-A7xx cores, even if the Intel Atom cores have always lagged in performance Cortex-A7xx by a year or two. For instance Intel Tremont had a very similar performance to Arm Cortex-A76, while Intel Gracemont and Crestmont have an extremely similar core backend with the series of Cortex-A78 to Cortex-A725 (like Gracemont and Crestmont, the 5 cores in the series Cortex-A78, Cortex-A710, Cortex-A715, Cortex-A720 and Cortex-A725 have only insignificant differences in the execution units).With Skymont, Intel has made a jump in E-core size, positioning it as a match for Cortex-X, not for Cortex-A7xx, like its predecessors.\n \nreply",
      "Skymont is an improvement but...Skymont area efficiency should be compared to Zen 5C on 3nm. It has higher IPC, SMT with dual decoders - one for each thread, and full rate AVX-512 execution.AMD didn't have major difficulties in scaling down their SMT cores to achieve similar performance per area. But Intel went with different approach. At the cost of having different ISA support on each core in consumer devices and having to produce an SMT version of their P cores for servers anyway.\n \nreply",
      "It should be noted that Intel Skymont has the same area and it should also have the same performance for any backend-limited application with Arm Cortex-X4 (a.k.a. Neoverse V3) (both use 1.7 square mm in the \"3 nm\" TSMC fabrication process, while a Zen 5 compact might have an almost double area in the less dense \"4 nm\" process, with full vector pipelines, and a 3 square mm area with reduced vector pipelines, in the same less dense process).Arm Cortex-X4 has the best performance per area of among the cores designed by Arm. Cortex-X925 has a double area in comparison with Cortex-X4, which results in a much lower performance per area. Cortex-A725 is smaller in area, but the area ratio is likely to be smaller than the performance ratio (for most kinds of execution units Cortex-X4 has a double number, while for some it has only a 50% or a 33% advantage), so it is likely that the performance per area of Cortex-A725 is worse than for Cortex-X4 and for Skymont.For any programs that benefit from vector instructions, Zen 5 compact will have a much better performance per area than Intel Skymont and Arm Cortex-X4.For programs that execute mostly irregular integer and pointer operations, there are chances for Intel Skymont and Arm Cortex-X4 to achieve better performance per area, but this is uncertain.Intel Skymont and Arm Cortex-X4 have a greater number of integer/pointer execution units per area than Zen 5 compact, even if Zen 5 compact were made with a TSMC process equally dense, which is not the case today.Despite that, the execution units of Zen 5 compact will be busy a much higher percentage of the time, for several reasons. Zen 5 is better balanced, it has more resources for ensuring out-of-order and multithreaded execution, it has better cache memories. All these factors result in a higher IPC for Zen 5.It is not clear whether the better IPC of Zen 5 is enough to compensate its greater area, when performing only irregular integer and pointer operations. Most likely is that in such cases Intel Skymont and Arm Cortex-X4 remain with a small advantage in performance per area, i.e. in performance per dollar, because the advantage in IPC of Zen 5 (when using SMT) may be in the range of 10% to 50%, while the advantage in area of Intel Skymont and Arm Cortex-X4 might be somewhere between 50% and 70%, had they been made with the same TSMC process.On the other hand, for any program that can be accelerated with vector instructions, Zen 5 compact will crush in performance per area (i.e. in performance per dollar) any core designed by Intel or Arm.\n \nreply",
      "> It seems like the overall discovery from Intel is that highly threaded tasks use less resources (cache, ROPs, etc. etc).Does that mean if I can take a single-threaded program and split it into multiple threads, it might use less power? I have been telling myself that the only reason to use threads is to get more CPU power or to call blocking APIs. If they're actually more power-efficient, that would change how I weigh threads vs. async\n \nreply",
      "Not... quite. I think you've got the cause-and-effect backwards.Programmers who happen to write multiple-threaded programs don't need powerful cores, they want more cores. A Blender programmer calculating cloth physics would rather have 4x weaker cores than 1x P-core.Programmers who happen to write powerful singled-threaded programs need powerful cores. For example, AMD's \"X3D\" line of CPUs famously have 96MB of L3 cache, and video games that are on these very-powerful cores have much better performance.Its not \"Programmers should change their code to fit the machine\". From Intel's perspective, CPU designers should change their core designs to match the different kinds of programmers. Single-threaded (or low-thread) programmers... largely represented by the Video Game programmers... want P-cores. But not very much of them.Multithreaded programmers... represented by Graphics and a few others... want E-cores. Splitting a P-core into \"only\" 2 threads is not sufficient, they want 4x or even 8x more cores. Because there's multiple communities of programmers out there, dedicating design teams to creating entirely different cores is a worthwhile endeavor.--------> Does that mean if I can take a single-threaded program and split it into multiple threads, it might use less power? I have been telling myself that the only reason to use threads is to get more CPU power or to call blocking APIs. If they're actually more power-efficient, that would change how I weigh threads vs. asyncPower-efficiency is going to be incredibly difficult moving forward.It should be noted that E-cores are not very power-efficient though. They're area efficient, IE Cheaper for Intel to make. Intel can sell 4x as many E-cores for roughly the same price/area as 1x P-core.E-cores are cost-efficient cores. I think they happen to use slightly less power, but I'm not convinced that power-efficiency is their particular design goal.If your code benefits from cache (ie: big cores), its probable that the lowest power-cost would be to run on large caches (like P-cores or Zen5 or Zen5 X3D). Communicating with RAM is always more power than just communicating with caches after all.If your code does NOT benefit from cache (ie: Blender regularly has 100GB+ scenes for complex movies), then all of those spare resources on P-cores are useless, as nothing fits anyway and the core will be spending almost all of its time waiting on RAM to do anything. So the E-core will be more power efficient in this case.\n \nreply",
      "> A Blender programmer calculating cloth physics would rather have 4x weaker cores than 1x P-core.Don\u2019t they really want GPU threads for that? You wouldn\u2019t get by with just weaker cores.\n \nreply",
      "> A Blender programmer calculating cloth physics would rather have 4x weaker cores than 1x P-core.Is this true? In most of my work I'd usually rather have a single serializable thread of execution. Any parallelism usually comes with added overhead of synchronization, and added mental overhead of having to think about parallel execution. If I could freely pick between 4 IPC worth of single core or 1 IPC per core with 4 cores I'd pretty much always pick a single core. The catch is that we're usually not trading like for like. Meaning I can get 3 IPC worth of single core or 4 IPC spread over 4 cores. Now I suddenly have to weigh the overhead and analyze my options.Would you ever rather have multiple cores or an equivalent single core? Intuitively it feels like there's some mathematics here.\n \nreply",
      "Indeed a single thread is most simple to reason about, but if you have a single task that can already use 2 cores uniformly, going to 8 cores (assuming enough workload) should be a pretty clean 4x speedup (as long as you don't run into memory bandwidth limits, but that'd cap the single-threaded code too).But the performance difference between E-core and P-core perf is way less than 4x; the OP article shows a 1.6x/1.7x difference in SPEC for skymont vs lion cove, and 1.3x/1.7x for crestmont vs redwood code; and some searching around for past generations gives numbers around 1.4x.Increasing core counts being a much more area- and energy-efficient way for hardware to provide more total performance than making the individual cores faster is a pretty fundamental thing.\n \nreply",
      "For stuff like path tracing you have to work very hard not to trash the caches, so you're often just waiting for memory.That's why such workloads gets a near linear scaling when using hyper-threads, unlike workloads like LLMs which are memory bandwidth bound.\n \nreply"
    ],
    "link": "https://chipsandcheese.com/p/skymont-intels-e-cores-reach-for-the-sky",
    "first_paragraph": ""
  },
  {
    "title": "Why do bees die when they sting you? (subanima.org)",
    "points": 193,
    "submitter": "ohjeez",
    "submit_time": "2025-01-18T15:32:41 1737214361",
    "num_comments": 164,
    "comments_url": "https://news.ycombinator.com/item?id=42749069",
    "comments": [
      "Honeybee queens are the only honeybees with stingers that don't die when they sting.  That's because the queen bee's stinger has no barbs, and the reason for that is that the queen must not die easily, and she must use her stinger, so if she's going to survive at all her stinger has to not have barbs.  The queen almost certainly has to use her stinger when she exits her cocoon: to kill ther other queens that are about to hatch or have hatched already.  She also has to possibly use her stinger when she goes out to mate (though she does go with attendants who will defend her if attacked).I was surprised not to find mention of this in TFA.> A honey bee dies when it stings you because its stinger is covered in barbs, causing its abdomen to get ripped out when it tries to fly away. And surviving with your guts spilling out everywhere is pretty bloody hard.There's another interesting detail here: when the worker tries to fly off after stinging, she has to try really hard because the barbs hold the stinger in place, and trying hard causes two things to happen:  - noise that attracts other workers\n    to attack the same creature\n  \n  - spreading of the dying bee's\n    distress pheromones that also\n    attract other workers to sting\n    the same creature\n\nSo when you get stung by a bee near other bees you will be in trouble.  That's how you go from one sting to hundreds.  And hundreds is enough to kill a human.  That's why you don't go near a hive without protection.  Being in or near a swarm is safer than being near a hive: the bees in a swarm don't have much (larvae, honey) to protect, so they don't attack.\n \nreply",
      "BTW, when I get stung, if the stinger doesn't get stuck I sometimes don't even notice till much later -- this happens when I get stung through my gloves.  If the stinger does get stuck in my skin the first thing I notice is the buzzing of the bee that stung me.  The frequency of the buzzing of the bee that stung me and that is trying to rip off the stinger is absolutely terrifying because I know what comes next: a dozen more bees will flock right to where the one bee stung me and will all try to sting me, and since the one bee was able to it might be the case that her sisters will succeed as well.  That buzzing spreads her injury pheromones, and the frequency of its sound also acts as a very loud and clear signal to her sisters.  The buzzing lasts about 1 second or less; the pain from the sting comes a second or two later.  It takes her sisters about 1 to 2 seconds to find the bee that stings successfully.  Getting stung by a dozen bees at once is panic inducing, and the swelling that will create will take two weeks to subside.  I was stung 13 times at once one time, and 5 times another time.  It's no fun.\n \nreply",
      "This maybe points to another theory (which may be entirely wrong, I'm just guessing!): honeybees die because they aren't supposed to attack each other. Like they can't be aggressively selfish because they'll just die in the process.\n \nreply",
      "Honeybees do attack other colonies' honeybees.  Africanized honeybees definitely do it.  As someone else points out the barbs don't get stuck in insects, but do get stuck in mammals (and presumably birds too?).\n \nreply",
      "The bees in a swarm have filled themselves with food before swarming, so they\u2019re stuffed, so it\u2019s hard to flex the abdomen to make a sting\n \nreply",
      "Yes, there is this too.\n \nreply",
      "The queen bee is a formidable final boss with a bad-ass origin story.\n \nreply",
      "She's also her daughters' slave.  They make her work (lay eggs).  They decide when to make new queens.  They decide when to swarm with the old queen, and when they do they put her on a diet first so she can lose weight so she can fly (they won't let her eat much for two weeks), and they'll push her out of the hive when the time comes.Humans only really get stung by queen honeybees when manipulating them.  Normally the queen will be inside the hive and stay inside the hive except once or twice early in her life when she goes out to mate.\n \nreply",
      "Why does she go out to mate? Aren't the drones in the hive?\n \nreply",
      "I think two answers, though as per TFA there's probably a lot more behind this:First, she wants to get the best genetic material, and flying high is one of those tests for drones to pass, andSecond, needs to acquire genetic material from different bloodlines, hence does the business with several drones (obviously not an option within the hive).\n \nreply"
    ],
    "link": "https://www.subanima.org/bees/",
    "first_paragraph": "As Richard Feynman points out, every 'why' question in science needs to be treated with caution. This is because there are always several different levels at which a why problem can be answered, depending on what kind of response you're looking for. Taking Feynman's illustrative example, if pizza arrives at your door and your partner asks why pizza has come, you could fully answer the question in any of the following ways:And so on and so forth. The same goes for why questions in biology - we can go pretty deep if we try hard enough. So like any good 5 year old, in this article I'll do just that and keep asking why until we get somewhere interesting.First, we need to be clear about what is actually happening when a honey bee stings you. Rather than a clean hypodermic needle, a honey bee's stinger is actually covered in barbs.Looks like a very fancy fishhook right? Except it's more gruesome than a fishhook, because after a bee has stung you, it tries to fly away. The barbs keep its stin"
  },
  {
    "title": "Australian Open resorts to animated caricatures to bypass broadcast restrictions (crikey.com.au)",
    "points": 42,
    "submitter": "defrost",
    "submit_time": "2025-01-16T00:50:46 1736988646",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=42719498",
    "comments": [
      "Love it. I'm surprised they're allowed to use the audio, though. In the USA, the licensing notices specifically prohibit use of even \"the descriptions\" that appear in a broadcast.I suppose they'll close that loophole in other countries.The whole regime is a big F-U to fans, and to the taxpayers who subsidize these teams out the wazoo.\n \nreply",
      "Let's do this for baseball.. would probably make everything way more interesting\n \nreply",
      "At one point data for every MLB game was available from MLB.com. I started writing a RBI baseball simulator using said data and the graphics from the NES game. But then I realized I'm not that good at programming but I still think it would've been neat to watch game 3 of the 1970 world series as played out by 1985 video game graphics.\n \nreply",
      "this 100% sounds like a project I'd see at the top of the front page on a Saturday\n \nreply",
      "MLB does something similar called Gameday 3D. https://www.mlb.com/news/mlb-gameday-3d-guideThe main difference is that it's rendered client side so you can control the camera for yourself. You can watch in real time during the season, the latency is around 30 seconds behind live action.\n \nreply",
      "MLB StarCast generates ~7TB of data per game. I assume the bulk of that is video from the high speed cameras.\n \nreply",
      "Doesn't this harm the long-term value of AOs rights sales to the broadcasters?If I know AO is going to broadcast on youtube, why am I as a European broadcaster going to pay them the same amount as I did when they weren't trying to work against me?\n \nreply",
      "I don\u2019t think long-term value is their guiding star, if their previous NFT forays are anything to go by\n \nreply",
      "Yeah it very much feels like a case of biting the hand that feeds. What\u2019s the long term goal of AO by doing this?\n \nreply",
      "Survival amid viewer shifts away from traditional broadcasts.The people who they sell the rights to are less valuable partners, so AO feels comfortable making them a less valuable offer while pursuing other audiences.\n \nreply"
    ],
    "link": "https://www.crikey.com.au/2025/01/16/australian-open-animated-cartoon-caricatures-broadcast-restrictions/",
    "first_paragraph": "Support independent journalismContact us on: support@crikey.com.auThe Australian Open has implemented a cheeky workaround for international viewers that dodges its own broadcast obligations. Daanyal SaeedJan 16, 2025 3The first Grand Slam of the year is well and truly underway, with the Australian Open at Melbourne Park beginning earlier this week. As one of the biggest events on the Australian and international sporting calendar, it\u2019s available in Australia to watch on free-to-air television through Channel 9, as well as through its associated streaming services and in 4K on its subscription streaming service, Stan Sport.\u00a0However, that ease of access to tennis\u2019 first major of the year is not necessarily replicated worldwide. To watch the Australian Open in Europe, you need access to pay TV channel Eurosport, while the cable channel ESPN broadcasts it in North America.\u00a0Sports fans may have noticed another broadcast option: an animated caricature version. Broadcast on YouTube, the Austr"
  },
  {
    "title": "Second-Hand Bookshops in Britain: 2024 Report (wormwoodiana.blogspot.com)",
    "points": 15,
    "submitter": "fogus",
    "submit_time": "2025-01-15T15:50:01 1736956201",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "http://wormwoodiana.blogspot.com/2024/12/second-hand-bookshops-in-britain-2024.html",
    "first_paragraph": "This blog is devoted to fantasy, supernatural and decadent literature. It was begun by Douglas A. Anderson and Mark Valentine, and joined by friends, to present relevant news and information.About\nninety second-hand bookshops in the UK were reported as closed in 2024. This figure is based on\nreports to The Book Guide, usually by customers, occasionally by the\nbookdealers themselves or from their announcements. It compares to about 40 in\n2023 and 60 in 2022. I say 'reported as closed' in 2024 because some\nhad in fact closed earlier. This year\u2019s total includes quite a bit of 'catching up'. About thirty shops\nthat had been listed on the guide but with few signs of activity have been\nchecked up on during the year by doughty volunteer researchers, and in fact\nfound to have gone some time ago. Thus, the higher total than recent years is partly\nthe effect of better data. Probably, therefore, the truer total for the one\nyear, discounting this backlog, is around 60.\u00a0And they were not all full-s"
  },
  {
    "title": "The AMD Radeon Instinct MI300A's Giant Memory Subsystem (chipsandcheese.com)",
    "points": 167,
    "submitter": "pella",
    "submit_time": "2025-01-18T12:28:53 1737203333",
    "num_comments": 72,
    "comments_url": "https://news.ycombinator.com/item?id=42747864",
    "comments": [
      "I've often thought that one of the places AMD could distinguish itself from NVIDIA is bringing significantly higher amounts of VRAM (or memory systems that are as performant as what we currently know as VRAM) to the consumer space.A card with a fraction of the FLOPS of cutting-edge graphics cards (and ideally proportionally less power consumption), but with 64-128GB VRAM-equivalent, would be a gamechanger for letting people experiment with large multi-modal models, and seriously incentivize researchers to build the next generation of tensor abstraction libraries for both CUDA and ROCm/HIP. And for gaming, you could break new grounds on high-resolution textures. AMD would be back in the game.Of course, if it's not real VRAM, it needs to be at least somewhat close on the latency and bandwidth front, so let's pop on over and see what's happening in this article...> An Infinity Cache hit has a load-to-use latency of over 140 ns. Even DRAM on the AMD Ryzen 9 7950X3D shows less latency. Missing Infinity Cache of course drives latency up even higher, to a staggering 227 ns. HBM stands for High Bandwidth Memory, not low latency memory, and it shows.Welp. Guess my wish isn't coming true today.\n \nreply",
      "> Of course, if it's not real VRAM, it needs to be at least somewhat close on the latency and bandwidth frontIt is close to VRAM*, just not close to DRAM on a conventionally designed CPU. This thing is effectively just a GPU that fits in a CPU slot and has CPU cores bolted to the side. This approach has the downside of worse CPU performance and the upsides of orders of magnitude faster CPU<->GPU communication, simpler programming since coherency is handled for you, and access to substantial amounts of high bandwidth memory (up to 512GB with 4 MI300As).* https://chipsandcheese.com/p/microbenchmarking-nvidias-rtx-4...\n \nreply",
      "I was curious because given the latencies between the CCXs, the number of NUMA domains seems small.\n \nreply",
      "They are. Strix Halo is going after that same space of Apple M4 Pro/Max where it is currently unchallenged. Pairing it with two 64GB LPCAMM2 modules will get you there.Edit: The problem with AMD is less the hardware offerings, but more that their compute software stack historically tends to handwave or be very slow with consumer GPU support \u2014 even more so with their APUs. Maybe the advent of MI300A will change the equation, maybe not.\n \nreply",
      "I don't know of any non-soldered memory Strix Halo devices, but both HP and Asus have announced 128GB SKUs (availability unknown).For LLM inference, basically everything works w/ ROCm on RDNA3 now (well, Flash Attention is via Triton and doesn't have support for SWA and some other stuff; also I mostly test on Linux, although I did check that the new WSL2 support works). I've tested some older APUs w/ basic benchmarking as well. Notes here for those interested: https://llm-tracker.info/howto/AMD-GPUs\n \nreply",
      "Thanks for that link. I'm interested in either getting the HP Mini Z1 G1a or an NVidia Digits for LLM experimentation. The obvious advantage for the Digits is the CUDA ecosystem is much more tried & true for that kind of thing. But the disadvantage is trying to use it as a replacement for my current PC as well as the fact that it's going to run an already old version of Ubuntu (22.04) and you're dependent on Nvidia for updates.\n \nreply",
      "Yeah, I think anyone w/ old Jetsons knows what it's like to be left high and dry by Nvidia's embedded software support. Older models are basically just ewaste. Since the Digits won't be out until May, I guess there's enough time to wait and see - at least to get a sense of what the actual specs are. I have a feeling the FP16 TFLOPS and the MBW are going to be much lower than what people have been hyping themselves up for.Sadly, my feeling is that the big Strix Halo SKUs (which have no scheduled release dates) aren't going to be competitively priced (they're likely to be at a big FLOPS/real-world performance disadvantage, and there's still the PITA factor), but there is something appealing about about the do-it-all aspect of it.\n \nreply",
      "DIGITS looks like a serious attempt, but they don\u2019t have too much of an incentive to have people developing for older hardware. I wouldn\u2019t expect them to supor it for more than five years. At least the underlying Ubuntu will last more than that and provide a viable work environment far beyond the time it gets really boring.\n \nreply",
      "If only they could get their changes upstreamed to Ubuntu (and possible kernel mods upstreamed), then we wouldn't have to worry about it.\n \nreply",
      "Getting their kernel mods upstreamed is very unlikely, but they might provide just enough you can build a new kernel with the same major version number.\n \nreply"
    ],
    "link": "https://chipsandcheese.com/p/inside-the-amd-radeon-instinct-mi300as",
    "first_paragraph": ""
  },
  {
    "title": "Dusa Programming Language (Finite-Choice Logic Programming) (dusa.rocks)",
    "points": 116,
    "submitter": "febin",
    "submit_time": "2025-01-18T15:45:26 1737215126",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=42749147",
    "comments": [
      "As someone whose day job involves a lot of graph analysis and logic programming[0], I'm always excited to see new applied research in this area. More energy is needed here.Logic systems will be a key part of solving problems of hybrid data analysis (e.g. involving both social graphs, embedding spaces, and traditional relational data) - Cozo[1] sticks out as a great example.[0] https://codeql.github.com/docs/ql-language-reference/about-t...[1] https://www.cozodb.org/\n \nreply",
      "Oh, hello hacker news!Also potentially interesting to this crowd are the underlying editor, which I split out from the online Dusa editor and called \"sketchzone\" (https://github.com/robsimmons/sketchzone). Some of my motivations and hopes for sketchzone are blogged here: https://typesafety.net/rob/blog/endless-sketchzoneAlso, I more-or-less did Advent of Code 2024 in Dusa: journal entries and links to solutions are at https://typesafety.net/rob/blog/advent-of-dusa-2024\n \nreply",
      "Additional shout out to the Recurse Center (https://www.recurse.com/) which was instrumental in giving me the space and environment to start working on Dusa. I did a partially-remote, partially-in-person batch at Recurse in late 2023.\n \nreply",
      "Yay RC. I was in a remote batch. Was great.Zig was also apparently partly developed whilst Andrew Kelley was there. Fun place.\n \nreply",
      "My mind is blown, a new language where I can see new reach. Back in the day, APL was good at multidimensional arrays, and from there could outstrip Fortran shops at anything. A surprising swath of discrete reality can be viewed as a graph, or graphs of graphs. For me, computational group theory, combinatorial enumeration, canonical forms... All topics Claude 3.5 Sonnet happens to be exceptional at.Even a month ago, I'd have asked \"Where's the parallelism?\" looking at any new language. AI has upended my world. My subscriptions are getting out of hand, they're starting to look like some peoples' sports channel cable bills. I'll be experimenting with the right specification prompt to get AI to write correct programs in three languages side by side, in either Cursor or Windsurf. Then ask it to write a better prompt, and go test that in the other editor. I'm not sleeping much, it's like buying my first Mac.One constant debate I have with Claude is how much the choice of language affects AI reasoning ability. There's training corpus, but AI is even more appreciative of high level reasoning constructs than we are. AI doesn't need our idioms; when it taught itself the game Go it came up with its own.So human documentation is nice, but who programs that way anymore? Where's the specification prompt that suffices for Claude to code whatever we want in Dusa?\n \nreply",
      "Have to admit as a \u201cregular\u201d\ndeveloper using general purpose languages such as Java, C, Ruby, Perl, etc., most of this goes over my head, but at the same time I find the mix of Prolog and VB syntax fascinating and confusing.\n \nreply",
      "Is there an implicit algorithm for how this language is evaluated? It seems hard to use without having an understanding of the likely performance of your code.\n \nreply",
      "There is an implicit algorithm, and I'm so happy about this question. The inability to reason about likely performance of one's code is, to me, one of the things that bothers me most about Answer Set Programming, the programming paradigm that's probably the most like Finite-Choice Logic Programming.The Dusa implementation has a couple of ways to reason at a high level about the performance of programs. The academic paper that febin linked to elsethread spends a fair amount of time talking about this, and if you really want to dig in, I recommend searching the paper for the phrases \"deduce, then choose\" and \"cost semantics\".There's work to do in helping non-academics who just want to use Dusa reason about program performance, so I appreciate your comment as encouragement to prioritize that work when I have the chance.\n \nreply",
      "Is it different from SQL though?\n \nreply",
      "Research Paper\nhttps://arxiv.org/pdf/2405.19040\n \nreply"
    ],
    "link": "https://dusa.rocks/docs/",
    "first_paragraph": "Dusa is a logic programming language designed by\nRob Simmons and\nChris Martens,\nthe first implementation of finite-choice logic programming.The easiest way to use Dusa is in our web editor.\nDusa is also available as a command-line utility and JavaScript API via the\nNode package manager."
  },
  {
    "title": "Show HN: LLMpeg (github.com/jjcm)",
    "points": 71,
    "submitter": "jjcm",
    "submit_time": "2025-01-15T02:29:43 1736908183",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=42706637",
    "comments": [
      "FFMpeg is one of those tools that is really quite hard to use. The sheer surface area of the possible commands and options is incredible and then there is so much arcane knowledge around the right settings. Its defaults aren't very good and lead to poor quality output in a lot of cases and you can get some really weird errors when you combine certain settings. Its an amazingly capable tool but its equipped with every foot gun going.\n \nreply",
      "It's good that you have a \"read\" statement to force confirmation by the user of the command, but all it takes is one errant accidental enter to end up running arbitrary code returned from the LLM.I'd constrain the tool to only run \"ffmpeg\" and extract the options/parameters from the LLM instead.\n \nreply",
      "One option is to simply not run LLM-hallucinated commands. For example, you could read the documentation and write the command yourself, which would ensure it's more likely to do what you want. Alternatively, you could run arbitrary commands and accept the consequences.\n \nreply",
      "Why did you create this account just to post repeatedly complaining about this project?\n \nreply",
      "Adding an explanation of the patented and what they do is a great step as well to teach the tool to help build muscle memory\n \nreply",
      "i think this type of interaction is the future in lots of areas. i can imagine we replace API's completely with a single endpoint where you hit it up with a description of what you want back. like, hit up 'news.ycombinator.com/api' with \"give me all the highest rated submissions over the past week about LLMs\". a server side LLM translates that to SQL, executes the query, returns the results.this approach is broadly applicable to lots of domains just like FFMpeg. very very cool to see things moving in this direction.\n \nreply",
      "Except you don't need an LLM to do any of this, and it's already computationally cheaper. If you don't know the results you want, you should figure that out first, instead of asking a Markov chain to do it.\n \nreply",
      "I believe this approach is destined for a lot of disappointment. LLMs enable a LOT of entry- and mid-level performance, quickly. Rightfully, you and I worry about the edge cases and bugs. But people will trend towards things that enable them to do things faster.\n \nreply",
      "probably more helpful for learning than actual productivity with ffmpeg but really like this project (zap emoji)\n \nreply",
      "Parsing simple English and converting it to ffmpeg commands can be done without an LLM, running locally, using megabytes of RAM.Check out this AI:  $ apt install cdecl\n  [ ... ]\n  After this operation, 62.5 kB of additional disk space will be used.\n  [ ... ]\n  $ cdecl\n  Type `help' or `?' for help\n  cdecl> declare foo as function (pointer to char) returning pointer to array 4 of pointer to function (double) returning double\n  double (*(*foo(char *))[4])(double )\n\nGranted, this one has a very rigid syntax that doesn't allow for variation, but it could be made more flexible.If FFMpeg's command line bugged me badly enough, I'd write \"ffdecl\".\n \nreply"
    ],
    "link": "https://github.com/jjcm/llmpeg",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Uses an llm to generate ffmpeg commands\n      Allows for simple usage of ffmpeg via an llm.llmpeg remove audio from exampleVid.mov\n        Uses an llm to generate ffmpeg commands\n      "
  },
  {
    "title": "Generating an infinite world with the Wave Function Collapse algorithm (marian42.de)",
    "points": 166,
    "submitter": "klaussilveira",
    "submit_time": "2025-01-14T17:25:06 1736875506",
    "num_comments": 48,
    "comments_url": "https://news.ycombinator.com/item?id=42700483",
    "comments": [
      "I think proc gen like this often falls victim to the 1000 bowls of oatmeal problem.Unless things _feel_ different, it just feels same-y.Some games handle this well (often through biomes with different rules and/or special areas with different rules) and others (interestingly, often those that talk about how many billions of possibilities there are) are techniquely different... But just all feel the same.It's interesting to see how game devs continue to be creative in this area and how many games continue to have this problem.\n \nreply",
      "Any game that wants a ton of content has this problem. The many caves in Skyrim start to feel the same after a while. You can often recognize the same assets over and over in very content-rich games. The total number of assets used in the article is tiny compared to what a production game would have. Procedural generation can't be used as an excuse to do less work, or it just feels same-y. A game is still a ton of work; it's just different work for different goals with procedural generation.Generating a huge world is boring if you don't have interesting objects to put in that world. Interesting objects are meaningless unless they're connected to a story. Minecraft works because it's a sandbox for you to invent your own stories, so the infinite world ends up getting filled by the player. If any element feels repetitive, the whole game will. If you wanted a fully procedural game, you need to figure out how to procedurally generate all the different aspects: story, characters, enemies, places, objects. It's more work to do that well than to make a traditional game. So people going into procedural generation thinking it'll save them work are already destined to fail.\n \nreply",
      "Agree overall, and yet, and yet..> Procedural generation can't be used as an excuse to do less work> So people going into procedural generation thinking it'll save them work are already destined to failConsider the possibility that nobody really picks up proc-gen in the hopes they can laze out a RDR3 or such over the weekend.Another thing is, this applies to indies and AAAs alike: while a big world has to have interesting unique things in it, by definition not every square meter can/should be chockful of another \"interesting truly unique thing\" because if the whole world is filled like that, it's just another kind of sameyness in that the novelty factor would wear out just as quickly once you're getting that there truly is true novelty in absolutely every little square meter, which kills the novelty sensation in a heartbeat. Novelty delights us in a backdrop of routineness, sameyness, same-old-same-old-ness. So in between interesting things, the thusly necessary slightly-\"duller\" in-between areas are to pace and prep and make one anticipate novelty \"hopefully almost just around the corner\". Ideally it's so spaced to appear just in time before the player resigns such hopes.And so if you're going to have slightly-duller \"filler areas\" (and let me posit that any real-world say forest (in a biome one has traversed before), without the physical air and smells and winds (or friends/pets stringing along) is quickly \"proc-gen dull-ish\" within minutes \u2014 even in reality \u2014 or call it \"meditative\"/calming) \u2014 so again, if you're going to have slightly-duller \"filler areas\" just to connect and space apart the unique content things to good effect, then procedural placements/scatterings/variations are going to beat manual placement not just in \"effort time\" but because manual would swiftly look much more repetetive (being inevitably eventually effectively copy-paste driven) given the scale of environments under discussion, even if it would not take a human months of menial clickery.Rockstar gets it right imho but their approach would also get  your \"many caves in Skyrim\" right because they manifest little novel uniquenesses not in scattered objects or env textures/models but via lively interactive interludes, whether it's animals frolicking or chasing each other or attacking towards you or \"random stranger\" incidents etc. That's the right kind of \"filler but not boring\", nobody cares about the variety of the rocks, only noticing them if nothing seems to be happening.\n \nreply",
      "The interesting part comes with building a simulation on top of this. For example with simulated citizens that go about their day fulfilling needs, going to work etc.\n \nreply",
      "People seem to go ga-ga over this algorithm with some frequency on HN, and I think it is only because of its in-name-only connection to quantum mechanics, because I have to say that as procedural generation algorithm I find it very lacking. It works on the cases I saw for something like a 10x10 grid decently enough but for any large-scale work it just produces endless seas of structureless repetition. Even the long-in-the-tooth \"just fire Perlin noise at varying levels of detail at the problem\" at least produces some overarching structure even if it is itself just random because the low-frequency, high-amplitude components you use will produce some sort of higher-level variation.Literally anything with any hierarchy in it would be better, unless you're really leaning into that SCP-esque, liminal-space creepiness. But if that isn't your explicit goal I would definitely not recommend this for large-scale usage because in most projects that creepy liminal-space-ness is probably actively fighting your artistic goals.It wouldn't even take that much; lay down some sort of street network with some other algorithm, a bit of \"zoning\" to switch some tile sets between \"residential\" and some other zone or even just a couple different types of \"residental\", perlin noise it up if you've got no better ideas, then use \"wave function collapse\" (I really dislike the in-name-only \"quantumness\" of the name, can hardly call the algorithm that with a straight face) to fill in the blocks. A straightforward implementation of that would still be pretty same-y but it would at least have some sort of structure.\n \nreply",
      "I was working some time ago on a 4X with an hexagonal map (not only made of hex tiles, but the overall shape was an hexagon rather than a rectangle, no wrap-around).Using a noise function alone does not let you choose how many landmass you want to generate.Using \"wave function collapse\"/constraint solvers is too slow for large maps when the amount of constraints increase (please don't put a mountain in the middle of the ocean, or a snow tile in the middle of a desert). My implementation took almost 8h to generate a single map, and the result was not even good.In the end, I used a combination of multiple techniques:  - voronoi to split the map into regions\n  - use a noise function to make the regions boundaries a bit more natural\n  - fill the map with water tiles, place randomly some island seeds\n  - grow the islands from their seeds using a cellular automata\n  - to create continents, simply put a lot of island seeds in the same area, to generate a bigger one\n  - place mountains or rifts on region boundaries to simulate \"tectonic plates\"\n  - generate a heat map (influenced by position of the north/south poles and equator of the map), a humidity map (influenced by leftover ocean tiles), a height map (which is influenced by the already placed mountains/rifts) using a noise function\n  - using the previous heat/humidity/height maps, generate a wind map\n  - using the wind map, modify the humidity map (wind carries humidity over the land)\n\nThen, choose randomly some tiles that fit some criteria to place biomes:  - desert on hot/dry land\n  - forest on temperate land\n  - swamp on temperate/wet land\n  - jungle on hot/wet land\n  - ...\n\nThen a bunch of different cellular automata to grow the biomes naturally.The result was quite nice, but it still wasn't on par to the map gen in Civilization games. I still want to continue this project, but I think in my heart I gave up.EDIT: Some formatting because i always forget HN does not support markdown lists\n \nreply",
      "> but it still wasn't on par to the map gen in Civilization games.Which makes me wonder... do we know how map gen in Civilization works so well?\n \nreply",
      "\"You know, Hexagons are the Bestagons!\"https://www.youtube.com/watch?v=thOifuHs6eY\n \nreply",
      "This is pretty cool! As someone with virtually no experience in this area I\u2019d love to read the source code, is it open source?\n \nreply",
      "A fantastic resource for this kind of generation is here:http://www-cs-students.stanford.edu/~amitp/game-programming/...\n \nreply"
    ],
    "link": "https://marian42.de/article/infinite-wfc/",
    "first_paragraph": "\n\n\n\nThis article describes how I generate an infinite city using the Wave Function Collapse algorithm in a way that is fast, deterministic, parallelizable and reliable.\nIt's a follow-up to my 2019 article on adapting the WFC algorithm to generate an infinite world.\nThe new approach presented in this article removes the limitations of my original implementation.\nI first mentioned these ideas in this Twitter thread.The goal is to procedurally generate a 3D environment by placing human designed blocks on a 3D grid.\nThe blocks need to be placed in accordance with given adjacency contraints.\nFor each of the 6 sides of each block, some information about the face and its symmetry is used to generate a list of possible neighbors.\n\n\n\nThis is different from the original formulation of the WFC algorithm, where the possible blocks, their adjacency rules and their spawn probabilities are extracted automatically from an example texture.In this improved version, the generation method is robust enough"
  },
  {
    "title": "WASM GC isn't ready for realtime graphics (dthompson.us)",
    "points": 46,
    "submitter": "todsacerdoti",
    "submit_time": "2025-01-18T19:36:13 1737228973",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=42750781",
    "comments": [
      "Wasn't WASM GC a prerequisite for getting direct DOM access from WASM? Does progress for WASM GC mean progress for DOM access as well?Every time I check back on that the initiative seems to run under a different name. What is the best way to track progress on that front?\n \nreply",
      "Really liked NaCl (and PNaCl) idea, which allows running arbitrary code, sanitized, with ~90% speed of native execution. Playing Bastion game in browser was refreshing. Unfortunately communication with js code and bootstrap issues (can't run code without plugin, no one except chrome supported this) ruined that tech\n \nreply",
      "WASM nowadays has become quite the monstrosity compared to NaCl/PNaCl. Just look at this WASM GC spaghetti, trying to compile a GC'd language but hooking it up V8/JavaScriptCore's GC, while upholding a strict security model... That sounds like it won't cause any problems whatsoever!Sometimes I wonder if the industry would have been better off with NaCl as a standard. Old, mature tooling would by and large still be applicable (it's still your ordinary x86/ARM machine code) instead of the nascent and buggy ecosystem we have now. I don't know why, but the JS folks just keep reinventing everything all the time.\n \nreply",
      "Shouldn't it be possible to implement your own GC in WASM? Why does WASM try to be everything?\n \nreply",
      "You can't GC together with the host environment if you do a custom GC (i.e. a wasm object and a JS object in a cycle wouldn't have any way to ever be GC'd).\n \nreply",
      "Slower, single threaded, greatly increases binary size, separate heap from JS so bad interop with extern refs. Wasm GC is a great thing.\n \nreply",
      "I was excited to read this post because I haven't yet tried WasmGC for anything beyond tiny toy examples, but was disappointed to find no actual numbers for performance. I don't know the author well enough to be able to assess their assertions that various things are \"slow\" without data.\n \nreply",
      "so what about realtime graphics with wasm without GC? (compiled from languages not needing a GC like Rust, C/C++, Odin, ...)\n \nreply",
      "As mentioned, that works quite well already but it's not the topic of this post.\n \nreply",
      "It's sort of baffled me that people appear to be shipping real code using WasmGC since the limitations described in this post are so severe. Maybe it's fine because they're just manipulating DOM nodes? Every time I've looked at WasmGC I've gone \"there's no way I could use this yet\" and decided to check back a year later and see if it's There Yet.Hopefully it gets there. The uint8array example from this post was actually a surprise to me, I'd just assumed it would be efficient to access a typed array via WasmGC!Beyond the limitations in this post there are other things needed to be able to target WasmGC with existing stuff written in other languages, like interior references or dependent handles. But that's okay, I think, it can be worthwhile for it to exist as-is even if it can't support i.e. existing large-scale apps in memory safe languages. It's a little frustrating though.\n \nreply"
    ],
    "link": "https://dthompson.us/posts/wasm-gc-isnt-ready-for-realtime-graphics.html",
    "first_paragraph": "Wasm GC is a wonderful thing that is now available in all major web\nbrowsers since slowpoke Safari/WebKit finally shipped it in December.\nIt provides a hierarchy of heap allocated reference types and a set of\ninstructions to operate on them.  Wasm GC enables managed memory\nlanguages to take advantage of the advanced garbage collectors inside\nweb browser engines.  It\u2019s now possible to implement a managed memory\nlanguage without having to ship a GC inside the binary.  The benefits\nare smaller binaries, better performance, and better integration with\nthe host runtime.However, Wasm GC has some serious drawbacks when compared to linear\nmemory. I enjoy playing around with realtime graphics programming in\nmy free time, but I was disappointed to discover that Wasm GC just\nisn\u2019t a good fit for that right now.  I decided to write this post\nbecause I\u2019d like to see Wasm GC on more or less equal footing with\nlinear memory when it comes to binary data manipulation.For starters, let's take a look at "
  },
  {
    "title": "Reverse-engineering a carry-lookahead adder in the Pentium (righto.com)",
    "points": 47,
    "submitter": "DamonHD",
    "submit_time": "2025-01-18T18:53:47 1737226427",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42750454",
    "comments": [
      "Author here if anyone has questions about the obscure details of Kogge-Stone adders :-)\n \nreply",
      "Hmm, I thought you explained the FDIV bug was (in part) because the Boron\u2020 used carry-save adders to generate quotient digits?  I don't see how you'd combine carry-save adders and Kogge\u2013Stone carry lookahead.  This post does mention the carry-save adder and link the other post, but doesn't explain the relationship between the two adders (are they the same in some galaxy-brain way I can't imagine?  does one of them feed the other?)______\u2020 https://en.wikipedia.org/wiki/Systematic_element_name\n \nreply",
      "The short answer is that a large carry-save adder feeds into this 8-bit carry-lookahead adder to generate the table index for division. In more detail, the division algorithm uses a \u224864-bit carry-save adder to hold the partial remainder during a division. The problem is that a carry-save adder holds the result in two pieces, the sum bits and the carry bits, which is what makes it fast. However, the division algorithm needs to use the top 7 bits as an index into the infamous division table, but this won't work if the value is in two pieces. The solution is to add the two pieces together using the carry-lookahead adder and then you have the table index.The obvious question is why didn't they just use a carry-lookahead adder in the first place? The answer is that a carry-lookahead adder works better for smaller words (e.g. 8 bits), since its size is O(N^2) or O(N log N), depending on how you implement it. So you're better off with a large carry-save adder and a small carry-lookahead adder.\n \nreply",
      "I see.  I guess I had the impression that the carry and sum bits had been used directly to index the table, which is of course a thing you can do; it's just that 7 bits in the carry-save adder is the equivalent of \u22483 bits.\n \nreply",
      "> In the TMS 1000, the program counter steps through the program pseudo-randomly rather than sequentially. The program is shuffled appropriately in the ROM to counteract the sequence, so the program executes as expected and a few transistors are saved.Two wrongs make a right.\n \nreply"
    ],
    "link": "https://www.righto.com/2025/01/pentium-carry-lookahead-reverse-engineered.html",
    "first_paragraph": ""
  },
  {
    "title": "Porting the GNAT Ada compiler to macOS/aarch64 (briancallahan.net)",
    "points": 3,
    "submitter": "goranmoomin",
    "submit_time": "2025-01-15T18:00:53 1736964053",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://briancallahan.net/blog/20250112.html",
    "first_paragraph": "academic, developer, with an eye towards a brighter techno-social lifeAfter getting a port of GDC working on my new MacBook Pro, there are still two languages left in the GCC suite that I don't have: Ada and Go. Some searching around makes it seem pretty clear that Gccgo is not yet really on the table to macOS. But there should not be any reason we can't add Ada to our GCC suite, seeing as there is already support for macos/aarch64 in the repository. I wanted to get a macOS/aarch64 native Ada through the GNAT compiler in GCC.But try as I might, I could not find any precompiled packages for it. I guess part of the issue is that macOS/aarch64 support is not fully upstreamed into GCC propre; instead, Iain Sandoe has a GitHub repository that includes the necessary changes for full support. I used his gcc-14-branch repository to build GDC.So let's get to work.I don't have Rosetta on my machine. I know it takes all of two seconds to install it, but I also wanted an excuse to play around with"
  },
  {
    "title": "Saint Peter Basilica digital experience (basilicasanpietro.va)",
    "points": 84,
    "submitter": "matco11",
    "submit_time": "2025-01-16T22:31:06 1737066666",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=42731740",
    "comments": [
      "For any heathens like me, the history makes a lot more sense when you know that Simon and Peter are the same person. It chops and changes with no explanation.\n \nreply",
      "> It chops and changes with no explanation.The site provides the relevant information. In the \"Story of the Basilica\" the first section is \"The Life of St. Peter\" and it tells you why Simon came to be called Peter.https://virtual.basilicasanpietro.va/en/st-peter-history\n \nreply",
      "Jesus gave Simon a new name saying you will be the rock of my new church. Peter is anglicized Petros or rock in Greek.The story is that Jesus spoke Aramaic. He would address Simon as Kepha. But the early gentile Christians wrote in Greek the language of the educated.\n \nreply",
      "The first \u201cThe Rock\u201d\n \nreply",
      "The AI narration is annoying, with awkward pauses and occasional mispronunciations. For example, it described the motto \"urbi et orbi\" as \"urb-eye e.t. orbee\".Movement is very jerky (maybe because of my old computer?) and I was disappointed that I could only roam freely around the wireframe model, not any of the photographic images. Because of the jerkiness, it was sometimes hard to tell what part of the architecture I was looking at.\n \nreply",
      "Instant mute for me...As someone that is not very religious, it is still a site to behold in person.\n \nreply",
      "Interesting to read that a number of people struggle to view this using their laptops while it works decently for me on an iPhone. It makes sense and that comparison is meaningless, especially when not given any specific specs, but still, I find this fact fascinating.\n \nreply",
      "The frame rate is not great on a modern iPhone which is a bit of a bummer. Maybe missing some optimizations or something.\n \nreply",
      "I visited St. Peter's Basilica a few years ago. I highly recommend it.I had to exit the experience because my laptop (admittedly a few years old) could only give me 1-2 frames per second. I love the idea, but in the current state, viewing hi-res pictures would give you a better experience than this virtual 3d tour, IMO.\n \nreply",
      "Unplayable on my computer. I think the textures take too long to load. Most of the times I'm looking at jerky wireframes.\n \nreply"
    ],
    "link": "https://virtual.basilicasanpietro.va/en",
    "first_paragraph": "Select an experience"
  },
  {
    "title": "Amazon's AI crawler is making my Git server unstable (xeiaso.net)",
    "points": 447,
    "submitter": "OptionOfT",
    "submit_time": "2025-01-18T18:48:51 1737226131",
    "num_comments": 173,
    "comments_url": "https://news.ycombinator.com/item?id=42750420",
    "comments": [
      "It's time for a lawyer letter. See the Computer Fraud and Abuse Act prosecution guidelines.[1] In general, the US Justice Department will not consider any access to open servers that's not clearly an attack to be \"unauthorized access\". But,\"However, when authorizers later expressly revoke authorization\u2014for example, through unambiguous written cease and desist communications that defendants receive and understand\u2014the Department will consider defendants from that point onward not to be authorized.\"So, you get a lawyer to write an \"unambiguous cease and desist\" letter. You have it delivered to Amazon by either registered mail or a process server, as recommended by the lawyer. Probably both, plus email.Then you wait and see if Amazon stops.If they don't stop, you can file a criminal complaint. That will get Amazon's attention.[1] https://www.justice.gov/jm/jm-9-48000-computer-fraud\n \nreply",
      "> Then you wait and see if Amazon stops.That\u2019s if the requests are actually coming from Amazon, which seems very unlikely given some of the details in the post (rotating user agents, residential IPs, seemingly not interpreting robots.txt). The Amazon bot should come from known Amazon IP ranges and respect robots.txt. An Amazon engineer confirmed it in another comment: https://news.ycombinator.com/item?id=42751729The blog post mentions things like changing user agent strings, ignoring robots.txt, and residential IP blocks. If the only thing that matches Amazon is the \u201cAmazonBot\u201d\nUser Agent string but not the IP ranges or behavior then lighting your money on fire would be just as effective as hiring a lawyer to write a letter to Amazon.\n \nreply",
      "Honestly, I figure that being on the front page of Hacker News like this is more than shame enough to get a human from the common sense department to read and respond to the email I sent politely asking them to stop scraping my git server. If I don't get a response by next Tuesday, I'm getting a lawyer to write a formal cease and desist letter.\n \nreply",
      "Someone from Amazon already responded: https://news.ycombinator.com/item?id=42751729> If I don't get a response by next Tuesday, I'm getting a lawyer to write a formal cease and desist letter.Given the details, I wouldn\u2019t waste your money on lawyers unless you have some information other than the user agent string.\n \nreply",
      "It's computer science, nothing changes on corpo side until they get a lawyer letter.And even then, it's probably not going to be easy\n \nreply",
      "No one gives a fuck in this industry until someone turns up with bigger lawyers. This is behaviour which is written off with no ethical concerns as ok until that bigger fish comes along.Really puts me off it.\n \nreply",
      "Lol you really think an ephemeral HN ranking will make change?\n \nreply",
      "It's not unheard of. But neither would I count on it.\n \nreply",
      "It did yesterday!https://news.ycombinator.com/item?id=42740516\n \nreply",
      "There's only one way to find out!\n \nreply"
    ],
    "link": "https://xeiaso.net/notes/2025/amazon-crawler/",
    "first_paragraph": "\n        Published on 01/17/2025, 319 words, 2 minutes to read\n    Please, just stop.EDIT(2025-01-18 23:50 UTC):I wrote a little proxy that does a proof-of-work check before allowing requests to my Gitea server. It's called Anubis and I'll be writing a blog post about it soon.For now, check it out at https://git.xeserv.us/. It's a little rough around the edges, but it works enough.EDIT(2025-01-18 19:00 UTC):I give up. I moved the Gitea server back behind my VPN. I'm working on a proof of work reverse proxy to protect my server from bots in the future. I'll have it back up soon.EDIT(2025-01-17 17:50 UTC):I added this snippet to the ingress config:The bots are still hammering away from a different IP every time. About 10% of the requests do not have the amazonbot user agent. I'm at a loss for what to do next.I hate the future.Hi all. This is a different kind of post. This is not informative. This is a cry for help.To whoever runs AmazonBot, please add git.xeserv.us to your list of blocke"
  }
]