[
  {
    "title": "Baby is healed with first personalized gene-editing treatment (nytimes.com)",
    "points": 512,
    "submitter": "jbredeche",
    "submit_time": "2025-05-15T18:06:06 1747332366",
    "num_comments": 234,
    "comments_url": "https://news.ycombinator.com/item?id=43997636",
    "comments": [
      "> To accomplish that feat, the treatment is wrapped in fatty lipid molecules to protect it from degradation in the blood on its way to the liver, where the edit will be made. Inside the lipids are instructions that command the cells to produce an enzyme that edits the gene. They also carry a molecular GPS \u2014 CRISPR \u2014 which was altered to crawl along a person\u2019s DNA until it finds the exact DNA letter that needs to be changed.That is one of the most incredible things I have ever read.\n \nreply",
      "One other fun part of gene editing in vivo is that we don't actually use GACU (T in DNA). It turns out that if you use Pseudouridine (\u03a8) instead of uridine (U) then the body's immune system doesn't nearly alarm as much, as it doesn't really see that mRNA as quite so dangerous. But, the RNA -> Protein equipment will just make protiens it without any problems.Which, yeah, that's a miraculous discovery. And it was well worth the 2023 Nobel in Medicine.Like, the whole system for gene editing in vivo that we've developed is just crazy little discovery after crazy little discovery. It's all sooooo freakin' cool.https://en.wikipedia.org/wiki/Pseudouridine\n \nreply",
      "I remember from a few few years back that the lipid coating may have caused problems for the liver, when treating people for diseases that needed to target a lot of tissue, such as muscle disorders. Is that still the case?\n \nreply",
      "I suppose a downside (depending on your perspective) of this is that it will make people who are genetically modified in this fashion trivial to detect.That's good if your goals are to detect genetic modification which may be considered cheating in competitive sports.That's bad if your goals are to detect genetically modified people and discriminate against them.I see a near future where the kind of people who loathe things like vaccines and genuinely believe that vaccines can spread illness to the non-vaccinated feel the same way about other things like genetic modification and use legal mechanisms to discriminate and persecute people who are genetically modified.\n \nreply",
      "> it will make people who are genetically modified in this fashion trivial to detect.I'm not totally sure. If I understand it correctly, the mRNA contains pseudouridine, and it makes the protein that will edit the DNA. The edited DNA should look like a normal one.\n \nreply",
      "Ah.  That makes sense.  My mistake.\n \nreply",
      "I'm less interested in detecting genetic modification for the purposes of discrimination than making sure it's available to everyone.Assuming requisite safety of course.\n \nreply",
      "Don't be silly, the rich will want their babies to be perfect so gene editing will be legal and considered OK.\n \nreply",
      "If you're going to make the comparison with vaccines, and if history is any indication, the more realistic worry would be the other way around (since that's where the money is): that genetic modifications will be mandated, and that those who object will be discriminated against.[And no, I am not anti-vax, nor anti-gene-editing.]\n \nreply",
      "\u201cWhat do you mean you haven\u2019t modified your chromosome 7 CFTR gene?  And you\u2019re planning to have children???\u201d\n \nreply"
    ],
    "link": "https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html",
    "first_paragraph": ""
  },
  {
    "title": "A leap year check in three instructions (hueffner.de)",
    "points": 153,
    "submitter": "gnabgib",
    "submit_time": "2025-05-15T21:57:19 1747346239",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=43999748",
    "comments": [
      "> return ((y * 1073750999) & 3221352463) <= 126976;> How does this work? The answer is surprisingly complex.I don't think anyone is surprised in the complexity of any explanation for that algorithm :D\n \nreply",
      "This reminds me of once when I was giving an algo/ds interview (in Java) and the interviewer started asking me questions where one of the answers usually was \u201cto be memorised\u201d shit like this and he started pestering me to give him those answers (even though I said I don\u2019t know and definitely don\u2019t recall) and that too in C. As per him \u201ceveryone who coded knew C.. at least in college\u201d and started becoming a bit more hostile. I think it was the first interview that I had ended as an interviewee.And I call this thing \u201cbit gymnastics\u201d.\n \nreply",
      "I love these incomprehensible magic number optimizations. Every time I see one I wonder how many optimizations like this we missed back in the old days when we were writing all our inner loops in assembly?Does anyone have a collection of these things?\n \nreply",
      "Here is a short list:https://graphics.stanford.edu/~seander/bithacks.htmlIt is not on the list, but #define CMP(X, Y) (((X) > (Y)) - ((X) < (Y))) is an efficient way to do generic comparisons for things that want UNIX-style comparators. If you compare the output against 0 to check for some form of greater than, less than or equality, the compiler should automatically simplify it. For example, CMP(X, Y) > 0 is simplified to (X > Y) by a compiler.The signum(x) function that is equivalent to CMP(X, 0) can be done in 3 or 4 instructions depending on your architecture without any comparison operations:https://www.cs.cornell.edu/courses/cs6120/2022sp/blog/supero...It is such a famous example, that compilers probably optimize CMP(X, 0) to that, but I have not checked. Coincidentally, the expansion of CMP(X, 0) is on the bit hacks list.There are a few more superoptimized mathematical operations listed here:https://www2.cs.arizona.edu/~collberg/Teaching/553/2011/Reso...Note that the assembly code appears to be for the Motorola 68000 processor and it makes use of flags that are set in edge cases to work.Finally, there is a list of helpful macros for bit operations that originated in OpenSolaris (as far as I know) here:https://github.com/freebsd/freebsd-src/blob/master/sys/cddl/...There used to be an Open Solaris blog post on them, but Oracle has taken it down.Enjoy!\n \nreply",
      "there is \"Hacker's Delight\" by Henry S. Warren, Jr.https://en.wikipedia.org/wiki/Hacker's_Delight\n \nreply",
      "Looks awesome, thank you :)\n \nreply",
      "We didn't miss them.  In those days they weren't optimizations.  Multiplications were really expensive.\n \nreply",
      "Related, Computerphile had a video a few months ago where they try to put compute time relative to human time, similar to the way one might visualize an atom by making the proton the size of a golfball. I think it can help put some costs into perspective and really show why branching maters as well as the great engineering done to hide some of the slowdowns. But definitely some things are being marked simply by the sheer speed of the clock (like how the small size of a proton hides how empty an atom is)  https://youtube.com/watch?v=PpaQrzoDW2I\n \nreply",
      "and divides were worse. (1 cycle add, 10 cycle mult, 60 cycle div)\n \nreply",
      "Division still is worse:https://github.com/ridiculousfish/libdivide\n \nreply"
    ],
    "link": "https://hueffner.de/falk/blog/a-leap-year-check-in-three-instructions.html",
    "first_paragraph": "Falk H\u00fcffner \u00b7 15 May 2025\n      With the following code, we can check whether a year 0 \u2264 y \u2264 102499 is a leap year with only about 3 CPU instructions:\n    How does this work? The answer is surprisingly complex. This article explains it, mostly to have some fun with bit-twiddling; at the end, I'll briefly discuss the practical use.This is how a leap year check is typically implemented:We are using the proleptic Gregorian calendar, which extends the Gregorian calendar backward from its introduction in 1582 and includes a year 0. Thus, we don't need to treat years before 1582 any differently. For simplicity, we ignore negative years and use an unsigned year.Let's first do some simple speedups so that we get a good baseline. I'm not sure where to assign credit \u2013 these tricks have probably been reinvented independently many times.We can replace (y % 100) != 0 by (y % 25) != 0: we already know that y is a multiple of 22, so if it is also a multiple of 52, it is a multiple of 22 \u22c5 52 = 100. "
  },
  {
    "title": "Teal \u2013 A statically-typed dialect of Lua (teal-language.org)",
    "points": 23,
    "submitter": "generichuman",
    "submit_time": "2025-05-16T00:40:35 1747356035",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=44000759",
    "comments": [
      "I'm so relieved to see more types being added to good languages.So Teal is to Lua as TypeScript is to JavaScript. Which means it automatically plays well with any Lua environment. Unlike luau and nelua which are also statically typed but have their own runtimes.What version of Lua does it use? Lua gets new versions every few years so I don't know why so many impls don't continuously upgrade to the latest version.\n \nreply",
      "Lua is a good language. It's like C, if C were a scripting language.It's got an awesome C API. It's fast, lightweight, and embeddable. It's more performant than Python. It's a staple in video game scripting.\n \nreply",
      "I don't think it's a good language, and I hate it. I've made thousands of the same mistakes with it\u2014typing . instead of :. There's a reason Lua has a smaller audience than assembly.\n \nreply",
      "It's nothing like C, and that's so much of its charm.Semantically, Lua is almost identical to the core of JavaScript. Metatables are a genius alternative to prototype chains.Lua's syntax is beautifully simple and unambiguous, but at the cost of being moderately inconvenient in 2025 unfortunately. It could benefit from an ESNext-style renewal.I get why they made the C API that way, but in practice it's very easy to get wrong.I'm not sure how fast vanilla Lua is today compared to similar languages. I think LuaJIT (and Luau?) are most often used when performance is needed.\n \nreply",
      "> The core compiler has no dependencies and is implemented as a single tl.lua file which you can load into your projects. Running tl.loader() will add Teal support to your package loader, meaning that require() will be able to run .tl files.Genius design.\n \nreply",
      "Oh, clever name. Typed Lua \u2192 TL \u2192 \"Tee Ell\" \u2192 TealAnd the extension is .tl\n \nreply",
      "Off-topic comment, but as an ESL speaker I just this week randomly learned that teal the color is named after the duck species Anas crecca, called \"teal\" in English.\n \nreply",
      "Tuples: {number, string}Arrays: {number}How does it disambiguate it? Are single-element tuples just never used in practice? To be fair, maybe the only time I've had to use them in TypeScript is via Parameters<T>\n \nreply",
      "There is another TEAL (uppercase) programming language: <https://developer.algorand.org/docs/get-details/dapps/avm/te...>\n \nreply",
      "I vote we give the name to the lua one!\n \nreply"
    ],
    "link": "https://teal-language.org/",
    "first_paragraph": "\nTeal is a statically-typed dialect of Lua.\nIt extends Lua with type annotations, allowing you to specify arrays, maps and records,\nas well as interfaces, union types and generics.\n\nIt aims to fill a niche similar to that of TypeScript in the JavaScript world,\nbut adhering to Lua's spirit of minimalism, portability and embeddability.\n\nIs it implemented as a compiler, tl,\nwhich compiles .tl source code into .lua files.\n\nHere is a quick taste of what Teal code looks like:\n\n...or check out this page's source code!\n\nYou can also play with the Teal compiler right from your browser,\nusing the Teal Playground.\n\nIf you're using LuaRocks, you can install the compiler with:\n\nYou can also find pre-compiled\nbinaries for Linux and Windows.\nTo build larger projects, you probably won't want to run tl on each\nfile individually. We recommend using Cyan,\nthe build tool designed for Teal.\n\nYou can also install vscode-teal for\nVisual Studio Code integration, teal-language-server\nfor NeoVim and others, and"
  },
  {
    "title": "Initialization in C++ is bonkers (2017) (tartanllama.xyz)",
    "points": 81,
    "submitter": "todsacerdoti",
    "submit_time": "2025-05-15T21:27:10 1747344430",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=43999492",
    "comments": [
      "Heh, low comments on C++ posts now. A sign of the times. My two cents anyway.I've been using C++ for a decade. Of all the warts, they all pale in comparison to the default initialization behavior. After seeing thousands of bugs, the worst have essentially been caused by cascading surprises from initialization UB from newbies. The easiest, simplest fix is simply to default initialize with a value. That's what everyone expects anyway. Use Python mentality here. Make UB initialization an EXPLICIT choice with a keyword. If you want garbage in your variable and you think that's okay for a tiny performance improvement, then you should have to say it with a keyword. Don't just leave it up to some tiny invisible visual detail no one looks at when they skim code (the missing parens). It really is that easy for the language designers. When thinking about backward compatibility... keep in mind that the old code was arguably already broken. There's not a good reason to keep letting it compile. Add a flag for --unsafe-initialization-i-cause-trouble if you really want to keep it.C++, I still love you. We're still friends.\n \nreply",
      "> When thinking about backward compatibility... keep in mind that the old code was arguably already broken. There's not a good reason to keep letting it compile.Oh how I wish the C++ committee and compiler authors would adopt this way of thinking...\nSadly we're dealing with an ecosystem where you have to curate your compiler options and also use clang-tidy to avoid even the simplest mistakes :/Like its insane to me how Wconversion is not the default behavior.\n \nreply",
      "Compilers should add this as a non-standard extension, right?  -ftrivial-auto-var-init=zero is a partial solution to a related problem, but it seems like they could just... not have UB here.  It can't be that helpful for optimization.\n \nreply",
      "Yes but it\u2019s not portable. If zero initialization were the default and you had to opt-in with [[uninitialized]] for each declaration it\u2019d be a lot safer. Unfortunately I don\u2019t think that will happen any time soon.\n \nreply",
      "I don't really care if it isn't portable.  I only have to work with Clang, personally.> If zero initialization were the default and you had to opt-in with [[uninitialized]] for each declaration it\u2019d be a lot safer.I support that, too.  Just seems harder than getting a flag into Clang or GCC.\n \nreply",
      "Portability is always for the other guy\u2019s sake, not your own. That\u2019s why so many people don\u2019t care about it.\n \nreply",
      "Not to worry, there is a 278 page book about initialization in C++!https://leanpub.com/cppinitbook(I don't know whether it's good or not, I just find it fascinating that it exists)\n \nreply",
      "Wow! Exhibit 1 for the prosecution.\n \nreply",
      "Well, authors are incentivized into writing long books. Having said that it obviously doesn't take away the fact that C++ init is indeed bonkers.\n \nreply",
      "What would be the incentive for making this a long book? Couldn't be money.\n \nreply"
    ],
    "link": "https://blog.tartanllama.xyz/initialization-is-bonkers/",
    "first_paragraph": "C++ pop quiz time: what are the values of a.a and b.b on the last line in main of this program?The answer is that a.a is 0 and b.b is indeterminate, so reading it is undefined behaviour. Why? Because initialization in C++ is bonkers.Before we get into the details which cause this, I\u2019ll introduce the concepts of default-, value- and zero-initialization. Feel free to skip this section if you\u2019re already familiar with these.The rules for these different initialization forms are fairly complex, so I\u2019ll give a simplified outline of the C++11 rules (C++14 even changed some of them, so those value-initialization forms can be aggregate initialization). If you want to understand all the details of these forms, check out the relevant cppreference.com articles123, or see the standards quotes at the bottom of the article.Taking the simple example of int as T, global and all of the value-initialized variables will have the value 0, and all other variables will have an indeterminate value. Reading th"
  },
  {
    "title": "The unreasonable effectiveness of an LLM agent loop with tool use (sketch.dev)",
    "points": 198,
    "submitter": "crawshaw",
    "submit_time": "2025-05-15T19:33:44 1747337624",
    "num_comments": 120,
    "comments_url": "https://news.ycombinator.com/item?id=43998472",
    "comments": [
      "Strongly recommend this blog post too which is a much more detailed and persuasive version of the same point. The author actually goes and builds a coding agent from zero: https://ampcode.com/how-to-build-an-agentIt is indeed astonishing how well a loop with an LLM that can call tools works for all kinds of tasks now. Yes, sometimes they go off the rails, there is the problem of getting that last 10% of reliability, etc. etc., but if you're not at least a little bit amazed then I urge you go to and hack together something like this yourself, which will take you about 30 minutes. It's possible to have a sense of wonder about these things without giving up your healthy skepticism of whether AI is actually going to be effective for this or that use case.This \"unreasonable effectiveness\" of putting the LLM in a loop also accounts for the enormous proliferation of coding agents out there now: Claude Code, Windsurf, Cursor, Cline, Copilot, Aider, Codex... and a ton of also-rans; as one HN poster put it the other day, it seems like everyone and their mother is writing one. The reason is that there is no secret sauce and 95% of the magic is in the LLM itself and how it's been fine-tuned to do tool calls. One of the lead developers of Claude Code candidly admits this in a recent interview.[0] Of course, a ton of work goes into making these tools work well, but ultimately they all have the same simple core.[0] https://www.youtube.com/watch?v=zDmW5hJPsvQ\n \nreply",
      "Can't think of anything an LLM is good enough at to let them do on their own in a loop for more than a few iterations before I need to reign it back in.\n \nreply",
      "They're extremely good at burning through budgets, and get even better when unattended\n \nreply",
      "The main problem with agents is that they aren't reflecting on their own performance and pausing their own execution to ask a human for help aggressively enough.  Agents can run on for 20+ iterations in many cases successfully, but also will need hand holding after every iteration in some cases.They're a lot like a human in that regard, but we haven't been building that reflection and self awareness into them so far, so it's like a junior that doesn't realize when they're over their depth and should get help.\n \nreply",
      "I built android-use[1] using LLM. It is pretty good at self healing due to the \"loop\", it constantly checks if the current step is actually a progress or regress and then determines next step. And the thing is nothing is explicitly coded, just a nudge in the prompts.1. clickclickclick - A framework to let local LLMs control your android phone (https://github.com/BandarLabs/clickclickclick)\n \nreply",
      "Ah, it\u2019s Thorsten Ball!I thoroughly enjoyed his \u201cwriting an interpreter\u201d. I guess I\u2019m going to build an agent now.\n \nreply",
      "There's also this one which uses pocketflow, a graph abstraction library to create something similar [0]. I've been using it myself and love the simplicity of it.[0]  https://github.com/The-Pocket/PocketFlow-Tutorial-Cursor/blo...\n \nreply",
      "For \"that last 10% of reliability\" RL is actually working pretty well right now too! https://openpipe.ai/blog/art-e-mail-agent\n \nreply",
      "Should we change the link above to use `?utm_source=hn&utm_medium=browser` before opening it?\n \nreply",
      "fixed :)\n \nreply"
    ],
    "link": "https://sketch.dev/blog/agent-loop",
    "first_paragraph": "2025-05-15 by Philip Zeyliger\nMy co-workers and I have been working on an AI Programming Assistant called\nSketch\n for the last few months. The thing I've been most surprised by is how shockingly simple the main loop of using an LLM with tool use is:\n\nThere's some pomp and circumstance to make the above work (here's the full script)\n, but the core idea is the above 9 lines. Here, llm() is a function that sends the system prompt, the conversation so far, and the next message to the LLM API.\n\nTool use is the fancy term for \"the LLM returns some output that corresponds to a schema,\" and, in the full script, we tell the LLM (in its system prompt and tool description prompts) that it has access to bash.\n\nWith just that one very general purpose tool, the current models (we use Claude 3.7 Sonnet extensively) can nail many problems, some of them in \"one shot.\" Whereas I used to look up an esoteric git operation and then cut and paste, now I just ask Sketch to do it. Whereas I used to handle git"
  },
  {
    "title": "Tek \u2013 A music making program for 24-bit Unicode terminals (codeberg.org)",
    "points": 80,
    "submitter": "smartmic",
    "submit_time": "2025-05-15T19:56:59 1747339019",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=43998707",
    "comments": [
      "I love the top down tracker UI. For some more you can look at Aphex Twin using PlayerPRO [0] and Hitori Tori using Renoise [1][0] https://vimeo.com/223378825[1] https://www.youtube.com/watch?v=VnJY5u6cC7ABoth are mindblowing.\n \nreply",
      "This program doesn't look top down to me, it seems like the main sequencing interface is piano roll, although the sample editing looks tracker-inspired. It'll be interesting to see what people do with it. Maybe fans of piano roll will find a lightweight way to get their ideas down that feels simpler than a full Ableton setup?For me I still find piano roll awkward to read. I can't visualize what the music is going to sound like the way I can with a tracker pattern. Perhaps it's because my eyes need to scan left and right and up and down too much to see what each note is? Let's not even get into the problem of having effects configured on a totally different screen. When I transitioned from tracker to hardware and later to Ableton I felt like I wasn't really writing music any more, I was just rearranging sounds till they eventually turned into music. It made for perhaps more surprising and interesting results, but I never really got back the feeling of control and finesse that I had with trackers.\n \nreply",
      "I bought playerpro and we used it for a while, been a long time since I thought of it. I just checked my old Mac CD case and it's not in there, oh well. We also switched to reason around version 3-5.\n \nreply",
      "It's open-source now, but never made the transition to 64-bit APIs.https://sourceforge.net/projects/playerpro/\n \nreply",
      "If you ever do find that disc I'd love to buy it from you lol.\n \nreply",
      "nit: Renoise; good stuff, thanks for sharing.\n \nreply",
      "There's demo video at https://v.basspistol.org/w/eUCnnNJugRZ5haqyuYQ2CK\n \nreply",
      "In case anyone else wondered, it doesn't seem to involve Tek (Tektronix) terminal graphics.  Maybe the name was an homage?\n \nreply",
      "Are there any DAWs that support VIM keybinds?\n \nreply",
      "Really awesome UI. Is there a quick YouTube demo?\n \nreply"
    ],
    "link": "https://codeberg.org/unspeaker/tek",
    "first_paragraph": "a music making program for 24-bit unicode terminals.written in rust\nwith ratatui on crossterm\nfor jack and pipewire.tek is available as source,\nstatically linked binaries, and on the\naur.author is reachable via mastodon @unspeaker@mastodon.social\nor matrix @unspeaker:matrix.orgyou can download tek 0.2.0 \"almost static\"\nfrom codeberg releases. this standalone binary release, should work on any glibc-based system.tek 0.2.0-rc7 is available as a package in the AUR.\nyou can install it using your preferred AUR helper (e.g. paru):requires docker.inspired by trackers and hardware sequencers,\nbut with the critical feature that 90s samplers lack:\nable to resample, i.e. record while playing!pop-up scratchpad for musical ideas.\nlow resource consumption, can stay open in background.\nbut flexible enough to allow expanding on compositionshuman- and machine- readable project format\nsimple representation for project data\nenable scripting and remapping."
  },
  {
    "title": "NASA keeps ancient Voyager 1 spacecraft alive with Hail Mary thruster fix (theregister.com)",
    "points": 51,
    "submitter": "nullhole",
    "submit_time": "2025-05-16T00:29:09 1747355349",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44000700",
    "comments": [
      "Such a beautiful tribute to the tenacity of humanity's creativity to beat the odds.\n \nreply",
      "Very nice, amazing they were able to keep them working.I remember when they were launched, I saw an article saying somehow the engineers added better components some functionalities even when they were forbidden.  Somehow they hid it.I forgot exactly what the articles said, but it indicated this was done due to a once in many centuries of the alignment.\n \nreply"
    ],
    "link": "https://www.theregister.com/2025/05/15/voyager_1_survives_with_thruster_fix/",
    "first_paragraph": ""
  },
  {
    "title": "The current state of TLA\u207a development (ahelwer.ca)",
    "points": 86,
    "submitter": "todsacerdoti",
    "submit_time": "2025-05-15T18:53:55 1747335235",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=43998115",
    "comments": [
      "> I think we can increase the TLC model checker throughput to 1 billion states per minute (1000x speedup) by writing a bytecode interpreter.Truffle [1] can convert an interpreter to a JIT compiler -- there's no need to invent bytecode, and instead of an interpreter you get compilation to native, and it's easy to add intrinsics in Java; optimisations can be added gradually over time. This would probably be the most effective strategy, even in general, and certainly compared to cost.[1]: https://www.graalvm.org/latest/graalvm-as-a-platform/languag...\n \nreply",
      "My current thinking on model checking (still evolving):Modeling languages are useful to check the correctness of an algorithm during development. During development, a model can also serve as a specification for the actual implementation. This requires that your the modeling language is readable to a broad range of developers, which TLA+ is not. We have been experimenting with FizzBee (fizzbee.io) which looks promising in this regards.When you go to prod, you really want to test your actual implementation, not a model of it. For this you want something like https://github.com/awslabs/shuttle (for Rust), or https://github.com/cmu-pasta/fray (for Java). Or use something custom.\n \nreply",
      "TBH, as cool as TLA+ is, the biggest issue I generally see with trying to use formal methods in practice is that you need to keep the specification matching the actual implementation. Otherwise whatever you've formally verified doesn't match what actually got built.So formal methods may be used for extremely critical parts of systems (eg; safety critical systems or in embedded where later fixes cannot be readily rolled out) but they fail to make inroads in most other development because it's a lot of extra work.\n \nreply",
      "I've always seen it as a tool for validating a design rather than an implementation.\n \nreply",
      "On the other hand, how many million man hours are spent re inventing the wheel that could instead be spent contributing to a library of extremely well-specified wheels?\n \nreply",
      "Hillel Wayne wrote a post[0] about this issue recently, but on a practical level I think I want to address it by writing a \"how-to\" on trace validation & model-based testing. There are a lot of projects out there that have tried this, where you either get your formal model to generate events that push your system around the state space or you collect traces from your system and validate that they're a correct behavior of your specification. Unfortunately, there isn't a good guide out there on how to do this; everybody kind of rolls their own, presents the conference talk, rinse repeat.But yeah, that's basically the answer to the conformance problem for these sort of lightweight formal methods. Trace validation or model-based testing.[0] https://buttondown.com/hillelwayne/archive/requirements-chan...)\n \nreply",
      "Plus my Kayfabe system [0], which was partly inspired by Ron Pressler's article on trace validation:0. https://conf.tlapl.us/2020/11-Star_Dorminey-Kayfabe_Model_ba...\n \nreply",
      "why are the lower case L's in that document bolded? a different weight? Not sure what the right technical change term is for the visual difference but it was extremely noticeable immediately upon opening the document\n \nreply",
      "What do you think of embedding it in a formal system like Lean as a frontend?\n \nreply",
      "\"I think we can increase the TLC model checker throughput to 1 billion states per minute (1000x speedup) by writing a bytecode interpreter. C\"I never used TLC with a large model, but I bet making the tool faster would make it more useful to lots of people.I wonder what the speedup would be if the code targeted a GPU?\n \nreply"
    ],
    "link": "https://ahelwer.ca/post/2025-05-15-tla-dev-status/",
    "first_paragraph": "The 2025 TLA\u207a Community Event was held last week on May 4th at McMaster University in Hamilton, Ontario, Canada.\nIt was a satellite event to ETAPS 2025, which I also attended, and plan to write about in the near future.\nI gave a talk somewhat-hucksterishly titled It\u2019s never been easier to write TLA\u207a tooling! which I will spin into a general account of the state of TLA\u207a development here.\nThe conference talks were all recorded, so if you\u2019d like this blog post in video form you can watch it below:The thesis of this post is that almost all the dreams & desires we have for TLA\u207a are downstream of making it easy to develop in or on TLA\u207a language tooling.\nWe break this down into three parts:I\u2019m very optimistic about the future of TLA\u207a, despite the challenges.\nWe now have the TLA\u207a Foundation, which has very generously been paying me a comfortable living wage to work on the tools for the past six months.\nI will talk about some of the things I\u2019ve accomplished.\nBeing paid to work on FOSS is a bles"
  },
  {
    "title": "Launch HN: Tinfoil (YC X25): Verifiable Privacy for Cloud AI",
    "points": 100,
    "submitter": "FrasiertheLion",
    "submit_time": "2025-05-15T16:19:00 1747325940",
    "num_comments": 80,
    "comments_url": "https://news.ycombinator.com/item?id=43996555",
    "comments": [
      "Just noticed Tinfoil runs Deepseek-R1 \"70b\". Technically this is not the original 671b Deepseek R1; it's just a Llama-70b trained by Deepseek R1 (called \"distillation\").\n \nreply",
      "How large do you wager your moat to be? Confidential computing is something all major cloud providers either have or are about to have and from there it's a very small step to offer LLM-s under the same umbrella. First mover advantage is of course considerable, but I can't help but feel that this market will very quickly be swallowed by the hyperscalers.\n \nreply",
      "Cloud providers aren't going to care too much about this.I have worked for many enterprise companies e.g. banks who are trialling AI and none of them have any use for something like this. Because the entire foundation of the IT industry is based on trusting the privacy and security policies of Azure, AWS and GCP. And in the decades since they've been around not heard of a single example of them breaking this.The proposition here is to tell a company that they can trust Azure with their banking websites, identity services and data engineering workloads but not for their model services. It just doesn't make any sense. And instead I should trust a YC startup who statistically is going to be gone in a year and will likely have their own unique set of security and privacy issues.Also you have the issue of smaller sized open source models e.g. DeepSeek R1 lagging far behind the bigger ones and so you're giving me some unnecessary privacy attestation at the expense of a model that will give me far better accuracy and performance.\n \nreply",
      "Being gobbled by the hyperscalers may well be the plan. Reasonable bet.\n \nreply",
      "GCP has confidential VMs with H100 GPUs; I'm not sure if Google would be interested. And they get huge discount buying GPUs in bulk. The trade-off between cost and privacy is obvious for most users imo.\n \nreply",
      "Confidential computing as a technology will become (and should be) commoditized, so the value add comes down to security and UX. We don\u2019t want to be a confidential computing company, we want to use the right tool for the job of building private & verifiable AI. If that becomes FHE in a few years, then we will use that. We are starting with easy-to-use inference, but our goal of having any AI application be provably private\n \nreply",
      "This. Big tech providers already offer confidential inference today.\n \nreply",
      "Yes Azure has! They have very different trust assumptions though. We wrote about this here https://tinfoil.sh/blog/2025-01-30-how-do-we-compare\n \nreply",
      "Last I checked it was only Azure offering the Nvidia specific confidential compute extensions, I'm likely out of date - a quick Google was inconclusive.Have GCP and AWS started offering this for GPUs?\n \nreply",
      "GCP, yes: https://cloud.google.com/confidential-computing/confidential...\n \nreply"
    ],
    "link": "item?id=43996555",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Easel \u2013 Code multiplayer games like singleplayer (easel.games)",
    "points": 35,
    "submitter": "BSTRhino",
    "submit_time": "2025-05-14T10:31:18 1747218678",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=43982892",
    "comments": [
      "This is cool, but its hard to make a game engine full featured enough to be worth developing for.  Have you thought about making this an addon for another engine like Godot or Bevy?  Godot in particular is a nice engine to develop on, but the multiplayer support could be improved.\n \nreply",
      "I'm curious how your engine decides to trigger rollbacks without explicit knowledge of which parts of the game state each input can actually affect. Naively rolling back on every missed input can and will be abused - in early versions of Street Fighter V players found out that while being comboed you could churn out inputs as fast as possible to cause rollbacks for your opponent, even though none of those inputs could actually do anything (this was made worse by other issues with that game's inability to keep opponent game clocks synced and constant one-sided rollbacks).\n \nreply",
      "Yes, it's true Easel does rollback on every input. I may be able to improve this. One great thing about controlling the programming language is there are a lot more places I can hook into for things like this. For example, Easel does detect at compile time which types inputs are used somewhere in the codebase and only sends those across the network.Easel constantly synchronises the clocks (there's an interesting algorithm for this which I will write up at some point). It also adaptively assigns two different kinds of delay to every client - command delay and display delay. Command delay is related to how much lag you are introducing into the game. Basically people take on their own lag. It can be different amounts for different people in the game. The display delay is where the rollback netcode kicks in. It keeps track of how much rollback your computer can handle imperceptibly. If your computer can't handle it, then you won't get as much rollback (and will just experience more input latency). But in either case, whatever number it picks, it should be smooth.\n \nreply",
      "I wonder if SAT solvers could prove the maximum \"reach\" of inputs into the state...\n \nreply",
      "I exclusively make games that include online multiplayer, so this really caught my eye. Really looking forward to digging in.\n \nreply",
      "Woohoo! I'm excited for you to dig in!\n \nreply",
      "How do you handle hidden information, or \"need to know\"/\"potentially visible set\" style revelations to player clients?\n \nreply",
      "Unfortunately, this is not compatible with the rollback netcode model. However, all the world state is stored in WASM linear memory and so it is not the most straightforward to decode.The rollback netcode model is necessary to make the multiplayer invisible to the developer. The other client-server/state-synchronization approach which is used in other multiplayer games, while great in many ways, requires you to assign authorities to every entity and to send a remote procedure call when attempting to affect entities you do not control. Rollback netcode was the only way for me to achieve the primary mission.I may look into supporting the client-server/state-synchronization multiplayer model in the future though, which would enable \"need to know\" style revelations. Given we already have a deterministic programming language it is not at all infeasible as a future project.From my experience running a multiplayer game, I only had 1 in every 50000 players attempt to perform some kind of hack and it was faster and more reliable for me to shadow IP ban the players. That feature is built into Easel. There is also a replays system built-in which makes it easy for people to submit evidence of people hacking. This is the current pragmatic solution that I suggest.\n \nreply",
      "WASM-4 has a simiar multiplayer model. There's some things you still have to be careful with, like delaying screen transitions so you don't have a \"you win\" screen flash for a few frames when the other player didn't lose but their input hasn't gotten t o you yet.\n \nreply",
      "Yes. I've made a dozen games using rollback and it really is magical. I built a strategy game in three weeks and didn't test it in multiplayer until the weekend of release. It Just Worked.But it's not a total silver bullet from a UX perspective when rollbacks happen.Showing a player dying and then come back to life and actually you're dead will absolutely happen. It's very weird if they were ragdolling.If players don't have high inertia, like a platformer, they'll teleport around the place as you get the information that actually they aren't falling they jumped 100ms ago.This is all fixable, but requires first class confirmation (i.e\n has the player you shot been dead longer than the max rollback window), and hand tuned interpolation on critical entities.Luckily, I'm sure it's possible to add them to this engine.I'm curious as to why a custom programming language was designed if the system uses WASM anyway - which you can make deterministic.I wrote my system in C# and it worked great if you Followed The Rules (eg you must use immutable data structures). But WASM would have been a big step up.\n \nreply"
    ],
    "link": "https://easel.games/about",
    "first_paragraph": "Easel is a 2D game programming language that makes multiplayer as easy as singleplayer.Whether you are a beginner or expert coder, you'll love making games with Easel!\"can i just say that i absolutely [...] love easel\"see more testimonialsMaking games is fun, but it is even more fun when you can play them with other people! Unfortunately, online multiplayer games are difficult to make. They require managing networking, synchronization, authorities and prediction. That is a lot of work when you just want to get straight into making a game!Multiplayer is baked in to the Easel programming language. Code as if all your players are inside one shared game world, and Easel will take care of the rest. While other game engines give you the building blocks to make multiplayer, Easel has already built it into the foundations of the engine itself so you don't even have to think about it!Not only does Easel make multiplayer effortless, Easel's advanced rollback netcode implementation will give your"
  },
  {
    "title": "GTK Krell Monitors (srcbox.net)",
    "points": 18,
    "submitter": "Deeg9rie9usi",
    "submit_time": "2025-05-13T06:19:43 1747117183",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=43970057",
    "comments": [
      "The original author of gkrellm, Bill Wilson, also made PiKrellCam. <https://billw2.github.io/pikrellcam/pikrellcam.html>Which was/is the best Raspberry Pi Audio/Video Recording, OSD Motion Detect Program. It was made to run perfectly on a RPI Zero with its limited CPU and memory.I was very sad to learn that Bill passed away in Oct 2021. <https://github.com/billw2/pikrellcam/issues/78#issuecomment-...>I'm glad gkrellm got a new maintainer and continues to exist.\n \nreply",
      "Wow, it\u2019s funny because the last time I ran gkrellm was 23 years ago when I first started using Linux and I thought I was a l33t h4x0r\u2026And just today, now that I actually write code for a living and use Linux on my work machine, I found myself really wanting a good display to tell me when my memory usage was growing.[0] I was using the gnome activity monitor but it takes up way too much screen space and was always behind the window I was using. It looks like this could actually be useful for me to run now![0] I was running a local kubernetes cluster with an opentracing implementation, where I hadn\u2019t quite worked out the configs for memory usage yet, and it kept spiking and OOMing when I wasn\u2019t looking. It\u2019s fun when your mouse cursor just stops moving and you\u2019re wondering whether you need to hold down the power button or what\u2026\n \nreply",
      "https://extensions.gnome.org/extension/3010/system-monitor-n... is good.\n \nreply",
      "You might also consider Conky: https://github.com/brndnmtthws/conky\n \nreply",
      "What a blast from the past! Stoked it\u2019s still under development!Had a look for the etymology of \u201cKrell\u201d -  in the readme it mentions \u201cGKrellM - keeping an eye on your computer\u2019s Id\u201d.Seems to be a reference to this 1956 movie, Forbidden Planet / Monsters from the Id: http://guidetomonsters.com/html/50s/Id%20Monster.htmlCan I get a fact check?\n \nreply",
      "The Wiki says you're spot on.  Impressive sleuthing.https://archive.ph/20120710180815/http://members.dslextreme....\n \nreply",
      "It has a client/server mode. I used to run the server on my wrt54g and the GUI client on my desktop.\n \nreply",
      "I ran this thing with slackware and window manager back in the day. My goodness, I have forgotten about this entirely.I might have to spin it up for fun.For all the nix fols, \"nix run nixpkgs#gkrellm\" works =P\n \nreply",
      "These days it is one of the most underrated tool!  I never has a problem with it under any DE or WM.FWIW, it is still bundled with Slackware.\n \nreply",
      "I loved this tool. Always ran it with fluxbox as the WM.\n \nreply"
    ],
    "link": "https://gkrellm.srcbox.net/",
    "first_paragraph": "\nThemes_Doc\nPlugins\n\n            GKrellM is a single process stack of system monitors which supports\n            applying themes to match its appearance to your window\n            manager, Gtk, or any other theme.\n          \n            1: The CPU chart's nice time (green) is de-emphasized\n            by splitting it into a small view, and the sys time is inverted.\n            The disk hda chart is split at the 50% point.\n            The 5 volt supply is triggering a warning. The animated penguin is\n            announcing there is new mail.\n            \n            Three audio plugins are shown enabled:\n            GKrellMSS sound scope, Volume, and GKrellMMS . See the plugins\n            page for these and many more interesting plugins.\n          \n            2: A more simple configuration with nice time hidden and\n            two net interfaces being monitored. The battery time left\n            is triggering an alarm. The theme is one I put together as a test\n            and demo for"
  },
  {
    "title": "A Tiny Boltzmann Machine (eoinmurray.info)",
    "points": 219,
    "submitter": "anomancer",
    "submit_time": "2025-05-15T13:41:37 1747316497",
    "num_comments": 39,
    "comments_url": "https://news.ycombinator.com/item?id=43995005",
    "comments": [
      "Fun article on David Ackley https://news.unm.edu/news/24-nobel-prize-in-physics-cited-gr...Do check out his T2 Tile Project.\n \nreply",
      "The key takeaways are that there are lots of people involved with making these breakthroughs.The value of grad students is often overlooked, they contribute so much and then later on advance the research even more.Why does America look on research as a waste, when it has move everything so far?\n \nreply",
      "It's more accurate to say that businesspeople consider research a waste in our quarter-by-quarter investment climate, since it generally doesn't lead to immediate gains.And our current leadership considers research a threat, since science rarely supports conspiracy theorists or historical revisionism.\n \nreply",
      "More charitably, America's current government has an unusually large concentration of business people. Interestingly, they were elected as a vote for change by a population of non business people who were tired of the economic marginalization they suffered when their government consisted largely of non business people not once but twice. It will be interesting to see how this plays out.\n \nreply",
      "My understanding is that the Harmonium (Smolensky) was the first restricted Boltzmann machine, but maximized \u201charmony\u201d instead of minimizing \u201cenergy.\u201d When Smolensky, Hinton and Rummelhart collaborated, they instead called it \u201cgoodness of fit.\u201dThe harmonium paper [1] is a really nice read. Hinton obviously became the superstar and Smolensky wrote long books about linguistics.Anyone know more about this history?[1] https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap6_PDP8...\n \nreply",
      "I mistook the title for \"A Tiny Boltzmann Brain\"! [0]My own natural mind immediately solved the conundrum. Surely this was a case where a very small model was given randomly generated weights and then tested to see if it actually did something useful!After all, the smaller the model, the more likely simple random generation can produce something interesting, relative to its size.I stand corrected, but not discouraged!I propose a new class of model, the \"Unbiased-Architecture Instant Boltzmann Model\" (UA-IBM).One day we will have quantum computers large enough to simply set up the whole dataset as a classical constraint on a model defined with N serialized values, representing all the parameters and architecture settings. Then let a quantum system with N qubits take one inference step over all the classical samples, with all possible parameters and architectures in quantum superposition, and then reduce the result to return the best (or near best) model's parametesr and architecture in classical form.Anyone have a few qubits laying around that want to give this a shot? (The irony that everything is quantum and yet so slippery we can hardly put any of it to work yet.(Sci-fi story premise: the totally possible case of an alien species that evolved one-off quantum sensor, which evolved into a whole quantum sensory system, then a nervous system, and subsequently full quantum intelligence out of the gate. What kind of society and technological trajectory would they have? Hopefully they are in close orbit around a black hole, so the impact of their explosive progress has not threatened us yet. And then one day, they escape their gravity well, and ...)[0] https://en.wikipedia.org/wiki/Boltzmann_brain\n \nreply",
      "Poor quantum beings. They don't have access to a computation model that exceeds the speeds of their own thoughts and they are forever doomed to be waiting a long time for computations to happen\n \nreply",
      "That isn't how quantum computers work.\n \nreply",
      "My understanding, which is far from certain, is that problems like this (try a large combination of solutions, cooperate between superpositions to identify the best, then set up the quantum outputs so when they are classically sampled, the best answer is the most likely sample obtained) are solved in four stages:1. N^2 potential solutions encoded in superposition across N qubuts.2. Each superposition goes through the quantum version of a normal inference pass, using each superpositions weights to iteratively process all the classical data, then calculates performance. (This is all done without any classical sampling.)3. Cross-superposition communication that results in agreement on the desired version. This is the weakest part of my knowledge. I know that is an operation type that exists, but I don't know how circuits implement it. But the number of bits being operated on is small, just a performance measure. (This is also done without any classical sampling.)4. Then the output is sampled to get classical values.  This requires N * log2(N) circuit complexity, where N is the number of bits defining the chosen solution, i.e. parameters and architectural settings.  This can be a lot of hardware, obviously, perhaps more than the rest of the hardware, given N will be very large.Don't take anything I say here for granted. I have designed parts of such a circuit, using ideal quantum gates in theory, but not all of it. I am not an expert, but I believe that every step here is well understood by others.The downside relative to other approaches: It does a complete search of the entire solutions space directly, so the result comes from reducing N^2 superpositions across N qubits, where for interesting models the N can be very large (billions of parameters x parameter bit width), to get N qubits. No efficiencies are obtained from gradient information or any other heuristic. This is the brute force method. So an upper bound of hardware requirements.Another disadvantage, is since the solution was not obtained by following a gradient, the best solution might not be robust. Meaning, it might be a kind of vertex between robust solutions that manages to do best on the designed data, but small changes of input samples from the design data might not generalize well. This is less likely to be a problem if smoothing/regularization information is taken into account account for determining each superpositions performance.\n \nreply",
      "IIUC, we need gibbs sampling(to compute the weight updates) instead of using the gradient based forward and backward passes with today's NNetworks that we are used to. Any one understand why that is so?\n \nreply"
    ],
    "link": "https://eoinmurray.info/boltzmann-machine",
    "first_paragraph": "Boltzmann MachinesHere we introduce introduction to Boltzmann machines and\npresent a Tiny Restricted Boltzmann Machine that runs in the browser.Boltzmann Machines are used for unsupervised learning, which means they can learn\nfrom data without being told what to look for.The can be used for generating new data that is similar to the data they were trained on, also known as generative AI.A Boltzmann Machine is a type of neural network that tries to learn patterns by mimicking\nhow energy works in physics.Each neuron can be on or off, the machine is made up of many of these neurons connect to each other.Some neurons are  visible (we can see them and even set their state), and some are  hidden (we can't see them).The connections between neurons are called weights, and they can be  positive or  negative.A General Boltzmann Machine has connections between all neurons. This makes it powerful, but its training involves calculating an O(2n)O(2^n)O(2n) term.A Restricted Boltzmann Machine is a sp"
  },
  {
    "title": "Malicious compliance by booking an available meeting room (clientserver.dev)",
    "points": 300,
    "submitter": "jakevoytko",
    "submit_time": "2025-05-15T13:20:28 1747315228",
    "num_comments": 278,
    "comments_url": "https://news.ycombinator.com/item?id=43994765",
    "comments": [
      "> When 2:50 rolled around and your meeting was supposed to end, do you think people actually ended the meeting? Noooooo. Absolutely not!At U of M, they solved this problem by having classes officially start 10 minutes after the time they were advertised as.  That is, a class listed as being 10-11am was actually 10:10-11am; nobody showed up until 10:10.Sure, technically it's the same thing, but there's a pretty massive anchoring effect for things on the hour.  Still being in the meeting room at 11:01 feels a lot later than still being in the meeting room at 10:51.\n \nreply",
      "In Finland the universities (and I believe in many other European universities have/had this as well) there was \"academic quarter\" which meant that if something was scheduled for 10am it would actually start at 10:15am. IIRC if they used precise time (10:00) then it would actually start at that time.I've heard it dates back to when people didn't have easy access to precise time. It would allow students to hear the hourly bells and walk to the class.\n \nreply",
      "Same in Germany. Times are usually assumed to be ct (cum tempore) and start XY:15. When something starts sharp, it's specified as st (sine tempore).\n \nreply",
      "It also allows you to have \"1 hour\" classes that are at 10am and 11am, and you aren't forced to leave early or arrive late.  A 5m gap isn't enough for huge numbers of classes in many campuses.\n \nreply",
      "I confirm, we have it in Italian universities (it's called \"quarto d'ora accademico\" in Italian).\n \nreply",
      "This thread is absolutely fascinating \u2014 American, never heard of this practice (esp ct/st), and desperately want it in my life now!\n \nreply",
      "For the most part, American Universities were established after railroad time tables were a thing\u2026and in the US Latin and the other liberal arts were never the primary curriculum at most US universities, so cum tempore might as well be Latin.\n \nreply",
      "In Poland \"academic quarter\" has a sense that if the teacher didn't show up and it's 15 minues past, the students can leave. They still need to show up for the class at 00 every time and are scolded to varying degree if they showed up after the teacher started which they do right after they arrive.\n \nreply",
      "At my university in New Zealand they didn't take attendance for lectures. You attended the lectures so you could learn stuff so you could pass the exams. It's surprising that isn't considered normal.(There's some nuance to that statement as science courses tende to have labs - I don't remember why first-year physics was a requirement for software engineering, but it was - mathematics courses tended to have weekly assignments, and at least one software course had a very unusual style of putting us in a room one whole day per week for a semester to work on group projects.)\n \nreply",
      "... so the old American high school \"if the teacher is 15 minutes late, we're legally allowed to leave\" meme has some roots in reality? Huh.\n \nreply"
    ],
    "link": "https://www.clientserver.dev/p/malicious-compliance-by-booking-an",
    "first_paragraph": ""
  },
  {
    "title": "Refactoring Clojure (orsolabs.com)",
    "points": 74,
    "submitter": "luu",
    "submit_time": "2025-05-15T19:28:01 1747337281",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=43998423",
    "comments": [
      "Seems fair, but refactoring is generally tricky and:1) replacing a func with a simpler recursive func may or may not be ideal.2) this fills me with no joy when I read it:> It seems easier to start from scratch as opposed to refactor it. The characterization tests above will help ensuring that the returned hash map stays the same.Its enormously hmm\u2026 what word to use. Ambitous? Foolish? Brave?Anyway, taking code, writing some tests for it and then rewriting the entire thing so it passes the tests you wrote only barely falls into \u201crefactoring\u201d.If anyone refactored my code base by simply rewriting from scratch the parts they dont like, I would be upset.Rewriting things is hard, and its \u2026 brave\u2026 to think you can just write a few tests and then if they all pass its all good!That only works in very trivial examples, and really, doesnt show case the strengths of clojure at all.\n \nreply",
      "I\u2019ve been writing a lot of functional lisp lately for the first time and one thing that\u2019s struck me is how easy it is to refactor. Because a form can always be reduced to its result, you can pluck them out, move them around, and evaluate them at any level of the tree. While this is true of functional programming in general the structure of lisp makes its super easy in a way it\u2019s not in a Haskell or just functional JavaScript.\n \nreply",
      "First few comments are nothing but shallow {rem,sn}arks on the formatting of the site, by users that don't even have the excuse of being recent joiners. If this is how we lead by example then it's no wonder why the quality of content+comments on this site are (imo) on the decline.\n \nreply",
      "Even as someone who lives with transparent terminals and poor color schemes the article is _very_ hard to read though. It's not a shallow remark when the formatting completely distracts/detracts from the rest of article.\n \nreply",
      "In general, it's a dream to test and refactor Clojure IMHO. Working with mostly static top-level functions, immutability by default, and mocking when necessary with the use of \"with-redefs\" makes writing tests easy. Also, the immutability makes it hard to break shared copies of memory which seemed to plague my earlier tangos with large Java code bases and standard OOP practices.\n \nreply",
      "most people I know eschew the use of with-redefs for testing because it's hard to verify that the testing environment is configured correctly as the codebase changes (but otherwise I second the points about immutability by default, and static/pure functions!)\n \nreply",
      "Agreed - concretely with-redefs  forces single threaded test execution. So eg you can\u2019t use the eftest multithreaded mode.Explicit dynamic bindings are better if you need something like this since those are thread local.\n \nreply",
      "Usually the controversial decision for Clojure code highlighting is rainbow parens. This color scheme is horrific and unreadable (on mobile at least).\n \nreply",
      "I don't like the color scheme, and in some of the snippets I don't understand the correlation, but some of them, I think the structural highlighting is very nice.\n \nreply",
      "> Our mission is to take this code and make it readableYou failed. Between the unreadable text colors and the word wrap, the code is incomprehensible. I cut and pasted it into a plaintext notes app and it was way easier to understand\n \nreply"
    ],
    "link": "https://www.orsolabs.com/post/refactoring-clojure-1/",
    "first_paragraph": "This article is based on Writing Friendlier Clojure by Adam Bard, where he shows his approach at refactoring some Clojure code that implements an order-1 word-level Markov text generator.Our mission is to take this code and make it readable:After playing a bit with it in the REPL to get a feeling of what is going on and of the shape of the data structure involved, we can start thinking how to refactor.Remember that refactoring means changing the code without changing its behavior, so to know that we are not changing the behavior we need tests. Since the code exists already, we need  characterization tests, well explained in the book Working Effectively with Legacy Code by Michael Feathers.The following tests characterize the function markov-data:From the REPL and the tests we see that function markov-data takes a string and returns a hash map whose keys are the words in the string and whose values are sequences of the words just after the key, implementing an  order-1 word-level Markov"
  },
  {
    "title": "Show HN: Min.js style compression of tech docs for LLM context (github.com/marv1nnnnn)",
    "points": 150,
    "submitter": "marv1nnnnn",
    "submit_time": "2025-05-15T13:40:04 1747316404",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=43994987",
    "comments": [
      "I applaud this effort, however the \"Does it work?\" section answers the wrong question. Anyone can write a trivial doc compressor and show a graph saying \"The compressed version is smaller!\"For this to \"work\" you need to have a metric that shows that AIs perform as well, or nearly as well, as with the uncompressed documentation on a wide range of tasks.\n \nreply",
      "I totally agreed with your critic. To be honest, it's even hard for myself to evaluate.\nWhat I do is select several packages that current LLM failed to handle, which are in the sample folder, `crawl4ai`, `google-genai` and `svelte`. And try some tricky prompt to see if it works.\nBut even that evaluation is hard. LLM could hallucinate. I would say most time it works, but there are always few runs that failed to deliver. \nI actually prepared a comparison, cursor vs cursor + internet vs cursor + context7 vs cursor + llm-min.txt. But I thought it was stochastic, so I didn't put it here. Will consider add to repo as well\n \nreply",
      "> But even that evaluation is hard. LLM could hallucinate. I would say most time it works, but there are always few runs that failed to deliverYou can use success rate % over N runs for a set of problems, which is something you can compare to other systems. A separate model does the evaluation. There are existing frameworks like DeepEval that facilitate this.\n \nreply",
      "To be honest with you, it being stochastic is exactly why you should post it.Having data is how we learn and build intuition. If your experiments showed that modern LLMs were able to succeed more often when given the llm-min file, then that\u2019s an interesting result even if all that was measured was \u201cdid the LLM do the task\u201d.Such a result would raise a lot of interesting questions and ideas, like about the possibility of SKF increasing the model\u2019s ability to apply new information.\n \nreply",
      "> LLM could hallucinateThe job of any context retrieval system is to retrieve the relevant info for the task so the LLM doesn't hallucinate. Maybe build a benchmark based on less-known external libraries with test cases that can check the output is correct (or with a mocking layer to know that the LLM-generated code calls roughly the correct functions).\n \nreply",
      "Thanks for the feedback. This will be my next step. Personally I feel it's hard to design those test cases (by myself)\n \nreply",
      "It's also missing the documentation part. Without additional context, method/type definitions with a short description will only go so far.Cherry picking a tiny example, this wouldn't capture the fact that cloudflare durable objects can only have one alarm at a time and each set overwrites the old one. The model will happily architect something with a single object, expecting to be able to set a bunch of alarms on it. Maybe I'm wrong and this tool would document it correctly into a description. But this is just a small example.For much of a framework or library, maybe this works. But I feel like (in order for this to be most effective) the proposed spec possibly needs an update to include  little more context.I hope this matures and works well. And there's nothing stopping me from filling in gaps with additional docs, so I'll be giving it a shot.\n \nreply",
      "Was going to point this out too. One suggestion would be to try this on libraries having recent major semvar bumps. See if the compressed docs do better on the backwards incompatible changes.\n \nreply",
      "Yea I was disappointed to see that they just punted (or opted not to show?) on benchmarks.\n \nreply",
      "92% reduction is amazing. I often write product marketing materials for devtool companies  and load llms.txt into whatever AI I\u2019m using to get accurate details and even example code snippets. But that instantly adds 60k+ tokens which, at least in Google AI Studio, annoyingly slows things down. I\u2019ll be trying this.Edit: After a longer look, this needs more polish. In addition to key question raised by someone else about quality, there are signs of rushed work here. For example the critical llm_min_guideline.md file, which tells the LLM how to interpret the compressed version, was lazily copy-pasted from an LLM response without even removing the LLM's commentary:\"You are absolutely right! My apologies. I was focused on refining the detail of each section and overlooked that key change in your pipeline: the Glossary (G) section is no longer part of the final file...\"Doesn't exactly instill confidence.Really nice idea. I hope you keep going with this as it would be a very useful utility.\n \nreply"
    ],
    "link": "https://github.com/marv1nnnnn/llm-min.txt",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Min.js Style Compression of Tech Docs for LLM Context\n      \n\nIf you've ever used an AI coding assistant (like GitHub Copilot, Cursor, or others powered by Large Language Models - LLMs), you've likely encountered situations where they don't know about the latest updates to programming libraries. This knowledge gap exists because AI models have a \"knowledge cutoff\" \u2013 a point beyond which they haven't learned new information. Since software evolves rapidly, this limitation can lead to outdated recommendations and broken code.Several innovative approaches have emerged to address this challenge: llms.txt\nA community-driven initiative where contributors create reference files (llms.txt) containing up-to-date library information specifically formatted for AI consumption. Context7\nA service that dynamically provides contextual information "
  },
  {
    "title": "Dr. Dobb's Journal Interviews Jef Raskin (1986) (computeradsfromthepast.substack.com)",
    "points": 59,
    "submitter": "rbanffy",
    "submit_time": "2025-05-15T18:43:43 1747334623",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=43998008",
    "comments": [
      "I love, love, love flipping through old magazines. Look, an ad for a commercial Emacs! A C compiler for just $495! A port of vi to MS-DOS for $149! A sort command for just $135! A PC card with a 68000 coprocessor for heaven knows why!The good old days were fun for their sense of everything-is-new adventure, but there's an awful lot I don't miss.\n \nreply",
      "I recently have also been thinking about Jef Raskin\u2019s book The Humane Interface. It feels increasingly relevant to now.Raskin was deeply concerned with how humans think in vague, associative, creative ways, while computers demand precision and predictability.His goal was to humanize the machine through thoughtful interface design\u2014minimizing modes, reducing cognitive load, and anticipating user intent.What\u2019s fascinating now is how AI, changes the equation entirely. Instead of rigid systems requiring exact input, we now have tools that themselves are fuzzy, and probabilistic.I keep thinking that the gap Raskin was trying to bridge is closing\u2014not just through interface, but through the architecture of the machine itself.So AI makes Raskin\u2019s vision more feasible than ever but also challenges his assumptions:Does AI finally enable truly humane interfaces?\n \nreply",
      "\"Does AI finally enable truly humane interfaces?\"I think it does; LLMs in particular. AI also enables a ton of other things, many of them inhumane, which can make it very hard to discuss these things as people fixate on the inhumane. (Which is fair... but if you are BUILDING something, I think it's best to fixate on the humane so that you conjure THAT into being.)I think Jef Raskin's goal with a lot of what he proposed was to connect the computer interface more directly with the user's intent. An application-oriented model really focuses so much of the organization around the software company's intent and position, something that follows us fully into (most of) today's interfaces.A magical aspect of LLMs is that they can actually fully vertically integrate with intent. It doesn't mean every LLM interface exposes this or takes advantage of this (quite the contrary!), but it's _possible_, and it simple wasn't possible in the past.For instance: you can create an LLM-powered piece of software that collects (and allows revision) to some overriding intent. Just literally take the user's stated intent and puts it in a slot in all following prompts. This alone will have a substantial effect on the LLMs behavior! And importantly you can ask for their intent, not just their specific goal. Maybe I want to build a shed, and I'm looking up some materials... the underlying goal can inform all kinds of things, like whether I'm looking for used or new materials, aesthetic or functional, etc.To accomplish something with a computer we often thread together many different tools. Each of them is generally defined by their function (photo album, email client, browser-that-contains-other-things, and so on). It's up to the human to figure out how to assemble these, and at each step it's easy to lose track, to become distracted or confused, to lose track of context. And again an LLM can engage with the larger task in a way that wasn't possible before.\n \nreply",
      "Tell me, how does doing any of the things you've suggested help with the huge range of computer-driven tasks that have nothing to do with language? Video editing, audio editing, music composition, architectural and mechanical design, the list is vast and nearly endless.LLMs have no role to play in any of that, because their job is text generation. At best, they could generate excerpts from a half-imagined user manual ...\n \nreply",
      "Everything has to do with language! Language is a way of stating intention, of expression something before it exists, of talking about goals and criteria. Everything example you give can be described in language. You are caught up in the mechanisms of these tools, not the underlying intention.You can describe your intention in any of these tools. And it can be whatever you want... maybe your intention in an audio editor is \"I need to finish this before the deadline in the morning but I have no idea what the client wants\" and that's valid, that's something an LLM can actually work with.HOW the LLM is involved is an open question, something that hasn't been done very well, and may not work well when applied to existing applications. But an LLM can make sense of events and images in addition to natural language text. You can give an LLM a timestamped list of UI events and it can actually infer quite a bit about what the user is actually doing. What does it do with that understanding? We're going to have to figure that out! These are exciting times!\n \nreply",
      "Because some LLMs are now multimodal\u2014they can process and generate not just text, but also sound and visuals. In other words, they\u2019re beginning to handle a broader range of human inputs and outputs, much like we do.\n \nreply",
      "Those are not LLMs. They use the same foundational technology (pick what you like, but I'd say transformers) to accomplish tasks that require entirely different training data and architectures.I was specifically asking about LLMs because the comment I replied to only talked about LLMs - Large Language Models.\n \nreply",
      "At this point in time calling a multimodal LLM an LLM is pretty uncontroversial. Most of the differences lie in the encoders and embedding projections. If anything I'd think MoE models are actually more different from a basic LLM than a multimodal LLM is from a regular LLM.Bottom line is that when folks are talking about LLM applications, multimodal LLMs, MoE LLMs, and even agents are all in the general umbrella.\n \nreply",
      "Multimodal LLMs are absolutely LLMs, the language is just not human language.\n \nreply",
      "What if you could pilot your video editing tool through voice? Have a multimodal LLM convert your instructions into some structured data instruction that gets used by the editor to perform actions.\n \nreply"
    ],
    "link": "https://computeradsfromthepast.substack.com/p/dr-dobbs-journal-interviews-jef-raskin",
    "first_paragraph": ""
  },
  {
    "title": "I don't like NumPy (dynomight.net)",
    "points": 338,
    "submitter": "MinimalAction",
    "submit_time": "2025-05-15T16:05:22 1747325122",
    "num_comments": 153,
    "comments_url": "https://news.ycombinator.com/item?id=43996431",
    "comments": [
      "If your arrays have more than two dimensions, please consider using Xarray [1], which adds dimension naming to NumPy arrays. Broadcasting and alignment then becomes automatic without needing to transpose, add dummy axes, or anything like that. I believe that alone solves most of the complaints in the article.Compared to NumPy, Xarray is a little thin in certain areas like linear algebra, but since it's very easy to drop back to NumPy from Xarray, what I've done in the past is add little helper functions for any specific NumPy stuff I need that isn't already included, so I only need to understand the NumPy version of the API well enough one time to write that helper function and its tests. (To be clear, though, the majority of NumPy ufuncs are supported out of the box.)I'll finish by saying, to contrast with the author, I don't dislike NumPy, but I do find its API and data model to be insufficient for truly multidimensional data. For me three dimensions is the threshold where using Xarray pays off.[1] https://xarray.dev\n \nreply",
      "Xarray is great. It marries the best of Pandas with Numpy.Indexing like `da.sel(x=some_x).isel(t=-1).mean([\"y\", \"z\"])` makes code so easy to write and understand.Broadcasting is never ambiguous because dimension names are respected.It's very good for geospatial data, allowing you to work in multiple CRSs with the same underlying data.We also use it a lot for Bayesian modeling via Arviz [1], since it makes the extra dimensions you get from sampling your posterior easy to handle.Finally, you can wrap many arrays into datasets, with common coordinates shared across the arrays. This allows you to select `ds.isel(t=-1)` across every array that has a time dimension.[1] https://www.arviz.org/en/latest/\n \nreply",
      "Seconded. Xarray has mostly replaced bare NumPy for me and it makes me so much more productive.\n \nreply",
      "Is there anything similar to this for something like Tensorflow, Keras or Pytorch? I haven't used them super recently, but in the past I needed to do all of the things you just described in painful to debug ways.\n \nreply",
      "For Torch, I have come across Named Tensors, which should work in a similar way: https://docs.pytorch.org/docs/stable/named_tensor.htmlThe docs say that it's a prototype feature, and I think it has been that way for a few years now, so no idea how production-ready it is.\n \nreply",
      "It's a much worse API than Xarrays, it seems like somebody should build it on top of PyTorch.\n \nreply",
      "I really like einops. This works for numpy, pytorch and keras/tensorflow and has easy named transpose, repeat, and eimsum operations.\n \nreply",
      "Same - I\u2019ve been using einops and jaxtyping together pretty extensively recently and it helps a lot for reading/writing multidimensional array code. Also array_api_compat, the API coverage isn\u2019t perfect but it\u2019s pretty satisfying to write code that works for both PyTorch and numpy arrayshttps://docs.kidger.site/jaxtyping/https://data-apis.org/array-api-compat/\n \nreply",
      "For pytorch the analogue is Named Tensors, but it's a provisional feature and not supported everywhere.https://docs.pytorch.org/docs/stable/named_tensor.html\n \nreply",
      "Thanks for sharing this  library. I will give it a try.For a while I had a feeling that I was perhaps a little crazy for seeming to be only person to really dislike the use of things like \u2018array[:, :, None]\u2019 and so forth.\n \nreply"
    ],
    "link": "https://dynomight.net/numpy/",
    "first_paragraph": "\n\n    May 2025\n    \nThey say you can\u2019t truly hate someone unless you loved them first. I don\u2019t know if that\u2019s true as a general principle, but it certainly describes my relationship with NumPy.NumPy, by the way, is some software that does computations on arrays in Python. It\u2019s insanely popular and has had a huge influence on all the popular machine learning libraries like PyTorch. These libraries share most of the same issues I discuss below, but I\u2019ll stick to NumPy for concreteness.NumPy makes easy things easy. Say A is a 5\u00d75 matrix, x is a length-5 vector, and you want to find the vector y such that Ay=x. In NumPy, that would be:So elegant! So clear!But say the situation is even a little more complicated. Say A is a stack of 100 5\u00d75 matrices, given as a 100\u00d75\u00d75 array. And say x is a stack of 100 length-5 vectors, given as a 100\u00d75 array. And say you want to solve A\u1d62y\u1d62=x\u1d62 for 1\u2264i\u2264100.If you could use loops, this would be easy:But you can\u2019t use loops. To some degree, this is a limitatio"
  },
  {
    "title": "Show HN: Undetectag, track stolen items with AirTag (undetectag.com)",
    "points": 77,
    "submitter": "pompidoo",
    "submit_time": "2025-05-15T15:46:26 1747323986",
    "num_comments": 79,
    "comments_url": "https://news.ycombinator.com/item?id=43996251",
    "comments": [
      "After I saw third-party \"10 year battery enclosure\" offerings for AirTags, was wondering when other workaround customizations like this might appear.Other impactful variants might be:* senses whether another 'sibling' AirTag is present, if so, stays off. If not, waits X hours & then turns on.* has its own motion sensor; only after X minutes of being stationary, it waits Y hours to turn on briefly* has its own clock & (original-user-known) randomization seed; turns on at pseudorandom intervals the original user can predict* low-power/low-bandwidth receivers so cheap & tiny now: could wait for national or even global unit-specific 'wake' request - perhaps even with parameters for duration/intervals \u2013 before powering-on AirTag portion\n \nreply",
      "So glad you built this. I thought of this about three weeks ago, and wanted to try the concept out. Now I can just buy it.\n \nreply",
      "I think you should put an accelerometer on it, and only enable the tag a minute or two after it's completely stopped moving. That reduces the likelihood of an \"it's following you\" notification, as it's not really following you.\n \nreply",
      "A poor workaround to a problem created by unnecessary restrictions.  Stalking is already illegal, so why are the tags crippled in the first place?  This \"feature\" severely limits the usefulness of tags for tracking stolen items.  Why not just sell some tags that don't alert everyone to their presence?  Police and intelligence agencies have those already, so who are we protecting?\n \nreply",
      "> Stalking is already illegal, so why are the tags crippled in the first place?I assume because it's a network that relies on its reputation among participating nodes to trust it will not be used to track them involuntarily, else they would opt out and collapse the network.\n \nreply",
      "Breaking and entering is already illegal, why do people sell locks?\n \nreply",
      "Locks keep the dyslexic realtor from showing my house to people interested in the neighboring property.\n \nreply",
      "yeah, but they do nothing to stop a locksmith in a fugue state from rekeying all the bolts to match the neighboring property and vice-versa, and then the realtor can walk right in, and you're locked out!so it's better to just leave the door open.\n \nreply",
      "You\u2019re proving the other persons point - the reason why the a thief doesn\u2019t trivially pick locks isn\u2019t because of the lock, it\u2019s because it\u2019s illegal and there\u2019s a consequence.\n \nreply",
      "Like tags, locks are also deliberately encumbered so that locksmiths and law enforcement can easily defeat them.https://www.youtube.com/watch?v=s5jzHw3lXCQ\n \nreply"
    ],
    "link": "https://undetectag.com/",
    "first_paragraph": "Have an account?\nLog in to check out faster.\n                \n              Loading...\n            \u20ac0,00 EURWith this little add-on, your AirTag lets you track stolen items, reducing by 95% the probability to be found and removed by the thief.Our device is guaranteed to work with the current version of the AirTag. Events outside our control, such as Apple updating the firmware in the future to prevent the device from working, will not qualify for a refund. Airtag is a trademark registered by apple and we have nothing to do with apple."
  },
  {
    "title": "Show HN: Real-Time Gaussian Splatting (github.com/axbycc)",
    "points": 122,
    "submitter": "markisus",
    "submit_time": "2025-05-15T13:26:49 1747315609",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=43994827",
    "comments": [
      "Correct me if I'm wrong but looking at the video this just looks like a 3D point cloud using equal-sized \"gaussians\" (soft spheres) for each pixel, that's why it looks still pixelated especially at the edges. Even when it's low resolution the real gaussian splatting artifacts look different with spikes an soft blobs at the lower resolution parts.\nSo this is not really doing the same as a real gaussian splatting of combining different sized view-dependent elliptic gaussians splats to reconstruct the scene and also this doesn't seem to reproduce the radiance field as the real gaussian splatting does.\n \nreply",
      "I had to make a lot of concessions to make this work in real-time. There is no way that I know to replicate the fidelity of \"actual\" Gaussian splatting training process within the 33ms frame budget.However, I have not baked in the size or orientation into the system. Those are \"chosen\" by the neural net based on the input RGBD frames. The view dependent effects are also \"chosen\" by the neural net, but not through an explicit radiance field. If you run the application and zoom in, you will be able to see the splats of different sizes pointing in different directions. The system as limited ability to re-adjust the positions and sizes due to the compute budget leading to the pixelated effect.\n \nreply",
      "OP, this is incredible. I worry that people might see a \"glitchy 3D video\" and might not understand the significance of this.This is getting unreal. They're becoming fast and high fidelity. Once we get better editing capabilities and can shape the Gaussian fields, this will become the prevailing means of creating and distributing media.Turning any source into something 4D volumetric that you can easily mold as clay, relight, reshape. A fully interactable and playable 4D canvas.Imagine if the work being done with diffusion models could read and write from Gaussian fields instead of just pixels. It could look like anything: real life, Ghibli, Pixar, whatever.I can't imagine where this tech will be in five years.\n \nreply",
      "Thanks so much! Even when I was putting together the demo video I was getting a little self-critical about the visual glitches. But I agree the tech will get better over time. I imagine we will be able to have virtual front row seats at any live event, and many other applications we haven't thought of yet.\n \nreply",
      "> I imagine we will be able to have virtual front row seats at any live event, and many other applications we haven't thought of yet.100%. And style-transfer it into steam punk or \nH.R. Giger or cartoons or anime. Or dream up new fantasy worlds instantaneously. Explore them, play them, shape them like Minecraft-becomes-holodeck. With physics and tactile responses.I'm so excited for everything happening in graphics right now.Keep it up! You're at the forefront!\n \nreply",
      "I know enough about 3D rendering to know that Gaussian splatting's one of the Big New Things in high-performance rendering, so I understand that this is a big deal -- but I can't quantify why, or how big a deal it is.Could you or someone else wise in the ways of graphics give me a layperson's rundown of how this works, why it's considered so important, and what the technical challenges are given that an RGB+D(epth?) stream is the input?\n \nreply",
      "Gaussian Splatting allows you to create a photorealistic representation of an environment from just a collection of images. Philosophically, this is a form of geometric scene understanding from raw pixels, which has been a holy grail of computer vision since the beginning.Usually creating a Gaussian splat representation takes a long time and uses an iterative gradient-based optimization procedure. Using RGBD helps me sidestep this optimization, as much of the geometry is already present in the depth channel and so it enables the real-time aspect of my technique.When you say \"big deal\", I imagine you are also asking about business or societal implications. I can't really speak on those, but I'm open to licensing this IP to any companies which know about big business applications :)\n \nreply",
      "Thanks! That makes a lot of sense, I might dig into this after work some more.By \"big deal,\" I meant more for people specializing around computer graphics, computer vision, or even narrower subfields of either of those two -- a big deal from an academic interest perspective.Sure, this might also have implications in society and business, but I'm a nerd, and I appreciate a good nerding out over something cool, niche, and technically impressive.\n \nreply",
      "So, is there some amount of gradient-based optimization going on here? I see RGBD input, transmission, RGBD output. But, other than multi-camera registration, it's difficult to determine what processing took place between input and transmission. What makes this different from RGBD camera visualizations from 10 years ago?\n \nreply",
      "There is no gradient-based optimization. It's (RGBD input, Current Camera Pose) -> Neural Net -> Gaussian Splat output.I'm not aware of other live RGBD visualizations except for direct pointcloud rendering. Compared to pointclouds, splats are better able to render textures, view-dependent effects, and occlusions.\n \nreply"
    ],
    "link": "https://github.com/axbycc/LiveSplat",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Live Gaussian Splatting for RGBD Camera Streams\n      LiveSplat is an algorithm for realtime Gaussian splatting using RGBD camera streams. Join our discord for discussion and help. Check out the demo video below to see the the type of output that LiveSplat produces.LiveSplat was developed as a small part of a larger proprietary VR telerobotics system. I posted a video of the Gaussian splatting component of this system on Reddit and many people expressed an interest in experimenting with it themselves. So I spun it out and I'm making it publicly available as LiveSplat (see installation instructions below).LiveSplat should be considered alpha quality. I do not have the resources to test the installation on many different machines, so let me know if the application does not run on yours (assuming your machine meets the requirements).I'"
  },
  {
    "title": "Stop using REST for state synchronization (2024) (mbid.me)",
    "points": 37,
    "submitter": "Kerrick",
    "submit_time": "2025-05-15T17:30:03 1747330203",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=43997286",
    "comments": [
      "While I superficially agree with the point, the article author states that they haven't tried the alternatives they suggest. Implementing CRDT or OT can be a complex endeavour, especially if you're retrofitting on an existing system that already has a REST or similar API. Use case is ever important, don't spend your all important time implementing a complex state sync if it turns out your customers don't do the kind of collaboration that requires it.\n \nreply",
      "It seems like the author has never worked on an interface that uses realtime UI syncing. The points about cumbersome and mundane issues when working with REST UIs are not wrong, but the challenges of working with realtime synced UIs are substantially more difficult. And I'm not talking about conflict resolution, which can be abstracted by many existing solutions. I'm talking about the intricacies of UI behaviors that are specific to each application, when suddenly every interactive element can potentially be interacted with by multiple parties simultaneously.For most applications, the status quo whereby a little bit of recent state is maintained locally, and periodically pushed to a centralized store, seems to provide the best overall balance of complexity and functionality. One exception is that most applications do a poor job of actually maintaining the local state (e.g. most forms clear when you refresh the page, which is not hard to fix).\n \nreply",
      "State synchronization is a bit of a hot topic for front end development since most of the \u201cmiddle end\u201d deals squarely with solving that problem.I also agree that the way many APIs are built are pushing complexity to the FE: https://bower.sh/dogma-of-restful-apiHowever, virtually every company I\u2019ve joined has needed to serve multiple clients. In those cases REST is the status quo. Some people have opted for graphql but even that makes some uneasy because tuning gql is non trivial. CRDTs are completely foreign to most organizations unless they deal specifically with multiplayer modes.So while it sounds nice to ditch REST it\u2019s just not realistic for most orgs. This is why I\u2019ve been developing a side effect and state sync system that can easily interface with REST, websockets, graphql, etc because it\u2019s built off of structured concurrency.https://starfx.bower.sh/https://bower.sh/why-structured-concurrency\n \nreply",
      "What part of this implementation is ReST?  I see a CRUD API that uses HTTP verbs.  State transfer implies multiple steps at least, where I call the initial API which then returns several other API endpoints \"transfering the state\" typically in a HATEOAS style - perhaps that part isn't required.https://en.wikipedia.org/wiki/REST> ... although this term is more commonly associated with the design of HTTP-based APIs and what are widely considered best practices regarding the \"verbs\" (HTTP methods) a resource responds to, while having little to do with REST as originally formulated ...Ah, I see, the industry has taken to just calling HTTP ReST for no apparent reason.As far as not being sure about CRDTs, these protocols were made to overcome the obvious and terrible shortcomings of CRUD APIs (lack of communicability and idempotency).  Who ever wants to see \"the data on the server has changed, do you want to overwrite the data on the server or overwrite your local copy\".  If you're not doing some sort of event sourcing for state management (ES, OT, CRDT) you're probably doing it wrong or doing it for a simple project.\n \nreply",
      "Not sure why you think that\u2019s not rest.  Because Roy Fielding wrote a paper that most people don\u2019t care about?Language isn\u2019t controlled by a single authority or the first person to use a term. Words mean what people use them to mean. You can complain that it\u2019s not the \u2018proper\u2019 definition, but if the majority of people use it a certain way, that becomes the definition in practice.edit: Fielding not Crawford (thx ackfoobar)\n \nreply",
      "Similarly Java falls outside of Alan Kay's definition of OOP. But unlike Roy Fielding's ReST, I don't see many people bringing that up.\n \nreply",
      "Its more fun at parties to just choose to believe that many of these APIs companies deploy are REST APIs, where the word \"REST\" is just a word, doesn't stand for anything, and is entirely unrelated to whatever that Roy guy was talking about all those years ago.\n \nreply",
      "REST as the author finds out is not for state synchronisation. CRDTs are also not about state synchronisation, only so in an eventual sense, so similar to REST, you can make it work, but you are twisting the approach with assumptions and sticky tape. State synchronisation is a \u201csolved\u201d problem, in the form of distributed data stores, or in some more relaxed sense, database replication, CDC etc. Not an expert, so this is just an opinion even if it sounds assertive.\n \nreply",
      "Curious to her other people's opinion of this. I arrived at a similar conclusion after making web apps for research control software in my PhD. I went through a Yjs tutorial and looked into integrating it with fastapi websockets. But this seems like a pretty unusual thing; there just isn't enough people doing this.Nice user-friendly libraries and tutorials don't exist for smoothing the transition from REST to CRDTs, should your app need that.\n \nreply",
      "If you're hesitating to use YJS take the word of someone who took the plunge, it's totally worth it on the other side.I've written several apps now with it now.  Very easy to use and quite robust to failures.  It's a bit of a mental load to take on at first but it's totally worth it for the problems it solves out of the box.  I've tried other things too from rolling my own ES stack to OT and more.Lately I've got it running on AWS API Gateway V2 over websockets to lambdas + DynamoDB with a small army of daily users.  The only expensive part is the event audit logs I keep due to my inherent mistrust of all computers.\n \nreply"
    ],
    "link": "https://www.mbid.me/posts/stop-using-rest-for-state-synchronization/",
    "first_paragraph": "September 22, 2024tl;dr: Most apps need state\nsynchronization, not state transfer. We should replace\nREST and friends with proper state synchronization protocols where\nappropriate.Apart from getting into van life in Europe and working on the Eqlog Datalog engine,\nI\u2019ve also spent some time of my sabbatical building various webapps. The\ntech stack I used was React + Typescript for the frontend and a REST\nserver implemented with Rust and the Axum library as backend. Rust might\nbe somewhat unusual, but I think otherwise this is a very typical\nsetup.What struck me is how incredibly cumbersome, repetitive and brittle\nthis programming model is, and I think much of this is due to using REST\nas interface between the client and the server. REST is a state\ntransfer protocol, but we usually want to synchronize\na piece of state between the client and the server. This mismatch means\nthat we usually implement ad-hoc state synchronization on top of REST,\nand it turns out that this is not entirely triv"
  }
]