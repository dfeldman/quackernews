[
  {
    "title": "Everyone knows your location: tracking myself down through in-app ads (timsh.org)",
    "points": 879,
    "submitter": "apokryptein",
    "submit_time": "2025-02-02T17:07:31 1738516051",
    "num_comments": 285,
    "comments_url": "https://news.ycombinator.com/item?id=42909921",
    "comments": [
      "One big privacy issue is that there is no sane way to protect your contact details from being sold, regardless of what you do.As soon as your cousin clicks \"Yes, I would like to share the entire contents of my contacts with you\" when they launch TikTok your name, phone number, email etc are all in the crowd.And I buy this stuff. Every time I need customer service and I'm getting stonewalled I just go onto a marketplace, find an exec and buy their details for pennies and call them up on their cellphone. (this is usually successful, but can backfire badly -- CashApp terminated my account for this shenanigans)\n \nreply",
      "<< find an exec and buy their details for pennies and call them up on their cellphone. (this is usually successful, but can backfire badly -- CashApp terminated my account for this shenanigans)Honestly, kudos. The rules should apply to the ones foisting this system upon us as well. This is probably the only way to make anyone in power reconsider current setup.<< As soon as your cousin clicks \"Yes, I would like to share the entire contents of my contacts with you\" when they launch TikTok your name, phone number, email etc are all in the crowd.And people laughed at Red Reddington when he said he had no email.\n \nreply",
      "> The rules should apply to the ones foisting this system upon us as well. This is probably the only way to make anyone in power reconsider current setup.Unless your problem is with the company doing the privacy violations, this doesn\u2019t make any sense.\n \nreply",
      "There was a post from someone a long time ago who has an email address and name similar to Make Cuban but not quite. He got quite a few cold call emails meant for Cuban. A lot of them were quite sad (people asking for money for medical procedures and such).\n \nreply",
      "Exactly this was tried by the likes of James Oliver and journalists/comedians of that caliber running ads and gathering data from politicians in Washington.It was some years ago and resulted in nothing\n \nreply",
      "Do you mean John Oliver?\n \nreply",
      ">One big privacy issue is that there is no sane way to protect your contact details from being sold, regardless of what you do.>As soon as your cousin clicks \"Yes, I would like to share the entire contents of my contacts with you\" when they launch TikTok your name, phone number, email etc are all in the crowd.Fortunately this is changing with iOS 18 with \"limited contacts\" sharing.https://mobiledevmemo.com/wp-content/uploads/2024/09/image.p...The interface also seems specifically designed to push people to allow only a subset of contacts, rather than blindly clicking \"allow all\".The far bigger issue is the contact info you share with online retailers. Scraping contact info through apps is very visible, drawing flak from the media and consumers. Most of the time all you get is a name (could be a nickname), and maybe some combination of phone/email/address, depending on how diligent the person in filling out all the fields. On the other hand placing any sort of order online requires you to provide your full name, address, phone number, and email address. You can also be reasonably certain that they're all accurate, because they're plausibly required for delivery/billing purposes. Such data can also be surreptitiously fed to data brokers behind the scenes, without an obvious \"tiktok would like access to your contacts\" modal.\n \nreply",
      "People will share their whole list because it\u2019s simpler\n \nreply",
      "Or because they were tricked. eg. LinkedIn\u2019s \u201cConnect with your contacts\u201d onboarding step which sounds like it\u2019ll check your contacts against existing LinkedIn users but actually spam invites anyone on your contact list that doesn\u2019t have an account.\n \nreply",
      ">Fortunately this is changing with iOS 18 with \"limited contacts\" sharing.Its not. Apple still owns your stuff. There is no difference between Apple and other 3p retailers. Apple just wants more of your money.\n \nreply"
    ],
    "link": "https://timsh.org/tracking-myself-down-through-in-app-ads/",
    "first_paragraph": ""
  },
  {
    "title": "Waydroid \u2013 Android in a Linux container (waydro.id)",
    "points": 198,
    "submitter": "birdculture",
    "submit_time": "2025-02-02T19:29:45 1738524585",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=42911042",
    "comments": [
      "What I\u2019d love to see is a containerized android that can be fired up on a Mac (using docker desktop or orbstack or whatever) that I can modify the docker image of to have rooted man in the middle proxy already setup, making it much easier to drop an Android app onto to observe the network traffic and api calls.\n \nreply",
      "If you want to do this now with little setup, run waydroid and then run wireshark inside the network namespace that is created for waydroidsudo ip netns exec <netns> wireshark\n \nreply",
      "Wireshark is nice, but for HTTPS MitM you'll need a tool like mitmproxy/Burp to do the proxying and either modifications to the system image or a Frida daemon running as root to make most apps trust the MitM'd certificates.To get the traffic routed right, the Wireguard option for mitmproxy is pretty useful in my experience. Not sure how well Waydroid + Android VPNs work together, though.\n \nreply",
      "Surprised to see this on the frontpage - it's a well known piece of software.It's unfortunate that there are no Google-vended images (e.g. the generic system image) that run on Waydroid.  Typing my password into random ROMs from the internet sketches me out.https://source.android.com/docs/core/tests/vts/gsi\n \nreply",
      "I wouldn't say it runs a \"random ROM from the internet\" - LineageOS is a very well-established project and is fully FOSS (free and open source software) except for firmware necessary for specific devices. It is the natural choice for any project, such as Waydroid, that requires good community support and ongoing availability.Over a number of years, Google have progressively removed many of the original parts of AOSP (the FOSS foundation upon which Android is based), which means that alternative components have to be developed by projects like LineageOS. In spite of this, I suspect that LineageOS makes fewer modifications to AOSP than most phone vendors do, including Google themselves!\n \nreply",
      "Would you hire a well known electrician who was not bonded and insured? Sometimes it's nice to know there is more than blind trust.\n \nreply",
      "It's FOSS, you're more than welcome to inspect the code yourself\n \nreply",
      "Would you hire Electroboom?\n \nreply",
      "Just had a conversation about this on a waydroid github issue. The LineageOS X86 image is outdated compared to  also open source Bliss OS' Android 12./? Android play store APK GitHub actionsIt looks like Android Emulator has the most current version of Android that will run on x86?\n \nreply",
      "Google provides one that runs in Docker\n \nreply"
    ],
    "link": "https://waydro.id/",
    "first_paragraph": " A container-based approach to boot a full Android system on regular GNU/Linux systems running Wayland based desktop environments. Using Waydroid's Multi-Window ModeBrought To Life With WaydroidWith Waydroids Fullscreen Mode For Desktops & Kiosks Waydroid uses Linux namespaces (user, pid, uts, net, mount, ipc) to run a full Android system in a container and provide Android applications on any GNU/Linux-based platform (arm, arm64, x86, x86_64). The Android system inside the container has direct access to needed hardware through LXC and the binder interface.  The Project is completely free and open-source, currently our repo is hosted on Github.  Waydroid integrated with Linux adding the Android apps to your linux applications folder.  Waydroid expands on Android freeform window definition, adding a number of features.  For gaming and full screen entertainment, Waydroid can also be run to show the full Android UI.  Get the best performance possible using wayland and AOSP mesa, taking thi"
  },
  {
    "title": "Costa rican supermarket wins trademark battle against Nintendo (ticotimes.net)",
    "points": 92,
    "submitter": "type0",
    "submit_time": "2025-02-02T21:07:48 1738530468",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=42911842",
    "comments": [
      "So, Nintendo of America, whose president is Doug Bowser, and who previously sued a Gary Bowser, had gone to court with a Jos\u00e9 Mario Alfaro Gonz\u00e1lez, who owns the supermarket chain Super Mario.I'm pretty sure, at this point, they're just joshing us all.\n \nreply",
      "Knowing quite a bit about the world of Costa Rican grocery stores -- many of which started using \"Hiper-\" (spanish for hyper-) as a prefix in their names a few decades ago, to one-up markets merely named \"Super-\" [1] -- I'm actually quite suprised they didn't just rename themselves \"Hiper Mario\" and save the legal fees.  But bravo to them for winning against all odds.[1] https://ticotimes.net/2004/04/02/hipermas-supermarket-aims-f...\n \nreply",
      "He also could have named it \"Super Alfaro\" or \"Super Jose\".\n \nreply",
      "\"Hiper\" refers to something bigger than \"Super\", and it's common to respect that convention. Wikipedia defines both [1][2]. From this photo[3] the size of \"Super Mario\" follows that convention. It is obviously true that there are more options for names but congratulations to \"Don Mario\".[1] https://en.wikipedia.org/wiki/Hypermarket[2] https://en.wikipedia.org/wiki/Supermarket[3] https://www.techspot.com/news/106591-super-mario-supermarket...\n \nreply",
      "i think hypermarkets achieve their bigness not by being bigger supermarkets, but by selling everything a supermarket sells, and then also selling everything a department store sells.\n \nreply",
      "Yes.A super is a like Kroger's or Walmart Express.A hyper is a Walmart Supercenter.\n \nreply",
      "Nintendo continues to be petty assholes to ordinary folks around the world.I don\u2019t know many other companies with such a reputation of petty litigation with dubious claims.\n \nreply",
      "The Mouse.\n \nreply",
      "Touch\u00e9\n \nreply",
      "There is virtually no downsides to being petty assholes. People will even come out of the woodwork to justify your behavior and explain you have no choice than being an asshole.Apple, Disney, Coca Cola, Nike back in the days. Any company with enough money in both marketing and legal department will usually be utter assholes regarding their trademark.\n \nreply"
    ],
    "link": "https://ticotimes.net/2025/01/30/david-vs-goliath-costa-rican-super-mario-defeats-nintendo-in-court",
    "first_paragraph": "A small Costa Rican supermarket has emerged victorious from a legal battle against the renowned video game giant, Nintendo. Jos\u00e9 Mario Alfaro Gonz\u00e1lez, owner of \u201cSuper Mario\u201d in San Ram\u00f3n de Alajuela, found himself in an unexpected legal showdown when he attempted to formally register his store\u2019s name. In Costa Rica, the term \u201csuper\u201d is often used as shorthand for \u201csupermarket.\u201dWhat began as a routine trademark registration process turned into a clash of the titans when Nintendo of America filed an appeal, claiming exclusive rights to the \u201cSuper Mario\u201d name. The video game behemoth, creator of iconic titles like Super Mario Bros., asserted that the name was registered under its trademark in various classes, including clothing, games, and accessories.However, Alfaro intended to register \u201cSuper Mario\u201d specifically for international class 35, which covers \u201csupply services for third parties of products from the basic food basket.\u201d \u201cWe knew these big companies have vast resources to fight, "
  },
  {
    "title": "F-strings for C++26 proposal [pdf] (open-std.org)",
    "points": 43,
    "submitter": "HeliumHydride",
    "submit_time": "2025-02-02T22:19:10 1738534750",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=42912438",
    "comments": [
      "So the f-string literal produces a basic_formatted_string, which is basically a reified argument list for std::format, instead of a basic_string. This allows eg. println to be overloaded to operate on basic_formatted_string without allocating an intermediate string  std::println(\"Center is: {}\", getCenter());\n  std::println(f\"Center is: {getCenter()}\");  // same thing, no basic_string allocated\n\nIn exchange we have the following problems  // f-strings have unexpected type when using auto or type deduction.\n  // basic_string is expected here, but we get basic_formatted_string.\n  // This is especially bad because basic_formatted_string can contain\n  // dangling references.\n  auto s = f\"Center is: {getCenter()}\";\n\n  // f-strings won't work in places where providing a string currently\n  // works by using implicit conversion. For example, filesystem methods\n  // take paths. Providing a string is okay, since it will be implicitly\n  // converted to a path, but an f-string would require two implicit\n  // conversions, first to a string, then to path.\n  std::filesystem::exists(f\"file{n}.dat\");  // error, no matching overload\n\nThere are two other proposals to fix these problems.\n \nreply",
      "Yes. The basic idea is that there's a specifier that allows a formatted string to transparently decay into an ordinary string (\u00e0 la array-to-pointer decay) so that \"auto\" doesn't produce dangling references, and so that chains of more than one implicit conversion can take place.\n \nreply",
      "Tangent: this sort of thing can be implemented without any change to libc++ (the runtime).  Updates to compiler versions are sometimes postponed by users with big codebases that treat a libc++ change as something major.Why don't we see gcc or clang or msvc back porting stuff like this to an older version with a sort of future tag. It's normal to see __future__ in the python ecosystem, for instance.\n \nreply",
      "If a codebase is fragile enough that libc++ changes have to be assumed breaking until proven otherwise, why take the risk? Presumably the application already has a \"standard\" way of formatting strings. If it ain't broke yada yada\n \nreply",
      "It's not about assumed breaking, it's that when you upgrade libc++ you can become incompatible at runtime with your distro or any other number of libraries outside your control in ways that are difficult to detect\n \nreply",
      "Somehow I manage to get by just fine with c++11. I have refactored more than a few codebases that use 17 or greater.Strangely, the codebase became more maintainable afterwards.\n \nreply",
      "I'm pretty sure boost::format can do this, though not inline in the string. Do we really need more complexity in cpp? isn't it complex enough?\n \nreply",
      "This is the sort of change that adds complexity to the language but reduces complexity in the code written in the language. We take those\n \nreply",
      "how would this work with internationalized strings? especially if you have to change the order of things? You'd still need a string version with object ordering I would think\n \nreply",
      "I'm skeptical that people would want to do this in a single expression.\n \nreply"
    ],
    "link": "https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p3412r0.pdf",
    "first_paragraph": ""
  },
  {
    "title": "GarminDB (github.com/tcgoetz)",
    "points": 59,
    "submitter": "haltcatchfire",
    "submit_time": "2025-02-02T22:27:54 1738535274",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=42912515",
    "comments": [
      "it always surprises me how much data is locked up in garmin's (ancient) sdk.for example, the FIT file format, used exclusively for programming workouts on all bike computers, are only supported in one or two apps!I suppose that's our fault and we should build on the sdk\n \nreply",
      "I was hoping this would be a way to import and track data directly from the device, but it appears it talks to the Connect API, so I'm still tied to the official app.\n \nreply",
      "Activities are just FIT files that can be accessed over USB.\n \nreply",
      "You are right, but I've worn out the USB ports on multiple Garmins from using the ports to recharge and download the files. Now I just use the app and download the files later from the Garmin website, and use a magnetic charger plug also to eliminate the wear and tear on the port. I wish I could wirelessly access the files and cut out the Garmin app/download steps.\n \nreply",
      "Whats your magnetic charger of choice? I am tired of wearing through my charging ports.\n \nreply",
      "I gamble and buy the cheapest ones on EBay in lots of three (some have adapters included plugs for USB-C/micro USB, though it seems like as time passed by, more and more are separating the plug purchase from the cable purchase). The recently announced tariffs will put a damper to this since the last few I have bought were dropshipped from China."
    ],
    "link": "https://github.com/tcgoetz/GarminDB",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Download and parse data from Garmin Connect or a Garmin watch, FitBit CSV, and MS Health CSV files into and analyze data in Sqlite serverless databases with Jupyter notebooks.\n      Python scripts for parsing health data into and manipulating data in a SQLite database. SQLite is a light weight database that doesn't require a server.What they can do:Once you have your data in the DB, I recommend using a supplied Jupyter notebooks, third party Jupyter notebooks, and/or SQLite browser like SQLite Studio, HeidiSQL, or DB Browser for SQLite for browsing and working with the data. The scripts create some default views in the DBs that make browsing the data easier.GarminDb releases are hosted on PyPI. GarminDb requires Python 3.x. With Python installed, install the latest release with pip by running pip install garmindb in a terminal.Updat"
  },
  {
    "title": "Emergence of a second law of thermodynamics in isolated quantum systems (aps.org)",
    "points": 46,
    "submitter": "westurner",
    "submit_time": "2025-02-02T22:15:30 1738534530",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=42912412",
    "comments": [
      "This paper is basically statistical mechanics with a quantum veneer. Two major issues:1. Scale: They're simulating just 13 qubits with QuTiP and making grand claims about quantum thermodynamics. The computational complexity they're glossing over here is astronomical. Anyone who's actually worked with quantum systems knows you can't just handwave away the scaling problems.2. Measurement Problem: Their whole argument about instantaneous vs time-averaged measurements is just repackaging the quantum measurement problem without actually solving anything. They're doing the same philosophical shell game that every \"breakthrough\" quantum paper does by moving around where they put the observer and pretending they've discovered something profound.\n \nreply",
      "> This implies that for macroscopic systems, the expected time one would be required to wait to observe such a decrease in entropy occurring is unobservably large.Yea but we have virtual particles and the Casimir effect.  Am I wrong or isn't this these perturbations evidencing themselves on a macroscopic scale?\n \nreply",
      "ScholarlyArticle: \"Emergence of a Second Law of Thermodynamics in Isolated Quantum Systems\" (2025) https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuan...NewsArticle: \"Even Quantum Physics Obeys the Law of Entropy\" https://www.tuwien.at/en/tu-wien/news/news-articles/news/auc...NewsArticle: \"Sacred laws of entropy also work in the quantum world, suggests study\" ... \"90-year-old assumption about quantum entropy challenged in new study\"\nhttps://interestingengineering.com/science/entropy-also-work...\n \nreply",
      "\"The second law of thermodynamics states that the entropy of an isolated system can only increase over time. \"Isn't there a difference between \"can only increase\" and \"cannot decrease\"?\n \nreply",
      "Over long enough time, fluctuations to lower entropy states will happen, so the jaw is statistical.\n \nreply",
      "The trusty laws of thermodynamics strike again\n \nreply"
    ],
    "link": "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.6.010309",
    "first_paragraph": ""
  },
  {
    "title": "Deep Research (openai.com)",
    "points": 110,
    "submitter": "mfiguiere",
    "submit_time": "2025-02-03T00:06:10 1738541170",
    "num_comments": 97,
    "comments_url": "https://news.ycombinator.com/item?id=42913251",
    "comments": [
      "Not sure if people picked up on it, but this is being powered by the unreleased o3 model. Which might explain why it leaps ahead in benchmarks considerably and aligns with the claims o3 is too expensive to release publicly. Seems to be quite an impressive model and the leading out of Google, DeepSeek and Perplexity.\n \nreply",
      "It was expensive as they wanted to charge more for it but deepseek has forced their hand\n \nreply",
      "Rightfully so, some models are getting super efficient.\n \nreply",
      "Has anyone here tried it out yet?\n \nreply",
      "Interesting, thanks for highlighting! Did not pick up on that. Re:\"leading\", tho:Effectiveness in this task environment is well beyond the specific model involved, no? Plus they'd be fools (IMHO) to only use one size of model for each step in a research task -- sure, o3 might be an advantage when synthesizing a final answer or choosing between conflicting sources, but there are many, many steps required to get to that point.\n \nreply",
      "Surprised more comments aren't mentioning deepseek has exactly this feature (for free) already. Assuming this is why OpenAI scrambled to release it.The examples they have on the page work well on chat.deepseek.com with r1 and search options both enabled.Do I blindly trust the accuracy of either though? Absolutely not. I'm pretty concerned about these models falling into gaming SEO and finding inaccurate facts and presenting them as fact. (How easy is it to fool / prompt inject these models?)But has some utility if held right.\n \nreply",
      "If I understood the graphs correctly, it only achieves 20% pass rate on their internal tests. So I have to wait 30min and pay a lot of money just to sift through walls of most likely incorrect text?\nUnless the possibility of hallucinations is negligible, this is just way too much content to review at once. The process probably needs to be a lot more iterative.\n \nreply",
      "26.6% on humanity's last exam is actually impressive.pass rate really only matters in context of the difficulty of the tasks\n \nreply",
      "Only if you are asking questions at the level of a cutting edge benchmark\n \nreply",
      "This is one of the actual questions:> In Greek mythology, who was Jason's maternal great-grandfather?https://www.google.com/search?q=In+Greek+mythology%2C+who+wa...\n \nreply"
    ],
    "link": "https://openai.com/index/introducing-deep-research/",
    "first_paragraph": ""
  },
  {
    "title": "The Legacy of Lies in Alzheimer's Science (nytimes.com)",
    "points": 91,
    "submitter": "apsec112",
    "submit_time": "2025-02-02T19:00:10 1738522810",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=42910829",
    "comments": [
      "Sadly this stems from a structural problem in biology and medicine, and is far from exclusive to the field of Alzheimer's. Some reforms are urgent, otherwise progress is going to be incredibly slow. The same pattern is repeated again and again. Someone publishes something that looks novel, but is either an exaggeration or a downright lie.This person begins to attract funding, grant reviews and article reviews. Funding is used to expand beyond reasonable size and co-author as many articles as possible. Reviews mean this person now has the power to block funding and/or publication of competing hypotheses. The wheel spins faster and faster. And then, we know what the outcome is.The solution is to make sure reviews are truly independent plus limitations on funding, group size and competing interests. I think that tenured researchers that receive public funding should have no stock options nor receive \"consulting\" fees from private companies. If they want to collaborate with industry, that's absolutely fine, but it should be done pro bono.Furthermore, if you are a professor who publishes 2 articles per week and is simultaneously \"supervising\" 15 postdocs and 20 PhD students at 2 different institutions then, except in very few cases, you are no longer a professor but a rent seeker that has no clue what is going on. You just have a very well oiled machine to stamp your name into as many articles as possible.\n \nreply",
      "I'm not a fan of many of the practices you complain about here, but I will say this: We get paid too little for what we do for way too long....6 years of grad school (24k/yr) 6 year post-doc (42k/yr) in California, when I was in those positions anyway. Today, at UC Davis, assistant professors in the UC system start at $90,700 [1, for salary scale], which is often around 12 years after their undergraduate degree. That's in California, where a mortgage costs you $3,000 a month, minimum.[1] https://aadocs.ucdavis.edu/policies/step-plus/salary-scales/...\n \nreply",
      ">Sadly this stems from a structural problem in biology and medicinebut happily it illustrates the structural problems with oversimplified coercions like, \"believe the science!\" which  NYTimes-land has recently been urging, so that is to the good.\n \nreply",
      "Instead of amyloid and tau, we now have a bunch of promising new leads:- insulin and liver dysregulation impacting the brain downstream via metabolic dysfunction- herpesviruses crossing the blood brain barrier, eg after light head injury or traveling the nervous system- gut microbiota imbalance causing immune, metabolic, or other dysregulation- etc.These same ideas are also plausible for MS, ADHD, etc.\n \nreply",
      "curious if you could link to relevant papers? Thanks!\n \nreply",
      "There\u2019s a good discussion in the previous article discussed on HN, including links to various papers.1.  https://news.ycombinator.com/item?id=428936272. https://pmc.ncbi.nlm.nih.gov/articles/PMC8234998/\n \nreply",
      "The article says:Yet despite decades of research, no treatment has been created that arrests Alzheimer\u2019s cognitive deterioration, let alone reverses it.Nowhere in the article does it mention that anti-amyloid therapies such as donanemab and lecanemab have so far successfully slowed decline by about 30%. They may not yet be \"arresting\" (fully stopping) the disease, but it's pretty misleading for the article to completely omit reference to this huge success.We are currently in the midst of a misguided popular uprising against the amyloid hypothesis. There were several fraudulent studies on amyloid, and those responsible should be handled severely by the scientific community. But these fraudulent studies do not constitute the foundational evidence for the amyloid hypothesis, which remains very solid.\n \nreply",
      "From what I've read, those drugs are very good at removing amyloid, but despite that, they don't seem to make much of a noticeable (clinically meaningful) difference in the people treated with them. I personally would not call that a \"huge success\".If they are so good at cleaning up the amyloid, why don't people have more of an improvement? I think everyone agrees amyloid is associated with Alzheimer's, the question is how much of a causative role does it play.\n \nreply",
      "From what I've read, those drugs are very good at removing amyloid, but despite that, they don't seem to make much of a noticeable (clinically meaningful) difference in the people treated with them. I personally would not call that a \"huge success\".After many decades of research, we've gone in the last few years from no ability whatsoever to affect the underlying disease, to 30% slowdown. To be clear, that's a 30% slowdown in clinical, cognitive endpoints. Whether you call that \"meaningful\" is a bit subjective (I think most patients would consider another couple years of coherent thinking to be meaningful), and it has to be weighed against the costs and risks, and there's certainly much work to be done. But it's a huge start.If they are so good at cleaning up the amyloid, why don't people have more of an improvement?No one is expected to improve after neurodegeneration has occurred. The best we hope for is to prevent further damage. Amyloid is an initiating causal agent in the disease process, but the disease process includes other pathologies besides amyloid. So far, the amyloid therapies which very successfully engage their target have not yet been tested in the preclinical phase before the amyloid pathology initiates further, downstream disease processes. This is the most likely reason we've seen only ~30% clinical efficacy so far. I expect much more efficacy in the years to come as amyloid therapies are refined and tested at earlier phases. (I also think other targets are promising therapeutic targets; this isn't an argument against testing them.)I think everyone agrees amyloid is associated with Alzheimer's, the question is how much of a causative role does it play.To be clear, the evidence for the amyloid hypothesis is causal. The association between amyloid and Alzheimer's has been known since Alois Alzheimer discovered the disease in 1906. The causal evidence came in the 1990's, which is why the scientific community waited so long to adopt that hypothesis.\n \nreply",
      ">If they are so good at cleaning up the amyloid, why don't people have more of an improvement?I have zero knowledge in this field, but there's a very plausible explanation that I think is best demonstrated by analogy:If you shoot a bunch of bullets into a computer, and then remove the bullets, will the computer be good as new?\n \nreply"
    ],
    "link": "https://www.nytimes.com/2025/01/24/opinion/alzheimers-fraud-cure.html",
    "first_paragraph": ""
  },
  {
    "title": "An open-source, extensible AI agent that goes beyond code suggestions (block.github.io)",
    "points": 50,
    "submitter": "sansui12",
    "submit_time": "2025-01-30T16:27:15 1738254435",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=42879323",
    "comments": [
      "So I gave goose a whirl and I actually really like the approach they are taking, especially because I use emacs and not vscode. I would recommend people try it out on an existing project\u2014the results are quite good for small, additive features and even ones that are full stack.Here's a short writeup of my notes from trying to use it https://notes.alexkehayias.com/goose-coding-ai-agent/\n \nreply",
      "It advertises that it runs locally and that it is \"extensible\" but then requires you to set up a remote/external provider as the first step of installation? That's a rather weird use of \"local\" and \"extensible\". Do words mean anything anymore?\n \nreply",
      "Can\u2019t you just run ollama and provide it a localhost endpoint? I dont think its within scope to reproduce the whole local LLM stack when anyone wanting to do this today can easily use existing better tools to solve that part of it.\n \nreply",
      "Did you not see Ollama?\n \nreply",
      "Yeah, they seem to be referring to the Goose agent/CLI that are local. Not models themselves.\n \nreply",
      "Today I decided that what I need is:- prompt from command line directly to Claude- suggestions dumped into a file under ./tmp/ (ignored by git)- iterate on those files- shuttle test results over to ClaudeGetting those files merged with the source files is also important, but I\u2019m not confident in a better way than copy-pasting at this point.\n \nreply",
      "Aider is fantastic. Worth a look.\n \nreply",
      "I\u2019ve been playing with it and I don\u2019t like it that much? I\u2019m not sure why. It feels a little buggy and like it\u2019s doing too much.\n \nreply",
      "this is the page i\u2019d link tohttps://block.github.io/goose/docs/goose-architecture/\n \nreply",
      "I don't know how useful this is, but my immediate reaction to the animation on the front page was \"that's literally worse then the alternative\".Because the example given was \"change the color of a component\".Now, it's obviously fairly impressive that a machine can go from plain text to identifying a react component and editing it...but the process to do so literally doesn't save me any time.\"Can you change the current colour of headercomponent.tsx to <some color> and increase the size vertical to 15% of vh\" is a longer to type sentence then the time it would take to just open the file and do that.Moreover, the example is in a very \"standard\" format. What happens if I'm not using styled components? What happens if that color is set from a function? In fact none of the examples shown seem gamechanging in anyway (i.e. the Confluence example is also what a basic script could do, or a workflow, or anything else - and is still essentially \"two mouseclicks\" rather then writing out a longer English sentence and then I would guess, waiting substantially more time for inferrence to run.\n \nreply"
    ],
    "link": "https://block.github.io/goose/",
    "first_paragraph": "Your on-machine AI agent, automating engineering tasks seamlessly.Built with transparency and collaboration in mind, goose empowers developers to contribute, customize, and innovate freely.Goose runs locally to execute tasks efficiently, keeping control in your hands.Customize goose with your preferred LLM and enhance its capabilities by connecting it to any external MCP server or API.Goose independently handles complex tasks, from debugging to deployment, freeing you to focus on what matters most.With Goose, I feel like I am Maverick. Thanks a ton for creating this. \ud83d\ude4f I have been having way too much fun with it today.I wanted to construct some fake data for an API with a large request body and business rules I haven't memorized. So I told Goose which object to update and a test to run that calls the vendor. Got it to use the errors descriptions from the vendor response to keep correcting the request until it was successful. So good!I asked Goose to write up a few Google Scripts that m"
  },
  {
    "title": "Federal government grant award search (datarepublican.com)",
    "points": 11,
    "submitter": "paganel",
    "submit_time": "2025-02-03T00:28:19 1738542499",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://datarepublican.com/award_search/",
    "first_paragraph": "\n  Below is a keyword search over a selection of active federal government grants.\n  Each typed word is treated as an AND condition (i.e., all must match).\n  This search runs in two passes per keyword:\n  (1) exact keyword match,\n  (2) prefix matches (i.e., other keywords that start with that string).\n  For performance reasons, only the first 100 matching rows are displayed.  \n\n  You can search by EIN (eg: 13-2574854), UEI (eg: M2NYTB5V2D77) or keywords (eg: unaccompanied refugee)\n"
  },
  {
    "title": "Global Variables Are Not the Problem (codestyleandtaste.com)",
    "points": 20,
    "submitter": "levodelellis",
    "submit_time": "2025-01-31T20:04:30 1738353870",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=42891284",
    "comments": [
      "The bug in the program reveals a poor understanding of object lifecycles by whoever wrote it. The `obj` argument to `simple` is not globally unique and so it makes a poor location to store global state information (a count of how often `simple` is called, in this example).Never tie global state information to ephemeral objects whose lifetime may be smaller than what you want to track. In this case, they want to know how many times `simple` is called across the program's lifetime. Unless you can guarantee the `obj` argument or its `counter` member exists from before the first call to `simple` and through the last call to `simple` and is the only `obj` to ever be passed to `simple`, it is the wrong place to put the count information. And with those guarantees, you may as well remove `obj` as a parameter to both `simple` and `complex` and just treat it as a global.State information needs to exist in objects or locations that last as long as that state information is relevant, no more, no less. If the information is about the overall program lifecycle, then a global can make sense. If you only need to know how many times `simple` was invoked with a particular `obj` instance, then tie it to the object passed in as the `obj` argument.\n \nreply",
      "Could you tell me where this was posted? I thought no one would see this after I got no comments the first dayNo one I showed this to complained about the first example but online many people did. I wrote a completely different article which I think is much better that uses examples I would have used in the follow up. I'll post that article next week\n \nreply",
      "I find the concept of a context structure passed as the first parameter to all your functions with all your \"globals\" to be very compelling for this sort of stuff.\n \nreply",
      "That's exactly why I used this specific example. I seen many code bases that use clone to avoid mutation problems so I wrote this specifically to show it can become a problem too.I wrote a better article on globals. I plan on posting it next week\n \nreply",
      "That just seems like globals with extra steps. Suddenly if your context structure has a weird value in it, you\u2019ll have to check every function to see who messed it up.\n \nreply",
      "That's 2 parts:\n1. Global variable (mutable)\n2. Local function with context argument (mutations)You have clear tracking of when and how functions change the global variable\n \nreply",
      "The \"god object\"\n \nreply",
      "The \"environment\".\n \nreply",
      "Hard disagree.If I have 500 functions, I don't want to extrapolate out the overhead of passing a state object around to all of them. That's a waste of effort, and frankly makes me think you want to code using an FP paradigm even in imperative languages.Module-level and thread-level \"globals\" are fine. You gain nothing (other than some smug ivory tower sense of superiority) by making your functions pure and passing around a global state object to every single method invocation.\n \nreply",
      "You get functions that are easily testable in isolation with all state provided in parameters.You also get explicit dependencies and scoping controlled by caller.I don't mind globals but saying you get nothing for avoiding then is :/\n \nreply"
    ],
    "link": "https://codestyleandtaste.com/globals-are-not-the-problem.html",
    "first_paragraph": "In this article I'll show an example where avoiding a global variable has led to a bug, I'll define what global variables are, explain the problem, and then give examples where I have used them successfully.We're all taught that global variables are bad. They can be modified from anywhere, sometimes force functions to be called in a specific order, and can be impossible to debug if the program is large enough or the state is random enough. We're usually taught not to use them within the first year of programming, but many of us never figure out when we should.First, we'll look at code without globals. Here we want to see how many times the 'simple' function is entered before it throws an exception (not shown) so we can set a breakpoint at the top of the function. There's a bug in our counter logic. If the problem isn't obvious you may find this frustrating.\nIf you run the code you'll see 1 2 3 4 3 printed instead of 5. Before I tell you the problem let's look at the version that uses a"
  },
  {
    "title": "Hytradboi 2025 Program (hytradboi.com)",
    "points": 34,
    "submitter": "jamii",
    "submit_time": "2025-02-01T03:07:35 1738379255",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=42895265",
    "comments": [
      "I thought this had something to do with \"Trad Boys\" (tradbois?) like \"Trad Wife\" ;)\n \nreply",
      "Looks like a great lineup, can't wait!\n \nreply",
      "I can't take that logo seriously with that unholy union of a hard drive and a floppy\n \nreply"
    ],
    "link": "https://www.hytradboi.com/2025/",
    "first_paragraph": "HYTRADBOI is a fun online conference about databases, programming languages, and everything in between. A place for ideas to cross-fertilize between the two disciplines.The conference is hosted in a chat room and all the talks are recorded and captioned in advance. Join from a different time-zone, watch talks on your lunch break, answer questions in your pajamas, pause the talk to go feed your baby. Conferences work better async.HYTRADBOI takes place 2025 Feb 28 0900-1500 PDT. The chat will stay open for weeks afterwards for those who can't attend live.Get updates via  or follow on mastodon, bluesky, or twitter.\"HYTRADBOI is so addictive. I'm just clicking on every talk one-by-one and can't stop. Lots of interesting material and perfect talk length.\" -- Nikita Prokopov\"HYTRADBOI was the best remote conference I've attended and should be used as an example for how we should reconsider the medium of conferences\" -- Alexander Bandukwala\"The embodiment of \"the future is here but not evenly"
  },
  {
    "title": "ScatterBrain: Unmasking the shadow of PoisonPlug's obfuscator (cloud.google.com)",
    "points": 60,
    "submitter": "tux3",
    "submit_time": "2025-02-02T19:46:12 1738525572",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=42911162",
    "comments": [
      "The source for the de-obfuscator: https://github.com/mandiant/poisonplug-scatterbrain\n \nreply",
      "This is the result when an elite attacker meets an elite analyst group.That's some very heavy stuff.\n \nreply",
      "This is very cool. Can someone help me understand the behind the scenes, what\u2019s their strategy? Their motivations? Are they targeting specific industries or nations for a reason?\n \nreply",
      "Yes, this in an interesting question. Are they just trying to hide from anti-virus signatures, or are they hiding code they perceive as valuable?\n \nreply",
      "Is it correct to presume that the obfuscated samples might be hard to come by for the average interested viewer?\n \nreply",
      "You can search open threat exchange for files tagged with \"scatterbrain\" and it will give you various hashes: https://otx.alienvault.com/browse/global/indicators?q=scatte...You can then use the hashes with platforms like virustotal to download some samples.\n \nreply",
      "Given that this was made by a nation-state attacker I'd expect something more sophisticated than pairipcore VM..So, still waiting for full pairipcore (the newer one) writeup.\n \nreply"
    ],
    "link": "https://cloud.google.com/blog/topics/threat-intelligence/scatterbrain-unmasking-poisonplug-obfuscator",
    "first_paragraph": "Written by: Nino IsakovicSince 2022, Google Threat Intelligence Group (GTIG) has been tracking multiple cyber espionage operations conducted by China-nexus actors utilizing POISONPLUG.SHADOW. These operations employ a custom obfuscating compiler that we refer to as \"ScatterBrain,\" facilitating attacks against various entities across Europe and the Asia Pacific (APAC) region. ScatterBrain appears to be a substantial evolution of ScatterBee, an obfuscating compiler previously analyzed by PWC.GTIG assesses that POISONPLUG is an advanced modular backdoor used by multiple distinct, but likely related threat groups based in the PRC, however we assess that POISONPLUG.SHADOW usage appears to be further restricted to clusters associated with APT41.GTIG currently tracks three known POISONPLUG variants:POISONPLUG.SHADOW\u2014often referred to as \"Shadowpad,\" a malware family name first introduced by Kaspersky\u2014stands out due to its use of a custom obfuscating compiler specifically designed to evade det"
  },
  {
    "title": "Ask HN: What is interviewing like now with everyone using AI?",
    "points": 169,
    "submitter": "ramesh31",
    "submit_time": "2025-02-02T15:19:32 1738509572",
    "num_comments": 217,
    "comments_url": "https://news.ycombinator.com/item?id=42909166",
    "comments": [
      "I've let people use GPT in coding interviews, provided that they show me how they use it. At the end I'm interested in knowing how a person solves a problem, and thinks about it. Do they just accept whatever crap the gpt gives them, can they take a critical approach to it, etc.So far, everyone that elected to use GPT did much worse. They did not know what to ask, how to ask, and did not \"collaborate\" with the AI. So far my opinion is if you have a good interview process, you can clearly see who are the good candidates with or without ai.\n \nreply",
      "I imagine most of the things that would be good uses for seniors in AI aren't great uses for a coding interview anyway.\"Oh, I don't remember how to do parameterized testing in junit, okay, I'll just copy-paste like crazy, or make a big for-loop in this single test case\"\"Oh, I don't remember the API call for this one thing, okay, I'll just chat with the interviewer, maybe they remember - or I'll just say 'this function does this' and the interviewer and I will just agree that it does that\".Things more complicated than that that need exact answers shouldn't exist in an interview.\n \nreply",
      "We do the same thing. It's perfectly fine for candidates to use AI-assistive tooling provided that they can edit/maintain the code and not just sit in a prompt the whole time. The heavier a candidate relies on LLMs, the worse they often do. It really comes down to discipline.\n \nreply",
      "This has been my experience as well. The ones that have most heavily relied on GPT not only didn't really know what to ask, but couldn't reason about the outputs at all since it was frequently new information to them. Good candidates use it like a search engine - filling known gaps.\n \nreply",
      "I don't really can't imagine being it usefull in the way where it writes logical part of the code for you. If you are not being lousy you still need to think about all the edge cases when it generates the code which seems harder for me.\n \nreply",
      "I like that you\u2019re openminded to allow candidates to be who they are and judge them for the outcome rather than using a prescribed rigid method to evaluate them. Im not looking to interview right now but I\u2019d feel very comfortable interviewing with someone like you, I\u2019d very likely give out my best in such an interview. Id probably choose not to use an LLM during the interview unless I wanted to show how I brainstormed a solution.\n \nreply",
      "same thing here. Interview is basically a representative thing of what we do, but also depends on the level of seniority. I ask people just to share the screen with me and use whatever you want / fell comfortable with. Google, ChatGPT, call your mom, I don't care as long as you walk me through how you're approaching the thing at hand. We've all googled tar xvcxfgzxfzcsadc, what's that permission for .pem is it 400, etc.. no shame in anything and we all use all of the things through day. Let's simulate a small task at hand and see where we end up at. Similarly, there is a bias where people leaning more on LLMs doing worse than those just googling or, gasp, opening documentation.\n \nreply",
      "i like this. it seems like a good and honest use of time.\n \nreply",
      "Yeah I've been doing the same.  Have been pretty stunned at people's inability to use AI tools effectively.\n \nreply",
      "What does effective use look like? I have attempted messing around with a couple of options, but was always disappointed with the output. How do you properly present a problem to a LLM? Requiring an ongoing conversation feels like a tech priest praying to the machine spirit.\n \nreply"
    ],
    "link": "item?id=42909166",
    "first_paragraph": ""
  },
  {
    "title": "Caps Lock Key Question",
    "points": 10,
    "submitter": "Tommix11",
    "submit_time": "2025-01-30T09:08:25 1738228105",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=42876203",
    "comments": [
      "I think an AutoHotKey script could do that (if it runs on Windows 11. I couldn\u2019t easily find that on its site). Approach would be:- in a \u201ccaps lock key pressed\u201d handler, set a timer (https://www.autohotkey.com/docs/v2/lib/SetTimer.htm) that runs every ten seconds- in that timer, check the value of A_TimeIdleKeyboard (https://www.autohotkey.com/docs/v2/Variables.htm#TimeIdleKey...)- if it\u2019s too large, call SetCapsLockState (https://www.autohotkey.com/docs/v2/lib/SetNumScrollCapsLockS...) and stop the timer- otherwise, compute a new ideal delay from the desired run interval and A_TimeIdleKeyboard, and update the time intervalYou also wail want to prevent starting multiple timers if you enable caps lock multiple times.\n \nreply",
      "Thank you, I'll check it out.\n \nreply"
    ],
    "link": "item?id=42876203",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Lume \u2013 OS lightweight CLI for MacOS and Linux VMs on Apple Silicon (github.com/trycua)",
    "points": 234,
    "submitter": "sandropuppo",
    "submit_time": "2025-02-02T11:46:22 1738496782",
    "num_comments": 64,
    "comments_url": "https://news.ycombinator.com/item?id=42908061",
    "comments": [
      "congrats on the open sourcing and launching! beyond the desire to run VMs in \"1 command\", i don't quite get the reasoning behind this project. could you elucidate? like, besides running macOS VMs, how is it different from lima, colima, and friends? the name lume is quite unfortunate.the hard part about running VMs isn't really how to launch them (well, ahem, i'm looking at you, qemu), but getting data in and out, and controlling them. some feature requests, if i may ;)    # take screenshot\n    # this should do the right thing(TM) and take a screenshot of the logged-in user session, which may not necessarily be the console\n    lume screenshot <vm name> [-o <file.png> | -]\n\n    # execute command\n    lume exec <vm name> [--as-user <user>] <command> [args]\n\n    # copy files in and out\n    lume cp <vm name>:<vm path> <local path>\n    lume cp <local path> <vm name>:<vm path>\n\n    # run clone as new VM\n    # this should appropriately roll the MAC address, IPs, and reseed any RNGs, of course\n    lume run --clone <clone name> <vm name>\n\nCan you clone a VM while it's running?The ability to resume a VM within < 1 second would be useful for on-demand workflows without waiting for a full VM bootup sequence, similar to how you can get a firecracker microVM into the state you want, snapshot it.. then clone as you wish, and resume back into the guest.You may need to preinstall an agent (a la Parallel/VMware Tools) to make sure this is seamless and fast.\n \nreply",
      "Thanks for the feedback! I actually went into more detail about the project's reasoning and how it compares to lima/tart in another comment.Interestingly, we left out the screenshot and exec features from this initial release so the CLI wouldn\u2019t get too cluttered. We\u2019re rolling out another update next week that will let you request screenshots programmatically, stream the VNC session in your browser via noVNC, run commands remotely over SSH, add clipboard support (finally!), and more.As for cloning, you can indeed clone a running VM. However, suspending a VM isn\u2019t supported in this release yet, even though it\u2019s possible with the Apple Virtualization.framework. I\u2019ll open an issue to track that work. Thanks again for the suggestions!\n \nreply",
      "Honestly, LXD commands and LXD cli would probably have worked with any backend.\n \nreply",
      "Would this allow to run a near-native speed VM with a decent-speed Docker server inside, and thus allow to have an Orb-like speed for docker containers?\n \nreply",
      "How does this compare to Lima[1] and Tart[2], which are similar?Also, would it be possible to run BSDs with this?[1] https://lima-vm.io[2] https://tart.run\n \nreply",
      "Yes, lume relies on Apple's Virtualization framework and can run BSD on a Mac with Apple silicon: https://wiki.freebsd.org/AppleSiliconI'll definitely document the option in the README, thanks!On Lima:- Lima focuses on Linux VMs and doesn't support managing macOS VMs.- It is more of a container-oriented way of spinning up Linux VMs. We're still debating whether to go in that direction with lume, but the good news is that it would mostly require tweaking the hooks to expose lume\u2019s core to adopt the containerd standard. I\u2019d love to hear your thoughts - would you find it useful to have a Docker-like interface for starting macOS workloads? Similarly to: https://github.com/qemus/qemu-docker- Still many dependencies on QEMU, which doesn't play well with Apple silicon - while we opted to support only M chips (80-90% of the market cap today) by relying on the latest Apple Virtualization.Framework bits.On Tart:- We share some similarities when it comes to tart's command-line interface. We extend it and make it more accessible to different frameworks and languages with our local server option (lume serve). We have also an interface for python today: https://github.com/trycua/pylume- Going forward, we'd like to focus more on creating tools around developers, creating tools for automation and extending the available images in our ghcr registry. Stay tuned for more updates next week!- Lastly, lume is licensed under MIT, so you\u2019re free to use it for commercial purposes. Tart, on the other hand, currently uses a Fair Source license.\n \nreply",
      "Hey, thanks for responding.I think it is great to have more options in this space, as all of these options are still quite immature.  In terms of approaches to take, I find one of the challenges in using lima is that, they way lima works makes the VM different enough from production that I can't be confident testing covers everything.In terms of feature set, I think some of these have been mentioned below, but these would be great:\n- network bridging\n- snapshotting\n- usb / bluetooth passthrough (this is probably dependent on Apple's framework)\n \nreply",
      "Colima is a container focused macOS VM runner.\n \nreply",
      "Would you mind educating me about use cases for having one or even multiple MacOS VMs on an apple silicon machine please?\n \nreply",
      "I have an M4 Mac Mini running the following in a VM:- OpenWRT (previously OPNSense & once Mikrotik RouterOS) using 2x 2.5Gbps Ethernet NICs via USB-C- OpenMediaVault (Exposing a 4-bay DAS via USB-C, 2x3TB Drives in Btrfs RAID-1)- HassOS (Home Assistant OS)On the host, I'm running OLlama and a reverse proxy through Docker.The whole thing uses 7 watts of power at any given time - I've seen max peaks of 12w when running LLM queries. The drive bay actually uses more than it.Through power saving alone, it will pay for itself in 5 years over my previous AMD Zen 2 build.\n \nreply"
    ],
    "link": "https://github.com/trycua/lume",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A lightweight CLI and local API server to create, run and manage macOS and Linux virtual machines (VMs) natively on Apple Silicon.\n      \n\n\nlume is a lightweight Command Line Interface and local API server to create, run and manage macOS and Linux virtual machines (VMs) with near-native performance on Apple Silicon, using Apple's Virtualization.Framework.For a python interface, check out pylume.You can also download the lume.pkg.tar.gz archive from the latest release, extract it, and install the package manually.Pre-built images are available on ghcr.io/trycua.\nThese images come with an SSH server pre-configured and auto-login enabled.For additional disk space, resize the VM disk after pulling the image using the lume set <name> --disk-size <size> command.lume exposes a local HTTP API server that listens on http://localhost:3000/lum"
  },
  {
    "title": "Reverse-engineering and analysis of SanDisk High Endurance microSDXC card (2020) (ripitapart.com)",
    "points": 225,
    "submitter": "userbinator",
    "submit_time": "2025-02-02T10:32:40 1738492360",
    "num_comments": 98,
    "comments_url": "https://news.ycombinator.com/item?id=42907766",
    "comments": [
      "Every time Raspberry Pi comes up as a topic on HN, there are scores of comments decrying the futility and frustration of unreliable SD cards.The thing with SD cards is that there are wide variances in quality, even among the top name-brand manufacturers. You can get the normal crappy cards that everyone buys which are likely to last not much more than 2-3 years under very occasional use all the way up to industrial cards with 10 year warranties and reliability matching the best enterprise SSDs. And everything in between.The point is: If you buy the cheapest thing on Amazon, you're going to get exactly what you paid for. For my part, I have been using \"High Endurance\" cards in my Pis for several years as normal OS and data disks. They are not actually much more expensive than \"normal\" cards. I have not had any of them fail yet.\n \nreply",
      "I don't disagree with you, but the other perennial unanswered question on HN is:  how do I ensure I'm paying extra for actual quality and not just sucker tax?Memory cards and ssds are famously obtuse like this. There are the branded versions (custom made for your Switch, for your Xbox,etc) which are exactly the same as their cheaper counterparts. Sandisk itself recently started a \"creator\" line of cards and ssds which are, again, exactly the same but more expensive. Samsung had Evo Plus which was different than Evo+ but same as Evo Select. Go figure!Sometimes looking at specs helps but usually not - I.e. The \"up to\" speed marks are famously irrelevant for real time usage, and brand name companies will offer any number of cards with same specs and tags (u2/v3/class10/mark5/whatever) at varying price points. And then there's the WD Red saga where drives branded \"NAS\" were completely inappropriate for NAS usage.I ran a photography business a while back and always bought extreme pro because I just couldn't risk it, but honestly, I felt like a bit of a sucker. It's hard to know when you're actually buying real quality and endurance increase.\n \nreply",
      "Especially with the Raspberry Pi, another point is power delivery. You need a beefy power source, but also a good cable. I bought a USB-C charging cable for my Pi4, and it was very unstable and crashed a lot. Corrupted the install a couple of times, and so I got some new SD cards.Well, turned out the \"charging cable\" had 1 ohm resistance and so when the Pi load spiked, the voltages would drop well below 4.5V...Tossed it away and got a proper cable, and the thing has been rock solid since.Highly recommend getting a USB cable tester, I had several black sheep in my collection.\n \nreply",
      "> you're going to get exactly what you paid for.And nearly always the cheap cards seem to fail due to firmware shortcomings rather than hardware faults.    Ie. the built in wear levelling and error recovery is bad.Considering firmware is effectively free once written, the fact the cheap cards still don't have good firmware leads me to believe it might be deliberate market segmentation.   Ie. \"we need to make the cheap cards glitch and fail often, or nobody would buy our expensive cards\".If so, it really raises moral questions about how many peoples wedding pictures have been gobbled simply so some executive can get his christmas bonus...\n \nreply",
      "I suspect the cheap cards are bad because of cheap flash, which ironically requires much stronger ECC and wear leveling to function at all.\n \nreply",
      "These moral questions stand unsolved for decades, and many people are acutely aware of them. IMO it's time we start putting those executives in jail for it. But we all know it ain't ever happening.\n \nreply",
      "> And nearly always the cheap cards seem to fail due to firmware shortcomings rather than hardware faults. Ie. the built in wear levelling and error recovery is bad.Well, higher quality cards have better performing controllers that operate at higher speeds or have dedicated hardware offloads for stuff that a \"cheaper\" controller has to do in software.\n \nreply",
      "Something I found interesting when learning about flash was how it is manufactured and how much reliable single layer cells are (SLC)There\u2019s certainly a very visible increase in price and decrease in capacity but it\u2019s certainly interesting when you get sd cards with a proper datasheet vs. customer level deviceshttps://www.digikey.com/en/products/filter/memory-cards/501?...\n \nreply",
      "a very visible increase in price and decrease in capacityUnfortunately SLC costs disproportionately more compared to MLC, TLC, and now QLC -- the actual die cost is only 3x for SLC compared to TLC of the same capacity, but the prices are closer to 10x or more.Related: https://news.ycombinator.com/item?id=40405578\n \nreply",
      "Most eMMC chips (basically the chip version of an SD card) can be configured to work in pseudo-SLC (pSLC) mode. This halves the capacity but improves the write endurance by several times.\n \nreply"
    ],
    "link": "https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/",
    "first_paragraph": "As seen on Hackaday!TL;DR \u2013 The SanDisk High Endurance cards use SanDisk/Toshiba 3D TLC Flash. It took way, way more work than it should have to figure this out (thanks for nothing, SanDisk!).\nIn contrast, the SanDisk MAX Endurance uses the same 3D TLC in pMLC (pseudo-multi-level cell) mode.In a previous blog post, I took a look at SanDisk\u2019s microSD cards that were aimed for use in write-intensive applications like dash cameras. In that post I took a look at its performance metrics, and speculated about what sort of NAND Flash memory is used inside. SanDisk doesn\u2019t publish any detailed specifications about the cards\u2019 internal workings, so that means I have no choice but to reverse-engineer the can of worms card myself.In the hopes of uncovering more information, I sent an email to SanDisk\u2019s support team asking about what type of NAND Flash they are using in their High Endurance lineup of cards, alongside endurance metrics like P/E (Program/Erase) cycle counts and total terabytes writte"
  },
  {
    "title": "Autothrottle: Resource Management for SLO-Targeted Microservices (usenix.org)",
    "points": 7,
    "submitter": "mlerner",
    "submit_time": "2025-02-02T23:17:38 1738538258",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.usenix.org/conference/nsdi24/presentation/wang-zibo",
    "first_paragraph": "Zibo Wang, University of Science and Technology of China and Microsoft Research; Pinghe Li, ETH Zurich; Chieh-Jan Mike Liang, Microsoft Research; Feng Wu, University of Science and Technology of China; Francis Y. Yan, Microsoft Research\nAwarded Outstanding Paper!Achieving resource efficiency while preserving end-user experience is non-trivial for cloud application operators. As cloud applications progressively adopt microservices, resource managers are faced with two distinct levels of system behavior: end-to-end application latency and per-service resource usage. Translating between the two levels, however, is challenging because user requests traverse heterogeneous services that collectively (but unevenly) contribute to the end-to-end latency. We present Autothrottle, a bi-level resource management framework for microservices with latency SLOs (service-level objectives). It architecturally decouples application SLO feedback from service resource control, and bridges them through the "
  },
  {
    "title": "Nevada Ivanpah Solar Plant Accidentally Incinerates Up to 6k Birds a Year (2016) (sciencealert.com)",
    "points": 22,
    "submitter": "walterbell",
    "submit_time": "2025-02-02T23:45:14 1738539914",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=42913104",
    "comments": [
      "Jan 2025 article, https://apnews.com/article/california-solar-energy-ivanpah-b... via https://news.ycombinator.com/item?id=42913376> What was once the world\u2019s largest solar power plant of its type appears headed for closure just 11 years after opening... \u201cThe Ivanpah plant was a financial boondoggle and environmental disaster,\u201d Julia Dowell of the Sierra Club said in an email. \u201cAlong with killing thousands of birds and tortoises, the project\u2019s construction destroyed irreplaceable pristine desert habitat along with numerous rare plant species,\u201d Dowell said. \u201cWhile the Sierra Club strongly supports innovative clean energy solutions.. Ivanpah demonstrated that not all renewable technologies are created equal.\u201d\n \nreply",
      "6000 sounds like a lot, right?But in context, crashing into glass buildings kills about 600 million birds a year.Cats? About 2.4 BILLION bird kills a year.https://www.fws.gov/library/collections/threats-birds\n \nreply",
      "To be fair that is all glass buildings everywhere compared to a single place.Not quite a fair comparison.\n \nreply",
      "Divide by the 111 million buildings in the US of which 90% are single family homes.So it\u2019s what 100x as bad as the average structure but we\u2019re talking about something several times the size of the average single family home.\n \nreply",
      "Context is always important. Part of why this gets attention is that the plant is killing protected species, including many that don't fall victim to cats.More than that, the solar plant itself is an entirely unnecessary addition. Photovoltaic cells were already viable in 2014. Other net-zero CO2 power options are also available, though wind turbines frequently suffer the same problem.This plant exists to feel good about the energy it produces, and being able to sit around and watch it immolate protected species every day cuts against that goal.\n \nreply",
      "> More than that, the solar plant itself is an entirely unnecessary addition.The interesting thing about Ivanpah is that it's both generation and storage.  It's probably a technological dead end, but that is not an absolutely sure thing now and it certainly wasn't obvious a decade ago.  Batteries have come a long way in that time.\n \nreply",
      "It would be interesting to see video of the birds getting incinerated in mid flight.I presume there is video of the aftermath (\"workers have nicknamed the smouldering birds \"streamers\", because they leave tiny wisps of white smoke behind as they burn up in the sky.\") of the incinerated birds here: https://www.latimes.com/local/california/la-me-solar-bird-de...\n \nreply",
      "I found myself accidentally behind the secured area of this solar installation while driving in the Mojave National Preserve. It is truly bizarre to see up close - the glow that you see around the towers in the photo on the article is quite bright in person.I wondered when I saw it - is that glow the air turning into plasma? Are otherwise-invisible dust particles reflecting the absurd amount of light hitting them? Is the heat enough for the air to start scattering light?It's no surprise that it would incinerate birds, in any case.\n \nreply",
      "Trying some fun with napkin math to compare it versus coal.1. That plant produces ~392 megawatts in^h^ while ~6000 bird deaths occur, meaning ~65 kilowatts provided per dead bird.2. One ton of coal can give you ~21 gigajoules in a coal plant, or ~665 watts over the course of a year.3. So the same output for a coal plant means burning ~98 tons of coal per bird.4. Unknown: Does mining and burning ~98 tons of coal will lead to >1 bird deaths?If the answer is yes, then the solar thermal plant is--on average--killing fewer birds. (Specific sub-populations near it are another story.)\n \nreply",
      "> 1. That plant produces ~392 megawatts in a year.It produces 640 gigawatt-hours in a year.  Power in a year doesn't make sense, and it doesn't make its nameplate power all day long.\n \nreply"
    ],
    "link": "https://www.sciencealert.com/this-solar-plant-accidentally-incinerates-up-to-6-000-birds-a-year",
    "first_paragraph": ""
  },
  {
    "title": "Analyzing the codebase of Caffeine, a high performance caching library (adriacabeza.github.io)",
    "points": 194,
    "submitter": "synthc",
    "submit_time": "2025-02-02T09:37:05 1738489025",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=42907488",
    "comments": [
      "Years ago I encountered a caching system that I misremembered as being a plugin for nginx and thus was never able to track down again.It had a clever caching algorithm that favored latency over bandwidth. It weighted hit count versus size, so that given limited space, it would rather keep two small records that had more hits than a large record, so that it could serve more records from cache overall.For some workloads the payload size is relatively proportional to the cost of the request - for the system of record. But latency and request setup costs do tend to shift that a bit.But the bigger problem with LRU is that some workloads eventually resemble table scans, and the moment the data set no longer fits into cache, performance falls off a very tall cliff. And not just for that query but now for all subsequent ones as it causes cache misses for everyone else by evicting large quantities of recently used records. So you need to count frequency not just recency.\n \nreply",
      "For every caching algorithm you can design an adversarial workload that will perform poorly with the cache. Your choice of caching algorithm/strategy needs to match your predicted workload. As you're alluding there's also the question of which resource are you trying to optimize for, if you're trying to minimize processing time that might be a little different than optimizing for bandwidth.\n \nreply",
      "If you have to refetch on a cache miss you're going to be doing both. But all optimizations are always playing with the trigraph of cpu time, memory, and IO (with the hidden fourth dimension of legibility), so I don't think you're saying anything that can't be assumed as given. Even among people who tend to pick incorrectly, or just lose track of when the situation has changed.\n \nreply",
      "I understood the OP to have said something along the lines of if we have a fixed cost per object then we should bias towards smaller objects if we want to minimize that cost.And totally legibility and/or simplicity. I'll take something I can reason about and maintain over something more complicated just to eek out a tiny better hit ratio. That said, if you're caching at scale your 0.05% hit ratio can be a big deal.As a matter of personal taste/opinion I also shy away from close loop systems. Feedback makes things complicated in non-intuitive ways. Caffeine seems neat in terms of using feedback to adjust the cache to the workload - as always test with your workloads and pick what is best for your situation.\n \nreply",
      "Caffeine is a gem. Does what it claims, no drama, no scope creep, just works. I've used it in anger multiple times, most notably in Apache Cassandra and DataStax Astra, where it handles massive workloads invisibly, just like you'd want.Shoutout to author Ben Manes if he sees this -- thanks for the great work!\n \nreply",
      "Plus Ben made it extremely easy to migrate from Google Guava\u2019s cache. It\u2019s mostly the same API and way more performant to switch to Caffeine.\n \nreply",
      "Thanks Jonathan!\n \nreply",
      "It would be interesting to see this on reddit's workload.  The entire system was designed around the cache getting a 95%+ hit rate, because basically anything on front page of the top 1000 subreddits will get the overwhelming majority of traffic, so the cache is mostly filled with that.In other words, this solves the problem of \"one hit wonders\" getting out of the cache quickly, but that basically already happened with the reddit workload.The exception to that was Google, which would scrape old pages, and which is why we shunted them to their own infrastructure and didn't cache their requests.  Maybe with this algo, we wouldn't have had to do that.\n \nreply",
      "Wouldn\u2019t one hit wonders still be an issue? They might get evicted relatively fast anyway but assuming an LRU each will still take a cache entry until they go through the entire thing and finally get evicted.Although if that\u2019s your concern you can probably just add a smaller admission cache in front of the main cache, possibly with a promotion memory.\n \nreply",
      "what are/were Reddit's top two or three cached structures / things?guessing post bodies and link previews feels too easy.comment threads? post listings?was there a lot of nesting?it sounds like you're describing a whole post--use message, comments, and all--for presentation to a browser or crawler.(sorry, saw the handle and have so many questions :D)\n \nreply"
    ],
    "link": "https://adriacabeza.github.io/2024/07/12/caffeine-cache.html",
    "first_paragraph": "\nJul 12, 2024\n      The other day, while wasting time reading reddit, I stumbled upon a blogpost mentioning S3 FIFO, a method claiming to outperform LRU (Least Recently Used) in terms of cache miss ratio. Notable companies like RedPandas, Rising Wave, and Cloudflare have already implemented it in various capacities, so this piqued my interest. Caches are a pretty darn interesting and at Datadog, we rely heavily on them in several services, so I knew I had to put S3 FIFO to the test, or at least, make sure I understood its core ideas.However, diving into a new caching approach without a deep understanding of our current system seemed premature. In my team we extensively use Caffeine and let\u2019s be sincere, I do not know it\u2019s internals and I have never actually checked if there were knobs and parameters to fine tune. This post is a summary of my notes trying to understand Caffeine\u2019s inner workings, to dissect its code.Join me as we unravel some of the complexities of one of the most used c"
  }
]