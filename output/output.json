[
  {
    "title": "SnowflakeOS: Beginner friendly and GUI focused NixOS variant (snowflakeos.org)",
    "points": 41,
    "submitter": "tkz1312",
    "submit_time": "2024-07-31T23:06:11",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=41124472",
    "comments": [
      "The site is bereft of details, but it appears to be related to https://snowfall.org, which has a little more detail.\n \nreply",
      "I'm a user of Nix for about seven years and finally decided  back in March, during the second rehash of $sponsor controversy, that I'll stop investing and move away from the tech as soon as I have a suitable replacement.I've been critical of Nix for a variety of reasons along the way but one of those reasons is how so many people (and consultancies) seem hell-bent to set up their own little fiefdoms within the community.The community is already niche, we certainly didn't need people purposely driving further fragmentation and incompatibility with these frameworks with bad bus factor and NIH designs.I've seen a lot of tribalism and cargo culting around some of the other efforts in this vein, too.\n \nreply",
      "How ironic that the post for the beginner friendly OS isn't beginner friendly\n \nreply",
      "Is this still being worked on?The most recent commits in the pinned Github repos are two months old. For a project that considers itself \"Alpha\" and \"Not yet ready for daily use\" that's a pretty long time of inactivity.\n \nreply",
      "In the non-pinned, non-auto updated repos, I see commits from two weeks ago.Though the main contributors are pretty active with nixpkgs, so a lack of commits here doesn't necessarily mean they aren't working on improving snowflakeos, but working on getting improvements they need implemented upstream.\n \nreply",
      "Yeah, I think it is still being worked on. It's only got a handful of contributors.\n \nreply",
      "This is nothing to do with Snowflake the company, right? What an odd name to pick.\n \nreply",
      "This seems great, and I think would be an amazing distribution, but it doesn't seem that much visible progress has been made in a long while. If there is indeed progress happening behind the scenes, it would be much better if the devs did this in a more visible fashion, and did some evangelizing to attract more contributors and traction.\n \nreply",
      "the words 'beginner friendly' and nixos don't belong anywhere near each other\n \nreply",
      "I can't see any reason they couldn't go together. Fully using nix is a trip off the deep end of the learning curve, but there's nothing preventing a beginner using a predefined nixos configuration so long as they stay inside the guiderails. With a bit of care, it should even be possible to let them enable whole add-in .nix files to do things like \"check this box to enable proprietary nvidia drivers\" (which includes a predefined module that handles that).\n \nreply"
    ],
    "link": "https://snowflakeos.org/",
    "first_paragraph": "SnowflakeOS is a NixOS based Linux distribution focused on\n                            beginner friendliness and ease of use.Not yet ready for daily use!\nSnowflakeOS\n\n\nGitHub\n\n\nDiscord\n\n\nMatrix\n                            chat\n\n\nTwitter\n\n\nMastodon\n\n\nSource\n\n"
  },
  {
    "title": "Cardie \u2013 An open source business card designer and sharing platform (github.com/nfoert)",
    "points": 45,
    "submitter": "nfoert",
    "submit_time": "2024-07-31T19:50:37",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=41122793",
    "comments": [
      "Pretty neat!\n \nreply",
      "Do people stil do business cards?\n \nreply",
      "Yep, I do. It's a baseline expectation in my experience when working within less technical industries.\n \nreply",
      "But does the font selection include Silian Rail and Romalian Type?\u201cLook at that subtle off-white coloring. The tasteful thickness of it. Oh my God, it even has a watermark\u2026\u201d\n \nreply",
      "\u201cLet\u2019s see Paul Allen\u2019s Card!\u201d\n \nreply",
      "Why do I need an account to make a card? Shouldn't that only be required if I want to share / save the card?\n \nreply",
      "Right now it's a bit simpler to have the user create their account first, as it's mostly intended for them to immediately be saving it into their account so they can have it ready to be shared with others, or to print out once that feature gets added. In future having an option for the user to test it out first would be a good idea though\n \nreply",
      "I have to have an account to put text on a background...and your privacy policy page currently throws http error 500.https://cardie-uwtwy.ondigitalocean.app/privacy\n \nreply",
      "Thanks for pointing that out, I figured out why that's happening, it'll be fixed in the next release\nAnd it's currently in open alpha, it's not yet finished. I have a lot of plans on how to offer more customization options in future.\n \nreply",
      "Designing business cards has nothing to do with sharing them.Someone there wants some free data? (With privacy policy page showing error 500 for the demo)\n \nreply"
    ],
    "link": "https://github.com/nfoert/cardie",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        An open source business card designer and sharing platform\n      \nDesign a unlimited number of business or information cards about yourself, share a link or QR code to them, print it out, and save other people's cards to your virtual wallet for later. Once you've created a card you can get analytics data on how your cards are getting visited, you can edit your cards as things change, and you can keep cards private so only people with a link to your card can see it.ImportantCardie is currently in an open alpha. Things will be rapidly changing and bugs are to be expected.First, clone this repository using the following commandThen, navigate to that directory and create a new python virtual environmentActivate the virtual environment using the command for your system (Linux is used here) and install the required dependenciesNext, creat"
  },
  {
    "title": "Suspicious data pattern in recent Venezuelan election (columbia.edu)",
    "points": 531,
    "submitter": "kgwgk",
    "submit_time": "2024-07-31T20:34:31",
    "num_comments": 272,
    "comments_url": "https://news.ycombinator.com/item?id=41123155",
    "comments": [
      "More on this. The Carter Foundation, the only impartial observers who were in Venezuela for the election, and who previously defended Venezuela's election system following Chavez's 2004 win, has called on Maduro's government to release local vote tallies, which apparently it is never going to do:  https://www.nytimes.com/2024/07/31/world/americas/venezuela-...\n \nreply",
      "Maduro just responded to a journalist from the Washington Post that they will not be able to show the local vote tallies now because, at this moment, the National Electoral Council is in \u201ca cyber battle never seen before.\u201d\n \nreply",
      "I was curious to check the National Electoral Council website, to get their latest tally straight from the source...http://www.cne.gov.ve/And the website is currently not loading. They might simply be struggling to keep up with traffic, but being under some kind of \"cyber attack\" is also plausible\n \nreply",
      "This would also be fairly trivial to fake if you wanted to appear to be under attack.\n \nreply",
      "Not to put too fine a point on it, but you wouldn't have to fake anything if you just didn't use electronic voting.Simply give a ballot to every voter and have them fill it out. Especially nowadays, as pretty much every election in every nation will come under cyber attack precisely because they are using electronic voting and hackers can manipulate the votes.If I were being ungenerous, I would say all the politicians want electronic voting everywhere because they can manipulate it with cyber attacks.\n \nreply",
      "The Carter Foundation is absolutely not impartial here. They get funding from CIA cutout USAID, the US State Department, Meta, the Walton Family Foundation, National Democratic Institute, Open Society Foundation and so on: https://www.cartercenter.org/donate/corporate-government-fou...\n \nreply",
      "The issue in this post has nothing to do with any of the outside election observers. You could replace the Carter Foundation with the CIA itself and you'd still be left with the same problem, which is that these vote counts are fictitious.\n \nreply",
      "How should they be funded, for you to consider them impartial?\n \nreply",
      "Is anyone really impartial?\n \nreply",
      "[flagged]"
    ],
    "link": "https://statmodeling.stat.columbia.edu/2024/07/31/suspicious-data-pattern-in-recent-venezuelan-election/",
    "first_paragraph": ""
  },
  {
    "title": "Launch HN: Martin (YC S23) \u2013 Using LLMs to Make a Better Siri",
    "points": 108,
    "submitter": "darweenist",
    "submit_time": "2024-07-31T14:24:27",
    "num_comments": 86,
    "comments_url": "https://news.ycombinator.com/item?id=41119443",
    "comments": [
      "I want this, but very concerned about the security and privacy - you're talking about getting my most personal of personals (email, calendar, messages, phone calls). This could be a nightmare of privacy or security breaches. That's why I'm likely waiting for Apple's version within their corporate security and privacy commitments (and they already have my data). \nI don't see anything on the website about SOC2, or privacy commitments beyond a boilerplate policy?\n \nreply",
      "Thanks for leaving the comment! We totally understand your concerns, and you're not alone! We ourselves are very privacy sensitive, and never liked the idea of hardware devices always listening to us. And, as you said, our integrations are dealing with the most personal of personal information.We recently got our CASA Tier-2 compliance done (Cloud Application Security Assessment). We've also gone through Google's OAuth compliance process for every new integration we add that's related to Google. These assessments scan our app and make sure that our software meets pretty stringent standards when it comes to data security and encryption, and that we're not using the data for anything other than the specific features we promise (i.e. not sharing or selling to advertisers, etc.). You can read more about CASA here (https://appdefensealliance.dev/casa). We haven't gone through SOC2 yet, but planning on soon once we have a few more integrations.\n \nreply",
      "This really doesn't say much though. What specific measures are in place to ensure user privacy and data protection?Does personal information get sent to OpenAI or Claude as part of the functionality? Can users request deletion of their data, and if so, what is the process? Are there specific protocols in place to ensure security? (i.e. Do you use encryption at rest?).\n \nreply",
      "> What specific measures are in place to ensure user privacy and data protection?Unless you intend to personally audit their code, I'd argue it couldn't possibly matter. Even businesses like Apple publish all kinds of documentation that belies the reality of their infrastructure. The iMessage Security Overview doesn't mention the NSA's retention period for encrypted communique; the push notification documentation doesn't tell you about the government middleman processing each alert.You either trust people blindly, or you validate them personally. Getting a pinkie-promise about privacy from the CEO is worth absolutely nothing in real-world security terms.\n \nreply",
      "But there is provable, verifiable transparency with what Apple is doing with private cloud compute.https://security.apple.com/blog/private-cloud-compute/\n \nreply",
      "> We want to ensure that security and privacy researchers can inspect Private Cloud Compute software, verify its functionality, and help identify issues \u2014 just like they can with Apple devices.So... in Apple's own words, they are allowed to cherry-pick who's allowed to read their code and audit their privacy, in the same way they strategically deny researchers the ability to audit certain iOS features.You're still taking them on their word, here.\n \nreply",
      "> Making the log and associated binary software images publicly available for inspection and validation by privacy and security experts.The measurement logs will be publicly available.\n \nreply",
      "We don't know what those logs will tell us, and if it was designed with privacy in mind it shouldn't say much. Binary software images also don't tell us what the binary is doing, similar to how having all of the iOS files doesn't give you insight into how the OS was programmed. If the server source was fully open source and each machine could attest it was running an unmodified binary, then we might have a level of accountability. As-is, this is no better than Apple's \"Trust me, bro\" mindset they exercise securing iOS and MacOS.\n \nreply",
      "Microsoft and OpenAI aren\u2019t even providing users with services with any actual confidential compute architecture. You actually need to trust them, but they don\u2019t even claim to do what apple does, so you would need to hallucinate they are making promises they aren\u2019t and believe THAT, and also hope they aren\u2019t hacked or served with a warrant. It\u2019s a different matter with what Apple is doing.\n \nreply",
      "A different matter without distinction. Apple is equally as unaccountable as OpenAI and Microsoft, their only difference is their usual marketing strategy that the industry never took seriously in the first place. If it came out that they were sending the NSA all of your \"private\" LLM requests (like what happened with push notifications[0]), Apple would just sheepishly admit it and continue advertising their same security-oriented shtick. They're shameless.[0] https://arstechnica.com/tech-policy/2023/12/apple-admits-to-...\n \nreply"
    ],
    "link": "item?id=41119443",
    "first_paragraph": ""
  },
  {
    "title": "How great was the Great Oxidation Event? (eos.org)",
    "points": 171,
    "submitter": "Brajeshwar",
    "submit_time": "2024-07-31T13:40:49",
    "num_comments": 70,
    "comments_url": "https://news.ycombinator.com/item?id=41119080",
    "comments": [
      "My coffee table photobook describes the role of molybdenum in determining the GOE's timeline:https://impacts.to/downloads/lowres/impacts.pdf#page=11\n \nreply",
      "I mis-parsed what you are saying here.  Molybdenum lets us track the timeline of when the GOE happened; it didn't change or cause the timeline.\n \nreply",
      "What book is this :)\n \nreply",
      "https://www.impacts.to/index.html\n \nreply",
      "Looking at chart 1, it seems to me that the distribution of the chromium-53 ratio in today's seawater is a reasonable match to the ratios seen in today's sediments, and not to that seen in ancient rocks, while the distribution for today's rivers and estuaries is not a good match for today's sediments, and, if anything, is a better match to the ancient rocks.Absent any other evidence, this seems to suggest that the fractionation seen in today's sediments may be the result of processes occurring in seawater rather than in rivers, and if so, that would in turn suggest that what happens in rivers and estuaries is not a good guide to the fractionation we should see in ancient rocks, even if we assume ancient rivers were mostly like the Rio Tinto - unless the ancient seawaters were acidic enough to prevent fractionation occurring there.\n \nreply",
      "PBS had a wonderful series Ancient Earth that covered the geological history of the earth. https://www.pbs.org/wgbh/nova/series/ancient-earth/My naive understanding was always that the earth or planets just sort of found a natural state of being after a while and were / are just that way now.   It's very interesting to see the sea saw type scale of changes that occurred over time.\n \nreply",
      "The dynamic nature of the planet earth is likely what drove the evolutionary changes to develop the highly complex life today. In the most stable period of earth\u2019s history, the so called \u201cboring billion,\u201d (Mid Proterozoic) a period of a billion years of a stable environment resulted in extremely slow evolutionary advancements. This period interestingly was bordered by two oxygenation events, the first being the topic of this thread.\n \nreply",
      "It's a remarkable thing to step back for a second and realize that while we try to figure out the exact impact of a parts-per-million change in CO2 concentration, that it's astonishing that CO2 is not 20% of the atmosphere, that the only thing keeping O2 in the atmosphere at all is the large-scale actions of living things. [1]The fact that living organisms are responsible for something so large seems almost dumbfounding -- planets are big, atmospheres are big, and life is small; what is a pool of algae compared to a mountain, etc. But even such a basic thing as \"the only reason we can have something as fundamental as FIRE is because of living things\" is a bit of a mindblowing realization.[1] probably not literally true; if you eliminated all life on earth then most of the O2 would probably be sequestered in oxides rather than remaining resident as CO2, but still. Although I guess a lot of non-living organic matter would eventually burn away as long as there is oxygen to support combustion.\n \nreply",
      "I'm pretty sure it is literally true that the oxygen in the atmosphere is there only because living things keep putting it there.  You're right that without life, it would be sequestered in oxides pretty quickly.  That's the local minima for energy dissipation.  Life is good at breaking those minimas for cyclic matter dissipation.  If there was a natural source for oxygen, it would have to come from some sort of cycle in order to be maintained, and there's not a energetically favorable cycle for that.It's why it makes a good biomarker when looking at exoplanets.  If we find an exoplanet with high amounts of oxygen in the atmosphere, we can be fairly confident\n \nreply",
      "I\u2019m surprised by this statement.Here is a quote from Nick Lane\u2019s great text: Power, Sex, Suicide (p 153):> The early Earth, as envisaged by [Michael J] Russell, is a giant electrochemical cell, which depends in the power of the sun to oxidize the oceans. UV rays split water and oxidize iron. Hydrogen, released from the water, is so light that it is not retained by gravity, and evaporates into space. The oceans become gradually oxidized relative to the more reduced conditions of the mantle.\u201dLanes cites this paper \u201cOn the origins of cells: A hypothesis for the evolutionary transition from abiotic to nucleated cells\u201d, 2003, by Martin and Russellhttps://pubmed.ncbi.nlm.nih.gov/12594918/Am I missing something? This text forces me to assume that solar UV splitting water is the cause of the O2 atmospheric flooding.\n \nreply"
    ],
    "link": "https://eos.org/science-updates/how-great-was-the-great-oxidation-event",
    "first_paragraph": "Eos\n\t\t\t\t\tScience News by AGU\t\t\t\tIf water is the key to life, then oxygen is the key to animal life. All animals breathe oxygen. Despite decades of research, however, scientists still don\u2019t know when Earth\u2019s atmosphere held enough free oxygen to support the planet\u2019s early animals. Most geologists agree that oxygen first accumulated in the atmosphere around 2.4 billion years ago. But they don\u2019t agree on how much there was at that time or if it was enough for animals to thrive.My colleagues and I recently found new clues to help answer these questions from an unlikely source: the acidic, metal-rich waters of Rio Tinto in southern Spain. The composition of these waters is considered extreme today, yet the sort of acid rock drainage that causes these conditions was widespread long ago, when newly available atmospheric oxygen first began interacting with sulfur minerals on land.In our work, we showed that the chemistry occurring in these acidic waters can reconcile seemingly contradictory es"
  },
  {
    "title": "Etleap (YC W13) Is Hiring a Customer Success Manager (SF) (etleap.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-08-01T01:02:01",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://etleap.com/careers/customer-success-manager/",
    "first_paragraph": "Etleap is a leading provider of data integration solutions, empowering organizations to effortlessly centralize, transform, and manage their data for enhanced analytics and decision-making. Our innovative platform enables seamless data integration across disparate sources and democratization of data access throughout the organization. Etleap is proudly backed by First Round Capital, Liquid 2, SV Angel, Y Combinator, and a number of other top-tier investors.We are looking for a dynamic customer relationship professional to execute Etleap\u2019s customer success strategy and ensure customers are receiving the tools and support needed to achieve their goals, resulting in optimal customer success, engagement and retention.Etleap is not just shaping the future of data integration; we're also building a team passionate about making a significant impact. If you're driven, entrepreneurial, and thrive in fast-paced environments, we're eager to have you. Apply now to be part of our mission to redefin"
  },
  {
    "title": "macOS in QEMU in Docker (github.com/sickcodes)",
    "points": 546,
    "submitter": "lijunhao",
    "submit_time": "2024-07-31T04:51:12",
    "num_comments": 143,
    "comments_url": "https://news.ycombinator.com/item?id=41116473",
    "comments": [
      "The only chance at GPU acceleration is passing through a supported dGPU (>= AMD RX 6xxx @ 14.x, no chance modern nvidia) with PCI passthrough. Intel iGPUs work up to Comet lake, and some Ice Lake, but anything newer will not work.Apple Silicon build of MacOS probably not going to be emulatable any time soon, though there is some early work in booting ARM darwinAlso Intel VT-x is missing on AMD, so virtualization is busted on AMD hosts although some crazy hacks with old versions of virtualbox can make docker kind of work through emulation\n \nreply",
      "> Also Intel VT-x is missing on AMD, so virtualization is busted on AMD hostsWouldn\u2019t that work with AMD-V?\n \nreply",
      "Nope. There's only ever been Intel x86 apple computers so x86 mac software is Intel specific. Most things work fine on AMD, but some things don't work without hacks, such as digital audio workstations, some adobe applications etc. And you can't run hypervisors on an AMD hackintosh, the work around for docker is to install an old version of virtualbox and make it emulate instead.\n \nreply",
      "Yeah I would expect it too. As far as I know, AMD has had better luck with hackintoshes and VMacs.\n \nreply",
      "AMD is far more complicated than Intel based machines in this regard. There's never been an apple computer with an AMD CPU...\n \nreply",
      "Related:Docker-OSX: Run macOS VM in a Docker - https://news.ycombinator.com/item?id=34374710 - Jan 2023 (110 comments)macOS in QEMU in Docker - https://news.ycombinator.com/item?id=23419101 - June 2020 (186 comments)\n \nreply",
      "I did an interview with Sick Codes a while back where he talked about his approach to this product: https://www.vice.com/en/article/akdmb8/open-source-app-lets-...Also wanna point out the existence of OSX-PROXMOX, which does something similar for Proxmox home servers: https://github.com/luchina-gabriel/OSX-PROXMOXI\u2019ve personally been using the latter on my HP Z420 Xeon; it\u2019s very stable, especially with GPU passthrough.\n \nreply",
      "This would be awesome to run iCloud sync on my homeserver.  Currently, there is no good way to physically backup iCloud on a homeserver/nas, because it only runs on windows/apple.\n \nreply",
      "This might assist you in syncing this data and then either storing locally or pushing elsewhere for backups:https://github.com/steilerDev/icloud-photos-synchttps://github.com/icloud-photos-downloader/icloud_photos_do...\n \nreply",
      "How would this help with that?  What would this let you do that's different than just rsync'ing your iCloud folder from a connected Mac/PC to your NAS?\n \nreply"
    ],
    "link": "https://github.com/sickcodes/Docker-OSX",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Run macOS VM in a Docker! Run near native OSX-KVM in Docker! X11 Forwarding! CI/CD for OS X Security Research! Docker mac Containers.\n      Run Mac OS X in Docker with near-native performance! X11 Forwarding! iMessage security research! iPhone USB working! macOS in a Docker container!Conduct Security Research on macOS using both Linux & Windows!The Discord is active on #docker-osx and anyone is welcome to come and ask questions, ideas, etc.\n\nOr reach out via Linkedin if it's private: https://www.linkedin.com/in/sickcodesOr via https://sick.codes/contact/This project is maintained by Sick.Codes. (Twitter)Additional credits can be found here: https://github.com/sickcodes/Docker-OSX/blob/master/CREDITS.mdAdditionally, comprehensive list of all contributors can be found here: https://github.com/sickcodes/Docker-OSX/graphs/contributorsBi"
  },
  {
    "title": "After 10 years, Yelp gave my app 4 days (observationalhazard.com)",
    "points": 394,
    "submitter": "WoodenChair",
    "submit_time": "2024-07-29T23:09:58",
    "num_comments": 205,
    "comments_url": "https://news.ycombinator.com/item?id=41104597",
    "comments": [
      "I built a service around helping podcasters automatically convert their audio podcast into a YouTube channel. I went through tons of review with Google in order to get access to the YouTube API and make sure everything I was doing was in compliance with their terms - literally months of back and forth. I had been testing in my development and staging environments against their API for 6+ months. I launched in production, got a few videos uploaded to YouTube, and they disabled my API key. I spent months emailing them and never got anything more than the same boilerplate copy/pasted answer. I could have pivoted or something, but I just shut it down and moved on.  Lesson learned.\n \nreply",
      "I used to not bother with APIs and instead use selenium to drive the websites. It neatly avoided all of the politics around API access.\n \nreply",
      "That may be a CFAA violation, a felony in the United States. See Ryanair DAC v. Booking Holdings, Inc.\n \nreply",
      "That's because Booking was also committing some type of misrepresentation and taking revenue away from Ryanair through their browser automation. Even then, the infraction was sooo bad that they got a $5k fine.I think the hiQ Labs vs LinkedIn case is a better representation that scraping is generally allowed: https://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn\n \nreply",
      "This literally made me lol. Not sure if it\u2019s true. I might be true. But come on!\n \nreply",
      "Their loss of additional content, traffic and ad revenue.\n \nreply",
      "The impact on google losing $10m of revenue is exactly equal to the impact of John Doe losing $10m of revenueIn a competitive market John Doe would go to one of googles competitors. We don't have a competitive market. We have  winner takes all market.\n \nreply",
      "Sure, its $10m today, but think of all the other potential startups that they're doing this to, and how many millions they're throwing away, and billions over the long haul. Reddit, Twitter, and company all got big because they had open APIs and people were able to use them extensively for really creative things.I agree, Google has swallowed up the video streaming market unfortunately.I keep thinking back to how Vine was basically TikTok, and they threw it away.\n \nreply",
      "It's a testimate to the health of our free market that the company throwing away millions, and billions over the long haul, is the overwhelmingly dominant market leader.\n \nreply",
      ">Sure, its $10m today, but think of all the other potential startups that they're doing this to, and how many millions they're throwing away, and billions over the long haul.Google made $300B last year. \"billions over the long haul\" is a lot of money, an unimaginable amount even, to you and I. But to Google?\n \nreply"
    ],
    "link": "http://www.observationalhazard.com/2024/07/after-10-years-yelp-gave-my-app-4-days_29.html",
    "first_paragraph": "After 10 years, I have removed from sale my macOS app for figuring out where to eat, Restaurants. In this post I\u2019ll explain what Restaurants was, how I developed it, and why it was removed from sale. It speaks to the challenges developers face when incorporating third-party APIs.The year was 2014 and Apple had recently released a beta of its new programming language, Swift. As an exercise for an early adopter of the language, I thought it would be fun to leverage my experience in a couple startups using the Yelp API to make a restaurant-search-specific Yelp client for the Mac.I emailed Yelp, explained the app I intended to build, and got permission. The person in developer relations said since they didn\u2019t have a native Mac client, they would approve my use of the Yelp API for Restaurants. In fact, without me specifically asking for it, they provided a 25,000 per day API call limit (a limit my app never even came close to reaching). It seemed they were in fact encouraging me to finish t"
  },
  {
    "title": "Amazon's exabyte-scale migration from Apache Spark to Ray on EC2 (amazon.com)",
    "points": 138,
    "submitter": "nojito",
    "submit_time": "2024-07-29T22:14:51",
    "num_comments": 53,
    "comments_url": "https://news.ycombinator.com/item?id=41104288",
    "comments": [
      "I'm one of the creators of Ray. A few thoughts :)1. This is truly impressive work from AWS. Patrick Ames began speaking about this a couple years ago, though at this point the blog post is probably the best reference. https://www.youtube.com/watch?v=h7svj_oAY142. This is not a \"typical\" Ray use case. I'm not aware of any other exabyte scale data processing workloads. Our bread and butter is ML workloads: training, inference, and unstructured data processing.3. We have a data processing library called Ray Data for ingesting and processing data, often done in conjunction with training and inference. However, I believe in this particular use case, the heavy lifting is largely done with Ray's core APIs (tasks & actors), which are lower level and more flexible, which makes sense for highly custom use cases. Most Ray users use the Ray libraries (train, data, serve), but power users often use the Ray core APIs.4. Since people often ask about data processing with Ray and Spark, Spark use cases tend to be more geared toward structured data and CPU processing. If you are joining a bunch of tables together or running SQL queries, Spark is going to be way better. If you're working with unstructured data (images, text, video, audio, etc), need mixed CPU & GPU compute, are doing deep learning and running inference, etc, then Ray is going to be much better.\n \nreply",
      "> this is not a typical ray use caseMust be good enough if you're willing to dogfood it though?\n \nreply",
      "I'm just learning about this tool now and had a brief question if you have the time:The paper mentions support for zero-copy intranode object sharing which links to serialization in the Ray docs - https://docs.ray.io/en/latest/ray-core/objects/serialization...I'm really curious how this is performant - I recently tried building a pipeline that leveraged substantial multiprocessing in Python, and found that my process was bottlenecked by the serialization/deserialization that occurs during Python multiprocessing. Would love any reading or explanation you can provide as to how this doesn't also bottleneck a process in Ray, since it seems that data transferred between workers and nodes will need to serialized and deserialized.Thanks in advance! Really cool tool, hopefully I'll be able to use it sooner rather than later.\n \nreply",
      "Your right that the serialization / deserialization overhead can quickly exceed the compute time. To avoid this you have to get a lot of small things right. And given our focus on ML workloads, this is particularly important when sharing large numerical arrays between processes (especially processes running on the same node).One of the key things is to make sure the serialized object is stored in a data format where the serialized object does not need to be \"transformed\" in order to access it. For example, a numpy array can be created in O(1) time from a serialized blob by initializing a Python object with the right shape and dtype and a pointer to the right offset in the serialized blob. We also use projects like Apache Arrow that put a lot of care into this.Example in more detail:Imagine the object you are passing from process A to process B is a 1GB numpy array of floats. In the serialization step, process A produces a serialized blob of bytes that is basically just the 1GB numpy array plus a little bit of metadata. Process A writes that serialized blob into shared memory. This step of \"writing into shared memory\" still involves O(N) work, where N is the size of the array (though you can have multiple threads do the memcpy in parallel and be limited just by memory bandwidth).In the deserialization step, process B accesses the same shared memory blob (process A and B are on the same machine). It reads a tiny bit of metadata to figure out the type of the serialized object and shape and so on. Then it constructs a numpy array with the correct shape and type and with a pointer to the actual data in shared memory at the right offset. Therefore it doesn't need to touch all of the bytes of data, it just does O(1) work instead of O(N).That's the basic idea. You can imagine generalizing this beyond numpy arrays, but it's most effective for objects that include large numerical data (e.g., objects that include numpy arrays).There are a bunch of little details to get right, e.g., serializing directly into shared memory instead of creating a serialized copy in process A and then copying it into shared memory. Doing the write into shared memory in parallel with a bunch of threads. Getting the deserialization right. You also have to make sure that the starting addresses of the numpy arrays are 64-byte aligned (if memory serves) so that you don't accidentally trigger a copy later on.EDIT: I edited the above to add more detail.\n \nreply",
      "This is probably a naive question, but how do two processes share address space? mmap?\n \nreply",
      "Yeah, mmap, I think this is the relevant line [1].Fun fact, very early on, we used to create one mmapped file per serialized object, but that very quickly broke down.Then we switched to mmapping one large file at the start and storing all of the serialized objects in that file. But then as objects get allocated and deallocated, you need to manage the memory inside of that mmapped file, and we just repurposed a malloc implementation to handle that.[1] https://github.com/ray-project/ray/blob/21202f6ddc3ceaf74fbc...\n \nreply",
      "Super cool to see you here.I've also looked at ray for running data pipelines before (at much much smaller scales) for the reasons you suggest (unstructured data, mixed CPU/GPU compute).One thing I've wanted is an incremental computation framework (i.e., salsa [1]) built on ray so that I can write jobs that transparently reuse intermediate results from an object store if their dependents haven't changed.Do you know if anyone has thought of building something like this?[1] https://github.com/salsa-rs/salsa\n \nreply",
      "Other folks have built data processing libraries on top of Ray: Modin and Daft come to mind.But I'm not aware of anything exactly like what you're referring to!\n \nreply",
      "Curious if you know how well Ray works with multithreaded python libraries? For example, when using jax with ray, I have to ensure the import ordering imports ray first, as forking a threaded process leads to deadlocks in Python. Do you know how to ensure that ray handles forking the python interpreter correctly?\n \nreply",
      "Multi-threaded libraries (e.g., numpy and PyTorch on CPUs come to mind) are well supported. In scenarios where many processes are each running heavily multi-threaded computations, it can help to pin specific processes to specific cores (e.g., using tools like psutil) to avoid contention.The scenario where a Ray task forks is probably not very well supported. You can certainly start a subprocess from within a Ray task, but I think forking could easily cause issues.You can definitely use Ray + Jax, but you probably need to avoid forking a process within a Ray worker.\n \nreply"
    ],
    "link": "https://aws.amazon.com/blogs/opensource/amazons-exabyte-scale-migration-from-apache-spark-to-ray-on-amazon-ec2/",
    "first_paragraph": "Large-scale, distributed compute framework migrations are not for the faint of heart. There are backwards-compatibility constraints to maintain, performance expectations to meet, scalability limits to overcome, and the omnipresent risk of introducing breaking changes to production. This all becomes especially troubling if you happen to be migrating away from something that successfully processes exabytes of data daily, delivers critical business insights, has tens of thousands of customers that depend on it, and is expected to have near-zero downtime.But that\u2019s exactly what the Business Data Technologies (BDT) team is doing at Amazon Retail right now. They just flipped the switch to start quietly moving management of some of their largest production business intelligence (BI) datasets from Apache Spark over to Ray to help reduce both data processing time and cost. They\u2019ve also contributed a critical component of their work (The Flash Compactor) back to Ray\u2019s open source DeltaCAT projec"
  },
  {
    "title": "Tell HN: I am going to host \"Real Analysis\" book club meetings",
    "points": 59,
    "submitter": "susam",
    "submit_time": "2024-07-27T18:19:30",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=41088355",
    "comments": [
      "Since the link in the \"Tell HN\" post is not clickable, here is a clickable link: https://susam.net/cc/real-analysis/\n \nreply",
      "Can we do Newton's philosophia naturalis?\n \nreply",
      "Anyone looking at Real Analysis might look at Terry Tao's book, which is both conversational and friendly but also rigorous, starting from set theory and building all the way up.\n \nreply",
      "What is it called?\n \nreply",
      "Analysis 1/2.\n \nreply",
      "That seems like a rational title.\n \nreply",
      "Get real.\n \nreply",
      "You are approaching the limit of good taste\n \nreply",
      "Excellent idea.  Real Analysis was the hardest course I took back in college.  Too bad I've lost the book over the years.  It's an old edition.\n \nreply",
      "Just curious, did you take other higher level math classes (complex analysis, topology, etc)? If yes, do you think they are easier?\n \nreply"
    ],
    "link": "item?id=41088355",
    "first_paragraph": ""
  },
  {
    "title": "Skribilo: The Document Programming Framework (nongnu.org)",
    "points": 23,
    "submitter": "Tomte",
    "submit_time": "2024-07-31T19:23:51",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=41122509",
    "comments": [
      "I really like scheme. Strongly in favour of prefix notation. Still cannot imagine having the patience to write significant amounts of prose in this layout when commonmark -> s-expressions or XML or HTML is readily available.(item [Packages are available to produce: ,(ref :skribe\n%user-sui :ident \"slides\" :text [slides]) (overhead transparencies),\n,(ref :skribe %user-sui :ident \"pie-charts\" :text [pie charts]), ,(ref\n:skribe %user-sui :ident \"equations\" :text [equation formatting]), ,(ref\n:skribe %user-sui :ident \"programs\" :text [syntax highlighting of\ncomputer programs]), and others.])The signal to noise there is not good enough to compete. There's way too much repetition of annotation words.The links to markup language and the simpler syntax were expected to tell me what the syntax is, not what a markup language is. Maybe the simpler syntax is more workable - the page doesn't show what that looks like.edit found it in the docs, looks a lot more like markdown. https://www.nongnu.org/skribilo/doc/user-3.html#skribe-synta..., can't seem to link to the subheading.So maybe this is a good thing with a landing page that drives newcomers away? Quite on brand for scheme if so.\n \nreply",
      "Unless we have linters support for various IDEs and tools, it gets extremely annoying to keep adding tabs/spaces wherever required. Markdown shines in this aspect as I can take a basic vi and create a document that gets formatted to acceptable levels.\n \nreply",
      "I'm sorry but this looks terrible.This is an example on the main page:  (p [Skribilo is a ,(freedom [free]) document production tool that\n  takes a structured document representation as its input and renders that\n  document in a variety of output formats: HTML and ,(info [Info]) for\n  on-line browsing, and ,(lout [Lout]) and ,(latex [LaTeX]) for\n  high-quality hard copies.])\n\n...imagine trying to write like this. It's essentially the opposite of what Markdown accomplished, which was taking a bunch of human conventions which were developed mostly out of ease of use (table formatting being the one which doesn't work well IMO) and adapting them so they can be parsed into nice text.Whereas this takes the most awkward possible syntax and completely ruins what you're looking at.\n \nreply",
      "To fix the formatting there:  121:    (p [Skribilo is a ,(freedom [free]) document production tool that\n  122: takes a structured document representation as its input and renders that\n  123: document in a variety of output formats: HTML and ,(info [Info]) for\n  124: on-line browsing, and ,(lout [Lout]) and ,(latex [LaTeX]) for\n  125: high-quality hard copies.])\n\nThe numbers on the left are line numbers, not part of the code.\n \nreply",
      "Just noticed that (HN formatting for quote embedding always surprises me).But I did know they were line numbers, I just wasn't going to strip them out - my point was that the scheme-style brackety inline syntax looks atrocious to try and write, follow or manage compared to the alternatives when the target is \"documents\" not \"programming\". Documents are first and foremost about content production.\n \nreply"
    ],
    "link": "https://www.nongnu.org/skribilo/",
    "first_paragraph": "Skribilo is a free document production tool that\ntakes a structured document representation as its input and renders that\ndocument in a variety of output formats: HTML and Info for\non-line browsing, and Lout and LaTeX for\nhigh-quality hard copies.The input document can use Skribilo's markup\nlanguage to provide information about the document's structure, which\nis similar to HTML or LaTeX and does not require expertise.\nAlternatively, it can use a simpler, \u201cmarkup-less\u201d format that borrows from Emacs'\noutline mode and from other conventions used in emails, Usenet and\ntext.Last but not least, Skribilo can be thought of as a complete\ndocument programming framework for the Scheme\nprogramming language that may be used to automate a variety of\ndocument generation tasks.  Technically, the Skribilo language/API is an\nembedded domain-specific language (EDSL), implemented via\nso-called \u201cdeep embedding\u201d.  Skribilo uses GNU Guile 3.0 or 2.x\nas the underlying Scheme implementation.Releases are avail"
  },
  {
    "title": "`find` + `mkdir` is Turing complete (ogiekako.vercel.app)",
    "points": 317,
    "submitter": "thunderbong",
    "submit_time": "2024-07-31T02:22:41",
    "num_comments": 82,
    "comments_url": "https://news.ycombinator.com/item?id=41115941",
    "comments": [
      "From the top of the page:find + mkdir is Turing complete (retracted)\nThe proof is flawed and I retract the claim that I proved that find + mkdir is Turing complete. See https://news.ycombinator.com/item?id=41117141. I will update the article if I could fix the proof.\n \nreply",
      "So can you implement Folders with it?https://www.danieltemkin.com/Esolangs/Folders/\n \nreply",
      "> In Windows, folders are entirely free in terms of disk space! For proof, create say 352,449 folders and get properties on it.To be that guy for a moment: well, hackchewally\u2026\u00b9Directory entries do take up space in the MFT, but that doesn't show up in explorer which is only counting allocated blocks elsewhere. You will eventually hit a space issue creating empty directories as the MTF grows to accept their allocation.You can do similar tricks with small files. Create an empty text file and check, it will show 0 bytes size and 0 bytes on disk. Put in ~400 bytes of text and check again: explorer will show 400 bytes in length but 0 size on disk because the data is in the directory entry in the pre-allocated MFT. Double up that data, and it will be big enough that a block on the disk is allocated: in properties in Explorer you'll now see 800 bytes length and 4,096 bytes (one block) on disk. Drop it back to 400 bytes and it won't move the data back into the MFT, you'll now see 400 bytes length, 4096 bytes consumed on disk.--[1] though don't let this put you off enjoying the splendid thing overall!\n \nreply",
      "Remember that esolangs are arguably the \"purest\" medium of artistic expression for programming, and that artworks often are about challenging or confronting implicit assumptions. Or to put it differently: yes, that is indeed the joke :).Thank you for elaborating the technical details though (plus I did not know the small file trick, that's a neat bit of trivia).\n \nreply",
      "Also MFT can also grow in size if you exceed the current allocated size.  Depending on your version of windows that growth rate is different.  Also once the MFT grows it will not shrink.  The tools for MFT cleanup are rather poor.  With the usual recommendation of 'just format a new drive and start over'.\n \nreply",
      "Apple finally managed to switch its file system and escape the pains of late 80s technology. When will Microsoft replace NTFS?\n \nreply",
      "\"Don't touch if it works\" (c) /s\n \nreply",
      "Master File Table\n \nreply",
      "\"Disk manufacturers hate this simple trick for free data storage.\"\n \nreply",
      "Oh, that might make a great April 1st release: a mirror filesystem module for WinFSP that splits files into 500 byte chunks on disk. \u201cSee, we saved that 4Mbyte photo to the new filesystem, and it, using the NTFS infinite space for small files trick, made it take absolutely zero disk space! Now we'll make a sub-folder and drop a few more files in that, and look, they show as taking no space in the magic backing store but can be properly opened as normal!\u201d.Drop it on youtube or tt, get your popular friends (this might scupper me, if anyone I know is an online influenza they have the good sense not to let me know about such proclivities!) to make a review of it, and see how far and wide it spreads with people either in on the joke or idiots just parroting it for views.\n \nreply"
    ],
    "link": "https://ogiekako.vercel.app/blog/find_mkdir_tc",
    "first_paragraph": ""
  },
  {
    "title": "Crafting Interpreters with Rust: On Garbage Collection (tunglevo.com)",
    "points": 173,
    "submitter": "amalinovic",
    "submit_time": "2024-07-30T12:57:18",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=41108662",
    "comments": [
      "In case the author reads this: please explicitly cite all of Nystrom's figures. A link is not enough.Even with a citation, I'm not quite comfortable just reusing someone else's figures so many times when they're doing so much heavy lifting. But an explicit citation is the minimum.\n \nreply",
      "Thanks for the suggestion. I updated it with more explicit citations. After seeing your comment, I just looked around and saw that no license was given for the images.I should probably draw my own.\n \nreply",
      "idk if he/she change it, but i see the name big and center after each use.\n \nreply",
      "Crafting Interpreters is such an amazing work.There's at least one other Rust implementation of lox that I know of (https://github.com/tdp2110/crafting-interpreters-rs) (no unsafe)It's always interesting to see how different people approach the problems in their own language or relative isolation. I agree with others here, the real value of the original work lies in avoiding copy and paste.\n \nreply",
      "There are a whole pile of Lox implementations in Rust (as well as many other lanugages):https://github.com/munificent/craftinginterpreters/wiki/Lox-...\n \nreply",
      "Lox must have the highest ratio of (implementations : production usage) of any language on the planet. And I mean that as the highest praise -- it's proven a fantastic teaching language, and your book is infectious at encouraging others to jump in and follow along in various different languages.I've also found the exercise of implementing Lox in another language as highly instructive in learning how to write idiomatic code in that language. I continue to learn more about the best way to express patterns as I work on my own implementation. I'd recommend the journey to any professional developer for this side-benefit alone.\n \nreply",
      "> Lox must have the highest ratio of (implementations : \n   > production usage) of any language on the planet.\n\nIt's probably up there, for sure!  But I'd guess that there are a million toy Lisp implementations, and more people are interested in writing a FORTH interpreter than actually using one in production.  So I'd guess if we tried to get statistics it wouldn't be at the top.Though there's probably a similar claim to be made for the Monkey-language from Torsten Bell, via his books on compilers and interpreters.https://monkeylang.org/\n \nreply",
      "> Lox must have the highest ratio of (implementations : production usage) of any language on the planet.Maybe! It's definitely getting there. I suspect \"semi-arbitrary subset of C\" still has me beat but who knows for how much longer.\n \nreply",
      "All praise Bob!On a more serious note, have you thought about trying to aim lightning at the same spot again and write another book about implementing something most programmers take for granted?\n \nreply",
      "I've definitely thought about writing a third book. I don't know if it would be about \"something most programmers take for granted\". I'm more interested in writing about whatever happens to excite me the most at that time.\n \nreply"
    ],
    "link": "https://www.tunglevo.com/note/crafting-interpreters-with-rust-on-garbage-collection/",
    "first_paragraph": "\nprogramming language\n\n#crafting interpreters\n#garbage collector\n#interpreter\n#lox\n#memory management\n#mark and sweep\n#programming language\n#reference counting\n#rust\n#unsafe\n#virtual machine\nTL;DR Here\u2019s the link to the project.I became interested in implementing programming languages a few years ago and discovered Crafting Interpreters by Bob Nystrom. At the time, I had experience with Rust and decided to use it to follow the book. Albeit, being a noob, I managed to implement a fully functional bytecode interpreter that supported every feature of the Lox language as described. However, my implementation suffered memory leaks due to reference counting. Back then, I didn\u2019t fully grasp Rust to design and implement a proper garbage collector (GC)! Now that I have more confidence in the language, I decided to revisit the project and improve its memory management scheme.As for the goals, I aim to implement a mark-and-sweep GC that is on par with the C implementation. You probably guessed th"
  },
  {
    "title": "Creativity fundamentally comes from memorization? (shwin.co)",
    "points": 314,
    "submitter": "shw1n",
    "submit_time": "2024-07-30T22:37:24",
    "num_comments": 256,
    "comments_url": "https://news.ycombinator.com/item?id=41114825",
    "comments": [
      "OP and others here are stretching the definition of \u201cmemorize\u201d to mean \u201canything that leads to something being retained in memory.\u201d I reject this idea.The trauma of burning your hand on a hot pan creates a memory you won\u2019t soon forget, but almost no one would understand it as an act of memorization.Memorization to me refers to a set of cargo-culty \u201clearning\u201d practices wherein we believe that by using language to drill exposure to an abstract representation of a concept, that somehow we will absorb the concept itself.We do this mainly because experts suck at empathizing with learners and fail to understand that the symbol has meaning for them but not for the learner.It\u2019s the difference between drilling vocabulary flashcards and actually reading, listening, or talking to someone.Young children do not use vocab flashcards to learn their L1. They aren\u2019t being \u201cdrilled\u201d to learn \u201cmama.\u201d They have actual needs in an actual social context and attend to nuanced details of that context to make complex statistical inferences about the world, their perceptions, and their body. Mostly subconsciously.Yes, there are specific areas where drilling can help us accelerate or catch up. Many kids seem to need explicit phonetics instruction in order to make the leap to reading words. Phonological speech interventions are often drill-like. Practicing musical scales does make you more fluent in improvisation. Drilling the mechanics of a repertoire piece frees your mind to focus on higher-order expression and interpretation. They\u2019re valuable, they have a place.But this is just a small slice of learning. It\u2019s disproportionately important for passing tests (And getting hired at tech companies!), which to me is the crux of the issue.If I had to reformulate OP\u2019s argument to something I can get behind, it would be more about deliberate practice or \u201cputting in the reps.\u201d This is also often boring, and differentiates highly successful people from average performers. But it\u2019s a broader and more purposeful set of activities than \u201cmemorization\u201d would imply.\n \nreply",
      "> We do this mainly because experts suck at empathizing with learners and fail to understand that the symbol has meaning for them but not for the learner.I hear you; but teaching deep expertise is really hard. We can use your example of a child learning their first language. They will really understand it. But people are famously, hilariously terrible at teaching their native tongue. We know how to conjugate, and how to use verbs and adverbs and all the rest. But it\u2019s all intuitive - we have no symbolic understanding of it. If that\u2019s the case, we can\u2019t explain it in words.Here\u2019s a weird fact: if you look around the room you\u2019re in now, I bet you know what it would feel like on your tongue to lick everything you see. We probably learned that in the \u201cput everything in your mouth\u201d baby phase.You are an expert. But if you wanted to, how would you teach that? I think the learner would just have to go lick a lot of things for themself.I believe a lot of real learning is actually like that. When I taught programming, I think I was a frustrating teacher. My students would ask things like \u201cwhat\u2019s the best way to structure this program?\u201d And I would say \u201cI don\u2019t know. Let\u2019s brainstorm a few ways then you should pick at least one and try writing it like that. Figure it out in code.\u201d I think you become great at programming by licking all the programs you can find. Same with music and art and languages (go have conversations with native speakers).There is only so much the best teacher can teach. Sometimes you just have to walk around licking things.\n \nreply",
      "Yes. Tacit knowledge is hard to teach.\n \nreply",
      "Right. And, I think, real expertise in any subject is choc full of tacit knowledge. Even - and especially - in areas where we have good symbolic representations. (Like music, math, programming and languages.)\n \nreply",
      "The EEs I have known that carried around a card with:    V = I * R\n    I = V / R\n    R = V / I\n\nbecause they couldn't remember it were all bad at EE and bad at math.If you can't remember the pieces making up a concept, how are you going to remember the concept?> It\u2019s disproportionately important for passing tests (And getting hired at tech companies!)I don't remember anyone who couldn't pass tests but was really a great engineer.BTW, one of the tests fighter pilots go through is they are blindfolded, and then have to put their hand on each control the instructor calls out.I also have some written tests for certifying pilots. There are questions like max takeoff weight, fuel burn rate, max dive speeds, etc. Stuff a pilot had better know or he's a dead pilot.\n \nreply",
      "> because they couldn't remember it were all bad at EE and bad at math.> If you can't remember the pieces making up a concept, how are you going to remember the concept?You touch upon the different levels of knowing it. Yes, having to carry a card with the formulas on it shows no knowledge. But, if you have to memorize the formulas, your knowledge is still not adequate. You're just regurgitating a formula that you memorized so you can plug in numbers. You don't understand the \"why\" of Ohm's law. Of course voltage is equal to current times resistance, it's obvious by what these things are! It should be as self-evident as \"Of course distance is equal to speed times time!\"Another example: You can have the Lorentz transformation formulas memorized but still not really understand the \"why\" of Special Relativity.\n \nreply",
      "> There are questions like max takeoff weight, fuel burn rate, max dive speeds, etc. Stuff a pilot had better know or he's a dead pilot.You don't want a pilot who is creative when it comes to max takeoff weight.Obviously there are good reasons to memorize certain things, creativity just isn't one of them.\n \nreply",
      ">You don't want a pilot who is creative when it comes to max takeoff weight.In certain combat situations, or when smuggling coke across South and Central America, you certainly do.\n \nreply",
      "Gravity works just as well if you're in combat or smuggling coke. They don't suddenly give you the ability to takeoff under too much load\n \nreply",
      "Sure, there are hard laws of physics.Until you get to those, you'd be surprised how far some creativity with weight distribution, getting rid of unneeded cargo or even plane parts, using stuff to your advantage, and a little daring to push the plane to its limits, goes...Way beyond what the \"by the book\" pilot who isn't creative can achieve in times of need.\n \nreply"
    ],
    "link": "https://shwin.co/blog/creativity-fundamentally-comes-from-memorization",
    "first_paragraph": "July 2024I'm often made fun of for bringing a \"system\" into creative outlets.Things like:But I think this critique misunderstands what creativity truly is: a flash of inspiration connecting internalized concepts.The inspirational lightning bolt writers and artists experience can't happen unless they know how to write or draw. A pun can't be created unless the author sees the similarity between one word and another they already know. A DJ can't mashup two songs unless they're familiar with both.By definition, you can't even be certain of novelty without familiarity of existing works.Creativity comes to those who have internalized the patterns of their art -- they can see the connection or novelty because it's all in their head.Therefore autonomy enables creativity, and a system helps achieve autonomy quicker.Some time back I discovered a method for learning anything quickly.It involved two steps:For many that remember school, this won't sound novel. Yet what's often missed is the applic"
  },
  {
    "title": "Construction of the AT&T Long Lines \"Cheshire\" underground site (coldwar-ct.com)",
    "points": 168,
    "submitter": "walrus01",
    "submit_time": "2024-07-31T03:47:42",
    "num_comments": 81,
    "comments_url": "https://news.ycombinator.com/item?id=41116253",
    "comments": [
      "That's an AUTOVON switching center.[1] There were at least 38 of those centers in the US. They were located in places some distance from major cities and military targets. They were hardened telephone central offices, but with many more redundant links between switches than the commercial system. So this system really was intended to survive a nuclear war.The technology was Western Electric's 1ESS (#1 Electronic Switching System), and all 4-wire out to the handsets, so that conference calls would work clearly without feedback. 1ESS was a very bulky system. It was basically a pair of large mainframe computers running a big dumb switch fabric. The switch fabric is analog and electromechanical, using reed switches with a ferrite element so they stay in the last state to which they were set. That's why these were such big installations, even though they didn't have a huge number of lines.[1] http://autovon.org\n \nreply",
      "The underground sites were primarily for Long Lines equipment, i.e. L-carrier Coaxial repower, regen, cross-connect and HVAC and power continuity for these.  You can see in the illustration that switching at its least efficient was maybe 1/4 of the facility (lowest level), L (and TD MW) would be a bit more dense but similar floor space on first level.  These multistory sites had a lot of extra room for training rooms, service bureau, and some nod toward continuity in terms of sheltering a number of people with some token supplies although if you look at enough pictures over time it doesn't appear like it was ever taken very seriously... ultimately I think these just turned out to be a way AT&T and the DoD came up with to get the US Government to more heavily subsidize Long Lines network construction.Switching of copper end lines would often happen closer to the user, i.e. on base although some sites did have switching due to favorable proximity (i.e. Soccoro, N.M) or presumably function like a tandem (maybe this site?).  You can see a little of a 1A ESS in this video (https://www.facebook.com/CheshireVolFireDept/videos/a-brief-...) and maybe some 5ESS in the background as well although it is too brief for me to tell.Some undergrounds were dual purposed for Microwave pathing and cross connect (like this linked one), but most microwave was instead in above ground hardened facilities elsewhere for path diversity.Some undergrounds had Echo Fox transceivers and switching http://www.coldwar-c4i.net/Echo-Fox/index.html.Project Offices are an interesting related rabbit hole to pursue http://www.coldwar-c4i.net/ATT_Project/index.html.Source: I own an L-3 regen bunker and have done a lot of research on them.\n \nreply",
      "> and maybe some 5ESS in the background as well although it is too brief for me to tell.The frame at 2:22 looks to me like it has a 5E in view.\n \nreply",
      "They also had Number 5 Crossbar switches as well, the switching fabric wasn't huge in size, like I've seen what the frames look like, ESS was still much smaller than the crossbar that preceded it, and not that much physically larger than a comparable 5ESS\n \nreply",
      "How deep did they bury the wires?Were they run full depth from point to point?\n \nreply",
      "It would vary by terrain and land use (e.g. agricultural), but generally just 2-3 feet deep by vibratory plow. Deeper emplacement and directional drilling were used as required to handle obstacles. For most L-carrier the entire en-route infrastructure was below ground, but it was more extensive than just the cable, with active repeaters in manholes required at 2-mile intervals for L-4. L-4 also required an \"equalizing repeater\" about every 50 miles, which was installed in a manhole but had a shed on top to facilitate technicians adjusting the equalization. Main stations, such as this one, were required every 150 miles.\n \nreply",
      "Google \"L4 transcontinental cable\", but the majority of the long lines network was the famously known horn antennas on towers for FDD microwave point to point links in the 6GHz band.\n \nreply",
      "I\u2019m surprised they didn\u2019t use crossbar electromechanical switches for EMP resistance.\n \nreply",
      "These AT&T facilities were hardened against EMP to the era's military design standards, and AT&T participated in validation experiments using both actual nuclear detonations and EMP simulators. I've never seen any indication that AT&T thought the electromechanical switches had an advantage in this context, but it's an interesting question if they performed any evaluation. In general, though, EMP protection was done at the \"envelope\" of the facility, and equipment inside of the facility did not need to itself be EMP hardened. Hardening of the facility was achieved mostly by a shielding system embedded in the outside walls, and of course AT&T had already performed considerable research into suppressing transients on the outside plant due to lightning.Crossbar switches were indeed in use in the AUTOVON network for simple scheduling reasons, a number of AUTOVON switches were installed before the 1ESS was ready. Eventually all of the 5XBs were replaced by 1ESS. Some Automatic Electric switches were used at AUTOVON sites outside of Bell territory, these were at least semi-custom (AE just called them \"the AUTOVON switch\") electromechanical machines.\n \nreply",
      "They did - it wasn't all ESShttp://autovon.org/wp-content/uploads/2018/01/BELL-LAB-RECOR...\n \nreply"
    ],
    "link": "https://coldwar-ct.com/Home_Page_S1DO.html",
    "first_paragraph": "The Cheshire ATT facility is an underground complex originally built in 1966. It was an underground terminal and repeater station for the hardened analog L4 carrier cable (coax) that went from Miami to New England carrying general toll circuits and critical military communication circuits.\u00a0It reportedly\u00a0housed an AUTOVON 4-wire switch as part of the switching fabric of that critical global military communications network. Cheshire also connected via terrestrial\u00a0microwave to the major, semi-hardened\u00a0AT&T Durham station which linked to many other sites including paths to New London (Navy Sub base) and to Green Hill, RI to meet a transatlantic cable to Europe.\n\r\nThe Official ATT facility description from 1966:\n\u00a0\u00a0\u00a0\u00a0 The Cheshire central office is a two-story underground, hardened building owned by Lone Lines and located in Cheshire, CT.\r\n\u00a0\u00a0\u00a0\u00a0 This office is manned 24 hours daily.\u00a0 The Plant Manager has a staff comprising four Plant Supervisors and eighteen Craftsmen.\r\n\u00a0\u00a0\u00a0\u00a0\u00a0 Cheshire serves"
  },
  {
    "title": "Porffor: A from-scratch experimental ahead-of-time JS engine (porffor.dev)",
    "points": 421,
    "submitter": "bpierre",
    "submit_time": "2024-07-30T18:55:10",
    "num_comments": 119,
    "comments_url": "https://news.ycombinator.com/item?id=41112854",
    "comments": [
      "Oliver (the main developer) just announced that they\u2019re going to work full time on Porffor: https://x.com/canadahonk/status/1818347311417938237\n \nreply",
      "Financed by defunkt[1], GitHub cofounder and ex CEO, for an undisclosed future project.[1] https://news.ycombinator.com/user?id=defunkt\n \nreply",
      "Also funded the Ladybird browser recently. Seems to like his web.\n \nreply",
      "Not familiar with this stuff at all, is Porffor a js engine that Ladybird could end up using? Or are they still writing their own?\n \nreply",
      "Or just two separate moonshots for now.\n \nreply",
      "Porffor compiles the JS to WASM, so it would be kind of a waste. Though there might be no reason the two projects cannot share some logic, like parsing the JS and such. I kind of doubt this is why its being funded. It sounds like a useful project.\n \nreply",
      "It can also compile to Native\n \nreply",
      "I thought ladybird used WebKit?\n \nreply",
      "No. Completely green field. Well, it _was_. I believe they\u2019ve recently accepted that third party libraries will be allowed in the non-Serenity OS version.\n \nreply",
      "Since js is dynamic you can't compile all ahead of time, so you need an interpreter\n \nreply"
    ],
    "link": "https://porffor.dev/",
    "first_paragraph": "Porffor is a unique JS engine/compiler/runtime, compiling JS code to WebAssembly or native ahead-of-time.It is limited for now; intended for research, not serious use!Porffor's WebAssembly output is much faster and smaller compared to existing JS -> Wasm projects. This is because Porffor compiles JS AOT instead of bundling an interpreter.Due to compiling JS ahead-of-time, Porffor can compile to real native binaries without just packaging a runtime like existing solutions. This leads to:Porffor is safe as it compiles to Wasm (and then native). It is also written in a memory safe language (JS).Porffor is written from the ground-up with AOT in mind instead of being based on any existing JS engine. The only dependency is a JS parser.Porffor supports TypeScript input, no clunky transpiler step needed: just feed it a TS file."
  },
  {
    "title": "Is a 'slow' swimming pool impeding world records? (yahoo.com)",
    "points": 100,
    "submitter": "phkx",
    "submit_time": "2024-07-30T10:49:29",
    "num_comments": 201,
    "comments_url": "https://news.ycombinator.com/item?id=41107811",
    "comments": [
      "I was a college swimmer, qualified for Olympic Trials in 2012 and 2016. There are absolutely slow and fast pools. It basically comes down to two things:1. The depth - which is only 7ft in Paris, unusually shallow for a competition pool.2. The sides. Does the water spill over the sides into the gutters, or smash into a wall and bounce back, creating more chop.A trained eye can see all the swimmers in Paris struggling in their last 10-20 meters (heck, an untrained eye can spot some of these). Bummer that it makes the meet feel slow but at least it generally affects all the swimmers equally\n \nreply",
      "My untrained eye has noticed. But I also think it's not really a big deal. There are so few events where the conditions are exactly the same every 4 years. Just kinda the luck of the draw if you happen to be competing in the most ideal conditions for WR setting in any event\n \nreply",
      "Also, the important bit is fairness for all competitors. As the OP said, the same conditions affect everybody. I have little sympathy for the (few) swimmers complaining. They are not owed a world record and if they\u2019re that good, they\u2019re going to get one anyway.\n \nreply",
      "Wouldn't the sides affect swimmers on the edge lanes next to them more?  And is this one reason why the strongest swimmers are usually placed in the center lanes?\n \nreply",
      "Swimmers on edge lanes get affected more, and they leave the outside lanes unoccupied except for the first stage of qualifying.\n \nreply",
      "From the races I saw they specifically had the edge lanes (left most, right most) empty probably for this reason. I.e. 8 out of 10 lanes are used. I think one of them did have someone in one of those lanes because one of the qualifying heats was a dead heat.\n \nreply",
      "Correct.\n \nreply",
      "Additionally, anybody good enough to be prevented from setting a world record because of this pool will undoubtedly have multiple chances in other competitions with faster pools.\n \nreply",
      "> But I also think it's not really a big deal.the difference of the resulting turbulence from the wave bounced back from the bottom surface at 2m here and from the more traditional 3m is a big deal. The water is pushed by the swimmer's hands with the speed of something on the scale of 2 meters per second, so, as the swimmer moves forward, that turbulent movement of the water reflected by the pool bottom may as well come behind the legs in the 3m depth case while in the 2m depth case it would catch the legs decreasing the efficiency of their movement.\n \nreply",
      "Dumb question I never thought about: do the circulation/filtration pumps get turned off during races? And for what minimum time before hand to let things settle?If so, I guess this would be a serious competition only thing because you wouldn\u2019t want them off for hours.\n \nreply"
    ],
    "link": "https://sports.yahoo.com/paris-olympics-2024-is-a-slow-swimming-pool-impeding-world-records-133347713.html",
    "first_paragraph": "Medal table | Olympic schedule | Olympic newsPARIS \u2014 In the men\u2019s 100-meter breaststroke final here at the 2024 Olympics on Sunday, eight world-class swimmers glided through a pool \u2026 and not a single one would have finished better than eighth at the Tokyo Olympics three years ago.In the \u201cRace of the Century\u201d one night earlier, the women\u2019s 400 freestyle, three one-time world record holders all fell well short of their personal bests; Katie Ledecky failed to even break 4 minutes.After two cacophonous days of swimming here at Paris La D\u00e9fense Arena, no world records have fallen, and murmurs have rippled through the sport: Is the pool the problem?If so, many believe, the specific problem is its depth.World Aquatics, swimming\u2019s global governing body, recommends that Olympic pools be 3 meters deep. The pool here in suburban Paris \u2014 a temporary vessel plopped into a rugby stadium, similar to the one built at Lucas Oil Stadium in Indianapolis for last month\u2019s U.S. Olympic trials \u2014 is 2.15 mete"
  },
  {
    "title": "Show HN: Turn any website into a knowledge base for LLMs (embedding.io)",
    "points": 146,
    "submitter": "tompec",
    "submit_time": "2024-07-30T00:54:05",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=41105130",
    "comments": [
      "In my opinion this is a transitional niche.Soon websites/apps whatever you want to call them will have their own built-in handling for AI.It's inefficient and rude to be scraping pages for content. Especially for profit.\n \nreply",
      "In my opinion this is a transitional niche.Soon websites/apps whatever you want to call them will have their own built-in handling for AI.It's inefficient and rude to be scraping pages for content\n \nreply",
      "I spent a lot of time thinking about how to manage embeddings for docs sites. This is basically the same solution that I landed on but never got around to shipping as a general-purpose product.A key question that the docs should answer (and perhaps the \"How it works\" page too): chunking. You generate an embedding for the entire page? Or do you generate embeddings for sections? And what's the size limit per page? Some of our docs pages have thousands of words per page. I'm doubtful you can ingest all that, let alone whether the embedding would be that useful in practice.\n \nreply",
      "I chunk pages and generate embeddings for each chunk. So there's no real size limit per page.\n \nreply",
      "I like this. Abstracting away the management of embeddings and vector database is something I desperately want, and adding in website crawling is useful as well.\n \nreply",
      "I like this a lot!But: I feel the more of these services come to being, the more likely it is that every website starts putting up gates to keep the bots away.Sort of like a weird GenAI take on Cixin Liu's Dark Forest hypothesis (https://en.wikipedia.org/wiki/Dark_forest_hypothesis).(Edited to add a reference.)\n \nreply",
      "Responding just because it's a pet peeve of mine: Cixin Liu did not invent the dark forest hypothesis. People were discussing it, and writing science fiction books about it, for decades before the 3BP books were published. Nothing against him, and he definitely helped popularize the concept, but I think it's incorrect to refer to it as \"Cixin Liu's hypothesis\".\n \nreply",
      "But he is responsible for the name, not the concept. So yes it is Cixin Liu's Dark Forest hypothesis.\n \nreply",
      "Just like Amerigo Vespucci put the name \"America\" on a map and people starting referring to the New World as such, although he didn't discover it himself.\n \nreply",
      "This would be amazing\n \nreply"
    ],
    "link": "https://www.embedding.io/",
    "first_paragraph": ""
  },
  {
    "title": "AMD sold $1B of Instinct GPUs in 2Q, driving 3-digit datacenter growth (theregister.com)",
    "points": 3,
    "submitter": "nabla9",
    "submit_time": "2024-08-01T00:48:18",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.theregister.com/2024/07/31/amd_q2_2024/",
    "first_paragraph": ""
  },
  {
    "title": "Where does the name \"algebraic data type\" come from? (poisson.chat)",
    "points": 126,
    "submitter": "082349872349872",
    "submit_time": "2024-07-26T05:32:05",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=41076021",
    "comments": [
      "I've always thought a nifty thing about algebraic data types is the ability to deduce their size (in the set theoretic sense, the number of possible elements) by algebra. Consider Bool which is a fundamental primitive type with size 2. Now consider the product type Bool \u00d7 Bool \u00d7 Bool (a three-tuple), it's clear that it has size 2\u00d72\u00d72=8. On the other hand if we have the sum type Bool + Bool + Bool (a discriminated union), it's also clear it has size 2+2+2=6. And this works with recursion too. The list type whose element type is A is defined as either Nil or an element plus the remainder of the list: that's exactly L(A)=1+A*L(A). And that directly solves to L(A)=1/(1-A). If you've studied generating functions or even just Taylor series, you know this can be expanded into 1+A+A^2+A^3+\u2026 which perfectly matches the intuition of a list having any number of elements. What if I want to define a special list that only allows even number of elements? It would be L(A)=1+A*A*L(A), or 1/(1-A^2) which expands to 1+A^2+A^4+\u2026 Both of course are \"infinite\" but it allows us to distinguish precisely what kind of \"infinity.\"\n \nreply",
      "This also helps with figuring out isomorphisms, i.e. when two types are equivalent. So for example, if someone tries to represent an \"Either Bool Bool\" (2+2 cases) with three cases, we see immediately there's a singularity in there which represents at least two of the possible values of the original, i.e. we won't be able to losslessly round trip.\n \nreply",
      "The thing about isomorphisms between algebraic structures is that they require a representational+inversible mapping (what you mentioned) that preserves relations (something you didn't mention). That distinction is actually relatively important in programming because you might be using something like polymorphism to operate on two representationally equivalent types in different ways.If you have a base class CollidableObject that both PoolBall and GlobOfHoney extend, they could be representationally equivalent (both having two vector member fields, one for Position and one for Velocity) but with different implementations of CollidableObject::HandleCollision(CollidableObject& target) to reflect the fact that PoolBall can elastically collide with certain kinds of targets but GlobOfHoney only collides inelastically. So you'd not want to treat them as isomorphic.I'm mostly just being pedantic but one of my pet peeves is when programmers use terms like Isomorphism or Vector in a way that is partly but not entirely true to the mathematical definition. I think \"bijection\" is a better term for what you're talking about, because it's both necessary and sufficient for information-preserving mapping between types.\n \nreply",
      "This works with function types too. You can get a hint that objects with signature A -> (A -> A) have an isomorphism with objects of signature (A \u00d7 A) -> A, by observing that the cardinalities are equal as algebraic expressions, ( |A|^|A| )^|A| = |A|^( |A|*|A| ). That one's just \"currying\" [0].[0] https://en.wikipedia.org/wiki/Currying\n \nreply",
      "Yep, that\u2019s one of the more mind-blowing \"tricks\" as it were. Maybe less moreso, but still nifty, is that  |T -> bool| = |bool|^|T| = 2^|T|\n\n\u2026which also happens to be the number of subsets of a set of size |T|. And indeed that\u2019s what a predicate function is, a subset-generator; for every subset there is exactly one predicate that picks the members of that subset from the larger set.\n \nreply",
      "That's true for any type, not just bool right? And is equivalent of to the observation that any pure function can be replaced by a lookup table.\n \nreply",
      "Sure, any function T -> U partitions T into |U| subsets. But the case where |U|=2 is special because 2^N is special in being the number of subsets of a set of size N.\n \nreply",
      "Author here. It's funny you mention this. I was (am still) writing a post about it, and I went on a rant that while it's a (fun!) way to do \"algebra with types\", it's not actually the original meaning of it afaik, and then I went to look for a source for it and that grew into this whole blogpost.\n \nreply",
      "First of all, thank you for writing that up!!!Disclaimer 1:  I have the same last name as Burstall's collaborator, Joseph Goguen.  If we're related, it's VERY distant, so I truly can't add anything.Disclaimer 2: I'm going to tie this back into algebras.Setup 1: However, I have been a little obsessed with Burstall and Goguen lately and I've been SLOWLY working through some papers and trying to make sense of it.  I'm just an F# programmer who likes languages, not an academic.Setup 2: I've known about Burstall/Goguen for some time, but I never understood the gravity of their respective work until recently.  And I have quite literally been annoying with my tech friends and family with all my \"Oh my god!\" moments as I've been going papers and festschrifts.This whole time, I haven't been sure if I've been interpreting what I'm reading correctly, so your post has been wonderful in settling some of that.I understand that some people may resist the idea of describing algebraic data types as analogous to the fundamental theorem of arithmetic for types. Burstall and Goguen were motivated by creating tools and systems built on solid foundations, such as universal algebras, to utilize analytical tools like logic systems and category theory for deeper insights.However, I believe they were particularly focused on how algebraic data types model the cardinality of state spaces and transitions between states for this reason:They were designing a specification system that helped working programmers design systems accomplish two things (In today's parlance):1. Make invalid states impossible.\n2. Make invalid transitions between states impossible.While learning Maude (OBJ's successor), I've been particularly fascinated with how its parent language let you define very fine grained domains.And it makes sense, because if you're going to search through that domain to find counter-examples violating your theories, it makes sense to reduce the search space proactively.But what really drove that for me (and maybe I'm projecting here), is how that seemed to correspond to a little program I wrote that constructed a bijection between the natural numbers and all closed lambda functions.https://gist.github.com/sgoguen/46200419194eb29b82079b0804e5...But what really convinces me that they had that in mind is how Maude and OBJ let you add attributes to constructors to identify them as associative or commutative or whether it has an \"id\" where the impact of these attributes on the constructors actually reduces the state space.Again, I could be projecting here, but I would love to hear your thoughts and I am excited to read your next post!Thank you!\n \nreply",
      "I learned a lot from this and your other comment here!The way I connected the dots when I read about Clear (and OBJ) is that it let me explain free algebras by example, by just showing Clear code. That hopefully makes the concept more accessible than the dry mathy definitions that would have the eyes of most readers glaze over.I must say that I'm not familiar with Burstall and Goguen's line of research beyond what I've written so far. In the future I'd especially like to know what lessons these specification languages can teach (or already taught) about ML-style module systems, OO languages, and formalizations of mathematical structures in proof assistants (Clear reminds me of the little I've seen of Hierarchy Builder in Coq).One concrete question I have is how Clear or OBJ or Maude code is actually meant to be run.\nYour description of these languages reminds me of model checking tools like TLA+/Alloy. Does Maude target a similar niche?\n \nreply"
    ],
    "link": "https://blog.poisson.chat/posts/2024-07-26-adt-history.html",
    "first_paragraph": "\u201cAlgebraic data types\u201d is a beloved feature of programming languages with\nsuch a mysterious name. Where does this name come from?There are two main episodes in this saga: Hope and Miranda.\nThe primary conclusion is that the name comes from universal algebra,\nwhereas another common interpretation of \u201calgebraic\u201d as a reference\nto \u201csums of products\u201d is not historically accurate.\nWe drive the point home with Clear. CLU is extra.Disclaimer: I\u2019m no historian and I\u2019m nowhere as old as these languages\nto have any first-hand perspective.\nCorrections and suggestions for additional information are welcome.Algebraic data types were at first simply called \u201cdata types\u201d.\nThis programming language feature is commonly attributed to\nHope, an experimental applicative language by Rod Burstall et al..\nHere is the relevant excerpt from the paper, illustrating its concrete syntax:A data declaration is used to introduce a new data type along with\nthe data constructors which create elements of that type. For e"
  }
]