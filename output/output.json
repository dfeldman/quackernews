[
  {
    "title": "Avoid ISP Routers (routersecurity.org)",
    "points": 115,
    "submitter": "dp-hackernews",
    "submit_time": "2025-02-01T23:17:36 1738451856",
    "num_comments": 43,
    "comments_url": "https://news.ycombinator.com/item?id=42903576",
    "comments": [
      "I wish. I own my own modem and router, but Comcast won\u2019t let me use them unless I pay a whole bunch of extra fees or accept a stupidly low monthly data cap. I\u2019ve got my router downstream of theirs which is a bit annoying, especially considering their modem-router combo overheats and needs to be rebooted via unplugging power at least once a month.Sadly I have no other options here in San Francisco. My house is not wired for phone service so I cannot get DSL. The various fiber services that are becoming more available in San Francisco are generally only available downtown or large apartment buildings. My freestanding house can\u2019t get any of that. AT&T\u2018s new fiber doesn\u2019t connect to me either. And webpass doesn\u2019t have a good line of sight from my location to any of their microwave towers so I can\u2019t get that. It is Comcast or nothing. It always amazes me that San Francisco is supposedly the tech capital of the world but internet connectivity here is worse than rural China. (And that\u2019s not an exaggeration, I\u2019ve spent plenty of time in rural China and in the mountains there, both the cellular and hardline service is infinitely better than San Francisco, aside from the firewall issues of course)\u2026I guess that turned into a bit of a personal rant but holy crap how is it 2025 and this is still a problem in a major tech city?\n \nreply",
      "We used to be stuck with Comcast, but we had no trouble using our own modem and router.We moved from lower Nob Hill to Russian Hill and were finally able to get fiber from Sonic. We went from ~300Mbps down to 1G (more like 750Mbps) and from $137/month to $50/month. Oh and it\u2019s symmetric, very much unlike cable. So happy to get rid of Comcast.\n \nreply",
      "I live in SF and Comcast doesn't charge me to have my own router.I pay $130 for 1.4gbit and unlimited data. It's expensive but I also have no other choices. Sonic stops only one block over and we haven't been able to convince them to wire up my block.\n \nreply",
      "I'm so jealous of SF internet options.In LA I pay $105 for \"supposedly\" 2-300mbit, but this week I've been seeing 30.I keep looking for alternatives but haven't found any in my area.\n \nreply",
      "I'm surprised by this, is Comcast super regional with it's restrictions?  I have a Comcast 1gig plan in the Bay Area, and last I checked I get a small ($5?) discount for using my own modem.  I've been on the plan for a least a few years now... so alternatively maybe I'm grandfathered in or something?  Or maybe some Comcast sales person was lying to you about your options?\n \nreply",
      "It's not regional. The rental is $15-25/mo these days. You might be grandfathered in.If you choose the $25/mo option, you don't have have to pay to waive the monthly data cap.\n \nreply",
      "You probably live in a zip code where ISP choice is an option l. Thus not getting bent like parent comment\n \nreply",
      "In EU the second poorest country has the fastest internet. The richest country cannot provide cellular signal to all of its area.\n \nreply",
      "The crappiness of national ISPs is a feature, not a bug. ISPs have lobbied at state and federal levels to get their way. In many states, they have lobbied _for_ the ban of municipal ISPs.Then between major ISPs they have under the table agreements to avoid competing in certain areas. This impacts all types of residential areas - suburban, urban, and rural.  I believe it\u2019s much worse in rural areas.Why bother with providing good customer service or improving? They know you have no other choice.Cellular networks functioning as ISPs have provided _some_ relief in this aspect but comes with its own drawbacks (congestion can get bad and you get throttled, and latency tends to be shitty all around).The ideal municipal ISP I have seen is in Chattanooga TN. They (EPB) offer _residential_ customers symmetrical  access starting at  1000 Mbps, up to 25,000Mbps. [1]The 1gig plan is cheaper than GFi er and 2.5G plan is competitive.Plus this money is kept within the ecosystem of this area. Creates high paying jobs. Profits reinvested into network rather than stock buybacks or some C-level executive that \u201csuper commutes\u201d in a private jet.[1] https://epb.com/fi-speed-internet/?#choose-your-plan\n \nreply",
      "cant you just put their crap infront of yours?\n \nreply"
    ],
    "link": "https://routersecurity.org/ISProuters.php",
    "first_paragraph": "In my opinion, you are safest using both a modem and a router that you purchased on your own. That is, avoid  \n  equipment from your ISP. I say this for a number of reasons:   \n The devices shipped by ISPs suffer from a general level of incompetence both in their initial configuration and ongoing maintenance. \n\t  If nothing else, just the fact that an ISP would install a device with the default password, tells you everything you need to know about \n\t  their  interest in your security.\nSecurity and convenience are always at odds with each other. Most likely an ISP will configure a router for maximum convenience to cut down on tech support calls, which cost them money. \nSpying: We have seen that ISPs, at times, co-operate with spy agencies and governments. Even without outside influence, an ISP may well put a \n\t        backdoor in the devices they give to their customers, if for no other reason than to make their life easier in some way. In this article, Your ISP is Probably Spying On Yo"
  },
  {
    "title": "Apple is open sourcing Swift Build (swift.org)",
    "points": 417,
    "submitter": "dayanruben",
    "submit_time": "2025-02-01T16:44:53 1738428293",
    "num_comments": 161,
    "comments_url": "https://news.ycombinator.com/item?id=42899703",
    "comments": [
      "I believe that this long game of Swift being \"good for everything\" but \"better for Apple platforms\" will be detrimental to the language. This does not help the language nor seems to bring more people to the ecosystem.Competitors seems to have a combination of:\n- Being more open-source\n- Have more contributors\n- Have a narrower scopeMaybe they should consider open sourcing all the tooling (like Xcode) otherwise the gap will only grow over time when compared to other languages.\n \nreply",
      "I don't get this reaction.Apple: here, we're open-sourcing this previously closed-source Apple-specific thing that made Swift better on Apple platforms. We're moving the Apple stuff into a plugin so Windows and Linux can be equal peers to Apple in the new system. We've implemented preliminary support for Windows & Linux and plan to continue work to bring them up to parity.Hacker News: I believe that this long game of Swift being \"good for everything\" but \"better for Apple platforms\" will be detrimental to the language. This does not help the language nor seems to bring more people to the ecosystem.Like, what more do you want from them? For them to only open-source Swift Build once they've fully implemented complete parity for Windows and Linux? In the years you'd be waiting for full parity, we'd still see this same kind of comment on every story about swift, asking when they're going to open source a production-level build system.\n \nreply",
      "They should have been fully open source with full linux support and parity since day one.That would actually help the language get traction. At this point it's a dying language.\n \nreply",
      "Is it dying? I think it's still pretty popular for app development isn't it?I was pretty excited to hear that Ladybird is doing a lot of stuff in Swift, because I think it's a pretty decent and fast language and I think it'd be pretty neat to see a browser written in it.\n \nreply",
      "> At this point it's a dying language.I disagree.Source: Someone who has been programming Apple since 1986, and have heard last rites being administered to Apple, many times.\n \nreply",
      "This has been my experience for a long time. Swift is nice but why would I waste my time working on a language that is too tied to the Apple platform even if it's open-source when we have more universal scripting languages like Python, or languages like Kotlin that are compiled but have more support (because I trust JetBrains way more than Apple at the moment), or languages that are most strict like Rust but have more momentum and safety?They painted themselves in a corner. Apple being the best computing platform while trying to please everyone can never be a serious proposition. Either they are the best and everyone uses macOS, or we have to be so careful that any alternative is more interesting that what they propose.\n \nreply",
      "> why would I waste my time working on a language that is too tied to the Apple platformThis might work the other way round: starting from people familiar with macos or ios development who want to write for other platforms.Then the question becomes: why would a developer learn a different open source language when they can use what they already know. And sure, depending on the context they might still go with Python/Kotlin/Rust/etc.\n \nreply",
      "That crowd has the disadvantage of not being primarily interested in the other platforms, so they won't be much invested in optimizing or better matching the target capabilities.That's the same dynamic as web devs writing React Native apps: you won't expect them to contribute extensions that manipulate local apfs metadata for instance.So while it's nice to have them use the tools, you still need people who primarily care for non Apple platform and embrance swift for their purpose to have it expand.\n \nreply",
      "Hmm Snowflake and Apple are rewriting FoundationDB in Swift. Swift has pretty good dev. ergonomics and good interop with C/C++ so it might find it's niche outside of Apple.\n \nreply",
      "> people familiar with macos or ios development who want to write for other platforms.This is a rather small userbase when it comes to enterprise.Especially because Swift will never be as versatile as Python or as efficient as Rust.And then there's also Go, C# and Kotlin with much better tooling.\n \nreply"
    ],
    "link": "https://www.swift.org/blog/the-next-chapter-in-swift-build-technologies/",
    "first_paragraph": ""
  },
  {
    "title": "How to Run DeepSeek R1 Distilled Reasoning Models on RyzenAI and Radeon GPUs (guru3d.com)",
    "points": 19,
    "submitter": "waltercool",
    "submit_time": "2025-02-02T00:27:05 1738456025",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=42904116",
    "comments": [
      "As an aside, either the latest Linux or 6.14 has/will have support for Ryzen XDNA AI chips on their mobile APUs.Might not be appropriate for this model, but it could be for small models.\n \nreply",
      "any idea how they will appear to the OS? As additional processors?\n \nreply"
    ],
    "link": "https://www.guru3d.com/story/amd-explains-how-to-run-deepseek-r1-distilled-reasoning-models-on-amd-ryzen-ai-and-radeon/",
    "first_paragraph": " Published 2025-01-29 18:04 by Hilbert HagedoornAMD supports different sizes of DeepSeek R1 distillations across its processor and graphics card lineup. Larger processors, such as the Ryzen AI Max+ 395 Series, can run bigger distills like Qwen-32B, while mid-range products like Ryzen AI HX 370 or 7040/8040 often handle Qwen-14B or Llama-14B. For graphics cards, models like the Radeon RX 7900 XTX can accommodate Qwen-32B, but lower-tier cards generally work best with smaller versions. It is recommended to quantize these models in Q4 K M format to reduce memory usage and make the most of the available GPU resources.\u00a0To deploy a DeepSeek R1 distill, install the Adrenalin 25.1.1 driver or newer and download LM Studio 0.3.8 or above. Use the \u201cDiscover\u201d tab in LM Studio to select your preferred model, confirm Q4 K M quantization, and adjust GPU offload layers to suit your system\u2019s capacity. Once everything is configured, load the model in the \u201cChat\u201d tab to start interacting with its chain-of"
  },
  {
    "title": "A bookmarklet to kill sticky headers (2013) (mcdiarmid.org)",
    "points": 80,
    "submitter": "clockworksoul",
    "submit_time": "2025-02-01T21:12:37 1738444357",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=42902395",
    "comments": [
      "Bookmarklet aficionado and maintainer of bookmarkl.ink here. Took the liberty of posting this bookmarklet: https://bookmarkl.ink/ashtonmeuser/849a972686e1505093c6d4fc5...\n \nreply",
      "Wow, the Doom bookmarklet is kind of cool. https://bookmarkl.ink/ashtonmeuser/04710d3befc446e849108a587...It seems there isn't a way to browse existing bookmarklets, other than the small, curated lists?\n \nreply",
      "I'm not seeing any way to browse them either. I'd love such a feature.\n \nreply",
      "Nice, this is very cool, thanks for the work & for sharing :)\n \nreply",
      "I've been using the Kill-Sticky Chrome extension for years:https://chromewebstore.google.com/detail/kill-sticky/lekjlgf...Because it has a configurable keyboard shortcut.Can't imagine browsing the web without it. At this point hitting Cmd+K when I visit an article is pure reflex.A bookmarklet would be more secure, but I don't know of a way to assign keyboard shortcuts to one.\n \nreply",
      "These things are so annoying. Is there a website benchmark not for speed, not for security but for \"annoyingness\"? I guess there is some overlap with accessibility, but that's not exactly what I'm thinking of.I asked Copilot and searched Google but couldn't really come up with anything.\n \nreply",
      "CLS was roughly designed to measure annoyingness in terms of \"moving layout elements\". I'm sure Google has even more nuanced signals to measure e.g. ad coverage vs content coverage\n \nreply",
      "It is the inverse of what engagement is supposed to be measuring, but it turns out that optimizing for engagement takes you down a darker path...\n \nreply",
      "Anecdote: I was in charge of a complete rebuild for an e-commerce website a few years back, which included a new design. We were debating various layout options, as it was tricky to get the information hierarchy right and show everything necessary even on smaller screens. Then we had an internal review and the CEO complicated things considerably by insisting it was very important that the header be sticky --- to ensure that the company logo would always remain visible even when scrolling, reminding users of our brand.\n \nreply",
      "CEOs need to be stopped or at least be told to shut the fuck up.\n \nreply"
    ],
    "link": "https://alisdair.mcdiarmid.org/kill-sticky-headers/",
    "first_paragraph": "There is currently a trend for using sticky headers on websites. There's even a sticky header web startup.I hate sticky headers. I want to kill sticky headers.So I made this bookmarklet. Drag the link to your bookmarks bar:Kill StickyI use an 11\" MacBook Air, which means that I don't have much vertical screen space. The 50 pixels taken up by the sticky header could have been three lines of text.I also normally scroll down using the space key, which scrolls just less than one full viewport at a time. When you cover up part of your content with a sticky element, that means that the space key scrolls too far down. Then I lose my position and have to scroll back up.Finally, I just don't care right now about navigating your website, or following you on Twitter. I'm trying to read. Let me focus on that for now, please.It's really simple, because querySelectorAll is awesome. Here's the source:The bookmarklet just finds all fixed-position elements on the page, and removes them. This might remo"
  },
  {
    "title": "New thermogalvanic tech paves way for more efficient fridges (cosmosmagazine.com)",
    "points": 51,
    "submitter": "westurner",
    "submit_time": "2025-02-01T21:39:03 1738445943",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=42902624",
    "comments": [
      "Something I've wondered about recently - often times in a kitchen you want hot water and a cold box for food - and lately there are heat pump water heaters available on the market. Is there a fridge/water heater combo yet?\n \nreply",
      "There's not enough heat in the fridge to heat up any significant amount of water.I would however like to see that waste heat being used for something like dishwashers or just being vented out (instead of into the room that might have to be actively cooled).\n \nreply",
      "Waste water heat recovery, which heat exchanges with inlet water, would probably be more effective than heat exchanging with a fridge. Realistically both options are too costly for the limited benefit.\n \nreply",
      "If you're interested in going down a rabbit hole, other thermo tech that always seems to be right around the corner of practicality: thermoacoustic chillers, magnetocaloric fridges.\n \nreply",
      "If we cared about making refrigerators more efficient, we would just put the heat exchanger outside or better yet underground\n \nreply",
      "AFAIK, that's how lots of restaurants keep their stock chilled, at least in my area. They have relatively noisy exchangers outside connected to their fridges that are indoors.\n \nreply",
      "Imagine heat pumps 400+% efficient with 60% heat to electric conversion.https://en.m.wikipedia.org/wiki/Johnson_thermoelectric_energ...\n \nreply",
      "Does it work?\n \nreply",
      "10x Improvements in this kind of effect don't happen very often. I'm wondering if a chain of these can \"pump\" coolth uphill, or if it can be used alongside other methods.\n \nreply",
      "Yeah am I reading that? 1.7 degrees Celsius is what they've achieved? That paves the way for more efficient refrigerators?I thought paving the way meant a clear path to the end goal, because you already know the route and all the necessary engineering to get there.\n \nreply"
    ],
    "link": "https://cosmosmagazine.com/science/chemistry/improved-fridge-technology/",
    "first_paragraph": ""
  },
  {
    "title": "RLHF Book (rlhfbook.com)",
    "points": 50,
    "submitter": "jxmorris12",
    "submit_time": "2025-02-01T22:11:45 1738447905",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42902936",
    "comments": [
      "Glad to see the author making a serious effort to fill the gap in public documentation of RLHF theory and practice. The current state of the art seems to be primarily documented in arXiv papers, but each paper is more like a \"diff\" than a \"snapshot\" - you need to patch together the knowledge from many previous papers to understand the current state. It's extremely valuable to \"snapshot\" the current state of the art in a way that is easy to reference.My friendly feedback on this work-in-progress: I believe it could benefit from more introductory material to establish motivations and set expectations for what is achievable with RLHF. In particular, I think it would be useful to situate RLHF in comparison with supervised fine-tuning (SFT), which readers are likely familiar with.Stuff I'd cover (from the background of an RLHF user but non-specialist):Advantages of RLHF over SFT:- Tunes on the full generation (which is what you ultimately care about), not just token-by-token.- Can tune on problems where there are many acceptable answers (or ways to word the answer), and you don't want to push the model into one specific series of tokens.- Can incorporate negative feedback (e.g. don't generate this).Disadvantages of RLHF over SFT:- Regularization (KL or otherwise) puts an upper bound on how much impact RLHF can have on the model. Because of this, RLHF is almost never enough to get you \"all the way there\" by itself.- Very sensitive to reward model quality, which can be hard to evaluate.- Much more resource and time intensive.Non-obvious practical considerations:- How to evaluate quality? If you have a good measurement of quality, it's tempting to just incorporate it in your reward model. But you want to make sure you're able to measure \"is this actually good for my final use-case\", not just \"does this score well on my reward model?\".- How prompt engineering interacts with fine-tuning (both SFT and RLHF). Often some iteration on the system prompt will make fine-tuning converge faster, and with higher quality. Conversely, attempting to tune on examples that don't include a task-specific prompt (surprisingly common) will often yield subpar results. This is a \"boring\" implementation detail that I don't normally see included in papers.Excited to see where this goes, and thanks to the author for willingness to share a work in progress!\n \nreply",
      "Has r1 made RLHF obsolete?\n \nreply",
      "DeepSeek-R1 had an RLHF step in their post-training pipeline (section 2.3.4 of their technical report[1]).In addition, the \"reasoning-oriented reinforcement learning\" step (section 2.3.2) used an approach that is almost identical to RLHF in theory and implementation. The main difference is that they used a rule-based reward system, rather than a model trained on human preference data.If you want to train a model like DeepSeek-R1, you'll need to know the fundamentals of reinforcement learning on language models, including RLHF.[1] https://arxiv.org/pdf/2501.12948\n \nreply",
      "> Reinforcement learning from human feedback (RLHF)In case anyone else didn\u2019t know the definition.Knowing the definition it sounds kind of like \u201clearn what we tell you matters\u201d in a sense.Not unlike how the world seems to work today. High hopes for the future\u2026\n \nreply"
    ],
    "link": "https://rlhfbook.com/",
    "first_paragraph": "A short introduction to RLHF and post-training\nfocused on language models.Nathan LambertReinforcement learning from human feedback (RLHF) has become an\n  important technical and storytelling tool to deploy the latest machine\n  learning systems. In this book, we hope to give a gentle introduction\n  to the core methods for people with some level of quantitative\n  background. The book starts with the origins of RLHF \u2013 both in recent\n  literature and in a convergence of disparate fields of science in\n  economics, philosophy, and optimal control. We then set the stage with\n  definitions, problem formulation, data collection, and other common\n  math used in the literature. We detail the detail the popular\n  algorithms and future frontiers of RLHF.I would like to thank the following people who helped me directly with this project: Costa Huang, (and of course Claude). Indirect shout-outs go to Ross Taylor, Hamish Ivison, John Schulman, and others in my RL sphere.Additionally, thank you to the "
  },
  {
    "title": "Python 3, Pygame, and Debian Bookworm on the Miyoo A30 (jtolio.com)",
    "points": 81,
    "submitter": "todsacerdoti",
    "submit_time": "2025-02-01T19:57:37 1738439857",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=42901616",
    "comments": [
      "Nice writeup. Lots of potential in those budget handhelds. Nice to see you were able to access the GPU with python. Waiting to see what sort of games you come up with!\n \nreply",
      "Are there similar devices in different form factors for custom IoT purposes? I would guess that BoM costs would be pretty similar, but I don't know that there's enough demand.\n \nreply",
      "No screenshots were a bummer but a nice writeup nonetheless\n \nreply",
      "There are a bunch of these little Linux game emulation consoles and they are pretty neat. Each of my kids has a Miyoo Mini. Lots of fun and a bunch of community add-ons and support. Almost got one for myself but the D-Pad and buttons are a bit too cramped for my average adult-sized hands.\n \nreply",
      "You could get Miyoo Mini Plus for plus sized hands ;)\n \nreply",
      "This is very cool. How different would this be for Miyoo Mini Plus?\n \nreply",
      "I assume the basics are similar. The Miyoo Mini Plus has WiFi, which is great, but no GPU, so I assume you end up with different drivers. But the CPU is the same so everything else should be equivalent.\n \nreply",
      "This looks very interesting.The CPU is pretty close to rpi2 and zero W2, which can run latest vanilla Debian. Not sure why the official OS is stuck on such an old kernel.\n \nreply",
      "The kernel works and so far no-one has been motivated enough to update it. The Rocknix projects offers an up-to-date kernel for a whole bunch devices including the 30 USD R36S which offers amazing value for the money (but does require an USB WiFi adapter for networking).\n \nreply",
      "AFAIK that's the case with pretty much every ARM SBC, Raspberry Pi being the exception. They still support the original model from 2012 with their latest OS and it's a huge part of their success IMO.\n \nreply"
    ],
    "link": "https://www.jtolio.com/2025/02/py3-pygame-miyoo-a30/",
    "first_paragraph": "What if I told you you can get a full Linux computer with WiFi and a GPU in a\nsmall, pocket form factor for about $30 USD on AliExpress? You might say \u201cbut what about the new tariffs?\u201d and I\u2019d say \u201coof, oh right, okay who knows, but it\u2019s probably still pretty cheap!\u201d (Edit: looks like you can get one for about $40 now).It wasn\u2019t a big hit with the\nretro handheld gaming community, but the\nMiyoo A30 has a 4 core\nCortex A7 with a Mali-400 MP GPU, 512 MB of RAM, WiFi, and an incredible little\n640x480 IPS screen. It has enough power to emulate many\nNintendo 64 games, but that\u2019s not what I bought it for.\nIn fact, I bought two so my son and I could make networked multiplayer games\ntogether.\n\n\n\n\nI already have a game I made in Pygame back in\ncollege, so naturally, I wanted to first get modern Python 3 and Pygame working on\nit.If you want to ignore my work log and just skip to the files, at the bottom\nI have premade files for you so you can install Python programs on your Miyoo\nA30 by just copy"
  },
  {
    "title": "Macrodata Refinement (lumon-industries.com)",
    "points": 225,
    "submitter": "gaws",
    "submit_time": "2025-02-01T21:46:16 1738446376",
    "num_comments": 128,
    "comments_url": "https://news.ycombinator.com/item?id=42902691",
    "comments": [
      "Please try to enjoy all comments equally, and not show preference for any over the others.\n \nreply",
      "For those who haven't seen the show:\nhttps://www.youtube.com/watch?v=Q99KYhD9BpQP.S. The numbers are scary.\n \nreply",
      "The work is mysterious and important.\n \nreply",
      "Working at a startup before product-market fit can feel like this.You don\u2019t know why the work is important, but it must be done so we can at least discover whether it was important. You may not get that information, but you can take comfort in assuming someone does have it.You\u2019re mostly disconnected from your previous life.There is a guy in the next office feeding baby goats, and your reaction is: \u201cYes, it makes sense that we\u2019re also exploring feeding baby goats.\u201dPeople come in as blank slates and you\u2019re grateful to have their companionship in the shared madness.\n \nreply",
      "This completely misses the disturbing horror aspect of the show.\n \nreply",
      "Severe enough burnout at a startup (or crunch-mode game studio, or similar) can give you a reasonable simulacrum of that.Come in, go home, come back. Did something actually happen that wasn\u2019t work? Unclear.\n \nreply",
      ">come in, go home, come back.https://youtu.be/2n34NrkDlZkthis felt germane\n \nreply",
      "Yeah, to me the show was much more about how every modern real company secretly wished they could sever their employees, and how much they'd abuse that power dynamic if they actually had it.Work without workers? Perfect!\n \nreply",
      "ah yes the pineapple\n \nreply",
      "This is a recreation of a fictional computer program from the excellent Apple TV show - Severance.The work is mysterious, and important.Season 2 is going now. It\u2019s one of my top 3 shows of the last decade, highly recommend it.\n \nreply"
    ],
    "link": "https://lumon-industries.com/",
    "first_paragraph": ""
  },
  {
    "title": "Bzip3: A spiritual successor to BZip2 (github.com/kspalaiologos)",
    "points": 192,
    "submitter": "tosh",
    "submit_time": "2025-02-01T16:46:01 1738428361",
    "num_comments": 110,
    "comments_url": "https://news.ycombinator.com/item?id=42899713",
    "comments": [
      "I've studied the Burrows-Wheeler Transform, I understand the transformation, I've re-implemented it countless times for kicks, I see how it improves compressability, but for the life of me the intuition of _why_ it works has never really clicked.It's a fantastic bit of algorithmic magic that will always impress me to see it.\n \nreply",
      "The Burroughs-Wheeler transform has been described as a unique algorithm idea in that there are no non-trivial variations or related algorithms, unlike more conventional compression algorithms, which can be tweaked and improved in so many ways. There is no general compression theory in which BWT could be described as a special case.It looks to me that the above still holds: Bzip2 and Bzip3 are simply combining more conventional compression algorithms with the BWT, which itself is still the same old transform. Bzip2 does Huffman coding after BWT, and Bzip3 does arithmetic coding.\n \nreply",
      "Can BWT be combined with zstd, which uses asymmetric numeral systems?\n \nreply",
      "Yes, it would actually be interesting to just have a bwt pass which does no compression, so we can then try lots of post compression options.\n \nreply",
      "Thank you for the reference. I learned something new today. That algorithm is wild. If you had shown me the transform and asked if it had an inverse, I would have said of course it doesn't, it's too weird.\n \nreply",
      "I always understood it as working because of the predictability of a symbol/letter/token given the previous one.Sorting all the shifts of a string puts all the characters in order, then looking at the last column shows you all the _preceding_ characters.  If there's any predictability there (which there often is), it's now easier to compress.  It's sorta like an entropy coder in that way.I've never thought of it as being that deep, and understood them since I was a kid -- building an intuition for \"why\" the FFT works is much harder -- but that being said, I clicked quickly to reply thinking \"that's easy! I can explain this!\" then struggled for a while trying to get the picture in my mind into text. :)\n \nreply",
      "But shouldn't Huffman coding already detect that same predictability and compress it the same?What I don't get isn't the benefits of BWT on its own. It's why BWT should add any additional benefit if you're already doing Huffman.\n \nreply",
      "> What I don't get isn't the benefits of BWT on its own. It's why BWT should add any additional benefit if you're already doing Huffman.Ahhhh.  Now we're on the same page. :) Seeing how it helps when combined is somewhat subtle/non-obvious.  I believe it relates to BWT and Huffman both being approximations of something more optimal.  The two transforms could also have different window sizes -- one rarely does BWT on a whole 1GB file -- which introduce inefficiencies.  Huffman coding is also only optimal in the very large alphabet and very long data limits.  As your data length and alphabet size decrease, it gets less optimal.Put differently, \"I think that's a wonderfully phrased question, this _is_ my specialization/subfield, and I'm gonna need to chew on it for a while.\"\n \nreply",
      "Thanks. Yeah, I can see how that would make more sense if BWT was redundant under a theoretically perfect Huffman compression, but it happens to pick up some things that real-world Huffman encoders don't, with their practical limits on CPU and memory.\n \nreply",
      "One way to look at data compression is splitting it into a model and an encoder. The model describes the data, while the encoder encodes (or equivalently predicts) the data according to the model. The compressed output consists of the serialized model and the encoded data. BWT is a model, while Huffman is an encoder.Huffman takes a probability distribution and a symbol and encodes the symbol according to the distribution. If you encode all symbols independently according to the same distribution, you probably don't get very good compression.You get a bit better results with a model that has a separate distribution for each context. If the previous symbols were X, Y, and Z, you encode the next symbol according to the distribution for context XYZ. This approach doesn't really scale, because the size of the model grows rapidly (exponentially in a naive implementation) with context length. You get better compression with an adaptive model. You start with a uniform distribution and update the available contexts and distributions after encoding each symbol. On the one hand, you don't have to store the model explicitly. But on the other hand, updating the model is very slow.Burrows-Wheeler transform is an implicit model. It sorts the symbols according to the context that follows them, and it does that simultaneously for each context length. Because you don't have to store the model explicitly, you can effectively use longer context lengths than with a fixed explicit model. And because you don't have to update an explicit model after encoding each symbol, using the BWT is much faster than using an adaptive model.\n \nreply"
    ],
    "link": "https://github.com/kspalaiologos/bzip3",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A better and stronger spiritual successor to BZip2.\n      A better, faster and stronger spiritual successor to BZip2. Features higher compression ratios and better performance thanks to a order-0 context mixing entropy coder, a fast Burrows-Wheeler transform code making use of suffix arrays and a RLE with Lempel Ziv+Prediction pass based on LZ77-style string matching and PPM-style context modeling.Like its ancestor, BZip3 excels at compressing text or code.Alternatively, you might be able to install bzip3 using your system's package manager:On macOS, you can use Homebrew to easily install:First, I have downloaded every version of Perl5 ever released and decompressed them.Then, I put all the resulting .tar files in a single .tar file and tried to compress it using various compressors:The results follow:Finally, wall clock time decomp"
  },
  {
    "title": "Enhancing your MIDI devices with Perl (fuzzix.org)",
    "points": 43,
    "submitter": "oalders",
    "submit_time": "2025-01-29T14:59:23 1738162763",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://fuzzix.org/enhancing-midi-hardware-with-perl",
    "first_paragraph": "These days, even modestly priced MIDI hardware comes stuffed with features.\nYou should expect a budget device to provide at least some of clock, sequencer,\narpeggiator, chord voicing, Digital Audio Workstation (DAW) integration, and\ntransport control features.\nFitting all\nthis into a small device's form factor may result in some amount of compromise\n\u2014 perhaps modes aren't easily combined, or some amount of menu diving is\nrequired to switch between modes. Your device may even lack the precise\nfunctionality you require.\nThis post will walk through the implementation of a pair of features to augment\nthose found in a MIDI keyboard \u2014 a M-Audio Oxygen Pro 61 in this case, though\nthe principle should apply to any device.\nA recent video by Polarity Music opened with some exploration of using a pedal tone\nin Bitwig to compose progressions. A pedal tone (or pedal note, or pedal point)\nis a sustained single note, over which other potentially dissonant parts are\nplayed. In this case, the pedal ton"
  },
  {
    "title": "String of recent killings linked to Bay Area rationalist 'death cult' (sfgate.com)",
    "points": 296,
    "submitter": "davikr",
    "submit_time": "2025-01-30T14:21:20 1738246880",
    "num_comments": 362,
    "comments_url": "https://news.ycombinator.com/item?id=42877910",
    "comments": [
      "A later article by the same author: https://www.sfgate.com/bayarea/article/leader-alleged-bay-ar.... Probably makes sense to read both or neither.",
      "Most of the news coverage I've seen of this story is omitting what some might consider a relevant detail:  almost all the members of this group are trans.This is a divisive topic, but failing to mention this makes me worry a story is pushing a particular agenda rather than trying to tell the facts.  Here's what the story looks like if the trans activism is considered central to the story:https://thepostmillennial.com/andy-ngo-reports-trans-terror-...While Ngo's version is definitely biased, and while I don't know enough about the story to endorse or refute his view, I think it's important to realize that this part of the story is being suppressed in most of the coverage elsewhere.\n \nreply",
      "it's been an exhausting couple of weeks for me, as a trans person. one executive order after another, explicitly attacking us. scrambling to update all my documents, navigating a Kafkaesque bureaucracy with constantly shifting rules.now this.there are like six Zizians. there are millions of trans people. I'm sure that many of the Zizians being trans says something about the Ziz cult, but Ziz doesn't say anything about \"trans activism.\"any evil one trans person does, is used to stain all trans people. recognize this tendency; don't let this become like blood libel.\n \nreply",
      "I\u2019m not a big George W Bush fan but this quote of his has stuck with me for years:> Too often, we judge other groups by their worst examples while judging ourselves by our best intentions\n \nreply",
      "https://en.m.wikipedia.org/wiki/Fundamental_attribution_erro...\n \nreply",
      "> any evil one trans person does, is used to stain all trans people. recognize this tendency; don't let this become like blood libel.As a Christian, I can empathize. The wrongs and hypocrisies of so many are heaped on those who have no relation to the actions.\n \nreply",
      "I don't think that you need to be a Christian to empathise with anyone.  The big geezer's (JC) teachings imply to me that keeping quiet about your faith and simply doing good (for a given value of good) is the Way.Declaring a religion might rile someone, before you have even engaged.  I suggest that you simply proffer a hand.  Empathise as best you can.  Be careful.At best, with mentioning religion, you have declared your rightful intentions and at worst you have added yet another layer of tribalism to a ... debate.I'm pretty sure that at least the synoptic gospels and probably John too (for the full set) tell you to keep it quiet.  There is no need to put your religious heart on your sleeve - that's between you and God.\n \nreply",
      "> At best, with mentioning religion, you have declared your rightful intentions and at worst you have added yet another layer of tribalism to a ... debate.I don't think you understood what they were saying. They were saying that as a Christian, they've experienced being blamed for the misdeeds of other Christians which they personally had nothing to do with.The most important part of that statement isn't the word \"Christian\": you could replace \"Christian\" with Muslim or atheist or whatever and there'd be someone else who could truthfully say it.\n \nreply",
      "> don't think that you need to be a Christian to empathiseThey're saying that a minority of evangelists and rapey priests have been making Christianity look bad for decades.\n \nreply",
      "I think their point was that they feel they have experienced the same treatment that the trans person is describing, but because of their faith. So it was kinda necessary to bring it up in order to convey that.\n \nreply"
    ],
    "link": "https://www.sfgate.com/bayarea/article/bay-area-death-cult-zizian-murders-20064333.php",
    "first_paragraph": ""
  },
  {
    "title": "YouTube audio quality \u2013 How good does it get? (2022) (audiomisc.co.uk)",
    "points": 82,
    "submitter": "fhinson",
    "submit_time": "2025-02-01T19:11:12 1738437072",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=42901182",
    "comments": [
      "For music listening, the YouTube Music app has settings for higher audio quality under \"Settings > Playback\".According to their documentation, this is \"Upper bound of 256kbps AAC & OPUS\":\nhttps://support.google.com/youtubemusic/answer/9076559?hl=en...EDIT: FWIW, I'm a Premium member. I'm not sure if this is a standard feature.\n \nreply",
      "I wonder if I upload a video to standard Youtube and tag it 'music' so it's shows up over on Youtube Music does it get a better audio quality than the video encode gets?\n \nreply",
      "The worse problem for YouTube Music is that some of the source material is not the same as what you may have on physical media. There are entire labels that provide worse or at least different materials than they published to record stores. And then there is remaster roulette. What am I going to hear if I press play on \"Rumours\": the original that was born perfect, or one of the half-dozen remasters that have been issued as the original engineer progressively loses his hearing?\n \nreply",
      "One of the advantages of Apple Music\u2019s digital masters. They require no clipping which alone vastly improves a majority of albums.\n \nreply",
      "See \"Why not use graphs / frequency analysis to compare codecs?\" https://wiki.hydrogenaud.io/index.php?title=FAQ#:~:text=Why%...\"I decided that analysis should focus on the higher, more conventional rates \u2013 48k and 44k1\" - opus is always 48khz, so that doesn't mean much.\n \nreply",
      "This. Your ears are not an oscilloscope. Lossy audio codecs are designed to exploit two major weaknesses in human hearing:1. Poor sensitivity in bass and treble. See:https://en.wikipedia.org/wiki/Equal-loudness_contour2. Limited ability to hear multiple sounds simultaneously, or almost simultaneously. See:https://en.wikipedia.org/wiki/Auditory_maskingBernhard Seeber has some videos on Youtube with demonstrations of auditory masking:https://www.youtube.com/watch?v=R9UZnMsm9o8https://www.youtube.com/watch?v=bU0_Kaj7cPkThe only fair way to evaluate lossy codecs is with double blind listening tests.\n \nreply",
      "I have YouTube premium and I have access to experimental features and coincidentally they offer one feature called audio in high quality. So they are working on improving it.Description:High-Quality AudioAvailable until February 22With high-quality audio, you can listen to music on YouTube in the best audio quality.How it works:\nWatch an eligible music video on YouTube and enjoy the benefits of higher-quality audio.Only available on iOS and Android.\n \nreply",
      "why only on mobile? very curious\n \nreply",
      "likely end-to-end drm\n \nreply",
      "There's no \"end-to-end drm\" for audio. Any audio can be captured near losslessly by plugging in a USB-C to 3.5mm adapter, and a sound card on the other end. There's also no HDCP-like mechanism for sound, so if you're willing to put the work into it, you could make a fake USB DAC that produces a bit perfect capture of the audio output.\n \nreply"
    ],
    "link": "https://www.audiomisc.co.uk/YouTube/SpotTheDifference.html",
    "first_paragraph": "YouTube Audio Quality - How Good Does it Get?\nYou Tube is clearly used by a very large number of people. In general, they will be interested in watching videos of various types of content. One specific purpose is that it gets used to distribute, and make people aware of, audio recordings. A conversation on the \u201cPink Fish\u201d webforum devoted to music and audio set me wondering about the technical quality of the audio that is on offer from You Tube (YT) videos. The specific comment which drew my attention was a claim that the \u2018opus\u2019 audio codec gives better results than the \u2019aac / mp4\u2019 alternative. So I decided to investigate...\nIdeally to assess this requires a copy of what was uploaded to YT as a \u2018source\u2019 version which can then be compared with what YT then make available as output. By coincidence I had also quite recently joined the Ralph Vaughan-Williams Society (RVWSoc). They have been putting videos up onto YT which provide excerpts of the recordings they sell on Audio CDs. These pro"
  },
  {
    "title": "A mouseless tale: trying for a keyboard-driven desktop (lwn.net)",
    "points": 33,
    "submitter": "signa11",
    "submit_time": "2025-01-31T02:04:14 1738289054",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42884099",
    "comments": [
      "So many good options here.Vimium is the gateway drug to the fantastic qutebrowser (more integration and customisation, scripting, based on qtwebkit/engine).\n \nreply",
      "I switched to mousing left-handed at some point, years ago. I noticed some of my left-handed colleagues mousing wrong-handed, possibly because of right-handed siblings, and getting some good use out of having pen and paper more easily accessible because of it. I decided I'd do the same.Something I didn't expect: with my left hand on the home row, I can touch the mouse by stretching my little finger. So it's no effort at all to put my hand on it and use it to move the cursor. And so I've pretty much given up entirely on using the keyboard for arbitrary caret motion. It's much easier to just put my hand on the mouse and move the cursor and click where I want it to go.\n \nreply",
      "There's so many ways to do this with tiling window managers, keyboard managers for browsers, scriptable keyboards, etc.My personal setup is Guake for instant terminal, chat gpt cli, Firefox + vimium, and a tiling gnome extension (wintile I believe).It's minimally invasive, and bog standard Ubuntu otherwise.\n \nreply",
      "On Mac so don\u2019t know when I\u2019ll try this but suggest uhk keyboard for anyone going mouseless\n \nreply",
      "PaperVM is really neat. Much simpler to setup, learn and understand than for example i3.\n \nreply"
    ],
    "link": "https://lwn.net/Articles/1005332/",
    "first_paragraph": "\nSubscriptions are the lifeblood of LWN.net.  If you appreciate this\ncontent and would like to see more of it, your subscription will\nhelp to ensure that LWN continues to thrive.  Please visit\nthis page to join up and keep LWN on\nthe net.\nThe computer mouse is a wonderful invention, but for the past few\nmonths I've been working to use mine as little as possible for\nproductivity and ergonomic reasons. It should not be surprising that\nthere are quite a few open-source applications, utilities, and\nconfiguration options that are either designed to or incidentally\nassist in creating a keyboard-driven desktop. This includes tiling window\nmanagement with PaperWM, the Vimium browser extension, Input Remapper, and more.A tiling window manager can make it more practical to avoid using\nthe mouse by letting the window manager arrange application windows\nautomatically, rather than forcing the user to manually move them around and\nadjust their size. However, the traditional tiling window manager\napp"
  },
  {
    "title": "LMD: A New, Less Wasteful Metal 3D Printing Technique (core77.com)",
    "points": 8,
    "submitter": "surprisetalk",
    "submit_time": "2025-01-29T01:00:03 1738112403",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.core77.com/posts/135194/LMD-A-New-Less-Wasteful-Metal-3D-Printing-Technique",
    "first_paragraph": "Metal 3D printing is currently dominated by powder-based techniques like SLM (Selective Laser Melting). These processes yield incredibly precise parts, but the build times are slow. Furthermore, dealing with the powder increases manufacturing complexity: Whenever the powder is transported, loaded into the machine, or cleaned up afterwards, rigorous steps must be followed to prevent the loose spread of powder. (The powders are flammable and present an explosion risk, and can also cause respiratory issues for workers.) Following all of these steps adds cost, time, and risk.A Spain-based company called Meltio has developed a new metal 3D printing technology that does away with powder-based hassles. Rather than powder, Meltio's feedstock is metal wire, which is easy to handle on spools. The wire is fed into a point where three to six low-power diode lasers converge, creating what's known as a \"melt pool.\" (This is sort of like the \"contact patch\" on a car tire, in that it's paradoxically a"
  },
  {
    "title": "MindsDB (YC W20) Is Hiring an Office Manager in SF (grnh.se)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-02-01T21:00:34 1738443634",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://grnh.se/83c3fffa7us",
    "first_paragraph": "MindsDB is a fast-growing AI startup headquartered in San Francisco, California. As a leading innovator bringing AI and Data together, our passion is empowering companies to easily build AI capabilities that can Think, Understand and Orchestrate: enabling teams to move from prototyping & experimentation to production in a fast & scalable way.MindsDB was founded in 2017 by Adam Carrigan and Jorge Torres, inspired by Ian M. Banks's Culture series, in which super AI systems called Minds collaborate with other forms of life to accomplish incredible goals. Starting as an Open-Source project, MindsDB has grown to be one of the most widely used AI-Data platforms in the world, with a growing community and more than 700 contributor developers from every corner of the globe.We are backed with over $55M in funding from Mayfield, Benchmark, YCombinator, and nVidia. MindsDB is also recognized by Forbes as one of America's most promising AI companies (2021) and by Gartner as a Cool Vendor for Data a"
  },
  {
    "title": "Visualizing all books of the world in ISBN-Space (phiresky.github.io)",
    "points": 325,
    "submitter": "phiresky",
    "submit_time": "2025-02-01T09:27:06 1738402026",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=42897120",
    "comments": [
      "Wow.When we started Amazon, this was precisely what I wanted to do, but using Library of Congress triple classifications instead of ISBN.It turned out to be impossible because the data provider (a mixture of Baker & Tayler (book distributors) and Books In Print) munged the triple classification into a single string, so you could not find the boundaries reliably.Had to abandon the idea before I even really got started on it, and it would certainly have been challenging to do this sort of \"flythrough\" in the 1994-1995 version of \"the web\".Kudos!\n \nreply",
      "It\u2019s not uncommon for an ISBN to have been assigned multiple times to different books [0]. Thus \u201call books in ISBN space\u201d may be an overstatement.There\u2019s also the problem of books with invalid ISBNs, i.e. where the check digit doesn\u2019t match the rest of the ISBN, but where correcting the check digit would match a different book. These books would be outside of the ISBN space assumed by the blog post.[0] https://scis.edublogs.org/2017/09/28/the-dreaded-case-of-dup...\n \nreply",
      "And possibly not even assigned at all. I looked at the lowest known ISBNs for Czech publishers and a different color stood out: no, https://books.google.cz/books?vid=ISBN9788000000015&redir_es... is not a correct ISBN, I'd say :-) (But I don't know if the book includes such obviously-fake ISBN, or the error is just in Google Books data.)\n \nreply",
      "Publishers buy blocks of isbns based on expected need, how the actually assign them may be arbitrary.\n \nreply",
      "Impressive presentation.Note: The presentation reflects the contents of Anna's archive exclusively, rather than the entire ISBN catalog. There is a discernible bias towards a limited range of languages, due to Anna's collection bias to those languages. The sections marked in black represent the missing entries in the archive.\n \nreply",
      "That's not entirely accurate since AA has separate databases for books they have as files, and one for books they only know the metadata of. The metadata database comes from various sources and as far as I know is pretty complete.Black should mostly be sections that have no assigned books\n \nreply",
      "I found some books which are available from dozens of online bookshops but which are not in this visualisation. Perhaps they're not yet in any library that feeds into worldcat.org, though some of them were about five years old.\n \nreply",
      "Did you search by title or ISBN? If you search by title, the search goes through Google Books, which is very incomplete (since I didn't build a search database myself). If you put in an ISBN13 directly, you'll find a lot more books are included (I'd say you can only find 10-30% of books via the Google Books API)It's a bit misleading I guess the way I added that feature.\n \nreply",
      "Wow, that is really cool. What an amazing passion project and what an incredible resource!Zooming in you can see the titles, the barcode and hovering get a book cover and details. Incredible, everything you could want!Some improvement ideas: checkbox to hide the floating white panel at top left, and the thing at top right. Because I really like to \"immerse\" in these visualizations, those floaters lift you out of that experience to some extent, limiting fun and functionality for me a bit.\n \nreply",
      "I think you can reasonably think about the flight path by modeling the movement on the hyperbolic upper half plane (x would be the position along the linear path between endpoints, y the side length of the viewport).I considered two metrics that ended up being equivalent. First, minimizing loaded tiles assuming a hierarchical tiled map. The cost of moving x horizontally is just x/y tiles, using y as the side length of the viewport. Zooming from y_0 to y_1 loads abs(log_2(y_1/y_0)) tiles, which is consistent with ds = dy/y. Together this is just ds^2 = (dx^2 + dy^2)/y^2, exactly the upper-half-plane metric.Alternatively, you could think of minimizing the \"optical flow\" of the viewport in some sense. This actually works out to the same metric up to scaling - panning by x without zooming, everything is just displaced by x/y (i.e. the shift as a fraction of the viewport). Zooming by a factor k moves a pixel at (u,v) to (k*u,k*v), a displacement of (u,v)*(k-1). If we go from a side length of y to y+dy, this is (u,v)*dy/y, so depending how exactly we average the displacements this is some constant times dy/y.Then the geodesics you want are just the horocycles, circles with centers at y=0, although you need to do a little work to compute the motion along the curve. Once you have the arc, from \u03b8_0 to \u03b8_1, the total time should come from integrating dtheta/y = d\u03b8/sin(\u03b8), so to be exact you'd have to invert t = ln(csc(\u03b8)-cot(\u03b8)), so it's probably better to approximate. edit: mathematica is telling me this works out to \u03b8 = atan2(1-2*e^(2t), 2*e^t) which is not so bad at all.Comparing with the \"blub space\" logic, I think the effective metric there is ds^2 = dz^2 + (z+1)^2 dx^2, polar coordinates where z=1/y is the zoom level, which (using dz=dy/y^2) works out to ds^2 = dy^2/y^4 + dx^2*(1/y^2 + ...). I guess this means the existing implementation spends much more time panning at high zoom levels compared to the hyperbolic model, since zooming from 4x to 2x costs twice as much as 2x to 1x despite being visually the same.\n \nreply"
    ],
    "link": "https://phiresky.github.io/blog/2025/visualizing-all-books-in-isbn-space/",
    "first_paragraph": "Libraries have been trying to collect humanity\u2019s knowledge almost since the invention of writing. In the digital age, it might actually be possible to create a comprehensive collection of all human writing that meets certain criteria. That\u2019s what shadow libraries do - collect and share as many books as possible.One shadow library, Anna\u2019s Archive (which I will not link here directly due to copyright concerns), recently posed a question: How could we effectively visualize 100,000,000 books or more at once? There\u2019s lots of data to view: Titles, authors, which countries the books come from, which publishers, how old they are, how many libraries hold them, whether they are available digitally, etc.International Standard Book Numbers (ISBNs) are 13-digit numbers that are assigned to almost all published books. Since the first three digits are fixed (currently only 978- and 979-) and the last digit is a checksum, this means the total ISBN13-Space only has two billion slots. Here is my interac"
  },
  {
    "title": "Archivists work to save disappearing data.gov datasets (404media.co)",
    "points": 344,
    "submitter": "johnneville",
    "submit_time": "2025-01-30T19:40:33 1738266033",
    "num_comments": 111,
    "comments_url": "https://news.ycombinator.com/item?id=42881367",
    "comments": [
      "I'm quoted in this article. Happy to discuss what we're working on at the Library Innovation Lab if anyone has questions.There's lots of people making copies of things right now, which is great -- Lots Of Copies Keeps Stuff Safe. It's your data, why not have a copy?One thing I think we can contribute here as an institution is timestamping and provenance. Our copy of data.gov is made with https://github.com/harvard-lil/bag-nabit , which extends BagIt format to sign archives with email/domain/document certificates. That way (once we have a public endpoint) you can make your own copy with rclone, pass it around, but still verify it hasn't been modified since we made it.Some open questions we'd love help on --* One is that it's hard to tell what's disappearing and what's just moving. If you do a raw comparison of snapshots, there's things like 2011-glass-buttes-exploration-and-drilling-535cf being replaced by 2011-glass-buttes-exploration-and-drilling-236cf, but it's still exactly the same data; it's a rename rather than a delete and add. We need some data munging to work out what's actually changing.* Another is how to find the most valuable things to preserve that aren't directly linked from the catalog. If a data.gov entry links to a csv, we have it. If it links to an html landing page, we have the landing page. It would be great to do some analysis to figure out the most valuable stuff behind the landing pages.\n \nreply",
      "A common metric for how much actual content has changed is the Jaccard Index. Even for large numbers of datasets that are too large to fit in memory it can be approximated with various forms of MinHash algorithms. Some write up here: https://blog.nelhage.com/post/fuzzy-dedup/https://en.wikipedia.org/wiki/Jaccard_index\n \nreply",
      "Just commenting to double-down on the need for cryptographic timestamping - especially in the current era of generative AI.\n \nreply",
      "How can people help? Sounds like a global index of sources is needed and the work to validate those sources, over time, parceled out. Without something coordinated I feel like it is futile to even jump in.\n \nreply",
      "I'd love to learn more about what is in scope of the Library Innovation Lab projects. Is it targeting data.gov specifically or all government agency websites?Given the rapid take downs of websites (cdc, usaid) do you have a prioritization framework for which website pages to prioritize or do you have \"comprehensive\" coverage of pages (in scope of the project)?As you allude to, I've been having a hard time learn about what sort of duplicate work might be happening given that there isn't a great \"archived coverage\" source of truth for government websites (between projects such as End of Term archive, Internet archive, research labs, and independent archivists).Your open questions are interesting. Content hashes for each page/resource would be a way to do quick comparisons, but I assume you might want to set some threshold to determine how much it's changed vs if it changed?Is the second question about figuring out how to prioritize valuable stuff behind two depth traversals? (ex data.gov links to another website and that website has a csv download)\n \nreply",
      "Hi! Is there any one place that would be easiest for folks to grab these snapshots from? Would love to try my hand at finding documents that moved/documents that were removed.\n \nreply",
      "Thank you for this effort.\n \nreply",
      "Trump did this last time too. Is there a difference in the level of preparedness in archiving data compared to last time? If so, in what way is it different? Is there institutional or independent preparedness?\n \nreply",
      "One of the USA greatest strengths is the almost unprecedented degree of transparency of governments records going back decades. We can actually see the true facts including when our government has lied to us or covered things up. Many other nations do not have this luxury and it has provided the evidentiary basis for both legal cases and \"progress\" in general.  Not surprising that authoritarians would target and destroy data as it makes their objective of a post-truth society that much easier\n \nreply",
      "I\u2019ve been archiving data.gov for over a year now and it\u2019s not unusual to see large fluctuations on the order of hundreds or thousands of datasets. I\u2019ve never bothered trying to figure out what exactly is changing, maybe I should build a tool for that\u2026\n \nreply"
    ],
    "link": "https://www.404media.co/archivists-work-to-identify-and-save-the-thousands-of-datasets-disappearing-from-data-gov/",
    "first_paragraph": "Datasets aggregated on data.gov, the largest repository of U.S. government open data on the internet, are being deleted, according to the website\u2019s own information. Since Donald Trump was inaugurated as president, more than 2,000 datasets have disappeared from the database. As people in the Data Hoarding and archiving communities have pointed out, on January 21, there were 307,854 datasets on data.gov. As of Thursday, there are 305,564 datasets. Many of the deletions happened immediately after Trump was inaugurated, according to snapshots of the website saved on the Internet Archive\u2019s Wayback Machine. Harvard University researcher Jack Cushman has been taking snapshots of Data.gov\u2019s datasets both before and after the inauguration, and has worked to create a full archive of the data.Because data.gov is an aggregator that doesn\u2019t always host the data itself, this doesn\u2019t always mean that the data itself has been deleted, that it doesn\u2019t exist elsewhere on federal government websites, or "
  },
  {
    "title": "Show HN: We're building a desktop app for browser agents (meha.ai)",
    "points": 41,
    "submitter": "jawerty",
    "submit_time": "2025-02-01T13:41:19 1738417279",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=42898201",
    "comments": [
      "The headline needs another word. Maybe browser-based agents. I assumed this was about browser user-agents.\n \nreply",
      "maybe browser AI agent? Good note we didn't think of that.\n \nreply",
      "Interesting idea.  With the web scraping utility, do I need to specify which websites I wish for the api to scrape from or do I essentially just say, \"hey I want this data, go get it\"?If it's the latter, how do you go about making sure you're not about to download malicious data to my machine?\n \nreply",
      "Great question, so right now you can do both. It does work better if you simply enter in the url for your task.For the url generation we do we have safety checks for the urls however it's simply in the prompting. I would love to hear what sort of safety suggestions you have and/or concerns about this sort of experience. Right now we're still figuring out how best to enable people to utilize agents safely.\n \nreply",
      "Hi everyone, this is Art!Happy to hear all the thoughts for those who try the app out! Even if you just have ideas about how agents might look in their final form, there's so many avenue's this tech can take and we have a ton of wild ideas we'll be building so stay tuned. :D\n \nreply",
      "I would be very interested in your research on compressing HTML pages!\n \nreply",
      "Great! I will work on open sourcing that on our Github. It's basically a semantic format of html for AI agents to use the browser easily.\n \nreply",
      "Looked through their privacy policy, and they state the collect and use basically everything they can from your browser & system metadata, to the content you share and/or create. Not that different from every other attempt in the frothy AI space, but a real turn-off and hard no for me.\n \nreply",
      "Thank you for the feedback. Personally besides using our API server, we would like to find another way to deploy to anyone who has an issue with this/wants to run everything local (not just the client). Also I think if we had a OSS plug and play version where you could enter in your API keys locally it would help us ship to more devs. Would you be interested in this?\n \nreply",
      "Op, any comment on this?\n \nreply"
    ],
    "link": "https://meha.ai",
    "first_paragraph": ""
  },
  {
    "title": "The origin and unexpected evolution of the word \"mainframe\" (righto.com)",
    "points": 41,
    "submitter": "todsacerdoti",
    "submit_time": "2025-02-01T19:11:26 1738437086",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=42901184",
    "comments": [
      "> Based on my research, the earliest computer to use the term \"main frame\" was the IBM 701 computer (1952)> This shows that by 1962, \"main frame\" had semantically shifted to a new word, \"mainframe.\"> IBM started using \"mainframe\" as a marketing term in the mid-1980s.I must conclude it takes the competition 10 years to catch up to IBM, and IBM about 20 years to realize they have competition. Setting a countdown timer for IBM to launch an LLM in 2040.Thanks for researching and writing this up. It's a brilliant read!\n \nreply",
      "Author here. Anyone have interesting mainframe stories?\n \nreply",
      "A rumour from my mainframe days was that Digital Equipment hired lacemakers from france to show people how they did it. This was wiring up the core memory planes for the Dec-10 (I have one, a folded 3 part card) which just barely squeezes into the mainframe class.The guy who told me this was the Australian engineer sent over to help make the machine to bring back for UQ. He parked in the quiet side of the Maynard factory, not realising why the other drivers avoided it. Then his car got caught in a snowdrift.A prior engineer told me about the UUO wire wrap feature on the instruction set backplane: you were allowed to write your own higher level ALU \"macros\" in the instruction space by wiring patches in this backplane. Dec 10 had a 5 element complex instruction model. Goodness knows what people did in there but it had a BCD arithmetic model for the six bit data (36 bit word so 6 bytes of six bits in BCD mode)A guy from Latrobe uni told me for their Burroughs, you edited the kernel inside a permanently resident Emacs like editor which did recompile on exit and threw you back in on a bad compile. So it was \"safe to run\" when it decided your edits were legal.We tore down our IBM 3030 before junking it to use the room for a secondhand Cray 1. We kept so many of the water cooled chip pads (6\" square aluminium bonded grids of chips, for the water cooler pad. About 64 chips per pad) the recycler reduced his bid price because of all the gold we hoarded back.The Cray needed two regenerator units to convert Australian 220v to 110v for some things, and 400hz frequency for other bits (this high voltage ac frequency was some trick they used doing power distribution across the main CPU backplane) and we blew one up spectacularly closing a breaker badly. I've never seen a field engineer leap back so fast. Turned out reusing the IBM raised floor for a Cray didn't save us money: we'd assumed the floor bed for liquid cooled computers was the same; not so - Cray used a different bend radius for flourinert. The flourinert recycling tank was clear plastic, we named the Cray \"yabby\" and hung a plastic lobster in it. This tank literally had a float valve like a toilet cistern.When the Cray was scrapped one engineer kept the round tower \"loving seat\" module as a wardrobe for a while. The only CPU cabinet I've ever seen which came from the factory with custom cushions.\n \nreply"
    ],
    "link": "https://www.righto.com/2025/02/origin-of-mainframe-term.html",
    "first_paragraph": ""
  },
  {
    "title": "The Power of Poetry: Why Everyone Should Write (domofutu.substack.com)",
    "points": 5,
    "submitter": "domofutu",
    "submit_time": "2025-02-01T23:45:48 1738453548",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://domofutu.substack.com/p/the-power-of-poetry",
    "first_paragraph": ""
  }
]