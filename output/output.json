[
  {
    "title": "Python: The Documentary (lwn.net)",
    "points": 48,
    "submitter": "chmaynard",
    "submit_time": "2025-08-28T23:27:27 1756423647",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=45058171",
    "comments": [
      "I used to love Python, back when it was basically just an alternative to perl for scripting. Now it strikes fear into my heart when I encounter something largish written in Python because it usually means \"super slow bloated researchy untyped ai/math code that's a nightmare to work with\"reply",
      "1 hour and 24 minutes of python history, love it!reply",
      "[flagged]",
      "Justify that claim.reply"
    ],
    "link": "https://lwn.net/Articles/1035537/",
    "first_paragraph": "\nAttendees at EuroPython had the chance to preview part of\nPython: The Documentary during a\nkeynote panel. The full film, created by CultRepo, is now available on YouTube:This is the story of the world's most beloved programming language:\nPython. What began as a side project in Amsterdam during the 1990s\nbecame the software powering artificial intelligence, data science and\nsome of the world's biggest companies. But Python's future wasn't\ncertain; at one point it almost disappeared.This 90-minute documentary features Guido van Rossum, Travis\nOliphant, Barry Warsaw, and many more, and they tell the story of\nPython's rise, its community-driven evolution, the conflicts that\nalmost tore it apart, and the language's impact on... well...\neverything.The video\nof the keynote is also available.\n\n\n\n            Copyright \u00a9 2025, Eklektix, Inc.\n            \n            Comments and public postings are copyrighted by their creators.\n            Linux  is a registered trademark of Linus Torvalds\n\n"
  },
  {
    "title": "Ask HN: The government of my country blocked VPN access. What should I use?",
    "points": 727,
    "submitter": "rickybule",
    "submit_time": "2025-08-28T16:43:05 1756399385",
    "num_comments": 420,
    "comments_url": "https://news.ycombinator.com/item?id=45054260",
    "comments": [
      "Hello! I've got experience working on censorship circumvention for a major VPN provider (in the early 2020s).- First things first, you have to get your hands on actual VPN software and configs. Many providers who are aware of VPN censorship and cater to these locales distribute their VPNs through hard-to-block channels and in obfuscated packages. S3 is a popular option but by no means the only one, and some VPN providers partner with local orgs who can figure out the safest and most efficient ways to distribute a VPN package in countries at risk of censorship or undergoing censorship.- Once you've got the software, you should try to use it with an obfuscation layer.Obfs4proxy is a popular tool here, and relies on a pre-shared key to make traffic look like nothing special. IIRC it also hides the VPN handshake. This isn't a perfectly secure model, but it's good enough to defeat most DPI setups.Another option is Shapeshifter, from Operator (https://github.com/OperatorFoundation). Or, in general, anything that uses pluggable transports. While it's a niche technology, it's quite useful in your case.In both cases, the VPN provider must provide support for these protocols.- The toughest step long term is not getting caught using a VPN. By its nature, long-term statistical analysis will often reveal a VPN connection regardless of obfuscation and masking (and this approach can be cheaper to support than DPI by a state actor). I don't know the situation on the ground in Indonesia, so I won't speculate about what the best way to avoid this would be, long-term.I will endorse Mullvad as a trustworthy and technically competent VPN provider in this niche (n.b., I do not work for them, nor have I worked for them; they were a competitor to my employer and we always respected their approach to the space).reply",
      "> First things first, you have to get your hands on actual VPN software and configs.It would be nice if one of the big shortwave operators could datacast these packages to the world as a public service.reply",
      "There isn't enough bandwidth in HF to transmit data. Digital HF audio is 20 kHz wide so maybe 50kbps. The entire HF band is only 3-30 MHz.reply",
      "sure there is, you can send files over HF, it may not be FAST, but once you get it into the country, you can just copy the file with a faster method (eg: usb drive), WINLINK supports attachments, so you could absolutely send these files over HFreply",
      "If you're going to be using USB drives anyway, then using them to move files into the country would be faster.reply",
      "Wait until you find out what people used to do with phone lines!reply",
      "The problem is the countries, which censor Internet and block VPNs, also jam shortwave radio signals.reply",
      "I\u2019m not sure that\u2019s super feasible any longer with the advent of cheap SDRs. Over-the-horizon HF broadcast can be heard with a simple speaker wire antenna inside your house. If anyone is interested in trying to deploy such an idea, I\u2019d love to participate as an avid ham.reply",
      "Could I ask for a source on that and how common it is?Seems like it was used way back in the cold war (and even then not blocked/jammed) and I'd guess that current authoritarian regimes would perhaps not bother considering how few could use it.reply",
      "The USSR had an extensive shortwave radio jamming program!reply"
    ],
    "link": "item?id=45054260",
    "first_paragraph": ""
  },
  {
    "title": "Fuck up my site \u2013 Turn any website into beautiful chaos (fuckupmysite.com)",
    "points": 140,
    "submitter": "coloneltcb",
    "submit_time": "2025-08-28T21:04:36 1756415076",
    "num_comments": 43,
    "comments_url": "https://news.ycombinator.com/item?id=45057020",
    "comments": [
      "Many years ago the agency I worked for was tasked with delivering a new website for a major UK brand. The hipster London marketing agency we had to work alongside pushed so many garish ideas that I ended up creating a jQuery plugin called \"disco mode\". It set a timer and on every tick would select a random element on the page and apply a random effect. Slowly the UI would disintegrate into a maddening, incoherent mess of clashing colours and animations, and then there was also the plugin I mentioned.reply",
      "remember https://jackrugile.com/jrumble/ ?reply",
      "I somehow missed seeing this before and kinda love it.reply",
      "This reminds me of the Katamari Hack back in the day when bookmarklets were more popular. Surprised that it's still fully functional including the music considering it was released in 2011![1]: http://kathack.com/reply",
      "Incredible. Plays music and generates a snowball that absorbs page contents as one touches/clicks around a webpage.reply",
      "Unfortunately only on HTTP sites since the assets aren't served over HTTPS. Though thankfully it's easy enough to grab the resources and re-host them behind HTTP.And, of course, you can test it out on that website since it's served over HTTP.reply",
      "Reminds me of Desktop Destroyer! https://www.gamenora.com/game/desktop-destroyer/reply",
      "Yes, I remember using it, call for my dad while I was hiding in my room, he could see that I broke my monitor. :Dreply",
      "I thought of this the instant I saw the OP, I remember being in school and causing chaos with this, great memories.reply",
      "Oh wow that brings back memories. I remember playing with this back in my middle school daysreply"
    ],
    "link": "https://www.fuckupmysite.com/?url=https%3A%2F%2Fnews.ycombinator.com&torchCursor=true&comicSans=true&fakeCursors=true&peskyFly=true",
    "first_paragraph": "\u2593Some people just want to watch the web burnHeads up: Not every site plays nice with the chaos. Got feedback or discovered something broken? Let me know on Twitter"
  },
  {
    "title": "Some thoughts on LLMs and software development (martinfowler.com)",
    "points": 183,
    "submitter": "floverfelt",
    "submit_time": "2025-08-28T18:52:00 1756407120",
    "num_comments": 165,
    "comments_url": "https://news.ycombinator.com/item?id=45055641",
    "comments": [
      "> I\u2019ve often heard, with decent reason, an LLM compared to a junior colleague.No, they're like an extremely experienced and knowledgeable senior colleague \u2013 who drinks heavily on the job. Overconfident, forgetful, sloppy, easily distracted. But you can hire so many of them, so cheaply, and they don't get mad when you fire them!reply",
      "These metaphors all suck. Well, ok, yours is funny. But anyway, LLMs are just very different from any human.They are extremely shallow, even compared to a junior developer. But extremely broad, even compared to the most experienced developer. They type real fuckin fast compared to anyone on earth, but they need to be told what to do much more carefully than anyone on earth.reply",
      "I've gotten Claude Code to make CUDA kernels and all kinds of advanced stuff that there's zero percent chance a junior would pull off.AI is like a super advanced senior wearing a blindfold. It knows almost everything, it's super fast, and it gets confused pretty quickly about things after you tell it.reply",
      "it's not a senior though, because of the amount of oversight and guidance required. I trust  senior-plus human developers to do the right thing and understand why it's the right thing. For mission critical things I get another human senior to verify. There's no way I'd autonomously trust 2, 10 or any number of LLMs to do the same thing.reply",
      "You'd be surprised at what juniors can pull off. I have seen fresh-out-of-college new grads write performant GPU kernels that are used in real world library implementations for particular architectures.reply",
      "Most of the juniors I've worked with would make numerical errors and give up/declare premature victory before getting the implementation to a robust state. I'm sure there are exceptional young folks out there though.reply",
      "What's your sample size?reply",
      "have you ever asked a junior developer to write a cuda kernel?reply",
      "I've asked juniors to write easier things without success, and I applied the transitive property.reply",
      "I just spent a good 2 hours trying to debug a SM6 Vulkan issue with unreal engine using an LLM, it had got me to good state but UE kept falling to load a project, it transpired that the specific error message would provide a fix as the top Google result, which I found when I eventually decided to look for myself.LLM did help a lot to get some busy work out of the way, but it's difficult to know when you need to jump out of the LLM loop and go old skool.reply"
    ],
    "link": "https://martinfowler.com/articles/202508-ai-thoughts.html",
    "first_paragraph": "ArchitectureRefactoringAgileDeliveryMicroservicesDataTestingDSLAboutBooksFAQVideosContent IndexBoard GamesPhotographyHomeInsightsCareersRadarEngineeringRSSMastodonLinkedInBlueskyXBGGI\u2019m about to head away from looking after this site for a few weeks (part vacation, part work stuff). As I contemplate some weeks away from the daily routine, I feel an urge to share some scattered thoughts about the state of LLMs and AI.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2744\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2744\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2744\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2744I\u2019ve seen a few early surveys on the effect AI is having on software development, is it really speeding folks up, does it improve or wreck code quality? One of the big problems with these surveys is that they aren\u2019t taking into account how people are using the LLMs. From what I can tell the vast majority of LLM usage is fancy auto-complete, often using co-pilot. But those I know who get the most value from LLMs reckon that auto-complete isn\u2019t very useful, preferring approaches that allow the LLM to directly"
  },
  {
    "title": "My startup banking story (2023) (mitchellh.com)",
    "points": 169,
    "submitter": "dvrp",
    "submit_time": "2025-08-28T19:38:20 1756409900",
    "num_comments": 75,
    "comments_url": "https://news.ycombinator.com/item?id=45056177",
    "comments": [
      "Funny to see this pop up again (I'm the author). The year is now 2025 and I still use Chase as a personal bank and I'm now discovering new funny banking behaviors. I'll use this as a chance to share. :)My company had an exit, I did well financially. This is not a secret. I'm extremely privileged and thankful for it. But as a result of this, I've used a private bank (or mix) for a number of years to store the vast majority of my financial assets (over 99.99% of all assets, I just did the math). An unfortunate property of private banks is they make it hard to do retail-like banking behaviors: depositing a quick check, pulling cash from an ATM, but ironically most importantly Zelle.As such, I've kept my Chase personal accounts and use them as my retail bank: there are Chase branches everywhere, its easy to get to an ATM, and they give me easy access to Zelle! I didn't choose Chase specifically, I've just always used Chase for personal banking since I was in high school so I just kept using them for this.Anyways, I tend to use my Chase account to pay a bunch of bills, just because it's more convenient (Zelle!). I have 3 active home construction projects, plus pay my CC, plus pretty much all other typical expenses (utilities, car payments, insurance, etc.). But I float the money in/out of the account as necessary to cover these. We do accounting of all these expenses at the private bank side, so its all tracked, but it settles within the last 24-48 hours via Chase.Otherwise, I keep my Chase balance no more than a few thousand dollars.This really wigs out automated systems at Chase. I get phone calls all the time (like, literally multiple times per week) saying \"we noticed a large transfer into your account, we can help!\" And I cheekily respond \"refresh, it's back to zero!\" And they're just confused. To be fair, I've explained the situation in detail to multiple people multiple times but it isn't clicking, so they keep calling me.I now ignore the phone calls. Hope I don't regret that later lol.reply",
      "The fraud story is interesting, but something I had hoped would be addressed by the end of the post, wasn't. You wrote:>Someone out there is probably mentally screaming at me \"you fool!\" at this point. With hindsight, I agree...I was hoping the piece would end with what you would do now (or what should you have done) when Alex called you. Did I fail to understand something in the piece, and simply staying on the phone with Alex would have somehow avoided the fraud situation down the line? It doesn't seem so?If I were to get such a call, today, my instincts would be to engage in the same \"I'm fine\" get-off-the-phone-fast actions. What is the alternative?reply",
      "I would imagine this phone call's primary goal is trying to sell you higher margin financial products/services.reply",
      "The way you phrase that, it sounds like these services are more advantageous to the bank, but possibly not to the account holder? If so, that goes against the tone of the article, where Mitchell implies (I think?) that he would have benefited more from speaking to Alex than the other way around.reply",
      "Not really, these are probably mutually beneficial.  Having $35 million sitting in a cash account is, as Mitchell recalls in the article, pretty bad -- you're forgoing a lot of benefit for yourself.What the bank is probably mainly trying to do is not \"extract a higher margin\" from you, but rather to \"maintain your business.\"  While Alex might not have recognized that a startup that took in a bunch of financing and plopped it into a cash account and then didn't do much with it is a customer who might disappear at any time, on some institutional level the bank is aware of it.  They want you to regard them not as an interchangeable place that money sits, but as a valued partner who helps you do things with your money, and as a result you feel like it would both be a risk and a bother to extract all your money and go to some other bank.reply",
      "It can also be to maintain the bank. They don't like it when accounts get several more million dollars in them than expected and sometimes it can be too much to handle.reply",
      "Perhaps not directly/immediately - banks like others know that \u201crelationships\u201d drive long term repeat sales.reply",
      "I read the whole story and I'm still struggling to understand what you did wrong here. You indicated many times \"I know, that was a mistake\" (or similar phrases), but each time I was baffled because I saw no mistakes. It was your business, and you had every right to move around the funds within your account. What gives anyone at Chase the right to say diddly squat about how you manage your business' finances?reply",
      "I don't think he was trying to imply that he did anything wrong. He was admitting the ignorance regarding how banks and banking work... plus acknowledging that a lot of readers will have had experiences that would make them think \"Oh you young fool.\"In 2022 I lost my business banking and had to shut down a business that I owned for 20 years because it was related to the adult entertainment industry and, despite being completely legal and aboveboard, a single wire transfer that got a little bit of scrutiny resulted in them asking questions about what we did and, knowing that I was doing absolutely nothing wrong, I answered all of their questions truthfully and completely. A few months later I was told that we \"fell outside of their risk appetite\", our accounts were being closed... and for two months we searched for any bank or credit union in Canada but none would take us.A lot of industry insiders had that exact reaction: \"Are you stupid? Did you not know?! You NEVER admit that you're in this industry you moron!\" etc. We even had a very sympathetic branch manager suggest that we re-incorporate, re-brand and hide what we do (a front, in other words). I couldn't do that. And I mean, we had no issues for 20 years. 10 of which were banking as a corporation (was personal accounts before that since we ran it as a proprietorship) and I thought that being in Canada we were pretty progressive. No one I told on a personal or professional level had ever cared. So why would the banks? We were lawful so why should they care?reply",
      "Chargebacks.  Tons of people buy porn on a card then later deny it when their SO finds the bill.The industry is rife with this kind of fraud, and chargebacks represent tangible risk of financial loss, so banks just blanket ban working with certain industries.People who sell precious metal jewelry for credit card payments are another.reply"
    ],
    "link": "https://mitchellh.com/writing/my-startup-banking-story",
    "first_paragraph": "As a relatively new member of adult society, and an absolute infant of\nthe business world, I didn't think much about bank choice. I figured: you\nput money in, you take money out, they're all the same. I also figured a local\nbranch of a global bank is just a fungible tentacle of the giant banking\nmachine, so also... who cares. Both incorrect assumptions, but let's relive and\nrediscover the effect of these assumptions as I did.I start my company. I am a 22 year old recent college graduate living in San\nFrancisco and pursuing the startup dream. I file my incorporation paperwork\nand wait to receive the necessary information for one of the first\nsteps in the life of any new business: opening a bank account.My filing is processed and I receive my EIN while visiting my parents\nin a suburb of Los Angeles. I have time to kill during one of the days so\nI drive down to the nearest Chase bank branch and open a business banking\naccount. We'll call the person who helped me at the local branch Alex ("
  },
  {
    "title": "Death by PowerPoint: the slide that killed seven people (mcdreeamiemusings.com)",
    "points": 60,
    "submitter": "scapecast",
    "submit_time": "2025-08-28T21:44:47 1756417487",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=45057404",
    "comments": [
      "My dad headed up the redesign effort on the Lockheed Martin side to remove the foam PAL ramps (where the chunk of foam that broke off and hit the orbiter came from) from the external tank, as part of return-to-flight after the Columbia disaster. At the time he was the last one left at the company from when they had previously investigated removing those ramps from the design. He told me how he went from basically working on this project off in a corner on his own, to suddenly having millions of dollars in funding and flying all over for wind tunnel tests when it became clear to NASA that return-to-flight couldn't happen without removing the ramps.I don't think his name has ever come up in all the histories of this\u2014some Lockheed policy about not letting their employees be publicly credited in papers\u2014but he's got an array of internal awards from this time around his desk at home (he's now retired). I've always been proud of him for this.reply",
      "This article (as it makes clear) owes it's analysis at least largely to what Tufte has written about the Challenger disaster (1986) and Columbia Disaster (2003). He wrote about the Columbia one more fully in the second edition of The Cognitive Style of Powerpoint.Given that the link in the article to his report on his website is now broken, people might be interested in teh few page grabs that he has included in the \"comments\" on his site here[0].See also the article that he has re-posted under the \"comments\" section on his page on the matter[1].[0]: https://www.edwardtufte.com/notebook/new-edition-of-the-cogn...\n[1]: https://www.edwardtufte.com/notebook/the-columbia-evidence/reply",
      "If you haven't read it, I highly suggest you read Feynman's addendum to the Challenger disaster report:https://www.nasa.gov/history/rogersrep/v2appf.htmThe words \"a safety factor of three\" will live with me for every day of my life.reply",
      "This was an interesting article but it doesn't really provide solutions. I watched a few tech talks teaching a new API. Most slides were split, left side bullet poitns, right side either code or an image. As I was watching I was thinking \"isn't this supposed to be almost the worst style\"? but I was also thinking \"I can't think of any way to do this better\".  It's an API. It requires examples. And it requires something describing what to concentrate on, what the example or image is showing.I've been the plenty of great talks with just images, no words. But they fit the type of talk. I'm not sure an API talk would be better without bullet points. If you know of some to reference, please post links.reply",
      "Tufte did make specific recommendations that one should prepare a real document that your audience can and should read, and that they would have in front of them during the meeting. I'm not sure how best to translate that to your API example.reply",
      "How realistic was the idea of sending another shuttle up to rescue them? Would they have had enough oxygen?If they did a spacewalk and found the damage, what were their options?reply",
      "> There was actually an exercise done to work this out, at the direction of the Columbia Accident Investigation Board (CAIB). [...] In the CAIB\u2019s scenario, Atlantis would have launched with a four-person crew: two pilots, and two EVA mission specialists. [...]> A Columbia rescue mission would have been the most monumentally difficult and epic space mission in history, and it would have required absolutely everything going right to bring the crew home safely. But NASA has shown time and again its ability to rise to the occasion and bring its formidable engineering and piloting expertise to bear. Instead, the worst instincts of the agency - to micromanage and engage in wishful thinking instead of clear-eyed analysis - doomed the crew.https://www.quora.com/If-NASA-had-known-ahead-of-time-Columb...A much more deeply researched article here:https://arstechnica.com/science/2016/02/the-audacious-rescue...reply",
      "They did a bunch of studies. While it was POSSIBLE to get a rescue shuttle up to them if they ignored a bunch of safety and refurbishment procedures, the sheer amount of complexities probably meant they would lose 2 shuttles and 2 crew.reply",
      "The second shuttle would've flown with a crew of two, working with the knowledge that their ship was even more vulnerable than the one they intended to rescue.reply",
      "One option would have been to place whatever high melting point metal tools they could spare into the hole, and freeze water around them to hold it in place. It also might have been possible to change the series of s-curves and other maneuvers done during re-entry, in order to lessen the heating on the left wing.reply"
    ],
    "link": "https://mcdreeamiemusings.com/blog/2019/4/13/gsux1h6bnt8lqjd7w2t2mtvfg81uhx",
    "first_paragraph": "The space shuttle Columbia disintegrating in the atmosphere (Creative Commons)We\u2019ve all sat in those presentations.  A speaker with a stream of slides full of text, monotonously reading them off as we read along.  We\u2019re so used to it we expect it.  We accept it.  We even consider it \u2018learning\u2019. As an educator I push against \u2018death by PowerPoint\u2019 and I'm fascinated with how we can improve the way we present and teach.  The fact is we know that PowerPoint kills.  Most often the only victims are our audience\u2019s inspiration and interest.  This, however, is the story of a PowerPoint slide that actually helped kill seven people.January 16th 2003.  NASA Mission STS-107 is underway. The Space Shuttle Columbia launches carrying its crew of seven to low orbit.  Their objective was to study the effects of microgravity on the human body and on ants and spiders they had with them.  Columbia had been the first Space Shuttle, first launched in 1981 and had been on 27 missions prior to this one.  Where"
  },
  {
    "title": "Uncertain<T> (nshipster.com)",
    "points": 244,
    "submitter": "samtheprogram",
    "submit_time": "2025-08-28T17:22:54 1756401774",
    "num_comments": 53,
    "comments_url": "https://news.ycombinator.com/item?id=45054703",
    "comments": [
      "A small note, but GPS is only well-approximated by a circular uncertainty in specific conditions, usually open sky and long-time fixes. The full uncertainty model is much more complicated, hence the profusion of ways to measure error. This becomes important in many of the same situations that would lead you to stop treating the fix as a point location in the first place. To give a concrete example, autonomous vehicles will encounter situations where localization uncertainty  is dominated by non-circular multipath effects.If you go down this road far enough you eventually end up reinventing particle filters and similar.reply",
      "Vehicle GPS is usually augmented by a lot of additional sensors and assumptions, notably the speedometer, compass, and knowledge the you'll be on one of the roads marked on its map. Not to mention a fast fix because you can assume you haven't changed position since you last powered on.reply",
      "As well as a fast fix because you know what mobile cell or wifi network you're on.reply",
      "Does this handle covariance between different variables? For example, the location of the object your measuring your distance to presumably also has some error in it's position, which may be correlated with your position (if, for example, if it comes from another GPS operating at a similar time).Certainly a univarient model in the type system could be useful, but it would be extra powerful (and more correct) if it could handle covariance.reply",
      "To properly model quantum mechanics, you\u2019d have to associate a complex-valued wave function with any set of entangled variables you might have.reply",
      "If you need to track covariance you might want to play with gvar https://gvar.readthedocs.io/en/latest/ in python.reply",
      "for mechanical engineering drawings to communicate with machinists and the like, we use toleranceseg. 10cm +8mm/-3mmfor what the acceptable range is, both bigger and smaller.id expect something like \"are we there yet\" referencing GPS should understand the direction of the error and what directions of uncertainty are better or worsereply",
      "Something that's bugged me about this notation though is that sometimes it means \"cannot exceed the bounds\" and sometimes it means \"only exceeds the bounds 10% of the time\"reply",
      "I don\u2019t think I\u2019ve ever seen mechanical drawings have \u201c90% confidence\u201d dimensions like this. If a part\u2019s too big then it won\u2019t fit, and it\u2019s probably useless.reply",
      "If a test procedure is verifying all dimensional accuracy, it can be assumed to be bounding tolerance. If it's a mass production line with less than 100% testing of parts, you'd have to expect that some outliers get by and the tolerance is something like 3-sigma on a Gaussian.reply"
    ],
    "link": "https://nshipster.com/uncertainty/",
    "first_paragraph": "You know what\u2019s wrong with people?\n                They\u2019re too sure of themselves.Better to be wrong and own it than be right with caveats.\n                Hard to build a personal brand out of nuance these days.\n                People are attracted to confidence \u2014 however misplaced.But can you blame them? (People, that is)\n                Working in software,\n                the most annoying part of reaching Senior level\n                is having to say \u201cit depends\u201d all the time.\n                Much more fun getting to say\n                \u201clet\u2019s ship it and iterate\u201d as Staff or\n                \u201cthat won\u2019t scale\u201d as a Principal.Yet, for all of our intellectual humility,\n                why do we write vibe code like this?GPS coordinates aren\u2019t exact.\n                They\u2019re noisy. They\u2019re approximate. They\u2019re probabilistic.\n                That horizontalAccuracy property tucked away in your CLLocation object\n              is trying to tell you something important:\n              you\u2019"
  },
  {
    "title": "Expert LSP the official language server implementation for Elixir (github.com/elixir-lang)",
    "points": 49,
    "submitter": "pimienta",
    "submit_time": "2025-08-28T21:36:36 1756416996",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=45057322",
    "comments": [
      "There are several different LSP implementations of Elixir, each with their own pros and cons. Last year they all agreed to collaborate on an LSP; is this going to be the result of that?https://elixir-lang.org/blog/2024/08/15/welcome-elixir-langu...reply",
      "Yes, that's correct. Pretty exciting.reply",
      "Yupreply",
      "The architecture is remarkable. The lengths they\u2019ve gone to for language version compatibility, and protecting app namespaces is especially impressive.https://github.com/elixir-lang/expert/blob/main/pages/archit...reply",
      "Interesting choice to use justreply",
      "It has both a justfile and a makefile at the root, even. Most of us seem to want to use it to throw make away entirely.That said, I consider `just` very language-agnostic and useful because of that, and I consider mix pretty bad at any workflow needs that isn't directly concerned with BEAM.reply",
      "It's not technically a make replacement (make does do things like incremental build management etc.), but it just goes to show how bad the DX of make is.reply",
      "IMO 'just' replaces make where make shouldn't be used - generic task runner.reply",
      "I think it's hard for me to name better software than make. TeX, maybe? that seems like an insanely high bar to clear.reply",
      "i'm a pretty big fan of just, personally, but do not consider that to be the world's most well-considered position by any means...reply"
    ],
    "link": "https://github.com/elixir-lang/expert",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n        Official Elixir Language Server Protocol implementation\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Expert is the official language server implementation for the Elixir programming language.You can download Expert from the releases page for your\noperating system and architecture.For editor specific installation instructions, please refer to the Installation InstructionsIf you want to try out the latest features, you can download a nightly build.Using the GH CLI, you can run the following command to download the latest nightly build:Then point your editor to the downloaded binary.To build Expert from source, you need Zig 1.14.1 installed o"
  },
  {
    "title": "Building your own CLI coding agent with Pydantic-AI (martinfowler.com)",
    "points": 107,
    "submitter": "vinhnx",
    "submit_time": "2025-08-28T18:34:29 1756406069",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=45055439",
    "comments": [
      "we[0] have a pretty complex agent[1] running on Pydantic AI. The team is very responsive to bugs / feature requests. If I had to do it over again, I'd pick Pydantic AI again.0 - https://www.definite.app/1 - https://pydantic.dev/articles/building-data-team-with-pydant...reply",
      "I wanted to love pydantic AI as much as I love pydantic but the killer feature is pydantic-model-completion and weirdly.. it has always seemed to work better for me when I naively build it from scratch without pydantic AI.I haven't looked deeply into pydantic's implementation but this might be related to tool-usage vs completion [0], the backend LLM model, etc.  All I know is that with the same LLM models, `openai.client.chat.completions` + a custom prompt to pass in the pydantic JSON schema + post-processing to instantiate SomePydanticModel(*json) creates objects successfully whereas vanilla pydantic-ai rarely does, regardless of the number of retries.I went with what works in my code, but didn't remove the pydantic-ai dependency completely because I'm hoping something changes.  I'd say that getting dynamic prompt context by leveraging JSON schemas, model-and-field docs from pydantic, plus maybe other results from runtime-inspection (like the actual source-code) is obviously a very good idea. Many people want something like  \"fuzzy compilers\" with structured output, not magical oracles that might return anything.Documentation is context, and even very fuzzy context is becoming a force multiplier.  Similarly languages/frameworks with good support for runtime-inspection/reflection and have an ecosystem with strong tools for things like ASTs really should be the best things to pair with AI and agents.[0]: https://github.com/pydantic/pydantic-ai/issues/582reply",
      "> All I know is that with the same LLM models, `openai.client.chat.completions` + a custom prompt to pass in the pydantic JSON schema + post-processing to instantiate SomePydanticModel(*json) creates objects successfully whereas vanilla pydantic-ai rarely does, regardless of the number of retries.That's very odd, would you mind sharing the Pydantic model / schema so I can have a look? (I'm a maintainer) What you're doing with a custom prompt that includes the schema sounds like our Prompted output mode (https://ai.pydantic.dev/output/#prompted-output), but you should get better performance still with the Native or Tool output modes (https://ai.pydantic.dev/output/#native-output, https://ai.pydantic.dev/output/#tool-output) which leverage the APIs' native strict JSON schema enforcement.reply",
      "Thanks for the reply.  Native output is indeed what I'm shooting for. I can't share the model directly right now, but putting together a min-repro and moving towards and actual bug report is something on todo list.One thing I can say though.. my models differ from the docs examples mostly in that they are not \"flat\" with simple top-level data structures.  They have lots of nested models-as-fields.reply",
      "Thanks, a reproducible example would be very useful. Note that earlier this month I made Pydantic AI try a lot harder to use strict JSON mode (in response to feedback from Python creator Guido of all people: https://github.com/pydantic/pydantic-ai/issues/2405), so if you haven't tried it in a little while, the problem you were seeing may very well have been fixed already!reply",
      "> https://github.com/pydantic/pydantic-ai/issues/2405Thanks, this is a very interesting thread on multiple levels. It does seem related to my problem and I also learned about field docstrings :) I'll try moving my dependency closer to the bleeding edgereply",
      "After maintaining my own agents library for a while, I\u2019ve switched over to pydantic ai recently. I have some minor nits, but overall it's been working great for me. I\u2019ve especially liked combining it with langfuse.Towards coding agents, I wonder if there are any good / efficient ways to measure how much different implementations work on coding? SWE-bench seems good, but expensive to run. Effectively I\u2019m curious for things like: given tool definition X vs Y (eg. diff vs full file edit), prompt for tool X vs Y (how it\u2019s described, does it use examples), model choice (eg. MCP with Claude, but python-exec inline with GPT-5), sub-agents, todo lists, etc. how much across each ablation, does it matter? And measure not just success, but cost to success too (efficiency).Overall, it seems like in the phase space of options, everything \u201ckinda works\u201d but I\u2019m very curious if there are any major lifts, big gotchas, etc.I ask, because it feels like the Claude code cli always does a little bit better, subjectively for me, but I haven\u2019t seen a LLMarena or clear A vs B, comparison or measure.reply",
      "These abstractions are nice to not get locked in with one llm provider - but like with langchain - once you use some more niche feature the bugs do shine through. I tried it out with structured output for azure openai but had to give up since somewhere somewhat was broken and it's difficult to figure out if it's the abstraction or the library of the llm provider which the abstraction uses.Nevertheless i would strongly recommend to not use directly the libraries of the ai providers as you get quickly locked in a extremely fast paced market where today's king can change weekly.reply",
      "Pydantic AI maintainer here! Did you happen to file an issue for the problem you were seeing with Azure OpenAI?The vast majority of bugs we encounter are not in Pydantic AI itself but rather in having to deal with supposedly OpenAI Chat Completions-compatible APIs that aren't really, and with local models ran through e.g. Ollama or vLLM that tend to not be the best at tool calling.The big three model providers (OpenAI, Claude, Gemini) and enterprise platforms (Bedrock, Vertex, Azure) see the vast majority of usage and our support for them is very stable. It remains a challenge to keep up with their pace of shipping new features and models, but thanks to our 200+ contributors we're usually not far behind the bleeding edge in terms of LLM API feature coverage, and as you may have seen we're very responsive to issues and PRs on GitHub, and questions on Slack.reply",
      "In this example, you get locked into pydantic_ai, another proprietary provider.reply"
    ],
    "link": "https://martinfowler.com/articles/build-own-coding-agent.html",
    "first_paragraph": "ArchitectureRefactoringAgileDeliveryMicroservicesDataTestingDSLAboutBooksFAQVideosContent IndexBoard GamesPhotographyHomeInsightsCareersRadarEngineeringRSSMastodonLinkedInBlueskyXBGGLearning by doingCLI coding agents are a fundamentally different tool to chatbots or\n  autocomplete tools - they're agents that can read code, run tests, and update\n  a codebase. While commercial tools are impressive, they don't understand the\n  particular context of our environment and the eccentricities of our specific\n  project. Instead we can build our own coding agent by assembling open source\n  tools, using our specific development standards for: testing, documentation\n  production, code reasoning, and file system operations. 27 August 2025Ben O\u2019Mahony is Principal AI Engineer at Thoughtworks. He is a results-driven AI/Engineering leader with a track record of building high-performing teams and shipping business-critical AI, ML and data products and platforms at scale. He has deep expertise across the"
  },
  {
    "title": "RSS Is Awesome (evanverma.com)",
    "points": 67,
    "submitter": "edverma2",
    "submit_time": "2025-08-28T23:04:01 1756422241",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=45058024",
    "comments": [
      "Amen! If you're looking to fill out your RSS reader, I maintain a directory of tech blogs (ctrl+f \"feed\" for rss links):[0] https://blogs.hnOther good directories:[1] https://ooh.directory/[2] https://blogroll.org/reply",
      "Boy do I feel old if a short, low content PSA about the existence of RSS is considered \"hacker news.\"reply",
      "RSS is the antidote to algorithm feeds. I\u2019m glad for any mention of it. 90% of the tools users need were built 1970-2000 including RSS.reply",
      "Not only that, but an ad for their mobile app. Pretty low quality content.reply",
      "It\u2019s not their app. NetNewsWire is developed by Brent Simmons and has been around for over 20 years. It\u2019s free and open source. Last I saw, he didn\u2019t even accept donations.reply",
      "Seen this? https://microformats.org/wiki/h-feedreply",
      "I mean.. it shouldn't be controversial.. but people keep claiming rss is dead.not in my world it aint.reply",
      "Podcasts needs an RSS feed, that about sums it up how not dead it is.reply",
      "You're one of today's unlucky 10,000.reply",
      "No love for Atom?* https://en.wikipedia.org/wiki/Atom_(web_standard)reply"
    ],
    "link": "https://evanverma.com/rss-is-awesome",
    "first_paragraph": "\n\n      NetNewsWire\n    \n\n\n       is my latest most-used iPhone app. It is a simple, free RSS reader. \n    \n\n\n      RSS is an old technology that it seems most people have forgotten about. Here's how it works: you enter a link to an RSS \"feed\", and your app pulls data from this feed every few minutes or so. When there is a new post from your feed, that post is pulled directly to your app. \n    \n\n\n      RSS is really simple, so it is still very well supported. Notably, all substack publications automatically have an RSS feed included at \n    \nhttps://{{substack-domain}}/feed\n\n      . \n    \n\n\n      Blogs are great but I don't enjoy reading posts in my email, having to remember the websites each one is hosted at, or reading from each publications' different typesetting opinions with varying pop-ups and advertisements. An RSS reader centralizes all content from your blogs into a single place for reading. \n    \n\n\n      Since I started using this app I spend much more of my \"mindless phone t"
  },
  {
    "title": "TuneD is a system tuning service for Linux (tuned-project.org)",
    "points": 32,
    "submitter": "tanelpoder",
    "submit_time": "2025-08-25T15:03:57 1756134237",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=45014656",
    "comments": [
      "There is something a bit wacky about a performance service implemented in an interpreted language like Python whether it is tuned or auto-cpufreq. Tuned does seem to be as good as it gets for the moment.x86 cpus don't have the power efficiency to do the work we now expect of them in thin and light laptops with difficult thermal constraints. You can push them one way or another. You can have them fast with a fan like a jet engine or you can have them cold and running like a 10 year old computer or put the dial somewhere inbetween but there is only so much you can do.reply",
      "I haven't tested Intel's efficiency cores (E-cores) myself - would these address the need for desktops/laptops?reply",
      "Apple and many arm mobile platforms also have a mix of performance and efficiency cores so it seems to be a proven approach. I guess it comes down to implementation. Intel's efficiency cores by themselves (eg N series) apparently make nice little appliances, often better value than something like a RPi. I don't know how much they help their higher performance devices conserve energy.I have one of Intel's old desktop class processors in a refurbished ex-office mini-desktop plugged into a power meter running a few services for the household and the idle performance isn't bad at all. I don't understand why my laptop doesn't run colder and longer given the years of development between them.There is also the race to idle strategy where you spike the performance cores to get back to idle which probably works well with a lot of office usage but not so well with something more demanding like games or benchmarks.reply",
      "I saw really big power savings when I started using TuneD. Such a huge upgrade for Linux users! From 8 months ago, going from 120 -> 85W. More recently got my desktop down to 65, yay.\nhttps://news.ycombinator.com/item?id=42636350There's also API compatibility with the power-profiles-daemon, which didn't ever help me that much (I'd also done some basic tuning myself), and which has been unmaintained for a while now. But there's still a variety of utilities which target the old ppd.reply",
      "I've used `tuned` a lot.  It's really extremely good for personal machines/workstations, and really okay for servers.  In my case I'm almost 50/50 with it in professional cases, where 50% of the time I had a real good time with it, and 50% of the time I turned it off and used startup scripts (like cloud-init per-boot and whatnot).Overall, I'd say give it a shot as it can be really powerful and I do actually like it.  Don't be afraid to go 'no, I know how to do this better, myself' and turn it off though.reply",
      "I disable it whenever setting up a new system.  It gets irq bindings for networking wrong every single time, and moves irqs around in ways that defeats the whole point of having per CPU queues.  Not sure why that behaviour is enabled by default as it makes no sense.reply",
      "one hot tip is that tuned has a translation tool for power-profiles-daemon, meaning you can change the profile via gnome / kdehttps://archlinux.org/packages/extra/any/tuned-ppd/reply",
      "What I'd like is a tool that can be run on a fresh linux install to show what's not working correctly and maybe some diagnostics. Does that exist?Things like suspend to RAM/disk working, GPU performance is reasonable, WiFi and disk speeds aren't slower than expected.reply",
      "> Things like suspend to RAM [...] workingIf you're on and AMD laptop then suspend to ram can be tested with amd-debug-tools[0].> WiFiHere[1] is a list of public iperf3 servers. You can test your connection speed with (change host name and port to appropriate server):  # Test upload speed\n  iperf3 -c host-name-here -p 5201\n\n  # Test download speed\n  iperf3 -c host-name-here -p 5201 -R\n\nYou can also launch your own server so you're not limited by your internet speed (I usually run one on my router):  iperf3 -s -p 5201\n\n  [0] https://git.kernel.org/pub/scm/linux/kernel/git/superm1/amd-debug-tools.git/about/\n  [1] https://iperf.fr/iperf-servers.phpreply"
    ],
    "link": "https://tuned-project.org/",
    "first_paragraph": "TuneD is a system tuning service for Linux. It:TuneD profiles:can be defined hierarchically, which reduces duplication \n        and simplifies maintenance:More specialized profiles can inherit generic profiles and just\n        change what is needed instead of duplicating the code.\n        For example, you can built a generic profile for HTTP server upon\n        the throughput-performance profile and later create two \n        more specialized profiles for the Apache server and the Nginx server, \n        basing both profiles on the generic HTTP server profile.\n        support full rollback:The system can be easily returned to the\n        state before the profile was applied. This can be handy for testing,\n        benchmarking, experimenting, and so on.\n        For example, you can set up a cron rule \n        to apply a certain profile during business hours and \n        a different one at night. include a number of predefined profiles for common use cases:For example, presets for high thr"
  },
  {
    "title": "AI adoption linked to 13% decline in jobs for young U.S. workers: study (cnbc.com)",
    "points": 181,
    "submitter": "pseudolus",
    "submit_time": "2025-08-28T14:13:44 1756390424",
    "num_comments": 268,
    "comments_url": "https://news.ycombinator.com/item?id=45052423",
    "comments": [
      "High interest rates + tariff terror -> less investment -> less jobsBut let's blame AIreply",
      "Let's read the paper instead: https://digitaleconomy.stanford.edu/wp-content/uploads/2025/...It presents a difference-in-differences (https://en.wikipedia.org/wiki/Difference_in_differences) design that exploits staggered adoption of generative AI to estimate the causal effect on productivity. It compares headcount over time by age group across several occupations, showing significant differentials across age groups.Page 3: \"We test for a class of such confounders by controlling for firm-time effects in an event study regression, absorbing aggregate firm shocks that impact all workers at a firm regardless of AI exposure. For workers aged 22-25, we find a 12 log-point decline in relative employment for the most AI-exposed quintiles compared to the least exposed quintile, a large and statistically significant effect.\"reply",
      "I appreciate the link to differences in differences, I didn't know what to call this method.The OP's point could still be valid: it\u2019s still possible that macro factors like inflation, interest rates, or tariffs land harder on the exact group they label \u2018AI-exposed.\u2019 That makes the attribution messy.reply",
      "Those fixed effects are estimated separately for each age group, controlling for that.pg. 19, \"We run this regression separately for each age group.\"reply",
      "Were entry level jobs the first to go in earlier developer downturns?Is AI being used to attempt to mitigate that effect?I don't think their methods or any statistical method could decouple a perfectly correlated signal.Without AI, would junior jobs have grown as quickly as other?reply",
      "You really do have to account for why this is mainly happening in industries that are adopting AI, why it's almost exclusively impacting entry-level positions (with senior positions steady or growing), and why controlling for broad economic conditions failed to correct this. I doubt very much that these three Stanford professors would be blindsided by the concept of rates and tarriffs.reply",
      "My personal theory is that the stock market rewards the behavior of cutting jobs as a signal of the company being on the AI bandwagon. Doesn't matter if the roles were needed or not. Line goes up, therefore it is good.This is a complete reversal in the past where having a high headcount was an easy signal of a company's growth (i.e. more people, means more people building features, means more growth).Investors are lazy. They see one line go down, they make the other line go up.CEOs are lazy. They see line go up when other line goes down. So they make other line go down.(I am aware that \"line go up\" is a stupid meme. But I think it's a perfect way to describe what's happening. It is stupid, lazy, absurd, memetic. It's the only thing that matters, stripped off of anything that is incidental. Line must go up.)reply",
      "Given the timeline this is more likely a reversion to the mean following the end of zero interest rate policy.reply",
      "Juniors become seniors.If we replace all juniors with AI, in a few years there won't be skilled talent for senior positions.AI assistance is a lot different than AI running the company.  Making expensive decisions.  While it could progress, bear in mind that some seniors continue to move up in the ranks.  Will AI eventually be the CEO?We all dislike how some CEOs behave, but will AI really value life at all?  CEOs have to have some place to live, after all.reply",
      "The AI will at least be cheaper than a CEO, it might also be more competent and more ethical. The argument against making a Large Language Model the CEO seems to mostly be about protecting the feelings of the existing CEO, maybe the Board should  look past these \"feelings\" and be bold ?reply"
    ],
    "link": "https://www.cnbc.com/2025/08/28/generative-ai-reshapes-us-job-market-stanford-study-shows-entry-level-young-workers.html",
    "first_paragraph": ""
  },
  {
    "title": "Are OpenAI and Anthropic losing money on inference? (martinalderson.com)",
    "points": 436,
    "submitter": "martinald",
    "submit_time": "2025-08-28T10:15:22 1756376122",
    "num_comments": 418,
    "comments_url": "https://news.ycombinator.com/item?id=45050415",
    "comments": [
      "This article's math is wrong on many fundamental levels. One of the most obvious ones is that prefill is nowhere near bandwidth bound.If you compute out the MFU the author gets it's 1.44 million input tokens per second * 37 billion active params * 2 (FMA) / 8 [GPUs per instance] = 13 Petaflops per second. That's approximately 7x absolutely peak FLOPS on the hardware. Obviously, that's impossible.There's many other issues with this article, such as assuming only 32 concurrent requests(?), only 8 GPUs per instance as opposed to the more efficient/standard prefill-decode disagg setups, assuming that attention computation is the main thing that makes models compute-bound, etc. It's a bit of an indictment of HN's understanding of LLMs that most people are bringing up issues with the article that aren't any of the fundamental misunderstandings here.reply",
      "Agree that the writeup is very wrong, especially for the output tokens. Here is how anyone with enough money to allocate a small cluster of powerful GPUs can decode huge models at scale, since nearly 4 months ago, with costs of 0.2 USD/million output tokens.https://lmsys.org/blog/2025-05-05-large-scale-ep/This has gotten significantly cheaper yet with additional code hacks since then, and with using the B200s.reply",
      "As much as I appreciate you saying the math is wrong, it doesn\u2019t really help me adjust my expectations unless you provide correct numbers as well.reply",
      "So, bottom line, do you think it\u2019s probable that either OpenAI or Anthropic are \u201closing money on inference?\u201dreply",
      "No. In some sense, the article comes to the right conclusion haha. But it's probably >100x off on its central premise about output tokens costing more than input.reply",
      "Thanks for the correction (author here). I'll update the article - very fair point on compute on input tokens which I messed up. Tbh I'm pleased my napkin math was only 7x off the laws of physics :).Even rerunning the math on my use cases with way higher input token cost doesn't change much though.reply",
      "The 32 parallel sequences is also arbitrary and significantly changes your conclusions. For example, if they run with 256 parallel sequences then that would result in a 8x cheaper factor in your calculations for both prefill and decode.The component about requiring long context lengths to be compute-bound for attention is also quite misleading.reply",
      "Anyone up to publishing their own guess range?reply",
      "I\u2019m pretty sure input tokens are cheap because they want to ingest the data for training later no? They want huge contexts to slice up.reply",
      "I've done the modeling on this a few times and I always get to a place where inference can run at 50%+ gross margins, depending mostly on GPU depreciation and how good the host is at optimizing utilization. The challenge for the margins is whether or not you consider model training costs as part of the calculation. If model training isn't capitalized + amortized, margins are great. If they are amortized and need to be considered... yikesreply"
    ],
    "link": "https://martinalderson.com/posts/are-openai-and-anthropic-really-losing-money-on-inference/",
    "first_paragraph": ""
  },
  {
    "title": "Launch HN: Dedalus Labs (YC S25) \u2013 Vercel for Agents",
    "points": 44,
    "submitter": "windsor",
    "submit_time": "2025-08-28T16:22:02 1756398122",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=45054040",
    "comments": [
      "Congrats on the launch. I see that you're charging 5% on Balance Reloads. This pricing model seems to be getting popular across multi-LLM applications. Was curious to know how did you go about implementing it? or are you just passing on the 5% of openrouterreply",
      "Good eye. A ~5% surcharge on prepaid credits is the standard model right now for most multi-LLM services. We actually do not use OpenRouter internally, so this number is flexible. One thing I'll note is that we try to be as upfront and transparent about our platform fee as possible so that no one is surprised.reply",
      "Oh interesting. I've previously looked into implementing it myself but seemed like it would require a lot of effort. I would love to connect and learn more about your implementation. What's the best way to reach out to you? My email is available on my profile.reply",
      "Congrats on the launch and looks interesting! I love how easy it is to combine local code \"tools\" with remote mcp servers. The marketplace looks promising, but would be helpful to have some curation as many of the servers don't have descriptions and link to private github repo's. Neat vision and look forward to trying this.reply",
      "Thanks! Really glad you noticed this feature. Mixing client-side and server-side tool calls was something we spent a lot of time thinking through.The current SOTA, e.g. OpenAI\u2019s Responses API or Anthropic\u2019s Computer Use API, basically mandates that server-side tool results return directly, while client-side tool results have to be manually parsed and executed by the user (for obvious security reasons). As a result, it was extremely unclear how a user would be able to chain together tool calls that mixed local and remote tools.We wanted to close this DX gap, which surprisingly had no real incumbent solution. Users should be able to just define tools and get back clean responses. For power users, we still support manual JSON parsing for full low-level control, but our belief is simple: developers should spend their time building, not doing plumbing work like post-processing tool results.reply",
      "Congrats on the launch! I\u2019m curious, do you have to store the tool inputs and outputs on the server side while either of the sides are waiting for response? I\u2019m building a specialized coding agent for integrations and I had to avoid stateful api, because I don\u2019t want to store user code.reply",
      "We tried to keep things simple, so our runner utility class is currently stateless.However, it's highly extensible, and we can support stateful logic if we wanted to. For instance, we have a cool concept called \"Policy\" in our SDKs, which is basically a user-defined callback function that is run after each runner step (see our docs for more info). You can build some pretty advanced use cases with this, e.g. executing conditional database calls on a per-step basis.The code for the runner is open source, in case you wanted to check out how we did it: https://github.com/dedalus-labs/dedalus-sdk-python/blob/main...reply",
      "Congratulations on the launch. Been trying this out for the past hour, and really like how easy it is to host your own MCP servers. Would love to see more public MCPs on the marketplace. Also, I was wondering when you plan to support auth?reply",
      "Thanks for trying us out! Let me know if you run into any issues with deployment or the like. If you want a clean MCP template to get up and running, check out this example: https://github.com/dedalus-labs/brave-search-mcpAuth is a big one for us, and we're working really hard to provide a robust auth experience that is easy to use for both LLM agents and human users. One of our goals is to help mold the shape that the community takes with MCP. We\u2019ll be launching our auth solution around end of September. I\u2019m personally really excited to tackle this problem.reply",
      "Congratulations on the launch!I\u2019ve been writing Go for the past 4 years, and I\u2019d strongly suggest avoiding Stainless for auto-generating Go SDKs. Some of the issues I\u2019ve run into:\n- Awkward package naming (e.g., githubcomdedaluslabsdedalussdkgo)\n- Methods with unnecessary complexity, including excessive use of reflection for JSON handling\n- Other codegen quirks that make the SDK harder to use and maintainFrom experience, I\u2019d recommend either using another code generator or hand-writing the Go SDK for a cleaner and more idiomatic developer experience.reply"
    ],
    "link": "item?id=45054040",
    "first_paragraph": ""
  },
  {
    "title": "A forgotten medieval fruit with a vulgar name (2021) (bbc.com)",
    "points": 67,
    "submitter": "ohjeez",
    "submit_time": "2025-08-27T16:34:31 1756312471",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=45041879",
    "comments": [
      "> Medieval Europeans were fanatical about a strange fruit that could only be eaten rotten.To be clear, you do not let the medlar \"rot\" before eating.Rotting involves decay by microorganisms -- fungi, bacteria, yeasts.What the medlar does is totally different. It has an enzyme within it that continues to break down the fruit, so it goes from rock hard to soft and edible.Because this is a different chemical process from traditional ripening, someone gave this the name \"bletting\". But it's definitely not \"rotting\".There's an evolutionary theory that by delaying when the fruit could be eaten, it could attract animals in the winter that would be more likely to eat it (since other fruits were no longer available) and potentially transport its seeds longer distances.reply",
      "Quince and some persimmons are also commonly bletted before consumption... so it's not even unique to medlars.reply",
      "You can say a fruit is ripening, rotting, or bletting.Would you say a fruit is ripe, rotted\u2026 bletted? Blet?reply",
      "Bletted.> \"In Notes on a Cellar-Book, the great English oenophile George Saintsbury called bletted medlars the \"ideal fruit to accompany wine.\"https://en.wikipedia.org/wiki/Blettingreply",
      "The 1989 Baird and Thieret paper referenced in the article might be my favorite research paper ever.  I read it soon after it came out in the reading room of my college library.  After finishing it, I genuinely was uncertain whether it was a real paper or a Borgesian spoof. Bletted for months before it's edible?  A Shakespearean insult?  On the Unicorn Tapestry and I'd never heard of it?Here's a full copy of the paper if this intrigues you: https://sci-hub.se/10.1007/BF02858732Since then, I've confirmed that it actually exists.  I've even tasted the fruit.  It's... OK.  It's a reasonably tasty spiced brown apple/pear sauce with a grainy texture, but with the spices already built in.  I've got my own tree planted---more for the novelty than desire for the fruit---and hope I'll finally get a few of my own this year.Edit:  If you are looking for more bizarre ways the Medlar pops up in strange places, here's a page about its traditional use in Basque culture as a symbol of authority: https://alberdimakila.com/en/medlar-tree-wood-basque-walking...reply",
      "Speaking of forgotten fruit.The evolution of watermelon is fascinating.  It happened in (relatively) recent human history and has really stark changes.There are old paintings of watermelon from the 17th century and it looks nothing like modern watermelon. [1]Another wild human guided evolution is the evolution of the chicken. [2] That one literally happened in the last 100 years.  A modern chicken is 3x larger than a chicken from the 1950s.[1] https://en.wikipedia.org/wiki/Watermelon#/media/File:Pastequ...[2] https://www.zmescience.com/feature-post/how-chickens-tripled...reply",
      "I have trouble believing this, though I've heard it before. The watermelon in the painting looks exactly like the insides I've seen in my homegrown watermelons when things don't go right, i.e. under watered, not fully pollinated, or just underripe.reply",
      "I'm reminded of what we did to the pug, which used to look like this:https://en.wikipedia.org/wiki/Pug#/media/File:Henry_Bernard_...Some breeders are trying to breed these traits back in, yielding the \"Retro Pug\" unofficial breed. Even the old pug is quite a heavy hand we've exerted on dog evolution.reply",
      "The watermelon in [1] is what you'll get when you try to grow one in your garden.The chicken in [2] is what you'll see when you look at a feral chicken.reply",
      "Forgotten by some, maybe, but there are many Iranian-American and Armenian-American families with medlar trees in their suburban LA yards. It is sold at Paradise Nursery in Chatsworth.reply"
    ],
    "link": "https://www.bbc.com/future/article/20210325-the-strange-medieval-fruit-the-world-forgot",
    "first_paragraph": "Medieval Europeans were fanatical about a strange fruit that could only be eaten rotten. Then it was forgotten altogether. Why did they love it so much? And why did it disappear?In 2011, archaeologists found something unusual in a Roman toilet.The team were excavating the ancient village of Tasgetium (now Eschenz, Switzerland), ruled by a Celtic king who was personally given the land by Julius Caesar. It was built on the banks of the river Rhine, along what was then an important trade route \u2013 and as a result, its remains have been steeped in water ever since. What should have rotted away centuries ago was uncovered in a remarkable state of preservation, protected by the lack of oxygen in the boggy conditions.It was here that, nestled among the remains of familiar foods such as plums, damsons, cherries, peaches and walnuts in an ancient cesspit, the archaeologists found 19 curiously large seeds. Though they were, let's say, \"deposited\" there nearly 2,000 years ago, they almost looked fr"
  },
  {
    "title": "You no longer need JavaScript: an overview of what makes modern CSS so awesome (lyra.horse)",
    "points": 94,
    "submitter": "todsacerdoti",
    "submit_time": "2025-08-28T20:49:34 1756414174",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=45056878",
    "comments": [
      "i will say that though i am predisposed to appreciate and agree with an article like this, any sort of value proposition around \"some users don't want javascript\" just doesn't... hit for me. and, mind you: i am a card-carrying arch user and have spent more time messing with browser scripting and web crawling, and am more of a True Believer than most. it's just such a niche user preference that i think it should largely be simply ignored. yes, i would love the world to be better for the \"noscript\" universe, no, i don't think that any individual \"grassroots\" effort should stake itself on \"no javascript\" being any part of its utility. i think there are a million other reasons why CSS should win out that are more compelling than an appeal to what feels, extremely ironically, like a callback to the \"but 10% of your users use IE6\" days... all in all, yes: this is somewhat of a minor point wrt. to the article (which btw i think is great), but i am just calling the \"trend\", such as it is / has been, for what (i think) it is.reply",
      "They mention the users who don\u2019t want JavaScript as an aside, but most of the post is devoted to just showing the CSS functionality off.The other motivation mentioned is performance. But they don\u2019t belabor the whole motivation thing anyway. IMO that\u2019s a good, focusing on showing off the tech seems more productive anyway.reply",
      "fwiw, i've been using the internet with noscript and i find it perfectly usablefor any sites that do need js, i simply enable it for them from the extension, so it never gets in the way with sites i use regularlyit's pretty nice for performance/battery and securityhave you ever tried living with noscript for over a week? i feel like your perspective could be a bit mislead, because i felt the exact same way as you before i started using noscriptdisclaimer: i'm the author of the blogpostreply",
      "Same here, I have noscript almost always on. The problem is some things don't work without JS. Google and Bing search, youtube, even duckduckgo in plane FireFox. The later works in Tor browser, that's what I'm usually using. I usually skip on most other things that require JS to drive blinking ads.reply",
      "I agree; I don't find the noscript-ians to be useful or worth targeting.At the same time, I want to emphasize more strongly the flip side that I think you don't at but don't go much I to: I do find that writing less code & using the platform is enormously valuable! Doing less & letting the browser do the thing is a very nice win.reply",
      "\"Doing less & letting the browser do the thing is a very nice win.\"If only they would do it nice and consistently, I would agree. Sadly they don't. On one plattform you get sliders in this color who pop in when the mouse moves there, on another you have fixed size sliders of a different color and style. Impossible to make a coherent style like this.reply",
      "> some users dont want javascriptcorrect, nearly all dontreply",
      "More than 99.9% of web users have never heard of javascript.reply",
      "A lot of people haven't heard of pancreatic cancer either. Explain it to them and see if they approve.edit: IE javascript was probably responsible for at least half a dozen times their system has been ruined, and they know what tracking is.reply",
      "> You are allowed to just make up elements as long as their names contain a hyphenRIP any semblance of using meaningful tags for machine readability I guess.reply"
    ],
    "link": "https://lyra.horse/blog/2025/08/you-dont-need-js/",
    "first_paragraph": "So much of the web these days is ruined by the bloat that is modern JavaScript frameworks. React apps that take several seconds to load. NextJS sites that throw random hydration errors. The node_modules folder that takes up gigabytes on your hard drive.It\u2019s awful. And you don\u2019t need it.The intro paragraph of this post is tongue-in-cheek. It\u2019s there to get you to read the rest of the post. I suspect the megabytes of tracking scripts intertwined with bad code is far more likely to be the real culprit behind all the terrible sites out there. Web frameworks have their time and place. And despite my personal distaste for them, I know they are used by many teams to build awesome well-optimized apps.Despite that, I think there\u2019s some beauty in leaving it all behind. Not just the frameworks, but JavaScript altogether. Not every site needs JavaScript. Perhaps your e-commerce site needs it for its complex carts and data visualization dashboards, but is it really a necessity for most of what\u2019s ou"
  },
  {
    "title": "Thrashing (tive.org)",
    "points": 18,
    "submitter": "pch00",
    "submit_time": "2025-08-27T07:55:43 1756281343",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://exple.tive.org/blarg/2025/08/26/thrashing/",
    "first_paragraph": "blargAugust 26, 2025I\u2019ve invested in a new odometer to help my car go faster. Accountability is important, and as a driver I believe the most important thing I can do is set measurable, achievable and inspiring goals for the people pushing my car down the road.I\u2019m never sure what to say to people who\u2026 don\u2019t quite seem to realize they sound like this. I think, what chain of reasoning brought us to this? How did we get here? But I think I know how we got here.So, about this link.No disrespect to my old team \u2013 hey, team, hey \u2013 but\u2026 look, the fact that \u201cmanager\u201d, \u201cleader\u201d and \u201cculture\u201d are nowhere in this article (are nowhere in so many articles like it about how people struggle to manage their time) is telling.\u201cThe underlying message is clear: We\u2019re all just distracted by our devices, victims of shrinking attention spans who could stop multitasking if we\u2019d just exercise more self-control.\u201d\u2026 and this is victim-blaming. People multitask because they\u2019ve been assigned multiple unprioritized t"
  },
  {
    "title": "Rupert's Property (johncarlosbaez.wordpress.com)",
    "points": 22,
    "submitter": "robinhouston",
    "submit_time": "2025-08-28T22:02:28 1756418548",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://johncarlosbaez.wordpress.com/2025/08/28/a-polyhedron-without-ruperts-property/",
    "first_paragraph": "You can cut a hole in a cube that\u2019s big enough to slide an identical cube through that hole!   Think about that for a minute\u2014it\u2019s kind of weird.Amazingly, nobody could prove any convex polyhedron doesn\u2019t have this property!  It\u2019s called \u2018Rupert\u2019s property\u2019.Until this week.This week Steininger and Yurkevich proved there is a convex polyhedron that you can\u2019t cut a hole in big enough to slide the entire polyhedron through the hole.  It has 90 vertices, and apparently 240 edges and 152 faces.To prove that no such hole is possible, they had to do a computer search of 18 million different holes, plus use a lot of extra math to make sure they\u2019d checked enough possibilities:\u2022 Jakob Steininger and Sergey Yurkevich, A convex polyhedron without Rupert\u2019s property.To celebrate their discovery, they gave this polyhedron a silly name.  Since this polyhedron lacks Rupert\u2019s property, they called it a \u2018noperthedron\u2019.Why is this property called \u2018Rupert\u2019s property\u2019?  Wikipedia explains:\nIn geometry, Princ"
  },
  {
    "title": "Dependent types I \u203a Universes, or types of types (jonmsterling.com)",
    "points": 8,
    "submitter": "matt_d",
    "submit_time": "2025-08-27T06:30:36 1756276236",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.jonmsterling.com/01ET/index.xml",
    "first_paragraph": ""
  },
  {
    "title": "Will AI Replace Human Thinking? The Case for Writing and Coding Manually (ssp.sh)",
    "points": 112,
    "submitter": "articsputnik",
    "submit_time": "2025-08-28T14:40:58 1756392058",
    "num_comments": 93,
    "comments_url": "https://news.ycombinator.com/item?id=45052784",
    "comments": [
      "I have gone from using Claude Code all day long since the day it was launched to only using the separate Claude app. In my mind that is a nice balance of using it, but not too much, not too fast.there is the temptation to just let these things run in our codebases, which I think for some projects is totally fine. For most websites I think this would usually be fine. This is for two reasons: 1) these models have been trained on more websites than probably anything else and 2) if a div/text is off by a little bit then usually there will be no huge problems.But if you're building something that is mission critical, unless you go super slowly, which again is hard to do because these agents are tempting to go super fast. That is sort of the allure of them: to be able to write sofware super fast.But as we all know, in some programs you cannot have a single char wrong or the whole program may not work or have value. At least that is how the one I am working on is.I found that I lost the mental map of the codebase I am working on. Claude Code had done too much too fast.I found a function this morning to validate futures/stocks/FUT-OPT/STK-OPT symbols where the validation was super basic and terrible that it had written. We had implemented some very strong actual symbol data validation a week or two ago. But that wasn't fully implemented everywhere. So now I need to go back and do this.Anyways, I think finding where certain code is written would be helpful for sure and suggesting various ways to solve problems. But the separate GUI apps can do that for us.So for now I am going to keep just using the separate LLM apps. I will also save lots of money in the meantime (which I would gladly spend for a higher quality Claude Code ish setup).reply",
      "The reality is that you can't have AI do too much for you or else you completely lose track of what is happening. I find it useful to let it do small stupid things and use it for brainstorming.I don't like it to do complete PR's that span multiple files.reply",
      "Losing the mental map is the number one issue for me. I wonder if there could be a way to keep track of it, even at a high level. Keeping the ability to dig in is crucial.reply",
      "Spend time reviewing outputs like a tech lead does when managing multiple developers. That's the upgrade you hust got in your career, you are now bound to how many \"team members\" you can manage at a single time. I'm grateful to live in such a time.reply",
      "You need to spend more time in Plan mode. Ask it to make diagrams or pseudocode of whats and hows, iterate on that and then Accept Edits.reply",
      "I think the whole AI vs non. AI debate is a bit besides the point. Engineers are stuck in the old paradigm of \"perfect\" algorithms.I think the image you post at the beginning basically sums it up for me: ChatGPT o3/5 Thinking can one-shot 75% of most reasonably sized tasks I give it without breaking a sweat, but struggles with tweaks to get it to 100%. So I make those tweaks myself and I have cut my code writing task in half or one third of the time.ChatGPT also knows more idioms and useful libraries than I do so I generally end up with cleaner code this way.Ferrari's are still hand assembled but Ford's assembly line and machines help save up human labor even if the quality of a mass-produced item is less than a hand-crafted one. But if everything was hand-crafted, we would have no computers at all to program.Programming and writing will become niche and humans will still be used where a quality higher than what AI can produce is needed. But most code will be done by minotaur human-ai teams, where the human has a minimal but necessary contribution to keep the AI on track... I mean, it already is.reply",
      "Hard disagree.  We'll be able to use more expressive languages with better LLM support for understanding how to express ourselves and to understand compiler results.  LLMs are only good at stuff that better languages don't require you to do.  After that they fall off the cliff quickly.LLMs are a communication technology, with a huge trained context of conversation.  They have a long way to go before becoming anything intelligent.reply",
      "LLMs lack intentionality, and they lack the ability to hold a series of precepts \"in mind\" and stick to those precepts. That is, if I say \"I want code that satisfies properties A, B, C, D...\" at some point the LLM just can't keep track of all the properties, which ones are satisfied, which ones aren't, what needs to be done or can be done to make them all satisfied.But LLMs aren't \"only good at stuff that better languages don't require you to do.\" In fact they are very good at taking a bad function definition and turning it into an idiomatic one that does what I wanted to do. That's very intelligent, there is no language that can take a bad spec and make it specific and fit for the specified task. LLMs can. (not perfectly mind you, but faster and often better than I can.) The problem is they just can't always figure out when what they've written is off-spec. But \"always\" isn't \"never\" and I've yet to meet an intelligence that is perfect.reply",
      "> LLMs ... lack the ability to hold a series of precepts \"in mind\" and stick to those precepts.That is perhaps the biggest weakness I've noticed lately, too. When I let Claude Code carry out long, complex tasks in YOLO mode, it often fails because it has stopped paying attention to some key requirement or condition. And this happens long before it has reached its context limit.It seems that it should be possible to avoid that through better agent design. I don't know how to do it, though.reply",
      "However, from my experience, the quality of code produced by developers in project we are working for the last 3+ years gone south. Amount of bugs now literally tripled year on year. I bet the reason is extensive use of AI tools, as the developers are the same.reply"
    ],
    "link": "https://www.ssp.sh/brain/will-ai-replace-humans/",
    "first_paragraph": "Search\n          \n          Last updated \nAug 28, 2025\n\n\nby Simon Sp\u00e4ti\nLearning to Think Again, and the Cost of AI Dependency.There are so many (hype/boring) posts about AI coming out every day. It\u2019s OK to use it, and everyone does it, but still learn your craft, and try to think.Similar to what DHH said:It\u2019s also more fun to be competent in something than constantly waiting for an AI to complete.The probability that AI will make us unhappy is very high IMO. Use it, yes, but not for every task. For discovering, creating a historical overview, or creating diagrams (Canva, Figma), but a big no to the writing (or coding). Someone needs to add knowledge or new insights; AI cannot train itself. So articles, books, and words will be written, and writers will be more in demand as everyone relies on AI, which at some point just plateaus.It will be a long-term loss; people stop thinking and learning. Time will tell. My two cents, if you are a senior in something, you know better. \n\nBskyI \n\nhea"
  }
]