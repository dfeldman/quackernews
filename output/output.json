[
  {
    "title": "Lossless LLM compression for efficient GPU inference via dynamic-length float (arxiv.org)",
    "points": 262,
    "submitter": "CharlesW",
    "submit_time": "2025-04-25T18:20:53 1745605253",
    "num_comments": 87,
    "comments_url": "https://news.ycombinator.com/item?id=43796935",
    "comments": [
      "This is just a consequence of the fact that bfloat16 has a very high dynamic range which is not all used. People like hyperparameters that look like 0.01 not 10^10, even though there is the same fractional precision available at each exponent and if you multiplied everything - hyperparameters, initialized weights, training data, etc in a network by 10^6 things will still work more or less the same since the upper range is hardly used (with the possible exception of some small number of special functions).Typical entropy of bfloat16 values seen in weights (and activations) are about 10-12 bits (only 65-75% or so of the value range is used in practice). Sign and mantissa bits tend to be incompressible noise.This has been exploited several times before in the context of both classical HPC and AI, with lossless compression work from Martin Burtscher's lab (https://userweb.cs.txstate.edu/~burtscher/), fpzip from LLNL (https://computing.llnl.gov/projects/fpzip) and my library dietgpu from 2021 (https://github.com/facebookresearch/dietgpu) which we used to speed training on a large GPU cluster by about 10% wall clock time overall by losslessly compressing all data prior to send and decompressing upon receive (e.g., gradients, weights from backup, etc), which is still computing the same thing as it did before as it is lossless.Also, rANS is more efficient and easier to implement in SIMD-like instruction sets than Huffman coding. It would reduce the performance latency/throughput penalties as well with DFloat11 (since we have to decompress before we do the arithmetic).\n \nreply",
      "For those who don't bother to click through profiles, Jeff really knows what he's talking about. Much of Meta/FAIR + community benefits from his code.\n \nreply",
      "I really love HN for this reason. Full of some of the brightest minds on the internet. Often the comments have very interesting information, instead of stupid knee jerk reactions to post titles.\n \nreply",
      "Thanks Jeff -- can you point me to something written up about rANS? All I find on line is turbulence modeling solutions; I presume this is not what you're referring to.As we know, quantizations are a critical tool for local LLM runners; RAM is typically the gating factor. Are you aware of other better lossless compression of BF16 weights out there?The reason I ask is this Dfloat11 seems relatively easy to plug in to existing quantization workflows, but you seem dismissive of the paper -- I presume it's my gap in understanding, and I'd like to understand.\n \nreply",
      "I don't know of any great write-ups unfortunately, but the rANS you're looking for is range asymmetric numeral systems.\n \nreply",
      "Do you think there\u2019s a call for introducing an even smaller float that can pack more values into a SIMD register? Like a 12 bit?\n \nreply",
      "What stands out most is the practical implication: enabling lossless inference of a 405B-parameter model on a single node with 8\u00d780GB GPUs is wild. That\u2019s a huge unlock for research labs and startups alike that want to run frontier models without massive infrastructure costs.\n \nreply",
      "> That\u2019s a huge unlock for research labs and startups alike that want to run frontier models without massive infrastructure costs.Or let one of the neoclouds take care of the infrastructure costs and rent it out from them. Disclosure: I run one of them.\n \nreply",
      "Keep up the great work! We need more of you and other players.Some unsolicited feedback: I would suggest reworking your landing page so that the language is always from your customers' perspective. Your customers want to solve a real internal problem that they have. Talking about how great your company is will always have less impact than talking about how you know what that problem is and how you intend to solve it.Your mission is relevant to you and your investors, not to your customers. They care about themselves.Your \"quick start\" should be an interactive form. I shouldn't have to remember what to put in an email to reach out to you. Make it easy for me. Also move that to the front page, provide a few \"standard\" packages and a custom one. Reduce the friction to clicking the CTA.Since your pricing is transparent, you should be able to tell me what that price will be before I even submit a request. I assume you're cheaper than the competition (otherwise why would I not go with them?) so make that obvious. Check out Backblaze's website for an example page: https://www.backblaze.com/cloud-storage/pricingShell out a few grand and hire a designer to make your page look more professional. Something like https://oxide.computer/ but with the points above, as they also make the same mistake of making their home page read like a pitch deck.\n \nreply",
      "Fantastic unsolicited feedback, I'm definitely taking this to heart!Website is intended to be more like documentation instead of a pitch deck or useless splash with a contact us form. I dislike sites like Oxide, I scroll past and don't read or ingest any of the fancy parts. Of course, you're right, this probably needs to be less about me. =)Friction definitely needs to be improved. That part is being worked on right now. Our intention is to be fully self-service, so that you don't have to talk to us at all, unless you want to. Credit card and go.We recently lowered our prices to be competitive with the rest of the market vs. focusing on people who care more about what we offer. We weren't trying to be cheaper than everyone else, we were trying to offer a better service. Lesson learned and pricing adjusted. Streisand effect, I don't like to mention the other players much.Again, thanks!\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2504.11651",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "I wrote a book called \"Crap Towns\". It seemed funny at the time (samj.substack.com)",
    "points": 17,
    "submitter": "url",
    "submit_time": "2025-04-26T00:30:41 1745627441",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://samj.substack.com/p/that-joke-isnt-funny-any-more",
    "first_paragraph": ""
  },
  {
    "title": "I designed my LED matrix PCB with code (tscircuit.com)",
    "points": 51,
    "submitter": "imrishabh18",
    "submit_time": "2025-04-25T21:43:33 1745617413",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=43798832",
    "comments": [
      "Atopile is another thing in the circuits-as-code space: https://github.com/atopile/atopileAs a half EE/half SWE I think there are significant benefits to circuits as code but I'm not impressed with this one. Atopile has a narrower focus (autorouters are really really hard) and doesn't use as many buzzwords. Like why on earth does a \"web first approach\" matter at all for hardware development?But also, GUI tools are getting better, Kicad 9 had a lot of changes that made templating / reusing blocks easier. And it works fine if not great with version control.I don't see circuit-as-code taking off with humans anytime soon, it's much better but not enough better to convince EEs many of which don't code much or at all. But I can see it becoming much more common as LLMs get better at complex circuits.\n \nreply",
      "I don't know why anyone wants to shove big heavy applications into browsers. Are they imagining you'd use your phone for this?Are we not teaching kids how to publish desktop applications these days or what?\n \nreply",
      "Wish I could comment on the routing like others but the render is stuck at 96.2% for me. Nothing else on the page shows it on my phone so I assume that's the problem. (And of course nothing in the log of error tabs).I think just looking at the first code example you can already see the problem. A lot of duplication and hand written or auto generated code. I thought the point was to define it in the code. Put an array of pin functions. For loop the footprint. That kinda thing. This looks like a mess to get started with and even worse is at higher point count parts it looks like it'll balloon in maintainability. Altium has very solid footprint generators with a nice menu. This looks like it's missing an overarching API for creating these long lists of parameters. Doesn't feel like the juice is worth the squeeze on this one. If it's a simple schematic, just do it by hand. If it's complicated, this feels harder to wrangle.Another example of weird code is the previousLedName. Like like really that variable isn't used and the first term of the && should be that indeed check. But even more so, it should be an if statement not rely on remembering short circuiting (lazy evaluation) tricks. Because that's what you mean. You mean if it's not the first one, connect to the previous one. So, the code should say that. I find it hard to believe such a high level language would prevent it.I think the pin label lists don't make sense. They're maps where the value is an array where the first element is the key? Why is this not just a list of pin numbers to names or a map of they're not contiguous whole numbers?And then the icing on the cake: you still have to define where in XY everything is.So really, in thinking about this, this looks more like a file format than a tool. And maybe that's fine. But I'll stick to the native formats of the tools.\n \nreply",
      "Interesting, reminds me of a project my colleague has just been working on: using KiCad + python  to auto-place LEDs at specific coordinates on a PCB. Instead of positioning them as a regular matrix, the LED locations are derived from the shape of a racing circuit: https://ledsrace.at/zandvoort\n(routing and non-LED part of the layout was still done manually)I don't think 100% auto-placement/routing will be able to take over manual layout, but there is definitely lots of potential for further automation!\n \nreply",
      "I took a stab at something like this. My intent was that you\u2019d be sitting at a Common Lisp REPL, and start entering commands.As you did this, a graphical window would show your components and traces. I considered similar to a legacy AutoCAD, way back before pointing devices were commonplace.I had a whole simple dialect to easily identify traces, which you would continuously split with points, then nudge them around or anchor next to other points.I honestly felt, especially for the small board project I was working on that writing this from scratch would have been faster than fighting KiCAD or any of the similar tools.But I was stymied with finding a good way to get graphics out of CL on my Mac. I even considered doing it all in SVG with some kind of auto reloading file, and just rewriting it each time.Obviously I did not put a lot of effort into it, had enough friction to move on to something else.\n \nreply",
      "SVG PCB is a similiar concept.  The nice thing is that it allows both code and gui manipulation of the design.https://leomcelroy.com/svg-pcb-website/#/home\n \nreply",
      "I built a WiFi-controlled RGB LED matrix using tscircuit, a library that lets you design PCBs with JavaScript instead of traditional CAD tools.\n \nreply",
      "I'm genuinely surprised by how awful the resulting PCB is. A trivial LED matrix like this is pretty much the best-case scenario for tooling like this. Not being able to handle this is a pretty damning - although a decent bunch of the blame should probably be placed on whatever third-party autorouter they are calling out to.Even with better results, the big issue with tools like these is that they simply don't match with the kind of development style that's needed for the problems at hand. You see the same issue with those drag-and-drop visual programming languages, or scripted modeling like OpenSCAD: they end up making fairly trivial details slightly easier, while significantly complicating the meat-and-potatoes. Nobody cares about a neat little part placement for-loop if it generates a living hell of autorouted spaghetti you can't possibly interpret and fix manually.In reality the low-hanging fruit has already been covered by existing tooling. Automatically generating symbols and footprints from textual descriptions is routine practice, and software like KiCad already has a reasonably usable scripting API and a well-documented file format. A lack of code-based code-based EDA hasn't stopped projects like Ergogen[0] from popping up. The main limitation for additional automation is a lack of reliable input data, and user-side tooling can't fix that.[0]: https://docs.ergogen.xyz/\n \nreply",
      "(tscircuit maintainer here) It might be easier to think of tscircuit as an electronics CAD kernel. We're MIT licensed and web-compatible, so we would make a great foundation for people building new EDA tools or people who would like to generate electronics (think domain-specific tools, e.g. a website that allows you to quickly build a custom keyboard)FWIW In this case I think this board called out to freerouting for the routing. Companies have reached out to us with autorouting APIs, so we'll support different vendors and hopefully allow enough constraint-specification for people to get good results. Autorouting is important for reusability, even if it's routing between manually-routed sections (e.g. fanouts)\n \nreply",
      "I guess it kind of works, but why would you? I find it quite therapeutic designing PCCB's. My last try at getting AI to draw a circuit diagram was actually quite hilarious: https://rodyne.com/?attachment_id=1753\n \nreply"
    ],
    "link": "https://docs.tscircuit.com/tutorials/building-led-matrix",
    "first_paragraph": "This tutorial will walk you through building a 3x5 LED matrix controlled by a Raspberry Pi Pico using tscircuit.Some practical applications of building an LED Matrix include:The matrix connects to the Pico microcontroller via a data chain. The Pico connects to WiFi through the PICO_W module.\nThe components and connections between them are shown in the diagram below:Let's import the Pico microcontroller and LED components by following the steps in the Importing from JLCPCB section.We will follow the following steps to build the circuit step by step:Schematic of the Pico microcontroller imported from JLCPCB is shown below:We are using IC LEDs (specifically WS2812B), which have an RGB LED and control chip integrated into the same package. These IC LEDs offer several advantages over traditional RGB LEDs:To connect two LEDs together, we need to connect the data output DO of the first LED to the data input DI of the second LED. This creates a chain of LEDs.The Pico is connected to the LED ma"
  },
  {
    "title": "World Emulation via Neural Network (madebyoll.in)",
    "points": 59,
    "submitter": "treesciencebot",
    "submit_time": "2025-04-25T21:33:57 1745616837",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=43798757",
    "comments": [
      "I think this is very interesting because you seem to have reinvented NeRF, if I\u2019m understanding it correctly. I only did one pass through but it looks at first glance like a different approach entirely.More interesting is that you made an easy to use environment authoring tool that (I haven\u2019t tried it yet) seems really slick.Both of those are impressive alone but together that\u2019s very exciting.\n \nreply",
      "Very nice work. Seems very similar to the Oasis Minecraft simulator.https://oasis.decart.ai/\n \nreply",
      "Appreciate this article that shows some failures on the way to a great result. Too many times, people only show how the polished end-result: look, I trained this AI and it produces these great results. The world dissolving was very interesting to see, even if I'm not sure I understand how it got fixed.\n \nreply",
      "Thanks! My favorite failure mode (not mentioned in the post - I think it was during the first round of upgrades?) was a \"dry\" form of soupification where the texture detail didn't fully disappear https://imgur.com/c7gVRG0\n \nreply",
      "Is this a solo/personal project? If it is is indeed very cool.Is OP the blog\u2019s author? Because in the post the author said that the purpose of the project is to show why NN are truly special and I wanted a more articulate view of why he/she thinks that?\nGood work anyway!\n \nreply",
      "Yes! This was a solo project done in my free time :) to learn about WMs and get more practice training GANs.The special aspect of NNs (in the context of simulating worlds) is that NNs can mimic entire worlds from videos alone, without access to the source code (in the case of pokemon) or even without the source code having existed (as is the case for the real-world forest trail mimicked in this post). They mimic the entire interactive behavior of the world, not just the geometry (note e.g. the not-programmed-in autoexposure that appears when you look at the sky).Although the neural world in the post is a toy project, and quite far from generating photorealistic frames with \"trees that bend in the wind, lilypads that bob in the rain, birds that sing to each other\", I think getting better results is mostly a matter of scale. See e.g. the GAIA-2 results (https://wayve.ai/wp-content/uploads/2025/03/generalisation_0..., https://wayve.ai/wp-content/uploads/2025/03/unsafe_ego_01_le...) for an example of what WMs can do without the realtime-rendering-in-a-browser constraints :)\n \nreply",
      "You mentioned it took 100 gpu hours, what gpu did you train on?\n \nreply",
      "author is: https://x.com/madebyollin\n \nreply",
      "This is great but I think I'll stick to mushrooms.\n \nreply",
      "Yeah, the similarities to psychedelics with some of this stuff is remarkable.\n \nreply"
    ],
    "link": "https://madebyoll.in/posts/world_emulation_via_dnn/",
    "first_paragraph": "I turned a forest trail near my apartment into a playable neural world.\nYou can explore that world in your web browser by clicking right here:By \"neural world\", I mean that the entire thing is a neural network generating new images based on previous images + controls. There is no level geometry, no code for lighting or shadows, no scripted animation. Just a neural net in a loop.By \"in your web browser\" I mean this world runs locally, in your web browser. Once the world has loaded, you can continue exploring even in Airplane Mode.So, why bother creating a world this way? There are some interesting conceptual reasons (I'll get to them later), but my main goal was just to outdo a prior post.See, three years ago, I got a simple two-dimensional video game world to run in-browser by training a neural network to mimic gameplay videos from YouTube.\n\nMimicking a 2D video game world was cute, but ultimately kind of pointless;\nexisting video games already exist and we can already emulate them jus"
  },
  {
    "title": "Parallel ./configure (tavianator.com)",
    "points": 19,
    "submitter": "brooke2k",
    "submit_time": "2025-04-25T23:19:56 1745623196",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=43799396",
    "comments": [
      "Very nice! I always get annoyed when my fancy 16 thread CPU is left barely used as one thread is burning away with the rest sitting and waiting. Bookmarking this for later to play around with whatever projects I use that still use configure.Also, I was surprised when the animated text at the top of the article wasn't a gif, but actual text. So cool!\n \nreply",
      "I've spent a fair amount of time over the past decades to make autotools work on my projects, and I've never felt like it was a good use of time.It's likely that C will continue to be used by everyone for decades to come, but I know that I'll personally never start a new project in C again.I'm still glad that there's some sort of push to make autotools suck less for legacy projects.\n \nreply",
      "is this really a big deal given you run ./configure once?it's like systemd trading off non-determinism for boot speed, when it takes 5 minutes to get through the POST\n \nreply",
      "If you do a lot of bisecting, or bootstrapping, or building compatibility matrices, or really anything that needs you to compile lots of old versions, the repeated ./configure steps really start feeling like a drag.\n \nreply",
      "> it's like systemd trading off non-determinism for boot speed, when it takes 5 minutes to get through the POSTThat's a bad analogy: if a given deterministic service ordering is needed for a service to correctly start (say because it doesn't start with the systemd unit), it means the non-deterministic systemd service units are not properly encoding the dependencies tree in the Before= and After=When done properly, both solutions should work the same. However, the solution properly encoding the dependency graph (instead of just projecting it on a 1-dimensional sequence of numbers) will be more flexible: it's the better solution, because it will give you more speed but also more flexibility: you can see the branches any leaf depends on, remove leaves as needed, then cull the useless branches.It's like using the dependencies of linux packages, and leaving the job of resolving them to package managers (apt, pacman...): you can then remove the useless packages which are no longer required.Compare that to doing a `make install` of everything to /usr/local in a specific order, as specified by a script: when done properly, both solutions will work, but one solution is clearly better than the other as it encodes more finely the existing dependencies instead of projecting them to a sequence.\n \nreply"
    ],
    "link": "https://tavianator.com/2025/configure.html",
    "first_paragraph": "I'm sorry, but in the year 2025, this is ridiculous:I paid good money for my 24 CPU cores, but ./configure can only manage to use 69% of one of them.\nAs a result, this random project takes about 13.5\u00d7 longer to configure the build than it does to actually do the build.The purpose of a ./configure script is basically to run the compiler a bunch of times and check which runs succeeded.\nIn this way it can test whether particular headers, functions, struct fields, etc. exist, which lets people write portable software.\nThis is an embarrassingly parallel problem, but Autoconf can't parallelize it, and neither can CMake, neither can Meson, etc., etc.The problem is that most build configurations scripts pretty much look like this:This is written in an inherently sequential way, but in principle many of these tests could be run in parallel.\nIn fact, we already have an effective tool for parallelizing lots of commands (make), so let's use it.\nWe'll have a configuration makefile that generates ou"
  },
  {
    "title": "Show HN: Formalizing Principia Mathematica using Lean (github.com/ndrwnaguib)",
    "points": 98,
    "submitter": "ndrwnaguib",
    "submit_time": "2025-04-25T18:49:30 1745606970",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=43797256",
    "comments": [
      "When I saw \u201cLean\u201d I thought https://en.m.wikipedia.org/wiki/Lean_manufacturing\n \nreply",
      "This is useful to anyone who wants to reason through the proofs constructively and tinker with the approaches. Thank you!\n \nreply",
      "Thank you!\n \nreply",
      "I only see these very initial propositional theorems.Am I missing something, or has the project only just begun?https://github.com/ndrwnaguib/principia/blob/main/Principia/...\n \nreply",
      "You're not missing something. The project begun several months ago (I had to pause while I was writing my thesis). I resumed working on it recently.\n \nreply",
      "It looks like you just have a few pages written. Is that right?Which theorem are you trying to prove?\n \nreply",
      "Yes; the goal is to finish the first volume. I am particularly looking forward to formalizing the well-known 1+1 proof.\n \nreply",
      "My understanding is the first bit follows first order logic fairly close but then diverges as Russel builds different classes of sets etc, do you have line of sight of how it\u2019s going to translate?\n \nreply",
      "What do you think of using something like naproche?\n \nreply",
      "I have not used `naproche` before; thanks for the suggestion. I will try several propositions and see what do I get!\n \nreply"
    ],
    "link": "https://github.com/ndrwnaguib/principia",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Rewriting Prof. Bertrand Russell's Principia Mathematica in Lean\n      This project aims to formalize the first volume of Prof. Bertrand Russell\u2019s\n  Principia Mathematica using the Lean theorem prover. The goal is to ensure that\n  the formalization aligns clearly with the corresponding theorems in the book to\n  avoid confusion (See Metaprogramming =Syll=)Principia Mathematica\u2019s notation (Peano-Russell notation) is exceptionally known\n  for its sophistication that it has a separate entry on the Stanford Encyclopedia\n  of Philosophy (SEP). Also, Prof. Landon Elkind\u2019s Squaring the Circles: a\n  Genealogy of Principia\u2019s Dot Notation explains the notation skillfully.I do not think there is a need to read them, I would like to believe that\n  after reading a few examples of how some formulas were formalized and\n  contrasting them against Pr"
  },
  {
    "title": "Show HN: I used OpenAI's new image API for a personalized coloring book service (clevercoloringbook.com)",
    "points": 80,
    "submitter": "darajava",
    "submit_time": "2025-04-25T10:05:39 1745575539",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=43791992",
    "comments": [
      "This is a cute and simple idea!I'd like to see what a real physical book looks like before I buy it though. Do you have real pictures of a printed one?I think our kids would appreciate seeing the original (even if a small thumbnail) along side it. You can't always tell from these AI drawings that it was originally you and your family.Also, it's REALLY expensive. $30 for a book that my kids will draw on in one or two nights and then never touch again is probably too much.\n \nreply",
      "It\u2019s not cheap, but my kids treasure coloring books for a long time and probably one like this until it falls apart.\n \nreply",
      "To the author, I have this idea, for each page, put a sheet of transparent plastic or something like that. So the owner will color the plastic which can be erased.\nBut it may increase the cost anc the color may not stick to the plastic.\n \nreply",
      "For those interested in building something similar, I prompted a story book generator using v0 and Gemini\u2019s image generation a few weeks ago:Demo: https://v0-story-maker.vercel.app/The chat: https://v0.dev/chat/ai-story-book-creator-zw7TrmkN2Eb\n \nreply",
      "For what it\u2019s worth (and it\u2019s probably not much), it doesn\u2019t cost that much to commission comic book-style art from an actual artist online. When you do that, the proceeds go to an artist, not to an AI company that stole from them and a software developer who wrote a wrapper around their API.\n \nreply",
      "In fairness no artists are advertising a personal coloring book.  The time, effort and cost would put this out of reach for 99.99 of people.No artists are losing income because of this and no industry is being upended.  This is a new product that's available because of a technology advanced.Why the focus the artist?  Everytime you order in food online you take away a tip from a host, server, bartender and take away a job from a person who answers a phone.  Why focus on artists when so many have been affected by technology.\n \nreply",
      "And yet there\u2019s plenty of adult coloring books made by a human out there if you\u2019re willing to go to a brick and mortar shop. Got a super cool one from dick blicks, with a lot of underwater scenes. Also paper quality is important. I can\u2019t imagine getting as far as I did in mine if it was newspaper\n \nreply",
      "My opinion isn\u2019t fully formed but I currently think either all content producers have a claim (potentially workable as eg a discount), or only those who contribute should get access to AI\u2019s.And by all I mean the AI companies owe a huge debt to all humans who wrote or designed or drew anything. The vast majority of the benefit of this technology relies on volume: the billions of pages and lines of code we wrote for other humans, but have now been repurposed. This technology relies on bulk, which was mainly unprofessional or freely given content, by those who intended it for other humans. It was not 100% built only on the output of the few who charge for their exquisite words or designs, even if their output is higher quality.Alternatively, let the AI companies go for it but everyone who uses any kind of AI should understand that they\u2019re standing on the shoulders of the millions of developers and nonprofessional writers whose work has now been repurposed. Not the few artists and journalists. So those artists and journalists should both refuse to contribute to, and use, AI.* I\u2019ve written very little of this useful content, but would be happy to pay my share to those that have built what we have. I also turn off training on my content, but I pay a lot for models. Feel free to help me think through this with comments of your own.\n \nreply",
      "Usually when you commission something you're asking the artist to do art and create something unique with their own artistic flair... not just line-trace an existing photo.The intention and cost of something like that is not at all comparable to what is being offered here.\n \nreply",
      "If it does not cost that much, that is obviously because the artist is too cheap. If you find that to be a preferable equilibrium, that's a choice I guess, but I find it fairly ironic in light of the purported motivation.\n \nreply"
    ],
    "link": "https://clevercoloringbook.com/",
    "first_paragraph": "Upload your memories - we print and send a physical personalized coloring book\n\t\tfor just\n\t\t$23.99 + shipping.Perfect for screen-free time with your loved ones.Step 1: Upload a bunch of your favorite photosStep 2: We convert them into a high-quality physical coloring book with OpenAI\u2019s brand-new Sora model, then send it out for printing.Step 3: You receive your personalized coloring book in the mailPlease only upload photos that are in line with OpenAI's Usage Policy.  We are not able to include any photos that do not follow their policy in the final printed book.Made with \u2764\ufe0f by Soliloquy Apps Limited"
  },
  {
    "title": "Berkeley Humanoid Lite \u2013 open-source robot (berkeley-humanoid.org)",
    "points": 3,
    "submitter": "ratsbane",
    "submit_time": "2025-04-26T01:03:40 1745629420",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://lite.berkeley-humanoid.org/",
    "first_paragraph": "\n        Robotics: Science and Systems (RSS) 2025\n      \nYufeng Chi,\n        Qiayuan Liao,\n        Junfeng Long,\n        Xiaoyu Huang,\n        Sophia Shao,\n        Borivoje Nikolic,\n        Zhongyu Li,\n        Koushil Sreenath\n\n        University of California, Berkeley\n      \n        Despite significant interest and advancements in humanoid robotics, most existing commercially available hardware remains high-cost, closed-source, and non-transparent within the robotics community. This lack of accessibility and customization hinders the growth of the field and the broader development of humanoid technologies. To address these challenges and promote democratization in humanoid robotics, we demonstrate Berkeley Humanoid Lite, an open-source humanoid robot designed to be accessible, customizable, and beneficial for the entire community. \n      \n        The core of this design is a modular 3D-printed gearbox for the actuators and robot body. All components can be sourced from widely availab"
  },
  {
    "title": "Writing \"/etc/hosts\" breaks the Substack editor (scalewithlee.substack.com)",
    "points": 477,
    "submitter": "scalewithlee",
    "submit_time": "2025-04-25T13:48:30 1745588910",
    "num_comments": 273,
    "comments_url": "https://news.ycombinator.com/item?id=43793526",
    "comments": [
      "The people configuring WAF rules at CDNs tend to do a poor job understanding sites and services that discuss technical content. It's not just Cloudflare, Akamai has the same problem.If your site discusses databases then turning on the default SQL injection attack prevention rules will break your site. And there is another ruleset for file inclusion where things like /etc/hosts and /etc/passwd get blocked.I disagree with other posts here, it is partially a balance between security and usability. You never know what service was implemented with possible security exploits and being able to throw every WAF rule on top of your service does keep it more secure. Its just that those same rulesets are super annoying when you have a securely implemented service which needs to discuss technical concepts.Fine tuning the rules is time consuming. You often have to just completely turn off the ruleset because when you try to keep the ruleset on and allow the use-case there are a ton of changes you need to get implemented (if its even possible). Page won't load because /etc/hosts was in a query param? Okay, now that you've fixed that, all the XHR included resources won't load because /etc/hosts is included in the referrer. Now that that's fixed things still won't work because some random JS analytics lib put the URL visited in a cookie, etc, etc... There is a temptation to just turn the rules off.\n \nreply",
      "> I disagree with other posts here, it is partially a balance between security and usability.And economics. Many people here are blaming incompetent security teams and app developers, but a lot of seemingly dumb security policies are due to insurers. If an insurer says \"we're going to jack up premiums by 20% unless you force employees to change their password once every 90 days\", you can argue till you're blue in the face that it's bad practice, NIST changed its policy to recommend not regularly rotating passwords over a decade ago, etc., and be totally correct... but they're still going to jack up premiums if you don't do it. So you dejectedly sigh, implement a password expiration policy, and listen to grumbling employees who call you incompetent.It's been a while since I've been through a process like this, but given how infamous log4shell became, it wouldn't surprise me if insurers are now also making it mandatory that common \"hacking strings\" like /etc/hosts, /etc/passwd, jndi:, and friends must be rejected by servers.\n \nreply",
      "Not just economics, audit processes also really encourage adopting large rulesets wholesale.We're SOC2 + HIPAA compliant, which either means convincing the auditor that our in-house security rules cover 100% of the cases they care about... or we buy an off-the-shelf WAF that has already completed the compliance process, and call it a day. The CTO is going to pick the second option every time.\n \nreply",
      "Yeah. SOC2 reminds me that I didn't mention sales as well, another security-as-economics feature. I've seen a lot of enterprise RFPs that mandate certain security protocols, some of which are perfectly sensible and others... not so much. Usually this is less problematic than insurance because the buyer is more flexible, but sometimes they (specifically, the buyer's company's security team, who has no interest besides covering their own ass) refuse to budge.If your startup is on the verge of getting a 6 figure MRR deal with a company, but the company's security team mandates you put in a WAF to \"protect their data\"... guess you're putting in a WAF, like it or not.\n \nreply",
      ">guess you're putting in a WAF, like it or not.Install the WAF crap, and then feed every request through rot13(). Everyone is happy!\n \nreply",
      "Up until you need to exercise the insurance policy and the court room \"experts\" come down on you like a ton of bricks.\n \nreply",
      "now you've banned several different arbitrary strings!\n \nreply",
      "Good luck debugging why the string \"/rgp/cnffjq\" causes your request to be rejected :)\n \nreply",
      "I wish IT teams would say \"sorry about the password requirement, it's required by our insurance policy\". I'd feel a lot less angry about stupid password expiration rules if they told me that.\n \nreply",
      "Sometime in the past few years I saw a new wrinkle:\npassword must be changed every 90 days unless it is above a minimum length\n(12 or so as best I recall)\nin which case you only need to change it yearly.\nSince the industry has realized length trumps dumb \"complexity\" checks,\nit's a welcome change to see that encoded into policy.\n \nreply"
    ],
    "link": "https://scalewithlee.substack.com/p/when-etchsts-breaks-your-substack",
    "first_paragraph": ""
  },
  {
    "title": "Curry: A functional logic programming language (curry-lang.org)",
    "points": 102,
    "submitter": "hyperbrainer",
    "submit_time": "2025-04-25T18:46:22 1745606782",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=43797212",
    "comments": [
      "That example for \"some permutation\" is not at all easy for me to understand.  I'm assuming I'm just not familiar with the general style?\n \nreply",
      "I'm unfamiliar as well, but my best guess is that it relies on non-determinism. i.e. both definitions of 'insert' might be valid, and the runtime chooses which to use at random, resulting in either x or y being prepended to the returned list.\n \nreply",
      "Any one know how Curry (which has a Haskell-like syntax extended to support prologish features) compares with Mercury (which has a Prolog-like syntax extended to support Haskellish features)?\n \nreply",
      "Mercury feels like if the Ada people wrote Prolog. it's very verbose. you have to declare signatures in separate files and determinism modes. grounding is strictly enforced. it's statically typed. there's no REPL, remarkably.in exchange, the compiler catches a lot of bugs and the code is blazing fast.Curry is a superset of Haskell. it takes Haskell's pattern matching and makes it extremely general (full unification), extends it to non-determinism with choice points. it does have a REPL, like ghci.Like Haskell, Curry is lazy. Mercury (like Prolog) uses mostly eager, depth-first evaluation (SLDNF resolution.) Clause order doesn't matter in Curry, which uses a strategy of \"needed narrowing\" - variables are narrowed when they need to be.Unlike Mercury (and Prolog), and like Haskell and other FP languages, Curry draws a distinction between function inputs and outputs. You can do relational programming via guards and pattern matching, but it doesn't feel as Prolog-y.Curry is more niche than Mercury, which is at least being used to build Souffle (a static analysis language built on Datalog), which is actually being used in industry somewhat. But it's a shame because Curry has a lot to offer, especially to Haskellers. They're both worth checking out though.\n \nreply",
      "Not a technical difference, but I think Mercury is somewhat more \"commerical\" in that it's out of development and can be used in real projects, compared to Curry, which is very much in development.\n \nreply",
      "How does Curry manage ambiguity in non-deterministic computations\u2014especially when multiple valid instantiations exist for a free variable?\n \nreply",
      "Probably like Prolog, we get to generate all possible variations.\n \nreply",
      "The documentation, current-report, is good for learning Curry.https://curry-lang.org/docs/report/curry-report.pdfInteresting, the email at the end of this thread: https://news.ycombinator.com/item?id=12668591\n \nreply",
      "type classes existed the last time I tried it. maybe they're not 100% complete but I didn't notice anything missing.\n \nreply",
      "Curry is already at version 3.x. https://curry-lang.org/downloads/\n \nreply"
    ],
    "link": "https://curry-lang.org/",
    "first_paragraph": "Curry is a declarative multi-paradigm programming language\n        which combines in a seamless way features from\n        functional programming\n        (nested expressions, higher-order functions, strong typing, lazy evaluation)\n        and logic programming\n        (non-determinism, built-in search, free variables, partial data structures).\n        Compared to the single programming paradigms, Curry provides\n        additional features, like optimal evaluation for logic-oriented\n        computations and flexible, non-deterministic pattern matching\n        with user-defined functions.\n    \nCurry is called a declarative language, because computed results are independent of the time and order of evaluation, which simplifies reasoning on programs. Side effects can be modeled as \u201cIO\u201d operations, i.e., a declarative description of what to do. Operations are constructed by expressions only, there are no statements or instructions, and every binding to a variable is immutable.\nCurry is calle"
  },
  {
    "title": "Eurorack Knob Idea (mitxela.com)",
    "points": 240,
    "submitter": "po",
    "submit_time": "2025-04-25T13:19:02 1745587142",
    "num_comments": 96,
    "comments_url": "https://news.ycombinator.com/item?id=43793288",
    "comments": [
      "Very cool idea! It reminds me of how software works\u2014 you usually drag an LFO to the knob to modulate a parameter.A few people have pointed out that the knob can act as an attenuator when it's being modulated via a jack, but sometimes the goal is space. I can also imagine a version of this that uses the same technique, but where the knob has an audio jack on it. You wouldn't need to unplug it to connect the incoming cable, and you could use the still-connected knob as an attenuator if you wanted to. This would get you the best of all worlds\u2014 maximum space, an intuitive interface, and attenuators if you so choose to have them.\n \nreply",
      ">It's a nice dream, of a synthesizer where any knob can be pulled out and replaced with a patch cable, and any jack can have a knob plugged into it to set it to a fixed value.What's even better, though, is a coupled knob + jack where the knob turns into an attenuator for the input when a cable is plugged in, and works as a standalone knob otherwise. I think this is quite a common design.I believe I've also seen patch cables with built-in attenuators.\n \nreply",
      "This is why I really like Intellijel\u2019s designs. They generally have attenuators on the inputs for which it makes sense, and those attenuators are the small stick knobs. While they use larger knobs for more central module functions.Eg: https://intellijel.com/downloads/manuals/rubicon_manual.pdf\n \nreply",
      "Another common pattern is jack + offset. The most useful is when you have jack + offset + attenuator\u2026 but most modules pick one or the other for space reasons.\n \nreply",
      "Totally. Also, an attenuator is easier and cheaper to implement, because it just requires normalizing V+ into the jack plug. An offset requires an adder.My preference is: attenuator < offset < attenuator + offset. I see no benefit of having to remove the knob to get to the jack as proposed in the article.\n \nreply",
      "The benefit is saving space. Imagine a 10x10 grid of such jack / knob inputs.\n \nreply",
      "The attenuator-inverter is super handy too. A gain knob that goes from -1 to +1 X.\n \nreply",
      "the smartest pattern is used in mutable instruments beads, the \"attenurandomizers\"it packs a ridiculous amount of functionality into a single plug & knob combo\n \nreply",
      "I like it, but the best modules already have knobs and jacks for everything. When you have CV going into the jack, the knob acts as an attenuator or attenuverter. This means that the modules are generally larger. Make Noise generally does this and their modules are consistently bigger than everyone else, and they're also some of the most popular. Look at Maths. It's a slope generator and a mixer. It's fucking huge. But everyone has it because it's patch programmable. The problem in Eurorack is instead of making things patch programmable, they try to fit in a ton of functionality into a small space, so you have a lot of modules that have multiple modes where buttons and knobs all have different meanings depending on what \"page\" you're on. Fuck that. Almost every time I try a module like that, I end up selling it.He's right about the interface being the point of Eurorack. Plugging things into other things is the whole point. When I have a module that has hidden state, I forget what state it's in or what the knobs mean. I end up avoiding those modules. With cables and knobs, I can see the state of the whole system. I need good cable management to make sure it's not spaghetti, but I already do that in code already, and it's not that different.\n \nreply",
      "It's an interesting idea (truly a clever way to accomplish this!), but I think it's addressing the symptom, not the problem. The symptom is that some jacks don't have associated knobs. The problem is that either the module designer or the module user is overly obsessed with miniaturization. The designer is at fault if it's a parameter that really should have had a knob with the jack and they avoided including one in order to keep things small. The user is at fault if they're trying to stay so space-constrained that they can't fit a module that outputs an DC voltage set by a knob into their case. There are numerous modules that do this (and often that attenuvert as well) and many of them are fairly small too.\n \nreply"
    ],
    "link": "https://mitxela.com/projects/euroknob",
    "first_paragraph": "\r\n\r\nBut it had me thinking about Eurorack and the weird compromises people often make to fit more and more modules into a tiny case. I know a thing or two about tiny synthesizers. But my creations are often whimsical and useless. When it comes to Eurorack, where people spend crazy amounts of money on their setups, it's weird to see people compromise on the main aspect that gives it an edge over simulating the whole thing in software.\r\n\r\nTo clean up our Eurorack panels, perhaps we need a new knob idea? Watch the following video for a prototypical demo.\n\r\n\r\nIn essence, we're using a 3.5mm jack in front of a magnetic encoder chip, and a small magnet embedded in the plug turns it into a knob and patch cable hybrid.\r\n\r\nThe magnetic encoder in question is an AS5600. These are not the cheapest parts but they do make prototyping very easy. It has two hall sensors in an XY configuration and a dollop of DSP to give us an angle and a magnitude. They're easily available on breakout boards and have"
  },
  {
    "title": "Paper2Code: Automating Code Generation from Scientific Papers (arxiv.org)",
    "points": 76,
    "submitter": "Jerry2",
    "submit_time": "2025-04-25T17:36:15 1745602575",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=43796419",
    "comments": [
      "I like the idea of having automatic code creation from papers, but I\u2019m scared of it.Suppose you get a paper, you automatically implement the code, and then modify it a bit with a novel idea, and publish your paper. Then somebody else does that with your paper, and does the same.. at some point, we will have a huge quantity of vibe coded code on github, and two similar papers will have very different underlying implementations, so hard to reason about and hard to change.From a learning perspective, you try to understand the code, and it's all spaghetti, and you loose more time understanding the code than it would take to just reimplement it. You also learn a lot by not only reading the paper but reading the authors code where most of the small details reside.And I'm not even talking about the reliability of the code, test to know that it's the correct implementation. Authors try to make papers as close as possible to the implementation but sometimes subtle steps are removed, sometimes from inadvertance, sometimes because the number of pages is lionmited.A paper and an implementation are not one-to-one mappings\n \nreply",
      "> we will have a huge quantity of vibe coded code on githubThat may actually be an improvement over much of the code that is generated for papers.\n \nreply",
      "It would be neat to run their pdf through their implementation[1] and compare results.https://github.com/going-doer/Paper2Code\n \nreply",
      "Damn, i was hoping the link was your result of that. Please do that. I can't start another project currently. But i'd love the short result as an anecdote. But if you don't do it, i might have to. Please let me know. Great idea, really.\n \nreply",
      "If I was the paper author I would have done it and include the results as an appendix or a repo.\n \nreply",
      "haha would that itself be a product of the paper then?\n \nreply",
      "Maybe by doing it enough times o3-mini will end up reimplementing itself?\n \nreply",
      "Imagine, this was actually the consequence\u2014 against all odds. The true power of AI is about to be discovered... through this silly experiment... and you are the one... all you gotta do\u2014 is do it. And imagine you don't do it, because you think it can't lead to such a serious result... and if we miss this great leap forward... go, throw your life away and do this. now. the universe is waiting on you, my friend.\n \nreply",
      "I have had good results doing bidirectional programming in Tex <=> Python.\n \nreply",
      "It relies on OpenAI's o3-mini model which (I think) you have to pay for.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2504.17192",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
  },
  {
    "title": "Reproducibility project fails to validate dozens of biomedical studies (nature.com)",
    "points": 127,
    "submitter": "rntn",
    "submit_time": "2025-04-25T16:14:08 1745597648",
    "num_comments": 79,
    "comments_url": "https://news.ycombinator.com/item?id=43795300",
    "comments": [
      "https://archive.is/mmzWj",
      "It would be interesting for reproducibility efforts to assess \u201cconsequentiality\u201d of failed replications, meaning: how much does it matter that a particular study wasn\u2019t reproducible? Was it a niche study that nobody cited anyway, or was it a pivotal result that many other publications depended on, or anything in between those two extremes?I would like to think that the truly important papers receive some sort of additional validation before people start to build lives and livelihoods on them, but I\u2019ve also seen some pretty awful citation chains where an initial weak result gets overegged by downstream papers which drop mention of its limitations.\n \nreply",
      "It is an ongoing crisis how much Alzheimer\u2019s research was built on faked amyloid beta data. Potentially billions of dollars from public and private research which might have been spent elsewhere had a competing theory not been overshadowed by the initial fictitious results.\n \nreply",
      "The amyloid hypothesis is still the top candidate for at least a form of Alzheimer's. But yes, the issues with one of the early studies has caused significant issues.I say \"a form of Alzheimer's\" because it is likely we are labelling a few different diseases as Alzheimer's.\n \nreply",
      "I went searching for more info on this and found https://www.science.org/content/blog-post/faked-beta-amyloid... which was an interesting read.\n \nreply",
      "Glad this is getting some attentionFor central limit theorem to hold, the random variables must be (independently and identically dustributed) i.i.d.\nHow do we know our samples are i.i.d.? We can only show if they are notAdd to that https://en.m.wikipedia.org/wiki/Why_Most_Published_Research_...We've got to do better or science will stagnate\n \nreply",
      "The median sample size of the studies subjected to replication was n = 5 specimens (https://osf.io/atkd7).  Probably because only protocols with an estimated cost less than BRL 5,000 (around USD 1,300 at the time) per replication were included.  So it's not surprising that only ~ 60% of the original biomechemical assays' point estimates were in the replicates' 95% prediction interval.  The mouse maze anxiety test (~ 10%) seems to be dragging down the average.  n = 5 just doesn't give reliable estimates, especially in rodent psychology.\n \nreply",
      "pretty crazy reading all this and realizing how shaky some \"facts\" really are -  you think the root problem comes from pressure to publish or is  it just sloppy science piling up over time?\n \nreply",
      "This doesn\u2019t really surprise me at all. It\u2019s an unrelated field, but part of the reason I got completely disillusioned with research to the point I switched out of a program with a thesis was because I started noticing reproducibility problems in published work. My field is CS/CE, generally papers reference publicly available datasets and can be easily replicated\u2026 except I kept finding papers with results I couldn\u2019t recreate. It\u2019s possible I made mistakes (what does a college student know, after all), but usually there were other systemic problems on top of reproducibility. A secondary trait I would often notice is a complete exclusion of [easily intuited] counter-facts because they cut into the paper\u2019s claim.To my mind there is a nasty pressure that exists for some professions/careers, where publishing becomes essential. Because it\u2019s essential, standards are relaxed and barriers lowered, leading to the lower quality work being published. Publishing isn\u2019t done in response to genuine discovery or innovation, it\u2019s done because boxes need to be checked. Publishers won\u2019t change because they benefit from this system, authors won\u2019t change because they\u2019re bound to the system.\n \nreply",
      "All it takes is 14 grad students studying the same thing targeting a 95% confidence interval for, on average, one to stumble upon a 5% case. Factor in publication bias and you get a bunch of junk data.I think I heard this idea from Freakonomics, but a fix is to propose research to a journal before conducting it and being committed to publication regardless of outcome.\n \nreply"
    ],
    "link": "https://www.nature.com/articles/d41586-025-01266-x",
    "first_paragraph": "Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\n            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\n            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\n            and JavaScript.AdvertisementYou can also search for this author in PubMed\n\u00a0Google Scholar\n\n                You have full access to this article via your institution.A replication drive focused on results that lean on three methods commonly used in biomedical research in Brazil. Credit: Mauro Pimentel/AFP/Getty In an unprecedented effort, a coalition of more than 50 research teams has surveyed a swathe of Brazilian biomedical studies to double-check their findings \u2014 with dismaying results.The teams were able to replicate the results of less than half of the tested experiments1. That rate is in keeping with that found by other large-scale a"
  },
  {
    "title": "A $20k American-made electric pickup with no paint, no stereo, no screen (theverge.com)",
    "points": 929,
    "submitter": "kwindla",
    "submit_time": "2025-04-25T15:01:37 1745593297",
    "num_comments": 870,
    "comments_url": "https://news.ycombinator.com/item?id=43794284",
    "comments": [
      "For anyone curious, if you made a similarly sized gas-powered pickup with an i4 engine, it would be penalized more than a full-sized pickup for being too fuel inefficient, despite likely getting much better mileage than an F-150 because, since 2011, bigger cars are held to a lesser standard by CAFE[1].1: https://en.wikipedia.org/wiki/Corporate_average_fuel_economy...\n \nreply",
      "Automotive industry is one of the biggest scams on planet earth. One of my favorite cases recently is how Suzuki Jimny is banned in Europe and US because of emission standards allegedly, so the little Jimny is emitting 146g/km but somehow there is no problem to buy a G-Class that is emitting 358g/km oh and surprise surprise Mercedes are going to release a smaller more affordable G-Class [1].[1] - https://www.motortrend.com/news/2026-mercedes-benz-baby-g-wa...\n \nreply",
      "Manufacturers must hit a level of CO2 emissions on average across their whole fleet. As such, Suzuki is choosing to discontinue the Jimny because of the tougher fleet average targets starting in 2025.\n Overall you\u2019re right that it\u2019s a bit of a fix; Mercedes \u2018pools\u2019 its emissions with other manufacturers/brands. It currently pools with Smart, but may also pool with Volvo/Polestar? [0]\nIt\u2019s such an obvious approach to \u2018game\u2019 the targets, it\u2019s a wonder the EU didn\u2019t see it coming when they introduced the scheme.\n[0] https://www.schmidtmatthias.de/post/mercedes-benz-intends-to...\n \nreply",
      "They likely saw it coming\u2026 and deliberately did it this way.All local industry distorts their relevant politics. There\u2019s lobbyists in the EU too.The EU economy has a lot of car manufacturing, so cars are probably a big deal in Brussels.\n \nreply",
      "Is that weighted for individual car popularity? Because couldn\u2019t you put three push cars in your lineup that you don\u2019t realistically expect to sell and be fine?\n \nreply",
      "I wonder if that's why Ford, Ram, and Nissan all at the same time decide to discontinue their mini cargo vans a year ago.\n \nreply",
      "The Jimny or similar Suzuki models would not be offered for sale in the U.S. because it\u2019s basically the latest iteration of the Samuri, which died there after Consumer Reports falsely claimed that it was dangerously prone to rollover.\n \nreply",
      "I had rented a barebones Jimny last month when I was in Auckland for the week. Not saying it was prone to roll. But holy hell was it feeling like I could roll that bad boy on some curvy gravel roads. I also loved it.\n \nreply",
      "Example #5621 that a simple carbon tax would be miles better than the complex morass of regulations we currently have.\n \nreply",
      "That's overly reductive.1. Poorer people tend to drive older vehicles, so if you solely encourage higher fuel economies by taxing carbon emissions, then the tax is (at least short-term) regressive.2. You can work around #1 by applying incentives for manufacturers to make more efficient cars should lead any carbon tax3. If you just reward companies based on fleet-average fuel economy without regard to vehicle size, then it would be rather bad for US car companies (who employ unionized workers) that historically make larger cars than Asian and European companies.4. So the first thing done was to have a separate standard for passenger vehicles and light-trucks, but this resulted in minivans and SUVs being made in such a way as to get the light-truck rating5. We then ended up with the size-based calculation we have today, but the formula is (IMO) overly punitive on small vehicles.  Given that the formula was forward looking, it was almost certain to be wrong in one direction or the other, but it hasn't been updated.\n \nreply"
    ],
    "link": "https://www.theverge.com/electric-cars/655527/slate-electric-truck-price-paint-radio-bezos",
    "first_paragraph": "\ufeffIs the market ready for a four-wheeled digital detox? by  Tim Stevens\ufeffIs the market ready for a four-wheeled digital detox? by  Tim StevensAsk just about anybody, and they\u2019ll tell you that new cars are too expensive. In the wake of tariffs shaking the auto industry and with the Trump administration pledging to kill the federal EV incentive, that situation isn\u2019t looking to get better soon, especially for anyone wanting something battery-powered. Changing that overly spendy status quo is going to take something radical, and it\u2019s hard to get more radical than what Slate Auto has planned.Meet the Slate Truck, a sub-$20,000 (after federal incentives) electric vehicle that enters production next year. It only seats two yet has a bed big enough to hold a sheet of plywood. It only does 150 miles on a charge, only comes in gray, and the only way to listen to music while driving is if you bring along your phone and a Bluetooth speaker. It is the bare minimum of what a modern car can be, and yet"
  },
  {
    "title": "Gym Class (YC W22) Is Hiring Character Animation Engineering Lead (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-04-25T21:01:05 1745614865",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/gym-class-by-irl-studios/jobs/7UKmLED-gameplay-animation-engineer-staff-principal",
    "first_paragraph": "Play sports in virtual realityGym Class is a leading social game on Meta Quest with millions of downloads, 71K ratings, and 4.9 stars. We\u2019re now expanding the game to new interests and platforms, and looking for a talented and experienced Animation Engineer to drive the future of character movement and interaction in our evolving game world.In this pivotal role, you'll take ownership of our animation systems built in Unity, focusing on delivering high-quality, performant, and engaging experiences, particularly for mobile and VR platforms. You'll work at the intersection of character motion, gameplay, and cutting-edge technology, collaborating closely with our co-founders, engineers, artists, and designers. If you're passionate about building robust and performant animation systems, leading a critical area of development, and shipping to an engaged community, we want to hear from you.WHAT YOU'LL DOQUALIFICATIONSBonus PointsPAY AND BENEFITSSalary ranges may be inclusive of several career"
  },
  {
    "title": "Show HN: Magnitude \u2013 open-source, AI-native test framework for web apps (github.com/magnitudedev)",
    "points": 120,
    "submitter": "anerli",
    "submit_time": "2025-04-25T17:00:35 1745600435",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=43796003",
    "comments": [
      "> The idea is the planner builds up a general plan which the executor runs. We can save this plan and re-run it with only the executor for quick, cheap, and consistent runs. When something goes wrong, it can kick back out to the planner agent and re-adjust the test.I've been recently thinking about testing/qa w/ VLMs + LLMs, one area that I haven't seen explored (but should 100% be feasible) is to have the first run be LLM + VLM, and then have the LLM(s?) write repeatable \"cheap\" tests w/ traditional libraries (playwright, puppeteer, etc). On every run you do the \"cheap\" traditional checks, if any fail go with the LLM + VLM again and see what broke, only fail the test if both fail. Makes sense?\n \nreply",
      "This is exactly our workflow, though we defined our own YAML spec [1] for reasons mentioned in previous comments.We have multiple fallbacks to prevent flakes; The \"cheap\" command, a description of the intended step, and the original prompt.If any step fails, we fall back to the next source.1. https://docs.testdriver.ai/reference/test-steps\n \nreply",
      "So this is a path that we definitely considered. However we think its a half-measure to generate actual Playwright code and just run that. Because if you do that, you still have a brittle test at the end of the day, and once it breaks you would need to pull in some LLM to try and adapt it anyway.Instead of caching actual code, we cache a \"plan\" of specific web actions that are still described in natural language.For example, a cached \"typing\" action might look like:\n{\n    variant: 'type';\n    target: string;\n    content: string;\n}The target is a natural language description. The content is what to type.\nMoondream's job is simply to find the target, and then we will click into that target and type whatever content.\nThis means it can be full vision and not rely on DOM at all, while still being very consistent. Moondream is also trivially cheap to run since it's only a 2B model.\nIf it can't find the target or it's confidence changed significantly (using token probabilities), it's an indication that the action/plan requires adjustment, and we can dynamically swap in the planner LLM to decide how to adjust the test from there.\n \nreply",
      "Did you consider also caching the coordinates returned by moondream? I understand that it is cheap, but it could be useful to detect if an element has changed position as it may be a regression\n \nreply",
      "So the problem is if we cache the coordinates and click blindly at the saved positions, there's no way to tell if the interface changes or if we are actually clicking the wring things (unless we try and do something hacky like listen for events on the DOM). Detecting whether elements have changed position though would definitely be feasible if re-running a test with Moondream, could compared against the coordinates of the last run.\n \nreply",
      "This is pretty much exactly what I was going to build. It's missing a few things, so I'll either be contributing or forking this in the future.I'll need a way to extract data as part of the tests, like screenshots and page content. This will allow supplementing the tests with non-magnitude features, as well as add things that are a bit more deterministic. Assert that the added todo item exactly matches what was used as input data, screenshot diffs when the planner fallback came into play, execution log data, etc.This isn't currently possible from what I can see in the docs, but maybe I'm wrong?It'd also be ideal if it had an LLM-free executor mode to reduce costs and increase speed (caching outputs, or maybe use accessibility tree instead of VLM), and also fit requirements when the planner should not automatically kick in.\n \nreply",
      "Hey, awesome to hear! We are definitely open to contributions :)We plan to (very soon) enable mixing standard Playwright or other code in between Magnitude steps, which should enable doing exact assertions or anything else you want to do.Definitely understand the need to reduce costs / increase speed, which mainly we think will be best enabled by our plan-caching system that will get executed by Moondream (a 2B model). Moondream is very fast and also has self-hosted options. However there's no reason we couldn't potentially have an option to generate pure Playwright for people who would prefer to do that instead.We have a discord as well if you'd like to easily stay in touch about contributing:\nhttps://discord.gg/VcdpMh9tTy\n \nreply",
      "Interesting! My first concern is - isn\u2019t this the ultimate non-deterministic test? In practice, does it seem flaky?\n \nreply",
      "So the architecture is built with determinism in mind. The plan-caching system is still a work in progress, but especially once fully implemented it should be very consistent. As long as your interface doesn't change (or changes in trivial ways), Moondream alone can execute the same exact web actions as previous test runs without relying on any DOM selectors. When the interface does eventually change, that's where it becomes non-deterministic again by necessity, since the planner will need to generatively update the test and continue building the new cache from there. However once it's been adapted, it can once again be executed that way every time until the interface changes again.\n \nreply",
      "In a way, nondeterminism could be an advantage. Instead of using these as unit tests, use them as usability tests. Especially if you want your site to be accessible by AI agents, it would be good to have a way of knowing what tweaks increase the success rate.Of course that would be even more valuable for testing your MCP or A2A services, but could be useful for UI as well. Or it could be useless. It would be interesting to see if the same UI changes affect both human and AI success rate in the same way.And if not, could an AI be trained to correlate more closely to human behavior. That could be a good selling point if possible.\n \nreply"
    ],
    "link": "https://github.com/magnitudedev/magnitude",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Open source, AI-native testing framework for web apps\n      \n\n\n     \nEnd-to-end testing framework powered by visual AI agents that see your interface and adapt to any changes in it.\u2195\ufe0f Magnitude test case in action! \u2195\ufe0f1. Install our test runner in the node project you want to test (or see our demo repo if you don't have a project to try it on)2. Setup Magnitude in your project by running:This will create a basic tests directory tests/magnitude with:Magnitude requires setting up two LLM clients:For the planner, you can use any multi-modal LLM, but we recommend Gemini 2.5 pro. You can use Gemini via Google AI Studio or Vertex AI. If you don't have either set up, you can create an API key in Google AI Studio (requires billing) and export to GOOGLE_API_KEY.If no GOOGLE_API_KEY is found, Magnitude will fallback to other common providers ("
  },
  {
    "title": "Programming in D: Tutorial and Reference (ddili.org)",
    "points": 53,
    "submitter": "teleforce",
    "submit_time": "2025-04-25T20:06:33 1745611593",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=43798009",
    "comments": [
      "D deserves more recognition. It's a cool language under the radar for too long. I wish a major corporation backed it. I had a great time learning D. Also I admire Walter Bright If I could achieve even a fraction of his productivity, that would be awesome.\n \nreply",
      "Currently I'm working on adding an AArch64 code generator to the venerable dmd D compiler. It's fun as it's completely differernt from the X86_64. In some ways very clever and in some ways completely wacky.\n \nreply",
      "Slightly off topic:\nIs D a good language for creating tiny windows or Linux executables?\nThere is an upcoming game jam (4mb jam 2025) which gives extra points for game submissions <= 8KB.\nWith c you can fit a window with graphics update in an executable of less than 900 bytes[0]. Granted it's using crinkler for linking which does some compression.0: https://gist.github.com/ske2004/336d8cce8cd9db59d61ceb13c1ed...\n \nreply",
      "I feel like D is such an underrated language.\n \nreply",
      "Just to add, I learned D in a day and finished most of project euler without needing to look up the manual. D is more \"python\" than python in that it makes coding very.IMHO, Zig is the closest thing to being D-esk (like with comptime), but it's still not a mainstream option yet.\n \nreply",
      "We're not good at marketing! But we're very good at language ergonomics.\n \nreply",
      "Anything compiled with LDC2 >=1.29 (3y old) will immediately crash/segfault on macOS >=15.4A fix is on master/beta but will still take some time to be released.https://github.com/dlang/dmd/issues/21126\n \nreply",
      "Love D! I used it a bit in college when it was required for a programming language class. It's hard to justify using it nowadays though.\n \nreply",
      "Utah Valley University? Or Romania, or Turkey? And why is D's usage hard to justify (because of Rust and/or contemporary C++)?\n \nreply",
      "Mike Shah has been using D for teaching software engineering at Northeastern University and Yale. Here is his DLang playlist: https://www.youtube.com/playlist?list=PLvv0ScY6vfd9Fso-3cB4C...\n \nreply"
    ],
    "link": "https://ddili.org/ders/d.en/",
    "first_paragraph": ""
  },
  {
    "title": "Tumor-derived erythropoietin acts as immunosuppressive switch in cancer immunity (science.org)",
    "points": 111,
    "submitter": "bookofjoe",
    "submit_time": "2025-04-25T14:42:27 1745592147",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=43794110",
    "comments": [
      "Tumors excreting chemicals to prevent destruction doesn\u2019t sound like DNA damage, that sounds like evolution.We know some cancers can be caused by viruses. And we know a few cancers that act like viruses in dogs and Tasmanian devils, and some rare cases in humans.We only figured out that ulcers are bacterial in origin within the lifetimes of many HN readers, and there are signs that other GI issues may be bacterial or viral (or bacteria-targeting viral) as well.Maybe we need to start culturing and DNA testing cancers.\n \nreply",
      "We already culture and DNA test cancers.  Sometimes we can point at a secondary tumor and say \"it came from this primary tumor\".  And we already know viral and bacterial infections can increase the likelihood of people getting malignant tumorws.Most scientists wouldn't call the hallmarks of cancer \"evolution\".  I think instead most would say that cancer is an almost certainly unavoidable outcome of the complexity of eukaryotic organism's control of cellular replication.There's a series of papers organized around the \"Hallmarks of Cancer\" which help explain why nearly all tumors show the same properties- and how they are effectively due to dysregulation of evolutionary checkpoints and signalling.  generally, an organism with a malignant tumor is less likely to reproduce. However, it's really far more complex than that ,\n \nreply",
      "You're right, DNA damage is just one of the types of genetic variation in cancer. There are many other structural variations that act like remixes.\"Maybe we need to start culturing and DNA testing cancers.\"\nI assure you this is being done at a massive scale.Due to cellular stress, cancer cells disobey multi-cellular governance. They behave more like independent organisms fighting for survival, reverting to primal programming.\n \nreply",
      "Oh I know we are trying to genomically test them for oncology research and potential treatment plans, but do they do paternity tests on them?I was trying to remember which mammal in Australia gets tumors from fighting, and I found a reference to a mother getting melanoma from her daughter. It\u2019s unclear to me whether the cancer transmission was rare or the identification is rare.\n \nreply",
      "There\u2019s very often a comparison to the somatic (i.e. non-cancer) genome of the same patient. It\u2019s a great way to quality control that there wasn\u2019t some sample mixup in the lab.Transmission of cancer is rare in humans\u2014if it were not, it would make someone\u2019s career to find many cases of it. While we can\u2019t say that all sheep are white, we\u2019ve looked at enough of them to say that black sheep are not common. Furthermore, it\u2019s very clear how the Tasmanian devil cancer is spread\u2014it\u2019s around the mouth while they are biting each others faces; it\u2019s not as obvious how one would spread most human cancers.\n \nreply",
      "Oh that makes sense. I forgot about differential analysis.\n \nreply",
      "Is HPV an example?\n \nreply",
      "Not really.  It's a virus that can cause cancer and not the cancer itself.\n \nreply",
      "tasmanian devils  [edit: I guess you already said that] https://en.wikipedia.org/wiki/Devil_facial_tumour_disease\n \nreply",
      "Yup. Link is handy though. Someone will post it to the front page in 14 hours :)\n \nreply"
    ],
    "link": "https://www.science.org/doi/10.1126/science.adr3026",
    "first_paragraph": ""
  },
  {
    "title": "GCC 15.1 (gcc.gnu.org)",
    "points": 193,
    "submitter": "jrepinc",
    "submit_time": "2025-04-25T10:53:59 1745578439",
    "num_comments": 118,
    "comments_url": "https://news.ycombinator.com/item?id=43792248",
    "comments": [
      "> {0} initializer in C or C++ for unions no longer guarantees clearing of the whole union (except for static storage duration initialization), it just initializes the first union member to zero. If initialization of the whole union including padding bits is desirable, use {} (valid in C23 or C++) or use -fzero-init-padding-bits=unions option to restore old GCC behavior.This is going to silently break so much existing code, especially union based type punning in C code. {0} used to guarantee full zeroing and {} did not, and step by step we've flipped the situation to the reverse. The only sensible thing, in terms of not breaking old code, would be to have both {0} and {} zero initialize the whole union.I'm sure this change was discussed in depth on the mailing list, but it's absolutely mind boggling to me\n \nreply",
      "Fun fact: GCC decided to adopt Clang's (old) behavior at the same time Clang decided to adopt GCC's (old) behavior.So now you have this matrix of behaviors:\n* Old GCC: Initializes whole union.\n* New GCC: Initializes first member only.\n* Old Clang: Initializes first member only.\n* New Clang: Initializes whole union.\n \nreply",
      "That's funny and sad at the same time.And it shows a deeper problem, even though they are willing to align behavior between each other, they failed to communicate and discuss what would be the best approach. That's a bit tragic, IMO\n \nreply",
      "I would argue the even deeper problem is that it's implementation defined. Should be in the spec and they should conform to the spec. That's why I'm so paranoid and zeroize things myself. Too much hassle to remember what is or isn't zero.\n \nreply",
      "Since having multiple compilers is often touted as an advantage, how often do situations like what you're describing happen compared to the opposite \u2014 when a second compiler surfaces bugs in one's application or the other compiler?\n \nreply",
      "This was my instinct too, until I got this little tickle in the back of my head that maybe I remembered that Clang was already acting like this, so maybe it won't be so bad.  Notice 32-bit wzr vs 64-bit xzr:    $ cat union.c && clang -O1 -c union.c -o union.o && objdump -d union.o\n    union foo {\n        float  f;\n        double d;\n    };\n\n    void create_f(union foo *u) {\n        *u = (union foo){0};\n    }\n\n    void create_d(union foo *u) {\n        *u = (union foo){.d=0};\n    }\n\n    union.o: file format mach-o arm64\n\n    Disassembly of section __TEXT,__text:\n\n    0000000000000000 <ltmp0>:\n           0: b900001f      str wzr, [x0]\n           4: d65f03c0      ret\n\n    0000000000000008 <_create_d>:\n           8: f900001f      str xzr, [x0]\n           c: d65f03c0      ret\n \nreply",
      "Ah, I can confirm what I see elsewhere in the thread, this is no longer true in Clang.  That first clang was Apple Clang 17---who knows what version that actually is---and here is Clang 20:    $ /opt/homebrew/opt/llvm/bin/clang-20 -O1 -c union.c -o union.o && objdump -d union.o\n\n    union.o: file format mach-o arm64\n\n    Disassembly of section __TEXT,__text:\n\n    0000000000000000 <ltmp0>:\n           0: f900001f      str xzr, [x0]\n           4: d65f03c0      ret\n\n    0000000000000008 <_create_d>:\n           8: f900001f      str xzr, [x0]\n           c: d65f03c0      ret\n \nreply",
      "Looks like that change is clang \u226419 to clang 20: https://godbolt.org/z/7zrocxGaq\n \nreply",
      "> This is going to silently break so much existing codeThe code was already broken. It was an undefined behavior.That's a problem with C and it's undefined behavior minefields.\n \nreply",
      "GCC has long been known to define undefined behavior in C unions. In particular, type punning in unions is undefined behavior under the C and C++ standards, but GCC (and Clang) define it.\n \nreply"
    ],
    "link": "https://gcc.gnu.org/gcc-15/",
    "first_paragraph": "April 25, 2025The GCC developers are pleased to announce the release of GCC 15.1.This release is a major release, containing new features (as well\nas many other improvements) relative to GCC 14.x.GCC used to stand for the GNU C Compiler, but since the compiler\nsupports several other languages aside from C, it now stands for the\nGNU Compiler Collection.The GCC developers would like to thank the numerous people that have\ncontributed new features, improvements, bug fixes, and other changes as\nwell as test results to GCC.\nThis amazing\ngroup of volunteers is what makes GCC successful.For additional information about GCC please refer to the\nGCC project web site or contact the\nGCC development mailing list.To obtain GCC please use our mirror sites\nor our version control system.Copyright (C)\nFree Software Foundation, Inc.\nVerbatim copying and distribution of this entire article is\npermitted in any medium, provided this notice is preserved.These pages are\nmaintained by the GCC team.\nLast modifie"
  },
  {
    "title": "Differential Coverage for Debugging (swtch.com)",
    "points": 42,
    "submitter": "todsacerdoti",
    "submit_time": "2025-04-25T16:01:24 1745596884",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=43795090",
    "comments": [
      "https://www.debuggingbook.org/html/StatisticalDebugger.htmlA related method. Not quite as straightforward as running with and without the failing test and comparing coverage reports. This technique goes through and collects many test runs and identifies lines only associated with or most often associated with failing runs.\n \nreply",
      "I had no idea this had (or was worthy of) a name.That's the whole point of coverage diffs.The tough ones are the tests that sometimes fail and give you the same coverage results - the problem is not in the code under test! And the lazy/common things to do are re-run the test or add a sleep to make things \"work.\"\n \nreply"
    ],
    "link": "https://research.swtch.com/diffcover",
    "first_paragraph": "\nI have been debugging some code I did not write and was reminded of this technique.\nI\u2019m sure it\u2019s a very old debugging technique (like bisection),\nbut it should be more widely known.\nSuppose you have one test case that\u2019s failing.\nYou can get a sense of what code might be involved by comparing the code coverage\nof successful tests with the code coverage of the failing test.\n\n\nFor example, I\u2019ve inserted a bug into my development copy of math/big:\n\n$ go test\n--- FAIL: TestAddSub (0.00s)\n    int_test.go:2020: addSub(-0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff, 0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff) = -0x0, -0x1fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe, want 0x0, -0x1fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe\nFAIL\nexit status 1\nFAIL\tmath/big\t7.528s\n$\n\n\nLet\u2019s collect a passing and failing profile:\n\n$ go test -coverprofile=c1.prof -skip='TestAddSub$'\nPASS\ncoverage: 85.0% of statements\nok  \tmath/"
  }
]