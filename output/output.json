[
  {
    "title": "Volkswagen Introduces ID. EVERY1, Its Most Affordable EV (ttnews.com)",
    "points": 66,
    "submitter": "herbertl",
    "submit_time": "2025-03-06T00:36:40 1741221400",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=43274698",
    "comments": [
      "That's way too expensive for an \"affordable EV\".The BYD Seagull retails here in Uruguay for less than that and we tax cars at about 100%. On China it seems to go for 10-12k.It's a proper, basic city car. 4 to 6 air bags, ~300km range (more than what this article's car indicates), all basic security features and standard gadgets out of a modern car.Our EV infrastructure is not viable if you don't have a charger at work/home and yet these have sold like hot cakes.Legacy carmakers are making increasingly worse ICE cars for the most part (btw does GM sell a C-segment hatchback on any market, anymore?) and their EVs are simply uncompetitive. What's it going to take for them to wake up to the fact they're going to have to stop fleecing their customers with crappy products? Bankruptcy?\n \nreply",
      "However good this thing is otherwise, \"ID Everyone\" is a horrible slogan for a new car. I wouldn't mind some manufacturer agreeing to not buy/sell your sexual history, creating radios which aren't vulnerable to remote controlling the car via buffer overflows, and otherwise treating cars like physical goods you own and can rely opon instead of hooks into your personal life.\n \nreply",
      "From the borders to the polls to the security office, ID Everyone is for you!\n \nreply",
      "Their marketing people have been detoxing from their cocaine binges recently, so the air is outhttps://apnews.com/article/china-volkswagen-executive-deport...\n \nreply",
      "The significant part of that story is there's no claim he used cocaine or recreational drugs \"on the job\"  in China ...  A senior executive for Volkswagen in China has been deported for allegedly using cocaine and marijuana while on vacation in Thailand, according to Chinese authorities and German media reports.\n\n  Germany\u2019s top-selling Bild newspaper [..] reported he tested positive for drug use after returning from a holiday in Thailand.\n\n  Drug use is an administrative offense in China punishable by a 10- to 15-day detention and a fine of up to 2,000 yuan ($280).\n\n  Thailand legalized marijuana in 2022 but Chinese authorities have warned that use of the drug overseas is equivalent to using it at home and subject to the same penalties.\n \nreply",
      "> it\u2019s due to start production in two yearsThe specs are great for 2025. Let\u2019s wait for 2027 when BYD has a better performing model for much less.I am all vouching for VW\u2019s electric efforts. I drove the ID Buzz which is great. I just wonder if they could sell competitively without tariffs in the EU.\n \nreply",
      "For interested parties in the US, it is not expected to be brought here https://www.caranddriver.com/news/a64054081/vw-id-every1-con...\n \nreply",
      "Sounds like the US needs to target it with tariffs quick. Surely it will be better for our economy to ensure the survival of a couple zombie automakers  than it would be to bring accessible EVs to all.\n \nreply",
      "Sounds like the US needs to target it with tariffs quick.As if VW would ever bring this to the U. S. Which they\u2019re not.\n \nreply",
      "It\u2019s too small for the US market, depressingly\n \nreply"
    ],
    "link": "https://www.ttnews.com/articles/vw-introduces-id-every1",
    "first_paragraph": "Bloomberg News[Stay on top of transportation news: Get TTNews in your inbox.]\nVolkswagen AG became a global car giant thanks to the Beetle, an affordable model for the masses. Now, the German automaker is finally unveiling a budget car for the electric-vehicle age as it tries to catch up with Chinese brands and other rivals with cheaper options.\n      \t  \t\t\t            \n\nVW on March 5 presented the compact ID. EVERY1 concept car that will cost around \u20ac20,000 ($21,000) \u2014 its lowest-priced EV yet. The first VW-brand model to use electric architecture jointly developed with Rivian Automotive Inc., it\u2019s due to start production in two years at an undecided location in Europe.\n      \t  \t\t\t            \n\nThe model will offer more than 155 miles of range and is the \u201clast piece of the puzzle\u201d for VW\u2019s next-generation lineup, said Thomas Schaefer, CEO of the brand.\n      \t    \n\n\n\n\n\n\nEuropean carmakers are struggling to convince consumers to make the switch to EVs given their higher sticker prices"
  },
  {
    "title": "Tailscale is pretty useful (6nok.org)",
    "points": 395,
    "submitter": "marban",
    "submit_time": "2025-03-05T19:09:19 1741201759",
    "num_comments": 225,
    "comments_url": "https://news.ycombinator.com/item?id=43270835",
    "comments": [
      "Tailscale is one of my favorite companies. They're clearly on to something. Here's a great post by their CTO explaining a lot of the motivation and vision behind it: https://crawshaw.io/blog/remembering-the-lanIMO the main outstanding questions/concerns are:* Is the VPN model really the way to go? If someone gets their hands on one of your Tailscale nodes, they can access every service on your tailnet, which are likely running with reduced security since that's a huge part of the appeal. This is exactly the situation BeyondCorps/Zero Trust was created to avoid. Tunneling services[0] are more of a Zero Trust approach, but they can't match the seamlessness of Tailscale once a node is connected to the tailnet.* Can it expand into the layman market? I wonder if the average person will ever be willing to install a VPN app on all their devices. On the flipside, I could see TS partnering with someone like Google to integrate TS tightly with Android and set up a private network between all your Google-signed-in devices.* The relay system - DERP is nice, but it's primarily intended for signaling/fallback. It feels like CGNAT adoption is growing faster than IPv6 is, and I wouldn't be surprised if fewer and fewer p2p connections succeed over time[1]. DERP forces everything over a single TCP connection (HOL blocking), and I'm not sure it even has any flow control.* Use in web browsers - They got a demo of this working, but it's pretty involved. You have to compile the entire Tailscale Golang library to WebAssembly which is a large artifact, and it's DERP-exclusive.* Portability in general - Depending on WireGuard, as awesome as it is, is fairly limiting. You either need admin privileges to create the TUN device, or you need to run an entire TCP stack in userspace alongside your own WireGuard implementation. I'd be interested to see something like Tailscale implemented on top of WebTransport.[0]: https://github.com/anderspitman/awesome-tunneling[1]: https://tailscale.com/blog/how-nat-traversal-works\n \nreply",
      "> * Is the VPN model really the way to go? If someone gets their hands on one of your Tailscale nodes, they can access every service on your tailnet, which are likely running with reduced security since that's a huge part of the appeal. This is exactly the situation BeyondCorps/Zero Trust was created to avoid. Tunneling services[0] are more of a Zero Trust approach, but they can't match the seamlessness of Tailscale once a node is connected to the tailnet.At the very least there's ACLs so you can tag devices and restrict access down to specific ports and protocols based on either user identity or device tag.At my org we use tailscale much like a VPN, to give users access to a few internal web apps, and with ACLs those users can only hit the webserver on 443 and nothing else to that node. This way the web server itself has no ports exposed on the host, ufw deny all incoming.I can't answer if the VPN model is really the way to go, long term - probably not, but for our use case Tailscale has been absolutely perfect, and we accepted the tradeoffs were worth it over a more \"complete\" zero-trust approach, and the complexities that come along with it.What Tailscale doesn't solve is access to the data that web app serves if the user's machine is compromised, as tailscale is just determining \"can the user hit the webserver on port 443?\" and does nothing to evaluate the state of the user's host.I guess that's all to say, I/we don't see Tailscale as a zero-trust solution, but more or less a more convenient VPN with easier to use ACLs. Cloudflare Tunnel and the likes are much better suited to implementing a zero trust approach.I think there's still value though. A zero trust approach is the correct way for most organizations, but there's still a big niche for Tailscale especially for small-medium orgs and self-hosters/homelabbers.\n \nreply",
      "Tailscale is not just more convenient but also more efficient if your VPN meshes a lot ( not all traffic going to the same place). Because nodes can establish connections directly. A traditional VPN can't do that.This is the main reason I use a mesh vpn (though not tailscale)\n \nreply",
      "> What Tailscale doesn't solve is access to the data that web app serves if the user's machine is compromised, as tailscale is just determining \"can the user hit the webserver on port 443?\" and does nothing to evaluate the state of the user's host.Tailscale has some cybersecurity integrations to configure access depending on the device posture. For example, blocking access to a webserver if the device is out of date, or if malware is detected, or if the firewall is disabled, etc. But I don't use any of those integrations and can't speak to them.\n \nreply",
      "I don't think most users use those integrations, they're mostly just a feature bullet point.\n \nreply",
      "SO those features are unusable?\n \nreply",
      "On your first point, I've been using tailscale for a bit and its ACL feature addresses most of my concerns there. My laptop can ssh into any of my servers but not the other way around, and my servers cant talk to each other unless I set them to.\n \nreply",
      "Could you share your ACL setup? I haven't had time to look at it much but this sounds like exactly what I want to do.\n \nreply",
      "Agree that they are on to something. I gave a tech talk about them a while ago at work and said that I think they are on the cusp of providing a consumer VPN product that appeals to mainstream consumers. The Apple of VPNs, everything \"just works\" and is easy to understand.\n \nreply",
      "Do mainstream consumers really need a VPN?\n \nreply"
    ],
    "link": "https://blog.6nok.org/tailscale-is-pretty-useful/",
    "first_paragraph": ""
  },
  {
    "title": ">8 token/s DeepSeek R1 671B Q4_K_M with 1~2 Arc A770 on Xeon (github.com/intel)",
    "points": 19,
    "submitter": "colorant",
    "submit_time": "2025-03-06T00:23:33 1741220613",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=43274613",
    "comments": [
      "No... this headline is incorrect. You can't do that. I think they've confused the performance of running one of the small distills to existing smaller models. Two Arc cards cannot fit a 4 bit k-quant of a 671b model.But a portable (no install) way to run llama.cpp on intel GPUs is really cool.\n \nreply",
      "It is theoretically possible. Each token only needs 37B parameters and if the same experts are chosen often, it would behave closer to a 37B model than a 671B model, since reusing experts can skip loads from system RAM.You might still be right since I have not confirmed that the selected experts change infrequently doing prompt processing / token generation, and someone could have botched the headline. However, treating Deepseek like llama 3 when reasoning about VRAM requirements is not necessarily correct.\n \nreply",
      "You don't have to go that far down the page to see it is paging to system RAM:Requirements:    380GB CPU Memory\n    1-8 ARC A770\n    500GB Disk\n \nreply",
      "Yep. That's why the headline is incorrect. 380GB of the model on CPU system RAM and 32GB on some ARC GPUs. The ratio, 380/32, is obvious. Most of the processing is being done on the CPU. The GPU are little bit icing in this context. Fast, sure, but having to wait for the CPU layers (that's how layer splits work with llama.cpp).I think changing the end of headline to \"Xeon w/380GB RAM\" would stop it from being incorrect and misleading.\n \nreply",
      "What if it does not need to read from system RAM for every token by reusing experts whenever they just happen to be in VRAM from being used for the previous token?",
      "\"with\" does not mean \"entirely on\"\n \nreply",
      "yep, title is inaccurate. it's a distill into Qwen 7B DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf\n \nreply"
    ],
    "link": "https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n   < English | \u4e2d\u6587 >\nImportantWe can now run DeepSeek-R1-671B-Q4_K_M with 1 or 2 Arc A770 on Xeon using the latest llama.cpp Portable Zip.This guide demonstrates how to use llama.cpp portable zip to directly run llama.cpp on Intel GPU with ipex-llm (without the need of manual installations).Notellama.cpp portable zip has been verified on:Check your GPU driver version, and update it if needed:For Intel Core Ultra processors (Series 2) or Intel Arc B-Series GPU, we recommend updating your GPU driver to the latestFor other Intel iGPU/dGPU, we recommend using GPU driver version 32.0.101.6078Download IPEX-LLM llama.cpp portable zip for Windows users from the link.Then, extract the zip file to a folder.Here we provide a simple example to show how to run a community GGUF model with IPEX-LLM.Before running, you should download or copy community GGUF"
  },
  {
    "title": "Git without a forge (greenend.org.uk)",
    "points": 134,
    "submitter": "todsacerdoti",
    "submit_time": "2025-03-05T20:56:25 1741208185",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=43272275",
    "comments": [
      "This is what I love about Fossil[1]. You get all of those extra tools (wiki, chat, forums, bug tracker) in the server that is also in the binary that you use to manage the repo.So, if you want to serve it, just `fossil server file.fossil` or serve a whole directory of them. Or, if you want, you can just `fossil ui` and muck around yourself, locally. The server supports SSH and HTTPS interactions for cloning and pushing.All by the same people who make SQLite, Pikchr, and more.[1]: https://fossil-scm.org\n \nreply",
      "Yup. +1 for fossil. I wanted an issue tracker that wasn\u2019t text files in the repo. Lots of git-based things that were heavier (gitea and friends) or hackier than I wanted. Decided to finally try out fossil and I think it\u2019s really really neat.\n \nreply",
      "In terms of everyday workflow, does Fossil differ radically from git? If so, what's the learning curve like?\n \nreply",
      "> Multiple files to herd. When I get an email with five patch attachments, I have five files to copy around instead of one, with long awkward names.That\u2019s not correct. You can write the email to an mbox file (your MUA lets you do that, right?) and then use `git am` to pull it all into git.> Why I don\u2019t like it: because the patch series is split into multiple emails, they arrive in my inbox in a random order, and then I have to save them one by one to files, and manually sort those files back into the right order by their subject lines.The patch series arrives threaded, so your MUA should be able to display them in order. Export them to an mbox file, and then use `git am` again.There might be ways that someone can send you email in this way and for the patches to be broken such that `git am` won\u2019t work, of course. I take no issue with that part of the argument.\n \nreply",
      "Not everyone has a fancy client-side MUA that gives them trivial access to mbox files. E.g., a typical webmail service will make exporting mboxes into a whole process at best. (And on the sending side, have fun with the lack of git send-email integration. I've spent far more time than I'd like manually patching up References and In-Reply-To headers.)Of course, the classic response is \"get a better MUA you luser\", but that just adds even more steps for people who use webmail services for 99.9% of their email needs.\n \nreply",
      "For those that don't have an MUA, I have made git-receive-mail[1]. It really is very doable these days to do the email workflow, on both ends.1: https://github.com/djha-skin/git-receive-mail\n \nreply",
      "People can use webmail for regular email, but then connect a \u201cbetter\u201d MUA for patch handling. I get that this would be more steps, but for those who don\u2019t want to do this, they probably just use GitHub PRs, and that\u2019s fine, they can carry on doing that :-)I\u2019m just completing the picture by pointing out that for those who choose to use emails to jockey patches around by mutual agreement, including patches in emails really shouldn\u2019t be a problem.\n \nreply",
      "Yes, this all stands and falls with using a competent email client. \nThere are some hints regarding email clients here, though focused on sending patches: https://github.com/torvalds/linux/blob/master/Documentation/...\n \nreply",
      "> People can \u2018git clone\u2019 my code, and there\u2019s a web-based browsing interface (the basic gitweb) for looking around without having to clone it at all.I host my own public Git repositories, but statically--read-only, no HTML views. I don't want to run any specialized server-side code, whether dynamic or preprocessed, as that's a security vector and system administration maintenance headache I don't want to deal with. You can host a Git repository as a set of static files using any web server, without any special configuration. Just clone a bare repo into an existing visible directory. `git update-server-info` generates the necessary index files for `git clone https://...` to work transparently. I add a post-receive hook to my read-write Git repositories that does `cd /path/to/mirror.git && git fetch && git --bare update-server-info` to keep the public repo updated.In theory something like gitweb could be implemented purely client-side, using JavaScript or WASM to fetch the Git indices and packs on-demand and generate the HTML views. Some day I'd like to give that a try if someone doesn't beat me to it. You could even serve it as index.html from the Git repository directory, so the browser app and the Git clone URL are identical.\n \nreply",
      "I like this idea quite a bit. It's lightweight and read-only, which makes it far easier to host. I'm in the process of publishing some of my own repositories so I'm going to give this a try.\n \nreply"
    ],
    "link": "https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/git-no-forge/",
    "first_paragraph": "[Simon Tatham, 2025-03-05]I\u2019ve written quite a lot of free software in my life. Most of\n      it was from scratch: projects I started myself. So I get to\n      choose where to host them \u2013 or rather, I have to choose\n      where to host them.These days, all my projects are held in Git. And mostly, I put\n      them in \u2018bare\u2019 git repositories on my personal website.I don\u2019t use any git \u2018forge\u2019 system layered on top of Git, like\n      Gitlab or Github, which automatically makes a bug tracking\n      database for each project, and provides a convenient button for\n      a user to open a merge request / pull request. I just use plain\n      Git. People can \u2018git clone\u2019 my code, and there\u2019s a\n      web-based browsing interface (the basic gitweb) for\n      looking around without having to clone it at all. But that\u2019s all\n      the automated facilities you get.Occasionally this confuses people, so I thought I should write\n      something about it.Sometimes people just can\u2019t work out how to send me pa"
  },
  {
    "title": "Apple M3 Ultra (apple.com)",
    "points": 790,
    "submitter": "ksec",
    "submit_time": "2025-03-05T13:59:50 1741183190",
    "num_comments": 761,
    "comments_url": "https://news.ycombinator.com/item?id=43266453",
    "comments": [
      "512GB of unified memory is truly breaking new ground. I was wondering when Apple would overcome memory constraints, and now we're seeing a half-terabyte level of unified memory. This is incredibly practical for running large AI models locally (\"600 billion parameters\"), and Apple's approach of integrating this much efficient memory on a single chip is fascinating compared to NVIDIA's solutions. \nI'm curious about how this design of \"fusing\" two M3 Max chips performs in terms of heat dissipation and power consumption though\n \nreply",
      "They didn't increase the memory bandwidth. You can get the same memory bandwidth, which is available on the M2 Studio. Yes, yes, of course you can get 512 gigabytes of uRAM for 10 grand.The the question is if a llm will run with usable performance at that scale? The point is there's diminishing returns despite having enough uRAM with the same amount of memory bandwidth even with increased processing speed of the new chip for AI.So there must be a min-max performance ratio between memory bandwidth and the size of the memory pool in relation to the processing power.\n \nreply",
      "Since no one specifically answered your question yet, yes, you should be able to get usable performance. A Q4_K_M GGUF of DeepSeek-R1 is 404GB. This is a 671B MoE that \"only\" has 37B activations per pass. You'd probably expect in the ballpark of 20-30 tok/s (depends on how much actually MBW can be utilized) for text generation.From my napkin math, the M3 Ultra TFLOPs is still relatively low (around 43 FP16 TFLOPs?), but it should be more than enough to handle bs=1 token generation (should be way <10 FLOPs/byte for inference). Now as far is its prefill/prompt processing speed... well, that's another matter.\n \nreply",
      "I actually think it\u2019s not a coincidence and they specifically built this M3 Ultra for DeepSeek R1 4-bit. They also highlight in their press release that they tested it with 600B class LLMs (DeepSeek R1 without referring to it by name). And they specifically did not stop at 256 GB RAM to make this happen. Maybe I\u2019m reading too much into it.\n \nreply",
      "Any ideas on power consumption? I wonder how much power would that use. It looks like it would be more efficient than everything else that currently exists.\n \nreply",
      "I don\u2019t think you understand hardware timelines if you think this product had literally anything to do with anything DeepSeek.\n \nreply",
      "I would be curious about context window size that would be expected when generating ballpark 20 to 20 tokens per second using Deepseek-R1 Q4 on this hardware?\n \nreply",
      "Probably helps that models like deepseek are mixture of expert. Having all weights in VRAM means you don\u2019t have to unlod/reload. Memory bandwidth usage should be limited to the 37B active parameters.\n \nreply",
      "> Probably helps that models like deepseek are mixture of expert. Having all weights in VRAM means you don\u2019t have to unlod/reload. Memory bandwidth usage should be limited to the 37B active parameters.\"Memory bandwidth usage should be limited to the 37B active parameters.\"Can someone do a deep dive above quote. I understand having the entire model loaded into RAM helps with response times. However, I don't quite understand the memory bandwidth to active parameters.Context window?How much the model can actively be processed despite being fully loaded into memory based on memory bandwidth?\n \nreply",
      "With a mixture of experts model you only need to read a subset of the weights from memory to compute the output of each layer. The hidden dimensions are usually smaller as well so that reduces the size of the tensors you write to memory.\n \nreply"
    ],
    "link": "https://www.apple.com/newsroom/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/",
    "first_paragraph": "Text of this articleMarch 5, 2025PRESS RELEASEApple reveals M3\u00a0Ultra, :br(xl)::br(l):taking Apple silicon to a new extremeThe new chip delivers up to 2.6x the performance of M1 Ultra, along with Thunderbolt 5 connectivity and support for more than half a terabyte of unified memory \u2014 the most ever in a personal computerCUPERTINO, CALIFORNIA Apple today announced M3 Ultra, the highest-performing chip it has ever created, offering the most powerful CPU and GPU in a Mac, double the Neural Engine cores, and the most unified memory ever in a personal computer. M3 Ultra also features Thunderbolt 5 with more than 2x the bandwidth per port for faster connectivity and robust expansion. M3 Ultra is built using Apple\u2019s innovative UltraFusion packaging architecture, which links two M3 Max dies over 10,000 high-speed connections that offer low latency and high bandwidth. This allows the system to treat the combined dies as a single, unified chip for massive performance while maintaining Apple\u2019s indu"
  },
  {
    "title": "QwQ-32B: Embracing the Power of Reinforcement Learning (qwenlm.github.io)",
    "points": 211,
    "submitter": "nwjsmith",
    "submit_time": "2025-03-05T19:09:39 1741201779",
    "num_comments": 62,
    "comments_url": "https://news.ycombinator.com/item?id=43270843",
    "comments": [
      "Note the massive context length (130k tokens). Also because it would be kinda pointless to generate a long CoT without enough context to contain it and the reply.EDIT: Here we are. My first prompt created a CoT so long that it catastrophically forgot the task (but I don't believe I was near 130k -- using ollama with fp16 model). I asked one of my test questions with a coding question totally unrelated to what it says:<QwQ output>\nBut the problem is in this question. Wait perhaps I'm getting ahead of\nmyself.Wait the user hasn't actually provided a specific task yet. Let me check\nagain.The initial instruction says:\"Please act as an AI agent that can perform tasks... When responding,\nfirst output a YAML data structure with your proposed action, then wait\nfor feedback before proceeding.\"But perhaps this is part of a system prompt? Wait the user input here\nseems to be just \"You will be given a problem. Please reason step by\nstep...\" followed by a possible task?\n</QwQ>Note: Ollama \"/show info\" shows that the context size set is correct.\n \nreply",
      "Ollama defaults to a context of 2048 regardless of model unless you override it with /set parameter num_ctx [your context length]. This is because long contexts make inference slower. In my experiments, QwQ tends to overthink and question itself a lot and generate massive chains of thought for even simple questions, so I'd recommend setting num_ctx to at least 32768.In my experiments of a couple mechanical engineering problems, it did fairly well in final answers, correctly solving mechanical engineering problems that even DeepSeek r1 (full size) and GPT 4o did wrong in my tests. However, the chain of thought was absurdly long, convoluted, circular, and all over the place. This also made it very slow, maybe 30x slower than comparably sized non-thinking models.I used a num_ctx of 32768, top_k of 30, temperature of 0.6, and top_p of 0.95. These parameters (other than context length) were recommended by the developers on Hugging Face.\n \nreply",
      "My understanding is that top_k and top_p are two different methods of decoding tokens during inference. top_k=30 considers the top 30 tokens when selecting the next token to generate and top_p=0.95 considers the top 95 percentile. You should need to select only one.https://github.com/ollama/ollama/blob/main/docs/modelfile.md...Edit: Looks like both work together. \"Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\"Not quite sure how this is implemented - maybe one is preferred over the other when there are enough interesting tokens!\n \nreply",
      "I tried the 'Strawberry' question which generated nearly 70k words of CoT.\n \nreply",
      "Yeah it did the same in my case too. it did all the work in the <think> tokens. but did not spit out the actual answer. I was not even close to 100K tokens\n \nreply",
      "that's interesting... i've been noticing similar issues with long context windows & forgetting. are you seeing that the model drifts more towards the beginning of the context or is it seemingly random?i've also been experimenting with different chunking strategies to see if that helps maintain coherence over larger contexts. it's a tricky problem.\n \nreply",
      "Neither lost-in-the-middle nor long context performance have seen a lot of improvement in the recent year. It's not easy to generate long training examples that also stay meaningful, and all existing models still become significantly dumber after 20-30k tokens, particularly on hard tasks.Reasoning models probably need some optimization constraint put on the length of the CoT, and also some priority constraint (only reason about things that need it).\n \nreply",
      "My burning question: Why not also make a slightly larger model (100B) that could perform even better?Is there some bottleneck there that prevents RL from scaling up performance to larger non-MoE model?\n \nreply",
      "It says \"wait\" (as in \"wait, no, I should do X\") so much while reasoning it's almost comical.  I also ran into the \"catastrophic forgetting\" issue that others have reported - it sometimes loses the plot after producing a lot of reasoning tokens.Overall though quite impressive if you're not in a hurry.\n \nreply",
      "My informal testing puts it just under Deepseek-R1. Very impressive for 32B.  It maybe thinks a bit too much for my taste. In some of my tests the thinking tokens were 10x the size of the final answer.  I am eager to test it with function calling over the weekend.\n \nreply"
    ],
    "link": "https://qwenlm.github.io/blog/qwq-32b/",
    "first_paragraph": "QWEN CHAT\nHugging Face\nModelScope\nDEMO\nDISCORDScaling Reinforcement Learning (RL) has the potential to enhance model performance beyond conventional pretraining and post-training methods. Recent studies have demonstrated that RL can significantly improve the reasoning capabilities of models. For instance, DeepSeek R1 has achieved state-of-the-art performance by integrating cold-start data and multi-stage training, enabling deep thinking and complex reasoning.Our research explores the scalability of Reinforcement Learning (RL) and its impact on enhancing the intelligence of large language models. We are excited to introduce QwQ-32B, a model with 32 billion parameters that achieves performance comparable to DeepSeek-R1, which boasts 671 billion parameters (with 37 billion activated). This remarkable outcome underscores the effectiveness of RL when applied to robust foundation models pretrained on extensive world knowledge. Furthermore, we have integrated agent-related capabilities into t"
  },
  {
    "title": "Zentool \u2013 AMD Zen Microcode Manipulation Utility (github.com/google)",
    "points": 77,
    "submitter": "taviso",
    "submit_time": "2025-03-05T21:10:35 1741209035",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=43272463",
    "comments": [
      "This is not the first case of accidental reuse of example keys in firmware signing, https://kb.cert.org/vuls/id/455367Would it be useful to have a public list of all example keys that could be accidentally used, which could be CI/CD tested on all publicly released firmware and microcode updates?If there was a public test suite, Linux fwupd and Windows Update could use it for binary screening before new firmware updates are accepted for distribution to endpoints.\n \nreply",
      "The blog post that explains the exploit and how this whole thing works is at https://bughunters.google.com/blog/5424842357473280/zen-and-...\n \nreply",
      "Is the mitigation something that has to be installed on every system boot and only protects against microcode exploits later on that boot?\n \nreply",
      "> The fix released by AMD modifies the microcode validation routine to use a custom secure hash function. This is paired with an AMD Secure Processor update which ensures the patch validation routine is updated before the x86 cores can attempt to install a tampered microcode patch.\n \nreply",
      "Both AMD and Google note, that Zen[1-4] are affected, but what changed about Zen5? According to the timeline, it released before Google notified AMD [1].Is it using different keys, but same scheme (and could possibly be broken via side-channels as noted in the article)? Or perhaps AMD notices something and changed up the microcode? Some clarification on that part would be nice.[1] https://github.com/google/security-research/security/advisor...\n \nreply",
      "Was the microcode signing scheme documented by AMD, or did the researchers have to reverse engineer it somehow? I couldn't see a mention in the blog post.\n \nreply",
      "From the blog post:> We plan to provide additional details in the upcoming months on how we reverse engineered the microcode update process, which led to us identifying the validation algorithms\n \nreply",
      "> You can use the `resign` command to compensate for the changes you made:How does that work?  Did someone figure out AMD's private keys?\n \nreply",
      "The intro document mentions> Here's the thing - the big vendors encrypt and sign their updates so that you cannot run your own microcode. A big discovery recently means that the authentication scheme is a lot weaker than intended, and you can now effectively \"jailbreak\" your CPU!But there's no further details. I'd love to know about the specifics too!\n \nreply",
      "They accidentally used the example key from AES-CMAC RFC, the full details are in the accompanying blog post: https://bughunters.google.com/blog/5424842357473280/zen-and-...\n \nreply"
    ],
    "link": "https://github.com/google/security-research/blob/master/pocs/cpus/entrysign/zentool/README.md",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          This package provides a suite of tools for analyzing, manipulating and\ngenerating microcode patches for AMD Zen processors.The command zentool is a frontend to various utilities. There is also a\nmcas, a simple assembler and mcop a simple disassembler.Note: We also have an introduction to microcoding, and\na programming reference.The general format of a command is:Type zentool help to see a list of available commands.You can examine the header of a microcode update file using the print\ncommand:Let's modify that revision number using the edit command, and save the result\nto modified.bin:That worked, but now the signature will be incorrect:You can use the resign command to compensate for the changes you made:Now you can apply that update to your processor with the load command, this\nrequires root privileges:Now we can verify that worked by query"
  },
  {
    "title": "Solving First Order Differential Equations with Julia (ritog.github.io)",
    "points": 40,
    "submitter": "__rito__",
    "submit_time": "2025-03-03T18:41:37 1741027297",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=43245172",
    "comments": [
      "Good on the author for writing a blog post, but this is so elementary as to not be interesting.I wrote an ODE solver in GDScript in a weekend that uses RK4, implicit Euler, and BDF. It's not a terribly difficult thing to do and there are much more interesting things I'm sure you can do in Julia like use implicit differentiation to easily get the Jacobian.\n \nreply",
      "Would that be worth a blogpost?\n \nreply",
      "X^2 - 3x - 18 =0[...]This is how the equation is created: x(x+3) = 18watMaybe don't do very fast recaps if you're not going to proofread them. Incidentally I assume the formulae in this article were done with MathJax or its Julia equivalent, they render great but can't be copied from the text.Overall a good article (and a great ad for Julia) but stumbling blocks like the one above ensure some readers won't make it any farther.\n \nreply",
      "Is the issue here quadratic factorization is not obvious or that the sign is wrong?Note that x(x+3) = 18 and x(x-3) = 18 are valid models of the same problem. One finds the longer side in terms of the shorter, or vice versa\n \nreply"
    ],
    "link": "https://ritog.github.io/posts/1st-order-DE-julia/1st_order_DE_julia.html",
    "first_paragraph": "Using DifferentialEquations.jl to solve Linear 1st Order DEsRitobrata Ghosh March 3, 2025Image by Pete Linforth from PixabayIn this tutorial, we will learn about solving Differential Equations with Julia. Differential Equations are everywhere in Science and Engineering problems. And, being able to solve Differential Equations using computers is very convenient. We will use the DifferentialEquations.jl package for solving Differential Equations.This is for people who are already familiar with Differential Equations from Mathematics, and who can code, preferably in Julia. Only the very basics of Julia langauge is required. If you are unfamiliar with Julia, and yet you have non-trivial experience with any of Python, C/C++, Ruby, JavaScript, golang, etc., you are fine, and you can follow along.In this tutorial, we will only focus on using the high level API of the DifferentialEquations.jl package, and we will learn, from scratch, how to translate first order Differential Equations to Julia"
  },
  {
    "title": "In Severance, Office Perks Couldn't Be More Sinister (theoffcut.substack.com)",
    "points": 35,
    "submitter": "nickcotter",
    "submit_time": "2025-03-04T07:25:02 1741073102",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=43251416",
    "comments": [
      "The irony being that the show is made by Apple... How many people are de facto severed by their 9-to-5 soulless tech job, come home and binge watch a TV show made by another soulless tech corp about being severed by a 9-to-5 soulless tech job and... I don't get it.\n \nreply",
      "It's being financed by Apple. Ben Stiller and Dan Erickson are still the artists creating the show.\n \nreply",
      "It is also approved by Apple. But to the GP's point nearly 5 years later I still dont think they should have done Apple TV+. How many billons invested into Apple TV+ programme that still didn't get any return of investment while they could have done a lot better job elsewhere.\n \nreply",
      "It sorta reminds me of the series \"Electric Dreams\" on Amazon Prime. Select stories from Philip K. Dick! Great stuff!The hubris of Amazon airing the \"Autofac\" episode though, holy crap. I mean if ANYONE on this planet is the likely source of some \"Autofac\"-styled nightmare its Amazon. Makes me throw up a little in my mouth.\n \nreply",
      "Yes, it is all a meaningless cycle - companies worth hundreds of billions of dollars want to keep making more and more engaging content and make you spend as much time on them as possible and keep using their services forever. \nAt the same time, people who have busy careers and soul-sucking tech jobs at companies cannot or do not simply afford to spend so much time on these things.If you're one of those - make your call. I decided to not touch any of these TV shows and fall into the web of content until I retire from the tech industry, which I assume will start in my 50's.\n \nreply",
      "cool story, but severance is a really good show, though.\n \nreply",
      "All artistic mediums are plagued in some way. For film and TV, this is the incredible cost of production, which necessitates the involvement of large corporations.\n \nreply",
      "> How many people are de facto severed by their 9-to-5 soulless tech job, come home and binge watch a TV show...everyone I know who works at Apple is incredibly proud of and satisfied with the work they do. (They're also richly remunerated.)\n \nreply",
      "I'm ambivalent about Severance, though folks here may appreciate that it has many excellent establishing shots of the magnificent Bell Labs Holmdel Complex.https://en.wikipedia.org/wiki/Bell_Labs_Holmdel_Complex\n \nreply",
      "My teenage son and I have been watching, and loving, this show.  And, I wonder if watching it has been innoculating him against a corporate life of quiet cubicle desperation.  If so, then thank you, Apple.\n \nreply"
    ],
    "link": "https://theoffcut.substack.com/p/severance-office-perks-sinister",
    "first_paragraph": ""
  },
  {
    "title": "MacBook Air M4 (apple.com)",
    "points": 464,
    "submitter": "tosh",
    "submit_time": "2025-03-05T14:06:54 1741183614",
    "num_comments": 542,
    "comments_url": "https://news.ycombinator.com/item?id=43266537",
    "comments": [
      "My 4-year old Dell XPS 15 is up for replacement, but somehow no manufacturer aside from Apple is making laptop with decent specs nowadays? I want 2TB storage, a 4k (or close) HiDPI display, good build quality, and not a bulky gaming laptop. The XPS 15 was perfect, it had those specs, except it only had 1TB storage which is now full. I was expecting that to not be an issue 4 years later ... But now Dell discontinued XPS, and their new Pro/Premium models have worse specs in almost all ways. The only non-Apple thing that I can find that even comes close, is a bulky 16\" ThinkPad.And then there is Apple who pack everything I want in a sleek 14\" or 15\" device, plus a very fast CPU and battery life that is years ahead of anything else ... Why is there no competition here? I'm willing to compromise on battery life, and I don't need the fastest CPU, just a good quality work laptop where I can run `cargo build` / `docker pull` without worrying about filling up the disk, and mostly just a browser aside from that. Why is the gap so large?\n \nreply",
      "There's nothing close, Apple has better talent and the vertical integration gives them an edge (especially on performance per watt on their chip designs).Since the M series chips, there's been no other option if you care about quality. There are crappy alternatives with serious tradeoffs if for some reason you are forced to not use Apple or choose for non-quality reasons.\n \nreply",
      "The leap from intel to the M series chips really left everyone else behind. I can't even use my 2019 Macbook anymore it feels so sluggish.I have an M3 Pro and it blows all my old computers out of the water. Can handle pretty insane dev workflows (massive Docker composed environments) without issue and the battery life feels unfair. I can put in an 8 hour workday without my charging cable, I don't think I have turned it fully off in a few months, it just chugs along. It really embodies the \"it just works\" mindset.\n \nreply",
      "I can easily take my M3 MBA on trips, using in on the plane both ways and a couple hours there for a few days, and not charge it once.I honestly looked for alternatives when I bought it last summer but there weren\u2019t any competitive options.\n \nreply",
      "Is the performance gap so huge? Power efficiency yes, absolutely, but for peak performance last I saw the last AMD vs M3 benchmarks were a slightly slower single core, and a little faster in multicore. Doesn't seem as world changing as described.\n \nreply",
      "My $2000 linux desktop is still faster and snappier than the $4000 macbook, but it\u2019s the only thing laptop sized that feels even close.\n \nreply",
      "> My $2000 linux desktop is still faster and snappier than the $4000 macbook, but it\u2019s the only thing laptop sized that feels even close.What brand?\n \nreply",
      "Yes. You need to go to server class chips (eg. threadripper) before beating the raw multi-core performance of a top-spec M4 Max in a Macbook pro, and the battery life is still crazy good!\n \nreply",
      "Yes. No other laptop can sustain peak performance as long as the M-series Macs. The only thing that competes is a dedicated desktop with a big cooler and fan.Mac laptops feel faster, even if the synthetic benchmarks say otherwise.\n \nreply",
      "I don\u2019t agree. Compile times are definitely and very noticeably quicker on my Intel gaming laptop (that\u2019s actually a few years old now) vs my M3 MBP.That said, I\u2019ve never once felt that the M3 MBPs are sluggish. They are definitely super quick machines. And the fact that I can go a full day without charging even when using moderately heavy workloads is truly jaw droppingly impressive.I\u2019d definitely take the power performance over that small little extra saved in compile times any day of the week. So Apple have made some really smart judgements there.\n \nreply"
    ],
    "link": "https://www.apple.com/macbook-air/",
    "first_paragraph": "\n\t\t\t\t\t\t\t\tNow through April 2, get extra trade-in credit toward a new Mac with Apple\u00a0Trade\u00a0In.*\nShop Mac\nSpeed of lightness.Built for Apple\u00a0Intelligence.**\nAvailable starting March 12thAvailable starting 3.12MacBook\u00a0Air is the world\u2019s most popular laptop for a reason. Actually, for a lot of reasons. It delivers up to 18 hours of battery life.1 The M4 chip unlocks a whole new level of performance for work and play. Apple\u00a0Intelligence is built in to help you get things done effortlessly.2 And it now comes in a stunning Sky Blue color. With the perfectly portable MacBook\u00a0Air, you\u2019ll be ready to take on just about anything,\u00a0anywhere.Built to go places.Remarkably light and less than half an inch thin, MacBook\u00a0Air fits easily into your on-the-go lifestyle \u2014 and your bag. MacBook\u00a0Air with M4 is made with over 50\u00a0percent recycled materials and has a durable recycled aluminum\u00a0enclosure.13\u201d display15\u201d displayThe 13\u2011inch MacBook\u00a0Air is the ultimate on-the-go laptop, and the 15\u2011inch model gives you"
  },
  {
    "title": "There Was a Texas Lottery Arbitrage (bloomberg.com)",
    "points": 165,
    "submitter": "ioblomov",
    "submit_time": "2025-03-05T17:49:13 1741196953",
    "num_comments": 196,
    "comments_url": "https://news.ycombinator.com/item?id=43269846",
    "comments": [
      "https://archive.ph/DOCG0",
      "> In April 2023, an entity called Rook TX effectively purchased the jackpot, collecting a one-time payment of $57.8 million, by acquiring virtually all of the 25.8 million possible number combinations. The operation was planned in Malta and funded by a London betting company. It was carried out by four Texas retailers, all connected to online sales companies called couriers.Well, as the saying goes, \"It takes money to make money.\"\n \nreply",
      "A lot of people are focusing on the courier aspect of all this which is fine, but I didn't know there were any lotteries you could just outright win, with a profit, if you had enough money to buy enough tickets. I assumed people would design lotteries where the cost/reward ratio was such that this would never make sense.\n \nreply",
      "There's a couple ways this works. Progressive jackpot games (Powerball, Mega Millions) allocate some amount of every ticket sold for winnings, but if the jackpot isn't won on a particular drawing, excess winnings are added to the prize pool for the next drawing. After a certain point, the jackpot prize for a single winner is more than the cost of buying all possible tickets. There's a chance of sharing a jackpot, which is hard to model, but makes the payout worse.A similar game feature is \"roll down\", again excess prize money accumulates over several drawings, and when a certain criteria is met, the excess prize money is distributed over some set of tickets (possibly all winners). Again, this sets up the possibility of a positive expected value, and you have to consider other ticket buyers as well.A trickier one is for scratch off games. Many lotteries share the number of tickets sold and the prizes left. If you assume all (big?) prizes are redeemed shortly after their ticket is sold, you can estimate the expected value of purchasing the remaining tickets. When the game opens, the expected value of a ticket is less than the purchase price, but depending on the observations of tickets sold and prizes redeemed, you might estimate that the expected value of the remainder of tickets has improved.Ex: if there were 1 million scratchers printed, the cost per scratcher was $1, and there was only one prize $500,000on open the expected value of a $1 ticket would be $0.50. If the winning ticket was redeemed, the expected value of remaining tickets would be $0. If it was reported that 999,999 tickets were sold and the winner had not yet been claimed, it might be reasonable to assume a higher expected value for the last ticket --- although there's no rigorous proof there, someone may have purchased the winning ticket already and not redeemed it for whatever reason.\n \nreply",
      "I'd imagine scratchers would be almost impossible unless you could somehow get all the tickets from every place they are sold. I'm not into gambling, but I guess it might work if there is X number of prizes/money per roll of tickets.\n \nreply",
      "Expected Value is Expected Value, though.Even if you can't buy every ticket, there is well-established math about how to optimize profit from a venture with known risk and reward, and the math does not require you to exhaust the statistical universe.\n \nreply",
      "There is clearly a limit on practicality though. Unless you are suggesting someone should sell their 401k and buy Powerball tickets any time the jackpot exceeds the odds of winning simply because it has positive EV?\n \nreply",
      "The math also tells you what percentage of your wealth to bet: https://en.wikipedia.org/wiki/Kelly_criterion\n \nreply",
      "> I guess it might work if there is X number of prizes/money per roll of tickets.In most cases, there is, which is part of why a huge percentage of scratchoff prizes are won by workers at the place that sells them. Most players will scratch and redeem their prizes right in front of you, so if you watch a certain number of scratches occur in a roll and you know the prize structure of the particular card, you can calculate how many non-winning scratches you need to see for the odds to be in your favor.I looked into this a few years ago and considered starting one of those stands that sells scratchoffs to do just this, but decided a) it wasn't quite lucrative enough to be worth it, and b) I wasn't sure of the ethics of skewing the odds against your customers like this anyway.\n \nreply",
      "> I wasn't sure of the ethics of skewing the odds against your customers like this anyway.This is interesting because I don\u2019t think anyone would view the store as unethical for continuing to sell tickets from a roll when they know there have already been X winners from that role and therefore customer odds have gone down.\n \nreply"
    ],
    "link": "https://www.bloomberg.com/opinion/articles/2025-03-05/there-was-a-texas-lottery-arbitrage",
    "first_paragraph": ""
  },
  {
    "title": "Richard Sutton and Andrew Barto Win 2024 Turing Award (acm.org)",
    "points": 442,
    "submitter": "camlinke",
    "submit_time": "2025-03-05T10:03:31 1741169011",
    "num_comments": 100,
    "comments_url": "https://news.ycombinator.com/item?id=43264847",
    "comments": [
      "Very cool to see this! It turns out my wife and I bought Andy Barto\u2019s (and his wife\u2019s) house.During the process, there was a bidding war. They said \u201cmake your prime offer\u201d so, knowing he was a mathematician, we made an offer that was a prime number :-)So neat to see him be recognized for his work.\n \nreply",
      "Ha haa, that is fantastic. You should have joked and said - \"I'd like to keep things even between us, how about $2?\"\n \nreply",
      "> we made an offer that was a prime number$12345678910987654321?\n \nreply",
      "Nice! Well deserved. They make both editions of their RL textbook available as a free to read PDF. I have been a paid AI practitioner since 1982, and I must admit that RL is one subject I personally struggle mastering, and the Sutton/Barto book, the Cousera series on RL taught by Professors White and White, etc. personally helped me: recommended!EDIT: the example programs for their book are available in Common Lisp and Python. http://incompleteideas.net/book/the-book-2nd.html\n \nreply",
      "Good time to re-read The Bitter Lesson: https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson...\n \nreply",
      "Canonical URL: <http://www.incompleteideas.net/IncIdeas/BitterLesson.html>\n \nreply",
      "Indeed a bitter lesson. I once enjoyed encoding human knowledge into a computer because it gives me understanding of what's going on. Now everything is becoming a big black box that is hard to reason about. /sigh/Also, Moore's law has become a self-fulfilling prophecy. Now more than ever, AI is putting a lot of demand on computational power, to the point which drives chip makers to create specialized hardware for it. It's becoming a flywheel.\n \nreply",
      "I am still hoping AI progress will get to the point where the AI can eventually create AI's that are built up out of robust and provable logic which can be read and audited. Until that time, I wouldn't trust it for risky stuff. Unfortunately, it's not my choice and within a scarily short timespan, black boxes will make painfully wrong decisions about vital things that will ruin lives.\n \nreply",
      "AI assisted theorem provers will go a bit in that direction. You may not know exactly how they managed to construct a proof, but you can examine that proof in detail and verify its correctness.\n \nreply",
      "Yes, I have a small team of (me being 1/3) doing formal verification in my company and we do this and it doesn't actually matter if how the AI got there; we can mathematically say it's correct which is what matters. We do (and did) program synthesis and proofs but this is all very far from doing anything serious at scale.\n \nreply"
    ],
    "link": "https://awards.acm.org/about/2024-turing",
    "first_paragraph": ""
  },
  {
    "title": "Tesla gets more than 20% of parts from Mexico, it will be affected by tariffs (electrek.co)",
    "points": 16,
    "submitter": "zfg",
    "submit_time": "2025-03-06T00:49:59 1741222199",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=43274795",
    "comments": [
      "It will get a waiver.\n \nreply",
      "Mexico should fill in that gap with an export tax then\n \nreply",
      "why would it get a waiver?Mexico is already paying for the wall, so... why should they pay here too?\n \nreply",
      "trump gonna do this oprah style on tv, \u201cyou get a waiver, you get a waiver, EVERYBODY gets a waiver\u201d :)\n \nreply"
    ],
    "link": "https://electrek.co/2025/03/05/tesla-gets-more-than-20-of-its-parts-from-mexico-yes-it-will-be-affected-by-tariffs/",
    "first_paragraph": "Tesla gets more than 20% of its parts from Mexico, as well as some from Canada on top of it. So, yes, Tesla will be negatively affected by the tariffs.However, there\u2019s another one-month delay.I didn\u2019t think I would have to write this article, but I have seen plenty of \u201cTesla influencers\u201d claim that Tesla would not be affected by President Trump\u2019s current trade war:This is false. Tesla gets a significant percentage of its car parts from Mexico and Canada.NHTSA releases data about the sourcing of parts for all vehicles in the US. Unfortunately, it doesn\u2019t account for the US and Canada together, but it also lists the country of origin for the next largest source of parts.For Tesla, that\u2019s Mexico for all car models:This means that Tesla gets more than 20% of its parts from Mexico in addition to what it gets from Canada.It\u2019s also noteworthy that Tesla\u2019s most popular car, Model Y, gets 25% of its parts from Mexico.Despite free trade agreements with Canada and Mexico, Trump has implemented 25"
  },
  {
    "title": "SepLLM: Accelerate LLMs by Compressing One Segment into One Separator (sepllm.github.io)",
    "points": 7,
    "submitter": "limoce",
    "submit_time": "2025-03-03T13:27:26 1741008446",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://sepllm.github.io/",
    "first_paragraph": "\n                            Large Language Models (LLMs) have exhibited exceptional performance across \n                            a spectrum of natural language processing tasks. However, their substantial \n                            sizes pose considerable challenges, particularly in terms of computational \n                            demands and inference speed, due to its quadratic complexity. In this work, \n                            we have identified a noteworthy pattern: certain meaningless special tokens \n                            (i.e., separators) contribute massively to attention scores compared to other \n                            semantically meaningful tokens. This insight has led us to hypothesize that \n                            the information of the segment between these special tokens can be condensed \n                            into these tokens without significant loss of information. Based on this hypothesis, \n                            we introduce Sep"
  },
  {
    "title": "Datafold (YC S20) is hiring Engineer to build AI-powered data migration tools (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-03-05T21:01:01 1741208461",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/datafold/jobs/ieGYiSG-senior-software-engineer-ai-agents",
    "first_paragraph": "Deal with analytical data quality in the pull requestDatafold is a fast-growing, Series A startup at the forefront of data quality and observability\u2014think Datadog for data engineers. Backed by top-tier investors including YC, Amplify, and NEA, we're redefining how companies like Disney, FanDuel, and Perplexity maintain quality across the entire data lifecycle. Although headquartered in the US, we\u2019re a fully remote team with employees across the US and EU.We\u2019re looking for an experienced backend (or full-stack) engineer to help build and scale the Datafold Migration Agent (DMA)\u2014an AI-powered tool that\u2019s changing the game in data migration. Combining large language models with our unique data diffing technology, DMA automates SQL dialect translation and data reconciliation, slashing migration timelines by 5-10x and eliminating the need for manual work and costly consultants.If building a high-impact, innovative product at the intersection of AI and data engineering sounds exciting, we\u2019d "
  },
  {
    "title": "Show HN: Leaflet.pub \u2013 a web app for creating and sharing rich documents",
    "points": 54,
    "submitter": "jpereira",
    "submit_time": "2025-03-05T17:55:29 1741197329",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=43269928",
    "comments": [
      "Just as an FYI: \"leaflet\" is the name of a popular web mapping JS library. https://leafletjs.com/May or may not be relevant for you (and is definitely a very different field/product), but if you're releasing client libraries/SDKs of any sort, it might be good to be aware of. There's a large ecosystem of plugin libraries named \"leaflet-foo\" or \"foo-leaflet\" etc in addition to the \"main\" one. If you start releasing any libraries to work with your app (even if they're not JS), you'll likely want to be aware of and work around naming collisions for library names.Either way, looks nifty!!  I love the approach and we need more people willing to do something like this that competes with google docs / etc, but does so by targeting a specific use case / niche / etc and not by trying to do everything.\n \nreply",
      "Still playing around with it but I have to say, I absolutely adore the side-scrolling feature!\n \nreply",
      "In our last product (hyperlink.academy) we went through a lot of iterations on multipage designs, from carousels, to stacking decks, to a kind of 2-pane view, and eventually settled on the side scrolling view. Really useful for drilling down into things, looking at things side-by-side, etc. Glad to hear you like it!\n \nreply",
      "Reminds me of https://gingkowriter.com/ ... Side scrolling and spatial notetaking was the two reasons to stick with it. Perhaps it's time to switch to Leaflet.Thanks for sharing <3\n \nreply",
      "I see its source available, which implies you\u2019re planning on monetizing somehow, do you mind sharing any insight into what that would look like?I\u2019m always weary of getting involved in a new tool only for the rug pull to happen later.\n \nreply",
      "Very fair question! Honestly the main reason it's source available rn instead of open source is we haven't decided on a license internally, as opposed to business reasons. As for monetization, the two areas we're exploring are publication, subscriptions to blogs, a la substack, and a \"pro\" tier, with better tools for managing lots of documents.\n \nreply",
      "I really love the ui exploration!I wish canvases had edges connecting nodes\n \nreply",
      "The appearance (both the text font and the UI chrome icons and colors) really is not to my liking. I see that you can already change the foreground and background color of your document. Any plans to add more options?\n \nreply",
      "You should be able to change all the colors in the theme options! As for fonts, currently working on adding options there! Probably not going to support custom icons though, but maybe an minimal mode (sans icons, maybe only text) could be interesting?\n \nreply",
      "I can't seem to scroll horizontally very easily, like I have to middle click drag in empty space, is there any reason you are hiding scrollbars?\n \nreply"
    ],
    "link": "item?id=43269928",
    "first_paragraph": ""
  },
  {
    "title": "The \"Take It Down\" Act (eff.org)",
    "points": 32,
    "submitter": "panarky",
    "submit_time": "2025-03-06T00:30:01 1741221001",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=43274656",
    "comments": [
      "Whichever the ruling party is, it seems to never be one that actually supports free speech.\n \nreply",
      "The \"Take Action\" link leads to \"Bad Gateway\"\n \nreply"
    ],
    "link": "https://www.eff.org/deeplinks/2025/03/trump-calls-congress-pass-overbroad-take-it-down-act-so-he-can-use-it-censor",
    "first_paragraph": "We've opposed the Take It Down Act because it could be easily manipulated to take down lawful content that powerful people simply don't like. Last night, President Trump demonstrated he has a similar view on the bill. He wants to sign the bill into law, then use it to remove content about \u2014 him. And he won't be the only powerful person to do so.\u00a0Here\u2019s what Trump said to a joint session of Congress: \u00a0\u00a0\u00a0The Senate just passed the Take It Down Act\u2026. Once it passes the House, I look forward to signing that bill into law. And I\u2019m going to use that bill for myself too if you don\u2019t mind, because nobody gets treated worse than I do online, nobody.\u00a0The Take It Down Act is an overbroad, poorly drafted bill that would create a powerful system to pressure removal of internet posts, with essentially no safeguards. While the bill is meant to address a serious problem\u2014the distribution of non-consensual intimate imagery (NCII)\u2014the notice-and-takedown system it creates is an open invitation for powerf"
  },
  {
    "title": "Things we've learned about building products (posthog.com)",
    "points": 186,
    "submitter": "fmerian",
    "submit_time": "2025-03-05T14:44:04 1741185844",
    "num_comments": 56,
    "comments_url": "https://news.ycombinator.com/item?id=43267095",
    "comments": [
      "Wow. 900 applications down to 10 \"SuperDay\" participants down to 4 hires. All to work at.... posthog. What a depressing statistic.This felt like a humble brag to help make their point about hiring good talent and how many people want to be a hogger (or whatever they call people that work there) but this just really highlights how brutal the job market is. Yes the market is also flooded with unqualified applicants and or bots that will apply to any job listing thats posted, but still this is ridiculous.I really feel bad for the 6 people who had to endure the technical interview AND THEN were given the honor of attending the \"SuperDay\" which sounds like a full day of at least 5 interviews, 2 - 3 being technical, and still got rejected. Not sure what the technical interview is like at posthog, but assuming this is just an hour phone screen those 6 people still probably had more than 7 hours devoted just to interviewing at this place just to get rejected. That's not including any time spent preparing for interviews or anything else either.There must be a better way to do interviews. Posthog is not Google, Posthog (or any other startup) does not need to hire to the same standard that Google does.Let me know when you're on par with Google in terms of revenue or benefits or prestige, or anything else really that Google offers then sure I will jump through as many hoops as you want for the interview. Until then, hard pass.\n \nreply",
      "Having attended a SuperDay, I can hands down state that their interview process is the best I've ever had (didn't get the job tho, which was probably for the best at this phase of life). Designed to perfectly lift signal and minimize noise, for what they're trying to achieve. Don't change a thing PostHog.\n \nreply",
      "I personally think there are more efficient ways to get a high signal to noise ratio on if you are going to be a good hire or not without having the candidate invest almost 9 hours into an interview process, but that\u2019s just me\n \nreply",
      "Superday is a paid day of work with a 30-minute talk with a founder + a 30min review about the day with an engineer\n \nreply",
      "Ah ok my mistake, so that\u2019s 8 hours including the review and discussion portion for the super day, then let\u2019s say 45 minutes for the technical interview so 8 hours and 45 minutes of time spent interviewing at a minimum.\n \nreply",
      ">> Wow. 900 applications down to 10 \"SuperDay\" participants down to 4 hires. All to work at.... posthog. What a depressing statistic.Yeah, at first I thought it was some kind of parody, then I realized it's a serious article and was astonished.\n \nreply",
      "> \u201cIf you aren't excited about what you're working on, pivot. It's as simple as that. You'll achieve more if you're working on something that feels yours.\u201dI doubt the rank and file ICs feel this way at all. It's analytics plumbing, and it's all for the sake of the paycheck.\n \nreply",
      "This list mentions A/B testing a few times and it's worth noting that A/B testing is great but it's not free.I've seen a nontrivial number of smart engineers get bogged down in wanting to A/B test everything that they spend more time building and maintaining the experiment framework than actually shipping more product and then realizing the A/B testing was useless because they only had a few hundred data points. Data-driven decisions are definitely valuable but you also have to recognize when you have no data to drive the decisions in the first place.Overall, I agree with a lot of the list but I've seen that trap one too many times when people take the advice too superficially.\n \nreply",
      "I think A/B testing is one of the most expensive ways of getting feedback on a product feature.- You have to make good decisions about what you're going to test- You have to build the feature twice- You have to establish a statistically robust tracking mechanism. Using a vendor helps here, but you still need to correctly integrate with them.- You have to test both versions of the feature AND the tracking and test selection mechanisms really well, because bugs in any of those invalidate the test- You have to run it in production for several weeks (or you won't get statistically significant results) - and ensure it doesn't overlap with other tests in a way that could bias the results- You'd better be good at statistics. I've seen plenty of A/B test results presented in ways that did not feel statistically sound to me.... and after all of that, my experience is that a LOT of the tests you run don't show a statistically significant result one way or the other - so all of that effort really didn't teach you much that was useful.The problem is that talking people out of running an A/B test is really hard! No-one ever got fired for suggesting an A/B test - it feels like the \"safe\" option.Want to do something much cheaper than that which results in a much higher level of information? Run usability tests. Recruit 3-5 testers and watch them use your new feature over screen sharing and talk through what they're doing. This is an order of magnitude cheaper than A/B testing and will probably teach you a whole lot more.\n \nreply",
      "Some teams think they can A/B test their way to a great product. It can become a socially acceptable mechanism to avoid having opinions and reduce friction.Steve Blank's quote about validating assumptions: \"Lean was designed to inform the founders\u2019 vision while they operated frugally at speed. It was not built as a focus group for consensus for those without deep convictions\"Is the Lean Startup Dead? (2018)\nhttps://medium.com/@sgblank/is-the-lean-startup-dead-71e0517...Discussed on HN at the time: https://news.ycombinator.com/item?id=17917479\n \nreply"
    ],
    "link": "https://newsletter.posthog.com/p/50-things-weve-learned-about-building",
    "first_paragraph": ""
  },
  {
    "title": "Apple takes UK to court over 'backdoor' order (theregister.com)",
    "points": 339,
    "submitter": "latexr",
    "submit_time": "2025-03-05T18:07:06 1741198026",
    "num_comments": 99,
    "comments_url": "https://news.ycombinator.com/item?id=43270079",
    "comments": [
      "I am very glad they are doing this as a UK based ADP user. Waiting to see how long before they forcibly turn it off for existing users. I will of course just remove everything from iCloud at that point.\n \nreply",
      "It's not like you can use an alternative without facing jail time if you don't give up the keys.\n \nreply",
      "The penalty for not giving up keys is max 2 years in prison. Most offences that they're trying to use the encrypted data to use as prosecution evidence (for example, child pornography), have penalties that are way more than 2 years in prison.If you're genuinely innocent, the 2 years is horrid. If you're actually guilty, it's a cheap way to serve your time.It's a weird and perverse law that shouldn't exist, but it's likely in time the government will need to move the needle one way or the other, as habitual criminals are getting used to doing the maths.\n \nreply",
      "This is correct - but I\u2019d rather my law enforcement had a pre-existing reason to investigate me rather than just stumbling upon something in random hidden searches. Innocent until proven guilty is key here.I have nothing to hide, but I\u2019m still not giving you access to my photo library.\n \nreply",
      "Whether the data is encrypted or not, they still need a warrant.\n \nreply",
      "[flagged]",
      "You seem to think that the indexing and searching happens only if there is a reason. Why do you think that? There are all kinds of cases where government agents were found to have abused access to data for reasons that had nothing to do with illegal or immoral behavior by a target.\n \nreply",
      "Irritatingly naive\n \nreply",
      "Because they can.\n \nreply",
      "So they randomly picked a person to search?\n \nreply"
    ],
    "link": "https://www.theregister.com/2025/03/05/apple_reportedly_ipt_complaint/",
    "first_paragraph": ""
  },
  {
    "title": "Math Academy pulled me out of the Valley of Despair (mikelikejordan.bearblog.dev)",
    "points": 107,
    "submitter": "gmays",
    "submit_time": "2025-03-03T13:27:07 1741008427",
    "num_comments": 91,
    "comments_url": "https://news.ycombinator.com/item?id=43241499",
    "comments": [
      "For child, being precocious in a subject is usually a curse.  Being bright and a generally fast learner is also a trap.  Hitting the wall is inevitable for almost everyone, but until that point your self-image is built on forward velocity, and especially relative velocity \u2014 you\u2019re just faster than your peers.  Turns out there are faster kids, they just aren\u2019t at your school.Parents can make this worse but it\u2019s pretty hard to prevent it.\n \nreply",
      "> Turns out there are faster kids, they just aren\u2019t at your schoolMoving from Slovenia to SFBA in my mid 20's (~2015) was ... super fun like that. Sooo many people here are that most brilliant super talented engineer/founder/whatever from their home locale. But here we are just the norm.\n \nreply",
      "We've got an idiom for that: being a big fish in a small pond.Then you move or have some experience that opens your eyes and you see that there are so many people out there that you're not actually as special / smart / talented / athletic etc as you thought.I've had the experience a few times myself, and it's always a bit of an existential wakeup call.\n \nreply",
      "Yeah, i was the best physicist the tiny impoverished state school had seen in years. I'm_significantly_ behind everyone else in my PhD program. But then, i tell myself if i can't be the best prepared, i can be the hardest worker. But realistically... nah. I'd rather stay _inside_ the 5th story window of my office. It's not a race, unless you're losing\n \nreply",
      "Erdos could learn as much in five minutes as the average person can in five years, but there are more things to study than he had five-minute intervals.\n \nreply",
      "What\n \nreply",
      "Being in the 99.9th percentile for intelligence just means that there are 8 million people in the world who are smarter than you. And a few more every day.\n \nreply",
      "Yep, we all have to hit the wall and that\u2019s where we find out what we\u2019re made of. It can be a valuable experience with the right people around to help.\n \nreply",
      "Can you give more detail on what you mean by it can be a valuable experience with the right people around to help.My son (7 years old) is gifted in Math and as a parent I find it extremely hard to decide how much I should push him (register him to math competition, weekend math club ...) and how much I should just let him get 100% on exam and not accelerate the learning.\n \nreply",
      "In my experience as a parent, you can provide the resource but don\u2019t need to push. Love of math will happen if it has the right environment. For a 7yo I might suggest looking onto Epsilon camp, and Art of Problem Solving (which is on line).My own kid went to MathPath (middle school camp by same people as Epsilon Camp). Loved it. \u201cYes, dad really, I want to spent a whole month of my summer doing math.\u201d  The social experience is great for kids to be with other kids that like math.\n \nreply"
    ],
    "link": "https://mikelikejordan.bearblog.dev/how-math-academy-pulled-me-out-of-the-valley-of-despair/",
    "first_paragraph": ""
  }
]