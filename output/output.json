[
  {
    "title": "New LLM optimization technique slashes memory costs up to 75% (venturebeat.com)",
    "points": 28,
    "submitter": "hochmartinez",
    "submit_time": "2024-12-13T19:14:20 1734117260",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=42411409",
    "comments": [
      "Wonder how this compares with Microsoft's HeadKV paper [1] which claims a 98% percent reduction in memory while retaining 97% of the performance.[1] https://arxiv.org/html/2410.19258v3\n \nreply",
      "This only decreases memory cost of input context window, not the memory cost to load and run the models.\n \nreply",
      "Context window requires ram too.\n \nreply",
      "TFP: https://arxiv.org/abs/2410.13166\n \nreply",
      "TFS: https://github.com/SakanaAI/evo-memory\n \nreply",
      "LLM hype is fizzling\n \nreply",
      "This is for inference right? Not training?\n \nreply"
    ],
    "link": "https://venturebeat.com/ai/new-llm-optimization-technique-slashes-memory-costs-up-to-75/",
    "first_paragraph": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn MoreResearchers at the Tokyo-based startup Sakana AI have developed a new technique that enables language models to use memory more efficiently, helping enterprises cut the costs of building applications on top of large language models (LLMs) and other Transformer-based models.The technique, called \u201cuniversal transformer memory,\u201d uses special neural networks to optimize LLMs to keep bits of information that matter and discard redundant details from their context.\u00a0The responses of Transformer models, the backbone of LLMs, depend on the content of their \u201ccontext window\u201d \u2014 that is, what they receive as input from users.The context window can be considered the model\u2019s working memory. Tweaking the content of the context window can have a tremendous impact on the model\u2019s performance, which has given rise to an entire field of \u201cprompt engineering.\u201dCurrent models support ve"
  },
  {
    "title": "Xiaomi Home Integration for Home Assistant (github.com/xiaomi)",
    "points": 406,
    "submitter": "coherence73",
    "submit_time": "2024-12-16T15:52:47 1734364367",
    "num_comments": 207,
    "comments_url": "https://news.ycombinator.com/item?id=42432151",
    "comments": [
      "If you are thinking of deploying Home Assistant (HA), let me give you a few tips that I should have known when started.  HA environment is vast.  There are myriads of options, features, and functions.  There are some gotchas that can be costly down the road.All of the following are \"for now, as you start out\":Do not gyrate over which version of HA to run.  Run the HAOS loaded on RasPi, or a dedicated machine.  You can migrate to other solution if that does not end up to be the best choice.Make sure you get the latest Zigbee radio dongle the HA Forum recommends.  I use in all my HA installs a combination dongle for Zigbee & Z-Wave and it works flawlessly (NORTEK Quickstick Combo).Start with a few cheap Zigbee devices.  Stay away from WiFi, LoRaWAN and other solutions.  Z-Wave devices are great, but more expensive.  You know you do not know if you even want to do this.Initially, just use the built-in Zigbee (ZHA) integrations.  Once you are comfortable deploying other ways, like MQTT can be established easily.Do not spend too much on the devices now.  I suggest you pick up a PIR motion sensors, door & window opening sensors, temperature & humidity sensors, smart plugs, and light bulbs.  Anything else is overkill to try out.  Sub-tip: all of these are battery operated, so try to consolidate on a standard battery.  The ones I listed can all run off of 2032 cell batteries.  Ikea sells most of these, Aqara is well known, and so on.  If it is Zigbee (not Matter) you will have 96% chance it will work with HA.Have fun!\n \nreply",
      "What I wish I had known: HA is just a python program, it's pretty easy to run it manually.After my Pi's sdcard died, I wanted to use an old laptop (more reliable with a built-in backup battery) so I followed the deployment recommendations and used the VM image. After that, for the first time I started having problems with HA not running - because VirtualBox would run only about a month before crashing. And I didn't like how much memory a VM locked up on the host.The documentation makes it sound super complicated, but if you can make a venv and `pip install`, the setup is that easy. I tend to just run `hass` manually but there instructions for setting up supervised. I wish the documentation made this clearer, it really tries to scare people away from that method but running a whole VM is a lot of overhead for my pretty simple zwave-js setup.\n \nreply",
      "Just use Docker?\n \nreply",
      "There are a few things you can't do with docker (my case was an addon that I wanted). The install docs have a good grid of the differences.\n \nreply",
      "Can't do with docker or can't do as easily with docker as you can with HAOS? My understanding has been that everything can be done by just adding new containers or files, and it's worked for me thus far.\n \nreply",
      "I also don\u2019t think Docker can do ARP network interrogation, though I could be wrong about that. Also not sure how it handles mDNS.\n \nreply",
      "Overlooked option for running these things in containers is macvlan networking. Just give it its own MAC address on the network. Works great and you don't have to compromise on isolation.\n \nreply",
      "The benefit of docker for home assistant is the packaging of it, rather than isolation. You can always run a container with host network mode and privileged mode so that it can access everything it needs to the same as if it were running directly on the host.\n \nreply",
      "As an alternative to host network mode, you can give the container a dedicated IP on your network using the macvlan driver.\n \nreply",
      "You can run in host network mode to avoid that.\n \nreply"
    ],
    "link": "https://github.com/XiaoMi/ha_xiaomi_home",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Xiaomi Home Integration for Home Assistant\n      English | \u7b80\u4f53\u4e2d\u6587Xiaomi Home Integration is an integrated component of Home Assistant supported by Xiaomi official. It allows you to use Xiaomi IoT smart devices in Home Assistant.Home Assistant version requirement:We recommend this installation method, for it is convenient to switch to a tag when updating xiaomi_home to a certain version.For example, update to version v1.0.0HACS > Overflow Menu > Custom repositories > Repository: https://github.com/XiaoMi/ha_xiaomi_home.git & Category: Integration > ADDXiaomi Home has not been added to the HACS store as a default yet. It's coming soon.Download and copy custom_components/xiaomi_home folder to config/custom_components folder in your Home Assistant.Settings > Devices & services > ADD INTEGRATION > Search Xiaomi Home > NEXT > Click here to "
  },
  {
    "title": "Go Protobuf: The New Opaque API (go.dev)",
    "points": 166,
    "submitter": "secure",
    "submit_time": "2024-12-16T20:18:01 1734380281",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=42434947",
    "comments": [
      "To be honest I kind of find myself drifting away from gRPC/protobuf in my recent projects. I love the idea of an IDL for describing APIs and a great compiler/codegen (protoc) but there's just soo many idiosyncrasies baked into gRPC at this point that it often doesn't feel worth it IMO.Been increasingly using LSP style JSON-RPC 2.0, sure it's got it's quirks and is far from the most wire/marshaling efficient approach but JSON codecs are ubiquitous and JSON-RPC is trivial to implement. In-fact I recently even wrote a stack allocated, server implementation for microcontrollers in Rust https://github.com/OpenPSG/embedded-jsonrpc.Varlink (https://varlink.org/) is another interesting approach, there's reasons why they didn't implement the full JSON-RPC spec but their IDL is pretty interesting.\n \nreply",
      "My favorite serde format is Msgpack since it can be dropped in for an almost one-to-one replacement of JSON. There's also CBOR which is based on MsgPack but has diverged a bit and added and a data definition language too (CDDL).Take JSON-RPC and replace JSON with MsgPack for better handling of integer and float types. MsgPack/CBOR are easy to parse in place directly into stack objects too. It's super fast even on embedded. I've been shipping it for years in embedded projects using a Nim implementation for ESP32s (1) and later made a non-allocating version (2). It's also generally easy to convert MsgPack/CBOR to JSON for debugging, etc.There's also an IoT focused RPC based on CBOR that's an IETF standard and a time series format (3). The RPC is used a fair bit in some projects.1: https://github.com/elcritch/nesper/blob/devel/src/nesper/ser...\n2: https://github.com/EmbeddedNim/fastrpc\n3: https://hal.science/hal-03800577v1/file/Towards_a_Standard_T...\n \nreply",
      "Same; at my previous job for the serialisation format for our embedded devices over 2G/4G/LoRaWAN/satellite I ended up landing on MessagePack, but that was partially because the \"schema\"/typed deserialisation was all in the same language for both the firmware and the server (Nim, in this case) and directly shared source-to-source. That won't work for a lot of cases of course, but it was quite nice for ours!\n \nreply",
      "> I love the idea of an IDL for describing APIs and a great compiler/codegen (protoc)Me too. My context is that I end up using RPC-ish patterns when doing slightly out-of-the-ordinary web stuff, like websockets, iframe communications, and web workers.In each of those situations you start with a bidirectional communication channel, but you have to build your own request-response layer if you need that. JSON-RPC is a good place to start, because the spec is basically just \"agree to use `id` to match up requests and responses\" and very little else of note.I've been looking around for a \"minimum viable IDL\" to add to that, and I think my conclusion so far is \"just write out a TypeScript file\". This works when all my software is web/TypeScript anyway.\n \nreply",
      "Apart from being text format, I'm not sure how well JSON-RPC handles doubles vs long integers and other types, where protobuf can be directed to handle them appropriately. That is a problem in JSON itself, so you may neeed to encode some numbers using... \"string\"\n \nreply",
      "I'd say the success of REST kind of proves that's something that for the most part can be worked around. Often comes down to the JSON codec itself, many codecs will allow unmarshalling/marshalling fields straight into long int types.Also JS now has BigInt types and the JSON decoder can be told to use them. So I'd argue it's kind of a moot point at this stage.\n \nreply",
      "It's not, because some middleman (library, framework, etc.) would assume that JSON is really about sending integers as doubles, hence you are getting only 53 or was it 54 bits precision, and then you end up sending an integer as \"string\" - but then what is this really?I get it, it's probably not a concern for a lot of applications, but when comes to science, games, data it's of big concern... and this excluding the fact that you have to convert back and forth that number a... number of times, and send it on the wire inefficiently - and also miss a way to send it more efficiently using gorilla encoding or something else like that.JSON is great for a lot of things, but not for high throughput RPC.\n \nreply",
      "Sure, but you can work around gRPC's issues too\u2014\"workable\" might be the only bar that matters in practice, but it's a remarkably low bar.The risk with JSON is that too many systems understand it, and intermediate steps can mess up things like numeric precision as well as being inconsistent about handling things out of spec (field order, duplicate fields... etc). This definitely bites people in practice\u2014I saw an experience report on that recently, but can't find the link just now :/\n \nreply",
      "> Also JS now has BigInt types and the JSON decoder can be told to use them.The parser needs to know when to parse as BigInt vs String.\n \nreply",
      "That's sort of where I've landed too.  Protobufs would seem to fit the problem area well, but in practice the space between \"big-system non-performance-sensitive data transfer metaformat\"[1] and \"super-performance-sensitive custom binary parser\"[2] is... actually really small.There are just very few spots that actually \"need\" protobuf at a level of urgency that would justify walking away from self-describing text formats (which is a big, big disadvantage for binary formats!).[1] Something very well served by JSON[2] Network routing, stateful packet inspection, on-the-fly transcoding.  Stuff that you'd never think to use a \"standard format\" for.\n \nreply"
    ],
    "link": "https://go.dev/blog/protobuf-opaque",
    "first_paragraph": "Common problems companies solve with GoStories about how and why companies use GoHow Go can help keep you secure by defaultTips for writing clear, performant, and idiomatic Go codeA complete introduction to building software with GoReference documentation for Go's standard libraryLearn what's new in each Go releaseVideos from prior eventsMeet other local Go developersLearn and network with Go developers from around the worldThe Go project's official blog.Get help and stay informed from Go\n      Michael Stapelberg\n      16 December 2024\n      [Protocol Buffers (Protobuf)\nis Google\u2019s language-neutral data interchange format. See\nprotobuf.dev.]Back in March 2020, we released the google.golang.org/protobuf module, a\nmajor overhaul of the Go Protobuf API. This\npackage introduced first-class support for\nreflection,\na dynamicpb\nimplementation and the\nprotocmp\npackage for easier testing.That release introduced a new protobuf module with a new API. Today, we are\nreleasing an additional API for "
  },
  {
    "title": "Lfgss shutting down 16th March 2025 (day before Online Safety Act is enforced) (lfgss.com)",
    "points": 382,
    "submitter": "buro9",
    "submit_time": "2024-12-16T17:18:08 1734369488",
    "num_comments": 239,
    "comments_url": "https://news.ycombinator.com/item?id=42433044",
    "comments": [
      "Is there some generalized law (yet) about unintended consequences?  For example:Increase fuel economy -> Introduce fuel economy standards -> Economic cars practically phased out in favour of guzzling \"trucks\" that are exempt from fuel economy standards -> Worse fuel economy.orProtect the children -> Criminalize activites that might in any way cause an increase in risk to children -> Best to just keep them indoors playing with electronic gadgets -> Increased rates of obesity/depression etc -> Children worse off.As the article itself says:  Hold big tech accountable -> Introduce rules so hard to comply with that only big tech will be able to comply -> Big tech goes on, but indie tech forced offline.\n \nreply",
      "> Introduce rules so hard to comply with that only big tech will be able to complyWhen intentional, this is Regulatory Capture. Per https://www.investopedia.com/terms/r/regulatory-capture.asp :> Regulation inherently tends to raise the cost of entry into a regulated market because new entrants have to bear not just the costs of entering the market but also of complying with the regulations. Oftentimes regulations explicitly impose barriers to entry, such as licenses, permits, and certificates of need, without which one may not legally operate in a market or industry. Incumbent firms may even receive legacy consideration by regulators, meaning that only new entrants are subject to certain regulations.A system with no regulation can be equally bad for consumers, though; there's a fine line between too little and too much regulation. The devil, as always, is in the details.\n \nreply",
      "Maybe one way to do it is to exempt smaller operations from regulation. eg less than say 20,000 users, no regulations.\n \nreply",
      "It's called \"Perverse incentive\" and Wikipedia runs an illustrative set of examples:https://en.wikipedia.org/wiki/Perverse_incentive\n \nreply",
      "There's the Cobra Effect popularized by FreakonomicsToo many cobras > bounty for slain cobras > people start breeding them for the bounty > law is revoked > people release their cobras > even more cobras around\n \nreply",
      "The Cobra Effect is an example of a Perverse Incentive, which is where an attempt to incentivize a behavior ends up incentivizing the opposite: https://en.wikipedia.org/wiki/Perverse_incentiveI think most of the examples fit this, but a few don't.\n \nreply",
      "This also sounds similar to Goodhart's Law which states that \u201cwhen a measure becomes a target, it ceases to be a good measure.\u201dhttps://en.m.wikipedia.org/wiki/Goodhart%27s_law\n \nreply",
      "Politicians should take a mandatory one-week training in:- very basic macro economics- very basic game theory- very basic statisticsCome to think of it, kids should learn this in high school\n \nreply",
      "I think you\u2019re being overly charitable in thinking this happens because they don\u2019t understand these things. The main thing is that they don\u2019t care. The purpose of passing legislation to protect the children isn\u2019t to protect the children, it\u2019s to get reelected.If we can get the voters to understand the things you mention, then maybe we\u2019d have a chance.\n \nreply",
      "It\u2019s more than just politicians not caring: Big Tech firms hite people on millions of dollars per year to lobby and co-operate with governments, in order to ensure that processes like this result in favourable outcomes to them. See e.g. Nick Clegg.\n \nreply"
    ],
    "link": "https://www.lfgss.com/conversations/401475/",
    "first_paragraph": ""
  },
  {
    "title": "Veo 2: Our video generation model (deepmind.google)",
    "points": 321,
    "submitter": "mvoodarla",
    "submit_time": "2024-12-16T17:04:14 1734368654",
    "num_comments": 153,
    "comments_url": "https://news.ycombinator.com/item?id=42432914",
    "comments": [
      "I got access to the preview, here's what it gave me for \"A pelican riding a bicycle along a coastal path overlooking a harbor\" - this video has all four versions shown:https://static.simonwillison.net/static/2024/pelicans-on-bic...Of the four two were a pelican riding a bicycle. One was a pelican just running along the road, one was a pelican perched on a stationary bicycle, and one had the pelican wearing a weird sort of pelican bicycle helmet.All four were better than what I got from Sora: https://simonwillison.net/2024/Dec/9/sora/\n \nreply",
      "As long as at least one option is exactly what you asked for throwing variations at you that don't conform to 100% of your prompt seems like it could be useful if it gives the model leeway to improve the output in other aspects.\n \nreply",
      "His little bike helmet is adorable\n \nreply",
      "The AI safety team was really proud of that one.\n \nreply",
      "It looks much better than Sora but still kind of in uncanny valley\n \nreply",
      "Superficially impressive but what is the actual use case of the present state of the art? It makes 10-second demos, fine. But can a producer get a second shot of the same scene and the same characters, with visual continuity? Or a third, etc? In other words, can it be used to create a coherent movie --even a 60-second commercial -- with multiple shots having continuity of faces, backgrounds, and lighting?This quote suggests not: \"maintaining complete consistency throughout complex scenes or those with complex motion, remains a challenge.\"\n \nreply",
      "Winning 2:1 in user preference versus sora turbo is impressive. It seems to have very similar limitations to sora. For example- the leg swapping in the ice skating video and the bee keeper picking up the jar is at a very unnatural acceleration (like it pops up). Though by my eye maybe slightly better emulating natural movement and physics in comparison to sora. The blog post has slightly more info:>at resolutions up to 4K, and extended to minutes in length.https://blog.google/technology/google-labs/video-image-gener...\n \nreply",
      "It looks Sora is actually the worst performer in the benchmarks, with Kling being the best and others not far behind.Anyways, I strongly suspect that the funny meme content that seems to be the practical uses case of these video generators won't be possible on either Veo or Sora, because of copyright, PC, containing famous people, or other 'safety' related reasons.\n \nreply",
      "I\u2019ve been using Kling a lot recently and been really impressed, especially by 1.5.I was so excited to see Sora out - only to see it has most of the same problems. And Kling seems to do better in a lot of benchmarks.I can\u2019t quite make sense of it - what OpenAI were showing when they first launched Sora was so amazing. Was it cherry picked? Or was it using loads more compute than what they\u2019ve release?\n \nreply",
      "The SORA model available to the public is a smaller, distilled model called SORA Turbo. What was originally shown was a more capable model that was probably too slow to meet their UX requirements for the sora.com user interface.\n \nreply"
    ],
    "link": "https://deepmind.google/technologies/veo/veo-2/",
    "first_paragraph": "Latest postsLatest research postsLatest technology postsLatest postsOur state-of-the-art video generation modelVeo creates videos with realistic motion and high quality output, up to 4K. Explore different styles and find your own with extensive camera controls.Veo 2 is able to faithfully follow simple and complex instructions, and convincingly simulates real-world physics as well as a wide range of visual styles.Enhanced realism and fidelitySignificantly improves over other AI video models in terms of detail, realism,\u00a0and artifact reduction.Advanced motion capabilitiesVeo represents motion to a high degree of accuracy, thanks to its understanding of physics and its ability to follow detailed instructions.Greater camera control optionsInterprets instructions precisely to create a wide range of shot styles, angles, movements \u2013 and combinations of all of these.Prompt: An extreme close-up shot focuses on the face of a female DJ, her beautiful, voluminous black curly hair framing her featur"
  },
  {
    "title": "Making a watch from scratch (reddit.com)",
    "points": 115,
    "submitter": "dcminter",
    "submit_time": "2024-12-14T14:30:06 1734186606",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=42417357",
    "comments": [
      "In case you haven't seen it before, my absolutely favorite watch resource:https://ciechanow.ski/mechanical-watch/\n \nreply",
      "I love this article so much. Demystified so much about how watches work.\n \nreply",
      "This has got to be one of the best explanations of anything I've ever seen in any subject.Thank you for sharing - hope you have a great day.\n \nreply",
      "I can fully appreciate the amount of skill involved here. I'm currently on a journey to make a clock from scratch using traditional clockmaker's methods.  I started from pretty much zero knowledge of even basic machining, let alone specific clock and watch work. I'm a couple years in now (working in my spare time between work and family obligations), and I can repair most types of issues with clocks and many watches. But I'm not quite tooled up and practiced in making the gears yet.  That's the most major stumbling block currently to my ability to make a traditional clock completely from scratch.\n \nreply",
      "One of my very vague goals is to build a clock, from scratch, without looking anything up.It wouldn't have to look like a wall clock; just something that keeps 24-hour time within reasonable tolerances.(It's been impossible to avoid knowledge of all clockmaking stuff, but I still avoid watching videos on how clocks are made. This is probably going to be something I do once the child is in college!)\n \nreply",
      "This person's build is amazing! Mostly photos, but still super cool to see how \"from scratch\" it is - if you're expecting \"bought a few components from Swiss companies\" then, yeah, it's not that!I submitted this a week ago and sadly it sank without trace (poor choice of posting time I suspect), so I'm trying again. I have no affiliation with the builder.\n \nreply",
      "> bought a few components from Swiss companiesI know what you're referring to and it is exactly what I thought of immediately.Thanks for pointing out that this is not that!\n \nreply",
      "Thank you for this, it's amazing!\n \nreply",
      "Almost everyone that's not already a large company is buying components.The only truly independent watchmaker I've ever heard of machining absolutely every component is Masahiro Kikuno.\n \nreply",
      "Even the main spring? For some of the components it seems like a significant sacrifice in quality to make it yourself. I appreciate people can make their own, but I also think the obsession some folks have with in house movements is misplaced.\n \nreply"
    ],
    "link": "https://old.reddit.com/r/watchmaking/comments/1gvdmyo/i_made_a_watch_from_scratch_link_to_the_build/",
    "first_paragraph": ""
  },
  {
    "title": "In Search of a Faster SQLite (avi.im)",
    "points": 200,
    "submitter": "avinassh",
    "submit_time": "2024-12-16T16:46:49 1734367609",
    "num_comments": 77,
    "comments_url": "https://news.ycombinator.com/item?id=42432730",
    "comments": [
      "The article discusses the specific use case of serverless computing, e.g. AWS Lambda, and how a central database doesn't always work well with apps constructed in a serverless fashion.I was immediately interested in this post because 6-7 years ago I worked on this very problem- I needed to ingest a set of complex hierarchical files that could change at any time, and I needed to \"query\" them to extract particular information.  FaaS is expensive for computationally expensive tasks, and it also didn't make sense to load big XML files and parse them every time I needed to do a lookup in any instance of my Lambda function.My solution was to have a central function on a timer that read and parsed the files every couple of minutes, loaded the data into a SQLite database, indexed it, and put the file in S3.Now my functions just downloaded the file from S3, if it was newer than the local copy or on a cold start, and did the lookup.  Blindingly fast and no duplication of effort.One of the things that is not immediately obvious from Lambda is that it has a local /tmp directory that you can read from and write to.  Also the Python runtime includes SQLite; no need to upload code besides your function.I'm excited that work is going on that might make such solutions even faster; I think it's a very useful pattern for distributed computing.\n \nreply",
      "We have that issue at work, though I solved it by including the sqlite database within the container image that we use. We then deploy the new container image (with the same code as before, but with a revised database file) at most every fifteen minutes.This gives you an atomic point at which you are 100% confident all instances are using the same database, and by provisioning concurrency, you can also avoid a \"thundering herd\" of instances all fetching from the file on S3 at startup (which can otherwise lead to throttling).Of course, that's only feasible if it's acceptable that your data can be stale for some number of minutes, but if you're caching the way you are, and periodically checking S3 for an updated database, it probably is.\n \nreply",
      ">  \"thundering herd\" of instances all fetching from the file on S3 at startup (which can otherwise lead to throttling).Have any \"thundering herd\" problems with S3, including throttling, actually been seen?I think S3 is advertised to have no concurrent connection limit, and support up to at least 5,500 GETs per second (per \"prefix\", which I'm confused about what that means exactly in practice). I don't think S3 ever applies intentional throttling, although of course if you exceed it's capacity to deliver data you will see \"natural\" throttling.Do you have a fleet big enough that you might be exceeding those limits, or have people experienced problems even well under these limits, or is it just precautionary?\n \nreply",
      "Sorry--the throttling was at the AWS Lambda layer, not S3. We were being throttled because we'd deploy a new container image and suddenly thousands of new containers are all simultaneously trying to pull the database file from S3.We aim to return a response in the single digit milliseconds and sometimes get tens of thousands of requests per second, so even if it only takes a second or two to fetch that file from S3, the request isn't getting served while it's happening, and new requests are coming in.You very quickly hit your Lambda concurrency limit and get throttled just waiting for your instances to fetch the file, even though logically you're doing exactly what you planned to.By having the file exist already in the container image, you lean on AWS's existing tools for a phased rollout to replace portions of your deployment at a time, and every one is responding in single digit milliseconds from its very first request.EDIT: The same technique could be applied for other container management systems, but for stuff like Kubernetes or ECS, it might be simpler to use OP's method with a readiness check that only returns true if you fetched the file successfully. And maybe some other logic to do something if your file gets too stale, or you're failing to fetch updates for some reason.\n \nreply",
      "I actually versioned my database file - I had a small metadata table with version number and creation time.Then in the output from each of my other functions, I included the database version number.  So all my output could be subsequently normalized by re-running the same input versus an arbitrary version of the database file.\n \nreply",
      "Have you looked at the user_version pragma? I've been able to avoid use of special metadata tables with this.\n \nreply",
      "> One of the things that is not immediately obvious from Lambda is that it has a local /tmp directory that you can read from and write to.The other big thing a lot of people don't know about Python on Lambda is that your global scope is also persisted for that execution context's lifetime like /tmp is. I ran into issues at one point with Lambdas that processed a high volume of data getting intermittent errors connecting to S3. An AWS engineer told me to cache my boto3 stuff (session, client, resources, etc.) in the global namespace, and that solved the problem overnight.\n \nreply",
      "> Now my functions just downloaded the file from S3, if it was newer than the local copyif you have strong consistency requirements, this doesn't work. synchronizing clocks reliably between different servers is surprisingly hard. you might end up working with stale data. might work for use cases that can accept eventual consistency.\n \nreply",
      "If you have strong consistency requirements, then it doesn't work by the very nature of making multiple copies of the database.  Even if the clocks are perfect.  (Though the clocks are probably close enough that it doesn't matter.)\n \nreply",
      "One of the announcements from AWS this year at Re:invent is that they now can guarantee that the instances clocks are synced within microseconds of each other.  Close enough that you can rely on it for distributed timekeeping.\n \nreply"
    ],
    "link": "https://avi.im/blag/2024/faster-sqlite/",
    "first_paragraph": ""
  },
  {
    "title": "Forgotten Collection of Charles de Gaulle's Manuscripts Discovered in a Safe (smithsonianmag.com)",
    "points": 29,
    "submitter": "Thevet",
    "submit_time": "2024-12-16T05:12:38 1734325958",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.smithsonianmag.com/smart-news/a-forgotten-collection-of-charles-de-gaulles-personal-letters-speeches-and-manuscripts-has-been-discovered-in-a-safe-180985666/",
    "first_paragraph": ""
  },
  {
    "title": "Why is it so hard to buy things that work well? (2022) (danluu.com)",
    "points": 480,
    "submitter": "janandonly",
    "submit_time": "2024-12-16T12:38:11 1734352691",
    "num_comments": 413,
    "comments_url": "https://news.ycombinator.com/item?id=42430450",
    "comments": [
      "I have some insight into this because this claim is about my company Fivetran:\u201c\u2026relies on the data source being able to seek backwards on its changelog. But Postgres throws changelogs away once they're consumed, so the Postgres data source can't support this operation\u201dDan\u2019s understanding is incorrect, Postgres logical replication allows each consumer to maintain a bookmark in the WAL, and it will retain the WAL until you acknowledge receipt of a portion and advance the bookmark. Evidently, he tried our product briefly, had an issue or thought he had an issue, investigated the issue briefly and came to the conclusion that he understood the technology better than people who have spent years working on it.Don\u2019t get me wrong, it is absolutely possible for the experts to be wrong and one smart guy to be right. But at least part of what\u2019s going on in this post is an arrogant guy who thinks he knows better than everyone, coming to snap conclusions that other people\u2019s work is broken.\n \nreply",
      "> When their product attempts to do this and the operation fails, we end up with the sync getting \"stuck\", needing manual intervention from the vendor's operator and/or data loss. Since our data is still on Postgres, it's possible to recover from this by doing a full resync, but the data sync product tops out at 5MB/s for reasons that appear to be unknown to them, so a full resync can take days even on databases that aren't all that large. Resyncs will also silently drop and corrupt dataI don't know, but it sounds like you skipped over most of the reasons why the author was annoyed by Fivetran. You advertise \"Connect data sources to PostgreSQL in minutes using Fivetran\" but if Dan Luu -- who is certainly an intelligent and capable engineer -- and his coworkers can't figure out how to use your product correctly, and if your customer support also can't figure out why the sync breaks, then maybe this isn't mere customer 'arrogance'.\n \nreply",
      "> if Dan Luu -- who is certainly an intelligent and capable engineer -- and his coworkers can't figure out how to use your product correctly, and if your customer support also can't figure out why the sync breaks, then maybe this isn't mere customer 'arrogance'.Dan Luu claims, among other things, to experience hundreds of software bugs per week. If you believe the things he writes then he's not at all representative of a normal customer.\n \nreply",
      "Fivetran works perfectly fine for syncing Postgres databases into Snowflake. My company syncs dozens of them without problems. I can only assume their Postgres database has a non standard set up.\n \nreply",
      "Considering the comment chain involved here chiming in with \"I can only assume non standard setup\" is pretty hilarious.\n \nreply",
      "I have no idea who is right or wrong as to the capabilities. But I believe his story that he couldn't get it working. And I believe your statement that it can be made to work.When very smart people can't get your product to work as advertised, that's a problem with either the advertising, or the documentation, or maybe the default settings. Or maybe it needs the source data set up in a very specific way.That kind of plays into the larger point of the essay that outsourcing this sort of thing still requires significant internal knowledge, and therefore may not be as cheap as it looks at first glance.\n \nreply",
      "In general, I absolutely agree with you. It\u2019s basically an instance of \u201cthe customer is always right\u201d: if a smart customer can\u2019t get our product working, there is a problem with the product. But this post made a much bolder (and wrong) claim: \u201cthe product has a number of major design flaws that mean that it literally cannot work\u201d.\n \nreply",
      "You went too far in your pushback though.\"part of what\u2019s going on in this post is an arrogant guy who thinks he knows better than everyone, coming to snap conclusions that other people\u2019s work is broken\"He's probably wrong about why it was broken, but it was broken.And it's not exactly \"arrogance\" to give the best explanation you have, in a blog post about something else, while not mentioning the company name.\n \nreply",
      "Weird approach chastising your customers lack of expertise in something they\u2019re actively trying to pay you to solve for them. He shouldn\u2019t have to be an expert in it.I was a longtime customer of fivetran who hit these sync issues constantly. Forced resyncs every other month. Was so thankful when our contract ended.\n \nreply",
      "I assume Dan Luu was using the old \u201cXMIN\u201d method and not Logical Replication.https://fivetran.com/docs/connectors/databases/postgresql/tr...\n \nreply"
    ],
    "link": "https://danluu.com/nothing-works/",
    "first_paragraph": ""
  },
  {
    "title": "What did Ada Lovelace's program actually do? (2018) (twobithistory.org)",
    "points": 203,
    "submitter": "mitchbob",
    "submit_time": "2024-12-16T16:58:57 1734368337",
    "num_comments": 63,
    "comments_url": "https://news.ycombinator.com/item?id=42432867",
    "comments": [
      "> In her \u201cdiagram of development,\u201d Lovelace gives the fourth operation as v5 / v4. But the correct ordering here is v4 / v5. This may well have been a typesetting error and not an error in the program that Lovelace devised. All the same, this must be the oldest bug in computing. I marveled that, for ten minutes or so, unknowingly, I had wrestled with this first ever bug.The real mark of a non-trivial program is that it doesn't work on the first try.It's incredible how Babbage, frustrated that the mass production precision machining technology necessary to make his simple engine work didn't exist yet, decides that the best way forward is to design a new system an order of magnitude more complex and then go to Italy to find more advanced manufacturing somehow.\n \nreply",
      "I had an employee like that.He'd want to do something, and hit a roadblock, so he'd design his own tool (He wrote his own font, once, because he didn't like the way the built-in ones worked at teeny point sizes).Best damn engineer I ever knew, but I had to keep an eye out for rabbitholing.\n \nreply",
      "Obviously yak shaving is a hazard in that it can result in the original project getting abandoned or deadlines being missed, but often the tools you develop along the way are more (economically) valuable than the original project. They're often more widely applicable and narrower in scope, so they're more likely to get done and more likely to find an audience.An example that comes to my mind is the Rust library mio. The Metal database for which it was the I/O component never materialized. But mio is a core component in the Rust ecosystem.Similarly, many applications could benefit from a font that's legible at tiny sizes, not just the one that it was developed for. (Though obviously in most work cultures, this would be considered inappropriate, and for good reasons. My remarks apply mostly to greenfield research/personal projects where deadlines are loose.)\n \nreply",
      "Babbage would have likely had more success if he stayed in England and opened his own precision machine shop.\n \nreply",
      "We had a developer write his own System.out.println in Java. But it wasn't because he didn't like the built-in System.out.println, it was because he didn't know it existed. He had a PhD and was supposedly a senior developer. He didn't last very long.\n \nreply",
      "> (He wrote his own font, once, because he didn't like the way the built-in ones worked at teeny point sizes).That's similar to how Donald Knuth came up with TeX and Metafont.\n \nreply",
      "> He wrote his own font, once, because he didn't like the way...Wonder how many folks here have done the same thing, building and discarding in the throes of creation like tibetan monks:https://en.wikipedia.org/wiki/Sand_mandala\n \nreply",
      "+1 for Babbage/Lovelace history, however IMO, although the two facts are separately true:(1) he was let down by precision machining not existing (Tim Robinson https://www.meccano.us/difference_engines/rde_1/ says that \"I have no doubt that if the Meccano of the 1920's had existed 100 years earlier, Babbage would have been entirely successful in his quest\"), and(2) he designed a more complex system, tried Italy, etc,I don't think it's fair to say that he decided that (2) was the best way forward from (1); it's rather that both were consequences of his ideas outpacing what was realistically feasible: he was a software guy thrust into hardware, coming with up ideas that seemed straightforward and discovering that manufacturing was impossible. Apart from his lack of business/project-planning sense (scope it down; don't aim for 10 digits etc), I think other complicating factors that went into the tragedy of Babbage were:(1) He kept coming up with new/better ideas and pursued them (basically rabbitholing as mentioned),(2) He had won a bunch of awards at a young age simply for proposing the Difference Engine (everyone could see it was a good idea and also seem to have expected it to be straightforward to build: a fait accompli) \u2014 so in the intervening decades he must have felt like he couldn't give up,(3) He got entangled with the government. IMO the tragedy here is that he was just middle-class enough to have a romantic idea of government: while the nobles distrusted government/politics as they sort of looked down on it, and the lower classes distrusted government as it had never done anything much for them, he was of just the right class (his father came from humble origins and had made money in banking) to have patriotic notions of government and all that \u2014 he wanted to offer his invention to \"the nation\" (government), and conversely thought the government \"ought to\" reward him for it, rather than understanding the practical problems of government officials in funding his project. (The government offered to give his invention back to him, but he refused.)(4) Possibly as a result of these awards, he seems to have been attached to the idea of being a \"smart\" person (many examples, e.g. the anecdote quoted in one of the appendices in Sydney Padua's wonderful book, where he refused to judge an award along with Faraday \u2014 he thought he \"deserved\" to be the sole judge) \u2014 this also probably got in the way of doing practical things rather than pie-in-the-sky \"genius-type\" ideas.I think the government entanglement is probably a big part of the story (he asked them basically \"I haven't completed the Difference Engine but I have a much better Analytical Engine that I could implement with more money, what should I do?\" and they sat indecisively for twenty years!), and it's interesting to read his accounts (in his memoirs) vs others', e.g. Lord Playfair's account from the same appendix:> \"He was in chronic war with the Government because it refused to furnish supplies for his new machine, the ground of refusal being that he never completed the first. [\u2026] Babbage always considered himself a badly treated man, and this feeling at last produced an egotism which restricted the numbers of his friends. [\u2026] Babbage, who was delighted with the suggestion, but made it a condition that he alone should be appointed, as a reparation for all the neglect of the Government towards his inventions. Even the association of such a distinguished man as Faraday would take away from the recognition which was due to him.\"Anyway Padua's book (The Thrilling Adventures of Lovelace and Babbage) seems very well-researched (I admit I haven't read much of it but read all the appendices in detail; would strongly recommend anyway).\n \nreply",
      "Reminds me of Bell who was funded to work on an update to the telegraph, but got distracted by his side-project,  the phonograph.\n \nreply",
      "> The real mark of a non-trivial program is that it doesn't work on the first try.not true.\n \nreply"
    ],
    "link": "https://twobithistory.org/2018/08/18/ada-lovelace-note-g.html",
    "first_paragraph": "Two-Bit HistoryTwo-Bit History\n          Computing throughthe ages\n        18 Aug 2018The story of Microsoft\u2019s founding is one of the most famous episodes in\ncomputing history. In 1975, Paul Allen flew out to Albuquerque to demonstrate\nthe BASIC interpreter that he and Bill Gates had written for the Altair\nmicrocomputer. Because neither of them had a working Altair, Allen and Gates\ntested their interpreter using an emulator that they wrote and ran on Harvard\u2019s\ncomputer system. The emulator was based on nothing more than the published\nspecifications for the Intel 8080 processor. When Allen finally ran their\ninterpreter on a real Altair\u2014in front of the person he and Gates hoped would\nbuy their software\u2014he had no idea if it would work. But it did. The next month,\nAllen and Gates officially founded their new company.Over a century before Allen and Gates wrote their BASIC interpreter, Ada\nLovelace wrote and published a computer program. She, too, wrote a program for\na computer that had only"
  },
  {
    "title": "Advent of Code on the Nintendo DS (sailor.li)",
    "points": 28,
    "submitter": "zdw",
    "submit_time": "2024-12-16T22:57:24 1734389844",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=42436440",
    "comments": [
      "Ask HN (comment edition):I know we have plenty of people here doing AoC with their own personal challenges/restrictions on top. What are yours? Solving in an eso-lang? Self-imposed resource constraints (runtime/memory)? Only using Excel? Let's hear 'em.\n \nreply",
      "Thanks article for teaching me about BlocksDS! Looks somewhat more healthy and competent than devkitpro.\n \nreply",
      "I'm such a baby programmer that Advent of Code is still challenging for me, so it's fascinating to see people taking it and exploring all these new languages and platforms. I found (and subsequently lost) someone who had started creating little 8-bit style visualizations from the data outputs.I do feel like programmers are a bit of a masochistic lot when it comes to AoC, though.> The logical way to output the solution for the AoC problem is to create sprites for every digit, upload them to video memory, and arrange the sprites on screen. I'm not going to do that and instead I will use Display Mode 2...\n \nreply",
      "Don't beat yourself up.This guy - \"I do actually know Rust, but I never learned how to use it. I just started writing it because I was born with an innate knowledge of the language, similar to how I know Java or Kotlin despite never having learned them.\"My first words were \"Dada!\". This guys was \"public static void main(String[] args)\"\n \nreply",
      "Don't beat yourself up so much, it's challenging to the majority of developers.The article itself sums things up well \"Most of the puzzles are in the realm of either string processing (somewhat applicable to programming), logic puzzles (not really applicable to most programming), or stupid gotchas in the input format (annoyingly, very applicable to most programming).\"\n \nreply",
      "> I do feel like programmers are a bit of a masochistic lot when it comes to AoC, though.Maybe. But for some it's really just the enjoyment of a good challenge that lets them explore and try out things. I personally love hardware like the DS simply for its accessibility. Modern PCs and consoles are way too complex, while the DS allows you to poke at its guts with no OS or virtualization in the way. It's fun to approach problems like those in AoC in completely novel ways on systems that require and allow you to do things differently.It's not for everyone, sure, but it's certainly not torture for those who do it.\n \nreply"
    ],
    "link": "https://sailor.li/aocnds.html",
    "first_paragraph": ""
  },
  {
    "title": "Ruby Video \u00e2\u20ac\u201c On a mission to index all Ruby conferences (rubyvideo.dev)",
    "points": 148,
    "submitter": "faebi",
    "submit_time": "2024-12-15T13:17:34 1734268654",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=42423253",
    "comments": [
      "Really nice to see a deployed modern Rails site. I just recently decided to try Rails 8 out for a side-project of mine (Paranoia RPG virtual table top) and had a generally pleasant experience.The biggest pain point was the lack of Grade A documentation for the best way to use ActionCable and Turbo \u2013 information is spread out between Rails Guides, API docs, and then the Turbo / Stimulus documentation. The actual API docs do a poor job of explaining basic concepts like \"streamables\", and I kept wondering if I was doing things the \"right\"/idiomatic way.Still, as always, ActiveRecord is my biggest draw for Rails, and the new first-class Sqlite integrations are a huge draw for me. I've yet to find an ORM that allows me to be anywhere near as productive.\n \nreply",
      "Some extra notes: Ruby Video is a Rails application backed by SQLite, its frontend powered by Hotwire. It is open source on GitHub. I\u2019d consider the repo a great resource for anyone looking to learn more about how modern Rails apps are built.https://github.com/adrienpoly/rubyvideohttps://hotwired.dev/\n \nreply",
      "Still sad that the one ruby conference I spoke at \u2014 Steel City Ruby way back in 2014 \u2014 left behind no video recordings due to a mixup between the conference organizers and the venue staff.\n \nreply",
      "This looks awesome and the site looks like it\u2019s custom built. For anyone looking for a tool I\u2019ve been using Pyro (https://www.pyro.app) to collect startup pitch videos (https://video.heystartup.com)\n \nreply",
      "nice ad for pyro.app but yt-dlp + .txt files are free, offline, and will work forever\u00b9 regardless of the operators' wallets.1: notwithstanding storage degradation, but I trust myself more than this platform\n \nreply",
      "Wow this is fantastic. Stealing some ideas from this.\n \nreply",
      "Ruby on Reels! :D Great effort, thanks.I wish there were more sites like this, devoted to particular topics that proved to be valuable enough to be presented in a public talk. I currently use YouTube for this a lot (like most people, I guess). But sadly, count(views) is not always correlated to quality.\n \nreply",
      "Is it just me, or is Ruby experiencing a renaissance of sorts? There has been a bunch of positive press, and the community seems more active and vibrant than ever?\n \nreply",
      "I used to work primarily in Rails from Rails 3 and 4, then stopped to work with Go, then finally Elixir since 2016. For a long while there Rails was left behind with the crazy growth of clientside javascript and react. It was destined to become a BackboneJS/EmberJS of sorts, a once big thing but now dead and legacy.With Rails 8 they really knocked it out of the park. Now there are real compelling reasons to use Rails 8. The job system, the auth generator, the rich text inputs, the file uploads, shit -- they have so much just baked in and omakase'd to death. It's quite an achievement. And it's beautiful to use. You get a phoenix liveview \"feel\" without breaking out of Rails vibes. If that makes sense.I'm using it for a pet project of mine and have a really smooth experience leaving Elixir dayjob and jumping into Rails.\n \nreply",
      "Yupp, been saying the same. I think the Ruby Team did the right thing to focus on performance for Ruby 3.\n \nreply"
    ],
    "link": "https://www.rubyvideo.dev/",
    "first_paragraph": ""
  },
  {
    "title": "Kevin Langdon's 'Omni Magazine' unsupervised IQ test (1979) (lumifont.co.uk)",
    "points": 8,
    "submitter": "walterbell",
    "submit_time": "2024-12-17T00:10:22 1734394222",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42436962",
    "comments": [
      "Those \"spatial reasoning\" questions look a lot like the IQ test I took as a kid (my parent brought me to the psychology department to be tested for some reason).  I really struggled with those and it wasn't until years later, talking with some other folks, that I realized I couldn't visualize mental rotations as well as most people.  because of that, I invested heavily in 3d computer graphics, which helped tremendously when I worked in structural biology.\n \nreply",
      "I was just thinking recently about how much I hate the imposition of time limits on things like this! Although I'm not super interested in figuring out my own or others' IQ, these are a fun set of questions to mull over.\n \nreply",
      "Eh, if one person takes 10 min to solve a problem that takes another person 10 hours, we would usually say the faster individual is more intelligent.\n \nreply",
      "Speed is just one dimension of intelligence, no?Who's more intelligent, person A that would score 80% on a test given 30 minutes, and 85% given 60 minutes; or person B that would score 50% given 30 minutes, but 99% given 60 minutes?Person A might be perceived as quick, have an accurate intuition, and able to come up with a lot of ideas. Person B on the other hand is of the slow and steady kind that will methodically and tirelessly work on a problem until it's solved.\n \nreply"
    ],
    "link": "http://www.lumifont.co.uk/omnitest.php",
    "first_paragraph": "\u00a0First published in the April 1979 edition of Omni magazine \r\n\t\tin an article by Scot Morris. \r\n\t\tWhy have I bothered to reproduce this test? - This IQ test was \r\n\t\tfascinating to me because there is no time limit. Kevin Langdon created \r\n\t\ta beautiful set of questions that are obviously 'quite hard' - hard \r\n\t\tenough to need considerable time to answer. His test was intended to be a thorough \r\n\t\ttest of logical problem-solving ability, rather than factual memory, \r\n\t\tspeed-thinking, general knowledge or literary learning. \r\n\t\t\tStandard IQ tests, such as the famous Stanford-Binet scale, are \r\n\t\t\tusually time-limited and designed to measure scores clustering around 100, the average in the \r\n\t\t\thuman population. Unsurprisingly, there is also considerable interest in tests \r\n\t\t\ttargeted at a higher IQ range - \r\n\t\t\tfor example see Darryl Miyaguchi's compilation at\r\n\t\t\t\r\n\t\t\tUncommonly Difficult IQ tests.\r\n\t\t\t\u00a0The test reproduced here was designed in 1979 by \r\n\t\tSan Fransisco systems analyst"
  },
  {
    "title": "Modelica (modelica.org)",
    "points": 211,
    "submitter": "v9v",
    "submit_time": "2024-12-16T14:22:03 1734358923",
    "num_comments": 67,
    "comments_url": "https://news.ycombinator.com/item?id=42431186",
    "comments": [
      "From the website:> Modelica is a high-level declarative language for describing mathematical behavior. It is typically applied to engineering systems...We use Modelica quite a bit in HVAC industry. In my case (controls engineer), I can request FMUs of various components from systems engineers for optimization work. (Functional Mockup Unit (FMU)[1]: stand-alone binary representing a dynamical system that can be driven by another application). My background is in Reinforcement learning/Model predictive control/python. Having a physics-driven model written in a domain-specific language which I can embed into my python workflow [2] is convenient.I will say, Modelica requires a different perspective from \"regular\" imperative programming (python/matlab). It is a declarative language: you define equations, variables, constraints for a system, regardless of order. The compiler decides how to run the simulation; which variables to solve first etc.While OpenModelica[3] has come a long way towards making an open source implementation of the language standard, proprietary applications (Dymola) still have an edge in the industry.[1]: https://fmi-standard.org/  \n[2]: https://fmpy.readthedocs.io/en/latest/  \n[3]: https://openmodelica.org/\n \nreply",
      "Another up-and-coming solution is Julia's simulation ecosystem [1]. It is powered by the commercial organization behind the Julia programming language, which has received DARPA funding [2] to build out these tools. This ecosystem unifies researchers in numerical methods [3], scalable compute, and domain experts in modeling engineering systems (electrical, mechanical, etc.) I believe this is where simulation is headed.[1] https://juliahub.com/products/juliasim[2] https://news.ycombinator.com/item?id=26425659[3] https://docs.sciml.ai/DiffEqDocs/stable/\n \nreply",
      "JuliaSim looks interesting! From my understanding, it's 100% proprietary/commercial, but built on top of the open source https://github.com/SciML/ModelingToolkit.jl?\n \nreply",
      "I believe it's open source but requires a commercial license; free for academics.\n \nreply",
      "To be slightly more precise, it's not open source, but source available.\n \nreply",
      "Does MATLAB also compete in the same field?\n \nreply",
      "MATLAB/Simulnk is imperative. They have signal flow/causal approach. So you should know ahead of time which variable causes another variable to change i.e. which is defined first.Modelica is acausal. You define the variables and how they are related (equations). The compiler handles variable dependencies and resolution internally.There are pros & cons of each. Both are used for simulating cyber-physical systems.\n \nreply",
      "Sounds like Simulink to me, which is one of the major MATLAB programming environments.ETA: Apparently MathWorks has Simscape in this category.\n \nreply",
      "And Mathematica has this: https://www.wolfram.com/system-modeler/\n \nreply",
      "I would love to use Modelica but in my field Simulink is king\n \nreply"
    ],
    "link": "https://modelica.org/",
    "first_paragraph": "\n        Modelica is an object oriented language to model cyber-physical systems. \n        It supports acausal connection of reusable components governed by mathematical equations to facilitate modeling from first principles.\n      "
  },
  {
    "title": "Ask HN: SWEs how do you future-proof your career in light of LLMs?",
    "points": 206,
    "submitter": "throwaway_43793",
    "submit_time": "2024-12-16T14:11:19 1734358279",
    "num_comments": 441,
    "comments_url": "https://news.ycombinator.com/item?id=42431103",
    "comments": [
      "The last fairly technical career to get surprisingly and fully automated in the way this post displays concern about - trading.I spent a lot of time with traders in early '00's and then '10's when the automation was going full tilt.Common feedback I heard from these highly paid, highly technical, highly professional traders in a niche indusry running the world in its way was:- How complex the job was\n- How high a quality bar there was to do it\n- How current algos never could do it and neither could future ones\n- How there'd always be edge for humansToday, the exchange floors are closed, SWEs run trading firms, traders if they are around steer algos, work in specific markets such as bonds, and now bonds are getting automated. LLMs can pass CFA III, the great non-MBA job moat. The trader job isn't gone, but it has capital-C Changed and it happened quickly.And lastly - LLMs don't have to be \"great,\" they just have to be \"good enough.\"See if you can match the above confidence from pre-automation traders with the comments displayed in this thread. You should plan for it aggressively, I certainly do.Edit - Advice: the job will change, the job might change in that you steer LLMs, so become the best at LLM steering. Trading still goes on, and the huge, crushing firms in the space all automated early and at various points in the settlement chain.\n \nreply",
      "Having also worked on desks in the 00s and early 10s I think a big difference here is what trading meant really changed; much of what traders did went away with innovations in speed. Speed and algos became the way to trade neither of which humans can do. While SWE became significantly more important on trading desks, you still have researchers, quants, portfolio analysts, etc. that spend their working days developing new algos, new market opportunities, ways to minimize TCOS, etc.That being said, there's also a massive low hanging fruit in dev work that we'll automate away, and I feel like that's coming sooner rather than later, yes even though we've been saying that for decades. However, I bet that the incumbents (Senior SWE) have a bit longer of a runway and potentially their economic rent increases as they're able to be more efficient, and companies need not hire as many humans as they needed before. Will be an interesting go these next few decades.\n \nreply",
      "I don't see it.Trading is about doing very specific math in a very specific scenario with known expectations.Software engineering is anything but like that.\n \nreply",
      "\"See if you can match the above confidence from pre-automation traders with the comments displayed in this thread. You should plan for it aggressively, I certainly do.\"Sounds like it was written by someone trying to keep any grasp on the fading reality of AI.\n \nreply",
      "They most insightful thing here would have been to learn how those traders survived, adapted, or moved on.It's possible everyone just stops hiring new folks and lets the incumbents automate it. Or it's possible they all washed cars the rest of their careers.\n \nreply",
      "I like this comment, it is exceptionally insightful.Any interesting question is \"How is programming like trading securities?\"I believe an argument can be made that the bulk of what goes for \"programming\" today is simply hooking up existing pieces in ways that achieve a specific goal. When the goal can be adequately specified[1] the task of hooking up the pieces to achieve that goal is fairly mechanical. Just like the business of tracking trades in markets and extracting directional flow and then anticipating the flow by enough to make a profit is something trading algorithms can do.What trading software has a hard time doing is coming up with new securities. What LLMs absolutely cannot do (yet?) is come up with novel mechanisms. To illustrate that, consider the idea that an LLM has been trained on every kind of car there is. If you ask it to design a plane it will fail. Train it on all cars and plans and ask it to design a boat, same problem. Train it on cars, planes, and boats and ask it to design a rocket, same problem.The sad truth is that a lot of programming is 'done' , which is to say we have created lots of compilers, lots of editors, lots of tools, lots of word processors, lots of operating systems. Training an LLM on those things can put all of the mechanisms used in all of them into the model, and spitting out a variant is entirely within the capabilities of the LLM.Thus the role of humans will continue to be to do the things that have not been done yet. No LLM can design a quantum computer, nor can it design a compiler that runs on a quantum computer. Those things haven't been \"done\" and they are not in the model. The other role of humans will continue to be 'taste.'Taste, as defined as an aesthetic, something that you know when you see it. It is why for many, AI \"art\" stands out as having been created by AI, it has a synthetic aesthetic. And as one gets older it often becomes apparent that the tools are not what determines the quality of the output, it is the operator.I watched Dan Silva do some amazing doodles with Deluxe Paint on the Amiga and I thought, \"That's what I want to do!\" and ran out and bought a copy and started doodling. My doodles looked like crap :-). The understanding that I would have to use the tool, find its strengths and weaknesses, and then express through it was clearly a lot more time consuming than \"get the tool and go.\"LLMs let people generate marginal code quickly. For so many jobs that is good enough. People who can generate really good code taking in constraints that the LLM can't model, is something that will remain the domain of humans until GAI is achieved[2]. So careers in things like real-time and embedded systems will probably still have a lot of humans involved, and systems where every single compute cycle needs to be extracted out of the engine is a priority, that will likely be dominated by humans too.[1] Very early on there were papers on 'genetic' programming. Its a good thing to read them because they arrive at a singularly important point, \"How do you define 'Which is better'?\" For a solid, qualitative and testable metric for 'goodness' genetic algorithms out perform nearly everything. When the ability to specify 'goodness' is not there, genetic algorithms cannot out perform humans. What is more they cannot escape 'quality moats' where the solutions on the far side the moat are better than the solutions being explored but they cannot algorithmically get far enough into the 'bad' solutions to start climbing up the hill on the other side to the 'better' solutions.[2] GAI being \"Generalized Artificial Intelligence\" which will have to have some way of modelling and integrating conceptual systems. Lots of things get better then (like self driving finally works), maybe even novel things. Until we get that though, LLMs won't play here.\n \nreply",
      "Nothing because I\u2019m a senior and LLM\u2019s never provide code that pass my sniff test, and it remains a waste of time.I have a job at a place I love and get more people in my direct network and extended contacting me about work than ever before in my 20 year career.And finally I keep myself sharp by always making sure I challenge myself creatively. I\u2019m not afraid to delve into areas to understand them that might look \u201csolved\u201d to others. For example I have a CPU-only custom 2D pixel blitter engine  I wrote to make 2D games in styles practically impossible with modern GPU-based texture rendering engines, and I recently did 3D in it from scratch as well.All the while re-evaluating all my assumptions and that of others.If there\u2019s ever a day where there\u2019s an AI that can do these things, then I\u2019ll gladly retire. But I think that\u2019s generations away at best.Honestly this fear that there will soon be no need for human programmers stems from people who either themselves don\u2019t understand how LLM\u2019s work, or from people who do that have a business interest convincing others that it\u2019s more than it is as a technology. I say that with confidence.\n \nreply",
      "For those less confident:U.S. (and German) automakers were absolutely sure that the Japanese would never be able to touch them. Then Koreans. Now Chinese. Now there are tariffs and more coming to save jobs.Betting against AI (or increasing automation, really) is a bet against not against robots, but against human ingenuity. Humans are the ones making progress, and we can work with toothpicks as levers. LLM's are our current building blocks, and people are doing crazy things with them.I've got almost 30 years experience but I'm a bit rusty in e.g. web. But I've used LLM's to build maybe 10 apps that I had no business building, from one-off kids games to learn math, to building a soccer team generator that uses Google's OR tools to optimise across various axes, to spinning up four different test apps with Replit's agent to try multiple approaches to a task I'm working on. All the while skilling up in React and friends.I don't really have time for those side-quests but LLM's make them possible. Easy, even. The amount of time and energy I'd need pre-LLM's to do this makes this a million miles away from \"a waste of time\".And even if LLM's get no better, we're good at finding the parts that work well and using that as leverage. I'm using it to build and check datasets, because it's really good at extraction. I can throw a human in the loop, but in a startup setting this is 80/20 and that's enough. When I need enterprise level code, I brainstorm 10 approaches with it and then take the reins. How is this not valuable?\n \nreply",
      "As a senior-ish programmer who struggled a lot with algorithmic thinking in college, it's really awe-inspiring.Truly hit the nail on the head there. We HAD no business with these side-quests, but now? They're all ripe for the taking, really.\n \nreply",
      "quantum computers still can\u2019t factor any number larger than 21\n \nreply"
    ],
    "link": "item?id=42431103",
    "first_paragraph": ""
  },
  {
    "title": "Popeye and Tintin enter the public domain in 2025 along with Faulkner, Hemingway (apnews.com)",
    "points": 203,
    "submitter": "sohkamyung",
    "submit_time": "2024-12-16T10:02:11 1734343331",
    "num_comments": 63,
    "comments_url": "https://news.ycombinator.com/item?id=42429606",
    "comments": [
      "I love Tintin! People in the US grew up with Marvel comic books. We grew up with Tintin, as did our parents before us. Can't wait to see what people do with it.\n \nreply",
      "Same here. I grew up on Asterix & Tintin comics. Some of the most idyllic parts of my childhood.The smell of old books, dusty libraries, and having to pick 1 book per month from what seemed like endless shelves of these comics.\n \nreply",
      "American here; our kids pore over their Tintin books. I suspect we\u2019re part of a trend; unsure how large. I vaguely knew about Tintin growing up but never had the opportunity to read more than a few pages, and no friend ever brought them up.\n \nreply",
      "I also love Tintin and was a big fan as a kid. Recently though, a friend of mine highlighted many of its racist elements stemming from its colonial context -- someone I hadn't fully internalized. Don't want to generate hate for Tintin, I still love it and will introduce it to my kids -- just wanted to highlight this.\n \nreply",
      "Yes, the earlier Tintin comics, (like Tintin in Congo, which I don't think is published any more), are quite blatant in its colonial stereotypes. But to his credit Herge did change his world views (and that of Tintin), to become more \"multicultural\", when he became friends with a chinese artist and sculptor. (See: Tintin, Herg\u00e9 and Chang \u2013 A Friendship That Changed the World - https://thewire.in/books/tintin-herge-and-chang-a-friendship... and the HN discussion on this - https://news.ycombinator.com/item?id=36468028 ).> When Herg\u00e9 published this comic book, he was only 23 years old and wrote it as part of government-led initiative to encourage Belgians to take up commissions in Congo ... Later on his life, Herg\u00e9 would say: \"For the Congo as with Tintin in the Land of the Soviets, the fact was that I was fed on the prejudices of the bourgeois society in which I moved ... It was 1930. I only knew things about these countries that people said at the time: 'Africans were great big children ... Thank goodness for them that we were there!' Etc. And I portrayed these Africans according to such criteria, in the purely paternalistic spirit which existed then in Belgium\".See: Tintin In The Congo, by Herg\u00e9 (Georges Remi) - https://archive.org/details/tintin-in-the-congo-herge-george...\n \nreply",
      "> which I don't think is published any moreIsn't it? Not sure about today but I bought a copy maybe 10 years ago after never having seen it for sale in living memory before that. It did come with some wrapping around the front that said it was of its time though.I got a new French copy more recently than that as well (can't remember when exactly but it may have been in the last two years).\n \nreply",
      "This guy was an asshole, even for the times. https://www.smithsonianmag.com/smart-news/when-nazis-took-be...\n \nreply",
      "> Can't wait to see what people do with it.Yikes.\n \nreply",
      "what do you even mean by this? I can't understand what your take is.\n \nreply",
      "If Tintin in the Congo is the part entering the public domain, perhaps a yikes is in order?\n \nreply"
    ],
    "link": "https://apnews.com/article/public-domain-2025-popeye-tintin-e71ca89b7a430e68e66a7c6ce45a98eb",
    "first_paragraph": ""
  },
  {
    "title": "Quick takes on the recent OpenAI public incident write-up (surfingcomplexity.blog)",
    "points": 52,
    "submitter": "azhenley",
    "submit_time": "2024-12-15T06:01:43 1734242503",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=42421824",
    "comments": [
      "Something doesn't add up - CoreDNS's kubernetes plugin should be serving Service RRs from its internal cache even if APIServer is down because it's using cache.Indexer. The records would be stale but unless their application pods all restarted, which they could not since APIServer was down, or all CoreDNS pods got restarted, which, again, they could not, just records expiring from the cache shouldn't have caused full discovery outage.\n \nreply",
      "wouldn't it be coredns caches the information and records from API server for X amount of time (it seems like this might be 20 minutes?) then once the 20 minutes expired coredns would query api server, receive no response, then fail?I think the idea of just serving cached responses indefinitely when api server is unreachable is what you're describing but not sure if this is default. (and probably has other tradeoffs that I'm not sure about too)\n \nreply",
      "Based on my understanding of the plugin code it is the default. The way cache.Indexer works is it's continuously streaming resources from APIServer using Watch API and updates internal map. I think if Watch API is down it just sits there and doesn't purge anything but I haven't tested that. The 20 min expiry is probably referring to CodeDNS cache stanza which is a separate plugin[0].[0] - https://coredns.io/plugins/cache\n \nreply",
      "I caused an API server outage once with a monitoring tool, however in my case it was a monstrosity of a 20,000 line script. We quickly realized what we had done and turned it off, and I have seen in very large clusters with 1000+ nodes that you need to be especially sensitive about monitoring API server resource usage depending on what precisely you are doing. Surprised they hadn't learned this lesson yet, given the likely scale of their workloads.\n \nreply",
      "> 20,000 line scriptDude.\n \nreply",
      "They meant manuscript I assume.\n \nreply",
      ">  In order to make that fix, we needed to access the Kubernetes control plane \u2013 which we could not do due to the increased load to the Kubernetes API servers.Something that I wish all databases and API servers would do, and that few actually do in practice, is to allocate a certain amount of headroom (memory and CPU) to \"break glass in case of emergency\" sessions. Have an interrupt fired periodically that listens exclusively on a port that will only be used for emergency instructions (but uses equal security measures to production, and is only visible internally). Ensure that it can allocate against a preallocated block of memory; allow it to schedule higher-priority threads. A small concession to make in the usual course of business, but when it's useful it's vital.\n \nreply",
      "Resource exhaustion can be super frustrating, and always feels like a third world slum situation when it happens.Like, why would operating systems allow themselves to run out of headroom entirely in this day and age?\n \nreply",
      "Wow, sounds like a nightmare. Operations staff definitely have real jobs.\n \nreply",
      "splitting the control and data plane is a great way to improve resilience and prevent everything from being hard down. I wonder how it could be accomplished with service discovery / routing.maybe instead of relying on kubernetes DNS for discovery it can be closer to something like envoy.the control plane updates configs that are stored locally (and are eventually consistent) so even if the control plane dies the data plane has access to location information of other peer clusters.\n \nreply"
    ],
    "link": "https://surfingcomplexity.blog/2024/12/14/quick-takes-on-the-recent-openai-public-incident-write-up/",
    "first_paragraph": "Surfing ComplexityLorin Hochstein's ramblings about software, complex systems, and incidents.OpenAI recently published a public writeup for an incident they had on December 11, and there are lots of good details in here! Here are some of my off-the-cuff observations:With thousands of nodes performing these operations simultaneously, the Kubernetes API servers became overwhelmed, taking down the Kubernetes control plane in most of our large clusters.The term saturation describes the condition where a system has reached the limit of what it can handle. This is sometimes referred to as overload or resource exhaustion. In the OpenAI incident, it was the Kubernetes API servers saturated because they were receiving too much traffic. Once that happened, the API servers no longer functioned properly. As a consequence, their DNS-based service discovery mechanism ultimately failed.Saturation is an extremely common failure mode in incidents, and here OpenAI provides us with yet another example. Y"
  },
  {
    "title": "Principles of Educational Programming Language Design (vu.lt)",
    "points": 24,
    "submitter": "azhenley",
    "submit_time": "2024-12-16T02:15:05 1734315305",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=42427428",
    "comments": [
      "I find the core position of the author unconvincing - that is, the author advocates for non-professional languages for beginners, instead using languages designed specifically for teaching. The main argument put forward in favor of professional languages is crossover: if a student learns a language in class, they may be able to use that language professionally. The author then argues against that main point.I think students should be taught in \"professional\" languages, but crossover is not my main reason. Rather, it's that professional languages have an enormous corpus of examples that students can look up. If a student is learning on a teaching language without much adoption, there's just not much else a student can do but use the materials that part of the course. Teaching languages don't let students expand their universe of examples.I agree with the author's point about real insight coming on learning the second (and third, etc.) language and systems. But I don't find it as a compelling point in favor of teaching languages - quite the opposite. To me it means there's no need to obsess over first languages.Designing programming languages is fun. Designing a programming language which meets some platonic ideal of teachability is moreso because it feels possible to \"solve\" the design and craft the perfect jewel of a language. But I'm unconvinced it's useful research.For the record, my first language was C++, and I'd default to teaching beginners in Python.\n \nreply",
      "I also find it more useful to teach something real, not a toy.Even PASCAL was never a toy, though it was designed motivated by teaching purpose, it became professional because it was capable of that.When I told some friends I was going to teach 11-year-olds to program, and that I was considering some BASIC versus Python, they suggested Scratch.  But 11-year-olds aren't babies. They can understand a lot, and they should be enabled to talk about their code (which textual representations enable, but not Scratch-style visual programs).So I picked Python (with pyturtle for easy turtle graphics), and it worked well.\n \nreply",
      "I actually think Scratch is fine for 10ish year olds, mainly because all of my above holds true: scratch.mit.edu is an online community where kids can copy, tweak and in general be inspired by and learn from what other kids have done. Your universe can expand with your curiosity. When my nephew was 10, he started with Scratch. My brother guided him towards using Python on a Raspberry Pi soon after.For kids around 10, I think it's all about what the kid thinks is more fun.\n \nreply",
      "exactly... just getting rid of boilerplate on syntax feels so friendly with the tiny humanswe should also don't forget that learning for its own sake, sometimes, is the meaning/end of pedagogy; not luring them to the professional path of computer science/programming\n \nreply",
      "I would be surprised if your first program was C++?  Specifically, getting a decent C++ toolchain that can produce a meaningful program is not a small thing?I'm not sure where I feel about languages made for teaching and whatnot, yet; but I would be remiss if I didn't encourage my kids to use https://scratch.mit.edu/ for their early programming.  I remember early computers would boot into a BASIC prompt and I could transcribe some programs to make screensavers and games.  LOGO was not uncommon to explore fractals and general path finding ideas.Even beyond games and screensavers, MS Access (or any similar offering, FoxPro, as an example) was easily more valuable for learning to program interfaces to data than I'm used to seeing from many lower level offerings.  Our industries shunning of interface builders has done more to make it difficult to get kids programming than I think we admit.Edit to add: Honestly, I think my kids learned more about game programming from Mario Builder at early ages than makes sense.\n \nreply",
      "> I would be surprised if your first program was C++? Specifically, getting a decent C++ toolchain that can produce a meaningful program is not a small thing?Visual C++ (some version) was in a book I received as a gift in high school, it was my second language after BASIC (some version on a Tandy running MS-DOS). It was not hard to set up. You ran the installer, you had the language set up. If someone had ended up in the same situation as me but without the BASIC experience, I could see it being an easy to set up (not easy to learn) first language.\n \nreply",
      "Apologies for making you prove the statement.  That wasn't my intention.I was musing on how expensive a C++ capable setup was back when I was learning.  I was probably closer to having it as an opportunity than I realize.  But MS Access and the like was already something that was beyond my realistic budget for things.  That was largely helping out with business software friends of the family were using.I am probably also more sour on just how silly difficult it is to put pixels on a screen nowadays.  Python's turtle graphics kind of works ok, if you are only doing turtle graphics.  But just getting a sprite and moving it around can be surprisingly involved, it seems.  I wanted my kids to learn with the Code the Classics book.  May have them give that a try again, soon.  First pass, they all have far more mileage with Scratch.\n \nreply",
      "My first program was indeed C++. In 1998, my high school had a computer lab setup with Turbo C++, and I took a non-AP computer science class. In college, starting in 1999, after entering as a computer science major, we were guided to use Visual C++ on Windows. We got Visual C++ from our department - I can't remember if we paid or if it was just provided to us.\n \nreply",
      "Ah, I see that not only did I accidentally force you to prove this, but I accidentally got others to do so.  My humble apologies on that!I'm not super shocked that some people got started with Visual C++ sooner than I would have had access to it.  It remains surprising to me, though.  See my other post on more of the why, for that.\n \nreply",
      "The abstract asks:> Why do we not have a programming language that is designed for education and in widespread use across the worldIt is important for a teacher to immediately demonstrate subject-matter mastery. If a student asks a question that goes beyond the planned lesson, you need to have an answer. You can't say, \"I don't know how to do that.\" That would make you look incompetent.When you're teaching programming, it is easiest to do this with a programming language that you know well and use everyday. That language is unlikely be a language designed explicitly for education.\n \nreply"
    ],
    "link": "https://infedu.vu.lt/journal/INFEDU/article/797/info",
    "first_paragraph": "Article infoviewsFull articleviewsPDFdownloadsXMLdownloads\n\nShare\n\n\nRSS\n"
  },
  {
    "title": "Using Guile for Emacs (lwn.net)",
    "points": 117,
    "submitter": "todsacerdoti",
    "submit_time": "2024-12-16T15:37:07 1734363427",
    "num_comments": 39,
    "comments_url": "https://news.ycombinator.com/item?id=42432004",
    "comments": [
      "\"what are the differences between resurrected GuileEmacs that was also announced in EmacsConf2024 and Gypsum? At first glance seems like both projects have the same goal.\"Response from @ramin_hal9001@fe.disroot.org@ram535  thanks for asking! The goal for both projects are similar, but they are achieved in slightly different ways.Gypsum is a clone written in Scheme, meaning it is software the behaves exactly like Emacs, but it is written from scratch in a new code base. In this case, it is also being written in a completely different programming language, Scheme instead of C. The larger goal is to have an Emacs that is backward compatible with GNU Emacs but is written in Scheme that runs on any R7RS standard compliant Scheme implementation. There is no C code in this project at all, it is purely Scheme. I would like to also target other compilers such as MIT Scheme, Gambit, Stklos, and possibly Chicken and Larceny as well, though this will be pretty difficult and rely on a lot of cond-expand code. The larger goal is to have an Emacs app platform that encourages the use of the Scheme language for creating applications and text editing work flows, regardless of the underlying compiler.@lispwitch \u2018s \u201cGuileEmacs\u201d is not a clone, but a fork of both GNU Emacs and GNU Guile, meaning it modifies the existing GNU Emacs code base and some of the Guile source base, replacing some of the C source code in GNU Emacs with other C source code from Guile. Then, the Emacs Lisp interpreter written in C is replaced with an Emacs Lisp interpreter written in Guile Scheme. This allows Emacs Lisp to be JIT compiled using Guile\u2019s JIT compiler, and also make use of all of the Guile software ecosystem to extend Emacs. This is incredibly useful, because there is quite a lot of Guile software, including things like web servers and game engines, and soon it could all be available for use by Emacs programmers. It will probably also be production ready much sooner than my Gypsum project because it only needs to implement the core of Emacs Lisp to work. However, it relies on language features specific to Guile to achieve this, so it is not fully R7RS standards compliant, and will not work on other Scheme implementations.\n \nreply",
      "Have you experimented with Edwin?Edwin is a clone of GNU Emacs version 18 written in MIT Scheme.https://www.gnu.org/software/mit-scheme/documentation/stable...\n \nreply",
      "Can anyone tell me (a GNU Emacs user for 35+ years) something that I can't do in elisp but would be able to do if Guile was used instead?\n \nreply",
      "One example that comes to mind is the delimited continuation primitive (what Guile calls a \"prompt\" [0]) that would allow for lightweight concurrency in elisp code, something that elisp is not good at today.[0] https://www.gnu.org/software/guile/manual/html_node/Prompts....\n \nreply",
      "there is a presentation on youtube (a new one)https://www.youtube.com/watch?v=yjC162DnsKI&t=594sfor me the most interesting point, guile will allow emacs to be more lispy, replace c code with guile code and integrate with CLISP and CLISP libraries\n \nreply",
      "For people who are more involved in this world than me: What're the chances of actually getting a supported full feature parity version of guile emacs? Like not anytime soon but, say, in the next 10 years.It's my understanding it would be a significant speed improvement but the amount of work and the - ahem - stallman factor are big roadblocks.\n \nreply",
      "My recollection of past guile-emacs stuff is that rms is not a blocker here.\n \nreply",
      "What's your rush, sonny?  You say 'not anytime soon' but then you say 'next 10 years'.  In the world of GNU software, to say 'glacial pace' is basically asking 'what's your hurry?'  Fine wine, fine wine... give it at least 30-40 years...\n \nreply",
      "The biggest roadblocks are 1) lack of continued effort and 2) reticence on the Emacs side. I'd say 1) is the most important by far.\n \nreply",
      "Although I think I understand Stallman and his role in Emacs pretty well, I cannot guess as to what \"Stallman factor\" refers to in this context.\n \nreply"
    ],
    "link": "https://lwn.net/SubscriberLink/1001645/b1e4453a8c6c16d7/",
    "first_paragraph": "\n\n\n\nWelcome to LWN.net\n\nThe following subscription-only content has been made available to you \nby an LWN subscriber.  Thousands of subscribers depend on LWN for the \nbest news from the Linux and free software communities.  If you enjoy this \narticle, please consider accepting the discount offer on the right.  Thank you\nfor visiting LWN.net!\n\n\nSpecial discount offer\n\nSubscribe to LWN now at the\n           \"professional hacker\" level for at least six months,\n           and you will\n           receive a special discount of 25%.\n           \n\n\n\n\n\n\n\n           By Jake EdgeDecember 16, 2024\n           \nEmacsConf\n\n\nEmacs is, famously, an\neditor\u2014perhaps far more\u2014that is extensible using its own\nvariant of the Lisp programming language, Emacs\nLisp (or Elisp).  This year's\nedition of EmacsConf, which is an annual \"gathering\" that has been held\nonline for the past five years, had two separate talks on using a different\nvariant of Lisp, Guile,\nfor Emacs.  Both projects would preserve Elisp compati"
  },
  {
    "title": "U2's Larry Mullen Jr: my dyscalculia makes 'counting like climbing Everest' (thetimes.com)",
    "points": 25,
    "submitter": "bookofjoe",
    "submit_time": "2024-12-14T13:39:15 1734183555",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=42417056",
    "comments": [
      "https://archive.ph/vGH8O",
      "I have this, I was diagnosed after a formal neuropsych eval at the hospital after many years struggling with math.The numbers I can work around, everyone keeps a calculator in their pockets these days. What sucked was that it's not a common diagnosis, so I was constantly berated and insulted for being bad at math, as if I didn't apply myself or work hard enough.\n \nreply",
      "This is a very personal story and I\u2019ll keep it personal because I don\u2019t know what anyone else\u2019s lived experience is. If you\u2019re young and this resonates for you, you\u2019ll figure it out.My undiagnosed dyscalculia combined with an unnecessarily difficult calculus course at mun.ca cost me my degree. All other requirements met.Yeah I am throwing shade because I was made to feel stupid by callous faculty when I fully understood why you\u2019d want tests for sequences and series but just couldn\u2019t hack those exams and assignments where they threw three curveballs per problem.I grew to understand trig in high school by programming a rotating cube on a Sharp calculator. It ran at .5 fps. My teacher passed me because it was clear I had a grasp.I would have done better at UofT - I compared their curriculum after the fact. One maybe two curve balls per question. Just couldn\u2019t afford it.I still have to use my fingers to count basic addition and subtraction. I can\u2019t read an analog clock unless I really think about it. I have immense problems remembering the names of people I just met.Luckily, at the time, I had a history of good IT admin work to make up for it.And 25 years later, I am very happy with how my career has gone, no letters to my name.It\u2019s good to see someone else\u2019s famous and major success despite such a burden.For anyone else, get whatever exemption you can. Lord knows the golf caddy of their rich parents\u2019 friends do the same.I\u2019d be as bitter as the prof who smoked cigars in his office, but my Cayley medal still hangs on the wall as a reminder of something that would take way too long to put to words.\n \nreply",
      "https://www.clac.ca/Your-voice/Article/wired-to-work-differe...\n \nreply",
      "Maybe it was Larry who actually wrote the opening words to the song Vertigo.\n \nreply",
      "I don't have dsycalculia, but I can feel for him.  I have a terrible working memory, so simple calculations in my head are very difficult.  I always struggled with math in school.  Even though the concepts weren't hard for me to understand, I'd always get the wrong answer because my arithmetic was a disaster.  Spelling was also always a disaster, because I couldn't keep track of where I was in the word.\n \nreply",
      "out of curiosity, was this only something while trying to \"do it in your head\" or also when doing it long form on paper. which is a question I was asked a lot, but I hated. Even when doing long form on paper, there's still numbers floating in my head.It took until I started receiving partial credit for showing work before I'd stop doing the entire thing in my head. Bad math teachers thought I was cheating while the good teachers saw me working it out but convinced me partial credit was a good thing.\n \nreply",
      "This makes me wonder if in spite of all these years of education by play, Elementary Piagetian* education is still overlooking mental methods of arithmetic that musicians use. Musicians have an easy time understanding structure - and also knowing how to learn by repetition (to tell a student to repeat something is often a task in itself)!* https://en.wikipedia.org/wiki/Piaget%27s_theory_of_cognitive...\n \nreply",
      "If he can't count, how does he know where he is in a measure?\n \nreply",
      "Feel. I think it's mostly the case that people who play rock are not constantly counting to figure out where they are in a measure. In fact, I'd imagine that's the case for a lot of jazz players too.\n \nreply"
    ],
    "link": "https://www.thetimes.com/culture/music/article/larry-mullen-jr-u2-dyscalculia-mxlwq5swz",
    "first_paragraph": "Larry Mullen Jr, the U2 drummer, has revealed that he cannot count or add numbers because he suffers from dyscalculia \u2014 a learning difficulty that affects numeracy. The acclaimed musician compared \u201ccounting bars\u201d of music to \u201cclimbing Everest\u201d as he revealed his diagnosis for the first time in an interview with Times Radio. Dyscalculia is a learning disability affecting a person\u2019s capacity to learn skills like telling the time or reading music. People with dyscalculia find it difficult to comprehend arithmetic. Mullen, 63, said: \u201cI\u2019ve always known that there\u2019s something not particularly right with the way that I deal with numbers. I\u2019m numerically challenged.\u201cAnd I realised recently that I have dyscalculia, which is a sub-version of dyslexia. So I can\u2019t count [and] I can\u2019t add.\u201dMullen said that this difficulty explained the \u201cpained\u201d look on his face while drumming \u2014 which has been noted by concerned fans of U2, one of the world\u2019s most influential rock bands. Mullen founded U2 with singe"
  }
]